\section{Experimental Details}
\label{app:experimental_details}
\subsection{Benchmark details}
\label{app:benchmark_details}
\input{tab_tex/benchmark}

\textbf{Reproduction of small MAE models.} The MAE-T and MAE-S are reproduced following the settings in \citet{mae}. We make several adjustments to the decoder to make it suitable for small encoders. For both MAE-S and MAE-T, the decoder includes 8 transformer blocks, each with 8 attention heads. The decoder dimensions in MAE-S and MAE-T are 256 and 192, respectively.

\textbf{Implementation details.} The weights of all pre-trained models are downloaded from corresponding official repositories. The models are trained for 100 epochs using MMSegmentation \citep{mmseg}. For different methods and model sizes, we keep the learning rate constant and sweep the layerwise decay rate across \{0.5, 0.65, 0.75, 0.85, 1.0\}. To adapt models pre-trained on three-channel RGB images for single-channel infrared images, we duplicate the infrared images three times to create pseudo-three-channel images.

\subsection{Evaluation metrics}
\label{app:appendix_evaluation}

\textbf{Fine-tuning.} \textit{Fine-tuning} is the default evaluation metric in this work, which utilizes the pre-trained model as the backbone of existing semantic segmentation models. Following previous works \citep{mae, iBOT}, we employ UperNet \citep{upernet} as the semantic segmentation model. As illustrated in \figref{fig:ft_lp}\textcolor{red}{a}, to build the feature pyramid based on the non-hierarchical ViT model, features from different layers are passed through the MaxPooling layers or DeConv layers, to obtain features of different resolutions. These multi-scale features are then input into the decoder for segmentation results. Following \citet{mae} and \citet{iBOT}, we use features of the \{4, 6, 8, 12\} layers in ViT-T, ViT-S, and ViT-B, and the features of the \{8, 12, 16, 24\} layers in ViT-L, to build the feature pyramid. Remarkably, in fine-tuning, all parameters including the pre-trained model, the feature pyramid, and the decoder, are tuned with the labeled downstream datasets. Hyperparameters are listed in \tabref{tab:setting_seg}.

\textbf{Linear Probing.} As mentioned above, \textit{fine-tuning} introduces additional learnable parameters and alters the pre-trained feature representation. Its performance may not fully reflect the characteristics of the pre-trained features. Therefore, \textit{linear probing} is also employed as an evaluation metric. As shown in \figref{fig:ft_lp}\textcolor{red}{b}, features from different layers are resized to $1/4$ of the input resolution and then concatenated together. Finally, a linear head ($1\times1$ conv) utilizes these concatenated features to predict segmentation results. Notably, only the linear head is trainable, while all other parameters are frozen. The layer settings of output features are the same as \textit{fine-tuning}.

\textbf{Fine-tuning (LL-FPN).} This metric is discussed in \secref{sec:distill}, which aims to enhance the fine-tuning performance of UNIP models by using the last layer to obtain features of different resolutions, as depicted in \figref{fig:ft_lp}\textcolor{red}{c}. Specifically, we employ the features of the \{12, 12, 12, 12\} layers in ViT-T, ViT-S, and ViT-B, and the features of the \{24, 24, 24, 24\} layers in ViT-L, to build the feature pyramid. Other settings remain the same as \textit{fine-tuning}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/ft_lp_v7.pdf}
\vspace{-20pt}
\caption{Illustrations of different transfer architectures for semantic segmentation tasks.}
\label{fig:ft_lp}
\vspace{-8pt}
\end{figure}

\textbf{Layerwise Linear Probing.} This metric is a layerwise version of the \textit{linear probing} metric. It is designed to assess the pre-trained feature representation at each layer. As shown in \figref{fig:ft_lp}\textcolor{red}{d}, only the features of a single layer are forwarded to the linear head following the resize operation. Other settings are the same as \textit{linear probing}.

\input{tab_tex/segmentation_setting}


\subsection{Evaluation datasets}
\label{app:evaluation_datasets}

\textbf{SODA} \citep{soda}. This dataset features a variety of indoor and outdoor scenes. It comprises 1,168 training images and 1,000 test images, spanning 20 distinct semantic categories, including road, building, car, chair, lamp, table, monitor, and others.

\textbf{MFNet} \citep{mfnet}. This dataset focuses on RGBT semantic segmentation for automotive driving scenarios and includes 1,569 image pairs of infrared and RGB images. It is divided into 784 training images, 392 validation images, and 393 test images, covering 8 semantic categories such as car, person, bike, curve, and others. When benchmarking the performance of different pre-training methods, we combine the validation set with the test set, resulting in a larger test set of 785 images. When comparing UNIP models with other SOTA semantic segmentation models, we follow their settings, \ie, using the original 393 test images for evaluation.

\textbf{SCUT-Seg} \citep{mcnet}. This dataset includes 1345 training images and 665 test images in nighttime driving scenes. It has 10 classes including road, person, fence, pole, and others.

\textbf{ADE20K} \citep{ade20k}. ADE20K is a large-scale RGB semantic segmentation dataset, covering a variety of scenes from indoor to outdoor and nature to urban. It consists of 20,210 training images and 2,000 test images, with 150 different semantic categories.

\textbf{ImageNet-1K} \citep{imagenet}. ImageNet-1K is a subset of the ImageNet database, consisting of 1,000 categories with roughly 1.2 million training images, 50,000 validation images, and 100,000 test images. It is widely used in computer vision research like image classification and pre-training.

\subsection{UNIP.}
\label{app:pre-training}

\input{tab_tex/vits}


\textbf{Head Misalignment.} To solve the head misalignment between teacher and student models during distillation, we experiment with two methods. (1) The first method is the adaptive block proposed in  \citet{tinymim}. Specifically, during distillation, the number of attention heads in the student model's last layer is adjusted to be the same as that of the teacher model by changing the head dimension while keeping the overall dimension constant. When performing fine-tuning or linear probing on downstream tasks, the number of attention heads is reverted to the standard setting in \tabref{tab:vit_config}. (2) The second method involves adding a self-attention layer at the end of the student model during distillation. The number of attention heads in the extra attention layer is equivalent to the teacher model's. This layer is removed when transferring to downstream tasks. These two methods achieve similar performance, but the latter consumes slightly more training time. Therefore, we use the first method in practice.

\textbf{Feature Distillation.} For the feature distillation in \tabref{tab:target_ablation}, we employ a linear projection layer to match the dimension of the student model to that of the teacher model. The distillation and fine-tuning settings are the same as UNIP. The loss function is the cosine similarity loss between the $L_2$ normalized student feature $l_2(F_T)$ and teacher feature $l_2(F_S)$: 
\begin{align}
    L=1-\cos(l_2(F_T)\cdot l_2(F_S)).
\end{align}

\textbf{Implementation Details.} All experiments are conducted using the PyTorch toolkit \citep{pytorch} on 8 NVIDIA RTX 3090 GPUs. The default settings are shown in \tabref{tab:distill_setting}. We use the linear \textit{learning rate} scaling rule: $lr=base\_lr \times$ batchsize / 256, following \citet{mae}. The semantic segmentation settings of UNIP models are the same as those in \Appref{app:appendix_evaluation}.





