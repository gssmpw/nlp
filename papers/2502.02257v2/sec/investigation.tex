\section{What matters for infrared semantic segmentation?}
\label{sec:investigation}
To determine which characteristics of the pre-trained models are critical for infrared semantic segmentation, we analyze different models from multiple perspectives.

\subsection{The Pre-training tasks influence attention patterns}
\label{sec:attention_pattern}
The self-attention mechanism is a key component of ViT. In semantic segmentation tasks, the spatial interactions between tokens are crucial. Thus, we visualize the attention maps of pre-trained models. 

\textbf{Attention maps of supervised/CL and MIM methods differ significantly.} As shown in \figref{fig:query_attn}\textcolor{red}{a}, DINO-S exhibits three distinct attention patterns: (1) \textit{Local}: In shallow layers (Layer 5), different query tokens focus only on their spatially nearby key tokens; (2) \textit{Hybrid}: In middle layers (Layer 9), query tokens attend to both nearby tokens and foreground tokens; (3) \textit{Global}: In deep layers (Layer 12), different query tokens all focus on foreground tokens with nearly identical attention maps, a phenomenon known as \textit{attention collapse} \citep{ssl_vit}. This attention pattern distribution is consistent across different sizes in CL and supervised methods, as shown in \figref{fig:query_attn_cl}. However, in MIM methods like MAE, the distribution varies. In MAE-S (\figref{fig:query_attn}\textcolor{red}{b}), attention maps are mainly \textit{local}, with slight \textit{hybrid} patterns emerging in deep layers. Conversely, in MAE-L (\figref{fig:query_attn}\textcolor{red}{c}), shallow and deep layers exhibit \textit{local} patterns, while middle layers show \textit{hybrid} patterns. CKA \citep{CKA} analysis in \Appref{app:CKA} reveals similar phenomena regarding feature representation.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/query_attn_v9.pdf}
\vspace{-20pt}
\caption{Attention maps for different query tokens in three representative layers. Each query token's attention map corresponds to a row in the attention matrix, averaged over different heads.}
\label{fig:query_attn}
\vspace{-8pt}
\end{figure}


\textbf{Differences in attention patterns stem from the pre-training tasks.} CL methods, similar to supervised approaches, treat views from the same image as belonging to the same class. This setup encourages models to focus on foreground tokens, as images in the same class often share similar foreground objects but may differ in background. Consequently, attention maps in later layers present \textit{global} patterns. The \textit{local} and \textit{hybrid} patterns can be regarded as the intermediate states in forming the \textit{global} pattern. This high-level pre-training task causes models of different sizes and methods to have similar pattern distributions across layers.
In contrast, pre-training tasks of MIM methods focus on reconstructing features or raw pixels of masked tokens, which is a relatively low-level task relying heavily on spatially nearby tokens. Consequently, models are not compelled to capture global image information, leading small models to primarily exhibit \textit{local} patterns. In larger models like MAE-L, the increased representation capacity allows \textit{hybrid} patterns to spontaneously emerge in the middle layers to capture broader context. In deep layers near the decoder, \textit{local} patterns reappear to support the pre-training task of reconstructing nearby masked tokens.

As a supplement, iBOT exhibits similar patterns with DINO in shallow and middle layers but shows less \textit{attention collapse} in deep layers (see \figref{fig:query_attn_cl}). This can be attributed to that iBOT combines DINO with masked feature prediction \citep{iBOT}, which encourages the later layers to leverage spatial information to predict features of masked tokens.


\subsection{How to quantitively identify different attention patterns?}
\label{sec:nmi}


Three distinct attention patterns are qualitatively summarized in \secref{sec:attention_pattern}, prompting the question of whether a metric can quantitatively measure them. Attention distance measures the average distance 
\begin{wrapfigure}{r}{0.32\textwidth}
\vspace{-1pt}
\centering
\includegraphics[width=1.0\linewidth]{figures/nmi_v9.pdf}
\vspace{-23pt}
\caption{NMI on ImageNet.}
\label{fig:nmi}
\vspace{-11pt}
\end{wrapfigure}
between the query and key tokens, while attention entropy implies the concentration of the attention distribution. However, both metrics depict the relationship between one query and multiple key tokens and are unable to reflect differences in the attention maps of various queries. \textbf{We find that the normalized mutual information (NMI) between query and key tokens is an effective indicator.} The calculation process is elaborated in \Appref{app:NMI}. Let $A\in \mathbb{R}^{N\times N}$ denote the attention matrix, where $N$ is the number of tokens. The NMI is a function of $A$, ranging from 0 to 1.  We highlight two special cases to clarify NMI: (1) When query tokens focus solely on their spatially corresponding key tokens (an extreme \textit{local} pattern), $A$ becomes an identity matrix. Thus the joint probability of query and key tokens is equivalent to their marginal probability and the NMI reaches its maximum value of 1; (2) When all query tokens attend to the same key tokens (an extreme \textit{global} pattern), each row of $A$ is identical. Consequently, the probability distribution of query and key tokens are independent, leading to the minimum NMI of 0.


\textbf{Therefore, \textit{local} patterns have larger NMI values, while \textit{global} patterns exhibit lower ones.} As illustrated in \figref{fig:nmi}, the NMI of DINO-S and iBOT-S decreases with depth and consistently stays below that of MAE models. Especially, the NMI of DINO-S approaches 0 in later layers, revealing its \textit{attention collapse}. In contrast, the NMI of MAE-L first decreases and then increases, due to the \textit{hybrid} patterns in middle layers and \textit{local} patterns in later layers. 


\subsection{The Hybrid pattern matters for semantic segmentation}
\label{sec:lg_matter}
Semantic segmentation is a dense prediction task where all pixels in an image are classified into different semantic classes. Local information is crucial as nearby pixels usually belong to the same class, while global cues are also essential since instances of the same class may appear in different positions within the image. Therefore, we hypothesize that \textbf{\textit{hybrid} patterns, which capture both local and global information, are more important for semantic segmentation than purely \textit{local} or \textit{global} patterns}. To demonstrate this, we conduct the \textit{layerwise linear probing} (LLP) experiments, where frozen features of only one layer are passed to the linear head, as shown in \figref{fig:ft_lp}\textcolor{red}{d}.

\textbf{The LLP performance peaks where \textit{hybrid} attention patterns emerge.} As shown in \figref{fig:layerwise_lp}, supervised and CL methods peak at about three-quarters of the model's depth. Large MIM models (ViT-L) perform better in the middle layers. These peaks commonly occur near the \textit{hybrid} patterns. In contrast, the performance of small MIM models (ViT-S and ViT-B) gradually increases with depth, peaking in the last two layers. This is because, although all layers exhibit \textit{local} patterns, deep layers focus more on foreground tokens (\figref{fig:query_attn}\textcolor{red}{b}) and have smaller NMI values (\figref{fig:nmi}), leading to better LLP performance. Additionally, the performance degradation in iBOT's deep layers is less pronounced than that in DINO and supervised methods. This aligns with observations that iBOT's deep-layer attention maps contain more local information (\secref{sec:attention_pattern}) and have larger NMI values than those of DINO (\figref{fig:nmi}), underscoring the importance of the \textit{hybrid} attention pattern.



\textbf{This hypothesis can explain the phenomena in \secref{sec:benchmark}.} Small MIM models struggle to learn the \textit{hybrid} pattern, resulting in a notable performance gap compared to supervised and CL methods. Conversely, large MIM models successfully develop the \textit{hybrid} pattern, making their fine-tuning performance comparable to other methods. iBOT performs best across different model sizes and evaluation metrics because the \textit{hybrid} pattern occurs more frequently than in other methods.



\subsection{The Texture bias hinders the model's generalization on infrared images}
\label{sec:texture}


\begin{wraptable}{r}{0.51\textwidth}
    \vspace{-7.5mm}
    \caption{The FT performance on RGB and infrared semantic segmentation datasets.}
    \label{tab:rgb_infrared}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{1.0mm}{
    \scalebox{1.0}{
    \begin{tabular}{l c c c c c}
        \toprule
        \multirow{2}{*}{Methods} & \multicolumn{2}{c}{RGB} & \multicolumn{3}{c}{Infrared} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-6}
        & ADE20K & MFNet-RGB & MFNet-T & SODA & SCUT-Seg \\
        \midrule
        DeiT-B & 47.4 & 57.07 & \textbf{48.59} & 69.73 & 69.35 \\
        DINO-B & 46.8 & 55.20 & 48.54 & \textbf{69.79} & \textbf{69.82} \\
        MAE-B & \textbf{48.1} & \textbf{57.29} & 46.78 & 68.18 & 67.86 \\
        \bottomrule
    \end{tabular}}}
    \vspace{-4mm}
\end{wraptable}

When transferring RGB pre-trained models to infrared tasks, the distribution shift between these modalities significantly impacts performance. A major difference between RGB and infrared images is that RGB images can capture fine-grained textures, which are scarce in infrared images. Therefore, we assume that \textbf{the model's bias towards texture would exacerbate the distribution shift, thereby impairing the transfer performance on infrared tasks.}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/layer_lp_v9.pdf}
\vspace{-20pt}
\caption{The layerwise linear probing performance of different methods on SODA \citep{soda}.}
\label{fig:layerwise_lp}
\vspace{-8pt}
\end{figure}

According to \citet{ssl_vit}, MIM methods are texture-biased while CL and supervised methods are shape-biased. This bias is evident in attention maps in \figref{fig:query_attn}, where MAE models focus on textures while DINO emphasizes edges. We conduct experiments to investigate the bias's impact on infrared segmentation. As shown in \tabref{tab:rgb_infrared}, MAE-B outperforms DeiT-B and DINO-B on RGB datasets like ADE20K \citep{ade20k} and MFNet-RGB \citep{mfnet}, but consistently underperforms on infrared datasets. Notably, the paired MFNet-RGB and MFNet-T share the same scenario and image counts, differing only in modality. This indicates that MAE models pre-trained on ImageNet rely on low-level texture information to reconstruct masked patches, leading to poor generalization on texture-less infrared images. Therefore, \textbf{reducing the texture bias is a promising way to enhance the transfer performance of RGB-pre-trained models on infrared tasks.}
