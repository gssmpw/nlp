\def\draft{1} % a flag for turning on/off authors' comments
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{array}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{tcolorbox}
\usepackage{amsmath}
\tcbuselibrary{skins}


% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{cleveref}

\title{The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models}


\author{
Artem Kirsanov \\
New York University \\
\texttt{kirsaa01@nyu.edu}
\\\And
Chi-Ning Chou \\
Flatiron Institute 
\\\And
Kyunghyun Cho \\
New York University \\
Genentech
\\\And
SueYeon Chung \\
New York University \\
Flatiron Institute
}


\newcommand{\Cnotes}[1]{\ifnum\draft=1{\color{red} [CNC: #1]}\fi}
\newcommand{\Anotes}[1]{\ifnum\draft=1{\color{blue} [AK: #1]}\fi}
\newcommand{\Snotes}[1]{\ifnum\draft=1{\color{magenta} [SYC: #1]}\fi}


\begin{document}
\maketitle

\begin{abstract}

Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights the critical role of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.

\end{abstract}


\section{Introduction}

A striking feature of modern language models (LMs) is their computational flexibility. Unlike traditional neural networks trained for specific tasks, LMs function as flexible computers that can be programmed (prompted) with natural language to perform a wide array of tasks. 

This adaptability, often termed in-context learning (ICL), has revolutionized natural language processing by enabling rapid task adaptation without expensive fine-tuning. However, despite ICL's widespread success, its underlying mechanisms remain poorly understood.

While some research has linked ICL to gradient-based learning \cite{vonoswald2023transformerslearnincontextgradient, akyürek2023learningalgorithmincontextlearning}, recent evidence in naturalistic settings suggests that ICL may not be pure "learning", but rather a method of steering the model to select familiar tasks from its pretraining corpus \cite{pan2023incontextlearninglearnsincontext, hendel2023incontextlearningcreatestask}. Recent studies have also highlighted importance of prompt design and demonstrated that the choice of examples and output labels can significantly impact performance \cite{zhao2021calibrateuseimprovingfewshot, min2022rethinkingroledemonstrationsmakes}. However, these works have primarily focused on the input-output behavior of LMs, leaving the internal dynamics of ICL largely unexplored.

In this work, we aim to illuminate ICL by investigating how different prompting methods modify internal representations in pre-trained language models. When a model is prompted to perform a classification task, we analyze the separability and geometric properties of category manifolds --- point clouds in the model's embedding space corresponding to examples sharing a category label. We leverage the recently developed framework of \textbf{manifold capacity} \cite{chung2018classification, chou2024neural}, which analytically connects task performance to the geometric properties of these representations.

Our core contributions are:

\begin{enumerate}

    \item A comprehensive analysis of how various prompting methods affect internal representations in language models, revealing distinct computational mechanisms despite similar performance outcomes.
    
    \item Novel insights into in-context learning dynamics, including the role of label semantics, synergistic effects of demonstrations on unrelated tasks, and representational trade-offs during task adaptation.
    
\end{enumerate}

\section{Related work}

\subsection{Prompting as task-adaptation}

The idea that a language model pretrained on next-token prediction can adapt to various tasks without parameter updates was popularized by \cite{brown2020languagemodelsfewshotlearners}. This phenomenon, known as in-context learning (ICL), relates to the model's ability to effectively "learn" a novel task by analogy from a few demonstration examples provided in the input sequence. To distinguish conventional few-shot ICL from other recently proposed input-based task-adaption methods, we refer to it as providing \textbf{demonstrations}, highlighting the crucial role of task examples.

While performance generally improves with more examples \cite{brown2020languagemodelsfewshotlearners, bertsch2024incontextlearninglongcontextmodels}, ICL exhibits counter-intuitive features, with performance being heavily dependent on the exact choice of examples, their ordering, formatting, and other factors \cite{zhao2021calibrateuseimprovingfewshot, wang2024largelanguagemodelslatent, liu2024understandingincontextlearningcontrastive}. Additionally, the actual input-output mapping matters less than expected \cite{min2022rethinkingroledemonstrationsmakes}, suggesting that few-shot ICL involves a complex interplay of true task learning from examples and task recognition from the pre-training corpus \cite{pan2023incontextlearninglearnsincontext}.

Language models also demonstrate zero-shot learning abilities, performing tasks based on abstract descriptions without explicit examples \cite{Radford2019LanguageMA, wei2022finetunedlanguagemodelszeroshot}. We refer to such task-adapting prompts without examples as \textbf{instructions}\footnote{We use "instruction" referring only to the format of the prompt for zero-shot learning and do all experiments on base models that were not instruction fine-tuned}. While often considered together under the umbrella of ICL, our results reveal that despite comparable performance, these two prompt types affect internal representations differently, highlighting the crucial role of input distribution examples.

Recently, prompt-tuning has emerged as an alternative approach to task adaptation \cite{lester2021powerscaleparameterefficientprompt, liu2022fewshotparameterefficientfinetuningbetter}. This method involves learning a small set of continuous vectors (soft prompts) that are concatenated to the input embeddings, while keeping the model parameters frozen. Prompt-tuning offers a middle ground between full model fine-tuning and static prompting, allowing for task-specific adaptations with significantly fewer trainable parameters.

\subsection{Internal representations}
Language computations rely on mapping individual words or tokens to vectors in a continuous embedding space, which possesses rich structure learned through model pretraining. The emerging \textit{linear representation hypothesis} \cite{park2024linearrepresentationhypothesisgeometry} suggests that this embedding space contains "feature directions" encoding human-interpretable concepts, allowing the model to perform vector operations with meaningful semantics \cite{mikolov2013efficientestimationwordrepresentations,pennington-etal-2014-glove, bowman2016generatingsentencescontinuousspace}.

The concept of feature superposition \cite{elhage2022toymodelssuperposition,arora2018linearalgebraicstructureword} provides insight into how a model can operate on more features than it has orthogonal directions in the embedding space. This is achieved by utilizing almost-orthogonal vectors for feature encoding with minimal interference, potentially circumvented by non-linear activation functions.

A popular method for uncovering encoded features involves training linear probes \cite{belinkov2021probingclassifierspromisesshortcomings} to "read out" information linearly from the embedding space. Probing methods have revealed the encoding of part-of-speech tags \cite{belinkov-etal-2017-neural}, parse-tree geometry \cite{hewitt-manning-2019-structural}, and higher-level semantic features such as spatial location of landmarks \cite{gurnee2024languagemodelsrepresentspace} and color \cite{abdou-etal-2021-language}. However, while these studies are usually performed on and averaged over a very diverse input corpus of text, there is a lack of understanding how the context preceding a given input (particularly, task adaptation) affects feature representation.

\subsection{Representational geometry}

The notion that underlying representations in the embedding space shape task performance has gained traction in both machine learning and computational neuroscience \cite{chungNeuralPopulationGeometry2021,fleschOrthogonalRepresentationsRobust2022, ansuiniIntrinsicDimensionData, fawziEmpiricalStudyTopology2018}. Intuitively, for a classification task, this implies that collective representations of inputs sharing a target category (a category manifold) must be well-separated from other categories. This concept of "manifold untangling" has been a prominent perspective on computational objectives in neuroscience \cite{dicarloUntanglingInvariantObject2007}.

The recently developed framework of manifold capacity \cite{chung2018classification, wakhloo2023linear, chou2024neural} proposes a formal link between representational geometry and separability. Manifold capacity quantifies how efficiently task-relevant features are encoded from the perspective of a linear downstream decoder. Essentially, it measures the separability of target classes in the embedding space, capturing the effectiveness of task-relevant feature encoding.

This framework has been successfully applied to investigate representational geometry in vision networks \cite{stephenson2019untangling, cohenSeparabilityGeometryObject2020, stephenson2021geometrygeneralizationmemorizationdeep} and language models \cite{mamou2020emergence}. By examining how different prompting methods affect manifold capacity, we can gain insights into the internal dynamics of ICL and the efficiency of various task adaptation strategies.

\section{Methods}

\subsection{Dataset details}

To investigate effects of various prompting methods on representations in different task-specific contexts, we required a dataset with control over multiple categorical dimensions of text. We could not find an existing text classification dataset with a sufficient number of samples and a comprehensive multilabel scheme suitable for tractable manifold analysis. Therefore, we leveraged a separate language model (Claude 3.5 Sonnet) to generate a synthetic dataset tailored to our research requirements. This synthetic dataset consists of diverse sentences, each simultaneously labeled with three types of categories: Sentiment, Topic, and Intent, with five categories for each type. Such multidimensional labeling allowed us to investigate representational effects in a multitasking setting (see sections \ref{multitasking} and \ref{prompt-tuning}).

For consistency, all experiments, including those focused on single-task performance (section \ref{ICL-single-task}), utilized this dataset, with the sentiment classification task serving as our primary focus. To validate our findings, we also replicated key single-task experiments using established open datasets as a control.

Full details on the datasets, including generation process, category distributions, and example sentences, are provided in the \cref{sec:appendix_dataset_details}.

\subsection{Task setup}

Our work focuses on text  classification tasks with a fixed set of categories, as such tasks have an analytically-grounded link between the geometry of underlying representation and separability of categories in the embedding space, ultimately determining the end performance. 

In contrast to traditional encoder-based models, where separate linear classifiers are trained to predict target category directly from the embedding vectors, we investigate decoder-only language models. These models can prompted to generate class labels directly in the vocabulary space.

This approach introduces two key factors affecting performance:
\begin{enumerate}
\item \textbf{Representation Quality}: The underlying representation in the embedding space must support the separation of class manifolds.
\item \textbf{Readout Alignment}: The alignment between the model's unembed layer and the ideal decoder directions impacts the final output quality.
\end{enumerate}

Manifold capacity theory allows us to disentangle these components by quantifying the representation quality at each layer, independently of the specific unembed module being used for vocabulary readout. This idea is schematically illustrated in \cref{fig:readout_representation_schematics}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/readout_representation_schematics.png}
    \caption{Two components of the model’s performance. Low accuracy can be caused by either suboptimal and tangled representation in the embedding space (left), as well as misalignment between the representation and model’s readout layer (right). Manifold capacity, which relates the performance of an ideal decoder to the underlying geometry can differentiate between the 2 cases.}
    
    % ~\Cnotes{Maybe add texts in the figure on something like manifold untangle (check or not), readout alignment (check or not)}
    
    \label{fig:readout_representation_schematics}
\end{figure}


\subsection{Prompting strategies}
Throughout the work we compared two main types of natural-language prompting. \textbf{Instruction} prompt consisted of the following text (using sentiment as an example): 

\begin{tcolorbox}[  enhanced,
  interior style={
    top color=gray!5,
    bottom color=gray!5,
  },
  frame style={
    color=gray!35,
  },
  left=2pt,    % left padding
  right=2pt,   % right padding
  top=2pt,     % top padding
  bottom=2pt,  % bottom padding
]
  This is a text classification task. Possible categories are Joy, Sadness, Fear, Anger, Surprise.
  
  Text: \textit{[Test Sentence]}
  
  Category:
\end{tcolorbox}

where \textit{[Test Sentence]} stands for the sentence text from the dataset that is being evaluated.

\textbf{Demonstration} prompt consisted of a variable number of examples following a similar format:

\begin{tcolorbox}[
  enhanced,
  interior style={
    top color=blue!2,
    bottom color=blue!5,
  },
  frame style={
    left color=blue!10,
    right color=blue!10,
    middle color=blue!10
  },
  left=2pt,    % left padding
  right=2pt,   % right padding
  top=2pt,     % top padding
  bottom=2pt,  % bottom padding
]
Text: \textit{[Demo sentence 1]}

Category: Joy

Text: \textit{[Demo sentence N]}

Category: Fear

Text: \textit{Test Sentence}

Category:
\end{tcolorbox}

As a baseline control for the representation analysis, we also extracted embedding using the \textbf{raw sentence} input of the following format:

\begin{tcolorbox}[  enhanced,
  interior style={
    top color=gray!2,
    bottom color=gray!2,
  },
  frame style={
    color=gray!10,
  },
  left=2pt,    % left padding
  right=2pt,   % right padding
  top=2pt,     % top padding
  bottom=2pt,  % bottom padding
]
  Text: \textit{[Test Sentence]}
  
  Category:
\end{tcolorbox}

\subsection{Embedding Extraction}
Analyzing representational geometry in decoder-only models presents unique challenges due to masked self-attention, distributed sentence-level features, and last token dependency. To address these challenges and investigate the effects of prompting on representations, we consider two types of embeddings:
\begin{enumerate}

\item \textbf{Sentence Embeddings}: We extract and residual stream activations for tokens corresponding only to the input sentence, excluding the task prompt, and average their embedding vectors along sequence dimension. This provides insight into the model's intermediate processing stage.

\item\textbf{Last-token Embeddings}: We extract residual stream activations of the last token in the sequence at each layer. This allows us to track how sentence-level features are aggregated into the final representation used for output generation.

\end{enumerate}

These embedding types and possible effects of prompting are illustrated in \cref{fig:two_embeddings_schematics}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/two_embeddings_schematics.png}
    \caption{Possible effect sites of prompting. Task-specific prefix might affect extraction of relevant features at the sentence-level, reorganizing intermediate representations (top). High performance would also imply more efficient repackaging of extracted features into the embedding of the last token, as well readout alignment (bottom).}
    \label{fig:two_embeddings_schematics}
\end{figure}


\subsection{Analysis of representations}

To analyse representational geometry we first construct category manifolds (point clouds) by accumulating the embedding vectors of all sentences sharing a class label. So for a classification task with $P$ categories, the resulting representation can be thought of as $P$ distinct collections of vectors in the embedding space. We then compute the following properties of the resulting collective representation.

\paragraph{Manifold capacity} 

Capacity is a positive scalar measure, that measures how separable the underlying category manifolds are, with higher capacity values corresponding to higher degree of separability. Intuitively, it can be thought of as the "number of linearly decodable classes per dimension", quantifying how efficiently manifolds are packed in the embedding space \cite{gardner1988optimal, chung2018classification}. We provide a more mathematically detailed explanation of one interpretation of capacity in \cref{appendix:manifold_capacity}, and for full rigorous treatment, refer the reader to  \cite{chou2024neural}.

\paragraph{Geometry of individual manifolds}

In this work we make a few simplifications, compared to the original formulation \cite{chou2024neural}.  Manifold capacity is analytically expressed as a function of so called \textit{effective} radius and dimension of manifolds, that are determined by the spatial arrangement of manifolds' anchor points, that can be thought of as support vectors for the classification problem. In the presence of correlated structure, these measures might have complicated form, not necessary corresponding to intuitive notions of radius and dimension. To bring our results into a more direct interpretation, we measure geometry in the following way instead:

\begin{enumerate}
    \item \textbf{Dimension} of each manifold was measured as participation ratio of principal components,  which roughly corresponds the number of dimensions needed to explain around 80--90\% of total variance \cite{gaoTheoryMultineuronalDimensionality2017}.
    \item \textbf{Radius} of each manifold was taken to be the maximum distance between any pair of points on the manifold.
\end{enumerate}

Both metrics were averaged across manifolds, each resulting in a single scalar value. We refer to these measures as geometric properties of individual manifolds, since they do not depend on the relative positions and orientations of manifolds in the embedding space. 


\paragraph{Correlation structure}

Manifold capacity also depends on the spatial arrangement of individual manifolds relative to each other and to the global origin. We measure correlation coefficients between axes of variation of individual manifolds (\textbf{Axes-alignment}) and correlations between each manifold's axes and its centroid (\textbf{Center-axes alignment}). For an extended discussion of how these correlation measures affect manifold capacity in different regimes, see \cite{chou2024neural}. In this work, we consider these measures collectively as \textit{correlation structure} to explain capacity changes driven by the relative arrangements of manifolds in the embedding space, rather than by changes in individual manifold properties.

\section{Results}

Our analysis reveals complex dynamics in how prompting affects the internal representations of language models, with distinct patterns emerging at different processing stages and for various prompting methods.

\subsection{Representational changes during text-classification task}
\label{ICL-single-task}

We first investigated performance and representational effects of prompting during a conventional ICL setting, comparing demonstrations and instruction prompts.

\paragraph{Task performance} 
Instruction alone achieved good accuracy, outperforming demonstration prompts with few examples ($\leq 5$). Larger example sets ($>5$) surpassed explicit instruction, with performance quickly plateauing (\cref{fig:performance_barplots_emotion}). Replacing meaningful category words (gold labels) with abstract letters required more demonstration examples to infer category nature.  When category labels were consistently shuffled (e.g. "Joy" $\rightarrow$ "Anger"), the model failed to generalize beyond pretrained associations, achieving low accuracy for both target (shuffled) and original labels. This suggests that the model is not purely learning a novel task from scratch, but rather (at least partially) relies on existing associations encoded in label semantics. \cite{pan2023incontextlearninglearnsincontext}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/performance_barplots_emotion.png}
    \caption{Performance of demonstrations and instruction prompting on sentiment analysis task.}
    \label{fig:performance_barplots_emotion}
\end{figure}

\paragraph{Sentence-level effects}

Analysis of sentence-level embeddings (\cref{fig:geometry_mean_pooled_emotion}) revealed that demonstration examples, but not abstract instruction, significantly reorganized intermediate representations at early-mid layers. This reorganization increased the separability of sentiment manifolds by reducing manifold dimension and improving correlation structure (see \cref{fig:ICL_sentence_level_extended}). Surprisingly, there was little difference in resulting geometry between demonstrations across three labeling strategies, indicating that sentence representation is primarily influenced by input distribution examples, rather than input-output mapping.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/geometry_mean_pooled_emotion.png}
    \caption{Manifold capacity of \textbf{sentence-level} embeddings during demonstrations prompting compared to instruction and raw sentence control}
    \label{fig:geometry_mean_pooled_emotion}
\end{figure}

\paragraph{Last-token effects}

At the last-token level, instruction prompts significantly increased manifold capacity relative to raw sentences, with effects emerging as early as layer 8 and persisting to final layers (\cref{fig:geometry_last_token_emotion}). Geometrically, the increased separability was driven mostly by the reduction in dimension along with correlation structure (supplementary \cref{fig:ICL_last_token_extended}). Demonstrations further increased manifold capacity compared to instruction, despite lower task performance for cases with few examples. This suggests that while instruction alone achieves better accuracy due to high alignment between the model's readout and category manifolds, demonstrations improve both readout alignment and representation structure. Even just for a couple of demonstrations, the underlying representation is already more optimal compared to instruction-prompted case, but this separability is not utilized properly by the unembed layer. Notably, last-token capacity during letter code labeling was much lower compared to category words, explaining lower performance when output labels lack meaningful semantics. For shuffled labels capacity values were similar to the gold label setting, suggesting that model's inability to overwrite existing associations is explained by the readout misalignment, while the underlying representation is intact.
 
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/geometry_last_token_emotion.png}
    \caption{Manifold capacity of \textbf{last token} embeddings during demonstrations prompting compared to instruction and raw sentence control.}
    \label{fig:geometry_last_token_emotion}
\end{figure}

\paragraph{Sensitivity to the choice of demonstrations}

Performance of few-shot ICL has been previously reported to depend heavily on the choice of particular examples and their ordering, even for a fixed number of demonstrations provided \cite{zhao2021calibrateuseimprovingfewshot}. To investigate whether such failure modes of certain training sets stem from changes in the underlying geometry, we analyzed the relationship between last-token manifold capacity and end performance across multiple random samplings of demonstrations, while keeping the number of examples fixed. In accordance with prior work, we observed large variance in performance (\cref{fig:ICL_stability} left), particularly in settings with fewer examples. For instance, with five demonstrations, accuracy varied dramatically from below 0.1 to approximately 0.6. Despite this substantial performance variability, the changes in manifold capacity of the last token embedding at the final layer were minimal. Even in "failure" runs with lowest accuracy, manifold capacity was significantly higher than in the instruction setting, and the layer-wise profile of capacity in the worst runs was nearly identical to the best runs (\cref{fig:ICL_stability} right). These results provide further evidence that the instability of few-shot ICL and its sensitivity to particular examples is driven primarily by poor readout alignment rather than differences in representational geometry

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/ICL_stability_plot.png}
    \caption{Left: Manifold capacity at the final layer versus accuracy for individual ICL runs with different numbers of demonstrations compared to instruction. Right: Layer-wise capacity profiles for the five best- and worst-performing 5-demonstration runs compared to instruction (accuracies shown in brackets)}
    \label{fig:ICL_stability}
\end{figure}



\subsection{Cross-Task Interactions in Multi-Task Prompting}

\label{multitasking}

\paragraph{Multi-task setup} We then investigated whether prompting a model to perform one task would affect the quality of representation for another unrelated task. To this end, we constructed an artificial sentence-classification dataset containing three independent sets of labels, each with five categories. This design allowed each sentence to be classified by its sentiment, topic, or intent (see \cref{sec:appendix_dataset_details} for details).

We created instruction and demonstration prompts for each of the three tasks. We then computed representational metrics for each of the three possible sets of manifolds, resulting in nine possible pairs between a prompt and a representation. We termed the three cases where the manifold-inducing labels coincided with the classification objective (e.g., performing sentiment analysis and computing separability of sentiment manifolds) as \textbf{\textcolor[HTML]{2f7139}{coherent}}. The remaining six cases, where manifold capacity was evaluated for a different set of labels, were termed \textbf{\textcolor[HTML]{9e281e}{incoherent}}.

This setup allowed us to explore how prompting for one task affects the model's internal representations not just for that task, but also for other potential tasks on the same input. All experiments used gold category labels, and manifold metrics for each configuration were normalized by the corresponding value in the raw sentence case.

\paragraph{Synergistic Effects at the Sentence Level} 

Increasing the number of demonstrations robustly led to increased manifold capacity at intermediate layers for coherent configurations, while instruction had a much weaker effect (\cref{fig:multitasking_sentence_level}). Surprisingly, demonstrations for an incoherent task also increased capacity with a similar layerwise profile, albeit to a lesser extent. This highlights the role of input distribution: providing example sentences enhances representation capacity for supporting other tasks on the same input distribution, even in the context of a different task. While the trend of increased capacity with growing number of examples was similar for both coherent and incoherent scenarios, the amplitude of such increase was larger when the task was coherent with the manifold labels. Notably, while the overall trend is captured by the decrease in manifold dimension, the difference between coherent and incoherent settings is not fully explained by the geometry of individual manifolds (supplementary \cref{fig:multitask_extended_sentence}). Instead, it likely arises due to changes in the correlation structure and relative positions of manifolds in the embedding space.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/multitasking_sentence_level.png}
    \caption{Effect of prompting in a multitask setting at sentence-level}
    \label{fig:multitasking_sentence_level}
\end{figure}

\paragraph{Task Interference in Last Token Representations} 
 
Analysis of last token embeddings revealed an interesting dichotomy of layerwise dynamics (\cref{fig:multitasking_last_token}). At earlier layers, additional demonstrations of incoherent tasks increased manifold capacity, but at later layers, this trend reversed, with additional examples decreasing capacity. Coherent demonstrations significantly increased capacity starting with layer 12 and persisting to the final layer. The increase in capacity driven by coherent prompts at intermediate layers was much more prominent, compared to incoherent prompts, indicating a larger role of task-specific input-output pairings. Decrease in capacity at final layers with growing number of demonstrations suggests an intriguing idea of representational tradeoff:  as the model prepares the output, features for irrelevant tasks, that were emphasized at intermediate processing stages are compromised in favor of better separability of task-relevant features. Interestingly, this effect could not be explained by the geometry of individual manifolds --- we observed a reduction in dimension with increased number of examples for both coherent and incoherent tasks. Instead, we observed that center-axes correlations behaved differently for coherent and incoherent cases, capturing the trend in capacity (see supplementary \cref{fig:multitask_extended_last_token}).

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figures/multitasking_last_token.png}
    \caption{Effect of prompting in a multitask setting at last-token level}
    \label{fig:multitasking_last_token}
\end{figure}

\subsection{Distinct representational mechanisms of prompt-tuning}


\label{prompt-tuning}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/prompt_tuning_wide.png}
    \caption{Schematic of the prompt-tuning setup (A) and performance at various tasks for different lengths of the soft prompt (B). Right: Manifold capacity changes during training across layers for sentence-level (C) and last token (D) representations}
    \label{fig:prompt_tuning}
\end{figure*}

\paragraph{Performance and Setup of Soft Prompts}

Finally, we extended our investigation to \textbf{prompt-tuning}, an alternative method of task adaptation \cite{lester2021powerscaleparameterefficientprompt} that optimizes a task-specific prompt directly in the embedding space (hence "soft"), which is prepended to the test input (\cref{fig:prompt_tuning} A). This approach allowed us to examine whether gradient-based methods for task adaptation affect internal representations similarly to traditional prompting methods.

To validate performance, we trained separate soft prompts of varying lengths for all three tasks, computing test accuracy across intermediate training iterations (\cref{fig:prompt_tuning}, B). Notably, soft prompts consistently outperformed demonstrations ((see \ref{sec:appendix}), and we found no significant correlation between final performance and prompt length, consistent with prior work \cite{lester2021powerscaleparameterefficientprompt}. 
Given the similarity in performance and representational effects across different prompt lengths, we present representative plots for a 5-token soft prompt in the following analysis.

\paragraph{Minimal Impact on Intermediate Representations}  Analysis of sentence-level representations during soft prompt training revealed a striking mechanistic difference compared to hard natural-language prompts (\cref{fig:prompt_tuning} C). The optimization-based solution did not alter intermediate representations in earlier layers, as illustrated by the absence of the characteristic peak around layer 12 observed with other prompting methods. Instead, effects were concentrated in later layers, where we observed a similar representational trade-off even at the sentence level: prompts for incoherent tasks led to decreased capacity. Importantly, this capacity difference could not be explained by the geometry of individual manifolds, suggesting the critical role of relative manifold arrangement  (see \cref{fig:prompt_tuning_extended_sentence}).

\paragraph{Enhanced Trade-off in Last Token Representations} 

At the last token level, prompt-tuning also exhibited distinct effects. For coherent prompts, manifold capacity increased substantially in later layers as training progressed, surpassing instruction-prompted capacity but remaining below that induced by demonstrations. Notably, this effect emerged at later layers compared to both instruction and demonstration methods. In the incoherent case, soft prompts dramatically reduced the capacity of representations for unrelated tasks. This suggests that gradient-based input optimization compromises the representation of task-irrelevant features even more than natural-language demonstrations. As with demonstrations, the capacity difference between coherent and incoherent settings was primarily attributable to the relative alignment of manifolds in the embedding space, rather than geometry of individual manifolds ( \cref{fig:prompt_tuning_extended_last_tooken}).

Taken together, our results on prompt-tuning indicate that soft-prompts, often proposed to be alternative to demonstration-based ICL, operate through fundamentally different internal mechanisms compared to demonstrations and zero-shot instruction.

\section{Discussion}

Our study illuminates the mechanisms of how language models adapt to various tasks by analyzing the geometry of internal representations under different prompting methods. We found that zero-shot instruction, few-shot demonstrations, and tunable soft-prompts, while achieving comparable performance, operate through distinctly different representational mechanisms.

Zero-shot instructions, while effective, primarily influence the final stages of processing, affecting how features are "packaged" in the last token embedding without significantly altering intermediate representations. In contrast, demonstration examples have a more profound impact, reshaping intermediate representations to optimize them for the classification objective. In a multitask setting, demonstrations optimize early-layer representations to support multiple potential tasks, regardless of the specific task being demonstrated. This suggests a form of general feature enhancement triggered by exposure to diverse input examples. Soft-prompts, despite being trained on the same input distribution examples, operate differently, mainly affecting later layers responsible for output preparation, distinguishing them from the broader impact of natural language demonstrations.


A key insight emerging from our analysis is the distinction between representational geometry and readout alignment in determining model performance. Manifold capacity measures the inherent separability of category representations and their potential for supporting robust classification across all possible linear readouts. However, actual model performance also depends on a specific readout -- the model's unembed layer --- which may fail to optimally utilize well-structured representations. This effect manifests itself in two notable "failure modes" of few-shot ICL --- dramatic sensitivity to the choice and ordering of specific examples and the inability to generalize beyond label associations in the pretraining corpus. In both cases, the internal representations remain well-organized for classification, but the unembed layer fails to effectively leverage this structure, resulting in poor accuracy. High separability, as measured by manifold capacity, suggests that one could train a simple linear readout module on top of existing representations to overcome this, leveraging the feature-extraction power of decoder-only LLMs for efficiently adapting their representations to specific tasks. The success of prompt-tuning further supports this view: its effectiveness appears to stem primarily from improving the alignment between representations and the vocabulary readout layer, rather than fundamentally altering the geometric organization of the embedding space.


These findings suggest two promising directions for future research. First, given that internal representations often maintain high manifold capacity even when ICL performance is poor, there is significant potential in better understanding and optimizing readout alignment. Quantifying decoder alignment by comparing the performance of independently trained classifiers with the model's unembed layer could provide deeper insights into this bottleneck and suggest ways to overcome it. Second, our observation that demonstrations can drastically change representational geometry suggests opportunities for more direct geometric optimization. Recent work has shown promising results in related fields: optimizing vision network parameters to directly maximize manifold separability has achieved SoTA performance \cite{yerxa2023learning}, while regularizing learned embeddings to respect structural characteristics has improved performance in causal inference tasks \cite{balashankar-subramanian-2021-learning}. We anticipate that similar insights into LM representational geometry could drive innovations in language model prompting, enhancing performance and stability across a wide range of objectives.

\section*{Limitations}

Our study provides insights into the representational geometry of language models under different prompting methods, but it has limitations. First, we used synthetic datasets generated by Claude 3.5 Sonnet, which allowed precise control over task parameters. However, this approach may not fully capture the complexity and variability of real-world language structure. To enhance the generalizability of our findings, future research should expand testing to include a broader range of natural datasets.

Second, the metrics used to quantify representational geometry in our study, such as manifold capacity and individual manifold geometry, though informative, simplify the more complex tasks that occur in language models, by focusing on a classification task with given target labels. Future work should examine how other tasks, such as those requiring multi-token outputs (e.g., chain-of-thought prompting), affect representational geometry. Additionally, more advanced measures that link geometry to complex computations could provide further insights into the fine-grained changes during task adaptation.

\section*{Acknowledgments}

This work was funded by the Center for Computational Neuroscience at the Flatiron Institute of the Simons Foundation. S.C. is supported by the Klingenstein-Simons Award, a Sloan Research Fellowship, NIH award R01DA059220, and the Samsung Advanced Institute of Technology (under the project "Next Generation Deep Learning: From Pattern Recognition to AI"). All experiments were performed on the Flatiron Institute's high-performance computing cluster.

\bibliography{acl_latex}

\appendix


\include{latex/appendix/appendix_dataset_details}

\subsection{Models}
All experiments presented in the main text were performed on Llama3.1 8b base model \cite{dubey2024llama3herdmodels} (32 layers, 4096 embedding dimension). We also repeated the results with Gemma2 (2b base model) \cite{gemmateam2024gemma2improvingopen} (26 layers, 2304 embedding dimension). Results are presented in the \cref{sec:supplementary_plots}.


\input{latex/appendix/appendix_methods}

\subsection{Computational resources}
All experiments were performed on a high-performance computing cluster, using Nvidia H100 GPUs, resulting in total of 1000 GPU hours.


\newpage


\subsection{Supplementary plots}

To maintain a reasonable number of figures in the paper, we present a curated subset in this appendix, highlighting key points with representative plots. The complete set of figures, detailing geometric measures for all combinations of models, datasets, and tasks, along with the source code, will be available on GitHub. The repository will be made public upon publication.

\label{sec:supplementary_plots}



\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/appendix/llama3_ICL_performance.png}
    \caption{\textbf{Llama3.1-8b} performance of demonstrations and instruction prompts on open datasets (ag\_news and TREC coarse) and on all three subtasks of the synthetically generated multitask dataset (sentiment, topic and intent).}
    \label{fig:ICL_extended_performance_llama}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/appendix/gemma2_ICL_performance.png}
    \caption{\textbf{Gemma2-2b} performance of demonstrations and instruction prompts on open datasets (ag\_news and TREC coarse) and on all three subtasks of the synthetically generated multitask dataset (sentiment, topic and intent).}
    \label{fig:ICL_extended_performance_gemma}
\end{figure*}


\include{latex/appendix/appendix_ICL_single_task_geometry_figures}


\include{latex/appendix/appendix_multitask_geometry}

\label{sec:prompt-tuning-sentence-level}

\include{latex/appendix/appendix_prompt_tuning_figures}


\end{document}
