\subsection{Methods}

\subsubsection{Embedding Extraction Methodology}
\label{appendix:embedding_extraction}


\paragraph{Challenges in Decoder-Only Models}

\begin{enumerate}
    \item \textbf{Masked Self-Attention:} In decoder-only models, each token's embedding is limited to information from itself and preceding tokens. This requires the model to progressively accumulate and propagate relevant contextual information along the sequence, influencing how global features are represented across different token positions.

    \item \textbf{Distributed Sentence-Level Features:} Unlike models with dedicated [CLS] tokens, global sentence-level features (such as sentiment) might be distributed across embedding vectors of intermediate tokens.

    \item \textbf{Last Token Dependency:} The model's output is a function of the last token's embedding vector only, implying that task-relevant features must be aggregated and represented in this final embedding for good task performance.

\end{enumerate}


\paragraph{Sentence Embeddings}
To examine how task-specific prompts influence feature extraction and computation on intermediate tokens, we construct sentence embeddings as follows:

\begin{enumerate}
    \item We extract residual stream activations at each layer for tokens corresponding only to the input sentence, excluding the task prompt itself. This ensures that the resulting embeddings for each sentence are of the same length across different prompting conditions.

    \item We perform mean-pooling across these embedding vectors to obtain a fixed-size embedding for each sentence.
\end{enumerate}

While this method differs from using dedicated sentence-level embeddings, it provides insight into the model's intermediate processing stage. Based on the idea of feature superposition, we hypothesize that directions in the embedding space corresponding to task-irrelevant token-level features will be averaged out, while task-relevant global features (which might be distributed among various tokens in the sentence) will be preserved or enhanced through mean-pooling.


\paragraph{Last-token Embeddings}
While mean-pooled embeddings allow us to capture an intermediate processing stage, the underlying sentence tokens are not immediately utilized for the task. To understand the model's final representation before output generation, we also extracted residual stream activation of the last token in the sequence at each layer. The last token is special because, for the model to perform the task, all relevant sentence-level features must get "packaged" into the embedding vector of the last token via self-attention. By analyzing last token embeddings across layers, we can track at what point such feature repackaging takes place to collect information about the sentence.


\subsubsection{Manifold Capacity}
\label{appendix:manifold_capacity}

This section provides additional background on the idea of manifold capacity. Consider a set of $N$ points in $D_\text{emb}$-dimensional space: $\vec x_i \in \mathbb R^{D_\text{emb}}$. Each point has a corresponding class label $l_i \in \{1,\dots P\ \}$. Capacity measures how well a particular representation supports linear separability of a random one-vs-rest label dichotomy that doesn't break category boundaries. Namely, for $P$ classes there are $P$ possible dichotomies: $\{ y_i^\mu \}$, where $i \in \{1,\dots N\ \}$ – index of a data point, $\mu \in \{1,\dots P\ \}$ – index of a dichotomy, and:
$$
\begin{cases} y_i^\mu = 1 \ \operatorname{if} \left( l_i = \mu \right)  \\
y_i^\mu = -1 \ \operatorname{otherwise}
\end{cases}
$$
Consider performing a random projection of data into a $D_\text{proj}$- dimensional space, where $D_\text{proj} \leq D_\text{emb}$. We can compute a probability that a randomly chosen dichotomy will be linearly separable, when the data is projected randomly to $D_\text{proj}$ dimensions, formalized as follows:

$$
F(D_\text{proj}) = \displaystyle \mathop{\operatorname{Pr}}_{\substack{
S\sim \mathcal N^{(D_\text{proj}, D_\text{emb})} \\
                  \mu \sim \text{Unif.}(\{1 \dots P\})}
 \\ } \left[  \exists \vec w: \ y_i^\mu \vec w S \vec x_i \geq 0 \ \forall i 
\right]
$$

Where $\vec w\in D_\text{proj}$. In a thermodynamic limit of $N, P \to \infty$, $F(D_\text{proj}$) undergoes a sharp phase transition from $0$ to $1$ as $D_\text{proj}$ interpolates between $0$ and $D_\text{emb}$. In the finite data case, the transition is smooth, but we can still detect an approximate critical dimension $D^*$, that corresponds to the inflection point of $F(D_\text{proj})$. Then, manifold capacity $\alpha$ is defined to be
$$
\alpha = \frac{P}{D^*}
$$

Intuitively it captures decoding efficiency, quantifying how many dimensions are sufficient for a downstream readout to perform classification. $\alpha$ depends on the geometry of individual manifolds (such as radius and dimension), as well as relative positioning and alignment of different class manifolds in the embedding space.

\subsubsection{Manifold dimension}

We use the participation ratio (PR) as a proxy for manifold dimensionality, as described in \cite{gaoTheoryMultineuronalDimensionality2017}. For a manifold $\mathbf{X} \in \mathbb{R}^{(N, D)}$ consisting of $N$ points in a $D$-dimensional space ($N\leq D$), the participation ratio is defined as:
$$
\operatorname{PR} = \frac{\left( \sum_i^N \lambda_i  \right)^2}{\sum_i^N \lambda_i^2 }
$$
where $\lambda_i$ is the $i^\text{th}$ eigenvalue of the manifold covariance matrix $\mathbf{X} \mathbf{X}^T$.
Intuitively, PR measures how evenly the total variance is distributed among individual principal components. Lower values of $\operatorname{PR}$ indicate a more rapid decay of covariance eigenvalues, signifying lower effective dimensionality. We compute the PR for each manifold and then average these values to obtain a single measure of dimensionality for the entire representation.
 
\subsubsection{In-context learning}

\paragraph{Demonstration Prompts}
We constructed demonstration prompts by randomly sampling sentences from the training split. The number of examples varied from 1 to 40, ensuring as uniform a label coverage as possible. For instance, in a 4-category classification task with 10 demonstration examples, 8 examples were guaranteed to cover all categories equally (2 per category), with the remaining 2 examples randomly chosen. We computed the forward pass of the model with 3 random seeds for each number of demonstrations and reported averaged measures across these runs.

\paragraph{Accuracy Measurement}
We measured accuracy as the proportion of test sentences for which the token with the highest logit value corresponded to the first token of the target output (for cases where the target label was tokenized into multiple tokens). Importantly, we considered logits for the entire vocabulary, not restricting the scope to target outputs. If the highest probability output was a token not corresponding to any class label, the run was treated as incorrect, irrespective of relative logit values of other tokens.

\subsubsection{Prompt-tuning}

\paragraph{Description}
We replicated our main experimental setup, replacing natural language task instructions and demonstrations with tunable prompts of varying lengths. A tunable prompt $\mathbf{X}$ (also referred to as a soft prompt) of length $l$ is a matrix in the model's embedding space $\mathbb{R}^{l \times D_\text{emb}}$, where $D_\text{emb}$ is the dimensionality of the model's token embeddings.

Unlike discrete text prompts, these tunable prompts are continuous vectors that can be optimized directly through gradient descent. They provide a more flexible way to convey task-specific information to the model, unconstrained by the token embedding matrix. This allows them to occupy highly specific regions of the embedding space that are inaccessible through natural language input alone.

\paragraph{Soft-prompt methodology}

\begin{enumerate}
    \item \textbf{Initialization}: Each tunable prompt $\mathbf{X}$ was initialized using the embedding vector of the word ``Category''. For soft prompts with $l > 1$, this embedding vector was repeated $l$ times along the sequence length dimension, providing a starting point for optimization.

    \item \textbf{Prepending}: For each input sequence $\mathbf{s}$ (after token embedding), we prepended the tunable prompt $\mathbf{X}$ to create an augmented input:
    
    \[ \mathbf{s}_\text{augmented} = [\mathbf{X}; \mathbf{s}] \]
    
    where $[;]$ denotes concatenation along the sequence length dimension.

    \item \textbf{Optimization}: During training, while keeping the pretrained language model parameters fixed, we optimized the elements of $\mathbf{X}$ to minimize the task-specific loss function:
    
    \[ \mathbf{X}^* = \text{argmin}_\mathbf{X} \mathcal{L}(\text{Model}([\mathbf{X}; \mathbf{s}]), \mathbf{y}) \]
    
    where $\mathcal{L}$ is the Cross Entropy loss function, $\text{Model}(\cdot)$ represents the frozen pretrained language model, and $\mathbf{y}$ is the ground truth label.

    \item \textbf{Length Variation}: We trained soft prompts of various lengths $l \in \{1,2,5,10,20\}$ to investigate the impact of prompt size on performance. Longer prompts can theoretically capture more details about general task structure, the nature of categories, and meta-information about specific training examples (although in practice, we did not observe significant performance differences across different lengths).
    
\end{enumerate}

\paragraph{Training procedure and checkpoints}

Soft prompts were optimized on the training subset of each dataset (see \cref{sec:appendix_dataset_details}). We trained each soft prompt for 30 epochs with a batch size of 16 using the Adam optimizer \cite{kingma2017adammethodstochasticoptimization}. The initial learning rate was set to $3 \times 10^{-4}$ with an exponential decay of $\gamma = 0.9$ after each epoch.
To analyze how representations evolved during the training of soft prompts, we selected 50 intermediate points, logarithmically spaced across training iterations.