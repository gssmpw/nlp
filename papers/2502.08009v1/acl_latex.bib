% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@string{acl = {Association for Computational Linguistics}}
@string{anth = {https://aclanthology.org/}}

@string{WOAH:2024:1 = {Proceedings of the 8th Workshop on Online Abuse and Harms (WOAH 2024)}}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{wei2022finetunedlanguagemodelszeroshot,
      title={Finetuned Language Models Are Zero-Shot Learners}, 
      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
      year={2022},
      eprint={2109.01652},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.01652}, 
}

@misc{lester2021powerscaleparameterefficientprompt,
      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
      author={Brian Lester and Rami Al-Rfou and Noah Constant},
      year={2021},
      eprint={2104.08691},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08691}, 
}


@misc{zhao2021calibrateuseimprovingfewshot,
      title={Calibrate Before Use: Improving Few-Shot Performance of Language Models}, 
      author={Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
      year={2021},
      eprint={2102.09690},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2102.09690}, 
}

@misc{min2022rethinkingroledemonstrationsmakes,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.12837}, 
}

@misc{bertsch2024incontextlearninglongcontextmodels,
      title={In-Context Learning with Long-Context Models: An In-Depth Exploration}, 
      author={Amanda Bertsch and Maor Ivgi and Uri Alon and Jonathan Berant and Matthew R. Gormley and Graham Neubig},
      year={2024},
      eprint={2405.00200},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.00200}, 
}

@misc{wang2024largelanguagemodelslatent,
      title={Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning}, 
      author={Xinyi Wang and Wanrong Zhu and Michael Saxon and Mark Steyvers and William Yang Wang},
      year={2024},
      eprint={2301.11916},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.11916}, 
}

@misc{liu2024understandingincontextlearningcontrastive,
      title={Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps}, 
      author={Fuxiao Liu and Paiheng Xu and Zongxia Li and Yue Feng and Hyemi Song},
      year={2024},
      eprint={2307.05052},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.05052}, 
}

@misc{pan2023incontextlearninglearnsincontext,
      title={What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning}, 
      author={Jane Pan and Tianyu Gao and Howard Chen and Danqi Chen},
      year={2023},
      eprint={2305.09731},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.09731}, 
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:160025533}
}

@misc{park2024linearrepresentationhypothesisgeometry,
      title={The Linear Representation Hypothesis and the Geometry of Large Language Models}, 
      author={Kiho Park and Yo Joong Choe and Victor Veitch},
      year={2024},
      eprint={2311.03658},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.03658}, 
}

@misc{mikolov2013efficientestimationwordrepresentations,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@misc{bowman2016generatingsentencescontinuousspace,
      title={Generating Sentences from a Continuous Space}, 
      author={Samuel R. Bowman and Luke Vilnis and Oriol Vinyals and Andrew M. Dai and Rafal Jozefowicz and Samy Bengio},
      year={2016},
      eprint={1511.06349},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.06349}, 
}

@misc{elhage2022toymodelssuperposition,
      title={Toy Models of Superposition}, 
      author={Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and Tom Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah},
      year={2022},
      eprint={2209.10652},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.10652}, 
}

@misc{arora2018linearalgebraicstructureword,
      title={Linear Algebraic Structure of Word Senses, with Applications to Polysemy}, 
      author={Sanjeev Arora and Yuanzhi Li and Yingyu Liang and Tengyu Ma and Andrej Risteski},
      year={2018},
      eprint={1601.03764},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1601.03764}, 
}

@misc{belinkov2021probingclassifierspromisesshortcomings,
      title={Probing Classifiers: Promises, Shortcomings, and Advances}, 
      author={Yonatan Belinkov},
      year={2021},
      eprint={2102.12452},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2102.12452}, 
}

@misc{gurnee2024languagemodelsrepresentspace,
      title={Language Models Represent Space and Time}, 
      author={Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2310.02207},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.02207}, 
}


@misc{yerxa2023learningefficientcodingnatural,
      title={Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations}, 
      author={Thomas Yerxa and Yilun Kuang and Eero Simoncelli and SueYeon Chung},
      year={2023},
      eprint={2303.03307},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.03307}, 
}

@misc{vonoswald2023transformerslearnincontextgradient,
      title={Transformers learn in-context by gradient descent}, 
      author={Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
      year={2023},
      eprint={2212.07677},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.07677}, 
}

@misc{akyürek2023learningalgorithmincontextlearning,
      title={What learning algorithm is in-context learning? Investigations with linear models}, 
      author={Ekin Akyürek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
      year={2023},
      eprint={2211.15661},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.15661}, 
}

@misc{hendel2023incontextlearningcreatestask,
      title={In-Context Learning Creates Task Vectors}, 
      author={Roee Hendel and Mor Geva and Amir Globerson},
      year={2023},
      eprint={2310.15916},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.15916}, 
}

@article{chou2024neural,
  title={Neural Manifold Capacity Captures Representation Geometry, Correlations, and Task-Efficiency Across Species and Behaviors},
  author={Chou, Chi-Ning and Arend, Luke and Wakhloo, Albert J and Kim, Royoung and Slatton, Will and Chung, SueYeon},
  journal={bioRxiv},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}


@article{chung2018classification,
  title={Classification and geometry of general perceptual manifolds},
  author={Chung, SueYeon and Lee, Daniel D and Sompolinsky, Haim},
  journal={Physical Review X},
  year={2018},
  publisher={APS}
}

@article{wakhloo2023linear,
  title={Linear classification of neural manifolds with correlated variability},
  author={Wakhloo, Albert J and Sussman, Tamara J and Chung, SueYeon},
  journal={Physical Review Letters},
  year={2023},
  publisher={APS}
}

@article{chungNeuralPopulationGeometry2021,
  title = {Neural Population Geometry: {{An}} Approach for Understanding Biological and Artificial Neural Networks},
  shorttitle = {Neural Population Geometry},
  author = {Chung, SueYeon and Abbott, L.F.},
  year = {2021},
  month = oct,
  journal = {Current Opinion in Neurobiology},
  volume = {70},
  pages = {137--144},
  issn = {09594388},
  doi = {10.1016/j.conb.2021.10.010},
  urldate = {2023-09-29},
  abstract = {Advances in experimental neuroscience have transformed our ability to explore the structure and function of neural circuits. At the same time, advances in machine learning have unleashed the remarkable computational power of artificial neural networks (ANNs). While these two fields have different tools and applications, they present a similar challenge: namely, understanding how information is embedded and processed through high-dimensional representations to solve complex tasks. One approach to addressing this challenge is to utilize mathematical and computational tools to analyze the geometry of these high-dimensional representations, i.e., neural population geometry. We review examples of geometrical approaches providing insight into the function of biological and artificial neural networks: representation untangling in perception, a geometric theory of classification capacity, disentanglement, and abstraction in cognitive systems, topological representations underlying cognitive maps, dynamic untangling in motor systems, and a dynamical approach to cognition. Together, these findings illustrate an exciting trend at the intersection of machine learning, neuroscience, and geometry, in which neural population geometry provides a useful population-level mechanistic descriptor underlying task implementation. Importantly, geometric descriptions are applicable across sensory modalities, brain regions, network architectures, and timescales. Thus, neural population geometry has the potential to unify our understanding of structure and function in biological and artificial neural networks, bridging the gap between single neurons, population activities, and behavior.},
  langid = {english},
  file = {/Users/artemkirsanov/My Drive/Zotero attachments/Chung_Abbott_2021_Neural population geometry.pdf}
}

@article{fleschOrthogonalRepresentationsRobust2022,
  title = {Orthogonal Representations for Robust Context-Dependent Task Performance in Brains and Neural Networks},
  author = {Flesch, Timo and Juechems, Keno and Dumbalska, Tsvetomira and Saxe, Andrew and Summerfield, Christopher},
  year = {2022},
  month = apr,
  journal = {Neuron},
  volume = {110},
  number = {7},
  pages = {1258-1270.e11},
  issn = {08966273},
  doi = {10.1016/j.neuron.2022.01.005},
  urldate = {2024-10-13},
  langid = {english},
  file = {/Users/artemkirsanov/Zotero/storage/M54Q4TR2/Flesch et al. - 2022 - Orthogonal representations for robust context-dependent task performance in brains and neural networ.pdf}
}
@article{cohenSeparabilityGeometryObject2020,
  title = {Separability and Geometry of Object Manifolds in Deep Neural Networks},
  author = {Cohen, Uri and Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
  year = {2020},
  month = feb,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {746},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-14578-5},
  urldate = {2023-10-30},
  abstract = {Abstract             Stimuli are represented in the brain by the collective population responses of sensory neurons, and an object presented under varying conditions gives rise to a collection of neural population responses called an `object manifold'. Changes in the object representation along a hierarchical sensory system are associated with changes in the geometry of those manifolds, and recent theoretical progress connects this geometry with `classification capacity', a quantitative measure of the ability to support object classification. Deep neural networks trained on object classification tasks are a natural testbed for the applicability of this relation. We show how classification capacity improves along the hierarchies of deep neural networks with different architectures. We demonstrate that changes in the geometry of the associated object manifolds underlie this improved capacity, and shed light on the functional roles different levels in the hierarchy play to achieve it, through orchestrated reduction of manifolds' radius, dimensionality and inter-manifold correlations.},
  langid = {english},
  file = {/Users/artemkirsanov/Zotero/storage/KFPNF53L/Cohen et al. - 2020 - Separability and geometry of object manifolds in d.pdf}
}

@misc{mamouEmergenceSeparableManifolds2020,
  title = {Emergence of {{Separable Manifolds}} in {{Deep Language Representations}}},
  author = {Mamou, Jonathan and Le, Hang and Del Rio, Miguel and Stephenson, Cory and Tang, Hanlin and Kim, Yoon and Chung, SueYeon},
  year = {2020},
  month = jul,
  number = {arXiv:2006.01095},
  eprint = {2006.01095},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-21},
  abstract = {Deep neural networks (DNNs) have shown much empirical success in solving perceptual tasks across various cognitive modalities. While they are only loosely inspired by the biological brain, recent studies report considerable similarities between representations extracted from task-optimized DNNs and neural populations in the brain. DNNs have subsequently become a popular model class to infer computational principles underlying complex cognitive functions, and in turn, they have also emerged as a natural testbed for applying methods originally developed to probe information in neural populations. In this work, we utilize mean-field theoretic manifold analysis, a recent technique from computational neuroscience that connects geometry of feature representations with linear separability of classes, to analyze language representations from large-scale contextual embedding models. We explore representations from different model families (BERT, RoBERTa, GPT, etc.) and find evidence for emergence of linguistic manifolds across layer depth (e.g., manifolds for part-of-speech tags), especially in ambiguous data (i.e, words with multiple part-of-speech tags, or part-of-speech classes including many words). In addition, we find that the emergence of linear separability in these manifolds is driven by a combined reduction of manifolds' radius, dimensionality and inter-manifold correlations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/artemkirsanov/Zotero/storage/NET2MV38/Mamou et al. - 2020 - Emergence of Separable Manifolds in Deep Language .pdf}
}

@misc{gaoTheoryMultineuronalDimensionality2017,
  title = {A Theory of Multineuronal Dimensionality, Dynamics and Measurement},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  year = {2017},
  month = nov,
  publisher = {Neuroscience},
  doi = {10.1101/214262},
  urldate = {2024-10-13},
  abstract = {Abstract           In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
  archiveprefix = {Neuroscience},
  langid = {english},
  file = {/Users/artemkirsanov/Zotero/storage/XWFMN6IY/Gao et al. - 2017 - A theory of multineuronal dimensionality, dynamics and measurement.pdf}
}


@misc{stephenson2021geometrygeneralizationmemorizationdeep,
      title={On the geometry of generalization and memorization in deep neural networks}, 
      author={Cory Stephenson and Suchismita Padhy and Abhinav Ganesh and Yue Hui and Hanlin Tang and SueYeon Chung},
      year={2021},
      eprint={2105.14602},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2105.14602}, 
}

@inproceedings{mamou2020emergence,
  title={Emergence of Separable Manifolds in Deep Language Representations},
  author={Mamou, Jonathan and Le, Hang and Del Rio, Miguel and Stephenson, Cory and Tang, Hanlin and Kim, Yoon and Chung, SueYeon},
  booktitle={International Conference on Machine Learning},
  pages={6713--6723},
  year={2020},
  organization={PMLR}
}

@article{stephenson2019untangling,
  title={Untangling in invariant speech recognition},
  author={Stephenson, Cory and Feather, Jenelle and Padhy, Suchismita and Elibol, Oguz and Tang, Hanlin and McDermott, Josh and Chung, SueYeon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={{Llama Team, AI @ Meta}},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={{Gemma Team}},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@inproceedings{Zhang2015CharacterlevelCN,
  title={Character-level Convolutional Networks for Text Classification},
  author={Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
  booktitle={NIPS},
  year={2015}
}

@misc{liu2022fewshotparameterefficientfinetuningbetter,
      title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning}, 
      author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
      year={2022},
      eprint={2205.05638},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.05638}, 
}

@article{dicarloUntanglingInvariantObject2007,
  title = {Untangling Invariant Object Recognition},
  author = {DiCarlo, James J. and Cox, David D.},
  year = {2007},
  month = aug,
  journal = {Trends in Cognitive Sciences},
  volume = {11},
  number = {8},
  pages = {333--341},
  issn = {13646613},
  doi = {10.1016/j.tics.2007.06.010},
  urldate = {2024-10-14},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@article{gardner1988optimal,
  title={Optimal storage properties of neural network models},
  author={Gardner, Elizabeth and Derrida, Bernard},
  journal={Journal of Physics A: Mathematical and general},
  volume={21},
  number={1},
  pages={271},
  year={1988},
  publisher={IOP Publishing}
}

@article{ansuiniIntrinsicDimensionData,
  title = {Intrinsic Dimension of Data Representations in Deep Neural Networks},
  author = {Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  year = {2019},
  abstract = {Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality (ID) of datarepresentations, i.e. the minimal number of parameters needed to describe a representation. We find that, in a trained network, the ID is orders of magnitude smaller than the number of units in each layer. Across layers, the ID first increases and then progressively decreases in the final layers. Remarkably, the ID of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g., with principal component analysis), nor in representations that had been artificially linearized. They are neither found in untrained networks, nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional, but not necessarily flat manifolds.},
  langid = {english},
  file = {/Users/artemkirsanov/Zotero/storage/RRVF6B4C/Ansuini et al. - Intrinsic dimension of data representations in deep neural networks.pdf}
}

@inproceedings{fawziEmpiricalStudyTopology2018,
  title = {Empirical {{Study}} of the {{Topology}} and {{Geometry}} of {{Deep Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Fawzi, Alhussein and {Moosavi-Dezfooli}, Seyed-Mohsen and Frossard, Pascal and Soatto, Stefano},
  year = {2018},
  month = jun,
  pages = {3762--3770},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00396},
  urldate = {2024-10-15},
  isbn = {978-1-5386-6420-9}
}

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}
@inproceedings{
yerxa2023learning,
title={Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations},
author={Thomas Edward Yerxa and Yilun Kuang and Eero P Simoncelli and SueYeon Chung},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=og9V7NgOrQ}
}


@inproceedings{hovy-etal-2001-toward,
	title = "Toward Semantics-Based Answer Pinpointing",
	author = "Hovy, Eduard  and
	  Gerber, Laurie  and
	  Hermjakob, Ulf  and
	  Lin, Chin-Yew  and
	  Ravichandran, Deepak",
	booktitle = HLT:2001:1,
	year = "2001",
	url = anth # {H01-1069},
}

@inproceedings{li-roth-2002-learning,
	title = "Learning Question Classifiers",
	author = "Li, Xin  and
	  Roth, Dan",
	booktitle = COLING:2002:1,
	year = "2002",
	url = anth # {C02-1150},
}

@inproceedings{balashankar-subramanian-2021-learning,
	title = "Learning Faithful Representations of Causal Graphs",
	author = "Balashankar, Ananth  and
	  Subramanian, Lakshminarayanan",
	editor = "Zong, Chengqing  and
	  Xia, Fei  and
	  Li, Wenjie  and
	  Navigli, Roberto",
	booktitle = ACL:2021:long,
	month = aug,
	year = "2021",
	address = "Online",
	publisher = acl,
	url = anth # {2021.acl-long.69},
	doi = "10.18653/v1/2021.acl-long.69",
	pages = "839--850",
}

@inproceedings{abdou-etal-2021-language,
	title = "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
	author = "Abdou, Mostafa  and
	  Kulmizev, Artur  and
	  Hershcovich, Daniel  and
	  Frank, Stella  and
	  Pavlick, Ellie  and
	  S{\o}gaard, Anders",
	editor = "Bisazza, Arianna  and
	  Abend, Omri",
	booktitle = CONLL:2021:1,
	month = nov,
	year = "2021",
	address = "Online",
	publisher = acl,
	url = anth # {2021.conll-1.9},
	doi = "10.18653/v1/2021.conll-1.9",
	pages = "109--132",
}

@inproceedings{hewitt-manning-2019-structural,
	title = "{A} Structural Probe for Finding Syntax in Word Representations",
	author = "Hewitt, John  and
	  Manning, Christopher D.",
	editor = "Burstein, Jill  and
	  Doran, Christy  and
	  Solorio, Thamar",
	booktitle = NAACL:2019:1,
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = acl,
	url = anth # {N19-1419},
	doi = "10.18653/v1/N19-1419",
	pages = "4129--4138",
}

@inproceedings{belinkov-etal-2017-neural,
	title = "What do Neural Machine Translation Models Learn about Morphology?",
	author = "Belinkov, Yonatan  and
	  Durrani, Nadir  and
	  Dalvi, Fahim  and
	  Sajjad, Hassan  and
	  Glass, James",
	editor = "Barzilay, Regina  and
	  Kan, Min-Yen",
	booktitle = ACL:2017:1,
	month = jul,
	year = "2017",
	address = "Vancouver, Canada",
	publisher = acl,
	url = anth # {P17-1080},
	doi = "10.18653/v1/P17-1080",
	pages = "861--872",
}

@inproceedings{pennington-etal-2014-glove,
	title = "{G}lo{V}e: Global Vectors for Word Representation",
	author = "Pennington, Jeffrey  and
	  Socher, Richard  and
	  Manning, Christopher",
	editor = "Moschitti, Alessandro  and
	  Pang, Bo  and
	  Daelemans, Walter",
	booktitle = EMNLP:2014:1,
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = acl,
	url = anth # {D14-1162},
	doi = "10.3115/v1/D14-1162",
	pages = "1532--1543",
}