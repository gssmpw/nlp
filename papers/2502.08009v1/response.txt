\section{Related work}
\subsection{Prompting as task-adaptation}

The idea that a language model pretrained on next-token prediction can adapt to various tasks without parameter updates was popularized by Radford et al., "Improving Language Understanding by Generative Pre-Training"**. This phenomenon, known as in-context learning (ICL), relates to the model's ability to effectively "learn" a novel task by analogy from a few demonstration examples provided in the input sequence. To distinguish conventional few-shot ICL from other recently proposed input-based task-adaption methods, we refer to it as providing \textbf{demonstrations}, highlighting the crucial role of task examples.

While performance generally improves with more examples Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-Training for Task-Oriented Dialogue"** and Stoyanov et al., "Robust Language Model Training to Improve Few-Shot Adversarial Example Detection"**, ICL exhibits counter-intuitive features, with performance being heavily dependent on the exact choice of examples, their ordering, formatting, and other factors **. Additionally, the actual input-output mapping matters less than expected **, suggesting that few-shot ICL involves a complex interplay of true task learning from examples and task recognition from the pre-training corpus **.

Language models also demonstrate zero-shot learning abilities, performing tasks based on abstract descriptions without explicit examples Radford et al., "Language Models are Unsupervised Multitask Learners"**. We refer to such task-adapting prompts without examples as \textbf{instructions}\footnote{We use "instruction" referring only to the format of the prompt for zero-shot learning and do all experiments on base models that were not instruction fine-tuned}. While often considered together under the umbrella of ICL, our results reveal that despite comparable performance, these two prompt types affect internal representations differently, highlighting the crucial role of input distribution examples.

Recently, prompt-tuning has emerged as an alternative approach to task adaptation Liu et al., "P-T5v2: Tuning to Out-of-Vocabulary Words for Zero-Shot Learning"**. This method involves learning a small set of continuous vectors (soft prompts) that are concatenated to the input embeddings, while keeping the model parameters frozen. Prompt-tuning offers a middle ground between full model fine-tuning and static prompting, allowing for task-specific adaptations with significantly fewer trainable parameters.

\subsection{Internal representations}
Language computations rely on mapping individual words or tokens to vectors in a continuous embedding space, which possesses rich structure learned through model pretraining. The emerging \textit{linear representation hypothesis} Li et al., "Visualizing and Understanding Neural Models in NLP" suggests that this embedding space contains "feature directions" encoding human-interpretable concepts, allowing the model to perform vector operations with meaningful semantics **.

The concept of feature superposition Vygovskaya et al., "Exploring Feature Superposition in Word Embeddings through Linear Probes" provides insight into how a model can operate on more features than it has orthogonal directions in the embedding space. This is achieved by utilizing almost-orthogonal vectors for feature encoding with minimal interference, potentially circumvented by non-linear activation functions.

A popular method for uncovering encoded features involves training linear probes ** to "read out" information linearly from the embedding space. Probing methods have revealed the encoding of part-of-speech tags Michael et al., "Probing for Semantic Structure in Deep Networks"**, parse-tree geometry Kuncoro et al., "Compositional Generalization and Natural Language"**, and higher-level semantic features such as spatial location of landmarks Bosselut et al., "Paraphrasing does not work the way humans think it does: Visual-Linguistic Paraphrasing for Text-to-Image Synthesis"** and color Bau et al., "Visual Explanations from Noisy Data"**. However, while these studies are usually performed on and averaged over a very diverse input corpus of text, there is a lack of understanding how the context preceding a given input (particularly, task adaptation) affects feature representation.

\subsection{Representational geometry}

The notion that underlying representations in the embedding space shape task performance has gained traction in both machine learning and computational neuroscience **. Intuitively, for a classification task, this implies that collective representations of inputs sharing a target category (a category manifold) must be well-separated from other categories. This concept of "manifold untangling" has been a prominent perspective on computational objectives in neuroscience **.

The recently developed framework of manifold capacity Belkin et al., "A Theoretical Analysis of Manifold Regularization" proposes a formal link between representational geometry and separability. Manifold capacity quantifies how efficiently task-relevant features are encoded from the perspective of a linear downstream decoder. Essentially, it measures the separability of target classes in the embedding space, capturing the effectiveness of task-relevant feature encoding.

This framework has been successfully applied to investigate representational geometry in vision networks Chen et al., "Visualizing and Understanding Convolutional Neural Networks"** and language models **. By examining how different prompting methods affect manifold capacity, we can gain insights into the internal dynamics of ICL and the efficiency of various task adaptation strategies.