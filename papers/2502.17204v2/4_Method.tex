\subsection{Background}
In this paper, we mainly focus on the multi-constraint instruction $I_c$. It can be formulated as a seed instruction incorporated with ${n}$ constraints:
\begin{equation}
\label{eq1}
    I_c = I_s \oplus C_1 \oplus ... \oplus C_n,
\end{equation}
where the seed instructions $I_s$ describe a task, e.g., write a story, while these constraints $\sum_{i=1}^n C_i$ limit the output from different aspects, e.g., format, length, content, etc. $\oplus$ stands for the concatenation operation. 
% \footnote{We provide more examples of multi-constraint instruction in Appx.~\ref{appx:con_samp}.} 
% seed + 1 2 3 4


\subsection{Probing Task} \label{method}
% 我们尝试从约束的难度角度出发 量化指令中的不同排布情况 to achieve this, two problems need to be soluted: how to quantify the 
% 我们参考约束的分类建模
\subsubsection{Task Formulation}
To investigate the impact of constraint order, we introduce a probing task. In this task, the LLM is given multi-constraint instructions with constraints arranged in various orders. The LLM's task is to generate a response that follows all constraints. We evaluate the LLM in two practical scenarios: single-round and multi-round inference. The LLM's responses are then evaluated to determine its performance across various constraints. The overall procedure is illustrated in Fig.~\ref{fig:method}. In the following sections, we will provide a detailed explanation.





\subsubsection{Multi-constraint Instruction Synthesis}\label{sec:ins_cons}
To ensure the generalizability of probing data, we construct the initial multi-constraint instructions which include a variety of tasks and diverse constraint combinations. The multi-constraint instruction synthesis can be further divided into two parts: seed sampling and constraint sampling. 

For the seed sampling, we sample data from three source datasets: (1) Natural Instructions V2~\cite{wang2022supernaturalinstructions}. It is an instruction collection covering more than 1600 NLP tasks. We filter those tasks that are too easy and could potentially conflict with complex constraints, e.g., object classification and sentiment tagging. Then, we randomly sample 52 instructions from the remaining tasks. (2) Self-Instruct~\cite{wang2023self}. We only sample 83 instances from their initial 175 seed instructions which are formulated by humans. (3) Open Assistant~\cite{kopf2024openassistant}. Following the strategy of Suri~\cite{li2023self}, we filter out the first turn of the conversation with the highest quality (marked as rank 0 in the dataset) and sample 65 instances from them. Overall, we obtain 200 seed instructions, where the number of instructions is denoted as $n_{seed}$.

As for the constraint sampling, we first categorize the constraints into 8 groups with 25 fine-grained types~\cite{zhou2023instructionfollowing}. For each type of constraint, we employ 8 different expressions to describe it\footnote{More details are shown in Appx.~\ref{appx:cons_tax}}. Then, we sample $n$ constraints from the constraint taxonomy and use the predefined rules to avoid possible conflicts. To ensure diversity, we repeat the sampling process to obtain $n_{cc}$ distinct constraint combinations, deriving $n_{seed}\times n_{cc}$ multi-constraint instructions.






\subsubsection{Constraint Reordering} \label{reorder}
To quantitatively construct instructions with different constraint orders, here are two questions that need to be answered: (1) \textit{How do we distinguish the disparity of different constraints}? (2) After we order the constraints based on their disparity, \textit{how do we quantitatively describe the disparity of constraint orders}?

An appropriate solution for the first question is to categorize the constraints based on their difficulty~\cite{chen2024sifo}. In this paper, we also sort the constraints based on their difficulty. However, different from existing works which designate the difficulty of the constraints based on handcraft rules, we measure the difficulty of a constraint via the overall accuracy of following it in our probing datasets. The formulation is as follows:
\begin{equation}
\label{eq2}
    % \small
    \text{Dff}_{C_x}= \text{Softmax}(1-\text{Acc}_{C_{x}}), 
\end{equation}
\begin{equation}
    \label{eq3}
    \text{Acc}_{C_x} = \frac{1}{N_{x}}\sum_{i=1}^{N_{x}}c_x^i.
\end{equation}
The $C_x$ refers to a specific type of constraint, the $N_{x}$ stands for the total number of instructions corresponding to the constraint $C_{x}$, and the $c_x^i$ is a binary value to reflect whether the constraint $C_{x}$ is followed in the $i^{th}$ instruction. 

To quantitatively describe the disparity of constraint order, we propose a novel metric called the Constraint Difficulty Distribution Index (CDDI) which quantifies a specific constraint order based on its difficulty distribution. Given the difficulty of different types of constraints, we can readily attain the difficulty distribution of the constraints incorporated in the multi-constraint instructions. Specifically, for a multi-constraint instruction, we rank the incorporated constraints based on their difficulty, from the hardest to the easiest. We set this “hard-to-easy” constraint order as an anchor since it depicts an extreme situation, i.e., we designate the $\text{CDDI}=1$ when the constraints fall in this order. Consequently, akin to the Kendall tau distance~\cite{cicirello2020kendall}, we measure the difficulty distribution of a specific constraint order $o$ by comparing it with the ``hard-to-easy'' constraint order $o_{max}$. The formula is shown as:
\begin{equation}
    \label{eq4}
    \text{CDDI}_{o} = \frac{N_{con}-N_{dis}}{N_{pair}} = \frac{2(N_{con}-N_{dis})}{n(n-1)}.
\end{equation}
where $N_{con}$ and $N_{dis}$ represent the number of concordant and discordant distribution pairs of constraints between $o$ and $o_{max}$, respectively. The $N_{pair}$ is the total number of compared constraint pairs. Overall, we select $n_{dd}$ different difficulty distributions, finally comprising $n_{seed}\times n_{cc}\times n_{dd}$ instances.







\subsubsection{Sequential-Sensitive Inference}
Given the multi-constraint instructions with different constraint orders, we evaluate the model's performance in two common scenarios: single-round inference and multi-round inference. In single-round inference, the LLM is directly given the multi-constraint instructions with different constraint distributions. We argue that different constraint distributions could impose different levels of difficulty on the LLM to handle. The multi-round inference introduces a more typical setting: the user will first provide the LLM with the core intention (i.e., the seed instruction in this work), and then iteratively put forward the constraints in order to obtain a final response.

To evaluate the model performance, apart from the constraint following accuracy mentioned in Eq.(\ref{eq3}), we also verify its constraint-level accuracy $Acc_{cons}$ and instruction-level accuracy $Acc_{inst}$. Corresponding formulas are shown below:
\begin{equation}
    \label{eq5}
    \small
    \text{Acc}_{\text{cons}} = \frac{1}{mn}\sum_{i=1}^{m}\sum_{j=1}^{n}c_i^j,     \text{Acc}_{\text{inst}} = \frac{1}{m}\sum_{i=1}^{m}\prod_{j=1}^{n}c_i^j.
\end{equation}
% \begin{equation}
%     \label{eq5_1}
%     \text{Acc}_{\text{inst}} = \frac{1}{m}\sum_{i=1}^{m}\prod_{j=1}^{n}c_i^j.
% \end{equation}
where $m$ and $n$ refer to the number of instructions and constraints in the instruction, respectively. Similar to Eq.(\ref{eq3}), the $c_i^j$ is a binary value which equals 1 when the constraint is followed in the $i^{th}$ instruction. All the evaluation is conducted by leveraging the script introduced in ~\cite{zhou2023instructionfollowing}. We only evaluate the final responses produced by the LLMs.


\begin{figure}[t] 
    \centering
        \includegraphics[width=0.5\textwidth]{statistic.pdf}
    % \captionsetup{font={small}} 
    \caption{The statistic of different types of constraints in the probing data. The 7cons and 9cons stand for the setting when $n$=7 and $n$=9, respectively.}
    \label{fig:statistic}
\end{figure}