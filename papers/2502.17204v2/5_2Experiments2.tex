\begin{figure*}[t] 
    \centering
        \includegraphics[width=1\textwidth]{position_scores.pdf}
    % \captionsetup{font={small}} 
    \caption{(a) The importance weights assigned by the LLM when handling constraints in different positions. (b) The total importance weights which designated to the constraint part in the multi-constraint instructions among three different constraint distributions.}
    \label{fig:position_score}
\end{figure*}


\begin{figure}[t] 
    \centering
        \includegraphics[width=0.48\textwidth]{types_scores.pdf}
    % \captionsetup{font={small}} 
    \caption{The importance weights across different types of constraint in three different constraint distributions.}
    \label{fig:type_score}
\end{figure}







\subsection{Explanation Metric}
To make an explanation for the influence brought by the constraints of different orders, we make an explanation study on where the LLMs mainly focus when handling multi-constraint instructions via a feature attribution-based explanation method~\cite{li2016visualizing, wu2020perturbed}. Specifically, we leverage the importance of the input tokens to measure the LLMs' attention to them. To obtain the importance of a specific instruction token $t_x$ to a response token $t_y$, we calculate the confidence change after the removal of the $t_x$, as formulated below:
\begin{equation}
    \label{eq6}
    I_{t_x,t_y}=p(t_y|Z_y)-p(t_y|Z_{y,/t_x}),
\end{equation}
where $p(\cdot|\cdot)$ is the conditional probability produced by the LLM $f$, $Z_y$ is the tokens before the $t_y$ and $Z_{y,/t_x}$ is the tokens of $Z_y$ after removing the token $t_x$. To reduce the computation, we approximate the $I_{t_x,t_y}$ with the first-order gradient $\frac{\partial f\left(t_y \mid Z_y\right)}{\partial \mathbf{E}\left[t_x\right]}$ ~\cite{wu2023language}, where $\mathbf{E}\left[t_x\right]$ is the token embedding of $t_x$. We normalize the importance $I_{t_x,t_y}$ and obtain the standard importance $S_{t_x,t_y}$ with the formula:
\begin{equation}
    \label{eq7}
    S_{t_x,t_y}= \frac{L\times I_{t_x,t_y}}{{\max_{i=1}^{N_{X}}}I_{t_i,t_y}},
\end{equation}
where $N_X$ is the number of instruction tokens and $L$ is a hyper-parameter which helps to filter the noise brought by the first-order approximation. To visualize the LLMs' attention to different constraints, we calculate the importance weight of a specific constraint $C_x$ to the final response $Y$ with the formula:
\begin{equation}
    \label{eq8}
    S_{C_x,Y}=\frac{1}{N_Y}\sum_{t_y\in Y}\sum_{t_x\in C_x}S_{t_x,t_y},
\end{equation}
where $N_Y$ is the number of response tokens.







\subsection{Experiment Set-up}
We conduct our explanation study on the LLaMA3-8B-Instruct model. We set the hyper-parameter $L$ to 10 in Eq.(\ref{eq7}) and select three most typical difficulty distributions: hard-to-easy (indicated by CDDI=1), easy-to-hard (indicated by CDDI=-1) and random (indicated by CDDI=-0.05) to conduct our experiments. We randomly sample 200 instances from the corresponding data which fall in the required CDDI value in the probing task to serve as the dataset.







\subsection{Results}
\paragraph*{Hard-to-easy constraint order induces the LLM to pay more attention to the constraint part in the multi-constraint instructions.} We visualize the importance weights of the model on the constraints in different positions. As shown in Fig.~\ref{fig:position_score} (a), in the multi-constraint instruction following, the model's attention on different positions varies with changes in the constraint orders. Specifically, when the constraints are randomly distributed across different positions (represented by CDDI=-0.05), the model assigns similar attention to all positions. As the constraint order becomes more structured (represented by CDDI=-1 and CDDI=1), the model's attention neither exhibits the “lost in the middle” phenomenon observed in long-context processing~\cite{liu2024lost}, nor a simply sequential distribution, but follows an iterative, laddered order. Then, in Fig.~\ref{fig:position_score} (b), we present the total importance weight the model assigns to the constraint part. We observe that the “hard-to-easy” constraint order attracts the most attention from the model towards the constraint part, which provides an explanation for the superiority of this constraint order.

\paragraph*{The LLM's performance on various constraints is strongly correlated with its attention patterns.} The importance weights of the model on different types of constraints are presented in Fig.~\ref{fig:type_score}. Among the three distinct difficulty distributions, the “hard-to-easy” (represented by CDDI = 1) assigns the highest importance weights to various types of constraints except for the Content and Startend. It is worth noting that this is exactly in accord with quantitative results in Tab.~\ref{tab:main}, i.e., as the CDDI value increases, the model's performance on the Content and Startend constraints shows a decreasing trend instead. Overall, the results show that the model's accuracy in following a specific type of constraint is strongly correlated with the attention assigned to it by the model.