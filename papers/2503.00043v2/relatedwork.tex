\section{Related work}
\paragraph{Visual Analogical Reasoning.}
Visual Analogical Reasoning is a complex task that requires models to recognize abstract similarities between visual contexts and underlying relational rules and to apply these rules to novel visual content to generate solutions. Ravenâ€™s Progressive Matrices (RPMs) \cite{raven1938progressive}, introduced in 1938, are one of the earliest visual analogical reasoning tests, designed to assess human intelligence by completing a 3x3 matrix of visual patterns based on relational reasoning. Recent visual analogy benchmarks, such as VISALOGY \citep{sadeghi2015visalogyansweringvisualanalogy} and VASR \citep{bitton2022vasrvisualanalogiessituation}, offer valuable perspectives but have limitations. VISALOGY, which is not publicly available, focuses on attributes and actions in Google images with manual annotations, while VASR emphasizes general image understanding rather than feature-based relationships. In contrast, \voila introduces a more complex and dynamic approach to visual analogical reasoning. It incorporates diverse subject relationships and rule-based structures and manipulates up to three properties at a time. Additionally, \voila introduces distraction elements to increase task difficulty, requiring models to discover and filter out the irrelevant changes among properties while solving analogy questions. Unlike static datasets, \voila allows the generation of over 6.4M distinct visual analogy scenarios across 14 subject types, 13 actions, and 4 numeric values by adjusting flexible property-rule configuration, offering a scalable and adaptable evaluation platform for MLLMs.

\paragraph{Prompting.} 
Prompting strategies are critical to improving the performance of MLLMs, especially in complex reasoning tasks. Zero-shot prompting provides models with task instructions without examples, while few-shot prompting includes example-based prompts to guide the model toward the correct answer \citep{brown2020languagemodelsfewshotlearners}. Chain-of-Thought (CoT) prompting enhances reasoning by breaking down problems into sequential steps, allowing the model to tackle each sub-problem in a structured manner \citep{wei2022chain}. Two common CoT approaches include zero-shot CoT \citep{kojima2022large}, which encourages step-by-step reasoning without examples, and few-shot CoT \citep{wei2022chain}, which includes rationales or explanations to guide reasoning.
Research has shown that few-shot CoT, by providing exemplar rationales, often outperforms zero-shot approaches \citep{zhang2024multimodalchainofthoughtreasoninglanguage}. Another method, Least-to-Most (L2M) prompting \citep{zhou2023leasttomostpromptingenablescomplex}, adopts a similar multi-phase structure as CoT but without the use of rationales. Instead, L2M progressively breaks down the task, using the solution to each sub-problem as input for the next. In \voila, we extend the use of L2M to gain a deeper understanding of MLLMs' behavior across sub-tasks, from recognizing visual content to generating accurate images based on relational reasoning.


\paragraph{Multimodal Reasoning Benchmarks.}

Multimodal reasoning benchmarks have been instrumental in advancing the evaluation of MLLMs, integrating both textual and visual information to assess models' capabilities across a variety of domains. Domain-specific benchmarks like ScienceQA \citep{lu2022learn}, A-OKVQA \citep{schwenk2022aokvqabenchmarkvisualquestion}, Math-Vision \citep{wang2024measuringmultimodalmathematicalreasoning}, and MMMU-Pro \citep{yue2024mmmu} focus on specialized knowledge, such as scientific, mathematical, and visual question-answering reasoning tasks. Meanwhile, benchmarks such as CompBench \citep{kil2024compbenchcomparativereasoningbenchmark}, MMRel \citep{nie2024mmrelrelationunderstandingdataset}, MARVEL \citep{jiang2024marvelmultidimensionalabstractionreasoning}, and ScanReason \citep{zhu2024scanreasonempowering3dvisual} address more generalized multimodal relational reasoning.
Several studies also focus on multi-step reasoning tasks, where models must process information sequentially, such as VisualCoT \citep{shao2024visualcotadvancingmultimodal}, LogicVista \citep{xiao2024logicvistamultimodalllmlogical}, and VideoCoT \citep{wang2024videocotvideochainofthoughtdataset}, while datasets such as MuirBench \cite{wang2024muirbenchcomprehensivebenchmarkrobust}, MIRB \cite{zhao2024mirb} and MANTIS-Eval \cite{Jiang2024MANTISIM} assess multiple image reasoning. 
Visual action planning \cite{gokhale2019cooking} and visual procedural planning \cite{chang2020procedure,su2024actplan} has also been explored to find relationships between pairs of images.
However, \voila distinguishes itself by focusing on high-order abstract relations and knowledge transfer across multiple images, challenging models not only to understand but also to generate both images and text while applying correct relational reasoning. These positions \voila as a unique benchmark for testing MLLMs' higher-level cognitive abilities.