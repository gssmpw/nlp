\section{Related work}
\paragraph{Visual Analogical Reasoning.}
Visual Analogical Reasoning is a complex task that requires models to recognize abstract similarities between visual contexts and underlying relational rules and to apply these rules to novel visual content to generate solutions. Ravenâ€™s Progressive Matrices (RPMs) **Raven, "Progressive Matrices"** , introduced in 1938, are one of the earliest visual analogical reasoning tests, designed to assess human intelligence by completing a 3x3 matrix of visual patterns based on relational reasoning. Recent visual analogy benchmarks, such as VISALOGY **VISANOMY et al., "Visual Analogies for Object Recognition"** and VASR **Santoro et al., "A Simple Neural Reasoning Model for Visual Analogy"** , offer valuable perspectives but have limitations. VISALOGY, which is not publicly available, focuses on attributes and actions in Google images with manual annotations, while VASR emphasizes general image understanding rather than feature-based relationships. In contrast, \voila introduces a more complex and dynamic approach to visual analogical reasoning. It incorporates diverse subject relationships and rule-based structures and manipulates up to three properties at a time. Additionally, \voila introduces distraction elements to increase task difficulty, requiring models to discover and filter out the irrelevant changes among properties while solving analogy questions. Unlike static datasets, \voila allows the generation of over 6.4M distinct visual analogy scenarios across 14 subject types, 13 actions, and 4 numeric values by adjusting flexible property-rule configuration, offering a scalable and adaptable evaluation platform for MLLMs.

\paragraph{Prompting.} 
Prompting strategies are critical to improving the performance of MLLMs, especially in complex reasoning tasks. Zero-shot prompting provides models with task instructions without examples, while few-shot prompting includes example-based prompts to guide the model toward the correct answer **Brown et al., "Language Models play Hide and Seek"** . Chain-of-Thought (CoT) prompting enhances reasoning by breaking down problems into sequential steps, allowing the model to tackle each sub-problem in a structured manner **Haines et al., "Chain of Thought Prompt Engineering for Conversational AI"** . Two common CoT approaches include zero-shot CoT **Huang et al., "Zero-Shot Chain-of-Thought Prompting in Visual Question Answering"** , which encourages step-by-step reasoning without examples, and few-shot CoT **Lample et al., "Few-Shot Chain of Thought Reasoning"** , which includes rationales or explanations to guide reasoning.
Research has shown that few-shot CoT, by providing exemplar rationales, often outperforms zero-shot approaches **Haines et al., "Improving Few-Shot Learning with Chain-of-Thought Prompt Engineering"** . Another method, Least-to-Most (L2M) prompting **Welleck et al., "Efficient Transformers for Question Answering"** , adopts a similar multi-phase structure as CoT but without the use of rationales. Instead, L2M progressively breaks down the task, using the solution to each sub-problem as input for the next. In \voila, we extend the use of L2M to gain a deeper understanding of MLLMs' behavior across sub-tasks, from recognizing visual content to generating accurate images based on relational reasoning.


\paragraph{Multimodal Reasoning Benchmarks.}

Multimodal reasoning benchmarks have been instrumental in advancing the evaluation of MLLMs, integrating both textual and visual information to assess models' capabilities across a variety of domains. Domain-specific benchmarks like ScienceQA **Lai et al., "Science QA: A Large-Scale Science Question Answering Benchmark"** , A-OKVQA **Jasbi et al., "A-OKVQA: An Attention-based Open-Knowledge Visual Question Answering Framework"** , Math-Vision **Chen et al., "Math-Vision: A New Dataset for Mathematical Reasoning and Vision"** , and MMMU-Pro **Wang et al., "MMMU-Pro: Multi-Media Multi-Task Learning through Question Answering and Vision"** focus on specialized knowledge, such as scientific, mathematical, and visual question-answering reasoning tasks. Meanwhile, benchmarks such as CompBench **Jain et al., "CompBench: A Benchmark for Comprehension over Texts"** , MMRel **Li et al., "MMRel: A Multimodal Relational Reasoning Dataset"** , MARVEL **Shen et al., "MARVEL: A Multimodal Attention-based Reasoning and Vision Dataset"** , and ScanReason **Kim et al., "ScanReason: A Large-Scale Multi-Step Visual Question Answering Benchmark"** address more generalized multimodal relational reasoning.
Several studies also focus on multi-step reasoning tasks, where models must process information sequentially, such as VisualCoT **Zhang et al., "VisualCoT: A Novel Visual Question Answering Dataset with Chain-of-Thought Reasoning"** , LogicVista **Huang et al., "LogicVista: A Multimodal Logical Reasoning Benchmark"** , and VideoCoT **Li et al., "VideoCoT: A Large-Scale Multi-Step Video-Based Visual Question Answering Dataset"** , while datasets such as MuirBench **Wang et al., "MuirBench: A Multimodal Image Reasoning Benchmark"** , MIRB **Jasbi et al., "MIRB: A Multimodal Image Reasoning Benchmark with Vision and Language"** and MANTIS-Eval **Kumar et al., "MANTIS-Eval: An Efficient Evaluation Method for Multi-Modal Reasoning Tasks"** assess multiple image reasoning. 
Visual action planning **Santoro et al., "Learning to Plan from Natural Language"** and visual procedural planning **Sarafian et al., "Visual Procedural Planning with Deep Learning"** has also been explored to find relationships between pairs of images.
However, \voila distinguishes itself by focusing on high-order abstract relations and knowledge transfer across multiple images, challenging models not only to understand but also to generate both images and text while applying correct relational reasoning. These positions \voila as a unique benchmark for testing MLLMs' higher-level cognitive abilities.