\newpage
\appendix
% \section{Appendix}
% \label{sec: appendix}
\section{The \voila Dataset}
\subsection{Property Details}
\label{property-types}

We select three properties to utilize in the \voila: subject types, number of subjects and actions. In total 14 distinct subject types used in the images: cat, dog, fox, hamster, rabbit wolf, bear, and monkey, male child, female child, man, woman, senior man, and senior woman. Also, each image includes one subject type. The numbers selected for the dataset are 1, 2, 3 and 4. We want to restrict the numbers to avoid creating more complex configurations. Action property includes 13 physical activities: playing soccer, driving a car, ice-skating, walking, swimming, jumping, typing, writing, digging a hole, carrying something, reading, running, and eating food.

\subsection{Generated Images By SDXL}
\label{sdxl}

Utilizing the SDXL text-to-image model, we generated 30 images for each prompt using various properties. Some of the generated images are provided in Figures \ref{fig:8}, \ref{fig:9}, \ref{fig:10}, and \ref{fig:11} with their text prompts.


\begin{figure}[h]
    \centering
    \begin{minipage}{0.23\textwidth}
        \centering
         \includegraphics[width=\textwidth]{four_brown_bears_drivingacar_5.png} % Replace with your image
          \caption\small{Four brown bears driving a car}
          \label{fig:8}
    \end{minipage}
     \begin{minipage}{0.23\textwidth}
        \centering
         \includegraphics[width=\textwidth]{three_senior_women_writing_5.png} % Replace with your image
          \caption\small{Three senior women writing}
          \label{fig:9}
    \end{minipage}
    \begin{minipage}{0.23\textwidth}
        \centering
         \includegraphics[width=\textwidth]{one_dog_swimming_10111.png} % Replace with your image
          \caption\small{One dog swimming}
          \label{fig:10}
    \end{minipage}
    \begin{minipage}{0.23\textwidth}
        \centering
         \includegraphics[width=\textwidth]{two_hamsters_carrying_something_8111.png} % Replace with your image
          \caption\small{Two hamsters carrying something}
          \label{fig:11}
    \end{minipage}
\end{figure}



\subsection{Image Cleaning}
\label{image-cleaning} 

Some of the images generated by the model obtain some problems like object hallucinations and not depicting the action clearly. To eliminate these failure cases, we filter the images manually. Figure~\ref{fig:example_unmatched} shows some of the faulty images generated by the model.

\begin{figure}[h]
\centering
\subfloat[Three male adolescents swimming]{{\includegraphics[width=4cm]{three_male_adolescents_swimming_12.png} }}%
\quad
\subfloat[Two white foxes writing]{{\includegraphics[width=4cm]{two_white_foxes_writing_17.png} }}%
\caption{Unmatched images with text prompt.}%
\label{fig:example_unmatched}%
 \end{figure}

\subsection{\voila  Structure}
\label{voila-structure} 

The configuration of \voila benchmarks is determined by the rules and the number of properties. Table~\ref{tab:voila_rules_appendix} shows all possible rule settings for three property-based cases using all rules that represent the configuration of \voilawd. On the other hand, Table~\ref{tab:voila_cases_appendix} shows seven distinct cases excluding the Distraction rule. 


\begin{table}[h]
\caption{In total 19 cases are required to generate visual analogy questions in \voilawd.}
\small
  \centering
  \begin{tabular}{@{}lcccc@{}} \\\hline
    Rule  & Action  & Subject Type & Number of Subject \\\hline
    1 &Change    & Stable & Stable  \\
    2 &Change    & Stable & Distraction  \\ 
    3 &Change    & Distraction & Stable  \\
    4 &Change   & Distraction & Distraction  \\  
    5 &Stable  & Change & Stable  \\
    6 &Stable  & Change & Distraction  \\
    7 &Distraction  & Change & Stable  \\
    8 &Distraction  & Change & Distraction  \\
    9 &Stable  & Stable & Arithmetic  \\
    10 &Stable  & Distraction & Arithmetic  \\
    11 &Distraction  & Stable & Arithmetic  \\
    12 &Distraction  & Distraction & Arithmetic  \\\hline
    13 &Change    & Change & Stable  \\
    14 &Change    & Change & Distraction  \\ 
    15 &Stable  & Change & Arithmetic  \\
    16 &Distraction  & Change & Arithmetic  \\
    17 &Change  & Stable & Arithmetic  \\
    18 &Change  & Distraction & Arithmetic  \\\hline
    19 &Change  & Change & Arithmetic \\\hline
   \end{tabular}
  \label{tab:voila_rules_appendix}
\end{table}

\begin{table}[!h]
  \centering
    \caption{In total 7 cases are required to generate visual analogy questions in \voiland.}
  \begin{tabular}{@{}lcccc@{}} \\\hline
            Rule  & Action  & Subject Type & Number of Subject \\\hline
            1 &Change    & Stable & Stable  \\ 
            2 &Stable  & Change & Stable  \\
            3 &Stable  & Stable & Arithmetic  \\ \hline 
            4 &Change    & Change & Stable  \\         
            5 &Stable  & Change & Arithmetic  \\       
            6 &Change  & Stable & Arithmetic  \\  \hline    
            7 &Change  & Change & Arithmetic \\\hline
        

  \end{tabular}
  \label{tab:voila_cases_appendix}
\end{table}
 

\begin{table}[!h]
\caption{Number of different cases created by rules.}
  \centering
  \begin{tabular}{@{}lcccc@{}}\\\hline
    Properties & Stable &  Change & Arithmetic & Distraction \\\hline
    Action & 156 & 156 & -  & 22308  \\
    Number of Subjects & 12  & -  &16   & 24 \\ 
    Subject Type & 182 & 182 & - & 30576  \\\hline
  \end{tabular} 
  \label{tab:appendix_different_total_cases}
\end{table}

\subsection{Case Numbers}
\label{case-numbers} 

To calculate how many different cases we can generate using properties, we implement dual permutations for the Stable and Change rules and a permutation of three for the Distraction rules. Potential cases of the number of subjects are manually calculated. Table~\ref{tab:appendix_different_total_cases} shows the number of various possibilities for each property after assigning the rules. For 14 subject types, the Stable and Change rules generate 182 cases that can be used in the visual analogy questions. Since the Distraction number has a large amount of data variation, I fixed the generated number of data according to the permutation results of Changes and Stable rules. Utilizing these variations, we can create more than 6.5M analogy questions.



\section{Experiment Results}
\label{main-results}

\subsection{Direct Answering}
\label{direct-answering}

The reasoning workflow is applied with final determined prompts progressively for each model. We tested the direct answering approach for some benchmark models on both \voilawd and \voiland. The models are tested on a zero-shot prompting approach without a multi-step process. The steps of image description and understanding the relations are skipped in that study to directly receive the answer from the model. The results are provided in Table~\ref{tab:results-one-prompt}. 


\begin{table}[!ht]
\begin{tabular}{@{}cc@{}}
    
    \begin{minipage}{0.48\textwidth}
    %\begin{subfigure}[b]{0.5\textwidth}
    \input{tables/voila_results_one_prompt}
    \end{minipage}&
    \begin{minipage}{0.48\textwidth}
        \input{tables/voila_results_gen_images}
    \end{minipage}
\end{tabular}
\end{table}

\subsection{Image Generation}
\label{image-generation}

Image generation is the last step of the visual analogy questions. To answer the question correctly, MLLMs that are capable of creating visual outputs, need to generate the accurate images aligned with the ground truths. We tested GPT-4o and Seed LLaMa on \voilawd and \voiland datasets to evaluate their performance of image generation after the relational inference step. Since \voiland has less challenging questions, the models perform better on \voiland rather than \voilawd which obtains some distraction rules. Since the models struggle to maintain their performance until the last step, the accuracy of the image generation performance is below 5\% of the best-performing model (GPT-4o) on \voilawd. The more detailed results are provided in Table~\ref{tab:results-gen-images}.

\subsection{\voilawd Property-based Evaluation}
\label{property-voilawd}

 We comprehensively analyze the performance of the baseline models against the accuracy of the properties of each step. Table~\ref{property-table} and Figure~\ref{fig:property_bar_chart} show the performance of various models in \voilawd with respect to different configurations such as using L2M or directly answering and entering an image collage or three sequential images. The results demonstrate that GPT-4o performs better than other models in understanding and identifying relations across all properties in visual content on \voilawd. However, LLaMa 3.2 shows more thriving performance in applying relationships on subject types and actions.

\setlength{\tabcolsep}{3pt} 
\begin{table}[t]
    \centering
    \caption{Performance of various models on different steps \voilawd. Model names: IC = image collage, 3I = three sequential images, L2M = least-to-most prompting.}
    \label{property-table}
    \scriptsize
    \begin{tabular}{@{}lccccccccccccccccc@{}}
        \toprule
          & \multicolumn{3}{c}{\textit{Describing Images}} & \multicolumn{3}{c}{\textit{Identifying Relations}} & \multicolumn{3}{c}{\textit{Applying Relationship}} & \multicolumn{3}{c}{\textit{Generating Image}} \\ 
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}  \cmidrule(lr){11-13} 
        \textit{Model} & Number & Subject & Action & Number & Subject & Action & Number & Subject & Action & Number & Subject & Action \\ 
        \midrule
        Humans (MTurk) &-  &- &-  &-  &-  &- &\textbf{93.6}  &\textbf{82.3}  &\textbf{91.4} &-  &-  &-  \\
        \midrule
        CogVLM2 (L2M)  &71.1  &94.1  & 78.2 &34.3  &50.6  &37.3   &6.6   &12.6  &26.9 &-  &-  &-   \\ 
        LLaVa (L2M) &75.4  &87.8  &67.5  &30.7  &32.6  &27.8  &31.2  &30.0  &32.9   &-  &-  &-  \\
        QWEN2-VL (L2M) &62.0 &89.2  &76.1  &32.2  &53.6  &38.8  &3.16  &21.8  &1.9  &-  &-  &- \\ 
        QWEN2-VL (3I) (L2M) &96.9  &95.8  &81.0  &58.9  &57.9  &41.1 &11.6  &17.6  &20.8  &-  &-  &-  \\
        MolmoE (3I) (L2M)   &14.57  &32.22  &22.18  &6.61     &10.92    &8.02  &5.62  &10.06   &10.63&-   &-  &- \\
        LLaMa 3.2 (IC) (L2M)   &79.2  &91.3  &87.6  &50.2   &58.5   &49.9  &29.7  &\textbf{40.1}   &\textbf{39.1} &-   &-  &- \\
        SEED-LLaMa (3I) (L2M) &89.1  &87.3 &\textbf{93.7}  &48.8  &45.8 &48.0  &12.1  &19.7  &24.8  &- &- &- \\
        GPT-4o (L2M) &86.0 &95.9  &78.8  &68.1  &70.9  &46.5  &25.4  &33.8  &33.2 &3.4  &3.7  &3.2   \\ 
        GPT-4o (3I)(L2M)  &\textbf{99.6} &\textbf{97.0}  &81.7  &\textbf{94.3}  &\textbf{78.5}   &\textbf{55.9}  &\textbf{33.8} &36.5  &35.0 &\textbf{5.6}  &\textbf{6.1}  &\textbf{5.6}   \\
        GPT-4o (3I) &-  &-  &-  &-  &-  &-  &10.9  &13.7  &27.5 &0.8  &0.8  &0.8    \\ 
        SEED-LLaMa (3I) &-  &-  &-  &-  &-  &- &5.1  &16.9  &25.8  &1.1  &0.7 &0.7 \\  
        Emu-2 &-  &-  &-  &-  &-  &- &3.0  &6.1  &12.5  &0.2  &0.2  &0.1  \\  
        CogVLM2   &-  &-  &-  &-  &-  &- &7.8  &8.2  &23.7   &-  &-  &-  \\    
        SEED-LLaMa &-  &-  &-  &-  &-  &- &9.2  &30.2  &6.8  &-  &- &-  \\ 
       
        
        \bottomrule
    \end{tabular}
\end{table}



\subsection{\voiland Property-based Evaluation}
\label{property-voiland}

We evaluated models regarding property-based performance at each step on \voiland under different configurations, including the use of L2M versus direct answering, as well as the input format of an image collage compared to three sequential images. As the distraction rule affects the application relationships stage, the model performances at the initial two stages show a similar pattern as in the \voilawd dataset. The results provided in Table~\ref{tab:voiland_property_appendix} and Figure~\ref{fig:appendix_voiland} demonstrate that all models, excluding LLaMa 3.2, achieved better performance on \voiland and increased the accuracy of property predictions in the final step. In contrast, LLaMa 3.2 exhibited a decrease in performance at the last stage, particularly in the application of number relations. Notably, GPT-4o outperformed other models in applying number and subject relations.

\setlength{\tabcolsep}{3pt} 
\begin{table}[h]
    \centering
    \caption{Performance of various models on different steps \voiland. Model names: IC = image collage, 3I = three separate images, L2M = least-to-most prompting.}
    \label{tab:voiland_property_appendix}
    \scriptsize
    \begin{tabular}{@{}lccccccccccccccccc@{}}
        \toprule
         & \multicolumn{3}{c}{\textit{Describing Images}} & \multicolumn{3}{c}{\textit{Identifying Relations}} & \multicolumn{3}{c}{\textit{Applying Relationship}} & \multicolumn{3}{c}{\textit{Generating Image}} \\ 
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}  \cmidrule(lr){11-13} 
        \textit{Model} & Number & Subject & Action & Number & Subject & Action & Number & Subject & Action & Number & Subject & Action \\ 
        \midrule
        Humans (MTurk) &- &-  &-  &-  &-  &-  &\textbf{92.4}  &\textbf{85.4}  &\textbf{91} &-  &-  &-  \\ 
        \midrule
        CogVLM2 (L2M)   &72.9  &95  &79  &45.7  &59.1  &45.8  &23.8  &37.1  &21.4   &-  &-  &-  \\
        QWEN2-VL (L2M) &56.8 &86.7  &74.9  &30.7  &54  &34.9  &19.8  &25.8  &21.7  &-  &-  &- \\  
        QWEN2-VL (3I)(L2M) &96.9 &95.4  &81  &61.9  &57.5  &42.4  &15.8  &27  &27.2  &-  &-  &- \\ 
        MolmoE (3I) (L2M)   &15.5  &37.6  &26.9  &6.23  & 10.6  &8.5 &8.9  &22   &14.2 &-   &-  &- \\
        LLaMa 3.2 (IC) (L2M)   &77.8  &90.9  &\textbf{88.4}  &50.9   &60.9    &49.2  &15.1  &50.1   &48.3 &-   &-  &- \\
        SEED-LLaMa (3I)(L2M) &77.9  &86.8  &73.1  &48.1  &54.9  &37.4 &7.2  &21.8  &27.2  &- &-  &-  \\  
        GPT-4o (L2M) &84.8 &95.9  &78.9  &69.2  &74.3  &45.8  &45.8  &61.2  &44.9 &15.9  &17.9  &16.9     \\ 
          GPT-4o(3I)(L2M)  &\textbf{99.6} &\textbf{96.6}  &81.5  &\textbf{94}  &\textbf{77.6}  &\textbf{51.4}   &\textbf{66.4} &\textbf{65.6}  &\textbf{49.1} &\textbf{24.3}  &\textbf{27.2}  &\textbf{26.1}  \\
          GPT-4o(3I) &-  &-  &-  &-  &-  &- &28.9  &45.5  &18.7 &14.4  &15.1  &14.6   \\  
        Emu-2 &-  &-  &-  &-  &-  &-  &6.2  &8.7  &13.1 &0.4  &0.7  &0.3 \\ 
        CogVLM2  &-  &-  &-  &-  &-  &-  &13  &19.9 &17.9  &-  &-  &-  \\     
        SEED-LLaMa (3I) &-  &-  &-  &-  &-  &- &6.8  &7.8  &24.3  &-  &- &-  \\ 
        SEED-LLaMa &-  &-  &-  &-  &-  &- &4.3  &11.9  &13.1  &-  &- &- \\ 

        \bottomrule
    \end{tabular}
\end{table}


\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{images/no_distractions_bar.pdf}
    \caption{Property performance of models at each step on \voiland}
    \label{fig:appendix_voiland}
\end{figure}


\subsection{GPT-4o Evaluation Performance}
\label{evaluation}
We measured the gap between human and GPT-4 evaluations utilizing confusion tables for each step including all attributes. In total 50 visual analogy questions and 180 responses were evaluated.
\begin{itemize}
    \item True Positive (TP): The number of cases where both the human and the GPT-4o agree that the response is correct.
    \item False Negative (FN): The number of cases where the human expresses the answer is correct, but the GPT-4o says it is incorrect.
    \item False Positive (FP): The number of cases where the human says the answer is incorrect, but the GPT-4o states it is correct.
    \item True Negative (TN): The number of cases where both the human and the GPT-4o agree that the response is incorrect.
    \item Accuracy (Agreement): The number of cases where both the human and the GPT-4o agree. 
    Accuracy (Agreement rate) = (TP + TN) / (TP + TN + FP + FN)
\end{itemize}

First, we calculated false negative and false positive cases between human and GPT-4o evaluations for each step, attribute, and question answer. Then we computed the accuracy also called the agreement rate. The results of each step regarding the attributes are provided in Tables \ref{tab:step1}, \ref{tab:step2}, \ref{tab:step3}, and \ref{tab:step4}. The “Question” in tables represents the question answer merged with three properties (number + subject + action).

The results show the agreement rate between human evaluation and GPT-4o was 91\% to describe images, 94\% to identify relationships, 92\% to apply relationships, and 91\% to generate images. Although the attribute agreement rate is the lowest at 74\%, the precision to answer questions, which require integration properties, exceeds 91\%.


% Step 1 - Describing Images and Step 2 - Identifying Relations side by side
\begin{table}[!ht]
    \centering
    %\caption{Step 1 - Describing Images and Step 2 - Identifying Relations}
    \begin{minipage}{0.5\textwidth}
        \centering
        \caption{Step 1 - Describing Images}
        \label{tab:step1}
        \begin{tabular}{lcccc}
            \toprule
            Attribute & FP & FN & TP + TN & Accuracy \\
            \midrule
            Number   & 1  & 7  & 142 & 95\% \\
            Subject  & 5  & 5  & 140 & 93\% \\
            Action   & 5  & 14 & 131 & 87\% \\
            Question & 6  & 7  & 137 & \textbf{91}\% \\
            \bottomrule
        \end{tabular}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \caption{Step 2 - Identifying Relations}
        \label{tab:step2}
        \begin{tabular}{lcccc}
            \toprule
            Attribute & FP & FN & TP + TN & Accuracy\\
            \midrule
            Number   & 4  & 5  & 39  & 78\% \\
            Subject  & 1  & 12 & 37  & 74\% \\
            Action   & 2  & 8  & 40  & 80\% \\
            Question & 1  & 2  & 47  & \textbf{94}\% \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}

% Step 3 - Applying Relationships and Step 4 - Generating Images side by side
\begin{table}[!ht]
    \centering
    %\caption{Step 3 - Applying Relationships and Step 4 - Generating Images}
    \begin{minipage}{0.5\textwidth}
        \centering
        \caption{Step 3 - Applying Relationships}
        \label{tab:step3}
        \begin{tabular}{lcccc}
            \toprule
            Attribute & FP & FN & TP + TN & Accuracy\\
            \midrule
            Number   & 3  & 8  & 39  & 78\% \\
            Subject  & 4  & 7  & 39  & 78\% \\
            Action   & 8  & 4  & 38  & 76\% \\
            Question & 2  & 2  & 46  & \textbf{92}\% \\
            \bottomrule
        \end{tabular}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \caption{Step 4 - Generating Images}
        \label{tab:step4}
        \begin{tabular}{lcccc}
            \toprule
            Attribute & FP & FN & TP + TN & Accuracy\\
            \midrule
            Number   & 5  & 2  & 23  & 77\% \\
            Subject  & 1  & 4  & 25  & 83\% \\
            Action   & 2  & 4  & 24  & 80\% \\
            Question & 1  & 2  & 27  & \textbf{90}\% \\
 \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}


\subsection{AnyRes Strategy With Image Collages}
\label{anyres}
To investigate whether the AnyRes strategy would improve the performance of collaged images, we experimented with the LLaVA-OneVision \cite{li2024llavaonevisioneasyvisualtask} on \voiland utilizing both image collage and multiple image formats. We tested the model only in the first stage where the images were processed. The model utilizing image collage achieves an accuracy of 53\%, while the model using multiple images performs slightly better, with 57\% accuracy in describing images. Table \ref{tab:llava} summarizes the results, showing that the AnyRes approach improves the image resolution and closes the performance gap between the process of image collage and multiple images. 

\begin{table}[!ht]
    \centering
    \caption{Performance of LLaVA-OneVision on different input types}
    \label{tab:llava}
    \begin{tabular}{l l c c c c}
        \toprule
        Method & Input Type & Number & Subject & Action & Total \\
        \midrule
        LLaVA-OneVision & Image Collage & 63.8\% & \textbf{94.5}\% & \textbf{84.3}\% & 53\% \\
        LLaVA-OneVision & Three Sequential Images & \textbf{67.9}\% & 73.7\% & 67.5\% & \textbf{57.5}\% \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Impact of Sub-prompts on Identifying Relationships}
\label{subprompts}
We experimented with GPT-4o on the \voilawd dataset to discover the impact of employing sub-prompts for determining property relationships between first and second images. For a fair comparison, we froze the first-step answers and requested the model to find whether the number/subject/action changed or remained the same from the first image to the second image. After merging the results from three sub-tasks, we achieved a similar accuracy of 42\% with a single prompt experiment. The accuracy of properties is also similar with 94\% for numbers, 79\% for subjects, and 56\% for action. The relationship identification performance of GPT-4o with single and three sub-prompts, provided in Table \ref{tab:3subprompts} demonstrates that GPT-4o's ability to identify relationships is not affected by the number of properties asked in the prompt. 

\begin{table}[h]
    \centering
    \caption{Performance of GPT-4o on identifying relationships with different prompting approaches}
        \label{tab:3subprompts}
    \begin{tabular}{l l c c c c}
        \toprule
        Model & Approach & Number & Subject & Action & Total \\
        \midrule
        GPT-4o & Single Prompt & \textbf{94.3}\% & 78.5\% & 55.9\% & \textbf{42.8}\% \\
        GPT-4o & Three Sub-prompts & 94.1\% & \textbf{79.3}\% & \textbf{56.3}\% & 42.3\% \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{CoT vs L2M For Applying Relations}
\label{sec:COT}

To evaluate the effectiveness of CoT in the “Applying Relationship” step, we conducted an experiment utilizing GPT-4o on the \voilawd dataset. For a fair comparison of implementing CoT and L2M for step 3, we froze the first and second-step answers and provided two textual examples and their rationales. We requested from model to find the properties of the fourth image by providing previous sub-task answers. The result of the study provided in Table \ref{CoT} shows that the L2M approach with detailed instructions performs slightly better than the CoT approach for this task.

\begin{table}[!ht]
    \centering
    \caption{Performance of GPT-4o with different approaches}
    \label{CoT}
    \begin{tabular}{l l l c c c c}
        \toprule
        Model & Approach & Inputs & Number & Subject & Action & Total \\
        \midrule
        GPT-4o & CoT & Two Examples & \textbf{48.8}\% & 33.1\% & 29\% & 5.96\% \\
        GPT-4o & L2M & Instructions & 33.8\% & \textbf{36.5}\% & \textbf{35.0}\% & \textbf{6.44}\% \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Human vs Models Using Examples and Options}
\label{HumanEx}
We conducted an ablation study using GPT-4o on the \voilawd dataset to evaluate the performance gap between human and model responses in the same conditions where two example questions and property options are provided. As the human evaluation was performed in one step, we set up the experiment for the direct answering approach. In addition to the example images and lists, we also provided detailed rationales to support the solutions. The experimental results provided in Table \ref{tab:humanev} demonstrate that offering examples and option lists impacts the model’s performance similar to the L2M approach employing step-by-step instructions. Although the model selects each property answer from the given list and analyzes example images and logic, its performance is significantly below the human performance, with a notable 65\% accuracy gap. 

\begin{table}[h]
    \centering
    \caption{Performance comparison between human evaluation and models}
    \label{tab:humanev}
    \begin{tabular}{l l l c c c c}
        \toprule
        Method & Approach & Inputs & Number & Subject & Action & Total \\
        \midrule
        Human & Direct Answering & Two Examples and List & \textbf{93.6}\% & \textbf{82.3}\% & \textbf{91.4}\% & \textbf{71.36}\% \\
        GPT-4o & Direct Answering & Two Examples and List & 46.6\% & 25.28\% & 64.4\% & 6.8\% \\
        GPT-4o & L2M & Instructions & 33.8\% & 36.5\% & 35.0\% & 6.44\% \\
        \bottomrule
    \end{tabular}
\end{table}



\subsection{Failure Case}
\label{failure}

An example of how visual analogy questions evaluate the MLLMs step-by-step is provided in Figure \ref{fig:failure}. In the example, we employ GPT-4o using the L2M approach with a sequence of three images.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{failure_case_gpt4o.pdf}
    \caption{Failure L2M case of GPT-4o on \voilawd}
    \label{fig:failure}
\end{figure}

\subsection{Success Case}
An example of how GPT-4o successfully reaches the solution is provided in Figure \ref{fig:success}. To show all stages of the task, including image generation, we utilize GPT-4o in the example. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{success_case_gpt-4o_voilawd_any.pdf}
    \caption{Success L2M case of GPT-4o on \voilawd}
    \label{fig:success}
\end{figure}



\subsection{Evaluation Prompts Step by Step}
After receiving the answers from MLLMs for using the L2M method, we execute the evaluation pipeline which consists of multiple stages. Figures \ref{fig:step1ev}, \ref{fig:step2ev}, \ref{fig:step3ev}, and \ref{fig:step4ev} show the evaluation of the models' outputs step by step.

\label{eval-step-by-step}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{prompt1_eva.pdf}
    \caption{Step 1 evaluation}
    \label{fig:step1ev}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{prompt2_eva.pdf}
    \caption{Step 2 evaluation}
    \label{fig:step2ev}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{prompt3_eva.pdf}
    \caption{Step 3 evaluation}
    \label{fig:step3ev}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{eva-prompt4.png}
    \caption{Step 4 evaluation}
    \label{fig:step4ev}
\end{figure}

\clearpage

% \label{sec: appendix}
\section{Prompts}
\subsection{Prompt Selecting}
\label{prompt-selecting}
To find the best-performing parameters, we tested various text prompts on MLLMs on \voiland and \voilawd utilizing 0-shot and least-to-most prompting. Since \voilawd includes distractions, we modify the text prompts for \voiland by excluding the explanation of distraction rules. Figure~\ref{fig:zeroshot_pormpts} displays diverse zero-shot prompts we tried before selecting the final instructions. For L2M prompting, we need multiple text instructions for each step. Various prompts are tested for each sub-problem in a multiple-step reasoning process, see Figures \ref{fig:zeroshot_pormpts}, \ref{fig:first_prompt}, \ref{fig:second_prompt}, and \ref{fig:third_prompt}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{single_prompt.pdf}
    \caption{Zero-shot prompt selection.}
    \label{fig:zeroshot_pormpts}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{prompt1-try.png}
    \caption{First prompt selection of L2M process}
    \label{fig:first_prompt}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{prompt2-try.png}
    \caption{Second prompt selection of L2M process.}
    \label{fig:second_prompt}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{multi-3-prompt.pdf}
    \caption{Third prompt selection of L2M process.}
    \label{fig:third_prompt}
\end{figure}



% \label{sec: appendix}
\newpage
\section{Human Experiment}

\subsection{MTurk Instructions}
\label{mturk-human-experiment}

The human study was conducted by using the Amazon Mechanical Turk platform. Participants solve 440 various analogy questions in total. The instructions and example questions of the study were provided in Figure~\ref{fig:mturk_example}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=14cm]{MTurk_screenshot.png}  % Replace filename.png with the actual image file name
  \caption{Human experiment of MTurk platform.}
  \label{fig:mturk_example}
\end{figure}

