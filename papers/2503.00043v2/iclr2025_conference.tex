
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
\usepackage{graphicx} 
\usepackage{url}
\usepackage{subfig}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{ragged2e}
\usepackage{enumitem}
\usepackage{xspace}

\newcommand{\voila}{\texttt{VOILA}\xspace}
\newcommand{\voilawd}{\texttt{VOILA-WD}\xspace}
\newcommand{\voiland}{\texttt{VOILA-ND}\xspace}
\usepackage{gradient-text}

\newcommand{\hrefgrad}[2]{\href{#1}{\gradientRGB{#2}{65,105,225}{65,105,225}}}


\title{\voila: Evaluation of MLLMs for Perceptual Understanding and Analogical Reasoning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
  Nilay Yilmaz\textsuperscript{$\diamondsuit$}\thanks{Corresponding author: \hrefgrad{mailto:nyilmaz3@asu.edu}{nyilmaz3@asu.edu}. Code and data: \hrefgrad{https://github.com/nlylmz/Voila}{github.com/nlylmz/Voila}}\quad
  Maitreya Patel\textsuperscript{$\diamondsuit$}\quad
  Yiran Lawrence Luo\textsuperscript{$\diamondsuit$}\quad
  \textbf{Tejas Gokhale}\textsuperscript{$\heartsuit$}\quad
  \textbf{Chitta Baral}\textsuperscript{$\diamondsuit$}\quad \\
   \textbf{Suren Jayasuriya}\textsuperscript{$\diamondsuit$}\quad
  \textbf{Yezhou Yang}\textsuperscript{$\diamondsuit$} \\
  \textsuperscript{$\diamondsuit$}Arizona State University \quad \textsuperscript{$\heartsuit$}University of Maryland, Baltimore County\\
  % \hrefgrad{https://github.com/nlylmz/Voila}{github.com/nlylmz/Voila}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\let\cite\citep % added by tgokhale

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

Multimodal Large Language Models (MLLMs) have become a powerful tool for integrating visual and textual information. Despite their exceptional performance on visual understanding benchmarks, measuring their ability to reason abstractly across multiple images remains a significant challenge. To address this, we introduce \voila, 
% \footnote{Code and data: \hrefgrad{https://github.com/nlylmz/Voila}{github.com/nlylmz/Voila}}
a large-scale, open-ended, dynamic benchmark designed to evaluate MLLMs' perceptual understanding and abstract relational reasoning. \voila employs an analogical mapping approach in the visual domain, requiring models to generate an image that completes an analogy between two given image pairs, reference and application, without relying on predefined choices. Our experiments demonstrate that the analogical reasoning tasks in \voila present a challenge to MLLMs.
Through multi-step analysis, we reveal that current MLLMs struggle to comprehend inter-image relationships and exhibit limited capabilities in high-level relational reasoning. Notably, we observe that performance improves when following a multi-step strategy of least-to-most prompting. Comprehensive evaluations on open-source models and GPT-4o show that on text-based answers, the best accuracy for challenging scenarios is 13\% (LLaMa 3.2) and even for simpler tasks is only 29\% (GPT-4o), while human performance is significantly higher at 70\% across both difficulty levels.

\end{abstract}



\section{Introduction}
Multimodal Large Language Models (MLLMs) have made remarkable strides in advancing human-level language processing and visual perception in tasks such as image captioning \citep{vinyals2015tellneuralimagecaption}, visual question answering \citep{agrawal2016vqavisualquestionanswering}, object detection, and scene understanding \citep{bochkovskiy2020yolov4optimalspeedaccuracy}. While these advancements are promising, perceptual reasoning tasks such as relational and analogical reasoning, where models must infer and understand visual information, remain a significant challenge. Achieving human-level cognitive intelligence in these tasks demands greater attention and development.

According to Bloomâ€™s taxonomy of educational objectives, creation, rather than evaluation, requires the highest cognitive skills in the learning process \citep{bloom1956}. However, many current multimodal reasoning tasks \citep{wang2024mmluprorobustchallengingmultitask, plummer2016flickr30kentitiescollectingregiontophrase} rely on multiple-choice formats, where models select a solution from a predefined set. Although this approach provides insight into the learning and understanding capabilities of a model, we argue that it fails to reveal the model's ability to engage in high-level cognitive tasks involving the interpretation of visual context and abstract reasoning.
To attain human-level cognitive intelligence, MLLMs must go beyond evaluating options; they must generate solutions for complex tasks that require advanced reasoning skills. 
Existing studies on open-ended visual reasoning tasks \citep{zellers2019recognitioncognitionvisualcommonsense,zhang2019ravendatasetrelationalanalogical,johnson2016clevrdiagnosticdatasetcompositional}, which assess MLLMs' cognitive capabilities, are limited in scope and do not fully explore these higher-order reasoning abilities.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/f1.pdf}
    \caption{
    Examples of visual analogy questions from the \voila benchmark with distractions (\voilawd) and without distractions (\voiland). 
    The aim is to generate an image that completes the analogy problem following perceptual and relational reasoning. 
    Each visual analogy question has a specific rule configuration that leads to the answer. 
    The questions in \voilawd benchmark can apply the distraction rule when no relational pattern exists between images. 
    For the examples shown above, the answers that complete the \voiland analogy by following the relation rules are \textit{``two bears driving a car''} and \textit{``two female children swimming''}. 
    The answers to the \voilawd analogy are \textit{``four [any subject] reading''} and \textit{``two male children doing anything''}.}%
   \label{fig:onecol}
\end{figure}

In response to these challenges, we introduce \voila: an open-ended reasoning benchmark designed to evaluate whether MLLMs possess vision-level understanding and relational reasoning capabilities. Our benchmark focuses on an \textit{analogical reasoning} task which has been developed as a cognitive assessment of problem-solving, decision-making, classification, and learning processes \cite{goswami1992analogical}. Analogical reasoning consists of diverse atomic abilities; perceptual understanding, mapping abstract relationships between visual contents \citep{GENTNER1983155}, and transferring relational patterns to novel cases. These integral sub-tasks are essential for achieving a coherent solution to the analogy problem. A key aspect of analogical reasoning is the transfer of knowledge from previously learned relations to new concepts. The task involves two image pairs: a reference pair and an application pair. The reference pair shares both visual and abstract contextual information. The goal is to infer the relationship and apply it to predict the unknown content in the application pair.  A few examples are shown in Figure~\ref{fig:onecol}.

Our benchmark incorporates a multi-step reasoning process, which is crucial for analyzing where MLLMs encounter difficulties. This process allows for a detailed examination of the models' limitations about the key properties of the task. To introduce varying levels of difficulty, we created two sub-datasets: \voilawd and \voiland. Our findings indicate that \voilawd presents a greater challenge than \voiland due to the introduction of visual distractions where a certain property of the image (e.g. subject, number, task) is irrelevant to the analogical reasoning. Experimental results reveal that state-of-the-art MLLMs struggle on \voila. Although GPT-4o reaches 79\% accuracy in the description of the image and 43\% precision in the identification of the relationship stages, it struggles to produce correct answers to complete the analogy.
LLaMa 3.2 achieves the highest performance, attaining 13\% accuracy in implementing the relationship stage on \voilawd. Interestingly, GPT-4o outperforms other models on \voiland, achieving an accuracy of 29\% in applying relationships. However, human performance significantly surpasses these results, achieving 71\% and 69\% accuracy on \voilawd and \voiland, respectively.  

We observe that least-to-most (L2M) prompting~\citep{zhou2023leasttomostpromptingenablescomplex} improves model accuracy compared to direct answering strategies. Input formats also influence model performance: using sequential images instead of a single image collage results in an average of 40\% improvement. Additionally, the image generation step notably reduces model performance.  We conduct an ablation study on \voilawd using GPT-4o, showing that even when ground truth information (image descriptions or relationships) is provided at each step, the model achieves only 17\% accuracy on the applying relations step. 
Another ablation study on the \voiland benchmark demonstrates that the model's performance improves by 27\% when textual information is used instead of visual input.
We describe our experimental setup and findings in Sections \ref{sec:exp_setup} and \ref{sec:exp_results} in detail.
 


Our contributions and findings are summarized below:
\begin{itemize}[nosep,noitemsep,leftmargin=*]
    \item We present \voila, a large-scale, open-ended benchmark to evaluate MLLMs' high-level visual reasoning capabilities.
    We introduce a method for the dynamic creation of extensive visual analogy questions utilizing text-to-image models.
    \item We assess state-of-the-art MLLMs on \voila, revealing a significant performance gap between humans (71\%) and MLLMs (13\%). 
    \item We conduct comprehensive investigation of factors influencing performance: image format, prompting techniques, distraction rules, input information types, and provision of ground truths.
\end{itemize}
% (1) We present \voila, a large-scale, open-ended benchmark to evaluate MLLMs' high-level visual reasoning capabilities. (2) We introduce a method for the dynamic creation of extensive visual analogy questions utilizing text-to-image models. (3) We assess state-of-the-art MLLMs on \voila, revealing a significant performance gap between humans (71\%) and MLLMs (13\%). (4) We conduct comprehensive studies to analyze factors influencing model performance, including image format, prompting techniques, distraction rules, input information types, and the provision of ground truths.
\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{voila_data_pipe.png}  % Replace filename.png with the actual image file name
  \caption{Dataset creation pipeline of \voila.}
  \label{fig:data_pipe}
\end{figure}
\section{Related work}
\paragraph{Visual Analogical Reasoning.}
Visual Analogical Reasoning is a complex task that requires models to recognize abstract similarities between visual contexts and underlying relational rules and to apply these rules to novel visual content to generate solutions. Ravenâ€™s Progressive Matrices (RPMs) \cite{raven1938progressive}, introduced in 1938, are one of the earliest visual analogical reasoning tests, designed to assess human intelligence by completing a 3x3 matrix of visual patterns based on relational reasoning. Recent visual analogy benchmarks, such as VISALOGY \citep{sadeghi2015visalogyansweringvisualanalogy} and VASR \citep{bitton2022vasrvisualanalogiessituation}, offer valuable perspectives but have limitations. VISALOGY, which is not publicly available, focuses on attributes and actions in Google images with manual annotations, while VASR emphasizes general image understanding rather than feature-based relationships. In contrast, \voila introduces a more complex and dynamic approach to visual analogical reasoning. It incorporates diverse subject relationships and rule-based structures and manipulates up to three properties at a time. Additionally, \voila introduces distraction elements to increase task difficulty, requiring models to discover and filter out the irrelevant changes among properties while solving analogy questions. Unlike static datasets, \voila allows the generation of over 6.4M distinct visual analogy scenarios across 14 subject types, 13 actions, and 4 numeric values by adjusting flexible property-rule configuration, offering a scalable and adaptable evaluation platform for MLLMs.

\paragraph{Prompting.} 
Prompting strategies are critical to improving the performance of MLLMs, especially in complex reasoning tasks. Zero-shot prompting provides models with task instructions without examples, while few-shot prompting includes example-based prompts to guide the model toward the correct answer \citep{brown2020languagemodelsfewshotlearners}. Chain-of-Thought (CoT) prompting enhances reasoning by breaking down problems into sequential steps, allowing the model to tackle each sub-problem in a structured manner \citep{wei2022chain}. Two common CoT approaches include zero-shot CoT \citep{kojima2022large}, which encourages step-by-step reasoning without examples, and few-shot CoT \citep{wei2022chain}, which includes rationales or explanations to guide reasoning.
Research has shown that few-shot CoT, by providing exemplar rationales, often outperforms zero-shot approaches \citep{zhang2024multimodalchainofthoughtreasoninglanguage}. Another method, Least-to-Most (L2M) prompting \citep{zhou2023leasttomostpromptingenablescomplex}, adopts a similar multi-phase structure as CoT but without the use of rationales. Instead, L2M progressively breaks down the task, using the solution to each sub-problem as input for the next. In \voila, we extend the use of L2M to gain a deeper understanding of MLLMs' behavior across sub-tasks, from recognizing visual content to generating accurate images based on relational reasoning.


\paragraph{Multimodal Reasoning Benchmarks.}

Multimodal reasoning benchmarks have been instrumental in advancing the evaluation of MLLMs, integrating both textual and visual information to assess models' capabilities across a variety of domains. Domain-specific benchmarks like ScienceQA \citep{lu2022learn}, A-OKVQA \citep{schwenk2022aokvqabenchmarkvisualquestion}, Math-Vision \citep{wang2024measuringmultimodalmathematicalreasoning}, and MMMU-Pro \citep{yue2024mmmu} focus on specialized knowledge, such as scientific, mathematical, and visual question-answering reasoning tasks. Meanwhile, benchmarks such as CompBench \citep{kil2024compbenchcomparativereasoningbenchmark}, MMRel \citep{nie2024mmrelrelationunderstandingdataset}, MARVEL \citep{jiang2024marvelmultidimensionalabstractionreasoning}, and ScanReason \citep{zhu2024scanreasonempowering3dvisual} address more generalized multimodal relational reasoning.
Several studies also focus on multi-step reasoning tasks, where models must process information sequentially, such as VisualCoT \citep{shao2024visualcotadvancingmultimodal}, LogicVista \citep{xiao2024logicvistamultimodalllmlogical}, and VideoCoT \citep{wang2024videocotvideochainofthoughtdataset}, while datasets such as MuirBench \cite{wang2024muirbenchcomprehensivebenchmarkrobust}, MIRB \cite{zhao2024mirb} and MANTIS-Eval \cite{Jiang2024MANTISIM} assess multiple image reasoning. 
Visual action planning \cite{gokhale2019cooking} and visual procedural planning \cite{chang2020procedure,su2024actplan} has also been explored to find relationships between pairs of images.
However, \voila distinguishes itself by focusing on high-order abstract relations and knowledge transfer across multiple images, challenging models not only to understand but also to generate both images and text while applying correct relational reasoning. These positions \voila as a unique benchmark for testing MLLMs' higher-level cognitive abilities.



\section{Constructing the \voila Benchmark }
\label{gen_inst}
 
The \voila benchmark was designed to evaluate the abstract reasoning capabilities of MLLMs. This task challenges models to process perceptual information and apply relational reasoning by interpreting visual content from three given images to generate a fourth image according to a specified pattern. \voila is a large-scale dataset that dynamically generates visual analogy questions based on demand and configuration. The dataset can generate over 6.4M questions, distributed across 19 unique structures and utilizing a total of 7,280 images which makes \voila highly scalable and adaptable to various configurations. Figure \ref{fig:data_pipe} illustrates the dataset creation pipeline of \voila.


\subsection{Dataset Creation Pipeline}
\paragraph{Property Identification.}

The \voila dataset is generated using an image analogy framework ($A : A^\prime :: B : B^\prime$). Each of the first three images contains distinct properties that form the basis for the visual analogy questions. We identified three key properties: the number of subjects, subject type, and action. In the \voila benchmark, each question \(q_{i}\) includes three images \({(I_{1}, I_{2}, I_{3})}\), with each image containing corresponding properties  \({(n_{i}, s_{i}, a_{i}) \in P}\). A total of 14 subjects, 4 numbers, and 13 actions were used to create the image dataset. For further details regarding the categorical information of the property types, please refer to Appendix \ref{property-types}.



\medskip\noindent\textbf{Rule Definition.} To structure each visual analogy question, four types of rules are applied in \voila: Stable, Change, Arithmetic, and Distraction. The rules are assigned to the properties as outlined in Table~\ref{tab:rules}, with each image containing rule-property pairs \({I(r_{i}, p_{i})}\). Let \({N}\) symbolize the number of the subject property, \({P_{1}}\), \({P_{2}}\) and \({P_{3}}\) represent the same property (subject type or action) but different values. The rule patterns are defined as follows:

\begin{itemize}[nosep,noitemsep,leftmargin=*]
    \item \textbf{Stable:} The property value in the first image is the same as in the second image:
     % \({P_{1}}\) : \({P_{1}}\)   ::  \({P_{2}}\) :   \({P_{2}}\)
     $${P_{1}} : {P_{1}}   ::  {P_{2}} :   {P_{2}}.$$
    \item \textbf{Change:} The property value in the first image changes in the second image:
    $${P_{1}} :  {P_{2}}   ::  {P_{1}} :  {P_{2}}.$$
    \item \textbf{Arithmetic:} The number of subjects changes by either increasing or decreasing from the first image to the second image. \({N}\geq1\):
    $$ {N_{2}} : {N_{4}}    ::  {N_{1}}  : {N_{3}}  \rightarrow 4 -2 = 2	:: 1 + 2 = 3 .$$
    
    \item \textbf{Distraction:} The property values except the number of subjects are different in three images. There is no correlation among these values, so  \({P}\) is a distraction. After applying increase or decrease changes, if \({N}\leq 0\), then \({N}\) is a distraction:
    $${P_{1}} :  {P_{2}}   ::  {P_{3}} :  {ANY}. 
   {N_{4}} :  {N_{1}}     ::  {N_{2}} : {ANY}	\rightarrow 1 - 4 = -3 :: 2 - 3 = -1 .$$
\end{itemize}





\medskip\noindent\textbf{Text Prompt and Image Generation.}
To ensure that the relationships between images are easily recognizable, the images in \voila must be clear and object-centered. We employ the open-source SDXL model, which generates high-quality images based on a simple text prompt structure that includes the number of subjects, subject types, and actions, for example \textit{``Two dogs walking''}. Complex or overly detailed prompts can lead to incorrect image generation \citep{podell2023sdxlimprovinglatentdiffusion}, so we maintain straightforward prompt structures.
After generating text prompts, the images for the analogy questions are produced using the SDXL pipeline, with output resolution set to  $1024 \times 1024$ and a guidance scale of 8, which controls the fidelity to the text prompt. For each prompt, 30 images are generated. Appendix \ref{sdxl} provides examples of generated images and their text descriptions.

\medskip\noindent\textbf{Data Cleaning.}
It is essential to verify the alignment between the text prompts and the generated images. Some images may not correspond correctly to their prompts (see Appendix \ref{image-cleaning}), making them unsuitable for visual analogy construction. These images were manually filtered, and from this process, we retained 10 images per prompt, resulting in a total of 7,280 diverse images.

\begin{table}[t]
\small
  \centering
  \caption{\voila contains analogies with three properties and four rules applied to these properties.}
  \begin{tabular}{@{}lcccc@{}}
  \toprule
    \textbf{Properties} & \textbf{Stable} &  \textbf{Change} & \textbf{Arithmetic} & \textbf{Distraction} \\
    \midrule
    Action & \checkmark & \checkmark & $\times$  & \checkmark  \\
    Number of Subjects & \checkmark  & $\times$  & \checkmark  & \checkmark \\ 
    Subject Type & \checkmark & \checkmark & $\times$ & \checkmark  \\
    \bottomrule
  \end{tabular} 
  \label{tab:rules}
\end{table}

\medskip\noindent\textbf{Building Image Analogies.} 
To construct visual analogy questions, we combine image features using predetermined rules. Each question follows a specific rule-property configuration, with the number of properties undergoing changes determining the dataset configuration. Since \voila includes three properties, it proposes three different analogy structures. The number of cases required for each structure is calculated by pairing unchanged properties with unmodified rules. Given that Arithmetic and Change rules modify properties, the remaining two rules are used for the process.
\begin{equation}
    %\small
  C = r^{n-p}\cdot{\frac{n!}{{p!}\cdot{(n - p)!}}},
\end{equation}
where $r$ is the number of rules (excluding Arithmetic and Change), $n$ is the total number of properties, and $p$ indicates the number of changed properties. This yields 19 unique cases for three configurations: 12 cases for 1 property change, 6 cases for 2 property changes, and 1 case where all properties change. Appendix \ref{case-numbers} details the possible cases and assigned rules for each property.

To create a dynamic \voila benchmark, we propose the Visual Analogy Generation strategy (see Algorithm 1), which takes input properties, structure, and rule configurations and outputs the corresponding image sequence for each \voila question. Each structure is assigned a property-rule configuration, and the number of distinct analogy questions for each structure is calculated based on rule-property permutations, including dual permutations for the Stable and Change rules, and length-3 permutations for the Distraction rule. Unique number combinations for Arithmetic and Distraction rules are manually calculated to eliminate redundancy (see Appendix \ref{case-numbers} for details).

In \voila, the analogy questions are equally distributed among the 19 structures, ensuring a distinct combination of properties for each question. Images can be reused across different configurations, preserving uniqueness while distributing image usage evenly.

\begin{algorithm}[t]
\small
% \SetAlgoLined
% \nlset{}
\caption{Visual Analogy Generation}
\begin{algorithmic}
\STATE \textbf{Input:} Three property arrays: $numbers[]$, $subjects[]$, $actions[]$
\STATE \textbf{Output:} Question array: $Q[Image_0, Image_1, Image_2, Image_3]$
\STATE \textbf{Define} rules $R_y$ where $y = 1, 2, 3, 4$ \COMMENT{Stable, Changes, Arithmetic, Distraction}
\STATE \textbf{Define} analogy structures $A_i$ where $i = 1, 2, \ldots, 19$ 
\STATE Each structure $A_i$ contains variables $n_i$, $s_i$, $a_i$ and different rule configurations 
\FOR{$i = 1$ to $19$} 
    \STATE $n_i \gets$ apply$R_y(numbers[ ])$ \COMMENT{Generate all possible permutations of properties}
    \STATE $s_i \gets$ apply$R_y(subjects[ ])$
    \STATE $a_i \gets$ apply$R_y(actions[ ])$
    \STATE $combinations[] \gets$ combineProperties$(n_i, s_i, a_i, count)$
    \COMMENT {Generate random selections with balanced distribution}
    \FOR{$n$, $s$, $a$ in $combinations[]$} 
        \FOR{$x = 0$ to $3$} 
            \STATE $Image_x  \gets$ findImage$(n[x], s[x], a[x], index)$ \COMMENT{Find images that matches requested properties. Use the index to provide a variety of images.}
        \ENDFOR
        \STATE $Q  \gets$ $(Image_0, Image_1, Image_2, Image_3)$
    \ENDFOR
\ENDFOR

\STATE \textbf{End}
\end{algorithmic}
\end{algorithm}


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{voila_arch.pdf} 
  \caption{\voila multi-step reasoning and evaluation pipeline. The top section illustrates two visual input formats. The left side of the MLLMs connection displays the four primary tasks along with their corresponding prompts, while the right side presents the expected outcomes for each task. The results are scored in the evaluation stage utilizing GPT-4o and ground truths.}

  
  \label{fig:VOILA}
\end{figure}

\section{Experimental Setup} \label{sec:exp_setup}

\paragraph{\voila  Test Dataset.}

For evaluating MLLMs, the \voila benchmark is split into two distinct test datasets: \voilawd, which includes Distraction rules, and \voiland, which excludes them. \voilawd consists of 10K unique questions, applying all four rules across 19 structures, while \voiland comprises 3.6K questions with seven configurations and three rules (see Appendix \ref{voila-structure} for details). Each dataset contains 527 questions per configuration, with 728 images annotated with image contents, relationship explanations, and descriptions of the requested image.

\paragraph{Models.}
We evaluated several baseline models on both \voilawd and \voiland: GPT-4o \citep{OpenAI2024GPT-4o}, Emu2-37B \citep{sun2024generativemultimodalmodelsincontext}, Qwen2-VL-7B-Instruct \citep{wang2024qwen2vlenhancingvisionlanguagemodels}, CogVLM2-19B \citep{hong2024cogvlm2visuallanguagemodels}, SEED-LLaMA-8B \citep{ge2023makingllamadrawseed}, Llama 3.2-11B, and MolmoE-7B \citep{deitke2024molmopixmoopenweights}. Models incapable of producing visual output were excluded from the image generation task.

\paragraph{Image Input.} 
Given that some baseline models, such as CogVLM2, do not support multiple image inputs, we implemented two visual input formats: a sequential presentation of the three images and an image collage combining the three images into a single visual representation.

\paragraph{Prompting.}
We applied the Least-to-Most (L2M) prompting strategy \citep{zhou2023leasttomostpromptingenablescomplex} and manually decomposed the visual analogy task into four sub-problems: (1) understanding the visual content, (2) identifying relationships between images, (3) applying those relationships to the third image, and (4) generating the content of the fourth image. Instead of using sub-questions, we employed sub-instructions, asking the models to solve each sub-task sequentially, with the previous answer appended to the next problem. This structured reasoning process allowed us to evaluate performance at each sub-task. We tested various prompts on the baseline models using both L2M and direct answer approaches. The prompts used for multi-step reasoning and direct answering are detailed in Appendix \ref{prompt-selecting}.

\paragraph{Evaluation.}
Performance on \voila is assessed at each step based on correct property prediction. Using GPT-4o and four distinct text prompts, we scored model responses for each step, see Figure \ref{fig:VOILA}. In the first phase, the modelsâ€™ ability to understand the visual content is evaluated. Given three images  \(I_{i_j}\) and their properties \(P_{i_j}\) and ground truth descriptions \(G_{i}\), the score in this phase is calculated as:
\begin{equation}
f(Q_i) = 
\begin{cases} 
1 & \text{if } Q_i(P_{i_j}(n, s, a), I_{i_j}) = Q_i(G_i, I_{i_j}) \text{ and } j \in [0,2] \\
0 & \text{otherwise}.
\end{cases}
\end{equation}

This scoring strategy is applied across the first three phases, where models are evaluated on whether they correctly identify the properties of the images. In the second phase, we assess the modelsâ€™ ability to extract relationships between the images. In the third phase, we evaluate the application of these relationships to a new domain. In the final step, the model-generated images are assessed using a VQA-style approach. For each property, we generated three questions based on the ground truth text and used GPT-4o to answer these questions in relation to the generated image. A similar property-based scoring method is applied to evaluate the generated images. Detailed evaluation prompts for each step are provided in Appendix \ref{eval-step-by-step}.

\paragraph{Human Performance.}
We also evaluated human performance on the visual analogy task using the Amazon Mechanical Turk (MTurk) platform. A total of 440 distinct analogy questions, incorporating different rule configurations, were presented to participants. Instruction sets and example questions, with and without Distraction rules, were provided during the experiment. Human participants were tasked with predicting the properties of the missing fourth image. The results were collected and evaluated against the ground truth text. Additional details on the MTurk human evaluation study are available in Appendix \ref{mturk-human-experiment}.

\input{tables/voila_results_l2m}


\section{Experimental Results} \label{sec:exp_results}

\subsection{Main Results}

We evaluated the high-level reasoning abilities of state-of-the-art MLLMs on the \voila benchmark. Table~\ref{tab:results-l2m} presents the step-by-step accuracy of the models on both \voilawd and \voiland. Although both GPT-4o and Qwen-VL2 achieved peak performance with an average accuracy of 78\% during the image description stage, GPT-4o distinguished itself with exceptional results in relational reasoning by reaching an average precision of 40\% in both datasets. In the application relationship step, while GPT-4o outperformed other models by 29\%, emerging as the top performer on \voiland, LLaMa 3.2 took the lead on \voilawd with a 13\% accuracy rate, surpassing GPT-4o by 6.7\%. This result shows LLaMa 3.2's enhanced ability to identify distractions compared to other models. However, human participants still significantly outperformed all MLLMs, particularly in understanding relationships and making inferences. The performance gap between human participants and the top-performing modelsâ€”LLaMa 3.2 on \voilawd and GPT-4o on \voilandâ€”equals approximately 58\% and 40\%, respectively.

Model accuracy notably declines after each reasoning step, illustrating the growing difficulty as tasks become more complex. Figure~\ref{fig:main_line_plot} shows the accuracy of models across the four reasoning stages on \voilawd and \voiland. While most models achieve over 50\% accuracy in the initial image understanding stage, their performance drops sharply in the second stage, where they are required to interpret relationships. This decline continues in the third stage, where models apply these relationships to generate inferences for the third image. Due to the limited image generation capabilities of baseline models, we evaluated this step only on GPT-4o, Seed-LLaMa, and Emu-2. Across both datasets, performance dropped at the image generation stage (see Appendix \ref{image-generation}). These results demonstrate that current MLLMs struggle with relational understanding and their inference accuracy. Additional details on successful and failed cases are available in Appendix \ref{failure}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/accuracy_graphs.pdf}
    \caption{Accuracy of \voilawd and \voiland at each step, respectively.}
    \label{fig:main_line_plot}
\end{figure}

\subsection{Property Success of Models}

To further analyze how baseline models understand and predict properties, we evaluated model performance based on three properties: the number of subjects, subject type, and action, across each reasoning step on \voilawd and \voiland. Figure~\ref{fig:property_bar_chart} highlights the property-based accuracy of four models that perform best on \voilawd. Each model exhibited different strengths in identifying properties at each step. QWEN2-VL, for instance, exhibited strong capabilities in identifying numbers and subject types during the initial two steps, but its performance notably decreased when applying relationships, particularly for these properties similar to CogVLM-2. GPT-4o maintained high accuracy in predicting numbers during the first and second stages but experienced a significant decrease of 60\% in the relation application phase on \voilawd, followed by an additional 6\% decline in the image generation step. Although QWEN2-VL and GPT-4o performed best during the first stage, QWEN2-VL struggled more than GPT-4o to identify the relationships. LLaMa 3.2 achieves the most balanced performance across all categories, maintaining relatively high accuracy in the complex task of relationship application. While transferring the relationship step is the most challenging part for CogVLM-2, QWEN2-VL, and GPT-4o, LLaMa 3.2 struggles more with identifying relationships on \voilawd. Further property analysis for \voilawd is provided in Appendix \ref{property-voilawd}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/distractions_bar.pdf}
    \caption{Accuracy of baseline MLLMs regarding properties at each step on \voilawd.}
    \label{fig:property_bar_chart}
\end{figure}

\subsection{L2M vs Direct Answer}

We conducted experiments comparing Least-to-Most (L2M) prompting with direct answering approaches on \voilawd and \voiland. Table~\ref{tab:l2m_ablation} summarizes the results, showing that L2M prompting consistently improves model performance by breaking down the visual analogy task into sub-problems. This approach leads to higher accuracy in both settings, with a particularly strong impact on \voilawd, which involves more complex rule configurations. These findings suggest that L2M prompting enhances the reasoning process by encouraging models to solve each part of the problem incrementally. Additional results on direct answering are available in Appendix \ref{direct-answering}.

\subsection{Image Collage vs Sequential Image}
We examined the impact of input formatsâ€”image collage versus sequential imagesâ€”on model performance in \voilawd and \voiland. Table~\ref{tab:imagecollage} compares the results for Qwen2-VL and GPT-4o, which accept both input formats. Our findings indicate that input configuration has a significant effect on visual perception: performance dropped by approximately 40\% when models were presented with image collages compared to sequential images. This pattern of performance degradation was consistent across both \voilawd and \voiland, highlighting that models struggle with visual analogy tasks when images are combined in a collage format, which is counterintuitive and can be attributed to the image resolution constraints. However, our additional study with LLaVA-OneVision \cite{li2024llavaonevisioneasyvisualtask} detailed in Appendix \ref{anyres}, demonstrates that the AnyRes approach enhances the interpretation of collaged images and helps to reduce the performance differences. 

\setlength{\tabcolsep}{3pt}
\begin{table}[t]
    \centering
    \begin{minipage}{0.43\linewidth}
        \caption{Accuracy of the models in the third step using L2M and direct answering.}
      \label{tab:l2m_ablation}
      \centering
      \scriptsize
      \begin{tabular}{lccccccccc}
         \multicolumn{1}{c}{} &\multicolumn{2}{c}{\voilawd} & \multicolumn{2}{c}{\voiland} \\ 
        \cmidrule(r){2-3} \cmidrule(l){4-5}  
        Models   & L2M   &\makecell{Direct \\ Answer}   & L2M &\makecell{Direct \\ Answer}   \\\hline
        \midrule
        COG-VLM2 (IC)   & 0.41  &0.18        & 6.39  &3.57   \\
    
        GPT-4o (3I)   & 6.44    &0.9     &29.03  &16.94  \\
        Seed LLama 14B (3I)   &2.99     & 1.35        &2.35    &2.03 \\\hline
      \end{tabular}
    \end{minipage}
    \hspace{0.03\linewidth} % Adjust space between tables
    \begin{minipage}{0.43\linewidth}
      \centering
      \scriptsize
        \caption{Accuracy of the models in the third step using L2M on image collage and sequential image settings.}
        \label{tab:imagecollage}
      \begin{tabular}{lccccccccc}
         \multicolumn{1}{c}{} &\multicolumn{2}{c}{\voilawd} & \multicolumn{2}{c}{\voiland} \\ 
        \cmidrule(r){2-3} \cmidrule(l){4-5}  
        Models   & \makecell{Image \\ Collage}   &\makecell{Sequential \\ Images}  & \makecell{Image \\ Collage}   &\makecell{Sequential \\ Images}    \\\hline
        \midrule
        QWEN-VL2 (L2M)   & 0.52  &0.85        & 3.77  &6.8   \\
        GPT-4o (L2M)   & 3.94    &6.44     &19.43  &29.03  \\\hline
      \end{tabular}
    \end{minipage}
\end{table}



\subsection{\voilawd vs \voiland}
\begin{wrapfigure}{r}{0.5\linewidth}
    \centering
    \vspace{-\intextsep}
    \includegraphics[width=\linewidth]{images/comparison.pdf} % Replace with your image
    \caption{Performance of the top-8 models on \voiland and \voilawd.}
    % \vspace{-\intextsep}
    \label{fig:distraction_bar}
\end{wrapfigure}
According to the accuracy outcomes of models on both \voilawd and \voiland benchmarks, all models, excluding LLaMa 3.2, perform better addressing the \voiland questions, see Figure~\ref{fig:distraction_bar}. The accuracy of best-performer GPT-4o, dropped by 22\% when solving \voilawd questions. The results demonstrate that implementing the Distraction rule increases the difficulty level of the \voila benchmark and proves that \voilawd introduces more complex challenges compared to \voiland. We also analyzed the rule-based performances of LLaMa 3.2 and discovered that it applies the Distraction rule better than other rules (Arithmetic and Stable), particularly in number property which explains why it achieves better results on the \voilawd unlike other models. 


\subsection{Error Analysis}
To assess GPT-4o's performance on the evaluation task and to quantify the differences between human and GPT-4o evaluations, we examined 50 visual analogy questions and 180 responses, 30 corresponding to the image generation step, answered by various models using diverse rule configurations including the distraction rule. The results show that GPT-4o has an error rate of up to 10\% per step, and its challenges in recognizing relationships do not influence the benchmark assessment procedure. Additional details of the analysis are available in Appendix \ref{evaluation}


\section{Ablation Study}

% \subsection{How Model Performs If We Provide Ground Truths?}
\subsection{Model Performance with Access to Ground Truth Information}

To better understand model behavior at each reasoning step, we conducted an ablation study on GPT-4o by providing ground truth inputs starting from the second phase. This study aims to evaluate the modelâ€™s reasoning abilities under ideal conditions. Using L2M prompting on the \voilawd benchmark, we provided GPT-4o with ground truth image descriptions at phase 2. The model's performance in identifying relationships increased significantly, reaching 97\%, indicating that GPT-4o can effectively analyze changes in text descriptions when given correct inputs. However, when we provided the model with ground truth relationships in the third step, its performance dropped dramatically to 17\%, well below human performance (71\%). These results suggest that GPT-4o struggles to apply known relationships to new visuals, revealing limited reasoning in practical inference.


\subsection{How Does Visual Information Affect Performance?}
We also investigated how visual versus textual information affects model performance on the analogy task by conducting an ablation study on \voiland. In this experiment, we used a direct answering prompt without any explanation of the rules. Two experiments were conducted with GPT-4o, one using three sequential images as input and the other using text descriptions of those images. When processing image data, GPT-4o achieved an accuracy of 22\%, while with textual input, its accuracy rose to 49\%. These results highlight the importance of input format in MLLM performance, exposing a gap between visual and textual reasoning abilities.

\begin{wrapfigure}{r}{0.45\linewidth}
    \centering
    \vspace{-\intextsep}
    \includegraphics[width=\linewidth]{line_steps.png}
    \caption{Correct number of questions in \voiland.}
    \label{fig:correctq_ablation}
\end{wrapfigure}
We also analyze the results based on the number of correctly answered questions on \voiland. Figure~\ref{fig:correctq_ablation} demonstrates the rule-based performance comparison of models that accept visual and textual information. The outcomes of both models show that the Arithmetic rule included in rule numbers 6 and 7 influences GPT-4o's performance. The models present better accuracy when Stable or Change rules are applied to the number property. We also observed that models achieve the lowest accuracy in predicting the properties in rule 7 where all properties in question change at a time.  


\section{Conclusion}
\label{sec:formatting}

We introduced \voila, a large-scale, open-ended, and dynamic reasoning benchmark designed to evaluate the visual understanding and analogical reasoning capabilities of state-of-the-art MLLMs. \voila comprises two sub-tasks: the more complex \voilawd and the simpler \voiland. Our evaluations revealed that humans outperform the best-performing models by a substantial margin of 58\% on \voilawd and 40\% on \voiland. These results demonstrate that current MLLMs not only struggle to generate accurate visual or textual outputs but also lack the ability to recognize and apply relational reasoning across images. The significant performance gap between humans and MLLMs underscores the limitations of current models in higher-level cognitive tasks. We anticipate that \voila will serve as a rigorous benchmark for advancing MLLMs, systematically evaluating their ability to tackle complex reasoning tasks that demand human-like intelligence.

\section*{Acknowledgments}

NY is supported by the Republic of TÃ¼rkiye Ministry of National Education. 
MP, CB, and YY are supported by US NSF RI grant \#2132724. 
TG was supported by the SURFF award from UMBC ORCA.
We thank the NSF NAIRR initiative, the Research Computing (RC) at Arizona State University (ASU), and \href{https://www.cr8dl.ai/}{cr8dl.ai} for their generous support in providing computing resources.
The views and opinions of the authors expressed herein do not necessarily state or reflect those of the funding agencies and employers.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\input{sections/appendix}

\end{document}
