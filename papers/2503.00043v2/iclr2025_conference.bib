@article{yue2024mmmu,
  title={MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark},
  author={Yue, Xiang and Zheng, Tianyu and Ni, Yuansheng and Wang, Yubo and Zhang, Kai and Tong, Shengbang and Sun, Yuxuan and Yin, Ming and Yu, Botao and Zhang, Ge and others},
  journal={arXiv preprint arXiv:2409.02813},
  year={2024}
}


@article{Hertzmann2001ImageA,
author={Aaron Hertzmann and Charles E. Jacobs and Nuria Oliver and Brian Curless and D. Salesin},
journal={Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
title={Image Analogies},  
year={2001},
url={https://api.semanticscholar.org/CorpusID:2201072}
}

@misc{podell2023sdxlimprovinglatentdiffusion,
      title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis}, 
      author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas Müller and Joe Penna and Robin Rombach},
      year={2023},
      eprint={2307.01952},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.01952}, 
}

@misc{bochkovskiy2020yolov4optimalspeedaccuracy,
      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, 
      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
      year={2020},
      eprint={2004.10934},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2004.10934}, 
}
@misc{agrawal2016vqavisualquestionanswering,
      title={VQA: Visual Question Answering}, 
      author={Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh},
      year={2016},
      eprint={1505.00468},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1505.00468}, 
}

@misc{vinyals2015tellneuralimagecaption,
      title={Show and Tell: A Neural Image Caption Generator}, 
      author={Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan},
      year={2015},
      eprint={1411.4555},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1411.4555}, 
}

@book{bloom1956,
  title = {Taxonomy of educational objectives. The classification of educational goals. Handbook 1: Cognitive domain},
  added-at = {2011-07-29T10:18:43.000+0200},
  address = {New York},
  author = {Bloom, B. S. and Engelhart, M. B. and Furst, E. J. and Hill, W. H. and Krathwohl, D. R.},
  biburl = {https://www.bibsonomy.org/bibtex/2824bf3e3329813e7eec00d9777882aac/mleidl},
  interhash = {099af9deb220d8451a8b26cc6360dc85},
  intrahash = {824bf3e3329813e7eec00d9777882aac},
  keywords = {bloom bloom1956 krathwohl1956 taxonomie taxonomy},
  location = {New York},
  priority = {2},
  publisher = {Longmans Green},
  timestamp = {2011-09-18T06:40:49.000+0200},
  year = 1956
}

@article{GENTNER1983155,
title = {Structure-mapping: A theoretical framework for analogy},
journal = {Cognitive Science},
volume = {7},
number = {2},
pages = {155-170},
year = {1983},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(83)80009-3},
url = {https://www.sciencedirect.com/science/article/pii/S0364021383800093},
author = {Dedre Gentner},
}

@misc{sadeghi2015visalogyansweringvisualanalogy,
      title={VISALOGY: Answering Visual Analogy Questions}, 
      author={Fereshteh Sadeghi and C. Lawrence Zitnick and Ali Farhadi},
      year={2015},
      eprint={1510.08973},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1510.08973}, 
}

@misc{bitton2022vasrvisualanalogiessituation,
      title={VASR: Visual Analogies of Situation Recognition}, 
      author={Yonatan Bitton and Ron Yosef and Eli Strugo and Dafna Shahaf and Roy Schwartz and Gabriel Stanovsky},
      year={2022},
      eprint={2212.04542},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.04542}, 
}
@book{raven1938progressive,
  title     = {Raven's Progressive Matrices},
  author    = {John Raven and J. C. Raven and J. H. Court and Psychological Corporation and J. C. Raven and others},
  year      = {1938},
  publisher = {Western Psychological Services}
}

@misc{zhang2019ravendatasetrelationalanalogical,
      title={RAVEN: A Dataset for Relational and Analogical Visual rEasoNing}, 
      author={Chi Zhang and Feng Gao and Baoxiong Jia and Yixin Zhu and Song-Chun Zhu},
      year={2019},
      eprint={1903.02741},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1903.02741}, 
}
@misc{zellers2019recognitioncognitionvisualcommonsense,
      title={From Recognition to Cognition: Visual Commonsense Reasoning}, 
      author={Rowan Zellers and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1811.10830},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1811.10830}, 
}

@misc{johnson2016clevrdiagnosticdatasetcompositional,
      title={CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning}, 
      author={Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Li Fei-Fei and C. Lawrence Zitnick and Ross Girshick},
      year={2016},
      eprint={1612.06890},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1612.06890}, 
}

@misc{suhr2019corpusreasoningnaturallanguage,
      title={A Corpus for Reasoning About Natural Language Grounded in Photographs}, 
      author={Alane Suhr and Stephanie Zhou and Ally Zhang and Iris Zhang and Huajun Bai and Yoav Artzi},
      year={2019},
      eprint={1811.00491},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.00491}, 
}

@article{wei2022chain,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}
@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@inproceedings{zhang2023automatic,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  booktitle={The Eleventh International Conference on Learning Representations (ICLR)},
  year={2023}
}
@misc{zhang2024multimodalchainofthoughtreasoninglanguage,
      title={Multimodal Chain-of-Thought Reasoning in Language Models}, 
      author={Zhuosheng Zhang and Aston Zhang and Mu Li and Hai Zhao and George Karypis and Alex Smola},
      year={2024},
      eprint={2302.00923},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.00923}, 
}

@misc{shum2024automaticpromptaugmentationselection,
      title={Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data}, 
      author={KaShun Shum and Shizhe Diao and Tong Zhang},
      year={2024},
      eprint={2302.12822},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.12822}, 
}
@misc{wang2023selfconsistencyimproveschainthought,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.11171}, 
}
@inproceedings{lu2022learn,
    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
    booktitle={The 36th Conference on Neural Information Processing Systems (NeurIPS)},
    year={2022}
}

@misc{schwenk2022aokvqabenchmarkvisualquestion,
      title={A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge}, 
      author={Dustin Schwenk and Apoorv Khandelwal and Christopher Clark and Kenneth Marino and Roozbeh Mottaghi},
      year={2022},
      eprint={2206.01718},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.01718}, 
}

@misc{zhou2023leasttomostpromptingenablescomplex,
      title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models}, 
      author={Denny Zhou and Nathanael Schärli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc Le and Ed Chi},
      year={2023},
      eprint={2205.10625},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2205.10625}, 
}
@misc{sun2024generativemultimodalmodelsincontext,
      title={Generative Multimodal Models are In-Context Learners}, 
      author={Quan Sun and Yufeng Cui and Xiaosong Zhang and Fan Zhang and Qiying Yu and Zhengxiong Luo and Yueze Wang and Yongming Rao and Jingjing Liu and Tiejun Huang and Xinlong Wang},
      year={2024},
      eprint={2312.13286},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.13286}, 
}

@misc{wang2024qwen2vlenhancingvisionlanguagemodels,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.12191}, 
}
@misc{hong2024cogvlm2visuallanguagemodels,
      title={CogVLM2: Visual Language Models for Image and Video Understanding}, 
      author={Wenyi Hong and Weihan Wang and Ming Ding and Wenmeng Yu and Qingsong Lv and Yan Wang and Yean Cheng and Shiyu Huang and Junhui Ji and Zhao Xue and Lei Zhao and Zhuoyi Yang and Xiaotao Gu and Xiaohan Zhang and Guanyu Feng and Da Yin and Zihan Wang and Ji Qi and Xixuan Song and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Yuxiao Dong and Jie Tang},
      year={2024},
      eprint={2408.16500},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.16500}, 
}

@article{antol2015vqa,
  title={VQA: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  journal={Proceedings of the IEEE international conference on computer vision},
  year={2015}
}
@inproceedings{young2014image,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  year={2014}
}
@misc{chen2015microsoftcococaptionsdata,
      title={Microsoft COCO Captions: Data Collection and Evaluation Server}, 
      author={Xinlei Chen and Hao Fang and Tsung-Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Dollar and C. Lawrence Zitnick},
      year={2015},
      eprint={1504.00325},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1504.00325}, 
}

@inproceedings{singh2019towards,
  title={Towards VQA Models that can Read},
  author={Singh, Amanpreet and Natarajan, Vivek and Jiang, Yunyang and Chen, Xinlei and Shah, Murtaza and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2019}
}

@misc{ge2023makingllamadrawseed,
      title={Making LLaMA SEE and Draw with SEED Tokenizer}, 
      author={Yuying Ge and Sijie Zhao and Ziyun Zeng and Yixiao Ge and Chen Li and Xintao Wang and Ying Shan},
      year={2023},
      eprint={2310.01218},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2310.01218}, 
}
@article{OpenAI2024GPT-4o,
        title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{wang2024mmluprorobustchallengingmultitask,
      title={MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark}, 
      author={Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen},
      year={2024},
      eprint={2406.01574},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.01574}, 
}

@misc{kil2024compbenchcomparativereasoningbenchmark,
      title={CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs}, 
      author={Jihyung Kil and Zheda Mai and Justin Lee and Zihe Wang and Kerrie Cheng and Lemeng Wang and Ye Liu and Arpita Chowdhury and Wei-Lun Chao},
      year={2024},
      eprint={2407.16837},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.16837}, 
}

@misc{zhu2024scanreasonempowering3dvisual,
      title={ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities}, 
      author={Chenming Zhu and Tai Wang and Wenwei Zhang and Kai Chen and Xihui Liu},
      year={2024},
      eprint={2407.01525},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.01525}, 
}

@misc{nie2024mmrelrelationunderstandingdataset,
      title={MMRel: A Relation Understanding Dataset and Benchmark in the MLLM Era}, 
      author={Jiahao Nie and Gongjie Zhang and Wenbin An and Yap-Peng Tan and Alex C. Kot and Shijian Lu},
      year={2024},
      eprint={2406.09121},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09121}, 
}

@misc{jiang2024marvelmultidimensionalabstractionreasoning,
      title={MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning}, 
      author={Yifan Jiang and Jiarui Zhang and Kexuan Sun and Zhivar Sourati and Kian Ahrabian and Kaixin Ma and Filip Ilievski and Jay Pujara},
      year={2024},
      eprint={2404.13591},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.13591}, 
}

@misc{wang2024measuringmultimodalmathematicalreasoning,
      title={Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset}, 
      author={Ke Wang and Junting Pan and Weikang Shi and Zimu Lu and Mingjie Zhan and Hongsheng Li},
      year={2024},
      eprint={2402.14804},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.14804}, 
}
@misc{shao2024visualcotadvancingmultimodal,
      title={Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning}, 
      author={Hao Shao and Shengju Qian and Han Xiao and Guanglu Song and Zhuofan Zong and Letian Wang and Yu Liu and Hongsheng Li},
      year={2024},
      eprint={2403.16999},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.16999}, 
}
@misc{xiao2024logicvistamultimodalllmlogical,
      title={LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts}, 
      author={Yijia Xiao and Edward Sun and Tianyu Liu and Wei Wang},
      year={2024},
      eprint={2407.04973},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.04973}, 
}

@misc{wang2024videocotvideochainofthoughtdataset,
      title={VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool}, 
      author={Yan Wang and Yawen Zeng and Jingsheng Zheng and Xiaofen Xing and Jin Xu and Xiangmin Xu},
      year={2024},
      eprint={2407.05355},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.05355}, 
}

@misc{deitke2024molmopixmoopenweights,
      title={Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models}, 
      author={Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Jen Dumas and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi},
      year={2024},
      eprint={2409.17146},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.17146}, 
}

@misc{plummer2016flickr30kentitiescollectingregiontophrase,
      title={Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}, 
      author={Bryan A. Plummer and Liwei Wang and Chris M. Cervantes and Juan C. Caicedo and Julia Hockenmaier and Svetlana Lazebnik},
      year={2016},
      eprint={1505.04870},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1505.04870}, 
}

@book{goswami1992analogical,
  author    = {Usha Goswami},
  title     = {Analogical Reasoning in Children},
  year      = {1992},
  publisher = {Psychology Press}
}

@misc{wang2024muirbenchcomprehensivebenchmarkrobust,
      title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding}, 
      author={Fei Wang and Xingyu Fu and James Y. Huang and Zekun Li and Qin Liu and Xiaogeng Liu and Mingyu Derek Ma and Nan Xu and Wenxuan Zhou and Kai Zhang and Tianyi Lorena Yan and Wenjie Jacky Mo and Hsiang-Hui Liu and Pan Lu and Chunyuan Li and Chaowei Xiao and Kai-Wei Chang and Dan Roth and Sheng Zhang and Hoifung Poon and Muhao Chen},
      year={2024},
      eprint={2406.09411},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09411}, 
}

@article{zhao2024mirb,
  author    = {Bingchen Zhao and Yongshuo Zong and Letian Zhang and Timothy Hospedales},
  title     = {Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning},
  journal   = {arXiv preprint},
  year      = {2024},
}

@article{Jiang2024MANTISIM,
  title={MANTIS: Interleaved Multi-Image Instruction Tuning},
  author={Dongfu Jiang and Xuan He and Huaye Zeng and Cong Wei and Max W.F. Ku and Qian Liu and Wenhu Chen},
  journal={Transactions on Machine Learning Research},
  year={2024},
  volume={2024},
  url={https://openreview.net/forum?id=skLtdUVaJa}
}

@misc{li2024llavaonevisioneasyvisualtask,
      title={LLaVA-OneVision: Easy Visual Task Transfer}, 
      author={Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Peiyuan Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
      year={2024},
      eprint={2408.03326},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.03326}, 
}

@inproceedings{chang2020procedure,
  title={Procedure planning in instructional videos},
  author={Chang, Chien-Yi and Huang, De-An and Xu, Danfei and Adeli, Ehsan and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={European Conference on Computer Vision},
  pages={334--350},
  year={2020},
  organization={Springer}
}

@inproceedings{gokhale2019cooking,
  title={Cooking with blocks: A recipe for visual reasoning on image-pairs},
  author={Gokhale, Tejas and Sampat, Shailaja and Fang, Zhiyuan and Yang, Yezhou and Baral, Chitta},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={5--8},
  year={2019}
}

@inproceedings{su2024actplan,
  title={ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities},
  author={Su, Ying and Ling, Zhan and Shi, Haochen and Jiayang, Cheng and Yim, Yauwai and Song, Yangqiu},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={14953--14965},
  year={2024}
}