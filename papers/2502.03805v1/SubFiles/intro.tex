\section{Introduction}

Autoregressive large language models (LLMs) using transformer architecture have excelled in tasks, likes dialogue systems \citep{yi2024survey}, chatbots \citep{achiam2023gpt}, intelligent agents \citep{Wang_2024}, and code generation \citep{gu2023llm}. However, the quadratic computational cost inherent in the transformerâ€™s self-attention mechanism poses significant challenges for practical deployment.
To mitigate this, LLMs often use a Key-Value (KV) cache, which stores intermediate results from the self-attention mechanism. Each KV cache entry corresponds to the KV states of a past token, thus allowing for the bypassing of recomputation of these tokens during autoregressive generation. However, as sequence lengths increase, the number of KV cache entries expands dramatically.
This expansion not only leads to considerable GPU memory overhead but also significantly increases I/O latency, hindering the deployment in real-world applications \citep{sun2024triforce}.

Recent research has identified that only a subset of KV cache entries substantially contribute to the output of the self-attention mechanism \citep{h2o,liu2024scissorhands,quest}. As a result, many methods, known as {\it cache eviction}, have been developed to reduce the KV cache size to fit within a given budget by evicting non-critical entries during inference. These methods effectively save GPU memory and improve subsequent decoding speed.
Notably,  H2O \citep{h2o} and Scissorhands \citep{liu2024scissorhands} observe a power-law distribution of attention weights: a small fraction of KV cache entries consistently dominates the majority of attention weights, aligning closely with the concept of cache entry criticality during inference. 
These methods introduce frameworks that leverage accumulated attention weights to identify and preserve critical cache entries.
Building on this, subsequent works \citep{adnan2024keyformer,SnapKV,ada} have refined attention weight accumulation and added operations like pooling and budget allocation to better preserve key information.
However, while these methods generally assume that entries with higher attention weights---determined by the similarity between key states in the KV cache and the target query state---are critical, the identification and characterization of ``critical cache entries'' remain unformalized.

This assumption raises two key questions:
\begin{center}
\vspace{-0.4cm}
\colorbox{lightgray!20}{\parbox{0.98\linewidth}{
\begin{enumerate}
	\vspace{-0.05cm}
	\item  {\it What criteria determine the critical KV cache?}
	\vspace{-0.25cm}
	\item  {\it Is reliance on attention weights alone sufficient for identifying critical cache entries?}
	\vspace{-0.2cm}
\end{enumerate}
}}	
\vspace{-0.4cm}
\end{center}


In this paper, we define the problem of critical cache identification from the perspective of output perturbation and introduce a theoretical framework to bound the worst-case scenario for optimizing practical perturbation.
Specifically, we focus on minimizing attention output perturbation when replacing the full KV cache with only critical entries under a given budget.
To quantify this perturbation, we employ the simple $L_1$ distance and derive its upper bound\footnote{We choose the $L_1$ distance due to its simplicity and effectiveness. More complex metrics, such as the $L_2$ distance, are equally valid; see Appendix~\ref{apdx:distance} for related discussions.}, corresponding to the worst-case perturbation. Our analysis shows that this upper bound is influenced by both the attention weights and the value states projected through the parameter matrix. Based on these insights, we propose a perturbation-constrained selection algorithm that goes beyond mere reliance on attention weights, underscoring the significance of previously overlooked value states and pretrained parameter matrix in identifying critical cache entries.



We integrate our algorithm into two state-of-the-art (SOTA) cache eviction methods, SnapKV~\cite{SnapKV} and AdaKV~\cite{ada}, replacing their reliance on solely attention-weight-based strategies.
Through extensive evaluations using Needle-in-a-Haystack tests and 16 datasets from LongBench across two compression scenarios, our algorithm more accurately selects critical cache entries, significantly enhancing post-eviction generation quality under various budget constraints.
Further empirical analysis confirms and elucidates the practical benefits of our algorithm: (1) It effectively reduces output perturbation in most attention heads, achieving over 92\% heads in the Llama model. (2) Its advantages accumulate across layers, significantly lowering the perturbation in final-layer hidden states. (3) It consistently performs well across various cache sizes, robustly mitigating quality loss under different resource constraints in practical applications. Our contributions can be summarized as follows:

\begin{enumerate}
	\item We highlight that current cache eviction methods neglect the crucial problem of identifying critical KV cache entries. To address this, we propose using output perturbation as a criterion for determining criticality. Our analysis shows that attention weights alone are insufficient; the value states projected by the parameter matrix are also essential.
	\item  Building on the constraint of the worst-case output perturbation, we propose a novel critical entry selection algorithm. When integrated into SOTA eviction methods, comprehensive evaluations across the Needle-in-a-Haystack test and Longbench benchmark demonstrate its effectiveness in improving generation quality.
	\item  Further empirical analysis examines and confirms the benefits of our perturbation-constrained selection algorithm. This analysis also highlights the significant potential for optimizing critical cache selection from the theoretical perspective of output perturbation.
\end{enumerate}

