
\section{Experiments}



\subsection{Models}
We select two advanced open-source LLMs for evaluation: Mistral-7B-Instruct-v0.3 (Mistral-7B) \citep{jiang2023mistral} and Llama-3.1-8B-Instruct (Llama-3.1-8B) \citep{dubey2024llama}, which support maximum sequence lengths of 32K and 128K, respectively. 



\subsection{Compression Scenario}
We evaluate performance under two scenarios. The first is \textbf{regular compression} scenario, widely adopted in previous research~\cite{SnapKV,ada}. In this scenario, question instructions are compressed alongside context, allowing for targeted compression for specific question. The second scenario is more challenging, referred to as \textbf{context-only compression} \cite{kvpress}. Here, only the context is provided during compression, with the problem instruction introduced afterward. This scenario is particularly relevant to more challenging tasks, such as multi-turn QA with long documents, where future questions cannot be anticipated during compression. For details, please refer to Appendix \ref{apdx:prompt_templates}.


\subsection{Baselines}


We integrated our algorithm with two cache eviction methodsâ€”SnapKV \citep{SnapKV} and AdaKV \citep{ada}. These represent the current SOTA methods in Top-K-based and budget allocation-optimized eviction, respectively. By comparing the quality of their generated output before and after integration, we demonstrate the performance improvements our algorithm brings to these methods.
The hyper-parameter $\alpha$ in Algorithm \ref{alg:selection} was set to 0.5 for all experiments. Other fundamental settings for SnapKV and AdaKV were kept as originally defined, with a max-pooling kernel size of 7 and an observation window size of 32 \cite{ada}.  We also included earlier work based on the sliding window approach, such as StreamingLLM (SLM), which discards the KV Cache outside the current window. We use its performance as a reference. %For more details, please refer to the code provided in our supplementary materials.






\begin{figure*}[t]
	
	\begin{minipage}{0.495\linewidth}
		\centering
		\begin{minipage}{\linewidth}
			\includegraphics[width=\textwidth]{./Figures/snap_legend.pdf}
		\end{minipage}
		\begin{subfigure}[b]{0.49\linewidth}
			\centering
			\includegraphics[width=\textwidth]{./Figures/img_group_by_model/Ave_criti_snap_wq_longbench_score.pdf}
			\vspace{-0.5cm}
			\caption{\centering Regular compression }
			%\label{fig:ada_overview}
		\end{subfigure}
		\begin{subfigure}[b]{0.49\linewidth}
			\centering
			\includegraphics[width=\linewidth]{./Figures/img_group_by_model/Ave_criti_snap_longbench_score.pdf}
			\vspace{-0.5cm}
			\caption{\centering Context-only compression}
			\label{fig:overview_snapkv_longbench_context}
		\end{subfigure}
		\vspace{-0.3cm}
		\caption{Overview of LongBench (Integrated into SnapKV). }
		\label{fig:overview_snapkv_longbench}
	\end{minipage}
	\begin{minipage}{0.495\linewidth}
		\begin{minipage}{\linewidth}
			\includegraphics[width=\textwidth]{./Figures/ada_legend.pdf}
		\end{minipage}
		\begin{subfigure}[b]{0.49\linewidth}
			\centering
			\includegraphics[width=\linewidth]{./Figures/img_group_by_model/Ave_criti_ada_wq_longbench_score.pdf}
			\vspace{-0.5cm}
			\caption{\centering Regular compression}
			\label{subfig:overview_adakv_longbench_regular}
			%\label{fig:pyramid_overview}
		\end{subfigure}
		\begin{subfigure}[b]{0.49\linewidth}
			\centering
			\includegraphics[width=\linewidth]{./Figures/img_group_by_model/Ave_criti_ada_longbench_score.pdf}
			\vspace{-0.5cm}
			\caption{\centering Context-only compression}
			\label{subfig:overview_adakv_longbench_context}
			%\label{fig:snap_overview}
		\end{subfigure}
		\vspace{-0.2cm}
		\caption{Overview of LongBench (Integrated into AdaKV). }
		\label{fig:overview_adakv_longbench}
	\end{minipage}
	
	
\end{figure*}



\begin{table*}[t!]
	\centering
	\small
	\vspace{-0.1cm}
	\caption{Quality scores of Llama model on LongBench.}
	\vspace{-0.1cm}
	\label{tab:llama}
	\begin{tabular}{@{}l>{\hspace{-0.8em}}l>{\hspace{-1.5em}}c>{\hspace{0em}} c>{\hspace{-1em}}c  c>{\hspace{-0.8em}}c c>{\hspace{-0.8em}}c c>{\hspace{-0.8em}}c@{}}
		\toprule
		& \multirow{2}{*}{Domain} &  \multirow{2}{*}{\makecell{6711 \\Full Cache}} & \multicolumn{2}{c}{\small \makecell{SnapKV $b =$ 20\%}} & \multicolumn{2}{c}{\small \makecell{SnapKV $b =$ 40\%}} & \multicolumn{2}{c}{\small \makecell{AdaKV $b =$ 20\%}} & \multicolumn{2}{c}{\small \makecell{AdaKV $b =$ 40\%}} \\
		\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
		& &  & w/o ours   & w/ ours   & w/o ours  & w/ ours   & w/o ours  & w/ ours  & w/o ours  & w/ ours   \\
		
		\toprule
		
		% % Mistral
		\multirow{6}{*}{\small\rotatebox[origin=c]{90}{\makecell{Llama-3.1-8B \\ Regular }}}
		& Single\-Doc. QA & 43.10 & 43.17 & \textbf{43.23} & 43.61          & \textbf{43.63} & 42.73          & \textbf{43.05} & 43.31          & \textbf{43.59} \\
		& Multi\-Doc. QA  & 46.49 & 46.63 & \textbf{47.14} & \textbf{46.96} & 46.64          & \textbf{46.64} & 46.42          & \textbf{47.02} & 46.97          \\
		& Summarization   & 28.97 & 25.35 & \textbf{25.94} & 27.05          & \textbf{27.36} & 25.49          & \textbf{26.05} & 27.24          & \textbf{27.79} \\
		& Few\-shot       & 69.45 & 68.46 & \textbf{68.59} & \textbf{69.43} & 69.25          & \textbf{69.19} & 69.03          & 69.36          & \textbf{69.40} \\
		& Synthetic       & 53.73 & 53.07 & \textbf{54.26} & 53.77          & \textbf{55.00} & 53.56          & \textbf{54.45} & 53.96          & \textbf{54.59} \\
		& Code            & 57.86 & 57.88 & \textbf{58.25} & 58.12          & \textbf{58.29} & 58.43          & \textbf{58.57} & 58.27          & \textbf{58.46} \\
		\hline
		& Ave.            & 49.20 & 48.29 & \textbf{48.73} & 49.06          & \textbf{49.20} & 48.51          & \textbf{48.73} & 49.08          & \textbf{49.33} \\
		\hline
		\hline
		% % Llama
		\multirow{6}{*}{\small\rotatebox[origin=c]{90}{\makecell{Llama-3.1-8B \\ Context-only}}}
		% & SnapKV w/o Question 20\% & Criti-SnapKV w/o Question 20\% & Ada-SnapKV w/o Question 20\% & Criti-Ada-SnapKV w/o Question 20\% &       & SnapKV w/o Question 40\% & Criti-SnapKV w/o Question 40\% & Ada-SnapKV w/o Question 40\% & Criti-Ada-SnapKV w/o Question 40\% \\
		& Single\-Doc. QA & 43.10 & 28.78 & \textbf{30.43} & 35.27          & \textbf{38.27} & 31.39          & \textbf{32.74} & 36.63          & \textbf{39.24} \\
		& Multi\-Doc. QA  & 46.49 & 33.51 & \textbf{35.87} & 40.50          & \textbf{43.17} & 34.90          & \textbf{35.31} & 41.36          & \textbf{45.11} \\
		& Summarization   & 28.97 & 23.82 & \textbf{24.64} & 26.11          & \textbf{27.15} & 24.29          & \textbf{24.98} & 26.66          & \textbf{27.31} \\
		& Few\-shot       & 69.45 & 61.95 & \textbf{63.04} & 65.10          & \textbf{66.87} & 63.70          & \textbf{64.74} & 66.43          & \textbf{68.19} \\
		& Synthetic       & 53.73 & 48.19 & \textbf{52.16} & 53.17          & \textbf{54.22} & 50.39          & \textbf{52.30} & 53.00          & \textbf{53.60} \\
		& Code            & 57.86 & 60.05 & \textbf{60.75} & 60.49          & \textbf{60.91} & 61.14          & \textbf{61.16} & 60.30          & \textbf{60.60} \\
		\hline
		& Ave.            & 49.20 & 41.29 & \textbf{42.99} & 45.52          & \textbf{47.29} & 42.87          & \textbf{43.77} & 46.24          & \textbf{48.00} \\
		
		\hline
		
		
		
		
	\end{tabular}%
	% }

\end{table*}


\subsection{Needle-in-a-Haystack Evaluation}


In the Needle-in-a-Haystack test, a key sentence (the "needle") is inserted within a lengthy context (the "haystack") to evaluate the model's retrieval ability. Given the 32K context window limit of the Mistral model, we set the haystack length to 32K and create 100 test samples. The retrieval score is based on successful retrievals, with a maximum of 100. Following the Ruler settings~\cite{hsieh2024ruler}, we conduct two tests: 1. Single-retrieval, where one sentence with a magic number is randomly inserted, requiring retrieval at the end. This simple task sees both full-cache Llama and Mistral scoring 100; 2. Multi-retrieval, where four sentences with magic numbers are inserted, requiring all to be retrieved. Here, full-cache Llama maintains a score of 100, while Mistral scores 97. 

\textbf{Regular Compression.}
Figures ~\ref{subfig:snapkv_single_regular}, \ref{subfig:snapkv_multi_regular}, \ref{subfig:adakv_single_regular} and \ref{subfig:adakv_multi_regular} show the results under regular compression. Whether using SnapKV or AdaKV, the Llama model exhibits strong retrieval capabilities, with only a slight drop in retrieval score in the multi-retrieval task when compressed to 2.5\% cache size. Our algorithm improves performance where existing methods show losses and maintains lossless scores in other cases. 
However, for models like Mistral, which face challenges in handling long-context processing, KV cache compression significantly affects retrieval performance, particularly in the multi-retrieval task. In such cases, integrating our algorithm with existing methods significantly enhances performance.
For instance, as shown in Figure~\ref{subfig:snapkv_multi_regular}, our approach raises the retrieval score from 84.5 to 95.5 when paired with SnapKV at 20\% cache size in the Mistral model. Similarly, in Figure~\ref{subfig:adakv_multi_regular}, integrating with the more advanced AdaKV method achieves a comparable improvement, increasing scores from 91.75 to 96.25.



\textbf{Context-only Compression.}
In the context-only compression scenario, the model is unaware of future questions during compression, making it difficult to assess the importance for specific queries. As a result, both eviction methods on Llama and Mistral models show a significant drop in retrieval scores, as illustrated in Figures~\ref{subfig:snapkv_single_context}, \ref{subfig:snapkv_multi_context}, \ref{subfig:adakv_single_context}, and \ref{subfig:adakv_multi_context}. In this case, the advantages of our method become particularly evident.
Using the advanced AdaKV method as a reference, our algorithm achieves substantial improvements. For the Llama model with a 20\% cache size, it raises the single retrieval score from 93 to 100 (Figure \ref{subfig:adakv_single_context}) and the multi retrieval score from 60.5 to 91.25 (Figure \ref{subfig:adakv_multi_context}). Similarly, for the Mistral model with a 40\% cache size, our approach boosts the single retrieval score from 53 to 100 (Figure \ref{subfig:adakv_single_context}) and the multi-retrieval score from 21.75 to 96 (Figure \ref{subfig:adakv_multi_context}), achieving up to a 4.41x improvement.


\subsection{LongBench Evaluation}
For a comprehensive evaluation, we incorperated the real-world task benchmark LongBench, consisting of 16 datasets across six task domains: single-document QA, multi-document QA , summarization, few-shot learning, synthetic, and code generation.  For each dataset, we apply the LongBench-recommended metrics for quality assessment. Detailed information for each dataset can be found in Appendix \ref{apdx:details_datasets}.

\textbf{Overview.}
Figures \ref{fig:overview_snapkv_longbench} and \ref{fig:overview_adakv_longbench} illustrate the average quality loss of different methods across 16 datasets in Longbench. As the cache size increases, the quality loss for all compression methods gradually decreases to near-lossless. Notably, the Top-K eviction methods, SnapKV and AdaKV, consistently outperform the sliding window method, StreamingLLM, in both the regular and context-only compression scenarios. However, these two different compression scenarios exhibit significantly different compression losses. For instance, in Figure \ref{subfig:overview_adakv_longbench_regular}, AdaKV shows only around 1.4\% and 2.4\% quality loss with a 20\% cache size in Llama and Mistral, whereas in Figure \ref{subfig:overview_adakv_longbench_context}, the more challenging context-only compression scenario results in higher quality losses of 12.9\% and 11.8\%, respectively.
Our method effectively reduces the losses of existing methods in both scenarios. For example, with a 20\% cache size in the Llama model, when integrated with SnapKV, our method reduces the losses in both the regular and context-only scenarios from 1.8\% and 16\% to 0.9\% and 13\%. When combined with AdaKV, it reduces the original losses of 1.4\% and to 13\% to 1\% and 11\%, respectively.


\textbf{Task domain analysis}
Tables \ref{tab:llama} present a detailed overview of the Llama model's scores across various task domains in both regular and context-only compression scenarios when our algorithm is integrated into SnapKV and AdaKV on two cache sizes (20\% and 40\%). Due to space constraints, the corresponding scores for the Mistral model are available in Appendix \ref{apdx:longbench_mistral}.
In the regular compression scenario, where original cache eviction results in minimal loss, all methods achieve scores close to the full cache case. Even in this situation, our algorithm enhances quality in 19 out of 24 task domains with two cache size of two base methods, demonstrating its effectiveness. In the more challenging context-only compression scenario, the quality loss from original SnapKV and AdaKV compression provides ample room for optimization by our algorithm, ultimately achieving higher scores across all 24 task domains. This indicates that our method is task-agnostic for enhancing the quality of existing cache eviction methods.




