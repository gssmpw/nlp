
\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{./Figures/head_wise_legend.pdf}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen1/head/multi_news_SnapKVPress_ws32_ks7_wqFalse_Meta-Llama-3.1-8B-Instruct_head_heat_0.8_gen1.png}
		\vspace{-0.5cm}
		\caption{Token1 }
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen3/head/multi_news_SnapKVPress_ws32_ks7_wqFalse_Meta-Llama-3.1-8B-Instruct_head_heat_0.8_gen3.png}
		\vspace{-0.5cm}
		\caption{Token3 }
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen5/head/multi_news_SnapKVPress_ws32_ks7_wqFalse_Meta-Llama-3.1-8B-Instruct_head_heat_0.8_gen5.png}
		\vspace{-0.5cm}
		\caption{Token5 }
	\end{subfigure}
	\vspace{-0.4cm}
	\caption{ Perturbation reduction across heads. (Llama)} 
	\label{fig:head_wise}
		\vspace{0.1cm}
	% \includegraphics[width=\textwidth]{./Figures/head_wise_legend.pdf}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen1/layer/multi_news_wqFalse_SnapKVPress_ws32_ks7_reduction_across_layers_budget0.8_gen1.png}
		\vspace{-0.5cm}
		\caption{Token1}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen3/layer/multi_news_wqFalse_SnapKVPress_ws32_ks7_reduction_across_layers_budget0.8_gen3.png}
		\vspace{-0.5cm}
		\caption{Token3}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen5/layer/multi_news_wqFalse_SnapKVPress_ws32_ks7_reduction_across_layers_budget0.8_gen5.png}
		\vspace{-0.5cm}
		\caption{Token5}
	\end{subfigure}
	\vspace{-0.4cm}
	\caption{Perturbation reduction across layers.} 
	\label{fig:layer_wise}
	\vspace{0.1cm}
	\begin{subfigure}[b]{0.325\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen1/budget/multi_news_wqFalse_SnapKVPress_ws32_ks7_gen1_reduction_across_budgets.png}
		\vspace{-0.5cm}
		\caption{Token1}
	\end{subfigure}
	\begin{subfigure}[b]{0.325\linewidth}		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen3/budget/multi_news_wqFalse_SnapKVPress_ws32_ks7_gen3_reduction_across_budgets.png}
		\vspace{-0.5cm}
		\caption{Token3}
	\end{subfigure}
	\begin{subfigure}[b]{0.325\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen5/budget/multi_news_wqFalse_SnapKVPress_ws32_ks7_gen5_reduction_across_budgets.png}
		\vspace{-0.5cm}
		\caption{Token5}
	\end{subfigure}
	\vspace{-0.8cm}
	\caption{Perturbation reduction across budgets.} 
	\vspace{-0.7cm}
	\label{fig:budget-wise}
\end{figure}


\subsection{Analysis of Practical Output Perturbation}
\label{sc:ana}



We empirically evaluate our algorithm's effectiveness in reducing practical output perturbation using the MultiNews dataset, which comprises 200 summarization samples. Tracking the hidden states of the first, third, and fifth decoding tokens with a 20\% cache size, we visualize output perturbations with and without our algorithm using the SnapKV method (additional results in Appendix \ref{apdx:detail_analysis}).

\textbf{Head-wise Analysis:} Our algorithm significantly reduces head-wise average output perturbation across all samples in the Llama model, achieving lower perturbations in 92\%, 92\%, and 95\% of attention heads for tokens 1, 2, and 3, respectively (Figure \ref{fig:head_wise}). 

\textbf{Layer-wise Analysis:} Figure \ref{fig:layer_wise} shows how our algorithm progressively reduces perturbation across layers, leading to substantial decreases in the final layer, which directly impacts the generated token vocabulary distribution.


\textbf{Budget-wise Analysis:} Figure \ref{fig:budget-wise} illustrates that our method effectively lowers output perturbation across different cache sizes from 2.5\% to 40\%, underscoring its robustness of varying budget constrains in real world application.



These analyses demonstrate that our method robustly reduces practical output perturbation by theoretically constraining worst-case perturbation by Algorithm~\ref{alg:selection}. This results in the post-eviction output hidden states that are more consistent with those from the full KV cache, thereby enhancing generation consistency and reducing quality loss.



