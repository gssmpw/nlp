\section{Critical KV Cache Entry Selection}

For critical cache entry selection, we aim to choose cache entries that effectively represent the entire KV cache during self-attention computation, producing an output that is a close approximation, if not identical.
Preliminaries about the relationship between KV cache and generation output are introduced in Section~\ref{subsec:pre}.
Based on that, we formalize the problem of identifying critical cache entries from the perspective of output perturbation (Definition \ref{def:problem}) in Section~\ref{subsec:def}. Subsequently, in Section~\ref{subsec:how}, we formalize the output perturbation and derive its upper bound.
Then, we propose a two-stage greedy algorithm in Section~\ref{sc:alg} that constrains worst-case perturbations for selecting critical entries, with theoretical analysis provided in Section~\ref{sc:alg_analy}.
Finally, in Section~\ref{sc:integration} we integrate the algorithm into current SOTA cache eviction methods.


\subsection{Preliminaries}
\label{subsec:pre}
LLMs utilizing the multi-head self-attention mechanism operate with an autoregressive generation approach. In this setup, each decoding step leverages the most recently generated token to predict the next one. To illustrate this process, we focus on a single attention head as an example.
Let $X \in \mathbb{R}^{n \times d}$ denote the embedding matrix for all tokens in the sequence, with $x = X_{-1,:} \in \mathbb{R}^{1 \times d}$ representing the embedding vector of the most recent token, which serves as input at the current time step.
The parameter matrices, denoted by $W^Q$, $W^K$, and $W^V \in \mathbb{R}^{d \times d_h}$ are used to map the token embeddings into their respective Query, Key, and Value states with head dimension $d_h$ as follows:

{\small
\begin{equation}
	q = xW^Q ; K = XW^K; V = XW^V
\end{equation}
}

During the decoding phase, the Key and Value states of previously generated tokens (represented by $X$) are stored in the KV cache, allowing for the elimination of redundant computation.
Accordingly, the query $q$, derived from the most recent token $x$, attends to the cached Key $K$ to compute the attention weights $A$. These weights are then applied to the cached Value $V$, producing an intermediate output.
This intermediate result is subsequently transformed into the final output $o$ of the self-attention mechanism by the output parameter matrix $W^O \in \mathbb{R}^{d_h \times d}$:

{
\small
\begin{equation}
	\label{eqn:o}
	o = A V W^O, \: \text{where} \: A = \text{softmax}\left(qK^T/\sqrt{d}\right)
\end{equation}
}
\subsection{What criteria determine the critical KV cache?}
\label{subsec:def}
Recent research has demonstrated only a small portion of critical KV cache entries do substantially contribute to the attention output~\citep{h2o,liu2024scissorhands}.
This insight presents promising opportunities to reduce inference costs by evicting a large number of non-critical KV cache entries~\cite{SnapKV,pyramidkv,ada,ge2024modeltellsdiscardadaptive,adnan2024keyformer,ge2024model}.
However, the key challenge lies in accurately identifying the critical KV cache entries.
Ideally, from a high-level perspective, the set of critical KV cache entries should completely represent the entire cache, ensuring for given query state, the selected entries yield the same attention output as the full set of KV pairs. In practice, the number of selected critical cache entries will be constrained by a predefined budget, which is closely tied to the computational resources available in downstream deployments. Consequently, our goal shifts toward minimizing the output perturbation introduced by the replacement. So, the problem can be reformulated as follows.
\begin{definition}[Critical KV Cache Identification Problem]
	\label{def:problem}
	%(Critical KV Cache Identification Problem)
Given a critical cache budget $b$, the task is to select $b$ critical KV cache entries  $\langle \hat{K}, \hat{V}\rangle$ from a total of $n$ cache entries $\langle K , V \rangle$, with the goal of minimizing the perturbation in the attention output $o$. By using the $L_1$ distance $\mathcal{L}$ for quantification, the objective is formalized as:

	{\small
		\begin{align}
		\argmin_{selection\: of \:\langle\hat{K}, \hat{V}\rangle} \: \mathcal{L} =  \lVert o - \hat{o} \rVert_1
		% \:  \text{where}\:
	\end{align}}
	where $\hat{o}$ represents the attention output produced by the selected $\langle\hat{K}, \hat{V}\rangle$.
\end{definition}


\subsection{Are attention weights sufficient for identifying critical cache entries?}
\label{subsec:how}
According to Definition \ref{def:problem}, the goal of identifying critical KV cache entries is to minimize the perturbation $\mathcal{L} = \lVert o -\hat{o} \rVert_1$.
To achieve this, we can employ an additive masking $ \mathcal{M} $ to simulate the removal of non-critical cache entries' contributions to the final output $\hat{o}$, thereby altering $\hat{o}$.


{
\small
	\begin{align}
        \hat{o} &= A' V W^O, \: A' = \text{softmax}\left(\mathcal{M}+qK^T/\sqrt{d} \right) \\
        \text{where} \: \mathcal{M}_{i} &=
        \begin{cases}
            -\infty 		&  \text{if  $K_i \: \text{and} \: V_i$ {are non-critical} }\\
            0 &\text{otherwise.}\\
        \end{cases} \notag
    \end{align}
}
Thus, the perturbation $\mathcal{L}$ can be further expressed as:
{\small
\begin{align}
	\mathcal{L} = \lVert (A-A')VW^O\rVert_1
\end{align}}
\begin{comment}
\end{comment}
\begin{theorem}
	\label{thm:mask_rewrite}
	By introducing a mask $\mathcal{N}\in \mathbb{R}^{n}$ applied through element-wise multiplication denoted by $\odot$,  we can establish the relation between $A'$ and $A$ as follows:
	
 % the perturbation $\mathcal{L}$ can be rewritten as	
	 {\small
 	\begin{align}
        A' &= \frac{\mathcal{N} \odot A}{\sum_{i=1}^{n} \mathcal{N}_i A_i} \quad \\
        \text{where} \: \mathcal{N}_{i}  &=
		\begin{cases}
			0 		&  \text{if  $K_i , V_i$ {is non-critical} } \\
			1 &\text{otherwise.}\\
		\end{cases} \notag
		\text{and} \sum\nolimits_{i=1}^{n} \mathcal{N}_i = b
\end{align}}
\end{theorem}
\begin{proof}
Let $a = qK^T/\sqrt{d}$, we can express the attention weights $A'$ under critical cache entries as:
{
\small
\begin{align}
    A' &= \frac{exp(\mathcal{M}+ a)}{ \sum_{i=1}^{n} exp(\mathcal{M}+ a)_i} \\
       &= \frac{\mathcal{N} \odot exp(a)}{\sum_{i=1}^{n} \mathcal{N}_i  exp(a)_i } \notag \\
       &= \mathcal{N} \odot \frac{ exp(a)}{\sum_{i=1}^{n} exp(a)_i} \frac{\sum_{i=1}^{n} exp(a)_i}{\sum_{i=1}^{n} \mathcal{N}_i  exp(a)_i } \notag
\end{align}
}
	Considering $A = \frac{exp(a)}{\sum_{i=1}^{n} exp(a)_i}$, thus $\sum_{i=1}^{n} \mathcal{N}_i A_i = \frac{\sum_{i=1}^{n} \mathcal{N}_i exp(a)_i}{\sum_{i=1}^{n} exp(a)_i}$. Therefore, $A' = \frac{\mathcal{N} \odot A}{\sum_{i=1}^{n} \mathcal{N}_i A_i}$.
\end{proof}
Theorem \ref{thm:mask_rewrite} utilizes a multiplicative mask $\mathcal{N}$ to  quantifies how their selection impacts the attention weights.
However, directly minimizing $\mathcal{L}$ for critical cache selection is challenging due to complex matrix operations it requires. Thus we turn to establish an upper bound $\theta$, as shown in Theorem \ref{thm:bound}.
\begin{theorem}
	\label{thm:bound}
	The output perturbation $\mathcal{L}$ can be bounded by $\theta$:
	{\small
		\begin{align}
		\mathcal{L} \leq \theta =  C -  \left( 2- \frac{1}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} \right) \sum\nolimits_{i=1}^{n}  \mathcal{N}_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1  ,
	\end{align}
	}
	where $C$ denotes the $\sum\nolimits_{i=1}^{n} A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1$ and $\boldsymbol{\mathcal{V}} \in \mathbb{R}^{n \times d} = VW^O$ denotes all projected values states through parameter matrix $W^O$.
\end{theorem}
\begin{proof}
	Let $\boldsymbol{\mathcal{V}} \in \mathbb{R}^{n \times d} = VW^O$ denote all projected value states, thus:
	{\small
		\begin{align}
        \mathcal{L} &= \lVert \left(A - \frac{\mathcal{N} \odot A}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}}\right)\boldsymbol{\mathcal{V}}\rVert_1 \\
                    &= \lVert \sum\nolimits_{i=1}^{n} \left(A_i -  \frac{\mathcal{N}_i A_i}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} \right) \boldsymbol{\mathcal{V}}_{i,:}\rVert_1 \notag \\
        \leq \theta &= \sum\nolimits_{i=1}^{n} \lVert \left(A_i -  \frac{\mathcal{N}_i A_i}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}}\right) \boldsymbol{\mathcal{V}}_{i,:}\rVert_1 \\
                    &= \sum\nolimits_{i=1}^{n} \lvert A_i -  \frac{\mathcal{N}_i A_i}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} \rvert \times \lVert  \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 \notag
	\end{align}
	}
Given that the multiplicative mask $\mathcal{N}$ is either $0$ or $1$, the index set $i \in [1,n]$ can be split into  $I_0$ and $I_1$, according to its value. Thus:
{\small
	% \mathcal{L} \leq
	\begin{align}
	 \theta =  \sum\nolimits_{i \in I_0} A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 + \sum\nolimits_{i \in I_1} \left(  \frac{ A_i}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} - A_i \right) \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1
\end{align}
}
	Let $C$ represent $\sum\nolimits_{i=1}^{n} A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1$, a constant independent of the selection of critical entries. We can express $\sum\nolimits_{i \in I_0} A_i \lVert \boldsymbol{\mathcal{V}}_{i,:}\rVert_1$ as $C - \sum\nolimits_{i \in I_1} A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1$. Thus:
	{\small
		\begin{align}
			%  &= C - \sum\nolimits_{i \in I_1} A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 +\sum\nolimits_{i \in I_1} \left(  \frac{ A_i}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} - A_i \right) \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 \notag \\
		\mathcal{L} &\leq \theta = C + \sum\nolimits_{i \in I_1}  \left(  \frac{ A_i}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} - 2A_i \right) \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 \\
            &= C -  \left(2- \frac{1}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} \right) \sum\nolimits_{i=1}^{n}  \mathcal{N}_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 \notag
	\end{align}
	}
\end{proof}
% In the upper bound \(\theta\),
 We can observe that $\theta$ encompasses not only the attention weights but also the projected value states. This highlights that prior selection methods relying solely on attention weights are suboptimal.
\begin{algorithm}[tb]
	\small
	\caption{Perturbation-Constrained Selection }
	\label{alg:selection}
	\textbf{Input}: Budgets $b$, Query State $q$, KV Cache Entries $K,V$, Parameter Matrix $W^O$, Hyper Parameter $\alpha$ = 0.25\\
	\textbf{Output}: Critical Cache Entries $\hat{K},\hat{V}$
	\begin{algorithmic}[1] %[1] enables line numbers		
		\STATE initialize  empty cache  $\hat{K},\hat{V}$ % $\mathcal{N} \in \mathbb{R}^{n}$  = \textbf{0} and
		\STATE $A = \text{softmax}(qK^T)$; $\boldsymbol{\mathcal{V}} = VW^O$
		\STATE $\boldsymbol{\mathcal{A}} = (A + \epsilon) \odot ( L_1 \: \text{norm of each rows in}\: \boldsymbol{\mathcal{V}})$
		\STATE $b' = b \times \alpha$;  $b'' = b - b'$

        \STATE \textbf{for} {$A_i,K_i,V_i \in A,K,V$} \textbf{do} \hfill {Start of \textbf{Stage 1}}\\
        \begin{ALC@for}



		\IF{$A_i \in \text{Top}_k(A,b')$}
		\STATE add $K_i, V_i$ to $\hat{K},\hat{V}$
		\STATE remove $\boldsymbol{\mathcal{A}}_i, K_i, V_i$ from $\boldsymbol{\mathcal{A}},K,V$
		\ENDIF


        \end{ALC@for}
        \STATE \textbf{endfor} \hfill {End of \textbf{Stage 1}}\\


        \STATE \textbf{for} {$ \boldsymbol{\mathcal{A}}_i,K_i,V_i \in \boldsymbol{\mathcal{A}},K,V$} \textbf{do} \hfill {Start of \textbf{Stage 2}}\\
        \begin{ALC@for}

		\IF{$\boldsymbol{\mathcal{A}}_i \in \text{Top}_k(\boldsymbol{\mathcal{A}},b'')$}
		\STATE add $K_i, V_i$ to $\hat{K},\hat{V}$
		%\STATE remove $\boldsymbol{\mathcal{A}}_i, K_i, V_i$ from $\boldsymbol{\mathcal{A}},K,V$
		\ENDIF

		% \ENDFOR
        % \COMMENT{End of \textbf{Stage 2}}
        \end{ALC@for}
        \STATE \textbf{endfor} \hfill {End of \textbf{Stage 2}}\\
		\RETURN  Critical Cache Entries $\hat{K},\hat{V}$
	\end{algorithmic}
\end{algorithm}

\subsection{Identify critical cache entries by constraining worst-case perturbation.}

\label{sc:alg}
Drawing on optimization strategies in machine learning,  we propose lowering the upper bound of perturbation, effectively constraining the worst-case perturbation and thereby reducing actual perturbations for identifying critical cache entries.
However, directly minimizing the upper bound $\theta$ remains non-trivial.
 To balance both the complexity and selection effectiveness, we introduce a two-stage greedy perturbation-constrained selection Algorithm \ref{alg:selection}, specifically designed to lower the perturbation upper bound for critical cache entry identification.

In this algorithm, the total budget \(b\) is divided into two portions based on a hyperparameter \(\alpha\). In the first stage, a fraction of the budget, \(b' = b \times \alpha\), is allocated to prioritize KV cache entries with high attention weights. In the second stage, the remaining budget, \(b'' = b-b'\), is used to consider both the norms of the projected value states and the attention weights \footnote{A small constant $\epsilon$ (e.g., 1E-4) is added to attention weights to ensure numerical stability.}. This two-stage selection employs a Top-K operation to effectively constrain the worst-case perturbation. To substantiate the effectiveness of our proposed algorithm, we provide a theoretical analysis in the following section.

\subsection{Theoretical analysis of Algorithm~\ref{alg:selection}}
\label{sc:alg_analy}

Our proposed algorithm consists of two stages, referred to as stage 1 and stage 2, which work collaboratively to select critical cache entries.  Under the guarantee provided by Assumption~\ref{asp:power_law}, the selection in stage 1 ensures that stage 2 adheres to the constraints on perturbations, as formalized in Theorem \ref{thm:target}. Let  $\mathcal{N}'$ and $\mathcal{N}''$	represent the selections from the stage 1 and 2, respectively, satisfying: $\sum\nolimits_{i=1}^{n} \mathcal{N}'_i =b'$ and  $\sum\nolimits_{i=1}^{n} \mathcal{N}''_i =b''$.Thus,  the overall selection is $\mathcal{N} = \mathcal{N}' +  \mathcal{N}''$.
\begin{assumption}
	\label{asp:power_law}
	In the first stage, a portion of the overall budget $b' =  b \times \alpha$ is sufficient to collect the cache entries corresponding to the highest attention weights, ensuring their cumulative attention weights $\sigma $ exceed half of the total, i.e., $ \sigma =\sum\nolimits_{i=1}^{n}  \mathcal{N}'_i A_i = \sum \text{Top}_k(A,  b') > 0.5$.
\end{assumption}



In this paper, we set $\alpha$ in Assumption~\ref{asp:power_law} to a fixed value 0.5 based on two key considerations. First, as verified in Appendix \ref{apdx:check_asp}, allocating 50\% of the total budget is sufficient to capture enough attention weight in over 99\% of attention heads, thereby satisfying Assumption \ref{asp:power_law} across various settings. This is attributed to the power-law distribution of attention weights \citep{h2o}, where a small fraction of cache entries accounts for the majority of the weights.
Second, this choice is both robust and easy to apply across different cache budgets and models. While using different $\alpha$ values for specific models, budgets, or attention heads could yield finer optimization, it would also introduce significant search overhead and complicate deployment. Thus, we defer such granular adjustments to future work.  Subsequent experiments and visual analyses further confirm that setting $\alpha$ to 0.5 is a simple yet effective choice.


\begin{theorem}
	\label{thm:target}
	Given the stage 1 selection $\mathcal{N}'_i$, the objective $\mathcal{N}''_i$ of stage 2  is to minimize an upper bound $\hat{\theta}$ of the output perturbation $\mathcal{L}$, using the remaining budget $b'' = b - b'$.
	\begin{align}
		\small
	\argmin_{\mathcal{N}''_i}\hat{\theta} \:  \text{where} \: \hat{\theta} =   C' - \left(2 - \frac{1}{\sigma}\right)&\sum\nolimits_{i=1}^{n}  \mathcal{N}''_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:}  \rVert_1 \notag \\ \text{subject to}  \: \sum\nolimits_{i=1}^{n} \mathcal{N}''_i = b'', \notag \\
	 C' =   C -  \left(2 - \frac{1}{\sigma}\right) \sum\nolimits_{i=1}^{n} & \mathcal{N}'_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1.
	\end{align}	
\begin{proof}

	From Assumption \ref{asp:power_law}, the first stage selection ensures:  $\sum\nolimits_{i=1}^{n}  \mathcal{N}_i A_i >\sum\nolimits_{i=1}^{n}  \mathcal{N}'_i A_i = \sigma > 0.5$, leading to the inequality: $2- \frac{1}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} > 2 - \frac{1}{\sigma} >0$.
	{
		\small
		%C - \left(2- \frac{1}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} \right)\sum\nolimits_{i=1}^{n}  \mathcal{N}_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1    =
			\begin{align}
			\theta =&   C - \left(2- \frac{1}{\sum\nolimits_{i=1}^{n} \mathcal{N}_i A_{i}} \right) \sum\nolimits_{i=1}^{n}  (\mathcal{N}'_i+\mathcal{N}''_i) A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1  \notag \\
			< & C -  \left(2 - \frac{1}{\sigma}\right)\sum\nolimits_{i=1}^{n}  \mathcal{N}'_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 \notag \\
			 & -\left(2 - \frac{1}{\sigma}\right) \sum\nolimits_{i=1}^{n}  \mathcal{N}''_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1
		\end{align}
	}
	Let $C' =  C -  \left(2 - \frac{1}{\sigma}\right) \sum\nolimits_{i=1}^{n}  \mathcal{N}'_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1$, then we can derive a new upper bound $\hat{\theta}$ for $\mathcal{L}$ factoring by second stage selection $\mathcal{N}''_i$: $ \theta <  C' - \left(2 - \frac{1}{\sigma}\right)\sum\nolimits_{i=1}^{n}  \mathcal{N}''_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 = \hat{\theta} $
	Thus, minimizing $\hat{\theta}$ corresponds to selecting the $b''$ entries with the highest values of $\boldsymbol{\mathcal{A}}_i = A_i \lVert \boldsymbol{\mathcal{V}}{i,:} \rVert_1$, as implemented in the stage 2 selection  (Algorithm \ref{alg:selection}).
\end{proof}
\end{theorem}

Theorem  \ref{thm:target}  demonstrates that our second stage selection directly minimizes an upper bound of output perturbation for identifying critical cache entries. Unlike traditional strategies that rely solely on high attention weights for entry selection, the second stage of our algorithm jointly leverages both the attention weights and the value states projected through the parameter matrix, to directly constrain the worst-case output perturbation.



\begin{algorithm}[tb]
	\small
	\caption{Observation Window Based Cache Eviction.}
	\label{alg:cache_eviction}
	\textbf{Input}: All Query States $Q \in \mathbb{R}^{n \times d_{h}}$, KV Cache Entries $K,V  \in \mathbb{R}^{n \times d_{h}}$, Window Size $n'$\\	% Cache Budget $b$,
	\textbf{Output}: Critical Cache Entries $\hat{K},\hat{V}$
	\begin{algorithmic}[1] %[1] enables line numbers
		\STATE  allocating budget $b$ for one head   //  AdaKV \cite{ada}
		\STATE  $\hat{Q} = Q[-n':,:] \:$ 	%// extract query states in observation window
		\STATE  $A = \text{softmax}(\hat{Q}K^T) \:$
		\STATE $\bar{A} = A.\text{mean}(dim = 0)$
		\STATE $\bar{A} =  \text{maxpooling}(\bar{A}) $ // SnapKV \cite{SnapKV}
		\IF{ using regular selection}
		\STATE  select $b$ critical entries $\hat{K},\hat{V}$ by $\text{Top}_k(\bar{A}', b)$
		\ELSIF{ using our selection}
		\STATE select $b$ critical entries $\hat{K},\hat{V}$ by  Algorithm~\ref{alg:selection}
		\ENDIF\\
		\RETURN $\hat{K},\hat{V}$
	\end{algorithmic}
\end{algorithm}



\begin{figure*}[t!]
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=\textwidth]{./Figures/snap_niah_legend.pdf}
	\end{minipage}
	\centering
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/ruler_img/niah_single_2_criti_snap_wq_longbench_score.pdf}
		\vspace{-0.4cm}
		\caption{\centering Single retrieval\newline (Regular compression)}
		\label{subfig:snapkv_single_regular}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\textwidth]{./Figures/ruler_img/niah_multivalue_criti_snap_wq_longbench_score.pdf}
		\vspace{-0.4cm}
		\caption{\centering Multi retrieval\newline (Regular compression) }
		\label{subfig:snapkv_multi_regular}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/ruler_img/niah_single_2_criti_snap_longbench_score.pdf}
		\vspace{-0.4cm}
		\caption{\centering Single retrieval\newline(Context-Only compression)}
		\label{subfig:snapkv_single_context}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/ruler_img/niah_multivalue_criti_snap_longbench_score.pdf}
		\vspace{-0.4cm}
		\caption{\centering  Multi retrieval\newline(Context-only compression)}
		\label{subfig:snapkv_multi_context}
	\end{subfigure}
	\vspace{-0.2cm}
	\caption{Needle-in-a-Haystack test(Integrated into SnapKV). }
	\vspace{-0.2cm}
\end{figure*}

\begin{figure*}[t!]
	\centering
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=\textwidth]{./Figures/ada_niah_legend.pdf}
	\end{minipage}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/ruler_img/niah_single_2_criti_ada_wq_longbench_score.pdf}
		\vspace{-0.4cm}
		\caption{\centering  Single retrieval \newline (Regular compression)}
		\label{subfig:adakv_single_regular}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\textwidth]{./Figures/ruler_img/niah_multivalue_criti_ada_wq_longbench_score.pdf}
		\vspace{-0.4cm}
		\caption{\centering  Multi retrieval \newline(Regular compression)}
		\label{subfig:adakv_multi_regular}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/ruler_img/niah_single_2_criti_ada_longbench_score.pdf}
		\vspace{-0.4cm}
		\caption{\centering Single retrieval \newline (Context-only compression)}
		\label{subfig:adakv_single_context}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/ruler_img/niah_multivalue_criti_ada_longbench_score.pdf}
		\vspace{-0.4cm}
		\caption{\centering  Multi retrieval \newline (Context-only compression)}
		\label{subfig:adakv_multi_context}
	\end{subfigure}
	\vspace{-0.2cm}
	\caption{Needle-in-a-Haystack test(Integrated into AdaKV). }
	\vspace{-0.2cm}
\end{figure*}


\subsection{Integrating into SOTA cache eviction methods}
\label{sc:integration}
We showcase the effectiveness of our algorithm by integrating it into existing cache eviction methods that rely on accumulated attention weights for selecting critical entries.
Current SOTA cache eviction workflow is established by SnapKV~\cite{SnapKV}, which introduces an observation window mechanism to stably accumulate attention weights and employs the max pooling operations to avoid missing key information.
 Subsequent research~\cite{pyramidkv,ada} highlights the uneven distribution of critical cache entries across different heads, prompting the development of budget allocation strategies.
 The latest advancement, AdaKV~\cite{ada},dynamically detects variations in critical KV cache entries across heads during runtime, allowing for flexible budget scheduling and improving output quality based on SnapKV.
 These two SOTA methods, SnapKV and AdaKV, can be unified as Algorithm \ref{alg:cache_eviction}, with two main components: budget allocation across heads (line 1) and the observation window and pooling mechanism for attention weight accumulation (lines 2â€“5).
 Our algorithm could integrate smoothly by replacing the original solely attention weight-based selection(lines 6-10).


