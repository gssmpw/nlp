\definecolor{question_color}{RGB}{0,100,0}

\newpage

\appendix

{
\section{Validation of Assumption~\ref{asp:power_law}}
\label{apdx:check_asp}
We ensure the reliability of Assumption \ref{asp:power_law} by analyzing the cumulative attention weights of critical KV Cache entries $\sum\nolimits_{i=1}^{n} \mathcal{N}i A{i}$ in individual heads. As shown in Figure \ref{fig:check_asp}, for varying models and budget levels, the majority of attention heads can effectively accumulate over half of the attention weights. The only exceptions are a few attention heads in the first layer. This is primarily due to the low sparsity of attention weights in certain heads of the first layer, a phenomenon that has been noted in many related studies \cite{quest,h2o,pyramidkv}. This observation aligns with the fact that certain heads in the first layer violate Assumption \ref{asp:power_law}, which may lead our algorithm to increase the output perturbation of these heads as shown in Figure \ref{fig:head_wise}.  
However, this is not a significant issue in our current algorithm, as these heads constitute less than 1\% of the total, and their negative impact is easily offset by the gains from other heads, resulting in substantial overall benefits. A potential solution to this issue is to set the algorithm's threshold $\alpha$ based on the characteristics of each head to achieve greater benefits. However, considering the additional complexity this approach might introduce, especially given the potential impact of different cache sizes and models, we leave this further optimization for future research.




\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{./Figures/assumption/legend.pdf}
	\hspace{-0.1cm}

	\begin{subfigure}[b]{0.245\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/weight_acc/multi_news_Llama_acc_weight_0.975.png}
		\caption{Llama Cache Size 2.5\%}
	\end{subfigure}
	\begin{subfigure}[b]{0.245\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/weight_acc/multi_news_Llama_acc_weight_0.95.png}
		\caption{Llama Cache Size 5\%}
		%	\label{fig:sub2}
	\end{subfigure}
	\begin{subfigure}[b]{0.245\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/weight_acc/multi_news_Llama_acc_weight_0.9.png}
		\caption{Llama Cache Size 10\%}
		%	\label{fig:sub2}
	\end{subfigure}
	\begin{subfigure}[b]{0.245\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/weight_acc/multi_news_Llama_acc_weight_0.8.png}
		\caption{Llama Cache Size 20\%}
	\end{subfigure}


	\begin{subfigure}[b]{0.245\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/weight_acc/multi_news_Mistral_acc_weight_0.975.png}
		\caption{Mistral Cache Size 2.5\%}
	\end{subfigure}
	\begin{subfigure}[b]{0.245\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/weight_acc/multi_news_Mistral_acc_weight_0.95.png}
		\caption{Mistral Cache Size 5\%}
		%	\label{fig:sub2}
	\end{subfigure}
	\begin{subfigure}[b]{0.245\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/weight_acc/multi_news_Mistral_acc_weight_0.9.png}
		\caption{Mistral Cache Size10\%}
		%	\label{fig:sub2}
	\end{subfigure}
	\begin{subfigure}[b]{0.245\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/weight_acc/multi_news_Mistral_acc_weight_0.8.png}
		\caption{Mistral Cache Size 20\%}
		%	\label{fig:sub2}
	\end{subfigure}
	\caption{Accumulated attention weights: Assumption~\ref{asp:power_law} validates in over 99\% of heads across various cache sizes.}
	\label{fig:check_asp}
\end{figure}
}


\section{Additional Empirical Analysis}
\label{apdx:detail_analysis}
Figure~\ref{fig:head_wise_mistral} provides a visualization of the reduced perturbations achieved by our algorithm across different heads in the Mistral model, demonstrating its continued effectiveness.
We then present a more detailed visual analysis to support the conclusions in the main paper. Figures \ref{fig:detail_layer_reduction}~\ref{fig:detail_budget_reduction} illustrate the reduced perturbation when combining AdaKV with our algorithm under various layers and budgets. Clearly, in all scenarios, our algorithm significantly reduces output perturbation of AdaKV after cache eviction, resulting in lower quality loss and improved final generation quality.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.75\linewidth]{./Figures/head_wise_legend.pdf}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen1/head/multi_news_SnapKVPress_ws32_ks7_wqFalse_Mistral-7B-Instruct-v0.3_head_heat_0.8_gen1.png}
		\vspace{-0.5cm}
		\caption{Token1 }
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen3/head/multi_news_SnapKVPress_ws32_ks7_wqFalse_Mistral-7B-Instruct-v0.3_head_heat_0.8_gen3.png}
		\vspace{-0.5cm}
		\caption{Token2 }
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/snap/gen5/head/multi_news_SnapKVPress_ws32_ks7_wqFalse_Mistral-7B-Instruct-v0.3_head_heat_0.8_gen5.png}
		\vspace{-0.5cm}
		\caption{Token3 }
	\end{subfigure}
	\caption{ Perturbation reduction across heads. (Mistral)} 
	\label{fig:head_wise_mistral}
	

\end{figure}


\begin{figure}[t]
			\centering
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/ada/gen1/layer/multi_news_wqFalse_AdaSnapKVPress_ws32_ks7_a0.2_reduction_across_layers_budget0.8_gen1.png}
		\caption{Token 1}
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/ada/gen3/layer/multi_news_wqFalse_AdaSnapKVPress_ws32_ks7_a0.2_reduction_across_layers_budget0.8_gen3.png}
		\caption{Token 2}
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/ada/gen5/layer/multi_news_wqFalse_AdaSnapKVPress_ws32_ks7_a0.2_reduction_across_layers_budget0.8_gen5.png}
		\caption{Token 3}
	\end{subfigure}
	\caption{Perturbation reduction across layers on AdaKV.}
	\label{fig:detail_layer_reduction}
	
	\centering
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/ada/gen1/budget/multi_news_wqFalse_AdaSnapKVPress_ws32_ks7_a0.2_gen1_reduction_across_budgets.png}
		\caption{Token1}
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/ada/gen3/budget/multi_news_wqFalse_AdaSnapKVPress_ws32_ks7_a0.2_gen3_reduction_across_budgets.png}
		\caption{Token2}
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{./Figures/analyze/ada/gen5/budget/multi_news_wqFalse_AdaSnapKVPress_ws32_ks7_a0.2_gen5_reduction_across_budgets.png}
		\caption{Token3}
	\end{subfigure}
	\caption{Perturbation reduction across budgets on AdaKV.}
	\label{fig:detail_budget_reduction}
\end{figure}






\section{Task Domain Analysis of Longbench Results on Mistral Model}
\label{apdx:longbench_mistral}
Table~\ref{tab:mistral} presents the performance of the Mistral model on the LongBench benchmark. It shows that our enhanced cache eviction method improves the quality across various task domains. In the context-only compression scenario, the enhanced method shows improvements in 22 out of 24 task domains, while in the regular compression scenario, 18 domains show improvements, demonstrating a clear advantage.

\section{Details of 16 Datasets in Longbench}
\label{apdx:details_datasets}

 As a widely used long-context benchmark~\cite{ada,SnapKV,pyramidkv}, LongBench consists of 16 datasets across six task domains: single-document question answering (QA) \citep{kovcisky2018narrativeqa,dasigi2021dataset}, multi-document QA \citep{multi_hop1,ho-etal-2020-constructing,trivedi2022musique}, summarization \citep{huang2021efficient,zhong2021qmsum,fabbri2019multi}, few-shot learning \citep{joshi2017triviaqalargescaledistantly,gliwa2019samsum,li2002learning}, synthetic tasks \citep{bai2023longbench}, and code generation \citep{guo2023longcoderlongrangepretrainedlanguage,liu2023repobenchbenchmarkingrepositorylevelcode}. The average token length across all 16 datasets is 6,711. Table \ref{tab:detail_datasets} provides detailed information on the 16 datasets in LongBench.


\begin{table*}[t!]
	\centering
	\small
	\caption{LongBench results on Mistral model.}
	\label{tab:mistral}
	\begin{tabular}{@{}l>{\hspace{-0.8em}}l>{\hspace{-1.5em}}c>{\hspace{0em}} c>{\hspace{-1em}}c  c>{\hspace{-0.8em}}c c>{\hspace{-0.8em}}c c>{\hspace{-0.8em}}c@{}}
		\toprule
		& \multirow{2}{*}{Domain} &  \multirow{2}{*}{\makecell{6711 \\Full Cache}} & \multicolumn{2}{c}{\small \makecell{SnapKV $b =$ 20\%}} & \multicolumn{2}{c}{\small \makecell{SnapKV $b =$ 40\%}} & \multicolumn{2}{c}{\small \makecell{AdaKV $b =$ 20\%}} & \multicolumn{2}{c}{\small \makecell{AdaKV $b =$ 40\%}} \\
		\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
		& &  & w/o ours   & w/ ours   & w/o ours  & w/ ours   & w/o ours  & w/ ours  & w/o ours  & w/ ours   \\
		
		\toprule
		\multirow{6}{*}{\small\rotatebox[origin=c]{90}{\makecell{Mistral-7B \\ Regular}}}
		& Single-Doc. QA & 38.37 & 25.13                   & \textbf{28.24}                & 31.64                   & \textbf{34.33}                & 27.25                       & \textbf{29.06}                    & 33.42                       & \textbf{36.06}                    \\
		& Multi-Doc. QA  & 39.40 & 29.64                   & \textbf{32.07}                & 33.57                   & \textbf{36.61}                & 31.71                       & \textbf{33.04}                    & 35.97                       & \textbf{38.00}                    \\
		& Summarization  & 28.76 & 24.18                   & \textbf{24.61}                & 26.24                   & \textbf{26.94}                & 24.18                       & \textbf{24.83}                    & 26.24                       & \textbf{27.06}                    \\
		& Few-shot       & 70.33 & 63.89                   & \textbf{65.54}                & 67.95                   & \textbf{68.57}                & 66.00                       & \textbf{67.08}                    & 68.67                       & \textbf{69.79}                    \\
		& Synthetic      & 52.50 & 45.25                   & \textbf{46.53}                & 49.75                   & \textbf{51.05}                & 48.00                       & \textbf{49.25}                    & 50.75                       & \textbf{50.84}                    \\
		& Code           & 61.25 & 61.40                   & \textbf{61.94}                & \textbf{63.41}          & 62.06                         & 62.55                       & \textbf{62.56}                    & \textbf{63.35}              & 62.68                             \\
		\hline
		& Ave.           & 47.38 & 40.11                   & \textbf{41.77}                & 44.03                   & \textbf{45.35}                & 41.78                       & \textbf{42.85}                    & 45.07                       & \textbf{46.23}                    \\
		
		\hline
		\hline
		
		\multirow{6}{*}{\small\rotatebox[origin=c]{90}{\makecell{Mistral-7B \\ Context-only}}}
		& Single-Doc. QA & 38.37 & \textbf{37.85}          & 37.42                         & 37.97                   & \textbf{38.12}                & 36.98                       & \textbf{37.27}                    & \textbf{38.40}              & 37.71          \\
		& Multi-Doc. QA  & 39.40 & 37.67                   & \textbf{38.06}                & 38.75                   & \textbf{39.37}                & 38.00                       & \textbf{38.08}                    & \textbf{39.39}              & 39.23          \\
		& Summarization  & 28.76 & 25.99                   & \textbf{26.28}                & 27.19                   & \textbf{27.80}                & 26.00                       & \textbf{26.29}                    & 27.48                       & \textbf{27.82} \\
		& Few-shot       & 70.33 & 70.15                   & \textbf{70.23}                & 70.23                   & \textbf{70.68}                & 70.12                       & \textbf{70.44}                    & 70.46                       & \textbf{70.72} \\
		& Synthetic      & 52.50 & \textbf{51.75}          & 51.03                         & 51.75                   & \textbf{52.05}                & 51.75                       & \textbf{52.00}                    & \textbf{51.75}              & 51.34          \\
		& Code           & 61.25 & 61.15                   & \textbf{61.30}                & 61.52                   & \textbf{61.67}                & 61.47                       & \textbf{61.52}                    & \textbf{61.34}              & 61.30          \\
		\hline
		& Ave.           & 47.38 & \textbf{46.30}          & 46.29                         & 46.81                   & \textbf{47.21}                & 46.23                       & \textbf{46.45}                    & \textbf{47.08}              & 46.98          \\
		\hline
		
	\end{tabular}%
	% }

\end{table*}

\begin{table*}[thb!]
	\centering
	\small
		\caption{Details of 16 datasets in LongBench.}
	\label{tab:detail_datasets}
	\begin{tabular}{@{}lllllr@{}}
		\toprule
		Task                & Task Type     & Eval metric & Avg len & Language & Sample Num        \\ \midrule
		NarrativeQA         & Single-Doc. QA & F1          & 18,409   & EN       & 200            \\
		Qasper              & Single-Doc. QA & F1          & 3,619    & EN       & 200            \\
		MultiFieldQA-en     & Single-Doc. QA & F1          & 4,559    & EN       & 150            \\
		HotpotQA            & Multi-Doc. QA  & F1          & 9,151    & EN       & 200            \\
		2WikiMultihopQA     & Multi-Doc. QA  & F1          & 4,887    & EN       & 200            \\
		MuSiQue             & Multi-Doc. QA  & F1          & 11,214   & EN       & 200            \\
		GovReport           & Summarization & Rouge-L     & 8,734    & EN       & 200            \\
		QMSum               & Summarization & Rouge-L     & 10,614   & EN       & 200            \\
		MultiNews           & Summarization & Rouge-L     & 2,113    & EN       & 200            \\
		TREC                & Few-shot Learning & Accuracy    & 5,177    & EN       & 200            \\
		TriviaQA            & Few-shot Learning & F1          & 8,209    & EN       & 200            \\
		SAMSum              & Few-shot Learning & Rouge-L     & 6,258    & EN       & 200            \\
		PassageCount        & Synthetic     & Accuracy    & 11,141   & EN       & 200            \\
		PassageRetrieval-en & Synthetic     & Accuracy    & 9,289    & EN       & 200            \\
		LCC                 & Code          & Edit Sim    & 1,235      & Python/C\#/Java & 500\\
		RepoBench-P         & Code          & Edit Sim    & 4,206      & Python/Java & 500    \\ \bottomrule
	\end{tabular}
\end{table*}




{




\section{Analysis of Previous Solely Attention Weights-Based Selection from a Perturbation Perspective}
Our algorithm differs from the previous solely attention weights-based selection method primarily in Stage 2. Specifically, by modifying stage 2 of our algorithm to perform the same attention weights-based selection operation as in stage 1, our approach will degrade into the previous method. This modification allows us to conveniently apply perturbation-constrained theory to analyze the earlier attention weights-based selection strategy.
\begin{theorem}
	\label{thm:attn_weights_select}
	Previous solely attention weights-based selection is equivalent to minimizing another upper bound $\hat{\theta}^{relax}$, a relaxed form of $\hat{\theta}$, with remaining budget $b''$ based on stage 1 selection.	 
	\begin{align}
		\hat{\theta}^{relax} =  C' - M\left(2- \frac{1}{\sigma}\right)\sum\nolimits_{i=1}^{n}  \mathcal{N}''_i A_i \quad \text{where} \quad M = MIN(\lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 )
	\end{align}
\end{theorem}
\begin{proof}
	We relax the upper bound $\hat{\theta}$ by utilizing $M = MIN(\lVert \boldsymbol{\mathcal{V}}_{i,:} \rVert_1 )$:
	\begin{equation}
		\hat{\theta} =  C' - \left(2- \frac{1}{\sigma}\right)\sum\nolimits_{i=1}^{n} \mathcal{N}''_i A_i \lVert \boldsymbol{\mathcal{V}}_{i,:}  \rVert_1 \leq C' - M \left(2- \frac{1}{\sigma}\right)\sum\nolimits_{i=1}^{n} \mathcal{N}''_i A_i  = \hat{\theta}^{relax}
	\end{equation}

	
	In the solely attention weights-based selection strategy, the $\mathcal{N}''$ selection is performed using \(Top-K(A_i, b'')\) to maximize \(\sum\nolimits_{i=1}^{n} \mathcal{N}''_i A_i\). This is therefore equivalent to minimizing the relaxed upper bound, \(\hat{\theta}^{relax}\).

	
\end{proof}
Theorem~\ref{thm:attn_weights_select} demonstrates that the solely attention weights-based selection strategy is equivalent to minimizing the relaxed upper bound \(\hat{\theta}^{relax}\).
In contrast, our algorithm optimizes a tighter upper bound, $\hat{\theta}$. While this does not guarantee that our approach will yield a strictly better solution, intuitively, an algorithm designed to optimize a tighter bound often achieves better results.
Theorem~\ref{thm:attn_weights_select} also provides some insight into why a critical KV Cache subset can replace the entire KV Cache in cache eviction methods. Due to the power-law distribution of attention weights~\cite{h2o}, removing most cache entries with near-zero attention weights has a negligible impact on this upper bound. Consequently, the perturbation to the actual output is also bounded by this upper bound.





\begin{figure*}[t!]
	\centering
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=\textwidth]{./Figures/l2_ablate/l2_snap_legend.pdf}
	\end{minipage}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/l2_ablate/niah_single_2_criti_snap_llama_l2_ablate.pdf}
		\vspace{-0.5cm}
		\caption{\centering  Single retrieval \newline Llama}
		\label{subfig:snap_llama_l2_single}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/l2_ablate/niah_multivalue_criti_snap_llama_l2_ablate.pdf}
		\vspace{-0.5cm}
		\caption{\centering  Multi retrieval \newline Llama}
		\label{subfig:snap_llama_l2_multi}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/l2_ablate/niah_single_2_criti_snap_mis_l2_ablate.pdf}
		\vspace{-0.5cm}
		\caption{\centering  Single retrieval \newline Mistral}
		\label{subfig:snap_mis_l2_single}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/l2_ablate/niah_multivalue_criti_snap_mis_l2_ablate.pdf}
		\vspace{-0.5cm}
		\caption{\centering  Multi retrieval \newline Mistral}
		\label{subfig:snap_mis_l2_multi}
	\end{subfigure}
	
	\vspace{-0.3cm}
	\caption{Choice of Distance Metric: $L_1$ distance and $L_2$ distance. (SnapKV)}
	\label{fig:choice_distance_metric_snap}
	
	
	\centering
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=\textwidth]{./Figures/l2_ablate/l2_ada_legend.pdf}
	\end{minipage}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/l2_ablate/niah_single_2_criti_ada_llama_l2_ablate.pdf}
		\vspace{-0.5cm}
		\caption{\centering  Single retrieval \newline Llama}
		\label{subfig:ada_llama_l2_single}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/l2_ablate/niah_multivalue_criti_ada_llama_l2_ablate.pdf}
		\vspace{-0.5cm}
		\caption{\centering  Multi retrieval \newline Llama}
		\label{subfig:ada_llama_l2_multi}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/l2_ablate/niah_single_2_criti_ada_mis_l2_ablate.pdf}
		\vspace{-0.5cm}
		\caption{\centering  Single retrieval \newline Mistral}
		\label{subfig:ada_mis_l2_single}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{./Figures/l2_ablate/niah_multivalue_criti_ada_mis_l2_ablate.pdf}
		\vspace{-0.5cm}
		\caption{\centering  Multi retrieval \newline Mistral}
		\label{subfig:ada_mis_l2_multi}
	\end{subfigure}
	
	\vspace{-0.3cm}
	\caption{Choice of Distance Metric: $L_1$ distance and $L_2$ distance. (AdaKV)}
	\label{fig:choice_distance_metric_ada}
	
	
	
\end{figure*}
\section{Choice of Distance Metric}
\label{apdx:distance}
 As demonstrated in preliminary evaluations shown in Figures \ref{fig:choice_distance_metric_snap} and \ref{fig:choice_distance_metric_ada}, both $L_1$ and $L_2$ distance-based perturbation-constrained selection algorithms effectively enhance the retrieval scores of original SnapKV and AdaKV. However, we don't observe notable quality improvements when using the $L_2$ distance compared to the simpler $L_1$ distance. Additionally, the $L_1$ distance provides advantages in terms of simplicity and greater stability in half-precision floating-point computations. Thus, we adopt the $L_1$ distance metric in this paper for our analysis.  Future work could explore more complex distance metrics within our framework, offering a promising research direction.
 
 

\section{Additional Related Works}
Sparse attention methods~\cite{jiang2024minference10acceleratingprefilling,tang2024questqueryawaresparsityefficient,lv2024critiprefillsegmentwisecriticalitybasedapproach} are conceptually related to the KV cache eviction methods discussed in this paper. While KV cache eviction retains only a small subset of essential KV cache entries, sparse attention methods maintain all entries during inference. However, during computation, only the most critical entries are selectively utilized in the sparse attention mechanism. Consequently, sparse attention methods do not reduce the memory footprint of the KV cache but enhance inference speed and often offer better output quality than cache eviction methods~\cite{tang2024questqueryawaresparsityefficient}. Existing sparse attention methods typically rely on approximate estimations of attention weights to identify critical entries~\cite{tang2024questqueryawaresparsityefficient,lv2024critiprefillsegmentwisecriticalitybasedapproach}. Future works could explore integrating our proposed perturbation-constrained selection algorithm to refine these methods by achieving more accurate critical cache entry identification.

Some adaptive methods in KV cache eviction or sparse attention, such as~\cite{ge2024modeltellsdiscardadaptive, jiang2024minference10acceleratingprefilling}, employ varying critical cache selection strategies tailored to the characteristics of different attention heads. For example, some heads use attention weights based selection, while others utilize fixed patterns, such as recent window-based or special token-based approaches. Our method can also be applied to enhance performance in the head which according to attention weights-based selection strategies, providing a boost to adaptive methods.

KV cache quantization refers to the application of quantization techniques to reduce the size of the KV cache by lowering the precision of individual cache entries \citep{liu2024kivi,hooper2024kvquant}. For example, this can involve quantizing the original 16-bit KV cache entries to 4-bit or 2-bit precision. However, these methods typically retain all cache entries, making them fundamentally orthogonal to the cache eviction methods explored in this paper.







{
\section{Prompt Templates for Ruler and Longbench in Regular and Context-only Compression Scenarios}
\label{apdx:prompt_templates}
}

Below are prompt templates for various tasks. We assess performance under two scenarios: regular compression and context-only compression. We adhere to the input prompt format from KVPress \cite{kvpress}, dividing the input into context and question segments. The question segment is highlighted in green, while other colors represent the context segment. In regular compression, both the context and question segments are input into the model and compressed. For context-only compression, where future questions are unpredictable, only the context segment is input for compression. After compression, the question segment is input for answer generation.
\subsection{NIAH Template}


In the Needle-in-A-Haystack task, a keyword, referred to as the "needle", is embedded within a lengthy context known as the "haystack". The objective of this task is to extract the "needle" from the "haystack", which is composed of essays by Paul Graham \cite{needle}.

For the Single Needle-in-A-Haystack(S-NIAH) task, the goal is to retrieve a single "needle". Similarly, the Multi-Value Needle-in-A-Haystack(MV-NIAH) task requires the extraction of multiple inserted "needles". To prevent models from refusing to answer our questions, we append the answer prefix to the input, prompting the models to generate answers.


\begin{table*}[h]
	\small
	\centering
		\caption{Single retrieval and multi retrieval templates in Needle-in-A-Haystack tests.}
	\label{tab:ruler_task_template1}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{cp{0.9\linewidth}}
			\toprule
			
			
            \begin{tabular}{@{}c@{}}Single retrieval\end{tabular} & 
			\begin{tabular}{@{}p{\linewidth}@{}} 
				\textbf{Task Template:} \\
				Some special magic numbers are hidden within the following text. Make sure to memorize it. I will quiz you about the numbers afterwards.\\
				\textcolor{lightgray}{Paul Graham Essays.} \\
				\textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{number\}}. \textcolor{lightgray}{......}\\
                \textcolor{question_color}{What is the special magic number for \{word\} mentioned in the provided text?} \\ \\
				% \textbf{Task Answer Prefix:} \\
                \textcolor{question_color}{The special magic number for \{word\} mentioned in the provided text is}
            \end{tabular}\\
			
            \midrule

            \begin{tabular}{@{}c@{}}Multi retrieval\end{tabular} &
            \begin{tabular}{@{}p{\linewidth}@{}} 
            \textbf{Task Template:} \\
            Some special magic numbers are hidden within the following text. Make sure to memorize it. I will quiz you about the numbers afterwards.\\
            \textcolor{lightgray}{Paul Graham Essays.} \\
            \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{number-1\}}. \textcolor{lightgray}{......}\\
            \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{number-2\}}. \textcolor{lightgray}{......}\\
            \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{number-3\}}. \textcolor{lightgray}{......}\\
            \textcolor{lightgray}{......} One of the special magic numbers for \textcolor{violet}{\{word\}} is: \textcolor{orange}{\{number-4\}}. \textcolor{lightgray}{......}\\
            \textcolor{question_color}{What are all the special magic numbers for \{word\} mentioned in the provided text?} \\ \\
            % \textbf{Task Answer Prefix:} \\
            \textcolor{question_color}{The special magic numbers for \{word\} mentioned in the provided text are}
            \end{tabular}\\

			
			\bottomrule
	\end{tabular}}

\end{table*}


\subsection{LongBench Template}


The construction of the LongBench template follows the official formats \cite{longbench} to evaluate performance under regular compression and context-only compression.


\begin{table*}[h]
    \small
    \centering
    \caption{LongBench templates. Single-Doc. QA Tasks.}
    \label{tab:task_template3}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cp{0.9\linewidth}}
    \toprule
    
    \begin{tabular}{@{}c@{}}NarrativeQA\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

        You are given a story, which can be either a novel or a movie script, and a question. Answer the question asconcisely as you can, using a single phrase if possible. Do not provide any explanation. \\\\
        Story: \textcolor{orange}{\{context\}} \\\\

        \textcolor{question_color}{Now, answer the question based on the story asconcisely as you can, using a single phrase if possible. Do not provide any explanation.}\\\\

        \textcolor{question_color}{Question: \textcolor{question_color}{\{question\}}}\\

    % \textbf{Task Answer Prefix:} \\
    % Answer: 
    \end{tabular}\\


    \midrule

    \begin{tabular}{@{}c@{}}Qasper\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable". Do not provide any explanation.\\\\
    Article: \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable". Do not provide any explanation.}\\\\
    \textcolor{question_color}{Question: \textcolor{question_color}{\{question\}}}\\

    % \textbf{Task Answer Prefix:} \\
    % Answer: 
    \end{tabular}\\


    \midrule

    \begin{tabular}{@{}c@{}}MultifieldQA EN\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Read the following text and answer briefly.\\\\
    \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Now, answer the following question based on the above text, only give me the answer and do not output any other words.}\\\\
    \textcolor{question_color}{Question: \textcolor{question_color}{\{question\}}} \\
    % \textbf{Task Answer Prefix:} \\
    % Answer: 
    \end{tabular}\\


    \bottomrule
    \end{tabular}}
\end{table*}


\begin{table*}[h]
    \small
    \centering
    \caption{LongBench templates. Multi-Doc. QA Tasks.}
    \label{tab:task_template3}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cp{0.9\linewidth}}
    \toprule

    \begin{tabular}{@{}c@{}}HotpotQA\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Answer the question based on the given passages. Only give me the answer and do not output any other words.\\\\
    The following are given passages.\\
    \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Answer the question based on the given passages. Only give me the answer and do not output any other words.}\\\\
    \textcolor{question_color}{Question: \textcolor{question_color}{\{question\}}} \\

    % \textbf{Task Answer Prefix:} \\
    % Answer: 
    \end{tabular}\\
    

    \midrule

    \begin{tabular}{@{}c@{}}2WikimQA\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Answer the question based on the given passages. Only give me the answer and do not output any other words.\\\\
    The following are given passages.\\
    \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Answer the question based on the given passages. Only give me the answer and do not output any other words.}\\\\
    \textcolor{question_color}{Question: \textcolor{question_color}{\{question\}}}

    % \textbf{Task Answer Prefix:} \\
    % Answer: 
    \end{tabular}\\


    \midrule

    \begin{tabular}{@{}c@{}}Musique\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Answer the question based on the given passages. Only give me the answer and do not output any other words.\\\\
    The following are given passages.\\
    \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Answer the question based on the given passages. Only give me the answer and do not output any other words.}\\\\
    \textcolor{question_color}{Question: \textcolor{question_color}{\{question\}}}

    % \textbf{Task Answer Prefix:} \\
    % Answer: 
    \end{tabular}\\

    %%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
    \end{tabular}}
\end{table*}


\begin{table*}[h]
    \small
    \centering
    \caption{LongBench templates. Summarization Tasks.}
    \label{tab:task_template3}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cp{0.9\linewidth}}
    \toprule

    \begin{tabular}{@{}c@{}}Gov Report\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    You are given a report by a government agency. Write a one-page summary of the report.\\\\
    Report:\\
    \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Now, write a one-page summary of the report.} \\
    % \textbf{Task Answer Prefix:} \\
    % Summary: 
    \end{tabular}\\


    \midrule

    \begin{tabular}{@{}c@{}}QMSum\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    You are given a meeting transcript and a query containing a question or instruction. Answer the query in one or more sentences.\\\\
    Transcript:\\
    \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Now, answer the query based on the above meeting transcript in one or more sentences.}\\\\
    \textcolor{question_color}{Query: \textcolor{question_color}{\{question\}}} \\
    % \textbf{Task Answer Prefix:} \\
    % Answer:
    \end{tabular}\\



    \midrule

    \begin{tabular}{@{}c@{}}Multi News\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    You are given several news passages. Write a one-page summary of all news. \\\\
    News:\\
    \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Now, write a one-page summary of all the news.}\\
    % \textbf{Task Answer Prefix:} \\
    % Summary:
    \end{tabular}\\


    %%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
    \end{tabular}}
\end{table*}


\begin{table*}[h]
    \small
    \centering
    \caption{LongBench templates. Few-shot Learning Tasks.}
    \label{tab:task_template3}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cp{0.9\linewidth}}
    \toprule


    \begin{tabular}{@{}c@{}}TREC\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Please determine the type of the question below. Here are some examples of questions.\\\\
    \textcolor{orange}{\{context\}}\\
    % \textcolor{violet}{\{question\}} \\
    \textcolor{question_color}{\{question\}} \\
    % \textbf{Task Answer Prefix:} \\

    \end{tabular}\\


    \midrule

    \begin{tabular}{@{}c@{}}TriviaQA\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Answer the question based on the given passage. Only give me the answer and do not output any other words. The following are some examples.\\\\
    \textcolor{orange}{\{context\}}\\\\
    % \textcolor{violet}{\{question\}} \\
    \textcolor{question_color}{\{question\}} \\
    % \textbf{Task Answer Prefix:} \\

    \end{tabular}\\



    \midrule

    \begin{tabular}{@{}c@{}}SAMSum\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Summarize the dialogue into a few short sentences. The following are some examples.\\\\
    \textcolor{orange}{\{context\}}\\\\
    % \textcolor{violet}{\{question\}} \\
    \textcolor{question_color}{\{question\}} \\
    % \textbf{Task Answer Prefix:} \\
    \end{tabular}\\


    %%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
    \end{tabular}}
\end{table*}


\begin{table*}[h]
    \small
    \centering
    \caption{LongBench templates. Synthetic Tasks.}
    \label{tab:task_template3}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cp{0.9\linewidth}}
    \toprule


    \begin{tabular}{@{}c@{}}Passage Count\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    There are some paragraphs below sourced from Wikipedia. Some of them may be duplicates. Please carefully read these paragraphs and determine how many unique paragraphs there are after removing duplicates. In other words, how many non-repeating paragraphs are there in total?\\\\
    \textcolor{orange}{\{context\}}\\\\
    \textcolor{question_color}{Please enter the final count of unique paragraphs after removing duplicates. The output format should only contain the number, such as 1, 2, 3, and so on.}\\
    % \textbf{Task Answer Prefix:} \\
    % The final answer is: 
    \end{tabular}\\


    \midrule

    \begin{tabular}{@{}c@{}}Passage Retrieval EN\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Here are 30 paragraphs from Wikipedia, along with an abstract. Please determine which paragraph the abstract is from.\\\\
    \textcolor{orange}{\{context\}}\\\\
    The following is an abstract.\\\\
    \textcolor{question_color}{\{question\}}\\\\
    \textcolor{question_color}{Please enter the number of the paragraph that the abstract is from. The answer format must be like "Paragraph 1", "Paragraph 2", etc.}\\
    % \textbf{Task Answer Prefix:} \\
    % The answer is
    \end{tabular}\\



    %%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
    \end{tabular}}
\end{table*}


\begin{table*}[h]
    \small
    \centering
    \caption{LongBench templates. Code Tasks.}
    \label{tab:task_template3}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cp{0.9\linewidth}}
    \toprule


    \begin{tabular}{@{}c@{}}Lcc\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Please complete the code given below. \\
    \textcolor{orange}{\{context\}} \\
    \textcolor{question_color}{Next line of code:}
    % \textbf{Task Answer Prefix:} \\
    % Next line of code:
    \end{tabular}\\

    \midrule

    \begin{tabular}{@{}c@{}}Repobench-P\end{tabular} & 
    \begin{tabular}{@{}p{\linewidth}@{}} 
    \textbf{Task Template:} \\

    Please complete the code given below. \\
    \textcolor{orange}{\{context\}} \\
        % \textcolor{violet}{\{question\}} \\
    \textcolor{question_color}{\{question\}} \\
    \textcolor{question_color}{Next line of code:}
    % \textbf{Task Answer Prefix:} \\
    % Next line of code:
    \end{tabular}\\

    %%%%%%%%%%%%%%%%%%%%%%%%

    \bottomrule
    \end{tabular}}
\end{table*}


