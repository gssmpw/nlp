\vspace{-4mm}
\section{Related Work}
\label{sec:formatting}

\begin{figure*}[!t]
\centering
    \includegraphics[width=\linewidth]{fig/figure_2_hexplane_cr.jpg}
    \vspace{-5mm}
    \caption{\textbf{Overview of 4D Gaussian Splatting}: 4DGS represents dynamic scenes by separating static (canonical 3D Gaussians $\mathcal{G}_\text{canon}$) and dynamic components (Gaussian deformation field $\left\{\mathcal{E}(\cdot), \mathcal{D}(\cdot)\right\}$). Given a Gaussian primitive's position $p$ and timestep $t$, a spatio-temporal embedding voxel feature $f_d$ is queried from the Hexplane. This feature is then processed by a multi-head MLP decoder $\{\phi_p(\cdot), \phi_s(\cdot), \phi_r(\cdot)\}$ to generate per-Gaussian deformation parameters $\Delta p_{t,i}$, $\Delta s_{t,i}$, and $\Delta r_{t,i}$. By adding these parameters to the canonical Gaussians $\mathcal{G}_\text{canon}$, we obtain the deformed Gaussians $\mathcal{G}_{\text{def},t}$. Finally, by repeatedly generating and rendering $\mathcal{G}_{\text{def},t}$ across timesteps, the dynamic scene video is obtained.}
    \label{fig:4dgs}
    \vspace{-2mm}
\end{figure*}


\subsection{4D Dynamic Scene Representation}
Recent advancements in computer vision and graphics have fueled interest in 4D dynamic scene representation, which models both spatial and temporal information. As high-quality 4D content capture continues to improve, multi-view 4D data has become increasingly available, highlighting the need for efficient representations to mitigate the high computational costs of 4D modeling. Many approaches~\cite{ref_22_nerfplayer, ref_27_tensor4d, ref_45_dnerf, ref_46_neuralradianceflow, ref_47_devrf, ref_48_nerfies, ref_49_nonrigid} have reduced the complexity of dynamic scene representation by handling the temporal dimension separately, leading to the decoupling of the canonical 3D representation and the deformation field. Specifically, K-plane and Hexplane~\cite{ref_15_kplanes,ref_16_hexplane} construct spatio-temporal encoding structures within the deformation field using multi-scale parameter grids through planar factorization. Other methods~\cite{ref_11_spacetimegs,ref_12_iclr24,ref_13_gaufre, ref_14_4drotergs} have enhanced the overall performance of dynamic scenes by employing 3D Gaussian Splatting~\cite{ref_8_gs} as the canonical 3D representation, which has recently gained attention for its real-time rendering capabilities and high visual quality. Notably, 4D Gaussian Splatting~\cite{ref_10_4dgs} combines 3DGS with the Hexplane deformation field to achieve real-time rendering speeds while more accurately modeling dynamic scenes. Given its excellent performance, 4DGS holds great potential for dynamic scene generation~\cite{ref_17_dreamgaussian4d, ref_19_aligngs, ref_25_4dfy}, editing~\cite{ref_9_i4d24d, ref_33_control4d}, and tracking~\cite{ref_50_dynamic3dgs, ref_51_motionaware}. In this paper, we employ 4DGS to maximize the efficiency of the dynamic scene editing process.


\subsection{Instruction-Guided Scene Editing}
User instructions provide one of the most intuitive and user-friendly approaches to scene editing. InstructPix2Pix~\cite{ref_1_ip2p} introduced instruction-guided editing by fine-tuning the Stable Diffusion~\cite{ref_2_ldm} model on a dataset of \textit{source image}–\textit{instruction}–\textit{target image} triplets. Recent studies~\cite{ref_3_in2n, ref_5_gaussianeditor, ref_6_dge, ref_7_tiger, ref_28_i3dto3d, ref_32_vicanerf} have extended IP2P's capabilities to 3D scenes by developing methods to ensure spatial consistency in editing guidance, thereby making significant progress in instruction-guided 3D scene editing despite the limited availability of 3D datasets. Among these approaches, one of the key trends is the iterative dataset update method, where all 2D images used for 3D scene synthesis are edited, followed by re-training the 3D scene. Recently, Instruct 4D-to-4D~\cite{ref_9_i4d24d} extended this iterative dataset update approach to 4D space, presenting the first instruction-guided 4D editing method. By employing flow-based~\cite{ref_53_raft} and depth-based warping to ensure spatio-temporal consistency during dataset updates, they achieved notable results. However, editing all images for 4D dynamic scenes remains extremely time-consuming, highlighting the need for a more efficient approach that can effectively leverage diffusion priors for dynamic scene editing. Therefore, we propose an efficient dynamic scene editing method that significantly reduces total editing time.


\subsection{Score Distillation Sampling}
 The Score Distillation Sampling (SDS) mechanism was introduced in DreamFusion~\cite{ref_18_dreamfusion} for text-to-3D scene generation, enabling the transfer of pre-trained 2D diffusion model priors~\cite{ref_1_ip2p, ref_2_ldm, ref_26_ddpm} to other data domains. When SDS is used with diffusion networks incorporating specific hypotheses—such as multiview diffusion models~\cite{ref_20_mvdream, ref_52_imagedream} or video diffusion models~\cite{ref_21_tuneavideo, ref_54_vdm, ref_55_lumiere, ref_56_makeavideo, ref_57_align, ref_58_sora}—it produces guidance that aligns with those hypotheses. Many studies~\cite{ref_17_dreamgaussian4d, ref_18_dreamfusion, ref_19_aligngs, ref_25_4dfy, ref_59_dreamgs, ref_60_dreamer, ref_61_magic3d} have leveraged this property to develop SDS-based 3D/4D generation methods. Meanwhile, other approaches~\cite{ref_7_tiger, ref_28_i3dto3d, ref_62_prolific, ref_63_dreameditor, ref_64_focal, ref_65_progressive} have explored SDS for editing tasks, adapting it to improve spatial and temporal consistency during the editing process. Furthermore, some works~\cite{ref_34_deltadenoising,ref_35_posterior,ref_36_collaborative} have enhanced editing performance by modifying the score loss function to better suit editing-specific objectives.
\vspace{4mm}