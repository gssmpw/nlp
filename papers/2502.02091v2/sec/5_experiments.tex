%\vspace{4mm}
\section{Experiments}
\subsection{Experimental Setup}
\paragraph{Datasets.}
We use DyNeRF~\cite{ref_37_neural3dvideo} and Technicolor~\cite{ref_71_technicolor}, a real-world multiview video dataset, to train and edit 4D dynamic scenes. The DyNeRF dataset includes six 10-second video sequences captured at 30 fps by 15 to 20 cameras with a face-forward perspective. Technicolor includes a wider variety of motion and scenarios, captured with 16 cameras. For comparison with the baseline~\cite{ref_1_ip2p}, we trim the videos into 50-frame-long segments. We have also included the results on monocular datasets~\cite{ref_73_hypernerf,ref_72_dycheck} in the supplementary.


\vspace{-4mm}\paragraph{Baselines.}
We conduct a qualitative and quantitative comparison with Instruct 4D-to-4D~\cite{ref_9_i4d24d}, the only prior work addressing instruction-guided 4D dynamic scene editing. Instruct 4D-to-4D utilizes NeRFPlayer~\cite{ref_22_nerfplayer} as its backbone 4D representation and employs an iterative dataset update method, which involves editing all 2D images used for synthesizing the dynamic scene. It utilizes optical flow-based warping~\cite{ref_53_raft} and depth-based warping to ensure consistency across all edited 2D images. To alleviate the time-consuming dataset update process, Instruct 4D-to-4D employs two GPUs in parallel: one for the dataset update thread and the other for the dynamic scene editing thread.
\vspace{-5mm}\paragraph{Implementation Details.}
In Sec.~\ref{subsec::4.1}, we follow the experimental settings of~\cite{ref_10_4dgs}. Throughout the experiments utilizing InstructPix2Pix~\cite{ref_1_ip2p}, we set the CFG~\cite{ref_43_classifierfree} scales for image condition and text instruction to 1.2 and 8.5 to 10.5, respectively. In the 3D Gaussian editing stage (Sec.~\ref{subsec::4.2}), we train for 800 to 1000 iterations, depending on the editing style. For the score-based refinement stage (Sec.~\ref{subsec::4.3}), an average of 800 iterations is sufficient to complete the dynamic scene editing successfully. All experiments are conducted using a single NVIDIA A40 GPU.


\begin{figure}[!t]
\centering
    \includegraphics[width=1.0\columnwidth]{fig/figure_4_sds_cr.jpg}
    \vspace{-7mm}
    \caption{\textbf{Effectiveness of score-based temporal refinement}: Score-based temporal refinement effectively resolves misalignment between the canonical 3D Gaussians and the original deformation field that arises during the 3D Gaussian editing process. Without requiring additional 2D image updates, this process completes dynamic scene editing within a few hundred iterations.}
    \label{fig:scoredistillation}
\vspace{-3mm}
\end{figure}


\vspace{-1mm}
\subsection{Results}
\vspace{-1mm}
\paragraph{Quantitative Results.} 
To quantitatively evaluate the visual quality of the edited dynamic scene, we measure PSNR, SSIM~\cite{ref_38_ssim}, and LPIPS~\cite{ref_39_lpips} between the 2D multiview images used as supervision for dynamic scene editing and the images rendered from the edited dynamic scene using the corresponding camera parameters. Additionally, to assess how well the edited dynamic scene aligns with the input instruction, we also measure CLIP~\cite{ref_40_clip} similarity.

Table~\ref{tab:comparison_metrics} presents a quantitative comparison of our method, Instruct-4DGS, and the baseline, Instruct 4D-to-4D, on DyNeRF. While our method shows slightly worse PSNR and SSIM in some cases, this is expected due to our efficient editing strategy. Unlike the baseline, which directly optimizes pixel-level accuracy using all edited images as training targets, our approach optimizes the dynamic scene using only instructions, without additional image editing during the temporal refinement stage. Consequently, pixel-wise accuracy (\ie, PSNR and SSIM) may be worse, but our method demonstrates superior perceptual quality, as shown in the consistently lower LPIPS across all cases. Additionally, our approach excels in instruction-following fidelity, achieving higher CLIP similarity than the baseline. Notably, our method accomplishes this 2--3 times faster while requiring fewer GPUs, making it significantly more efficient for real-world applications.

Table~\ref{tab:computing_time_comparison} compares the efficiency of the baseline and our method. In terms of editing time, our method completes editing 2--3 times faster while using only a single GPU, whereas the baseline requires two identical GPUs. This speed advantage could potentially become more pronounced as the number of timesteps in the dynamic scene increases. These results demonstrate that our method achieves efficiency by leveraging the static-dynamic separability of 4DGS and employing score-based temporal refinement, enabling significantly faster dynamic scene editing without extensive, time-consuming dataset updates.


\begin{table}[!t]
\centering
\footnotesize
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\toprule
Instruction & Method & PSNR$\uparrow$ & SSIM$\uparrow$ & $\text{LPIPS}_{\scriptscriptstyle \text{VGG}}$$\downarrow$ & CLIP sim.$\uparrow$ \\
\hline
\multirow{2}{*}{\textit{Statue}}          & I4D24D  & 18.73 & 0.713 & 0.567 & 0.202 \\ \cline{2-6} 
                                          & Ours    & \textbf{21.41} & \textbf{0.829} & \textbf{0.259} & \textbf{0.220} \\ 
\hline
\multirow{2}{*}{\makecell{\textit{Roman}\\\textit{Sculpture}}} & I4D24D  & \textbf{24.24} & \textbf{0.865} & 0.372 & 0.229 \\ \cline{2-6}
                                          & Ours    & 18.69 & 0.801 & \textbf{0.329} & \textbf{0.252} \\ 
\hline
\multirow{2}{*}{\makecell{\textit{Wood}\\\textit{Sculpture}}}  & I4D24D  & \textbf{18.23} & 0.631 & 0.535 & 0.258 \\ \cline{2-6} 
                                          & Ours    & 17.64 & \textbf{0.718} & \textbf{0.321} & \textbf{0.276} \\ 
\hline
\multirow{2}{*}{\textit{(Average)}}       & I4D24D  & \textbf{20.40} & 0.736 & 0.491 & 0.230 \\ \cline{2-6} 
                                          & Ours    & 19.25 & \textbf{0.783} & \textbf{0.303} & \textbf{0.249} \\ 
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\caption{\textbf{Quantitative comparison of editing quality}: Comparison of performance metrics between Instruct 4D-to-4D (I4D24D) and our Instruct-4DGS (Ours) under various editing instructions on DyNeRF. Higher values indicate better performance for PSNR, SSIM, and CLIP similarity; lower values are better for $\text{LPIPS}_{\text{VGG}}$.}
\label{tab:comparison_metrics}
\end{table}


\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|c|c}
    \toprule
    Method & Computing units & Avg. editing time \\
    \hline
    Instruct 4D-to-4D~\cite{ref_9_i4d24d} & 2 GPUs & 2 hours \\
    \hline
    Instruct-4DGS & \textbf{1 GPU} & \textbf{40 minutes} \\
    \bottomrule
    \end{tabular}
    }
\vspace{-2mm}
\caption{\textbf{Quantitative comparison of editing efficiency}: Our proposed Instruct-4DGS significantly reduces editing time even with fewer GPU resources compared to the baseline.}
\vspace{-3mm}
\label{tab:computing_time_comparison}
\end{table}




\vspace{-5mm}\paragraph{Qualitative Results.} 
Our qualitative results are shown in Fig.~\ref{fig:qual_1}. Our dynamic scene editing method effectively follows various editing styles based on the provided instructions. Leveraging the capabilities of 4DGS~\cite{ref_10_4dgs}, each rendered image exhibits high fidelity, accurately capturing the target details. Moreover, the rendered video output of our edited dynamic scenes maintains smooth motion. 

Qualitative comparison with the baseline is in Fig.~\ref{fig:qual_2} and Fig.~\ref{fig:qual_3}. As shown in the zoomed-in images of Fig.~\ref{fig:qual_2}, our method produces a high-quality edited dynamic scene with less noise and blurry artifacts compared to the baseline. Furthermore, as shown in Fig.~\ref{fig:qual_3}, a comparison of images across multiple timesteps from a fixed camera reveals that the baseline exhibits a noticeable flickering effect. These results indicate that, although the baseline attempts to ensure consistency across all 2D image edits, it falls short of achieving full temporal consistency. In comparison, our method avoids such artifacts by editing the dynamic scene across the temporal dimension through score refinement. It is worth emphasizing that these higher-quality results are achieved 2–3 times faster than the baseline.


\begin{figure}[!t]
\centering
    \includegraphics[width=\linewidth]{fig/figure_5_qual_1_cr.jpg}
    \vspace{-6mm}
    \caption{\textbf{Qualitative results across various editing styles}: Editing results of the scenes \emph{cook\_spinach}, \emph{flame\_steak}, and \emph{coffee\_martini} scenes from DyNeRF. Instruct-4DGS successfully edits dynamic scenes closely following the given user instructions.}
    \vspace{-3mm}
    \label{fig:qual_1}
\end{figure}


\vspace{-5mm}\paragraph{Ablation Studies.} 
We conduct an ablation study to evaluate the impact of each design choice in our method, particularly their contributions to efficiency and quality. The qualitative and user study results are shown in Fig.~\ref{fig:ablation}. We recruited 50 participants of varying demographics, collecting a total of 50 preference rankings on the editing quality of videos generated by the four method variants.


First, we examine dynamic scene editing using only score-based editing, without 3D Gaussian editing (denoted as ``Fully SDS''). As shown in Fig.~\ref{fig:ablation} \red{(a)}, this approach preserves smooth motion but fails to ensure sufficient instruction alignment, leading to low-fidelity results. In contrast, incorporating the 3D Gaussian editing stage significantly improves fidelity while enabling effective motion refinement in the temporal refinement process. This highlights the importance of direct supervision via edited 2D images in maintaining fidelity and quality. 


To mitigate inherent issues of SDS such as \emph{Janus problem}~\cite{ref_20_mvdream}, and to provide stable guidance for the score-based temporal refinement, we employ Coherent-IP2P. To validate this choice, we compare results by refining the \textit{pseudo-edited dynamic scene} with the original IP2P (denoted as ``Refine w/ original IP2P''). As shown in Fig.~\ref{fig:ablation} \red{(b)}, using the original IP2P for temporal refinement leads to severe visual artifacts and low-quality outputs, whereas Coherent-IP2P preserves details and retains the scene’s semantics. This confirms that Coherent-IP2P mitigates noisy guidance and blurry artifacts by enabling information sharing among images within the same batch.


\begin{figure*}[h]
\centering
    \includegraphics[width=1.0\linewidth]{fig/figure_6_qual_2_cr.jpg}
    \vspace{-7mm}
    \caption{\textbf{Qualitative comparison of visual quality}: We compare our method with the baseline~\cite{ref_9_i4d24d} on DyNeRF~\cite{ref_37_neural3dvideo} \emph{coffee\_martini} and \emph{sear\_steak} scenes, as well as Technicolor~\cite{ref_71_technicolor}'s \emph{Painter} and \emph{Train} scenes. See supplementary for more results.}
    \label{fig:qual_2}
\vspace{-4mm}
\end{figure*}


\begin{figure}[!t]
\centering
    \includegraphics[width=\columnwidth]{fig/figure_7_qual_3_cr.jpg}
    \vspace{-7mm}
    \caption{\textbf{Qualitative comparison of temporal consistency}: The baseline shows noticeable flickering artifacts across timesteps. In contrast, Instruct-4DGS effectively avoids such artifacts by editing only the static component with score-based temporal refinement.}
    \label{fig:qual_3}
\vspace{-4mm}
\end{figure}


\begin{figure}[!t]
\centering
    \includegraphics[width=\columnwidth]{fig/figure_8_ablation_cr.jpg}
    \vspace{-6mm}
    \caption{\textbf{Ablation study of the dynamic scene editing method}: Each pie chart shows the proportion of user preferences (1st-4th ranks) for each method variant. Our proposed method (denoted as ``Ours (w/o refine $\{\mathcal{E}, \mathcal{D}\}$)'') achieves the highest preference score.
    }
    \label{fig:ablation}
\end{figure}

Lastly, to evaluate the effectiveness of refining the deformation field, we compare our final method (denoted as ``Ours (w/o refine $\{\mathcal{E}, \mathcal{D}\}$)'')---which refines only the static 3D Gaussians, excluding the deformation field---with ``Ours (w/ refine $\{\mathcal{E}, \mathcal{D}\}$)''. As shown in Fig.~\ref{fig:ablation} \red{(c)}, refining the deformation field introduces temporal inconsistencies and motion artifacts. In contrast, our final method effectively preserves temporal coherence while maintaining high editing fidelity. These results indicate that refining the deformation field does not contribute positively to dynamic scene editing and can instead introduce undesirable distortions.