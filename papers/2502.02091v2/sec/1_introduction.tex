\section{Introduction}
\label{sec:intro}

\begin{figure}[!t]
\centering
    \includegraphics[width=\columnwidth]{fig/figure_1_teaser_cr.jpg}
    \vspace{-6mm}
    \caption{\textbf{Illustration of dynamic scene editing processes for baseline and our method}: (a) The existing method requires updating the 2D images for all timesteps. (b) In contrast, our method updates only the first timestep's dataset images, edits canonical 3D Gaussians, and efficiently completes dynamic scene editing through score-based temporal refinement. For a multi-camera dataset with $T=300$, our method reduces editing time by more than half compared to the baseline, using only a single GPU.}
    \label{fig:teaser}
\vspace{-4mm}
\end{figure}

Diffusion-based generative models~\cite{ref_26_ddpm, ref_2_ldm, ref_66_scalable, ref_67_plug, ref_68_controlnet, ref_69_dreambooth, ref_70_hierarchical} have recently achieved remarkable progress in the 2D image domain and are increasingly being integrated into practical applications. As the demand for generative tasks extends beyond 2D, the editing of 3D and 4D dynamic scenes has emerged as a significant area of research. In particular, user-instruction-guided editing is gaining traction as an intuitive and user-friendly approach.

In this context, InstructPix2Pix (IP2P)~\cite{ref_1_ip2p} has gained recognition by proposing a novel method for editing 2D images based on user instructions. Building on IP2P’s capabilities, research on instruction-guided 3D scene editing, particularly with NeRF~\cite{ref_4_nerf} and 3D Gaussian Splatting (3DGS)~\cite{ref_8_gs}, has become increasingly active. However, 4D dynamic scene editing remains relatively underexplored. One of the few existing methods, Instruct 4D-to-4D~\cite{ref_9_i4d24d} requires iterative dataset updates for \emph{``thousands of 2D images''} used in the dynamic scene synthesis, as shown in Fig.~\ref{fig:teaser} \red{(a)}, along with additional training loops to update the entire dynamic scene, resulting in several hours of processing to edit a single dynamic scene. Regardless of how efficiently the dataset is updated, such an approach fails to scale with the temporal dimension of dynamic scenes, making it impractical for real-world applications.

In this work, we propose \textbf{Instruct-4DGS}, an efficient 4D dynamic scene editing method that is more scalable with respect to the temporal dimension. To maximize computational efficiency, we focus on three key aspects: (1) Since 4D dynamic scenes require frequent rendering during the editing process, we employ 4D Gaussian Splatting (4DGS)~\cite{ref_10_4dgs} as our scene representation, enabling fast and efficient rendering. (2) Our objective is to edit the appearance of the scene while preserving its motion. To achieve this, we leverage the inherent separability of 4DGS into static and dynamic components—specifically, canonical 3D Gaussians (\emph{static}) and a Hexplane~\cite{ref_15_kplanes, ref_16_hexplane}-based deformation field (\emph{dynamic})—allowing us to improve efficiency by editing only the static component. (3) To ensure better alignment between the edited static 3D Gaussians and the original deformation field, we perform temporal refinement using a score distillation mechanism~\cite{ref_18_dreamfusion}.

Specifically, the Hexplane-based 4DGS offers notable advantages in both editing quality and rendering efficiency compared to the 4D NeRF~\cite{ref_22_nerfplayer} used in Instruct 4D-to-4D. By employing 3D Gaussians to represent the static canonical scene, we ensure high-quality, real-time rendering during the editing process. Additionally, Hexplane, which utilizes a spatio-temporal encoding structure based on planar factorization, is highly compact, further contributing to real-time rendering performance.

In addition to rendering efficiency, we aim to achieve computational efficiency in dynamic scene editing by focusing solely on the static component. Since our goal is to edit the scene’s appearance while preserving its motion, we modify only the static 3D Gaussians, which are the minimal yet sufficient elements for appearance editing. As shown in Fig.~\ref{fig:teaser} \red{(b)}, this approach allows us to edit the entire dynamic scene without updating every 2D images, even for scenes with extended timesteps. Specifically, we edit only a subset of 2D multiview images from the initial timestep using IP2P and then apply simple modifications to the static 3D Gaussians using L1 RGB loss.

While editing only the static 3D Gaussians is simple and efficient, it introduces motion artifacts in later timesteps. Specifically, modifying the static 3D Gaussians causes slight shifts in the positions of Gaussian primitives, leading to misalignment between the static canonical scene and the original deformation field. Additionally, only the Spherical Harmonics (SH) colors of 3D Gaussians visible in the first timestep are updated. As a result, when Gaussian primitives rotate through the deformation field in subsequent timesteps, previously unmodified SH values become exposed, introducing visual artifacts. In summary, the dynamic scene tends to overfit to the first timestep, leading to artifacts across other timesteps.

To address this temporal misalignment, we propose a refinement stage that adjusts the edited static 3D Gaussians to better align with the original deformation fields. Specifically, we utilize the score distillation mechanism proposed in DreamFusion~\cite{ref_18_dreamfusion} to transfer IP2P's editing guidance into 3D and even 4D spaces. We apply a score-based refinement stage to eliminate artifacts in the \textit{pseudo-edited dynamic scene}, where the edited static 3D Gaussians are misaligned with the deformation field. Additionally, inspired by MVDream~\cite{ref_20_mvdream} and Tune-a-Video~\cite{ref_21_tuneavideo}, we replace IP2P's self-attention module with a cross-attention module. This modified IP2P, Coherent-IP2P prevents the accumulation of non-uniform editing guidance during score distillation, which would otherwise result in blurry outputs.

Our evaluation demonstrates a significant reduction in editing turnaround time while improving visual quality. Furthermore, our method can effectively perform dynamic scene editing across various user instructions. Our main contributions are summarized as follows:
    \begin{itemize}
        \vspace{1mm}
        \item
        We propose Instruct-4DGS, the first efficient dynamic scene editing framework based on 4D Gaussian Splatting.
        \item
        \vspace{0.5mm}
        We achieve efficient dynamic scene editing by modifying only static 3D Guassians, the minimal but sufficient component for visual editing.
        \item
        \vspace{0.5mm}
        We propose a refinement method using score distillation with Coherent-IP2P, which removes motion artifacts while maintaining computational efficiency.
        \item
        \vspace{0.5mm}
        Our method reduces editing time by more than half while achieving higher visual quality.
        \vspace{0.5mm}
    \end{itemize}
\vspace{4mm}