\vspace{2mm}
\section{Method}
\label{sec::method}
In this section, we present \textbf{Instruct-4DGS}, our proposed method for efficient dynamic scene editing, as illustrated in Fig.~\ref{fig:method}. (Sec.~\ref{subsec::4.1}) We first train dynamic scenes for editing targets by optimizing the 4D Gaussian Splatting (4DGS) ~\cite{ref_10_4dgs}. (Sec.~\ref{subsec::4.2}) Motivated by the static-dynamic separability of Hexplane~\cite{ref_15_kplanes, ref_16_hexplane}-based 4DGS, we initially focus on editing the static canonical 3D Gaussians ~\cite{ref_8_gs} to efficiently edit the dynamic scene. (Sec.~\ref{subsec::4.3}) To mitigate overfitting issues that may arise during the 3D Gaussian editing process and refine the motion artifacts in the dynamic scene, we introduce a temporal refinement stage using score distillation.


\subsection{Optimizing 4D Gaussians for Target Scenes}
\label{subsec::4.1}
Our dynamic scene editing requires a 4D Gaussian representation of the target dynamic scene. To obtain this, we optimize the 4D Gaussians and use it for editing. Specifically, we use dynamic scene datasets~\cite{ref_37_neural3dvideo,ref_71_technicolor} composed of multi-camera captured videos, which can be represented as a set of images $\{I_{M,t}\}$ in which $M$ denotes the camera matrix and $t$ denotes the timestep within the videos. We synthesize images $\hat{I}_{M,t}$ by rendering the randomly initialized dynamic scene $\left\{\mathcal{G}_{\text{canon}}^\text{init},\mathcal{E}^\text{init}(\mathcal{G}_{\text{canon}}, t), \mathcal{D}^\text{init}\right\}$. Then we calculate the RGB L1 loss against the corresponding dataset image $I_{M,t}$, training the dynamic scene through this process. We also apply a grid-based total variational loss~\cite{ref_15_kplanes,ref_16_hexplane,ref_41_directvoxel,ref_42_fastdynamic}, $\mathcal{L}_\text{TV}$ to enforce smoothness in the deformation field output along the timesteps. Note that, as our editing method is highly dependent on the quality of the target dynamic scene, incorporating such regularization loss is helpful. The entire loss function used for 4DGS training is: $\mathcal{L}_\text{4DGS} = |\hat{I}_{M,t} - I_{M,t}| + \mathcal{L}_\text{TV}$.

As a result, we obtain the optimized 4D Gaussians $\left\{\mathcal{G}_{\text{canon}}^\text{opt},\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t), \mathcal{D}^\text{opt}\right\}$, which represent the editing target scene. The static component $\mathcal{G}_{\text{canon}}^\text{opt}$ serves as the main editing target in Sec.~\ref{subsec::4.2}--\ref{subsec::4.3}. In Sec.~\ref{subsec::4.2}, we edit the static component by modifying only the images corresponding to the first timestep, ensuring efficient editing. In Sec.~\ref{subsec::4.3}, we refine the edited static component $\mathcal{G}_{\text{canon}}^\text{edit}$ to better align with the original deformation field $\left\{\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t),\mathcal{D}^\text{opt}\right\}$ using score-based temporal refinement, mitigating potential motion artifacts. A more detailed training setup follows~\cite{ref_10_4dgs}.


\subsection{Stage 1: Efficient Dynamic Scene Editing with Static 3D Gaussians}
\label{subsec::4.2}
In Sec.~\ref{subsec::4.1}, we obtained the optimized canonical 3D Gaussians $\mathcal{G}_{\text{canon}}^\text{opt}$, which models the explicit appearance and geometry of a 3D scene, along with the optimized dynamic components $\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t)$ and $\mathcal{D}^\text{opt}$. For efficient dynamic scene editing, we perform editing only on $\mathcal{G}_{\text{canon}}^\text{opt}$, which is minimal but sufficient information for visual editing of the dynamic scene as shown in Fig.~\ref{fig:method}.


To generate supervision images for editing the $\mathcal{G}_{\text{canon}}^\text{opt}$, we extract a subset of multiview images fixed at the initial timestep and then edit them using InstructPix2Pix~\cite{ref_1_ip2p}. Subsequently, we edit optimized canonical 3D Gaussians $\mathcal{G}_{\text{canon}}^\text{opt}$ with an L1 RGB loss supervising the edited images. Compared to the latest 4D editing method Instruct 4D-to-4D~\cite{ref_9_i4d24d}, which requires editing $T\!\times\!\mathcal{M}$ images---where $T$ is the number of video timesteps and $\mathcal{M}$ is the number of cameras---through iterative dataset updates, our approach significantly reduces the computation required to address the editing of the dynamic scene. Moreover, this approach allows rapid transitions to the edited result, regardless of the number of timesteps $T$ of the dynamic scene. After completing the 3D Gaussian editing process, we obtain a \textbf{\textit{pseudo-edited dynamic scene}} $\left\{\mathcal{G}_{\text{canon}}^\text{edit},\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t), \mathcal{D}^\text{opt}\right\}$, which is obtained by simply recombining the edited canonical 3D Gaussians $\mathcal{G}_{\text{canon}}^\text{edit}$ with the original Gaussian deformation field $\left\{\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t), \mathcal{D}^\text{opt}\right\}$.


To ensure spatial consistency of the 3D Gaussian editing process, we utilize Coherent-IP2P~\cite{ref_9_i4d24d, ref_20_mvdream, ref_21_tuneavideo}, which replaces the 2D convolutional layer (self-attention module) with a 3D convolutional layer (cross-attention module), similar to Instruct 4D-to-4D (by reusing the original parameters of kernels). As shown in Fig.~\ref{fig:ablation}, this encourages collaborative editing among images within the multiview subset, preventing the results from becoming blurry. The entire editing process for the static canonical 3D Gaussian editing can be completed within a few tens of minutes by editing only multiview images of a single timestep and performing a few hundred 3DGS editing iterations.


\subsection{Stage 2: Refinement using Score Distillation for Temporal Alignment}
\label{subsec::4.3}
After the first editing stage proposed in Sec.~\ref{subsec::4.2}, the \textit{pseudo-edited dynamic scene} $\left\{\mathcal{G}_{\text{canon}}^\text{edit},\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t), \mathcal{D}^\text{opt}\right\}$ exhibits severe motion artifacts, as shown in Fig.~\ref{fig:scoredistillation} \red{(a)}. The primary cause is the slight shift in the positions $p$ of Gaussian primitives in $\mathcal{G}_{\text{canon}}^\text{opt}$ during the 3D Gaussian editing process, which results in discrepancies between the queried embedding voxel feature $f_h$ and those of the original dynamic scene. Moreover, only the Spherical Harmonics (SH) colors on the surface visible at the initial timestep are updated. As a result, if the Gaussian primitives in \textit{pseudo-edited} $\mathcal{G}_{\text{canon}}^\text{edit}$ rotate at later timesteps, unedited SH values that were previously hidden may become exposed, leading to artifacts. Therefore, we introduce a temporal refinement stage to resolve the misalignment between the original deformation field $\left\{\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t), \mathcal{D}^\text{opt}\right\}$ and the edited canonical 3D Gaussians $\mathcal{G}_{\text{canon}}^\text{edit}$.

To perform the refinement stage efficiently without editing additional dataset images, we employ the score distillation mechanism ~\cite{ref_18_dreamfusion}. Since the dynamic scene is edited using multiple 2D images generated by IP2P, the prior of the 2D diffusion model (\ie, IP2P) can be distilled into the 4D dynamic scene. The editing process can be continued using the noise prediction loss (\ie score) obtained from each IP2P inference as Eqs.~\ref{eq::eq1} and~\ref{eq::eq2}. Since we just use score distillation for editing refinement rather than generation or editing from scratch, this stage can be completed with a smaller number of iterations. Consequently, our approach is relatively less affected by inherent issues of Score Distillation Sampling (SDS), such as \emph{Janus problem}~\cite{ref_20_mvdream}. 


Similar to Sec.~\ref{subsec::4.2}, we apply Coherent-IP2P with the diffusion prior $\theta$ and observe that it reduces blurring effects and enhances qualitative performance compared to the original IP2P. At each refinement iteration, we rendered $B$ images of the \textit{pseudo-edited dynamic scene} $\tilde{I} = \left\{ \hat{I}_i = S(M_i, \mathcal{G}_{\text{def},t_i}^\text{edit}) \right\}_{i=1}^B$ using random camera matrices $\left\{ M_i \right\}_{i=1}^B$ and random timesteps $\left\{ t_i \right\}_{i=1}^B$ as input for Coherent-IP2P, where $S$ denotes the rendering process of the 3DGS (subscripts $M$ and $t$ on $\hat{I}$ omitted for simplicity). We optimize $\mathcal{G}_{\text{canon}}^\text{edit}$ using the following SDS loss to obtain the refined 3D Gaussians $\mathcal{G}_{\text{canon}}^\text{ref}$:
\vspace{-2mm}
\begin{equation}
    \scriptsize
    \nabla_{\mathcal{G}_{\text{canon}}^{\text{edit}}} \mathcal{L}_{\text{SDS}} = \mathbb{E}_{t, \tilde{t}, \epsilon, \mathcal{M}} \left[ \left( \epsilon_{\theta} \left( \tilde{I}, c_I, c_T; t, \tilde{t}, \mathcal{M} \right) - \epsilon \right) \frac{\partial \tilde{I}}{\partial \mathcal{G}_{\text{canon}}^{\text{edit}}} \right],
\label{eq::eq1}
\end{equation}

\vspace{-4mm}
\begin{equation}
    \footnotesize
    \begin{split}
        \epsilon_{\theta}(\tilde{I}, c_I, c_T) = \epsilon_{\theta}(\tilde{I}, \emptyset, \emptyset) + s_I \left( \epsilon_{\theta}(\tilde{I}, c_I, \emptyset) - \epsilon_{\theta}(\tilde{I}, \emptyset, \emptyset) \right) \quad \\ + s_T \left( \epsilon_{\theta}(\tilde{I}, c_I, c_T) - \epsilon_{\theta}(\tilde{I}, c_I, \emptyset) \right)
    \end{split}
\label{eq::eq2}
\end{equation}
, where $\tilde{t}$ is diffusion timestep, $\epsilon$ is diffusion noise, $c_I = \left\{ I_i \right\}_{i=1}^B$ is original dataset images, $c_T$ is user instruction, $s_I$ and $s_T$ are Classifier-Free-Guidance~\cite{ref_43_classifierfree} scale for $c_I$ and $c_T$, $\epsilon_{\theta}(\tilde{I}, c_I, c_T)$ is Coherent-IP2P denoiser networks including VAE~\cite{ref_44_vae}. This score-based guidance encourages a set of rendered 2D images from the \textit{pseudo-edited} 4D Gaussians at arbitrary timesteps $\tilde{I}$ to resemble the edited images that IP2P would generate based on the $c_I$ and $c_T$, thereby effectively refining motion artifacts. As a result, we obtain refined canonical 3D Gaussians $\mathcal{G}_{\text{canon}}^\text{ref}$ that aligns well with the original deformation field $\left\{\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t), \mathcal{D}^\text{opt}\right\}$ while maintaining the edited appearance. After the refinement stage, we obtain a completely edited 4D dynamic scene which is represented as $\left\{\mathcal{G}_{\text{canon}}^\text{ref},\mathcal{E}^\text{opt}(\mathcal{G}_{\text{canon}}, t), \mathcal{D}^\text{opt}\right\}$.