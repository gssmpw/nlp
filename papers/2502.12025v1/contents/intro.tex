\section{Introduction}

With the rapid evolution of Large Language Models (LLMs), significant efforts have been made to improve model capabilities, particularly in complex reasoning tasks such as mathematics and coding. Emerging large reasoning models (LRMs), such as OpenAI's o1 \cite{jaech2024openai} and DeepSeek-R1 series models, \cite{guo2025deepseek} are trained to learn reasoning by "thinking" through long chain-of-thought. 
LRMs follow a structured thought process, generating long reasoning traces with multiple intermediate steps when answering complex questions even without advanced prompting strategies.
These models have demonstrated remarkable performance on complex tasks and are increasingly integrated into daily tasks such as assisting coding development and scientific discovery \citep{chan2024mle, chen2025scienceagentbench}. 

As LRM gain broader attention, evaluating their safety is crucial as long reasoning traces do not inherently guarantee safe responses \cite{qi2024safety}.
Unsafe responses from reasoning models raise ethical concerns and lead to severe consequences, such as creating bugs and vulnerabilities in codebases and spreading misinformation that biases students' understandings.
At present, however, safety of LRMs remains less studied.

\begin{figure*}[!]
    \centering
    \includegraphics[width=\linewidth]{assets/safechain.pdf}
    \caption{
    \textbf{Left:} The structured thought process by LRM when answering an example instruction from StrongReject \citep{souly2024strongreject}. The safety-aware and harmful contents are marked in \textcolor{blue}{blue} and \textcolor{red}{red}, respectively. 
    \textbf{Middle:} We apply three prompting setups with varying CoT length, i.e., ZeroThink, LessThink and MoreThink (see Section \ref{sec:different-think}). Our results show that ZeroThink yields the best safety performance. \textbf{Right:} Our pipeline to synthesize safety alignment dataset, \textsc{SafeChain}, for LRMs (see Section \ref{sec:safechain-data}).
    Models fine-tuned with \textsc{SafeChain} exhibit improved safety performance while preserve reasoning capabilities across six math and coding benchmarks.
    }
    \label{fig:overview}
    \vspace{-1em}
\end{figure*}

Compared with regular LLMs, LRMs differ in two key ways: (1) their responses intrinsically include chains of thought alongside a final answer, (2) their outputs tend to be significantly longer. While an LRM’s final answer may appear safe—e.g., by refusing to comply with a harmful prompt—its intermediate reasoning can still contain harmful or policy-violating content, as the eample shown in Figure \ref{fig:overview}. This makes it crucial to adopt new safety evaluation methods that inspect both the chain of thought (CoT) and the final answer. Moreover, the sheer length of LRMs’ outputs makes manual evaluation prohibitively expensive at scale, yet the effectiveness of existing automated evaluators on long reasoning traces is largely unknown. Finally, developing approaches that improve LRMs’ safety without degrading their strong performance on complex reasoning tasks is an equally urgent goal.

In this paper, we address the aforementioned challenges and present comprehensive safety evaluations for reasoning models. 
We firstly conducted a pilot study on investigating the performance of safety evaluators calibrated with human annotators. Our study on various types of evaluators, including Llama-Guard \citep{inan2023llama}, Refusal String Matching \citep{zou2023universal}, OpenAI Moderation API \citep{openai2024moduration}, and fine-tuned LLM Judge from HarmBench\citep{pmlr-v235-mazeika24a}, show that Llama-Guard consistently outperforms other evaluators and exhibit robust performance. Following on this, we develop three metrics to evaluate the safety of LRMs by examing the reasoning thoughts and final answer jointly. With these metrics, we systematically evaluate the safety of state-of-the-art reasoning models including DeepSeek-R1 series \citep{guo2025deepseek}, Skywork-o1 \citep{skyworkopeno12024}, QwQ \citep{qwq-32b-preview}, Sky-T1 \citep{reduce_overthinking_2025} , Gemini-Thinking \cite{deepmind_gemini} and Kimi-k1.5 \citep{team2025kimi} across datasets including StrongReject \citep{souly2024strongreject} and WildJailbreak \citep{jiang2024wildteaming}. In addition, we dive into the fine-grained analysis of thought and answer for LRMs, and a thorough study with various thinking setups, namely ZeroThink, LessThink and MoreThink. Moreover, we construct a new dataset, named \textsc{SafeChain}, to enhance safety of models with long CoT capabilities. To the best of knowledge, we are the first to construct safety training dataset in long CoT style.
