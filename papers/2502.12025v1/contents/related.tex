\section{Related Work}
\paragraph{Chain-of-Thought and Reasoning Models.}
Despite LLMs have shown strong performance on a broad range of tasks, there remains a notable gap between LLMs and humans when it comes to more complex reasoning tasks, such as math and coding. \citet{wei2022chain} introduce Chain-of-Thought (CoT) prompting to enhance the reasoning capabilities of LLMs, prompting a surge of new prompting techniques \citep{NEURIPS2022_8bb0d291, zhou2023leasttomost}. Concurrently, studies have explored strategies to improve reasoning without explicit prompts, including process reward models \citep{lightman2023let}, advanced search algorithms \citep{feng2023alphazero, yao2024tree}, and reinforcement learning \citep{kumar2025training, guo2025deepseek}. OpenAIâ€™s recent o1 model \citep{jaech2024openai} has set a remarkable milestone by scaling test-time reasoning through extended CoT outputs.

\paragraph{LLM Safety.}
Ensuring that LLMs are both helpful and harmless is critical for developing trustworthy AI systems. To this end, safety alignment is commonly introduced during the post-training phase using supervised fine-tuning and/or reinforcement learning \citep{bai2022training, bai2022constitutional, NEURIPS2022_b1efde53, touvron2023llama, guan2024deliberative}. However, red-teaming evaluations reveal that these aligned models are often unsafe in the wild \citep{wei2023jailbroken, zou2023universal, liu2024autodan, jiang-etal-2024-artprompt, pmlr-v235-mazeika24a}. Even models endowed with advanced reasoning capabilities can become unsafe under certain conditions \citep{xiang2024badchain, jaech2024openai}. In response, researchers have proposed additional test-time safeguards to strengthen model safety \citep{inan2023llama, xu-etal-2024-safedecoding}. Our work is the first to systematically study the safety of reasoning models.