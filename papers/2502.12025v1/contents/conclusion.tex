\section{Conclusion}
In this paper, we evaluated the safety of large reasoning models (LRMs).
Our comprehensive evaluations on StrongReject and WildeJailbreak datasets showed that long chain-of-thought adopted by SOTA LRMs does not necessarily enhance model safety.
Based on these findings, we introduced \textsc{SafeChain}, a dataset to fine-tune LRMs while preserving reasoning capabilities.
We showed that SafeChain outperformed existing datasets on DeepSeek-R1-1.5B and DeepSeek-R1-7B.
Future work will explore extending \textsc{SafeChain} to multilingual settings and further refining safety evaluation methods for long CoT reasoning.

% https://aclrollingreview.org/cfp
\section*{Limitations}
In this work, we primarily focus on the safety of emerging LRMs. 
Our evaluations are conducted using policy-violating inputs in English, without considering multilingual inputs. Also we only focus on single-turn interaction with LRMs, the safety evaluation on multi-turn interaction with LRMs is yet an open-problem.

\section*{Ethical Statement}

In this work, we primarily focus on evaluating the safety of emerging LRMs. 
Our evaluations assess the safety of LRMs in a controlled setting with publicly available dataset.
This ensures that no new harmful data is created and can be misused, e.g., identify personal information.
Moreover, this paper introduces a safety alignment dataset named SafeChain, which is shown to be effective to develop safer LRMs without introducing ethical concerns.
