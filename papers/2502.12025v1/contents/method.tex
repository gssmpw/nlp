\section{Preliminary: LRMs with Long CoT}
We denote the sequence of tokens representing an instruction as $x$.
The token sequence representing a response generated by an auto-regressive model is denoted as $y$.
For LRMs, the response $y=y_{CoT}\oplus y_{ans}$ comprises two parts: the reasoning trace $y_{CoT}\subset y$ constitutes the CoT and the final answer $y_{ans}\subset y$.
Here $\oplus$ represents concatenation. 
The reasoning trace allows models to branch out and explore other paths to generate final answer, or revert to a previous checkpoint to correct errors. 
An illustrative example is shown in Figure \ref{fig:overview}.
Depending on the developer, the reasoning trace $y_{CoT}$ is not necessarily visible to users.
For example, OpenAI's o-series models do not reveal the reasoning thoughts whereas DeepSeek-R1 displays the intermediate steps as part of responses.

