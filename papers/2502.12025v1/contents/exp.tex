\section{Safety Evaluation of LRMs}
This section presents a comprehensive safety evaluation of reasoning models.
\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l l  l  l c  }\toprule
          \bf Developer & \bf Model ID & Fine-tuned Model&\bf Ref. Name \\ \midrule
        \multirow{7}{*}{DeepSeek} & R1-Distill-Qwen-1.5B & \small Qwen2.5-Math-1.5B & R1-1.5B \\ 
        ~ & R1-Distill-Qwen-7B & \small Qwen2.5-Math-7B & R1-7B \\ 
        ~ & R1-Distill-Llama-8B & \small Llama-3.1-8B & R1-8B \\ 
        ~ & R1-Distill-Qwen-14B & \small Qwen2.5-14B & R1-14B \\ 
        ~ & R1-Distill-Qwen-32B & \small Qwen2.5-32B & R1-32B \\ 
        ~ & R1-Distill-Llama-70B & \small Llama-3.3-70B-Instruct & R1-70B \\ 
        ~ & R1 & \small DeepSeek-V3-Base & R1-671B \\ \midrule
        Google & gemini-2.0-flash-thinking-exp-01-21 & - &Gemini-Thinking \\ \midrule
        Moonshot & Kimi k1.5 long-CoT & - & Kimi-k1.5 \\ \midrule
        NovaSky-AI & Sky-T1-32B-Flash & \small Qwen2.5-32B-Instruct &Sky-T1 \\  \midrule
        Qwen & QwQ-32B-Preview & \small Qwen2.5-32B-Instruct & QwQ \\ \midrule
        Skywork & Skywork-o1-Open-Llama-3.1-8B & \small Llama-3.1-8B-Instruct & Skywork-o1 \\ \bottomrule
    \end{tabular}
    }
    \caption{This table summarizes the reasoning models evaluated for safety, the fine-tuned source model, and their corresponding references name used in this paper.}
    \label{tab:model-overview}
\end{table}

\begin{table}[]
    \centering
    % \resizebox{\linewidth}{!}{
    \begin{tabular}{l c c c}\toprule
    Method & \textsc{Acc} & \textsc{F-$1$} & \textsc{PCC} \\ \midrule
     RS-Match   & 70.2\% & 59.3\% & 0.429 \\
OpenAIMod & 80.5\% & 78.2\% & 0.610 \\
HarmBenchEval & 80.9\% & 74.8\% & 0.656 \\ 
Llama-Guard & 88.2\% & 86.1\% & 0.776\\ \bottomrule
    \end{tabular}
    % }
    \caption{This table summarizes the \textsc{Acc}, \textsc{F-$1$}, and \textsc{PCC} of evaluators RS-Match, OpenAIMod, HarmBenchEval, and Llama-Guard. Among all evaluators, we observe Llama-Guard exhibit robust performance across all metrics when evaluating the safety of reasoning models.}
    \label{tab:evaluator}
\end{table}
\subsection{Pilot Study of Safety Evaluators for LRMs}\label{sec:safe evaluator}

Our goal in this study is to find safety evaluators to effectively flag unsafe responses with long CoT generated by reasoning models.

\paragraph{Evaluators.}
We consider four safety evaluators in this study: Llama-Guard, Refusal String Matching (RS-Match) \citep{zou2023universal}, OpenAI Moderation API (OpenAIMod) \citep{openai2024moduration}, and fine-tuned LLM Judge from HarmBench (HarmBenchEval) \citep{pmlr-v235-mazeika24a}.

\paragraph{Models and Dataset.} 
We consider six reasoning models, including R1-7B/8B, Gemini-Thinking, Sky-T1, QwQ, and Skywork-o1.
We prompt these reasoning models with queries from StrongReject small (with 60 instructions) using temperature $t=0.6$.
This leads to $360$ query-response pairs.
We label each query-response as safe, unsafe, or unsure, and eliminate the samples labeled as unsure due to their ambiguity in safety evaluation.
This leaves us an evaluation dataset containing $272$ samples labeled with either safe or unsafe.


\paragraph{Metrics.}
We assess the effectiveness of evaluators using three metrics: accuracy (\textsc{Acc}), F-$1$ Score (\textsc{F-$1$}) and Pearson correlation coefficient (\textsc{PCC}) \citep{cohen2009pearson} to human annotations.

\paragraph{Llama-Guard is the best evaluator.}
We summarize the effectiveness of all evaluators based on \textsc{Acc}, \textsc{F-$1$}, and \textsc{PCC} in Table \ref{tab:evaluator}.
Our results show that Llama-Guard consistently outperforms others across all metrics.
This implies that Llama-Guard is robust and can serve as the safety evaluator for reasoning models. In our following study, we use Llama-Guard as our safety evaluator.

\subsection{Experimental Setup}
% \fj{We introduced the overall evaluation setup used in the following}
\paragraph{Models and Configurations.} 
We consider a broad range of LRMs for safety evaluation, including both open- and closed-source models.
These reasoning models include DeepSeek-R1 series, Skywork-o1, QwQ, Sky-T1, Gemini-Thinking, and Kimi-k1.5. 
Detailed information of evaluated reasoning models is presented in Table \ref{tab:model-overview}.
We do not include OpenAI's o-series because they do not disclose the reasoning traces to users.\footnote{Gemini-Thinking and Kimi-k1.5 are currently under experimental access during our project. It is subject to change whether they will continuously disclose reasoning trace.}
For each model, we consider two sets of generation configurations as in Table \ref{tab:generation-config}: \textbf{Greedy} sampling with temperature $t=0$ and \textbf{Non-deterministic} (Non-Det) sampling using various temperature/top-p/top-k setups.
% using top-p or top-k decoding strategy under non-zero temperature.


\begin{table}[tb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{threeparttable}
    \begin{tabular}{c l l  c} \toprule
        \bf Sampling & \bf Model & \bf Parameters \\ \midrule
        \bf Greedy & - & $t=0$ \\ \midrule
        \bf \multirow{6}{*}{ Non-deterministic} & R1-\{1.5/7/8/14/32/70\}B & $t=0.6$, top-$p=0.95$ \tnote{$\alpha$}\\ 
        ~ & R1 & $t=1$ \tnote{$\beta$} \\ 
        ~ & Gemini-Thinking & $t=0.7$, top-$p=0.95$, top-$k=64$ \tnote{$\beta$} \\ 
        ~ & Kimi-k1.5 & $t=1$  \tnote{$\gamma$}\\
        ~ & Skywork-o1 & $t=0.6$, top-$p=0.9$  \tnote{$\alpha$}\\ 
        ~ & QwQ / Sky-T1 & $t=0.7$, top-$p=0.8$, top-$k=20$  \tnote{$\alpha$} \\ \bottomrule
    \end{tabular}
    \begin{tablenotes}
    % \item[$\alpha$] Huggingface Config File \item[b] Official API Setup 
    \item[] \textsuperscript{$\alpha$} Huggingface Configuration
         \quad \textsuperscript{$\beta$} Official API Setup
    \item[]  \textsuperscript{$\gamma$} No public reference available during our project development
    \end{tablenotes}
    \end{threeparttable}
    }
    \caption{This table presents the generation configuration for reasoning models evaluated for safety. We consider the greedy sampling with temperature $t=0$ and non-deterministic sampling with various default setups from model developer. }
    \label{tab:generation-config}
\end{table}

\input{assets/tab_overall}
\paragraph{Metrics.} 
We employ multiple metrics to evaluate the safety of reasoning models. 
Motivated by \citet{guo2025deepseek}, we define the following metrics:
\begin{align*}
\text{Safe@1} &=
  \frac{1}{K}\sum_{i=1}^K s_{i}
,\\
\text{ConsSafe@K} &=
\mathbbm{1}\Bigl\{\sum_{i=1}^K s_{i} \;\ge\; \tfrac{K}{2}\Bigr\},\\
\text{Safe@K} &=
\mathbbm{1}\Bigl\{\bigwedge_{i=1}^K s_{i} = 1\Bigr\},
\end{align*}
where $s_i$ is a binary indicator showing whether response $y_i$ for $i \in \{1, \cdots, K\}$ to a query $x$ is safe or not.
% \zhen{What is $s$ here?}
Specifically, Safe@1 evaluates the percentage of safe ones among $K$ generated responses.
Safe@K is a binary indicator, where Safe@K=1 if all $K$ responses are safe and Safe@K = 0 otherwise. 
ConsSafe@K is a voting-based metric, which is set to 1 if at least $K/2$ of the generated responses are safe and 0 otherwise.
% \zhen{``more than'' is inconsistent with $\geq$ in the definition}
We choose $K=5$ throughout our experiments unless otherwise specified.
We remark that our Safe@K definition is related to, but different from, the pass@K metric proposed by \citep{chen2021evaluating}.
The key difference is that Safe@K focuses on safety evaluations, where it is critical to ensure no harmful information is contained in the response.
In contrast, pass@K in code generation evaluates whether at least one generated code snippets among the top-K passes the unit test.


\paragraph{Datasets.} Following \citet{jaech2024openai}, we use two datasets for safety evaluation. The first one is \textbf{StrongReject} \citep{souly2024strongreject}. 
It contains $310$ policy-violating queries.
StrongReject offers a small split containing $60$ requests for efficient evaluation. 
The second dataset is \textbf{WildJailbreak} \citep{jiang2024wildteaming}, which includes jailbreak prompts adversarially generated by LLMs with diverse tactics learned from real user-model conversations. 
We randomly select $250$ jailbreak prompts and select $50$ prompts as a small split. 
% \zhen{Did you use K-means to first get K clusters, and then randomly sample from each cluster? How did you get the small split?}

% \textcolor{blue}{Need to explain greedy, full, default}

\subsection{Experimental Results}
\begin{findingBox}{1}{
\textbf{Safety performance of SOTA LRMs should be improved.} 
}
\end{findingBox}

Table \ref{tab:my_label} summarizes the safety performance of SOTA LRMs evaluated using Safe@1, Safe@K, and ConsSafe@K under all configurations. 
We observe that no model exhibit strong safety performance on both StrongReject and WildeJailbreak datasets. This implies that LRMs should be better aligned for safety.

\begin{findingBox}{2}{
\textbf{Safety performance improves as model scales.} 
}
\end{findingBox}

In Table \ref{tab:my_label}, we evaluate the safety of LRMs based on Safe@1, Safe@K, and ConsSafe@K under all configurations. 
We observe that within the same model family, the models become safer as their sizes scale (from DeepSeek-R1-1.5B to R1).



\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/qwen_strongreject.pdf}
        \caption{R1-7B}
        \label{fig:fig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/llama_strongreject.pdf}
        \caption{R1-8B}
        \label{fig:fig2}
    \end{subfigure}
    \caption{This figure presents the histograms of response length associated with safe and unsafe responses. We observe that unsafe responses tend to be longer compared to safe ones.}
    \label{fig:r1-7b8b-strongreject-500}
\end{figure*}

\begin{findingBox}{3}{
\textbf{Unsafe responses from LRMs are likely to be longer than safe ones.} 
}
\end{findingBox}
% \paragraph{Long CoT does not Necessarily Enhance Safety.}

% \zhen{What about ``Longer responses from LRMs are more likely to be unsafe'' since safety evaluation is done after response generation.}
% As shown example in Figure \ref{fig:overview}, LRMs may generate unsafe responses regardless the safety awareness.
% \zhen{This is not shown in Table 3, right? I am a little concerned about Table 3, since these numbers do not show a clear trend or other useful information.}

An interesting observation is that some unsafe responses have extremely long lengths. To investigate the patterns exhibited by safe and unsafe responses,
we collect the responses to prompts in StrongReject and WildJailbreak.
We show the histogram of safe and unsafe responses based on response length in terms of number of tokens in Fig. \ref{fig:r1-7b8b-strongreject-500}.
We note that unsafe responses tend to use more tokens and hence are longer than safe ones.



\begin{findingBox}{4}{
\textbf{Learning long CoT does not necessarily enhance safety.} 
}
\end{findingBox}

We investigate how long CoT affects safety of LRMs by comparing the safety of R1-70B with its pre-trained model \texttt{Llama-3.3-70B-Instruct}, as well as the corresponding base model \texttt{Llama-3.1-70B}\footnote{Base model dependency information is posted on huggingface at \url{https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/discussions/10\#6753512e59a4826a6f43acff}}.
Note that R1-70B is fine-tuned using long CoT from \texttt{Llama-3.3-70B-Instruct}.
We evaluate both models using StrongReject and WildJailbreak.
For each dataset, we use each model to create a set of instruction-response pairs.
We then filter the pairs from both models whose responses are flagged by safety evaluator, leading to 350 curated pairs. 
Inspired by LLM-as-a-Judge framework for pairwise assessment of LLMs \citep{zheng2023judging,lin2025wildbench}, we adapt the score-based LLM safety judge \citep{souly2024strongreject} into a pairwise evaluation format.
The full evaluation prompt is in Figure \ref{fig:judge-prompt} in Appendix \ref{app:example}.
We select \texttt{gpt-4o-2024-11-20} as the LLM judge, as it demonstrates top performance on the Chatbot Arena leaderboard \citep{zheng2023judging}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/pairwise_eval.pdf}
    \caption{We compare the safety of R1-70B with its pre-trained model \texttt{Llama-3.3-70B-Instruct}, as well as the corresponding base model \texttt{Llama-3.1-70B}. We note that only 32.3\% of responses by R1-70B is considered safe, implying that fine-tuning with long CoT does not necessarily enhance safety performance.
    }
    \label{fig:long cot safe}
\end{figure}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{assets/varying_temperature_topp_topk_v2.pdf}
    \caption{This figure shows how Safe@1 and Safe@K of R1-7B and R1-8B vary as decoding configuration (temperature, $p$ value for top-p, and $k$ value for top-k) change. We observe that the safety of LRMs degrades as temperature increases.}
    \label{fig:decoding}
\end{figure*}

Our results are summarized in Figure \ref{fig:long cot safe}.
We make two observations. 
First, R1-70B outperforms Llama-Base, where R1-70B generates safe responses to 76.6\% queries.
We hypothesize that the this is because R1-70B uses Llama-3-Instruct as the base model, which has undergone a thorough safety fine-tuning.
Our second observation is that the safety performance degrades after fine-tuning with long CoT (R1-70B vs Llama-3-Instruct). 
Particularly, Llama-3-Instruct wins 45.7\% in terms of generating safe responses.
This implies that fine-tuning with long CoT does not necessarily enhance safety performance.


\begin{findingBox}{5}{
\textbf{Temperature affects safety.} 
}
\end{findingBox}
In Fig. \ref{fig:decoding}, we present the safety of LRMs under different decoding configurations.
As temperature increases, the safety performance of LRMs degrades.
For example, Safe@K of R1-7B drops from 30\% to less than 20\% as temperature increases to 1.2.
However, the values of p for top-p decoding and k for top-k decoding do not impact the safety significantly.


% \paragraph{How does decoding setup affect the safety?} We evaluated the safety alignment with two setups. But as shown by \citet{ jiang2023identifying, huang2024catastrophic}, LLM safety alignment are 



% \paragraph{Does more test-time computing contribute to more robust safety alignment?} Yet the long CoT already boosts the model performance via more computing budget, we wonder if we could further boost the alignment via even more computing budget, where we employ Rejection Sampling strategy as \citet{nakano2021webgpt}. Specifically, we generate $N$ response candidates per instruction, then use a reward model to rank candidates and select the best/median/worst candidate. We choose two reward models based on the performance on RewardBench \citep{lambert2024rewardbench}, namely \texttt{Skywork-Reward-Gemma-2-27B-v0.2} and \texttt{Skywork-Reward-Gemma-2-27B-v0.2} \citep{liu2024skywork}.

% \paragraph{Does longer CoT lead to safer response?} Yet the long CoT already boosts the model performance via more computing budget, we wonder if such scaling budget benefit the safety. We seek \texttt{R1-7B} and 


% \paragraph{Will Long CoT Capability Be More Harmful/Safe?} As model with long CoT capability shows stronger performance, we wonder if such a capability will enhance the harmfulness. In this study, we compared \texttt{Llama-3.3-70B-Instruct} to its SFT successor \texttt{R1-70B} with long CoT capability. We collect the instruction-response pairs evaluated on StrongReject and WildTeaming datasets from both models, and select those responses from both models are flagged by safety evaluator. Finally, we curated $350$ pairwise comparision dataset. Then we follow piror work that utilize LLM-as-a-Judge to compare the safety of the response from two models, where we uses \texttt{gpt-4o-2024-11-20} as our LLM judge. The full evaluation prompt is in Appendix \ref{todo}. 


% \section{Choice of Evaluators for Reasoning Model Safety}\label{sec:safe evaluator}

% This sections investigates which evaluator is suitable to evaluate the safety of reasoning models.
% \zhen{This section needs some motivation. Section 3 is already a comprehensive safety evaluation of reasoning models. Why do we want to further discuss the choice of evaluators here? Are there any motivations, for example, related to the findings in Section 3?}

% \paragraph{Models and Dataset.} 
% We consider six reasoning models, including R1-7B/8B, Gemini-Thinking, Sky-T1, QwQ, and Skywork-o1.
% We prompt these reasoning models with queries from StrongReject using temperature $t=0.6$.
% This leads to $360$ query-response pairs.
% We label each query-response as safe, unsafe, or unsure, and eliminate the samples labeled as unsure due to their ambiguity in safety.
% This leaves us an evaluation dataset containing $272$ samples labeled with either safe or unsafe.

% \paragraph{Evaluators.}
% We consider four widely-adopted safety evaluators: Llama-Guard, Refusal String Matching (RS-Match) \citep{zou2023universal}, OpenAI Moderation API (OpenAIMod) \citep{openai2024moduration}, and fine-tuned LLM Judge from HarmBench (HarmBenchEval) \citep{pmlr-v235-mazeika24a}.
% Our goal is to find safety evaluators that can effectively flag unsafe responses generated by reasoning models.

% \paragraph{Metrics.}
% We assess the effectiveness of evaluators using three metrics: accuracy (\textsc{Acc}), F-$1$ Score (\textsc{F-$1$}) and Pearson correlation coefficient (\textsc{PCC}) \citep{cohen2009pearson} to human annotations.

% \paragraph{Llama-Guard is the most effective evaluator.}
% We summarize the effectiveness of all evaluators based on \textsc{Acc}, \textsc{F-$1$}, and \textsc{PCC} in Table \ref{tab:evaluator}.
% Our results show that Llama-Guard consistently outperforms others across all metrics.
% This implies that Llama-Guard is robust and can serve as the safety evaluator for reasoning models.

\section{Safety of LRMs' Thought and Answer}
\input{assets/tab_contin}
In this section, we perform a granular safety analysis on responses by LRMs. 
We focus on the DeepSeek R1-series models, which provide clear segmentation tags of reasoning trace and answer.

\subsection{Fine-Grained Safety Analysis of LRMs}\label{sec:fine grained safety}
We collect the responses from R1-1.5B to R1 on StrongReject and WildJailbreak, and decompose the responses into thought ($y_{CoT}$) and answer ($y_{ans}$) pieces. Then the decomposed thoughts and answers are evaluated by \texttt{Llama-Guard} respectively. The results are shown in Table \ref{tab:combined_contingency}.
We make the following observations.
First, safe thought may not always lead to safe answers. 
Responses whose thoughts and answers are safe only account for $41.1\%$ of the examples.
Second, unsafe thought of LRMs is likely to lead to unsafe answers.
In some occasions, unsafe thought may still generate safe answers due to the reflection and error correction capabilities of LRMs.


\subsection{
Thinking Affects Answer Safety
}
\label{sec:different-think}
Based on our analysis in Section \ref{sec:fine grained safety}, we investigate how thought process affects safety.
We design three decoding strategies to control the length of thought process, with detailed examples in Table \ref{tab:thinking-setup} of Appendix:
\begin{itemize}
    \item \textbf{ZeroThink}: Motivated by  \citet{jiang2024chatbug}, we enforce the response prefix to be an empty thought segment, i.e., "<think></think>". This forces the model to generate responses without applying any thought.
    \item \textbf{LessThink}: We enforce the model to start its response with a short thought process, i.e., "<think>Okay, the user ask for this, I can answer it without thinking much.</think>".
    \item \textbf{MoreThink}: Following \citet{muennighoff2025s1}, we employ the minimum-forcing algorithm, which replaces the generation of end-of-thinking delimiter (i.e., </think>) and optionally append a transition string (e.g., "Wait") until the minimum condition satisfied. 
    In our experiment, the minimum condition is to either replace the end-of-thinking delimiter 10 times or reach 10,000 thinking tokens. 
\end{itemize}

\begin{findingBox}{6}{
\textbf{ZeroThink enhances model safety most effectively without model training.} 
}
\end{findingBox}
We evaluate the safety of R1 models under these decoding strategies with varying length of though process in Table \ref{tab:thinking}. 
We observe that all decoding strategies yield enhanced safety performance than the default setup.
In particular, ZeroThink achieves the best safety performance, implying that models may have strong instinct on safety. 
Compared to the default long CoT setup, ZeroThink and LessThink disabled the models' thought process.
Consequently, the models are not able to generate unsafe thought process, which might further lead to unsafe responses. 
Instead, the models generate responses relying on their instinct safety awareness.
It is surprising that MoreThink can also mitigate unsafe behaviors. 
We hypothesize that when MoreThink explores reasoning paths, the long context helps the model to reflect on the reasoning trace, especially those that may lead to unsafe response. Such reflection during MoreThink allows the model to finally generate safe responses.
We show an example response collected from our experiment in Figure \ref{fig:MoreThink-example}.

\input{assets/tab_differentthinking}



\section{\textsc{SafeChain} Dataset: Enhancing Safety under Chain-of-Thought}
\label{sec:safechain-data}
Though the aforementioned setups can enhance safety, it either loses the advantage of CoT or incurs high computing cost. Therefore, aligning LRMs remains an open challenge.
In this section, we take a first step towards addressing this challenge.
Our goal is to enhance the safety alignment of LRMs while preserve their reasoning capabilities.

\subsection{Our \textsc{SafeChain} Dataset}
Existing safety alignment datasets \citep{hhh-2021,jiang2024wildteaming} primarily focus on regular LLMs response style and do not include CoT. To bridge this gap,
we construct a new dataset, named \textsc{SafeChain}, consisting of CoT data for safety alignment of LRMs. The pipeline is shown in Figure \ref{fig:overview}. 
We select 50,000 instructions from the WildJailbreak dataset using a uniform distribution. 
For each sampled instruction, we then use  \texttt{R1-70B} to generate 5 responses.
We next use \texttt{Llama-Guard} to filter the data.
We keep the instructions whose all five responses are safe. 
We finally randomly select one response for each remaining instruction.
This leads to the \textsc{SafeChain} dataset containing 40,000 instruction-response pairs. The details of \textsc{SafeChain} is in Appendix \ref{appx:dataset-detail}.


\input{assets/tab_safetrain_eval}
\subsection{Experiment Setup}

\paragraph{Baseline.} To evaluate the quality of \textsc{SafeChain}, we create a baseline dataset WildJailbreak-40K (WJ-40K). 
WJ-40K contains the same instructions as \textsc{SafeChain}, with safe responses being generated by \textbf{GPT-3.5}. 
In addition, we use the model without training on extra data as a vanilla baseline. 

\paragraph{Training Details.} We choose \texttt{R1-7B} and \texttt{R1-8B}, which are built from Qwen and Llama series respectively. We use supervised fine-tuning on these models with the baseline dataset and our \textsc{SafeChain}. Training details are deferred to Appendix \ref{appx:training-setup}. 

\paragraph{Evaluation Setup.}
Our goal is to enhance the safety of LRMs while preserve their reasoning capabilities. 
We assess their reasoning capabilities using GSM8K \citep{cobbe2021training}, MATH-500 \citep{lightman2023let}, and American Invitational Mathematics Examination (AIME) 2024 for math, and HumanEval \citep{chen2021evaluating}, MBPP \citep{austin2021program}, and LiveCodeBench (v5) \cite{jain2024livecodebench} for coding. 
We evaluate the safety of LRMs using StrongReject and WildJailbreak. We apply greedy decoding in our evaluations, and set repetition penalty to $1.1$ for coding benchmark only due to the higher repetition issue \citep{guo2025deepseek}. All math and coding benchmarks use pass@1 metric and safety benchmarks use Safe@1 metric. 


\subsection{Experiment Results}

Table \ref{tab:safechain_eval} summarizes the math, coding, and safety performance of R1-7B and R1-8B fine-tuned with different datasets. 
We observe that both models show improved safety performance after fine-tuning on WJ-40K and \textsc{SafeChain}. 
WJ-40K achieves the highest safety performance because GPT-3.5 is subject to strong moderation policies and generates safe responses for WJ-40K.
However, fine-tuning with WJ-40K degrades the models' performance on math and coding tasks.
For example, compared to the original model, R1-7B degrades from 39.3\% to 14.5\% on LiveCodeBench.
\textsc{SafeChain}, in contrast, successfully preserves the model's utilities on all benchmarks. 
Moreover, since \textsc{SafeChain} uses CoT data, which closely aligns with the distribution of those used to train LRMs, models fine-tuned with \textsc{SafeChain} may gain improved performance on math and coding benchmarks, e.g., MATH-500.
