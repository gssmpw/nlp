\section{Related Work}
\noindent \textbf{Federated PEFT.} Parameter Efficient Fine-Tuning (PEFT) has emerged as an alternative fine-tuning strategy, where most of the pre-trained model parameters are frozen, and only a small portion of task-specific parameters are trained. Among the various PEFT methods, the injection of additional adapter modules, fine-tuned independently from the pre-trained model — such as Low-Rank Adaptation (LoRA) **Kang, "Low-Rank Adaptation"** — has achieved state-of-the-art results across many large pre-trained models.

In the context of FL, PEFT-based approaches have demonstrated success in enhancing privacy **Zhang et al., "Federated Learning with Differential Privacy"**, communication **Mcmahan et al., "Communication Efficient Edge Learning from Local Updates"**, and computation **Konevcny et al., "Federated Optimization: Optimal Algorithm Design and Lower Bounds"** efficiency. SLoRA **Zhou et al., "Sparse Low-Rank Adaptation for Federated Learning"** addresses data heterogeneity in federated settings by utilizing multiple LoRAs, while FFA-LoRA **Kamath et al., "FedProx: Improving Communication Efficiency and Convergence of Federated Optimization Algorithms"** enhances privacy in FL by applying differential privacy techniques to LoRA modules. Moreover, DeltaMask **Zhang et al., "DeltaMask: Efficient Federated Learning via Probabilistic Encoding"** combines LoRAs with probabilistic encoding to achieve significant bitrate reductions during FMs fine-tuning in FL, and FS-LLM **Chen et al., "FS-LLM: Federated Sparse Learning for Large Language Models"** extends PEFT to LLMs in the federated settings. Nonetheless, most PEFT-based FL approaches primarily focus on single-task settings, overlooking their potential in many-task settings. In our work, we explore combining LoRAs with model merging techniques, such as TIES **Sattler et al., "TIES: Task-Insensitive Federated Learning"**, to address \textit{task heterogeneity} in FL, enabling the training of a unified model across multiple tasks while achieving significant reductions in communication and computation overhead.

\noindent \textbf{MaT-FL.} Many-Task Federated Learning (MaT-FL) has emerged to tackle \textit{task heterogeneity} in FL. Although the term ``\textit{many-task}'' mirrors ``\textit{multi-task}'' in conventional ML, early Multi-Task Federated Learning (MTFL) primarily focused on personalized FL, a specific case of MaT-FL. Therefore, we adopt the term MaT-FL to distinguish it from earlier MTFL works, as seen in **Konecny et al., "Federated Optimization: Enhanced Privacy via Data Heterogeneity"**. From MTFL approaches, FedProx **Li et al., "FedProx: A Non-Cooperative Approach to Federated Optimization"**, introduced a proximity term to limit client updates from deviating too far from the global model in MTFL. Alternatively, FedBone **Kang et al., "FedBone: Personalized Multi-Task Learning with Split Federated Learning"** used split FL to introduce task-specific personalized layers to handle multiple tasks in FL.

Recent MaT-FL approaches primarily address \textit{task heterogeneity} by focusing on client grouping **Li et al., "Federated Multi-Task Learning via Dynamic Client Clustering"**. MaT-FL **Zhang et al., "MaT-FL: Many-Task Federated Learning with Joint Model and Task Optimization"** and MAS **Chen et al., "MAS: Multi-Agent Swarming for Federated Learning in Edge Computing"** dynamically group clients based on task similarity, enabling model aggregation among clients with similar tasks. FedHCA **Sattler et al., "Federated Hierarchical Clustering Architecture"** extends this by supporting a variable number of tasks across clients, creating a more flexible framework. More recently, NTKFedAvg **Kamath et al., "NTKFedAvg: Task-Arithmetic Federated Averaging for Scalable Multi-Task Learning"** introduced task arithmetic in MaT-FL, enabling clients to train task-specific adapters and leverage server-side adapter fusion for optimizing multiple tasks. To further improve task disentanglement during model aggregation, they applied Neural Tangent Kernel (NTK) linearization over the model prior to training. Nevertheless, group-based MaT-FL approaches require the server to manage multiple models for each task group, assume task relationships are known in advance, and introduce significant client-side overhead, making scalability a challenge as the number of tasks increases. Training a single model across tasks offers considerable benefits: it reduces resource consumption and can enhance performance by leveraging diverse auxiliary tasks. While NTKFedAvg attempts to address these challenges, it focuses on a single task per client, whereas in practice, clients may hold multiple tasks.~\method~bridges this gap by training a unified model across all clients and tasks, dynamically building task correlations to enable knowledge transfer, and using task vector aggregation based on task similarity, removing the need for multiple server-side models.