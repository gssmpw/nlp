\section{Related Work}
\noindent \textbf{Federated PEFT.} Parameter Efficient Fine-Tuning (PEFT) has emerged as an alternative fine-tuning strategy, where most of the pre-trained model parameters are frozen, and only a small portion of task-specific parameters are trained. Among the various PEFT methods, the injection of additional adapter modules, fine-tuned independently from the pre-trained model — such as Low-Rank Adaptation (LoRA)____ — has achieved state-of-the-art results across many large pre-trained models. 

In the context of FL, PEFT-based approaches have demonstrated success in enhancing privacy____, communication____, and computation____ efficiency. SLoRA____ addresses data heterogeneity in federated settings by utilizing multiple LoRAs, while FFA-LoRA____ enhances privacy in FL by applying differential privacy techniques to LoRA modules. Moreover, DeltaMask____ combines LoRAs with probabilistic encoding to achieve significant bitrate reductions during FMs fine-tuning in FL, and FS-LLM____ extends PEFT to LLMs in the federated settings. Nonetheless, most PEFT-based FL approaches primarily focus on single-task settings, overlooking their potential in many-task settings. In our work, we explore combining LoRAs with model merging techniques, such as TIES____, to address \textit{task heterogeneity} in FL, enabling the training of a unified model across multiple tasks while achieving significant reductions in communication and computation overhead. \\

\noindent \textbf{MaT-FL.} Many-Task Federated Learning (MaT-FL) has emerged to tackle \textit{task heterogeneity} in FL. Although the term ``\textit{many-task}'' mirrors ``\textit{multi-task}'' in conventional ML, early Multi-Task Federated Learning (MTFL) primarily focused on personalized FL, a specific case of MaT-FL. Therefore, we adopt the term MaT-FL to distinguish it from earlier MTFL works, as seen in____. From MTFL approaches, FedProx____, introduced a proximity term to limit client updates from deviating too far from the global model in MTFL. Alternatively, FedBone____ used split FL to introduce task-specific personalized layers to handle multiple tasks in FL.

Recent MaT-FL approaches primarily address \textit{task heterogeneity} by focusing on client grouping____. MaT-FL____ and MAS____ dynamically group clients based on task similarity, enabling model aggregation among clients with similar tasks. FedHCA____ extends this by supporting a variable number of tasks across clients, creating a more flexible framework. More recently, NTKFedAvg____ introduced task arithmetic in MaT-FL, enabling clients to train task-specific adapters and leverage server-side adapter fusion for optimizing multiple tasks. To further improve task disentanglement during model aggregation, they applied Neural Tangent Kernel (NTK) linearization over the model prior to training. Nevertheless, group-based MaT-FL approaches require the server to manage multiple models for each task group, assume task relationships are known in advance, and introduce significant client-side overhead, making scalability a challenge as the number of tasks increases. Training a single model across tasks offers considerable benefits: it reduces resource consumption and can enhance performance by leveraging diverse auxiliary tasks. While NTKFedAvg attempts to address these challenges, it focuses on a single task per client, whereas in practice, clients may hold multiple tasks.~\method~bridges this gap by training a unified model across all clients and tasks, dynamically building task correlations to enable knowledge transfer, and using task vector aggregation based on task similarity, removing the need for multiple server-side models.