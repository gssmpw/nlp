\section{Related Work}
\subsection{Recursive Self-Improvement}
RSI was first introduced to solve harder problems through iterative refinement ____.
%
A significant sub-area is the use of synthetic data for self-retraining, which has gained much discussion about its associated challenges.
%
For instance, ____ demonstrates that iterative training of models like OPT-125M ____ with synthetic text can lead to severe distribution shifts.
%
Similarly, ____ has found that even a small number of synthetic samples can cause significant large model collapse during training.
%
Meanwhile, ____ reports substantial degradation in the quality of generated samples when using synthetic images to train StyleGAN2 ____ and DDPM models ____.

Despite these challenges, recent work by Meta ____ has proven the feasibility of RSI with synthetic data. They propose a self-rewarding model that utilizes large language models (LLMs) to generate responses and corresponding rewards based on a generated prompt set. These sample pairs are then used for Direct Preference Optimization (DPO) ____.
%
While these studies have effectively highlighted the challenges in RSI using synthetic data, they have not specifically discussed state-of-the-art (SoTA) text-to-image models.
%
In contrast, this paper aims to address these gaps by focusing on the application of RSI to SoTA diffusion models and providing practical solutions.

% 除了使用合成数据训练自己，目前还有其他的自我提升的策略，这些方法主要使用了
In addition to using synthetic data, there has been other exploration of self-improvement strategies. 
%
These methods primarily utilize self-critique mechanisms ____, where the model refines its response based on its own feedback. 
%
However, these dialogue-based methods cannot be directly applied in the context of text-to-image models.

\subsection{Human Preference Alignment}
In recent advancements of text-to-image models, a primary focus has been on enhancing alignment with human preferences. 
%
This typically requires extensive pre-training on high-quality image-text pairs ____.
%
Another research branch focuses on fine-tuning pre-trained models to yield better results.
%
For example, ____ fine-tunes SDXL ____ with 2,000 high-quality samples, which are filtered from a 1.1 billion dataset.
%
Additionally, some methods leverage reinforcement learning during the tuning phase. 
%
For example, DDPO ____ and DPOK ____ have trained a reward model to guide the optimization, while Diffusion-DPO ____ and D3PO ____ propose the implicit reward functions to fine-tune diffusion models directly on the preference dataset.
%
However, there remains limited exploration into the potential of using self-generated data for further improvement.

Meanwhile, a variety of human preference datasets and evaluation metrics have been developed. Pic-a-pic dataset ____ features 583,747 image pairs for binary comparison, and the HPD v2 dataset ____ includes 798,090 human preference choices across 433,760 image pairs. 
%
The evaluation metrics for assessing human preferences are also advanced significantly, including PickScore ____, Human Preference Score (HPS) ____, HPS v2 ____, and image rewards ____. 
%
These together provide a robust framework for evaluating how well generated content aligns with human expectations.


\begin{algorithm}[tb]
   \caption{RSIDiff Procedure}
   \label{alg:rsidiff_procedure}
\begin{algorithmic}
   \STATE {\bfseries Input:} Base model $\mathcal{G}_0$, prompt set $\mathcal{P}=\left \{ \mathcal{P}_0, \mathcal{P}_1, ...,\mathcal{P}_r \right \}$, total training round $r$.
   \STATE Initialize current training round $i=0$.
   \REPEAT
   \STATE Generate synthetic dataset $\mathcal{D}_i$ using prompt set $\mathcal{P}_i$ with the diffusion model $\mathcal{G}_i$.
   \STATE Apply preference sampling to obtain the synthetic training set $\mathcal{S}_i$ from dataset $\mathcal{D}_i$.
   \STATE Calculate the sample weights $\mathcal{W}_i$ of training set $\mathcal{S}_i$.
   \STATE Fine-tune the model $\mathcal{G}_i$ with weighted training set $(\mathcal{W}_i\circ\mathcal{S}_i,\mathcal{P}_i)$ and get the updated model $\mathcal{G}_{i+1}$.
   \STATE Update current training round $i=i+1$.
   \UNTIL{Round $i$ reaches the total training round $r$.}
\end{algorithmic}
\end{algorithm}