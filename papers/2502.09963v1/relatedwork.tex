\section{Related Work}
\subsection{Recursive Self-Improvement}
RSI was first introduced to solve harder problems through iterative refinement \cite{schmidhuber2003godel}.
%
A significant sub-area is the use of synthetic data for self-retraining, which has gained much discussion about its associated challenges.
%
For instance, \cite{shumailov2024ai} demonstrates that iterative training of models like OPT-125M \cite{zhang2022opt} with synthetic text can lead to severe distribution shifts.
%
Similarly, \cite{dohmatob2024strong} has found that even a small number of synthetic samples can cause significant large model collapse during training.
%
Meanwhile, \cite{alemohammad2023self} reports substantial degradation in the quality of generated samples when using synthetic images to train StyleGAN2 \cite{karras2020analyzing} and DDPM models \cite{ho2020denoising}.

Despite these challenges, recent work by Meta \cite{yuan2024self} has proven the feasibility of RSI with synthetic data. They propose a self-rewarding model that utilizes large language models (LLMs) to generate responses and corresponding rewards based on a generated prompt set. These sample pairs are then used for Direct Preference Optimization (DPO) \cite{rafailov2024direct}.
%
While these studies have effectively highlighted the challenges in RSI using synthetic data, they have not specifically discussed state-of-the-art (SoTA) text-to-image models.
%
In contrast, this paper aims to address these gaps by focusing on the application of RSI to SoTA diffusion models and providing practical solutions.

% 除了使用合成数据训练自己，目前还有其他的自我提升的策略，这些方法主要使用了
In addition to using synthetic data, there has been other exploration of self-improvement strategies. 
%
These methods primarily utilize self-critique mechanisms \cite{madaan2024self,chen2023teaching,welleckgenerating,han2024small,miao2023selfcheck}, where the model refines its response based on its own feedback. 
%
However, these dialogue-based methods cannot be directly applied in the context of text-to-image models.

\subsection{Human Preference Alignment}
In recent advancements of text-to-image models, a primary focus has been on enhancing alignment with human preferences. 
%
This typically requires extensive pre-training on high-quality image-text pairs \cite{betker2023improving,esser2024scaling}.
%
Another research branch focuses on fine-tuning pre-trained models to yield better results.
%
For example, \cite{dai2023emu} fine-tunes SDXL \cite{podell2023sdxl} with 2,000 high-quality samples, which are filtered from a 1.1 billion dataset.
%
Additionally, some methods leverage reinforcement learning during the tuning phase. 
%
For example, DDPO \cite{black2023training} and DPOK \cite{fan2024reinforcement} have trained a reward model to guide the optimization, while Diffusion-DPO \cite{wallace2024diffusion} and D3PO \cite{yang2024using} propose the implicit reward functions to fine-tune diffusion models directly on the preference dataset.
%
However, there remains limited exploration into the potential of using self-generated data for further improvement.

Meanwhile, a variety of human preference datasets and evaluation metrics have been developed. Pic-a-pic dataset \cite{kirstain2023pick} features 583,747 image pairs for binary comparison, and the HPD v2 dataset \cite{wu2023human} includes 798,090 human preference choices across 433,760 image pairs. 
%
The evaluation metrics for assessing human preferences are also advanced significantly, including PickScore \cite{kirstain2023pick}, Human Preference Score (HPS) \cite{wu2023humanhps}, HPS v2 \cite{wu2023human}, and image rewards \cite{xu2024imagereward}. 
%
These together provide a robust framework for evaluating how well generated content aligns with human expectations.


\begin{algorithm}[tb]
   \caption{RSIDiff Procedure}
   \label{alg:rsidiff_procedure}
\begin{algorithmic}
   \STATE {\bfseries Input:} Base model $\mathcal{G}_0$, prompt set $\mathcal{P}=\left \{ \mathcal{P}_0, \mathcal{P}_1, ...,\mathcal{P}_r \right \}$, total training round $r$.
   \STATE Initialize current training round $i=0$.
   \REPEAT
   \STATE Generate synthetic dataset $\mathcal{D}_i$ using prompt set $\mathcal{P}_i$ with the diffusion model $\mathcal{G}_i$.
   \STATE Apply preference sampling to obtain the synthetic training set $\mathcal{S}_i$ from dataset $\mathcal{D}_i$.
   \STATE Calculate the sample weights $\mathcal{W}_i$ of training set $\mathcal{S}_i$.
   \STATE Fine-tune the model $\mathcal{G}_i$ with weighted training set $(\mathcal{W}_i\circ\mathcal{S}_i,\mathcal{P}_i)$ and get the updated model $\mathcal{G}_{i+1}$.
   \STATE Update current training round $i=i+1$.
   \UNTIL{Round $i$ reaches the total training round $r$.}
\end{algorithmic}
\end{algorithm}