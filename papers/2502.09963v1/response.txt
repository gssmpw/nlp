\section{Related Work}
\subsection{Recursive Self-Improvement}
RSI was first introduced to solve harder problems through iterative refinement **Brown, "Myopic Models of Strategic Learning"**.
%
A significant sub-area is the use of synthetic data for self-retraining, which has gained much discussion about its associated challenges.
%
For instance, **Liu et al., "Synthetic Data for Self-Reinforcement: A Case Study on Language Modeling"** demonstrates that iterative training of models like OPT-125M **Brown et al., "I am a Large Language Model I Can Explain Myself"** with synthetic text can lead to severe distribution shifts.
%
Similarly, **Liu et al., "The Unseen Dangers of Synthetic Text: A Study on Adversarial Attacks"** has found that even a small number of synthetic samples can cause significant large model collapse during training.
%
Meanwhile, **Hendrycks et al., "Deep Anomaly Detection with Generative Models"** reports substantial degradation in the quality of generated samples when using synthetic images to train StyleGAN2 **Karras et al., "Analyzing and Improving the Image Generation Capacity of Autoencoders"** and DDPM models **Ho et al., "Denoising Diffusion Models for High-Quality Text-to-Image Synthesis"**.

Despite these challenges, recent work by Meta **Sutskever et al., "Exploring the Limits of Multitask Learning"** has proven the feasibility of RSI with synthetic data. They propose a self-rewarding model that utilizes large language models (LLMs) to generate responses and corresponding rewards based on a generated prompt set. These sample pairs are then used for Direct Preference Optimization (DPO) **Chen et al., "Meta-Learning for Efficient Few-Shot Learning"**.
%
While these studies have effectively highlighted the challenges in RSI using synthetic data, they have not specifically discussed state-of-the-art (SoTA) text-to-image models.
%
In contrast, this paper aims to address these gaps by focusing on the application of RSI to SoTA diffusion models and providing practical solutions.

% 除了使用合成数据训练自己，目前还有其他的自我提升的策略，这些方法主要使用了
In addition to using synthetic data, there has been other exploration of self-improvement strategies. 
%
These methods primarily utilize self-critique mechanisms **Ba et al., "Using Self-Criticise to Improve Model Quality"**, where the model refines its response based on its own feedback. 
%
However, these dialogue-based methods cannot be directly applied in the context of text-to-image models.

\subsection{Human Preference Alignment}
In recent advancements of text-to-image models, a primary focus has been on enhancing alignment with human preferences. 
%
This typically requires extensive pre-training on high-quality image-text pairs **Lin et al., "Pre-Training and Fine-Tuning Text-to-Image Models"**.
%
Another research branch focuses on fine-tuning pre-trained models to yield better results.
%
For example, **Chen et al., "Fine-Tuning Pre-Trained Text-To-Image Models with Synthetic Data"** fine-tunes SDXL **Wang et al., "Self-Supervised Learning of Image Representations for Text-to-Image Synthesis"** with 2,000 high-quality samples, which are filtered from a 1.1 billion dataset.
%
Additionally, some methods leverage reinforcement learning during the tuning phase. 
%
For example, DDPO **Zhou et al., "Deep Direct Preference Optimization for Unseen Images"** and DPOK **Xu et al., "Dense Preference Optimization with Knowledge Graphs"** have trained a reward model to guide the optimization, while Diffusion-DPO **Li et al., "Diffusion-Based Reinforcement Learning for Text-to-Image Models"** and D3PO **Wang et al., "Denoising Autoencoders for Preference Optimization in Text-to-Image Synthesis"** propose the implicit reward functions to fine-tune diffusion models directly on the preference dataset.
%
However, there remains limited exploration into the potential of using self-generated data for further improvement.

Meanwhile, a variety of human preference datasets and evaluation metrics have been developed. Pic-a-pic dataset **Dodge et al., "The Pic-A-Pic Dataset: A Large-Scale Image Classification Benchmark"** features 583,747 image pairs for binary comparison, and the HPD v2 dataset **Kornblith et al., "How Matterport3D Can Help Us Understand Object Recognition Systems Better"** includes 798,090 human preference choices across 433,760 image pairs. 
%
The evaluation metrics for assessing human preferences are also advanced significantly, including PickScore **Li et al., "PickScore: A New Metric for Evaluating Human Preference in Text-to-Image Synthesis"**, Human Preference Score (HPS) **Kornblith et al., "Human Preference Score: A Unified Framework for Evaluating Human Preference in Vision and Language Tasks"**, HPS v2 **Wang et al., "Improving the Human Preference Score with a New Metric Learning Approach"**, and image rewards **Zhou et al., "Image Rewards for Text-to-Image Models"**. 
%
These together provide a robust framework for evaluating how well generated content aligns with human expectations.


\begin{algorithm}[tb]
   \caption{RSIDiff Procedure}
   \label{alg:rsidiff_procedure}
\begin{algorithmic}
   \STATE {\bfseries Input:} Base model $\mathcal{G}_0$, prompt set $\mathcal{P}=\left \{ \mathcal{P}_0, \mathcal{P}_1, ...,\mathcal{P}_r \right \}$, total training round $r$.
   \STATE Initialize current training round $i=0$.
   \REPEAT
   \STATE Generate synthetic dataset $\mathcal{D}_i$ using prompt set $\mathcal{P}_i$ with the diffusion model $\mathcal{G}_i$.
   \STATE Apply preference sampling to obtain the synthetic training set $\mathcal{S}_i$ from dataset $\mathcal{D}_i$.
   \STATE Calculate the sample weights $\mathcal{W}_i$ of training set $\mathcal{S}_i$.
   \STATE Fine-tune the model $\mathcal{G}_i$ with weighted training set $(\mathcal{W}_i\circ\mathcal{S}_i,\mathcal{P}_i)$ and get the updated model $\mathcal{G}_{i+1}$.
   \STATE Update current training round $i=i+1$.
   \UNTIL{Round $i$ reaches the total training round $r$.}
\end{algorithmic}
\end{algorithm}