\section{Related Works}
Pre-trained multilingual language models have become the predominant approach for cross-lingual transfer tasks. Word alignment methods that depend on these models have also been proposed \citep{jalili-sabet-etal-2020-simalign, nagata-etal-2020-supervised}. Current realignment methods are typically applied to a multilingual pre-trained model before fine-tuning in a single language (usually English) and applying to other languages on tasks such as Natural Language Inference (NLI) \citep{conneau-etal-2018-xnli}, Named Entity Recognition (NER) \citep{rahimi-etal-2019-massively}, Part-of-speech tagging (PoS) \citep{daniel2020universal}, or Question Answering (QA) \citep{artetxe-etal-2020-cross}. This process is intended to enhance the model's ability to generalize to other languages for these tasks.


Realignment can be performed in different ways. \citet{Cao2020Multilingual} minimizes the l2 distance between translated pairs. But some regularization is needed to prevent the representations from collapsing, which can be done through an additional loss term \citep{Cao2020Multilingual,zhao-etal-2021-inducing}
or using contrastive learning \citep{wu-dredze-2020-explicit}. Since the alignment is done at the word level between contextualized representations, an alignment tool is needed to obtain translated pairs to realign. Most methods employ the statistical tool FastAlign \citep{dyer-etal-2013-simple}. However neural-based tools can be used like AwesomeAlign \citep{dou-neubig-2021-word}, which are indeed shown to work better for low-resource languages, although they come at a larger computational cost \citep{ebrahimi-etal-2023-meeting}. A bilingual dictionary can also be used as a look-up table but extracts fewer pairs of words \citep{gaschi-etal-2023-exploring}. Empirically, it was however shown that realignment has inconsistent results when evaluated across several tasks and languages \citep{Efimov_2023,wu-dredze-2020-explicit}.

The failure of realignment questions the very link between multilingual alignment and cross-lingual transfer \citep{gaschi2022multilingual}. Realignment can increase multilingual alignment, but it might also be detrimental to some monolingual or even multilingual features learned by the model. To alleviate this, \citet{gaschi-etal-2023-exploring} tried to optimize the realignment loss jointly with the fine-tuning loss, but they did not report improved performances.

Due to its black-box nature, it is not straightforward to determine what role each layer of an mLM plays, but \citet{peters-etal-2018-deep} empirically showed, for ELMo, that the lower layers might encapsulate more lower-level information like syntax while the top ones relate to semantics. In a multilingual setting, \citet{wu-dredze-2019-beto} showed that freezing the lower layers of mBERT during fine-tuning can increase its cross-lingual performances.