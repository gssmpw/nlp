\section{Related Works}
Pre-trained multilingual language models have become the predominant approach for cross-lingual transfer tasks. Word alignment methods that depend on these models have also been proposed ____. Current realignment methods are typically applied to a multilingual pre-trained model before fine-tuning in a single language (usually English) and applying to other languages on tasks such as Natural Language Inference (NLI) ____, Named Entity Recognition (NER) ____, Part-of-speech tagging (PoS) ____, or Question Answering (QA) ____. This process is intended to enhance the model's ability to generalize to other languages for these tasks.


Realignment can be performed in different ways. ____ minimizes the l2 distance between translated pairs. But some regularization is needed to prevent the representations from collapsing, which can be done through an additional loss term ____
or using contrastive learning ____. Since the alignment is done at the word level between contextualized representations, an alignment tool is needed to obtain translated pairs to realign. Most methods employ the statistical tool FastAlign ____. However neural-based tools can be used like AwesomeAlign ____, which are indeed shown to work better for low-resource languages, although they come at a larger computational cost ____. A bilingual dictionary can also be used as a look-up table but extracts fewer pairs of words ____. Empirically, it was however shown that realignment has inconsistent results when evaluated across several tasks and languages ____.

The failure of realignment questions the very link between multilingual alignment and cross-lingual transfer ____. Realignment can increase multilingual alignment, but it might also be detrimental to some monolingual or even multilingual features learned by the model. To alleviate this, ____ tried to optimize the realignment loss jointly with the fine-tuning loss, but they did not report improved performances.

Due to its black-box nature, it is not straightforward to determine what role each layer of an mLM plays, but ____ empirically showed, for ELMo, that the lower layers might encapsulate more lower-level information like syntax while the top ones relate to semantics. In a multilingual setting, ____ showed that freezing the lower layers of mBERT during fine-tuning can increase its cross-lingual performances.