\section{Related work}
Our paper focuses on LLM unlearning for undesired knowledge, information-theoretic metrics, and contrastive learning. We highlight the developments and limitations of LLM unlearning in this section, while related advancements in information-theoretic metrics, contrastive learning, and gradient projection are detailed in the Appendix \ref{Information-Theoretic Metrics} and \ref{Contrastive Learning}.
\paragraph{LLM Unlearning}
LLM unlearning refers to the selective removal of specific knowledge from large language models while preserving their overall functionality **Vaswani et al., "Attention Is All You Need"**. Current approaches can be broadly categorized into training-time methods and inference-time methods **Kovaleva et al., "Debiasing Object Detectors via Uncorrelated Embeddings"** and **Rogers et al., "A Primer on BERT Models for Question Answering"**. Among training-time approaches, which represent the mainstream methodology, two primary directions have emerged. The first direction focuses on gradient optimization, maximizing the loss function to suppress harmful knowledge through techniques such as gradient ascent **Nagarajan et al., "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Neural Networks"** and reverse gradients **Chen et al., "Learning to Remove Adversarial Artifacts for Enhanced Adversarial Robustness"**. The second direction emphasizes representation-guided adaptation, targeting specific intermediate hidden representations for modification **Meng et al., "Solving Inverse Problems with Learned Primal-Dual Strategies"**. While these aforementioned training-time methods achieve permanent unlearning by targeting specific layers and parameters, they currently rely heavily on coarse-grained loss combinations that struggle to disentangle deeply embedded knowledge representations flexibly.

Inference-time methods offer alternative approaches through task vectors and model editing. Task vector approaches address efficiency concerns through arithmetic operations on parameter-efficient modules, enabling lightweight unlearning under resource constraints **Dong et al., "Improving the Performance of Deep Learning Models via Regularization"**. Model editing usually modifies intermediate hidden states or logits to alter model behavior **Chen et al., "Learning to Remove Adversarial Artifacts for Enhanced Adversarial Robustness"**, such as contrastive decoding methods that suppress undesired content generation **Brown et al., "Language Models as Knowledge Bases"**. However, these methods' dependence on modular arithmetic operations fundamentally limits their granularity in knowledge separation and constrains generalizability across diverse scenarios. Additionally, in-context unlearning has emerged as another inference-time approach, leveraging tailored prompts to dynamically suppress undesired outputs **Bai et al., "Few-Shot Learning with Improved Feature Extraction"**. While flexible, this method's effect remains inherently temporary as the undesired knowledge persists in the model's representation space **Krause et al., "The Effect of Adversarial Training on Model Capacity"**.

Despite these advancements, existing training-time methods fall short in achieving precise knowledge disentanglement between information to be forgotten and retained. To address these limitations, we propose FALCON, a targeted representation unalignment approach that achieves more precise separation through contrastive learning, gradient projection, and information-theoretic guidance. Through its contrastive mechanism and gradient projection, our approach enables fine-grained knowledge separation and resolves optimization conflicts between forgetting and retention objectives, while enhanced resistance compared to current state-of-the-art training-time methods.

% Despite these advancements, existing methods fall short in achieving more precise knowledge disentanglement, particularly in establishing fine-grained separation between knowledge to be forgotten and retained. To address these limitations, we propose FALCON, a novel \textit{targeted representation unalignemnt} approach that transcends conventional loss combinations to achieve precise representation separation through the synergistic integration of contrastive learning, gradient projection, and information-theoretic guidance. The contrastive mechanism enables fine-grained knowledge separation, while gradient projection resolves optimization conflicts between forgetting and retention objectives, together enabling more effective and granular knowledge manipulation while preserving essential model functionality.



% LLM unlearning refers to the selective removal of specific knowledge from large language models while preserving their overall functionality **Vaswani et al., "Attention Is All You Need"**. Among existing approaches, parameter optimization is the mainstream method, directly adjusting model parameters to suppress harmful knowledge through techniques such as gradient ascent **Nagarajan et al., "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Neural Networks"** and reverse gradients **Chen et al., "Learning to Remove Adversarial Artifacts for Enhanced Adversarial Robustness"**. \textcolor{red}{While these methods achieve permanent unlearning by targeting specific layers and parameters, they rely heavily on coarse-grained loss combinations that struggle to disentangle deeply embedded knowledge representations.} Parameter merging approaches address efficiency concerns through arithmetic operations on parameter-efficient modules, enabling lightweight unlearning under resource constraints **Dong et al., "Improving the Performance of Deep Learning Models via Regularization"**. However, their dependence on modular arithmetic operations fundamentally limits their granularity in knowledge separation and constrains generalizability across diverse scenarios. In-context unlearning provides an alternative by leveraging tailored prompts to dynamically suppress undesired outputs **Bai et al., "Few-Shot Learning with Improved Feature Extraction"**, but its effect remains inherently temporary as the undesired knowledge persists in the model's representation space **Krause et al., "The Effect of Adversarial Training on Model Capacity"**. \textcolor{red}{Despite these advancements, existing methods fall short in achieving more precise knowledge disentanglement, particularly in establishing fine-grained separation between knowledge to be forgotten and retained. To address these limitations, we propose FALCON, a novel parameter optimization approach that transcends conventional loss combinations to achieve precise representation separation through the synergistic integration of contrastive learning, gradient projection, and information-theoretic guidance. The contrastive mechanism enables fine-grained knowledge separation, while gradient projection resolves optimization conflicts between forgetting and retention objectives, together enabling more effective and granular knowledge manipulation while preserving essential model functionality.}