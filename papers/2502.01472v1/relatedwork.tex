\section{Related work}
Our paper focuses on LLM unlearning for undesired knowledge, information-theoretic metrics, and contrastive learning. We highlight the developments and limitations of LLM unlearning in this section, while related advancements in information-theoretic metrics, contrastive learning, and gradient projection are detailed in the Appendix \ref{Information-Theoretic Metrics} and \ref{Contrastive Learning}.
\paragraph{LLM Unlearning}
LLM unlearning refers to the selective removal of specific knowledge from large language models while preserving their overall functionality \cite{zhang2024right}. Current approaches can be broadly categorized into training-time methods and inference-time methods \cite{barez2025openproblemsmachineunlearning}. Among training-time approaches, which represent the mainstream methodology, two primary directions have emerged. The first direction focuses on gradient optimization, maximizing the loss function to suppress harmful knowledge through techniques such as gradient ascent \cite{yao2023large,jang-etal-2023-knowledge} and reverse gradients \cite{eldan2023s}. The second direction emphasizes representation-guided adaptation, targeting specific intermediate hidden representations for modification \cite{li2024the}. While these aforementioned training-time methods achieve permanent unlearning by targeting specific layers and parameters, they currently rely heavily on coarse-grained loss combinations that struggle to disentangle deeply embedded knowledge representations flexibly.

Inference-time methods offer alternative approaches through task vectors and model editing. Task vector approaches address efficiency concerns through arithmetic operations on parameter-efficient modules, enabling lightweight unlearning under resource constraints \cite{ilharco2023editing,zhang2023composing}. Model editing usually modifies intermediate hidden states or logits to alter model behavior \cite{barez2025openproblemsmachineunlearning}, such as contrastive decoding methods that suppress undesired content generation \cite{zhong2024rose}. However, these methods' dependence on modular arithmetic operations fundamentally limits their granularity in knowledge separation and constrains generalizability across diverse scenarios. Additionally, in-context unlearning has emerged as another inference-time approach, leveraging tailored prompts to dynamically suppress undesired outputs \cite{zheng2023can,pawelczyk2024incontext}. While flexible, this method's effect remains inherently temporary as the undesired knowledge persists in the model's representation space \cite{liu2024rethinking}.

Despite these advancements, existing training-time methods fall short in achieving precise knowledge disentanglement between information to be forgotten and retained. To address these limitations, we propose FALCON, a targeted representation unalignment approach that achieves more precise separation through contrastive learning, gradient projection, and information-theoretic guidance. Through its contrastive mechanism and gradient projection, our approach enables fine-grained knowledge separation and resolves optimization conflicts between forgetting and retention objectives, while enhanced resistance compared to current state-of-the-art training-time methods.
% Despite these advancements, existing methods fall short in achieving more precise knowledge disentanglement, particularly in establishing fine-grained separation between knowledge to be forgotten and retained. To address these limitations, we propose FALCON, a novel \textit{targeted representation unalignemnt} approach that transcends conventional loss combinations to achieve precise representation separation through the synergistic integration of contrastive learning, gradient projection, and information-theoretic guidance. The contrastive mechanism enables fine-grained knowledge separation, while gradient projection resolves optimization conflicts between forgetting and retention objectives, together enabling more effective and granular knowledge manipulation while preserving essential model functionality.



% LLM unlearning refers to the selective removal of specific knowledge from large language models while preserving their overall functionality \cite{zhang2024right}. Among existing approaches, parameter optimization is the mainstream method, directly adjusting model parameters to suppress harmful knowledge through techniques such as gradient ascent \cite{yao2023large,jang-etal-2023-knowledge} and reverse gradients \cite{eldan2023s}. \textcolor{red}{While these methods achieve permanent unlearning by targeting specific layers and parameters, they rely heavily on coarse-grained loss combinations that struggle to disentangle deeply embedded knowledge representations.} Parameter merging approaches address efficiency concerns through arithmetic operations on parameter-efficient modules, enabling lightweight unlearning under resource constraints \cite{ilharco2023editing,zhang2023composing}. However, their dependence on modular arithmetic operations fundamentally limits their granularity in knowledge separation and constrains generalizability across diverse scenarios. In-context unlearning provides an alternative by leveraging tailored prompts to dynamically suppress undesired outputs \cite{zheng2023can,pawelczyk2024incontext}, but its effect remains inherently temporary as the undesired knowledge persists in the model's representation space \cite{liu2024rethinking}. \textcolor{red}{Despite these advancements, existing methods fall short in achieving more precise knowledge disentanglement, particularly in establishing fine-grained separation between knowledge to be forgotten and retained. To address these limitations, we propose FALCON, a novel parameter optimization approach that transcends conventional loss combinations to achieve precise representation separation through the synergistic integration of contrastive learning, gradient projection, and information-theoretic guidance. The contrastive mechanism enables fine-grained knowledge separation, while gradient projection resolves optimization conflicts between forgetting and retention objectives, together enabling more effective and granular knowledge manipulation while preserving essential model functionality.}