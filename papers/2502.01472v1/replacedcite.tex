\section{Related work}
Our paper focuses on LLM unlearning for undesired knowledge, information-theoretic metrics, and contrastive learning. We highlight the developments and limitations of LLM unlearning in this section, while related advancements in information-theoretic metrics, contrastive learning, and gradient projection are detailed in the Appendix \ref{Information-Theoretic Metrics} and \ref{Contrastive Learning}.
\paragraph{LLM Unlearning}
LLM unlearning refers to the selective removal of specific knowledge from large language models while preserving their overall functionality ____. Current approaches can be broadly categorized into training-time methods and inference-time methods ____. Among training-time approaches, which represent the mainstream methodology, two primary directions have emerged. The first direction focuses on gradient optimization, maximizing the loss function to suppress harmful knowledge through techniques such as gradient ascent ____ and reverse gradients ____. The second direction emphasizes representation-guided adaptation, targeting specific intermediate hidden representations for modification ____. While these aforementioned training-time methods achieve permanent unlearning by targeting specific layers and parameters, they currently rely heavily on coarse-grained loss combinations that struggle to disentangle deeply embedded knowledge representations flexibly.

Inference-time methods offer alternative approaches through task vectors and model editing. Task vector approaches address efficiency concerns through arithmetic operations on parameter-efficient modules, enabling lightweight unlearning under resource constraints ____. Model editing usually modifies intermediate hidden states or logits to alter model behavior ____, such as contrastive decoding methods that suppress undesired content generation ____. However, these methods' dependence on modular arithmetic operations fundamentally limits their granularity in knowledge separation and constrains generalizability across diverse scenarios. Additionally, in-context unlearning has emerged as another inference-time approach, leveraging tailored prompts to dynamically suppress undesired outputs ____. While flexible, this method's effect remains inherently temporary as the undesired knowledge persists in the model's representation space ____.

Despite these advancements, existing training-time methods fall short in achieving precise knowledge disentanglement between information to be forgotten and retained. To address these limitations, we propose FALCON, a targeted representation unalignment approach that achieves more precise separation through contrastive learning, gradient projection, and information-theoretic guidance. Through its contrastive mechanism and gradient projection, our approach enables fine-grained knowledge separation and resolves optimization conflicts between forgetting and retention objectives, while enhanced resistance compared to current state-of-the-art training-time methods.
% Despite these advancements, existing methods fall short in achieving more precise knowledge disentanglement, particularly in establishing fine-grained separation between knowledge to be forgotten and retained. To address these limitations, we propose FALCON, a novel \textit{targeted representation unalignemnt} approach that transcends conventional loss combinations to achieve precise representation separation through the synergistic integration of contrastive learning, gradient projection, and information-theoretic guidance. The contrastive mechanism enables fine-grained knowledge separation, while gradient projection resolves optimization conflicts between forgetting and retention objectives, together enabling more effective and granular knowledge manipulation while preserving essential model functionality.



% LLM unlearning refers to the selective removal of specific knowledge from large language models while preserving their overall functionality ____. Among existing approaches, parameter optimization is the mainstream method, directly adjusting model parameters to suppress harmful knowledge through techniques such as gradient ascent ____ and reverse gradients ____. \textcolor{red}{While these methods achieve permanent unlearning by targeting specific layers and parameters, they rely heavily on coarse-grained loss combinations that struggle to disentangle deeply embedded knowledge representations.} Parameter merging approaches address efficiency concerns through arithmetic operations on parameter-efficient modules, enabling lightweight unlearning under resource constraints ____. However, their dependence on modular arithmetic operations fundamentally limits their granularity in knowledge separation and constrains generalizability across diverse scenarios. In-context unlearning provides an alternative by leveraging tailored prompts to dynamically suppress undesired outputs ____, but its effect remains inherently temporary as the undesired knowledge persists in the model's representation space ____. \textcolor{red}{Despite these advancements, existing methods fall short in achieving more precise knowledge disentanglement, particularly in establishing fine-grained separation between knowledge to be forgotten and retained. To address these limitations, we propose FALCON, a novel parameter optimization approach that transcends conventional loss combinations to achieve precise representation separation through the synergistic integration of contrastive learning, gradient projection, and information-theoretic guidance. The contrastive mechanism enables fine-grained knowledge separation, while gradient projection resolves optimization conflicts between forgetting and retention objectives, together enabling more effective and granular knowledge manipulation while preserving essential model functionality.}