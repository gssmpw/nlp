\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{epsfig}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[comma,sort&compress,numbers]{natbib}
\usepackage{color}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{bbding}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{threeparttable}
% \usepackage{filecontents}
% \usepackage{lipsum}
% \usepackage[justification=centering]{caption}
% \usepackage[font=small]{caption}
\usepackage{bm}
\RequirePackage{CJKnumb}
\usepackage{algorithmicx,algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\usepackage{amsmath,mleftright,mathtools}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{overpic}
% \usepackage[font=small]{caption}
\usepackage{tikz}
\newcommand\red[1]{\textcolor{red}{#1}}
\newcommand\blue[1]{\textcolor{blue}{#1}}
\newcommand\green[1]{\textcolor{green}{#1}}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% \captionsetup[table]{labelsep=newline,singlelinecheck=false,}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\newcommand{\ostar}{\mathbin{\mathpalette\make@circled\star}}
\usepackage{stackengine} 
\newcommand\oast{\stackMath\mathbin{\stackinset{c}{0ex}{c}{0ex}{\ast}{\bigcirc}}}
\newcommand\white[1]{\textcolor{white}{#1}}
\graphicspath{{./figures/}}
\usepackage{soul}




\begin{document}

\title{FCVSR: A Frequency-aware Method for Compressed Video Super-Resolution}
\author{Qiang Zhu,
Fan Zhang,~\IEEEmembership{Member, IEEE,}
Feiyu Chen,~\IEEEmembership{Member, IEEE,}\\
Shuyuan Zhu,~\IEEEmembership{Member, IEEE,}
David Bull,~\IEEEmembership{Fellow, IEEE,}
and Bing Zeng,~\IEEEmembership{Fellow, IEEE}
\thanks{Q. Zhu, F. Chen, S. Zhu, and B. Zeng are with the School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China (e-mail: eezsy@uestc.edu.cn).}
\thanks{Q. Zhu is also with the School of Computer Science, University of Bristol, Bristol, United Kingdom. }
\thanks{F. Zhang and D. Bull are with the School of Computer Science, University of Bristol, Bristol, United Kingdom.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Compressed video super-resolution (SR) aims to generate high-resolution (HR) videos from the corresponding low-resolution (LR) compressed videos. Recently, some compressed video SR methods attempt to exploit the spatio-temporal information in the frequency domain, showing great promise in super-resolution performance. However, these methods do not differentiate various frequency subbands spatially or capture the temporal frequency dynamics, potentially leading to suboptimal results. In this paper, we propose a deep frequency-based compressed video SR model (FCVSR) consisting of a motion-guided adaptive alignment (MGAA) network and a multi-frequency feature refinement (MFFR) module. Additionally, a frequency-aware contrastive loss is proposed for training FCVSR, in order to reconstruct finer spatial details. The proposed model has been evaluated on three public compressed video super-resolution datasets, with results demonstrating its effectiveness when compared to existing works in terms of super-resolution performance (up to a 0.14dB gain in PSNR over the second-best model) and complexity.
\end{abstract}

\begin{IEEEkeywords}
video super-resolution, video compression, frequency, contrastive learning, deep learning, FCVSR.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{I}n recent years, video super-resolution (VSR) has become a popular research topic in image and video processing. It typically takes a low-resolution (LR) video clip and reconstructs its corresponding high-resolution (HR) counterpart with improved perceptual quality. VSR has been used for various application scenarios including video surveillance~\cite{zheng2024sac,farooq2021human}, medical imaging~\cite{chen2023cunerf,qiu2023rethinking} and video compression~\cite{kang2023super,lin2023luma}.  Inspired by the latest advances in deep learning, existing VSR methods leverage various deep neural networks~\cite{ranjan2017optical,teed2020raft,dai2017deformable,zhu2019deformable,arnab2021vivit,ho2022video} in model design, with notable examples including BasicVSR~\cite{chan2021basicvsr} and TCNet~\cite{liu2022temporal} based on optical flow~\cite{ranjan2017optical,teed2020raft}, TDAN~\cite{tian2020tdan} and EDVR~\cite{wang2019edvr} based on deformable convolution networks (DCN)~\cite{dai2017deformable, zhu2019deformable}, TTVSR~\cite{liu2022learning} and FTVSR++ ~\cite{qiu2023learning} based on vision transformers~\cite{arnab2021vivit}, and Upscale-A-Video~\cite{zhou2024upscale} and MGLD-VSR~\cite{yang2025motion} based on diffusion models~\cite{ho2022video}. 


\begin{figure}[!t]
\centering
\centerline{\includegraphics[width=\linewidth]{paramVSR1201.pdf}} % paramVSR1200.pdf
{\caption{{Illustration of performance-complexity trade-offs for different compressed VSR models. It can be observed that the proposed FCVSR model offers better super-resolution performance with lower complexity compared to benchmark methods.}}
\label{fig:performance_vs_complexity}}
\vspace{-5pt}
\end{figure}

When VSR is applied to video compression, it shows great potential in producing significant coding gains when integrated with conventional~\cite{afonso2018video,shen2011down} and learning-based video codecs~\cite{khani2021efficient,yang2022learned}.  In these cases, in addition to the quality degradation induced by spatial down-sampling, video compression also generates compression artifacts within low-resolution content \cite{bull2021intelligent}, which makes the super-resolution task more challenging. Previous works reported that general VSR methods may not be suitable for dealing with both compression \cite{ding2023blind,jiang2023video,luo2022spatio} and down-sampling degradations~\cite{wang2019edvr,chan2021basicvsr}, so bespoke compressed video super-resolution methods~\cite{li2021comisr,chen2021compressed,zhang2022codec,wang2023compression,zhu2024deep,qiu2023learning,he2021fm, conde2024aim,chen2023gaussian,qiu2022learning} have been proposed to address this issue. Among these methods, there is a class of compressed VSR models ~\cite{li2021comisr,qiu2022learning,qiu2023learning} focuses on performing super-resolution in the frequency domain, such as  COMISR~\cite{li2021comisr}, FTVSR~\cite{qiu2022learning} and FTVSR++~\cite{qiu2023learning}, which align well with the nature of super-resolution, recovering the lost high-frequency details in the low-resolution content. However, it should be noted that these methods do not differentiate various frequency subbands spatially or capture the temporal frequency dynamics. This limits the reconstruction of spatial details and the accuracy of temporal alignment, resulting in suboptimal super-resolution performance.


In this context, this paper proposes a novel deep \textbf{F}requency-aware \textbf{C}ompressed \textbf{VSR} model, \textbf{FCVSR}, which exploits both spatial and temporal information in the frequency domain. It employs a new motion-guided adaptive alignment (MGAA) module that estimates multiple motion offsets between frames in the frequency domain, based on which cascaded adaptive convolutions are performed for feature alignment. We also designed a multi-frequency feature refinement (MFFR) module based on a decomposition-enhancement-aggregation strategy to restore high-frequency details within high-resolution videos. To optimize the proposed FCVSR model, we have developed a frequency-aware contrastive (FC) loss for recovering high-frequency fine details. The main contributions of this work are summarized as follows:

\begin{enumerate}[leftmargin=*]

\item A new \textbf{motion-guided adaptive alignment (MGAA)} module, which achieves improved feature alignment through explicitly considering the motion relationship in the frequency domain. 
To our knowledge, it is the first time that this type of approach is employed for video super-resolution. Compared to commonly used deformable convolution-based alignment modules ~\cite{tian2020tdan,wang2019edvr,chan2022basicvsr++} in existing solutions, MGAA offers better flexibility,  higher performance, and lower complexity.

\item A novel \textbf{multi-frequency feature refinement (MFFR)} module, which provides the capability to recover fine details by using a decomposition-enhancement-aggregation strategy. Unlike existing frequency-based refinement models~\cite{xiao2024towards,li2023multi} that do not decompose features into multiple frequency subbands, our MFFR module explicitly differentiates features of different subbands, gradually performing the enhancement of subband features. 

\item  A \textbf{frequency-aware contrastive (FC)} loss is employed using contrastive learning based on the divided high-/low-frequency groups, supervising the reconstruction of finer spatial details. 

\end{enumerate}

Based on a comprehensive experiment, the proposed FCVSR model has demonstrated its superior performance in both quantitative and qualitative evaluations on three public datasets, when compared to five existing compressed VSR methods, with up to a 0.14dB PSNR gain. Moreover, it is also associated with relatively low computational complexity, which offers an excellent trade-off for practical applications (as shown in Fig. \ref{fig:performance_vs_complexity}).

\section{Related Work} \label{RW}

This section reviews existing works in the research areas of video super-resolution (VSR), in particular focusing on compressed VSR and frequency-based VSR which are relevant to the nature of this work. We have also briefly summarized the loss functions typically used for VSR. 

\subsection{Video Super-Resolution}

VSR is a popular low-level vision task that aims to construct an HR video from its LR counterpart. State-of-the-art VSR methods \cite{chan2021basicvsr,liu2022temporal,xiao2023online,zhu2022fffn,tian2020tdan,baniya2023omnidirectional,wang2019edvr,chan2022basicvsr++,liu2022learning} typically leverage various deep neural networks~\cite{ranjan2017optical,teed2020raft,dai2017deformable,zhu2019deformable,arnab2021vivit,ho2022video,tian2020tdan}, achieving significantly improved performance compared to conventional super-resolution methods based on classic signal processing theories~\cite{liu2013bayesian,xiong2010robust}. For example, 
BasicVSR~\cite{chan2021basicvsr}, IconVSR~\cite{chan2021basicvsr} and TCNet~\cite{liu2022temporal} utilize optical flow~\cite{ranjan2017optical,teed2020raft} networks to explore the temporal information between neighboring frames in order to achieve temporal feature alignment. Deformable convolution-based alignment methods~\cite{tian2020tdan, wang2019edvr} have also been proposed based on the DCN~\cite{dai2017deformable, zhu2019deformable}, with typical examples such as TDAN~\cite{tian2020tdan} and EDVR~\cite{wang2019edvr}. DCN has been reported to offer better capability in modeling geometric transformations between frames, resulting in more accurate motion estimation results. More recently, several VSR models~\cite{chan2022basicvsr++, zhu2024dvsrnet,qing2023video} have been designed with a flow-guided deformable alignment (FGDA) module that combines optical flow and DCN to achieve improved temporal alignment, among which BasicVSR++~\cite{chan2022basicvsr++} is a commonly known example. Moreover, more advanced network structures have been employed for VSR, such as Vision Transformer (ViT) and diffusion models. TTVSR~\cite{liu2022learning} is a notable ViT-based VSR method, which learns visual tokens along spatio-temporal trajectories for modeling long-range features. CTVSR~\cite{tang2023ctvsr} further exploits the strengths of Transformer-based and recurrent-based models by concurrently integrating the spatial information derived from multi-scale features and the temporal information acquired from temporal trajectories. Furthermore, diffusion models~\cite{hu2023lamd,ho2022video} have been utilized~\cite{zhou2024upscale,chen2024learning,yang2025motion} to improve the perceptual quality of super-resolved content. Examples include Upscale-A-Video~\cite{zhou2024upscale} based on a text-guided latent diffusion framework and MGLD-VSR~\cite{yang2025motion} that exploits the temporal dynamics based on diffusion model within LR videos. 
% The former has been applied to VSR task~\cite{liu2022learning,qiu2022learning,qiu2023learning,zhang2023multi,tang2023ctvsr,zhou2024video,liang2024vrt}  for capturing the spatio-temporal information of videos. For example,






% % on each frequency band, so that real visual texture can be distinguished from artifacts and further obtain the best video enhancement quality. 

% %~\cite{yi2019multi,wang2019edvr,qiu2022learning,zhou2024upscale,yang2025motion,liu2022temporal,chan2021basicvsr,tian2020tdan,wang2020deep,chan2022basicvsr++} 

% \subsection{Frequency-based Video Super-Resolution}

Recently, some VSR methods~\cite{qiu2022learning, qiu2023learning,li2023multi,dong2023dfvsr} are designed to perform low-resolution video up-sampling in the \textbf{frequency} domain rather than in the spatial domain. For example, FTVSR++~\cite{qiu2023learning} has been proposed to use a degradation-robust frequency-Transformer to explore the long-range information in the frequency domain; similarly, a multi-frequency representation enhancement with privilege information (MFPI) network~\cite{li2023multi} has been developed with a spatial-frequency representation enhancement branch that captures the long-range dependency in the spatial dimension, and an energy frequency representation enhancement branch to obtain the inter-channel feature relationship; DFVSR~\cite{dong2023dfvsr} applies the discrete wavelet transform to generate directional frequency features from LR frames and achieve directional frequency-enhanced alignment. Further examples include COMISR~\cite{li2021comisr} which applies a Laplacian enhancement module to generate high-frequency information for enhancing fine details, GAVSR~\cite{chen2023gaussian} that employs a high-frequency mask based on Gaussian blur to assist the attention mechanism and FTVSR~\cite{qiu2022learning} which is based on a Frequency-Transformer to conduct self-attention over a joint space-time-frequency domain. However, these frequency-based methods do not fully explore the multiple frequency subbands of the features or account for the motion relationships in the frequency domain, which restricts the exploration of more valuable information.
% \hl{[The issues with existing compressed VSR!]}

In many application scenarios, VSR is applied to compressed LR content, making the task even more challenging compared to uncompressed VSR. Recently, this has become a specific research focus, and numerous \textbf{compressed VSR} methods~\cite{li2021comisr,chen2021compressed,zhang2022codec,wang2023compression,zhu2024deep,qiu2023learning,he2021fm, conde2024aim,chen2023gaussian} have been developed based on coding priors. For example, CD-VSR~\cite{chen2021compressed} utilizes motion vectors, predicted frames, and prediction residuals to reduce compression artifacts and obtain spatio-temporal details for HR content; CIAF~\cite{zhang2022codec} employs recurrent models together with motion vectors to characterize the temporal relationship between adjacent frames; CAVSR~\cite{wang2023compression} also adopts motion vectors and residual frames to achieve information fusion. It is noted that these methods are typically associated with increased complexity in order to fully leverage these coding priors, which limits their adoption in practical applications. 
% \hl{Complexity issues: decoder}

% \begin{figure*}[!t]
% \centering
% \includegraphics[width=0.998\textwidth]{figures/pipeline13.pdf}
% \caption{ The architecture of FCVSR model. The consecutive LR compressed video frames are gradually fed into the convolution layers, MGAA, MFFR, and reconstruction module to generate HR videos. Besides, a combined loss function based on an energy-based temporal consistency loss and a frequency-aware contrastive learning loss is adopted to supervise the generation of high-quality HR videos in the frequency domain.}
% \label{fig_pipeline}
% \end{figure*}


\begin{figure*}[!t]
\centering
\includegraphics[width=0.998\textwidth]{figures/FCVSR_pipe2.pdf}
\caption{ The architecture of FCVSR model. An LR compressed video is fed into a convolution layer, MGAA, MFFR, and reconstruction (REC) modules to generate an HR video.}
\label{fig_pipeline}
\end{figure*}

\subsection{Loss Functions of Video Super-Resolution}

When VSR models were optimized, various loss functions were employed to address different application scenarios. 
These can be classified into two primary groups: spatial- and frequency-based. 
Spatial-based loss functions aim to minimize the pixel-wise discrepancy between the generated HR frames and the corresponding ground truth (GT) frames during training, with $L_1$ and $L_2$ losses are the commonly used objectives. Furthermore, the Charbonnier loss~\cite{lai2018fast} is a differentiable and smooth approximation of the $L_2$ loss, with similar robustness as the $L_1$ loss for reducing the weight of large errors and focusing more on smaller errors. Recently, some frequency-based loss functions~\cite{fuoli2021fourier,li2023multi,jiang2021focal} are proposed to explore the high-frequency information. For example, a Fourier space loss~\cite{fuoli2021fourier} calculated the frequency components in the Fourier domain for direct emphasis on the frequency content for restoration of high-frequency components. A focal frequency loss~\cite{jiang2021focal} generated the frequency representations using the discrete Fourier transform to supervise the generation of high-frequency information. However, these frequency-based loss functions typically observe global frequency information without decomposing features into different frequency subbands, which constrains VSR models to recover fine details. 

% Temporal-based losses focus on the temporal dynamics between frames to ensure the temporal consistency of the generated videos. For example, a SOF-VSR~\cite{wang2020deep} model utilizes an optical flow-based loss function, which compares the aligned frame and GT, while DFVSR~\cite{dong2023dfvsr} employs a directional frequency loss based on the discrete wavelet transform (DWT) to ensure the accuracy of motion information in the temporal alignment. % \blue{It is noted that however these temporal-based loss functions are used independently for optimizing aligned features, which lack of global optimization of temporal consistency in restored videos.}


% \subsubsection{Alignment for Video Super-Resolution}

% Here, we illustrate three representative alignment modules in VSR task, i.e., optical flow-based alignment~\cite{xue2019video, chan2021basicvsr,liu2022temporal}, deformable convolution-based alignment~\cite{tian2020tdan,wang2019edvr}, and flow-guided deformable alignment~\cite{chan2022basicvsr++}, in Fig. \ref{fig_tempalign}(a)-(c).  Specifically, in optical flow-based alignment\cite{xue2019video, chan2021basicvsr,liu2022temporal}, two frames are firstly fed into the optical flow network to estimate optical flow. Then, the estimated optical flow is adopted to warp the current frame to obtain the aligned frame.
% % The optical flow network decides the performance and efficiency of this alignment. 
% In deformable convolution-based alignment\cite{tian2020tdan,wang2019edvr}, the offsets are learned from two frames by several convolution layers and then the learned offsets and current frame are fed into deformable convolution to generate aligned frame. 
% % The deformable convolution mainly decides the performance and efficiency of this alignment. 
% In flow-guided deformable convolution-based alignment~\cite{chan2022basicvsr++}, the optical flow is adopted to guide the generation of more motion, which increase the diversity of offsets.  
% % The deformable convolution and optical flow network mainly decides the performance and efficiency of this alignment. 
% The proposal of flow-guided deformable convolution-based alignment~\cite{chan2022basicvsr++} greatly improves the accuracy of alignment and achieves a significant VSR performance gain. 

% Our adaptive convolution-based alignment is illustrated in Fig. \ref{fig_tempalign}(d), in our alignment module, the multi-motions are generated by the motion estimator in the frequency domain.  
% %Based on our above observation in Fig. \ref{fig_fluseq},  the temporal information is amplified in the frequency domain, which inspires us to learn the difference between frames to increase the diversity of motion for accurate alignment. 
% We design an adaptive convolution to flexibly customize the learnable kernel and receptive field with low complexity. Based on these designs, our adaptive convolution-based alignment can achieve better alignment performance with lower complexity than the existing three representative alignment modules. We cascaded applying separable adaptive convolution~\cite{xu2014deep,lee2021iterative} with the guidance of multi-motion to fully exploit the nonlinear nature of deep networks. Our separable adaptive convolution-based cascaded scheme also enables small-sized separable adaptive convolutions to be used for establishing large receptive fields.


% \begin{comment}
% Adaptive convolution~\cite{ref130} have been proposed to facilitate the adaptive handling of features in various tasks~\cite{ref131,ref131-1,ref132,ref133,ref135,ref136,ref137,lee2021iterative}, such as frame interpolation~\cite{ref131,ref131-1}, deblurring~\cite{ref132, ref133,lee2021iterative}, and semantic segmentation~\cite{ref135,ref136}. Adaptive convolution commonly consists of kernel prediction of adaptive convolution and feature transformation of adaptive convolution based on the predicted kernel. However, handling large receptive field requires predicting large kernels, which results in huge computational cost~\cite{ref133}. To reduce computational cost when predicting large kernel, separable adaptive convolutions~\cite{xu2014deep,lee2021iterative} were proposed to secure larger receptive fields at lower computational costs than the adaptive convolution~\cite{ref133}  by utilizing 1-dim kernels, instead of 2-dim kernels. However, compared to dense 2-dim convolution kernel, separable adaptive convolution may not provide enough accuracy for feature representation. 
% \end{comment}


% In recently years, the deformable convolution-based alignment~\cite{tian2020tdan,wang2019edvr,chan2022basicvsr++} is widely applied in video super-resolution task, due to strong ability of feature learning. However, this alignment still has following limitations:

% 1) \textbf{Fixed Kernel}. The fixed kernel of DCN is adopted for all computation of DCN, which reduces the diversity of feature representation learning. Although the deformable property enhances DCN to explore the more valuable position information for alignment, the fixed kernel decreases the representation feature. 2) \textbf{High Complexity}. In most deformable convolution-based alignment modules~\cite{tian2020tdan,wang2019edvr,chan2022basicvsr++}, the kernel size of DCN is 3$\times$3. As the kernel size of DCN increases, the complexity significantly increases so as to bring huge computational cost. The comparable results are provided in Section \ref{EXP}.  3) \textbf{Single Offset}. The simple offset is only learned in DCN, which limits the diversity of temporal information learning for improvement of alignment performance. Although the flow-guided deformable convolution~\cite{chan2022basicvsr++} is proposed to avoid the overflow of offset, the generation of signal offset lacks the diversity.

% To solve the limitations of deformable convolution-based alignment for high performance and high efficient alignment, we propose the  adaptive convolution-based alignment and we illustrate its advantages:

% 1) \textbf{Flexible Kernel}. Cascaded kernels are designed for specific motion alignment in adaptive convolution, so that the kernel of adaptive convolution are not fixed and the learning of kernels is flexible. 2) \textbf{Low Complexity}. We adopt a adaptive separable convolution, which significantly reduces the complexity of 2-dim adaptive convolution so as to improving large receptive fields with low complexity. Besides, we design a motion estimator in the frequency domain to learn more accurate motion while reducing complexity.  3) \textbf{Motion Diversity}. The design of grouping motion enhances the diversity of motion information and improve the alignment performance. 




% \subsection{Contrastive Learning}

% Contrastive learning has achieved promising results in unsupervised representation learning\cite{chen2020simple,he2020momentum}. The core idea of contrastive learning is to push the features of unrelated data (as negative samples) and pull the related data (as positive samples), thereby learning the representations which are discriminative to the negative samples and invariant between the positive samples. Contrastive learning can be effectively applied by appropriately defining the positive samples and negative samples in terms of the tasks, including multi-views~\cite{tian2020makes}, temporal coherence~\cite{han2020self}, augmented transformation~\cite{chen2020simple}. Recently, contrastive learning is applied to low-level tasks~\cite{chen2022unpaired,liang2022semantically,liu2021divco,wu2021contrastive,xia2022efficient}. Most existing contrastive learning methods in this filed take clean images as positives and degraded images as negative samples. For example,~\cite{wu2021contrastive,xia2022efficient} proposed a supervised framework where restored images are pulled closer to ground truth and pushed away from the hazy images in the feature space.  





\begin{comment}


\section{ Proposed Method} \label{PM}

\subsection{Problem Formulation}
The goal of compressed video SR is to restore HR videos from their LR compressed videos.  Let $V_{L R}=\left\{I_{t+i}\right\}_{i=-T}^{T}$ be a LR compressed video, its corresponding HR video is denoted as $V_{HR}=\left\{I^{HR}_{t+i}\right\}_{i=-T}^{T}$. Usually, the LR  videos are firstly generated by downsampling interpolation, such as bicubic or biliner interpolation, from  HR videos. Then, the LR videos are compressed by specific video compression configurations, such as  LDB configuration or  LDP configuration,  to obtain the LR compressed videos. The generation of LR compressed video can be denoted as

\begin{equation}
V_{LR} = \mathcal{VC}\left((V_{HR})\downarrow_s\right),
\end{equation}

\noindent
where $(\cdot)\downarrow_s$ represents the downsampling operation with scale $s$, $\mathcal{VC}(\cdot)$ represents the video compression algorithm. The generation of LR compressed video induces two degradations, i.e., downsampling degradation and compression degradation. We introduce these two degradations as follows. 

\end{comment}

%\subsubsection{Downsampling Degradation}
%The downsampling degradation is denoted as 
%
%\begin{equation}
%I_{\downarrow} = (I)\downarrow_s,
%\end{equation}
%
%\noindent
%where $I_{\downarrow}$ and $I$ represent the downsampling degraded image and HR image. $(\cdot)\downarrow_s$ can be nearest-neighbour, bilinear or bicubic interpolation. Each pixel value of the LR image is obtained by sampling local pixel points from the original image and interpolating these points. The LR image will inevitably lose some high-frequency information from HR image when processing the downsamping degradation. Therefore, its high-frequency information is necessary to restore high-quality HR image.
%
%
%\subsubsection{Compression Degradation}
%
%Video compression algorithms uses the  quantization process to remain main information and lose the unimportant information, this algorithm is denoted as 
%
%\begin{equation}
%I_{VC}=\mathcal{VC}(I_{\downarrow}),
%\end{equation}
%
%\noindent
%where $I_{VC}$ and $I_{\downarrow}$ represent the compression degraded image and downsampling degraded image. The quantization process of video compression algorithm is applied locally on patches, which easily introduce compression artifacts and lose mainly high-frequency information. Therefore, to restore lost high-frequency details and reduce compression artifacts. 



\begin{comment}

\subsection{Formulation in Frequency Domain}

To obtain the frequency representation of a compressed image, we conduct the Fast Fourier Transform to transfer the compressed image from spatial domain into frequency domain and conduct the inverse Fast Fourier Transform to transfer the compressed image from frequency domain into spatial domain. Besides, the energy of frequency spectrum is utilized to represent the characteristic of frequency information. Here we will illustrate these transforms and energy of frequency spectrum of compressed images. 




\subsubsection{Fast Fourier Transform }

For a compressed image $I$, the Fast Fourier Transform $\mathbb{F}(\cdot)$ transfer it into frequency domain, this process is denoted as

\begin{equation}
{I_{F}}(u, v)= \sum_{h=0}^{I_{h}-1} \sum_{w=0}^{I_{w}-1} {I}(h, w) e^{-j 2 \pi\left(\frac{uh}{I_{h}} +\frac{vw}{I_{w}} \right)},
\end{equation}

\noindent
where   $I_{h}$ is the height and $I_{w}$ is width of image ${I}$. The inverse Fast Fourier Transform   $\mathbb{F}^{-1}(\cdot)$   is denoted as

\begin{equation}
{I}(h, w)=\frac{1}{I_{h}I_{w}} \sum_{u=0}^{I_{h}-1} \sum_{v=0}^{I_{w}-1} {I_{F}}(u, v) e^{j 2 \pi\left(\frac{uh}{I_{h}} +\frac{vw}{I_{w}} \right)}.
\end{equation}

As mentioned above in Fig. \ref{fig_fluseq}, the information of a compressed image is represented by conducted Fast Fourier Transform  in frequency domain.

%\subsubsection{Matrix Multiply in Frequency Domain}
%
%We note that each element of $\mathbf{Q K}^{\top}$ is obtained by the inner product:
%\begin{equation}
%\left(\mathbf{Q K}^{\top}\right)_{i j}=\left\langle\mathbf{q}_i, \mathbf{k}_j\right\rangle,
%\end{equation}
%where $\mathbf{q}_i$ and $\mathbf{k}_j$ are the vectorized forms of $i$-th and $j$-th patches from $F_q$ and $F_k$. Based on (3), if we apply reshape functions to $\mathbf{q}_i$ and all the patches $\mathbf{k}_j$, respectively, all the i-th column elements of $\mathbf{Q K}^{\top}$ can be obtained by a convolution operation, i.e., $\widetilde{q}_i \otimes \widetilde{K}$, where $\widetilde{q}_i$ and $\widetilde{K}$ denote the reshaped results of $\mathbf{q}_i$ and $\mathbf{k}_j ; \otimes$ denotes the convolution operation.
%
%According to the convolution theorem, the correlation or convolution of two signals in the spatial domain is equivalent to an element-wise product of them in the frequency domain. Therefore, a natural question is that can we efficiently estimate the attention map by an element-wise product operation in a frequency domain instead of computing the matrix multiplication of $\mathrm{QK}^{\top}$ in the spatial domain?
%
%To this end, we develop an effective frequency domainbased self-attention solver. Specifically, we first obtain $F_q$, $F_k$, and $F_v$ by a $1 \times 1$ point-wise convolution and $3 \times 3$ depthwise convolution. Then, we apply the fast Fourier transform (FFT) to the estimated features $F_q$ and $F_k$ and estimate the correlation of $F_q$ and $F_k$ in the frequency domain by:
%$$
%A=\mathcal{F}^{-1}\left(\mathcal{F}\left(F_q\right) \overline{\mathcal{F}\left(F_k\right)}\right),
%$$
%where $\mathcal{F}(\cdot)$ denotes the FFT, $\mathcal{F}^{-1}(\cdot)$ denotes the inverse FFT, and $\overline{\mathcal{F}(\cdot)}$ denotes the conjugate transpose operation.

% As mentioned above in Fig. \ref{fig_fluseq}, the information of a compressed image is represented by conducted Fast Fourier Transform  in frequency domain.

\subsubsection{Energy of Frequency Spectrum}
The energy of frequency spectrum of a compressed image $I$ also utilized to represent the characteristic of frequency information and the energy of frequency spectrum is denoted as

\begin{equation} \label{ene_f}
\mathcal{E}_{F}(I)= \sum_{u=0}^{I_{h}-1} \sum_{v=0}^{I_{w}-1}|{I_{F}}(u, v)|^2.
\end{equation}
where $|\cdot|$ is the absolute value operation. As mentioned above in Fig. \ref{fig_fluseq}, the temporal information of a compressed video sequence represented by energy of frequency spectrum is amplified, which is beneficial for the excavation of temporal information.

\end{comment}





% \begin{figure}[!t]
% \begin{minipage}[b]{1\linewidth}
%     \centering
%     % \begin{overpic}[width=0.095\linewidth]{rgb_qp37_2.png}\put(5,80){\tiny{{4th}}}
%     % \end{overpic}\hspace{-0.8mm}
%     \begin{overpic}[width=0.16\linewidth]{rgb_qp37_3.png}\put(36,80){\small{\white{5th}}}
%     \end{overpic}\hspace{-0.8mm}
%     \begin{overpic}[width=0.16\linewidth]{rgb_qp37_4.png}\put(36,80){\small{\white{6th}}}
%     \end{overpic}\hspace{-0.8mm}
%     \begin{overpic}[width=0.16\linewidth]{rgb_qp37_5.png}\put(36,80){\small{\white{7th}}}
%     \end{overpic}\hspace{-0.8mm}
%     % \begin{overpic}[width=0.12\linewidth]{rgb_qp37_6.png}\put(36,80){\tiny{{8th}}}
%     % \end{overpic}
%     \hspace{-0.2mm}
%     % \begin{overpic}[width=0.12\linewidth]{rgb_qp37_2.png}\put(5,80){\tiny{{4th}}}
%     % \end{overpic}\hspace{-0.8mm}
%     \begin{overpic}[width=0.16\linewidth]{rgb_qp37_3.png}\put(36,80){\small{\white{5th}}}
%     \end{overpic}\hspace{-0.8mm}
%     \begin{overpic}[width=0.16\linewidth]{rgb_qp37_4.png}\put(36,80){\small{\white{6th}}}
%     \end{overpic}\hspace{-0.8mm}
%     \begin{overpic}[width=0.16\linewidth]{rgb_qp37_5.png}\put(36,80){\small{\white{7th}}}
%     \end{overpic}\hspace{-0.8mm}
%     \\
%     % \begin{overpic}[width=0.095\linewidth]{rgb_qp37_6.png}\put(5,80){\tiny{{8th}}}
%     % \end{overpic} 
%     \vspace{0.5mm} 
% \end{minipage} 

% \begin{minipage}[b]{1\linewidth}
%     \centering
%     % \includegraphics[width=0.095\linewidth]{freq_qp37_2.png}\hspace{-0.8mm}
%     \includegraphics[width=0.16\linewidth]{freq_qp37_3.png}\hspace{-0.8mm}
%     \includegraphics[width=0.16\linewidth]{freq_qp37_4.png}\hspace{-0.8mm} 
%     \includegraphics[width=0.16\linewidth]{freq_qp37_5.png}\hspace{-0.8mm}
%     %\includegraphics[width=0.12\linewidth]{freq_qp37_6.png}
%     \hspace{-0.2mm}
%     \includegraphics[width=0.16\linewidth]{freq_qp37_2.png}\hspace{-0.8mm}
%     \includegraphics[width=0.16\linewidth]{freq_qp37_3.png}\hspace{-0.8mm}
%     \includegraphics[width=0.16\linewidth]{freq_qp37_4.png}\hspace{-0.8mm}
%     % \includegraphics[width=0.12\linewidth]{freq_qp37_5.png}\hspace{-0.8mm}
%     \\
%     % \includegraphics[width=0.095\linewidth]{freq_qp37_6.png}
%     % \footnotesize{5th} \hspace{9mm} \footnotesize{6th } \hspace{9mm} \footnotesize{7th}  \hspace{9mm}\footnotesize{5th} \hspace{9mm} \footnotesize{6th} \hspace{9mm} \footnotesize{7th} \\
%     \footnotesize{(a)} \hspace{40mm} \footnotesize{(b)}
% \end{minipage} 

% \begin{minipage}[b]{0.98\linewidth}
%     \centering
%     \centerline{\epsfig{figure=Amp_FreqCurve_02.pdf,width=9.0cm}}
%     \footnotesize{ (c) }
% \end{minipage}
% {\caption{Comparison between compressed-uncompressed pair of $\emph{BasketballDrill}$ sequence at 5th-7th frames in spatial domain and frequency domain. (a) Compressed sequence and its frequency spectrum. (b) Uncompressed sequence and its frequency spectrum. (c) Amplitude-frequency curve of compressed-uncompressed image pair at 6th frame.}
%     \label{vision_freSpec}}
% \end{figure}





% \begin{figure}[!t]
% 	\centering
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_qp37_2.png,width=0.8cm}}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_qp37_3.png,width=0.8cm}}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_qp37_4.png,width=0.8cm}}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_qp37_5.png,width=0.8cm}}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_qp37_6.png,width=0.8cm}}
% 	\end{minipage}
% 	\hspace*{0.08cm}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_gt2.png,width=0.8cm}}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_gt3.png,width=0.8cm}}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_gt4.png,width=0.8cm}}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_gt5.png,width=0.8cm}}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=rgb_gt6.png,width=0.8cm}}
% 	\end{minipage} \\
%         \vspace{0.5pt}       
%         \begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_qp37_2.png,width=0.8cm}}
% 		\footnotesize{ \ }
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_qp37_3.png,width=0.8cm}}
% 		\footnotesize{ \ }
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_qp37_4.png,width=0.8cm}}
% 		\footnotesize{(a)}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_qp37_5.png,width=0.8cm}}
% 		\footnotesize{ \ } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_qp37_6.png,width=0.8cm}}
% 		\footnotesize{ \ } 
% 	\end{minipage}
% 	\hspace*{0.08cm}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_gt_2.png,width=0.8cm}}
% 		\footnotesize{ \ } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_gt_3.png,width=0.8cm}}
% 		\footnotesize{ \ } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_gt_4.png,width=0.8cm}}
% 		\footnotesize{(b)}
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_gt_5.png,width=0.8cm}}
% 		\footnotesize{ \ }
% 	\end{minipage}
% 	\begin{minipage}[b]{0.08\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=freq_gt_6.png,width=0.8cm}}
% 		\footnotesize{ \ } 
% 	\end{minipage} \\
% 	\begin{minipage}[b]{0.99\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=Amp_FreqCurve_new.pdf,width=8.80cm}}
% 		\footnotesize{ (c) }
% 	\end{minipage}
% 	{\caption{Comparison between compressed-uncompressed  pair of $\emph{BasketballDive}$ sequence at xxxth-xxxth frames in spatial domain and frequency domain. (a) Compressed sequence and its frequency spectrum. (b) Uncompressed sequence and its frequency spectrum. (c) Amplitude-frequency curve of compressed-uncompressed image pair at xxxth frame.}
% 		\label{vision_freSpec}}
% \end{figure}




% \begin{figure}[!t]
% 	\centering
% 	\includegraphics[width=0.485\textwidth]{Energy_PSNR_01.pdf}
% 	\caption{Energy difference curve and PSNR curve for compressed-uncompressed $\emph{BasketballDrive}$ sequence. }
% 	\label{fig_fluseq}
% \end{figure}

% % The frame quality fluctuation between consecutive frames is more significant measured by energy of frequency spectrum in the frequency domain than measured by PSNR in the spatial domain.







\section{Proposed Method} \label{PM}


% \subsection{Motivation}

% % We visually illustrate the characteristics of compressed-uncompressed video sequence pairs in the frequency domain to clarify our motivation. Specifically, 
% We first illustrate the consecutive compressed-uncompressed sequence pairs of  $BasketballDrill$ sequence (from the 5th to 7th frame) with 220 $\times$ 220 resolution in the spatial domain and frequency domain. Then, a compressed-uncompressed image pair from the sequence is selected to illustrate its amplitude-frequency curve. These visual results are illustrated in Fig. \ref{vision_freSpec}.  Additionally, the temporal-frequency information of the compressed sequence is measured by the energy of the frequency spectrum and is measured by PSNR on $\emph{BasketballDrive}$ sequence (from 130th to 230th frame). These two curves are illustrated in Fig. \ref{fig_fluseq}. We selected the compressed sequence generated from an uncompressed sequence under LDB configuration at QP=37 for illustration.
% Based on the above observations, we can infer the following motivations for designing our compressed video SR method:


% \begin{enumerate}[leftmargin=*]

% \item We compare the uncompressed-compressed video frame pairs and their frequency spectrum in Fig. \ref{vision_freSpec}. Compared with Fig. \ref{vision_freSpec} (a) and Fig. \ref{vision_freSpec} (b), we can find that their central area (representing low-frequency information) is almost the same, while their peripheral areas (representing high-frequency information) have significantly different, which implies that compression degradation mainly causes the lost of high-frequency information in the uncompressed video. Besides, we draw the amplitude-frequency curve of compressed-uncompressed image pair at $t$ time step for quantitative observation in Fig.\ref{vision_freSpec}  (c). It is found from Fig.\ref{vision_freSpec} (c) that the lose of information mainly occurs at the high-frequency band. This phenomenon implies that high-frequency information is more needed in the restored videos.

% \item We show the energy difference curve and PSNR curve of compressed-uncompressed pairs in Fig. \ref{fig_fluseq}. We can find from Fig. \ref{fig_fluseq} that compressed sequence measured by PSNR has obviously periodic frame quality fluctuation due to the GoP (Group of Pictures) setting in compression configuration and the change of frame quality fluctuation is slight. On the contrary, energy of frequency spectrum enlarges the range of the frame quality fluctuation, it isn't dependent on GoP setting, which demonstrates that energy of frequency spectrum of compressed sequence measured in frequency domain can explore more obvious difference between consecutive frames than PSNR measured in the spatial domain, and energy of frequency spectrum isn't dependent on specific GoP setting. This phenomenon implies that the motion information between consecutive frames is amplified in frequency domain for effectively constructing the temporal modeling. 

% \end{enumerate}




% \subsection{Overview of FCVSR}
To address the issues associated with existing video super-resolution (VSR) methods, this paper proposes a novel frequency-aware VSR model, FCVSR, specifically for compressed content, targeting improved trade-off between performance and complexity. As illustrated in Fig. \ref{fig_pipeline}, for the current LR video frame $I_{t}$, FCVSR takes seven LR video frames $\left\{I_{i}\right\}_{i=t-3}^{t+3}$ as input and produces an HR video frame $I_t^{\mathrm{SR}}$, targeting the uncompressed HR counterpart $I_t^{\mathrm{HR}}$ of $I_t$.

Specifically, each input frame is fed into a convolution layer with a 3$\times$3 kernel size,
%
\begin{equation} \label{eq_FE}
\mathcal{F}_{i}=\operatorname{Conv}\left(I_{i}\right) \in \mathbb{R}^{ h \times w \times c}, i = t-3, \dots, t+3
\end{equation}
where $h,w,c$ are the height, width, and channel of feature. 

In order to achieve pixel-level alignment between the current frame and other input neighboring frames, multiple motion-guided adaptive alignment (MGAA) modules are employed, which takes 3 sets of features generated by the convolution layer as input and outputs a single set of features. First, this is applied to the features corresponding to the first three frames, $\left\{\mathcal{F}_{i}\right\}_{i=t-3}^{t-1}$, and produces $\bar{\mathcal{F}}_{t-2}$. This operation is repeated for $\left\{\mathcal{F}_{i}\right\}_{i=t+1}^{t+3}$ to obtain $\bar{\mathcal{F}}_{t+2}$. $\bar{\mathcal{F}}_{t-2}$, $\mathcal{F}_{t}$ and $\bar{\mathcal{F}}_{t+2}$ are then fed into the MGAA module again to generate the final aligned feature set $\bar{\mathcal{F}}_{t}$.

% To generate SR frame $I_{t+m}^{\mathrm{SR}}$ of LR compressed frame $I_{t+m}$ from its $2T$ neighboring LR compressed frames $\left\{{I}_{t+m+i}\right\}_{i=-T}^{T}$, where $m$$\in$$[-T,T]$ is shifted temporal distance,  we divide $\left\{\mathcal{F}_{t+m+i}\right\}_{i=-T}^{T}$ into three groups, i.e.,  $\left\{\mathcal{F}_{t+m+i}\right\}_{i=-T}^{-1}$, $\left\{\mathcal{F}_{t+m}\right\}$, $\left\{\mathcal{F}_{t+m+i}\right\}_{i=1}^{T}$, for feature alignment. Based on the temporal distance between the frames of groups and target frame $I_{t+m}$, we gradually send these groups into the MGAA module to achieve the feature alignment within groups and obtain the correspondingly aligned features $\bar{\mathcal{F}}_{t+m\pm T/2-1}$, $\bar{\mathcal{F}}_{t+m}$.



\begin{comment}


\begin{algorithm}[t]
\caption{The Procedure of FCVSR Model.}
\label{Alg_1}
\begin{algorithmic}[0]
\State \textbf{Input:}
LR video sequence $V_{LR}$ = $\left\{I_{t-2T},..., I_{t},..., I_{t+2T}\right\}$.

\State \textbf{Output:} Super-resolving video sequence $V_{SR}$.

\State \textbf{Initialization:} Temporal reference radius $T$. 

\State  \texttt{// Feature extraction}

\State $\left\{\mathcal{F}_{t-2T},..., \mathcal{F}_{t},..., \mathcal{F}_{t+2T}\right\}$ = Conv($V_{LR}$).

\For{$m=-T:T$}

\State  \texttt{// Motion-guided adaptive alignment}

\State	\texttt{// First stage: Past \& Future}
\State $\bar{\mathcal{F}}_{t\pm k-2} = \mathcal{M}_{MGAA}(\mathcal{F}_{t\pm m-1}, ... , \mathcal{F}_{t\pm m-3})$.   

% \State	\texttt{// First stage: Future}
% \State  $\bar{\mathcal{F}}_{t+i+2}  = \mathcal{M}_{MGAA}(\mathcal{F}_{t+i+1}, ... , \mathcal{F}_{t+i+3})$.  

\State	\texttt{// Second stage: Current}
\State  $\bar{\mathcal{F}}_{t+m} = \mathcal{M}_{MGAA}(\bar{\mathcal{F}}_{t-m-2} , \mathcal{F}_{t+m}, \bar{\mathcal{F}}_{t+m+2})$.   

\State  \texttt{// Multi-frequency feature refinement}

\State  $\widetilde{\mathcal{F}}_{t+m} = \mathcal{M}_{MFFR}(\bar{\mathcal{F}}_{t+m})$.

\State  \texttt{// Reconstruction}

\State $\widetilde{{I}}_{t+m} = \mathcal{M}_{REC}(\widetilde{\mathcal{F}}_{t+m})$.

\State  $ I_{t+m}^{\mathrm{UP}} = U_{b\uparrow}\left({I}_{t+m}\right)$.

\State  $ I_{t+m}^{\mathrm{SR}}=\widetilde{{I}}_{t+m} + I_{t+m}^{\mathrm{UP}}$.
\EndFor
\State \textbf{end for}

\State \textbf{Return:} $V_{SR}$ = $\left\{I^{SR}_{t-T},..., I^{SR}_{t},..., I^{SR}_{t+T}\right\}$.
\end{algorithmic}

\end{algorithm}
\end{comment}

% Two groups with long temporal distance, i.e.,  $\left\{\mathcal{F}_{t+m+i}\right\}_{i=-T}^{-1}$ and $\left\{\mathcal{F}_{t+m+i}\right\}_{i=1}^{T}$, are firstly fed into the MGAA module to obtain their aligned features

% \begin{equation}
% \bar{\mathcal{F}}_{t+m-T/2-1}= \mathrm{MGAA}(\left\{\mathcal{F}_{t+m+i}\right\}_{i=-T}^{-1}),
% \end{equation}
% and 
% \begin{equation}
% 	\bar{\mathcal{F}}_{t+m+T/2+1}= \mathrm{MGAA}(\left\{\mathcal{F}_{t+i}\right\}_{i=1}^{T}).
% \end{equation}

% \noindent
% Then, a group with short temporal distance, i.e., $\left\{\mathcal{F}_{t+m}\right\}$, is collected with $\bar{\mathcal{F}}_{t+m-T/2-1}$ and  $\bar{\mathcal{F}}_{t+m+T/2+1}$ together to feed into the MGAA module to obtain the aligned feature

%  \begin{equation}
%  	\bar{\mathcal{F}}_{t+m}= \mathrm{MGAA}(\bar{\mathcal{F}}_{t+m-T/2-1},\bar{\mathcal{F}}_{t},\bar{\mathcal{F}}_{t+m+T/2+1}).
%  \end{equation}

% \noindent

% FCVSR employs multiple motion-guided adaptive alignment  (MGAA) modules and a multi-frequency feature refinement (MFFR) module, and is optimized based on a hybrid loss function combining an energy-based temporal consistency loss (ETC) and a frequency-aware contrastive loss (FC). 

Following alignment operation, the aligned feature $\bar{\mathcal{F}}_{t}$ is processed by into a multi-frequency feature refinement (MFFR) module to obtain the refined feature $\widetilde{{\mathcal{F}}}_{t}$, before input into a reconstruction (REC) module, which outputs the HR residual frame ${\widehat{I}}_{t}$. Finally, this is combined with the bilinear up-sampled compressed frame $I_t^{\mathrm{UP}}$ (from $I_t$) through element-wise sum to obtain the final HR frame $I_t^{\mathrm{SR}}$.

\subsection{Motion-Guided Adaptive Alignment}

Most existing VSR methods estimate a single optical flow~\cite{teed2020raft, dosovitskiy2015flownet, ilg2017flownet} or offset~\cite{tian2020tdan,wang2019edvr} between frames only once to achieve feature alignment, which limits the accuracy of feature alignment in some cases. In addition, existing optical flow-based alignment modules~\cite{liu2022temporal,chan2021basicvsr,xue2019video} or deformable convolution-based alignment modules~\cite{tian2020tdan,wang2019edvr} are typically associated with high complexity, restricting their adoption in practical applications. To address these problems, we developed a motion-guided adaptive alignment (MGAA) module that estimates different types of motion between frames, which are further used for feature alignment through adaptive convolutions. An MGAA module, as illustrated in Fig. \ref{fig_MGAA}, consists of a Motion Estimator, Kernel Predictor, and a motion-guided adaptive convolution (MGAC) layer in a bidirectional propagation manner.


\begin{figure*}[!t]
\centering
\includegraphics[width=0.998\textwidth]{figures/MGAA13.pdf} % MGAA_pipe
\caption{The architecture of motion-guided adaptive alignment (MGAA) module. The set of features $\left\{\mathcal{F}_{i}\right\}_{i=t-3}^{t-1}$ are divided into the forward set $\left\{\mathcal{F}_{i}\right\}_{i=t-3}^{t-2}$ and the backward set $\left\{\mathcal{F}_{i}\right\}_{i=t-2}^{t-1}$ for feature alignment.}
\label{fig_MGAA}
\end{figure*}

% A motion estimator constructs a multi-branch structure to estimate the motion cues between frames in the frequency domain to obtain the multiple motions for representing the diversity of motions. A kernel predictor predicts the kernel of each adaptive convolution to strengthen the representation ability of adaptive convolution. A motion-guided adaptive convolution layer cascades several adaptive convolutions and each adaptive convolution is guided by one type of motion to achieve the adaptive feature alignment, targeting high performance and high efficiency. % $N$ motions are generated in the motion estimator, and $N$ adaptive convolutions are applied to construct the motion-guided adaptive convolution layer in the MGAA module. 

Specifically, without loss of generality, when the MGAA module takes the set of features $\left\{\mathcal{F}_{i}\right\}_{i=t-3}^{t-1}$ as input (shown in Fig. \ref{fig_MGAA}), these features are first divided into the forward set $\left\{\mathcal{F}_{i}\right\}_{i=t-3}^{t-2}$ and the backward set $\left\{\mathcal{F}_{i}\right\}_{i=t-2}^{t-1}$ for bidirectional propagation in the MGAA module. The forward features $\left\{\mathcal{F}_{i}\right\}_{i=t-3}^{t-2}$ are then fed into the Motion Estimator $\mathrm{ME}(\cdot)$ to perform motion prediction, resulting in motion offsets $\emph{\textbf{O}}_{t-2}$:
%
\begin{equation} \label{ME}
\emph{\textbf{O}}_{t-2} = \left\{{o}_{n}\right\}_{n=1}^{N} =  \mathrm{ME}(\mathcal{F}_{t-2},\mathcal{F}_{t-3}),  {o}_{n} \in \mathbb{R}^{ h \times w \times 2},
\end{equation}
%
where $N$ is the number of motion offsets.

The feature $\mathcal{F}_{t-2}$ is also input into the Kernel Predictor $\mathrm{KP}(\cdot)$ to generate $N$ adaptive convolution kernels:
%
\begin{equation} \label{KP}
\mathbf{K} = \left\{\mathbf{K}_{n}\right\} = \mathrm{KP}(\mathcal{F}_{t-2}), \mathbf{K}_{n} \in \mathbb{R}^{ h \times w \times 2ck},
\end{equation}
%
where $k$ is the kernel size of the adaptive convolution. 

Based on the motion offsets and kernel sets, the feature $\mathcal{F}_{t-3}$, is processed by the MGAC layer $\mathrm{MGAC}(\cdot)$ to achieve the feature alignment (with $\mathcal{F}_{t-2}$):
%
\begin{equation}
\bar{\mathcal{F}}_{t-2}^f = \mathrm{MGAC}(\mathcal{F}_{t-3}, {\emph{\textbf{O}}}_{t-2}).
\end{equation}

In parallel, the same operation is performed for the backward set to obtain the aligned features $\bar{\mathcal{F}}_{t-2}^b$. Finally, $\bar{\mathcal{F}}_{t-2}^f$ and $\bar{\mathcal{F}}_{t-2}^b$ are concatenated and fed into a convolution layer to obtain the final aligned feature $\bar{\mathcal{F}}_{t-2}$.

\subsubsection{Motion Estimator}

The Motion Estimator is applied in the frequency domain by performing the Fast Fourier Transform (FFT) on the input feature sets, and the resulting frequency features are denoted as $\mathcal{\hat{F}}_{t-2}$ and $\mathcal{\hat{F}}_{t-3}$ corresponding to $\mathcal{F}_{t-2}$ and $\mathcal{F}_{t-3}$  respectively. The difference between these frequency features are then combined with their concatenated version (through a convolution block $\mathrm{CB}_1$), obtaining the difference feature $\mathcal{\hat{F}}_{d}$:
%
\begin{equation}
\mathcal{\hat{F}}_{d} =\mathcal{\hat{F}}_{t-2} - \mathcal{\hat{F}}_{t-3} + \mathrm{CB}_{1}(\mathcal{C}(\mathcal{\hat{F}}_{t-2},\mathcal{\hat{F}}_{t-3})),
\end{equation}
where $\mathrm{CB}_{1}$ is a convolution block consisting of a 3$\times$3 convolution layer with $2c$ channels, a ReLU activation function followed  a 3$\times$3 convolution layer with $c$ channels. $\mathcal{C}(\cdot)$ represents the concatenation operation.

The difference feature $\mathcal{\hat{F}}_{d}$ is then input into multiple branches with different kernel sizes to learn motion sets, $\left\{\hat{o}_{n}\right\}$ in the frequency domain. For the $n$-th branch, the motion offset is calculated as follows: 
%
\begin{equation}
\hat{o}_{n} = \mathrm{Conv}_\mathrm{n}(\mathcal{\hat{F}}_{d}) \oast \mathrm{CB}_2( \mathcal{\hat{F}}_{t-2}),
\end{equation}
%
where $\mathrm{Conv}_\mathrm{n}$ consists of two convolution layers with kernel size $2n + 1$, a PReLU activation function, and channel attention~\cite{zhang2018image}. $\oast$ is a correlation operation to obtain the correlation between features. $\mathrm{CB}_2$ is a convolution block consisting of a 3$\times$3 convolution layer with $c$ channels, a ReLU activation function and a 3$\times$3 convolution layer with 2 channels. 

The learned multiple frequency motion offsets are transformed into the spatial domain by inverse FFT, resulting in the motion offsets $\left\{{o}_{n}\right\}$. 

\subsubsection{Kernel Predictor}
To predict adaptive convolution kernels, we designed a Kernel Predictor $\mathrm{KP}(\cdot)$ (formulated by Eq. (\ref{KP})), which consists of a 3$\times$3 convolution layer and 1$\times$1 convolution layer to generate two directional kernels. The kernel set $\mathbf{K}$ predicted here is a $2Nck$-dim vector representing $N$ sets of kernels $\left\{\mathbf{K}_n\right\}_{n=1}^N$. For the $n$-th predicted kernel $\mathbf{K}_n$, it has two 1-dim kernels $\mathcal{K}^{v}_n$ and $\mathcal{K}^{h}_n$ with sizes $k \times 1$ and $1 \times k$ and $c$ channels.  

\begin{figure*}[!t]
\centering
\includegraphics[width=0.97\textwidth]{figures/MFFR_pipe.pdf}
\caption{The architecture of multi-frequency feature refinement (MFFR) module.}
\label{vision_SFIF}
\end{figure*}


\subsubsection{Motion-Guided Adaptive Convolution Layer}

We utilize the estimated multiple motion offsets to independently guide the feature spatial sampling for each adaptive convolution in the MGAC layer based on predicted kernels. As shown in Fig. \ref{fig_MGAA}, at the $n$-th adaptive convolution operation, $\mathbf{AC}\text{-}n$, the aligned feature $\bar{a}_n$ is calculated as:
%
\begin{equation}
\bar{{a}}_n = \mathbf{AC}\text{-}n(\bar{{a}}_{n-1}, o_{n}, \mathbf{K}_{n})=  \mathbb{S}(\bar{{a}}_{n-1}, o_n) * \mathcal{K}^h_n * \mathcal{K}^v_n,
\end{equation}
%
where $n=1,\dots, N$, $\bar{a}_0$ = $\mathcal{F}_{t-3}$, $\bar{a}_N$ = $\bar{\mathcal{F}}_{t-2}$,  $\mathbb{S}(\cdot,\cdot)$ represents the spatial sampling operation and $*$ is the channel-wise convolution operator that performs convolutions in a spatially-adaptive manner. 


\subsection{Multi-Frequency Feature Refinement}

\begin{figure*}[!t]
\centering
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=LRQP22_000_00000004.png,width=1.75cm}}
\footnotesize{LR Frame}  
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor.png,width=1.75cm}}
\footnotesize{$\bar{\mathcal{F}}_{t}$}  
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor_1.png,width=1.75cm}}
\footnotesize{$S_{1}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor_2.png,width=1.75cm}}
\footnotesize{$S_{2}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor_3.png,width=1.75cm}}
\footnotesize{$S_{3}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor_4.png,width=1.75cm}}
\footnotesize{$S_{4}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor_5.png,width=1.75cm}}
\footnotesize{$S_{5}$}  
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor_6.png,width=1.75cm}}
\footnotesize{$S_{6}$}  
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor_7.png,width=1.75cm}}
\footnotesize{$S_{7}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizbefor_8.png,width=1.75cm}}
\footnotesize{$S_{8}$} 
\end{minipage}

% \footnotesize{ (a) \emph{Before}.} \\
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=GT_000_00000004.png,width=1.75cm}}
\footnotesize{GT}  
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizenh.png,width=1.75cm}}
\footnotesize{$\mathcal{\widetilde{F}}_{t}$}  
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizout_1.png,width=1.75cm}}
\footnotesize{$E_{1}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizout_2.png,width=1.75cm}}
\footnotesize{$E_{2}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizout_3.png,width=1.75cm}}
\footnotesize{$E_{3}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizout_4.png,width=1.75cm}}
\footnotesize{$E_{4}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizout_5.png,width=1.75cm}}
\footnotesize{$E_{5}$}  
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizout_6.png,width=1.75cm}}
\footnotesize{$E_{6}$}  
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizout_7.png,width=1.75cm}}
\footnotesize{$E_{7}$} 
\end{minipage}
\begin{minipage}[b]{0.094\linewidth}
\centering
\centerline{\epsfig{figure=MFFR_vizout_8.png,width=1.75cm}}
\footnotesize{$E_{8}$} 
\end{minipage}
% \footnotesize{ (b) \emph{Enehanced}.} 
\caption{{Visualization of input feature, output feature, decoupled features, enhanced features in the MFFR module.} }\label{visionMFFR}
\end{figure*}

In this work, rather than restoring high-frequency information within the entire frequency range as existing works~\cite{li2023multi,dong2023dfvsr}, we designed a multi-frequency feature refinement (MFFR) module to refine the input feature in different frequency subbands, as shown in Fig. \ref{vision_SFIF}. It consists of \emph{Decoupler}, \emph{Enhancer}, and \emph{Aggregator} modules,  based on the decomposition-enhancement-aggregation strategy. 
% In the MFFR module, the ``\textbf{\emph{Decomposition}}-\textbf{\emph{Enhancement}}-\textbf{\emph{Aggregation}}" strategy is designed to decompose feature, enhance feature, and aggregate feature to obtain the refined feature.

Specifically, the \emph{Decoupler} module employs Gaussian band-pass filters to decompose the input feature $\bar{\mathcal{F}}_{t}\in \mathbb{R}^{h \times w \times c }$ into $Q$ features:
%
\begin{equation}
\textbf{\emph{S}} =  \left\{{S}_{j}\right\}_{j=1}^Q =   \mathrm{Decoupler}(\mathcal{\bar{F}}_{t}).
\end{equation}
%
The decomposed feature set $\textbf{\emph{S}}$ (or its subsets) is then fed into multiple \emph{Enhancer} modules to obtain the enhanced features $\emph{\textbf{E}} = \left\{{E}_{j} \right\}_{j=1}^Q$. Specifically, for the $q^{th}$ subband, the subset $\left\{S_j\right\}_{j=1}^q$, and enhanced features $\left\{E_j\right\}_{j=1}^{q-1}$ for the lower subbands (if applicable) are input into the \emph{Enhancer} module to obtain the enhanced feature $E_{q}$  at this subband level. This process is described by:
%
% \begin{equation}
% \emph{\textbf{E}} =  \left\{{E}_{q} \right\}_{q=1}^Q,
% \end{equation}
\begin{equation}
{E}_{q} = 
\begin{cases} 
\mathrm{Enhancer}({S}_{1}),  q = 1, \\
\mathrm{Enhancer}(\left\{S_j\right\}_{j=1}^q, \left\{E_j\right\}_{j=1}^{q-1}),  q = 2,\dots, Q.
\end{cases} 
\end{equation}
%
For the lowest subband, we additionally apply a mean filter on ${S}_{1}$ before inputting it into \emph{Enhancer}.

Finally, the \emph{Aggregator} module is employed to aggregate the enhanced features $\emph{\textbf{E}}$ and obtain the refined feature:
%
\begin{equation}
\mathcal{\widetilde{F}}_{t} = \mathrm{Aggregator}(\emph{\textbf{E}}).
\end{equation}




% \begin{algorithm}[t]
% \caption{The  Procedure of Decoupler.}
% \label{Alg_2}
% \begin{algorithmic}[0]
% \State \textbf{Input:}
% Input feature $\bar{\mathcal{F}}_{t}\in \mathbb{R}^{h \times w \times c }$.

% \State \textbf{Output:} Decoupled features $\textbf{\emph{S}}=\left\{S_1, S_2,\dots, S_q\right\}$.

% \State \textbf{Initialization:} The decomposition number is $q$. 

% \State  \texttt{// Generate frequency feature by FT}

% $F(u, v, c)= \mathbb{F}(\bar{\mathcal{F}}_{t}(x, y, c)) \in \mathbb{Z}^{h \times w \times c}$. 

% \State \texttt {// Generate frequency masks by Gaussian band-pass filter masks}

% $\emph{\textbf{M}}=\left\{M_{1},\dots, M_{j},\dots, M_{q}\right\}$, $M_{j}\in\mathbb{R}^{h \times w \times 1}$. % via Eq.(\ref{BP_filter})

% \For{$j=1:q$}	
% \For{$g=1:c$}	

% $\hat{s}_j = F(u,v,g)\cdot M_{j} \in \mathbb{R}^{h \times w \times 1}$.

% \texttt{// Generate spatial feature by IFT}

% ${s}_j = \mathbb{F}^{-1}(\hat{s}_j) \in \mathbb{R}^{h \times w \times 1}$.  

% \EndFor
% \State \textbf{end for}

% ${S}_j = \mathcal{C}([{s}_1,{s}_2,\dots,{s}_c]) \in \mathbb{R}^{h \times w \times c }$.

% \EndFor
% \State \textbf{end for}

% \State \texttt {// Obtain decomposed features}

% $\textbf{\emph{S}}=\left\{S_1, S_2,\dots, S_q\right\}$. 
% % $\textbf{\emph{S}} \in \mathbb{R}^{H \times W \times M \times C}$

% \State \textbf{Return:}  $\textbf{\emph{S}}$.
% \end{algorithmic}
% \end{algorithm}


\subsubsection{Decoupler}
 The workflow of the \emph{Decoupler} module is illustrated in Fig. \ref{vision_SFIF}. To decompose the input feature $\mathcal{\bar{F}}_{t}$ into different frequency subbands, the input feature is first transformed to the frequency domain by FFT. The resulting frequency feature $\hat{\mathcal{F}}_{t} \in\mathbb{R}^{h \times w \times c}$ is then split along the channel dimension by $Split(\cdot)$ operation to obtain $c$ frequency channel features. Sequentially, the \emph{Decoupler} module generates $Q$ Gaussian band-pass filter masks $\emph{\textbf{M}}=\left\{M_{j}\right\}_{j=1}^Q, M_j \in\mathbb{R}^{h \times w \times 1}$. For each $M_{j}$, its truncation frequency $d_j$ is calculated based on the width $h$ and height $w$ of the input feature:
%
\begin{equation}
d_j = \frac{j\sqrt{(\frac{h}{2})^2+(\frac{w}{2})^2}}{Q},
\end{equation}
%
and $M_j $ is given by:
%
\begin{equation}
\begin{split}
M_j (u,v) = \exp \left(\frac{-\left[(u-h/2)^2+(v-w/2)^2\right]}{2 d_j^2}\right) \\
 -\sum_{l=1}^{j-1} \exp \left(\frac{-\left[(u-h/2)^2+(v-w/2)^2\right]}{2 d_l^2}\right).
\end{split} \label{BP_filter}
\end{equation}
 %
The frequency channel features are multiplied by each of these band-pass filter masks and then concatenated to obtain the decomposed frequency feature $\hat{S}_{q}$. Finally, feature $\hat{S}_{q}$ is transformed to the spatial domain through inverse FFT, producing the corresponding decomposed feature $S_q$.   




% and the frequency feature $\hat{\mathcal{F}}_{t} \in\mathbb{R}^{h \times w \times c}$ by FFT to obtain the decomposed features $\textbf{\emph{S}}$. The structure of the \emph{Decoupler} module is illustrated in Fig. \ref{vision_SFIF}. For each band-pass filter mask $M_{j}$, its truncation frequency $d_j$ is calculated based on the width $h$ and height $w$ of the input feature:
% %
% \begin{equation}
% d_j = \frac{j\sqrt{(\frac{h}{2})^2+(\frac{w}{2})^2}}{Q},
% \end{equation}
% %
% and $M_j $ is given by:
% %
% \begin{equation}
% \begin{split}
% M_j (u,v) = \exp \left(\frac{-\left[(u-h/2)^2+(v-w/2)^2\right]}{2 d_j^2}\right) \\
%  -\sum_{l=1}^{j-1} \exp \left(\frac{-\left[(u-h/2)^2+(v-w/2)^2\right]}{2 d_l^2}\right).
% \end{split} \label{BP_filter}
% \end{equation}
% %
% The frequency feature $\hat{\mathcal{F}}_{t}$ is then split along the channel dimension by $Split(\cdot)$ operation to obtain $c$ frequency channel features. These frequency channel features are respectively multiplied with the band-pass filter mask $M_q$ and then concatenated to obtain the decomposed frequency feature $\hat{S}_{q}$. Finally, feature $\hat{S}_{q}$ is converted into the spatial domain by the IFFT to obtain the corresponding decomposed feature $S_q$.  
% Specifically, frequency feature $\hat{\mathcal{F}}_{t}$ is split along channel domination by $Split(\cdot)$ operation and each frequency channel feature $\hat{\mathcal{F}}_{t}(u,v,g), g=1,\cdots,c$ is multiplied with $M_j$ to obtain the filtered frequency channel feature 
% \begin{equation}
% \hat{S}_{j} = \mathcal{C}(\left\{\mathcal{\hat{F}}_{t}(u,v,g)\right\}_{g=1}^c\cdot M_{j}).
% \end{equation}
% $\left\{\mathcal{\hat{F}}_{t}(u,v,g)\right\}_{g=1}^c$

\subsubsection{Enhancer}

To enhance the decomposed frequency feature $\emph{\textbf{S}}$ within each subband, the corresponding subset of $\emph{\textbf{S}}$, $\left\{S_j\right\}_{j=1}^q$, and the enhanced feature set $\left\{E_j\right\}_{j=1}^{q-1}$ from the lower subband are feb into the \emph{Enhancer} module for feature enhancement. The \emph{Enhancer} module consists of a feedforward enhancement (FFE) branch and a feedback enhancement (FBE) branch, both contain an enhancement block. As show in Fig. \ref{vision_SFIF}, in the FFE branch, the input feature subset, $\left\{S_j\right\}_{j=1}^q$, is summed together, and subtracted by the decomposed feature $S_{q}$ to obtain a high-frequency feature $S^{'}_{q}$. The enhanced feature set $\left\{E_j\right\}_{j=1}^{q-1}$ is summed together in the FBE branch, obtaining another high-frequency feature $S^{''}_{q}$. The sum of $S^{'}_{q}$ and $S^{''}_{q}$ is then input into the enhancement block, which consists of a 3$\times$3 convolution layer $\mathrm{Conv}$, a sigmoid activation function $\sigma$ and a channel attention ($\mathrm{CA}$)~\cite{zhang2018image}, to obtain the feedforward enhanced feature ${{E}}^{f}_{q}$. 

% The structure of the enhancement block is illustrated in Fig. \ref{vision_SFIF} and this process is described by:
% \begin{equation}
% {{E}}^{f}_{q} = \mathrm{CA}({S}_{q} + \gamma({S}_{q}\cdot \sigma(\mathrm{Conv}(\widetilde{S}_{q})))),
% \end{equation}
% \noindent
% where $\gamma$ is the proportion factor. 

In the FBE branch, 
% the enhanced features $\left\{E_j\right\}_{j=1}^{q-1}$  are summed together, i.e., $\sum_{j=1}^{q-1} E_{j}$, and summed feature denoted as an another high-frequency feature $S^{''}_{q}$. 
$S^{''}_{q}$ is also processed by the enhancement block to obtain the feedback enhanced feature $E^{b}_{q}$, which will then be combined with ${{E}}^{f}_{q}$ to produce the final enhanced feature $E_{q}$. It is noted that when $q$ = 1 (correponding to the lowest subband), we additionally apply a mean filter on $S_{q}$ which replaces $\left\{S_j\right\}_{j=1}^q$ as the input of the \emph{Enhancer} module and there is no FBE branch here.


\subsubsection{Aggregator}

To aggregate the enhanced frequency feature in each subband, we use the following equation to sum them together before applying a channel attention ($\mathrm{CA}$) to strengthen the interaction between feature channels:
%
\begin{equation}
\mathcal{\widetilde{F}}_{t} = \mathrm{CA}(\sum_{j=1}^{Q} E_{j}).
\end{equation}


Figure \ref{visionMFFR} provides a visualization of the intermediate results generated in the MFFR module. It can be observed that the resulting features at each stage exhibit the characteristics expected in the design - features corresponding to high-frequency subbands contain finer details, and vice versa.

\subsection{Reconstruction Module }

To generate an HR video from the refined feature $\mathcal{\widetilde{F}}_{t}$,  the scale-wise convolution block (SCB)~\cite{fan2020scale} with the residual-in-residual structure and a pixelshuffle layer are adopted to compose our reconstruction (REC) module. The REC module contains $R$ residual groups for information interaction.  Each residual group has three SCBs and a short skip connection. The output feature of $R$ residual groups is upsampled by a pixelshuffle layer to obtain the final HR residual frame ${\widehat{I}}_{t}$.






\subsection{Loss Functions}

The proposed model is optimized using the overall loss function $\mathcal{L}_{all}$ given below:
%
\begin{equation}
\mathcal{L}_{all} = \mathcal{L}_{spa} + \alpha  \mathcal{L}_{fc},
\end{equation}
%
where $\alpha$ is the weight factor, $ \mathcal{L}_{spa}$, $\mathcal{L}_{fc}$ are the spatial loss, frequency-aware contrastive loss, respectively, and their definition are provided below.

\subsubsection{Spatial Loss}

The Charbonnier loss function~\cite{lai2018fast} is adopted as our spatial loss function for supervising the generation of SR results in the spatial domain:
%
\begin{equation}
\mathcal{L}_{spa}=\sqrt{\left\|I_{t}^{\mathrm{HR}}-I_{t}^{\mathrm{SR}}\right\|^{2}+\epsilon^{2}},
\end{equation}
%
in which $I_{t}^{\mathrm{HR}}$ is the uncompressed HR frame and the penalty factor $\epsilon$ is set to $1 \times 10^{-4}$.


\begin{figure}[!t]
\centering
\includegraphics[width=0.495\textwidth]{figures/Loss_pipe2.pdf}
\caption{The loss functions used for training the FCVSR model.}
\label{Fig_loss}
\end{figure}

\subsubsection{Frequency-aware Contrastive Loss}
% Based on the observation of the frequency band of compressed frames in Fig. \ref{vision_freSpec}, 
The frequency-aware contrastive loss is designed based on the 2D discrete wavelet transform (DWT) to differentiate positive samples and negative samples. Given a training group with an bi-linearly upsampled compressed image $I_i^{\mathrm{UP}}$, the corresponding uncompressed HR image $I_i^{\mathrm{HR}}$  and the restored SR image $I_i^{\mathrm{SR}}$, 2D-DWT decomposes each of them into four frequency subbands: LL, HL, LH and HH. Two positive sets are defined as $\mathcal{P}^{1}_{i}$ = $\left\{I_i^{\mathrm{HR(HH)}}, I_i^{\mathrm{HR(HL)}}, I_i^{\mathrm{HR(LH)}} \right\}$ and $\mathcal{P}^{2}_{i}$ = $\left\{I_i^{\mathrm{HR(LL)}}, I_i^{\mathrm{UP(LL)}} \right\}$,  while one negative set is demoted as $\mathcal{N}_{i}$ = $\left\{I_i^{\mathrm{UP(HH)}}, I_i^{\mathrm{UP(HL)}}, I_i^{\mathrm{UP(LH)}}\right\}$. Two anchor sets $\mathcal{A}^{1}_{i}$ = $\left\{I_i^{\mathrm{SR(HH)}},  I_i^{\mathrm{SR(HL)}}, I_i^{\mathrm{SR(LH)}}\right\}$, $\mathcal{A}^{2}_{i}$ = $\left\{I_i^{\mathrm{SR(LL)}}\right\}$ are constructed. Based on these definitions, two frequency-aware contrastive losses for the $i$-th train group are:
%
\begin{equation}
\mathcal{L}^{1}_{i}=-\frac{1}{G^{1}_p} \sum_{l=1}^{G^{1}_p}\log \frac{\exp (s(a^{1}_l, p^{1}_l) / \tau) }{\exp (s(a^{1}_l, p^{1}_l) / \tau)+\sum_{k=1}^{G_g} \exp (s(a^{1}_l, g_k) / \tau)},
\end{equation}
%
\begin{equation}
\mathcal{L}^{2}_{i}=- \frac{1}{G^{2}_p} \sum_{l=1}^{G^{2}_p}\log \frac{\exp (s(a^{2}, p^{2}_l) / \tau) }{\exp (s(a^{2}, p^{2}_l) / \tau)+\sum_{k=1}^{G_g} \exp (s(a^{2}, g_k) / \tau)},
\end{equation}
%
where $G^{1}_{p}$, $G^{2}_{p}$ and $G_{g}$ are the number of sets $\mathcal{P}^{1}$, $\mathcal{P}^{2}$ and $\mathcal{N}$, $\tau$ is the temperature parameter and $s(\cdot ,\cdot)$ is the similarity function.
$a$, $p$, and $g$ represent the anchor, positive, and negative samples, respectively.

The total frequency-aware contrastive loss is defined as:
%
\begin{equation}
\mathcal{L}_{fc} = \frac{1}{M_s} \sum_{i=1}^{M_s} ( \mathcal{L}_i^{1}+\mathcal{L}_i^{2}),
\end{equation}
%
where $M_s$ is the number of samples.
%  Both bands are included in $\mathcal{L}_i^{+}$, since both high and low frequencies of $I_i^{\mathrm{SR}}$ are expected to be pulled closer to $I_i^{\mathrm{GT}}$. Only high frequency is taken for $\mathcal{L}_i^{-}$to push the $f^h\left(I_i^{\mathrm{SR}}\right)$ away from $f^h\left(I_i^{\mathrm{UP}}\right)$ as compressed degradation mainly happens in the high-frequency parts.


% \subsubsection{Energy-based Temporal Consistency Loss}

%To ensure temporal consistency of super-resolved videos, an energy-based temporal consistency loss, $\mathcal{L}_{etc}$, is also designed, which assesses the frequency energy difference between neighboring frames in restored frames and uncompressed frames. 
%
% \begin{equation}
% \mathcal{L}_{etc} =\sum_{i=-5}^{6}(\mathcal{E}_{F}(I_{i}^{\mathrm{HR}})-\mathcal{E}_{F}(I_{i-1}^{\mathrm{HR}}))-\sum_{i=-5}^{6}(\mathcal{E}_{F}(I_{i}^{\mathrm{SR}})-\mathcal{E}_{F}(I_{i-1}^{\mathrm{SR}})),
% \end{equation}
% %
% where $\mathcal{E}_{F}(\cdot)$ is the energy function:
% %
% \begin{equation} \label{ene_f}
% \mathcal{E}_{F}(I)= \sum_{u=0}^{I_{h}-1} \sum_{v=0}^{I_{w}-1}|{I_{F}}(u, v)|^2.
% \end{equation}
% %
% Here ${I_{F}}$ is the frequency spectrum of image $I$.



\begin{table*}[!t]
\caption{Quantitative comparison in terms of  PSNR (dB), SSIM and VMAF on three public testing datasets under LDB configuration. The FLOPs is calculated on LR video frame with 64 $\times$ 64 resolution and FPS is calculated on REDS4 dataset. The best and the second-best results are highlighted and underlined.} \label{tab_PSNR}
\setlength{\tabcolsep}{1.0mm}
\renewcommand\arraystretch{1.2}
\fontsize{7}{9}\selectfont
\centering
\begin{tabular}{c|r|rrr|c|c|c|c}
\toprule
% [0.2mm]
% \toprule[0.2mm]
{\multirow{2}{*}{{Datasets} }} &  {\multirow{2}{*}{ {Methods} }} & \multicolumn{1}{c}{Param.}$\downarrow$& \multicolumn{1}{c}{FLOPs}$\downarrow$ & \multicolumn{1}{c|}{FPS$\uparrow$}  & \multicolumn{1}{c|}{{QP = 22}} & \multicolumn{1}{c|}{{QP = 27}} & \multicolumn{1}{c|}{{QP = 32}}  & \multicolumn{1}{c}{{QP = 37}}  \\
% \cmidrule(r){6-6}  \cmidrule(r){7-7} \cmidrule(r){8-8}    \cmidrule(r){9-9} 
&   & \multicolumn{1}{c}{(M)} & \multicolumn{1}{c}{(G)}  & \multicolumn{1}{c|}{(1/s)} & {PSNR}$\uparrow$ / {SSIM}$\uparrow$ / VMAF$\uparrow$  & {PSNR}$\uparrow$ / {SSIM}$\uparrow$ / VMAF$\uparrow$ & {PSNR}$\uparrow$ / {SSIM}$\uparrow$ / VMAF$\uparrow$ & {PSNR}$\uparrow$ / {SSIM}$\uparrow$ / VMAF$\uparrow$ \\
\midrule
&  EDVR-L~\cite{wang2019edvr} & 20.69   & 354.07  & 2.02    & 31.76 / 0.8629 / 68.23   & 30.58 / {0.8377} / 56.39  & {29.07} / 0.8045 / 41.72  & 27.38 / 0.7670 / 25.53   \\
% &  CD-VSR {\cite{chen2021compressed}} & 7    & 31.82 / 0.8637 / 77.66    & 30.59 / 0.8379  / 65.24   & {29.13} / 0.8055  / 49.86  & 27.41 / 0.7680 / 33.58   \\
&  {BasicVSR}~\cite{chan2021basicvsr} & \underline{6.30}  & 367.72  & 0.85    & 31.80 / 0.8631  / 76.44  & 30.46 / 0.8349 / 65.14   & 29.05 / 0.8031 / 47.06   & 27.33 / 0.7661 / 29.59    \\
\multirow [c]{2}{*}{\rotatebox{90}{CVCP~\cite{chen2021compressed} }}
&  {IconVSR}~\cite{chan2021basicvsr} & 8.70  & 576.45  & 0.51    & 31.86 / 0.8637 / 77.94  & 30.48 / 0.8354 /  64.69    & 29.10 / 0.8043 / 47.77   & 27.40 / 0.7678 / 30.05   \\
&  {BasicVSR++}~\cite{chan2022basicvsr++} & 7.32  & 395.69  & 0.74   & {31.89} / {0.8647} / 77.55    & {30.66} / {0.8388} / 66.43    & {29.13} / {0.8058} / 50.08    & {27.43} / {0.7682} / 34.11     \\
&  {FTVSR++}~\cite{qiu2023learning} & 10.80   & 1148.85  & 0.27  &  \underline{31.92 / 0.8656 / 78.52}    & \underline{30.69 / 0.8393 / 66.89}  & \underline{29.14 / 0.8063 / 51.96}    &  \underline{27.44} / 0.7697 / 35.06  \\
\cmidrule{2-9}
&  {FCVSR-S (ours)}  & \textbf{3.70}  & \textbf{68.82}  & \textbf{5.28}   & 31.86 / 0.8650 / 78.27 &  30.64 / 0.8388 / 65.96  & 29.10 / 0.8058  / 51.39  & \underline{27.44 / 0.7700 / 35.07}   \\
&  {FCVSR (ours)}  & 8.81  & \underline{165.36}  & \underline{2.39}    & \textbf{31.94 / 0.8669  / 78.69}  & \textbf{30.70 / 0.8403 / 66.97}  &  \textbf{29.18 / 0.8077 / 52.03}   &  \textbf{27.46 / 0.7704 / 35.63} \\
% &  {FCVSR(ours)} &   13   & \textbf{32.00 / 0.8679 / 79.80} & \textbf{30.76 / 0.8411 / 67.29} &   \textbf{29.22 / 0.8090 / 52.27} & \textbf{27.48 / 0.7713 / 35.93} \\
\midrule
&  EDVR-L~\cite{wang2019edvr} & 20.69   & 354.07  & 2.02   & 29.05 / 0.7991 / 81.60 & 27.60 / 0.7470 / 59.90   &  26.40 / 0.7072 / 46.31   &   24.87 / 0.6585 / 28.80  \\
&  {BasicVSR}~\cite{chan2021basicvsr} & \underline{6.30}  & 367.72  & 0.85  & 29.13 / 0.8005 / 81.13  & 27.62 / 0.7512 / 63.49 & 26.43 / 0.7079 / 46.82   & 24.99 / 0.6603 / 29.49 \\
\multirow [c]{2}{*}{\rotatebox{90}{REDS~\cite{nah2019ntire}} }
&  IconVSR~\cite{chan2021basicvsr}  & 8.70  & 576.45  & 0.51   & 29.17 / 0.8009 / 81.52  & 27.73 / 0.7519 / 62.91  & 26.45 / 0.7090 / 47.48  & 24.99 / 0.6609 / 29.73 \\
&   {BasicVSR++}~\cite{chan2022basicvsr++} & 7.32  & 395.69  & 0.74   &   29.23 / 0.8036 / 81.83  & 27.79 / 0.7543 / 63.63  & 26.50 / 0.7098 / 47.78   &  25.05 / 0.6620 / 31.25 \\
& {FTVSR++}~\cite{qiu2023learning} & 10.80   & 1148.85  & 0.27  &  \underline{29.26 / 0.8029 / 81.58} &  \underline{27.81 / 0.7564 / 65.22} &  \underline{26.53 / 0.7106 / 48.57}  &  \underline{25.09 / 0.6625 / 31.81}  \\
\cmidrule{2-9}
&  {FCVSR-S (ours)}  & \textbf{3.70}  & \textbf{68.82}  & \textbf{5.28}   &   29.14 / 0.8002 / 81.18   &  27.66 / 0.7505 / 63.14   &  26.42 / 0.7089 / 47.75   &  24.93 / 0.6611 / 31.56 \\
&  {FCVSR (ours)}  & 8.81  & \underline{165.36}  & \underline{2.39}   &  \textbf{29.28 / 0.8039 / 81.87}   & \textbf{27.92 / 0.7591  / 65.63}   & \textbf{26.64 / 0.7161  / 48.59}   &   \textbf{25.20 / 0.6694 / 32.05} \\
% &  {FCVSR(ours)}  & 13     & \textbf{29.30 / 0.8043 / 81.95}  & \textbf{27.99 / 0.7603 / 66.00} &   \textbf{26.73 / 0.7167 / 48.72}  &   \textbf{25.30 / 0.6706 / 32.46}  \\
\midrule
&  EDVR-L~\cite{wang2019edvr}  & 20.69   & 354.07  & 2.02  & 25.27 / 0.7135 / 66.57  &   24.31 / 0.6586 / 52.82   & 23.29 / 0.5958 / 34.74  & 22.09 / 0.5284 / 20.43  \\
\multirow [c]{2}{*}{\rotatebox{90}{Vimeo-90K~\cite{xue2019video}  } }
&  BasicVSR~\cite{chan2021basicvsr}  & \underline{6.30}  & 367.72  & 0.85  & 25.30 / 0.7155 / 67.23  &  24.36 / 0.6610 / 52.69    & 23.34 / 0.5989 / 35.51  &   22.15 / 0.5314 / 20.52 \\
&  {IconVSR}~\cite{chan2021basicvsr}  & 8.70  & 576.45  & 0.51 & 25.46 / 0.7225 / 68.77   & 24.41 / 0.6638 / 52.88  &  23.36 / 0.5993 / 35.53 &   22.16 / 0.5305 / 20.41  \\
&  {BasicVSR++}~\cite{chan2022basicvsr++}& 7.32  & 395.69  & 0.74   & 25.55 / 0.7270 / 70.35   &  24.43 / 0.6639 / \underline{53.93}  & 23.37 / 0.5976 / 35.30  & 22.18 / 0.5326 / 20.60  \\
&  {FTVSR++}~\cite{qiu2023learning} & 10.80   & 1148.85  & 0.27 & \underline{25.58 / 0.7278 / 70.68}  & \underline{24.44 / 0.6657} / 53.53   &   23.39 / \underline{0.6024} / 36.16 &  \underline{22.20} / 0.5338 / 20.90 \\
\cmidrule{2-9}
&  {FCVSR-S (ours)}  & \textbf{3.70}  & \textbf{68.82}  & \textbf{5.28}   & 25.35 / 0.7194 / 68.36  & 24.43 / 0.6647 / 53.50    & \underline{23.40} / 0.6021 / \underline{36.25}    &   22.19 / \underline{0.5340 / 21.08} \\
&  {FCVSR (ours)}  & 8.81  & \underline{165.36}  & \underline{2.39}   & \textbf{25.61 / 0.7307 / 71.50}  & \textbf{24.58 / 0.6707 / 54.79} &  \textbf{23.47 / 0.6052 / 37.20}  & \textbf{22.25 / 0.5366 / 21.60} \\
\bottomrule	
\end{tabular}
\end{table*}




% \begin{table}[t]
% \caption{Comparison of different methods in terms of model parameters, FLOPs and FPS on REDS4 dataset. The FLOPs is calculated on LR video frame with 64 $\times$ 64 resolution. The best and the second-best results are highlighted and underlined \label{Tab_inference}} 
% \setlength{\tabcolsep}{2.70mm}
% \fontsize{7}{9}\selectfont
% \centering
% \begin{tabular}{lccccccc}
% \midrule[0.2mm]
% \midrule[0.2mm]
% {\multirow{2}{*} {{Methods}}} &\multicolumn{1}{c}{Param.$\downarrow$}  &\multicolumn{1}{c}{FLOPs$\downarrow$}    &\multicolumn{1}{c}{PSNR$\uparrow$}  & \multicolumn{1}{c}{{FPS}$\uparrow$} \\
% % \cline { 5-7 } 
% & (M) & (G) & (dB) & (720P) \\  %  &1080P &WQXGA
% \midrule[0.2mm]
% EDVR-L~\cite{wang2019edvr}  & 20.69 & 354.07  & 24.87   & 2.02 \\  % & 1.12 & 0.55 \\
% % CD-VSR~\cite{chen2021compressed}   & 8.88  & \underline{143.51} & 31.82  & \underline{5.16} &  \underline{3.51}  & \underline{1.90} \\
% BasicVSR~\cite{chan2021basicvsr}  & \underline{6.30}  & 367.72  & 24.99  & 0.85  \\ % & 0.73 & 0.21 \\
% IconVSR~\cite{chan2021basicvsr} & 8.70 & 576.45 & 24.99  & 0.51   \\ % & 0.24 & 0.12 \\
% BasicVSR++~\cite{chan2022basicvsr++}  & 7.32 & 395.69  & 25.05 & 0.74  \\ %  & 0.35 & 0.16 \\
% FTVSR++~\cite{qiu2023learning}  & 10.80 & 1148.85  & \underline{25.09} & 0.27  \\  %  & 0.10  & 0.05 \\
% \midrule[0.2mm]
% {FCVSR-S} & \textbf{3.70}  & \textbf{68.82}  & 24.93 & \textbf{5.28}   \\ % & \textbf{3.64} & \textbf{2.08} \\
% {FCVSR} & 8.81 & \underline{165.36}  & \textbf{25.20} & \underline{2.39}  \\  % & \underline{1.14} & \underline{0.57} \\
% % {FCVSR}  & 8.81 & 1137.57  & \textbf{32.00} & {0.34} & {0.26} & {0.10} \\
% \midrule[0.2mm]
% \midrule[0.2mm]
% \end{tabular}
% \end{table}



\section{Experiment and Results} \label{EXP}


\subsection{Implementation Details}
In this work, an FCVSR model and its lightweight model, i.e., FCVSR-S, are proposed for the compressed VSR task. The FCVSR model employs the following hyper parameters and configurations: the number of the adaptive convolutions in the MGAA module is set as $N$ = 6; the decomposition number in the MFFR module is set as $Q$ = 8; the number of residual groups in the REC module is set as $R$ = 10. The FCVSR-S model is associated with lower computational complexity. Its number of the adaptive convolutions in the MGAA module is set as $N$ = 4, the decomposition number of the decoupler is set as $Q$ = 4 in the MFFR module, and the number of residual groups is set as $R$ = 3 in the REC module. 
These two models all take 7 frames as the model input and use the overall loss function to train them. The weight factor of the overall loss function $\alpha$ is set to 1. The $L_{1}$ distance is adopted as the similarity function $s(\cdot)$ and the temperature parameter $\tau$ is 1 in $\mathcal{L}_{fc}$. The compressed frames are cropped into 128$\times$128 patches and the batch size is set to 8. Random rotation and reflection operations are adopted to increase the diversity of training data. The proposed models are implemented based on PyTorch and trained by Adam~\cite{kingma2014adam}. The learning rate is initially set to 2$\times$$10^{-4}$ and gradually halves at 2K, 8K and 12K epochs. The total number of epochs is 30K. All experiments are conducted on PCs with RTX-3090 GPUs and Intel Xeon(R) Gold 5218 CPUs.

\subsection{Experimental Setup}

%Recently, some datasets \cite{ma2021bvi,nawala2024bvi,chen2021compressed,nah2019ntire,xue2019video} were proposed for application of deep compressed super-resolution. 
Following the common practice in the previous works~\cite{chan2021basicvsr,chan2022basicvsr++,qiu2023learning}, our models are trained separately on three public training datasets, CVCP~\cite{chen2021compressed}, REDS~\cite{nah2019ntire} and Vimeo-90K~\cite{xue2019video}, and evaluated their corresponding test sets, CVCP10~\cite{chen2021compressed}, REDS4~\cite{nah2019ntire}, and Vid4~\cite{xue2019video} respectively. 
% It is noted that each sequence in Vimeo-90K only contains 7 frames. As a result, only FCVSR-S and FCVSR models are benchmarked on them. 
The downsampled LR videos are generated using a Bicubic filter with a factor 4. All training and test compressed videos are created using the downsampling-then-encoding procedure and compressed by HEVC HM 16.20~\cite{peixoto2013h} under the Low Delay B mode with four different QP values: 22, 27, 32 and 37. 

% As~\cite{chen2021compressed} did, 10 standard testing videos, named as \emph{CVCP10} testing dataset. Following previous works\cite{chan2021basicvsr,chan2022basicvsr++}, 263 videos are adopted for training model and four videos are used for evaluation, i.e., \emph{Clips} \emph{000}, \emph{011}, \emph{015}, \emph{020}, termed as \emph{REDS4}, to evaluate the performance of models along with REDS dataset. Vid4 dataset is applied to evaluate the performance of VSR model on Vimeo-90K dataset. 


% \emph{CVCP:} CVCP dataset~\cite{chen2021compressed} is a compressed video super-resolution training dataset, which includes 589 training video sequences and each of them is an independent shot with 32 frames.  As~\cite{chen2021compressed} did, 10 standard testing videos, named as \emph{CVCP10} testing dataset,  with three different resolutions, i.e., 2560$\times$1600 (WQXGA),  1920$\times$1080 (1080P), and 1280$\times$720 (720P), from common test conditions~\cite{bossen2013common} of JCT-VC~\cite{ohm2012comparison} are used as our testing videos for evaluating the performance of the models along with CVCP dataset. 

% \emph{REDS:} REDS  dataset~\cite{nah2019ntire} is a public video super-resolution training dataset. The REDS dataset includes 270 original videos and each video consists of 100 frames with 1280$\times$720 resolution. Following previous works\cite{chan2021basicvsr,chan2022basicvsr++}, 263 videos are adopted for training model and  four videos are used for evaluation, i.e., \emph{Clips} \emph{000}, \emph{011}, \emph{015}, \emph{020}, termed as \emph{REDS4}, to evaluate the performance of models along with REDS dataset. 

% \emph{Vimeo-90K:} There are 64,612 training videos in the Vimeo-90K dataset\cite{xue2019video}, in which each video contains 7 frames with 448$\times$256 resolution. For evaluation, 4 videos are used as previous works\cite{wang2019edvr,chan2021basicvsr,chan2022basicvsr++}, i.e., \emph{calendar}, \emph{city}, \emph{foliage}, \emph{walk}, termed as \emph{Vid4} dataset. Each video in the Vid4 dataset contains 30 to 50 frames.





The peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) ~\cite{wang2004image}, and video multi-method assessment fusion (VMAF)~\cite{li2018vmaf} are adopted as evaluation metrics for the quantitative benchmark. PSNR and SSIM were widely used to evaluate the quality of videos while VMAF was proposed by Netflix to evaluate the perceptual quality of videos. We also measured the model complexity in terms of the floating point operations (FLOPs), inference speed (FPS) and the number of model parameters.




\begin{figure*}[!t]
\centering
\begin{minipage}[b]{0.22\linewidth}
\centering
\centerline{\epsfig{figure=FourPeople_LR_QP22_00011.png,width=4.05cm}}
\footnotesize{\emph{CVCP10\_FourPeople\_011}(QP=22)} 
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=FourPeople_GT_00011.png,width=2.25cm}}
\footnotesize{GT  } 
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=FourPeople_IconVSR_QP22_00011.png,width=2.25cm}}
\footnotesize{  IconVSR  }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=FourPeople_BasicVSRPP_QP22_00011.png,width=2.25cm}}
\footnotesize{ BasicVSR++ }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=FourPeople_FTVSR_QP22_00011.png,width=2.25cm}}
\footnotesize{  FTVSR++ }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=FourPeople_FCVSR_S_QP22_00011.png,width=2.25cm}}
\footnotesize{  {FCVSR-S}}
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=FourPeople_FCVSR_QP22_00011.png,width=2.25cm}}
\footnotesize{ {FCVSR}  }
\end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=FourPeople_FCVSR_ETC_QP22_00011.png,width=1.96cm}}
% \footnotesize{  {FCVSR} \\ \ }
% \end{minipage}

% \begin{minipage}[b]{0.190\linewidth}
% \centering
% \centerline{\epsfig{figure=BasketballDrive_LR_QP27_00106.png,width=3.50cm}}
% \footnotesize{(*) \emph{CVCP10\_Basketball\_106} \\ (QP=22)} 
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=BasketballDrive_GT_00106.png,width=1.96cm}}
% \footnotesize{ GT \\ \ } 
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=BasketballDrive_IconVSR_QP27_00106.png,width=1.96cm}}
% \footnotesize{ IconVSR \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=BasketballDrive_BasicVSRPP_QP27_00106.png,width=1.96cm}}
% \footnotesize{ BasicVSR++ \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=BasketballDrive_FTVSR_QP27_00106.png,width=1.96cm}}
% \footnotesize{ FTVSR++ \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=BasketballDrive_FCVSR_S_QP27_00106.png,width=1.96cm}}
% \footnotesize{ {FCVSR-S} \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=BasketballDrive_FCVSR_QP27_00106.png,width=1.96cm}}
% \footnotesize{ {FCVSR} \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=BasketballDrive_FCVSR_ETC_QP27_00106.png,width=1.96cm}}
% \footnotesize{{FCVSR} \\ \ }
% \end{minipage}



\begin{minipage}[b]{0.22\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_LR_QP27_000_00000024.png,width=4.05cm}}
\footnotesize{\emph{REDS4\_011\_019} (QP=27)  } 
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_GT_000_00000024.png,width=2.25cm}}
\footnotesize{GT } 
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_IconVSR_QP27_000_00000024.png,width=2.25cm}}
\footnotesize{  IconVSR  }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_BasicVSRPP_QP27_000_00000024.png,width=2.25cm}}
\footnotesize{  BasicVSR++  }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_FTVSR_QP27_000_00000024.png,width=2.25cm}}
\footnotesize{  FTVSR++ }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_FCVSR_S_QP27_000_00000024.png,width=2.25cm}}
\footnotesize{  {FCVSR-S} }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_FCVSR_QP27_000_00000024.png,width=2.25cm}}
\footnotesize{ {FCVSR} }
\end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_FCVSR_ETC_QP27_000_00000024.png,width=1.96cm}}
% \footnotesize{  {FCVSR} \\ \ }
% \end{minipage}

% \begin{minipage}[b]{0.190\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_LR_QP32_015_00000045.png,width=3.50cm}}
% \footnotesize{\emph{REDS4\_015\_045} \\  (QP=32)  } 
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_GT_00000045.png,width=1.96cm}}
% \footnotesize{GT  \\ \ } 
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_IconVSR_QP32_015_00000045.png,width=1.96cm}}
% \footnotesize{  IconVSR \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_BasicVSRPP_QP32_015_00000045.png,width=1.96cm}}
% \footnotesize{  BasicVSR++ \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_FTVSR_QP32_015_00000045.png,width=1.96cm}}
% \footnotesize{  FTVSR++ \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_FCVSR_S_QP32_015_00000045.png,width=1.96cm}}
% \footnotesize{  {FCVSR-S} \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_FCVSR_QP32_015_00000045.png,width=1.96cm}}
% \footnotesize{ {FCVSR} \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=REDS4_FCVSR_ETC_QP32_015_00000045.png,width=1.96cm}}
% \footnotesize{  {FCVSR} \\ \ }
% \end{minipage}

\begin{minipage}[b]{0.22\linewidth}
\centering
\centerline{\epsfig{figure=Vid4_LR_Calendar_QP32_00000020.png,width=4.050cm,height=3.250cm}}
\footnotesize{\emph{Vid4\_Calendar\_020} (QP=32)  }  
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=Vid4_GT_Calendar_QP32_00000020.png,width=2.25cm}}
\footnotesize{GT  } %\medskip
\end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=Vid4_BasicVSR_Calendar_QP32_00000020.png,width=1.96cm}}
% \footnotesize{ {BasicVSR} \\  \ }
% \end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=Vid4_IconVSR_Calendar_QP32_00000020.png,width=2.25cm}}
\footnotesize{  IconVSR  }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=Vid4_BasicVSRPP_Calendar_QP32_00000020.png,width=2.25cm}}
\footnotesize{ BasicVSR++  }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=Vid4_FTVSR_Calendar_QP32_00000020.png,width=2.25cm}}
\footnotesize{ FTVSR++ }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=Vid4_FCVSR_S_Calendar_QP32_00000020.png,width=2.25cm}}
\footnotesize{ {FCVSR-S} }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=Vid4_FCVSR_Calendar_QP32_00000020.png,width=2.25cm}}
\footnotesize{ {FCVSR} }
\end{minipage}

% \begin{minipage}[b]{0.190\linewidth}
% \centering
% \centerline{\epsfig{figure=Vid4_LR_Walk_QP37_00000014.png,width=3.50cm}}
% \footnotesize{\emph{Vid4\_Walk\_014} \\ (QP=37)\ }  
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=Vid4_GT_Walk_00000014.png,width=1.96cm}}
% \footnotesize{GT \\ \ }
% \end{minipage}
% % \begin{minipage}[b]{0.105\linewidth}
% % \centering
% % \centerline{\epsfig{figure=Vid4_BasicVSR_Walk_QP37_00000014.png,width=1.96cm}}
% % \footnotesize{ {BasicVSR} \\  \ }
% % \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=Vid4_IconVSR_Walk_QP37_00000014.png,width=1.96cm}}
% \footnotesize{  IconVSR \\ \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=Vid4_BasicVSRPP_Walk_QP37_00000014.png,width=1.96cm}}
% \footnotesize{ BasicVSR++ \\  \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=Vid4_FTVSR_Walk_QP37_00000014.png,width=1.96cm}}
% \footnotesize{ FTVSR++ \\  \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=Vid4_FCVSR_S_Walk_QP37_00000014.png,width=1.96cm}}
% \footnotesize{ {FCVSR-S} \\  \ }
% \end{minipage}
% \begin{minipage}[b]{0.105\linewidth}
% \centering
% \centerline{\epsfig{figure=Vid4_FCVSR_Walk_QP37_00000014.png,width=1.96cm}}
% \footnotesize{ {FCVSR} \\ \ }
% \end{minipage}


\begin{minipage}[b]{0.22\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_LR_020_069.png,width=4.05cm}}
\footnotesize{\emph{REDS4\_020\_069} (QP=37)\ }  
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_GT_020_069.png,width=2.25cm}}
\footnotesize{GT  }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_IconVSR_020_069.png,width=2.25cm}}
\footnotesize{  IconVSR }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_BasicVSRPP_020_069.png,width=2.25cm}}
\footnotesize{ BasicVSR++}
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_FTVSRPP_020_069.png,width=2.25cm}}
\footnotesize{ FTVSR++ }
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_FCVSR_S_020_069.png,width=2.25cm}}
\footnotesize{ {FCVSR-S}}
\end{minipage}
\begin{minipage}[b]{0.12\linewidth}
\centering
\centerline{\epsfig{figure=REDS4_FCVSR_020_069.png,width=2.25cm}}
\footnotesize{ {FCVSR} }
\end{minipage}
\caption{Visual comparison results between FCVSR models and three benchmark methods.}
\label{vision_RST}
\end{figure*}



Five state-of-the-art methods including EDVR-L~\cite{wang2019edvr}, BasicVSR~\cite{chan2021basicvsr}, IconVSR~\cite{chan2021basicvsr},  BasicVSR++~\cite{chan2022basicvsr++} and FTVSR++~\cite{qiu2023learning} are benchmarked against the proposed models.  To ensure a fair comparison, we retrained EDVR-L~\cite{wang2019edvr}, BasicVSR~\cite{chan2021basicvsr},  IconVSR~\cite{chan2021basicvsr}, BasicVSR++~\cite{chan2022basicvsr++} and FTVSR++~\cite{qiu2023learning} following the same training-evaluation procedure as FCVSR model, using their publicly released source code. 

\subsection{Comparison with State-of-the-Art VSR methods}

The quantitative results of our models for three training-test sets are summarized in Table \ref{tab_PSNR}. It can be observed that our FCVSR model achieves the best super-resolution performance in terms of all three quality metrics and for all QP values, compared with five State-of-the-Art (SoTA) VSR models. The FCVSR-S model also offers second-best results compared to other benchmarks in a few cases.

To comprehensively demonstrate the effectiveness of our models, visual comparison results have been provided in Fig. \ref{vision_RST}, in which example blocks generated by FCVSR models are compared with those produced by IconVSR, BasicVSR++ and FTVSR++. It is clear in these examples that our results contain fewer artifacts and finer details compared to other benchmarks. 





% \begin{figure}[!t]
% 	\centering
% 	\centerline{\epsfig{figure=PSNRflu_foliageQP22.pdf,width=11.0cm}}
% 	\footnotesize{ (a) \emph{Foliage} on Vid4 at QP=22. } 
% 	\centerline{\epsfig{figure=PSNRflu_REDS4_011QP27.pdf,width=11.0cm}}
% 	\footnotesize{ (b) \emph{Clip 011} on REDS4 at QP=27. } 
% 	\centerline{\epsfig{figure=PSNRflu_TrafficQP32.pdf,width=11.0cm}}
% 	\footnotesize{ (c) \emph{Traffic} on CVCP10 at QP=32. } 	\centerline{\epsfig{figure=PSNRflu_PeopleOnStreetQP37.pdf,width=11.0cm}}
% 	\footnotesize{ (d) \emph{PeopleOnStreet} on CVCP10 at QP=37. } 
% 	\caption{Comparison of quality fluctuation on three different testing datasets.}
% 	\label{vision_tp}
% \end{figure}



% \subsubsection{Quality fluctuation comparison}
% To evaluate the quality fluctuation performance, we show the comparison between our FCVSR-S, FCVSR models and two state-of-the-art methods, i.e., BasicVSR++ and FTVSR++, in Vid4 dataset in Fig. \ref{vision_tp} (a). Besides, we show the comparison of the quality fluctuation between our FCVSR-S, FCVSR and FCVSR-ETC models and BasicVSR++, FTVSR++ for three different testing sequences in Fig. \ref{vision_tp} (b)-(d). It is found from Fig. \ref{vision_tp} that our models can achieve the better quality fluctuation.

The results of the model complexity comparison in terms of model parameters, FLOPs, and FPS for all the models tested are provided in Table \ref{tab_PSNR}. Here, inference speed (FPS) is based on the REDS4 dataset. Among all the VSR methods, our FCVSR-S model is associated with the lowest model complexity based on three complexity measurements. The complexity-performance trade-off can also be illustrated by Fig. \ref{fig:performance_vs_complexity}, in which all FCVSR models are all above the Pareto front curve formed by five benchmark methods. This confirms the practicality of the proposed FCVSR models.



% \begin{table}[t]
% 	\caption{Verify the effectiveness of the MGAA and MFFR modules} \label{Tab_module}
% 	\setlength{\tabcolsep}{0.65mm}
% 	\fontsize{7}{9}\selectfont
% 	\centering
% 	\begin{tabular}{lcccccccc}
% 		\midrule[0.2mm]
% 		\midrule[0.2mm]
% 		{\multirow{2}{*}{{Models}}} & \multicolumn{2}{c}{MGAA} & \multicolumn{2}{c}{MFFR}  & {\multirow{2}{*} {PSNR(dB)$\uparrow$/SSIM$\uparrow$/VMAF$\uparrow$}} & \multicolumn{1}{c}{Param.$\downarrow$}  & \multicolumn{1}{c} {FLOPs$\downarrow$}  & \multicolumn{1}{c}{FPS$\uparrow$} \\
% 		\cline{2-5}  
% 		&ME  &AC   &FFE  &FBE  &   & (M)  & (G)  &  (720P)\\
% 		\midrule[0.2mm]
% 		\emph{Model-1} & - & -  & - & - & 24.93 / 0.6600 / 29.45 & 7.64   & 149.23 & 5.13  \\
% 		\emph{Model-2} & - & \Checkmark  & - & - &  25.01 / 0.6609 / 30.16 & 7.96   & 156.68 & 3.54 \\
% 		\emph{Model-3} & \Checkmark & \Checkmark & -  & - &  25.10 / 0.6630 / 31.45 & 8.20  & 159.57 & 3.02  \\
% 		\emph{Model-4} & \Checkmark & \Checkmark & \Checkmark & - &  25.16 / 0.6668 / 31.95  & 8.81  & 165.36  & 2.68 \\
% 		\emph{Model-5} & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \textbf{25.20 / 0.6694 / 32.05}   & 8.81  & 165.36  & 2.39 \\
% 		\midrule[0.2mm]
% 		\midrule[0.2mm]
% 	\end{tabular}
% \end{table}



\begin{table}[t]
\caption{Ablation study results for the proposed FCVSR model.} \label{Tab_module}
\setlength{\tabcolsep}{1.40mm}
\fontsize{7}{9}\selectfont
\centering
\begin{tabular}{lcccccccc}
\toprule
{\multirow{2}{*}{{Models}}}  & {\multirow{2}{*} {PSNR(dB)$\uparrow$/SSIM$\uparrow$/VMAF$\uparrow$}} & \multicolumn{1}{c}{Param.$\downarrow$}  & \multicolumn{1}{c} {FLOPs$\downarrow$}  & \multicolumn{1}{c}{FPS$\uparrow$} \\
% \cline{3-5}  
&   & (M)  & (G)  &  (1/s)\\
\midrule
(v1.1) w/o MGAA  & 25.04 / 0.6615 / 30.62 & 8.25   & 155.30 & 3.43  \\
(v1.2) w/o ME &  25.12 / 0.6641 / 31.63 & 8.49   & 157.91 & 3.62 \\
(v1.3) Flow(Spynet)  &  24.83 / 0.6565 / 28.67 & 6.82  & 129.29  & 4.89  \\
(v1.4) Flow(RAFT)  &  25.07 / 0.6620 / 31.27 & 10.63  & 173.74  & 2.85  \\
(v1.5) DCN   &  25.01 / 0.6598 / 30.79 & 8.79  & 170.84   & 3.02  \\
(v1.6) FGDA  &  25.10 / 0.6631 / 31.74 & 10.45  & 210.64  & 2.10  \\
\midrule
(v2.1) w/o MFFR  &  25.10 / 0.6630 / 31.45 & 8.20  & 159.57 & 3.02  \\
(v2.2) w/o FBE  &  25.16 / 0.6668 / 31.95  & 8.81  & 165.36  & 2.68  \\
(v2.3) w/o FFE & 25.14 / 0.6664 / 31.92 & 8.81  & 165.36 & 2.76  \\
\midrule
(v3.1) w/o $\mathcal{L}_{fc}$  & 25.12 / 0.6652 / 31.85   & 8.81  & 165.36  & 2.39 \\
(v3.2) w/o $\mathcal{L}^{1}_i$ & 25.15 / 0.6676 / 31.92   & 8.81  & 165.36  & 2.39 \\
(v3.3) w/o $\mathcal{L}^{2}_i$ & 25.17 / 0.6682 / 31.97   & 8.81  & 165.36  & 2.39 \\
\midrule
FCVSR  & \textbf{25.20 / 0.6694 / 32.05}   & 8.81  & 165.36  & 2.39 \\
\bottomrule
\end{tabular}
\end{table}



% \begin{table}[t]
% \caption{Comparison of different alignment modules with the MGAA module for our FCVSR model} \label{Tab_align}
% \setlength{\tabcolsep}{0.3mm}
% \fontsize{7}{9}\selectfont
% \centering
% \begin{tabular}{l|ccccccc}
% \midrule[0.2mm]
% \midrule[0.2mm]
% Alignment & PSNR(dB)$\uparrow$/SSIM$\uparrow$/VMAF$\uparrow$  & Param.(M)$\downarrow$  & FLOPs.(G)$\downarrow$ & FPS(720P)$\uparrow$ \\
% \midrule[0.2mm]
% Flow(Spynet)  &  24.83 / 0.6565 / 28.67 & 6.82  & 129.29  & 4.89  \\
% Flow(RAFT)  &  25.07 / 0.6620 / 31.27 & 10.63  & 173.74  & 2.85  \\
% DCN   &  25.01 / 0.6598 / 30.79 & 8.79  & 170.84   & 3.02  \\
% FGDA  &  25.10 / 0.6631 / 31.74 & 10.45  & 210.64  & 2.10  \\
% \midrule[0.2mm]
% MGAA  &  25.20 / 0.6694 / 32.05 & 8.81  & 165.36  & 2.39 \\
% \midrule[0.2mm]
% \midrule[0.2mm]
% \end{tabular}
% \end{table}


% \begin{table}[t]
% \caption{Comparison of different numbers of adaptive convolutions with the MGAA module for our FCVSR-M model} \label{Tab_MGAA}
% \setlength{\tabcolsep}{0.3mm}
% \fontsize{7}{9}\selectfont
% \centering
% \begin{tabular}{l|ccccccc}
% \midrule[0.2mm]
% \midrule[0.2mm]
% Alignment & PSNR(dB)$\uparrow$/SSIM$\uparrow$/VMAF$\uparrow$  & Param.(M)$\downarrow$  & FLOPs.(G)$\downarrow$ & FPS(720P)$\uparrow$ \\
% \midrule[0.2mm]
% MGAA(N=1)  & 24.81 / 0.6578 / 28.75 & 8.67 & 162.11  & 5.98  \\
% MGAA(N=2)  &  24.99 / 0.6606 / 29.78 & 8.70 & 162.72  & 4.67    \\
% MGAA(N=3)  & 25.06 / 0.6624 / 31.70 & 8.73  & 163.35  &  3.82   \\
% MGAA(N=4)  & 25.13 / 0.6647 / 31.84 & 8.75  & 163.99  & 3.06  \\
% MGAA(N=5) &  25.15 / 0.6658 / 31.94 & 8.78  & 164.66  & 2.62  \\
% MGAA(N=6) &  \textbf{25.20 / 0.6694 / 32.05}  & 8.81  & 165.36 & 2.39  \\
% \midrule[0.2mm]
% \midrule[0.2mm]
% \end{tabular}
% \end{table}




% \begin{figure*}[!t]
% \centering
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=reds4_015_046.png,width=2.95cm}}
% \footnotesize{ Reference Frame \\  } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=reds4_015_047.png,width=2.95cm}}
% \footnotesize{  Neighboring Frame } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=Alignfeat_Spynet.png,width=2.95cm}}
% \footnotesize{ Flow(Spynet) \\  } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=Alignfeat_RAFT.png,width=2.95cm}}
% \footnotesize{ Flow(RAFT) \\  } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=Alignfeat_DCN.png,width=2.95cm}}
% \footnotesize{ DCN \\   } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=Alignfeat_FGDA.png,width=2.95cm}}
% \footnotesize{  FGDA  } 
% \end{minipage}

% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=vizalign_for_1.png,width=2.95cm}}
% \footnotesize{ AC Num = 1 \\   } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=vizalign_for_2.png,width=2.95cm}}
% \footnotesize{  AC Num = 2 \\  } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=vizalign_for_3.png,width=2.95cm}}
% \footnotesize{  AC Num = 3 \\  } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=vizalign_for_4.png,width=2.95cm}}
% \footnotesize{  AC Num = 4  \\  } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=vizalign_for_5.png,width=2.95cm}}
% \footnotesize{  AC Num = 5 \\   } 
% \end{minipage}
% \begin{minipage}[b]{0.160\linewidth}
% \centering
% \centerline{\epsfig{figure=vizalign_for_6.png,width=2.95cm}}
% \footnotesize{  AC Num = 6 \\  } 
% \end{minipage}
% {\caption{Visualization of aligned features generated by different alignment modules (first row) and our MGAA module with different AC numbers (second row).}
% \label{vision_align}}
% \end{figure*}





% \begin{figure*}[!t]
% 	\centering
% 	% \begin{minipage}[b]{0.23\linewidth}
% 	% 	\centering
% 	% 	\centerline{\epsfig{figure=reds4_015_046.png,width=3.85cm}}
% 	% 	\footnotesize{ Reference Frame \\  } 
% 	% \end{minipage}
% 	% \begin{minipage}[b]{0.23\linewidth}
% 	% 	\centering
% 	% 	\centerline{\epsfig{figure=reds4_015_047.png,width=3.85cm}}
% 	% 	\footnotesize{  Neighboring Frame } 
% 	% \end{minipage}
% 	\begin{minipage}[b]{0.160\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=vizalign_for_1.png,width=2.95cm}}
% 		\footnotesize{ AC Num = 1 \\   } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.160\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=vizalign_for_2.png,width=2.95cm}}
% 		\footnotesize{  AC Num = 2 \\  } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.160\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=vizalign_for_3.png,width=2.95cm}}
% 		\footnotesize{  AC Num = 3 \\  } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.160\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=vizalign_for_4.png,width=2.95cm}}
% 		\footnotesize{  AC Num = 4  \\  } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.160\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=vizalign_for_5.png,width=2.95cm}}
% 		\footnotesize{  AC Num = 5 \\   } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.160\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=vizalign_for_6.png,width=2.95cm}}
% 		\footnotesize{  AC Num = 6 \\  } 
% 	\end{minipage}
%     {\caption{Visualization of aligned features generated by our MGAA module with different AC number.}
% 		\label{vision_MGAA}}
% \end{figure*}



%
%
%\begin{figure*}[!t]
%	\centering
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=reds4_015_046.png,width=2.95cm}}
%		\footnotesize{ Reference Frame \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=reds4_015_047.png,width=2.95cm}}
%		\footnotesize{  Neighboring Frame } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=Spynet00_aligfeat.png,width=2.95cm}}
%		\footnotesize{ Flow(Spynet) \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizalign_for_1.png,width=2.95cm}}
%		\footnotesize{ Flow(RAFT) \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=RAFT_alignfeat.png,width=2.95cm}}
%		\footnotesize{ DCN \\   } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=FGDA_vizfeat.png,width=2.95cm}}
%		\footnotesize{  FGDA  } 
%	\end{minipage}
%	
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizalign_for_1.png,width=2.95cm}}
%		\footnotesize{ AC Num = 1 \\   } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizalign_for_2.png,width=2.95cm}}
%		\footnotesize{  AC Num = 2 \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizalign_for_3.png,width=2.95cm}}
%		\footnotesize{  AC Num = 3 \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizalign_for_4.png,width=2.95cm}}
%		\footnotesize{  AC Num = 4  \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizalign_for_5.png,width=2.95cm}}
%		\footnotesize{  AC Num = 5 \\   } 
%	\end{minipage}
%	\begin{minipage}[b]{0.160\linewidth}
%		\centering
%		\centerline{\epsfig{figure=MMGA_vizfeat.png,width=2.95cm}}
%		\footnotesize{  AC Num = 6 \\  } 
%	\end{minipage}
%	{\caption{{Visualization of aligned features. The first row:  flow-based alignments using Spynet and RAFT, DCN-based alignment, and FGDA-based alignment.} The second row: our MGAA module with different ACs.}
%		\label{vision_align}}
%\end{figure*}
%
%




%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \\
%	\vspace{-0.2em}	
%	\small{Ideal Filter} \\
%	\vspace{0.3em}	
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \\
%	\vspace{-0.2em}	
%	\small{Gaussian Filter} \\
%	\vspace{0.3em}	
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \hspace{-2mm}
%	\includegraphics[width=0.055\textwidth]{freq_gt_6.png} \\
%	\vspace{-0.2em}	
%	\small{Bilateral Filter}
%	\vspace{-0.5em}	
%	\caption{ Comparision between different low-pass filters.}
%	\label{fig_CompLF}
%\end{figure}








% \begin{figure}[!t]
% \centering
% \begin{minipage}[b]{0.493\linewidth}
% \centering
% \centerline{\includegraphics[width=\linewidth]{MGAA_psnr_fps1.pdf}}
% \footnotesize{ (a) \\  } 
% \end{minipage}
% \begin{minipage}[b]{0.493\linewidth}
% \centering
% \centerline{\includegraphics[width=\linewidth]{MFFR_psnr_fps1.pdf}}
% \footnotesize{ (b) } 
% \end{minipage}
% {\caption{Illustration of performance-complexity trade-offs for different AC numbers (a) and different decomposition numbers (b). }
% \label{Fig_drawcurve}}
% \end{figure}







% \begin{figure}[!t]
% 	\centering
% \centerline{\includegraphics[width=\linewidth]{MGAA_psnr_fps_vs_number_with_curve.pdf}}
% 	{\caption{{Illustration of performance-complexity trade-offs for different AC numbers.}}
% 		\label{fig:AC_number}}
%         % \vspace{-5pt}
% \end{figure}






\subsection{Ablation Study}

% We conduct ablation studies to verify the effectiveness of our designed modules and loss functions. 
To further verify the effectiveness of the main contributions in this work, we have created different model variants in the ablation study, and used the REDS4 dataset (QP = 37) in this experiment.

We first tested the contribution of the MGAA module (and its sub-blocks) by creating the following variants. (v1.1) w/o MGAA - the MGAA module is removed and the features of frames are fused by a concatenation operation and a convolution layer to obtain the aligned features. We have also tested the effectiveness of the Motion Estimator within the MGAA module, obtaining (v1.2) w/o ME - the input neighboring features are directly fed into the MGAC layer without the guidance of motion offsets to generate the aligned features. The MGAA module has also been replaced by other existing alignment modules including flow-based alignment modules ((v1.3) Spynet~\cite{ranjan2017optical} and (v1.4) RAFT~\cite{teed2020raft}), deformable convolution-based alignment modules ((v1.5) DCN~\cite{zhu2019deformable}), and flow-guided deformable alignment module ((v1.6) FGDA~\cite{chan2022basicvsr++}) to verify the effectiveness of MGAA module.  % We have also tested the effectiveness of the Motion Estimator block within MGAA, obtaining (v1.2) w/o ME when it is removed and  (v1.3) Spynet~\cite{ranjan2017optical}  and (v1.4) RAFT~\cite{teed2020raft} when it is replaced by alternative optical flow methods.

The effectiveness of the MFFR module has also been fully evaluated by removing it from the pipe, resulting in (v2.1) w/o MFFR. The contributions of each branch in this module have also been verified by creating (v2.2) w/o FBE - removing the feedback enhancement branch and (v2.3) w/o FFE - disabling the feedforward enhancement branch.

The results of these variants and the full FCVSR model have been summarized in TABLE \ref{Tab_module}. It can be observed that the full FCVSR model is outperformed by all these model variants in terms of three quality metrics, which confirms the contributions of these key models and their sub-blocks.

Finally, to test the contribution of the proposed frequency-aware contrastive loss, we re-trained our FCVSR model separately by removing the $\mathcal{L}_{fc}$ (v3.1) or its high/low frequency terms, $\mathcal{L}_i^1$ (v3.2) and $\mathcal{L}_i^2$ (v3.3), respectively, resulting in three additional variants as shown in Table \ref{Tab_module}. It can be observed that the proposed frequency-aware contrastive loss (and its high/low frequency sub losses) does consistently contribute to the final performance according to the results.



% Moreover, we further The different numbers of AC in the MGAA module are set and the corresponding results of these models are provided in Table \ref{Tab_MGAA}. It is found from Tab. \ref{Tab_MGAA} that as the number of ACs increases, the PSNR value will be higher, but the inference speed will also be slower. Besides, the visual results of these models are provided in Fig. \ref{vision_align}.  It can be found from Fig. \ref{vision_align} that in the MGAA module with more ACs, the misalignment is less, such as in car tires, and building silhouettes. The performance-complexity trade-offs for different AC numbers is also illustrated in Fig. \ref{Fig_drawcurve}. It is found in Fig. \ref{Fig_drawcurve} that when the number of ACs exceeds $N$=4, the complexity in terms of FPS increases significantly, and performance in terms of PSNR is increases slightly.



% our \emph{Baseline} model. In \emph{Baseline} model, the features of frames are fused by a concatenation operation and a convolution layer to obtain the aligned features. The aligned feature is directly sent into the REC module. Then, the MGAA module is removed from the FCVSR model and the features of frames are fused by a concatenation operation and a convolution layer to obtain the aligned features. This model is denoted as the \emph{wo/ MGAA} model. To further verify the effectiveness of sub-blocks in the MGAA module,  the Motion Estimator is removed in the MGAA module from the FCVSR model and this model is denoted as \emph{wo/ME} model.
% The corresponding results of these three models are provided in Table \ref{Tab_module}. It is found from Table \ref{Tab_module} that the MGAA module brings 0.16 dB PSNR gain and the Motion Estimator brings 0.08 dB PSNR gain. 


% To further verify the effectiveness of the MGAA module, some ablation studies are conducted. The models with different numbers of AC, our Motion Estimator, and our MGAA module are discussed.

% \emph{(i) MGAA vs. Different Alignment Modules.} 
% To verify the effectiveness of our MGAA module with different alignments, the optical flow, DCN, and FGDA are adopted to replace our MGAA module in the FCVSR model and the corresponding results are given in Table \ref{Tab_align}. It is seen from Table \ref{Tab_align} that our MGAA module surpasses state-of-the-art alignment, i.e., FGDA, about 0.10 dB in terms of PSNR, and has lower parameters, lower FLOPs, and higher FPS. To further verify the effectiveness of our MGAA module, the visual results of aligned features from our MGAA module and other four modules are provided in Fig. \ref{vision_align}. It can be seen in Fig. \ref{vision_align} that the visual results of flow-based alignment modules have obvious misalignment artifacts, which will directly impact the final SR results. Besides, DCN-based alignment module has less artifacts but lack of detail of aligned features. Moreover, FDGA-based alignment module effectively generates more detail of aligned results but also has misalignment artifacts. Compared with these three alignment modules, our MGAA module with more than 4 ACs generates more clearly aligned features and fewer misalignment artifacts.

% \emph{(ii) The choice of number of adaptive convolution.}  The different numbers of AC in the MGAA module are set and the corresponding results of these models are provided in Table \ref{Tab_MGAA}. It is found from Tab. \ref{Tab_MGAA} that as the number of ACs increases, the PSNR value will be higher, but the inference speed will also be slower. Besides, the visual results of these models are provided in Fig. \ref{vision_align}.  It can be found from Fig. \ref{vision_align} that in the MGAA module with more ACs, the misalignment is less, such as in car tires, and building silhouettes. The performance-complexity trade-offs for different AC numbers is also illustrated in Fig. \ref{Fig_drawcurve}. It is found in Fig. \ref{Fig_drawcurve} that when the number of ACs exceeds $N$=4, the complexity in terms of FPS increases significantly, and performance in terms of PSNR is increases slightly.

% Our motion estimator with two representative optical flow networks, i.e., Spynet~\cite{ranjan2017optical}, RAFT~\cite{teed2020raft}, in our FCVSR network for compressed video super-resolution. 








% \emph{(iii) Motion Estimator vs. Optical Flow Networks.}  Our Motion Estimator is replaced with two representative optical flow networks, i.e., Spynet~\cite{ranjan2017optical}, and 
%  RAFT~\cite{teed2020raft}, in our FCVSR model for compressed VSR task. 
% The corresponding results are provided in Table \ref{Tab_MGAA}. It is found from  Table \ref{Tab_MGAA} that our Motion Estimator in the MGAA module with 1 AC has similar performance in terms of PSNR as the Spynet-based model but has lower complexity.  Besides, our Motion Estimator in the MGAA module with 3 ACs has similar performance in terms of PSNR as an RAFT-based model but has lower complexity. It reflects that our Motion Estimator has a more practical application.

% Besides, we illustrate the multi-motions generated by our motion estimator and optical flow generated by Spynet and RAFT for a visualization comparison. The visual results are provided in Fig. \ref{vision_flow}.  \blue{It can be seen that our motion estimator generates the different motion, which benefit for achieving the better performance}.


%
%\begin{figure}[!t]
%	\centering
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=LRQP22_015_00000018.png,width=2.90cm}}
%		\footnotesize{ Reference Frame \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizSpy_back.png,width=2.90cm}}
%		\footnotesize{  Spynet~\cite{ranjan2017optical} } 
%	\end{minipage}
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizRAFT_back.png,width=2.90cm}}
%		\footnotesize{  RAFT~\cite{teed2020raft} } 
%	\end{minipage}
%	
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizoffset_for_1.png,width=2.90cm}}
%		\footnotesize{ Motion $o_{1}$ \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizoffset_for_2.png,width=2.90cm}}
%		\footnotesize{ Motion $o_2$  \\  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizoffset_for_3.png,width=2.90cm}}
%		\footnotesize{  Motion $o_3$ \\   } 
%	\end{minipage}
%	
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizoffset_for_4.png,width=2.90cm}}
%		\footnotesize{  Motion $o_4$  } 
%	\end{minipage}
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizoffset_for_5.png,width=2.90cm}}
%		\footnotesize{  Motion $o_5$ } 
%	\end{minipage}
%	\begin{minipage}[b]{0.325\linewidth}
%		\centering
%		\centerline{\epsfig{figure=vizoffset_for_6.png,width=2.90cm}}
%		\footnotesize{  Motion $o_6$ } 
%	\end{minipage}
%	{\caption{\red{Visual results of multi-motions from our Motion Estimator and Spynet, RAFT.}}
%		\label{vision_flow}}
%\end{figure}
%







% \begin{table}[t]
% \caption{Verify the effectiveness of different decomposition numbers in the MFFR module for our FCVSR model} \label{Tab_MFFR}
% \setlength{\tabcolsep}{2.7mm}
% % \renewcommand\arraystretch{1.1}
% \fontsize{7}{9}\selectfont
% \centering
% \begin{tabular}{c|cccccccccc}
% \midrule[0.2mm]
% \midrule[0.2mm]
% $Q$ & PSNR(dB)$\uparrow$/SSIM$\uparrow$/VMAF$\uparrow$  & Param.(M)$\downarrow$  & FPS(720P)$\uparrow$ \\
% \midrule[0.2mm]
% 1  &  24.93 / 0.6583 / 29.05 & 8.54    & 2.64  \\  % & 138.01
% 2  &  25.02 / 0.6604 / 30.17 & 8.58    & 2.52  \\   %   &  138.01
% 4  &  25.08 / 0.6628 / 30.56  & 8.66    & 2.42  \\    %   & 138.02
% 8  &  25.20 / 0.6694 / 32.05  &  8.81    & 2.39   %  & 138.02
% \\
% 16 &  25.26 / 0.6702 / 32.22 & 9.11   & 2.14  %  &  138.02 
% \\
% 32  &  \textbf{25.29 / 0.6709 / 32.42} & 9.71   & 1.86   %  & 138.03
% \\
% \midrule[0.2mm]
% \midrule[0.2mm]
% \end{tabular}
% \end{table}


% \subsubsection{{Effectiveness of MFFR module}} The FFE and FBE of $\emph{Enhancer}$ are main blocks in the MFFR module. The MFFR module is first removed from the FCVSR model and the aligned feature generated by the MGAA module is directly sent into the reconstruction module. This model is denoted as \emph{wo/ MFFR} model. Then, the FBE branch and FFE branch are removed, respectively, from the FCVSR model to obtain \emph{wo/FFE} and \emph{wo/FBE} models.  


% \begin{figure}[!t]
% 	\centering
% \centerline{\includegraphics[width=\linewidth]{MFFR_psnr_fps_vs_number_with_curve.pdf}}
% 	{\caption{{Illustration of performance-complexity trade-offs for different decomposition numbers.}}
% 		\label{fig:Des_number}}
%         % \vspace{-5pt}
% \end{figure}


% \emph{(i) The Effectiveness of FFE and FBE branches.} 
% % We first implement the MFFR module only adopting the FFE in $\emph{Enhancer}$ into \emph{Model-3} to construct \emph{Model-4}. Then, we further implement the FBE into the \emph{Model-4} to construct \emph{Model-5}, which also is our full model. 
% The corresponding results of \emph{wo/FFB} and \emph{wo/FEB} models are provided in Table \ref{Tab_module}. It is found from Table \ref{Tab_module} that the FFE branch of the MFFR module brings 0.06 dB performance gain in terms of PSNR and the FEB branch of the MFFR module brings 0.04 dB performance gain in terms of PSNR. 

% \emph{(ii) The Choice  of Decomposition Number $Q$.} 
% To further verify the effectiveness of decomposition number $Q$ in the MFFR module, the decomposition number $Q$ is set as 2, 4, 8, 16, and 32 in the MFFR module to construct corresponding models. Their results are provided in Table \ref{Tab_MFFR}.  Besides, the visual results of the input feature, output feature, the decomposed features $S_{i}$, and enhanced features $E_{i}$ are also illustrated for a visualization comparison. These visual results are provided in Fig. \ref{visionMFFR}. It is found from Fig. \ref{visionMFFR} that the enhanced features have more clear than decomposed features, which verifies the effectiveness of the MFFR module. The performance-complexity trade-offs for different decomposition numbers $Q$ is also illustrated in Fig. \ref{Fig_drawcurve}. It is found in Fig. \ref{Fig_drawcurve} that when the decomposition number exceeds $Q$ = 32, the complexity in terms of FPS increases significantly, and performance in terms of PSNR also increases slightly. It reflects that our MFFR module can generate the more refined feature, as the number of decompositions increases.




% \begin{table}[t]
% \caption{The ablation study of the proposed contrastive frequency-aware losses.} \label{Tab_loss}
% \setlength{\tabcolsep}{3.0mm}
% \fontsize{7}{9}\selectfont
% \centering
% \begin{tabular}{ccccccccc}
% \midrule[0.2mm]
% \midrule[0.2mm]
% {\multirow{2}{*}{{$\mathcal{L}_{spa}$}}}  & \multicolumn{2}{c}{$\mathcal{L}_{fc}$}   & {\multirow{2}{*} {PSNR(dB)$\uparrow$/SSIM$\uparrow$/VMAF$\uparrow$}} \\
% \cline{2-3}  
% & $\mathcal{L}^{1}_{i}$  &$\mathcal{L}^{2}_{i}$   &  \\
% \midrule[0.2mm]
% \Checkmark & -  & -  &                        25.12 / 0.6652 / 31.85    \\
% \Checkmark & \Checkmark  & -  &               25.17 / 0.6682 / 31.97   \\
% \Checkmark & - & \Checkmark  &               25.15 / 0.6676 / 31.92  \\
% \Checkmark & \Checkmark & \Checkmark  &       \textbf{25.20} /\textbf{0.6694} / \textbf{32.05} \\
% %\Checkmark & - & - & \Checkmark &                25.24 / 0.6694 / 32.21 \\
% %\Checkmark & \Checkmark & \Checkmark & \Checkmark &  \textbf{25.30 / 0.6706 / 32.46}  \\
% \midrule[0.2mm]
% \midrule[0.2mm]
% \end{tabular}
% \end{table}




% \begin{table}[t]
% \caption{Verify the effectiveness of loss functions for FCVSR model} \label{Tab_loss}
% \setlength{\tabcolsep}{3.0mm}
% \fontsize{7}{9}\selectfont
% \centering
% \begin{tabular}{ccccccccc}
% \midrule[0.2mm]
% \midrule[0.2mm]
% {\multirow{2}{*}{{$\mathcal{L}_{spa}$}}}  & \multicolumn{2}{c}{$\mathcal{L}_{fc}$} & {\multirow{2}{*}{{$\mathcal{L}_{etc}$}}}  & {\multirow{2}{*} {PSNR(dB)$\uparrow$/SSIM$\uparrow$/VMAF$\uparrow$}} \\
% \cline{2-3}  
% & $\mathcal{L}^{1}_{i}$  &$\mathcal{L}^{2}_{i}$   &  \\
% \midrule[0.2mm]
% \Checkmark & -  & - & - &                        25.16 / 0.6669 / 31.89 \\
% \Checkmark & \Checkmark  & - & - &               25.21 / 0.6682 / 32.12\\
% \Checkmark & - & \Checkmark  & - &               25.20 / 0.6676 / 32.09  \\
% \Checkmark & \Checkmark & \Checkmark & - &       25.23 / 0.6687 / 32.16 \\
% \Checkmark & - & - & \Checkmark &                25.24 / 0.6694 / 32.21 \\
% \Checkmark & \Checkmark & \Checkmark & \Checkmark &  \textbf{25.30 / 0.6706 / 32.46}  \\
% \midrule[0.2mm]
% \midrule[0.2mm]
% \end{tabular}
% \end{table}




\section{Conclusion}  \label{CON}

In this paper, we proposed a frequency-aware video super-resolution network, FCVSR, for compressed video content, which consists of a new motion-guided adaptive alignment (MGAA) module for improved feature alignment and a novel multi-frequency feature refinement (MFFR) module that enhances the fine detail recovery. A frequency-aware contrastive loss is also designed for training the proposed framework with optimal super resolution performance. We have conducted a comprehensive comparison experiment and ablation study to evaluate the performance of the proposed method and its primary contributions, and the results show up to a 0.14dB PSNR gain over the SoTA methods. Due to its superior performance and relatively low computational complexity, we believe this work makes a strong contribution to the research field of video super resolution, and is suitable for various application scenarios. 

%\section*{Acknowledgments}
%This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.




%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}


\begin{comment}
\begin{figure*}[!t]
\centering
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_qp37_2.png,width=1.65cm}}
\footnotesize{ \ } 
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_qp37_3.png,width=1.65cm}}
\footnotesize{ \ }
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_qp37_4.png,width=1.65cm}}
\footnotesize{(a)}
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_qp37_5.png,width=1.65cm}}
\footnotesize{ \ } 
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_qp37_6.png,width=1.65cm}}
\footnotesize{ \ }
\end{minipage}
\hspace*{0.2cm}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_gt2.png,width=1.65cm}}
\footnotesize{ \ }
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_gt3.png,width=1.65cm}}
\footnotesize{ \ }
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_gt4.png,width=1.65cm}}
\footnotesize{(c)} 
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_gt5.png,width=1.65cm}}
\footnotesize{ \ }
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=rgb_gt6.png,width=1.65cm}}
\footnotesize{ \ } 
\end{minipage}



\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_qp37_2.png,width=1.65cm}}
\footnotesize{ \ }
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_qp37_3.png,width=1.65cm}}
\footnotesize{ \ }
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_qp37_4.png,width=1.65cm}}
\footnotesize{(b)}
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_qp37_5.png,width=1.65cm}}
\footnotesize{ \ } 
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_qp37_6.png,width=1.65cm}}
\footnotesize{ \ } 
\end{minipage}
\hspace*{0.2cm}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_gt_2.png,width=1.65cm}}
\footnotesize{ \ } 
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_gt_3.png,width=1.65cm}}
\footnotesize{ \ } 
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_gt_4.png,width=1.65cm}}
\footnotesize{(d)}
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_gt_5.png,width=1.65cm}}
\footnotesize{ \ }
\end{minipage}
\begin{minipage}[b]{0.09\linewidth}
\centering
\centerline{\epsfig{figure=freq_gt_6.png,width=1.65cm}}
\footnotesize{ \ } 
\end{minipage}

\begin{minipage}[b]{0.99\linewidth}
\centering
\centerline{\epsfig{figure=Amp_FreqCurve.png,width=18.10cm}}
\footnotesize{ (e) }
\end{minipage}

{\caption{Comparison between compressed sequence and uncompressed sequence at [$t-2$ : $t+2$] time step with 220 $\times$ 220 resolution in spatial domain and frequency domain.  (a) Compressed sequence under LDB configuration at QP=37. (b) Frequency spectrum of (a). (c) Uncompressed sequence. (d) Frequency spectrum of (c). (e) Amplitude-frequency curve of uncompressed-compressed image pair at $t$ time step. }
\label{vision_freSpec}}
\end{figure*}


% \begin{figure*}[!t]
% 	\centering
% 	\begin{minipage}[b]{0.22\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=FlowAlign.pdf,width=3.40cm}}
% 		\footnotesize{  (a) Optical Flow \\ \ } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.22\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=DeformAlign.pdf,width=3.54cm}}
% 		\footnotesize{   (b) Deformable Convolution \\ \  }
% 	\end{minipage}
% 	\begin{minipage}[b]{0.22\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=FGDA.pdf,width=3.63cm}}
% 		\footnotesize{  (c) Flow-guided Deformable Convolution } 
% 	\end{minipage}
% 	\begin{minipage}[b]{0.22\linewidth}
% 		\centering
% 		\centerline{\epsfig{figure=ourAlign.pdf,width=3.50cm}}
% 		\footnotesize{  (d) Motion-guided Adaptive Convolution (Ours) } 
% 	\end{minipage}
% 	{\caption{Different alignment modules: (a) Optical flow-based alignment~\cite{xue2019video, chan2021basicvsr,liu2022temporal}. (b) Deformable convolution-based alignment~\cite{tian2020tdan,wang2019edvr}.  (c) Flow-guided deformable convolution-based alignment~\cite{chan2022basicvsr++} and (d) Our adaptive convolution-based alignment.}
% 		\label{fig_tempalign}}
% \end{figure*}


% \begin{table}[t]
% 	\caption{Notations of the major variables} \label{Tab_index}
% 	\setlength{\tabcolsep}{1.3mm}
% 	\renewcommand{\arraystretch}{0.1}
% 	\fontsize{7}{9}\selectfont
% 	\centering
% 	\begin{tabular}{c|c|c|ccc}
% 		\toprule[0.2mm]
% 		\toprule[0.2mm]
% 		{\text { Notation  }}  & Description   &  {\text { Notation  }}  & Description   \\
% 		\midrule[0.2mm]
% 		$\bar{\mathcal{F}}$ & Aligned feature & $\widetilde{{\mathcal{F}}}$ & Refined feature \\
% 		\midrule[0.2mm]		
% 		$\hat{{\mathcal{F}}}$ & Frequency feature & $\hat{o}$ & Motion in frequency domain \\
% 		\midrule[0.2mm]	
% 		$\bar{a}$ & Output feature of AC &  $\hat{s}$ & Decomposed feature \\		
% 		\midrule[0.2mm]
% 		$\mathcal{K}^{v}$/$\mathcal{K}^{h}$ & Vertical/Horizontal kernel &  $\emph{\textbf{O}}$ & Multi-motions \\	
% 		\midrule[0.2mm]
% 		$\emph{\textbf{K}}$ & Predicted kernels &  $\emph{\textbf{S}}$ & Decomposed features \\		
% 		\midrule[0.2mm]
% 		$\emph{\textbf{E}}$ & Enhanced features &   $\emph{\textbf{M}}$ & Band-pass filter masks \\		
% 		\midrule[0.2mm]
% 		\toprule[0.2mm]
% 	\end{tabular}
% \end{table}


\begin{figure*}[!t]
\begin{minipage}[b]{0.22\linewidth}
\centering
\centerline{\epsfig{figure=rgb_qp37_4.png, width=1.90cm} \epsfig{figure=freq_qp37_4.png, width=1.90cm}} 
\footnotesize{(a)}
\centerline{\epsfig{figure=rgb_gt4.png, width=1.90cm} \epsfig{figure=freq_gt_4.png, width=1.90cm}}
\footnotesize{(b) \\ \ }
\end{minipage}
\hfill
\begin{minipage}[b]{0.77\linewidth}
\centering
\centerline{\epsfig{figure=Amp_FreqCurve_new.pdf,width=14.50cm}}
\footnotesize{ (c) }
\end{minipage}
{\caption{Comparison between compressed video frame and uncompressed video frame at $t$ time step with 220 $\times$ 220 resolution in spatial domain and frequency domain.  (a) Compressed video frame under LDB configuration at QP=37 and its frequency spectrum. (b) Uncompressed video frame and its frequency spectrum. (c) Amplitude-frequency curve of uncompressed-compressed image pair at $t$ time step. }
\label{vision_freSpec}}
\end{figure*}
\end{comment}

\small
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,freqCVSR}



\begin{comment}


% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
% your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
% simpler here.)

% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}


\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{QiangZhu.jpg}}]{Qiang Zhu}
received the the B.Sc. degree from University of Electronic Science and Technology of China, Chengdu, China, in 2019. He is currently pursuing the Ph.D. degree with the School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu. 
His research interests include image/video super-resolution, compressed video enhancement, and video compression.
\end{IEEEbiography}

\vspace{-10pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fan.jpg}}]{Fan Zhang} (Member, IEEE) received the B.Sc. and M.Sc. degrees from Shanghai Jiao Tong University, Shanghai, China, in 2005 and 2008, respectively, and the Ph.D. degree from the University of Bristol, Bristol, U.K., in 2012. He is currently a Senior Lecturer within the School of Computer Science, University of Bristol. He served as an Associate Editor for IEEE Transactions on Circuits and Systems for Video Technology (2022-2024), and was a guest editor of IEEE Journal on Emerging and Selected Topics in Circuits and Systems (in 2024) and Frontiers in Signal Processing (in 2022). Fan is also a member of the Visual Signal Processing and Communications Technical Committee associated with the IEEE Circuits and Systems Society. His research interests focus on low-level computer vision including video compression, quality assessment, super resolution and video frame interpolation.
\end{IEEEbiography}

\vspace{-10pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Chen.pdf}}]{Feiyu Chen} (Member, IEEE) received the M.E. degree in electrical and electronic engineering from the University of
Nottingham, Nottingham, U.K., in 2016, and the Ph.D. degree in Computer Science and Technology from the University of Electronic Science and Technology of China (UESTC), Chengdu, China, in 2022. 
She is currently an Associate Professor with School of Information and Communication Engineering at UESTC. Her research interests include multimodal emotion recognition,  multimodal information mining, and image processing.
\end{IEEEbiography}

\vspace{-20pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Zhu Shuyuan.pdf}}]{Shuyuan Zhu} (Member, IEEE)  received the Ph.D. degree from The Hong Kong University of Science and Technology (HKUST), Hong Kong, in 2010. 

From 2010 to 2012, he worked at HKUST and Hong Kong Applied Science and Technology Research Institute Company Ltd., Hong Kong, respectively. In 2013, he joined University of Electronic Science and Technology of China, Chengdu, China, where he is currently a Professor with School of Information and Communication Engineering. His research interests include image/video compression and computer vision.

Dr. Zhu served as a Committee Member for the IEEE ICME-2014, the IEEE DSP-2015, the VCIP-2016, the PCM-2017, the IEEE MIPR2020, and the IEEE ICIP-2021. He served as an Associate Editor for IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) and received the Best Associate Editor Award in 2021. He currently serves as the Senior Area Editor for TCSVT.  Dr. Zhu received the Top 10\% Paper Award at IEEE ICIP-2014 and the Best 10\% Paper Award at VCIP-2016.
\end{IEEEbiography}


\vspace{-20pt}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{dave.jpg}}]{David R. Bull} (Fellow, IEEE) received the B.Sc. degree from the University of Exeter, Exeter, U.K., in 1980, the M.Sc. degree from the University of Manchester, Manchester, U.K., in 1983, and the Ph.D. degree
from the University of Cardiff, Cardiff, U.K., in 1988. He was previously a Systems Engineer with Rolls Royce, Bristol, U.K., and a Lecturer with the University of Wales, Cardiff, U.K. In 1993, he joined the University of Bristol, Bristol, U.K., and is currently its Chair of Signal Processing and the Director of Bristol Vision Institute. He is also the Director of the recently announced £46 m UKRI ‘MyWorld’ Strength in Places Programme. In 2001, he co-founded a university spin-off company, ProVision Communication Technologies Ltd., specializing in wireless video technology. He has authored more than 450 papers on the topics of image and video communications and analysis for wireless, Internet and broadcast applications, together with numerous patents, several of which have been exploited commercially. He is the author of three books, and has delivered numerous invited/keynote lectures and tutorials. He was the recipient of the two IET Premium Awards for his work. Dr. Bull is a Fellow of the Institution of Engineering and Technology.
\end{IEEEbiography}

\vspace{-10pt}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Zeng Bing.pdf}}]{Bing Zeng} (Fellow, IEEE) received the B.Eng.
and M.Eng. degrees from the University of Electronic Science and Technology of China (UESTC), Chengdu, China, in 1983 and 1986, respectively, and the Ph.D. degree from Tampere University of
Technology, Tampere, Finland, in 1991.

He worked as a Post-Doctoral Fellow at University of Toronto, Toronto, ON, Canada, from September 1991 to July 1992, and as a Researcher at Concordia University, Montreal, QC, Canada, from August 1992 to January 1993. He then joined The Hong Kong University of Science and Technology (HKUST), Hong Kong, and
returned to UESTC in the summer of 2013. Currently, he focuses on
the research of image/video processing, computer vision and image/video compression. During his tenure at HKUST and UESTC, he received about 20 research grants, filed eight international patents, and published more than 260 papers.

Dr. Zeng received the 2011 Best Paper Award of IEEE Transactions on Circuits and Systems for Video Technology (TCSVT). He served
as an Associate Editor for IEEE TCSVT for eight years and received the Best Associate Editor Award in 2011. He was elected as an IEEE Fellow in 2016 for contributions to image and video coding.
\end{IEEEbiography}

\end{comment}



\vfill

\end{document}
