\section{\label{cases}Simulation Studies}

We now evaluate the performance of our proposed method across several simulation studies and on a real hardware testbed. For this, we compare the proposed approach against five baseline methods. Each baseline builds on the MPPI control approach but incorporates distinct modifications to enhance safety.\par

\noindent \textit{Obstacle Penalty (Obs. penalty)}: A classical MPPI approach to collision avoidance by adding a high penalty for entering the obstacles in the cost function, incentivizing the system to avoid these penalized regions.

\noindent \textit{BRT Penalty (BRT penalty)}: Advanced MPPI methods penalize hallucinated trajectories that enter a safe zone defined as augmented obstacles, Discrete Barrier States (DBaS), or Control Barrier Functions (CBF) \cite{testouri2024saferealtimemotionplanning,discrete_barrier_2024,parwana2024modelpredictivepathintegral,RAPA_2024}. To contrast against this technique, we implement a baseline that penalizes hallucinated trajectories that enter the BRT of the obstacles. This approach is more preemptive, as it penalizes samples that have entered states from which such a collision is unavoidable but may have not yet collided with obstacles within the sampling horizon.

\noindent \textit{Obstacle Penalty with Output LRF (Obs. pen. + LRF)}: Extension of the Obstacle Penalty baseline by adding a least restrictive filtering step. This corresponds to a naive safety filtering approach where the least restrictive filter is used to correct any control that would result in a safety violation just before it is applied.

\noindent \textit{BRT Penalty with Output LRF (BRT pen. + LRF)}: Similar to the previous baseline, this extends the BRT Penalty baseline by adding least restrictive filtering safety guarantees.

\noindent \textit{Shield Model Predictive Path Integral (Shield MPPI)}: This method leverages the approach described in Shield-MPPI\cite{mppi_shield}, where a cost related to the CBF condition is considered during hallucinations, followed by an approximate CBF-based repair step over the resulting updated control to promote safety. To ensure a fair comparison, we will use the HJ value function as a barrier function so that all methods work on the maximal safe set \cite{cbvf}.

\noindent \textbf{Evaluation Metrics:} To quantitatively assess the performance of each method, we define the following evaluation metrics. These metrics will be measured over batches of simulation runs for each method, providing insights into safety, efficiency, and computational demands.

\begin{itemize}
    \item Failure: Percentage of episodes that result in a safety violation (e.g., a collision).
    \item RelCost: Average normalized cost of the trajectory, using the proposed method as the baseline. We only consider successful executions across all methods for RelCost computation. This allows for a fair comparison and ensures that the RelCost is not affected by the high failure penalties for some of the baselines.
    % \item CompTime: Average computation time required to perform each iteration of the controller.
\end{itemize}

Furthermore, when deemed relevant, we define additional performance-related metrics specific to each case study.

\subsection{\label{case_dub3D}Safe Planar Navigation}

We assess the running-example safe planar navigation with dynamics defined in \eqref{eq:dyn_car}. In each episode, the system must navigate from an initial state $\state_0$ to a goal region of radius 0.1~m in the $xy$ plane centered at the goal state $\state_g$, within a time horizon of $T = 20\text{ s}$. The optimal control problem's cost function is defined as the squared distance from the goal, a control effort penalty, and a safety penalty $P(\state)$:
\begin{equation}\fontsize{9.5}{10}\selectfont
S=\sum_{t=0}^{T} (\state_t - \state_g)^T Q(\state_t - \state_g) + m(u_t)^2 + P(\state_t) \\
\end{equation}

\noindent with $Q = \text{diag}([1, 1, 0])$, $m = 0.2$, and $P=10000$ if the state is inside the obstacles or the BRT (corresponding to that controller's cost penalty type) and $K=0$ otherwise.

We test each controller in the same set of 100 episodes, where each episode is defined by a particular pair $(\state_0, \state_g)$. These initial-goal state pairs were selected randomly but constrained to be outside the BRT, near the environment boundary, and separated by at least 5 m for diversity.
The results are summarized in Table~\ref{tab:dubins_results}. In addition to the metrics previously defined, we also compute the Success rate, defined as the percentage of episodes that reach the goal region safely (i.e., without collisions) within the time horizon, and Timeout, defined as the percentage of episodes that fail to reach the target within the given time horizon. 
% Computations for this experiment were performed on CPU using an AMD Ryzen 5 3600X.

\begin{figure}[t] 
\begin{center} 
\vspace{0.0em}
\includegraphics[width=0.99\columnwidth]{fig/dubins_environment_BRT_v3.png}
\vspace{-1.5em}
\caption{Safe Planar Navigation environment, with obstacles in gray and a slice of the BRT for a southwestern heading is in teal.}
\vspace{-1.2em}
\label{fig:dubins_brt}
\end{center}
\end{figure}

\setlength\tabcolsep{1pt}
\begin{table}[b]
\fontsize{7pt}{7pt}\selectfont
\caption{Safe Planar Navigation results over 100 episodes.}
\centering
\renewcommand{\arraystretch}{1.2}   % Adjust row height for vertical centering
\begin{tabularx}{\columnwidth}{|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{2.2cm}|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}X|}
\hline
\textbf{K}&\textbf{Method}&\textbf{Success}&\textbf{Timeout}&\textbf{Failure}&\textbf{RelCost} \\
\hline
\multirow{6}{*}{1000}
& Obs. penalty    & 49 &  1 & 50 & 1.34 ± 1.19 \\
& BRT penalty     & 72 & 21 &  7 & 1.89 ± 2.11 \\
& Obs. pen. + LRF & 80 & 20 &  \textbf{0} & 1.35 ± 1.20 \\
& BRT pen. + LRF  & 74 & 26 &  \textbf{0} & 1.90 ± 2.12 \\
& Shield MPPI     & 83 & 17 &  \textbf{0} & 1.32 ± 0.87 \\
& DualGuard (Ours)      & 99 &  1 &  \textbf{0} & 1.00  \\
\hline
\multirow{6}{*}{250}
& Obs. penalty    & 31 &  0 & 69 & 1.10 ± 0.24 \\
& BRT penalty     & 58 & 15 & 27 & 2.35 ± 3.40 \\
& Obs. pen. + LRF & 81 & 19 &  \textbf{0} & 1.27 ± 0.92 \\
& BRT pen. + LRF  & 69 & 31 &  \textbf{0} & 1.86 ± 2.03 \\
& Shield MPPI     & 78 & 21 &  1 & 1.40 ± 0.86 \\
& DualGuard (Ours)      & 98 &  2 &  \textbf{0} & 1.00  \\
\hline
\multirow{6}{*}{60}
& Obs. penalty    & 11 &  0 & 89 & 0.99 ± 0.03 \\
& BRT penalty     & 43 & 21 & 36 & 3.16 ± 4.17 \\
& Obs. pen. + LRF & 70 & 30 &  \textbf{0} & 0.99 ± 0.03 \\
& BRT pen. + LRF  & 55 & 45 &  \textbf{0} & 3.16 ± 4.17 \\
& Shield MPPI     & 64 & 36 &  \textbf{0} & 1.08 ± 0.14 \\
& DualGuard (Ours)      & 96 &  4 &  \textbf{0} & 1.00  \\
\hline
\end{tabularx}

\label{tab:dubins_results}
\end{table}

First examining the results for $K=1000$, we see that our proposed method achieves the lowest failure rate and highest success rate. While all methods with an output Least Restrictive Filter avoid collisions as expected, our method also times-out in fewer episodes, indicating a better performance. The safety-performance co-optimization capabilities of the proposed method are also evident from its lowest RelCost.

% all for 1000 samples in Episode 23
To understand the different controllers' behavior, consider the trajectories shown in Figure~\ref{fig:dubins_brt}. Here, the obstacle penalty method fails early by entering a region in the northeast corner which is impossible to escape and hence avoided by all BRT-aware methods. The other baseline methods execute a reasonable trajectory to the goal, but the proposed method manages to find a lower-cost trajectory by taking a shortcut south after rounding the first obstacle. This is because the baseline methods are still fundamentally limited by the exploration capabilities of MPPI -- if the underlying samples are unsafe, they contribute little to the performance optimization. On the other hand, the proposed method generates safe hallucinations, allowing it to explore more relevant parts of the environment with the same number of samples, as previously visualized in Fig.~\ref{fig:safe_mppi_steps}, leading to a better performance optimization.
% Thus the safe-sampling process of the proposed method improves the exploratory properties of the algorithm without any additional exploration hyperparameter.

A consequence of the superior exploration capabilities of the proposed method is that it can achieve similar success rates with far fewer samples. We summarize the results for different methods as we vary the number of samples in Table~\ref{tab:dubins_results}. We note that while all methods degrade with fewer samples as expected, DualGuard remains relatively consistent. With only 60 samples, it succeeds in 96\% of the episodes, with the closest contender not going over 70\%.
This could enable the deployment of the proposed method on resource-constrained systems, which MPPI methods typically struggle with (as also indicated by the poor performance of other MPPI baselines with fewer samples).
% Finally, the sample efficiency of the proposed method could also help with amortizing the relatively higher computational cost of the proposed method, as shown in Table~\ref{tab:dubins_results}.

 % Furthermore, the cost distribution of successful trajectories (Fig.~\ref{fig:dubins_cost_comparison}) is qualitatively similar upon decreasing the number of samples for the proposed method, indicating a consistent cost optimization with far fewer samples. Indeed, the mean cost of the successful trajectories is 5699 and 6942 respectively, demonstrating only a 22\% degradation in the cost optimization with 50x fewer samples. 

% Since our method keeps all samples outside of obstacles, it is always able to synthesize its nominal trajectory over a consistent ensemble of samples, whereas other methods might have no collision-free samples over which to optimize. 

% Increasingly sophisticated methods generally improve on success rate and relative cost as expected. However, a notable exception to this trend is for the BRT pen. + LRF method, which tends to have more trajectories time-out than the Obs. pen. + LRF method.

% We note that the BRT penalty methods suffer in cost and completion rate performance. While we might expect that such methods would improve performance by giving the controller additional information about its environment, Since the BRT set is strictly larger than the obstacle set, it is more likely for random samples to enter the BRT and thus be penalized. Thus, the nominal control sequence is synthesized from fewer samples. This supports the argument in favor of abstaining from cost penalties In this case, the highly cluttered environment makes it highly likely for randomly perturbed samples to enter the BRT and thus receive a high-cost penalty, making that sampled perturbation mostly ignored, resulting in a small number of meaningful samples over which to synthesize the control sequence. In contrast, our method's use of the LR filter to force all samples into safe environment regions allows the controller to synthesize the control sequence considering all the hallucinations.

% \begin{figure}[h!] 
% \begin{center}
% \vspace{-0.5em}
% \includegraphics[width=\columnwidth]{fig/dubins_sim_cost_dists_6_shorter.pdf}
% \caption{Histograms showing the distribution of episode outcomes for different controllers \& number of samples. Successful episodes have cost distribution in blue, while Timeout outcomes (T) are in orange and Crash outcomes (C) are in red. Each histogram represents 100 episodes.}
% \label{fig:dubins_cost_comparison}
% \end{center}
% \vspace{-1.3em}
% \end{figure}



\subsection{\label{case_drone6D}Autonomous Quadrotor Navigation}
%
\begin{figure}[h]
\begin{center}
\vspace{-0.5em}
\includegraphics[width=1.0\columnwidth]{fig/example_trajectories2_final_v3.png}
% \vspace{-0.8em}
% \caption{Left: The quadrotor environment with obstacles and goal location. Trajectories of the quadrotor for each method are shown in different colors. Right: The hallucinations for Obs cost, BRT cost, and the proposed method.\ucnote{the figure is a placeholder}}
\caption{Quadrotor environment with obstacles (gray), initial position (olive), and goal (dark green). Trajectories for each method are shown in different colors, obstacle intrusions are denoted by red.}
\vspace{-1.5em}
\label{fig:quadrotor}
\end{center}
\end{figure}
%
Next, we demonstrate our method on a 6-dimensional nonlinear quadrotor system. 
The system obeys the following dynamics:
%
% \setlength{\arraycolsep}{2pt} % Adjust spacing as needed
% \begin{equation}\label{eq:dyn_hw}
% {\fontsize{8.5}{10}\selectfont
% \dot{\state}
% = \begin{bmatrix} \dot{x} & \dot{y} & \dot{\theta} \end{bmatrix}
% = \begin{bmatrix} V \cos(\theta)+ d_x, & V \sin(\theta)+ d_y, & V \tan(\delta) / L \end{bmatrix}
% }
% \end{equation}
% \setlength{\arraycolsep}{5pt} % Reset spacing
% \vspace{-1.5em}
\begin{equation} \fontsize{8.5}{10}\selectfont
\begin{aligned}
    \dot{\state} &= \begin{bmatrix} \dot{x} & \dot{y} & \dot{z} & \dot{v_x} & \dot{v_y} & \dot{v_z} \end{bmatrix}^T \\
    &= \begin{bmatrix} v_x & v_y & v_z & a_g \tan u_\theta & -a_g \tan u_\phi & u_T - a_g \end{bmatrix}^T
\end{aligned}
\end{equation}
% good enouth for now

% \begin{equation}
% \dot{\state}
% = \begin{bmatrix} \dot{x} \\ \dot{y} \\ \dot{z} \\ \dot{v_x} \\ \dot{v_y} \\ \dot{v_z} \end{bmatrix}
% = \begin{bmatrix} v_x \\ v_y \\ v_z \\  a_g \tan u_\theta \\ -a_g \tan u_\phi \\ u_T - a_g \end{bmatrix}
% \end{equation}

where $a_g = 9.81 \text{ m/s}^2$ is the acceleration due to gravity, and acceleration $u_T$, roll $u_\phi$, and pitch $u_\theta$ are the controls $u = [u_T, u_\phi, u_\theta]$ of the system with ranges $u_T \in [7.81, 11.81] \text{ m/s}^2$ and $u_\phi, u_\theta \in [-17.18^{\circ},17.18^{\circ}]$. Zero mean Gaussian noise is added to the actions to simulate real-world actuation errors.

The environment is a $10 \text{m} \times 10 \text{m} \times 4 \text{m}$ room containing 10 spherical obstacles of varying sizes.
The BRT of the system is computed using the LevelSetToolbox \cite{mitchell2004toolbox}.
The task of the quadrotor is to reach a goal sphere with radius $0.3m$ around the goal state $\state_g$ (shown by the green sphere in Fig.~\ref{fig:quadrotor}), starting from a random initial position $\state_0$ while avoiding collisions with the obstacles.
%
The system must reach the goal location within a time horizon of $T = 10\text{s}$; otherwise, the episode is considered timeout.
If the system collides with an obstacle or the environment boundary, the episode is considered unsafe.

The cost function for the MPPI algorithm consists of the distance to the goal location, control effort penalty, and a safety penalty $P(\state)$:
%
\begin{equation}
\small % or \footnotesize or \scriptsize for different sizes
    \begin{aligned}\label{eq:cost_mppi_quadrotor}
    % {\mathcal{L}}(\state(\tvar),\ctrl(\tvar),\tvar)
    S = & (\state_t - \state_g)^T Q(\state_t - \state_g) \\
    &+ \| (1/\ctrl_{range})^T (\ctrl_t - \ctrl_{mean})\|_2^2 + P(\state_t)
    \end{aligned}
\end{equation}
%
where
$Q = \text{diag}([20, 20, 20, 0, 0, 0])$,
$\ctrl_{range} = [2, 0.3, 0.3]$ and $\ctrl_{mean} = [9.81, 0, 0]$. 
$P(\state)$ is 10 if the state is inside an obstacle or the BRT, corresponding to that controller’s cost penalty type. For all methods, $500$ hallucinations are simulated for a horizon of $0.3s$. Each method is tested starting from the same 100 randomly sampled initial positions over the environment with at least $5m$ distance from the goal location. The results are summarized in Table~\ref{tab:quadrotor_results}.
%
% $K_{Obs}$ and $K_{BRT}$ are the penalty weights for entering the obstacle set and BRT respectively.
% The penalty weights $K_{Obs}$ and $K_{BRT}$ alternate between $(K_{Obs},K_{BRT})=(10,0)$ for the 'Obs. penalty' and 'Obs. pen. + LRF' cases and $(K_{Obs},K_{BRT})=(0,10)$  for the 'BRT penalty' and 'BRT penalty + LRF' cases.
% where the safety rate is defined as the percentage of episodes where the system doesn't collide.
% The success rate is defined as the percentage of episodes where the system reaches the goal location without colliding.
% To compare the performance of the controllers, we compute the accumulated cost of the successful trajectories common to all methods using $K_{Obs}=10$ and $K_{BRT}=0$.

In this case, the obstacle penalty method leads to a very poor success rate. It is partially due to the added disturbance in the system that makes it challenging to ensure safety. A BRT penalty improves the safety rate from 14\% to 30\%, but it still does not enforce safety, as shown in Fig.~\ref{fig:quadrotor}. 
By penalizing a decrease in safety in the cost function and further optimizing the MPPI solution to encourage safety, Shield MPPI decreases the number of collisions to 21.
In contrast, safety is guaranteed by a LRF of the controls, resulting in a 100\% safety rate. Nevertheless, a significant portion of the hallucinations end up inside obstacles and are unable to contribute to the control synthesis meaningfully. Therefore, methods with safety-filtered actions experience many trajectories that fail to reach the goal within the given task horizon despite maintaining safety, in Fig.~\ref{fig:quadrotor} we show such timeout executions. 
By filtering the hallucinations, our method incorporates the rollouts that would otherwise be unmeaningful into the optimization. Doing so achieves the best success and safety rates among all methods while achieving a similar cost on successful trajectories compared to the others that guarantee safety.
% however, it results in a lower success rate compared to the safety rate, especially for the BRT. pen. + LRF case where the underlying MPPI controller is worse at planning in regards to safety and gets stuck when a last resort safety control is applied.
% The proposed method achieves the best performance among the methods that ensure safety, with a xyz\% success rate and xyz\% safety rate and a comparable accumulated cost compared to the other methods.

\vspace{-1em}
\begin{table}[h]
\caption{Quadrotor simulation results.}
\centering
\begin{tabularx}{\columnwidth}{|>{\centering\arraybackslash}X|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{2.3cm}|}
\hline
\textbf{Method} & \textbf{Success} & \textbf{Timeout} & \textbf{Failure} & \textbf{RelCost} \\ 
\hline
Obs. penalty & 14 & 0 & 86             & 0.43 $\pm$ 0.05 \\ 
BRT penalty & 30 & 0 & 70             & 0.50 $\pm$ 0.07 \\ 
Obs. pen. + LRF & 68 & 32 & \textbf{0}             & 1.08 $\pm$ 0.35 \\ 
BRT pen. + LRF & 65 & 35 & \textbf{0}             & 1.02 $\pm$ 0.38 \\ 
Shield MPPI & 69 & 10 & 21             & 0.84 $\pm$ 0.18 \\ 
DualGuard (Ours) & \textbf{75} & 25 & \textbf{0}             & 1.00  \\ %$\pm$ 0.33
\hline
\end{tabularx}
\label{tab:quadrotor_results}
\end{table}
\vspace{-1em}
