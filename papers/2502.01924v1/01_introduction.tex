\section{Introduction}

Co-optimizing safety and performance is a critical challenge in the design of controllers for autonomous systems, especially in safety-critical domains such as autonomous vehicles, UAVs, and robotics. In such settings, controllers must ensure that performance objectives (such as efficiency, speed, or agility) are met while guaranteeing that safety constraints are never violated. Achieving this balance can be viewed as a constrained optimal control problem, where the goal is to satisfy safety constraints throughout the trajectory while optimizing performance objectives.

Several methods have been proposed to address this co-optimization challenge. Dynamic programming (DP) offers a rigorous solution to constrained optimization \cite{one_stage_DP,Wang2024coopt_constrained}. Still, it is computationally intractable for many real-time applications due to the ``curse of dimensionality'', often associated with DP-based solutions. 
On the other hand, MPC is more feasible for real-time applications, leveraging its ability to generate optimized control sequences over a receding horizon \cite{schwenzer2021review,borrelli2017predictive}. 
Among these, Model Predictive Path Integral (MPPI) control is a sampling-based MPC method that has gained significant popularity due to its flexibility in handling complex dynamics, uncertainties, and non-linear systems \cite{mppi,mppi_tro}. 
MPPI leverages stochastic sampling to optimize control trajectories, offering a scalable and efficient way to generate control actions. 
However, ensuring safety within MPPI has proven challenging, as the basic framework does not inherently account for hard safety constraints. 
Consequently, various extensions have been proposed to incorporate safety, each solving the co-optimization problem differently.

Classical MPPI-based approaches encourage safety constraints by penalizing unsafe sampled trajectories in the cost function, such that they are effectively ignored in the sample aggregation process.
However, this approach cannot guarantee safety and its satisfaction depends on the penalty function. Another drawback is that computation is wasted on ignored samples, reducing the effective number of samples used for optimization.
To enforce safety, approaches such as safety filtering and barrier functions have emerged, where constraints are enforced as a post-optimization correction \cite{borquezFiltering2023,ames2016control}. 
Such approaches solve the MPPI problem first and apply safety filtering afterward, or combine these two approaches \cite{mppi_theory,mppi_shield,wabersich2021predictive}.%parallel_ra_mppi
However, these approaches tend to ignore the long-term effect of safety action on the performance. 
Moreover, the overall performance of the system is still limited by the underlying MPPI algorithm. 
Other procedures use probabilistic methods to address the safety-performance co-optimization, such as modifying the sampling distribution to favor safer trajectories \cite{safe_importance_sampling}, using stochastic safety certificates to steer away from unsafe regions \cite{learn_stochastic_barrier}, or improving safety indirectly by enhancing adaptability and robustness to uncertainty \cite{proactive_mppi, unscented_mppi}.

In this work, we introduce DualGuard-MPPI, a novel MPPI algorithm that incorporates safety constraints directly into the MPPI algorithm using Hamilton-Jacobi (HJ) reachability. 
Our key idea is to incorporate two safety filtering stages in the MPPI algorithm: first, we perform safety filtering along the sampled control perturbations, leading to provably safe ``hallucinations'' of the system trajectories, which are then used to optimize the performance objective. 
Second, we apply safety filtering on the selected control sequence before it is applied to the system to filter potential multi-modal sequences that may compromise safety when combined.
The proposed approach imposes safety constraints
throughout the trajectory optimization and control application stages, ensuring that the system operates within safe bounds at all times, without requiring any safety penalty tuning.
Moreover, since all generated samples are provably safe, they all contribute to performance optimization, effectively reducing the MPPI sampling variance and leading to a better performance, as compared to vanilla MPPI algorithm.   

In summary, the key contributions of this work are: 
% 
\begin{itemize}
  \item An MPPI framework that co-optimizes safety and performance while ensuring safety at all times;
  \item Elimination of safety penalty tuning within MPPI and improvement in sampling efficiency by guaranteeing that all sampled control sequences are safe.
  % \item Performance-only optimal control problem formulation, as safety is guaranteed in the sampling stage.
  \item Extensive simulations and hardware experiments demonstrating the superior performance and robustness of the proposed approach without compromising safety.
  % \item Providing safety guarantees in the presence of combinations of individually-safe multimodal executions.
\end{itemize}
