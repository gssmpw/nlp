\section{Background}

\subsection{Model Predictive Path Integral (MPPI)}

MPPI~\cite{mppi,mppi_tro} aims to solve a classical stochastic optimal control setting that minimizes the total cost assigned over a trajectory $\traj$ for a system with dynamics $\dot{\state} = \dyn(\state, \ctrl, \dstb)$. It tries to find the optimal control signal $\ctrlseq^*(\cdot)$ such that:
%
\begin{equation}
    \mathbf{u}^{*}(\cdot)=\argmin_{\mathbf{u}(\cdot)}\mathbb{E}_{\mathbb{Q}}\left[S(\traj)\right]
\end{equation}
%
%
\begin{equation}    S(\traj)=\phi(\state(\thoralt),\thoralt)+\int_{\tinit}^{\thoralt}{\mathcal{L}}(\state(\tvar),\ctrl(\tvar),\tvar)\mathrm{d}\tvar
\end{equation}
%
Where $\mathbb{Q}$ is any probability measure defined over the space of trajectories, with $\phi$ and $\mathcal{L}$ representing the terminal and running cost correspondingly, $S(\traj)$ is the state-dependent cost-to-go over the planning time horizon $\thoralt$.

The optimal probability measure is represented by $\mathbb{Q}^*$, the controlled dynamics induce another probability measure on the space of trajectories denoted $\mathbb{Q}(\ctrlseq)$. The MPPI algorithm aims to find the optimal control sequence by approximating $\ctrlseq$ that minimizes the relative entropy $\mathbb{D}_{KL}$ between the optimal and control-induced probability measures.
%
\begin{equation}
    \mathbf{u}^{*}(\cdot)=\argmin_{\mathbf{u}(\cdot)}\mathbb{D}_{KL}(\mathbb{Q}^*||\mathbb{Q}(\ctrl))
\end{equation}
%
The method addresses the inevitable requirement of applying the control in discrete time; for this, it considers as a solution the class of step functions:
%
\begin{equation}
\mathbf{u}(\state,\tvar)={\left\{\begin{array}{l l}{\vdots}\\ {\mathbf{u}(\state,j)}&{{\mathrm{if~}}j\Delta t\leq t<(j+1)\Delta t}\\ {\vdots}\end{array}\right.}
\end{equation}
%
It is shown that this optimal control sequence can be approximated by the following update law~\cite{mppi_theory}:
%
\begin{equation}\label{eq:update_law}
u(\state,j)^{\ast}\approx u(\state,j)+{\frac{\sum_{k=1}^{K}\exp(-(1/\lambda)S(\traj_{j,k}))\delta u_{j,k}}{\sum_{k=1}^{K}\exp(-(1/\lambda)S(\traj_{j,k}))}}
\end{equation}
%
Where $K$ is the number of random rollouts, $\lambda \in \mathbb{R}^+$ is the temperature parameter, $S(\traj_{j,k})$ and $\delta u_{j,k}$ are the cost-to-go and the randomly sampled control perturbation of the \textit{k}th rollout at the \textit{j}th discrete time step.

\begin{mdframed}[style=MyFrame,nobreak=false]

\textbf{Running example \textit{(Dub3D)}:} 
As a running example we consider a Dubins' car trying to reach a goal in a cluttered environment. The system can be represented by the following three dimensional dynamics:

\begin{equation}\label{eq:dyn_car} 
\dot{\state}
= \begin{bmatrix} \dot{x} \\ \dot{y} \\ \dot{\theta} \end{bmatrix}
= \begin{bmatrix} V \cos(\theta) \\ V \sin(\theta) \\ \ctrl \end{bmatrix}
\end{equation}

With $x,y$ position of the car's center, $\theta$ its orientation, $V$ its fixed speed and $\ctrl$ its angular speed input.

In Fig.~\ref{fig:mppi_car}, the gray spheres correspond to obstacles; in dotted lines we represent possible rollouts of the system following a randomly perturbed control sequence starting from the present state, we refer to this as the hallucinations of the system for a given state. 

\vspace{1em}
{\centering      \includegraphics[width=0.75\columnwidth]{fig/mppi_car_sim.png}
      \captionof{figure}{Visualization of hallucination step in the MPPI algorithm for a Dubin's car in a cluttered environment.}
      \label{fig:mppi_car} 
\par}

A natural cost function for this scenario would have a set of goal-seeking terms and safety-related terms. These safety-related terms would assign higher costs to trajectories that fail to avoid obstacles (shown in red), such that the update law in (\ref{eq:update_law}) prioritizes control perturbations that will keep the car safe (shown in a blue-to-purple gradient, with blue indicating lower costs).

\end{mdframed}



\subsection{Hamilton-Jacobi Reachability}

One way to guarantee safe operation of autonomous continuous-time dynamical systems is through Hamilton-Jacobi (HJ) reachability analysis. This approach calculates the Backward Reachable Tube (BRT) for a set of undesirable states $\targetset$. The BRT captures from which states even under optimal control the system is not able to avoid entering $\targetset$ within some time horizon $\thor$.

In HJ reachability, the BRT computation is formulated as a zero-sum game between control and disturbance.
This results in a robust optimal control problem that can be solved using the dynamic programming principle. 
First, a obstacle function $\targetfunc(\state)$ is defined whose sub-zero level set is the obstacle set, i.e. . $\targetset= \{\state : \targetfunc(\state)\leq 0\}$. To characterize all states that could enter $\targetset$ at any point within the time horizon a cost function is defined as the minimum distance to $\targetset$ over time:
%  Typically, $l(x)$ is defined as a signed distance function to $\mathcal{L}$.
% 
\begin{equation}
\costfunctional(\state, \tvar, \cfunc, \dfunc)=\min _{\tdummy \in[\tvar, \thor]} \targetfunc (\state(\tdummy))
\end{equation}
% 
The goal is to capture this minimum distance for optimal system trajectories. Thus, we compute the optimal control that maximizes this distance (drives the system away from the obstacles) and the worst-case disturbance signal that minimizes the distance. The value function corresponding to this robust optimal control problem is:
% 
\begin{equation}\label{eq:hji}
 \vfunc(\state, \tvar)=\adjustlimits\inf_{\strat \in \stratset(\tvar)} \sup_{\cfunc} \{\costfunctional(\state, \tvar, \cfunc, \strat[\ctrl](\cdot))\},
\end{equation}
% 
where $\stratset(t)$ defines the set of non-anticipative strategies.
The value function in (\ref{eq:hji}) can be computed using dynamic programming, which results in the following final value Hamilton-Jacobi-Isaacs Variational Inequality (HJI-VI) \cite{bansal2017hamilton,lygeros2004reachability,mitchell2005time}:
% 
\begin{equation} \label{eq:pde}
    \begin{aligned}
    \min \{&D_{\tvar} \vfunc(\state, \tvar) + \ham(\state, \tvar, \nabla \vfunc(\state, \tvar)), \targetfunc(\state) - \vfunc(\state, \tvar) \} = 0 \\
    &\vfunc(\state, \horizon) = \targetfunc(\state), \quad \text{for} \ \tvar \in \left[\tinit, \horizon\right]
    % \text{and} \ \state \in \sset
    \end{aligned}
\end{equation}
% 
$D_t$ and $\nabla$ represent the time and spatial gradients of the value function. $\ham$ is the Hamiltonian, which optimizes over the inner product between the spatial gradients of the value function and the dynamics $\dot{\state} = \dyn(\state, \ctrl, \dstb)$ to compute the optimal control and disturbance:
% 
\begin{equation}\label{eq:HJIVI_ham_live}
    \ham(\state, \tvar, \nabla \vfunc(\state, \tvar)) = \min_{\ctrl \in \cset} \max_{\dstb \in \dset} \nabla \vfunc(\state, \tvar) \cdot \dyn(\state, \ctrl, \dstb)
\end{equation}
% 
%Intuitively, the term $l(x)-V(x, t)$ in (\ref{eq:pde}) restricts system trajectories that enter and leave the target set, enforcing that any trajectory with a negative distance at any time will continue to have a negative distance for the rest of the time horizon. 
For a detailed derivation and discussion of the HJI-VI in (\ref{eq:pde}), we refer the interested readers to \cite{mitchell2005time} and \cite{bansal2017hamilton}. 

Once the value function is obtained, the BRT is given as the sub-zero level set of the value function:
% 
\begin{equation}
\brs(\tvar)=\{\state: \vfunc(\state, \tvar) \leq 0\}
\end{equation}
% 
The corresponding optimal safe control can be derived as:
% 
\begin{equation}\label{eq:optctrl}
\ctrl^{*}_{safe}(\state, \tvar)=\argmax_{\ctrl \in \cset} \min_{\dstb \in \dset}\nabla \vfunc(\state, \tvar) \cdot \dyn(\state, \ctrl, \dstb)
\end{equation}
% 
%The system can guarantee reaching the target set as long as it starts inside the BRT and applies the optimal control in (\ref{eq:optctrl}) at the BRT boundary. The optimal adversarial disturbance can be similarly obtained as (\ref{eq:optctrl}).

%This formulation can also be used to provide safety guarantees by switching the roles of the control and disturbance in \eqref{eqn:ham}.
%In that case, the BRT represents the initial states that will eventually be driven into the target by optimal disturbance, despite the best control effort.
%Thus, safety can be maintained as long as the system stays outside the BRT and applies optimal control at the boundary of the BRT.

In safety-ensuring applications, we want to guarantee that the set of undesired states is never reached \textit{at any time}; for this reason, we use a time-converged BRT, as the set of unsafe states often stops growing after some amount of time. Consequently, we can use the converged value function $V(\state)$, which is no longer a function of time $\tvar$, which, when used to synthesize safety controllers, gives an expression identical to (\ref{eq:optctrl}) without its time dependency.

To compute the value function and obtain the BRT and the optimal controller, we can rely on methods that solve the HJI-VI in \eqref{eq:hji} numerically \cite{bansal2020provably, bui2022optimizeddp,mitchell2004toolbox,hj_reach_ASL2023} or using learning-based methods \cite{bansal2021deepreach,fisac2019bridging}.

\subsection{Least Restrictive Filtering}

If the BRT and associated converged value function are known, a strategy known as \textit{Least Restrictive Filtering} (LRF) can be deployed to guarantee the safe operation of the system.
The least restrictive controller is constructed as follows:
% 
\begin{equation}\label{eq:lst_restrict_safety_ctrl}
\controller(\state, \tvar) = \begin{cases}
  \ctrlnom(\state, \tvar) & \vfunc(\state)> 0 \\
   \controller^*_{\text{safe}}(\state) & \vfunc(\state) = 0
\end{cases}
\end{equation}
% 
Here, $\ctrlnom(\state, \tvar)$ corresponds to an arbitrary nominal controller that, in most cases, will be trying to optimize a performance criterion unrelated to the system's safety. This control is used when the value function $\vfunc(\state)$ is positive, as the system is not at risk of breaching safety. Whenever the system reaches the boundary of the BRT ($\vfunc(\state)=0$), it switches to $u^{*}_{safe}(x)$ \eqref{eq:optctrl} as this optimal control is guaranteed to maintain or increase $\vfunc(\state)$ keeping the system from entering the unsafe operation set of states determined by the BRT. We refer the reader to \cite{borquezFiltering2023} for details on this filtering technique and proof of the safety guarantees.

\begin{mdframed}[style=MyFrame,nobreak=false]

\textbf{Running example \textit{(Dub3D)}:}

Considering the dynamics in (\ref{eq:dyn_car}), and the gray obstacles as $\targetset$. We can use the HJI-VI in (\ref{eq:hji}) to calculate the value function and associated BRT, over which we can implement a least restrictive filter strategy to guarantee the system's safety.

In Fig.~\ref{fig:mppi_filter}, we show in red a trajectory for the system following a randomly perturbed control sequence that results in an unsafe execution. In blue, we show how this initially unsafe execution can be made safe by adding a least restrictive safety filtering step to each control action in the sequence just before applying it.
% The least restrictive filtering guaranteed safety by using the optimal control given by (\ref{eq:optctrl}) whenever the system's state reaches the boundary of the safe set defined by the BRT, while allowing direct application of the controls if safety is not at risk.

\vspace{0.5em}
{\centering      \includegraphics[width=0.75\columnwidth]{fig/mppi_filter.png}
      \captionof{figure}{Unsafe trajectory for the Dubin's car (red) transformed to a safe execution by using Least Restrictive Filtering (blue).}
      \label{fig:mppi_filter} 
}

\end{mdframed}









%%%%%%%%%%%%%%%%%DUB SIM Table full%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\setlength\tabcolsep{1pt}
\begin{table}[b]
\fontsize{7pt}{7pt}\selectfont
\caption{Results for the Safe Planar Navigation case study. The results are computed over 100 episodes.}
\centering
\renewcommand{\arraystretch}{1.2}   % Adjust row height for vertical centering
\begin{tabularx}{\columnwidth}{|>{\centering\arraybackslash}p{0.6cm}|>{\centering\arraybackslash}p{2.1cm}|>{\centering\arraybackslash}p{0.78cm}|>{\centering\arraybackslash}p{0.78cm}|>{\centering\arraybackslash}p{0.78cm}|>{\centering\arraybackslash}X|>{\centering\arraybackslash}p{1.4cm}|}
\hline
\textbf{K}&\textbf{Method}&\textbf{Success}&\textbf{Time-out}&\textbf{Failure}&\textbf{RelCost $(\mu\pm\sigma)$} & \textbf{CompTime (ms)}\\
\hline
\multirow{6}{*}{1000}
& Obs. penalty    & 49 &  1 & 50 & 1.34 ± 1.19 & 46.8 ± 0.8 \\
& BRT penalty     & 72 & 21 &  7 & 1.89 ± 2.11 & 46.8 ± 0.6 \\
& Obs. pen. + LRF & 80 & 20 &  0 & 1.35 ± 1.20 & 46.8 ± 0.5 \\
& BRT pen. + LRF  & 74 & 26 &  0 & 1.90 ± 2.12 & 46.9 ± 3.2 \\
& Shield MPPI     & 83 & 17 &  0 & 1.32 ± 0.87 & 132.5 ± 170 \\
& DualGuard (Ours)      & 99 &  1 &  0 & 1.00           & 65.3 ± 1.3 \\
\hline
\multirow{6}{*}{250}
& Obs. penalty    & 31 &  0 & 69 & 1.10 ± 0.24 & 36.8 ± 0.7 \\
& BRT penalty     & 58 & 15 & 27 & 2.35 ± 3.40 & 36.7 ± 0.2 \\
& Obs. pen. + LRF & 81 & 19 &  0 & 1.27 ± 0.92 & 36.7 ± 0.6 \\
& BRT pen. + LRF  & 69 & 31 &  0 & 1.86 ± 2.03 & 36.7 ± 0.2 \\
& Shield MPPI     & 78 & 21 &  1 & 1.40 ± 0.86 & 119.8 ± 81 \\
& DualGuard (Ours)      & 98 &  2 &  0 & 1.00           & 50.2 ± 0.8 \\
\hline
\multirow{6}{*}{60}
& Obs. penalty    & 11 &  0 & 89 & 0.99 ± 0.03 & 33.7 ± 0.6 \\
& BRT penalty     & 43 & 21 & 36 & 3.16 ± 4.17 & 33.8 ± 0.9 \\
& Obs. pen. + LRF & 70 & 30 &  0 & 0.99 ± 0.03 & 33.7 ± 0.5 \\
& BRT pen. + LRF  & 55 & 45 &  0 & 3.16 ± 4.17 & 33.7 ± 0.3 \\
& Shield MPPI     & 64 & 36 &  0 & 1.08 ± 0.14 & 146.3 ± 148 \\
& DualGuard (Ours)      & 96 &  4 &  0 & 1.00           & 45.7 ± 0.3 \\
\hline
% \multirow{6}{*}{20}
% & Obs. penalty    &  8 &  0 & 92 & 0.92 ± 0.14 & 33.2 ± 0.6 \\
% & BRT penalty     & 19 & 11 & 70 & 0.93 ± 0.14 & 33.2 ± 1.2 \\
% & Obs pen. + LRF  & 45 & 55 &  0 & 0.92 ± 0.14 & 33.1 ± 0.4 \\
% & BRT pen. + LRF  & 30 & 70 &  0 & 0.93 ± 0.14 & 33.3 ± 0.7 \\
% & Shield MPPI     & 42 & 56 &  2 & 3.34 ± 4.72 & 150.9 ± 108 \\
% & DualGuard (Ours)      & 91 &  9 &  0 & 1           & 44.8 ± 0.5 \\
% \hline

\end{tabularx}
\label{tab:dubins_results}
\end{table}
