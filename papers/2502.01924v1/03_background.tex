\section{\label{background}Background}

\subsection{\label{background_mppi}Model Predictive Path Integral (MPPI)}

MPPI~\cite{mppi,mppi_tro} aims to solve a classical stochastic optimal control problem using a zeroth-order sampling-based MPC scheme.
Specifically, at each state, MPPI samples K random control sequences around a nominal control sequence for a time horizon $\hat{T} \leq \tvar_f$ discretized over $H$ evenly spaced steps. The control input for the \textit{k}th sequence at \textit{j}th time step is given by: $u^k(x, j) = u(x, j) + \delta_j^k$, where $u(x, j)$ is a nominal control sequence and $\delta_j^k$ a randomly sampled control perturbation. Next, the state trajectory $\xi_j^k$ and the cost to go $S(\xi_j^k)$ corresponding to each control sequence is computed. It is shown that the optimal control sequence can be approximated by the following update law~\cite{mppi_theory}:
%
\begin{equation}\label{eq:update_law}
u(\state,j)^{\ast}\approx u(\state,j)+{\frac{\sum_{k=1}^{K}\exp[-(1/\lambda)S(\traj_j^k)]\delta_j^k}{\sum_{k=1}^{K}\exp[-(1/\lambda)S(\traj_j^k)]}}
\end{equation}
%
where $\lambda \in \mathbb{R}^+$ is the temperature parameter that weighs the contributions from different control sequences. The first control in the optimal sequence is applied to the system and the entire process is repeated at the next time step.

%
\begin{mdframed}[style=MyFrame,nobreak=false]
\textbf{Running example \textit{(Safe Planar Navigation)}:} 
As a running example we consider a Dubins' car trying to reach a goal in a cluttered environment. The system can be represented by the following three dimensional dynamics:
%
\begin{equation}\label{eq:dyn_car} \fontsize{8.5}{10}\selectfont
\dot{\state}
= \begin{bmatrix} \dot{x} & \dot{y} & \dot{\theta} \end{bmatrix}
= \begin{bmatrix} V \cos(\theta) & V \sin(\theta) & \ctrl \end{bmatrix}
\end{equation}
%
With $x,y$ position of the car's center, $\theta$ its orientation, $V=2 \text{ m/s}$ its fixed speed and $\ctrl \in [-3, 3] \text{ rad/s}$ its angular speed input. The failure set in this case is a $10 \text{m} \times 10 \text{m}$ square enclosure, randomly cluttered with 40 circular obstacles ranging in diameter from 0.35~m to 3.5~m, as shown in Fig ~\ref{fig:mppi_car} (zoomed in) and in Fig.~\ref{fig:dubins_brt} (full scale). The cost function S penalizes the distance to the goal and obstacle penetration (to encourage safety).

Dotted lines in Fig.~\ref{fig:mppi_car} represent possible rollouts of the system following a randomly perturbed control sequence starting from the current state. We refer to these as the hallucinations of the system for a given state. 

\vspace{1em}
{\centering      \includegraphics[width=0.65\columnwidth]{fig/mppi_car_sim.png}
      \captionof{figure}{Visualization of hallucination step in the MPPI algorithm.}
      \label{fig:mppi_car} 
\par}

The hallucinations that fail to avoid obstacles achieve a higher cost (shown in red). Thus, the update law in (\ref{eq:update_law}) prioritizes control perturbations that will keep the car safe (shown in a blue-to-purple gradient, with blue indicating lower costs). However, in general, it could be possible that all samples lead to safety violations. Moreover, if the number of safe samples is small, it could lead to a high variance in the performance of MPPI, necessitating the need to account for safety constraints rigorously.

\end{mdframed}


\subsection{\label{background_hj}Hamilton-Jacobi Reachability}

One way to guarantee safe operation of autonomous continuous-time dynamical systems is through Hamilton-Jacobi (HJ) reachability analysis. This approach involves computing the Backward Reachable Tube (BRT) of the failure set $\targetset$. The BRT captures the states from which the system is not able to avoid entering $\targetset$ within some time horizon $\thor$, despite the best control effort.

In HJ reachability, the BRT computation is formulated as a zero-sum game between control and disturbance. Specifically, a cost function is defined to capture the minimum distance to $\targetset$ over time:
% 
\begin{equation}
\costfunctional(\state, \tvar, \cfunc, \dfunc)=\min _{\tdummy \in[\tvar, \thor]} \targetfunc (\state(\tdummy))
\end{equation}
% 
The goal is to capture this minimum distance for optimal system trajectories. Thus, we compute the optimal control that maximizes this distance (drives the system away from the failure set) and the worst-case disturbance signal that minimizes the distance. The value function corresponding to this robust optimal control problem is:
% 
\begin{equation}\label{eq:hji}
 \vfunc(\state, \tvar)=\adjustlimits\inf_{\strat \in \stratset(\tvar)} \sup_{\cfunc} \{\costfunctional(\state, \tvar, \cfunc, \strat[\ctrl](\cdot))\},
\end{equation}
% 
where $\stratset(t)$ defines the set of non-anticipative strategies for the disturbance \cite{bansal2017hamilton}.
The value function in (\ref{eq:hji}) can be computed using dynamic programming, which results in the following final value Hamilton-Jacobi-Isaacs Variational Inequality (HJI-VI) \cite{bansal2017hamilton,lygeros2004reachability,mitchell2005time}:
% 
\begin{equation} \label{eq:pde}\fontsize{9.5}{10}\selectfont
    \begin{aligned}
    \min \{&D_{\tvar} \vfunc(\state, \tvar) + \ham(\state, \tvar, \nabla \vfunc(\state, \tvar)), \targetfunc(\state) - \vfunc(\state, \tvar) \} = 0 \\
    &\vfunc(\state, \horizon) = \targetfunc(\state), \quad \text{for} \ \tvar \in \left[\tinit, \horizon\right]
    % \text{and} \ \state \in \sset
    \end{aligned}
\end{equation}
% 
$D_t$ and $\nabla$ represent the time and spatial gradients of the value function. $\ham$ is the Hamiltonian, which optimizes over the inner product between the spatial gradients of the value function and the dynamics:
% 
\begin{equation}\label{eq:HJIVI_ham_live}
    \ham(\state, \tvar, \nabla \vfunc(\state, \tvar)) = \min_{\ctrl \in \cset} \max_{\dstb \in \dset} \nabla \vfunc(\state, \tvar) \cdot \dyn(\state, \ctrl, \dstb)
\end{equation}
% 
%Intuitively, the term $l(x)-V(x, t)$ in (\ref{eq:pde}) restricts system trajectories that enter and leave the target set, enforcing that any trajectory with a negative distance at any time will continue to have a negative distance for the rest of the time horizon. 
% For a detailed derivation and discussion of the HJI-VI in (\ref{eq:pde}), we refer the interested readers to \cite{mitchell2005time} and \cite{bansal2017hamilton}. 
Once the value function is obtained, the BRT is given as the sub-zero level set of the value function:
% 
\begin{equation}
\brs(\tvar)=\{\state: \vfunc(\state, \tvar) \leq 0\}
\end{equation}
% 
The corresponding optimal safe control can be derived as:
% 
\begin{equation}\label{eq:optctrl}
\ctrl^{*}_{safe}(\state, \tvar)=\argmax_{\ctrl \in \cset} \min_{\dstb \in \dset}\nabla \vfunc(\state, \tvar) \cdot \dyn(\state, \ctrl, \dstb)
\end{equation}
% 
%The system can guarantee reaching the target set as long as it starts inside the BRT and applies the optimal control in (\ref{eq:optctrl}) at the BRT boundary. The optimal adversarial disturbance can be similarly obtained as (\ref{eq:optctrl}).

%This formulation can also be used to provide safety guarantees by switching the roles of the control and disturbance in \eqref{eqn:ham}.
%In that case, the BRT represents the initial states that will eventually be driven into the target by optimal disturbance, despite the best control effort.
%Thus, safety can be maintained as long as the system stays outside the BRT and applies optimal control at the boundary of the BRT.

In safety-ensuring applications, we want to guarantee that the system does not enter $\targetset$ \textit{at any time}; for this reason, we use a time-converged BRT, as the set of unsafe states often stops growing after some amount of time. Consequently, we can use the converged value function $V(\state)$, which, when used to synthesize safety controllers, gives an expression identical to (\ref{eq:optctrl}) without its time dependency.

To compute the value function and obtain the BRT and the optimal controller, we can rely on methods that solve the HJI-VI in \eqref{eq:hji} numerically \cite{mitchell2004toolbox,hj_reach_ASL2023} or using learning-based methods \cite{bansal2021deepreach,fisac2019bridging}.
%bansal2020provably, bui2022optimizeddp
\subsection{\label{background_lr}Least Restrictive Filtering}

If the BRT and associated converged value function are known, a \textit{Least Restrictive Filter} (LRF) can be deployed to guarantee the safe operation of the system.
The LRF is constructed as follows:
% 
\begin{equation}\label{eq:lst_restrict_safety_ctrl}
\controller(\state, \tvar) = \begin{cases}
  \ctrlnom(\state, \tvar) & \vfunc(\state)> 0 \\
   \controller^*_{\text{safe}}(\state) & \vfunc(\state) = 0
\end{cases}
\end{equation}
% 
Here, $\ctrlnom(\state, \tvar)$ corresponds to an arbitrary nominal controller that might optimize a performance criterion without enforcing safety constraints. This control is used when the value function $\vfunc(\state)$ is positive, as the system is not at risk of breaching safety. Whenever the system reaches the boundary of the BRT ($\vfunc(\state)=0$), it switches to $u^{*}_{safe}(x)$ \eqref{eq:optctrl} as this optimal control is guaranteed to maintain or increase $\vfunc(\state)$ keeping the system from entering the unsafe states determined by the BRT, thereby enforcing safety at all times. We refer the reader to \cite{borquezFiltering2023} for details on this filtering technique and proof of the safety guarantees.

\begin{mdframed}[style=MyFrame,nobreak=false]

\textbf{Running example \textit{(Safe Planar Navigation)}:}

We can use the HJI-VI in (\ref{eq:hji}) to calculate the value function and associated BRT, over which we can implement a LRF to guarantee the system's safety. In this case considering the dynamics in \eqref{eq:dyn_car} and the environment definition, we compute the value function using the LevelSetToolbox{\cite{mitchell2004toolbox}}.

In Fig.~\ref{fig:mppi_filter}, the purple trajectory represents the system's response to a perturbed control sequence, reaching a critical state at the boundary of the safe set (yellow dot). The boundary of the safe set at this state is illustrated with a dotted line. The red trajectory shows the continuation of the system using the perturbed controls, which leads to the failure set. By contrast, the blue trajectory demonstrates how the unsafe execution can be made safe. This is achieved by applying a LRF step to each control action in the sequence before execution.
% The least restrictive filtering guaranteed safety by using the optimal control given by (\ref{eq:optctrl}) whenever the system's state reaches the boundary of the safe set defined by the BRT, while allowing direct application of the controls if safety is not at risk.

\vspace{0.5em}
{\centering      \includegraphics[width=0.45\columnwidth]{fig/mppi_filter_v2.png}
      \captionof{figure}{Trajectory correction by LRF. System under perturbed controls (purple) reaches the safe set boundary (yellow dot). The red path shows an unsafe execution, while the blue path demonstrates how safety filtering maintains the system outside the failure set.}
      \label{fig:mppi_filter} 
}

\end{mdframed}

