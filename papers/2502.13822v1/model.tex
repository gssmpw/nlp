%\section{Problem formulation}

%\subsection{General Markov chain}
%Throughout the paper, we consider a irreducible and aperiodic Markov chain $\{s_t\}_{t \geq 0}$ with state space $\mathcal{S}$ and transition kernel $P$. It can be guaranteed that such a Markov chain admits a unique stationary distribution, denoted as $\mu$. Furthermore, let $\lambda$ denote the spectral expansion of the Markov chain, and assume that the spectral gap $1-\lambda > 0$. We refer readers to Appendix \ref{app:MC-basics} for the specific definitions of these terms. Notably, the theoretical results in this paper does \emph{not} require the Markov chain to be reversible.

\subsection{TD learning with linear function approximation}
% \weichen{need to rewrite: plan is to briefly introduce the basic ideas of TD, following the precedent of, for example, \cite{mou2020linear}. Details postpone to appendix.}
% \yuting{yes, make it short. I think we might even start with solving a linear system of equations without really going into details about MDPs.}
% \weichen{Please check the current version}


Consider a Markov Reward Process (MRP), where a Markov chain  on a state space $\mathcal{S}$ is associated with a \emph{reward function} $r: \mathcal{S} \to [0,1]$ that maps each state to a reward. 
We observe a sequence of state-reward pairs, 
\begin{align*}
	(s_0,r_0),\ldots (s_t, r_t),\ldots
\end{align*}
where, for each $t$, $r_t = r(s_t)$ and $s_{t+1} \sim P(\cdot \mid s_t)$. 
The task of \emph{value function} evaluation concerns with estimating a function $V:\mathcal{S} \to \mathbb{R}$, where  for every $s \in \mathcal{S}$, 
\begin{align*}
	V(s):=\mathbb{E} \Bigg[\sum_{t=0}^\infty \gamma^t r(s_t)\Bigg].
\end{align*}
Here, $\gamma \in [0,1)$ denotes a discounted factor of future rewards towards the current state. 
In practice, the state space $\mathcal{S}$ used to describe the environment's configuration is often prohibitively large, making it necessary to consider various approximations of $V$. The most simple and tractable form of approximation is the linear function approximation, which has received considerable attention in literature; see, e.g., \cite{bhandari2018finite,patil2023finite,dalal2018finite,li2023sharp,samsonov2023finitesample,samsonov2024gaussian,wu2024statistical}.  
Specifically, the value function $V$ of a policy is approximated by the linear function
\begin{align}\label{eq:linear-approximation}
	 V_{\bm\theta}(s) := {\bm\phi}(s)^\top {\bm\theta}, \qquad \quad  s \in \mathcal{S},
\end{align}
for a set of feature maps $\bm{\phi}: \mathcal{S} \to \mathbb{R}^d$ and a linear coefficient vector  $\bm{\theta} \in \mathbb{R}^d$. Throughout the paper, we assume that $\|\bm{\phi}(s)\|_2 \leq 1$ for all $s \in \mathcal{S}$. 
Recall that we use $\mu$ to denote the stationary distribution of the Markov chain. 
The optimal coefficient vector, denoted as $\bm{\theta}^\star$, is defined by the projected Bellman equation \citep{tsitsiklis1997analysis}, which admits the fixed point equation 
\begin{align}
\label{eq:defn-theta-star}
\bm{A} \bm{\theta}^{\star}=\bm{b}, 
\end{align}
where %the matrix $\bm{A} \in \mathbb{R}^{d \times d}$ and the vector $\bm{b} \in \mathbb{R}^d$ are defined respectively as
%\begin{subequations} 
\begin{align}
	\bm{A} :=\mathop{\mathbb{E}}\limits _{s\sim\mu,s'\sim P(\cdot\mid s)}\Big[\bm{\phi}(s)\left(\bm{\phi}(s)-\gamma\bm{\phi}(s')\right)^{\top}\Big]\in\mathbb{R}^{d\times d},\quad
	\text{and}\quad \bm{b} :=\mathop{\mathbb{E}}\limits _{s\sim\mu}\Big[\bm{\phi}(s)r(s)\Big]\in\mathbb{R}^{d}.
	\label{eq:defn-At-mean}
\end{align}
%\end{subequations}
%Here, we use $\mu$ to denote the stationary distribution of $P^{\pi}.$
%As a result, to find the best linear function approximation of the value function, it is sufficient to find the solution to the linear equation~\eqref{eq:defn-theta-star}. 
 Below we indicate $\bm{\Sigma}$ as the feature gram matrix, i.e.
\begin{align}\label{eq:defn-Sigma}
	\bm{\Sigma} := \mathbb{E}_{s \sim \mu } \Big[\bm{\phi}(s)\bm{\phi}^\top(s)\Big],
\end{align}
and use $\lambda_{\Sigma}$ and $\lambda_0$ to denote its largest and smallest eigenvalues respectively. We assume $\lambda_0 > 0$.
% , namely
% \begin{align}
% \label{eq:defn-lambda0}
% \lambda_0 := \lambda_{\min}(\bm{\Sigma}), \quad \text{and} \quad \lambda_{\Sigma} := \lambda_{\max}(\bm{\Sigma}).
% \end{align}

\paragraph{The TD learning algorithm.} 
Stochastic approximation (SA) is a standard tool to solve the fixed point equation problems of the form \eqref{eq:defn-theta-star} given a sequence of random observations $\{(\bm{A}_t, \bm{b}_t)\}_{t\geq 1}.$
When specialized to the above setting, it is referred to as the Temporal Difference (TD) learning algorithm \citep{sutton1988learning}. 
% To estimate the optimal vector $\bm{\theta}^*$ given by \eqref{eq:defn-theta-star} we use the observed data, consisting of $(T+1)$ state-reward pairs, 
% %Given sequentially observed samples  
% \begin{align*}
% 	\{(s_0,r_0),(s_1,r_1),\ldots (s_T, r_T)\},
% \end{align*}
% where, for each $t=0,1,\ldots,{T-1}$, $r_t = r(s_t)$ and $s_{t+1} \sim P(\cdot \mid s_t)$. 
% The TD learning algorithm is a stochastic approximation algorithm that iteratively approximates the solution to the linear system $\bm{A} \bm{\theta} = \bm{b}$. 
Specifically, given a sequence of pre-selected step size $\{\eta_t\}_{t\geq 1}$ and an initial estimator $\bm{\theta}_0 = \bm{0}$, TD proceeds using the updating rule 
%\begin{subequations}
	\label{eq:TD-update-all} 
	\begin{align}
	\bm{\theta}_{t} & =\bm{\theta}_{t-1}-\eta_{t}(\bm{A}_{t}\bm{\theta}_{t-1}-\bm{b}_{t}),
	\label{eq:TD-update-rule}
\end{align}
where 
\begin{align}
% \label{eq:defn-At-bt}
\bm{A}_{t}  :=\bm{\phi}(s_{t-1})\left(\bm{\phi}(s_{t-1})-\gamma\bm{\phi}(s_{t})\right)^{\top} \quad
\text{and }\quad \bm{b}_{t} :=\bm{\phi}(s_{t-1})r_{t-1}.\label{eq:defn-At}
\end{align}
%\end{subequations}
After $T$ iterations, we deploy Polyak-Ruppert averaging \citep{polyak1992acceleration,ruppert1988efficient} and compute 
\begin{align}
	\label{eq:TD-averaging}
	\bar{\bm{\theta}}_T & =\frac{1}{T}\sum_{t=1}^{T}\bm{\theta}_{t}, 
\end{align}
as the estimator for $\bm{\theta}^\star$. In this work, we follow the precedent of \cite{polyak1992acceleration,ruppert1988efficient} and choose \emph{polynomial-decaying} stepsizes $\eta_t = \eta_0 t^{-\frac{1}{2}}$ with $\alpha \in (\frac{1}{2},1)$. %\ale{Say something about our choice of polynomially decaying stepsizes?}\weichen{check}





