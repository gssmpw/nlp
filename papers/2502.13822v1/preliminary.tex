\section{Preliminary facts}

\subsection{Markov chain basics}\label{app:MC-basics}

%\textcolor{violet}{Need to settle on notation about Radon-Nykodin derivatives, conditional densities, transition kernels, etc}
\subsubsection{Stationary distribution and corresponding norm}
Let $\mathcal{S}$ denote the state space of the Markov chain, which we assumed throughout to be a Borel space, and let $\mathscr{F}$ denote the corresponding Borel $\sigma$-algebra. For every $x \in \mathcal{S}$, the \emph{transition kernel} $P(x,\cdot)$ is a probability measure on $\mathcal{S}$ defined as
\begin{align*}
P(x,B) = \mathbb{P}(s_2 \in B \mid s_1 = x), \quad B \in \mathscr{F}
\end{align*}
which is guaranteed to be a regular conditional distribution.  
By definition, the  \emph{stationary distribution} $\mu$ is a probability distribution on $(\mathcal{S},\mathscr{F})$ satisfying
\begin{align*}
\mu(B) = \int_{y \in \mathcal{S}}P(y,B) \mu(\mathrm{d}y), \quad \forall B \in \mathcal{F}.
\end{align*}
 For a vector-valued function $\bm{g}: \mathcal{S} \to \mathbb{R}^{d}$, we use $\mu(\bm{G})$ to denote $\mathbb{E}_{x \sim \mu} [\bm{g}(x)]$, if well-defined. Furthermore, for simplicity, we use $\bm{g}^{\parallel}$ and $\bm{g}^{\perp}$ to denote the functions
\begin{align*}
&\bm{g}^{\parallel}(x) = \mu(\bm{g}), \quad \text{and} \\ 
&\bm{g}^{\perp}(x) = \bm{g}(x) - \bm{g}^{\parallel}(x), \quad \forall x \in \mathcal{S},
\end{align*}
provided that $\mu(\bm{g})$ exists.
We denote with $L_2(\mu)$ the space of Borel functions $\bm{f}\colon \mathcal{S} \to \mathbb{R}^{d}$ such that $\int_{\mathcal{S}} \| \bm{f}(x) \|^2 \mu(dx) < \infty $. Then, $L_2(\mu)$ is a Hilbert space, %with For $\bm{g}_1,\bm{g}_2 \in L_2(\mu)$$: \mathcal{S} \to \mathbb{R}^{d}$ such that , we define the 
with \emph{inner product} given by
\begin{align*}
\langle \bm{g}_1, \bm{g}_2 \rangle_{\mu} %= \mathbb{E}_{x \sim \mu} [\bm{g}_1^\top(x)\bm{g}_2(x)] 
= \int_{\mathcal{S}} \bm{g}_1^\top(x)\bm{g}_2(x)\mu(\mathrm{d}x), \quad \bm{g}_1,\bm{g}_2 \in L_2(\mu),
\end{align*}
which, in turn, induces the \emph{$\mu$-norm} 
\begin{align*}
\|\bm{g}\|_{\mu} %= \left(\mathbb{E}_{x \sim \mu} \|\bm{g}(x)\|_2^2 \right)^{\frac{1}{2}} 
= \left(\int\|\bm{g}(x)\|_2^2 \mu(\mathrm{d}x)\right)^{\frac{1}{2}}, \quad \bm{g} \in L_2(\mu).
\end{align*}
Note that, for a fixed vector $\bm{v}$, $\|\bm{v}\|_{\bm{\mu}}$ reduces to the Euclidean norm $\|\bm{v}\|_{2}$. 
We shall interchange these two ways of writing for ease of exposition. 


% \textcolor{violet}{*ALE*: say that this extends to matrix-valued function under the inner product given by trace of the product}\weichen{no longer needed.}
% \subsubsection{Adjoint transition kernel and corresponding operators}
%\textcolor{violet}{I would write $d \mu(x)$ instead of $\mu(dx)$}

To the transition kernel $P$ admitting a stationary distribution $\mu$ there corresponds a bounded linear operator, denoted as $\mathcal{P}$, which maps a vector-valued function $\bm{g} \in L_2(\mu)$ %$:\mathcal{S} \to \mathbb{R}^{d}$ 
to another vector-valued function in $L_2(\mu)$, denoted as $\mathcal{P}\bm{g}$, given by
\begin{align*}
x \in \mathcal{S} \mapsto \mathcal{P}\bm{g}(x) = \int_{y \in \mathcal{S}} \bm{g}(y)P(x,\mathrm{d}y) = \mathbb{E}_{s_2 \sim P(\cdot \mid s_1)}[\bm{g}(s_2)\mid s_1 = x],
\end{align*}
where, with a slight abuse of notation, we write $P(\cdot \mid s_1)$ for $P(s_1,\cdot)$.
Correspondingly, the \emph{adjoint operator}, denoted as $\mathcal{P}^*$, is a bounded operator mapping a vector valued function $\bm{g} \in L_2(\mu)$ to another vector-valued function in $L_2(\mu)$, denoted as $\mathcal{P}^*\bm{g}$ such that 
\begin{align}\label{eq:adjoint}
\langle\mathcal{P}\bm{g}_1, \bm{g}_2\rangle_{\mu} = \langle\bm{g}_1, \mathcal{P}^*\bm{g}_2\rangle_{\mu}.
\end{align}

The \emph{adjoint transition kernel} $P^*$ is the transition kernel (regular conditional probability) satisfying 
\begin{align*}
\int_{A }\mu(\mathrm{d}x) P^*(x,B) = \int_{B } P(y,A) \mu(\mathrm{d}y), \quad \forall A, B \in \mathcal{F}.
\end{align*}

Accordingly, $\mathcal{P}^*$ maps any $\bm{g} \in L_2(\mu)$ to a new function $\mathcal{P}^* \bm{g} \in L_2(\mu)$ given by
\begin{align*}
x \in \mathcal{S} \mapsto \mathcal{P}^*\bm{g}(x) = \int_{y \in \mathcal{S}} \bm{g}(y)P^*(x,\mathrm{d}y) = \mathbb{E}[\bm{g}(s_1)\mid s_2 = x],
\end{align*}
%\textcolor{violet}{*ALE*: I am confused by the last identity}\weichen{checked.}% $\mathcal{P}^*\bm{g}(x) = \mathbb{E}[\bm{g}(s_1)|s_2 = x]$??}
and condition \eqref{eq:adjoint} can be equivalently expressed as
\[
\int_{\mathcal{S}} \mu(dx)P^*(x,dy)  \bm{f}^\top(x)  \bm{g}(y) = \int_{\mathcal{S}} \mu(dy) P(y,dx) \bm{f}^\top(x)  \bm{g}(y), \quad \forall \bm{f}, \bm{g} \in L_2(\mu).
\]


A Markov chain is called \emph{self-adjoint}, or \emph{reversible}, if $P = P^*$ or, equivalently, if $\mathcal{P} = \mathcal{P}^*$.



Finally, with a minor abuse of notation, for every measure $\nu$ on $(\mathcal{S},\mathscr{F})$, we use $\mathcal{P}\nu$ to denote the measure defined by
\begin{align*}
\mathcal{P}\nu(B) = \int_{\mathcal{S}} P(y,B)\nu(\mathrm{d}y), \quad \forall B \in \mathcal{F}.
\end{align*}
By definition, the stationary distribution $\mu$ satisfies $\mathcal{P}\mu = \mu$.

% We use $\mathcal{L}_2(\mu)$ to denote the space of scaler functions $g:\mathcal{S} \to \mathbb{R}$ with a finite $\mu$-norm:
% \begin{align*}
% \mathcal{L}_2(\mu) = \left\{g:\mathcal{S} \to \mathbb{R} , \|g\|_{\mu}^2 = \int_{\mathcal{S}} g^2(x) \mu(\mathrm{d}x) < \infty\right\};
% \end{align*}
% and use $\mathcal{L}_2^0(\mu)$ to denote the subset of this space with zero mean:
% \begin{align*}
% \mathcal{L}_2^0(\mu) = \left\{\right\}
% \end{align*}
\subsubsection{Spectral expansion}
For a Markov chain with transition kernel $P$ and unique stationary distribution $\mu$, let $\mathcal{L}_2$ denote the Hilbert space
\begin{align*}
\mathcal{L}_2:= \{g:\mathcal{S} \to \mathbb{R} \mid \|g\|_{\mu} < \infty\}. 
\end{align*}

The \emph{spectral expansion} of the Markov chain is defined as
\begin{align*}
\lambda = \lambda(P):= \sup_{g \in \mathcal{L}_2,\mu(g) = 0} \frac{\|\mathcal{P} g\|_{\mu}}{\|g\|_{\mu}}.
\end{align*}
We now present some properties of the spectral expansion.

\begin{customproposition}\label{prop:adjoint-spectral}
For any Markov chain with transition kernel $P$, as long as the spectral expansion $\lambda(P)$ and the adjoint transition kernel $P^*$ are both well-defined, it can be guaranteed that
\begin{align*}
\lambda(P) = \lambda(P^*).
\end{align*}
\end{customproposition}
\begin{proof}
Define $\Pi$ as the projection of any function onto the subspace of constant functions:
\begin{align*}
\Pi g(x):= \mu(g), \quad \forall x\in \mathcal{S}.
\end{align*}
It is easy to verify that $\lambda(P)$ is equal to the operator norm of $\mathcal{P}-\Pi$, and that $\lambda(P^*)$ is equal to the operator norm of $\mathcal{P}^* - \Pi$. Furthermore, the operators $\mathcal{P}-\Pi$ and $\mathcal{P}^* - \Pi$ are adjoint. Therefore, the proposition follows from the facts that adjoint operators have the same spectrum in Hilbert spaces. 
\end{proof}

\begin{customproposition}\label{prop:spectral-expansion}
For any dimension $d$, and $\bm{g}:\mathcal{S} \to \mathbb{R}^d$, it can be guaranteed that as long as $\mu(\bm{g}) = \bm{0}$ and $\|\bm{g}\|_{\mu} < \infty$,
\begin{align*}
\|\mathcal{P}^*\bm{g}\|_{\mu} \leq \lambda \|\bm{g}\|_{\mu}
\end{align*}
\end{customproposition}
\begin{proof}
This property is easy to verify by representing $\bm{g} = \{g_{i}\}_{1 \leq i\leq d}$. 
\end{proof}

\begin{customproposition}\label{prop:P-top}
For any $\bm{g}: \mathcal{S} \to \mathbb{R}^{d}$, if $\mu(\bm{g}) = \bm{0}$, then $\mu(\mathcal{P}^* \bm{g}) = 0$ and therefore $\|\mathcal{P}^{*} \bm{g}\|_{\mu} \leq \lambda \|\bm{g}\|_{\mu}$.
\end{customproposition}
\begin{proof} 
By definition, $\mu(\mathcal{P}^* \bm{g})$ is featured by
\begin{align*}
\mu(\mathcal{P}^* \bm{g}) &= \int_{x \in \mathcal{S}} \mathcal{P}^* \bm{g}(x) \mu(\mathrm{d}x) \\ 
&= \int_{x \in \mathcal{S}} \left[\int_{y \in \mathcal{S}} \bm{g}(y)\frac{P(y,\mathrm{d}x)}{\mu(\mathrm{d}x)}\mu(\mathrm{d}y)\right] \mu(\mathrm{d}x)\\ 
&= \int_{y \in \mathcal{S}} \bm{g}(y) \left[\int_{x \in \mathcal{S}} P(y,\mathrm{d}x) \right] \mu(\mathrm{d}y)\\ 
&= \int_{y \in \mathcal{S}} \bm{g}(y) \mu(\mathrm{d}y) = \mu(\bm{g}) = \bm{0}.
\end{align*}
Consequently, $\|\mathcal{P}^* \bm{g}\|_{\mu} \leq \lambda \|\bm{g}\|_{\mu}$ follows from Proposition \ref{prop:spectral-expansion}. 
\end{proof}

As a direct corollary of Proposition \ref{prop:P-top}, it is easy to verify that $(\mathcal{P}^* \bm{g})^{\parallel} = \bm{g}^{\parallel}$ and $(\mathcal{P}^* \bm{g})^{\perp} = \mathcal{P}^{*}(\bm{g}^{\perp})$. 


% \textcolor{purple}{Introduce the operator $\mathcal{P}^k = \underbrace{\mathcal{P} \circ \cdots \circ \mathcal{P}}_{k \text{ times}}$, both for functions and measures}
% \weichen{I think we no longer use this notation.}

\subsection{Algebraic facts}

\paragraph{Supplementary notation.} For any complex number $z \in \mathbb{C}$, we use $\mathsf{Re}(z)$ to denote its real part. If a matrix $\bm{M}$ is positive (semi-)definite, i.e. $\lambda_{\min}(\bm{M}) > (\geq) 0$, we write $\bm{M} \succ (\succeq) \bm{0}$; when two matrices $\bm{X}$ and $\bm{Y}$ satisfy $\bm{X} - \bm{Y} \succ (\succeq) \bm{0}$, we write $\bm{X} \succ(\succeq) \bm{Y}$ or equivalently $\bm{Y} \prec (\preceq) \bm{X}$. 
For a vector $\bm{x} \in \mathbb{R}^d$, we use $\{x_i\}_{1 \leq i \leq d}$ to denote its entries; for a matrix $\bm{X} \in \mathbb{R}^{m \times n}$, we use $\{X_{ij}\}_{1 \leq i \leq m,1\leq j \leq n}$ to denote its entries. The \emph{vectorization} of matrix $\bm{X} \in \mathbb{R}^{m \times n}$, denoted as $\textbf{vec}(\bm{X})$, is defined as a vector in $\mathbb{R}^{mn}$ with entries $[\textbf{vec}(\bm{X})]_{(i-1)m+j} = \bm{X}_{ij}$ for every $1 \leq i \leq m$ and $1 \leq j \leq n$. 
For any matrices $\bm{X} \in \mathbb{R}^{m \times n}$, and $\bm{Y} \in \mathbb{R}^{p \times q}$, their \emph{Kronecker product}, denoted as $\bm{X} \otimes \bm{Y}$, is a matrix of the size of $mp \times nq$, and can be written in blocked form as
\begin{align*}
\bm{X} \otimes \bm{Y} = \begin{bmatrix}
X_{11}\bm{Y} & X_{12}\bm{Y} & \ldots & X _{1n}\bm{Y} \\ 
X_{21}\bm{Y} & X_{22}\bm{Y} & \ldots & X_{2n} \bm{Y} \\ 
\ldots & \ldots & \ldots & \ldots \\ 
X_{m1}\bm{Y} & X_{m2}\bm{Y} &\ldots &X_{mn} \bm{Y}
\end{bmatrix}.
\end{align*}
With subscripts, it can be represented as $(\bm{X} \otimes \bm{Y})_{(i-1)m + j, (k-1)p + \ell} = X_{ik}Y_{j\ell}$.

\subsubsection{General algebriac theorems}

\begin{customtheorem}[Properties of the Kronecker product]\label{thm:Kronecker}
The following properties hold:
\begin{enumerate}[label={(\arabic*)}]
\item Let $\bm{X},\bm{Y},\bm{Z},\bm{W}$ be four matrices such that the matrix products $\bm{XZ}$ and $\bm{YW}$ are well-defined. Then $(\bm{X} \otimes \bm{Y})(\bm{Z}\otimes \bm{W}) = (\bm{XZ}) \otimes \bm{YW}$;
\item As a direct consequence of (1), let $\bm{X} \in \mathbb{R}^{m \times n}$ and $\bm{Y} \in \mathbb{R}^{p \times q}$, then $\bm{X} \otimes \bm{Y} = (\bm{X} \otimes \bm{I}_p)(\bm{I}_n \otimes \bm{Y})$; 
\item For any matrices $\bm{X},\bm{Y}$, $(\bm{X} \otimes \bm{Y})^\top = \bm{X}^\top \otimes \bm{Y}^\top$;
\item For any matrices $\bm{X} \in \mathbb{R}^{m \times n}$, $\bm{Y} \in \mathbb{R}^{p \times q}$ and $\bm{Z} \in \mathbb{R}^{q \times n}$, it can be guaranteed that $(\bm{A} \otimes \bm{B})\textbf{vec}(\bm{Z}) = \textbf{vec}(\bm{Y}\bm{Z}\bm{X}^\top)$;
\item As a direct consequence of (4), let $\bm{X} \in \mathbb{R}^{m \times n}$ and $\bm{y} \in \mathbb{R}^n$, then $\bm{Xy} = (\bm{I}_m \otimes \bm{y})\textbf{vec}(\bm{X})$;
\item Also as a direct consequence of (5), let $\bm{X},\bm{Y} \in \mathbb{R}^{d \times d}$, then $\mathsf{Tr}(\bm{XY}^\top) = [\mathbf{vec}(\bm{I}_d)]^\top (\bm{X} \otimes \bm{Y}) \mathbf{vec}(\bm{I}_d)$;
\item For any $\bm{X},\bm{Y} \in \mathbb{R}^{d \times d}$, it can be guaranteed that $\exp(\bm{X}) \otimes \exp(\bm{Y}) = \exp(\bm{X} \otimes \bm{I}_{d^2} + \bm{I}_{d^2} \otimes \bm{Y})$.
\end{enumerate}
\end{customtheorem}
\begin{customtheorem}\label{thm:Klein}
For any positive definite functions $\bm{A},\bm{B} \in \mathbb{S}^{d \times d}$, it can be guaranteed that
\begin{align*}
\mathsf{Tr}(\bm{A}^{-1}(\bm{A}-\bm{B})) \leq \mathsf{Tr}(\log(\bm{A}) -\log(\bm{B})).
\end{align*}
\end{customtheorem}

%\yuting{try to use the proof environment; same for the other proofs}\weichen{checked.}
\begin{proof}
This conclusion is a direct application of the Klein's inequality (see, for example, Theorem 2.11 of \cite{Carlen2009TRACEIA}), which states that 
\begin{align}\label{eq:Klein-inequality}
\mathsf{Tr}(f(\bm{A})-f(\bm{B})-(\bm{A} - \bm{B})^\top f'(\bm{A})) \geq 0
\end{align}
for any concave function $f: (0,\infty) \to \mathbb{R}$, on the concave function $f(x) = \log x$. For the specific definition of $f(\bm{X})$ where $\bm{X}$ is a positive semidefinite matrix, and the proof of the inequality \eqref{eq:Klein-inequality}, we refer the readers to \cite{Carlen2009TRACEIA}.
\end{proof}

% For completeness, we here provide another proof, inspired by the work of \cite{Lanford1968MeanEntropy}, Lemma 1.
% Denote the spectral decomposition of $\bm{A}$ and $\bm{B}$ as
% \begin{align*}
% \bm{A} = \sum_{i=1}^d a_i \bm{\lambda}_i \bm{\lambda}_i^\top, \quad \text{and} \quad \bm{B} = \sum_{i=1}^d b_i \bm{\mu}_i \bm{\mu}_i^\top
% \end{align*}
% respectively. Furthermore, let $\bm{U} \in \mathbb{R}^{d \times d} = (U_{ij})_{1 \leq i,j \leq d}$ be defined such that
% \begin{align*}
% \bm{\lambda}_i = \sum_{j=1}^d U_{ij}\bm{\mu}_j, \quad \forall i \in [d].
% \end{align*}
% It is elementary that $\bm{U}$ is orthonormal and therefore $\sum_{j=1}^d U_{ij}^2 = 1$ for every $i \in [d]$. Therefore, by definition, it can be guaranteed for every $i \in [d]$ that 
% \begin{align*}
% \bm{\lambda}_i^\top f(\bm{A}) \bm{\lambda}_i &= f(a_i); \\ 
% \bm{\lambda}_i^\top f(\bm{B}) \bm{\lambda}_i &= \left(\sum_{j=1}^d U_{ij}\bm{\mu}_j\right)^\top  f(B) \left(\sum_{j=1}^d U_{ij}\bm{\mu}_j\right) \\ 
% &=\left(\sum_{j=1}^d U_{ij}\bm{\mu}_j\right)^\top \left(\sum_{j=1}^d U_{ij}f(b_j)\bm{\mu}_j\right) \\ 
% &= \sum_{j=1}^d f(b_j)U_{ij}^2; \quad \text{and} \\ 
% \bm{\lambda}_i^\top f'(\bm{A})(\bm{A} -\bm{B})\bm{\lambda}_i&=f'(a_i) \left[a_i - \left(\sum_{j=1}^d U_{ij}\bm{\mu}_j\right)^\top \bm{B}\left(\sum_{j=1}^d U_{ij}\bm{\mu}_j\right)\right]\\ 
% &= f'(a_i)\left(a_i -\sum_{j=1}^d U_{ij}^2 b_j\right).
% \end{align*}

\subsubsection{Properties of the matrices involved in the paper regarding TD learning}
In this section, we present several useful algebraic results, most of which were established by \cite{wu2024statistical}.
The following lemma reveals several critical algebriac features of the matrix $\bm{A}$.
\begin{customlemma}[Lemma A.4 of \cite{wu2024statistical}]\label{lemma:A}
Let $\bm{A}_t,\bm{A}$ be defined as in \eqref{eq:defn-At} and \eqref{eq:defn-At-mean} respectively, and $\bm{\Sigma}$ be defined as in \eqref{eq:defn-Sigma} with $\lambda_0$ and $\lambda_{\Sigma}$ being its smallest and largest eigenvalues. Then the following features hold:
\begin{align}
&2(1-\gamma) \bm{\Sigma} \preceq \bm{A} + \bm{A}^\top \preceq 2(1+\gamma) \bm{\Sigma}; \label{eq:lemma-A-1} \\ 
&\min_{1 \leq i \leq d} \mathsf{Re}(\lambda_i(\bm{A})) \geq (1-\gamma) \lambda_0; \label{eq:lemma-A-2}\\ 
&\mathbb{E}[\bm{A}_t^\top \bm{A}_t] \leq \bm{A} + \bm{A}^\top; \label{eq:lemma-A-3}\\ 
&\bm{A}^\top \bm{A} \preceq \lambda_{\Sigma}(\bm{A} + \bm{A}^\top); \label{eq:lemma-A-4}\\
&\left\|\bm{I}-\eta \bm{A}\right\| \leq 1-\frac{1-\gamma}{2}\lambda_0 \eta, \quad  \forall \eta \in \left(0,\frac{1}{2\lambda_{\Sigma}}\right); \label{eq:lemma-A-5}\\
&\|\bm{A}^{-1}\| \leq \frac{1}{\lambda_0(1-\gamma)}.\label{eq:lemma-A-6}
\end{align}
\end{customlemma}
Notice that the bound \eqref{eq:lemma-A-6} directly implies that
\begin{align}\label{eq:theta-norm-bound}
\|\bm{\theta}^\star\|_2 \leq \|\bm{A}^{-1}\|\|\bm{b}\|_2 \leq \frac{1}{\lambda_0(1-\gamma)}.
\end{align}

\begin{customlemma}[Lemma A.5 of \cite{wu2024statistical}]\label{lemma:Lyapunov}
For any matrix $\bm{E} \succeq \bm{0}$, there exists a unique positive definite matrix $\bm{X} \succeq \bm{0}$, such that
\begin{align*}
\bm{AX} + \bm{XA}^\top  = \bm{E}.
\end{align*}
Furthermore, it can be guaranteed that
\begin{align*}
\|\bm{X}\| \leq \frac{1}{2(1-\gamma)\lambda_0} \|\bm{E}\|, \quad \text{and} \quad \mathsf{Tr}(\bm{X}) \leq \frac{1}{2(1-\gamma)\lambda_0} \mathsf{Tr}(\bm{E}).
\end{align*}
\end{customlemma}

The following lemma further features the compressive power of the matrix $\prod_{k=1}^t (\bm{I} - \eta_k \bm{A})$.
\begin{customlemma}[Lemma A.6 of \cite{wu2024statistical}]\label{lemma:R}
Assume that $\beta \in (0,1)$ and $\alpha \in (\frac{1}{2},1)$. Let $\eta_t = \eta_0 t^{-\alpha}$ for all positive integer $t$, then the following inequalities hold:
\begin{align}
&t^{\alpha} \prod_{k=1}^t \left(1-\beta k^{-\alpha} \right) \leq e \left(\frac{\alpha}{\beta}\right)^{\frac{\alpha}{1-\alpha}} ;\label{eq:lemma-R-1} \\ 
&\sum_{i=1}^t i^{-\nu} \prod_{k=i+1}^t \left(1-\beta k^{-\alpha} \right) \leq \frac{1}{\nu-1} \left(\frac{2(\nu-\alpha)}{\beta}\right)^{\frac{\nu-\alpha}{1-\alpha}} t^{\alpha-\nu}, \quad \forall \nu \in (1,\alpha+1]; \label{eq:lemma-R-2} \\ 
&\sum_{i=1}^t i^{-2\alpha} \prod_{k=i+1}^t \left(1-\beta k^{-\alpha} \right)  = \frac{1}{\beta}t^{-\alpha} + o\left(t^{-\alpha}\right); \label{eq:lemma-R-3} \\ 
&\max_{1 \leq i \leq t} i^{-\alpha} \prod_{k=i+1}^t \left(1-\beta k^{-\alpha} \right) \leq e \left(\frac{\alpha}{\beta}\right)^{\frac{\alpha}{1-\alpha}} t^{-\alpha}. \label{eq:lemma-R-4}
\end{align} 
\end{customlemma}

\begin{customlemma}[Lemma A.7 of \cite{wu2024statistical}]\label{lemma:Q-uni}
For every $\beta \in (0,1)$, $\alpha \in (\frac{1}{2},1)$ and sufficiently large $T$, it can be guaranteed that
\begin{align}
&t^{-\alpha} \sum_{j=t}^T \prod_{k=t+1}^j (1-\beta k^{-\alpha}) < 3\left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}}, \quad \text{and} \label{eq:Q-uni-1}\\ 
&t^{-\alpha} \sum_{j=t}^\infty \prod_{k=t+1}^j (1-\beta k^{-\alpha}) - \frac{1}{\beta} \lesssim  \left(\frac{1}{\beta}\right)^{\frac{1}{1-\alpha}}\Gamma\left(\frac{1}{1-\alpha}\right) t^{\alpha-1}. \label{eq:Q-uni-2}
\end{align}
\end{customlemma}

\begin{customlemma}[Lemma A.8 of \cite{wu2024statistical}]\label{lemma:Q}
Let $\eta_t = \eta_0 t^{-\alpha}$ with $\alpha \in (\frac{1}{2},1)$ and $0 \leq \eta_0 \leq \frac{1}{2\lambda_{\Sigma}}$. For every $t \in [T]$, define $\bm{Q}_t$ as
\begin{align}
\bm{Q}_t = \eta_t \sum_{j=t}^{T}\prod_{k=t+1}^{j} (\bm{I}-\eta_k \bm{A}) \label{eq:defn-Qt} 
\end{align}

Then for any positive integer $\ell < T$, the following relation hold:
\begin{subequations}
\begin{align}
\sum_{t=1}^T (\bm{Q}_t - \bm{A}^{-1}) = -\bm{A}^{-1} \sum_{j=1}^T \prod_{k=1}^j (\bm{I}-\eta_k \bm{A}),\label{eq:Qt-Ainv-bound-1}
\end{align}
Furthermore, there exist a sequence of matrix $\{\bm{S}_t\}_{1 \leq t \leq T}$, such that the difference between $\bm{Q}_t$ and $\bm{A}^{-1}$ can be represented as
\begin{align}\label{eq:Qt-Ainv-decompose}
\bm{Q}_t - \bm{A}^{-1}  = \bm{S}_t- \bm{A}^{-1} \prod_{k=t}^T (\bm{I}-\eta_k \bm{A}),
\end{align}
in which $\bm{S}_t$ is independent of $T$, and its norm is bounded by
\begin{align}\label{eq:St1-bound}
\|\bm{S}_t\| \lesssim \eta_0 \Gamma\left(\frac{1}{1-\alpha}\right)\left(\frac{2}{(1-\gamma)\lambda_0 \eta_0}\right)^{\frac{1}{1-\alpha}}t^{\alpha-1}.
\end{align}
\end{subequations}
\end{customlemma}

% The following Lemma features the compressive rate of the matrix $\prod_{k=t}^T (\bm{I}-\eta_k\bm{A})$.
% \begin{customlemma}[Lemma A.9 of \cite{wu2024statistical}]\label{thm:St2-bound}
% Under the condition \ref{eq:eta0-alpha-condition}, for every positive definite matrix $\bm{Y} \in \mathbb{R}^{d \times d}$, it can be guarnateed that 
% \begin{align}\label{eq:St2-bound}
% T^{-\alpha} \sum_{t=1}^T \left(\prod_{k=t}^T (\bm{I}-\eta_k\bm{A}) \bm{Y} \prod_{k=t}^T (\bm{I}-\eta_k\bm{A}^\top) \right)= \bm{X} + \bm{O}(T^{\alpha-1}),
% \end{align}
% where $\bm{X}$ is the solution to the Lyapunov equation
% \begin{align}\label{eq:Lyapunov-St2}
% \eta_0(\bm{A} \bm{X} + \bm{XA}^\top)= \bm{Y}.
% \end{align}
% \end{customlemma}

\begin{customlemma}[Lemma A.10 of \cite{wu2024statistical}]\label{lemma:Q-bound}
Let $\eta_t = \eta_0 t^{-\alpha}$ with $\alpha \in (\frac{1}{2},1)$ and $0 \leq \eta \leq \frac{1}{2\lambda_{\Sigma}}$. For $\bm{Q}_t$ defined as in \eqref{eq:defn-Qt}, it can be guaranteed that
\begin{align}
&\|\bm{Q}_t\| \leq 3\eta_0^{-\frac{\alpha}{1-\alpha}} \left(\frac{4\alpha}{\lambda_0(1-\gamma)}\right)^{\frac{1}{1-\alpha}}, \quad \forall t \in [T];  \label{eq:Q-bound} \\ 
&\|\bm{A} \bm{Q}_t\| \leq 2 + \eta_0 \left(\frac{1}{\beta}\right)^{\frac{1}{1-\alpha}}\Gamma \left(\frac{1}{1-\alpha}\right)t^{\alpha-1}; \label{eq:AQ-bound} \quad \text{and}\\ 
&\frac{1}{T}\sum_{t=1}^T \|\bm{Q}_t - \bm{A}^{-1}\|^2 \leq \frac{1}{\lambda_0(1-\gamma)}T^{\alpha-1} + \widetilde{C}T^{2\alpha-2}\label{eq:AQ-I-bound}
\end{align}
where $\beta = \frac{1-\gamma}{2}\lambda_0 \eta_0$  and $\widetilde{C}$ depends on $\alpha,\eta_0,\lambda_0,\gamma$.
\end{customlemma}

The following lemma bounds the norms and trace of the time-averaging variance matrix $\tilde{\bm{\Gamma}}$.
\begin{customlemma}\label{lemma:Gamma}
Under Assumption \ref{as:mixing}, when $\tilde{\bm{\Gamma}}$ is defined as in \eqref{eq:defn-tilde-Gamma}, it can be guaranteed that
\begin{align*}
\|\tilde{\bm{\Gamma}}\| \leq \|\tilde{\bm{\Gamma}}\|_{\mathsf{F}} \leq \mathsf{Tr}(\tilde{\bm{\Gamma}}) \leq \left(1+\frac{2m}{1-\rho}\right) (2\|\bm{\theta}^\star\|_2+1)^2 \lesssim \frac{m}{1-\rho}\left(\frac{1}{\lambda_0(1-\gamma)}\right)^2.
\end{align*}
\end{customlemma}
\begin{proof} 
The first two inequalities follow directly from the fact that $\tilde{\bm{\Gamma}}$, a variance-covariance matrix, is positive semi-definite; the last inequality follows directly from \eqref{eq:theta-norm-bound}. It now boils down to proving that
\begin{align*}
\mathsf{Tr}(\tilde{\bm{\Gamma}}) \leq \left(1+\frac{2m}{1-\rho}\right) (2\|\bm{\theta}^\star\|_2+1).
\end{align*}
For every integer $\ell>1$, we firstly use the iteration of expectations to derive
\begin{align*}
&\mathsf{Tr}(\mathbb{E}[(\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)(\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1})^\top]) \\ 
&= \mathbb{E}_{s_0 \sim \mu,s_1 \sim P(\cdot \mid s_0), s_{\ell} \sim P^{\ell-1}(\cdot \mid s_1),s_{\ell+1} \sim P(\cdot \mid s_{\ell})}[(\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)^\top (\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1})] \\ 
&= \mathbb{E}_{s_0 \sim \mu,s_1 \sim P(\cdot \mid s_0)}[\mathbb{E}_{s_\ell \sim P^{\ell-1}(\cdot \mid s_1)}[\mathbb{E}_{s_{\ell+1} \sim P(\cdot \mid s_{\ell})} (\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)^\top (\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1}) \mid\mathscr{F}_1]];
\end{align*}
meanwhile, by definition,
\begin{align*}
&\mathbb{E}_{s_\ell \sim \mu}[\mathbb{E}_{s_{\ell+1} \sim P(\cdot \mid s_{\ell})} (\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)^\top (\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1}) \mid\mathscr{F}_1] \\ 
&=(\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)^\top \mathbb{E}_{s_\ell \sim \mu,s_{\ell+1} \sim P(\cdot \mid s_{\ell})}[\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1}] = 0.
\end{align*}
Therefore, the basic property of TV distance and Assumption \ref{as:mixing} guarantees
\begin{align*}
&\mathbb{E}_{s_\ell \sim P^{\ell-1}(\cdot \mid s_1)}[\mathbb{E}_{s_{\ell+1} \sim P(\cdot \mid s_{\ell})} (\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)^\top (\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1}) \mid\mathscr{F}_1] \\ 
&= \left(\mathbb{E}_{s_\ell \sim P^{\ell-1}(\cdot \mid s_1)} - \mathbb{E}_{s_{\ell} \sim \mu}\right)[\mathbb{E}_{s_{\ell+1} \sim P(\cdot \mid s_{\ell})} [(\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)^\top (\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1}) \mid\mathscr{F}_1]] \\ 
&\leq d_{\mathsf{TV}}(P^{\ell-1}(\cdot \mid s_1),\mu) \cdot \sup_{s_{\ell} \in \mathcal{S}} \mathbb{E}_{s_{\ell+1} \sim P(\cdot \mid s_{\ell})} [(\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)^\top (\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1}) \mid\mathscr{F}_1]\\ 
&\leq m\rho^{\ell-1} \cdot (2\|\bm{\theta}^\star\|_2+1)^2.
\end{align*}
By taking expectations with regard to $s_0$ and $s_1$, we obtain
\begin{align*}
&\mathsf{Tr}(\mathbb{E}[(\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)(\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1})^\top]) \\ 
&= \mathbb{E}_{s_0 \sim \mu,s_1 \sim P(\cdot \mid s_0)}[\mathbb{E}_{s_\ell \sim P^{\ell-1}(\cdot \mid s_1)}[\mathbb{E}_{s_{\ell+1} \sim P(\cdot \mid s_{\ell})} (\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)^\top (\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1}) \mid\mathscr{F}_1]] \\ 
&\leq m\rho^{\ell-1} \cdot (2\|\bm{\theta}^\star\|_2+1)^2.
\end{align*}
Hence by definition, the trace of $\tilde{\bm{\Gamma}}$ is bounded by
\begin{align*}
\mathsf{Tr}(\tilde{\bm{\Gamma}}) &= \mathbb{E}\|\bm{A}_1 \bm{\theta}^\star - \bm{b}_1\|_2^2 + 2\sum_{\ell=1}^{\infty} \mathsf{Tr}(\mathbb{E}[(\bm{A}_1 \bm{\theta}^\star - \bm{b}_1)(\bm{A}_{\ell+1} \bm{\theta}^\star - \bm{b}_{\ell+1})^\top]) \\ 
&\leq (1 + 2\sum_{\ell=1}^{\infty} m\rho^{\ell-1}) (2\|\bm{\theta}^\star\|_2+1)^2 = \left(1+\frac{2m}{1-\rho}\right) (2\|\bm{\theta}^\star\|_2+1)^2.
\end{align*}
This completes the proof of the lemma.
\end{proof}
% The decomposition \eqref{eq:Qt-Ainv-decompose}, combined with the bounds \eqref{eq:St1-bound} Theorem \ref{thm:St2-bound}, implies the following bound on the difference between $\bar{\bm{\Lambda}}_T$ and $\bm{\Lambda}^\star$.

\begin{customtheorem}[Analogy to Theorem A.11 of \cite{wu2024statistical}]\label{thm:Lambda}
Define $\tilde{\bm{\Lambda}}_T$ as 
\begin{align}\label{eq:defn-tilde-LambdaT}
\tilde{\bm{\Lambda}}_T = \frac{1}{T}\sum_{t=1}^T \bm{Q}_t \tilde{\bm{\Gamma}}\bm{Q}_t^\top,
\end{align}
and let $\tilde{\bm{\Lambda}}^\star$ be defined as in \eqref{eq:defn-tilde-Lambdastar}. Define $\bm{X}(\tilde{\bm{\Lambda}}^\star)$ as the unique solution to the Lyapunov equation
\begin{align}\label{eq:defn-X-Lambda}
\eta_0(\bm{AX+XA}^\top) = \tilde{\bm{\Lambda}}^\star;
\end{align}
then as $T \to \infty$, the difference between $\tilde{\bm{\Lambda}}_T$ and $\tilde{\bm{\Lambda}}^\star$ is featured by
\begin{align*}
&\|\tilde{\bm{\Lambda}}_T - \tilde{\bm{\Lambda}}^\star-T^{\alpha-1}\bm{X}(\tilde{\bm{\Lambda}}^\star)\| \leq \widetilde{C}T^{2\alpha-2}\|\tilde{\bm{\Gamma}}\|, \quad \text{and} \\ 
&\mathsf{Tr}(\tilde{\bm{\Lambda}}_T - \tilde{\bm{\Lambda}}^\star-T^{\alpha-1}\bm{X}(\tilde{\bm{\Lambda}}^\star)) \leq \widetilde{C}T^{2\alpha-2} \mathsf{Tr}(\tilde{\bm{\Gamma}}).
\end{align*}
Here, $\widetilde{C}$ is a problem-related quantity that can be represented by $\eta_0,\alpha$ and $\gamma$.
%\ale{See my comment in Theorem A.9 of the AISTATS paper.}\weichen{changed notation.}
\end{customtheorem}
\begin{proof} 
This theorem can be proved by replacing $\bar{\bm{\Lambda}}_T$ by $\tilde{\bm{\Lambda}}_T$, $\bm{\Lambda}^\star$ by $\tilde{\bm{\Lambda}}^\star$ and $\bm{\Gamma}$ by $\tilde{\bm{\Gamma}}$ in the proof of Theorem A.11 in \cite{wu2024statistical}. Details are omitted.
\end{proof}

\begin{customlemma}[Analogy to Lemma A.12 of \cite{wu2024statistical}]\label{thm:A-Lambda}
Let $\tilde{\bm{\Lambda}}_T$ and $\tilde{\bm{\Lambda}}^\star$ be defined as in \eqref{eq:defn-tilde-LambdaT} and \eqref{eq:defn-tilde-Lambdastar} respectively. When 
\begin{align}\label{eq:Lambda-T-condition}
T \geq 4 \left(\frac{2}{(1-\gamma)\lambda_0\eta_0}\right)^{\frac{1}{1-\alpha}}(1-\alpha)^{\frac{\alpha}{1-\alpha}}\Gamma(\frac{1}{1-\alpha})\mathsf{cond}(\Gamma),
\end{align}
it can be guaranteed that
\begin{align*}
\lambda_{\min}(\bm{A}\bm{\Lambda}_T \bm{A}^\top) \geq \frac{1}{2}\lambda_{\min}(\tilde{\bm{\Gamma}}) = \frac{1}{2\|\tilde{\bm{\Gamma}}^{-1}\|}.
\end{align*}
\end{customlemma}
\begin{proof} 
This lemma can be proved by replacing $\bar{\bm{\Lambda}}_T$ by $\tilde{\bm{\Lambda}}_T$, $\bm{\Lambda}^\star$ by $\tilde{\bm{\Lambda}}^\star$ and $\bm{\Gamma}$ by $\tilde{\bm{\Gamma}}$ in the proof of Lemma A.12 in \cite{wu2024statistical}. Details are omitted.
\end{proof}

\begin{customlemma}\label{lemma:delta-Q}
Let $\bm{Q}_t$ be defined as in \eqref{eq:defn-Qt}, then it can be guaranteed that
\begin{align*}
\frac{1}{T}\sum_{t=1}^T \|\bm{Q}_t - \bm{Q}_{t-1}\| \lesssim \eta_0 \left[\eta_0 \Gamma\left(\frac{1}{1-\alpha}\right)+\alpha\right]\left(\frac{2}{(1-\gamma)\lambda_0 \eta_0}\right)^{\frac{1}{1-\alpha}} \frac{\log T}{T}.
\end{align*}
\end{customlemma}
\begin{proof} By definition, the matrices $\bm{Q}_{t-1}$ and $\bm{Q}_t$ can be related by
\begin{align*}
\bm{Q}_{t-1}& = \eta_{t-1} \bm{I} + \eta_{t-1} \sum_{j=t}^T \prod_{k=t}^j (\bm{I} - \eta_k \bm{A})\\ 
&= \eta_{t-1} \bm{I} + \frac{\eta_{t-1}}{\eta_t} (\bm{I}-\eta_t \bm{A})\bm{Q}_t;
\end{align*}
Therefore, the difference between $\bm{Q}_t$ and $\bm{Q}_{t-1}$ is featured by
\begin{align*}
\bm{Q}_t - \bm{Q}_{t-1} &= -\eta_{t-1}(\bm{I} - \bm{AQ}_{t}) - \left(\frac{\eta_{t-1}}{\eta_t} - 1\right) \bm{Q}_{t} \\ 
&= \eta_{t-1}\bm{AS}_t - \eta_{t-1}\prod_{k=t}^T (\bm{I}-\eta_k \bm{A}) - \left(\frac{\eta_{t-1}}{\eta_t} - 1\right)\bm{Q}_{t}
\end{align*}
where we invoked \eqref{eq:Qt-Ainv-decompose} in the last equation. Hence by triangle inequality,
\begin{align*}
&\frac{1}{T}\sum_{t=1}^T \|\bm{Q}_t - \bm{Q}_{t-1}\|\\ 
&\leq \frac{1}{T}\|\bm{A}\|\sum_{t=1}^T  \eta_{t-1}\|\bm{S}_t\| + \frac{1}{T}\sum_{t=1}^T \eta_{t-1}\left\|\prod_{k=t}^T (\bm{I}-\eta_k \bm{A}) \right\| + \frac{1}{T}\sum_{t=1}^T\left(\frac{\eta_{t-1}}{\eta_t} - 1\right)\|\bm{Q}_{t}\|.
\end{align*}
By \eqref{eq:St1-bound}, \eqref{eq:Q-bound} and the telescoping method, it is easy to verify that the three terms on the right-hand-side can be bounded respectively by
\begin{align*}
&\frac{1}{T}\|\bm{A}\|\sum_{t=1}^T  \eta_{t-1}\|\bm{S}_t\| \lesssim \eta_0^2 \Gamma\left(\frac{1}{1-\alpha}\right)\left(\frac{2}{(1-\gamma)\lambda_0 \eta_0}\right)^{\frac{1}{1-\alpha}} \frac{\log T}{T}, \\ 
&\frac{1}{T}\sum_{t=1}^T \eta_{t-1}\left\|\prod_{k=t}^T (\bm{I}-\eta_k \bm{A}) \right\| \leq \frac{2}{(1-\gamma)\lambda_0 } \frac{1}{T}, \quad \text{and} \\ 
&\frac{1}{T}\sum_{t=1}^T\left(\frac{\eta_{t-1}}{\eta_t} - 1\right)\|\bm{Q}_{t}\| \lesssim \alpha \left(\frac{2}{(1-\gamma)\lambda_0 \eta_0}\right)^{\frac{1}{1-\alpha}} \frac{\log T}{T}.
\end{align*}
The lemma follows immediately.
\end{proof}




\subsection{Other basic facts}
The following theorem proposed by \cite{devroye2018total} gives an upper bound for the total variation (TV) distance between two Gaussian random variables with same means and different covariance matrixes.
\begin{customtheorem}[\cite{devroye2018total}]\label{thm:DMR}
 Let $\bm{\Lambda}_1, \bm{\Lambda}_2 \in \mathbb{S}^{d \times d}$ be two positive definite matrices, and $\bm{\mu}$ be any vector in $\mathbb{R}^d$. Then the TV distance between Gaussian distirbutions $\mathcal{N}(\bm{\mu},\bm{\Lambda}_1)$ and $\mathcal{N}(\bm{\mu},\bm{\Lambda}_2)$ is bounded by
\begin{align*}
\min\left\{1,\frac{1}{100}\left\|\bm{\Lambda}_1^{-1/2}\bm{\Lambda}_2 \bm{\Lambda}_1^{-1/2}-\bm{I}_d\right\|_{\mathsf{F}} \right\} 
 \leq d_{\mathsf{TV}}(\mathcal{N}(\bm{\mu},\bm{\Lambda}_1),\mathcal{N}(\bm{\mu},\bm{\Lambda}_2)) 
\leq \frac{3}{2}\left\|\bm{\Lambda}_1^{-1/2}\bm{\Lambda}_2 \bm{\Lambda}_1^{-1/2}-\bm{I}_d\right\|_{\mathsf{F}}. 
\end{align*}
\end{customtheorem}

\begin{customtheorem}[\cite{nazarov2003maximal}]\label{thm:Gaussian-reminder}
Let $\bm{z} \sim \mathcal{N}(\bm{0},\bm{\Lambda})$ be a $d$-dimensional Gaussian random variable, and $\mathcal{A}$ be any non-convex subset of $\mathbb{R}^d$. For any $\varepsilon \geq 0$, we define
\begin{align}\label{eq:defn-A-eps}
&\mathcal{A}^{\varepsilon} := \{\bm{x} \in \mathbb{R}^d: \inf_{y \in \mathcal{A}} \|\bm{x} - \bm{y}\|_2 \leq \varepsilon\}, \quad \text{and} \nonumber \\ 
&\mathcal{A}^{-\varepsilon}:=\{\bm{x} \in \mathbb{R}^d: B(\bm{x},\varepsilon) \subset \mathcal{A}\},
\end{align}
where $B(\bm{x},\varepsilon)$ represents the $d$-dimensional ball centered at $\bm{x}$ with radius $\varepsilon$. Then it can be guaranteed that
\begin{align*}
&\mathbb{P}(\bm{z} \in \mathcal{A}^{\varepsilon} - \mathcal{A}) \lesssim \|\bm{\Lambda}\|_{\mathsf{F}}^{\frac{1}{2}}\varepsilon, \quad \text{and} \\ 
&\mathbb{P}(\bm{z} \in \mathcal{A} - \mathcal{A}^{-\varepsilon})\lesssim \|\bm{\Lambda}\|_{\mathsf{F}}^{\frac{1}{2}}\varepsilon.
\end{align*}
\end{customtheorem}

The following theorem from \cite{nourdin2021multivariate} relates the distance on convex sets with the Wasserstein distance when one of the distributions being compared corresponds to a Gaussian random variable.
\begin{customtheorem}\label{thm:Gaussian-convex-Wass}
Let $\bm{x}$ be a random vector in $\mathbb{R}^d$, and $\bm{y} \sim \mathcal{N}(\bm{0},\bm{\Lambda})$ where $\bm{\Lambda} \in \mathbb{S}^{d \times d}$ is positive-definite. Then it can be guaranteed that
\begin{align*}
d_{\mathsf{C}}(\bm{x},\bm{y}) \lesssim \|\bm{\Lambda}\|_{\mathsf{F}}^{\frac{1}{4}}\sqrt{d_{\mathsf{W}}(\bm{x},\bm{y})}.
\end{align*}
\end{customtheorem}

We also recall the following concentration inequality. 

\begin{customtheorem}[Corollary A.16 of \cite{wu2024statistical}]\label{thm:vector-Azuma}
Let $\{\bm{x}_i\}_{i \geq 1}$ be a martingale in $\mathbb{R}^d$, and let $W_{\max}$ be a positive constant that satisfies
\begin{align*}
W_{\max} \geq \sum_{i=1}^t \|\bm{x}_i - \bm{x}_{i-1}\|_2^2 \quad \text{almost surely.}
\end{align*}
Then it can be guaranteed with probability at least $1-\delta$ that
\begin{align*}
\|\bm{x}_t\|_2 \leq 2\sqrt{2W_{\max}\log \frac{3}{\delta}}.
\end{align*}
\end{customtheorem}

