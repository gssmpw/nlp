\section{Proof of results regarding TD learning}\label{app:proof-TD}

Throughout this section, we denote
\begin{align}
&\bm{\Delta}_t = \bm{\theta}_t - \bm{\theta}^\star, \quad \forall t \in [T], \quad \text{and}  \\
&\bar{\bm{\Delta}}_T = \bar{\bm{\theta}}_T - \bm{\theta}^\star.
\end{align}

Furthermore, for every $t = 0,1,...,T$, we denote 
\begin{align*}
\mathbb{E}_t[\cdot] := \mathbb{E}[\cdot \mid s_0,s_1,...,s_t].
\end{align*}
Without any subscript, the operator $\mathbb{E}$ represents taking expectation with respect to all the samples starting from $s_0$.

\subsection{$L^2$ convergence of the TD estimation error}
The following theorem captures the asymptotic property of $\mathbb{E}\|\bm{\Delta}_t\|_2^2$ with Markov samples and is useful in our proofs for other results. 
Note that the bound holds, non-asymptotically, for all $t \geq t^\star$ where $t^\star$ is a problem-dependent quantity; we state it as an asymptotic result only for convenience.

\begin{theorem}\label{thm:markov-L2-convergence}
Consider TD with Polyak-Ruppert averaging~\eqref{eq:TD-update-all} with Markov samples and decaying stepsizes $\eta_t = \eta_0 t^{-\alpha}$ for $\alpha \in (\frac{1}{2},1)$. Suppose that the Markov transition kernel has a unique stationary distribution, a strictly positive spectral gap, and mixes exponentially as indicated by Assumption \ref{as:mixing}.  It can then be guaranteed that when $t \to \infty$,
\begin{align*}
\mathbb{E}\big[\|\bm{\Delta}_t\|_2^2\big] \lesssim (2\|\bm{\theta}^\star\|_2+1)^2 \left[\frac{1}{(1-\rho)^2}\frac{\eta_0}{\lambda_0(1-\gamma)}t^{-\alpha} \log^2 t + o\left(t^{-\alpha} \log^2 t\right)\right].
\end{align*}
%The following properties hold for the expected squared norm of the TD estimation error $\bm{\Delta}_t$:
% \begin{enumerate}
% \item Under Scenario \ref{case:alpha}, 
% \begin{align*}
% \mathbb{E}\|\bm{\Delta}_t\|_2^2 \leq O(t^{-\alpha} \log^2 t);
% \end{align*}
% \item Under Scenario \ref{case:nu},
% \begin{align*}
% \mathbb{E}\|\bm{\Delta}_t\|_2^2 \leq O((t+\nu)^{-1} \log^2 (t+\nu));
% \end{align*}
% \item Under Scenario \ref{case:eta} with the proviso that
% \begin{align}\label{eq:S3-eta-condition-L2}
% \eta_0 \frac{\log(m/\eta)}{\log(1/\rho)} \leq \frac{\lambda_0(1-\gamma)}{43},
% \end{align}
% it can be guaranteed that
% \begin{align*}
% \mathbb{E}\|\bm{\Delta}_t\|_2^2 \leq (2\|\bm{\theta}^\star\|_2+1)^2 \quad \text{for all} \quad t \in \mathbb{N}.
% \end{align*}
% \end{enumerate}
\end{theorem}
% \ale{Add a comment to say that the proof is non-asymptotic and holds for all  $t$ satisfying $t > 2 t_{\mix}$ and $\eta_t (1+\gamma)^2 < \lambda_0(1-\gamma)$ and such that  but for convenience we have expressed the statement as an asymptotic one} \weichen{added one sentence before the theorem.}

\begin{proof}
We firstly construct an iterative relation along the sequence $\{\mathbb{E}\|\bm{\Delta}_t\|_2^2\}_{t \geq 0}$ in general, and then refine our analysis using a specific choice of stepsizes. The TD iteration rule \eqref{eq:TD-update-all} directly implies that
\begin{align*}
\|\bm{\Delta}_{t}\|_2^2 &= \|\bm{\Delta}_{t-1}\|_2^2 -2\eta_t \bm{\Delta}_{t-1}^\top (\bm{A}_t \bm{\theta}_{t-1} - \bm{b}_t) + \eta_t^2 \|\bm{A}_t \bm{\theta}_{t-1} - \bm{b}_t\|_2^2 \\ 
&\leq \|\bm{\Delta}_{t-1}\|_2^2 -2\eta_t \bm{\Delta}_{t-1}^\top (\bm{A}_t \bm{\theta}^\star + \bm{A}_t \bm{\Delta}_{t-1} - \bm{b}_t) + 2\eta_t^2 (\|\bm{A}_t \bm{\Delta}_{t-1}\|_2^2 + \|\bm{A}_t \bm{\theta}^\star - \bm{b}_t\|_2^2)\\
&= \|\bm{\Delta}_{t-1}\|_2^2 -2\eta_t \bm{\Delta}_{t-1}^\top \bm{A \Delta}_{t-1} - 2\eta_t\bm{\Delta}_{t-1}^\top(\bm{A}_t-\bm{A})\bm{\Delta}_{t-1}\\ 
& - 2\eta_t \bm{\Delta}_{t-1}^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)+ 2\eta_t^2 (\|\bm{A}_t \bm{\Delta}_{t-1}\|_2^2 + \|\bm{A}_t \bm{\theta}^\star - \bm{b}_t\|_2^2).
\end{align*}
Since $\bm{\Delta}_{t-1}^\top \bm{A \Delta}_{t-1} \geq \lambda_0(1-\gamma)\|\bm{\Delta}\|_{t-1}$ due to \eqref{eq:lemma-A-1} and $\|\bm{A}_t\|\leq 1+\gamma$, we can bound $\mathbb{E}\|\bm{\Delta}_{t}\|_2^2$ by 
\begin{align}\label{eq:markov-L2-iter}
\mathbb{E}\|\bm{\Delta}_{t}\|_2^2 &\leq \mathbb{E}\|\bm{\Delta}_{t-1}\|_2^2 -2\lambda_0(1-\gamma) \eta_{t} \mathbb{E}\|\bm{\Delta}_{t-1}\|_2^2 +2\eta_t^2(1+\gamma)^2  \mathbb{E}\|\bm{\Delta}_{t-1}\|_2^2 \nonumber \\ 
&- 2\eta_t \mathbb{E}[\bm{\Delta}_{t-1}^\top(\bm{A}_t - \bm{A})\bm{\Delta}_{t-1}] - 2\eta_t \mathbb{E}[\bm{\Delta}_{t-1}^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)] + 2\eta_t^2 \mathbb{E}\|\bm{A}_t \bm{\theta}^\star - \bm{b}_t\|_2^2 \nonumber \\ 
&= \underset{I_1}{\underbrace{\left(1-2\lambda_0(1-\gamma)\eta_t + 2\eta_t^2(1+\gamma)^2\right) \mathbb{E}\|\bm{\Delta}_{t-1}\|_2^2 }}+ \underset{I_2}{\underbrace{2\eta_t^2 \mathbb{E}\|\bm{A}_t \bm{\theta}^\star - \bm{b}_t\|_2^2}} \nonumber \\ 
&- 2\eta_t\underset{I_3}{\underbrace{ \mathbb{E}[\bm{\Delta}_{t-1}^\top(\bm{A}_t - \bm{A})\bm{\Delta}_{t-1}]}} - 2 \eta_t \underset{I_4} {\underbrace{\mathbb{E}[\bm{\Delta}_{t-1}^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)]}}.
\end{align}

In this expression, $I_1$ is contractive with respect to $\mathbb{E}\|\bm{\Delta}_{t-1}\|_2^2$ as long as $\eta_t$ is sufficiently small, while $I_2$ is proportional to $\eta_t^2$ since $\mathbb{E}\|\bm{A}_t\bm{\theta}^\star - \bm{b}_t\|_2^2$ is independent of $t$. These two terms are desirable and can be left as they are; 

The difficulty of this proof lies in bounding $I_3$ and $I_4$ using Markov samples. Notice that with $i.i.d.$ sampling, both terms are actually $0$; hence, we aim to bound them by applying the mixing property of the Markov chain. 

To simplify notation, throughout the proof, we denote 
\begin{align*}
t_{\mix}:=t_{\mix}(\eta_t) + 1,
\end{align*}
so that with Markov samples, $s_{t-1}\mid \mathscr{F}_{t-t_{\mix}} \sim P^{t_{\mix}-1}(\cdot \mid s_{t-t_{\mix}})$, and that
\begin{align*}
d_{\mathsf{TV}}(P^{t_{\mix}-1}(\cdot \mid s_{t-t_{\mix}}),\mu) \leq \eta_t.
\end{align*}
Meanwhile, Assumption \ref{as:mixing} implies that

\begin{align}\label{eq:tmix-bound-L2}
t_{\mix} \leq \frac{\log(m/\eta_t)}{\log(1/\rho)} +1 = \frac{\log(m/\eta_0) + \alpha \log t}{\log(1/\rho)} + 1< \frac{\log(m/\eta_0) + \alpha \log t}{1-\rho} + 1.
\end{align}

%\ale{I think it would be good here to introduce $\bm{\Delta}_{t-t_{\mix}}$ and explain its role and the proof strategy.}\weichen{please see below.}
In other words, $t_{\mix}(\eta_t)$ grows at most by $O(\log t)$; therefore, in what follows, we can assume that $t$ is large enough such that $t \geq 2t_{\mix}$. The essential idea of bounding $I_3$ and $I_4$ involves decomposing $\bm{\Delta}_{t-1}$ by
\begin{align*}
\bm{\Delta}_{t-1} = (\bm{\Delta}_{t-1} - \bm{\Delta}_{t-t_{\mix}}) + \bm{\Delta}_{t-t_{\mix}},
\end{align*}
where the norm of $(\bm{\Delta}_{t-1} - \bm{\Delta}_{t-t_{\mix}})$ is bounded by the decaying stepsizes, while the correlation between $\bm{\Delta}_{t-t_{\mix}}$ and $\bm{A}_t, \bm{b}_t$ is bounded by the mixing property of the Markov chain.

We address $I_3$ and $I_4$ respectively.

\paragraph{Bounding $I_3$.} The definition of $t_{\mix}$ implies
\begin{align*}
&\left|\mathbb{E}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t - \bm{A})\bm{\Delta}_{t-t_{\mix}}]\right|\\
&= \left|\mathbb{E}[\mathbb{E}_{t-t_{\mix}}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t - \bm{A})\bm{\Delta}_{t-t_{\mix}}]]\right| \\ 
&= \left|\mathbb{E} \left[\mathbb{E}_{s_{t-1} \sim P^{t_{\mix}-1}(\cdot \mid s_{t-t_{\mix}})}[\mathbb{E}_{s_t \sim P(\cdot \mid s_{t-1})}[\bm{\Delta}_{t-t_{\mix}}^\top\bm{A}_t \bm{\Delta}_{t-t_{\mix}}]] - \mathbb{E}_{s_{t-1} \sim \mu}[\mathbb{E}_{s_t \sim P(\cdot \mid s_{t-1})}[\bm{\Delta}_{t-t_{\mix}}^\top\bm{A}_t \bm{\Delta}_{t-t_{\mix}}]  \right]] \right|\\ 
&\leq \mathbb{E}\sup_{s_{t-1}} |\mathbb{E}_{s_t \sim P(\cdot \mid s_{t-1})}\bm{\Delta}_{t-t_{\mix}}^\top\bm{A}_t \bm{\Delta}_{t-t_{\mix}}| \cdot d_{\mathsf{TV}}(P^{t_{\mix}-1}(\cdot \mid s_{t-t_{\mix}}),\mu) \\ 
&\leq \mathbb{E}[2\|\bm{\Delta}_{t - t_{\mix}}\|_2^2 ] \cdot \eta_t;
\end{align*}
notice that the inequality on the fourth line follows from the basic property of the TV distance.
As a direct consequence, $I_3$ is featured by %\ale{Here we need to say explicitly that $t > t_{\mix}$}\weichen{checked.}
\begin{align*}
I_3 &= \mathbb{E}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t - \bm{A})\bm{\Delta}_{t-t_{\mix}}] + 2\mathbb{E}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t - \bm{A})(\bm{\Delta}_{t-1} - \bm{\Delta}_{t-t_{\mix}})] \\ 
&+ \mathbb{E}[(\bm{\Delta}_{t-1} - \bm{\Delta}_{t-t_{\mix}})^\top (\bm{A}_t - \bm{A})(\bm{\Delta}_{t-1} - \bm{\Delta}_{t-t_{\mix}})] \\ 
&\geq -2\eta_t\mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2^2 -4 \mathbb{E}[\|\bm{\Delta}_{t-t_{\mix}}\|_2\|\bm{\Delta}_{t-1} - \bm{\Delta}_{t-t_{\mix}}\|_2]-2 \mathbb{E}\|\bm{\Delta}_{t-1} - \bm{\Delta}_{t-t_{\mix}}\|_2^2 \\ 
&= -2\eta_t\mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2^2 -4 \mathbb{E}[\|\bm{\Delta}_{t-t_{\mix}}\|_2\|\bm{\theta}_{t-1} - \bm{\theta}_{t-t_{\mix}}\|_2]-2 \mathbb{E}\|\bm{\theta}_{t-1} - \bm{\theta}_{t-t_{\mix}}\|_2^2.
\end{align*}
%\ale{Why is it the case that $| \mathbb{E}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t - \bm{A})\bm{\Delta}_{t-t_{\mix}}] | \leq 2\eta_t\mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2^2 $? Where does the $\eta_t$ come from? This looks like a repeating typo.}\weichen{This actually comes from the mixing property and the definition of $t_{\mix}$. Added an explanation earlier.}
To lower bound the right-hand-side of the last expression, the following lemma comes in handy, with its proof postponed to Appendix \ref{app:proof-lemma-E-delta-tmix}.
\begin{customlemma}\label{lemma:E-delta-tmix}
For the TD iterations \eqref{eq:TD-update-all} with Markov samples and non-increasing stepsizes $\eta_1 \geq ... \geq \eta_T$ it holds that, for all $t \geq t_{\mix}$, %\ale{add brief proof?}\weichen{added a reference to the section of its proof.}
\begin{subequations}
\begin{align}
&\mathbb{E}\|\bm{\theta}_{t-1} - \bm{\theta}_{t-t_{\mix}}\|_2 \leq t_{\mix}\eta_{t-t_{\mix}}(2\|\bm{\theta^\star}\|_2 + 1)+ 2\eta_{t - t_{\mix}}\sum_{i=t-t_{\mix}}^{t-2}\mathbb{E}\|\bm{\Delta}_i\|_2; \label{eq:E-delta-tmix-1}\\ 
&\mathbb{E}\|\bm{\theta}_{t-1} - \bm{\theta}_{t-t_{\mix}}\|_2^2 \leq 2t_{\mix}\eta_{t-t_{\mix}}^2\left[t_{\mix}(2\|\bm{\theta}^\star\|_2+1)^2 + 4 \sum_{i=t-t_{\mix}}^{t-2} \mathbb{E}\|\bm{\Delta}_i\|_2^2\right]; \label{eq:E-delta-tmix-2}\\ 
&\mathbb{E}[\|\bm{\Delta}_{t-t_{\mix}}\|_2 \|\bm{\theta}_{t-1} - \bm{\theta}_{t-t_{\mix}}\|_2] \nonumber \\ 
&\leq t_{\mix}\eta_{t-t_{\mix}}(2\|\bm{\theta}^\star\|_2 +1)\mathbb{E}\|\bm{\Delta}_{t- t_{\mix}}\|_2+ \eta_{t - t_{\mix}}\sum_{i=t-t_{\mix}}^{t-2}\mathbb{E}\|\bm{\Delta}_{i}\|_2^2 + t_{\mix} \eta_{t - t_{\mix}} \mathbb{E}\|\bm{\Delta}_{t- t_{\mix}}\|_2^2.\label{eq:E-delta-tmix-3}
\end{align}
\end{subequations}
\end{customlemma}

Lemma \ref{lemma:E-delta-tmix} directly leads to  
\begin{align}\label{eq:markov-L2-I3-bound}
I_3 &\geq -2\eta_t\mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2^2 - 4t_{\mix}^2 \eta_{t-t_{\mix}}^2(2\|\bm{\theta^\star}\|_2 + 1)^2 - 16 t_{\mix} \eta_{t-t_{\mix}}^2\sum_{i=t-t_{\mix}}^{t-2}\mathbb{E}\|\bm{\Delta}_i\|_2^2 \nonumber \\ 
&- 4t_{\mix}\eta_{t-t_{\mix}}(2\|\bm{\theta}^\star\|_2 +1)\mathbb{E}\|\bm{\Delta}_{t- t_{\mix}}\|_2 - 4\eta_{t - t_{\mix}}\sum_{i=t-t_{\mix}}^{t-2}\mathbb{E}\|\bm{\Delta}_i\|_2^2  - 4t_{\mix} \eta_{t - t_{\mix}} \mathbb{E}\|\bm{\Delta}_{t- t_{\mix}}\|_2^2 \nonumber \\ 
&= -(2\eta_t +4t_{\mix}\eta_{t-t_{\mix}})\mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2^2 - (16 t_{\mix} \eta_{t-t_{\mix}}^2+ 4\eta_{t - t_{\mix}})\sum_{i=t-t_{\mix}}^{t-2}\mathbb{E}\|\bm{\Delta}_i\|_2^2 \nonumber \\ 
&-4t_{\mix}^2 \eta_{t-t_{\mix}}^2(2\|\bm{\theta^\star}\|_2 + 1)^2 -4t_{\mix}\eta_{t-t_{\mix}}(2\|\bm{\theta}^\star\|_2 +1)\mathbb{E}\|\bm{\Delta}_{t- t_{\mix}}\|_2.
\end{align}

\paragraph{Bounding $I_4$.} Similarly, we decompose $I_4$ as
\begin{align*}
I_4 &=  \mathbb{E}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)] + \mathbb{E}[(\bm{\Delta}_{t-1} - \bm{\Delta}_{t-t_{\mix}})^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)]\\ 
&= \mathbb{E}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)] + \mathbb{E}[(\bm{\theta}_{t-1} - \bm{\theta}_{t-t_{\mix}})^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)].
\end{align*}
The first term can be bounded using  the $t_{\mix}$ separation: %\ale{Here we need to define $\mathbb{E}_{t-t_{\mix}}[\cdot]$}\weichen{yes, at the beginning of the section}.
\begin{align*}
&|\mathbb{E}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)]| \\ 
&= |\mathbb{E} [\mathbb{E}_{t-t_{\mix}}[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)]]|\\ 
&= |\mathbb{E} [\mathbb{E}_{s_{t-1} \sim P^{t_{\mix}-1}(\cdot \mid s_{t-t_{\mix}})}[\mathbb{E}_{s_t \sim P(\cdot \mid s_{t-1})}[\bm{\Delta}_{t-t_{\mix}}^\top(\bm{A}_t \bm{\theta}^\star - \bm{b}_t)]] -  \mathbb{E}_{s_{t-1}\sim \mu }[\mathbb{E}_{s_t \sim P(\cdot \mid s_{t-1}) }[\bm{\Delta}_{t-t_{\mix}}^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)]]]|\\
&\leq \mathbb{E} \sup_{s_{t-1}} |\mathbb{E}_{s_t \sim P(\cdot \mid s_{t-1})}\bm{\Delta}_{t-t_{\mix}}^\top(\bm{A}_t \bm{\theta}^\star - \bm{b}_t)| \cdot d_{\mathsf{TV}}(P^{t_{\mix}-1}(\cdot \mid s_{t-t_{\mix}}),\mu)\\
&\leq  \mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2 (2\|\bm{\theta}^\star\|_2 + 1) \cdot \eta_t,
\end{align*}
%\ale{where does $\eta_t$ come from in the last equation? } \weichen{please see above.}
while the second term can be bounded by stepsizes:
\begin{align*}
&\mathbb{E}[(\bm{\theta}_{t-1} - \bm{\theta}_{t-t_{\mix}})^\top (\bm{A}_t \bm{\theta}^\star - \bm{b}_t)]\\ 
&\geq -(2\|\bm{\theta}^\star\|_2 + 1)\mathbb{E}\|\bm{\theta}_{t-1} - \bm{\theta}_{t-t_{\mix}}\|_2 \\ 
&\geq -(2\|\bm{\theta}^\star\|_2 + 1) \left[t_{\mix} \eta_{t-t_{\mix}} (2\|\bm{\theta}^\star\|_2 + 1) + 2\eta_{t-t_{\mix}} \sum_{i=t-t_{\mix}}^{t-2} \mathbb{E}\|\bm{\Delta}_i\|_2\right]\\ 
&= -\eta_{t-t_{\mix}}(2\|\bm{\theta}^\star\|_2 + 1) \left[t_{\mix}(2\|\bm{\theta}^\star\|_2 + 1) + 2\sum_{i=t-t_{\mix}}^{t-2} \mathbb{E}\|\bm{\Delta}_i\|_2 \right].
\end{align*}
Therefore, $I_4$ can be bounded by
\begin{align}\label{eq:markov-L2-I4-bound}
I_4 &\geq -\eta_t \mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2 (2\|\bm{\theta}^\star\|_2 + 1) \nonumber \\ 
&-\eta_{t-t_{\mix}}(2\|\bm{\theta}^\star\|_2 + 1) \left[t_{\mix}(2\|\bm{\theta}^\star\|_2 + 1) + 2\sum_{i=t-t_{\mix}}^{t-2} \mathbb{E}\|\bm{\Delta}_i\|_2 \right].
\end{align}
\paragraph{Combining terms.} With $I_3$ and $I_4$ bounded, we now return to Equation \eqref{eq:markov-L2-iter}. $\mathbb{E}\|\bm{\Delta}_{t}\|_2^2$ can be upper bounded by 
\begin{align}\label{eq:markov-L2-iter-bound}
\mathbb{E}\|\bm{\Delta}_{t}\|_2^2 &\leq I_1 + I_2 - 2\eta_t(I_3 + I_4) \nonumber \\ 
&\leq [1-2\lambda_0(1-\gamma)\eta_t + 2\eta_t^2(1+\gamma)^2]\mathbb{E}\|\bm{\Delta}_{t-1}\|_2^2 + 2\eta_t^2(2\|\bm{\theta}\|_2+1)^2 \nonumber \\
&+2\eta_t (2\eta_t +4t_{\mix}\eta_{t-t_{\mix}})\mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2^2 + 2\eta_t (16 t_{\mix} \eta_{t-t_{\mix}}^2+ 4\eta_{t - t_{\mix}})\sum_{i=t-t_{\mix}}^{t-2}\mathbb{E}\|\bm{\Delta}_i\|_2^2 \nonumber \\ 
&+8\eta_t t_{\mix}^2 \eta_{t-t_{\mix}}^2 (2\|\bm{\theta^\star}\|_2 + 1)^2 + 8\eta_t t_{\mix}\eta_{t-t_{\mix}}(2\|\bm{\theta}^\star\|_2 +1)\mathbb{E}\|\bm{\Delta}_{t- t_{\mix}}\|_2 \nonumber \\ 
&+ 2\eta_t^2 \mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2 (2\|\bm{\theta}^\star\|_2 + 1) +2\eta_t \eta_{t-t_{\mix}}(2\|\bm{\theta}^\star\|_2 + 1)t_{\mix}(2\|\bm{\theta}^\star\|_2 + 1) \nonumber \\ 
&+4\eta_t \eta_{t-t_{\mix}}(2\|\bm{\theta}^\star\|_2 + 1)\sum_{i=t-t_{\mix}}^{t-2} \mathbb{E}\|\bm{\Delta}_i\|_2 .
\end{align}
\paragraph{Specifying the polynomially-decaying stepsizes.} With polynomially-decaying stepsizes, when $t$ is sufficiently large, it can be guaranteed that $t > 2t_{\mix}$, and therefore $\eta_{t-t_{\mix}} \geq 2^{-\alpha} \eta_t$. Furthermore, for sufficiently large $t$, $\eta_t (1+\gamma)^2 < \lambda_0(1-\gamma)$. Therefore, by dividing $(2\|\bm{\theta}^\star\|_2+1)^2$ on both sides and combining terms, we can simplify Equation \eqref{eq:markov-L2-iter-bound} as
\begin{align}\label{eq:markov-L2-iter-simplify}
\frac{\mathbb{E}\|\bm{\Delta}_{t}\|_2^2}{(2\|\bm{\theta}^\star\|_2+1)^2} &\leq (1-\widetilde{C}_1t^{-\alpha})\frac{\mathbb{E}\|\bm{\Delta}_{t-1}\|_2^2}{(2\|\bm{\theta}^\star\|_2+1)^2}  + \widetilde{C}_2 t^{-2\alpha}\log^2 t + \widetilde{C}_3 t^{-2\alpha} \log t \frac{\mathbb{E}\|\bm{\Delta}_{t-t_{\mix}}\|_2^2}{{(2\|\bm{\theta}^\star\|_2+1)^2} } \nonumber \\ 
&+ \widetilde{C}_4 t^{-2\alpha} \log t \sum_{i=t-t_{\mix}}^{t-2}\frac{\mathbb{E}\|\bm{\Delta}_i\|_2^2}{{(2\|\bm{\theta}^\star\|_2+1)^2} } + \widetilde{C}_5 t^{-2\alpha} \sum_{i=t-t_{\mix}}^{t-2}\frac{\mathbb{E}\|\bm{\Delta}_i\|_2}{2\|\bm{\theta}^\star\|_2+1} ,
\end{align}
where $\widetilde{C}_1$ throught $\widetilde{C}_5$ are constants depending on $\alpha,\eta_0,m$ and $\rho$. Notice that the $\log t$ terms occur due to $t_{\mix} = O(\log t)$; see \eqref{eq:tmix-bound-L2}. We will use an induction argument based on the relation \eqref{eq:markov-L2-iter-simplify}. For simplicity, let
\begin{align*}
X_t = \frac{\|\bm{\Delta}_t\|_2}{2\|\bm{\theta}^\star\|_2+1};
\end{align*}
now suppose that
\begin{align}\label{eq:markov-L2-induction-assumption}
\mathbb{E}[X_t^2] \leq \widetilde{C} \cdot \frac{\log^2 t}{t^{\alpha}}, \quad \forall 1 < t \leq k,
\end{align}
for some $\widetilde{C}$. 
% \ale{above we must have $t>0$} \weichen{checked.} \yuting{check the base case. $t=1$ does not work either?}\weichen{Yes.... this is annoying. Used $t > 1$ instead.}



Our goal is to demonstrate, inductively, that
\begin{align}\label{eq:markov-L2-induction-goal}
\mathbb{E}[X_{k+1}^2] \leq \widetilde{C} \cdot \frac{\log^2 (k+1)}{(k+1)^{\alpha}}.
\end{align}
%\ale{is this the induction relation?}\weichen{yes, just added the word "inductively".}
Towards this end, the iterative relation \eqref{eq:markov-L2-iter-simplify} implies that
\begin{align}\label{eq:markov-L2-induction-1}
\mathbb{E}[X_{k+1}^2] &\leq \left(1-\widetilde{C}_1(k+1)^{-\alpha}\right)\mathbb{E}[X_k^2] + \widetilde{C}_2 (k+1)^{-2\alpha}\log^2(k+1) \nonumber \\ 
&+ \widetilde{C}_3(k+1)^{-2\alpha} \log (k+1) \mathbb{E}[X_{k+1-t_{\mix}}^2] \nonumber \\ 
&+ \widetilde{C}_4(k+1)^{-2\alpha} \log(k+1) \sum_{i=k+1-t_{\mix}}^{k-1} \mathbb{E}[X_i^2] \nonumber \\ 
&+ \widetilde{C}_5(k+1)^{-2\alpha}\sum_{i=k+1-t_{\mix}}^{k-1} \mathbb{E}[X_i].
\end{align}
Here, the induction assumption guarantees that, as long as $k > 2t_{\mix}$, 
\begin{align*}
&\mathbb{E}[X_k^2] \leq \widetilde{C} k^{-\alpha} \log^2 k, \\ 
&\mathbb{E}[X_{k+1-t_{\mix}}^2] \leq \widetilde{C} (k+1-t_{\mix})^{-\alpha} \log^2(k+1-t_{\mix}) < 2^{-\alpha} \widetilde{C} (k+1)^{-\alpha} \log^2(k+1), 
\end{align*}
and that 
\begin{align*}
\sum_{i=k+1-t_{\mix}}^{k-1} \mathbb{E}[X_i] &\leq t_{\mix} \cdot \widetilde{C} (k+1-t_{\mix})^{-\frac{\alpha}{2}} \log(k+1-t_{\mix}) \\ 
&\lesssim \widetilde{C} \cdot (k+1)^{-\frac{\alpha}{2}} \log^2(k+1),\\ 
\sum_{i=k+1-t_{\mix}}^{k-1} \mathbb{E}[X_i^2] &\leq t_{\mix} \cdot \widetilde{C} (k+1-t_{\mix})^{-\alpha} \log^2(k+1-t_{\mix}) \\ 
&\lesssim \widetilde{C} \cdot (k+1)^{-\alpha} \log^3(k+1).
\end{align*}
Plugging these inequalities into the iteration relation \eqref{eq:markov-L2-induction-1}, we obtain that for sufficiently large $k$,
\begin{align*}
\mathbb{E}[X_{k+1}^2] &\leq \widetilde{C} \cdot \left[k^{-\alpha} \log^2 k - \widetilde{C}_1(k+1)^{-\alpha} k^{-\alpha} \log^2 k + \widetilde{C}_3 (k+1)^{-\frac{5}{2}\alpha}\log^2(k+1)\right]\\ 
& + \widetilde{C}_2(k+1)^{-2\alpha} \log^2(k+1).
\end{align*}
Here, $\widetilde{C}_1, \widetilde{C}_2$ and $\widetilde{C}_3$ are again constants independent of $t$, with there exact values can change from \eqref{eq:markov-L2-induction-1}. Therefore, it suffices to prove that
\begin{align}\label{eq:markov-L2-induction-2}
&\widetilde{C} \cdot \left[k^{-\alpha} \log^2 k - \widetilde{C}_1(k+1)^{-\alpha} k^{-\alpha} \log^2 k + \widetilde{C}_3 (k+1)^{-\frac{5}{2}\alpha}\log^2(k+1)\right]\nonumber \\ 
& + \widetilde{C}_2(k+1)^{-2\alpha} \log^2(k+1) \leq \widetilde{C} (k+1)^{-\alpha} \log^2(k+1).
\end{align}
Notice that when $x$ is sufficiently large, the function $f(x) = x^{-\alpha}\log^2(x)$ is monotonically decreasing; therefore, for sufficiently large $k$, it can be guaranteed that $k^{-\alpha} \log^2 k > (k+1)^{-\alpha} \log (k+1)$. Therefore, the left-hand-side of \eqref{eq:markov-L2-induction-2} is upper bounded by
\begin{align*}
&\widetilde{C} \cdot \left[k^{-\alpha} \log^2 k - \widetilde{C}_1(k+1)^{-\alpha} k^{-\alpha} \log^2 k + \widetilde{C}_3 (k+1)^{-\frac{5}{2}\alpha}\log^2(k+1)\right]\\ 
 &+\widetilde{C}_2(k+1)^{-2\alpha} \log^2(k+1) \\ 
&\leq \widetilde{C} \cdot \left[k^{-\alpha} \log^2 (k+1) - \widetilde{C}_1(k+1)^{-\alpha} (k+1)^{-\alpha} \log^2 (k+1) + \widetilde{C}_3 (k+1)^{-\frac{5}{2}\alpha}\log^2(k+1)\right]\\ 
 &+\widetilde{C}_2(k+1)^{-2\alpha} \log^2(k+1) \\ 
 &= \widetilde{C} \log^2(k+1) \cdot \left[k^{-\alpha} +\left(\frac{\widetilde{C}_2}{\widetilde{C}}- \widetilde{C}_1\right)(k+1)^{-2\alpha} + \widetilde{C}_3 (k+1)^{-\frac{5}{2}\alpha} \right].
\end{align*}
Hence, in order to prove \eqref{eq:markov-L2-induction-2}, it suffices to show that
\begin{align*}
k^{-\alpha} +\left(\frac{\widetilde{C}_2}{\widetilde{C}}- \widetilde{C}_1\right)(k+1)^{-2\alpha} + \widetilde{C}_3 (k+1)^{-\frac{5}{2}\alpha} \leq (k+1)^{-\alpha},
\end{align*}
which is equivalent to
\begin{align*}
(k+1)^{2\alpha}\left[k^{-\alpha} - (k+1)^{-\alpha}\right] + \widetilde{C}_3 (k+1)^{-\frac{\alpha}{2}} \leq \widetilde{C}_1 - \frac{\widetilde{C}_2}{\widetilde{C}}.
\end{align*}
Here, we further notice that the function $f(x) = x^{-\alpha}$ is monotonically decreasing and convex, so $k^{-\alpha}-(k+1)^{-\alpha} = f(k) -f(k+1) \leq -f'(k+1) = \alpha(k+1)^{-\alpha-1}$. Hence, the proof boils down to showing
\begin{align*}
\widetilde{C}_1 - \frac{\widetilde{C}_2}{\widetilde{C}} &\geq (k+1)^{2\alpha}\cdot \alpha(k+1)^{-\alpha-1} + \widetilde{C}_3 (k+1)^{-\frac{\alpha}{2}} \\ 
&= \alpha(k+1)^{\alpha-1} + \widetilde{C}_3 (k+1)^{-\frac{\alpha}{2}}
\end{align*}
for an appropriate $\widetilde{C}$ that is independent of $t$ and satisfies the induction assumption \eqref{eq:markov-L2-induction-assumption}. 
Towards this end, we define a function $f(\widetilde{C},k)$ as
\begin{align*}
f(\widetilde{C}, k):= \widetilde{C}_1 - \frac{\widetilde{C}_2}{\widetilde{C}} - \alpha(k+1)^{\alpha-1}- \widetilde{C}_3 (k+1)^{-\frac{\alpha}{2}}
\end{align*}
It is easy to verify that for any $\widetilde{C}$,
\begin{align*}
\lim_{k \to \infty} f(\widetilde{C}, k) = \widetilde{C}_1 - \frac{\widetilde{C}_2}{\widetilde{C}}.
\end{align*}
Therefore, we can take 
\begin{align*}
&k^\star = \min\left\{k:f\left(\max_{1 \leq t \leq k} \frac{t^\alpha}{\log^2 t} \mathbb{E}[X_t^2], k\right) \geq 0 \right\}, \quad \text{and} \\
&\widetilde{C} = \max_{1 \leq t \leq k^\star} \frac{t^\alpha}{\log^2 t} \mathbb{E}[X_t^2].
\end{align*}
On one hand, if $k^\star$ does not exist, then from our analysis we can conclude that 
\begin{align*}
\mathbb{E}[X_t^2] \leq \frac{\widetilde{C}_2}{\widetilde{C}_1} \frac{\log^2 t}{t^{\alpha}}
\end{align*}
for all $t \geq 1$; on the other hand, if $k^\star$ does exist, then an induction argument guarantees that
\begin{align*}
\mathbb{E}[X_t^2] \leq \widetilde{C} \frac{\log^2 t}{t^{\alpha}}
\end{align*}
for all $t \geq 1$. In both cases, \eqref{eq:markov-L2-induction-goal} holds true. 
\paragraph{Specification of the coefficient.} We next try to specify the coefficient corresponding to the leading term of the upper bound. In the previous paragraph, we have essentially proved that, there exists a $t^\star \in \mathbb{N}$ depending on $\alpha,\eta_0,\lambda_0,m$ and $\rho$ such that
\begin{align*}
\mathbb{E}[X_t^2] \leq 1, \quad \text{for all }\quad t \geq t^\star.
\end{align*}
Hence, when $t > t^\star$, a closer examination of \eqref{eq:markov-L2-iter-bound} yields
\begin{align*}
\mathbb{E}[X_t^2] \leq (1-\lambda_0(1-\gamma)\eta_t) \mathbb{E}[X_{t-1}^2] + C  \frac{\eta_0^2}{(1-\rho)^2} t^{-2\alpha}\log^2 t
\end{align*}
for a \emph{universal constant} $C$. Hence by iteration, it can be guaranteed that when $t > t^\star$,
\begin{align*}
\mathbb{E}[X_t^2] \leq \underset{I_1}{\underbrace{\prod_{i=t^\star+1}^t (1-\beta i^{-\alpha})X_{t^\star}}} + C  \underset{I_2}{\underbrace{\frac{\eta_0^2}{(1-\rho)^2}\sum_{i=t^\star}^t (i^{-2\alpha} \log^2 i)\prod_{k=i+1}^t (1-\beta k^{-\alpha})}}.
\end{align*}
Here, it is easy to verify that $I_1$ converges exponentially with respect to $t$, and that $I_2$ is upper bounded by
\begin{align*}
I_2 &\leq \frac{\eta_0^2}{(1-\rho)^2} \log^2 t \sum_{i=t^\star}^t i^{-2\alpha} \prod_{k=i+1}^t (1-\beta k^{-\alpha}) \\ 
&\leq \frac{\eta_0^2}{(1-\rho)^2} \log^2 t \sum_{i=1}^t i^{-2\alpha} \prod_{k=i+1}^t (1-\beta k^{-\alpha}) \\ 
&\overset{(i)}{=} \frac{\eta_0^2}{(1-\rho)^2} \log^2 t \left(\frac{1}{\beta} t^{-\alpha} + O(t^{-1})\right)\\ 
&= \frac{\eta_0}{(1-\rho)^2 \lambda_0(1-\gamma)} t^{-\alpha}\log^2 t + o(t^{-\alpha}\log^2 t).
\end{align*}
The theorem follows immediately.
% \paragraph{Specify to Scenario \ref{case:nu}.} The convergence of $\mathbb{E}\|\bm{\Delta}_t\|_2^2$ under Scenario \ref{case:nu} can be guaranteed by an induction argument similar to our reasoning under Scenario \ref{case:alpha}, and the details are omitted.
% \paragraph{Specify to Scenario \ref{case:eta}.} By replacing $\eta_t$, $\eta_{t-t_{\mix}}$ by $\eta_0$ and inductively bounding $\mathbb{E}\|\bm{\Delta}_i\|_2^2$ by $(2\|\bm{\theta}^\star\|_2+1)^2$ for all $i < t$, the iterative relation \eqref{eq:markov-L2-iter-bound} is translated to
% \begin{align*}
% \mathbb{E}\|\bm{\Delta}_t\|_2^2 &\leq (1-2\lambda_0(1-\gamma)\eta) (2\|\bm{\theta}^\star\|_2+1)^2 \\ 
% &+ \eta^2 (2\|\bm{\theta}^\star\|_2+1)^2 \cdot \bigg\{2(1+\gamma)^2 + 2 + 4 + 8\tmix + 32\eta \tmix^2 + 8\tmix \\ 
% &\qquad \qquad \qquad \qquad \quad + 8\eta \tmix^2 + 8\tmix + 2 + 2\tmix + 4\tmix \bigg\};
% \end{align*}
% furthermore, the condition \eqref{eq:S3-eta-condition-L2} guarantees $\eta \tmix \ll 1$, and hence
% \begin{align*}
% \mathbb{E}\|\bm{\Delta}_t\|_2^2 &\leq (1-2\lambda_0(1-\gamma)\eta) (2\|\bm{\theta}^\star\|_2+1)^2 + 86\eta^2 \tmix (2\|\bm{\theta}^\star\|_2+1)^2 \\ 
% &\leq (2\|\bm{\theta}^\star\|_2+1)^2,
% \end{align*}
% where the second inequality is also implied by condition \eqref{eq:S3-eta-condition-L2}.

\end{proof}


\subsection{High-probability convergence guarantee for the original TD estimation error}\label{app:proof-TD-original}
Similar to the case with $i.i.d.$ samples, we firstly state the following theorem for the high-probability convergence rate for the original TD estimation error $\bm{\Delta}_t$ with Markov samples.
\begin{theorem}
\label{thm:markov-deltat-convergence}
Consider TD with Polyak-Ruppert averaging~\eqref{eq:TD-update-all} with Markov samples and decaying stepsizes $\eta_t = \eta_0 t^{-\alpha}$ for $\alpha \in (\frac{1}{2},1)$. Suppose that the Markov transition kernel has a unique stationary distribution $\mu$, a strictly positive spectral gap $1-\lambda > 0$, and Assumption \ref{as:mixing} holds true. Then for any $\delta \in (0,1)$, there exists $\eta_0 > 0$, such that with probability at least $1-{\delta}$,
\begin{align*}
\|\bm{\Delta}_t\|_2 &\leq \frac{13}{2}\|\bm{\theta}^\star\|_2 + \frac{5}{4} \quad \text{and}\\
\|\bm{\Delta}_t\|_2 &\lesssim \eta_0 \sqrt{\frac{2\tmix(t^{-\frac{\alpha}{2}})}{(2\alpha-1)}\log \frac{9T\tmix(t^{-\frac{\alpha}{2}})}{\delta}}(2\|\bm{\theta}^\star\|_2  + 1)\left(\frac{(1-\gamma)\lambda_0 \eta_0}{4\alpha}\right)^{-\frac{\alpha}{2(1-\alpha)}}t^{-\frac{\alpha}{2}}
\end{align*}
hold simultaneously for all $t \in [T]$.
\end{theorem}

\begin{proof}
Recalling the TD update rule \eqref{eq:TD-update-all}, we represent $\bm{\Delta}_t$ as
\begin{align*}
\bm{\Delta}_t = \bm{\theta}_t - \bm{\theta}^\star
&= (\bm{\theta}_{t-1}-\eta_t(\bm{A}_{t}\bm{\theta}_{t-1}-\bm{b}_{t})) - \bm{\theta}^\star\\
&= \bm{\Delta}_{t-1} - \eta_t (\bm{A}\bm{\theta}_{t-1}-\bm{b} + \bm{\zeta}_{t})\\
&= (\bm{I}-\eta_t \bm{A})\bm{\Delta}_{t-1} -\eta_t \bm{\zeta}_{t},
\end{align*}
where $\bm{\zeta}_t$ is defined as
\begin{align}\label{eq:defn-zetat}
\bm{\zeta}_t := (\bm{A}_t -\bm{A})\bm{\theta}_{t-1} - (\bm{b}_t-\bm{b}).
\end{align}
Therefore by induction, $\bm{\Delta}_t$ can be expressed as a weighted sum of $\{\bm{\zeta}_i\}_{0 \leq i < t}$, namely
\begin{align}
& \bm{\Delta}_t = \prod_{k=1}^{t} (\bm{I}-\eta_k \bm{A}) \bm{\Delta}_0 -\sum_{i=0}^{t-1} \bm{R}_i^t \bm{\zeta}_i,  \label{eq:delta-t-markov} \\ 
&  \text{where} \quad \bm{R}_i^t = \eta_i \prod_{k=i+1}^{t-1} (\bm{I}-\eta_k \bm{A}).
\end{align}

The difficulty in bounding the second term of \eqref{eq:delta-t-markov} lies in the fact that with Markov samples, $\{\bm{\zeta_i}\}_{i > 0}$ is no longer a martingale difference process. Therefore, we further decompose $\bm{\zeta}_i$ into three parts, namely
%In order to characterize the second term under Markov samples, we further decompose $\bm{\zeta}_i$ into three parts, namely
\begin{align}\label{eq:markov-zetai-decompose}
\bm{\zeta}_i = \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}] + (\bm{\zeta}_{i,\mix} - \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}]) + (\bm{\zeta}_i - \bm{\zeta}_{i,\mix}).
\end{align}
%\ale{What is $\mathbb{E}_{i_{\mix}}$? The conditional expectation given $s_{i_{\mix}}$?}\weichen{Yes, added notation at the beginning of the section.}
In order to simplify notation, throughout this proof we denote
\begin{align*}
t_{\mix} = t_{\mix}(\varepsilon) + 1,
\end{align*}
where $\varepsilon \in (0,1)$ is to be specified later. 
Furthermore, for every $t > 0$, we denote
\begin{align} 
&i_{\mix} = \max\{0,i - t_{\mix}(\varepsilon)\}, \quad \text{and} \label{eq:defn-t-imix}\\ 
&\bm{\zeta}_{i,\mix} = (\bm{A}_i - \bm{A})\bm{\theta}_{i_{\mix}} - (\bm{b}_i - \bm{b}),\label{eq:defn-zeta-imix}
\end{align}
%\yuting{$t_{\mix}(\varepsilon)$ should be defined earlier when you first use this.} \weichen{changed to the main text.}

The intuition behind the construction of $\bm{\zeta}_{i,\mix}$ is to guarantee that the samples $(\bm{A}_i,\bm{b}_i)$ and the iterated estimator $\bm{\theta}_{i_{\mix}}$ are separated in the Markov chain by at least $t_{\mix}$ samples, so that their distributions are close to independent. Recall from \eqref{eq:tmix.bound} that the mixing property of the Markov chain featured by Assumption \ref{as:mixing} guarantees
\begin{align}
\tmix \leq \frac{ \log (m/\varepsilon)}{\log (1/\rho)} + 1.
\end{align}

Furthermore, the difference bewteen $\bm{\zeta}_i $ and $\bm{\zeta}_{i,\mix}$ is 
\begin{align}
\bm{\zeta}_i - \bm{\zeta}_{i,\mix} = (\bm{A}_i - \bm{A}) (\bm{\theta}_{i-1} - \bm{\theta}_{i,\mix}).
\end{align}
Therefore, with the decomposition \eqref{eq:markov-zetai-decompose}, $\bm{\Delta}_t$ can be characterized as
\begin{align}\label{eq:markov-deltat-decompose}
\bm{\Delta}_t &= \underset{I_1}{\underbrace{\prod_{k=1}^t (\bm{I}-\eta_k \bm{A}) \bm{\Delta}_0}} - \underset{I_2}{\underbrace{\sum_{i=1}^t \bm{R}_i^t \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}]}} - \underset{I_3}{\underbrace{\sum_{i=1}^t \bm{R}_i^t (\bm{A}_i - \bm{A}) (\bm{\theta}_{i-1} - \bm{\theta}_{i,\mix})}} \nonumber \\ 
&- \underset{I_4}{\underbrace{\sum_{i=1}^t \bm{R}_i^t (\bm{\zeta}_{i,\mix} - \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}])}}.
\end{align}

In what follows, we denote
\begin{align*}
& \beta=\frac{1-\gamma}{2}\lambda_0\eta_0,\\ %\quad
& R = \frac{13}{2}\|\bm{\theta}^\star\|_2 + \frac{5}{4}, \quad \text{and}\\
& \mathcal{H}_t = \left\{\max_{1 \leq i \leq t}\|\bm{\Delta}_i\|_2 \leq R\right\}.
\end{align*}
Furthermore, for any given $\delta$, let\footnote{Notice that the existence of $t^\star(\delta)$ is guaranteed by a similar reasoning to that in the proof of Theorem B.1 in \cite{wu2024statistical}.}
\begin{align}\label{eq:defn-tstar-markov}
t^\star = t^\star(\delta):= \inf\Bigg\{t \in \mathbb{N}^+: &\int_t^{\infty} \exp \left(-\frac{2\alpha-1}{2^{11}\eta_0^2} \frac{\log(1/\rho)}{\log(8m\eta_0/\beta)}\left(\frac{\beta}{2\alpha}\right)^{\frac{\alpha}{1-\alpha}}x^{-\alpha}\right)\mathrm{d}x \nonumber \\ 
&\leq \frac{\delta}{27} \cdot \frac{\log(8m\eta_0/\beta)}{\log(1/\rho)}\Bigg\},
\end{align}
and assume that 
\begin{align}\label{eq:deltat-condition-markov}
\eta_0 \sqrt{\frac{2}{2\alpha-1} \frac{\log(8m\eta_0/\beta)}{\log(1/\rho)}} \max\left\{\frac{1}{4\sqrt{1-\alpha}},\log \frac{9\log(8m\eta_0/\beta)t^\star}{\log(1/\rho)\delta}\right\}\leq \frac{1}{32}.
\end{align}
We break down the proof of the theorem into a sequence of steps:
\begin{enumerate}
\item We obtain convergence rates for the four terms on the right-hand-side of \eqref{eq:markov-deltat-decompose};
\item We lower bound  the probability of $\mathcal{H}_{t^\star}$ by %$\mathbb{P}(\mathcal{H}_{t^\star})$ 
 $1-\frac{\delta}{3}$;
\item We lower bound the probability of $\mathcal{H}_{\infty}$ by %$\mathbb{P}(\mathcal{H}_{\infty}) \geq$ 
  $1-\frac{2\delta}{3}$;
\item Using the results from the first steps, we arrive at a final bound on $\|\bm{\Delta}_t\|_2$.
\end{enumerate}

\paragraph{Step 1: Basic convergence properties of the four terms on the right-hand-side of \eqref{eq:markov-deltat-decompose}.} As is shown in the proof of Theorem B.1 in \cite{wu2024statistical}, the norm of $I_1$ is bounded by
\begin{align}\label{eq:markov-deltat-I1-bound}
\left\|\prod_{k=0}^{t-1} (\bm{I}-\eta_k \bm{A}) \bm{\Delta}_0\right\| \leq \left(\frac{1-\gamma}{2}\lambda_0 \eta_0\right)^{-\frac{\alpha}{1-\alpha}} t^{-\alpha} \|\bm{\Delta}_0\|_2.
\end{align}
For the term $I_2$, we observe that for $i \leq t_{\mix}$, since $i_{\mix} = 0$, 
\begin{align*}
\mathbb{E}_{i_{\mix}}\left[\bm{\zeta}_{i,\mix}\right]= \mathbb{E}_0 [(\bm{A}_i-\bm{A})\bm{\theta}_0 - (\bm{b}_i - \bm{b})] = \bm{0};
\end{align*}
otherwise when $i > t_{\mix}$, since $i_{\mix} = i-t_{\mix}$, 
\begin{align}\label{eq:E-zeta-imix}
\left\|\mathbb{E}_{i_\mix}[\bm{\zeta}_{i,\mix}]\right\|_2&\leq \left\| (\mathbb{E}_{i_{\mix}}[\bm{A}_i] - \mathbb{E}[\bm{A}_i]) \bm{\theta}_{i_{\mix}}\right\|_2  + \left\|\mathbb{E}_{i_{\mix}}[\bm{b}_i] - \mathbb{E}[\bm{b}_i]\right\|_2\nonumber \\
&\leq d_{\text{TV}}(P^{t_{\mix}}(\cdot|s_{i_{\mix}}),\mu) \left(\sup_{s_{i-1},s_i \in \mathcal{S}} \| \bm{A}_i \bm{\theta}_{i_{\mix}} \|_2 + \sup_{s_i \in \mathcal{S}} \|\bm{b}_i\|_2\right)\nonumber \\
&\leq (2\max_{1 \leq i < t}\|\bm{\Delta}_i\|_2+ 2\|\bm{\theta}^\star\|_2 + 1) d_{\text{TV}}(P^{t_{\mix}}(\cdot|s_{i_{\mix}}),\mu) \nonumber \\ 
&\leq \varepsilon (2\max_{1 \leq i < t}\|\bm{\Delta}_i\|_2+ 2\|\bm{\theta}^\star\|_2 + 1).
\end{align}
%\ale{in the first line above, replace $b$ with $\mathbb{E}[b_i]$}
Meanwhile, we observe that the sum of $\|\bm{R}_i^t\|$ can be bounded by %\yuting{use consistent notation for spectral norm} \weichen{checked.} 
\begin{align*}
\sum_{i=1}^t \|\bm{R}_i^t\| &\leq \sum_{i=1}^t \eta_i \prod_{k=i+1}^t \|\bm{I}-\eta_k \bm{A}\|\\ 
&\leq \sum_{i=1}^t \eta_0 i^{-\alpha} \prod_{k=i+1}^t \left(1-\frac{1-\gamma}{2}\lambda_0 \eta_0 k^{-\alpha}\right)\\ 
&= \sum_{i=1}^t \eta_0 i^{-\alpha} \prod_{k=i+1}^t (1-\beta k^{-\alpha})\\ 
&= \frac{\eta_0}{\beta} \sum_{i=1}^t \left(\prod_{k=i+1}^t - \prod_{k=i}^t\right)(1-\beta k^{-\alpha}) \\ 
&= \frac{\eta_0}{\beta}\left(1-\prod_{k=1}^t (1-\beta k^{-\alpha})\right) < \frac{\eta_0}{\beta},
\end{align*}
where the second inequality follows from \eqref{eq:lemma-A-5} in Lemma \ref{lemma:A}.
%\ale{I could not find what results in the manuscript we are using to justify these bounds - we can just cite bounds from the AISTATS paper}\weichen{checked; it's from Lemma \ref{lemma:A}.}
Therefore, the norm of $I_2$ is bounded by 
\begin{align}\label{eq:markov-deltat-I2-bound}
\left\| \sum_{i=1}^t \bm{R}_i^t \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}] \right\|&\leq \sum_{i=1}^t \left\|\bm{R}_i^t\right\| \left\|\mathbb{E}_{i_\mix}[\bm{\zeta}_{i,\mix}]\right\|_2 \nonumber \\ 
&\leq \frac{\varepsilon \eta_0}{\beta} (2\max_{1 \leq i \leq t}\|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1).
\end{align}
For the term $I_3$, we firstly bound the difference betweeen $\bm{\theta}_i$ and $\bm{\theta}_{i,\mix}$ by $R$ and the initial stepsize $\eta_0$. Notice that given the induction assumption, 
\begin{align*}
\left\|\bm{\theta}_{i-1} - \bm{\theta}_{i,\mix}\right\|_2 = \left\|\sum_{j=i_{\mix}+1}^{i-1} (\bm{\theta}_{j} - \bm{\theta}_{j-1}) \right\|_2 &= \left\| \sum_{j=i_{\mix}+1}^{i-1} \eta_j (\bm{A}_j \bm{\theta}_{j-1} - \bm{b}_j)\right\|_2 \\ 
&\leq \sum_{j=i_{\mix}}^{i-1} \eta_j \|\bm{A}_j \bm{\theta}_{j-1} - \bm{b}_j\|_2 \\ 
&\leq \frac{t_{\mix}}{1-\alpha} \eta_i (2 \max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1).
\end{align*}
Hence, the norm of $I_{3}$ can be bounded by
\begin{align}\label{eq:markov-deltat-I3-bound}
&\left\|\sum_{i=1}^t \bm{R}_i^t (\bm{A}_i - \bm{A}) (\bm{\theta}_i - \bm{\theta}_{i,\mix})\right\|_2  \nonumber \\
& \leq \sum_{i=1}^t \|\bm{R}_i^t\| \|\bm{A}_i - \bm{A}\| \left\|\bm{\theta}_{i-1} - \bm{\theta}_{i,\mix}\right\|_2 \nonumber \\ 
&\leq \sum_{i=1}^t \left(\eta_i \prod_{k=i+1}^t \left(1-\beta k^{-\alpha} \right)\right) \cdot 4 \cdot \frac{t_{\mix}}{1-\alpha} \eta_i (2 \max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\nonumber \\ 
&\lesssim \sum_{i=1}^t \eta_i^2 \prod_{k=i+1}^t \left(1-\beta k^{-\alpha} \right) \cdot  \frac{t_{\mix}}{1-\alpha} (2\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\nonumber \\ 
&\leq \frac{\eta_0^2t_{\mix}}{(1-\alpha)} (2\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1) \cdot \sum_{i=1}^t i^{-2\alpha} \prod_{k=i+1}^t (1-\beta k^{-\alpha})\nonumber \\
&\leq \frac{\eta_0^2t_{\mix}}{1-\alpha} (2\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1) \cdot \frac{1}{2\alpha-1}\left(\frac{\beta}{2\alpha}\right)^{\frac{\alpha}{1-\alpha}}t^{-\alpha}.
\end{align}
Notice that the last inequality follows by taking $\nu = 2\alpha$ in \eqref{eq:lemma-R-2} in Lemma \ref{lemma:R}. %\ale{I am confused - which results from \ref{lemma:R}?}\weichen{added more details.}

For the term $I_4$, we again invoke the vector Azuma's inequality (Theorem \ref{thm:vector-Azuma}). %\ale{I think this is Corollary A.16 in the AISTATS paper, right?}\weichen{yes,checked}.
For simplicity, we define
\begin{align}
	\bm{X}_i := \bm{\zeta}_{i,\text{mix}}-\mathbb{E}_{i_{\text{mix}}}[\bm{\zeta}_{i,\text{mix}}].
\end{align}
By definition, we can see that for every integer $r \in [\tmix]$, the sequence $\{ \bm{X}_{r + i t_{\text{mix}}} \}_{i=0,1,\ldots}$
%\begin{align*}
%\bm{X}_r, \bm{X}_{r+t_{\text{mix}}}, \bm{X}_{r+2t_{\text{mix}}},......,\bm{X}_{r+it_{\text{mix}}},.......
%\end{align*}
form a martingale difference process. Throughout this part, we assume, without loss of generality that $t = t' \cdot \tmix$ for a positive integer $t'$. Hence, we can first consider the norm of the summation
\begin{align}
\sum_{i'=0}^{t'} \bm{R}_{r+i'\tmix}^t \bm{X}_{r+i'\tmix},
\end{align}
which we will bound using vector Azuma's inequality. Towards that goal, we define 
\begin{align*}
W_{\max}^r :=  \sum_{i'=0}^{t'} \sup\left\|\bm{R}_{r+i'\tmix}^t \bm{X}_{r+i'\tmix}\right\|_2^2.
\end{align*}
By definition, this term is bounded by
\begin{align*}
W_{\max}^r &\leq \sum_{i'=0}^{t'}\|\bm{R}_{r+i'\tmix}^t\|^2 \sup \|\bm{X}_{r+i'\tmix}\|_2^2 \leq  \sum_{i'=0}^{t'}\|\bm{R}_{r+i'\tmix}^t\|^2 \cdot \max_{1 \leq i \leq t} \sup \|\bm{X}_i\|_2^2;
\end{align*}
by summing over $r = 1$ through $r = \tmix$, we obtain
\begin{align*}
\sum_{r=1}^{\tmix} W_{\max}^r &\leq \sum_{r=1}^{\tmix} \sum_{i'=0}^{t'}\|\bm{R}_{r+i'\tmix}^t\|^2 \cdot  \max_{1 \leq i \leq t} \sup \|\bm{X}_i\|_2^2 \\ 
&\leq  \left(\max_{1 \leq i \leq t} \sup \|\bm{X}_i\|_2^2\right)\cdot \sum_{i=1}^t \|\bm{R}_i^t\|^2 .
\end{align*}
Meanwhile, the triangle inequality directly implies
\begin{align*}
\max_{1 \leq i \leq t} \sup \|\bm{X}_i\|_2^2 &= \max_{1 \leq i \leq t} \sup \|\bm{\zeta}_{i,\text{mix}}-\mathbb{E}_{i_{\text{mix}}}[\bm{\zeta}_{i,\text{mix}}]\| \\ 
&\leq 4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1.
\end{align*}
In combination, we have that
\begin{align*}
\sum_{r=1}^{\tmix} \sqrt{W_{\max}^r} &\leq \sqrt{\tmix} \sqrt{\sum_{r=1}^{\tmix} W_{\max}^r} \\ 
&\leq \sqrt{\tmix} \sup \|\bm{X}_i\|_2 \sqrt{\sum_{i=1}^t \|\bm{R}_i^t\|^2} \\ 
&\leq \sqrt{\tmix} (4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\cdot \sqrt{\sum_{i=1}^t \|\bm{R}_i^t\|^2} \\ 
&\leq \sqrt{\tmix} (4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\cdot \sqrt{\sum_{i=1}^t \eta_i^2 \prod_{k=i+1}^t (1-\frac{1-\gamma}{2}\lambda_0\eta_k)^2}\\
&\leq \sqrt{\tmix} (4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\cdot \eta_0 \cdot \sqrt{\sum_{i=1}^t i^{-2\alpha} \prod_{k=i+1}^t (1-\beta k^{-\alpha})}\\
&\leq \sqrt{\tmix} (4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\cdot \eta_0 \cdot \sqrt{\frac{1}{2\alpha-1}}\left(\frac{\beta}{2\alpha}\right)^{\frac{\alpha}{2(1-\alpha)}}t^{-\frac{\alpha}{2}},
\end{align*}
%\yuting{can you explain why the second and the third inequalities holds?}\weichen{added some explanation before this long equation.}
where we invoked Lemma \ref{lemma:R} in the last inequality, following the same logic as in the last line of \eqref{eq:markov-deltat-I3-bound}. %\ale{I am confused - which results from \ref{lemma:R}?}\weichen{Added more details. Hope this helps!} 
Consequently, the vector Azuma's inequality (Theorem \ref{thm:vector-Azuma}) %\ale{Again we should cite Corollary A.16 of the AISTATS paper or include this for reference}\weichen{checked}
, combined with a union bound argument, yields the bound on the norm of $I_4$
\begin{align}\label{eq:markov-deltat-I4-bound}
&\left\|\sum_{i=1}^t \bm{R}_i^t (\bm{\zeta}_{i,\mix} - \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}])\right\|_2\nonumber \leq 2\sqrt{2\log \frac{9t_{\mix}}{\delta_t}} \sum_{r=1}^{\tmix} \sqrt{W_{\max}^r} \nonumber \\ 
&\leq 2 \eta_0 \sqrt{\frac{2\tmix}{2\alpha-1}\log \frac{3\tmix}{\delta_t}}(4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\cdot \left(\frac{\beta}{2\alpha}\right)^{\frac{\alpha}{2(1-\alpha)}}t^{-\frac{\alpha}{2}},
\end{align}
with probability at least $1-\delta_t/3$. 
\paragraph{Step 2: Bounding $\mathbb{P}(\mathcal{H}_{t^\star})$.} By definition, $\mathbb{P}(\mathcal{H}_0) = 1$. We will show via induction that, for all other $1 \leq t \leq t^\star$
\begin{align*}
\mathbb{P}(\mathcal{H}_t) \geq 1-\frac{t}{3t^\star} \delta.%, \quad \forall 1 \leq t \leq t^\star.
\end{align*}
By taking $\varepsilon = \frac{\beta}{8\eta_0}$ in \eqref{eq:markov-deltat-I2-bound}, we obtain that
\begin{align*}
\left\| \sum_{i=1}^t \bm{R}_i^t \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}] \right\|_2 \leq \frac{1}{8}\left(2\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1\right).
\end{align*}
Next, putting together \eqref{eq:markov-deltat-I3-bound}, condition \eqref{eq:deltat-condition-markov} and the bound on $\tmix$ as specified by \eqref{eq:tmix-bound} guarantees that
\begin{align*}
\left\|\sum_{i=1}^t \bm{R}_i^t (\bm{A}_i - \bm{A}) (\bm{\theta}_i - \bm{\theta}_{i,\mix})\right\|_2 \leq \frac{1}{8}\left(2\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1\right).
\end{align*}
Similarly, setting $\delta_t = \frac{\delta}{3t^\star}$ in \eqref{eq:markov-deltat-I4-bound} and using the condition \eqref{eq:deltat-condition-markov}, we have that, with probability at least $1-\frac{\delta}{3t^\star}$, 
\begin{align*}
\left\|\sum_{i=1}^t \bm{R}_i^t (\bm{\zeta}_{i,\mix} - \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}])\right\|_2 &\leq 2\tmix \eta_0 \sqrt{2\log \frac{9\tmix t^\star}{\delta}}(4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1) \\ 
&\leq \frac{1}{16}(4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1).
\end{align*}
Therefore, when $\mathcal{H}_{t-1}$ holds true, the norm of $\bm{\Delta}_t$ can be bounded using the triangle inequality as
\begin{align}\label{eq:markov-deltat-induction}
\|\bm{\Delta}_t\|_2 &\leq \|\bm{\theta}^\star\|_2 + \frac{1}{8}\left(2R + 2\|\bm{\theta}^\star\|_2  + 1\right) + \frac{1}{8}\left(2R + 2\|\bm{\theta}^\star\|_2  + 1\right) + \frac{1}{16}(4R + 2\|\bm{\theta}^\star\|_2  + 1) \nonumber \\ 
&= \frac{3}{4}R + \frac{13}{8}\|\bm{\theta}^\star\|_2 + \frac{5}{16} = R,
\end{align}
with probability of at least $1-\frac{\delta}{3t^\star}$. It then follows that
\begin{align*}
\mathbb{P}(\mathcal{H}_{t-1} \setminus \mathcal{H}_t) \leq \frac{\delta}{3t^\star},
\end{align*}
and thus that
\begin{align*}
\mathbb{P}(\mathcal{H}_{t^\star}) \geq 1-\frac{\delta}{3}.
\end{align*}
\paragraph{Step 3: Bounding $\mathbb{P}(\mathcal{H}_{\infty})$.} For $t > t^\star$, we sharpen the induction argument by a more refined choice of $\delta_t$. In detail, let 
\begin{align*}
\delta_t = 3\tmix(\beta/8\eta_0)\exp\left\{-\frac{2\alpha-1}{2^{11} \eta_0^2} \left(\frac{1}{\tmix(\beta/8\eta_0)}\right)\left(\frac{\beta}{2\alpha}\right)^{\frac{\alpha}{1-\alpha}} t^{-\alpha}\right\}.
\end{align*}
Then the norm of $I_4$ is bounded by
\begin{align*}
&\left\|\sum_{i=1}^t \bm{R}_i^t (\bm{\zeta}_{i,\mix} - \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}])\right\|_2\\ 
&\leq 2 \eta_0 \sqrt{\frac{2\tmix}{2\alpha-1}\log \frac{3\tmix}{\delta_t}}(4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\cdot \left(\frac{\beta}{2\alpha}\right)^{\frac{\alpha}{2(1-\alpha)}}t^{-\frac{\alpha}{2}} \\ 
&\leq \frac{1}{16}(4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1).
\end{align*}
with probability at least $1-\delta_t$. Hence, using induction, when $\mathcal{H}_{t-1}$ holds true,  the bound in \eqref{eq:markov-deltat-induction} implies that $\mathcal{H}_t$ also holds true with probability at least $1-\delta_t$. In other words,
\begin{align*}
\mathbb{P}(\mathcal{H}_{t-1}) - \mathbb{P}(\mathcal{H}_t) = \mathcal{P}(\mathcal{H}_{t-1} \setminus \mathcal{H}_t) \leq \delta_t.
\end{align*}
Consequently, the definition of $t^\star$ \eqref{eq:defn-tstar-markov} guarantees
\begin{align*}
\mathbb{P}(\mathcal{H}_{\infty}) &= \mathbb{P}(\mathcal{H}_{t^\star}) - \sum_{t=t^\star+1}^{\infty} \mathbb{P}(\mathcal{H}_{t-1}) - \mathbb{P}(\mathcal{H}_t) \\ 
&\geq \left(1-\frac{\delta}{3}\right) - \sum_{t=t^\star+1}^{\infty} \delta_t \\ 
&\geq \left(1-\frac{\delta}{3}\right) - \int_{t^\star}^{\infty} 3\tmix(\beta/8\eta_0)\exp\left\{-\frac{2\alpha-1}{2^{11} \eta_0^2} \left(\frac{1}{\tmix(\beta/8\eta_0)}\right)\left(\frac{\beta}{2\alpha}\right)^{\frac{\alpha}{1-\alpha}} x^{-\alpha}\right\}\mathrm{d}x \\ 
&\geq \left(1-\frac{\delta}{3}\right) - \frac{\delta}{3} = 1-\frac{2\delta}{3}.
\end{align*}
\paragraph{Step 4: Refining the bound on $\|\bm{\Delta}_t\|_2$.} In order to bound the norm of $\bm{\Delta}_t$ by $O(t^{-\frac{\alpha}{2}})$ and thus conclude the prooof, we take $\varepsilon = t^{-\frac{\alpha}{2}}$. Then,
\begin{align*}
\tmix(\varepsilon) \leq \frac{\log m + \frac{\alpha}{2}\log t}{\log(1/\rho)}.
\end{align*}
With this bound, \eqref{eq:markov-deltat-I2-bound}, \eqref{eq:markov-deltat-I3-bound} and \eqref{eq:markov-deltat-I4-bound} yield that, with probability at least $1-\frac{\delta}{3T}$,
\begin{align*}
&\left\| \sum_{i=1}^t \bm{R}_i^t \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}] \right\|\leq \frac{\eta_0}{\beta}(2\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)t^{-\frac{\alpha}{2}}, \\ 
&\left\|\sum_{i=1}^t \bm{R}_i^t (\bm{A}_i - \bm{A}) (\bm{\theta}_i - \bm{\theta}_{i,\mix})\right\|_2 \leq \frac{\eta_0^2}{(1-\alpha)(2\alpha-1)} \cdot \tmix(2\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1) \left(\frac{\beta}{2\alpha}\right)^{-\frac{\alpha}{1-\alpha}}t^{-\alpha}, \\ 
&\left\|\sum_{i=1}^t \bm{R}_i^t (\bm{\zeta}_{i,\mix} - \mathbb{E}_{i_{\mix}}[\bm{\zeta}_{i,\mix}])\right\|_2 \leq 2 \eta_0 \sqrt{\frac{2\tmix}{(2\alpha-1)}\log \frac{9T\tmix}{\delta}}(4\max_{1 \leq i < t} \|\bm{\Delta}_i\|_2 + 2\|\bm{\theta}^\star\|_2  + 1)\left(\frac{\beta}{2\alpha}\right)^{-\frac{\alpha}{2(1-\alpha)}}t^{-\frac{\alpha}{2}}.
\end{align*}
The final result follows from the triangle inequality and the union bound. 
\end{proof}

%With the convergence of $\bm{\Delta}_t$ guaranteed with high probability, we now proceed to the proof of Theorem \ref{thm:TD-whp}.

\subsection{Proof of Theorem \ref{thm:TD-whp}}\label{app:proof-markov-deltat-convergence}
Recall from Theorem \ref{thm:Lambda} that
\begin{align*}
\mathsf{Tr}(\tilde{\bm{\Lambda}}_T - \bm{\Lambda}^\star) = T^{\alpha-1}\mathsf{Tr}(\bm{X}(\tilde{\bm{\Lambda}}^\star)) + O(T^{2\alpha-2}),
\end{align*}
where $\bm{X}(\tilde{\bm{\Lambda}}^\star)$ is the solution to the Lyapunov equation
\begin{align*}
\eta_0(\bm{AX+XA}^\top) = \tilde{\bm{\Lambda}}^\star.
\end{align*}
By combining Lemma \ref{lemma:A}, Lemma \ref{lemma:Lyapunov} and Lemma \ref{lemma:Gamma}, we obtain
\begin{align*}
\mathsf{Tr}(\bm{X}(\tilde{\bm{\Lambda}}^\star)) &\leq \frac{\mathsf{Tr}(\tilde{\bm{\Lambda}}^\star)}{\eta_0\lambda_0(1-\gamma)} \leq \frac{\|\bm{A}^{-1}\|^2 \mathsf{Tr}(\tilde{\bm{\Gamma}})}{\eta_0\lambda_0(1-\gamma)} 
\leq \frac{\mathsf{Tr}(\tilde{\bm{\Gamma}})}{\eta_0\lambda_0^3(1-\gamma)^3} \\ 
&\lesssim \frac{m}{1-\rho} \cdot \frac{1}{\eta_0\lambda_0^5(1-\gamma)^5}.
\end{align*}
Hence, the difference between $\tilde{\bm{\Lambda}}_T$ and $\bm{\Lambda}^\star$ is given by
\begin{align*}
\mathsf{Tr}(\tilde{\bm{\Lambda}}_T - \bm{\Lambda}^\star)  \leq \widetilde{C}T^{\alpha-1},
\end{align*}
where $\widetilde{C}$ can be represented by $\lambda_0,\eta_0$ and $\gamma$.
%\ale{You mean the operator norm of the difference is $\bm{O}(T^{\alpha-1})$}
%\weichen{Actually, it can also be proved that the difference is $\bm{O}(T^{\alpha-1})$ when measured by trace, without introducing a $d$. Please see above.}
Therefore, it suffices to show that with probability at least $1-\delta$, the averaged TD error can be bounded by
\begin{align*}
\|\bar{\bm{\Delta}}_T\|_2 &\lesssim 2\sqrt{\frac{2\mathsf{Tr}(\widetilde{\bm{\Lambda}}_T)}{T} \log \frac{6d}{\delta}} + o\left(T^{-\frac{1}{2}}\log^{\frac{3}{2}}\frac{dT}{\delta}\right).
\end{align*}
%\ale{2 comments here: (1) the remainder term is larger than in the theorem statement because of the term $\log^{3/2}(dT)$ and (2) Theorem \ref{thm:Lambda} bounds the operator norm of the difference between $\tilde{\bm{\Lambda}}_T$ and $\bm{\Lambda}^\star$ but here we need a bound on $\mathsf{Tr}(\bm{\Lambda}^\star - \tilde{\bm{\Lambda}}_T)$, which will pick up an additional $d$. The same comment applies to the bounds in the AISTATS paper, I think. That is, all is correct but we need to include an extra $\sqrt{d}$ in the remainder term.}
As a direct implication of \eqref{eq:delta-t-markov}, $\bar{\bm{\Delta}}_T$ can be decomposed as 
\begin{align*}
\bar{\bm{\Delta}}_T &= \frac{1}{T}\sum_{t=1}^T \bm{\Delta}_t \\ 
 &= \frac{1}{T}\sum_{t=1}^T \left(\prod_{k=1}^t (\bm{I}-\eta_k \bm{A})\Delta_0 - \sum_{i=1}^{t} \bm{R}_{i}^t (\bm{A}_i\bm{\theta}^\star -\bm{b}_i) - \sum_{i=1}^{t} \bm{R}_{i}^t (\bm{A}_i - \bm{A})\bm{\Delta}_{i-1}\right) \\ 
&= \frac{1}{T}\sum_{t=1}^T\prod_{k=1}^t (\bm{I}-\eta_k \bm{A})\Delta_0 - \frac{1}{T}\sum_{t=1}^T\sum_{i=1}^{t} \bm{R}_{i}^t (\bm{A}_i\bm{\theta}^\star -\bm{b}_i) - \frac{1}{T}\sum_{t=1}^T\sum_{i=1}^{t}\bm{R}_{i}^t (\bm{A}_i - \bm{A})\bm{\Delta}_{i-1} \\ 
&= \frac{1}{T}\sum_{t=1}^T\prod_{k=1}^t (\bm{I}-\eta_k \bm{A})\Delta_0 - \frac{1}{T} \sum_{i=1}^T \sum_{t=i}^T \bm{R}_{i}^t (\bm{A}_i\bm{\theta}^\star -\bm{b}_i) - \frac{1}{T} \sum_{i=1}^T \sum_{t=i}^T \bm{R}_{i}^t (\bm{A}_i - \bm{A})\bm{\Delta}_{i-1},
\end{align*}
where we have switched the order of summation in the last equation. The definition of $\bm{Q}_t$ \eqref{eq:defn-Qt} implies 
\begin{align*}
\bar{\bm{\Delta}}_T = \underset{I_1}{\underbrace{\frac{1}{T\eta_0} \bm{Q}_0 \bm{\Delta}_0}} - \underset{I_2}{\underbrace{\frac{1}{T} \sum_{i=1}^T \bm{Q}_i(\bm{A}_i \bm{\theta}^\star-\bm{b}_i)}} - \underset{I_3}{\underbrace{\frac{1}{T}\sum_{i=1}^T \bm{Q}_i (\bm{A}_i-\bm{A})\bm{\Delta}_{i-1}}},
\end{align*}
where $I_1$ can be bounded by
\begin{align*}
\left\|\frac{1}{T\eta_0} \bm{Q}_0 \bm{\Delta}_0\right\|_2 \leq \frac{1}{T\eta_0} \|\bm{Q}_0\| \|\bm{\Delta}_0\|\leq \frac{3}{T}\left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}} \|\bm{\Delta}_0\|_2. 
\end{align*}
%\yuting{why almost surely? Is it a deterministic bound?} \weichen{checked.}


We now proceed to bounding $I_2$ and $I_3$respectively. 

%\yuting{$I_4$?} 
Throughout the proof, we let
\begin{align*}
&\beta = \frac{1-\gamma}{2}\lambda_0 \eta_0,\\ 
& R = \frac{13}{2}\|\bm{\theta}^\star\|_2 + \frac{5}{4}, \\ 
&t_{\mix} = \tmix(T^{-\frac{\alpha}{2}}) \leq \frac{\log m + (\alpha/2)\log T}{\log(1/\rho)} \quad \text{and}\\ 
&R' = \eta_0 \sqrt{\frac{2\tmix}{2\alpha-1}\log \frac{27T\tmix}{\delta}} (2\|\bm{\theta}^\star\|_2+1)\left(\frac{\beta}{2\alpha}\right)^{-\frac{\alpha}{2(1-\alpha)}}, 
\end{align*}
and, for each $1 \leq t \leq T$,
\[
\mathcal{H}_t = \Big\{ \|\bm{\Delta}_j\|_2 \leq \min\{R'j^{-\frac{\alpha}{2}},R\}, \forall j \leq t \Big\} \quad \text{and} \quad \tilde{\bm{\Delta}}_t = \bm{\Delta}_t \mathds{1}(\mathcal{H}_t).
\]
%Furthermore, we use $\mathcal{H}_t$ to denote the event that
%\begin{align*}
%\|\bm{\Delta}_j\|_2 \leq \min\{R'j^{-\frac{\alpha}{2}},R\}
%\end{align*}
%holds true for all $j \leq t$, and let $\tilde{\bm{\Delta}}_t = \bm{\Delta}_t \mathds{1}(\mathcal{H}_t)$. 
Theorem \ref{thm:markov-deltat-convergence} shows that $\mathbb{P}(\mathcal{H}_T) \geq 1-\frac{\delta}{3}$.
\paragraph{Bounding $I_2$.} In order to invoke the matrix Freedman's inequality on the term $I_2$, we firstly relate it to a martingale. Specifically, for every $i \in [T]$, we define $\bm{U}_i$ as
\begin{align}\label{eq:defn-Ui}
\bm{U}_i = \mathbb{E}_i \left[\sum_{j=i}^{\infty} (\bm{A}_j \bm{\theta}^\star - \bm{b}_j)\right].
\end{align}
It is then easy to verify that on one hand, the norm of $\bm{U}_i$ is uniformly bounded due to the exponential convergence of the Markov chain. Specifically, since for any positive integers $i<j$, it can be guaranteed that
\begin{align*}
&\left\|\mathbb{E}_i[\bm{A}_j \bm{\theta}^\star - \bm{b}_j]\right\|_2 \\ 
&=\left\|\mathbb{E}_{s_{j-1} \sim P^{j-i-1}(\cdot \mid s_i),s_j \sim P(\cdot \mid s_{j-1})}[\bm{A}_j \bm{\theta}^\star - \bm{b}_j]\right\|_2 \\ 
&= \left\|\mathbb{E}_{s_{j-1} \sim P^{j-i-1}(\cdot \mid s_i),s_j \sim P(\cdot \mid s_{j-1})}[\bm{A}_j \bm{\theta}^\star - \bm{b}_j]-\mathbb{E}_{s_{j-1} \sim \mu,s_j \sim P(\cdot \mid s_{j-1})}[\bm{A}_j \bm{\theta}^\star - \bm{b}_j]\right\|_2 \\ 
&\leq d_{\mathsf{TV}}(P^{j-i-1}(\cdot\mid s_i),\mu) \cdot \sup_{s_{j-1},s_j}\|\bm{A}_j \bm{\theta}^\star - \bm{b}_j\|_2 \\ 
&\leq m\rho^{j-i-1} (2\|\bm{\theta}^\star\|_2+1).
\end{align*}
Therefore, the norm of $\bm{U}_i$ is bounded by
\begin{align}\label{eq:U-bound}
\|\bm{U}_i\|_2 &\leq \|\bm{A}_i \bm{\theta}^\star - \bm{b}_i\|_2 + \sum_{j=i+1}^{\infty}\mathbb{E}_i \|\bm{A}_j \bm{\theta}^\star - \bm{b}_j\|_2 \nonumber \\ 
&\leq (2\|\bm{\theta}^\star\|_2+1) \left(1+\sum_{j=i+1}^{\infty} m\rho^{j-i-1}\right)\nonumber \\ 
& \lesssim \frac{1}{1-\rho} (2\|\bm{\theta}^\star\|_2+1);
\end{align}
%\yuting{add explanation here.}
On the other hand, $\bm{A}_i \bm{\theta}^\star - \bm{b}_i$ can be represented as
\begin{align}\label{eq:Ui-telescope}
\bm{A}_i \bm{\theta}^\star - \bm{b}_i &= \bm{U}_i - \mathbb{E}_i[\bm{U}_{i+1}]\nonumber \\
&= (\bm{U}_i - \mathbb{E}_{i-1}[\bm{U}_i]) + (\mathbb{E}_{i-1}[\bm{U}_i] - \mathbb{E}_i[\bm{U}_{i+1}])\nonumber \\ 
&=: \bm{m}_i + (\mathbb{E}_{i-1}[\bm{U}_i] - \mathbb{E}_i[\bm{U}_{i+1}])
\end{align}
Here, the first term $\bm{U}_i -\bm{U}_{i+1}$ can be analyzed by the telescoping technique, while 
\begin{align}\label{eq:defn-mi}
\bm{m}_i:= \bm{U}_{i} - \mathbb{E}_{i-1}[\bm{U}_{i}]
\end{align}
is a martingale difference process. Furthermore, we observe that when $s_0$ is drawn from the stationary distribution $\mu$, the covariance matrix $\mathbb{E}_{0}\left[\bm{m}_i \bm{m}_i^\top\right]$ is time-invariant, and can be expressed as
\begin{align}\label{eq:var-mi}
\mathbb{E}[\bm{m}_i \bm{m}_i^\top ] &= \mathbb{E}[\bm{m}_1 \bm{m}_1^\top]\nonumber \\ 
&= \mathbb{E}[(\bm{U}_1 - \mathbb{E}_0[\bm{U}_1])(\bm{U}_1 - \mathbb{E}_0[\bm{U}_1])^\top]\nonumber\\
&= \mathbb{E}[\bm{U}_1 \bm{U}_1^\top] - \mathbb{E}[\mathbb{E}_0[\bm{U}_1]\mathbb{E}_0[\bm{U}_1^\top]]\nonumber\\
&\overset{(i)}{=} \mathbb{E}[\bm{U}_1 \bm{U}_1^\top] - \mathbb{E}[\mathbb{E}_1[\bm{U}_2]\mathbb{E}_1[\bm{U}_2^\top]]\nonumber\\
&= \mathbb{E}[(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1 + \mathbb{E}_1[\bm{U}_2])(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1 + \mathbb{E}_1[\bm{U}_2])^\top] - \mathbb{E}[\mathbb{E}_1[\bm{U}_2]\mathbb{E}_1[\bm{U}_2^\top]]\nonumber\\
&= \mathbb{E}[(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1)(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1)^\top] + \mathbb{E}[(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1)\mathbb{E}_1[\bm{U}_2]^\top] + \mathbb{E}[\mathbb{E}_1[\bm{U}_2](\bm{A}_1 \bm{\theta}^\star -\bm{b}_1)^\top]\nonumber\\
&= \mathbb{E}[(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1)(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1)^\top]\nonumber \\ 
&+ \sum_{j=2}^{\infty} \mathbb{E}[(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1)(\bm{A}_j \bm{\theta}^\star - \bm{b}_j)^\top + (\bm{A}_j \bm{\theta}^\star - \bm{b}_j)(\bm{A}_1 \bm{\theta}^\star -\bm{b}_1)^\top]\nonumber\\
&= \widetilde{\bm{\Gamma}},
\end{align}
according to the definition of $\widetilde{\bm{\Gamma}}$ (as in eq.~\eqref{eq:defn-tilde-Gamma}). Notice here that we applied the rule of total expectation throughout this deduction, and took advantage of the time-invariant property of the distribution of $\{\bm{U}_i\}_{1 \leq i \leq T}$ in (i).

In order to relate $I_2$ to the martingale difference process $\bm{m}_i$, we invoke the  relation \eqref{eq:Ui-telescope} to obtain
\begin{align*}
\frac{1}{T}\sum_{i=1}^T \bm{Q}_i (\bm{A}_i \bm{\theta}^\star - \bm{b}_i)
&= \frac{1}{T}\sum_{i=1}^T \bm{Q}_i \bm{m}_i + \frac{1}{T}\sum_{i=1}^T \bm{Q}_i (\mathbb{E}_{i-1}[\bm{U}_i] - \mathbb{E}_i[\bm{U}_{i+1}]) \\ 
&= \frac{1}{T}\sum_{i=1}^T \bm{Q}_i \bm{m}_i + \frac{1}{T}\sum_{i=1}^T (\bm{Q}_{i-1}\mathbb{E}_{i-1}[\bm{U}_i] - \bm{Q}_{i}\mathbb{E}_i[\bm{U}_{i+1}]) + \frac{1}{T}\sum_{i=1}^T (\bm{Q}_i - \bm{Q}_{i-1})\mathbb{E}_{i-1}[\bm{U}_i] \\ 
&= \underset{I_{21}}{\underbrace{\frac{1}{T}\sum_{i=1}^T \bm{Q}_i \bm{m}_i}} + \underset{I_{22}}{\underbrace{\frac{1}{T}(\bm{Q}_0 \mathbb{E}_0[\bm{U}_1] - \bm{Q}_T \mathbb{E}_T [\bm{U}_{T+1}])}}+ \underset{I_{23}}{\underbrace{\frac{1}{T}\sum_{i=1}^T (\bm{Q}_i - \bm{Q}_{i-1})\mathbb{E}_{i-1}[\bm{U}_i]}}
\end{align*}
where we applied the telescoping technique in the last equation. The uniform boundedness of $\|\bm{Q}_t\|$, as indicated by Lemma \ref{lemma:Q-bound}, and the uniform boundedness of $\|\bm{U}_i\|_2$, as indicated by \eqref{eq:U-bound}, guarantee that
\begin{align}
\label{eq:markov-bar-deltat-I22-bound}
\left\|\frac{1}{T}(\bm{Q}_0 \mathbb{E}_0[\bm{U}_1] - \bm{Q}_T \mathbb{E}_T [\bm{U}_{T+1}])\right\|_2  
&\leq \frac{1}{T} (\|\bm{Q}_0\| \sup \|\bm{U}_1\|_2 + \|\bm{Q}_T\| \sup \|\bm{U}_T\|_2)\notag\\ 
&\lesssim  \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}}\left(\frac{2m}{1-\rho}\right)(2\|\bm{\theta}^\star\|_2+1)
\end{align}
%\yuting{add details} \weichen{added one line, please check if this clarifies the point.} 
deterministically.
Meanwhile, the norm of $I_{23}$ is bounded by invoking Lemma \ref{lemma:delta-Q}:
\begin{align}
\label{eq:markov-bar-deltat-I23-bound}
\left\|\frac{1}{T}\sum_{i=1}^T (\bm{Q}_i - \bm{Q}_{i-1})\mathbb{E}_{i-1}[\bm{U}_i]\right\| \lesssim \eta_0 \left[\eta_0 \Gamma\left(\frac{1}{1-\alpha}\right)+\alpha\right]\left(\frac{1}{\beta}\right)^{\frac{1}{1-\alpha}} \left(\frac{2m}{1-\rho}\right)(2\|\bm{\theta}^\star\|_2+1)\frac{\log T}{T}
\end{align}
almost surely. 

It now boils down to bounding the norm of $I_{21}$. Towards this end, we firstly observe that
\begin{align*}
\frac{1}{T}\sum_{i=1}^T \mathbb{E}_{s_{i-1}\sim\mu,s_i \sim P(\cdot \mid s_{i-1})}\|\bm{Q}_i\bm{m}_i\|_2^2 = \mathsf{Tr}(\tilde{\bm{\Lambda}}_T),
\end{align*}
and that 
\begin{align*}
\frac{1}{T}\|\bm{Q}_i\bm{m}_i\|_2 \leq \frac{1}{T} \eta_0 \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}}\left(\frac{2m}{1-\rho}\right)(2\|\bm{\theta}^\star\|_2+1)
\end{align*}
almost surely for all $i \in [T]$, according to Lemma \ref{lemma:Q-bound} and Equation \eqref{eq:U-bound}. Now consider a sequence of matrix-valued functions $\bm{F}_i:\mathcal{S} \times \mathcal{S} \to \mathbb{R}^{(d+1) \times (d+1)}$, defined as
\begin{align*}
\bm{F}_i(s,s') = \begin{pmatrix}
0 & (\bm{Q}_i\bm{m}_i(s,s'))^\top \\ 
\bm{Q}_i\bm{m}_i(s,s') & \bm{0}_{d\times d}.
\end{pmatrix}, \quad \forall i \in [T].
\end{align*}
It can then be verified that 
\begin{align*}
\left\|\mathbb{E}_{s\sim \mu, s' \sim P(\cdot \mid s)}[\bm{F}_i^2(s,s')] \right\|= \mathbb{E}_{s\sim \mu, s' \sim P(\cdot \mid s)}\|\bm{Q}_i\bm{m}_i(s,s')\|_2^2,
\end{align*}
and that
\begin{align*}
\|\bm{F}_i(s,s')\| = \|\bm{Q}_i\bm{m}_i(s,s')\|_2, \quad \forall s,s' \in \mathcal{S}.
\end{align*}
Therefore, a direct application of Corollary \ref{thm:matrix-bernstein-mtg} yields
\begin{align}\label{eq:markov-bar-deltat-I21-bound}
\left\|\frac{1}{T}\sum_{i=1}^T \bm{Q}_i \bm{m}_i\right\|_2 &\lesssim 2\sqrt{\frac{2\mathsf{Tr}(\tilde{\bm{\Lambda}}_T)}{T}\log \frac{12d}{\delta}} \nonumber \\ 
&+ \eta_0 \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}}\left(\frac{2m}{1-\rho}\right)(2\|\bm{\theta}^\star\|_2+1)(1-\lambda)^{-\frac{1}{4}} \frac{1}{T}\log^{\frac{3}{2}}\frac{6d}{\delta},
\end{align}
with probability at least $1-\frac{\delta}{3}$.


% \paragraph{Bounding $I_2$.} In order to invoke the matrix Freedman's inequality on the term $I_2$, we firstly relate it to a martingale. Specifically, we define $\widetilde{\bm{U}}_i$ as
% \begin{align}\label{eq:defn-tilde-Ui}
% \widetilde{\bm{U}}_i = \mathbb{E}_i \left[\sum_{j=i}^T \bm{Q}_j (\bm{A}_j \bm{\theta}^\star - \bm{b}_j)\right],
% \end{align}
% for all integers $i \geq 1$. Assumption \ref{as:mixing} and Lemma \ref{lemma:Q} directly implies that the norm of $\widetilde{\bm{U}}_i$ are uniformly bounded by
% \begin{align}\label{eq:tilde-Ui-bound}
% \|\widetilde{\bm{U}}_i\|_2 &\leq \sum_{j=i}^T \|\bm{Q}_j\|\mathbb{E}_i  \|(\bm{A}_j \bm{\theta}^\star - \bm{b}_j)\|_2\nonumber  \\ 
% &\lesssim \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}} \sum_{j=i}^T (2\|\bm{\theta}^\star\|_2+1) \cdot m \rho^{j-i}\nonumber \\ 
% &< \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}} \frac{m}{1-\rho}(2\|\bm{\theta}^\star\|_2+1).
% \end{align}
% Furthermore, we observe that the sequence $\{\bm{Q}_i(\bm{A}_i \bm{\theta}^\star - \bm{b}_i)\}$ is related to the sequence $\{\widetilde{\bm{U}}_i\}$ by
% \begin{align*}
% \widetilde{\bm{U}_i} &= \bm{Q}_i (\bm{A}_i\bm{\theta}^\star - \bm{b}_i) + \mathbb{E}_i \left[\sum_{j=i+1}^T \bm{Q}_j (\bm{A}_j \bm{\theta}^\star - \bm{b}_j)\right] \\ 
% &= \bm{Q}_i (\bm{A}_i\bm{\theta}^\star - \bm{b}_i) + \mathbb{E}_i [\widetilde{\bm{U}}_{i+1}].
% \end{align*}
% Hence, the term $I_2$ can be expressed as
% \begin{align*}
% &\frac{1}{T}\sum_{i=1}^T \bm{Q}_i (\bm{A}_i\bm{\theta}^\star - \bm{b}_i)\\ 
% &= \frac{1}{T}\frac{1}{T}\sum_{i=1}^T (\widetilde{\bm{U}}_i - \mathbb{E}_i[\widetilde{\bm{U}}_{i+1}])\\ 
% &= \frac{1}{T}\sum_{i=1}^T (\widetilde{\bm{U}}_i - \mathbb{E}_{i-1}[\widetilde{\bm{U}}_i] + \mathbb{E}_{i-1}[\widetilde{\bm{U}}_i]-\mathbb{E}_i[\widetilde{\bm{U}}_{i+1}]) \\ 
% &= \frac{1}{T}\sum_{i=1}^T (\widetilde{\bm{U}}_i - \mathbb{E}_{i-1}[\widetilde{\bm{U}}_i]) + \frac{1}{T}\sum_{i=1}^T(\mathbb{E}_{i-1}[\widetilde{\bm{U}}_i]-\mathbb{E}_i[\widetilde{\bm{U}}_{i+1}]) \\ 
% &= \frac{1}{T}\sum_{i=1}^T (\widetilde{\bm{U}}_i - \mathbb{E}_{i-1}[\widetilde{\bm{U}}_i]) + \frac{1}{T}\mathbb{E}_0[\widetilde{\bm{U}}_1] - \frac{1}{T}\mathbb{E}_T [\widetilde{\bm{U}}_{T+1}],
% \end{align*}
% where we applied the telescoping method in the last equation. By definition, $\mathbb{E}_0[\widetilde{\bm{U}}_1] = \mathbb{E}_T [\widetilde{\bm{U}}_{T+1}] = \bm{0}$, so we obtain
% \begin{align}\label{eq:markov-bar-deltat-I2-decompose}
% \sum_{i=1}^T \bm{Q}_i (\bm{A}_i\bm{\theta}^\star - \bm{b}_i) = \sum_{i=1}^T (\widetilde{\bm{U}}_i - \mathbb{E}_{i-1}[\widetilde{\bm{U}}_i]).
% \end{align}
% Define $\widetilde{\bm{m}}_i = \widetilde{\bm{U}_i} - \mathbb{E}_{i-1}[\widetilde{\bm{U}}_i]$, then it is easy to confirm that $\widetilde{\bm{m}}_i$ is a martingale difference process with respect to the filtration $\{\mathcal{F}_i\}$; equation \eqref{eq:markov-bar-deltat-I2-decompose} then indicates that the term $I_2$ can be expressed as a martingale. Consequently, the matrix Freedman's inequality can be applied to this term and yields
% \begin{align}\label{eq:markov-bar-deltat-Freedman}
% \left\|\frac{1}{T}\sum_{i=1}^T \bm{Q}_i (\bm{A}_i \bm{\theta}^\star - \bm{b}_i)\right\|_2 \leq 2\sqrt{2W_{\max} \log \frac{3d}{\delta}} + \frac{4}{3}B_{\max} \log \frac{3d}{\delta}
% \end{align}
% with probability at least $1-\frac{\delta}{3}$, in which $W_{\max}$ and $B_{\max}$ are defined respectively by
% \begin{align*}
% &W_{\max} := \frac{1}{T^2}\sum_{i=1}^T \mathbb{E}_{i-1} \|\tilde{\bm{m}}_i\|_2^2, \quad \text{and} \\ 
% &B_{\max} := \frac{1}{T} \max_{1 \leq i \leq t} \sup \|\tilde{\bm{m}}_i\|_2.
% \end{align*}
% Notice that the uniform boundedness of $\tilde{\bm{U}}_i$ directly implies
% \begin{align}\label{eq:markov-bar-deltat-Bmax-bound}
% B_{\max} \leq \frac{1}{T} \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}} \frac{m}{1-\rho}(2\|\bm{\theta}^\star\|_2+1);
% \end{align}
% meanwhile, $W_{\max}$ can be represented directly by
% \begin{align}\label{eq:markov-bar-deltat-Wmax-bound}
% W_{\max} &= \frac{1}{T^2} \mathbb{E}\left[\left(\sum_{i=1}^T \tilde{\bm{m}}_i\right)\left(\sum_{i=1}^T \tilde{\bm{m}}_i\right)^\top\right]\nonumber  \\ 
% &= \frac{1}{T^2} \mathbb{E}\left[\left(\sum_{i=1}^t \bm{Q}_i(\bm{A}_i\bm{\theta}^\star - \bm{b}_i)\right)\left(\sum_{i=1}^t \bm{Q}_i(\bm{A}_i\bm{\theta}^\star - \bm{b}_i)\right)^\top\right] \nonumber \\ 
% &= \frac{\mathsf{Tr}(\tilde{\bm{\Lambda}}_T)}{T}.
% \end{align}
% Plugging \eqref{eq:markov-bar-deltat-Wmax-bound} and \eqref{eq:markov-bar-deltat-Bmax-bound} into \eqref{eq:markov-bar-deltat-Freedman}, we obtain
% \begin{align}\label{eq:markov-bar-deltat-I2-bound}.
% &\left\|\frac{1}{T}\sum_{i=1}^T \bm{Q}_i (\bm{A}_i \bm{\theta}^\star - \bm{b}_i)\right\|_2 \nonumber \\ 
% &\leq 2\sqrt{\frac{2\mathsf{Tr}(\tilde{\bm{\Lambda}}_T)}{T}\log \frac{6d}{\delta}} + \frac{4}{3T}\left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}} \frac{m}{1-\rho}(2\|\bm{\theta}^\star\|_2+1)
% \end{align}
% with probability at least $1-\frac{\delta}{3}$.

\paragraph{Bounding $I_3$.} Applying a similar technique as in the proof of Theorem \ref{thm:markov-deltat-convergence}, we decompose the term $I_3$ as
\begin{align}\label{eq:markov-bar-deltat-I3-decompose}
&\frac{1}{T}\sum_{i=1}^T \bm{Q}_i (\bm{A}_i-\bm{A})\bm{\Delta}_{i-1} \nonumber \\ 
&=\frac{1}{T} \sum_{i=1}^T \bm{Q}_i(\bm{A}_i - \bm{A}) (\bm{\Delta}_{i-1} -\bm{\Delta}_{i_{\mix}}) + \frac{1}{T}\sum_{i=1}^T \bm{Q}_i(\bm{A}_i - \bm{A}) \bm{\Delta}_{i_{\mix}} \nonumber \\
&= \underset{I_{31}}{\underbrace{\frac{1}{T} \sum_{i=1}^T \bm{Q}_i(\bm{A}_i - \bm{A}) (\bm{\Delta}_{i-1} -\bm{\Delta}_{i_{\mix}})}} + \underset{I_{32}}{\underbrace{\frac{1}{T}\sum_{i=1}^T \bm{Q}_i(\bm{A}_i - \mathbb{E}_{i_{\mix}}[\bm{A}_i])\bm{\Delta}_{i_{\mix}} }}+ \underset{I_{33}}{\underbrace{\frac{1}{T}\sum_{i=1}^T \bm{Q}_i(\mathbb{E}_{i_{\mix}}[\bm{A}_i] - \bm{A}) \bm{\Delta}_{i_{\mix}} }}.
\end{align}
Recall from the proof of Theorem \ref{thm:markov-deltat-convergence} that 
\begin{align*}
\left\|\bm{\Delta}_{i-1} -\bm{\Delta}_{i_{\mix}}\right\|_2&= \left\|\bm{\theta}_{i-1} -\bm{\theta}_{i_{\mix}}\right\|_2 \\ 
&\leq \frac{t_{\mix}}{1-\alpha} \eta_i (2 \max_{1 \leq j < i} \|\bm{\Delta}_j\|_2 + 2\|\bm{\theta}^\star\|_2  + 1);
\end{align*}
hence the norm of $I_{31}$ can be bounded by
\begin{align}
\label{eq:markov-bar-deltat-I31-bound}
\left\|\frac{1}{T} \sum_{i=1}^T \bm{Q}_i(\bm{A}_i - \bm{A}) (\bm{\Delta}_{i-1} -\bm{\Delta}_{i_{\mix}})\right\|_2
&\lesssim \frac{1}{T} \sum_{i=1}^T \|\bm{Q}_i\| \cdot \frac{t_{\mix}}{1-\alpha} \eta_i (2 \max_{1 \leq j < i} \|\bm{\Delta}_j\|_2 + 2\|\bm{\theta}^\star\|_2  + 1) \nonumber \\ 
&\lesssim \frac{\tmix}{(1-\alpha)T}\left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}}\sum_{i=1}^T \eta_i (2 \max_{1 \leq j < T} \|\bm{\Delta}_j\|_2 + 2\|\bm{\theta}^\star\|_2  + 1) \nonumber \\ 
&\lesssim \frac{\tmix \eta_0 }{(1-\alpha)^2}  \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}} (2 \max_{1 \leq j < T} \|\bm{\Delta}_j\|_2 + 2\|\bm{\theta}^\star\|_2  + 1) T^{-\alpha}.
\end{align}
The term $I_{32}$ can be decomposed into $\tmix$ martingales and bounded by the vector Azuma's inequality, invoking a similar technique to the tackling of the term $I_4$ in the proof of Theorem \ref{thm:markov-deltat-convergence}. With details omitted, we obtain with probability at least $1-\frac{\delta}{3}$ that
\begin{align}\label{eq:markov-bar-deltat-I32-bound}
&\left\|\frac{1}{T}\sum_{i=1}^T \bm{Q}_i(\bm{A}_i - \mathbb{E}_{i_{\mix}}[\bm{A}_i])\tilde{\bm{\Delta}}_{i_{\mix}}\right\|_2^2 \lesssim \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}}  \sqrt{\frac{\tmix}{1-\alpha}\log \frac{9\tmix}{\delta}}R' T^{-\frac{\alpha+1}{2}}.
\end{align}
The term $I_{33}$ is bounded by the mixing property of the Markov chain, specifically
\begin{align}\label{eq:markov-bar-deltat-I33-bound}
\left\|\frac{1}{T}\sum_{i=1}^T \bm{Q}_i(\mathbb{E}_{i_{\mix}}[\bm{A}_i] - \bm{A}) \tilde{\bm{\Delta}}_{i_{\mix}}\right\|_2 \lesssim \frac{1}{T} \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}} T^{-\frac{\alpha}{2}} \sum_{i=1}^T R' (i_{\mix})^{-\frac{\alpha}{2}} \lesssim \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}} R' T^{-\alpha}.
\end{align}
\paragraph{Completing the proof.} Combining \eqref{eq:markov-bar-deltat-I21-bound}, \eqref{eq:markov-bar-deltat-I22-bound}, \eqref{eq:markov-bar-deltat-I23-bound}, \eqref{eq:markov-bar-deltat-I31-bound}, \eqref{eq:markov-bar-deltat-I32-bound}, and \eqref{eq:markov-bar-deltat-I33-bound} by a union bound argument and plugging in the definition of $R'$, we obtain
\begin{align*}
\|\bar{\bm{\Delta}}_T\|_2 &\lesssim 2\sqrt{\frac{2\mathsf{Tr}(\widetilde{\bm{\Lambda}}_T)}{T} \log \frac{6d}{\delta}} \\ 
&+ \frac{\eta_0\tmix(T^{-\frac{\alpha}{2}})}{(1-\alpha)^2}  (2\|\bm{\theta}^\star\|_2+1)\left(\frac{1-\gamma}{2}\lambda_0\eta_0\right)^{-\frac{1}{1-\alpha}}T^{-\alpha} \\ 
&+ \eta_0 \sqrt{\frac{2\tmix(T^{-\frac{\alpha}{2}})}{2\alpha-1}\log \frac{27T \tmix(T^{-\frac{\alpha}{2}})}{\delta}}(2\|\bm{\theta}^\star\|_2+1)\left(\frac{1-\gamma}{2}\lambda_0\eta_0\right)^{-\frac{2+\alpha}{2(1-\alpha)}}T^{-\alpha} \\ 
&+ \frac{\eta_0\tmix(T^{-\frac{\alpha}{2}})}{\sqrt{(1-\alpha)(2\alpha-1)}} \log \frac{27T \tmix(T^{-\frac{\alpha}{2}})}{\delta}(2\|\bm{\theta}^\star\|_2+1)\left(\frac{1-\gamma}{2}\lambda_0\eta_0\right)^{-\frac{2+\alpha}{2(1-\alpha)}}T^{-\frac{\alpha+1}{2}} \\ 
&+ \eta_0 \frac{m}{1-\rho} (2\|\bm{\theta}^\star\|_2+1)\left(\frac{1-\gamma}{4}\lambda_0\eta_0\right)^{-\frac{1}{1-\alpha}}T^{-1} \\ 
&\cdot \left[(1-\lambda)^{-\frac{1}{4}}\log^{\frac{3}{2}}\frac{6d}{\delta} + \left(\eta_0 \Gamma\left(\frac{1}{1-\alpha}\right)+\alpha\right) \log T\right].
\end{align*}
Notice that all the terms beginning from the second line can all be bounded by
\begin{align*}
\widetilde{C}T^{-\alpha}\log^{\frac{3}{2}} \frac{dT}{\delta},
\end{align*}
where $\widetilde{C}$ is a problem-related quantity depending on $\alpha,\eta_0,\lambda_0, \gamma, m,\rho$ and $\lambda$. The theorem follows immediately.

\subsection{Proof of Theorem \ref{thm:TD-berry-esseen}} \label{app:proof-TD-Berry-Esseen}
Following the precedent of \cite{wu2024statistical}, we approach this Berry-Esseen bound by introducing a Gaussian comparison term. Specifically, the triangle inequality indicates
\begin{align}\label{eq:TD-Berry-Esseen-decompose}
d_{\mathsf{C}}(\sqrt{T} \bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\widetilde{\bm{\Lambda}}^{\star}))\leq d_{\mathsf{C}}(\sqrt{T} \bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\widetilde{\bm{\Lambda}}_T)) + d_{\mathsf{C}}(\mathcal{N}(\bm{0},\widetilde{\bm{\Lambda}}_T), \mathcal{N}(\bm{0},\widetilde{\bm{\Lambda}}^{\star}))
\end{align}
where the second term on the right-hand-side can be bounded by the following proposition.
\begin{customlemma}\label{lemma:Gaussian-comparison}
With $\tilde{\bm{\Lambda}}^\star$ and $\tilde{\bm{\Lambda}}_T$ defined as in \eqref{eq:defn-tilde-Lambdastar} and \eqref{eq:defn-tilde-LambdaT} respectively, it can be guaranteed for any $\eta_0 \leq \frac{1}{2\lambda_{\Sigma}}$ that
\begin{align*}
d_{\mathsf{C}}(\mathcal{N}(\bm{0},\widetilde{\bm{\Lambda}}_T), \mathcal{N}(\bm{0},\widetilde{\bm{\Lambda}}^{\star})) \lesssim \frac{\sqrt{d\mathsf{cond}(\bm{\tilde{\Gamma}})}}{(1-\gamma)\lambda_0\eta_0} T^{\alpha-1} + O(T^{2\alpha-2}).
\end{align*}
\end{customlemma}
\begin{proof}
This lemma is a direct generalization of Theorem 3.3 in \cite{wu2024statistical}, where $\bar{\bm{\Lambda}}_T$ is replaced by $\tilde{\bm{\Lambda}}_T$ and $\bm{\Lambda}^\star$ is replaced by $\tilde{\bm{\Lambda}}^\star$. 
\end{proof}

We next focus on the first term on the right-hand-side of \eqref{eq:TD-Berry-Esseen-decompose}. For this, we notice that 
\begin{align*}
d_{\mathsf{C}}(\sqrt{T} \bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\widetilde{\bm{\Lambda}}_T)) = d_{\mathsf{C}}(\sqrt{T} \bm{A}\bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\bm{A}\widetilde{\bm{\Lambda}}_T\bm{A}^\top));
\end{align*}
and we will focus on bounding the latter.

Recall that $\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T$ can be decomposed as
\begin{align}\label{eq:delta-decomposition-markov}
\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T&= \underset{I_1}{\underbrace{\frac{\bm{A}}{\sqrt{T} \eta_0} \bm{Q}_0 \bm{\Delta}_0}} - \underset{I_2}{\underbrace{\frac{\bm{A}}{\sqrt{T}} \sum_{i=1}^T \bm{Q}_i (\bm{A}_i  - \bm{A})\bm{\Delta}_{i-1}}} -\underset{I_3} {\underbrace{\frac{\bm{A}}{\sqrt{T}} \sum_{i=1}^T \bm{Q}_i (\bm{A}_i \bm{\theta}^\star - \bm{b}_i) }}.
% &= \underset{I_1}{\underbrace{\frac{1}{\sqrt{T} \eta_0} \bm{Q}_0 \bm{\Delta}_0}} - \underset{I_2}{\underbrace{\frac{1}{\sqrt{T}} \sum_{i=1}^T \bm{Q}_i (\bm{A}_i  - \bm{A})\bm{\Delta}_i }} - \underset{I_3}{\underbrace{\frac{1}{\sqrt{T}} \sum_{i=1}^T (\bm{Q}_i - \bm{A}^{-1})(\bm{A}_i \bm{\theta}^\star - \bm{b}_i)}} - \underset{I_4}{\underbrace{\frac{1}{\sqrt{T}} \sum_{i=1}^T \bm{A}^{-1}(\bm{A}_i\bm{\theta}^\star - \bm{b}_i)}}.
\end{align}
In order to derive the non-asymptotic rate at which $\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T$ converges to its Gaussian distribution, we derive the convergence of $I_1$, $I_2$ and $I_3$ accordingly in the following paragraphs. For readability concerns, we will only keep track of dependence on $T$ and $d$ in this proof, and use $\widetilde{C}$ to denote any problem-related parameters that are related to $\alpha,\gamma,\eta_0,\lambda_0,m,\rho$.

\paragraph{The $a.s.$ convergence of $I_1$.} 
Lemma \ref{lemma:Q-bound} directly implies that as $T \to \infty$, $I_1$ is bounded by
% \blue{there exists a constant $\widetilde{C}$ such that}
% \yuting{do you want to use $\lesssim$ or this statement}
\begin{align}\label{eq:markov-CLT-I1-converge}
\left\|\frac{\bm{A}}{\sqrt{T} \eta_0} \bm{Q}_0 \bm{\Delta}_0\right\|_2 \leq \frac{1}{\sqrt{T} \eta_0} \|\bm{A}\bm{Q}_0\| \|\bm{\Delta}_0\|_2 \lesssim \lambda_{\Sigma} \left(\frac{2}{\beta}\right)^{\frac{1}{1-\alpha}}\|\bm{\theta}^\star\|_2 T^{-\frac{1}{2}}.
\end{align}
almost surely.

\paragraph{Bounding $I_2$ with high probability.} The convergence of $I_2$ is result of the uniform boundedness of  $\bm{Q}_i$, the convergence of $\{\bm{\Delta}_t\}$, and the mixing property of the Markov chain. Specifically, we again apply the technique in the proof of Theorem \ref{thm:markov-deltat-convergence} and define
\begin{align}\label{eq:defn-markov-CLT-tmix}
&\tmix = \tmix(T^{-\frac{1}{2}}), \quad \text{and} \quad i_{\mix} = \max\left\{i-\tmix, 0\right\}.
\end{align}
Assumption \ref{as:mixing} implies that (see \ref{eq:tmix-bound-L2}) 
\begin{align}
\label{eq:tmix.bound}
\tmix \leq \frac{\log m + \frac{1}{2}\log T}{\log(1/\rho)} \lesssim \frac{\log T}{1-\rho}.
\end{align}
The term $I_2$ can be decomposed as
\begin{align*}
&\frac{1}{\sqrt{T}} \sum_{i=1}^T \bm{AQ}_i(\bm{A}_i - \bm{A}) (\bm{\Delta}_i -\bm{\Delta}_{i_{\mix}}) + \frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i(\bm{A}_i - \bm{A}) \bm{\Delta}_{i_{\mix}} \\
&= \underset{I_{21}}{\underbrace{\frac{1}{\sqrt{T}} \sum_{i=1}^T \bm{AQ}_i(\bm{A}_i - \bm{A}) (\bm{\Delta}_{i-1} -\bm{\Delta}_{i_{\mix}})}} + \underset{I_{22}}{\underbrace{\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i(\bm{A}_i - \mathbb{E}_{i_{\mix}}[\bm{A}_i])\bm{\Delta}_{i_{\mix}} }}\\ 
&+ \underset{I_{23}}{\underbrace{\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i(\mathbb{E}_{i_{\mix}}[\bm{A}_i] - \bm{A}) \bm{\Delta}_{i_{\mix}} }};
\end{align*}
for the term $I_{21}$, recall that the difference between $\bm{\Delta}_{i-1}$ and $\bm{\Delta}_{i_{\mix}}$ can be further decomposed into
\begin{align*}
\bm{\Delta}_{i-1}-\bm{\Delta}_{i_{\mix}} = \bm{\theta}_{i-1} - \bm{\theta}_{i_{\mix}} &= \sum_{j=i_{\mix}+1}^{i-1} \eta_j (\bm{\theta}_{j} - \bm{\theta}_{j-1})\\  
&= -\sum_{j=i_{\mix}+1}^{i-1} \eta_j(\bm{A}_j \bm{\theta}_{j-1} - \bm{b}_j) \\ 
&= -\sum_{j=i_{\mix}+1}^{i-1} \eta_j(\bm{A}_j \bm{\theta}^\star - \bm{b}_j) - \sum_{j=i_{\mix}+1}^{i-1} \eta_j\bm{A}_j \bm{\Delta}_{j-1}.
\end{align*}
% \color{violet}
% *ALE* - minor corrections:
% \begin{align*}
% \bm{\Delta}_{i-1}-\bm{\Delta}_{i_{\mix}} = \bm{\theta}_{i-1} - \bm{\theta}_{i_{\mix}} &= \sum_{j=i_{\mix}+1}^{i-1}  (\bm{\theta}_{j} - \bm{\theta}_{j-1})\\  
% &= - \sum_{j=i_{\mix}+1}^{i-1} \eta_j(\bm{A}_j \bm{\theta}_{j-1} - \bm{b}_j) \\ 
% &= - \sum_{j=i_{\mix}+1}^{i-1} \eta_j(\bm{A}_j \bm{\theta}^\star - \bm{b}_j) - \sum_{j=i_{\mix}+1}^{i-1} \eta_j\bm{A}_j \bm{\Delta}_{j-1}.
% \end{align*}
% \weichen{checked}

\color{black}

Hence, the decomposition of $I_2$ can be expressed as
\begin{align}\label{eq:markov-Berry-Esseen-I2-decompose}
I_2 &= -\underset{I_{20}}{\underbrace{\frac{1}{\sqrt{T}}\sum_{i=1}^T \left[\bm{AQ}_i (\bm{A}_i - \bm{A})\sum_{j=i_{\mix}+1}^{i-1} \eta_j (\bm{A}_j \bm{\theta}^\star - \bm{b}_j)\right]}} \nonumber \\ 
&- \underset{I_{21}'}{\underbrace{\frac{1}{\sqrt{T}}\sum_{i=1}^T \left[\bm{AQ}_i (\bm{A}_i - \bm{A})\sum_{j=i_{\mix}+1}^{i-1} \eta_j \bm{A}_j \bm{\Delta}_{j-1}\right]}} + I_{22} + I_{23},
\end{align}
% \color{violet}
% *ALE* - minor correction:
% \[
% I_2 = - \underset{I_{20}}{\underbrace{\frac{1}{\sqrt{T}}\sum_{i=1}^T \left[\bm{AQ}_i (\bm{A}_i - \bm{A})\sum_{j=i_{\mix}+1}^{i-1} \eta_j (\bm{A}_j \bm{\theta}^\star - \bm{b}_j)\right]}} + \underset{I_{21}'}{\underbrace{\frac{1}{\sqrt{T}}\sum_{i=1}^T \left[\bm{AQ}_i (\bm{A}_i - \bm{A})\sum_{j=i_{\mix}+1}^{i-1} \eta_j \bm{A}_j \bm{\Delta}_{j-1}\right]}} + I_{22} + I_{23},
% \]
% \color{black}
where the norm of $I_{20}$ is bounded almost surely by
\begin{align}\label{eq:markov-Berry-Esseen-I20}
\|I_{20}\|_2&\leq \frac{1}{\sqrt{T}}\sum_{i=1}^T \|\bm{AQ}_i\| \|\bm{A}_i - \bm{A}\| \cdot \sum_{j=i_{\mix}+1}^{i-1} \eta_j (2\|\bm{\theta}^\star\|_2 + 1)\nonumber \\ 
&\lesssim \frac{1}{\sqrt{T}}\sum_{i=1}^T (2+\widetilde{C}i^{\alpha-1})\cdot \tmix \eta_i (2\|\bm{\theta}^\star\|_2 + 1)\nonumber \\ 
&\lesssim (2\|\bm{\theta}^\star\|_2 + 1) \left[\frac{\eta_0 }{1-\rho}T^{\frac{1}{2}-\alpha}\log T + \widetilde{C}T^{-\frac{1}{2}}\log^2 T\right]\nonumber \\
&= \frac{\eta_0 }{1-\rho}(2\|\bm{\theta}^\star\|_2 + 1) T^{\frac{1}{2}-\alpha}\log T + o(T^{\frac{1}{2}-\alpha}).
\end{align}
%\yuting{quote necessary inequalities.} 
Here, the second line follows from Lemma \ref{lemma:Q-bound}, and the third line uses the bound \eqref{eq:tmix.bound} on $\tmix$.

% \ale{Mention that the second line follows from Lemma \ref{lemma:Q-bound}, the third lines uses the bound \eqref{eq:tmix.bound} on $\tmix$ and $\tilde{C}$ is independent of $T$ and $d$.}
% \weichen{Near the beginning of the proof, we have said that we use $\widetilde{C}$ to denote parameters independent of $T$.}

For the term $I_{21}'$, we invoke the fact that for any vectors $\bm{x}_1,\bm{x}_2,...,\bm{x}_n \in \mathbb{R}^d$, it can be guaranteed that
\begin{align*}
\left\|\sum_{i=1}^n \bm{x}_i\right\|_2^2 \leq n \sum_{i=1}^n \|\bm{x}_i\|_2^2;
\end{align*}
Consequently, the norm of $I_{21}'$ is bounded, in expectation, by
\begin{align}\label{eq:markov-Berry-Esseen-I21}
\mathbb{E}\|I_{21}'\|_2^2 &= \frac{1}{T}  \mathbb{E}\left\|\sum_{i=1}^T \sum_{j=i_{\mix}+1}^{i-1}\bm{AQ}_i (\bm{A}_i - \bm{A}) \eta_j \bm{A}_j \bm{\Delta}_{j-1}\right\|_2^2 \nonumber \\ 
&\leq \frac{1}{T} \cdot (T\tmix) \sum_{i=1}^T \sum_{j=i_{\mix}+1}^{i-1} \mathbb{E}\|\bm{AQ}_i (\bm{A}_i - \bm{A}) \eta_j \bm{A}_j \bm{\Delta}_{j-1}\|_2^2 \nonumber 
\\ 
&\leq \tmix \sum_{i=1}^T \left\|\bm{AQ}_i\right\|^2 \|\bm{A}_i - \bm{A}\|^2 \cdot \left(4  \sum_{j=i_{\mix}+1}^{i-1} \eta_j^2 \mathbb{E}\|\bm{\Delta}_{j-1}\|_2^2\right)\nonumber \\ 
&\lesssim \eta_0^2 \tmix^2 (2\|\bm{\theta}^\star\|_2+1)^2 \sum_{i=1}^T (2+\widetilde{C}i^{\alpha-1})i^{-2\alpha} \left(\frac{\eta_0}{\lambda_0(1-\gamma)}\frac{1}{(1-\rho)^2}i^{-\alpha} \log^2 i + \widetilde{C}'i^{-1}\log^2 i\right)\nonumber \\ 
& \lesssim \widetilde{C}(2\|\bm{\theta}^\star\|_2+1)^2 T^{1-3\alpha} \log^4 T = o(T^{\frac{3}{2}-3\alpha}),
\end{align}
where we invoke Theorem \ref{thm:markov-L2-convergence} in the fourth line.

The term $I_{22}$ can be decomposed into $\tmix$ martingales:%, and we apply the AM-GM inequality to obtain that
\begin{align*}
&\left\|\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i(\bm{A}_i - \mathbb{E}_{i_{\mix}}[\bm{A}_i])\bm{\Delta}_{i_{\mix}}\right\|_2^2   \\
&= \frac{1}{T}\left\|\sum_{r=1}^{\tmix} \sum_{i=0}^{T'-1} \bm{AQ}_{i\tmix + r}(\bm{A}_{i\tmix + r} - \mathbb{E}_{(i-1)\tmix + r}[\bm{A}_{i\tmix + r}])\bm{\Delta}_{(i-1)\tmix + r}\right\|_2^2 \\ 
&\leq \frac{\tmix}{T} \sum_{r=1}^{\tmix} \left\|\sum_{i=0}^{T'-1} \bm{AQ}_{i\tmix + r}(\bm{A}_{i\tmix + r} - \mathbb{E}_{(i-1)\tmix + r}[\bm{A}_{i\tmix + r}])\bm{\Delta}_{(i-1)\tmix + r}\right\|.
\end{align*}
Notice here that for any $r \in [\tmix]$, the sequence
\begin{align*}
\left\{\bm{AQ}_{i\tmix + r}(\bm{A}_{i\tmix + r} - \mathbb{E}_{(i-1)\tmix + r}[\bm{A}_{i\tmix + r}])\bm{\Delta}_{(i-1)\tmix + r}\right\}_{i=0}^{T'-1}
\end{align*}
is a martingale difference. Therefore, its expected norm can be bounded by
\begin{align*}
&\mathbb{E}\left\|\sum_{i=0}^{T'-1}\bm{AQ}_{i\tmix + r}(\bm{A}_{i\tmix + r} - \mathbb{E}_{(i-1)\tmix + r}[\bm{A}_{i\tmix + r}])\bm{\Delta}_{(i-1)\tmix + r}\right\|_2^2 \\ 
&= \sum_{i=0}^{T'-1} \mathbb{E}\left\|\bm{AQ}_{i\tmix + r}(\bm{A}_{i\tmix + r} - \mathbb{E}_{(i-1)\tmix + r}[\bm{A}_{i\tmix + r}])\bm{\Delta}_{(i-1)\tmix + r} \right\|_2^2 \\ 
&\lesssim  \sum_{i=0}^{T'-1} \|\bm{AQ}_{i\tmix + r}\|^2 \mathbb{E}\|\bm{\Delta}_{(i-1)\tmix + r}\|_2^2 
\end{align*}
Therefore, the norm of $I_{22}$ is bounded by
\begin{align}
\label{eq:markov-CLT-I22-bound}
&\mathbb{E}\left\|\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i(\bm{A}_i - \mathbb{E}_{i_{\mix}}[\bm{A}_i])\bm{\Delta}_{i_{\mix}}\right\|_2^2 \nonumber \\
&\leq \frac{\tmix}{T}  \sum_{i=1}^T \|\bm{AQ}_i\|^2\mathbb{E}\|\bm{\Delta}_{i_{\mix}}\|_2^2 \nonumber \\ 
&\leq \frac{\log T}{(1-\rho) T} \sum_{i=1}^T (2+O(i^{\alpha-1}))^2  \left[\frac{\eta_0}{\lambda_0(1-\gamma)} \frac{1}{(1-\rho)^2} (2\|\bm{\theta}^\star\|_2+1)^2 i_{\mix}^{-\alpha} \log^2 i + O(i_{\mix}^{-1} \log^2 i)\right] \nonumber \\ 
&\leq \frac{\eta_0}{\lambda_0(1-\gamma)} \frac{1}{(1-\rho)^3}  (2\|\bm{\theta}^\star\|_2+1)^2 T^{-\alpha} \log^3 T + O(T^{-1}\log^3 T)\nonumber \\ 
&= \frac{\eta_0}{\lambda_0(1-\gamma)} \frac{1}{(1-\rho)^3}  (2\|\bm{\theta}^\star\|_2+1)^2 T^{-\alpha} \log^3 T + o(T^{-\alpha}).
\end{align}

For $I_{23}$, we make use of the fact that since 
\begin{align*}
\max_{s \in \mathcal{S}} d_{\mathsf{TV}}(P^{\tmix}(\cdot \mid s), \mu) \leq T^{-1/2},
\end{align*}
the difference between $\mathbb{E}_{i_{\mix}}[\bm{A}_i]$ and $\bm{A} = \mathbb{E}_{\mu}[\bm{A}_i]$ is bounded by
\begin{align*}
\left\|\mathbb{E}_{i_{\mix}}[\bm{A}_i] - \mathbb{E}_{\mu}[\bm{A}_i] \right\|\leq \max_{s \in \mathcal{S}} d_{\mathsf{TV}}(P^{\tmix}(\cdot \mid s), \mu) \cdot \sup_{s_{i-1},s_{i}} \|\bm{A}_i\| \leq 2T^{-1/2}.
\end{align*}
Hence, by AM-GM inequality, the expected norm of $I_{23}$ is bounded by
\begin{align}\label{eq:markov-CLT-I23-bound}
&\mathbb{E}\left\|\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i(\mathbb{E}_{i_{\mix}}[\bm{A}_i] - \bm{A}) \bm{\Delta}_{i_{\mix}}\right\|_2^2 \nonumber \\
&\leq \sum_{i=1}^T \mathbb{E}\left\|\bm{AQ}_i(\mathbb{E}_{i_{\mix}}[\bm{A}_i] - \bm{A}) \bm{\Delta}_{i_{\mix}} \right\|_2^2 \nonumber \\ 
&\leq \sum_{i=1}^T \mathbb{E} \left\{ \|\bm{AQ}_i\|^2 \|\mathbb{E}_{i_{\mix}}[\bm{A}_i] - \bm{A}\|^2 \|\bm{\Delta}_{i_{\mix}} \|_2^2 \right\}\nonumber \\ 
&\lesssim \sum_{i=1}^T (2+O(i^{\alpha-1})) (T^{-1}) \left[\frac{\eta_0}{\lambda_0(1-\gamma)} \frac{1}{(1-\rho)^2} (2\|\bm{\theta}^\star\|_2+1)^2 i_{\mix}^{-\alpha} \log^2 i + O(i_{\mix}^{-1} \log^2 i)\right]\nonumber \\ 
&\lesssim \frac{\eta_0}{\lambda_0(1-\gamma)} \frac{1}{(1-\rho)^2} (2\|\bm{\theta}^\star\|_2+1)^2 T^{-\alpha}\log^2 T + O(T^{-1}\log^2 T) \nonumber \\ 
&=  \frac{\eta_0}{\lambda_0(1-\gamma)} \frac{1}{(1-\rho)^2} (2\|\bm{\theta}^\star\|_2+1)^2 T^{-\alpha}\log^2 T +o(T^{-\alpha}). 
\end{align}
% Combining \eqref{eq:markov-CLT-I21-bound}, \eqref{eq:markov-CLT-I22-bound} and \eqref{eq:markov-CLT-I23-bound}, we obtain
% \begin{align}\label{eq:markov-CLT-I2-converge}
% \mathbb{E}\left\|\frac{1}{\sqrt{T}} \sum_{i=1}^T \bm{Q}_i (\bm{A}_i  - \bm{A})\bm{\Delta}_i\right\|_2^2 \leq \widetilde{C} T^{1-2\alpha} \log^2(T)
% \end{align}
% for a constant $\widetilde{C}$ independent of $T$.
Combining \eqref{eq:markov-Berry-Esseen-I2-decompose}, \eqref{eq:markov-Berry-Esseen-I20}, \eqref{eq:markov-Berry-Esseen-I21}, \eqref{eq:markov-CLT-I22-bound} and \eqref{eq:markov-CLT-I23-bound}, we obtain
\begin{align*}
\mathbb{E}\left\|I_2 - \widetilde{C}_1'T^{\frac{1-2\alpha}{2}} \log T + o(T^{\frac{1}{2}-\alpha})\right\|_2^2 
&= \mathbb{E}\left\|I_2 - I_{20}\right\|_2^2 \\ 
&\leq 3\left(\mathbb{E}\|I_{21}'\|_2^2 + \mathbb{E}\|I_{22}\|_2^2 + \mathbb{E}\|I_{23}\|_2^2\right)\\ 
&\lesssim (\widetilde{C}_2')^3 T^{-\alpha}\log^3 T + o(T^{\frac{3}{2}-3\alpha} + T^{-\alpha}),
\end{align*}
where we use $\widetilde{C}_1'$ and $\widetilde{C}_2'$ to denote problem-related quantities
\begin{align}
&\widetilde{C}_1' = \frac{\eta_0}{1-\rho}(2\|\bm{\theta}^\star\|_2+1), \quad \text{and} \label{eq:Berry-Esseen-C10}\\ 
&\widetilde{C}_2' = \frac{1}{1-\rho} \left(\frac{\eta_0(2\|\bm{\theta}^\star\|_2+1)^2}{\lambda_0(1-\gamma)}\right)^{\frac{1}{3}}. \label{eq:Berry-Esseen-C20}
\end{align}
Therefore, the Chebyshev's inequality  directly implies that
\begin{align*}
&\mathbb{P}\left(\left\|I_2 - \widetilde{C}_1'T^{\frac{1}{2}-\alpha}\log T -o(T^{\frac{1}{2}-\alpha}) \right\|_2 \gtrsim \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}})\right)\\
 &\lesssim \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}}).
\end{align*}
Applying the triangle inequality, we obtain the bound on $I_2$ with high probability by triangle inequality:
\begin{align}\label{eq:markov-Berry-Esseen-I2}
\mathbb{P}\left(\left\|I_2 \right\|_2 \gtrsim \widetilde{C}_1'T^{\frac{1}{2}-\alpha}\log T + \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}})\right)
 \lesssim \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}}).
\end{align}

\paragraph{A Berry-Esseen bound for $I_3$.}

% \textcolor{red}{An idea of deriving a tighter Berry-Esseen bound on $I_3$: Using technique developed by \cite{srikant2024rates}, firstly prove that for every $\beta \in (0,1)$,
% \begin{align*}
% &d_{\mathsf{W}}(\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i \bm{m}_i, \mathcal{N}(\bm{0},\bm{A}\check{\bm{\Lambda}}_T\bm{A}^\top)) \\ 
% &\lesssim \frac{1}{\sqrt{T}}\tilde{C}_1(d,\beta) \sum_{k=1}^T \mathbb{E}\|\bm{P}_k^{-1} \bm{Q}_k \bm{m}_k \|_2^{2+\beta} \|\bm{A}\bm{P}_k\|.
% \end{align*}
% Here, the matrix $\bm{P}_k$ is defined as
% \begin{align*}
% \bm{P}_k = \left(\sum_{j=k}^T \bm{Q}_j \mathbb{E}[\bm{m}_j\bm{m}_j^\top \mid \mathcal{F}_{k-1}]\bm{Q}_j^\top \right)^{\frac{1}{2}}.
% \end{align*}
% I can guarantee that this step is correct. And secondly, prove that
% \begin{align*}
% \|\bm{A}\bm{P}_k\| \lesssim (T-k+1)^{\frac{1}{2}}.
% \end{align*}
% THis is also guaranteed to be true. And the last remaining part is to prove that
% \begin{align*}
% \|\bm{P}_k^{-1} \bm{Q}_k\| \lesssim (T-k+1)^{-\frac{1}{2}}.
% \end{align*}
% }
Following the decomposition of the term $I_2$ in the proof of Theorem \ref{thm:TD-whp}, we represent $I_3$ as
\begin{align*}
&\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i(\bm{A}_i \bm{\theta}^\star - \bm{b}_i )\\
 &= \underset{I_{31}}{\underbrace{\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i \bm{m}_i}} + \underset{I_{32}}{\underbrace{\frac{\bm{A}}{\sqrt{T}}(\bm{Q}_0 \mathbb{E}_0[\bm{U}_1] - \bm{Q}_T \mathbb{E}_T [\bm{U}_{T+1}])}}+ \underset{I_{33}}{\underbrace{\frac{\bm{A}}{\sqrt{T}}\sum_{i=1}^T (\bm{Q}_i - \bm{Q}_{i-1})\mathbb{E}_{i-1}[\bm{U}_i]}}
\end{align*}
where $\bm{U}_i$ is defined as in \eqref{eq:defn-Ui} and $\bm{m}_i$ is defined as in \eqref{eq:defn-mi}. Here, the norm of $I_{32}$ and $I_{33}$ can be bounded by $O(T^{-\frac{1}{2}})$ almost surely; it now boils down to the term $I_{31}$, for which we aim to apply Corollary \ref{thm:Berry-Esseen-mtg}. Specifically, let 
\begin{align*}
\bm{f}_i(s_i,s_{i-1}) = \bm{AQ}_i \bm{m}_i,
\end{align*}
it is easy to verify that for all $i \in [T]$,
\begin{align*}
\|\bm{f}_i(s_i,s_{i-1})\|_2 &\leq \|\bm{A}\bm{Q}_i \bm{m}_i\|_2  \\ 
&\leq \|\bm{AQ}_i\|\|\bm{m}_i\|_2\\ 
&\leq (2+O(i^{\alpha-1})) \cdot \frac{m}{1-\rho}(2\|\bm{\theta}^\star\|_2+1)\\ 
&\lesssim \eta_0 \left(\frac{1}{\beta}\right)^{\frac{1}{1-\alpha}}\frac{m}{1-\rho}(2\|\bm{\theta}^\star\|_2+1), \quad \text{a.s.}
\end{align*}
Meanwhile,
\begin{align*}
&\frac{1}{T}\sum_{i=1}^T \mathbb{E}[\bm{f}_i\bm{f}_i^\top] = \bm{A}\tilde{\bm{\Lambda}}_T \bm{A}^\top, \quad \text{with} \\ 
&\|\bm{A}\tilde{\bm{\Lambda}}_T \bm{A}^\top - \tilde{\bm{\Gamma}}\| \leq O(T^{\alpha-1}).
\end{align*} 
%\yuting{similar to Ale's comment before. Specify matrix residual is in what norm.} \weichen{checked.}
and when $T$ satisfies \eqref{eq:Lambda-T-condition}, it can be guaranteed that $\lambda_{\min}(\bm{A}\tilde{\bm{\Lambda}}_T \bm{A}^\top) \geq \frac{1}{2}\lambda_{\min}(\tilde{\bm{\Gamma}})$. Hence, a direct application of Corollary \ref{thm:Berry-Esseen-mtg} reveals that
\begin{align}\label{eq:markov-Berry-Esseen-I31}
d_{\mathsf{C}}\left(\frac{1}{\sqrt{T}}\sum_{i=1}^T \bm{AQ}_i \bm{m}_i,\mathcal{N}(\bm{0},\bm{A}\tilde{\bm{\Lambda}}_T \bm{A}^\top)\right) \leq \widetilde{C}_3 T^{-\frac{1}{4}}\log T + o(T^{-\frac{1}{4}}),
\end{align}
where $\widetilde{C}_3$ is a problem-related quantity
\begin{align}
\widetilde{C}_3 &= \Bigg\{ \left(\frac{p}{(p-1)(1-\lambda)}\log\left(d\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)\right)^{\frac{1}{4}}\cdot \frac{m}{1-\rho}(2\|\bm{\theta}^\star\|_2+1) \nonumber \\ 
&+ \sqrt{\frac{m}{1-\rho}(2\|\bm{\theta}^\star\|_2+1)} \cdot \eta_0 \left(\frac{1}{(1-\gamma)\lambda_0\eta_0}\right)^{\frac{1}{2(1-\alpha)}} \log^{\frac{1}{4}}(d\|\tilde{\bm{\Gamma}}\|) \Bigg\} \cdot \sqrt{d}\|\tilde{\bm{\Gamma}}\|_{\mathsf{F}}^{\frac{1}{2}}.\label{eq:Berry-Esseen-C3}
\end{align}

\paragraph{Completing the proof.} 
The proof now boils down to combining the convergence rate of $I_1,I_2$ and the Berry-Esseen bound on $I_3$. For simplicity, we denote
\begin{align*}
\bm{\delta}_T := \sqrt{T} \bm{A}\bar{\bm{\Delta}}_T - \frac{1}{\sqrt{T}}\sum_{i=1}^T\bm{AQ}_i{\bm{m}}_i = I_1 - I_2 - I_{32} - I_{33}.
\end{align*}
From the previous calculations, we have shown that
\begin{align*}
\mathbb{P}\left(\left\|\bm{\delta_T} \right\|_2 \gtrsim \widetilde{C}_1'T^{\frac{1}{2}-\alpha}\log T + \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}})\right)
 \lesssim \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}}),
\end{align*}
and that
\begin{align*}
\sup_{\mathcal{A} \in \mathscr{C}}\left|\mathbb{P}\left(\frac{1}{\sqrt{T}}\sum_{i=1}^T{\bm{AQ}_i\bm{m}}_i\in \mathcal{A}\right) - \mathbb{P}(\bm{A}\widetilde{\bm{\Lambda}}_T^{ \frac{1}{2}}\bm{z} \in \mathcal{A})\right| \leq \widetilde{C}_3 T^{-\frac{1}{4}}{\log T} + o(T^{-\frac{1}{4}}).
\end{align*}
We now combine these two results to bound the difference between the distributions of $\sqrt{T} \bm{A}\bar{\bm{\Delta}}_T$ and $\mathcal{N}(\bm{0},\bm{A}\widetilde{\bm{\Lambda}}_T\bm{A}^\top)$. Considering any convex set $\mathcal{A} \subset \mathbb{R}^d$, define
\begin{align*}
&\mathcal{A}^{\varepsilon} := \{\bm{x} \in \mathbb{R}^d: \inf_{y \in \mathcal{A}} \|\bm{x} - \bm{y}\|_2 \leq \varepsilon\}, \quad \text{and}  \qquad \mathcal{A}^{-\varepsilon}:=\{\bm{x} \in \mathbb{R}^d: B(\bm{x},\varepsilon) \subset \mathcal{A}\}. 
\end{align*}
% For any $\bm{r} \in \mathbb{R}^d$ and any $\varepsilon>0$, define
% \begin{align*}
% &\mathcal{R}_{\bm{r}}:= \prod_{i=1}^d (-\infty, r_i),\\
% &\mathcal{R}_{\bm{r}+\varepsilon}:= \prod_{i=1}^d (-\infty, r_i+\varepsilon) \quad \text{and} \\ 
% &\mathcal{R}_{\bm{r}-\varepsilon}:= \prod_{i=1}^d (-\infty, r_i-\varepsilon).
% \end{align*}
Direct calculation yields
\begin{align*}
\mathbb{P}(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T \in \mathcal{A}) &= \mathbb{P}(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T \in \mathcal{A},\|\bm{\delta}_T\|_2 > \varepsilon) + \mathbb{P}(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T \in \mathcal{A},\|\bm{\delta}_T\|_2 \leq \varepsilon) \\ 
&\leq \mathbb{P}(\|\bm{\delta}_T\|_2 > \varepsilon) + \mathbb{P}(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T \in \mathcal{A},\|\bm{\delta}_T\|_2 \leq \varepsilon).
\end{align*}
Here, the triangle inequality implies
\begin{align*}
\left(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T \in \mathcal{A},\|\bm{\delta}_T\|_2 \leq \varepsilon \right) \Rightarrow \frac{1}{\sqrt{T}}\sum_{i=1}^T\bm{AQ}_i{\bm{m}}_i \in \mathcal{A}^{\varepsilon}.
\end{align*}
Hence, $\mathbb{P}(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T \in \mathcal{A})$ is upper bounded by
\begin{align*}
\mathbb{P}(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T \in \mathcal{A}) 
&\leq \mathbb{P}(\|\bm{\delta}_T\|_2 > \varepsilon) + \mathbb{P}\left(\frac{1}{\sqrt{T}}\sum_{i=1}^T{\bm{AQ}_i\bm{m}}_i \in \mathcal{A}^{\varepsilon}\right)\\
&\leq \mathbb{P}(\|\bm{\delta}_T\|_2 > \varepsilon) + \mathbb{P}\left(\bm{A}\widetilde{\bm{\Lambda}}_T^{\frac{1}{2}}\bm{z} \in \mathcal{A}^{\varepsilon}\right)+\widetilde{C}_3 T^{-\frac{1}{4}}{\log T}  + o(T^{-\frac{1}{4}})\\ 
&\leq \mathbb{P}(\|\bm{\delta}_T\|_2 > \varepsilon) + \mathbb{P}\left(\bm{A}\widetilde{\bm{\Lambda}}_T^{\frac{1}{2}}\bm{z} \in \mathcal{A}\right) + \|\tilde{\bm{\Gamma}}\|_{\mathsf{F}}^{\frac{1}{2}}\varepsilon +\widetilde{C}_3 T^{-\frac{1}{4}}{\log T}  + o(T^{-\frac{1}{4}}),
\end{align*}
where we invoked Theorem \ref{thm:Gaussian-reminder} in the last inequality. By letting 
\begin{align*}
\varepsilon \asymp \widetilde{C}_1'T^{\frac{1}{2}-\alpha}\log T + \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}}),
\end{align*}
we obtain
\begin{align*}
\mathbb{P}(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T \in \mathcal{A}) 
&\leq \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}})  + \mathbb{P}\left(\bm{A}\widetilde{\bm{\Lambda}}_T^{\frac{1}{2}}\bm{z} \in \mathcal{A}\right) \\ 
&+ \|\tilde{\bm{\Gamma}}\|_{\mathsf{F}}^{\frac{1}{2}} \left(\widetilde{C}_1'T^{\frac{1}{2}-\alpha}\log T + \widetilde{C}_2'T^{-\frac{\alpha}{3}}\log T + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}})\right) +\widetilde{C}_3 T^{-\frac{1}{4}}{\log T}  + o(T^{-\frac{1}{4}}) \\ 
&= \mathbb{P}\left(\bm{A}\widetilde{\bm{\Lambda}}_T^{\frac{1}{2}}\bm{z} \in \mathcal{A}\right) + (\widetilde{C}_1 T^{\frac{1}{2}-\alpha} + \widetilde{C}_2 T^{-\frac{\alpha}{3}}+ \widetilde{C}_3 T^{-\frac{1}{4}}){\log T}  + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}} + T^{-\frac{1}{4}}).
\end{align*}
Here, in the last equality, we denote, for simplicity,
\begin{align}
&\widetilde{C}_1 = \|\tilde{\bm{\Gamma}}\|_{\mathsf{F}}^{\frac{1}{2}}\widetilde{C}_1' = \|\tilde{\bm{\Gamma}}\|_{\mathsf{F}}^{\frac{1}{2}}\frac{\eta_0}{1-\rho}(2\|\bm{\theta}^\star\|_2+1), \quad \text{and} \label{eq:Berry-Esseen-C1} \\ 
&\widetilde{C}_2 = (\|\tilde{\bm{\Gamma}}\|_{\mathsf{F}}^{\frac{1}{2}} + 1) \widetilde{C}_2' = (\|\tilde{\bm{\Gamma}}\|_{\mathsf{F}}^{\frac{1}{2}} + 1)\frac{1}{1-\rho} \left(\frac{\eta_0(2\|\bm{\theta}^\star\|_2+1)^2}{\lambda_0(1-\gamma)}\right)^{\frac{1}{3}} \label{eq:Berry-Esseen-C2}.
\end{align}
Using the same technique as in the proof of Corollary \ref{thm:Berry-Esseen-mtg}, a lower bound can be derived symmetrically. By taking a supremum over $\mathcal{A} \in \mathscr{C}$, it can be guaranteed that
\begin{align}\label{eq:TD-Berry-Esseen-intermediate}
d_\mathsf{C}(\sqrt{T}\bm{A}\bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\bm{A}\tilde{\bm{\Lambda}}_T \bm{A}^\top)) &\lesssim (\widetilde{C}_1 T^{\frac{1}{2}-\alpha} + \widetilde{C}_2 T^{-\frac{\alpha}{3}}+ \widetilde{C}_3 T^{-\frac{1}{4}}){\log T} + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}} + T^{-\frac{1}{4}}).
\end{align}
%\yuting{why do you have $\lesssim$ and those $\widetilde{C}_i$?}\weichen{$\tilde{C}_i$ does not include universal constants.}
Further combining \eqref{eq:TD-Berry-Esseen-intermediate} with \eqref{eq:TD-Berry-Esseen-decompose} and Lemma \ref{lemma:Gaussian-comparison}, we obtain the Berry-Esseen bound
\begin{align*}
d_{\mathsf{C}}(\sqrt{T}\bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}^\star)) &\lesssim (\widetilde{C}_1 T^{\frac{1}{2}-\alpha} + \widetilde{C}_2 T^{-\frac{\alpha}{3}}+ \widetilde{C}_3 T^{-\frac{1}{4}} + \widetilde{C}_4 T^{\alpha-1}){\log T} \\ 
& + o(T^{\frac{1}{2}-\alpha} + T^{-\frac{\alpha}{3}} + T^{-\frac{1}{4}} + T^{\alpha-1})
\end{align*}
with $\widetilde{C}_1$, $\widetilde{C}_2$, $\widetilde{C}_3$ defined as in \eqref{eq:Berry-Esseen-C1}, \eqref{eq:Berry-Esseen-C2}, \eqref{eq:Berry-Esseen-C3} respectively, and
\begin{align*}
\widetilde{C}_4 = \frac{\sqrt{d\mathsf{cond}(\bm{\tilde{\Gamma}})}}{(1-\gamma)\lambda_0\eta_0}.
\end{align*}
Finally, when $\alpha = \frac{3}{4}$, we have, coincidentally,
\begin{align*}
\frac{1}{2}-\alpha = -\frac{\alpha}{3} = -\frac{1}{4} = \alpha-1.
\end{align*} 
Hence, Theorem \ref{thm:TD-berry-esseen} follows from taking $\tilde{C} = \max\{\tilde{C}_1,\tilde{C}_2,\tilde{C}_3,\tilde{C}_4\}$.

\subsection{Proof of Relation~\eqref{eq:TD-Berry-Esseen-tight}}\label{app:proof-Berry-Esseen-tight} 
Following the same logic as Appendix B.4.1 in \cite{wu2024statistical}, we can obtain a lower bound on the difference between $\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}_T)$ and $\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}^\star)$. Specifically, when $T$ is sufficiently large,
\begin{align*}
d_{\mathsf{C}}(\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}_T),\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}^\star)) = d_{\mathsf{TV}}(\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}_T),\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}^\star)) \gtrsim O(T^{\alpha-1}).
\end{align*}
Meanwhile, when $\alpha > \frac{3}{4}$, both $\frac{1}{2}-\alpha$ and $-\frac{\alpha}{3}$ are less than $-\frac{1}{4}$. Therefore, the upper bound \eqref{eq:TD-Berry-Esseen-intermediate} is transformed as
\begin{align*}
d_\mathsf{C}(\sqrt{T}\bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}_T )) \leq O(T^{-\frac{1}{4}}).
\end{align*}
In combination, the triangle inequality reveals
\begin{align*}
d_{\mathsf{C}}(\sqrt{T}\bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}^\star)) &\geq d_{\mathsf{C}}(\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}_T),\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}^\star)) - d_\mathsf{C}(\sqrt{T}\bar{\bm{\Delta}}_T,\mathcal{N}(\bm{0},\tilde{\bm{\Lambda}}_T ))\\ 
&\gtrsim O(T^{\alpha-1}) - O(T^{-\frac{1}{4}})\\ 
&\gtrsim O(T^{\alpha-1}) \gtrsim O(T^{-\frac{1}{4}}).
\end{align*}
Here, in the last line, we applied the fact that since $\alpha > \frac{3}{4}$, $\alpha - 1 \geq -\frac{1}{4}$.

\paragraph{Choose of stepsizes.} We conclude by noting that choice of the stepsize in Theorem \ref{thm:TD-whp} and Theorem \ref{thm:TD-berry-esseen} are different: in Theorem \ref{thm:TD-whp}, $\eta_0$ depends on $\delta$ and other problem-related quantities like $\lambda_0$ and $\gamma$, and $\alpha$ can take any value between $\frac{1}{2}$ and $1$; however, in Theorem \ref{thm:TD-berry-esseen}, the initial stepsize $\eta_0$ can take any value less than $1/2\lambda_{\Sigma}$, while $\alpha$ is set to the specific value of $\frac{3}{4}$. In fact, our proof of Theorem~\ref{thm:TD-berry-esseen} allows for a general choice of $\alpha$; however, using other values of $\alpha$ other than $3/4$ appears to be suboptimal.
