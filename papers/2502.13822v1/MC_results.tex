\section{High-dimensional concentration and Berry-Esseen bounds on Markov chains}%Theoretical results regarding general Markov chains}
%\yuting{define $\mu$-norm}\weichen{did in Section \ref{sec:notation}.}
In this section, we present our main results for uncertainty quantification of Markov chains. Section \ref{sec:MC-concentration} focuses on generalized and improved concentration inequalities for matrix-valued functions, while Section \ref{sec:MC-Berry-Esseen} focuses on multi-variate Berry-Esseen bounds on martingales. Section \ref{sec:MC-mtg} applies these results to the case where the martingale is generated from a Markov chain. 

Throughout the paper, we consider a Markov chain $\{s_t\}_{t \geq 0}$ with state space $\mathcal{S}$, transition kernel $P$ and a \emph{unique stationary distribution}, denoted as $\mu$. It can be guaranteed that any positive recurrent, irreducible and aperiodic Markov chain admits this property.
% \yuting{need to be positive recurrent for Markov chain with possibly infinite space to have unique stationary distribution}\weichen{checked.}
%It can be guaranteed that such a Markov chain admits a unique stationary distribution, denoted as $\mu$. 
Furthermore, let $\lambda$ denote the \emph{spectral expansion} of the Markov chain, and assume throughout the \emph{spectral gap condition} $1-\lambda > 0$. We refer the reader to Appendix \ref{app:MC-basics} for background on Markov chains. Notably, the theoretical results in this paper does \emph{not} require the Markov chain to be reversible. %As a final note, we will only consider Markov processes and martingales in discrete time.




% \paragraph{Markov chains}
% Throughout the paper, we consider Markov chains with transition kernel $\mathcal{P}$ that satisfies the following two properties:
% \begin{customassumption}
% \label{as:stationary}
% \yuting{shall we assume the Markov chain is irreducible and aperiodic? then it is always true that it has a unique stationary distribution}
% There exists a unique stationary distribution $\mu$, such that
% \begin{align*}
% \mu(B) = \int_{\mathcal{S}} P(x,B)\mu(\mathrm{d}x).
% \end{align*}
% \end{customassumption}
% \begin{customassumption}\label{as:spectral}
% The Markov chain admits a strictly positive spectral gap, namely, $1-\lambda > 0$.
% % \yuting{we use $\lambda$ for eigenvalues of $\Sigma$. Consider using $\rho$ for eigen-gaps.}
% \end{customassumption}



\subsection{Matrix Hoeffding's inequality on Markov chains}
\label{sec:MC-concentration}

In this section, we present a new matrix Hoeffding's inequality for sums of matrix-valued functions on Markov chains. 
Matrix Hoeffding's inequality, offering finite sample bounds for the spectral norm of sums of bounded random matrices, is a powerful and widely used result in statistics, machine learning and computer science.  
Initially developed for the sum of independent random matrices ~\citep[see, e.g.,][]{Tropp2011matrixtails,oliveira.matrix.hoef}, it has been recently generalized \citep{garg2018matrixexpanderchernoff,qiu2020matrix} to the case where the matrices are generated from a Markov chain. 
Our first theorem provides an extension and improvement of these results. See Appendix \ref{app:proof-matrix-hoeffding} for the proof.%  which turns out to be useful in developing statistical inference procedures for general Markov chains. 
\medskip
\begin{customtheorem}[Matrix Hoeffding's inequality for Markov Chains]\label{thm:matrix-hoeffding}
Consider a Markov chain $\{s_t\}_{t \geq 1}$ with a unique stationary distribution $\mu$ and a spectral gap $1-\lambda > 0$. Let $\{\bm{F}_i\}_{i \in [n]}$ be a sequence of matrix-valued functions from the state space $\mathcal{S}$ to $\mathbb{S}^{d \times d}$ satisfying $\mathbb{E}_{s \sim \mu}[\bm{F}_i(s)] = \bm{0}$ and $\|\bm{F}_i(s)\| \leq M_i$ almost surely for every $i \in [n]$. 
% \yuting{use $\|\cdot\|_2$ or $\|\cdot\|$ for spectral norm?}\weichen{checked.}
Then for any $\varepsilon > 0$, it can be guaranteed that when $s_1 \sim \mu$,
\begin{align}\label{eq:markov-matrix-hoeffding}
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right) &\leq 2d^{2-\frac{\pi}{4}} \exp\left\{-\frac{1-\lambda}{20}\left(\frac{\pi}{4}\right)^2\frac{n^2\varepsilon^2 }{\sum_{j=1}^n M_j^2} \right\}.
\end{align}
\end{customtheorem}
\medskip
% \begin{proof}
% See 
% \end{proof}

% \yuting{provide links to the proof of each result} \weichen{checked.}

It is noteworthy that the right-hand-side of the above expression depends on the dimension $d$ through $d^{2-\frac{\pi}{4}}$, a worse dependence than  for independent matrices \citep[see, e.g.,][Theorem 5.2]{Tropp2011matrixtails}. The exponent $2-\frac{\pi}{4}$ stems from an application of a multi-matrix Golden-Thompson inequality due to \cite{garg2018matrixexpanderchernoff}.
We refer readers to inequality \eqref{eq:multi-matrix-golden-thompson} in Appendix \ref{app:proof-matrix-hoeffding}. Indeed, the proof of this theorem follows the framework developed by \cite{garg2018matrixexpanderchernoff} and \cite{qiu2020matrix} but makes improvements on multiple fronts. Firstly, our proof is presented in the language of measure theory, thus allowing for infinite state spaces; secondly, we implement novel recursive algebraic analysis to obtain a tighter Hoeffding's inequality for \emph{arbitrary tolerance level} $\varepsilon$, instead of $\varepsilon \in (0,1)$,  and \emph{time-dependent} functions $\{\bm{F}_i\}_{i \in [n]}$, instead of a \emph{time-invariant} function $\bm{F}$ throughout the steps $i = 1,2,...,n$. 
%Table \ref{table:Hoeffding-compare} compares Theorem \ref{thm:matrix-hoeffding} with previous results from various aspects. \yuting{no table now.}
%Throughout the proof, we also made a non-trivial correction to the previous attempt by \cite{qiu2020matrix} to handle non-stationary starting distribution $\nu$. See Appendix \ref{app:proof-matrix-hoeffding} for details.

It is also important to point out that Theorem \ref{thm:matrix-hoeffding} implies that, for any $\delta \in (0,1)$, 
\begin{align}
\label{eq:thm-matrix-hoeffding-whp}
\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\|  \lesssim \sqrt{\frac{1}{1-\lambda} \frac{\sum_{j=1}^n M_j^2}{n} \log \Big(\frac{d}{\delta}\Big)} \cdot \frac{1}{\sqrt{n}},
\end{align} 
with probability at least $1-\delta$, where, here and throughout the paper, $\lesssim $ indicates weak inequality up to universal constants.
% Essentially, the operator norm of the sample mean is upper bounded with high probability by the following:
% \begin{enumerate}
% 	\itemsep0em
% \item The mixing property of the Markov chain, represented by $\frac{1}{\sqrt{1-\lambda}}$;
% \item The Root-Mean-Square of the upper bounds of the samples, $\sqrt{\frac{\sum_{j=1}^n M_j^2}{n}}$;
% \item The dimension of the matrix $d$, the tolerance level $\delta$, through the logarithm term $\sqrt{\log \frac{d}{\delta}}$;
% \item The sample size $n$, through the convergence rate $\frac{1}{\sqrt{n}}$.
% %\item The non-stationarity of the Markov chain, measured by $\sqrt{\log} \left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu}$.
% \end{enumerate}
% Specifically, it cannot be guaranteed for a series of matrices $\bm{X}_1,...,\bm{X}_n \in \mathbb{S}^{d \times d}$ that
% \begin{align*}
% \mathsf{Tr}\left(\exp\left(\sum_{i=1}^n \bm{X}_i \right)\right) \leq \mathsf{Tr}\left(\prod_{i=1}^n \exp(\bm{X}_i)\right).
% \end{align*}
% \yuting{comments on the $\pi/4$ exponent on $d$?}
% \weichen{That exponent comes from the multi-matrix Golden-Thompson inequality \eqref{eq:multi-matrix-golden-thompson}}
% \paragraph{Technical novelty.}
In the scalar case, i.e. when $d=1$, our result agrees, albeit with a worse dependence on the constant and on $\lambda$, with Theorem 1 in \cite{Fan2021Hoeffding}, which provides a sharp Hoeffding's inequality for averages of scalar functions on Markov chains. This is an indication that our bound is not loose. 




% \begin{table*}[t]
% \centering
% \renewcommand{\arraystretch}{2.3}
% \begin{tabular}{c|c|c|c|c} 
% \toprule
% paper & \makecell{multi- \\ dimension} & \makecell{arbitrary \\ tolerance level}  & \makecell{infinite \\ state space} & \makecell{time-dependent \\ functions}\\ 
% \toprule
% 	\cite{Fan2021Hoeffding}, Theorem 1 & No & Yes  & Yes & Yes \\ \hline

% 	\cite{garg2018matrixexpanderchernoff}, Theorem 3 & Yes & No  & No & No \\ \hline

% 	This work, Theorem \ref{thm:matrix-hoeffding} & Yes & Yes  & Yes & Yes \\ 
% \toprule
% \end{tabular}
% \caption{Comparisons between Theorem \ref{thm:matrix-hoeffding} and previous results. \yuting{should we include other comparisons too?}}
% \label{table:Hoeffding-compare}
% \end{table*}

In order to generalize Theorem \ref{thm:matrix-hoeffding} to non-stationary Markov chains, we impose the following assumption on the distribution of the initial state, denoted as $\nu$.
\medskip
\begin{customassumption}\label{as:nu}
The probability measure $\nu$ is absolutely continuous with respect to the stationary distribution $\mu$; furthermore, assume that there exists $p \in (1,\infty]$, such that the Radon-Nykodin derivative of $\nu$ with respect to $\mu$ satisfies 
% \begin{align*}
	$\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p} < \infty.$
    % \text{and}
% \end{align*}
%with the norm defined in Eq.~\eqref{eq:mu-p-norm}.
\end{customassumption}
\medskip
% \yuting{why do you need $q$ in this assumption?} \weichen{This is just for notational simplicity.}
We also let $q \in [1,\infty)$ denote the conjugate of $p$, i.e. $\frac{1}{p} + \frac{1}{q} = 1$ when $p < \infty$ and  $q = 1$ if $p = \infty$.  For simplicity, we say $\nu$ \emph{satisfies Assumption \ref{as:nu} with parameters $(p,q)$} if these conditions hold true.  The following corollary generalizes Theorem \ref{thm:matrix-hoeffding} to the case where $s_1$, the first state of the Markov chain, is not generated from the stationary distribution $\mu$ but rather from a distribution $\nu$ satisfying Assumption \ref{as:nu}. 

\medskip
\begin{customcorollary}\label{cor:matrix-hoeffding} 
Assume the conditions of Theorem \ref{thm:matrix-hoeffding}  and that $s_1 \sim \nu$, where $\nu$ is a probability measure on the state space $\mathcal{S}$ satisfying Assumption \ref{as:nu} with parameters $(p,q)$. 
Then for any $\varepsilon>0$, 
\begin{align*}
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right) &\leq 2d^{2-\frac{\pi}{4}} \left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}  \exp\left\{-\frac{1-\lambda}{20q}\left(\frac{\pi}{4}\right)^2\frac{n^2\varepsilon^2 }{\sum_{k=1}^n M_k^2} \right\}.
\end{align*}
\end{customcorollary}
\medskip
This corollary follows directly from Theorem \ref{thm:matrix-hoeffding} and Holder's inequality. See Appendix \ref{app:proof-cor-matrix-hoeffding} for details. As an application, for any $\delta \in (0,1)$, we have that, with probability at least $1-\delta$,
\begin{align}\label{eq:cor-matrix-hoeffding-whp}
\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\|  \lesssim \sqrt{\frac{q}{1-\lambda} \frac{\sum_{j=1}^n M_j^2}{n} \log \left(\frac{d}{\delta}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)} \cdot \frac{1}{\sqrt{n}}.
\end{align}
Comparing the above bound with the analogous one for stationary Markovian sequences (see  \ref{eq:thm-matrix-hoeffding-whp}), we observe that for a non-stationary Markov chain, the upper bound is larger by a factor of $\sqrt{q}$ and an additional factor in the logarithm term that reflects the difference between the starting distribution $\nu$ and the stationary distribution $\mu$. Of course, when $\nu = \mu$, then $\frac{\mathrm{d}\nu}{\mathrm{d}\mu} \equiv 1$ and we can take $p = \infty$ and $q = 1$, so that 
% \begin{align*}
$\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p} = 1,$
% \end{align*}
and the bound \eqref{eq:cor-matrix-hoeffding-whp} coincides with \eqref{eq:thm-matrix-hoeffding-whp}.




\subsection{Berry-Esseen bounds on vector-valued martingales}
\label{sec:MC-Berry-Esseen}

In our next result, we derive novel high-dimensional Berry-Esseen bounds for vector-valued martingales in terms of the  Wasserstein distance. See Appendix \ref{app:Srikant-generalize} for the proof.

\medskip
\begin{customtheorem}[Berry-Esseen bound on vector-valued martingales]\label{thm:Srikant-generalize}
Let $\{\bm{x}_k\}_{k=1}^n$ be a martingale difference process in $\mathbb{R}^d$ with respect to the filtration $\{\mathscr{F}_k\}_{k=0}^n$.
For every $k \in [n]$, define 
\begin{align*}
&\bm{V}_k := \mathbb{E}[\bm{x}_k\bm{x}_k^\top \mid \mathscr{F}_{k-1}], \quad  \text{and}  \quad \bm{P}_k := \sum_{j=k}^n \bm{V}_k.
\end{align*}
Furthermore, define
\begin{align*}
\bm{\Sigma}_n := \frac{1}{n}\sum_{k=1}^n \mathbb{E}[\bm{x}_k \bm{x}_k^\top \mid \mathscr{F}_0], 
\end{align*}
and assume that
\begin{align}\label{eq:as-P1}
\bm{P}_1 = n\bm{\Sigma}_n \quad \text{almost surely.}
\end{align}
Then for any  $d$-dimensional symmetric positive semi-definite matrix $\bm{\Sigma}$, it can be guaranteed that
\begin{align}\label{eq:Srikant-Berry-Esseen}
d_{\mathsf{W}}\left(\frac{1}{\sqrt{n}}\sum_{k=1}^n \bm{x}_k,\mathcal{N}(\bm{0},\bm{\Sigma}_n)\right) &\lesssim  \frac{(2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+}{\sqrt{n}} \sum_{k=1}^n \mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma})^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right]\right] \nonumber \\ 
&\qquad+ \frac{1}{\sqrt{n}}\left[\mathsf{Tr}(\log(n\bm{\Sigma}_n+\bm{\Sigma})) - \log(\bm{\Sigma}))\right]+ \sqrt{\frac{\mathsf{Tr}(\bm{\Sigma})}{n}}.
\end{align}
\end{customtheorem}
\medskip

% \begin{proof}
% See Appendix \ref{app:Srikant-generalize}.
% \end{proof}
% \yuting{comment on the assumption $\bm{P}_1 = n\bm{\Sigma}_n$ a.s.}\weichen{This follows the precedent of several papers considering the Berry-Esseen bound on martingales. I believe there are results showing even in uni-dimensional case that the Berry-Esseen bound is controlled by the difference between $\bm{P}_1$ and $n\bm{\Sigma}_n$.}

% \paragraph{Comparison with the uni-dimensional Berry-Esseen bound shown in Theorem 2.1 of \cite{rollin2018}.} 
Let us compare this result with analogous ones in the literature. When $d=1$, Theorem \ref{thm:Srikant-generalize} agrees with Theorem 2.1 of \cite{rollin2018}, aside from logarithmic  factors, by letting
\begin{align*}
	\bm{\Sigma}_n = s_n^2/n, \quad \bm{P}_k = \rho_k^2, \quad \text{and} \quad \bm{\Sigma} = a^2.
\end{align*}
In this sense,  Theorem \ref{thm:Srikant-generalize} may be be regarded as a multivariate generalization of the univariate bound of Theorem 2.1 of \cite{rollin2018}.
% \paragraph{Technical novelty.} 
% This theorem is an exact multi-dimensional generalization of Theorem 2.1 in \cite{rollin2018}, in the sense that when $d=1$, the upper bound \eqref{eq:Srikant-Berry-Esseen} reduces to the upper bound shown in Theorem 2.1 of \cite{rollin2018} up to a logarithm factor. 
Next, Theorem \ref{thm:Srikant-generalize} bears similarities with Theorem 1 of \cite{JMLR2019CLT}, also concerned with Gaussian approximations of multivariate martingale sequences. Our result offers a different but arguably more general guarantee because it establishes convergence in the weaker Wasserstein distance instead of the $d_2$ distance %and, furthermore, levetrages a refined result on the smoothness of the solution to Stein's equation which may be of independent interest (see Proposition \ref{prop:Stein-smooth} for details). 
We believe that these are meaningful improvements, since the Wasserstein distance can be directly related to the convex distance \citep{nourdin2021multivariate}, which, in turn,  is more amenable to statistical inference. %\ale{add a citation}
%\yuting{other differences?} \weichen{Not much. Actually their bound is smaller than ours, if we simply compare the terms regardless of the kind of distance in use. Maybe want to say $d_{\mathsf{W}}$ is more directly translated to $d_{\mathsf{C}}$, which is useful for statistical inference?} \yuting{yes, and add a sentence saying why this generalization is non-trivial?}\weichen{please see the amended text.}

Our strategy to prove Theorem \ref{thm:Srikant-generalize} above is heavily inspired by the recent, very interesting pre-print by \cite{srikant2024rates}, which deploys Stein's method and Lindeberg swapping. 
We refine the approach of \cite{srikant2024rates} in the following ways: firstly, we addressed a gap in their proof if assumption \eqref{eq:as-P1} does not hold; secondly, we obtained a tighter bound on the smoothness of the solution to the multivariate Stein's equation, which may be of independent interest; see Proposition \ref{prop:Stein-smooth} in Appendix \ref{app:Srikant-generalize}  and compare it to Proposition 2.2 and 2.3 in \cite{gallouët2018regularitysolutionssteinequation}. 
The following corollary offers a useful simplification of our upper bound under a slightly stronger condition; see 
Appendix \ref{app:proof-cor-Wu} for the proof.

% \paragraph{Comparison with Theorem 1 of \cite{srikant2024rates}.} While the framework of our proof of Theorem \ref{thm:Srikant-generalize} is mainly inspired by the proof of Theorem 1 of \cite{srikant2024rates}, there are some noteworthy differences. Most importantly, we observe that the assumption $P_1 = n\bm{\Sigma}_n$ almost surely is necessary to obtain a meaningful Berry-Esseen bound, which also follows the precedent of \cite{JMLR2019CLT}. The relaxation of this assumption would be addressed in Theorem \ref{thm:Berry-Esseen-mtg}. Another important improvement we made in Theroem \ref{thm:Srikant-generalize} is to tighten the upper bound through a closer scrutiny of the smoothness of the solution to the Stein's equation, as is indicated in Proposition \ref{prop:Stein-smooth}. This also paves the way for the following Corollary. 
\medskip
\begin{customcorollary}
\label{cor:Wu}
Under the settings of Theorem \ref{thm:Srikant-generalize}, further assume there exists a uniform constant $M > 0$, such that for any matrix $\bm{A} \in \mathbb{R}^{d \times d}$ and any $k \in [n]$, it satisfies 
\begin{align}\label{eq:3rd-momentum-condition}
\mathbb{E}\left[\|\bm{Ax}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right] \leq M \mathbb{E}\left[\|\bm{Ax}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right].
\end{align}
Then,
\begin{align*}
	d_{\mathsf{W}}\left(\frac{1}{\sqrt{n}}\sum_{k=1}^n \bm{x}_k,\mathcal{N}(\bm{0},\bm{\Sigma}_n)\right) \lesssim \left[M(2+\log(dn\|\bm{\Sigma}_n\|))^+ + 1\right]\frac{d \log n}{\sqrt{n}} +\sqrt{\frac{\mathsf{Tr}(\bm{\Sigma}_n)}{n}}.
\end{align*}
\end{customcorollary}
\medskip
% \begin{proof}
% See Appendix \ref{app:proof-cor-Wu}.
% \end{proof}

% \paragraph{Technical novelty.} 
% Applying a delicate telescoping method, \yuting{telescoping method: be more specific?} \weichen{hard to tell it in text here. Maybe add a reference to the proof?} 
Corollary~\ref{cor:Wu} should be compared with Corollary 2.3 in \cite{rollin2018} when $d=1$, and  Corollary 3 in \cite{JMLR2019CLT}, valid in multivariate settings but for the stronger $d_2$ distance. In establishing our bound, we have lifted the requirement on the conditional third momentum of $\bm{x}_k$ and addressed a potential gap in the proof of Corollary 3 of \cite{JMLR2019CLT}.

%compares favortightens and simplifies the upper bound on the Wasserstein distance when compared with similar results in prior work (e.g., Corollary 2.3 in \cite{rollin2018} and Corollary 3 in \cite{JMLR2019CLT}) while lifting the requirement assumed on the conditional third momentum of $\bm{x}_k$. 
%When $d=1$, our result recovers Corollary 2.3 of \cite{rollin2018}.

  
% Corollary \ref{cor:Wu} illustrates that the difference between the sum of the martingale difference process $\{\bm{x}_k\}$ and its Gaussian approximation $\mathcal{N}(\bm{0},\bm{\Sigma}_n)$ converges by the rate of $O(n^{-\frac{1}{2}}\log n)$, when measured by Wasserstein distance. \yuting{I am a little torn on whether we should state the convergence rate like this without mentioning the $d$ dependence} \weichen{alternatively, we can use $O(M\sqrt{\frac{d}{n}})$ to describe the convergence rate up to log factors, and note that $M$ may also depend on $d$. Which one do you think is better?}
% This is the first Berry-Esseen bound on vector-valued martingales to reach that convergence speed. 


% With Corollary \ref{cor:Wu}, we are ready to present our result on characterizing the non-asymptotic convergence of a vector-valued Markovian martingale to its Gaussian limit.
% \yuting{is ``Markovian martingale'' a standard name? }

\subsection{Uncertainty quantification for martingales generated from Markov chains}\label{sec:MC-mtg}
In this section, we leverage the results presented above to investigate a specific structure of sample dependency arising when a matrix- or vector-valued function sequence is \emph{both a martingale difference and generated from a Markov chain}. The study of dependent data of this type is  motivated by our analysis of TD learning, but it can also be of independent interest as it applies to MCMC algorithms. Specifically, we consider functions $\bm{F}:\mathcal{S}^2 \to \mathbb{R}^{m \times n}$ that satisfy the following assumption. %: \yuting{$d\times d$?}\weichen{The Berstein's inequality is on matrices and the Berry-Esseen bound is on vectors. So I wrote $m \times n$ for generality.}
\medskip
\begin{customassumption}\label{as:markov-mtg}
For every $s \in \mathcal{S}$, $\mathbb{E}_{s' \sim P(\cdot \mid s)}\bm{F}(s,s') = \bm{0}$.
\end{customassumption}
\medskip

Notice that when a sequence of functions $\{\bm{F}_i\}_{1 \leq i \leq n}$ with the same dimensions all satisfy Assumption \ref{as:markov-mtg}, it can be guaranteed that $\{\bm{F}_i(s_{i-1},s_i)\}_{1 \leq i \leq n}$ is a martingale, and hence better statistical properties can be derived. 

The next result presents a Bernstein-style convergence guarantee on matrix-valued functions satisfying Assumption \ref{as:markov-mtg}.

% Theorem \ref{thm:matrix-hoeffding} and Corollary \ref{cor:matrix-hoeffding} are useful if the aim is to develop high-probability convergence guarantees for matrix-valued functions generated from Markov chains. For example, when coupled with the matrix Freedman's inequality on martingales, Corollary \ref{cor:matrix-hoeffding} induces the following matrix Bernstein's inequality on a martingale generated by a Markov chain.
\medskip
\begin{customcorollary}[Matrix Bernstein's inequality on martingales generated from Markov chains]\label{thm:matrix-bernstein-mtg}
Consider a Markov chain $\{s_t\}_{t \geq 0}$ with a unique stationary distribution $\mu$ and a spectral gap $1-\lambda > 0$. Let $\{\bm{F}_i\}_{i \in [n]}$ be a sequence of functions mapping from  $\mathcal{S}^2$ to $\mathbb{S}^{d \times d}$, satisfying Assumption \ref{as:markov-mtg} and such that $\|\bm{F}_i(s,s')\| \leq M$ for every $i \in [n]$ and $s,s' \in \mathcal{S}$. Further define
\begin{align}
\bm{\Sigma}_n = \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{s \sim \mu, s' \sim P(\cdot \mid s)}[\bm{F}_i(s,s')\bm{F}_i^\top(s,s')].
\end{align} 
% \yuting{$\bm{F}_i^2$, you mean $\bm{F}_i \bm{F}_i^\top$?}
% \weichen{Since $\bm{F}_i$ is symmetric (as indicated in $\bm{F}_i \in \mathbb{S}^{d \times d}$), these are the same. Will clarify in the notation section.}
Let $\nu$ denote a probability distribution on $\mathcal{S}$  satisfying Assumption \ref{as:nu} with parameters $(p,q)$.
Then for any $\delta \in (0,1)$, it can be guaranteed that, when $s_0 \sim \nu$,
\begin{align}\label{eq:matrix-Bernstein}
\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_{i-1},s_i)\right\|  \lesssim \sqrt{\frac{\|\bm{\Sigma}_n\|}{n}\log \frac{d}{\delta}}+\frac{\sqrt{q}M}{(1-\lambda)^{\frac{1}{4}} n^{\frac{3}{4}}} \log^{\frac{3}{4}} \left(\frac{d}{\delta}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right) + \frac{M}{n} \log \frac{d}{\delta},
\end{align}
with probability at least $1-\delta$.
\end{customcorollary}
\medskip
% \paragraph{Technical novelty.} 

To establish Corollary ~\ref{thm:matrix-bernstein-mtg}, whose proof is in Appendix~\ref{app:proof-matrix-bernstein-mtg}, we first deploy  matrix Freedman's inequality, which gives an upper bound that is dependent on the \emph{conditional} covariance matrix
\begin{align*}
\bar{\bm{\Sigma}}_n := \frac{1}{n}\sum_{i=1}^n \mathbb{E}[\bm{F}_i\bm{F}_i^\top\mid \mathcal{F}_{i-1}].
\end{align*}
However, the above quantity is not measurable with respect to the trivial $\sigma$-field $\mathscr{F}_0$.
% When the martingale difference process $\{\bm{F}_i\}$ is generated by a Markov chain, 
Thus, we control the difference between $\bar{\bm{\Sigma}}_n$ and $\bm{\Sigma}_n$ using Corollary \ref{cor:matrix-hoeffding}, which leads to the first two terms in the upper bound \eqref{eq:matrix-Bernstein}. Notice that the second and third terms on the right hand side of \eqref{eq:matrix-Bernstein} both converge faster than $n^{-1/2}$, so Theorem \ref{thm:matrix-bernstein-mtg} shows that when measured by spectral norm, the sample mean of the sequence $\{\bm{F}_i\}$ converges to $\bm{0}$ by a rate determined by $\bm{\Sigma}_n$ with high probability. This bound plays a key role in 
% result is useful in the high-dimensional, non-asymptotic analysis of ML algorithms, as we will illustrate 
our analysis of TD learning in Section~\ref{sec:TD} and can be potentially used to understand the concentration of other machine learning algorithms. 

The following result, which is a corollary to Theorem \ref{thm:Srikant-generalize}, presents a multi-dimensional Berry-Esseen bound on vector-valued functions satisfying Assumption \ref{as:markov-mtg}. See Appendix \ref{app:proof-Berry-Esseen-mtg} for the proof.

\medskip
\begin{customcorollary}[High-dimensional Berry-Esseen bound on martingales generated from Markov chains]\label{thm:Berry-Esseen-mtg}
Consider a Markov chain $\{s_t\}_{t \geq 0}$ with a unique stationary distribution $\mu$ and a spectral gap $1-\lambda > 0$. Let $\{\bm{f}_i\}_{i \in [n]}$ be a sequence of functions from $\mathcal{S}^2$ to $\mathbb{R}^{d }$ satisfying Assumption \ref{as:markov-mtg} and such that $\|\bm{f}_i(s,s')\|_2 \leq M_i \leq M$ for all $s,s' \in \mathcal{S}$ and $i \in [n]$. Further define
\begin{align*}
\bm{\Sigma}_n = \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{s\sim\mu,s' \sim P(\cdot \mid s)}\Big[\bm{f}_i(s,s')\bm{f}_i^\top(s,s')\Big], \quad \text{and} \quad \bar{M} = \left(\frac{\sum_{i=1}^n M_i^4}{n}\right)^{\frac{1}{4}}.
\end{align*} 
% \yuting{in this definition, sum over $i$?}\ale{I agree}\weichen{yes, checked.}
Assume that $\lambda_{\min}(\bm{\Sigma}_n)\geq c $ for a constant $c>0$. For simplicity, we also assume that $M \geq 1$ and that $d\|\bm{\Sigma}_n\| \geq 1$.\footnote{These assumptions are made for ease of presentation, and are not essential.} 
Let $\nu$ denote a probability distribution on $\mathcal{S}$ satisfying Assumption \ref{as:nu} with parameters $(p,q)$. 
Then, when $s_0 \sim \nu$,
\begin{align}\label{eq:Berry-Esseen-mtg}
&d_{\mathsf{C}}\left(\frac{1}{\sqrt{n}}\sum_{i=1}^n \bm{f}_i(s_{i-1}, s_i), \mathcal{N}(\bm{0},\bm{\Sigma}_n)\right) \nonumber \\ 
&\lesssim \left\{\bar{M}\left(\frac{q}{1-\lambda}\right)^{\frac{1}{4}}\log^{\frac{1}{4}}\left(d\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{2}}+ \sqrt{M} \log^{\frac{1}{2}} (d\|\bm{\Sigma}_n\|)\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{4}}\right\}\sqrt{d} n^{-\frac{1}{4}}\log n.
\end{align}
\end{customcorollary}
\medskip

For readability, in the above expression we have omitted a lower-order term depending on the lower bound $c$ on $\lambda_{\min}(\bm{\Sigma}_n)$; see equation \eqref{eq:Wu-Gaussian-comparison} in Appendix \ref{app:proof-Berry-Esseen-mtg}.
To the best of our knowledge, Corollary \ref{thm:Berry-Esseen-mtg} presents the first high-dimensional Berry-Esseen bound on Markov chain-induced martingales in the convex distance. This distance is amenable for constructing confidence regions/sets, which would not be possible using the Wasserstein distance.
There are a number of notable differences between the conditions of Corollary \ref{thm:Berry-Esseen-mtg} and Corollary \ref{cor:Wu}: Firstly, Corollary \ref{thm:Berry-Esseen-mtg} does \emph{not} use the restrictive condition \eqref{eq:as-P1}, which demands a deterministic conditional variance, but instead only requires the martingale to be generated from a Markov chain with good mixing property, 
% \yuting{do you assume this anyway?} 
which means $\bm{P}_1$ is \emph{close to} $n \bm{\Sigma}_n$ with high probability. Secondly, Theorem \ref{thm:Berry-Esseen-mtg} requires that the norm of $\bm{f}_i$' s be \emph{uniformly bounded}, which is strictly stronger than \eqref{eq:3rd-momentum-condition}, as required by Corollary \ref{cor:Wu}. 

In order to demonstrate how the Berry-Esseen bound in Corollary \ref{thm:Berry-Esseen-mtg} depends on problem-related quantities, consider the scenario in which 
$\bm{f}_1 = \bm{f}_2 = \ldots = \bm{f}_n = \bm{f}$, and $M_1 = M_2 = \ldots = M_n = M$. Then, in this case,
% \yuting{we can even let $\nu = \mu$ for simplification?}
% \weichen{I personally prefer keeping the current form, to illustrate how the initial deviation from stationary distribution is reflected in the Berry-Esseen bound.}
\begin{align*}
\bm{\Sigma}_n = \frac{1}{n} \sum_{i=1}^n \mathbb{E}_{s_0 \sim \mu}\Big[\bm{f}_i(s_{i-1},s_i)\bm{f}_i^\top(s_{i-1},s_i)\Big] = \mathbb{E}_{s \sim \mu, s' \sim P(\cdot \mid s)}[\bm{f}(s,s')\bm{f}^\top(s,s')] =: \bm{\Sigma},
\end{align*}
and $\bar{M} = M$. Therefore, \eqref{eq:Berry-Esseen-mtg} implies that
\begin{align*}
d_{\mathsf{C}}\left(\frac{1}{\sqrt{n}}\sum_{i=1}^n \bm{f}(s_{i-1}, s_i), \mathcal{N}(\bm{0},\bm{\Sigma})\right)  \lesssim \left(\frac{q}{1-\lambda}\right)^{\frac{1}{4}}\log^{\frac{1}{4}}\left(d\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)M\|\bm{\Sigma}\|_{\mathsf{F}}^{\frac{1}{2}}\sqrt{d}n^{-\frac{1}{4}}\log n.
\end{align*}
We remark that the dependence on $d$ appears both explicitly in the $\sqrt{d}$ term, and implicitly in the $\|\bm{\Sigma}\|_{\mathsf{F}}^{\frac{1}{2}}$ and $M$ terms. 
% In other words, the rate by which the sample mean converges to its asymptotic Gaussian distribution is upper bounded by the followng:
% \begin{enumerate}
% \item The mixing speed of the Markov chain, through the term $(1-\lambda)^{-\frac{1}{4}}$;
% \item The derivation from the stationary distribution, through the term $q^{\frac{1}{4}}\log^{\frac{1}{4}}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}$;
% \item The properties of the function $\bm{f}$, including its dimension $d$ (through $\sqrt{d}\log^{\frac{1}{4}}d$), its variance matrix $\bm{\Sigma}$ (through $\|\bm{\Sigma}\|_{\mathsf{F}}^{\frac{1}{2}}$), and the bound on its norm $M$;
% \item The sample size $n$, through the convergence rate $n^{-\frac{1}{4}}\log n$. 
% \end{enumerate}

% \paragraph{Technical novelty.} 
The proof of Corollary \ref{thm:Berry-Esseen-mtg} is inspired by the arguments developed by \cite{rollin2018} to relax Assumption \eqref{eq:as-P1} in the uni-dimensional case, recently extended to the multivariate setting in 
\citet[][Lemma B.8]{cattaneo2024yurinskiiscouplingmartingales} and \citet[][Theorem 2.1]{
belloni2018highdimensionalcentrallimit}.
%A generalization to the multi-dimensional case is highly nontrivial, mainly due to the fact that the positive semi-definite order between matrices is incomplete. 
Specifically, we construct an auxiliary martingale satisfying \eqref{eq:as-P1}, and apply Corollary \ref{cor:Wu} to derive a Berry-Esseen bound. At the same time, we bound the difference between the target martingale and this auxiliary martingale by Corollary \ref{cor:matrix-hoeffding}. Finally, we combine these bounds using the properties of Gaussian distributions, as well as the relationship between convex distance and Wasserstein distance. 
See Appendix \ref{app:proof-Berry-Esseen-mtg}. %We refer the reader to  for more details.



