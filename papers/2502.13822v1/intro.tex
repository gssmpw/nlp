\section{Introduction}


% \subsection{Background and motivation}

%Markov chains serve as fundamental building blocks in many areas of machine learning, underpinning methods such as Markov Chain Monte Carlo (MCMC), Reinforcement Learning (RL), and generative modeling \citep{bremaud2013markov}. These algorithms leverage Markov processes to explore complex probability distributions and sequential decision-making spaces efficiently.

%Given their widespread use -- particularly in high-stakes applications such as healthcare, finance, and autonomous systems -- it is crucial to provide well-calibrated uncertainty estimates. 



Markov chains are important tools in statistical machine learning for modeling dependent data.
They provide a theoretical framework for analyzing sequential algorithms, are fundamental to MCMC sampling, Hidden Markov Models, and Reinforcement Learning (RL), and are widely used in a multitude of high-stakes applications, including NLP, finance, biology, and AI systems. 


Given their widespread use, it is crucial to provide rigorous probabilistic guarantees on the convergence, stability, and error bounds of Markovian sequences. Uncertainty quantification is essential for assessing the reliability, robustness, and generalizability of machine learning models built on Markov chains. For example, in MCMC sampling, precise uncertainty quantification ensures reliable convergence diagnostics and variance estimation, preventing misleading inferences in Bayesian models. In RL, uncertainty estimation for value functions helps balance exploration and exploitation, leading to more stable decision-making. Similarly, understanding uncertainty in reverse stochastic processes improves sample quality and diversity in diffusion-based generative models.

In this paper, we are concerned with uncertainty quantification for the temporal difference (TD) learning algorithm, a widely used method for value function estimation in RL \citep{sutton2018reinforcement}, assuming Markovian data. TD learning is an instance of stochastic approximation \citep{robbins1951stochastic}, designed to solve fixed-point equations via randomized approximations of residuals. In recent years, largely motivated by the diffusion and success of RL applications, there have been significant advancements in statistical inference techniques for Markov chain-based algorithms in RL; see, e.g., \cite{bhandari2018finite,mou2020linear,Fan2021Hoeffding,li2021sample,samsonov2023finitesample,samsonov2024gaussian,srikant2024rates}.% \ale{I removed the references \cite{toulis2017asymptotic}} 
These results can be broadly classified into two categories: (i) non-asymptotic bounds on the discrepancy between the algorithm’s output and the target quantity, and (ii) non-asymptotic distributional guarantees, such as Berry-Esseen bounds, which measure how fast the sequence of estimators converges to the limiting distribution.    However, when it comes to providing tight, non-asymptotic characterizations of general functions of Markov chains — an essential task in analyzing and calibrating machine learning procedures -- existing theoretical tools have limited scope and applicability, at least compared to the tools available for handling independent data.
 Our goal in this paper is to derive novel concentration bounds and Berry-Esseen bounds for vector- and matrix-valued functions of Markov chains and to exemplify their uses for tackling the crucial problem of value function estimation in RL.
% \vspace{3cm}
% \weichen{Two frontiers in finite sample analysis for ML models and algorithms: high-probability convergence guarantee, and non-asymptotic central limit theorems (Berry-Esseen bound)}

% \weichen{Motivation 1: analysis of TD learning, this is our actual motivation}

% \weichen{Motivation 2: theoretical results are initially proposed under independent samples, then generated to Markov chains or martingales in two directions. In some ML settings, the quantity of interest is both a martingale and generated from a Markov chain. We want to investigate how the combination of these two properties can lead to better statistical behavior...... We use TD to illustrate the application of our results.}




%\paragraph{TD learning.} 

\paragraph{Summary of the contributions.}
We make two types of theoretical contributions. First, we derive new high-dimensional finite sample approximations to Markov data and martingale processes in discrete time that are of broad applicability.
\begin{itemize}
    \setlength{\itemsep}{0pt}
\item In Theorem \ref{thm:matrix-hoeffding} and Corollary \ref{cor:matrix-hoeffding}, we derive powerful matrix Hoeffding inequalities for discrete-time Markov chains that generalize existing results in the literature \citep[see, e.g.][]{garg2018matrixexpanderchernoff, qiu2020matrix} by allowing for infinite state space, time-dependent functions, and arbitrary tolerance levels. Remarkably, when applied to univariate Markov chains, our bound nearly matches the optimal bound of \cite{Fan2021Hoeffding}.

\item In Theorem \ref{thm:Srikant-generalize} and Corollary \ref{cor:Wu}, we obtain novel high-dimensional  Berry-Esseen bounds in Wasserstein distance for normalized sums of discrete time vector-valued martingales with deterministic variance exhibiting dependence on the sample size $n$ of order $O(n^{-\frac{1}{2}}\log n)$. We establish this bound by refining and extending a proof strategy recently put forward by \cite{srikant2024rates}. %, is the first result of its kind. \weichen{Actually,  \cite{srikant2024rates} has a similar result, but is invalid because there are critical errors...... How should we present this?}

\item Leveraging the aforementioned results, we study martingales generated from Markov chains, a dependency structure that arises naturally in reinforcement learning algorithms but may be of independent interest. Corollary~\ref{thm:matrix-bernstein-mtg} and Corollary~\ref{thm:Berry-Esseen-mtg} establish matrix Bernstein type inequality and high-dimensional Berry-Esseen bound for these sequences respectively. 
\end{itemize}

Secondly, we establish rates of consistency and distributional approximations to the output of the TD learning algorithm (with Polyak-Ruppert averaging and polynomial step-size), arguably the most popular methodology for policy evaluation using linear approximations in reinforcement learning, with Markovian sequences of length $T$. Specifically, 
\begin{itemize}
\setlength{\itemsep}{0pt}
\item Theorem \ref{thm:TD-whp} gives a high-probability bound on the Euclidean norm of the estimation error of the TD learning algorithm featuring a sample dependence of order $T^{-1/2}$, up to log factors, and an optimal dependence on the asymptotic variance;
\item Theorem \ref{thm:TD-berry-esseen} provides a high-dimensional Berry-Esseen bound  for the TD estimator in the convex distance with a sample dependence of order $O(T^{-\frac{1}{4}}\log T)$. 
\end{itemize}
Both results are highly novel and do not have direct counterparts in the RL literature, which has, for the most part, focused on independent samples; see Section \ref{sec:TD} for a discussion of relevant work. 
% \ale{Probably not a good idea to mention Theorems \ref{thm:Berry-Esseen-mtg} and  Theorem \ref{thm:matrix-bernstein-mtg} here}
% \weichen{agreed. I change them to Corollaries then.}
%\item Theorem \ref{thm:Berry-Esseen-mtg} is the first high-dimensional Berry-Esseen bound on Markovian martingales, yielding an $O(n^{-\frac{1}{4}}\log n)$ convergence rate measured by convex distance;
%\item Theorem \ref{thm:matrix-bernstein-mtg} provides a tighter, Bernstein-style convergence guarantee for matrix-valued martingales generated from Markov chains;

\begin{comment}
\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[
  node distance=1.5cm and 1.0cm,
]

% Nodes
\node[box] (left0) {Matrix Hoeffding bounds \\ for general Markov chains\\ (Theorem \ref{thm:matrix-hoeffding})};
\node[box, right=1cm of left0] (right0) {Berry-Esseen bounds \\ for general martingales \\ (Theorem \ref{thm:Srikant-generalize})};
\node[box, below=2.5cm of left0] (left1) {Matrix Bernstein bounds\\ for Markov martingales\\ (Theorem \ref{thm:matrix-bernstein-mtg})};
\node[box, below=2.5cm of right0] (right1) {Berry-Esseen bounds\\ for Markov martingales \\ (Theorem \ref{thm:Berry-Esseen-mtg})};
\node[box, below=2.5cm of left1] (left2) {Converge w.h.p.\\ for TD Learning \\ (Theorem \ref{thm:TD-whp})};
\node[box, below=2.5cm of right1] (right2) {Berry-Esseen bounds\\ for TD Learning\\ (Theorem \ref{thm:TD-berry-esseen})};

% Labels
%\node[align=left, left=1.5cm of left1] (label1) {On Markov chains:};
%\node[align=left, left=1.5cm of left2] (label2) {For TD learning:};

% Arrows
\draw[arrow] (left0) -- node[right] {} (left1);%{support}
\draw[arrow] (left0) -- node[above right] {} (right1);%{support}
\draw[arrow] (right0) -- node[right] {} (right1);%{apply}
\draw[arrow] (left1) -- node[right] {} (left2);%{apply}
\draw[arrow] (right1) -- node[right] {} (right2);%{apply}

% Dashed horizontal line
%\draw[dashed-line] ($(label1.south west)!0.5!(label2.north west) - (0,0.5)$) -- ++(16,0);

\end{tikzpicture}
\end{center}
\caption{Logical structure of the main theoretical results in the paper \yuting{it is sufficient to describe the above figure in words?}\weichen{Yes, this is only for our reference. Will most likely delete in the submitted version.}}
\label{fig:diagram}
\end{figure}
\end{comment}
% \subsection{Other related works}

\paragraph{Notation.}
Throughout the paper, we use boldface small letters to denote vectors and boldface capital letters to denote matrices. For any vector $\bm{x}$, we let $\|\bm{x}\|_2$ be its $L_2$ norm; for any matrix $\bm{M}$, $\mathsf{Tr}(\bm{M})$ denotes its trace, $\mathsf{det}(\bm{M})$ its determinant, $\|\bm{M}\|$ its spectral norm (i.e., the largest singular value), and $\|\bm{M}\|_{\mathsf{F}}$ its Frobenius norm, i.e., $\|\bm{M}\|_{\mathsf{F}}=\sqrt{\mathsf{Tr}(\bm{M}^\top \bm{M})}$.  For any $\bm{M} \in \mathbb{S}^{d \times d}$, set of all $d \times d$ real symmetric matrices, we use $\lambda_{\max}(\bm{M}) = \lambda_1(\bm{M}) \geq \lambda_2(\bm{M}) \geq ... \geq \lambda_d(\bm{M}) = \lambda_{\min}(\bm{M})$ to indicate its eigenvalues.
%and $\mathrm{cond}(\bm{M})$  its condition number. For a vector $\bm{x} \in \mathbb{R}^d$ and a matrix $\bm{M} \in \mathbb{S}^{d \times d}$, we denote $\|\bm{x}\|_{\bm{M}} = \sqrt{\bm{x}^\top \bm{Mx}}$. 
For sequences $\{f_t\}_{t \in \mathbb{N}}$ and $\{g_t\}_{t \in \mathbb{N}}$ of numbers, we write $f_t \lesssim g_t$, or $f_t = O(g_t)$, to signify that there exists a quantity $C > 0$ independent of $t$ such that $f_t \leq C g_t$ for all $t$. We will use the notation $\tilde{C}$ to express a quantity independent of $T$, the number of iterations/sample size, but possibly dependent on other problem-related parameters. 

For two measures $\mu,\nu$ on the same measurable space $(\mathcal{X},\mathscr{F})$, if $\nu$ is absolutely continuous with respect to $\mu$, we use $\frac{\mathrm{d}\nu}{\mathrm{d}\mu}$ to denote its Radon-Nykodin derivative with respect to $\mu$. %, and use $\frac{\nu(\mathrm{d}x)}{\mu(\mathrm{d}x)}$ to denote this derivative evaluated at any $x \in \mathcal{X}$.
For any measurable function $f: \mathcal{X} \to \mathbb{R}$ and any number $p > 1$, we set $\|f\|_{\mu,p}^p:=\int_{x \in \mathcal{X}}|f(x)|^p \mathrm{d}\mu(x)$.
% \begin{align}
% \label{eq:mu-p-norm}
% \|f\|_{\mu,p} := \left(\int_{x \in \mathcal{X}}f^p(x)\mathrm{d}\mu(x)\right)^{\frac{1}{p}};
% \end{align}
This definition can also be extended to the case of $p = \infty$, by defining $\|f\|_{\mu,\infty} = \text{ess} \sup |f|,$ the essential supremum of $|f|$ with respect to $\mu$. To simplify the notation, when $p = 2$, we write $\|f\|_{\mu}:=\|f\|_{\mu,2}$.

We will consider the following measure of distance between two probability distributions $P,Q$ on $\mathbb{R}^d$: the \emph{Total Variance distance} $d_{\mathsf{TV}(P,Q)}:= \sup_{\mathcal{A} \in \mathscr{B}_d} |P(\mathcal{A}) - Q(\mathcal{A})|$, where $\mathscr{B}_d$ represents the class of all Borel sets in $\mathbb{R}^d$; the \emph{convex distance} $d_{\mathsf{C}}(P,Q) := \sup_{\mathcal{A} \in \mathscr{C}_d} |P(\mathcal{A}) - Q(\mathcal{A})|$, where  $\mathscr{C}_d$ represents the class of all convex subsets of $\mathbb{R}^d$; the \emph{Wasserstein distance} $d_{\mathsf{W}}(P,Q) := \sup_{h \in \mathsf{Lip}_1}|\mathbb{E}_{\bm{x} \sim P}[h(\bm{x})] - \mathbb{E}_{\bm{x} \sim Q}[h(\bm{x})]|$, where $\mathsf{Lip}_1$ represents the class of all 1-Lipchitz functions from $\mathbb{R}^d$ to $\mathbb{R}$; and  the \emph{Smooth Wasserstein distance} $d_{2}(P,Q) := \sup_{h \in C^2(\mathbb{R}^d)}|\mathbb{E}_{\bm{x} \sim P}[h(\bm{x})] - \mathbb{E}_{\bm{x} \sim Q}[h(\bm{x})]|$, where $C^2(\mathbb{R}^d)$ represents the class of all twice-differentiable functions from $\mathbb{R}^d$ to $\mathbb{R}$.  It can be easily verified by definition that $d_{\mathsf{TV}} \leq d_{\mathsf{C}}$ and $d_2 \leq d_{\mathsf{W}}$.

% \begin{align*}
% &\text{Total Variation (TV) distance: } d_{\mathsf{TV}(P,Q)}= \sup_{\mathcal{A} \in \mathscr{B}_d} |P(\mathcal{A}) - Q(\mathcal{A})|; \\ 
% &\text{convex distance: }d_{\mathsf{C}}(P,Q) = \sup_{\mathcal{A} \in \mathscr{C}_d} |P(\mathcal{A}) - Q(\mathcal{A})|;\\ 
% &\text{Wasserstein distance: } d_{\mathsf{W}}(P,Q) = \sup_{h \in \mathsf{Lip}_1}|\mathbb{E}_{\bm{x} \sim P}[h(\bm{x})] - \mathbb{E}_{\bm{x} \sim Q}[h(\bm{x})]|; \\ 
% &\text{Smooth Wasserstein distance: } d_{2}(P,Q) = \sup_{h \in C^2(\mathbb{R}^d)}|\mathbb{E}_{\bm{x} \sim P}[h(\bm{x})] - \mathbb{E}_{\bm{x} \sim Q}[h(\bm{x})]|
% \end{align*}
% Here, $\mathscr{B}_d$ represents the class of all Borel sets in $\mathbb{R}^d$, $\mathscr{C}_d$ represents the class of all convex subsets of $\mathbb{R}^d$, $\mathsf{Lip}_1$ represents the class of all 1-Lipchitz functions from $\mathbb{R}^d$ to $\mathbb{R}$, and $C^2(\mathbb{R}^d)$ represents the class of all twice-differentiable functions from $\mathbb{R}^d$ to $\mathbb{R}$. 

% \red{ define $\mathbb{S}^{d \times d}$}\weichen{checked.}

% \yuting{add definitions for wasserstein distance, convex distance, and $d_2$}\weichen{checked.}

% \yuting{not sure if we should recall the martingale definition and martingale difference sequence definition}\weichen{maybe not.... space is limited.}