\section{Proof of theoretical results regarding general Markov chains}

\subsection{Proof of Theorem \ref{thm:matrix-hoeffding}}\label{app:proof-matrix-hoeffding}
A classic Chernoff argument indicates
\begin{align}\label{eq:matrix-chernoff}
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right) &\leq 2\inf_{t \geq 0} \exp(-nt\varepsilon) \mathbb{E}\left[\mathsf{Tr}\left(\exp\left(t\sum_{i=1}^n \bm{F}_i(s_i)\right)\right)\right].
\end{align}
In order to bound the right-hand-side, \cite{garg2018matrixexpanderchernoff} illustrated in their Equation (11), through an application of the multi-matrix Golden-Thompson inequality, that there exists a probability distribution $\phi$ on the interval $[-\frac{\pi}{2},\frac{\pi}{2}]$, such that
% \yuting{give the theorem number}\weichen{added Equation number.}
\begin{align}\label{eq:multi-matrix-golden-thompson}
\mathsf{Tr}\left(\exp\left(t\sum_{i=1}^n \bm{F}_i(s_i)\right)\right) \leq d^{1-\frac{\pi}{4}}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\mathsf{Tr}\left[\prod_{i=1}^n \exp\left(\frac{2}{\pi}e^{\mathbf{i}\theta}t\bm{F}_i(s_i)\right)\prod_{i=n}^1 \exp\left(\frac{2}{\pi}e^{-\mathbf{i}\theta}t\bm{F}_i(s_i)\right)\right]\mathrm{d}\phi(\theta).
\end{align}
Furthermore, by repeatedly applying the basic properties of Kronecker products, the trace on the right-hand-side can be computed as 
\begin{align}\label{eq:matrix-hoeffding-kronecker}
&\mathsf{Tr}\left[\prod_{i=1}^n \exp\left(\frac{2}{\pi}e^{\mathbf{i}\theta}t\bm{F}_i(s_i)\right)\prod_{i=n}^1 \exp\left(\frac{2}{\pi}e^{-\mathbf{i}\theta}t\bm{F}_i(s_i)\right)\right] \nonumber \\ 
&= [\mathbf{vec}(\bm{I}_d)]^\top \left\{\prod_{i=n}^1 \exp\left(\frac{2}{\pi}e^{-\mathbf{i}\theta}t\bm{F}_i(s_i)\right) \otimes  \prod_{i=n}^1\exp\left(\frac{2}{\pi}e^{\mathbf{i}\theta}t\bm{F}_i(s_i)\right)\right\}\mathbf{vec}(\bm{I}_d) \nonumber \\ 
&= [\mathbf{vec}(\bm{I}_d)]^\top \prod_{i=n}^1\left\{\exp\left(\frac{2}{\pi}e^{-\mathbf{i}\theta}t\bm{F}_i(s_i)\right) \otimes \exp\left(\frac{2}{\pi}e^{\mathbf{i}\theta}t\bm{F}_i(s_i)\right)\right\}\mathbf{vec}(\bm{I}_d) \nonumber \\ 
&= [\mathbf{vec}(\bm{I}_d)]^\top \prod_{i=n}^1 \exp(t\bm{H}_i(s_i,\theta))\mathbf{vec}(\bm{I}_d),
\end{align}
% \yuting{add details for this equality}\weichen{a short explanation follows. Does that make things clear?}
where we define, for simplicity,
\begin{align}\label{eq:defn-Hi}
\bm{H}_i(s_i,\theta):= \frac{2}{\pi}e^{-\mathbf{i}\theta}\bm{F}_i(s_i) \otimes \bm{I}_{d^2} + \bm{I}_{d^2} \otimes \frac{2}{\pi}e^{\mathbf{i}\theta}\bm{F}_i(s_i).
\end{align}
Through the deduction of \eqref{eq:matrix-hoeffding-kronecker}, we applied properties (6), (1) and (7) in Theorem \ref{thm:Kronecker} in the second, third and fourth line respectively. 
It is easy to verify that the matrix function $\bm{H}_i$ defined as in \eqref{eq:defn-Hi} has the following properties:
\begin{align}\label{eq:Hi-properties}
&\mathbb{E}_{s_i \sim \mu} [\bm{H}_i(s_i,\theta)] = \bm{0}, \quad \forall i \in [n], \theta \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right];\\ 
&\|\bm{H}_i(s_i,\theta)\| \leq \frac{4}{\pi} M_i, \quad \text{a.s.}, \quad \forall i \in [n],  \theta \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right].
\end{align}

As a combination of \eqref{eq:matrix-chernoff}, \eqref{eq:multi-matrix-golden-thompson} and \eqref{eq:matrix-hoeffding-kronecker}, our goal is to bound
\begin{align*}
&\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right)  \\ 
&\leq 2d^{1-\frac{\pi}{4}} \inf_{t \geq 0} \exp(-nt\varepsilon) [\mathbf{vec}(\bm{I}_d)]^\top \mathbb{E}_{\theta \sim \phi} \mathbb{E}_s\left[\prod_{i=n}^1 \exp(t\bm{H}_i(s_i,\theta))\mathbf{vec}(\bm{I}_d)\right] \\ 
&\leq 2d^{1-\frac{\pi}{4}} \inf_{t \geq 0}  \sup_{\theta \in [-\frac{\pi}{2},\frac{\pi}{2}]} \exp(-nt\varepsilon) [\mathbf{vec}(\bm{I}_d)]^\top \mathbb{E}_s\left[\prod_{i=n}^1 \exp(t\bm{H}_i(s_i,\theta))\mathbf{vec}(\bm{I}_d)\right]
\end{align*}
% \red{expectation is over all the randomness. why only write $s_1$?}\weichen{checked.}
% \yuting{explain the notation $\mathbb{E}_{s_1 \sim \mu,s_{t+1}\sim P(\cdot \mid s_t)}$}
% \weichen{please see the explanation below.}
Here, we use $\mathbb{E}_s$ to denote the expectation taken over all the samples in the markov chain $s_1,s_2,...,s_n$, where $s_1 \sim \mu$ and $s_{i+1} \sim P(\cdot \mid s_i)$ for all $1 \leq i < n$. In order to bound this expectation, we define, for any $t>0$ and $\theta  \in [-\frac{\pi}{2},\frac{\pi}{2}]$, a sequence of vector-valued functions $\{\bm{g}_k\}_{k \in [n]}$ taking values in $\mathbb{R}^{d^2}$, where 
\begin{align*}
&\bm{g}_0 (x) = \textbf{vec}(\bm{I}_d), \quad \text{and} \\ 
&\bm{g}_k (x) = \mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k+1} = x\right], \quad k \geq 1.
\end{align*}
In this way, the left-hand-side of \eqref{eq:markov-matrix-hoeffding} is bounded by
\begin{align*}
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right) &\leq   2d^{1-\frac{\pi}{4}} \inf_{t \geq 0}  \sup_{\theta \in [-\frac{\pi}{2},\frac{\pi}{2}]} \exp(-nt\varepsilon) [\mathbf{vec}(\bm{I}_d)]^\top \mu(\bm{g}_n) \\ 
&\leq 2d^{1-\frac{\pi}{4}} \inf_{t \geq 0}  \sup_{\theta \in [-\frac{\pi}{2},\frac{\pi}{2}]} \exp(-nt\varepsilon) \left\|\mathbf{vec}(\bm{I}_d)\right\|_2 \cdot \|\mu(\bm{g}_n)\|_2 \\ 
&= 2d^{1-\frac{\pi}{4}} \inf_{t \geq 0}  \sup_{\theta \in [-\frac{\pi}{2},\frac{\pi}{2}]} \exp(-nt\varepsilon) \sqrt{d} \cdot \|\bm{g}_n^{\parallel}\|_{\mu}
\end{align*}
where we applied the fact that $s_{n+1} \sim \mu$ when $s_1 \sim \mu$. Recall that by definition, $\mu(\bm{g}_n) = \mathbb{E}_{x \sim \mu}[\bm{g}_n(x)]$ and $\bm{g}_n^{\parallel}$ represents the constant function taking this value.
% \yuting{recall the definition of $\mu(\bm{g}_n)$, $\mu$ norm, and $\bm{g}_n^{\parallel}$.} \weichen{checked.}
Next, we construct a recursive relation among the sequence $\{\bm{g}_k\}_{k \in [n]}$ for the purpose of bounding the norm of $\bm{g}_n$. Notice that, on one hand,
\begin{align*}
\exp\left(t\bm{H}_k(x,\theta)\right)\bm{g}_{k-1}(x) = \mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i))\bigg| s_{k} = x\right];
\end{align*}
%\textcolor{violet}{in the last line, is it $\bm{F}_i(s_i)$ or $\bm{H}_i(s_i,\theta)$?}\weichen{checked.}
on the other hand, %$\bm{g}_k(x)$ can be represented by
\begin{align*}
\bm{g}_k(x) &= \mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k+1} = x\right] \\ 
&=  \int_{y \in \mathcal{S}} \mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k} = y\right] \mathrm{d}\mathbb{P}(s_k = y\mid s_{k+1} = x) \\ 
&= \int_{y \in \mathcal{S}} \left\{\mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k} = y\right] \right\}P^*(x,\mathrm{d}y);
\end{align*}
% \color{violet}
% working on this. I think it will be notationally easier to use densities instead of measures when representing conditional distributions
% \begin{align*}
% \bm{g}_k(x) &= \frac{\mathcal{P}^k\nu(\mathrm{d}x)}{\mu(\mathrm{d}x)}\mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k+1} = x\right] \\ 
% &=  \frac{\mathcal{P}^k\nu(\mathrm{d}x)}{\mu(\mathrm{d}x)}\int_{y \in \mathcal{S}} \mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k} = y, s_{k+1} = x\right] \mathrm{d}\mathbb{P}_{S_k|S_{k+1}}( y\mid  x) \\ 
% &= \int_{y \in \mathcal{S}} \mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k} = y\right] \frac{\mathrm{d} \mathcal{P}^k\nu(x)}{\mathrm{d} \mu(x)} \cdot \frac{\mathrm{d} \mathcal{P}^{k-1}\nu(y)\mathrm{d}P_{S_{k+1} \mid S_k}(y|x)}{\mathrm{d} \mathcal{P}^k\nu(x)}\\ 
% &= \int_{y \in \mathcal{S}} \mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k} = y\right] \frac{\mathrm{d} \mathcal{P}^k\nu(x)}{\mathrm{d} \mu(x)} \cdot \frac{\mathrm{d} \mathcal{P}^{k-1}\nu(y) P(x,dy)}{\mathrm{d} \mathcal{P}^k\nu(x)}\\ 
% &= \int_{y \in \mathcal{S}} \left\{\mathbb{E}\left[\prod_{i=k}^1 \exp(t\bm{H}_i(s_i,\theta))\textbf{vec}(\bm{I}_d)\bigg| s_{k} = y\right] \frac{\mathrm{d}\mathcal{P}^{k-1}\nu(y)}{\mu(\mathrm{d}y)}\right\}P^*(x,\mathrm{d}y);
% \end{align*}
% \color{black}
% \weichen{I think an easier way is to just assume $s_0 \sim \mu$, the stationary distribution. The non-stationary starting distribution case can be handled by applying Holder's inequlity, though this would introduce a constant in the upper bound.}
here the contents in the curly bracket is equal to $\exp\left(t\bm{H}_k(y,\theta)\right)\bm{g}_{k-1}(u)$. Therefore, the sequence $\{\bm{g}_k\}$ is featured by the recursive relation
\begin{align}\label{eq:markov-matrix-hoeffding-G-recursive}
\bm{g}_{k}(x) &= \int_{y \in \mathcal{S}}\exp\left(t\bm{H}_k(y,\theta)\right)\bm{g}_{k-1}(y) P^*(x,\mathrm{d}y) \nonumber \\ 
&= (\mathcal{P}^* (\exp(t\bm{H}_k)\bm{g}_{k-1}))(x).
\end{align}
Based on this recursive relationship, the following proposition comes in handy for bounding the norm of $\bm{g}_k$ recursively.

\begin{customproposition}\label{prop:PE}
For any matrix function $\bm{g}: \mathcal{S} \to \mathbb{R}^{m}$ and any bounded symmetric matrix function $\bm{H}: \mathcal{S} \to \mathbb{S}^{m \times m}$ with  $\mu(\bm{H}) = \bm{0}$ and $\|\bm{F}(x)\| \leq M$ almost surely. Then the following hold for any $t > 0$:
\begin{align}
&\left\|\left(\mathcal{P}^* \exp(t\bm{H})\bm{g}^{\parallel}\right)^{\parallel}\right\|_{\mu} \leq \alpha_1 \|\bm{g}^{\parallel}\|_{\mu},\quad \text{where} \quad \alpha_1 = \exp(tM)-tM;\label{eq:PE1}\\
&\left\|\left(\mathcal{P}^* \exp(t\bm{H})\bm{g}^{\parallel}\right)^{\perp}\right\|_{\mu} \leq \alpha_2 \|\bm{g}^{\parallel}\|_{\mu},\quad \text{where} \quad \alpha_2 = \lambda(\exp(tM)-1);\label{eq:PE2}\\ 
&\left\|\left(\mathcal{P}^* \exp(t\bm{H})\bm{g}^{\perp}\right)^{\parallel}\right\|_{\mu} \leq \alpha_3 \|\bm{g}^{\perp}\|_{\mu},\quad \text{where} \quad \alpha_3 = \exp(tM)-1; \label{eq:PE3}\\ 
&\left\|\left(\mathcal{P}^* \exp(t\bm{H})\bm{g}^{\perp}\right)^{\perp}\right\|_{\mu} \leq \alpha_4 \|\bm{g}^{\parallel}\|_{\mu},\quad \text{where} \quad \alpha_4 = \lambda \exp(tM).\label{eq:PE4}
\end{align}
\end{customproposition}
\begin{proof} 
See Appendix \ref{proof-prop-PE}.
\end{proof}

A recursive relation can be constructed to bound the norm of $\bm{g}_n^{\parallel}$. Specifically, for every $k \in [n]$, \eqref{eq:markov-matrix-hoeffding-G-recursive},the triangle inequality and \eqref{eq:PE1}, \eqref{eq:PE2} guarantee that
\begin{align*}
\|\bm{g}_k^{\parallel}\|_{\mu} &= \left\|(\mathcal{P}^* \exp(t\bm{H}_k)\bm{g}_{k-1})^{\parallel}\right\|_{\mu} \\ 
&\leq \left\|(\mathcal{P}^* \exp(t\bm{H}_k)\bm{g}_{k-1}^{\parallel})^{\parallel}\right\|_{\mu} + \left\|(\mathcal{P}^* \exp(t\bm{H}_k))\bm{g}_{k-1}^{\perp})^{\parallel}\right\|_{\mu} \\ 
&\leq \alpha_{1k} \|\bm{g}_{k-1}^{\parallel}\|_{\mu} + \alpha_{2k} \|\bm{g}_{k-1}^{\perp}\|_{\mu};
\end{align*}
here, we define $\alpha_{1k} = \exp(\frac{4}{\pi}tM_k) - \frac{4}{\pi}tM_k$ and $\alpha_{2k} = \lambda(\exp(\frac{4}{\pi}tM_k)-1)$. In order to iteratively bound the norm of $\bm{g}_k^{\parallel}$, we also need to bound the norm of $\bm{g}_{k-1}^{\perp}$. Towards this end, we observe, due to \eqref{eq:markov-matrix-hoeffding-G-recursive}, the triangle inequality and \eqref{eq:PE3}, \eqref{eq:PE4} that for every $k \in [n]$,
\begin{align*}
\|\bm{g}_k^{\perp}\|_{\mu} &= \left\|(\mathcal{P}^* \exp(t\bm{H}_k))\bm{g}_{k-1})^{\perp}\right\|_{\mu} \\ 
&\leq \left\|(\mathcal{P}^* \exp(t\bm{H}_k))\bm{g}_{k-1}^{\parallel})^{\perp}\right\|_{\mu} + \left\|(\mathcal{P}^* \exp(t\bm{H}_k))\bm{g}_{k-1}^{\perp})^{\perp}\right\|_{\mu} \\ 
&\leq \alpha_{3k} \|\bm{g}_{k-1}^{\parallel}\|_{\mu} + \alpha_{4k} \|\bm{g}_{k-1}^{\perp}\|_{\mu}.
\end{align*}
Here again, we define $\alpha_{3k} = \exp(\frac{4}{\pi}tM_k) - 1$ and $\alpha_{4k} = \lambda \exp(\frac{4}{\pi}tM_k)$. For simplicity, we denote
\begin{align*}
&\bm{x}_0 = \begin{pmatrix}
\|\bm{g}_0^{\parallel}\|_{\mu} \\ 
\|\bm{g}_0^{\perp}\|_{\mu} 
\end{pmatrix}=\begin{pmatrix}
\sqrt{d} \\ 
0
\end{pmatrix}, \quad \text{and} \\ 
&\bm{x}_k = \underset{\bm{U}_j}{\underbrace{\begin{pmatrix}
\alpha_{1k} & \alpha_{3k} \\ 
\alpha_{3k} & \alpha_{4k}
\end{pmatrix}}} \bm{x}_{k-1}, \quad \forall k \in [n].
\end{align*}
It can then be easily verified through an induction argument that 
\begin{align*}
\|\bm{g}_n^{\parallel}\|_{\mu} \leq x_{n1}.
\end{align*}
Notice here that we applied the fact $\alpha_{2k}< \alpha_{3k}$, since $\lambda < 1$. Consequently, 
\begin{align*}
\|\bm{g}_n^{\parallel}\|_{\mu} &\leq x_{n1} 
\leq \|\bm{x}_n\|_2 
= \left\|\prod_{k=1}^n \bm{U}_k \bm{x}_0\right\|_2 
 \leq \prod_{k=1}^n \|\bm{U}_k\| \|\bm{x}_0\|_2 =  \sqrt{d} \cdot \prod_{k=1}^n \|\bm{U}_k\|.
\end{align*}
Since $\bm{U}_k$ is a symmetric $2 \times 2$ matrix, its operator norm is featured by
\begin{align*}
\|\bm{U}_k\| = \max\{|\sigma_{k1}|,|\sigma_{k2}|\}
\end{align*}
where $\sigma_{k1}$ and $\sigma_{k2}$ are the two eigenvalues of $\bm{U}_k$. Recall from elementary linear algebra that $\sigma_{j1}$ and $\sigma_{j2}$ are solutions to the equation
\begin{align*}
(\alpha_{1k} - x)(\alpha_{4k}-x) -\alpha_{3k}^2 = 0.
\end{align*}
Since $\alpha_{1k} > 0$ and $\alpha_{4k} > 0$, it can be easily verified that
\begin{align*}
\|\bm{U}_k\| = \frac{\alpha_{1k} + \alpha_{4k}}{2} + \frac{\sqrt{(\alpha_{1k} - \alpha_{4k})^2 + 4\alpha_{3k}^2}}{2}.
\end{align*}
The following lemma comes in handy for bounding the norm of $\bm{U}_k$.
\begin{customlemma}\label{lemma:Uk}
For any $x>0$ and $\lambda \in (0,1)$, denote $\alpha_1 = e^x - x$, $\alpha_3 = e^x - 1$ and $\alpha_4 = \lambda e^x$. It can then be guaranteed that
\begin{align*}
\frac{\alpha_1 + \alpha_4}{2} + \frac{\sqrt{(\alpha_1 - \alpha_4)^2 + 4\alpha_3^2}}{2} \leq \exp\left(\frac{5}{1-\lambda}x^2\right).
\end{align*}
\end{customlemma}  
\begin{proof} 
See Appendix \ref{app:proof-lemma-Uk}.
\end{proof}

As a direct implication of lemma \ref{lemma:Uk}, the norm of $\bm{U}_k$ is bounded by
\begin{align*}
\|\bm{U}_k\| \leq \exp\left(\frac{5}{1-\lambda} (\frac{4}{\pi}t)^2 M_k^2\right);
\end{align*}
consequently, the norm of $\bm{g}_n^{\parallel}$ is bounded by
\begin{align*}
\|\bm{g}_n^{\parallel}\|_{\mu} &\leq \sqrt{d}\exp\left(\left(\frac{4}{\pi}\right)^2\frac{5t^2}{1-\lambda} \sum_{k=1}^n M_k^2 \right) 
\end{align*}
for any $t \geq 0$ and $\theta \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right]$. Therefore, the left-hand-side of \eqref{eq:markov-matrix-hoeffding} is bounded by
\begin{align*}
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right) &\leq   2d^{2-\frac{\pi}{4}} \inf_{t \geq 0}  \exp(-nt\varepsilon)\exp\left(\left(\frac{4}{\pi}\right)^2\frac{5t^2}{1-\lambda} \sum_{k=1}^n M_k^2 \right).
\end{align*}

By letting
\begin{align*}
t = \frac{1-\lambda}{10} \left(\frac{\pi}{4}\right)^2 \frac{n}{\sum_{k=1}^n M_k^2} \cdot \varepsilon,
\end{align*}
we obtain
\begin{align*}
\mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right) &\leq 2d^{2-\frac{\pi}{4}} \exp\left\{-\frac{1-\lambda}{20}\left(\frac{\pi}{4}\right)^2\frac{n^2\varepsilon^2 }{\sum_{k=1}^n M_k^2} \right\}
\end{align*}
which completes the proof.


% \paragraph{An error in \cite{qiu2020matrix}.} In Claim 4 on page 24 of the paper, the authors attempted to bound the norm of $\|\bm{z}_i^{\perp}\|_{\pi}$; however, we note that the fourth line in the proof of this claim holds true only when $\|\bm{z}_0^{\perp}\|_{\pi} = 0$, which happens to not be the case since the initial distribution is non-stationary. This error seems to require a non-trivial correction, as we have done in the proof of Theorem \ref{thm:matrix-hoeffding} through the refinement of the iterative relationship along the sequence $\{\bm{g}_k\}_{k \in [n]}$.
% \yuting{will it be a problem if we assume $s_1\sim \mu$?}
% \weichen{Given our decision to only discuss $s_1 \sim \mu$ in this theorem and put non-stationary starting distribution to the next corollary, we may want to move this paragraph to the end of next subsection and delete the last sentence; or maybe remove it altogether.}

\subsection{Proof of Corollary \ref{cor:matrix-hoeffding}}\label{app:proof-cor-matrix-hoeffding}
For every $x \in \mathcal{S}$, define functions $f(x)$ and $g(x)$ as 
\begin{align*}
f(x):= \mathbb{P}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \Bigg| s_1 = x\right), \quad \text{and} \quad g(x) := \frac{\nu(\mathrm{d}x)}{\mu(\mathrm{d}x)}
\end{align*}
respectively. Then by definition, $g(x) \geq 0$ and $f(x) \in [0,1]$ for all $x \in \mathcal{S}$. Therefore, the Holder's inequality guarantees
\begin{align}\label{eq:matrix-hoeffding-holder}
\mathbb{P}_{s_1 \sim \nu}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right)&=\int_{\mathcal{S}}f(x) \nu(\mathrm{d}x)\nonumber \\
&= \int_{\mathcal{S}}f(x) g(x) \mu(\mathrm{d}x) \nonumber \\ 
&=\|fg\|_{\mu,1} \leq \|f\|_{\mu,q} \|g\|_{\mu,p} = \|f\|_{\mu,q}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p},
\end{align}
where, since $q > 1$ and $f(x) \in [0,1]$ almost surely, $\|f\|_{\mu,q}$ is bounded by
\begin{align}\label{eq:matrix-hoeffding-qbound}
\|f\|_{\mu,q} &= \left(\int_{\mathcal{S}}f^q(x)\mathrm{d}\mu \right)^{\frac{1}{q}}  
\leq \left(\int_{\mathcal{S}}f(x)\mathrm{d}\mu \right)^{\frac{1}{q}}  
\leq \left[\mathbb{P}_{s_1 \sim \mu}\left(\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_i)\right\| \geq \varepsilon \right)\right]^{\frac{1}{q}}.
\end{align}
Corollary \ref{cor:matrix-hoeffding} follows by combining Theorem \ref{thm:matrix-hoeffding}, with \eqref{eq:matrix-hoeffding-holder} and \eqref{eq:matrix-hoeffding-qbound}.






\subsection{Proof of Theorem \ref{thm:Srikant-generalize}}\label{app:Srikant-generalize}
This proof is a correction and improvement of Theorem 1 in \cite{srikant2024rates}, which applies Stein's method and Lindeberg's decomposition.
Using notation from \cite{JMLR2019CLT}, we denote, for every $k \in [n]$,
\begin{align*}
\bm{S}_k = \sum_{j=1}^k \bm{x}_k,  \quad \bm{T}_k = \sum_{j=k}^n \bm{V}_j^{\frac{1}{2}}\bm{z}_j, \quad \text{and} \quad \bm{T}_k' = \bm{T}_k + \bm{\Sigma}^{\frac{1}{2}}\bm{z}'.
\end{align*}
Here, $\{\bm{z}_k\}_{k=1}^n$ and $\bm{z}'$ are $i.i.d.$ standard Gaussian random variables in $\mathbb{R}^d$ and independent of the filtration $\{\mathscr{F}_k\}_{k=0}^n$. Note that since we assume $\bm{P}_1 = n\bm{\Sigma}_n$ almost surely, and that $\{\bm{V}_j\}_{1 \leq j \leq k}$ are all measurable with respect to the filtration $\mathscr{F}_{k-1}$, the matrices 
\begin{align*}
&\bm{P}_k = \bm{P}_1 - \sum_{j=1}^{k-1} \bm{V}_j = n\bm{\Sigma}_n - \sum_{j=1}^{k-1} \bm{V}_j, \quad \text{and} \\ 
&\bm{P}_{k+1} = \bm{P}_1 - \sum_{j=1}^{k} \bm{V}_j = n\bm{\Sigma}_n - \sum_{j=1}^{k} \bm{V}_j
\end{align*}
are both measurable with respect to $\mathscr{F}_{k-1}$. 
%\yuting{a bit hand-wavy.}\weichen{Added a short explanation. Does this work?}
It can then be guaranteed that $\frac{1}{\sqrt{n}}\bm{T}_1 \sim \mathcal{N}(\bm{0},\bm{\Sigma}_n)$ and that 
\begin{align}\label{eq:Srikant-Wasserstein}
d_{\mathsf{W}}\left(\frac{1}{\sqrt{n}}\sum_{k=1}^n \bm{x}_k,\mathcal{N}(\bm{0},\bm{\Sigma}_n)\right) &= \sup_{h \in \mathsf{Lip}_1} \left|\mathbb{E}\left[h\left(\frac{1}{\sqrt{n}}\bm{S}_n\right)\right] - \mathbb{E}\left[h\left(\frac{1}{\sqrt{n}}\bm{T}_1\right)\right]\right| \nonumber \\ 
&= \frac{1}{\sqrt{n}} \sup_{h \in \mathsf{Lip}_1}|\mathbb{E}[h(\bm{S}_n)]-\mathbb{E}[h(\bm{T}_1)]|\nonumber \\ 
&\leq \frac{1}{\sqrt{n}} \sup_{h \in \mathsf{Lip}_1}|\mathbb{E}[h(\bm{S}_n)]-\mathbb{E}[h(\bm{T}_1')]| + \frac{1}{\sqrt{n}} \sup_{h \in \mathsf{Lip}_1}|\mathbb{E}[h(\bm{T}_1)]-\mathbb{E}[h(\bm{T}_1')]|\nonumber \\ 
&\overset{(i)}\leq \frac{1}{\sqrt{n}} \sup_{h \in \mathsf{Lip}_1}|\mathbb{E}[h(\bm{S}_n)]-\mathbb{E}[h(\bm{T}_1')]| + \frac{1}{\sqrt{n}} \sup_{h \in \mathsf{Lip}_1} \mathbb{E}|h(\bm{T}_1) - h(\bm{T}_1')| \nonumber \\ 
&\overset{(ii)}\leq \frac{1}{\sqrt{n}} \sup_{h \in \mathsf{Lip}_1}|\mathbb{E}[h(\bm{S}_n)]-\mathbb{E}[h(\bm{T}_1')]| + \frac{1}{\sqrt{n}} \mathbb{E}\|\bm{\Sigma}^{\frac{1}{2}}\bm{z}\|_2 \nonumber \\ 
&\overset{(iii)}\leq \frac{1}{\sqrt{n}} \sup_{h \in \mathsf{Lip}_1}|\mathbb{E}[h(\bm{S}_n)]-\mathbb{E}[h(\bm{T}_1')]| + \sqrt{\frac{\mathsf{Tr}(\bm{\Sigma})}{n}},
\end{align}
where we applied the Jensen's inequality in (i)(iii) and Lipchitz property in (ii). 
Invoking the Lindeberg's decomposition, the triangle inequality yields
\begin{align}\label{eq:Srikant-Lindeberg}
|\mathbb{E}[h(\bm{S}_n)]-\mathbb{E}[h(\bm{T}_1')]| &\leq \sum_{k=1}^n |\mathbb{E}[h(\bm{S}_k+\bm{T}_{k+1}')]-\mathbb{E}[h(\bm{S}_{k-1}+\bm{T}_k')]|,
\end{align}
where we define $\bm{S}_0=\bm{T}_{n+1}=0$ for consistency. For each $k \in [n]$, define
\begin{align*}
\tilde{h}_k(\bm{x}):= h(\bm{P}_k'^{\frac{1}{2}}\bm{x}+\bm{S}_{k-1}),
\end{align*}
where we denote, for simplicity, $$\bm{P}_k' = \bm{P}_k + \bm{\Sigma}.$$ 
Since $h \in \mathsf{Lip}_1$, $\tilde{h}_k$ has Lipchitz coefficient $L_k = \|\bm{P}_k'^{\frac{1}{2}}\|$. Therefore, Lemma 1 in \cite{srikant2024rates} guarantees that there exists a function $f_k: \mathbb{R}^d \to \mathbb{R}$ such that
\begin{align*}
\tilde{h}_k(\bm{x})-\mathbb{E}[\tilde{h}_k(\bm{z})] = \Delta f_k(\bm{x}) - \bm{x}^\top \nabla f_k(\bm{x})
\end{align*}
holds for any $\bm{x} \in \mathbb{R}^d$, where $\bm{z}$ is the $d$-dimensional standard Gaussian random variable. Therefore, it can be guaranteed that
\begin{align*}
&h(\bm{S}_k + \bm{T}_{k+1}') -\mathbb{E}[h(\bm{S}_{k-1} + \bm{T}_k')\mid \mathscr{F}_{k-1}]\\ 
&= h(\bm{S}_k + \bm{T}_{k+1}') -\mathbb{E}[h(\bm{S}_{k-1} + \bm{P}_k'^{\frac{1}{2}}\bm{z})\mid \mathscr{F}_{k-1}] \\ 
&= \tilde{h}_k(\bm{P}_k'^{-\frac{1}{2}}(\bm{x}_k + \bm{T}_{k+1}'))-\mathbb{E}[\tilde{h}_k(\bm{z})] \\ 
&= \Delta f_k(\bm{P}_k'^{-\frac{1}{2}}(\bm{x}_k + \bm{T}_{k+1})) - \left[\bm{P}_k'^{-\frac{1}{2}}(\bm{x}_k + \bm{T}_{k+1}')\right]^\top \nabla f_k(\bm{P}_k'^{-\frac{1}{2}}(\bm{x}_k + \bm{T}_{k+1}')).
\end{align*}
Notice that by definition, $\bm{P}_k'^{-\frac{1}{2}}\bm{T}_{k+1} \mid \mathscr{F}_{k-1} \sim \mathcal{N}(\bm{0},\bm{P}_k'^{-\frac{1}{2}}\bm{P}_{k+1}'\bm{P}_k'^{-\frac{1}{2}})$. Therefore, we can denote $\tilde{\bm{z}}_k$ such that $\tilde{\bm{z}}_k \mid \mathscr{F}_{k-1} \sim \mathcal{N}(\bm{0},\bm{P}_k'^{-\frac{1}{2}}\bm{P}_{k+1}'\bm{P}_k'^{-\frac{1}{2}})$ and that $\tilde{\bm{z}}_k$ is independent of $\bm{x}_k$ when conditioned on $\mathscr{F}_{k-1}$. By taking conditional expectations, we obtain
\begin{align*}
&\mathbb{E}\left[h(\bm{S}_k + \bm{T}_{k+1}')\mid \mathscr{F}_{k-1}\right] - \mathbb{E}[h(\bm{S}_{k-1} + \bm{T}_k')\mid \mathscr{F}_{k-1}]\\ 
&=\mathbb{E}[\Delta f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)\mid \mathscr{F}_{k-1}]- \mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)^\top \nabla f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)\mid \mathscr{F}_{k-1}\right].
\end{align*}
Since by definition, $\Delta f_k = \mathsf{Tr}(\nabla^2 f_k)$, this can be further decomposed into
\begin{align}\label{eq:srikant-decompose}
&\mathbb{E}\left[h(\bm{S}_k + \bm{T}_{k+1}')\mid \mathscr{F}_{k-1}\right] - \mathbb{E}[h(\bm{S}_{k-1} + \bm{T}_k')\mid \mathscr{F}_{k-1}]\nonumber \\ 
&= \underset{I_1}{\underbrace{\mathbb{E}\left[\mathsf{Tr}\left(\bm{P}_k'^{-\frac{1}{2}}\bm{P}_{k+1}'\bm{P}_k'^{-\frac{1}{2}}\nabla^2 f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)\right)-\tilde{\bm{z}}_k^\top \nabla f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) \bigg| \mathscr{F}_{k-1}\right]}} \nonumber \\ 
&+ \underset{I_2}{\underbrace{\mathbb{E}\left[\mathsf{Tr}\left(\left(\bm{I}-\bm{P}_k'^{-\frac{1}{2}}\bm{P}_{k+1}'\bm{P}_k'^{-\frac{1}{2}}\right)\nabla^2 f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)\right)\bigg|\mathscr{F}_{k-1}\right]}} \nonumber \\ 
&- \underset{I_3}{\underbrace{\mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla f_k(\tilde{\bm{z}}_k) \bigg|\mathscr{F}_{k-1}\right]}} - \underset{I_4}{\underbrace{\mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \left(\nabla f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)- \nabla f_k(\tilde{\bm{z}}_k) \right)\bigg|\mathscr{F}_{k-1}\right]}}.
\end{align}
Of the four terms on the right-hand-side, $I_1$ is equal to $\bm{0}$ according to Lemma 2 in \cite{srikant2024rates}; $I_3$ is also $\bm{0}$ according to martingale property. 

\paragraph{Control the term $I_2$.} We first make the observation that
\begin{align*}
\bm{I}-\bm{P}_k'^{-\frac{1}{2}}\bm{P}_{k+1}'\bm{P}_k'^{-\frac{1}{2}}=\bm{P}_k'^{-\frac{1}{2}}(\bm{P}_k'-\bm{P}_{k+1}')\bm{P}_k'^{-\frac{1}{2}}=\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k\bm{P}_k'^{-\frac{1}{2}};
\end{align*}
Therefore, the term $I_2$ can be represented as
\begin{align}\label{eq:Srikant-I2}
&\mathbb{E}\left[\mathsf{Tr}\left(\left(\bm{I}-\bm{P}_k'^{-\frac{1}{2}}\bm{P}_{k+1}'\bm{P}_k'^{-\frac{1}{2}}\right)\nabla^2 f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)\right)\bigg|\mathscr{F}_{k-1}\right] \nonumber\\ 
&= \mathbb{E}\left[\mathsf{Tr}\left(\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k\bm{P}_k'^{-\frac{1}{2}} \nabla^2 f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)\right)\bigg|\mathscr{F}_{k-1}\right]\nonumber\\ 
&\overset{(i)}{=}\mathsf{Tr}\left\{\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k\bm{P}_k'^{-\frac{1}{2}}\mathbb{E}\left[\nabla^2 f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}\right]\right\} \nonumber\\ 
&= \mathsf{Tr}\left\{\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k\bm{P}_k'^{-\frac{1}{2}}\mathbb{E}\left[\nabla^2 f_k(\tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}\right]\right\}\nonumber \\ 
&+ \mathsf{Tr}\left\{\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k\bm{P}_k'^{-\frac{1}{2}}\mathbb{E}\left[\nabla^2 f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)-\nabla^2 f_k(\tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}\right]\right\},
\end{align}
where we invoked the linearity of trace and expectation in (i). 

% \paragraph{Control the term $I_4$.} We firstly represent the difference $\nabla f_k(\bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)- \nabla f_k(\tilde{\bm{z}}_k)$ as an integral:
% \begin{align*}
% &\nabla f_k(\bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)- \nabla f_k(\tilde{\bm{z}}_k) \\ 
% &= \left[\int_{t=0}^1 \nabla^2 f_k(t\bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) \mathrm{d}t\right] \cdot \bm{P}_k^{-\frac{1}{2}}\bm{x}_k \\ 
% &= \nabla^2 f_k(\tilde{z}_k) \cdot \bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \left\{\int_{t=0}^1 [\nabla^2 f_k(t\bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)- \nabla^2 f_k(\tilde{z}_k)]\mathrm{d}t\right\} \cdot \bm{P}_k^{-\frac{1}{2}}\bm{x}_k
% \end{align*}

\paragraph{Controlling $I_4$.} For clarity, we define a uni-dimensional function
\begin{align*}
F(t) := (\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla f_k(t\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k), \quad \forall t \in [0,1].
\end{align*}
Since $f_k$ is twice-differentiable, the derivative of $F(t)$ is featured by
\begin{align*}
F'(t) = (\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla^2 f_k(t\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) (\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k).
\end{align*}
Consequently, the Lagrange's mid-value theorem guarantees the existence of $\Theta  \in [0,1]$, such that
\begin{align*}
(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \left(\nabla f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)- \nabla f_k(\tilde{\bm{z}}_k) \right) 
&= F(1) - F(0) \\ 
&= F'(\Theta) = (\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla^2 f_k(\Theta\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) (\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k).
\end{align*}
% \yuting{write as integral form; there is no mid-value theorem in multivariate case} \weichen{Please check the updated version.} 
Consequently, the term $I_4$ is characterized by
\begin{align}\label{eq:Srikant-I4}
&\mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \left(\nabla f_k(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)- \nabla f_k(\tilde{\bm{z}}_k) \right)\bigg|\mathscr{F}_{k-1}\right]\nonumber \\ 
&=\mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla^2f_k(\Theta \bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k) \bigg|\mathscr{F}_{k-1}\right]\nonumber \\
&= \mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla^2f_k(\tilde{\bm{z}}_k)(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k) \bigg|\mathscr{F}_{k-1}\right] \nonumber \\ 
&+ \mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top (\nabla^2f_k(\Theta \bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) - \nabla^2 f_k(\tilde{\bm{z}}_k))(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k) \bigg|\mathscr{F}_{k-1}\right],
\end{align}
where $\Theta \in (0,1)$. Here, the first term on the right-most part of the equation can be further computed by
\begin{align}\label{eq:Srikant-I41}
&\mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla^2f_k(\tilde{\bm{z}}_k)(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k) \bigg|\mathscr{F}_{k-1}\right] = \mathbb{E}\left[\mathsf{Tr}\left((\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla^2f_k(\tilde{\bm{z}}_k)(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)\right)\bigg|\mathscr{F}_{k-1}\right]\nonumber \\ 
&\overset{(i)}{=} \mathbb{E}\left[\mathsf{Tr}\left((\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top \nabla^2f_k(\tilde{\bm{z}}_k)\right)\bigg|\mathscr{F}_{k-1}\right]\nonumber\\
&\overset{(ii)}{=} \mathsf{Tr}\left\{\mathbb{E}\left[\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\bm{x}_k^\top \bm{P}_k'^{-\frac{1}{2}}\nabla^2f_k(\tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}\right]\right\}\nonumber \\ 
&\overset{(iii)}{=}\mathsf{Tr}\left\{\mathbb{E}[\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\bm{x}_k^\top \bm{P}_k'^{-\frac{1}{2}} \mid \mathscr{F}_{k-1}] \mathbb{E}[\nabla^2f_k(\tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}]\right\}\nonumber \\
&\overset{(iv)}{=}\mathsf{Tr}\left\{\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k \bm{P}_k'^{-\frac{1}{2}}\mathbb{E}[\nabla^2f_k(\tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}]\right\};
\end{align}
notice that we applied the basic property of matrix trace in (i), the linearity of expectation in (ii), the conditional independence between $\bm{x}_k$ and $\tilde{\bm{z}}_k$ in (iii), and the definition of $\bm{V}_k$ in (iv). The right-most part of this equation is exactly the same as the first term on the right-most part of \eqref{eq:Srikant-I2}. 


\medskip
Consequently, putting relations \eqref{eq:srikant-decompose}, \eqref{eq:Srikant-I2}, \eqref{eq:Srikant-I4} and \eqref{eq:Srikant-I41}, we obtain
\begin{align}\label{eq:Srikant-reorganize}
&\mathbb{E}\left[h(\bm{S}_k + \bm{T}_{k+1}')\mid \mathscr{F}_{k-1}\right] - \mathbb{E}[h(\bm{S}_{k-1} + \bm{T}_k')\mid \mathscr{F}_{k-1}]\nonumber \\ 
&=\mathsf{Tr}\left\{\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k\bm{P}_k'^{-\frac{1}{2}}\mathbb{E}\left[\nabla^2 f_k(\bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)-\nabla^2 f_k(\tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}\right]\right\} \nonumber \\ 
&- \mathbb{E}\left[(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k)^\top (\nabla^2f_k(\Theta \bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) - \nabla^2 f_k(\tilde{\bm{z}}_k))(\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k) \bigg|\mathscr{F}_{k-1}\right]
\end{align}
Both terms on the right-hand-side can be bounded by the Holder's property of $\nabla^2 f_k$. Specifically, we have the following proposition:
\begin{customproposition}\label{prop:Stein-smooth}
Let $h: \mathbb{R}^d \to \mathbb{R} \in \mathsf{Lip}_1$, $\bm{\mu} \in \mathbb{R}^d$ and $\bm{\Sigma} \succ \bm{0} \in \mathbb{S}^{d \times d}$. Further define $g: \mathbb{R}^d \to \mathbb{R}$ as
\begin{align*}
g(\bm{x})= h(\bm{\Sigma}^{\frac{1}{2}}\bm{x} +\bm{\mu}), \quad \forall x \in \mathbb{R}^d,
\end{align*}
and use $f_g$ to denote the solution to Stein's equation
\begin{align*}
\Delta f(\bm{x}) - \bm{x}^\top \nabla f(\bm{x}) = g(\bm{x}) - \mathbb{E}[g(\bm{z})]
\end{align*}
where $\bm{z}$ is the $d$-dimensional standard Gaussian distribution. It can then be guaranteed that
\begin{align}
\left\|\nabla^2 f_g(\bm{x}) - \nabla^2 f_g(\bm{y})\right\| \lesssim (2+\log (d\|\bm{\Sigma}\|))^+ \cdot \|\bm{\Sigma}^{\frac{1}{2}}(\bm{x} - \bm{y})\|_2 + e^{-1}.
\end{align}
\end{customproposition}
\begin{proof} 
See Appendix \ref{app:proof-Stein-smooth}. 
\end{proof}

Proposition \ref{prop:Stein-smooth} guarantees for every $\Theta \in [0,1]$ that
\begin{align}
%\label{eqn:brahms}
\notag \left\|\nabla^2f_k(\Theta \bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) - \nabla^2 f_k(\tilde{\bm{z}}_k)\right\| &\leq (2+\log(d\|\bm{P}_k'\|))^+ \|\bm{x}_k\|_2 + e^{-1}\nonumber \\ 
&\leq (2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+ \|\bm{x}_k\|_2 + e^{-1}
\end{align}
% \yuting{not sure I understand this relation; isn't $\Sigma$ chosen arbitrary?} \weichen{checked.}
% Here, by taking $\beta = 1 - \frac{2}{\log n}$, we obtain
% \begin{align}
% \label{eqn:mozart}
% \left\|\nabla^2f_k(\Theta \bm{P}_k'^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) - \nabla^2 f_k(\tilde{\bm{z}}_k)\right\| &\leq (\sqrt{d} + \log n) \|\bm{x}_k\|_2.
% \end{align}
% \yuting{what about the dependence on $\Sigma_n$?} \weichen{checked.}

Consequently, the first term on the right hand side of \eqref{eq:Srikant-reorganize} can be bounded by
\begin{align*}
&\left|\mathsf{Tr}\left\{\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k\bm{P}_k'^{-\frac{1}{2}}\mathbb{E}\left[\nabla^2 f_k(\bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)-\nabla^2 f_k(\tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}\right]\right\}\right| \\ 
&\leq (2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+\mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right] \mathbb{E}[\|\bm{x}_k\|_2 |\mathscr{F}_{k-1}] + e^{-1} \mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right] \\ 
\end{align*}
Here, we further notice that since $\bm{P}_k'$ is measurable with respect to $\mathscr{F}_{k-1}$, and that $\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2$ and $\|\bm{x}_k\|_2$ are positively correlated \footnote{We refer readers to Appendix \ref{app:proof-correlated-norms} for the details of this claim.}, 
\begin{align}\label{eq:correlated-norms}
\mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right] \mathbb{E}[\|\bm{x}_k\|_2 |\mathscr{F}_{k-1}] \leq \mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right].
\end{align}
Therefore, the upper bound can be further simplified by
\begin{align*}
&\left|\mathsf{Tr}\left\{\bm{P}_k'^{-\frac{1}{2}}\bm{V}_k\bm{P}_k'^{-\frac{1}{2}}\mathbb{E}\left[\nabla^2 f_k(\bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k)-\nabla^2 f_k(\tilde{\bm{z}}_k)\bigg|\mathscr{F}_{k-1}\right]\right\}\right| \\ 
&\leq (2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+ \mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right]+ \mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right].
\end{align*}
Meanwhile, the second term can also be bounded by
\begin{align*}
&\left|\mathbb{E}\left[(\bm{P}_k^{-\frac{1}{2}}\bm{x}_k)^\top (\nabla^2f_k(\Theta \bm{P}_k^{-\frac{1}{2}}\bm{x}_k + \tilde{\bm{z}}_k) - \nabla^2 f_k(\tilde{\bm{z}}_k))(\bm{P}_k^{-\frac{1}{2}}\bm{x}_k) \bigg|\mathscr{F}_{k-1}\right]\right| \\ 
&\leq (2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+ \mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right]+ \mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right].
\end{align*}
Since these two bounds are equivalent, in combination, the triangle inequality guarantees
\begin{align*}
&\Big|\mathbb{E}\left[h(\bm{S}_k + \bm{T}_{k+1})\mid \mathscr{F}_{k-1}\right] - \mathbb{E}[h(\bm{S}_{k-1} + \bm{T}_k)\mid \mathscr{F}_{k-1}]\Big|\\ 
& \lesssim (2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+ \mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right]+ \mathbb{E}\left[\|\bm{P}_k'^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right].
\end{align*}
Plugging into \eqref{eq:Srikant-Lindeberg}, we obtain
\begin{align}\label{eq:Srikant-Lindeberg-2}
|\mathbb{E}[h(\bm{S}_n)]-\mathbb{E}[h(\bm{T}_1')]| &\leq \sum_{k=1}^n |\mathbb{E}[h(\bm{S}_k+\bm{T}_{k+1}')]-\mathbb{E}[h(\bm{S}_{k-1}+\bm{T}_k')]| \nonumber \\ 
&\lesssim (2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+ \sum_{k=1}^n \mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma})^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right]\right] \nonumber \\ 
&+ \sum_{k=1}^n \mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma})^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right]\right].
\end{align}
We now aim to proof the last summation on the right-hand-side of \eqref{eq:Srikant-Lindeberg-2}. The law of total expectation directly implies, for every $k \in [n]$, that
\begin{align*}
\mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma})^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right]\right] = \mathbb{E}\left[\mathsf{Tr}(\bm{P}_k'^{-1}\bm{V}_k)\mid \mathscr{F}_0\right].
\end{align*}
To further feature the summation, we invoke a telescoping technique. By taking $\bm{A} = \bm{P}_k'$ and $\bm{B} = \bm{P}_{k+1}'$ in Theorem \ref{thm:Klein}, the summand can be bounded by
\begin{align*}
\mathsf{Tr}(\bm{P}_k'^{-1}\bm{V}_k)&= \mathsf{Tr}(\bm{P}_k'^{-1}(\bm{P}_k' - \bm{P}_{k+1}')) \leq \mathsf{Tr}(\log (\bm{P}_k')) - \mathsf{Tr}(\log(\bm{P}_{k+1}')).
\end{align*}
Summing from $k=1$ through $k=n$, we obtain
\begin{align}\label{eq:Srikant-telescope}
\sum_{k=1}^n \mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma})^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right]\right] 
&=\sum_{k=1}^n \mathbb{E}\left[\mathsf{Tr}(\bm{P}_k'^{-1}\bm{V}_k)\mid\mathscr{F}_0\right] \nonumber \\
&\leq \sum_{k=1}^n \mathbb{E}\left[\mathsf{Tr}(\log (\bm{P}_k')) - \mathsf{Tr}(\log(\bm{P}_{k+1}')) \big|\mathscr{F}_0\right] \nonumber \\ 
&= \mathbb{E}[\mathsf{Tr}(\log \bm{P}_1')]-\mathbb{E}[\mathsf{Tr}(\log \bm{P}_n')]\nonumber \\
&= \mathsf{Tr}(\log(n\bm{\Sigma}_n+\bm{\Sigma})) - \log(\bm{\Sigma})).
\end{align}
Our target result follows by combining \eqref{eq:Srikant-Wasserstein}, \eqref{eq:Srikant-Lindeberg-2} and \eqref{eq:Srikant-telescope}. 


\subsection{Proof of Corollary \ref{cor:Wu}}\label{app:proof-cor-Wu}
In order to simplify the upper bound in Theorem \ref{thm:Srikant-generalize}, we firstly take $\bm{\Sigma} = \bm{\Sigma}_n$, yielding
\begin{align*}
d_{\mathsf{W}}\left(\frac{1}{\sqrt{n}}\sum_{k=1}^n \bm{x}_k,\mathcal{N}(\bm{0},\bm{\Sigma}_n)\right)  
& \lesssim  \frac{(2+\log(dn\|\bm{\Sigma}_n\|))^+}{\sqrt{n}} \sum_{k=1}^n \mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma}_n)^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right]\right] \\ 
&+ \frac{1}{\sqrt{n}}\left[\mathsf{Tr}(\log((n+1)\bm{\Sigma}_n)) - \log(\bm{\Sigma_n}))\right]+ \sqrt{\frac{\mathsf{Tr}(\bm{\Sigma_n})}{n}}.
\end{align*}
For the first term on the second line, we observe
\begin{align*}
\mathsf{Tr}(\log((n+1)\bm{\Sigma}_n)) - \log(\bm{\Sigma_n}))&= \sum_{i=1}^d \log (\lambda_i((n+1)\bm{\Sigma}_n)) - \sum_{i=1}^d \log (\lambda_i(\bm{\Sigma}_n))\\ 
&= \sum_{i=1}^n (\log (n+1) + \log \lambda_i(\bm{\Sigma}_n)) - \log \lambda_i(\bm{\Sigma}_n) = d \log(n+1).
\end{align*}
Meanwhile, the condition \eqref{eq:3rd-momentum-condition} can be invoked on the first term to obtain
\begin{align*}
\sum_{k=1}^n \mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma}_n)^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right]\right] &\leq M \cdot \sum_{k=1}^n\mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma}_n)^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right]\right] \\ 
&= M \cdot \left[\mathsf{Tr}(\log((n+1)\bm{\Sigma}_n)) - \log(\bm{\Sigma_n}))\right]\\ 
& = Md\log(n+1),
\end{align*}
% In order to tackle the summation in the upper bound of Theorem \ref{thm:Srikant-generalize}, we firstly apply the assumption \eqref{eq:3rd-momentum-condition} to obtain
% \begin{align}\label{eq:Wu-Wasserstein}
% d_{\mathsf{W}}\left(\frac{1}{\sqrt{n}}\sum_{k=1}^n \bm{x}_k,\mathcal{N}(\bm{0},\bm{\Sigma}_n)\right)  
% & \lesssim  \frac{(2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+}{\sqrt{n}} \sum_{k=1}^n \mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma})^{-\frac{1}{2}}\bm{x}_k\|_2^2 \|\bm{x}_k\|_2 \bigg|\mathscr{F}_{k-1}\right]\right] \nonumber \\ 
% &+ \frac{1}{\sqrt{n}}\left[\mathsf{Tr}(\log(n\bm{\Sigma}_n+\bm{\Sigma})) - \mathsf{Tr}(\bm{\Sigma}))\right]+ \sqrt{\frac{\mathsf{Tr}(\bm{\Sigma})}{n}} \nonumber \\ 
% &\lesssim  \frac{M}{\sqrt{n}} (2+\log(d\|(n\bm{\Sigma}_n + \bm{\Sigma})\|))^+\sum_{k=1}^n \mathbb{E}\left[\mathbb{E}\left[\|(\bm{P}_k + \bm{\Sigma})^{-\frac{1}{2}}\bm{x}_k\|_2^2 \bigg|\mathscr{F}_{k-1}\right]\right]  \nonumber \\ 
% &+ \frac{1}{\sqrt{n}}\left[\mathsf{Tr}(\log(n\bm{\Sigma}_n+\bm{\Sigma})) - \mathsf{Tr}(\bm{\Sigma}))\right]+ \sqrt{\frac{\mathsf{Tr}(\bm{\Sigma})}{n}}}.
% \end{align}
where the third line follows from \eqref{eq:Srikant-telescope}. In combination, the Wasserstein distance can be bounded by
\begin{align*}
d_{\mathsf{W}}\left(\frac{1}{\sqrt{n}}\sum_{k=1}^n \bm{x}_k,\mathcal{N}(\bm{0},\bm{\Sigma}_n)\right) &\lesssim \frac{M(2+\log(dn\|\bm{\Sigma}_n\|))^+ + 1}{\sqrt{n}}\cdot d\log n+ \sqrt{\frac{\mathsf{Tr}(\bm{\Sigma_n})}{n}}.
\end{align*}


\paragraph{Comparison with the corresponding uni-dimensional Corollary 2.3 in \cite{rollin2018}.} When $d=1$, the condition \eqref{eq:3rd-momentum-condition} reduces to
\begin{align*}
\mathbb{E}|x_k|^3\mid \mathcal{F}_{k-1} \leq M \mathbb{E}[x_k^2]\mid \mathcal{F}_{k-1}, \quad \forall k \in [n].
\end{align*}
Notice that this is a weaker assumption than the one outlined in Equation (2.13) of \cite{rollin2018}, which further assumes that the conditional third momentum of $x_k$ is uniformly bounded. In our proof of Corollary \ref{cor:Wu}, we invoke a telescoping method that not only simplifies the proof but also yields a tighter upper bound.

\paragraph{Comments on Corollary 3 in \cite{JMLR2019CLT}.} In a similar attempt to generalize this uni-dimensional corollary from \cite{rollin2018} to multi-dimensional settings, Corollary 3 of \cite{JMLR2019CLT} made the assumption that 
\begin{align}\label{eq:JMLR-Cor-assumption}
\mathbb{E}[\|\bm{x}_k\|_2^3 \mid \mathcal{F}_{k-1}] \leq \beta \vee \delta \mathsf{Tr}(\bm{V}_k), \quad \text{a.s.}
\end{align}
and applied the technique used by \cite{rollin2018}, which involved defining a sequence of stopping times $\{\tau_k\}_{k \in [n]}$. However, this generalization is non-trivial since the positive semi-definite order of symmetric matrices is \emph{incomplete}; in other words, when $\bm{A},\bm{B} \in \mathbb{S}^{d \times d}$, $\bm{A} \npreceq \bm{B}$ does not imply $\bm{A} \succ \bm{B}$. Consequently, the derivation from Equation (A.36) to (A.37) in the proof of Corollary 3 of \cite{JMLR2019CLT} is invalid: apparently, the authors' reasoning was, under their notation:
\begin{align*}
\mathsf{Tr}(\bar{\bm{V}}_{\tau_k} - \bar{\bm{V}}_{\tau_{k-1}})&= \mathsf{Tr}(\bar{\bm{V}}_{\tau_k} - \bar{\bm{V}}_{\tau_{k-1} + 1}) + \mathsf{Tr}(\bm{V}_k) \\ 
&\leq\mathsf{Tr}\left( \frac{k}{n}\bm{\Sigma}_n - \frac{k-1}{n}\bm{\Sigma}_n\right) + \beta^{\frac{2}{3}} = \frac{1}{n}\mathsf{Tr}(\bm{\Sigma}_n)+ \beta^{\frac{2}{3}}
\end{align*}
where the inequality on the second line follows from $\bm{V}_{\tau_k} \preceq \frac{k}{n}\bm{\Sigma}_n$, and that $\bm{V}_{\tau_{k-1}+1} \npreceq \frac{k-1}{n}\bm{\Sigma}_n$. However, since the positive semi-definite order is incomplete, the fact that $\bm{V}_{\tau_{k-1}+1} \npreceq \frac{k-1}{n}\bm{\Sigma}_n$ is not equivalent to $\bm{V}_{\tau_{k-1}+1} \succ \frac{k-1}{n}\bm{\Sigma}_n$; neither is there any guarantee that $\mathsf{Tr}(\bm{V}_{\tau_{k-1}+1})$ is greater than $\frac{k-1}{n}\mathsf{Tr}(\bm{\Sigma}_n)$.

In our proof of Corollary \ref{cor:Wu}, we solve this problem by invoking a different assumption \eqref{eq:3rd-momentum-condition} than \eqref{eq:JMLR-Cor-assumption}, and the telescoping method. 

\subsection{Proof of Corollary \ref{thm:matrix-bernstein-mtg}}\label{app:proof-matrix-berstein-mtg}
\label{app:proof-matrix-bernstein-mtg}

% \yuting{switch the order of theorem 2 and 3}\weichen{checked}

Since $\mathbb{E}_{s' \sim P(\cdot \mid s)}[\bm{F}_i(s,s')] = \bm{0}$ holds almost surely on $\mathcal{S}$ for every $i \in [n]$, it can be guaranteed that
\begin{align*}
\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_{i-1},s_i)
\end{align*}
is a matrix-valued martingale; therefore, if we define
\begin{align*}
\bar{\bm{\Sigma}}_n = \frac{1}{n}\sum_{i=1}^n\mathbb{E}_{i-1}[\bm{F}_i^2(s_{i-1},s_i)],
\end{align*}
% \yuting{sum over $i$?}\weichen{checked}
the matrix Freedman's inequality can be invoked on this martingale. Specifically, for every $\sigma^2$ and $\delta \in (0,1)$, it can be guaranteed with probability at least $1-\frac{\delta}{2}$ that
\begin{align}\label{eq:matrix-freedman}
\left\|\frac{1}{n}\sum_{i=1}^n \bm{F}_i(s_{i-1},s_i)\right\| \lesssim \sqrt{\frac{\sigma^2}{n}\log \frac{d}{\delta}} + \frac{M}{n} \log \frac{d}{\delta}, \quad \text{or} \quad \|\bar{\bm{\Sigma}}_n\| \geq \sigma^2.
\end{align}
In what follows, we aim to bound the norm of $\bar{\bm{\Sigma}}_n$ by controlling its different from $\bm{\Sigma}_n$. Specifically, define
\begin{align*}
\bm{G}_{i-1}(s_{i-1}) = \mathbb{E}_{i-1} [\bm{F}_i^2(s_{i-1},s_i)] - \mathbb{E}_{s \sim \mu, s' \sim P(\cdot \mid s)} [\bm{F}_i^2(s,s')].
\end{align*}
It can be easily verified that $\mu(\bm{G}_{i-1}) = \bm{0}$ and $\|\bm{G}_{i-1}\| \leq M^2$ almost surely for every $i \in [n]$. Therefore, Theorem \ref{thm:matrix-hoeffding} can be applied to the sequence $\{\bm{G}_{i-1}\}_{i \in [n]}$ to obtain
\begin{align*}
\left\|\bar{\bm{\Sigma}}_n - \bm{\Sigma}_n\right\| = \left\|\frac{1}{n}\sum_{i=1}^n \bm{G}_{i-1}(s_{i-1})\right\| \lesssim \frac{p}{p-1}\frac{M^2}{(1-\lambda)^{\frac{1}{2}}n^{\frac{1}{2}}}\log^{\frac{1}{2}}\left(\frac{d}{\delta}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)
\end{align*}
with probability at least $1-\frac{\delta}{2}$. Hence, the triangle inequality directly yields
\begin{align}\label{eq:matrix-bernstein-quadratic}
\|\bar{\bm{\Sigma}}_n\|\leq \|\bm{\Sigma}_n\| + \frac{p}{p-1} \frac{M^2}{(1-\lambda)^{\frac{1}{2}}n^{\frac{1}{2}}}\log^{\frac{1}{2}}\left(\frac{d}{\delta}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right).
\end{align}
The theorem follows by combining \eqref{eq:matrix-freedman}, \eqref{eq:matrix-bernstein-quadratic} using a union bound argument and taking
\begin{align*}
\sigma^2 = \|\bm{\Sigma}_n\| + \frac{p}{p-1} \frac{M^2}{(1-\lambda)^{\frac{1}{2}}n^{\frac{1}{2}}}\log^{\frac{1}{2}}\left(\frac{d}{\delta}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right).
\end{align*}

\subsection{Proof of Corollary \ref{thm:Berry-Esseen-mtg}}\label{app:proof-Berry-Esseen-mtg}

Part of the proof is inspired by the proof of Lemma B.8 in \citet{cattaneo2024yurinskiiscouplingmartingales} and Theorem 2.1 in \cite{
belloni2018highdimensionalcentrallimit}.
Borrrowing the notation from the proof of Theorem \ref{thm:Srikant-generalize} and Corollary \ref{cor:Wu}, we define, for each $k=1,\ldots,n$,
\begin{align*}
&\bm{S}_k := \sum_{j=1}^k \bm{f}_j(s_{j-1},s_j), \\ 
&\bm{V}_k := \mathbb{E}[\bm{f}_k(s_{k-1},s_k) \bm{f}_k^\top(s_{k-1},s_k)\mid \mathscr{F}_{k-1}]   \\ 
&\bm{T}_k := \sum_{j=k}^n \bm{V}_j^{1/2}\bm{z}_j, \text{ and } \\ %\ale{\text{ why not } \sum_{j=k}^n \bm{V}_j^{1/2} \bm{z}_j?} \weichen{\text{checked.}} \\ 
&\bm{P}_k := \sum_{j=k}^n \bm{V}_j.
\end{align*}
%for every $k \in [n]$. 
This proof approaches the theorem in four steps:
\begin{enumerate}
\item Find a value $\kappa = \kappa(n) = O(\frac{\log n}{\sqrt{n}})$, such that 
\begin{align*}
\mathbb{P}(\|\bm{P}_1 - n\bm{\Sigma}_n\| \geq n\kappa) \leq n^{-\frac{1}{2}}.
\end{align*}
\item Construct a martingale $\{\tilde{\bm{S}}_{j}\}_{j=1}^N$, whose differentiation satisfies the condition of Theorem \ref{thm:Srikant-generalize}, 
% \yuting{bad choice of notation $\bm{\Sigma}_{N}$}\weichen{abolished this notation,} 
such that
\begin{align*}
\mathbb{E}[\tilde{\bm{S}}_N \tilde{\bm{S}}_N^\top] = n(\bm{\Sigma}_n + \kappa \bm{I}), \quad \text{and} \quad \mathbb{P}\left(\|\bm{S}_n - \tilde{\bm{S}}_{N}\|_2 > \sqrt{2d\kappa n  \log n}\right)\leq  n^{-\frac{1}{2}}.
\end{align*}
\item Apply Theorem \ref{thm:Srikant-generalize} to $\tilde{\bm{S}}_{N}$ to derive a Berry-Esseen bound between the distributions of $\tilde{\bm{S}}_{N}$ and $\mathcal{N}(\bm{0},n(\bm{\Sigma}_n + \kappa \bm{I}))$;
\item Combine the results above to achieve the desired Berry-Esseen bound.
\end{enumerate}
\paragraph{Step 1: find $\kappa$.} Due to the Markovian property, the matrix $\bm{V}_k$ is a function of $s_{k-1}$ for every $k \in [n]$. Define
\begin{align*}
\bar{\bm{V}}_k = \mathbb{E}_{s \sim \mu,s' \sim P(\cdot \mid s)}[\bm{f}_i(s,s')\bm{f}_i^\top (s,s')],
\end{align*}
then it can be guaranteed that
\begin{align*}
\mathbb{E}_{s_{k-1}\sim \mu} [\bm{V}_k] = \bar{\bm{V}}_k, 
\end{align*}
and that
\begin{align*}
\|\bm{V}_k - \bar{\bm{V}}_k\| \leq M_k^2, \quad \text{a.s.}
\end{align*}
hold for every $k \in [n]$.
Consequently, a direct application of Theorem \ref{thm:matrix-hoeffding} yields
\begin{align*}
\|\bm{P}_1 - n\bm{\Sigma}_n\|= n \cdot \left\|\frac{1}{n}\sum_{i=1}^n (\bm{V}_k - \bar{\bm{V}}_k)\right\| &\leq \sqrt{\sum_{i=1}^n M_i^4} \sqrt{\frac{20q}{1-\lambda} \log \left(\frac{2d}{n^{-\frac{1}{2}}}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)} \\ 
 &\leq \sqrt{\sum_{i=1}^n M_i^4}\sqrt{\frac{40q}{1-\lambda} \log \left(2d n \left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)},
\end{align*}
with probability at least $1-n^{-\frac{1}{2}}$. In what follows, we take
\begin{align*}
\kappa &= \frac{1}{\sqrt{n}}\sqrt{\frac{\sum_{i=1}^n M_i^4 }{n} \frac{40q}{1-\lambda} \log \left(2d n \left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)} = \frac{\bar{M}^2}{\sqrt{n}} \sqrt{\frac{40q}{1-\lambda} \log \left(2d n \left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)}.
\end{align*}
\paragraph{Step 2: Construct $\{\tilde{\bm{S}}_j\}_{j=1}^N$.} Define the stopping time
\begin{align*}
\tau := \sup\left\{t \leq n: \sum_{i=1}^t \bm{V}_i \preceq n (\bm{\Sigma}_n + \kappa I)\right\},
\end{align*}
and let
\begin{align*}
&m:= \left\lceil \frac{1}{M^2} \mathsf{Tr}\Big(n (\bm{\Sigma}_n + \kappa I) - \sum_{i=1}^\tau \bm{V}_i\Big) \right\rceil, \quad \text{and} \quad N:= \left\lceil \frac{\mathsf{Tr}(n (\bm{\Sigma}_n + \kappa I))}{M^2} \right\rceil + n.
\end{align*}
By definition, it can be guaranteed that $n+m \leq N$, and that $N \asymp n$. We now construct a martingale difference process $\{ \tilde{\bm{x}}_i \}_{i=1}^{N}$ in the following way: for $1 \leq i \leq \tau$, let $\tilde{\bm{x}}_i = \bm{f}_i(s_{i-1},s_i)$ and for $\tau < i \leq \tau + m$, let 
\[
\tilde{\bm{x}}_i = \frac{1}{\sqrt{m}}\sum_{j=1}^d \epsilon_{ij} \sqrt{\lambda}_j \bm{u}_j,
\]
where the $\lambda_j$'s and $\bm{u}_j$'s are (possibly random) eigenvalues and eigenvectors of the spectral decomposition 
\[
n (\bm{\Sigma}_n + \kappa I) - \sum_{i=1}^\tau \bm{V}_i = \sum_{j=1}^d \lambda_j \bm{u}_j \bm{u}_j^\top,
\]
and $\{\epsilon_{ij}\}_{\tau < i \leq \tau + m, 1 \leq j \leq d}$ are $i.i.d.$ Rademacher random variables independent of the $s_i$'s, i.e.
\begin{align*}
\epsilon_{ij} = \begin{cases}
+1, \quad \text{w.p. }\frac{1}{2}; \\ 
-1, \quad \text{w.p. }\frac{1}{2}.
\end{cases}
\end{align*}
In particular, it holds that, for any $ \tau < i \leq \tau + m$,  
\begin{align*}
\mathbb{E}[\tilde{\bm{x}}_i] = \bm{0}, \quad \mathbb{E}[\tilde{\bm{x}}_i \tilde{\bm{x}}_i^\top\mid \mathscr{F}_{i-1}] = \frac{1}{m} \sum_{j=1}^d \lambda_j \bm{u}_j \bm{u}_j^\top,
\end{align*}
%\ale{shouldn't it be $\mathbb{E}[\tilde{\bm{x}}_i \tilde{\bm{x}}_i^\top] = \frac{1}{m} \sum_{j=1}^d \lambda_j \bm{u}_j \bm{u}_j^\top$?} \weichen{Yes, checked.}
and 
\begin{align*}
\|\tilde{\bm{x}}_i\|_2^2 = \frac{1}{m} \mathsf{Tr}\left(n (\bm{\Sigma}_n + \kappa I) - \sum_{i=1}^\tau \bm{V}_i\right) \leq M^2l
\end{align*}
almost surely. Finally, if $\tau + m < i \leq  N$, we simply set $\tilde{\bm{x}}_i = \bm{0}$. 

The martingale $\{\tilde{\bm{S}}_j\}_{j=1}^N$ is naturally constructed by
\begin{align*}
\tilde{\bm{S}}_j = \sum_{i=1}^j \tilde{\bm{x}}_i.
\end{align*}
%\ale{Shouldn't be say $\{ \tilde{S}_j\}_{j=1}^N$?} \weichen{checked.} 
In this step, we explore the difference between $\bm{S}_n$ and $\tilde{\bm{S}}_N$. Specifically, observe that
\begin{align*}
&\mathbb{P}\left(\|\bm{S}_n - \tilde{\bm{S}}_{N}\|_2 > \sqrt{2d\kappa n\log n}\right) \\ 
&= \mathbb{P}(\|\bm{S}_n - \tilde{\bm{S}}_{N}\|_2 > \sqrt{2d\kappa n\log n}, \|\bm{P}_1 - n\bm{\Sigma}_n\| \leq \kappa n) \\ 
&+ \mathbb{P}(\|\bm{S}_n - \tilde{\bm{S}}_{N}\|_2 > \sqrt{2d\kappa n\log n}, \|\bm{P}_1 - n\bm{\Sigma}_n\| > \kappa n)  \\ 
&\leq \mathbb{P}(\|\bm{S}_n - \tilde{\bm{S}}_{N}\|_2 > \sqrt{2d\kappa n\log n}, \|\bm{P}_1 - n\bm{\Sigma}_n\| \leq \kappa n) + \mathbb{P}(\|\bm{P}_1 - n\bm{\Sigma}_n\| > \kappa n) \\ 
&\leq \mathbb{P}(\|\bm{S}_n - \tilde{\bm{S}}_{N}\|_2 > \sqrt{2d\kappa n\log n}, \|\bm{P}_1 - n\bm{\Sigma}_n\| \leq \kappa n ) + n^{-\frac{1}{2}}.
\end{align*}
To bound the first term on the left hand side of the last inequality, notice that, when $\|\bm{P}_1 - n\bm{\Sigma}_n\| \leq \kappa n$,
\begin{align*}
\bm{P}_1 = \sum_{i=1}^n \bm{V}_i \preceq n(\bm{\Sigma}_n + \kappa \bm{I}).
\end{align*}
Thus, on the same event, $\tau = n$ and for every $j \in [d]$, 
\begin{align*}
\lambda_j \leq \|n(\bm{\Sigma}_n + \kappa \bm{I}) - \bm{P}_1\| \leq \|n\bm{\Sigma}_n - \bm{P}_1\| + \|n\kappa \bm{I}\| = 2\kappa n.
\end{align*}

%\ale{I am confused. Shouldn't it be $\lambda_j \leq M^2 + \kappa n$?} \weichen{please see above.}
Consequently, 
\begin{align*}
\|\bm{S}_n - \tilde{\bm{S}}_{N}\|_2^2 &= \left\|\sum_{i=n+1}^{n+m} \tilde{\bm{x}}_i \right\|_2^2 
= \frac{1}{m}\left\|\sum_{j=1}^d \sqrt{\lambda_j} \left(\sum_{i=n+1}^{n+m}\epsilon_{ij}\right)\bm{u}_j\right\|_2^2 
\leq \frac{2\kappa n}{m}  \sum_{j=1}^d \left(\sum_{i=n+1}^{n+m}\epsilon_{ij}\right)^2.
\end{align*}
Since $\{\epsilon_{ij}\}_{n+1 \leq i \leq n+m, 1 \leq j \leq d}$ are $i.i.d.$ Rademacher random variables, the Hoeffding's inequality guarantees that
\begin{align*}
\left|\sum_{i=n+1}^{n+m}\epsilon_{ij}\right| \lesssim \sqrt{m \log n}
\end{align*}
% \yuting{why there is a $d$ factor?} \ale{I agree: there is no $d$ here. Also, the above relation is not an $\leq$ but a $\preceq$ because we are ignoring universal constants} \weichen{all checked.} 
with probability at least $1-2n^{-\frac{1}{2}}$. As a direct consequence, the difference between $\bm{S}_n$ and $\tilde{\bm{S}}_N$ can be bounded by
\begin{align*}
\|\bm{S}_n - \tilde{\bm{S}}_{N}\|_2^2 \lesssim 2 d \kappa n\log n
\end{align*}
with probability at least $1-2n^{-\frac{1}{2}}$. In combination, we obtain
\begin{align*}
\mathbb{P}(\|\bm{S}_n - \tilde{\bm{S}}_N\|_2 \gtrsim \sqrt{2d\kappa n\log n}) \leq 3n^{-\frac{1}{2}}.
\end{align*}

\paragraph{Step 3: Berry-Esseen bound on $\tilde{\bm{S}}_N$.} It can be easily verified that the sequence $\{ \tilde{\bm{x}}_i\}_{i=1}^N$ is a martingale difference such that
\begin{align*}
&\sum_{i=1}^N \mathbb{E}[\tilde{\bm{x}}_i \tilde{\bm{x}}_i^\top \mid \mathscr{F}_{i-1}] = n(\bm{\Sigma}_n + \kappa \bm{I}), \quad \text{and} \quad \|\bm{\tilde{x}}_i\|_2 \leq M \quad \text{a.s., } \forall i \in [N].
\end{align*}
Hence, Theorem \ref{thm:Srikant-generalize} can be applied on $\tilde{\bm{S}}_N$ to obtain 
%\ale{Minor comment, though subtle (and following up on an earlier comment of Yuting's about the notation $\bm{\Sigma}_N$, which seems to suggest a sum of $N$ terms): here we apply Theorem \ref{thm:Srikant-generalize} to the sum $S_N$ but divide by $\sqrt{n}$. So we are effectively using the ``sum'' and not the ``average'' version of that theorem. I think everything checks out but just wated to point this out. } \weichen{Yes, this is a bit tricky. I just abolished the notation $\bm{\Sigma}_N$. Hope this reduces confusion!} 
\begin{align*}
&d_{\mathsf{W}}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}}, \mathcal{N}(\bm{0},\bm{\Sigma}_n + \kappa \bm{I})\right) \lesssim Md \log (d\|(\bm{\Sigma}_n + \kappa I)\|)\frac{\log^2 n}{\sqrt{n}},
\end{align*} 
where we applied the fact that $M\geq 1$ and $\|d\bm{\Sigma}_n\| \geq 1.$
\paragraph{Step 4: Completing the proof.} By the triangle inequality,
\begin{align}\label{eq:Wu-initial-decompose}
d_{\mathsf{C}}\left(\frac{\bm{S}_n}{\sqrt{n}},\mathcal{N}(\bm{0},\bm{\Sigma}_n)\right) \leq d_{\mathsf{C}}\left(\frac{\bm{S}_n}{\sqrt{n}},\mathcal{N}(\bm{0},\bm{\Sigma}_n + \kappa \bm{I})\right) + d_{\mathsf{C}}\left(\mathcal{N}(\bm{0},\bm{\Sigma}_n),\mathcal{N}(\bm{0},\bm{\Sigma}_n + \kappa \bm{I})\right)
\end{align}
where the second term on the right-hand-side can be bounded using a direct application of Theorem \ref{thm:DMR} by
\begin{align}\label{eq:Wu-Gaussian-comparison}
d_{\mathsf{C}}\left(\mathcal{N}(\bm{0},\bm{\Sigma}_n),\mathcal{N}(\bm{0},\bm{\Sigma}_n + \kappa \bm{I})\right) \lesssim \kappa \|\bm{\Sigma}_n^{-1}\|_{\mathsf{F}} \leq \kappa \frac{\sqrt{d}}{c}  = O\left( \sqrt{\frac{d}{c^2 n}} \log n \right),
\end{align}
where the last inequality follows from the assumption that $\lambda_{\min}(\bm{\Sigma}_n) \geq c$ and the choice of $\kappa$.
For the first term, consider any convex set $\mathcal{A} \subset \mathbb{R}^d$, the triangle inequality guarantees, for every $x > 0$, that
\begin{align}\label{eq:Wu-decompose-upper}
\mathbb{P}\left(\frac{\bm{S}_n}{\sqrt{n}} \in \mathcal{A}\right)&= \mathbb{P}\left(\frac{\bm{S}_n}{\sqrt{n}} \in \mathcal{A}, \left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 \leq x\right) + \mathbb{P}\left(\frac{\bm{S}_n}{\sqrt{n}} \in \mathcal{A}, \left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right) \nonumber \\ 
&\leq \mathbb{P}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}} \in \mathcal{A}^x \right) + \mathbb{P}\left(\left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right)\nonumber \\
&\leq \mathbb{P}\left(\frac{\tilde{\bm{T}}_N}{\sqrt{n}} \in \mathcal{A}^x \right) + d_{\mathsf{C}}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}},\frac{\tilde{\bm{T}}_N}{\sqrt{n}}\right)+ \mathbb{P}\left(\left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right) \nonumber \\ 
&\leq \mathbb{P}\left(\frac{\tilde{\bm{T}}_N}{\sqrt{n}} \in \mathcal{A} \right) + \left(\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{2}} + \sqrt{\kappa} d^{\frac{1}{4}}\right) x \nonumber \\ 
&+ d_{\mathsf{C}}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}},\frac{\tilde{\bm{T}}_N}{\sqrt{n}}\right)+ \mathbb{P}\left(\left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right).
\end{align}
Here, $\tilde{\bm{T}}_N \sim \mathcal{N}(\bm{0},n(\bm{\Sigma}_n +\kappa \bm{I}))$ and we applied Theorem \ref{thm:Gaussian-reminder} on the last line. On the right-most part of the inequality, the third term can be bounded by the Berry-Esseen bound on $\tilde{\bm{S}}_N$ and Theorem \ref{thm:Gaussian-convex-Wass}:
\begin{align*}
d_{\mathsf{C}}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}},\frac{\tilde{\bm{T}}_N}{\sqrt{n}}\right)
&\leq \|\bm{\Sigma}_n + \kappa \bm{I}\|_{\mathsf{F}}^{\frac{1}{4}} \sqrt{d_{\mathsf{W}}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}},\frac{\tilde{\bm{T}}_N}{\sqrt{n}}\right)} \\ 
&\leq \left(\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{4}} +  (\kappa \sqrt{d})^{\frac{1}{4}}\right)\sqrt{M} d^{\frac{1}{2}}\log^{\frac{1}{2}} (d\|\bm{\Sigma}_n+\kappa \bm{I}\|) n^{-\frac{1}{4}}\log n.
\end{align*}
Therefore, taking $x = \sqrt{2d\kappa \log n}$ in \eqref{eq:Wu-decompose-upper} yields
\begin{align*}
\mathbb{P}\left(\frac{\bm{S}_n}{\sqrt{n}} \in \mathcal{A}\right)& \leq \mathbb{P}\left(\frac{\tilde{\bm{T}}_N}{\sqrt{n}} \in \mathcal{A} \right) + \left(\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{2}} + \sqrt{\kappa} d^{\frac{1}{4}}\right) \cdot \sqrt{2d\kappa\log n} \\
&+ \left(\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{4}} +  (\kappa \sqrt{d})^{\frac{1}{4}}\right)\sqrt{M} d^{\frac{1}{2}}\log^{\frac{1}{2}} (d\|\bm{\Sigma}_n+\kappa \bm{I}\|) n^{-\frac{1}{4}}\log n + 3n^{-\frac{1}{2}}.
\end{align*}
Since $\sqrt{\kappa} = o(1)$, a simple reorganization yields
\begin{align}\label{eq:Wu-upper-bound}
&  \mathbb{P}\left(\frac{\bm{S}_n}{\sqrt{n}} \in \mathcal{A}\right)- \mathbb{P}\left(\frac{\tilde{\bm{T}}_N}{\sqrt{n}} \in \mathcal{A} \right)\nonumber \\ 
&\lesssim \left\{\bar{M}\left(\frac{q}{1-\lambda}\right)^{\frac{1}{4}}\log^{\frac{1}{4}}\left(d\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{2}}+ \sqrt{M} \log^{\frac{1}{2}} (d\|\bm{\Sigma}_n\|)\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{4}}\right\} \sqrt{d}n^{-\frac{1}{4}}\log n.
\end{align}
Meanwhile, a union bound argument and the triangle inequality guarantee
\begin{align*}
\mathbb{P}\left(\frac{\bm{S}_n}{\sqrt{n}} \in \mathcal{A}\right)& \geq \mathbb{P}\left(\frac{\bm{S}_n}{\sqrt{n}} \in \mathcal{A} \cup \left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right) - \mathbb{P}\left(\left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right) \\ 
&\geq \mathbb{P}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}} \in \mathcal{A}^{-x} \right) - \mathbb{P}\left(\left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right) \\ 
&\geq \mathbb{P}\left(\frac{\tilde{\bm{T}}_N}{\sqrt{n}} \in \mathcal{A}^{-x} \right) - d_{\mathsf{C}}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}},\frac{\tilde{\bm{T}}_N}{\sqrt{n}}\right)- \mathbb{P}\left(\left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right)  \\ 
&\geq \mathbb{P}\left(\frac{\tilde{\bm{T}}_N}{\sqrt{n}} \in \mathcal{A} \right) - \left(\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{2}} + \sqrt{\kappa} d^{\frac{1}{4}}\right) x \\ 
&- d_{\mathsf{C}}\left(\frac{\tilde{\bm{S}}_N}{\sqrt{n}},\frac{\tilde{\bm{T}}_N}{\sqrt{n}}\right)- \mathbb{P}\left(\left\|\frac{\bm{S}_n - \tilde{\bm{S}}_N}{\sqrt{n}}\right\|_2 > x\right);
\end{align*}
consequently, it can be symmetrically proved that
\begin{align}\label{eq:Wu-lower-bound}
&\mathbb{P}\left(\frac{\tilde{\bm{T}}_N}{\sqrt{n}} \in \mathcal{A} \right)-\mathbb{P}\left(\frac{\bm{S}_n}{\sqrt{n}} \in \mathcal{A}\right) \nonumber  \\ 
&\leq \left\{\bar{M}\left(\frac{q}{1-\lambda}\right)^{\frac{1}{4}}\log^{\frac{1}{4}}\left(d\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\mu}\right\|_{\mu,p}\right)\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{2}}+ \sqrt{M} \log^{\frac{1}{2}} (d\|\bm{\Sigma}_n\|)\|\bm{\Sigma}_n\|_{\mathsf{F}}^{\frac{1}{4}}\right\}\sqrt{d} n^{-\frac{1}{4}}\log n.
\end{align}
The theorem follows by combining \eqref{eq:Wu-initial-decompose}, \eqref{eq:Wu-Gaussian-comparison}, \eqref{eq:Wu-upper-bound}, \eqref{eq:Wu-lower-bound} and taking a supremum over $\mathcal{A} \in \mathscr{C}$. 



