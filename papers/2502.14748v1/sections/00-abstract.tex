A common use of \abr{nlp} is to facilitate the understanding of large
document collections, 
with a shift from using traditional topic models to Large Language Models.
% with models based on Large Language Models (\mm{}s) %based models
% replacing traditional machine learning topic models.
%The last decade has seen a shift from topic models for data exploration and content analysis to Large Language Models (\mm{}s) to generate or refine topics. 
%
Yet the effectiveness of using \mm{} for large corpus understanding in real-world
applications remains underexplored.
%Yet the ability of \mm{}-based models to help users understand content in real-world applications remains under explored.
%, motivating our empirical evaluation of prompt-based and probabilistic topic models.
%
% We propose \bass{}, an \mm{}-based topic modeling approach that puts users at the center of topic discovery. 
%We conduct an empirical evaluation of prompt-based topic models and probabilistic topic models.
%
This study measures the knowledge users acquire with unsupervised, supervised \mm{}-based exploratory approaches or traditional topic models on two datasets.
% with topic
% models---including traditional, unsupervised and supervised \mm{}-based
% approaches---on two datasets.
%In this study, we compare \emph{classical} and \mm{}-based topic models by evaluating the knowledge user acquire with the help of topic models on two datasets.
% measuring the knowledge users learn from topic models on two datasets.
%
% \jbgcomment{Be more concrete about what "results" mean.}
%Our evaluation combines standard clustering metrics, response consistency and quality through automatic evaluation, and knowledge acquisition preference through pairwise user preference.
% assessment through pretest/posttest question-answering tests, using both automatic and human evaluations. 
%
%We use an end-to-end framework to measure answer consistency between users and answer quality against gold answers.
%
While \mm{}-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, 
% are {\color{red} more effective for data exploration}, 
they produce overly generic topics for domain-specific datasets
that do not easily allow users to learn much about the documents.
%
Adding human supervision to the \mm{} generation process improves data
exploration by mitigating hallucination and over-genericity but requires greater human effort.
%
In contrast, traditional models like Latent Dirichlet Allocation
(\lda{}) remain effective for exploration but are less user-friendly.
%\mm{}-based methods produce more human-readable topics, and they are more effective for data exploration, but they appear to generate overly generic topics for three domain-specific datasets we tested~\ref{qualitative}.
% but hard to scale up and generate overly generic topics for domain-specific data.
%
%In contrast, traditional topic models like Latent Dirichlet Allocation (\lda{}) can still effectively support data exploration efforts, but is less user-friendly compared to \mm{}-involved topic models.
% remain effective for large-scale data exploration but  
% not as effective as \mm{}-based topic models for data exploration, especially for discovering abstract contents.
%While \mm{}-based approaches produce more human readable topics, they face scalability challenges. 
%
% Traditional topic models like Latent Dirichlet Allocation (LDA) is still effective for large-scale data exploration. 
%
%Furthermore, \mm{}-based models struggle with domain-specific data, often generating overly generic topics less useful for data exploration. 
%
We show that \mm{}s struggle to describe  the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints.
%
Dataset available at \textit{\url{https://huggingface.co/datasets/zli12321/Bills}}.
% \mm{}-based topic models cannot describe the haystack of a dataset alone and provides best practices---there is no one right model, the
% choice of models is situation-specific.

% potential
% improvements for scalable \mm{}-based topic models.

%Our quantitative results and qualitative analysis reveal an important lesson - there is no definitive \texttt{best} topic modeling approach that can universally achieve data exploration goals. 
%
%The choice of topic model should be carefully considered based on the specific tasks, goals, and intents of the researchers, rather than relying solely on a single automatic evaluation metric or claims from prior studies.
%
%We summarize best practices, the strengths of each approach, recommendations for topic model choices when dealing on different goals, and potential improvements for scalable \mm{}-based topic models.
%We summarize our study of topic models, highlighting best practices and advantages of traditional and \mm{}-based models. 
%
%We also identify future directions to improve scalability of \mm{}-based topic models.

%%%%%%%%%%%%%%%

%Our evaluation, using both automatic transformer pairwise similarity metrics and manual annotations of knowledge gain, shows that users without any tools perform worse than those provided with topic models, with statistical significance.
%
% We highlight the importance of topic models as essential tools for interpreting complex data 
% %We reinforce the role of topic models as a critical tool for making sense of complex data 
% and show that while \mm{}-based models enhance interpretability, traditional topic models like Latent Dirichlet Allocation (\lda{}) may still be sufficient for many tasks.

\begin{comment} % old abstract (9 Oct)
Data exploration and content analysis are essential to help social scientists understand a dataset, form a mental model and answer research questions.
%
Topic modeling has been popular for such tasks--identifying themes from unstructured text collections. 
%
Classical topic models such as Latent Dirichlet Allocation (\abr{lda}) and \bertopic{} have been extensively applied for data exploration as they discover latent clusters and themes for datasets. 
%
There is a trend of using Large Language Models (\mm{}s) to induce more readable topics over traditional topic models for content analysis. 
%
However, there still lacks a validation of measuring the usefulness of these emerging \mm{}-based topic modeling.
%
We propose \bass{}, an \mm{}-based topic modeling approach that interacts with users to build topics for a dataset.
%
We conduct user studies with \bass{}, two fully automated \mm{}-based approaches, and \lda{} on two case studies, and measure the usefulness of each method with qualitative analysis of user feedback and expert evaluation.
\end{comment}


% We introduce \bass{}, an approach which humans interact with \mm{}s and traditional topic models to induce abstract topics and analyze a dataset.
% \bass{} uses active learning to reduce the number of documents needed to be labeled by users and \mm{} while helping users build high-level topics grounded to the text contents and human judgments.
% %
% We conduct a case study with experts and a user study with \bass{} to generate topics for Bills and Education dataset. 
% %

% We let experts compare the outputs of \bass{}, \lda{}, and two other fully prompt-based methods and evaluate which method induce topics that are more informative and aligned with expert defined abstract concepts.
% %provides the most informative topics to answer their research questions about education teaching practices. 
% %
% In addition, we hire crowd source workers to annotate generated topic coverage rates against gold labels across four models. 
% %
% \bass{} can handle large text data more efficiently (fewer number of documents iterated) while maintaining good topic coverage rate than eixsting \mm{}-based approaches and induce more interpretable and descriptive topic than traditional topic models.



% existing prompt-based approaches require prompting \mm{}s many iterations (monetary and computation cost) and lack of human supervision and evaluation. The research question \textit{how do we measure topic model's interpretability and its usefulness to users} remains open?


% Topic models automatically discover latent themes within a set of documents and assign similar documents to a theme. However, most of existing topic models form clusters based on word distributions or pre-trained word embedding similarities. Thus, the interpretability and quality of topics have been called into question in regard to automatic evaluation metrics--coherence-- as automatic evaluation for neural topic models do not correlate with human preference of topic outputs. There is an increasing popularity of using large language models (\mm{} s) to generate more interpretable topics for a set of documents. However, existing \mm{} topic generation face challenges from needing to prompt \mm{}s for all the documents in the dataset, which can be computationally inefficient, and a lack of access to lower level meta features in classical topic models-- document topic distributions. We leverage the advantage of active learning to greatly reduce the number of documents needed to prompt an \mm, and generate topics that form better quality clusters than classical topic models and remains similar clusters than existing topic model methods with \mm{}. We show that using open-source \mm{}s can also work and better than classical topic models on both clustering and human rating scales.

% However, the formation of topics are not supervised by humans and humans usually conduct content analysis of documents after topic model automatically discovers themes of a corpus. We propose a new way of forming topic models to by strategically collaborating users with large language models, where users already conduct content analysis of a large corpus of documents when forming topics for the datasets, and the formation of topics are supervised by users. Users have more control on the output of the topic model. In addition, our proposed topic model method support multi-level topic formations, where the generated topics are completely controlled and supervised by humans, and have more interpretability than automatic unsupervised topic models. Our approach only require users to annotate topics until they are satisfied and users have full control on the outputs.


% Lorena, zongxia edit
% Topic models are used to identify latent themes within unstructured text data in an unsupervised manner and focus on grouping similar documents by low-level keyword distributions. 

%
% With a wealth of research and applications building on using Bayesian and Neural topic models-- classical topic models-- for data exploration, current work line of topic models do not answer the fundamental problem for content analysis- to what extent do topic models help users understand concepts from unstructured texts. 
% Recent works use Large Language Models (\mm{}s) to induce high-level and more meaningful topics from unstructured text documents. However, existing prompt-based approaches require prompting \mm{}s many iterations (monetary and computation cost) and lack of human supervision and evaluation. The research question \textit{how do we measure topic model's interpretability and its usefulness to users} remains open?
%
% Recent works use Large Language Models (\mm{}s) to induce topics from unstructured text documents. 
%
% Nevertheless, existing \mm{} approaches are costly (requires prompting \mm{}s for many iterations) and lack human supervision and evaluation.  
%

% helping humans induce abstract topics from unstructured data by making humans interact with \mm{}s and classical topic models. 
% BASS integrates \mm{}s, topic modeling, and human-in-the-loop supervision of \mm{} to maximize what humans learn from the text data and the quality of induced concepts, while using active learning to minimize the number of iterations to prompt \mm{}s. 
% \bass{} integrates traditional topic models, active learning, and \mm{}-approaches by displaying users initial topic model keywords and clusters, then uses active learning to pick most informative documents to show users, and a \mm{} to help users summarize and generate topics. 
%
% We introduce \bass{}, an approach which humans interact with \mm{}s and traditional topic models to induce abstract topics and analyze a dataset.
% \bass{} uses active learning to reduce the number of documents needed to be labeled by users and \mm{} while helping users build high-level topics grounded to the text contents and human judgments.
% %
% We conduct a case study with experts and a user study with \bass{} to generate topics for Bills and Education dataset. 
% %
% % Our second user evaluation study uses train/test sets to measure the interpretability, quality, and generalization of topics for \bass{}, traditional topic models, and existing fully automated \mm{}-based approaches.
% We let experts compare the outputs of \bass{}, \lda{}, and two other fully prompt-based methods and evaluate which method induce topics that are more informative and aligned with expert defined abstract concepts.
% %provides the most informative topics to answer their research questions about education teaching practices. 
% %
% In addition, we hire crowd source workers to annotate generated topic coverage rates against gold labels across four models. 
% %
% \bass{} can handle large text data more efficiently (fewer number of documents iterated) while maintaining good topic coverage rate than eixsting \mm{}-based approaches and induce more interpretable and descriptive topic than traditional topic models.


% We use BASS to conduct a user study and compare it with classical topic models and existing \mm{}-based models.
% %
% BASS generates more interpretable topics than classical topic models, and can scale up to large text data sizes with reasonable cost and time compared to other \mm{}-based approaches.

% {\color{blue}
% Topic modeling is used to identify themes from large unstructured text collections without supervision. While topic models building on the Latent Dirichlet Allocation topic model (LDA) --here classical topic models-- have been extensively applied for data exploration, they often struggle to effectively aid users in understanding concepts derived from such texts. 
% %
% This raises a critical question: How can we enhance topic models to facilitate user comprehension? Recent advancements have seen the utilization of Large Language Models (\mm{}s) to induce topics from unstructured text documents. However, existing \mm{} approaches suffer from high costs due to the need for prompting \mm{}s through numerous iterations and the lack of human supervision and rigorous evaluation.
% %
% To address these challenges, we propose BASS, a novel approach that integrates \mm{}s with traditional topic modeling techniques, augmented by human-in-the-loop supervision. BASS aims to enhance user comprehension by facilitating interaction between humans and \mm{}s, thereby improving the quality of induced topics from unstructured data while reducing the number of prompting iterations through active learning strategies.
% %
% Our evaluations, combining user studies and automatic assessments, demonstrate that BASS not only yields more interpretable topics compared to classical topic models but also boasts scalability to handle large text datasets efficiently, achieving significant cost and time savings compared to other \mm{}-based approaches.
% }

% Recent advancements highlight the effectiveness of Large Language Models (LLMs) in aligning with human expectations, leading to the development of LLM-based topic models. Nevertheless, existing LLM-based topic generation encounters two major hurdles: 1) the computational inefficiency of prompting LLMs for large datasets and 2) the absence of lower-level meta features \textendash{}document-topic and topic-word distributions\textendash.

% Users struggle to choose the best model due to missing options to automate evaluation. Even when a model is chosen, resulting topics might prove difficult to understand due to discrepancies between output and prior expectations.

% Recent advancements highlight the effectiveness of Large Language Models (LLMs) in aligning with human expectations, leading to the development of LLM-based topic models. Nevertheless, existing LLM-based topic generation encounters two major hurdles: 1) the computational inefficiency of prompting LLMs for large datasets and 2) the absence of lower-level meta features \textendash{}document-topic and topic-word distributions\textendash.

% To address these challenges, we propose a novel approach that integrates active learning and LLMs with classical topic model distributions. This strategy generates more meaningful and interpretable topics while retaining crucial lower-level meta features. By significantly reducing the number of documents required to prompt an LLM, our method improves the performance of classical topic models across both automatic evaluations and human rating scales using open-source LLMs.


%{\color{blue} The challenge of extracting high-level, abstract concepts from unstructured text collections useful for content analysis remains unmet by traditional topic models like Latent Dirichlet Allocation (\lda{}) and \bertopic{}. While Large Language Models (\mm{}s) offer the potential to induce high-level and more meaningful topics from unstructured text documents, existing prompt-based methods incur high computational costs and lack human oversight. We address the question of how to measure a topic model's interpretability and its practical value with our proposed approach, \bass{}. \bass{} integrates human interaction with \mm{}s and traditional topic models, utilizing active learning to minimize the number of documents users need to label. This process helps users construct high-level topics grounded in textual content and human judgment. In our case study and user study, experts and crowdsource workers evaluated \bass{} against \lda{} and two other prompt-based methods on datasets from Bills and Education. The results demonstrate that \bass{} generates topics that are more informative and aligned with expert-defined abstract concepts, with better topic coverage rates and efficiency than existing \mm{}-based methods.}
