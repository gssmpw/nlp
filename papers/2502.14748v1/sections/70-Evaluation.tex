% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/bills_boxplot.pdf}
%         \caption{Bills}
%         \label{fig:boxplot_bills}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/news_boxplot.pdf}
%         \caption{News}
%         \label{fig:boxplot_news}
%     \end{subfigure}
%     \caption{..}
%     \label{fig:boxplots}
% \end{figure*}

% \subsection{IDEA 1: Test for Specificity}
% Train/Set set evaluation.:
% \begin{itemize}
%     \item Select a fixed set of documents from the test set, and some documents from the train set, use it as the validation set.
%     \item Within each group, show users document sequentially. For each document, show the user the top 20-30 generated topics sorted by highest cosine similarity or relevance to the document. Let them match the topic to the document. For traditional topic models, show the users the topic keyword lists and let them match the topic keywords to documents.
%     \item At the end, calculate the inter-annotator agreement within each group. Or the coverage rate/accuracy, time/speed, mental challenge rating...
%     \item The train/test set split tests for generalization, interpretability of topics, quality of topics. Also tests the generated topics are not overly specific.
% \end{itemize}

% \subsection{IDEA 2: Test for too Generic}
% Train/Set set evaluation.:
% \begin{itemize}
%     \item Make the heldout test set such that some generic gold topic categories are not in the train set. For example, \textit{Health} is only in test set but not train set.
%     \item Within each group, show the users documents sequentially with top 20-30 relevant topics for each document. Ask the users if any can be a choice. If not, select none.
%     \item At the end, calculate the nones in each group. The more nones are in each group, we have better control on the generality of the topics.
% \end{itemize}

% \subsection{IDEA 2: Test for too Generic}
% Train/Set set evaluation.:
% \begin{itemize}
%     \item Select a fixed set of documents from the test set, and some documents from the train set, use it as the validation set.
%     \item Within each group, show users document sequentially. For each document, show the users the top 20-30 most relevant topics and add a distractor (can be a very generic topic: living things, organisms). Calculate the times the users can successfully pick the distractors for each group.
%     \item The generic distractors measures the generated topics or keywords are overly generic.
% \end{itemize}


% {\color{red}Formula Calculation to balance between specifity and generality:

% Use the F1 Precision and Recall formula.}

% NOTES: we let the users who have been through the \bass{} process to do follow-up test evaluation for each of the group. I suspect the users are more efficient and faster for the \bass{} topics. Then hire more users other than \bass{} users to do the same test set evaluation and check the results. This way we can measure users who have been through the content analysis process can better use and learn the generated topics than using topics from fully automated methods without going through the process. 

% %
% For users who have not been through the process but use the \bass{} topics to match test set documents, if the matching rates are similar to TopicGPT, better than LDA, then we can conclude that \bass{} retain high quality topics compared to TopicGPT even though it only traverses through only a fraction of the documents. We can also conclude that the generalization and interpretability of \bass{} topics from one user to a different user. 


% Once we have the above two conclusions, we can synthesize and say that 

% 1. users been through \bass{} can generate good quality labels that are more descriptive and can be understood by other users; 

% 2. users been through the \bass{} process learns more from the concepts, contents of the documents, and topics so they can match the labels to test documents more efficiently and with less effort than handing them a list of topic keywords or a list of topics fully generated by \mm{}s.  


% \subsection{IDEA 4: Coverage Rate between golds}
% \begin{itemize}
%     \item For each generated topic or topic keywords, let an LLM find whether the generated topic has a semantic similar/equivalent gold label from the gold label list
%     \item The higher the coverage rate, the better
% \end{itemize}

% \subsection{IDEA 5: Clustering evaluation}
% Train/Set set evaluation.:
% \begin{itemize}
%     \item Select a fixed set of documents from the test set, and some documents from the train set, use it as the validation set.
%     \item For each document, show the user the most relevant top 10 topics from each group (LDA, TopicGPT, CTM...). Let the user pick the most descriptive group. Or add a distractor option, let the user pick the distractor.
% \end{itemize}




% \subsection{IDEA 4: Clustering evaluation}
% Active learning with \mm{} is the only method that can show an increasing of clustering qualities among all the methods. We show the clustering evaluations for all the methods, and the evolution of the metrics for \bass{}.

% % \subsection{Advantages}
% % Our proposed method is the transferring and adjustment of classical topic models to \mm{}-based topic models. We retain the initial low-level features for classical topic models in the first place, but use that as an advantage to cluster similar documents that have similar labels generated by the \mm{}. Our final output of the topic model is a classifier, with tf-idf document embedding, and a document pairwise matri, and a assigned topic for each document. In comparison, TopicGPT only has an assigned topic for each document but no access to the pairwise matrix. 

% % \subsection{Automatic Evaluation Ideas}
% % \begin{itemize}
% %     \item Document matrix pairwise score?
% %     \item Balance of the topics?
% %     \item Alignment with a reference graph. Since we do not have access to a ground-truth reference graph (it would only be available if we were to use synthetic data, but we cannot use synthetic data when LLMs are involved), we replace it with an approximation based on the document metadata \cite{vazquez2022validation}. 
% % \end{itemize}

% \subsection{Human Validation of Topics}
% \begin{itemize}
%     \item Select the most related document for each topic, let users justify whether a label is better or a list of topic keywords is better
%     \item Select a document, show the user the topic keywords, and \mm{} labels and see which one is better. Related the documents with topics.
% \end{itemize}

% \subsection{Standard Evaluation}
% Mainly Bills dataset
% Purity, RandIndex, NMI.

% Human evaluation: 
% For each topic model method, we show the users all the gold labels, and the topic model topics. Ask them whether the topic model topics match any of the gold topics. Calculate the coverage rate of the topics. We assume the rate is high for all prompt-based methods.


% \paragraph{IDEA 5: }
% Give users a set of documents, and for each document the labels generated by each model (e.g., Lloom, Tenor, BASS, gold label as baseline) + topic keywords. People evaluating this is not the same that generated them.

% \subsection{Expert High-Level Evaluation} 
% Mainly on the \ncte{} dataset. The datasets have high-level expert defined concepts that can be used to answer research questions. We ask non-expert users to use each of the topic model method to generate descriptive labels/concepts that describe the open research question \textit{What high-inference teaching practices (i.e., MQI variables) are associated with overall high-quality math teaching?} in the dataset. We ask education study experts who have similar backgrounds and familiarity in the dataset to evaluate how closely the non expert written topics align with the gold topics; additional new insights grounded from the dataset and the topics are rewarded and not penalized, but for those not from the dataset or the topics (most likely daily common sense or google search) will be panalized.