\ahintext{Previous intro below:}
Navigating and analyzing large datasets poses significant challenges in today's data-driven world.
%
Probabilistic topic models (\lda{}, \citealt{blei2003lda}) represent documents as mixtures of topics, with topics defined as probabilistic distributions over words. Neural topic models integrate pre-trained transformer representations into an \lda{}-like process (\ctm{}, \citealt{bianchi2020pre}) or use them in combination with clustering techniques (\bertopic{}, \citealt{bertopicMark}). 
%
These models characterize topics as list of most probable keywords. %low-level
%
While users can analyze these low-level topics to derive high-level concepts that are more interpretable and descriptive to humans~\cite{Asmussen2019SmartLR}, these methods do not always produce interpretable topics useful for end-users analysis~\cite{hoyle2021automated}. %and valuable

% For example, in a scenario where a school principle wants to evaluate the good and bad teaching practices for teaching math given a set of teacher-student conversation documents for the school year, a classical topic model generates topics \textit{`round', `card', `win', `game', `winner'}\dots based on word frequency.
For example, if a social scientist aims to explore the common characteristics of land and wildlife in government policies, a classical topic model might generate a relevant topic such as \textit{`land', `forest', `area', `wilderness', `management'}\dots based on word frequency.
%
% Yet, these keywords are mathematical terms discussed most frequently in classrooms, which lack informativeness to help researchers in the given scenario to analyze and identify abstract topics to answer their question.
Yet, the effectiveness of keyword-based topic representation in real tasks, compared to discrete descriptions that synthesize these keywords into more interpretable concepts, remains unclear.
%compared to discrete descriptions from prompt-based topic models remain unclear. 

\jbgcomment{Below paragraphs don't belong here, move to next section}

\mm{}s such as GPT~\cite{openai2024gpt4,touvron2023llama} have gained popularity for inducing topics for content analysis.
%
\topicgpt{}~\cite{pham2024topicgpt} iteratively prompts GPT-4 with random sampled documents to generate high level concepts and assigns topics to all documents. 
%
\lloom{}~\cite{lam2024concept} uses \mm{}s to extract summaries from individual documents, and synthesize them to produce high-level and human readable topics. 
%
These prompt-based topic models are reported to produce topics that are more aligned with gold labels and more descriptive~\cite{lam2024concept}.
%
However, they come with certain drawbacks. %there are downsides of current prompt-based topic models. 
%
%Prompt-based topic models require many steps 
One key issue is that they require multiple rounds of prompting across many documents, which can become costly as the data size scales.
%
%In addition, there still lacks work evaluating how much can 
Additionally, there is still a lack of research evaluating how effectively prompt-based topic modeling help users build a mental model and analyze the dataset from scratch, which is the ultimate goal of topic modeling. This raises an import questions: are prompt-based approximations of topic models truly more useful for data analysts?
%leading to the question: are newer topic models more useful for data analysts?


With the increased interactive capabilities of \mm{}s, we also include \bass{}, an interactive interface that prompts \mm{}s to generate summary and topics for individual documents. Users can then edit the topics as feedback for the \mm{}, guiding it to generate more meaningful topics aligned with their intent. 
%
We use \abr{tenor}~\cite{alto, li-etal-2024-improving} to direct users' attention to the most relevant documents one at a time, reducing the number of topics they need to read.
%the individual documents that are the most beneficial to the users sequentially and reduce the number of documents humans need to read.
%

% \jbgcomment{Run style checking script, "conduct" should always be replaced with real verb}


%We measure how much the users learn with their change of confidence after using the tool and having experts to evaluate their responses.

% We propose \bass{}, an approach that combines prompt-based topic induction (fully automated) and active learning (fully manual) with human-in-the-loop to derive high-level topics from unstructured text data.
% %
% Our approach 1) iteratively selects the most informative document is based on previously induced topics through active learning 2) An \mm{} analyzes the document, its dominant topic keywords, and similar snippets from the same topic, then generates high-level concepts with descriptions and explanations; 3) A classifier--\textit{`bandit'}--, trained with user feedback, assesses the confidence of an \mm{}-induced topic. Users collaborate with \mm{}s in inducing topics, where \mm{}s generate concepts and users edit/approve them when the \textit{bandit} model detects low confidence.


% We create a single interface for \bass{} for users to conduct content analysis. \bass{} shows advantages compared to current topic model methods:
% \begin{enumerate}
%     \item \bass{} reduces the number of prompts needed for topic induction and can scale up for large text data.
%     \item \bass{} retain good quality concepts compared to fully prompt-based methods by traversing through a fraction of the most informative documents.
%     \item The \textit{bandit} model asks human assistance on low-confident induced topic and improves faithfulness of \mm{} outputs.
%     \item \bass{} generates high-level and more interpretable topics than classical topic models. It reduces users' mental effort for content analysis and help users capture meaningful concepts for data analysis and exploration.
% \end{enumerate}


%
% Additionally, \mm{}s can generate undesirable outputs that deviate from given contents and context~\cite{xu2024hallucination}, making the outputs unsatisfiable to users due to the lack of human intervention and evaluation during topic generation.
%
% Additionally, current prompt-based topic model evaluations focus on datasets that have generic one-word gold labels that often appear in the traditional topic model keyword lists such as \textit{Health, Defense, etc}~\cite{pham2024topicgpt}, which is okay and easy to prompt a \mm{} without human validation in the generation process. When it comes to datasets with more complex research goals, fully relying on \mm{} generated topics without human intervention or feedback can lead to undesirable outputs that deviate from given contents and context~\cite{xu2024hallucination}, making the outputs unsatisfiable to users due to the lack of human intervention and evaluation during topic generation. We focus on two datasets, where one dataset has generic labels for easy of comparison with existing work, another dataset is a teacher and student conversation in a math classroom to access teaching practices associated with overall high-quality math teaching~\cite{xu2024promises} (\abr{ncte}). \abr{ncte} has gold expert defined high-level concepts that require full understanding of education, teaching, and the dataset to derive, not just based on word frequencies-- Mathematical Language: captures how fluent teacher and students use mathematical language in a classroom. We have experts to evaluate outputs from \bass{}, other prompt-based methods, and \lda{} against the gold defined high-level concepts. 

% \citet{alto, li-etal-2024-improving} showed that combining topic modeling with active learning can reduce the number of documents required for topic induction. While topic models offer users a global overview of the entire dataset, active learning directs users' attention to the most informative text snippets. 

%These traditional topic models characterize topics as lists of most probable keywords (e.g., \textit{`health'}, \textit{`insurance'}\dots) and are popular for extracting low-level keywords from unstructured texts. 
%
%Users use the low-level topics to analyze text data and synthesize into high-level concepts that are more interpretable and descriptive to humans~\cite{Asmussen2019SmartLR}.
%
%However, traditional topic models can produce incoherent and less interpretable topics that are less useful for users to conduct data analysis~\cite{hoyle2021automated}.

%%%
% {\color{blue} In today's data-driven world, navigating and analyzing large datasets poses significant challenges. Bayesian-based topic models (\lda{}, \citealt{blei2003lda}) represent documents as mixtures of topics, with topics defined as probabilistic distributions over words under a Bag-of-Words (BoW) assumption. Recent neural approaches integrate pre-trained Transformer-based representations into an LDA-like process (\ctm{}, \citealt{bianchi2020pre}) or employ them in combination with clustering techniques (\bertopic{}, \citealt{bertopicMark}) . These methods --here classical topic models-- typically characterize topics as lists of words sorted by probability within the theme (e.g., \textit{`health'}, \textit{`insurance'}\dots), making them popular tools for extracting concepts from large collections of unstructured texts. However, despite their popularity and utility, they often produce topics that are less interpretable and less valuable to end-users~\cite{hoyle2021automated}.} 

% For example, the typical outputs of a topic from a classical topic model are \textit{`trade', `country', `export', `foreign', `cuba'}\dots, which requires users to analyze related documents and synthesize the topic keywords into abstract and concrete concepts that are easier for human to interpret-- \textit{"Trade: the exchange of goods or services with individuals, organizations, or foreign countries"}. 
% %
% Topic synthesis requires significant user efforts when the data size and number of topics get large. Nonetheless, classical topic models face the challenge of producing overly generic or specific keywords that are not useful to users~\cite{hoyle2021automated, li-etal-2024-improving}. 


%
% Users then face the task of manually analyzing these documents to synthesize the keywords into more interpretable concepts-- \textit{``Trade: the exchange of goods or services with individuals, organizations, or foreign countries''}. 
% Users then face the task of manually analyzing the  
%
% This manual process can be very time-consuming, especially when dealing with large datasets or high number of topics, and is made more challenging by the lack of guiding pointers to relevant documents.
%However, this process requires significant user efforts when dealing with large datasets or a high number of topics, especially when users have to manually pick documents to analyze without pointers.

% {\color{blue} For example, classical topic models might generate keywords like \textit{`trade', `country', `export', `foreign', `cuba'}\dots to identify a topic under which a set of documents are active. Yet, these keywords can sometimes be overly generic or specific and thus not very useful to end-users~\cite{hoyle2021automated, li-etal-2024-improving}. Users then face the task of manually analyzing these documents to synthesize the keywords into concepts that are more easily interpretable-- \textit{``Trade: the exchange of goods or services with individuals, organizations, or foreign countries''}. However, this process requires significant user effort, particularly when dealing with large datasets or a high number of topics.}

% \mm{}s such as GPT, Claude, LLaMA, Mistral~\cite{openai2024gpt4,touvron2023llama,jiang2023mistral} recently gain popularity for summarizing and inducing abstract concepts and topics from unstructured text data for their instruction following and text understanding ability.\footnote{\url{https://claude.ai}} 
% %
% \citet{pham2024topicgpt, reuter2024gptopic} iteratively prompt a \mm{} with random sampled documents to generate high level concepts, followed with merging similar topics, and assigning refined topic concepts to individual documents. \citet{lam2024concept} uses \mm{}s to extract key sentences from individual documents, summarize documents, then generate and synthesize concepts and rationales from summarized contents. 
% %
% These prompt-based concept induction techniques produce more high-level and human-interpretable concepts with descriptions than classical topic model keywords. 

% \mm{}s such as GPT, Claude, LLaMA, Mistral~\cite{openai2024gpt4,touvron2023llama,jiang2023mistral} have gained popularity for inducing topics from unstructured text data.\footnote{\url{https://claude.ai}} 
% %
% \citet{pham2024topicgpt} iteratively prompts GPT-4 with random sampled documents to generate high level concepts, followed with merging similar topics with GPT-3.5-turbo, and assigning refined topic to individual documents. 
% %
% \citet{reuter2024gptopic} introduces a method akin to BERTopic but allowing for a fixed number of topics and using agglomerative clustering, defining topics through titles, descriptions, assigned documents, and document embeddings, with RAG enabling dynamic interactions via a chat interface. 
% %
% \citet{lam2024concept} uses \mm{}s to extract key sentences from individual documents, summarize documents, then generate and synthesize concepts and rationales from summarized contents. 
% %
% Prompt-based techniques generate more human-understandable and descriptive concepts than traditional topic models.


% {\color{blue}
% \mm{}s such as GPT, Claude, LLaMA, Mistral~\cite{openai2024gpt4,touvron2023llama,jiang2023mistral} have recently gained popularity for inducing concepts or topics from unstructured text data.\footnote{\url{https://claude.ai}} 
% %
% \citet{pham2024topicgpt} iteratively prompt a \mm{} with random sampled documents to generate high level concepts, followed with merging similar topics, and assigning refined topic concepts to individual documents. \citet{reuter2024gptopic} introduce a method akin to BERTopic but allowing for a fixed number of topics and employing agglomerative clustering, defining topics through titles, descriptions, assigned documents, and document embeddings, with RAG enabling dynamic interactions via a chat interface. \citet{lam2024concept} uses \mm{}s to extract key sentences from individual documents, summarize documents, then generate and synthesize concepts and rationales from summarized contents. 
% %
% These prompt-based techniques generate more human-understandable concepts compared to traditional topic models.
% }

%
% However, current prompt-based topic modeling methods often face challenges at scaling up the data size that requires prompting the \mm{}s many times, which can be expensive in money and time. Additionally, \mm{}s can hallucinate and generate concepts or topics that deviate from given contents~\cite{xu2024hallucination}, and existing \mm{}-based approaches lack human supervision and evaluation of \mm{} outputs during the topic generation phase; the evaluations mainly rely on comparing generated topics with the \textit{'gold standard'} without using related documents to support the quality and validity of the generated topics. 
% %
% Humans want some trust in the \mm{} but not all to help them analyze unstructured data, and the interaction between \mm{}s and humans with thresholds becomes necessary, which human only supervise \mm{} outputs when necessary.

% {\color{blue} However, current prompt-based topic modeling methods face challenges in scaling up data size, requiring multiple prompts that can be costly in terms of both money and time. Additionally, \mm{}s can generate hallucinated concepts that deviate from given contents~\cite{xu2024hallucination}, making their output untrustworthy to end-users due to the lack of human supervision and evaluation during topic generation-- SOTA evaluations mainly rely on comparing generated topics with \textit{`gold standard'} labels without using related documents to support the quality and validity of the generated topics.  To build trust, it's crucial to establish thresholds for human-\mm{} interaction. This way, humans can supervise \mm{} outputs selectively, ensuring reliable analysis of unstructured data.
% }



% mainly rely on comparing generated topics with \textit{`gold standard'} labels without using related documents to support the quality and validity of the generated topics. To build trust, it's crucial to establish thresholds for human-\mm{} interaction, where humans can supervise \mm{} outputs selectively, ensuring reliable analysis of unstructured data.


%
% Then users can induce topics based on their interaction with topic model keywords, document clusters, and the selected document throughout the topic induction process.

% We leverage prompt-based topic induction approaches (fully automated with no human assistance) and active learning approach (fully manual) and propose \bass{}: a topic induction approach that integrates classical topic models, \mm{}s, and human to induce and learn high-level/abstract concepts from unstructured text data. 
% %
% \bass{} uses active learning to iteratively pick the most informative document based on previous induced topics. 
% %
% A \mm{} analyzes the document, its dominant topic keywords, and similar document snippets from the same topic, then generates high-level concepts with description and explanation.
% %
% Specifically, we introduce \textit{`bandit'}, a classifier trained with user feedback that checks the confidence of a \mm{} induced topic.
% %
% We have users collaborating with \mm{}s to induce topics, which \mm{}s are responsible for concept generation and users are responsible for editing/approving \mm{}-induced concepts when \textit{bandit} model detects low confidence. 

% {\color{blue} We propose \bass{}, an approach that combines prompt-based topic induction (fully automated) and active learning (fully manual) within a user-in-the-loop methodology to derive high-level concepts from unstructured text data. Our approach is three-fold: 1)  The most informative document is iteratively selected based on previously induced topics through active learning; 2) An \mm{} analyzes the document, its dominant topic keywords, and similar snippets from the same topic, then generates high-level concepts with descriptions and explanations; 3) A classifier--\textit{`bandit'}--, trained with user feedback, assesses the confidence of an \mm{}-induced topic. Users collaborate with \mm{}s in inducing topics, where \mm{}s generate concepts and users edit/approve them when the \textit{bandit} model detects low confidence.
% }

% We propose \bass{}, an approach that combines prompt-based topic induction (fully automated) and active learning (fully manual) with human-in-the-loop to derive high-level topics from unstructured text data.
% %
% Our approach 1) iteratively selects the most informative document is based on previously induced topics through active learning 2) An \mm{} analyzes the document, its dominant topic keywords, and similar snippets from the same topic, then generates high-level concepts with descriptions and explanations; 3) A classifier--\textit{`bandit'}--, trained with user feedback, assesses the confidence of an \mm{}-induced topic. Users collaborate with \mm{}s in inducing topics, where \mm{}s generate concepts and users edit/approve them when the \textit{bandit} model detects low confidence.


% We create a single interface for \bass{} for users to conduct content analysis. \bass{} shows advantages compared to current topic model methods:
% \begin{enumerate}
%     \item \bass{} reduces the number of prompts needed for topic induction and can scale up for large text data.
%     \item \bass{} retain good quality concepts compared to fully prompt-based methods by traversing through a fraction of the most informative documents.
%     \item The \textit{bandit} model asks human assistance on low-confident induced topic and improves faithfulness of \mm{} outputs.
%     \item \bass{} generates high-level and more interpretable topics than classical topic models. It reduces users' mental effort for content analysis and help users capture meaningful concepts for data analysis and exploration.
% \end{enumerate}

%{\color{blue} 
%We have integrated \bass{} within a single interface so users can conduct content analysis. Altogether, \bass{} offers several advantages over current topic modeling methods:
%\begin{itemize}
%    \item \bass{} reduces the number of prompts needed for topic induction and can scale up for large datasets.
%    \item Compared to fully prompt-based methods, \bass{} maintains high-quality concepts by analyzing only a fraction of the most informative documents.
%    \item The inclusion of the \textit{bandit} model prompts human assistance for low-confidence-induced topics, enhancing the faithfulness of \mm{} outputs.
%    \item \bass{} generates high-level and more interpretable topics than classical topic models, reducing users' cognitive load for content analysis and enabling them to capture meaningful concepts for data exploration and analysis.
%\end{itemize}}


% These topic keywords overlooks the abstract concepts that can be derived by looking at the document clusters and keywords, which are more useful for human to conduct content analysis to understand what is in the documents sets, such as \textit{`Trade: mentions the exchange of goods with other parties or foreign countries'}.


% They are popular for various applications in various fields, from text applications such as data exploration, content analysis to dimensionality reduction for classification. However, the power of topic models can be limited by their unsupervised nature, where the final clusters are simply based on word frequency distributions or pre-trained word embedding similarities. 

% where there is no human intervention or control of the generated topics; the final clusters are simply based on word frequency distributions or pre-trained word embedding similarities. On the other hand, although supervised topic models and labeled topic models can form topics based on pre-labeled human input labels, but they require users to first iteratively go through the documents and start making label categories from scratch, which can be labor-intensive and time-consuming. 

% Topics convey important information for data analysis and exploration, where users get a global sense of various topics in the documents, and analyzing related documents within different topics. The process of exploratory data analysis (\abr{eda}) can be started from the topics and analyze the documents, or from analyzing the documents to forming the topics depending on the purpose of the users. With the help of powerful large language models, we list out several contributions:

% \begin{itemize}
%     \item A topic model technique that forms topics by actively selecting documents from a set of documents, with large language models generating a generalizable topic based the actual contents of the documents rather than pure word distributions
%     \item We use active learning to greatly reduce the number of documents needed for prompt the \abr{llm} to generate topics
%     % \item Human supervised topic formation process: users can interrupt the process of topic formation by the \abr{llm} to supervise the quality of the new-formed topics
%     \item Evaluation: we propose new automatic evaluations to evaluate between classical topic models and \abr{llm} topic model
%     \item We conduct human ratings for the output topics with most associated formed
%     \item Our propose topic model combines the advantage of both classical topic models (access to document topic distribution that represent the lower level features of the documents) and topic models using \mm{}s (have a specific description for the topics instead of a list of keywords, where the topics are generated based on the actual contents from the documents.)
% \end{itemize}
