% \jbgcomment{For all of the plots, we should capitalize the conditions in the legends}

\begin{table*}[t]
    \centering
    \tiny
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{3.5pt}  % Slightly reduced spacing to fit new column
    \begin{tabular}{l|cccccc|cccccc}
    \toprule
    \multirow{2}{*}{\textbf{Metric}} & 
    \multicolumn{6}{c|}{\textbf{\texttt{Bills}}} & 
    \multicolumn{6}{c}{\textbf{\texttt{Sci-fi}}} \\
    \cmidrule(lr){2-7} \cmidrule(lr){8-13}
    & \textbf{\abr{lda}} & \textbf{TopicGPT} & \textbf{TopicMistral} & \textbf{TopicLLaMA} & \textbf{LLooM} & \textbf{\bass{}} &
      \textbf{\abr{lda}} & \textbf{TopicGPT} & \textbf{TopicMistral} & \textbf{TopicLLaMA} & \textbf{LLooM} & \textbf{\bass{}} \\
    \midrule
    Purity & \textbf{0.70} & 0.52 & 0.75 & 0.53 & 0.23 & 0.54 & 
             \textbf{0.63} & 0.35 &  0.44 & 0.26   & 0.26 & 0.28\\
    ARI & 0.27 & 0.23 & 0.09 & 0.21 & 0.16 & \textbf{0.45} & 
          \textbf{0.18} & 0.14 &  0.08 & 0.11 & 0.11 & \textbf{0.18} \\
    NMI & 0.47 & 0.42 & 0.13 & 0.39 & 0.14 & \textbf{0.54} & 
          0.56 & 0.18 & 0.06 & 0.17 &  0.19 & \textbf{0.69} \\
    % \midrule
    % Num Topics & 65 & 16 + 96 & 44 &  46 & 
    %          65 & 1 + 2 &  & 269 & 53
    Num Topics & 65 & 73 & \textbf{317} & 118 & 44 & \(\bar{u}\)=46 & 
          65 & 4 & 33 & 21 &  \textbf{269} & \(\bar{u}\)=53 \\
    \bottomrule
    \end{tabular}
    %\jbgcomment{The captions should stand on their own if you read them front to end.  This caption should talk about how these are the traditional, automatic, label-centric evaluation.  I think you also need to emphasize that \bass{} is our introduced method.}
    \caption{
       Traditional, automatic, label-centric clustering metrics (Purity, ARI, NMI) and number of topics for each model on the \texttt{Bills} and \texttt{Sci-Fi} datasets. In \lda{}, the number of topics is predefined, while in fully prompt-based models (\topicgpt{} and \lloom{}) it is discovered automatically. For \bass{}, we report the average number of topics generated across a 15-user sample per dataset with standard error. \bass{}, our proposed method, achieves competitive results, outperforming other models on most metrics. \lda{} performs well overall, \topicgpt{} generates generic topics---especially on the Sci-fi dataset---and \lloom{} shows lower scores. TopicMistral and TopicLLaMA uses the same pipeline as TopicGPT, but with local Mistral-7B-Instruct and LLaMA-2-70B. Small local \mm{}s have the lowest cluster scores than large local \mm{}s, which is comparable but still worse than close-source GPT models.
    }
    %\caption{\abr{lda} and \bass{} (Over a 15 user sample per dataset with standard error rate difference) win on overall clustering than \topicgpt{} and \lloom{}. Due to scaling issues, fully prompt-based models produce lower clustering metric results than \abr{lda} or semi-supervised topic models. \topicgpt{} only generates one generic topic and three subtopics for domain specific Sci-fi (Details in Section~\ref{qualitative}).}
    \label{tab:cluster_metrics}
\end{table*}


% \subsection{Participant Recruitment}
We recruit \(15\) users from Prolific for each control group and dataset, totaling \(120\) users. 
%
The recommended study time was \(45\) to \(60\) minutes, and each user could participate only once.
%
%Each user can only participate the study only once.
%We hire 15 users from Prolific for each group for the Bills and synthetic news dataset, a total of 120 users.
%
All annotators have an approval rate of at least $99\%$ and a minimum of \(30\) previous submissions.
%
%We had different topical requirements for the two datasets: 
Annotators have social science background for Bills and English literature background for Sci-Fi.
% The datasets had different topical requirements: a social science background for \texttt{Bills} and proficiency English for \texttt{Sci-fi}.
%All the annotators participating in the Bills dataset requires to have a social science background or degree and no specific expertise is required for the synthetic news dataset.
%
%We filter annotators to have at least $99\%$ approval rate and 30 previous submissions.
%
% \jbgcomment{I'd move this to the ethics section if we're short on
%   space, but I'd put the \$17 first.  In any event rewrite so it's not
%   passive voice.}
%
% Participants are compensated a base rate of
% $\$6.5$, which could increase to $\$17$ per hour if their answers are
% deemed unlikely to be AI-generated and showing they have done the
% task. We use a fine-tuned \textsc{RoBERTa}~\cite{sivesind_2023} to
% reject users likely submitting AI-generated responses.
% %
% They are provided with task instructions at the beginning
% (Appendix~\ref{tab:congressional-bills-study},~\ref{tab:news-post-analysis})
% and can quit the study at any time.
% %
% Upon completion, users are prompted with two survey questions to rate
% their experience using the tool (Appendix~\ref{fig:survey_ratings}).
%
The rest of this section analyzes the results from the user study.

% We present both quantitative and qualitative analyses of each topic model's effectiveness for content analysis.
%We provide quantitative and qualitative analysis of each topic models' effectiveness for content analysis.

% \jbgcomment{I don't see figures.py here.  Is this plotnine?}

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/bills_boxplot_v2.pdf}
%         \caption{\texttt{Bills}}
%         \label{fig:boxplot_bills}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/news_boxplot_v2.pdf}
%         \caption{\texttt{News}}
%         \label{fig:boxplot_news}
%     \end{subfigure}
%     \caption{\abr{lda} remains competitive to other \mm{}-based topic models on real (Bills) dataset. Users with no tools have the lowest knowledge gain.}
%     \label{fig:boxplots}
% \end{figure*}

% \begin{table*}[htbp]
% \centering
% \small
% \begin{minipage}{0.48\textwidth}
% \centering
% \begin{tabular}{lllcc}
% \hline
% Group 1 & Group 2 & Mean Diff & p-adj & Reject \\
% \hline
% \rowcolor{yellow!20} baseline & bass & -0.85 & 0.00 & True \\
% \rowcolor{yellow!20} baseline & lda & -0.85 & 0.00 & True \\
% baseline & lloom & -0.27 & 0.35 & False \\
% \rowcolor{yellow!20} baseline & topicgpt & -0.79 & 0.00 & True \\
% bass & lda & 0.00 & 1.00 & False \\
% \rowcolor{yellow!20} bass & lloom & 0.58 & 0.00 & True \\
% bass & topicgpt & 0.06 & 0.99 & False \\
% \rowcolor{yellow!20} lda & lloom & 0.59 & 0.00 & True \\
% lda & topicgpt & 0.06 & 0.99 & False \\
% \rowcolor{yellow!20} lloom & topicgpt & -0.52 & 0.00 & True \\
% \hline
% \end{tabular}
% \caption{Tukey's HSD Test (Bills)}
% \label{tab:tukey_hsd_1}
% \end{minipage}%
% \hfill
% \begin{minipage}{0.48\textwidth}
% \centering
% \begin{tabular}{lllcc}
% \hline
% Group 1 & Group 2 & Mean Diff & p-adj & Reject \\
% \hline
% \rowcolor{yellow!20} baseline & bass & -0.76 & 0.00 & True \\
% \rowcolor{yellow!20} baseline & lda & -0.51 & 0.01 & True \\
% \rowcolor{yellow!20} baseline & lloom & -0.64 & 0.00 & True \\
% \rowcolor{yellow!20} baseline & topicgpt & -0.87 & 0.00 & True \\
% bass & lda & 0.25 & 0.39 & False \\
% bass & lloom & 0.12 & 0.95 & False \\
% bass & topicgpt & -0.12 & 0.91 & False \\
% lda & lloom & -0.13 & 0.93 & False \\
% lda & topicgpt & -0.36 & 0.08 & False \\
% lloom & topicgpt & -0.23 & 0.60 & False \\
% \hline
% \end{tabular}
% \caption{Tukey's HSD Test (News)}
% \label{tab:tukey_hsd_2}
% \end{minipage}
% \label{tab:tukey_hsd_all}
% \end{table*}


% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/answer_consistency_scores.pdf}
% %     \jbgcomment{You need to say what the figure is representing:

% %     To test whether users have consistent answers (with each other? to the reference?), we compute the similarity\dots .  

% %     There's a mismatch between "consistency" in the text and "similarity" in the figure; if you want to make this distinction, it needs to be explained.
% % }

% %     \jbgcomment{You can probably make this a one figure faceted figure to save space.}
    
%     \caption{Users with topic model tools have higher answer consistencies than baseline users. \bass{} users have the highest answer consistency on Bills and TopicGPT has the highest answer consistency on the News dataset, but on average, prompt-based methods have higher answer consistency.}
%     \label{fig:answer_consistency}
% \end{figure*}


\subsection{Topic Clustering Metrics}
Evaluating cluster similarity is challenging~\cite{NIPS2002_43e4e6a6}.
%
Rather than comparing text-based summaries of topics---since \lda{} produces keywords and \topicgpt{} and \lloom{} generate phrases, which may be more fluent but not necessarily more useful, we focus on cluster assignments.
%
Specifically, we evaluate document assignment to each topic (cluster) based on a ground truth partition to measure the similarity between the ground truth partition and the
topics produced by a model.
%
%We are not focusing on the text-based summary of topics, as \abr{lda} generates keywords while \topicgpt{} creates phrases: the phrases are going to be more fluent but perhaps not more useful.
%
%Instead, we are going to focus on which documents are in which cluster
% partition (topic) based on a ground truth partition.
%
% \jbgcomment{Should have citations for the original metrics too}
% To measure the similarity between the ground truth partition and the
% topics produced by a model, we use several metrics:
Purity~\cite{purity}, adjusted
rand index (ARI)~\cite{ARI}, and normalized mutual information (NMI)~\cite{NMI} have
become common alternatives to traditional coherence measures in recent topic modeling evaluations~\cite{pham2024topicgpt,li-etal-2024-improving, angelov-inkpen-2024-topic}.
%
% \abr{lda} on 85 topics, it achieves the highest purity on both datasets compared to other models. 
%
% \lorenacomment{I don't think this is the right place for it. LDA's K is a setting, and here we're talking about results} Since \abr{lda} requires predefined number of topics, we use 65 to be the topic number, which is similar to the number of gold topics from the dataset.
%
Focusing on a single metric may introduce bias, as each can be manipulated:
%
for example, purity measures how well a clustering algorithm groups
similar documents by comparing clusters to ground truth labels but it can be exploited by assigning a unique label to each document.
%
Using all three metrics together provides a more balanced evaluation of how well the topics align with the gold topics.
%


% Since \abr{lda} has more topics than all other methods, it results in higher purity (Table~\ref{tab:cluster_metrics}).
% \lorenacomment{Why LDA generates more topics than all other methods? You can specify how many topics LDA generates... Wouldn't it be better something like ``Since the number of topics we train LDA with (equal to the number of categories) is larger than those discovered by LLoom or TopicGPT, ...''}
%
Table~\ref{tab:cluster_metrics} presents the clustering evaluation metrics for all methods. \bass{} scores the best overall, demonstrating that human-supervision
during the topic generation process combined with active learning to
refine the original \abr{lda} cluster distribution enhances clustering performance. Notably, users only need to label around 50 documents.
%
\lloom{} fundamentally differs from the other algorithms, resulting in the lowest overall clustering scores.
%(Table~\ref{tab:cluster_metrics}).
%
\lloom{} uses HDBSCAN~\cite{10.1007/978-3-642-37456-2_14} to cluster
\mm{}-extracted summaries and using \mm{} for topic synthesis,
its topics are the result of a series of high-level summarization and data
transformations. This process introduces information loss, which lowers clustering evaluation metrics, even though its topics remain reasonable and appealing.
%
\topicgpt{} performs matches \lda{} on \texttt{Bills}---a benchmark from its original study---but fails to achieve similar results on \texttt{Sci-Fi}.
%
Its tendency to generate overly generic topics limits its usefulness for exploring domain-specific data, as shown in (Section~\ref{qualitative}; Appendix~\ref{app:exmaple_generated_topics}).
%\topicgpt{} appears to generate overly generic topics, showing limitedvalue to users exploring domain-specific data, as discussed further with examples in Section~\ref{qualitative}.

% We analyze this divergence from the original claim that \topicgpt{} reaches the best clustering results than \abr{lda} and BERTopic with a detailed error analysis in Section~\ref{qualitative}.
%


Additionally, we run the \topicgpt{} pipeline using 
Mistral-7B~\cite{jiang2023mistral} and LLaMA-2-70B~\cite{touvron2023llama}.
%
Mistral struggles at topic generation and merging tasks and has the lowest overall clustering metrics, while LLaMA has comparable but still worst clustering metrics than GPT-4 (Table~\ref{tab:cluster_metrics}).
%
Local \mm{}s, particularly smaller \mm{}s struggle at reasoning and describing the contour map of a dataset, thus we omit using local \mm{}s as one of our user study groups.\footnote{Local \mm{}s are not included in the user study.}

% \zongxiacomment{Later add strength and weaknesses of topicgpt, lloom}

\subsection{Automatic Evaluation Metrics}
In data exploration, using a common tool for analyzing the same dataset improves reproducibility by ensuring consistent methodologies and conclusions~\cite{NationalAcademies2019}.
%In standard data exploration, researchers analyzing the same dataset can improve reproducibility by using a common tool that promotes consistent methodological approaches and research question analysis.
%
%Rigorous science demands reproducible methods and consistency among data exploration conclusions, as non-reproducible research lacks scientific rigor~\cite{NationalAcademies2019}.
%
To evaluate how these tools could support this data exploration, we compare user responses across groups using two metrics: \textit{Answer consistency} and \textit{Answer quality} (Section~\ref{study_setup}).
%Thus, we evaluate the \textit{consistency} and \textit{answer quality} to compare user responses across groups (Section~\ref{study_setup}).
%
\textit{Consistency} quantifies the similarity of responses within a group, with higher similarity indicates better incorporation of corpus information.
%
\textit{Answer quality} measures the alignment between user responses and ideal answers.
%
Both metrics are computed using \textsc{all-mpnet-base-v2}.\footnote{\url{https://sbert.net/docs/sentence_transformer/pretrained_models.html}}
%

%\textit{Consistency} measures how similar and consistent among all the
%users using a tool (under the assumption that if the answers become
%more consistent, they have incorporated more information from the
%underlying corpus); \textit{answer quality} measures how similar are
%he user answers to the given ideal answers.
%
%Consistency is the cosine similarity of all answer paris within an
%experimental group (Section~\ref{study_setup}) using
%\textsc{all-mpnet-base-v2}.\footnote{\url{https://sbert.net/docs/sentence_transformer/pretrained_models.html}}
%
%\textit{answer quality} is the cosine similarity between the
%reference answer averaged over each user's answers.
%
%
% Specifically, the previous section shows that there is no significant difference of user backgrounds in the pretest stage, we aim to answer the following question: Is the pairwise answer similarity significant among groups in the post test? 
% %
% We assess whether one method leads to greater improvements in answer consistency after participants are exposed to the data.
% We compute the median transformer similarity per question and the mean across all questions for each group in Figure~\ref{fig:automatic_evaluation}.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/consistency_quality_combined_difference_boxplots.pdf}
    \caption{
    No significant differences exist among groups, except for \texttt{Sci-Fi} answer quality (one-way ANOVA, \(p < 0.05\)).  
     %There is no statistical significance among groups except\texttt{Sci-Fi} answer quality using a one way ANOVA test at significance $p$ value (0.05). 
    %The $p$ values on top of each boxplot is one-sample $t$-tests comparing each group's differences (post vs. pre) against 0, where all groups except \lloom{} on answer consistency show an increase of answer consistency and quality after users use a tool.
    The \(p\)-values above each boxplot correspond to one-sample \(t\)-tests comparing each group's pre- vs. post-task differences against 0. All groups, except \lloom{} on answer consistency, show increased answer consistency and quality after using the tool.
    %
    Overall, \mm{}-based methods do not show significant advantages over \abr{lda} on automatic evaluations. %, but human raters \emph{prefer} responses generated by users using \mm{}-based models (Figure~\ref{fig:bradely_terry}).
    % \zongxiacomment{P value and test statistics}
    % On the other hand, \bass{} users show the highest answer quality gain on Sci-fi datasets.
    }
    \label{fig:automatic_evaluation}
\end{figure}
% In real life datasets (\texttt{Bills}), \bass{} has the highest average topic relevance scores and answer consistency scores, while \abr{lda} is the best on synthetic \texttt{Sci-fi} data.
%

Figure~\ref{fig:automatic_evaluation} shows the results for \textit{Answer consistency} and \textit{Answer quality}.
%as the average similarity of all answer pairs per group and \textit{Answer quality} as the similarity between the reference answer and group-averaged responses.
%
All tools help improve answers, with a statistically significant increase from pretest to posttest across groups. 
%with a statistically significant increase from pretest to posttest across groups.
%
Users' answers become both more consistent and better aligned with reference answers after using the tools. % (Figure~\ref{fig:automatic_evaluation}).
%
However, 
%a one-way ANOVA test shows 
there are no significant difference between tools, indicating all topic models are similarly effective in helping users improve their answers in terms of {\it Consistency} and {\it Answer quality}.
%there is not a significant \emph{difference} between tools
%under a one way ANOVA test.
% that fully \mm{}-based methods are better for exploration than \abr{lda}.
%

% For both datasets per question, groups with topic models show higher average similarity scores than the baseline group in most cases.
% %
% \bass{} on average has the highest similarity per question on Bills while \topicgpt{} has the highest on News, implying that different users under the help of \mm{}s on average have more answer consistency than traditional topic models or without any tools.


\subsection{\texttt{Pairwise Response Preference}}
% We use the refined evaluation rubric from Table~\ref{tab:rubric} to compare the user responses generated under different conditions for the same questions.
% %
% We hire crowdsource annotators and present them with the scoring rubric, the question, the reference answer, two user responses from different groups, and the dataset to search for relevant documents by topics.
% %
% We filter annotators by an English reading related or social science background and choose their preferred response.
% %
% Specifically, the answers are randomly shuffled for each question so annotators will not know the answer groups.
% %
% We have three annotators for each dataset to annotate the data, and use the majority vote as the gold preference with a Krippendorf's alpha~\cite{castro-2017-fast-krippendorff} score of \(0.73\) for Bills and \(0.76\) for Sci-fi
% %
% We use pairwise preference with Bradley Terry Model ranking to measure the strength and likelihood users prefer responses for one topic model than the other (Figure~\ref{fig:bradely_terry}).
We use the refined evaluation rubric from Table~\ref{tab:rubric} to
compare user responses generated under different conditions for the
same questions.
%
Prolific annotators are hired and provided with the scoring rubric,
the question, the reference answer, two user responses from different
groups, and the dataset to search for relevant documents by topic.
%
Annotators are filtered based on an English or Social Science
background and tasked to select their preferred response.
%
We randomly shuffle responses for each question so annotators cannot
identify the source groups.
%
% To ensure fairness, the responses are randomly shuffled for each question so annotators cannot identify their source groups. 
%
Each dataset is annotated by three annotators, and use the majority
vote as the gold preference, with a Krippendorff's
alpha~\cite{castro-2017-fast-krippendorff} score of \(0.73\) for \texttt{Bills}
and \(0.76\) for \texttt{Sci-Fi}.
%
% \jbgcomment{Did you cite BT the first time you used it?}
We use a Bradley-Terry Model, which measures the probability of responses from a group winning in pairwise comparisons, to rank users favoring responses of one group over another
(Figure~\ref{fig:bradely_terry}).

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/terry_strength.pdf}
    \caption{\bass{} has the highest preference strengths than other groups on both datasets, where users using \bass{} generate better responses than unsupervised topic models. However, the position of \topicgpt{} swaps from \texttt{Bills} to \texttt{Sci-Fi}, where \topicgpt{} only generates one super topic and three subtopics for \texttt{Sci-Fi} (Section~\ref{qualitative}).}
    \label{fig:bradely_terry}
\end{figure}

% Each annotator only needs to annotate one dataset, which we have two annotators each dataset, and four annotators in total, which a Krippendorf's alpha~\cite{castro-2017-fast-krippendorff} score of \(0.63\) for Bills and 0.74 for Sci-fi.
%
% The likert scales measures how much a user learns from the dataset after exploring it. 
%
% For a given user, we define the user's knowledge score as: %over a set of questions as: 
% \begin{equation}
% \bar{r} = \frac{1}{N} \sum_{i=1}^{N} r_i
% \end{equation}
% where $r_i$ is the average rating between two annotators for question $i$, and $N$ is the number of questions. Before drawing conclusions on the information gain, we first calculate the inter-annotator agreement. Achieving Krippendorf's alpha~\cite{castro-2017-fast-krippendorff} scores of \(0.63\) and \(0.51\) for Bills and Synthetic news datasets, respectively, we can confirm a reasonable level of agreement between annotators. 
%
% The average pre-test and post-test $\bar{r}$ scores across all users (Fig.~\ref{fig:boxplots}) show information gain for all groups and datasets. 
% %
% To assess the difference of information acquisition for different groups, we analyze the following questions.

% \paragraph{1. Are there significant differences in mean pre-test scores across groups?} 

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/score_differences_boxplot.pdf}
%         % \caption{\texttt{Bills}}
%         \label{fig:boxplot_bills}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/news_boxplot_v3.pdf}
%         % \caption{\texttt{News}}
%         \label{fig:boxplot_news}
%     \end{subfigure}
%     \caption{Users with no tools have the lowest knowledge gain, but \abr{lda} remains competitive to other \mm{}-based topic models for information acquisition.}
%     \label{fig:boxplots}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth]{figures/score_differences_boxplot.pdf}
%     \caption{The likert scale difference between pretest and post test do not show a significant gap between \abr{lda} and other \mm{} based approaches. Although \topicgpt{} has a relative high mean and median on sci-fi, users in that group have more negative scores on average than other groups, which is due to \topicgpt{} generating overaly generic and less useful topics (Section~\ref{qualitative}).}
%     \label{fig:boxplots}
% \end{figure*}

\paragraph{Do \mm{}s help users generate more preferable responses than traditional topic models?} 
%
Depending on the contents of the datasets, \mm{}-based methods do
not always help users generate preferable answers
(Figure~\ref{fig:bradely_terry}).
%
The standard Bills dataset is a well-organized corpus with documents
that have clearly identifiable topics:
%
% \jbgcomment{Should we ask the models if they know the top-level labels in their parametric memory?}
%
\mm{}s readily distinguish
between different documents or \mm{} have strong parametric memory, and \mm{}s can effectively replace human
annotators in identifying the distinct topics within the dataset.\footnote{We ask GPT-4 on topics about Bills and alien science fiction without providing it any documents. over 95\% topics GPT-4 generates are in the Bills gold topics, but none of the topics GPT-4 generates are in the sci-fi gole topics (Appendix\ref{sec:parametric}).}
%
For easy data sets that need simple classification, all user groups involving \mm{}s generate
better responses than keyword-based topic models (\abr{lda}),
with higher Bradley-Terry probabilities.
%
The Sci-Fi dataset is domain specific, and the gold topics are abstract and not
readily surfaced by \mm{}s: \textit{Ethics and morality: Delving into the moral and ethical dilemmas that arise from encountering a non-human intelligence...}
%
Here, \topicgpt{} falters: \colorbox{green!15}{it only generates one generic topic} \textit{Science and Technology: Involves the study and application of scientific advancements.} not
quite related to the corpus and three nonspecific subtopics due to its
inflexible prompt formats and prompt pipelines. 
%
Unsupervised \mm{}s like \topicgpt{} are sensitive to prompt templates, leading to hallucinations such as \colorbox{green!15}{generating \textit{agriculture} topics on a math dataset} (Example in Appendix~\ref{app:exmaple_generated_topics}).
%
Thus, unsupervised \mm{} algorithms may be unstable on different domain datasets.
%
However, under human supervision of the generation of \mm{} topics,
\mm{} helps users answer questions better about both datasets
(Figure~\ref{fig:bradely_terry}).
%
The next section provides a detailed qualitative analysis of \bass{},
unsupervised \mm{}-based methods, and \abr{lda} while also suggesting best
practices for data exploration.

% \paragraph{Is the knowledge gain significant among groups?} 
% We conducted a one-way ANOVA to compare pre-test scores across the groups. The results (\(F(4,N)_{Bills} = 0.906, p=0.466\) and \(F(4,N)_{Sci-fi}=0.78, p=0.55\)) show no significant differences at $p=0.05$, indicating comparable starting knowledge levels across all groups.
% %
% A linear regression model was applied to evaluate the differences between pre- and post-test scores within each group (see Table XX), confirming that the observed gains were not due to random variation.
% %
% There is no significant difference of posttest and pretest human ratings, showing that \mm{}-based topic models do not generally help users learn more in-depth knowledge from the datasets, although \mm{}-based models generate more direct topic phrases that seem easier to read.
% %
% Even 20 years on, \abr{lda} is still an effective tool for data exploration and analysis.

% \lorenacomment{Add tables with coefficients and p-values when baseline is ready.}

% \paragraph{3. Is the knowledge gain in one group better than in the others?} Pairwise t-tests were applied (see XX), but no statistically significant differences were found between the groups. Hence, although some models appear to perform better than others based on the visual data, there is no statistical evidence to support the claim that one model is superior.
% There is {\bf no} significance between groups using topic models and the baseline group.
% \lorenacomment{I think we have a problem here}

%\lorenacomment{Add values when baseline is ready. Include pairwise t-test matrix?}



% \paragraph{Users without tools have the lowest information gain among all groups, but \abr{lda} still has strong knowledge gain compared to other models with \mm{} involved.} 
% However, the variances of the pretest scores in the \abr{lda} group is quite large compared to that of the other groups, which indicates that some users using \abr{lda} have more expertise than other users and resulting in a higher median ratings than users from other groups. 
% The baseline group has the lowest gains of post-test average scores among all the groups, and lowest score difference between pretest and post-test (Table~\ref{fig:boxplots}), suggesting the helpfulness of topic models for data exploration.
%
% \zongxiacomment{Add the regression results here to see whether the results are actually reliable}.
%
% On the other hand, there is no large gap between \abr{lda} information gain and prompt-based methods and \bass{}.
%




% We compute one way \abr{anova} for post test pairwise similarities for both datasets and show that there is a significant difference among the five groups with $p$ values 0.019 (Bills) and 0.0 (News), showing a statistical significance of answer consistency among groups.
%
% We run a Tukey's HSD test to examine the significant pairs for pretest and post test (Table~\ref{tab:tukey_hsd_1}, ~\ref{tab:tukey_hsd_2}).
%
% The baseline group shows significantly different similarity compared to groups using topic models for both datasets.
%
% The baseline shows negative mean differences when compared to other groups, indicating lower answer similarity scores. 
% %
% This suggests that topic models contribute to increased consistency in user responses.
% The pretest pairwise similarities are not significantly different between baseline groups and groups with topic models (all scores are low and not significantly different), but becomes significant different between baseline and groups with topic models, where the mean difference is all negative (baseline group has lower consistency).\ahintext{Yeah, I would drop discussion of pretest results here. The question is whether posttest consistency is higher for one model than the others.}
% User responses with higher similarity scores suggest more consistent learning outcomes and reduced variability in understanding derived from the dataset.
% %
% Users using no tools have higher inconsistent answers on post-test answers than users using any of the topic models. 
%
% Although not significant, users answer with higher consistency when we examine ratings for individual answers when using \bass{} (Figure~\ref{fig:answer_consistency}), and show positive mean difference compared to \abr{lda}, \topicgpt{}, and \lloom{}. 

% prompt-based methods all results in an average higher scores than \abr{lda} on the synthetic news dataset when the variance of user expertise on a dataset is lower, which indicates that prompt-based methods help generate more descriptive topics that can help users understand a dataset.

% In the Bills data, although \bass{} and TopicGPT both have the same medians for post tests, the user scores are less variant on \bass{}, which users who use their own created topics to understand a dataset...


% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/Bills_and_News_mean_similarity_heatmap.pdf}
%     \caption{Users with topic model tools have higher answer consistencies than baseline users. \bass{} users have the highest answer consistency on Bills and TopicGPT has the highest answer consistency on the News dataset, but on average, prompt-based methods have higher answer consistency.}
%     \label{fig:answer_consistency}
% \end{figure*}



% \paragraph{Hypothesis: \mm{} have self preference and generate better topics on synthetic data. But hard to reason real world data that requires domain expertise}
%\zongxiacomment{Calculate the regression coefficient, and do statistical significance test...}

% \begin{table}[!t]
% \small
% \centering
% \begin{tabular}{ccc}
% \hline
% % \rowcolor{gray!50}
% {\bf Method} & {\bf Bills} &  {\bf News}  \\ \hline
% \lda{} & 0.31 & 0.39 \\ 
% \lloom{} & 0.35 & 0.41 \\ 
% \topicgpt{} & 0.32 & \textbf{0.56} \\
% \bass{} & \textbf{0.38} & 0.47 \\
% \hline
% \end{tabular}
% \caption{Average sentence transformer pairwise similarity within each group. Users use prompt-based models have higher answer consistency than users using classical topic models, with baseline users having the least consistent answers.}
% \label{tab:cost}
% \end{table}

%\subsection{Topic Coverage Analysis}
%We calculate the coverage rate of topics generated by different user groups compared to the gold labels.

% \zongxiacomment{(from Lorena) I moved this here from the Qualitative Analysis, it makes more sense here, I think}

%\subsection{Cost}

%


%In many of the cases on content analysis, cost is also an important factor social scientists consider depending the availability of resource they have access to.
%
%We discuss the time cost and monetary cost for each of the methods.
% \zongxiacomment{Make a table for costs}
%\abr{lda} is the most efficient and cheapest method among the four methods. 
%
% \zongxiacomment{(from Lorena): Not sure where to put this `` the length of the study participation is ranging from 40 minutes to 75 minutes for most participants. Also I think we should include somewhere the average time taken by group and dataset, maybe in Table 3 as Average Study Time (Bills) and Average Study Time (News)?''}
% \paragraph{Monentary cost}

% \begin{figure*}[t]
% \jbgcomment{Can save space by making this a vertical faceted figure}
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/bills_survey2.pdf}
%         \caption{\texttt{Bills}}
%         \label{fig:boxplot_bills}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/news_survey2.pdf}
%         \caption{\texttt{News}}
%         \label{fig:boxplot_news}
%     \end{subfigure}
%     \caption{Users prefer using the topics they create themselves \bass{} themselves to use to answer questions on average on Bills dataset. Users depend more on prompt-based model generated topics than \abr{lda} to explore the dataset. 
%     % \ahintext{Size of text could be increased, I think you should say "Survey ratings of X"}
%     }
%     \label{fig:survey_ratings}
% \end{figure*}






%%%%%%%%%%%%%%%%%
