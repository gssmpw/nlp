%In this section, we introduce the basic setup of the user study, including the datasets we use, control groups, and the pre-test and post-test questions.


% \jbgcomment{The section should be renamed something like "Measuring what Users Learn from Interacting with Data"}

% \jbgcomment{I'm not 100\% sure, but I \emph{think} this should come before the previous section}

% \jbgcomment{Can we rename / renumber the files to match the paper sections}

% \jbgcomment{Let's bring back the running example here to motivate the introduction: our hypothetical user wants to understand land management, how can we measure how well they accomplished that goal}

Suppose a researcher wants to
understand US policy on land management. How can we measure how
effectively they accomplish the goal?
%Suppose our hypothetical legislator from the introduction wants to understand common policy actions taken by governments in the United States to manage land use for a legislation draft, how can we measure how well they accomplish the goal effectively?
%
% Our goal is to conduct an end-to-end evaluation of content analysis by assessing the effectiveness of various topic modeling tools.
%Our goal is to do an end to end evaluation of content analysis-- how effective is various topic model tools? 
%
We need to ensure answers are faithful, comprehensive, and link
multiple documents while maintaining consistency.
%We want to ensure the answer to be faithful and comprehensive that links multiple policies, while insights are consistent independent of specific legislators.
%
With this in mind, we use a pretest--posttest method: users
answer questions before and after interacting with the models.
%
The better they answer the questions, the more the models helped them
ingest the key themes of the dataset.
%, asking the same questions before and after users interact with the topic model and datasets. 
%
The pretest is critical to control for users with prior expertise (or
skill in making up convincing-sounding answers) who can answer
question well without assistance.

% To validate the effectiveness of our topic modeling approach, we employ a rigorous experimental design that tests users' conceptual understanding through pre- and post-test stages, carefully controlling for prior knowledge by asking identical questions before and after dataset exploration.


% We aim not only to ensure that answers and analyses are faithful to the data, linking multiple policies to provide comprehensive perspectives, but also to produce stable and consistent analytical information, regardless of which legislators are involved. 
% %
% This is crucial for downstream applications, such as proposing new laws that efficiently manage land use while protecting wildlife.
% %
% Thus, we use conceptual question to test what users learn from a dataset after using a topic model tool.
% %
% Specifically, we divide the study into pretest and post test stages, where users are asked the same questions before and after seeing the datasets to avoid outlier users who already have perfect knowledge about the datasets and answer the questions before they see the dataset.

%We not only want to ensure that answers and analyses derived are faithful to the data sources, linking multiple policies to provide comprehensive perspectives. 
%
%Moreover, we aim to produce stable and consistent analytical information, regardless of which legislators are involved, which is crucial for downstream applications, such as proposing new laws that efficiently manage land use while protecting wildlife.





% \zongxiacomment{Not sure whether to put the formulas for pairwise similarity and formulas for information gain here or in later section}
% \lorenacomment{I think I'd put it here.}

% Initially, users answer based on their prior knowledge. Afterward, they interact with the topics generated by different topic modeling algorithms and answer the questions again using their newly acquired knowledge. In the post-test phase, users can utilize the topics and a search bar to look for relevant documents to answer the questions. By comparing pre- and post-test responses, we can measure the \emph{information/knowledge gain} that users acquire from using each topic model. This is achieved by evaluating the answers against a pre-defined rubric to ensure consistent and fair grading by different evaluators.
% %


%We propose the pretest and post test evaluation, where users are given a set of questions related to the dataset prior and after they see any topics from the topic models.
%
%In the very beginning of the user study, each user is presented with a list of questions such as \textit{What role do technological innovations in recycling and material science play in reducing plastic waste and the plastic pollution crisis in this data?} that aims to test the users' knowledge about a specific topic. 
%
%Then users are presented with topics generated by topic models, then they use the topics to search relevant documents to answer the questions.
% 
%The study includes two parts answering the same questions because we want to measure the \emph{information/knowledge gain} by using the tool.
%
%All the questions are made based on popular documents that can represent a topic from the data.
%
%At the end, we evaluate the answers based on a defined rubric to ensure fairness of answer grading. 

\subsection{Datasets}\label{sec:datasets}
We evaluate on two datasets, \texttt{Bills} and \texttt{Sci-Fi}. We choose them because they are less likely to be from \mm{}'s pretained data.

% After running synthetic experiments, we chose these to ensure they were absent from the \mm{}'s pretraining data and allowed measurable user knowledge gain (see Section~\ref{appen:synthetic_experiments} for details).

\paragraph{\texttt{Bills}}
%
is a standard
benchmark for topic
models~\cite{AdlerWilkersonBillsProject}.
% both traditional and prompt-based
%
The dataset contains 32,661 bill summaries from the
110\textsuperscript{th}--114\textsuperscript{th} U.S. Congresses,
categorized into 21 top-level and 112 secondary-level topics, and we collect 11,327.
%(\textit{health, education, defense, trade, etc.})  and 112
%secondary-level topics (\textit{public lands, electricity, etc.}),
%from which we collect \num{11,327}.


%To verify the generality and robustness of our conclusion, we use the Bills dataset~\cite{AdlerWilkersonBillsProject}, which has been used by traditional topic models like \lda{}~\cite{blei2003lda}, and prompt-based models such as \topicgpt{}~\cite{pham2024topicgpt}, and \lloom{}~\cite{lam2024concept}, and can provide us an easy and straightforward comparison with existing topic models.

%, which has become a standard benchmark for topic model.% that contains hierarchical labels refined through the years with inter-annotator agreement.
%
%The dataset contains 21 generic top level topics (\textit{health, education, defense, trade, etc}) and 112 secondary-level topics (\textit{public lands, electricity, etc}). 

% 
%We collected 11,327 documents from the Bills data.

\paragraph{\texttt{Sci-Fi}}%Synthetic 
%
Inspired by \citet{lam2024concept}, we use \textsc{LLaMA-3 70B} to
generate a synthetic dataset of two thousand
%
%\jbgcomment{don't write numbers like this, see style guide (are you running style checker script?)}
%
imaginary science fiction story summaries.
%
Our goal is to create a controlled
dataset with predefined questions, answers, and themes designed to
probe topics requiring cross-document reasoning--insights difficult to
extract from individual documents, and characteristics unavailable in
real-world datasets. For generation, we select 16 ground truth Sci-Fi themes requiring minimal expertise before prompting the \mm{} with these
themes and a set of question-answer pairs aligned with our research
objectives (more details in Appendix~\ref{appendix:synthetic}).

%\paragraph{\texttt{Wiki}} is sourced from wikipedia articles~\cite{Merity2017RegularizingAO} that includes popular public topics \textit{music, anime...} 
%
%The dataset contains \num{14,290} articles with 15 high-level and 45 mid-level topics.
%
%Since this dataset may be included in \mm{}' pretraining data and shares similarities with the Bills dataset, we only use it as part of our simulated user study (Section~\ref{appen:synthetic_experiments}) to determine which topic models to include in our real user study.

%\paragraph{\texttt{Synthetic Sci-Fi}} Inspired by \citet{lam2024concept}, we define a synthetic dataset as \mm{} generated documents of a specific length centered on specified topics. 
%
%Our goal is to create questions that probe specific themes requiring cross-document reasoningâ€”insights, and difficult to extract from individual documents. 
%
%Given the scarcity of real-world datasets meeting our rigorous evaluation requirements, we generate a controlled dataset with predefined questions, answers, and themes.
%
%We select four themes from alien science fiction (sci-fi) that require few specialized expertise. 
%
%We then prompt LLaMA-3 70B with prepared themes, questions, and desired answer parameters to generate documents aligned with our research objectives. 
%
%The result is a dataset of 2,000 sci-fi domain specific (Alien Science Fiction) story summaries with contents directly mappable to our target questions and ideal answers,which we can use as a comparative analysis for user answers.\footnote{More details of generation in Apendix~\ref{appendix}.}


% \paragraph{\texttt{News}} Inspired by \citet{lam2024concept}, we use synthetic news as second dataset for our evaluation, as it offers greater control over the topics while allowing us to generate data that aligns optimally with our user study objectives. 
%we adopt synthetic datasets to precisely control the content and topics without the need for extensive effort for data collection and annotation. 
%
%We opt for synthetic news in our study for two primary reasons
%-- 1: we have more control on the topics of the dataset to prepare a dataset that could improve the data exploration experience of users. 2: We can predetermine a set of ideal questions and then generate data that optimally aligns with our user study objectives.
%
% To ensure broad public interest, we predefined \(46\) diverse topics, including pets, sports, animals, and fashion, and generated \(4\,678\) news posts using GPT-4 with a temperature setting of \(1.0\) to maximize content diversity.

% \adcomment{We should add more details about the synthetic news data set in an Appendix.

% \jbgcomment{Even in the main text, we should at least gloss what is "synthetic" about it:

% \abr{gpt} generates documents of a specific length given a specified topic

% }

% }

%To ensure broad public interest, we pre-defined a set of 46 diverse topics, including e.g., pets, sports, animals, and fashion. 
%
%Using GPT-4 with a temperature setting of 1.0 to maximize content diversity, we generated 4,678 news posts across these topics. 
%
%Synthetic data allows us to create a varied and engaging dataset while maintaining control over the subjects.



\subsection{Question Generation}
% Generating suitable questions is not an easy task, especially to avoid ambiguity, asking for overly specific answers, not generic or broad enough cover multiple documents and represent a theme.
% %
% Thus, for each dataset, we have the authors review gold topics .\footnote{The gold topics are manually written and discussed by authors before generating the data for synthetic science fiction dataset.} 
% %
% Authors read a substantial number of documents associated with randomly selected topics, draft, and collaboratively revise questions whose answers require he reading and analysis of multiple documents under the same or similar topics.
% %
% For example, \textit{What policies and regulations does the U.S. government implement to address
% water contamination and ensure environmental protection?} from \texttt{Bills} and \textit{How did human perceptions of identity change when confronted with the non-human intelligence?} from \texttt{Sci-fi}.
% %
% Each question is designed to test users' comprehension of popular topics within the dataset and their ability to trace the contours of the underlying topical landscape of the dataset.
% %
% The reference answers are developed concurrently with the questions, grounded in the documents' common themes and gold topics. 
% %
% They have been made before any user studies to minimize potential evaluation ambiguities and unfairness, which are designed to be broad enough to encompass thematic responses while remaining sufficiently focused to reflect the dataset's specific content.
% %
% The refined questions for both datasets are listed in Appendix~\ref{tab:questions}.

Because we are not doing information retrieval or question answering,
our questions must force users to synthesize information from multiple
documents.
%
Ground truth answers are also necessary to evaluate knowledge
gain. For Sci-Fi, this is ensured given the controlled
environment in which it was generated---documents are created based on
predefined question-answer pairs and themes.
%\jbgcomment{Review em and en dashes, en dash was used incorrectly}
%
For Bills, authors review gold-standard topics along
with a substantial number of documents associated with selected topics, then draft and collaboratively revise questions whose
answers require the reading and analysis of multiple documents under
the same or similar topics.
%
All questions and reference answers are finalized before user studies
to ensure clarity and fairness (full questions in
Appendix~\ref{tab:questions}).

%%%%%%%%%%%%
%Creating suitable questions is challenging, requiring careful avoidance of ambiguity, overly specific prompts, or excessively broad queries. 
%
%For each dataset, the authors review gold topics and read a substantial number of documents associated with randomly selected topics, then draft and collaboratively refined questions, which requires analyzing multiple documents within related topics.\footnote{Gold topics are manually curated by the authors for sci-fi.}
%
%For example, questions like \textit{What policies and regulations does the U.S. government implement to address water contamination and ensure environmental protection?} from \texttt{Bills} and \textit{How did human perceptions of identity change when confronted with non-human intelligence?} from \texttt{Sci-fi} are designed to test users' understanding of major themes from the dataset.
%
%Reference answers are developed alongside the questions, grounded in the dataset's common themes and gold topics.
%
%All questions and reference answers are finalized before user studies to ensure clarity and fairness (Appendix~\ref{tab:questions}). 
%%%%%%%%%%%%
%
% Refined questions for both datasets are listed in Appendix~\ref{tab:questions}.

% \jbgcomment{I think we could probably cut the questions and the grading rubric (move to appendix) and just summarize this as:

% For each dataset, two authors review gold topics for both datasets, along with a substantial number of documents associated with randomly selected topics, draft, and collaboratively revise questions whose answers require he reading and analysis of multiple documents under the same or similar topics.
% %
% For example, in the X dataset, Y.  
% %
% Similarly, in the Z dataset, A.
% %
% Full list of questions in Appendix B.

% }

% To formulate questions that require analysis and connections of multiple documents and topics, two authors reviewed the gold topics for both datasets, along with a substantial number of documents associated with randomly selected topics.
% %
% We then discussed and analyzed documents and topics we have read and drafted questions that require the reading and analysis of multiple documents under the same or similar topics.
% %write up questions that require reading and analysis for multiple documents under the same or similar topics.
% %
% The questions are framed so they can be answered without referring to specific documents, but are difficult to answer without exploring the key topics in the dataset.
% %looking at very specific documents but hard to answer without exploring popular topics from the dataset.
% %
% Each question has a list of acceptable answers that are derived from the dataset.
% \jbgcomment{Describe when / how these answers are derived.  I.e., should have been before any user studies, etc.}
%
%We list out the refined questions for both datasets in Table~\ref{tab:questions}.

\subsection{\textit{Metrics} and \texttt{Evaluation}}

We use two automatic metrics and one human evaluation to assess the
quality of user responses generated with the aid of topic models.
%%%%%%%
%We introduce two automatic evaluations and one human evaluations to measure user response quality using different models: 
%1.Pairwise transformer similarity: measures answer consistency across different users. A higher average similarity score indicates greater consistency among users. 
%
%2. Answer quality: measures the similarity between user responses and the reference response. 
%
%A higher similarity score means the user answer quality is closer to the gold answer.
%
%3. Pairwise response preference: We evaluate user pairwise preferences by comparing responses generated by users using different models.
%
%For each pair of user responses, we ask annotators to choose the better response.
%
%We then use the Bradley-Terry model~\cite{Bradley1952RankAO} to rank the pairwise preference strength of each model, which allows us to quantitatively evaluate the user-perceived quality of the answers produced by the different models (Appendix~\ref{appendix}).
%%%%%%%

% Knowledge acquisition: uses a pre-test and post-test setup. Users answer the same questions in both phases, with each question based on multiple representative documents and topics. The difference between Pretest and Post test measures users' knowledge gain for a given dataset. 2. Pairwise transformer similarity: measures answer consistency across different users. A higher average similarity score indicates greater consistency among users. 

% \paragraph{Knowledge acquisition} assesses users' initial expertise on a specific topic before data exposure, as well as their learning gains after interacting with the data.
% %The knowledge acquisition measurement assesses users' initial expertise on a specific topic before data exposure. 
% %
% %Secondly, it measures the learning gains after users interact with the data. 
% %
% The pre-test/post-test comparison quantifies these knowledge gains, providing an objective evaluation of how effectively topic models enhance understanding and facilitate insights from the data.
% %
% For a given user, we define the user's knowledge score as: 
% \begin{equation}
% \bar{r} = \frac{1}{N} \sum_{i=1}^{N} r_i
% \end{equation}
% where $r_i$ is the average rating between two annotators for question $i$, and $N$ is the number of questions. 

\paragraph{\textit{Answer consistency.}}
%
%\jbgcomment{If the paragraph is not integrated into the next sentence put a period at the end.}
%
We compute the pairwise similarity using the cosine similarity of
transformer embeddings for each pair of answers to measure how consistent users' answers are to the same question.
%\paragraph{Pairwise similarity} quantifies the consistency and similarity of users' answers. 
For a question answered by \(K\) users, the consistency score is
%For a given question answered by $K$ users, we calculate the pairwise similarity by computing the cosine similarity of transformer embeddings of each pair of answers:
\begin{equation}
S = \frac{2}{K(K-1)} \sum_{i=1}^{K-1} \sum_{j=i+1}^K \cos(a_i, a_j)
\end{equation}
%
where $\cos(a_i, a_j)$ is the cosine similarity between the embedding representation of the answers
%
%\jbgcomment{This is the cosine between the representations, right?}
%
provided by two users, and $\frac{2}{K(K-1)}$ normalizes the sum to account
for the number of answer pairs.

\paragraph{\textit{Answer quality.}}
%
We evaluate the quality of user responses by comparing them to the
reference (gold) answers. The similarity is calculated as the cosine
similarity between the transformer embeddings of the user response and
the gold answer.

\paragraph{\texttt{Pairwise response preference.}}
%
Even though gold answers approximate the expected response, they are not the only acceptable answers. 
%
% \jbgcomment{perhaps we should have different typography for the evals
  % / metrics} 
To complement the {\it Answer quality} metric, we hire annotators to evaluate the quality of the responses using a fixed
rubric.\footnote{The scoring rubric was initially developed by two
social science experts and refined iteratively. It evaluates how well
answers synthesize information from multiple documents to address the
question and penalizes hallucinated content or references outside the
corpus (Appendix~\ref{appendix:rubric}).}
%
Assigning a precise score to a response is challenging, so evaluators
are asked to compare pairs of responses and select the better
one.
%
We then use the Bradley-Terry model~\cite{Bradley1952RankAO} to rank
the pairwise preference strength of each model, which quantitatively evaluates the user-perceived quality of the answers
users generate with the support of different models
(Examples in Appendix~\ref{sec:domain_specific_data}).
%
%\jbgcomment{Last sentence unclear.  Probably also good to emphasize
%  that annotator don't know what conditions answers are from to remove
%  bias.}
%

% \subsection{Human Evaluation Criteria}
%\paragraph{Human Evaluation Criteria}
% Even if we have reference answers for each question, evaluating the quality of a long-form response is hard, but picking a more favorable answer from two answers is an easier task.
% %
% We use pairwise preference to evaluate the groups that results in overall better responses.
% %
% Annotators first review the gold topics in the dataset then review users' answers to identify unreasonable made up answers. 
% %
% The evaluation is based on the dataset, the gold topics, the question, a reference answer, two user responses from different groups for the same question. 
% %
% Annotators need to verify any overly specific document references by cross-checking their existence within the corpus. 
% %
% Additionally, we use a fine-tuned RoBERTa model~\cite{sivesind_2023} to reject users with high likelihood of AI-generated answers. 
% %
% We adopt an initial evaluation rubric, developed by two social science experts, focuses on dataset topic exploration quality. 
% %
% The authors collaboratively refined the score rubric through iterative review of actual user study responses.\footnote{The authors will not be the annotators for the answers. Instead, we hire crowd workers to evaluate based on the refined rubric.}
% %
% The final rubric captures how well the answer addresses the question by synthesizing information from multiple documents in the corpus, with a penalty for responses that seem AI generated (e.g., hallucinating information from outside the corpus) with detailed criteria in Appendix~\ref{appendix:rubric}.

%%%%%%%%%%
%Evaluating the quality of long-form responses is challenging, even with reference answers, but selecting the more favorable answer between two is more manageable.
%
%We use pairwise preference evaluation to identify which group produces overall better responses.
%
%Annotators review the dataset's gold topics and users' answers to filter out unreasonable or made up responses.
%
%The evaluation considers the dataset, gold topics, the question, a reference answer, and two user responses from different groups for the same question.
%
%Annotators verify overly specific document references by cross-checking their existence within the corpus.
%
%We use a fine-tuned RoBERTa~\cite{sivesind_2023} to reject users likely submitting AI-generated responses.
%
%We adopt a scoring rubric, initially developed by two social science experts that emphasizes the quality of topic exploration and is refined iteratively through user study responses.\footnote{Authors are not the annotators; instead, crowd workers evaluate responses using the finalized rubric.}
%
%The rubric evaluates how well the response synthesizes information from multiple documents to address the question while penalizing responses that hallucinate or reference information outside the corpus (Appendix~\ref{appendix:rubric}).
%%%%%

% %
% Then we discuss the questions, reference answers, and refine the evaluation rubrics through iterative review of collected answers from the user study. 
% %

% We use an existing rubric drafted by two social science experts to
% developed a comprehensive rubric that assesses how effectively answers synthesize information across multiple documents. The rubric penalizes responses that appear AI-generated or introduce external hallucinations beyond the dataset's corpus.


% The authors discussed the questions, reference answers, topics in the document, reviewing existing answers from the user study and revise rubrics from social science experts drafts to accommodate on our dataset.
%



% \jbgcomment{again, you could say:

% graded on a rubric (full criteria in APpenix X) that captures how well the answer addresses the question by synthesizing information from multiple documents in the corpus, with a penalty for responses that seem AI generated (e.g., hallucinating information from outside the corpus).
% }

% Annotators evaluated the answers without knowing the control group or whether they were pre- or post-test responses, using a 1--5 score based on the rubric in Table~\ref{tab:rubric} for a fair evaluation.




%We shuffle the answers for different groups and have two annotators evaluate the answers without knowing the group names.
%
%Both annotators evaluate answers based on a 1-5 score according to the rubric present in Table~\ref{tab:rubric}.
%
%Since there is no strict \emph{correct} answers for each question, annotators need to be quite familiar with the topics and documents in the dataset, and also need to look at users' answer and search potential relevant documents to verify the faithfulness of the answers.
%
%The grading of the answers is based on the documents in the dataset and the human written reference answers.
%
%In addition, we use finetuned Roberta~\cite{sivesind_2023} to show the AI-generated and human-written probability of the answers to assist annotators judging answers.
%
%We have two annotators who made the questions
%The krippendorf's alpha~\cite{castro-2017-fast-krippendorff} between two annotator ratings are 0.63 for Bills and 0.51 for synthetic news.


%Annotators read the instructions and can quit the study at anytime.
%
%Then they will be redirected to take the pretest questions, and answer post-test questions with the help of each topic modeling tool.\footnote{Each annotator has a base rate of $\$5$. If their answers are not likely AI generated, they receive a pay rate of $\$17$ an hour.}
%
%Although we have a fixed study time to 40 minutes, the length of the study participation is ranging from 40 minutes to 75 minutes for most participants.
%
%After the study, users are prompted with three survey questions to rate their experience of using the tool and we will analyze in this section.
%
%Each user can only take the study once.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%OLD (Before 10 Oct)

% We use the Bills dataset~\cite{AdlerWilkersonBillsProject}, a standard benchmark dataset for topic models that contains hierarchical labels refined through the years with inter-annotator agreement. The dataset contains 21 generic top level topics (\textit{health, education, defense, trade, etc}) and 112 secondary-level topics (\textit{public lands, electricity, etc}). The Bills dataset has been used by traditional topic models like \lda{}~\cite{blei2003lda}, and prompt-based models such as TopicGPT~\cite{pham2024topicgpt}, and LLooM~\cite{lam2024concept} and can provide us an easy and straightforward comparison with existing topic models. We also use the National Center for Teacher Effectiveness (\ncte) dataset that contains anonymized teacher and student transcripts in math classrooms used for researchers, educators, and policy makers to improve K-12 mathematical instructions. The dataset contains well-defined expert rubrics to evaluate the teaching quality and instructions of teachers using Mathematical Quality Instruction (\abr{mqi})~\cite{mqi} and high-level abstract concepts that describe effective math teaching strategies \textit{Remediation of Student Errors and Difficulties, Student Provide Explanations}; such topics are beyond word frequencies, requiring understanding of abstract concepts from the dataset and relation to real-life experiences. The Bills dataset evaluates the basic clustering abilities of topic models and the \ncte dataset brings the evaluation of topic model to a higher level- evaluates topic models' ability to help humans extract, analyze and understand abstract high-level concepts.

% We use two of the datasets split to train/test sets (currently Bills and Wiki, Cordis). The two datasets have hierarchical topics for each document, one more generic (\textit{Health}), and another one more specific (\textit{Child Mental Health}). The test sets overlap with the training set in terms of general topics, but there is no overlap for the more specific topics. For example, the train and test set both have a set of documents labeled \textit{health}, but the more specific topics between the train and test set are different. 

% We select the four datasets below with their size:

% \begin{itemize}
%     \item Wiki(8,000)
%     \item Bills (13,000)
%     \item Semantic Scholar (50,000)
%     \item EURO (100,000)
% \end{itemize}

% \subsection{Control Groups}
% % We run five topic models (three classical and two \mm{}-based) on the datasets then calculate the cluster quality using purity, adjusted rand index (\ari{}), normalized mutual information (\nmi{})
% We run a user study with \bass{} to generate topics within forty minutes with the train sets. We run \abr{lda}, \bertopic{}, TopicGPT to automatically generate topics for e train sets:
% \begin{itemize}
%     \item \abr{lda}
%     % \item \abr{ctm}
%     \item \abr{bert}opic
%     \item TopicGPT
%     \item \bass{} (with human assistance for every document)
%     \item \bass{} (with human generation initially but automatic later)
% \end{itemize}

% \subsection{Comparing \bass{} with Other Models}
% % We run five topic models (three classical and two \mm{}-based) on the datasets then calculate the cluster quality using purity, adjusted rand index (\ari{}), normalized mutual information (\nmi{})
% We run a user study with \bass{} to generate topics within forty minutes with the train sets. We run \abr{lda}, \bertopic{}, TopicGPT to automatically generate topics for e train sets:
% \begin{itemize}
%     \item \abr{lda}
%     % \item \abr{ctm}
%     \item LLooM
%     \item TopicGPT
%     \item \bass{} (with human assistance for every document)
%     \item \bass{} (with human generation initially but automatic later)
% \end{itemize}
