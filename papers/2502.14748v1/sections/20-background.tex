% \jbgcomment{Rename this section something like "background: tools for corpora understanding".  THis section also needs to answer the question "why not just use QA directly"}

Data exploration is not just about finding \emph{an} answer in a
cropus.
%
While information retrieval and question answering can help a user
find a responsive passage or two in a dataset to provide a desired
factoid (what act established civilian government in Puerto Rico?),
data exploration is not about finding a needle in a haystack, it is
about \emph{describing} the shape an contours of that
haystack~\cite{karpukhin2020densepassageretrievalopendomain,
yang2018hotpotqadatasetdiverseexplainable}.
% \jbgcomment{cite DPR and multihop QA}
%
Instead, these processes follow a systematic approach, like grounded
theory~\cite{chun_tie_grounded_2019}, which involves identifying
themes, connecting information across multiple documents, and applying
complex human reasoning to uncover meaningful insights and reliable
findings.
%Instead, these processes comprise a systematic pipeline like grounded theory~\cite{chun_tie_grounded_2019}, which involves identifying themes within the data, connecting information across multiple documents, and complex human reasoning about interconnected themes for researchers to reach reliable and insightful downstream findings. 
%
For example, to understand \textit{What do policies about land use and
wildlife have in common?} from congressional policies,
researchers need to identify themes and topics about land management
and wildlife management first.
%
They then examine relevant documents for those themes, reasoning
through similarities and commonalities to reach a well-supported
answer.
%Then they have to read about the relevant documents for those themes, and reason the similarity and common grounds between documents for those themes to reach a reasonable answer. 
%
Unlike question answering, data exploration is iterative, requiring deeper reasoning to discover connection and meaningful insights within the data.
%Data exploration is iterative and reasoning for researchers to uncover meaningful insights and connections within the data, which differs from question answering.


Topic modeling is a popular tool to assist researchers in information
retrieval and data exploration to discover latent topics in
collections of documents~\cite{ABDELRAZEK2023102131}.
%
% With the rising popularity and effectiveness of \mm{}s, using them potential for answering research questions or extracting information seems promising. However, data exploration and content analysis typically require a systematic approach~\cite{chun_tie_grounded_2019}. This process involves comprehensive exploration of datasets, starting with theme identification and often necessitates synthesizing information across multiple documents while reasoning about interconnected themes. Relying solely on \mm{}s throughout this process presents challenges due to their context window limitations, faithfulness issues with large corpora, and difficulties in integrating diverse topics for complex dataset analysis.
%
Since the first probabilistic topic model---probabilistic
Latent Semantic Analysis~\cite[p\abr{lsa}]{hoffmann-99}---many
variants have emerged to help researches with information seeking,
exploration, and forming research questions in education~\cite{SunYan2023}, mental
health~\cite{gao2023discoveringmentalhealthresearch}, social media,
public opinion~\cite{LaureateBuntineLinger2023}, inter alia.
%grounded theory data exploration Different topic models retrieve and
%present information differently, but their ultimate purpose is to
%help researchers understand and learn from a
%dataset.  \jbgcomment{I'm not sure this level of detail is ncessary.
%I think that we could probably abstract this a little bit.  }
Regardless of how the methods find them, unlike traditional
question answering, topic modeling extracts topics from the documents:
words that appear together in thematically coherent contexts and
provide an initial topic landscape of the dataset.
%



% treat each topic as a probability distribution over words, with each document being an admixture mixture of topics. 
%Probabilistic topic models treat each word as a topic and each document as mixture of topics to cluster documents based on word frequency distributions within the dataset.
% \lorenacomment{I stopped since I realized you just wrote ideas in there, and I think some part will need to go in 3}
%
% \abr{lda} and supervised \abr{lda}~\cite{slda} use Bayesian statistical methods to uncover latent structure of data and making predictive inferences, treating each topic as a list of keywords.
%
%One advantage of probabilistic models is fast and efficient. 
%
%The downside is that they only use word distributions to inference latent topics, which is challenging to generate abstract topics that require human reasoning. 
% Neural topic models such as Contextualized topic model (\abr{ctm})~\cite{bianchi2020pre} and \bertopic{}~\cite{bertopicMark} use pre-trained transformer embeddings to encode text data. \abr{ctm} integrates these embeddings with a variational autoencoder-based approximation of LDA, while \bertopic{} combines them with clustering algorithms to generate latent topics.
%and combine with clustering algorithms to generate latent topics. 
%
% Traditional and neural topic models represent each topic as a list of keywords that identify latent themes in a dataset---\textit{`land', `forest', `area', `wilderness', `management'}---that can be used for further topic and data analysis.\ahintext{I will try to get to this myself, but because you're not using neural topic models, I would not include any dsicussion here. This is also partially redundant with section 4.}
% With the rise of popularity and effectiveness of \mm{}s, someone might ask, why not just use \mm{} to do everything like research question answering to extract information and answer questions for a given dataset? 
% %
% One caveat of data exploration or content analysis is that there is a systematic pipeline for data exploration~(cite grounded theory), where a research question requires the whole pipeline of data exploration work, to first identify what themes are in the data, and many of the times the questions require interconnection of multiple documents and complex reasonings of multiple themes to answer, which simply using an \mm{} is hard to accomplish due to their context window limit, faithfulness of the answers for large corpuses, and its ability to interconnect multiple topics and documents to reason within a dataset. 

%
While \mm{}s can complete diverse tasks, their data exploration
capabilities still require an iterative and systematic pipeline rather than simple
prompting.
%
More recent approaches use \mm{}s to generate more human readable and
descriptive topics~\cite{openai2024gpt4,touvron2023llama}.
%
% TopicGPT~\cite{pham2024topicgpt} samples documents to induce topics, merge topics, and then traverse documents to assign topics and cluster documents.
%
% \jbgcomment{I'm not sure models we don't compare against should be in this section; perhaps move to the end.  But if they do stay here, cite BERTopic}
% \citet{reuter2024gptopic} introduce a method akin to BERTopic but allowing for a fixed number of topics and using agglomerative clustering, defining topics through titles, descriptions, assigned documents, and document embeddings, with RAG enabling dynamic interactions via a chat interface. 
%
% LLooM~\cite{lam2024concept} uses \mm{}s to extract key sentences from individual documents, summarize documents, then use clustering algorithms to cluster summarizations and prompt \mm{}s to generate topics for all clusters.
%
% The above mentioned classical topic models and prompt-based topic models are fully automated with no human intervention to participate in the process of topic generation.
\topicgpt{}~\cite{pham2024topicgpt} and \lloom{}~\cite{lam2024concept} represent topics with more intuitive short descriptions
%each topic as a descriptive phrase with a short description to help researchers discover themes 
such as \textit{Land Management: Involves policies and actions related to the use, regulation, and conservation of land and natural resources}. 

Thus, we raise a new question for \mm{}s: are they ready to replace traditional topic models for describing the haystack of a corpus?
%
% \jbgcomment{I think we need to make it clear that the replacement is in the context of these corpus-exploration analyses}
%
To address the motivation, we design a end-to-end evaluation study to
compare \abr{lda}, \mm{}-based methods, and \bass{} to study whether
human supervision can make \mm{}ls a more
effective tool for large corpora understanding.
%
% We study whether human involvement is still necessary to supervise \mm{}-based topic models and whether human involvemen
% \bass{} allows users to review only a small set of documents and then revise the automatically generated topics to form a topic set for the entire dataset.
% \zongxiacomment{Add a little bit BASS description here.}
%
% To evaluate the usefulness of these topic models for data exploration, we study the following research questions: (1) Can we replace traditional topic models with \mm{}s for data exploration? (2) Does human involvement in the topic generation process have a positive or negative impact on user experience?

% We use an \mm{} in interactive topic modeling to alleviate this burden by leveraging \mm{}s to provide topic suggestions, we enable users to generate topics under human supervision, reducing the need for complete manual topic creation while maintaining control over the process.
%
% Fortunately, with existing interactive topic model techniques and prompt-based topic models, we can leverage interactive topic models such that humans collaborate with an \mm{} and active learning classifier to quickly learn knowledge about a dataset.
