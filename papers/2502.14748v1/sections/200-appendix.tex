% \newpage
\appendix
\begin{figure*}[!t]
    \centering
    \includegraphics[scale=0.44]{figures/synthetic_experiments.pdf}
    \caption{
    Synthetic user study experiment comparison of {\color{bertopiccolor}\bertopic{}}, {\color{ldacolor}\lda{}-\mallet{}}, {\color{ctmcolor}\ctm{}} and {\color{dvaecolor}\dvae{}} across the datasets of \texttt{Bills}, \texttt{Sci-Fi}, and \texttt{Wiki}. Each row represents a different label-centric clustering metric: Purity (top), Adjusted Rank Index (ARI, middle), and Normalized Mutual Information (NMI, bottom). The x-axis shows the number of labeled documents, while the y-axis the respective metric scores. {\color{ldacolor}\lda{}-\mallet{}} and {\color{ctmcolor}\ctm{}} generally achieve better clustering metrics with the same number of documents labeled, while {\color{bertopiccolor}\bertopic{}} lags behind in most cases. {\color{dvaecolor}\dvae{}} exhibits more variability and lower clustering scores across datasets. However, {\color{ldacolor}\lda{}-\mallet{}} is still considered the best compared to other neural topic models.
    }
    \label{fig:synthetic_experiment}
\end{figure*}

\section{Synthetic Experiments}\label{appen:synthetic_experiments}
Traditional topic models have many variants, including Bayesian probabilistic approaches (BTM) (e.g., \lda{}), neural methods (NTM), including contextualized topic models~\cite[\ctm{},]{bianchi2020pre,kitty_CTM} and the Dirichlet Variational Autoencoder Model~\cite[\dvae{}]{burkhardt2019decoupling}, as well as clustering-based models like \bertopic{}~\cite{bertopicMark}.

%
Our purpose in this work was to evaluate whether the new paradigm of \mm{}-based topic models truly surpasses traditional models for learning about data. Given the cost of human-based evaluation, which this work heavily relies on, testing all state-of-the-art traditional models is impractical. Instead, we select {\bf one traditional variant} and compare it against the {\bf two main \mm{}-based approaches}, \topicgpt{} and \lloom{}, as well as {\bf our supervised \mm{} model, \bass{}}. This section details the experiments conducted to select \lda{} as representative of the traditional variant.

%
Neural topic models generally achieve higher coherence scores than \lda{}, but~\citet{hoyle2021automated, doogan-buntine-2021-topic, li-etal-2024-improving} show that traditional coherence metrics do not generalize well to neural topic models,  as they tend to favor NTM topics over BTM ones without fully correlating with human assessments.
%
Hence, due to the lack of reliable automatic evaluation metrics for both Bayesian and neural topic models, we mimic~\citet{li-etal-2024-improving}'s synthetic experiment to find the most suitable traditional topic model to use in our user study.

%
We first provide a brief description of the evaluated topic models and their configurations, along with the datasets used for evaluation. We then detail the synthetic experiment process and results, demonstrating that \mallet{}-\lda{} is the most suitable traditional topic model for our user study.

\subsection{Datasets}
As datasets, we use \texttt{Bills} and \texttt{Sci-Fi}, as defined in Section~\ref{sec:datasets}, along with an additional dataset, \texttt{Wiki}, sourced from Wikipedia articles~\cite{Merity2017RegularizingAO}. The \texttt{Wiki} dataset consists of \num{14,290} articles spanning 15 high-level and 45 mid-level topics, including widely recognized public topics such as music and anime. It serves as a traditional baseline for topic modeling evaluation.
%

We chose not to include this dataset in the main experiments because it is part of \mm{}'s pretraining data. Additionally, most Wikipedia topics are widely known and highly diverse, making it difficult to measure knowledge gain according to the definition used in this work. 

For the candidate models representing the traditional variant of topic modeling, we consider \mallet{}-\lda{}, as explained in the main paper (see Section~\ref{subsec:study_conditions}), along with the following models\footnote{All implementations are integrated into our topic modeling training class\footnote{\url{https://github.com/zli12321/TopicModelLLM/blob/main/src/topic_modeling/topic_model.py}}.}:

\paragraph{\ctm{}.} Specifically, its \abr{Combinedtm}~\cite{kitty_CTM} variant extends \abr{Prodlda}~\cite{prodLDA} by incorporating Sentence-\abr{bert} embeddings~\cite[\abr{sbert}]{Reimers2019SentenceBERTSE} into the \abr{bow} representation used as input for its encoder-decoder architecture. The inference network transforms these combined representations into continuous latent document representations, while the decoder reconstructs the \abr{bow}. We use the authors' original implementation\footnote{\url{https://github.com/MilaNLProc/contextualized-topic-models}}, keeping all settings at their default values.


\paragraph{\bertopic{}.} This model follows an engineering-driven approach, generating topics by clustering \abr{sbert} embeddings without relying on word-topic or document-topic distributions. These distributions are approximated post hoc after clustering and dimensionality reduction. We train the model with \texttt{calculate\_probabilities=True}, ensuring that topic probabilities are computed for each document during the HDBSCAN clustering step. All other parameters remain at their default values, and we use the original implementation of the author\footnote{\url{https://github.com/MaartenGr/BERTopic/tree/master}}.

\paragraph{\dvae{}.} \citet{burkhardt2019decoupling} proposed reparameterizing the Dirichlet prior using Rejection Sampling Variational Inference (RSVI), preserving the properties of \lda{}-based methods while balancing interpretability and likelihood optimization. We utilize the implementation from \citet{hoyle2021automated}\footnote{\url{https://github.com/ahoho/dvae}}, where RSVI is replaced by path-wise gradients~\cite{jankowiak2018pathwise}. We set the number of training iterations to 250, while all other parameters remain at their default values.

Note that all of the latter are neural-based topic modeling algorithms. We do not consider other Bayesian-based topic models, as the superiority of \lda{}, particularly in its \mallet{} implementation, has been demonstrated multiple times in the literature~\cite{hoyle-21,doogan-buntine-2021-topic, an2023sodapopopenendeddiscoverysocial, liu2024understandingincontextlearningcontrastive}.
%

%\subsection{Synthetic Experiment Setup}
%
%Neural topic models generally achieve higher coherence scores than \abr{lda}, but \citet{hoyle2021automated, doogan-buntine-2021-topic, li-etal-2024-improving} show that traditional metrics do not generalize to neural topic models, where neural topic models often generate less user-interpretable and favored topics compared to \lda{}. 
%
%Due to the lack of reliable automatic evaluation metrics for topic models, we adopt \citet{li-etal-2024-improving}'s synthetic experiment pipeline to find the most suitable traditional topic models to use in our user study to reduce unnecessary human study costs. 
%

%\paragraph{Algorithmic definitions.} We train four topic models - \mallet{}-\lda{}, BERTopic, \ctm{}, and \dvae{} - on the Bills, Sci-fi, and Wiki, each configured to generate 65 topics.
%

\subsection{Synthetic Experiment Setup}
\label{app:synthetic_experiment_setup}
We train one topic model per model type---\mallet{}-\lda{}, \bertopic{}, \ctm{}, and \dvae{}---and dataset---\texttt{Bills}, \texttt{Sci-Fi}, and \texttt{Wiki}, with 65 topics. 

Let \(k^{*}\) be the index of the most predominant topic for document \(d\), and let \(\theta^d_{k^*}\) be its corresponding probability, where:
\begin{equation}
    k^{*} = \arg \max_{i=1}^{K} \theta_i^d
\end{equation}

Let \(L\) be the label set probability distribution for document \(d\), we train a logistic regression active learning classifier with \(\theta^d_{k^*}\) as feature and compute the classifier entropy as:

\begin{equation}
\mathbb{H}_{d}(L) = \mathbb{H}_d(L) \cdot \theta^d_{k^*},
\label{eq:topic_document_preference}
\end{equation}

%\begin{equation}
%\theta^d_{\text{max}} = \max_{i=1}^{K} \theta^d_i.
%\label{eq:topic_probability}
%\end{equation}

%For any document with a topic distribution represented by a topic model as \( \theta^d \equiv \{\theta^d_1, \theta^d_2, \ldots, \theta^d_K\} \), its predominant topic
%\begin{equation}
%\theta^d_{\text{max}} = \max_{i=1}^{K} \theta^d_i.
%\label{eq:topic_probability}
%\end{equation}

%We train a logistic regression classifier with topic model vector representations as features and compute the classifier's entropy:
%\begin{equation}
%\mathbb{H}_{d}^{t}(L) = \mathbb{H}_d(L) \cdot \theta^d_{\text{max}},
%\label{eq:topic_document_preference}
%\end{equation}
where higher entropy indicates greater classifier uncertainty in document classification.
%
In the original study by~\citet{li-etal-2024-improving}, a user-in-the-loop approach is employed, wherein a user iteratively revises the documents suggested by the active learning classifier. Here, we approximate user annotations by utilizing the dataset' gold labels as pseudo-labels. Since these labels are carefully curated, they provide a reliable approximation of human annotations and may even offer a slight advantage, as human annotators tend to assign more specific labels, intentionally introducing additional variability.


Our document selection process follows two steps: first we identify the topic \(k_H\) that shows the highest median document entropy among all \(K\) topics, indicating the topic where the classifier shows the most uncertainty. 
%
Then we choose the document with the highest entropy within topic \(k_H\) as the next document as the next document to be labeled and update the classifier using its corresponding pseudo-label.


%We use gold labels as pseudo-labels and iteratively select documents for labeling to simulate user input from our user study.
%
We use incremental learning~\cite{incrementallearning} to train and update the logistic regression classifier and compute the purity, \abr{ari}, and \abr{nmi}.
%
We retrain the classifier if a new label class if introduced.
%
For each topic model, we perform five iterations of simulated user labeling, labeling up to \textit{200} documents per iteration---the maximum number of documents that could be labeled by users within an hour. We then compute the median value for each document within each group.


\subsection{Results}
Figure~\ref{appen:synthetic_experiments} shows the label-centric clustering metrics obtained as a result of the synthetic experiment for each dataset and topic model.

\paragraph{Mallet \abr{lda} outperforms other neural topic models on three clustering metrics on all datasets with equal number of documents labeled (Figure~\ref{fig:synthetic_experiment}).}
%
\abr{ctm} is the only neural topic model that achieves comparable to \abr{lda} clustering performance. Thus, we choose \abr{lda} as the most suitable traditional topic model in our real user study.



\section{Time and monetary cost}
\label{app:time_monetary_cost}
Table~\ref{sec:cost} shows the average amount of time and costs to train each model on a size 10,000 dataset (Bills). 
%
\abr{lda} is the cheapest and fastest than any other models.
%
The other fully automated \mm{} approaches, however, are more expensive than adding human-in-the-loop approach (\bass{}).
%
Adding human-in-the-loop for \mm{}-aided data exploration can be cheaper and more efficient than existing fully \mm{}-based approaches.

\label{sec:cost}
\begin{table}[h!]
\small
\centering
\begin{tabular}{ccc}
\hline
% \rowcolor{gray!50}
{\bf Method} & {\bf Train Time} &  {\bf Cost}  \\ \hline
\lda{} & 5 mins & Free \\ 
\lloom{} & 30 mins & \$40 \\ 
\topicgpt{} & 9 hrs & \$65 \\
\bass{} & User dependent: 1 hr & \$30 \\
\hline
\end{tabular}
\caption{The train time and cost for each method is an approximation. Specifically, for all models besides, we use GPT-4o as the prompt model to generate topics. The estimated cost of \bass{} is one user hour cost \$20 plus the expected prompt cost \$10.}
\label{tab:cost}
\end{table}

% \section{User Study Page}
% \label{sec:user_study_section}
% See Figure~\ref{fig:bass_topic_generation} for user study interface for reference.

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[scale=0.8]{figures/demo.pdf}
%     \caption{
%     % Topic-generation process for \bass{} users. From top to bottom, and left to right: users are presented with a document and its summary (automatically generated by an \mm{}), three \mm{}-suggested labels (from which they can select one or input a new one), the already generated topics (hover to see the number of assigned documents), and a search bar to find similar documents using keyword and TF-IDF search.
%     Users will be displayed with selected documents and topics generated by topic models or topics they have added (hover to see the number of assigned documents). In \bass{}, the UI includes three \mm{}-suggested labels (from which they can select one or input a new one), but the users start with empty set of topics. The search bar can find similar documents using keyword and \textsc{tf-idf} search. Users in the baseline group will not have anything \mm{} suggested labels nor topics generated by a topic model, but they can add and make new labels.
%     }
%     \label{fig:bass_topic_generation}
% \end{figure*}

\section{Generated questions and Evaluation Rubric}
Two expert social scientists design a rubric to evaluate the quality of user responses. 
%
See Table~\ref{tab:questions} for reference.

\label{appendix:rubric}
\begin{table*}[h]
    \centering
    \tiny
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{p{0.025\textwidth}p{0.425\textwidth}|p{0.025\textwidth}p{0.425\textwidth}}
    \toprule
    \multicolumn{2}{c}{{\bf Bills}} & \multicolumn{2}{c}{{\bf Synthetic Science Fiction}} \\
    \midrule
    1 &  What policies and regulations does the U.S. government implement to address water contamination and ensure environmental protection? & 1 & How did human perceptions of identity change when confronted with the non-human intelligence? \\
    2 & What are common policy actions taken by governments in the United States to manage land use effectively? & 2 & How did the non-human intelligence perceive humanity during its interactions? \\
    3 & Which demographic or age groups are targeted by government initiatives to enhance educational opportunities and benefits? & 3 & What challenges did the humans face when trying to communicate with the non-human intelligence? \\
    4 & What basic rights should people have when receiving care at home? & 4 & What were the consequences of the successful communication with the non-human intelligence? \\
    5 & What do policies about land use and wildlife have in common? & \multicolumn{2}{c}{} \\
    6 & Why does the government sometimes pause taxes on importing certain chemicals and materials? & \multicolumn{2}{c}{} \\
    \bottomrule
    \end{tabular}
    \caption{Pre-test and post-test questions for both datasets. The test questions are testing the users' understanding of topics in the dataset, not testing users' ability to find a specific document to find the answer.}
    \label{tab:questions}
\end{table*}


\begin{table*}[h]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{c|p{0.4\textwidth}|p{0.4\textwidth}}
    \toprule
    \textbf{Score} & \textbf{Judgment} & \textbf{Examples} \\
    \midrule
    1 & Very low quality: The response is irrelevant to the subject, showing no understanding or effort. & 
    \begin{itemize}[leftmargin=*]
        \item Blank response
        \item "Affordable care"
        \item "Love"
        \item "I don't know"
        \item "no idea"
    \end{itemize} \\
    \midrule
    2 & Low quality:
    \begin{enumerate}[leftmargin=*]
        \item Shows minimal relevance or understanding of the subject, with little or limited effort.
        \item Using just the topics to answer the questions without providing explanation.
    \end{enumerate} & 
    \begin{itemize}[leftmargin=*]
        \item "trade, tariffs…"
        \item "Alien communication"
        \item "This document discusses…" (direct copy of the summary or label)
        \item Ballast Water Management Act (BWMA)
    \end{itemize} \\
    \midrule
    3 & Fair quality:
    \begin{enumerate}[leftmargin=*]
        \item Shows basic understanding of the subject and the dataset, showing some effort akin to a lay person's perspective.
        \item The response answers the question based on a single document, not the theme across multiple documents.
    \end{enumerate} & 
    \begin{itemize}[leftmargin=*]
        \item "Clean Water Act (CWA): This law regulates discharges of pollutants into U.S. waters and sets water quality standards for surface waters."
        \item young people, school leaver age 16-18 and young adult 18-26
    \end{itemize} \\
    \midrule
    4 & High quality:
    \begin{enumerate}[leftmargin=*]
        \item Shows good understanding of the subject, suggesting above-average knowledge or effort.
        \item The answers are from the documents. However, just a list of related documents or titles for the question are provided. Little analysis or insights, synthesis of those documents are given.
    \end{enumerate} & 
    \begin{itemize}[leftmargin=*]
        \item "The US government enforces Clean Water Act, regulating pollutants in waterways, Safe Drinking Water Act, ensuring safe public drinking water. EPA monitors."
        \item Younger age groups, likely from age of 4 or 5 up to 18.
        \item Also perhaps training programs are targeted at the unemployed.
    \end{itemize} \\
    \midrule
    5 & Very high quality:
    \begin{enumerate}[leftmargin=*]
        \item Shows exceptional understanding of the subject, indicating expertise or extensive effort.
        \item The answers are across themes that cover multiple documents.
        \item The answers are a synthesis, reasoning, and analysis of contents from multiple documents.
    \end{enumerate} & 
    \begin{itemize}[leftmargin=*]
        \item "Government programs frequently focus on providing assistance to low-income families, students in under-resourced schools, individuals who are the first in their family to attend college, and adults looking to enhance their job skills through training programs. Furthermore, certain programs may target minority communities and people with disabilities."
        \item "Home care recipients deserve dignity, respect, privacy, informed choices, tailored care, safety, and autonomy for well-being."
    \end{itemize} \\
    \bottomrule
    \end{tabular}
    \caption{Evaluation Scoring Rubric for Response Quality. We rate answers based on the refined rubric to reduce individual annotator subjectivity and biases.}
    \label{tab:rubric}
\end{table*}

% \begin{table}[t] % This specifies that the table should appear at the top of the page
% \centering
% \tiny
% \renewcommand{\arraystretch}{1.5} % Increase row spacing
% \begin{tabular}{p{0.5cm}p{6.3cm}} % Adjust the column widths to fit one column of the page
% \hline
% \textbf{Score} & \textbf{Criteria} \\ \hline
% $1$ & Contains I don’t know. Suspicious of AI generated text-- possibly very long and overly detailed answers. Answer is not relevant to the question or not informative; answer is vague. \\ 
% $2$ &  The answer uses outside knowledge other than the knowledge from the dataset. Answer is just a topic name or a list of topic keywords generated from the models.\\ 
% $3$ & The answer uses some document knowledge (maybe partly from the documents in pretests and answer the questions without having access to the documents), but it is too generic to provide an answer to the question. The supposed answer is just a copy-paste of a relevant document/topic description. \\
% $4$ &  The answer is mostly from the dataset documents but only uses knowledge of a very specific document or passage to answer the question without connecting information for multiple documents. \\
% $5$ & The answer is built based on multiple documents from the dataset and answers the question. \\
% \hline
% \end{tabular}
% \caption{We give an answer rating based on the rubric to reduce individual annotator subjectivity and biases.}
% % \jbgcomment{Use active verb "we rate answers"}
% \label{tab:rubric}
% \end{table}

\section{Parametric Memory and Generated Topics}
\label{sec:parametric}
A strong parametric memory of the topics in the datasets can affect the generated topic outputs. 
%
We examine GPT-4 parametric memory on the two test sets without providing any additional information about the documents, etc.
%
Unsurprisingly, GPT-4 generates 20 topics for  Bills and almost all of them are similar or overlap with the gold topics: \textit{agriculture, health care, education, environment and conservation, defense and national security, taxation and revenue, veteran affairs, energy and utilities}...
%
GPT-4 generates two topics that are similar to that in the Sci-fi data: \textit{first contact protocols}, \textit{Utopian or Dystopian Alien Societies}.
%
The remaining topics are not relevant to the gold topics in the dataset.


\mm{} API costs and time investments are important considerations for most users. Table \ref{tab:cost} summarizes the time and monetary costs for each method. For the automatic control groups, the time refers to the training time for topic generation, which depends on the number of documents in the dataset. In contrast, for \bass{}, time depends on the user's knowledge of the dataset and the diversity of topics: the more varied the topics and the less familiar the user is with the dataset, the more documents they need to review, leading to higher costs. Results show that \lda{} is by far the most efficient and cost-effective method among the four evaluated.

% \ahintext{If you're looking for places to cut, I would put this in an appendix, or summarize it in a sentence (rather than taking up space with a table)}

\begin{figure}[t]
    \centering
    \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=0.48\textwidth]{figures/survey_scores_boxplot.pdf}
    \caption{\bass{} has the highest user satisfaction and confidence on \texttt{Bills}, and \lloom{} has the highest ratings on \texttt{Sci-fi}. Users need to spend more mental efforts to complete the task using \abr{lda} and results in lower satisfaction rate, but the final quality of data exploration answers do not vary much from that of \mm{} models~\ref{fig:automatic_evaluation}.}
    \label{fig:survey_ratings}
\end{figure}

\begin{figure}[t]
\centering
\begin{minipage}{\columnwidth}
\rule{\columnwidth}{0.4pt}  % Top line

\vspace{0.5em}  % Add some space after the top line
\small
You will receive a document about the congressional Bills and a set of top-level topics from a topic hierarchy. Your task is to identify an policy topic within the document that can act as top-level topics in the hierarchy. If any relevant topics are missing from the provided set, please add them. Otherwise, output the existing top-level topics as identified in the document.

\medskip

\textbf{Follow the following format.}

DOCUMENT: [DOCUMENT]
\medskip

HIGH LEVEL CONCEPTS: [HIGH\_LEVEL\_CONCEPTS]
\medskip

YOU SHOULD STRICTLY FOLLOW THE FOLLOWING FORMAT AND OUTPUT THE FOLLOWING INFORMATION
\medskip

RATIONALE: Rationale for choosing the high-level concept
\medskip

PRED CONCEPT: High-level concept for the document

\medskip

----------

\textbf{Previous USER LABELED EXAMPLES (AT MOST THREE) IF AVAILABLE}

----------

\medskip

DOCUMENT: \{\}  HIGH LEVEL CONCEPTS: \{\}  

As a reminder, you should output the following information following the given output format. Your generated concept should not EXCEED FIVE words. Your generated concept should be the teacher's teaching strategy, not a general theme such as 'Education'

RATIONALE: Your rationale for making such a label

PRED CONCEPT: Your generated concept

\vspace{0.5em}  % Add some space before the bottom line
\rule{\columnwidth}{0.4pt}  % Bottom line
\end{minipage}
\caption{Prompt for generating topic suggestions for the \bass{}.}
\label{fig:bass_prompt}
\end{figure}


\section{Topic Modeling in Domain Specific Datasets}
\label{sec:domain_specific_data}
We show topic model outputs on three domain-specific datasets.
%
\begin{itemize}
    \item National Center for Teacher Effectiveness (\abr{ncte}): a teacher and student conversation in a math classroom to access teaching practices associated with overall high-quality math teaching~\cite{xu2024promises}. 
    %
    \abr{ncte} has gold expert defined high-level concepts that require full understanding of education, teaching, and the dataset to derive, not just based on word frequencies-- \textit{Mathematical Language: captures how fluent teacher and students use mathematical language in a classroom}.
    \item Synthetic Sci-fi: a synthetic news dataset generated by the authors about aliens science fiction. Example topics are \textit{Cultural and societal implications: Examining how humanity's institutions, values, and norms might be affected by contact with an alien intelligence}.
    \item Mathematics Aptitude Test of Heuristics (\abr{math}): a math competition question dataset involves AMC 10, AMC 12 with full step solutions and explanations~\cite{hendrycksmath2021}.
\end{itemize}

See Table~\ref{tab:topic_generation_comparison} for example output topics from those models.


\subsection{Post User Survey}
We evaluate users' experience by asking them them survey questions 1: \textit{How satisfied are you on topics that you use to answer the post test questions?} 2: \textit{How confident are you in the quality of answer you put after exploring the data with the tool?}

All the questions aim to understand the usefulness of topics for each method in helping users explore and understand essential contents in a dataset.
%
We plot the user reported ratings in Figure~\ref{fig:survey_ratings}. 
%
% One interesting thing is that what users feel about the tool is not what help them get the best answers. 
% %
% For example, \lloom{} users have high satisfactions and confidence on the \texttt{Sci-fi} data, 
% \zongxiacomment{Discuss the potential problems.}
Overall, users are more satisfied and confident with their answers when an \mm{} is involved than \abr{lda}.
%


\subsection{Example Generated Topics on Domain Specific Data}
\label{app:exmaple_generated_topics}
Table~\ref{tab:topic_generation_comparison} shows example generated topics on domain specific data using different \mm{}s. 
%
\topicgpt{} specifically struggles at generating suitable and specific enough topics for domain specific datasets.









% \begin{table*}[htbp]
% \centering
% \begin{tabularx}{\textwidth}{X}
% \toprule
% \multicolumn{1}{c}{\Large\textbf{Congressional Bills Policy Study}} \\
% \midrule
% In this study, we have a set of US congressional Bills that cover policies on various topics. We want to understand various policy topics the government want to implement. We will first ask you some questions to test your knowledge about the subject before you have access to the data. Then you will explore, label, and sort these documents into different topics. The labels will help you and us understand the distribution of policy topics the Congress care about. You will then use what you have learned from the dataset to answer the questions again. Afterwards, you will be directed to a survey and complete the study. \\
% \midrule
% \rowcolor{lightgray}
% \textbf{Note:} You will start with a base pay of \$6.5 for your participation. Additionally, we offer a performance-based incentive for the quality of your contributions. For EACH question you answer that is closely aligned with the actual dataset and meets our standards for high quality, you will earn an additional \$2. This performance bonus aims to reward accurate responses that demonstrate a good understanding of the dataset. \\
% \midrule
% You will be given a set of Congressional Bills documents. \\
% \midrule
% \textbf{Tasks:}
% \begin{enumerate}[leftmargin=*]
%     \item Think about the pre-test questions while reading the documents
%     \item Either assign an existing label you made to a document or create a new label
% \end{enumerate} \\
% \midrule
% \framedbox{%
% \textbf{An example of what makes a good answer:}
% \begin{enumerate}[leftmargin=*]
%     \item Is not AI generated.
%     \item Is not a direct copy from an AI summary or from the topics
%     \item Incorporate information from multiple documents, not just a single source
%     \item Is based on the contents of the datasets
% \end{enumerate}

% \textbf{Question:} How is the balance between state and federal authority addressed in regulating labor practices?

% \textbf{Answer:} Federal laws can override state regulations in specific labor-related areas. For example, recent changes allow federal agreements on union membership to take precedence over state laws that might otherwise prohibit such requirements.

% The above answer is derived from multiple documents covering related topics.
% } \\
% \midrule
% \textbf{Guidelines for Labeling:} \\
% Each label should be a short phrase (a label) rather than a long description. For example, \textbf{Healthcare Insurance} is a general category, while \textbf{Healthcare Act of 2013 to promote accessible healthcare resources for veterans} would be a description explaining the label, not the label itself. Label the topic associated with the documents. \\
% \bottomrule
% \end{tabularx}
% \caption{Congressional Bills Policy Study Instructions}
% \label{tab:congressional-bills-study}
% \end{table*}



% \begin{table*}[htbp]
% \centering
% \begin{tabularx}{\textwidth}{X}
% \toprule
% \multicolumn{1}{c}{\Large\textbf{Science Fiction Reading Comprehension}} \\
% \midrule
% In this study, you will be given SCIENCE FICTION story SUMMARIES imagined by an AI bot on various themes. We will first ask you some questions to test your knowledge about the subject before you have access to the data. Then you can read these passages, using another AI Assistant that we are evaluating to identify themes to help you answer the same questions. \\
% \midrule
% \rowcolor{lightgray}
% \textbf{Note:} You will start with a base pay of \$6.5 for your participation. The base pay is set lower to accommodate participants who may take less time, but please note that we will reject submissions where little to no effort is obvious. Additionally, we offer a performance-based incentive for the quality of your contributions. For EACH question you answer that is closely aligned with the actual dataset and meets our standards for high quality, you will earn an additional \$2. This performance bonus aims to reward accurate responses that demonstrate a good understanding of the dataset. \\
% \midrule
% You will be given a set of science fiction passages. \\
% \midrule
% \textbf{Tasks:}
% \begin{enumerate}[leftmargin=*]
%     \item Think about the pre-test questions while searching and reading documents
%     \item Either assign an existing label you made to a document or create a new label
% \end{enumerate} \\
% \midrule
% \framedbox{%
% \textbf{An example of what makes a good answer:}
% \begin{enumerate}[leftmargin=*]
%     \item Is not AI generated.
%     \item Is not a direct copy from the AI summary
%     \item Incorporate information from multiple documents, not just a single source
%     \item Is based on the contents of the datasets
% \end{enumerate}

% \textbf{Question:} What are the most popular sports and events you found in this dataset? (List them in order of frequency you see)

% \textbf{Answer:} The sports discussed in descending frequency are baseball, football, hockey, basketball.

% The above answer is derived from multiple documents covering related topics.
% } \\
% \midrule
% \textbf{Guidelines for Labeling (tagging) a document:} \\
% Each label should be a short phrase (a label) rather than a long description. For example, \textbf{Healthcare Insurance} is a general category, while \textbf{Healthcare Act of 2013 to promote accessible healthcare resources for veterans} would be a description explaining the label, not the label itself. Label the topic associated with the documents. \\
% \bottomrule
% \end{tabularx}
% \caption{Sci-fi Exploratory Data Analysis Instructions}
% \label{tab:news-post-analysis}
% \end{table*}


% \section{Efficiency and Time gain}
% Besides knowledge acquisition gain and answer consistency, efficiency and time are also important when analyzing a dataset.
% %
% The actual length of the study participation is ranging from 40 minutes to 95 minutes for most participants. 
% %
% However, topic models improve task completion times by roughly half an hour over users who don't have access to tools---from an average of 80 minutes (baseline) down to 54 minutes.
% %
% Users with the help of topic models not only have higher knowledge acquisition on average and more consistent answers (Figure~\ref{fig:boxplots}), but use less time to complete the task.

% \jbgcomment{It's not clear where the time results are.  Perhaps we could have a plot (perhaps two) with either user time or computer time/cost on the x axis and quality on the y axis}

% \jbgcomment{In addition to user efficiency, it would be good to have computational cost (where we'd do much better than TopicGPT)}











\begin{table*}[h!]
\centering
\small
\begin{tabular}{p{0.12\textwidth}p{0.26\textwidth}p{0.26\textwidth}p{0.26\textwidth}}
\toprule
\textbf{Model} & \textbf{\abr{ncte}} & \textbf{\texttt{Sci-fi}} & \textbf{\abr{math}} \\
\midrule
\abr{lda} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Topic 1}: `apple', `row', `thirteen', `story', `division'
    \item \textit{Topic 2}: `remainder', `row', `division', `sentence', `pencil'
    \item \textit{Topic 3}: `factor', `simple', `color', `fit', `parenthesis'
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Topic 1}: `silent', `quinlan', `prime', `erebus', `vaughn'
    \item \textit{Topic 2}: `humanity', `ravage', `world', `great', `planet'
    \item \textit{Topic 3}: `crew', `ship', `hope', `spaceship', `alien'
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Topic 1}: `day', `team', `dotlinewidthbp', `girl', `mile'
    \item \textit{Topic 2}: `log', `cdot', `lfloor', `rfloor', `frac'
    \item \textit{Topic 3}: `bead', `textif', `endcase', `blue', `begincases'
\end{itemize} \\
\midrule
\topicgpt{} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \texttt{[1]} \textit{Education}: The document discusses teaching methods and classroom interactions.
    \item \texttt{[2]} \textit{Mathematics Instruction}: Discusses teaching methods and student interactions in a mathematics classroom.
    \item \texttt{[2]} \textit{Classroom Management}: Discusses teacher-student interactions and classroom dynamics.
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textcolor{red}{\texttt{[1]} \textit{Science and Technology}: Involves the study and application of scientific and technological advancements.}
    \item \texttt{[2]} \textit{Non-Human Intelligence}: Mentions encounters and interactions with non-human intelligences, their behaviors, and the implications for humanity.
    \item \texttt{[2]} \textit{Interspecies Communication}: Discusses the challenges and methodologies of establishing communication with non-human intelligences.
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textcolor{red}{\texttt{[1]} \textit{Agriculture}: Mentions policies relating to agricultural practices and products.}
\end{itemize} \\
\midrule
\lloom{} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Volume and Dimensions}: Is the focus of this text on calculating volume and understanding dimensions?
    \item  \textit{Symmetry and Shapes}: Is this text about identifying symmetry in shapes or using shapes to teach symmetry?
    \item \textit{Real-world Math}: Does this example integrate math concepts into real-world scenarios or problems?
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Reality Manipulation}: Is reality manipulation or alteration a key theme in this text?
    \item \textit{Interspecies Communication}: Does the example involve efforts or challenges in communicating with a different species or entity?
    \item \textit{Existential Reevaluation}: Does this text describe a scenario that leads to an existential crisis and a reevaluation of human values or society?
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Complex Numbers}: Does this example deal with complex numbers or their properties?
    \item \textit{Probability and Statistics}: Does this example involve calculating probabilities, statistical analysis, or outcomes of random events?
    \item \textit{Divisibility and Primes}: Does this example deal with factors, multiples, divisibility rules, or properties of prime numbers?
\end{itemize} \\
\midrule
\bass{} & 
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Mathematics education}: The teacher employs a method of engaging students through continuous questioning and prompting them to explain their reasoning. This interactive approach helps students articulate their thought processes and understand the concepts being discussed.
    \item \textit{Interactive questioning}: The document showcases a teaching strategy where the teacher uses a game-based approach to teach addition and number sense. 
    \item \textit{Multiplication and division}: The teacher prompts students to explain their strategies, whether they have memorized facts or used other methods, and guides them through the process of writing multiplication and division sentences
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Relations between extra terrestrial and humanity}: The document revolves around the discovery of an alien signal, the subsequent decoding of messages from an ancient intelligence, and the ethical and moral implications of engaging with this non-human entity...
    \item \textit{universe challenges humanitys understanding of existence}: The document focus is on the interaction and communication between humans and an alien species, as well as the societal structures and knowledge exchange.
    \item \textit{interdimensional exploration and politics}: The document describes a scenario where humanity, under the Roman Empire, explores and interacts with a non-human intelligence across multiple realities...
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textit{Probability and prime numbers}: involves calculating the probability of a specific event involving prime numbers, which falls under the study of probability and prime numbers.
    \item \textit{Geometry}: involves geometric properties and relationships within a triangle.
    \item \textit{Number theory}: The topic involves the concepts of greatest common divisor (gcd) and least common multiple (lcm), which are fundamental topics in number theory.
    \item \textit{Polynomial equations analysis}: The topic involves solving an algebraic equation with specific conditions related to its roots.
\end{itemize} \\
\bottomrule
\end{tabular}
\caption{Generated topics for different datasets across various models. For \topicgpt{}, \texttt{[1]} means the first level topics, and \texttt{[2]} means second level topics. \topicgpt{} appears to hallucinate few first-level topics on domain-specific data.}
\label{tab:topic_generation_comparison}
\end{table*}


% \section{Domain-Specific Data}

\section{Generation of the Synthetic dataset}\label{appendix:synthetic}
\onecolumn


\begin{algorithm}
\label{app:scifi_alg}
\caption{Generate Synthetic Dataset (\texttt{Sci-Fi)}}
\begin{algorithmic}[1]

\State \textbf{Input:} Sets of styles $S$, themes $T$, settings $G$, moods $M$, and question-answer pairs $Q$
\State \textbf{Output:} Response text and input parameters stored in an output file

% Generate each user prompt combinations 
\State Initialize $U$ (user prompt combinations) as an empty list
\State Compute the set $P = \{ (k_1, k_2) \mid k_1, k_2 \in K, k_1 \neq k_2 \}$
\For{each $(s, (k_1, k_2), g) \in S \times P \times G$}
    \State Select a random mood $m \in M$
    \State Select a random $(q, a) \in Q$
    \State Add $(s, k_1, k_2, g, m, q, a)$ to $U$
\EndFor

% Randomly shuffle the user prompt combinations
\State Randomly shuffle $U$

% Initialize common words dictionary...
\State Initialize $D$ (common words dictionary) as an empty dictionary
\State Load sample text $T_s$
\State Tokenize $T_s$ into words $W$
\For{each $w \in W$}
    \State Strip punctuation and possessives from $w$
    \If{$|w| > 4$ AND $w$ starts with a capital letter AND $w \notin$ stop words}
        \State Add $w$ to $D$
    \EndIf
    \If{$w$ contains four consecutive digits}
        \State Add $w$ to $D$
    \EndIf
\EndFor
\State Add predefined opening words $\{\text{``In the''}, \text{``On the''}\}$ to $D$

% For each user prompt parameter combination:
\For{each $(s, k_1, k_2, g, m, q, a) \in U$}
    \State Define $W_{\text{avoid}}$ as the 250 most common words in $D$
    \State Generate $P_u$ (user prompt) using a predefined template with $(s, m, k_1, k_2, g, q, a, W_{\text{avoid}})$
    \State Send system and user prompts to LLM
    \State Receive response text $R$ from LLM

    % same process as when creating the dict in the prev step
    \State Tokenize $R$ into words $W_r$
    \For{each $w \in W_r$}
        \State Strip punctuation and possessives from $w$
        \If{$|w| > 4$ AND $w$ starts with a capital letter AND $w \notin$ stop words}
            \State Add $w$ to $D$
        \EndIf
        \If{$w$ contains four consecutive digits}
            \State Add $w$ to $D$
        \EndIf
    \EndFor
    
    \State Write $(R, s, m, k_1, k_2, g, q, a)$ to output file
\EndFor

\end{algorithmic}
\end{algorithm}

\begin{prompt}[title={\thetcbcounter: System Prompt + User Prompt example}, label=prompt:scifi]
\begin{lstlisting}
System Prompt
-------------
You are a clever research assistant generating synthetic data for a human subject study.

Using the style, mood, themes, setting, question/answer pair and the list of words to avoid provided to you in the user prompt, create a Wikipedia-style full plot summary of a science fiction story about first contact with a non-human intelligence including spoilers and the final plot resolution. 

Fastidiously adhere to following rules:
* Use the question/answer pair to provide the reader with descriptive information about the non-human-intelligence, but do not reveal the question in the generated text.
* Avoid cliche openings like: 'In the', 'Within the', 'On the', 'As the'
* Don't the use the words in the user provided list of words to avoid.
* Be creative in your choices of proper names for people, places, and entities.
* Don't choose a word to avoid as a proper name.
* Be creative in your choices of dates.
* Don't choose a year from the words to avoid.
* Do not start the summary with a title. 
* Do not directly reveal the themes to the reader in your generated text. 
* Be sure to emphasize the theme, but do not ask questions in your plot summary. 
* Do not preface your response with statments like: "Here is a Wikipedia-style science fiction plot summary:\n\n\" or make other statements suggesting that the output is generated.
"""

User Prompt
-----------
Style: Hard Science Fiction: This style focuses on scientific accuracy and technical details, often featuring engineers, scientists, and inventors as main characters. Examples: Isaac Asimov, Arthur C. Clarke, and Kim Stanley Robinson.
Mood: hopeful
Theme 1: The Other: Exploring the nature of the alien intelligence, its motivations, and its place in the universe.
Theme 2: Humanity's place in the universe: Questioning humanity's significance, morality, and purpose in the face of a non-human intelligence.
Setting: Space stations or colonies: Isolated and vulnerable, these settings can heighten the sense of tension and uncertainty.
Question: What challenges did the humans face when trying to communicate with the non-human intelligence?
Answer: Understanding the non-human intelligence's  motivations and intentions that are fundamentally different from human principles, making it difficult to comprehend its actions and goals. 
\end{lstlisting}
\end{prompt}
\twocolumn


