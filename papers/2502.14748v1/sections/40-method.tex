% \jbgcomment{Begin this section by saying something like 

% "This section outlines the N techniques which we compare in our user study in Section X"
% }
This section outlines the four methods compared in our study.



\subsection{Study Conditions}\label{subsec:study_conditions}
%We want to measure the information gain and knowledge learned by using various topic model tools and comparing them on content analysis.
% \jbgcomment{This section is talking about things that are not control groups.  Perhaps have a section called "conditions"?

% Or perhaps "control group" is used incorrectly: 
% \url{https://en.wikipedia.org/wiki/Treatment_and_control_groups}
% }

%
% To measure the effectiveness of topic models helping the data exploration process,
%\jbgcomment{Put citations next to name}
% We divide user study participants into four groups, where each group uses a different topic model to explore the data---\lda{}~\cite{blei2003lda},\topicgpt{}~\cite{pham2024topicgpt}, \lloom{}~\cite{lam2024concept} (unsupervised), and our new framework, \bass{}.

We assign participants to four groups, each using a different approach model to explore the data: one traditional model, two recent \mm{}-based models, and our framework. Specifically, we compare \lda{}~\cite{blei2003lda} against \topicgpt{}~\cite{pham2024topicgpt} and \lloom{}~\cite{lam2024concept} (unsupervised), as well as our approach, \bass{}. 
%
We select \lda{} as the representative traditional model because prior research identified it as the most stable, with recent neural topic models failing to outperform it and thus recommending it for content analysis~\cite{hoyle-21}.  Synthetic experiments further support these findings (Appendix~\ref{appen:synthetic_experiments}).



%We use \abr{lda}, TopicGPT, LLooM~\cite{blei2003lda, pham2024topicgpt, lam2024concept}, and \bass{}. 
%
While \lda{}, \topicgpt{}, and \lloom{} (unsupervised) automatically
generate topics for the dataset without human intervention, \bass{}
initially presents no topics to the user.
%
%Instead, the user generates topics by supervising the \mm{}
%suggestions and reviewing documents
Instead, users generate topics by reviewing representative documents selected via active learning and topics suggested by an \mm{} (Figure \ref{fig:bass_topic_generation}). 
%
They can adopt these suggestions or propose new ones.
%
The active learning classifier then uses user inputs to cluster documents and infer labels for the rest to reduce annotation cost.
%

% \jbgcomment{If you're going to abbreviate Figure, create a macro so we can undo it later}
%(Figure \ref{fig:bass_topic_generation}).
%
%Then an active learning classifier trained the users' labels
%infers labels for the remaining documents.
%

%\jbgcomment{These things don't belong together in the same paragraph.  This should just be about BASS.  The questions should be in a different paragraph}
%
The study consists of two phases: pretest and posttest. 
%
In both, users answer the same set of questions from their assigned dataset. 
%
During the pretest, users rely on their own knowledge, allowing the identification of participants with significant prior knowledge of the questions. 
%
In the posttest, users are assisted by topics generated by the assigned topic model---except in \bass{}, where users create the topics. 
%
In all conditions, a search bar with string matching and \textsc{tf-idf}\footnote{\url{https://www.npmjs.com/package/ts-tfidf}} is available during the posttest.
%In the pre-test stage, all users answer the same set of questions from their assigned dataset to evaluate their baseline expertise, helping to identify outlier participants with significant knowledge about the questions.  
%
%In the post-test stage, unsupervised groups use model-generated topics to answer the same questions, while the \bass{} creates their own topics. 
%
%Users need to review representative documents selected via active learning, review or revise \mm{} suggested topics, and rely on an active learning to cluster documents using their inputs.
%
%All users can search for relevant documents using a search bar with string matching and \textsc{tf-idf}.  
%
% After answering the questions, participants complete a survey.  
Next, we discuss the details of the topic models used.

% The baseline condition has no human involvement to get the answers.
%, but users can use the search bar to look for documents and create topics to organize documents.

%Specifically, \abr{lda}, TopicGPT, and LLooM automatically generates topics for the dataset without human intervention, where \bass{} includes no topics initially, and we use active learning to take human input labels to train a classifier that infers labels for the whole corpus. 
%
%Users search and read relevant documents, and build topics with the assistance of an \mm{}.
%
% In the pre-test stage, all users receive the same set of questions from their assigned dataset, designed to evaluate their baseline topical expertise. 
% %
% This standardized approach mitigates potential study outliers by identifying and screening participants with pre-existing knowledge of the subject matter.
% %
% The post-test stage differs between groups. 
% %
% Users in unsupervised groups proceed directly to the post-test with access to model-generated topics to answer the same set of questions. 
% %
% In contrast, the interactive group (\bass{}) proceeds to the post-test without pre-generated topics. 
% %
% Instead, users need to go through a handful representative documents (picked by active learning) and make topics using \mm{}-suggestions, after which an active learning classifier automatically clusters documents based on these user inputs.
% %
% In all cases, users have access to a search bar that allows them to search for relevant documents using string matching (based on keywords in the documents and labels) and \textsc{tf-idf}. 
%
% For the baseline group, this search bar is the only tool available to help answer the questions.
%All users will be presented with six questions as the pretest and use their self-knowledge to answer. 
%
%Later, users in the fully automated groups will be directed to the post test page, with a set of topics generated by \abr{lda} or prompt-based models, where prompt-based generated topics have topic descriptions.
%
%Users can use the search bar to search documents and topics to assist them answering their questions.
%
%Users in \bass{} are displayed no topics first, but they can use the search bar to search documents and start building topics, where an \mm{} generates a list of potential topics for the users to select.
%
% After answering the questions, users are redirected to a survey page to complete the study.
% Depending on the goal of the researchers, simply relying on unsupervised clustering algorithms to learning abstract or specific topics about a dataset might not be sufficient. 
%
% We explore now the technical details of the topic models used in these control groups.
%We delve into technique details of topic models in these groups.

% \begin{figure*}[!t]
%     \centering
% %    \includegraphics[scale=0.15]{figures/demo.jpg}
%     \caption{
%     % Topic-generation process for \bass{} users. From top to bottom, and left to right: users are presented with a document and its summary (automatically generated by an \mm{}), three \mm{}-suggested labels (from which they can select one or input a new one), the already generated topics (hover to see the number of assigned documents), and a search bar to find similar documents using keyword and TF-IDF search.
%     Users will be displayed with selected documents and topics generated by topic models or topics they have added (hover to see the number of assigned documents). In \bass{}, the UI includes three \mm{}-suggested labels (from which they can select one or input a new one), but the users start with empty set of topics. The search bar can find similar documents using keyword and \textsc{tf-idf} search. Users in the baseline group will not have anything \mm{} suggested labels nor topics generated by a topic model, but they can add and make new labels.
%     }
%     \label{fig:bass_topic_generation}
% \end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[scale=0.8]{figures/demo.pdf}
    \caption{
    % Topic-generation process for \bass{} users. From top to bottom, and left to right: users are presented with a document and its summary (automatically generated by an \mm{}), three \mm{}-suggested labels (from which they can select one or input a new one), the already generated topics (hover to see the number of assigned documents), and a search bar to find similar documents using keyword and TF-IDF search.
    Users will be displayed with selected documents and topics generated by topic models or topics they have added (hover to see the number of assigned documents). In \bass{}, the UI includes three \mm{}-suggested labels (from which they can select one or input a new one), but the users start with empty set of topics. The search bar can find similar documents using keyword and \textsc{tf-idf} search. Users in the baseline group will not have anything \mm{} suggested labels nor topics generated by a topic model, but they can add and make new labels.
    }
    \label{fig:bass_topic_generation}
\end{figure*}

\paragraph{Traditional Topic Models} are unsupervised, representing topics as keywords (e.g., \textit{`health'}, \textit{`insurance'}\dots) given the vocabulary and number of topics.
%and do not require prompting.
%\jbgcomment{rather than prompting, I think it would be better to say that they are completely unsupervised after the vocabulary and number of topics are chosen}
%
Probabilistic and neural topic models (\lda{}) are both considered keyword-based traditional topic models in our case.
%
\lda{} uses Bag-of-Words representation to approximate latent topics,
assigning documents with frequently co-occurring words to the same
topic. 
%
Neural topic models use pretrained embeddings such as Word2Vec~\cite{word2vec}, sentence-transformers representations to assign similar documents to the same topic.
%
The topic vector representations for probabilistic and neural topic models are the same, where given a set of documents and a predefined number of topics\footnote{We use \num{65} as the topic number, which is similar to the average number of topics generated by \lloom{} and \topicgpt{}}, each document can be represented as:
\begin{equation}
\theta^d = \{\theta^d_1, \theta^d_2, \ldots, \theta^d_K\},
\end{equation}
where $K$ is the number of topics, and $\theta^d_i$ is the probability of document $d$ assigned to topic $i$.
% \begin{split}
%     \boldsymbol{\theta}_d  &=  [\theta_{d,k}, k = 1, \dots, K] \\
%     &\text{with} \: \sum_{k=1}^K \theta_{d,k} = 1, \forall d, \: d=1,\dots,D
% \end{split}

% \(\theta_{d,k}\) is the probability of topic \(k\) for document \(d\)

%
We run simulated user study on \mallet{} \lda{}~\cite{mallet}, contexualized topic model~\cite{kitty_CTM}, BERTopic~\cite{bertopicMark}, and dynamic variational autoencoders~\cite{Girin_2021} on the three datasets and show that \lda{} generates the best clusterings than neural topic models (Appendix~\ref{appen:synthetic_experiments}), and we use \lda{} as our primary user study of traditional topic models.

% We use \mallet{} \lda{}~\cite{mallet} implementation, which prior research identified as the most preferred by users~\cite{inproceedings}.


%\jbgcomment{I think it's important to say that this is the Mallet implementation}
%We categorize classical topic models as unsupervised methods that require no prompting. From this group, we select \lda{}, proven to be the most effective tool per prior findings~\cite{neuralbroken}. 

%We categorize classical topic models as unsupervised methods that require no prompting. This includes both Bayesian- and Neural-based approaches, which rely on Bag-of-Words (\textsc{BoW}) representations~\cite{blei2003lda,prodLDA} and/or pretrained contextualized embeddings~\cite{bianchi2020pre,kitty_CTM,bertopicMark} to approximate latent topics.
%Here we categorize classical topic models as unsupervised methods that require no prompting. 
%Specifically, classical topic models use either pretrained word embeddings or \textsc{tf-idf} to perform variational inference to generate latent topics.
%
%In both approaches, documents with frequent co-occurring words or similar semantic content are more likely to be assigned the same topic. 
%Given a set of documents, these models automatically uncover the prevalent themes across the corpus. The latent representation of a document is expressed as a mixture of topics, where \(\theta_{d,k}\) is the probability of topic \(t_k\) for document \(d\):  
%In classical topic models, documents with the most common frequent word occurrences or closer word embeddings are likely to be assigned the same topic.
%
%They automatically cluster documents using word frequency prior distributions or embedding cosine similarities.
%
%Given a set of documents with $K$ topics, the latent representation a document is expressed as:
%\jbgcomment{Needs to be next to text or it will be treated as new paragraph}
%\begin{equation}
%\begin{split}
%    \boldsymbol{\theta}_d  &=  [\theta_{d,k}, k = 1, \dots, K] \\
%    &\text{with} \: \sum_{k=1}^K \theta_{d,k} = 1, \forall d, \: d=1,\dots,D
%\end{split}
%\end{equation}
%\begin{equation}
%\theta_d = (\theta_{1}, \theta_{2}, \ldots, \theta_{K}) \quad \text{with} \quad \sum_{k=1}^K %\theta_{k} = 1
%\end{equation}

%where \( \theta_{i} \) is the probability of document \( D \) being associated with topic \( k \).
% \lda{} (and other traditional topic models) typically represent topics with the most probable words within the theme (e.g., \textit{`health'}, \textit{`insurance'}\dots). 

\paragraph{Unsupervised \mm{}-based Exploration} generate topics by iteratively prompting an \mm{}.
%
Unlike probabilistic models, \mm{}-based models build a distributed
representation of the document and then generate a label from that
representation.
%Unlike traditional approaches, these models use documents' semantic
%meanings to generate and assign topics.
%\jbgcomment{I'm not sure that I agree with this.  Perhaps something like:
%Unlike probabilistic models, \mm{}-based models build a distributed
%representation of the document and then generate a label from that
%representation.  }
%
We select \topicgpt{}~\cite{pham2024topicgpt} and \lloom{}~\cite{lam2024concept} as exemplars from this group.
%\mm{}-based topic models generate topics by prompting an \mm{} iteratively until the topics converge.
%
%Unlike traditional models, prompt-based models generate topics based on semantic information from the documents and cluster documents based on their semantic information.
%

\topicgpt{} prompts an \mm{} with fixed examples from the dataset,
merges and refines similar topics (transformer cosine similarity \(<
0.5\)), and then assigns the refined topics to all documents. \lloom{}
first prompts an \mm{} to extract key sentences, clusters them using
cosine similarity, and then prompts the \mm{} again to summarize and
generate topics.

%For a set of given documents, \topicgpt{} samples $N$ documents to generate topics by prompting an \mm{} with fixed demonstration examples.
%
%Next \topicgpt{} prompts an \mm{} with a similar topics (transformer cosine similarity < 0.5) to merge and refine topics.
%
%The last step is to prompting an \mm{} to assign the refined topics to all the documents in the dataset.
%
%On the other hand, LLooM uses a different appraoch to prompt \mm{} to generate topics by first extracting key sentences from the documents, then use transformer cosine similarity to cluster relevant extracted sentences.
%
%Then LLooM uses an \mm{} to summarize clustered extracted sentences to synthesize and generate topics.
%

%Prompt-based topic models require access to a reasonable strong \mm{} that can extract and generate reasonable topics from the data.
%
Prompt-based topic models provide more descriptive topic descriptions,
like \textit{Trade: focuses on the exchange of goods} for
\topicgpt{}. 
%
However, they require numerous \mm{} calls (potentially
expensive for larger datasets), and their practical effectiveness for
user applications has not been fully evaluated.

% they are non-probabilistic and lack the document-to-topic and word-to-topic distributions of traditional models. Moreover, they require multiple \mm{} calls.
%Moreover, a key limitation of these models is that they require multiple \mm{} calls.
%Prompt-based topic models generate descriptive topics that summarizes a document cluster-- \textit{Trade: discusses the exchange of good}.
% The topic representation of topics for prompt-based topic models are a short phrase with a description that describes a common theme documents under the phrase share-- \textit{Trade: discusses the exchange of good}.
%

%On advantage of prompt-based method is that each document can have one more more associated topics, but the disadvantage is that we lose the probability of the document belonging to each topic, which is the $\theta$ in classical topic models. 

\paragraph{\bass{}: Add \mm{}s to Interactive Topic Models.}
% \jbgcomment{I think this section should both be earlier and should be called something like 

% BASS: Adding Generative Deep Learning to Interactive Topic Models
% }
% While algorithms and models
While algorithms often generate imperfect topics misaligned with user
intents, \citet{dietvorst2016overcoming} show users are willing to use
imperfect systems if they can control and modify.
%
The goal of mixed-initiative
interaction~\cite{horvitz1999mixed-initiative} is for humans and
\abr{ai} to collaborate, using their complementary strengths to
enhance accuracy and productivity.
%Mixed initiative interaction theory posits that humans and AI should collaborate as complementary agents, each contributing their suitable capabilities to enhance overall accuracy and productivity~\cite{horvitz1999mixed-initiative}.
%
Building on these insights, we use an \mm{} agent to generate topics
while allowing users to maintain control and modify them.
%
Users supervise the generated topics, approving or revising them
through an interactive process.
%Extending the insights, we want an AI agent generate topics that it can easily recognize, but the preserve the agency of the user.
%
% Unlike fully unsupervised topic models like \topicgpt{}, we focus on increasing user understanding of the topic generation process  to improve interpretability.
%Unlike fully unsupervised topic models like TopicGPT, we want to improve user visibility into the topic generation process to improve topic transparency and interpretability.
%
% Our goal is for users to gain new insights into a dataset through the interactive process.

% Thus, we integrate generative language models into interactive topic modeling, maintaining human supervision by allowing users to accept or modify topics generated by \mm{}s to assist topic generation~\cite{li-etal-2024-improving}. 
%
Specifically, we use the \(\boldsymbol{\theta}_{1:D}\) from a trained
\lda{} model to represent document clusters and similarities, combined
with \textsc{tf-idf} encodings, as features for the active learning
classifier~\cite{alto, li-etal-2024-improving}.
%
During the topic-generation process, users receive a summary and three
candidate labels an \mm{} for each document, which they can approve,
revise, or reject (Figure~\ref{fig:bass_topic_generation}).
%
The labeled documents serve as training data for active learning to update and refine the
topic model's document distributions.\footnote{We use incremental
learning to train the classifier. If a new label class is created, the
classifier is reinitialized and retrained. The prompt template to
generate suggested topics is in Appendix
Figure~\ref{fig:bass_prompt}.}
%
The classifier then generalizes these {\it ``finetuned''} \abr{lda}
topic representation to unseen documents.\footnote{Classifier training details in Appendix~\ref{app:synthetic_experiment_setup}}
%
% To reduce the need for user supervision in labeling, we additionally use a {\it Bandit} classifier, trained iteratively using human feedback on the suitability of labels generated by the \mm{}.\footnote{Prompt template to generate suggested topics is in Appendix Figure~\ref{fig:bass_prompt}.}

%In all conditions, users can always use a search bar to find topics or
%documents associated with the topics by \textsc{tf-idf} matching.
% \jbgcomment{make it clear the package / index you're using}

% \lorenacomment{I think this paragraph might be better placed elsewhere.}

% \jbgcomment{I think we need to lean into the name a little bit more.

% Just like TENOR () added neural representations to ALTO () and improved representations through deep learning, we add even more deep learning via generative language models to TENOR to create BASS: [expand acronym].

% }

% Aiming to generate topics that better match user expectations, interactive topic models involve users in the topic generation process. To combine the strengths of both prompt-based and interactive models, we introduce \bass{}, an approach that uses active learning~\cite{alto, li-etal-2024-improving} to allow users to build topics from scratch with the assistance of an \mm{}. 

%Different from traditional topic models and prompt-based models, interactive topic models require human supervision and refinement during the topic generation process. 
%
%We adopt active learning~\cite{alto, li-etal-2024-improving} to have users build topics for a dataset from scratch with the assistance of an \mm{}.
%
%Specifically, we train an \abr{lda} topic model in advance to calculate the $\theta$ and treat it as additional features for the active learning classifier. 

%The pre-calculated topic representations contain rich information about document clusters and similar documents, we use \textsc{tf-idf} to encode documents, and concatenate with document topic representations as features of active learning classifier.
%


% \jbgcomment{We should do more than just say what BASS does but have more motivation.

% E.g., cite things like 
% \url{https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/mixedinit.pdf}

% We want the AI to do label the things that it can but the preserve the agency of the user.

% We want the user to see everything (unlike TopicGPT) so that they can learn from the labeling process.

% }


%During study, users supervised \mm{} suggested topics for relevant documents, where the labeled documents becomes the training data to update and tweak unsupervised topic model cluster distributions.\footnote{We use incremental learning to train the classifier. If the user makes a new label class, we reinitialize the classifier and retrain it.}
% 
%The resulting active learning classifier generalizes the \textit{'finetuned'} topic representation to unseen documents.
%
%The topic presentations for interactive topic models are generally user-defined, and more flexible compared to that of traditional and prompt-based methods.
%
%Generally, the topics can be complex and abstract that require human reasoning through their experience and stereotype like \textit{Privilege and invisible advantages} or as simple as topics that regular promot-based topic models generate like \textit{trade}.
%
%Human involvement ensures topics align with user expertise and intent, enhancing their relevance and accuracy. This simplifies subsequent analysis tasks, such as answering research questions.
%The human involvement process ensures labels are aligned with user expertise and intent, enhancing the relevance and accuracy of the labels and simplifying subsequent analysis tasks, such as answering specific research questions.

% \zongxiacomment{Include LLM prompt in the App.}
%
% Although topic models can provide a quick summary of latent themes in a dataset, to be able to find research answers from a dataset that requires analysis and reasoning, reviewing relevant documents and to form topics that meet users' research criteria is still an essential step.
%
% Instead of getting all the latent topics and revise or add new topics without aligning them with users' intents in the first place, we propose a \bass{}, where users start the document reviewing process by collaborating with \mm{}s to generate topics that are aligned with their data exploration intents, while using active learning to infer topic distributions for the rest of the datasets.

% \bass{} incorporates \mm{} prompting, active learning, and topic modeling into the pipeline to maximize content analysis effectiveness and minimize costs.
%
% Given a set of documents, active learning directs a user's attention to the most beneficial document they need to look at, while also serve as a \textit{topic predictor} that refines and infers topic distributions for all the documents based on the user inputs.
%
% We use an \mm{} generating potential label suggestions for the selected document, with a summary to assist users making labels. 
% 
% Additionally, we initialize a binary classifier (\textit{bandit}) that trains with \mm{} generated and human selected/revised labels to detect the quality of \mm{} generated topics. 
%
% Once the bandit classifier has a reasonable number of training examples, it approximates an \mm{}'s generation confidence to hint users whether they should skim or read the given document carefully. 
%
% We delve into method details in the rest of this section.




% \bass{} first uses traditional topic models (\lda{}) to provide users with an initial global overview of document clusters fully based on word frequencies. Then, it employs active learning~\cite{active-learning} to sample the most informative documents guiding users to individual documents. We use prompt an \mm{} to generate a descriptive topic, two other random topics, and a rationale for the topic. Users can select or revise the most suitable topic. Additionally, we initialize a \textit{bandit} binary classifier that trains on \mm{} generated and human selected/revised labels to detect the quality of \mm{} generated topics. Once the classifier has a reasonable number of training examples, it automatically uses \mm{} generated labels without human intervention until it detects low-quality generated topics. Our pipeline is discussed in five separate parts.

% \subsection{Classical Topic Models as Active Learning Features}
% Classical topic models automatically cluster documents using word frequency prior distributions or embedding cosine similarities.
% %
% Given a set of documents with $K$ topics, the latent representation a document is expressed as:
% \begin{equation}
% \theta_D = (\theta_{1}, \theta_{2}, \ldots, \theta_{K}) \quad \text{with} \quad \sum_{k=1}^K \theta_{k} = 1
% \end{equation}
% %
% where \( \theta_{i} \) represents the probability that document \( D \) is associated with topic \( k \).



% In the \bass{} pipeline, the first step is training a primary topic model using \lda\footnote{In practice, any SOTA algorithm could be used, but we choose \lda{}-MALLET given its proven superior performance for generating high-quality topics from the content analysis perspective \cite{neuralbroken}.}. Let \(\boldsymbol{\beta}_k = [\beta_{k,v}]_{v=1}^V\) and \(\boldsymbol{\theta}_d = [\theta_{d,k}]_{k=1}^K\) represent the word-topic and document-topic distributions for each discovered topic \(k\) and document \(d\) in a corpus of size \(D\), respectively. Each of the \(K\) topics is identified based on the top-\(m\) words with the highest probabilities:
% \begin{equation}
% \text{Topic}_k = \{w_i \mid i \in \operatorname{arg\,max}_{i=1,\dots,V}^m (\beta_{k,i})\}
% \end{equation}
% Grouping documents by topics lets users quickly understand the main themes and structures in the corpus, facilitating further navigation and analysis.

% \subsection{Active Learning Data Sampling}
% Active learning not only infers topics for unseen documents, it also uses uncertainty sampling to direct a user's attention to the most beneficial document.
%


%Discuss active learning data sampling to hand users documents instead of letting them pick manually
% By leveraging the primary topic model's \(\boldsymbol{\theta}\) and concatenating it with the documents' TF-IDF representation, we construct an active learning framework that directs the user's attention to the most informative documents to label first.


% \subsection{\mm{} Assistance}
%Specifically, we use GPT-3.5-Turbo as our \footnote{https://platform.openai.com}
\begin{comment} % this was commented on 14.10.24
\paragraph{\mm{} Assistance}
Having the users build topics from scratch is an unfair comparison to other groups that users have access to \mm{} or \abr{lda} generated topics to conduct data exploration.
%
Thus, we add a small assistance to active learning that prompts an \mm{} to generate latent topics for user selected documents to assist users create topics.
%
Given a user selected document \( d \), we prompt an \mm{} to generate at most three candidate labels for it, and a description of the document.
%
Users can either approve, revise, or reject the \mm{} generated labels, which are used to train the classifier.
%
During the labeling process, users can either use the search bar to direct to relevant documents or use active learning to automatically direct them to the next document.
\end{comment}

% selected by the Active Learning classifier, we get its most dominant topic:
% \begin{equation}
% k^* = \arg\max_{k} \boldsymbol{\theta}_{d,k}
% \end{equation}
% and two more representative documents of \(k^*\):
% \begin{equation}
% d'_{1}, d'_{2} = \left\{ \arg\max_{d \in T} \boldsymbol{\theta}_{d,k^*} \right\}_i \mid i \in S
% \end{equation}
% where \( T \) is the set of top-10 documents with the highest \(\boldsymbol{\theta}_{d,k^*}\) values, and \( S \) is a randomly chosen subset of size \( |S| = 2 \) from \(T\). We then provide \(d, d'_{1}, d'_{2}\), along with the top keywords for the topic and any previously generated labels, as input to two independent of \mm{}s (we specifically use two instances of \texttt{GPT-3.5-Turbo}).

% The first \mm{} is instructed to generate a high-level label and a rationale for the selected document, aiming to balance specificity and generality. The second \mm{} is tasked with generating intentionally poor labels to obtain negative examples for the \textit{Bandit} classifier training. By presenting the user with both good and bad labels, we expect the user to identify the ``good label'' as positive, while the other two serve as negative samples.

% A low-temperature setting is used for the first \mm{} to ensure high-quality labels, while a high-temperature setting is used for the second \mm{} to encourage diverse and suboptimal labels.

% \subsection{\textit{Bandit} Classifier}
% %Discuss the bandit classifier to detect low quality generations. What features to use and how they make sense. Also, the features can take human feedback.
% To minimize the need for user supervision in the labeling process, we utilize a \textit{Bandit} classifier, trained iteratively using human feedback on the suitability of labels generated by the \mm{}. Under the hood, the \textit{Bandit} classifier is a Logistic Regression model trained via stochastic gradient descent (SGD) learning. To address the dataset imbalance --each labeled document has two negative samples for every positive one-- the classifier dynamically adjusts the class weights during each iteration.

% The classifier's features are a concatenation of three key elements: 1) the embeddings of the labels; 2) the element-wise product of the label embeddings and document embeddings; and 3) the average cosine similarity between the document embeddings and the top-k most representative topics identified by the underlying topic model.

% We derive topic embeddings by treating the chemical descriptions as sentences. The element-wise product captures complex interactions between label and document embeddings, while the average cosine similarity ensures contextual relevance and prevents labels from being overly specific to individual documents.


% \subsection{Human-in-the-loop Collaboration}
%Discuss human choose prompts, topics, generate topics based on given research questions.
% User involvement is essential for refining \mm{}-generated labels. During the first iterations --30 in our case--, users review documents and select the most appropriate labels from those generated by the \mm{}s, using this feedback to train the {\it Bandit} classifier. After this training phase, user input is only required when the {\it Bandit} classifier's confidence is low or new labels are introduced.

% This human-in-the-loop process ensures labels are aligned with user expertise, enhancing the relevance and accuracy of the labels and simplifying subsequent analysis tasks, such as answering specific research questions.







