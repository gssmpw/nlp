% Exploring and analyzing data strategically and efficiently have become more difficult with increasing data sizes nowadays.
% %
% Probabilistic topic models (\lda{}) use word distributions and neural topic models (\bertopic{}) use pre-trained word embedding to automatically discover latent themes within a corpus~\cite{blei2003lda, bertopicMark}. 
% %
% They have become popular tools for inducing concepts from large unstructured texts by analyzing low-level keyword features from the corpus such as \textit{`health'}, \textit{`insurance'}\dots
% %
% However, those classical topic models produce topics and clusters based on word low-level word signals and often generate topics that are less interpretable and less useful to users~\cite{hoyle2021automated}.
% %

% \jbgcomment{First two sections would be better with running example that ties to one of our two datasets.  E.g., User X wants to Y.  Put that in first paragraph then use that in section 2 extensively, the sorts of output you'd get for that problem for TMs / LLMs}

% 
When a researcher approaches a text corpus, they do so with particular
research goals or questions in mind~\cite{krippendorff2004content}:
%
``Is immigration news coverage focused narrowly on the economic costs
of immigration?''~\cite{annesley2013investigating}; ``How well are the
priorities of the American public reflected in the policy activities
of government?''~\cite{governing}.
%
To answer these questions, analysts often use corpus analysis
techniques like traditional topic models~\cite[Section~\ref{background}]{blei2003lda}.
%
Roughly, these tools help structure a document collection by organizing it into
interpretable, high-level categories or topics: newspaper
articles may produce topics relating to the national economy, local
gossip, or sports.
%
%To simulate and measure the learning process facilitated by topic models, we conduct a user study with two datasets: congressional bills and science fiction summaries generated by an \mm{}.
%
For example, consider a scenario where a federal legislator is
preparing for a congressional hearing on whether new infrastructure
projects harm existing ecosystems.
%
Anyone trying to tackle this issue should have a good answer to the
question: ``What are common policy actions taken by governments in the
US to manage land use?'' (Section~\ref{study_setup}).
%effectively
%


We want to evaluate users' ability to answer similar questions with the help of \abr{ai}.
%
In contrast, a long line of work has assessed the usability and
interpretability of the topics produced by traditional models~\cite{Newman2010AutomaticEO, doogan-buntine-2021-topic},
% \ahintext{could stand to add more probably}, 
but comparatively little attention has been given to their ability to
foster human understanding---that is, their capacity to help answer
essential questions.
%
Through this new evaluation, we test comparative advantages and disadvantages of \mm{}s with traditional topic models for large corpora exploration.
%yet
%This work addresses this shortcoming with a more systematic human evaluation of topic models (traditional and \mm{}-based topic models in Section~\ref{background}), asking: what do humans learn from topic models?
%
% \jbgcomment{Add a forward pointer, I don't think we need dataset details in the intro}
We run a human-in-the-loop evaluation to evaluate how effectively traditional topic models and \mm{}s help users understand content using two datasets~\ref{study_setup}.

% user study evaluating how effectively
% topic models help users understand content using two datasets~\ref{study_setup}.
% :congressional bills~\cite{AdlerWilkersonBillsProject} and a synthetic
% dataset of science fiction summaries.
%science fiction
                                                                                                                                                                                                                                               %summaries
%generated by an \mm{}.

% \jbgcomment{Forward point to literature review section.}

% \jbgcomment{BASS shouldn't be embedded in a paragraph.  We need to have a good introduction of why we use BASS:

% TopicGPT does it all!  why do you need BASS:

% - computer is imperfect
% - user has specific needs
% - IKEA effect / learning}

% \ahintext{Add a paragraph summary of the user study here, framed as a contribution...}

Traditional models and \mm{}s can automatically generate
topic landscapes without human supervision.
%Traditional topic models and \mm{}-based models like \topicgpt{}~\cite{pham2024topicgpt} automatically generate topic landscapes without human supervision. 
%
However, do these generated topics truly help researchers answer their
essential questions?
%
Despite high automatic evaluation scores, traditional models often
produce outputs that are technically coherent but practically
unreadable and unhelpful for understanding
datasets.
% ~\cite{hoyle2021automated}.
% \jbgcomment{would be better to avoid self-cites for submission}
%
Users have specific information needs that computational models
struggle to meet, particularly given challenges like hallucination and
information loss in long
documents~\cite{liu2023lostmiddlelanguagemodels}.
%
Critically, most \mm{}-based topic models have been evaluated solely
through automatic metrics like coverage rate~\cite{lam2024concept} and
adjusted rand index~\cite{ARI}, without meaningful human validation.
%
These metrics fail to capture how social scientists and experts use
and derive value from topic models in real-world applications.


% \jbgcomment{Add a topic sentence to the below paragraph introducing BASS by name}
Thus, human validation of topic outputs are more important than automatic scores.
%
To add personalization that can satisfy users'
specific needs for a dataset, we add an \mm{} to interactive topic model (\bass{}) to provide topic suggestions to users while users
have the freedom to revise the topics during the topic generation
process, and use active learning to infer topics for the remaining
documents, enabling guided exploration of the data and reducing the
number of documents needed to label.


% \jbgcomment{explicitly list all of the models used in the study in below paragraph}

% We design a user study where participants use different topic models
% to answer dataset-relevant questions. We compare traditional
% probabilistic models with newer \mm{}-based variations across two
% datasets, each involving 60 users.
We run a user study to evaluate traditional topic models (\abr{lda}), unsupervised \mm{} methods (\topicgpt{} and \lloom{}), and \bass{} on two datasets, each involving 60 users.
%Thus, we design a user study that tests users to explore dataset relevant answers to a set of research questions with the help of various topic models, which we compare traditional probabilistic topic models with newer variants based on language models using two datasets with 75 users per dataset.
%
% As part of this study, we introduce \bass{}, an \mm{}-assisted interactive topic model that suggests potential topics and uses active learning to efficiently infer topics for the remaining documents, enabling guided exploration of the data.
%
Our evaluation comprises: (1) standard cluster metrics to compare
generated clusters against gold clusters (2) transformer pairwise
similarity to assess the consistency and answer quality of user
responses within each group, and (3) manual pairwise preference
annotation to evaluate user preferences between answers generated by
humans using different topic models.
%
% We apply these measures to users' responses both before and after they explore the dataset: all approaches improve response quality on dataset-relevant questions compared to baseline users who only have access to a basic document search bar, without additional analytical tools.
%
Traditional models like \abr{lda} fall
behind \mm{}-based methods for data exploration,
but \mm{}-based methods still have limitations: they are hard
to scale up, and there is no one prompt fits all method
for \mm{}s, making \mm{}-based methods hard to
generalize to diverse datasets. 
%
Section~\ref{qualitative} summarizes the advantages, shortcomings and best practices for choosing between traditional topic models and \mm{}s.

% of traditional and modern topic modeling approaches.
%
% \jbgcomment{Give forward pointers to relevant sections}
%
% Additionally, we conclude best practices on when to use each technique
% and highlight situations where human involvement is necessary.

% While user answer quality does not vary substantially between models, the \emph{stability} of answer quality and self-reported user preferences tend to be higher for interactive and \mm{}-based variants compared to baselines and traditional topic model (Section~\ref{User_study})---stability being a key criterion for the validity of downstream findings and applications~\cite{neuralbroken}.



% \bass{}, \lloom{}, \topicgpt{}, and \lda{} on two case studies --analyzing congressional Bills government policies and exploring interesting topics for \abr{ai} generated news contents.
% organizing \abr{acl} Anthology Corpus~\cite{acl-ocl} to different tracks to facilitate a smoother and more targeted review process by matching papers with reviewers and identify the important big topics in recent \abr{nlp} community, and identifying good math teaching practices from e National Center for Teacher Effectiveness (\abr{ncte}) transcripts~\cite{demszky2023nctetranscriptsdatasetelementary}. 
%
% Our evaluation of topic models is end-to-end by asking pre-test questions to assess users' prior knowledge on the subject, followed by post-test questions after users interact with one of the above topic models.
%Our evaluation is end to end for topic models by asking pre-test questions to test users prior knowledge on the subject and post-test questions after users use one of the above topic models.
%
% We measure user learning through changes in their confidence levels after using the models, and expert evaluations of their responses.

% We compare traditional probabilistic topic models with newer variants based on language models using two datasets with 75 users per dataset, finding that all approaches improve the quality of responses to data-relevant questions significantly when compared to baseline users who only have access to a basic document search bar, without any additional analytical tools.
% \ahintext{todo: add statement about comparison to search baseline}.
%

% \ahintext{explanation of BASS here}.
%
% We use transformer pairwise similarity to measure the stability and consistency of user answers within each group, and manual annotations to measure the 







