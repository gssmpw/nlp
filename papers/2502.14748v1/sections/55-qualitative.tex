
% \section{Qualitative Analysis}
%
% \jbgcomment{``human evaluation'' is vague, be precise}
Although pairwise response preference and Bradley-Terry scores (Figure~\ref{fig:bradely_terry}) show that our human supervised \mm{}-based
topic model (\bass{}) has the highest winning probabilities, user comments 
%also point to dissatisfaction with the
highlight dissatisfaction with the substantial
%overwhelming
effort required---even for a relatively simple
dataset that can be fully unsupervised (\texttt{Bills}).
%
% \jbgcomment{You don't ``run'' a qualitative analysis}
%We first select user responses and comments from each model, then summarize the takeaways and things researchers should keep in mind when they have their specific goals on exploring a dataset.
We first select user responses and comments from each model, then summarize key takeaways and considerations for researchers based on their specific dataset exploration goals.

\subsection{User Response Selection}
%For each group, we select user responses who left a positive comment and user responses who left a negative comment and review what users like about each tool.
For each group, we select user responses with positive and negative comments to understand what users like about each tool.
%
% \jbgcomment{You didn't use \texttt{Bills} earlier in talking about the dataset, be consistent.}
\texttt{Bills} users initially demonstrated higher uncertainty, with 38\% responding ``I don't know'' during pre-test (31\% for \texttt{Sci-Fi}). 
%
In post-test, some participants (\topicgpt{}: 13\%, \lloom{}: 5\%, \bass{}: 7\%) %provided answers by verbatim recitation of topic descriptions, demonstrating limited comprehension and originality.
recited topic descriptions verbatim, showing limited comprehension and originality.
% \jbgcomment{I'm not sure what this sentence means.  Is it:
% The users who saw \mm{}-generated topics relied on the summaries more
% and answered more questions with {\it ``I don't know''} or simply
% copying the summaries verbatim.  }
%
% \jbgcomment{I think the first part of this next sentence was already said, so focused on the wider variance.}
% A notable trend was the overuse of labels and copy-pasting, with a wider range of response quality, as reflected by highly varied annotator ratings. 
%A notable trend is that overuse of copying and pasting \mm{} generated labels resulting in overall higher consistency than \abr{lda} (Figure~\ref{fig:automatic_evaluation}).
A notable trend is that excessive copying and pasting of \mm{}-generated labels leads to higher overall consistency than \abr{lda} (Figure~\ref{fig:automatic_evaluation}).
%
% \jbgcomment{This doesn't make sense to me\dots they can't have knowledge of a dataset that doesn't exist.  Were they more willing to make stuff up, and it happened to be right?}
\texttt{Sci-Fi} is synthetically designed to ensure that documents within the same topic are interconnected and relevant to the question, requiring less domain expertise to interpret. 
%
As a result, \texttt{Sci-Fi} user responses often link information from multiple documents rather than copying \mm{} topic descriptions.
% \zongxiacomment{Add a little more analysis here}.


% Based on the user comments and our study setup and results, we
% summarize the advantages and shortcomings of \mm{}-based topic models,
% traditional topic models, and human supervised \mm{}-based topic
% models.

% \paragraph{Traditional Topic Models} Advantages: Four positive user reviews report that \abr{lda} keywords can provide them an broad overview of possible themes from the dataset, giving them some guidance on what types of documents to search to answer the questions.
% %
% Additionally, the keywords add users more freedom and imagination to name a topic, which helps them further analyze the data.
% %
% \abr{lda} has the best computation advantage (fast, little computation resource, free) than all other conditions, which require prompting an \mm{}.

% Disadvantages: 1. keyword based topics are less user friendly and requires more mental effort to use them.
% Two users report that they need time to start getting used to using the tool since the topic keywords are a confusing and daunting to the users in the first place.
% %
% 2. Repetitive topics and keywords and uninterpretable topics: \abr{lda} is based on word distribution, which often generates topics with repetitive keywords that do not contribute to data understanding such as \textit{bank} and \textit{banks}.
% %
% In addition, there are several topics with nonsense keywords which also confuse and scare users.


% In sum, traditional topic models are fast, efficient, scalable, but less user friendly and require more user efforts to understand the topics.

% \paragraph{Unsupervised \mm{}-based topic models}
% On datasets with document contents that have clear topic boundaries, \mm{}-based topic models produce topic phrases that are more favorable by humans. 
% %
% Specifically, users prefer the topic descriptions over topic phrases to understand a topic, and pick the topic they are aiming to search.
% %
% \topicgpt{} generates better and more systematic topics than \lloom{} on the Bills dataset, but \lloom{} is more stable and generalizable to diverse datasets since the prompt and topic generation algorithms are intrinsically different between two methods.

% Disadvantages: Users complain that the topics generated are overly generic to be helpful for the task, especially on the Sci-fi dataset, where \topicgpt{} only generates on supertopic \textit{Science and Technology} and three subtopics \textit{Non-Human Intelligence, interspecies Communication, Ethical and Moral Implications}.
% %
% For domain specific datasets that require abstract human reasoning, \topicgpt{} fails.
% %
% \lloom{} can extract abstract topics, but its algorithmic limitation, where it extracts summaries from all the documents then put all the summaries as a prompt to ask an \mm{} to generate topics.
% %
% First, due the the size of our datasets (both over 2,000 documents), \lloom{} fails at fitting 2,000 document summaries in the prompt to generate topics, thus we had to process the documents chunk by chunk, which induces an issue on how to chunk the datasets and increase the cost.
% %
% In addition, users report \lloom{} and \topicgpt{} topics are inaccurate and they see the documents are incorrectly assigned to a topic (trade and policies), showing that unsupervised \mm{}-based topic models struggle on long-context hallucinations.
% %
% Cost: expensive, training time long. Does not work well on domain specific documents.


% \paragraph{Supervised \mm{}-based topic models}
% Advantages: unsupervised topic models can automatically generate topics and reduce human efforts. 
% %
% However, traditional topic models can generate garbage topics, where \mm{}-based topic models can generate hallucinated topics.
% %
% In addition, users do not have controls on the output topics and they do not always align with user intents.
% %
% A great advantage users feel using \bass{} is that they can see the \mm{} generated topics, but they can also revise or define their own topics that help them answer the questions.


% Disadvantages: users claim that there does not seem to have an end of labeling documents, and hard to know when to stop generating topics.
% %
% The definition and boundary of good stop is vague to define.
% %
% In addition, going through the documents with \mm{} suggestions is an overwhelming tasks for many users, where they need to read through documents, even under the help of an \mm{}.
% %
% For easy datasets like Bills, user intervention and verification does not help improving the process, which users usually approves the suggested topics without editing (93\% approval rate) compared to Sci-fi (62\% approval rate). 
% %
% Rather, using pre calculated topics from \topicgpt{} can reduce much user efforts.


\paragraph{Traditional Topic Models} 
% \jbgcomment{Not a huge fan of this advantages / disadvantages structure. Would be better to talk through it in words.  But changing this is lower priority than other tasks.}
are 
%less computationally expensive, 
more accessible to social scientists without advanced technical expertise, and pose fewer data privacy concerns than \mm{}s. However, their outputs are less user-friendly.
%
%\abr{lda} user reviews show that topic keywords can provide a broad overview of possible themes within a dataset, giving guidance on the types of documents to search for when answering questions.
\lda{} user reviews highlight that topic keywords offer a broad overview of dataset themes, helping guide document searches when answering questions. 
%
%Additionally, the flexibility of naming topics based on keywords gives users more freedom and imagination, aiding further data analysis.
The ability to name topics based on keywords provides users with flexibility and creative freedom for further analysis.
%
%Computationally, \abr{lda} is faster and more resource-efficient compared to other approaches that prompt \mm{}s.
Additionally, \abr{lda} is computationally faster and more resource-efficient than \mm{}-based approaches.

However, keyword-based topics 
%are less user-friendly and demand
require more effort to interpret.
%
%Two users reported needing time to adapt to the tool as the initial keyword-based topics were confusing and daunting.
%
%One users mentions that \abr{lda} generates repetitive and uninterpretable topics due to its reliance on word distributions.
%
%For example, \textit{bank} and \textit{banks} can appear in the same topics, or nonsensical keywords such as \textit{thing, matter...}, can confuse and deter users.
Two users reported an initial learning curve, finding keyword-based topics confusing and overwhelming. Another noted that \lda{} generates repetitive or uninterpretable topics. For example, \textit{bank} and \textit{banks} may appear in the same topic, while vague terms like \textit{thing} or \textit{matter} can confuse users.

% In summary, traditional topic models are fast, efficient, and scalable but are less user-friendly, requiring greater user effort to interpret the generated topics.

\paragraph{Unsupervised \mm{}-based methods}
%generate topics users favor more on datasets with clear topic boundaries that are easy to identify.
perform well on datasets with clear topic boundaries, generating topics that users find more intuitive.
% On datasets with clear topic boundaries, \mm{}-based topic models
% produce topic phrases that are more favorable to users.
%
Users prefer topic descriptions over topic name or word lists.
%
Among the models, \topicgpt{} generates more systematic topics on the
\texttt{Bills} dataset compared to \lloom{}, while \lloom{} is more stable, generalizable, and capable of extracting more abstract topics across diverse datasets due to differences in their prompting and topic generation algorithms (See topic examples in Appendix~\ref{app:exmaple_generated_topics}).
%


%On the other hand, 
However, users find overly generic topics less useful for realistic tasks.
%, making them less useful for our realistic tasks.
%
For instance, on the \texttt{Sci-Fi} dataset, \topicgpt{} generated a single
broad topic, \textit{Science and Technology}, with three
subtopics: \textit{Non-Human Intelligence, Interspecies
Communication,} and \textit{Ethical and Moral Implications}.
%
In domain-specific datasets requiring abstract reasoning, \topicgpt{} tends to hallucinate,
producing overly broad or unrelated topics---e.g., {\it agriculture} for a math dataset (Appendix~\ref{sec:domain_specific_data}).
%and is overly broad in its topic selections, such as generating \textit{agriculture} for a math dataset, entirely unrelated to the subject (Appendix~\ref{sec:domain_specific_data}).
% \jbgcomment{Would be much better to have a clear example of this}
%
\lloom{} can extract abstract topics but struggles with scalability due to its summarization-based approach. 
%
Processing large datasets (over \(2\,000\) documents) requires chunking,
complicating topic generation. % process.
%


%Additionally, users observed inaccurate topic assignments, such
% as documents incorrectly classified under environment and policies.
Users also noted inaccurate topic assignments, with documents misclassified under broad categories like {\it Environment} and {\it Policies}.
%
%For example, we ask \textit{What policies and regulations does the U.S. government implement to address water contamination and ensure environmental protection?} in Bills.
%
%The generated topic, \textit{Environmental Efforts: Does this text relate to efforts in environmental protection or conservation?} (\lloom{}), is where users were most likely to explore relevant documents. 
For example, in the \texttt{Bills} user study, when asked \textit{What policies and regulations does the U.S. government implement to address water contamination and ensure environmental protection?}, users found the \lloom{}-generated topic \textit{Environmental Efforts: Does this text relate to efforts in environmental protection or conservation?} most relevant. 
%
However, user feedback indicates that some documents such as a proposed State Plan Amendment (SPA) related to Medicaid or health services are incorrectly classified under this topic. 

%
% For example, we find a document describing the process of approving or disapproving a proposed State Plan Amendment (SPA) related to Medicaid or health services, which is unrelated to environmental protection.
% \jbgcomment{again, make this explicit: what's the document, what's the topic}
%
% Unsupervised \mm{}-based models have long-context hallucinations.
%
Unsupervised \mm{}-based models are also computationally expensive, requiring long training
times, and tend to produce overly generic topics in domain-specific datasets. 
% (Appendix~\ref{appendix} provides additional examples).
%for more details on the topics generated for two additional domain-specific datasets not included in the user study).

\begin{table*}[h!]
\centering
\small
\begin{tabular}{>{\centering\arraybackslash}m{0.15\textwidth} m{0.25\textwidth} m{0.25\textwidth} m{0.25\textwidth}}
\toprule
\textbf{Approach} & \textbf{Advantages} & \textbf{Disadvantages} & \textbf{Suitable For} \\
\midrule
Traditional Topic Models &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Fast computation
\item Low resource use
\item Less data privacy concerns
\item Broad theme overview
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Less user friendly
\item Potential repetitive keywords, topics, and useless topics
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Large document collections
\item Low resource Settings
\item Preliminary exploratory analysis
\end{itemize} \\
\midrule
Unsupervised \mm{}-based Models &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Descriptive topic phrases and descriptions
\item Sometimes can induce abstract topics
\item Cluster based on semantic, not words distribution
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Overly generic topics
\item Document assignment hallucinations
\item Expensive computation
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Small document collections
\item Data with clear topic boundaries
% \item Diverse document types
\end{itemize} \\
\midrule
Supervised \mm{}-based Models &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item No need to traverse all documents manually
\item Flexible user topic definition and supervision
\item Avoids garbage topics
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Need mental efforts
\item Require user expertise
\item Inconsistent effectiveness
\end{itemize} &
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
\item Iterative and advanced data analysis
\item Answering abstract conceptual questions
\item Low resource settings
\end{itemize} \\
\bottomrule
\end{tabular}
\caption{Even though \mm{}s can generate more interpretable topics than traditional topic models, they face challenges at scaling up on large corpora data. Combining humans with \mm{}s can reduce \mm{} hallucinations and brings scalability.}
\label{tab:topic_modeling_comparison}
\end{table*}

\paragraph{Human supervised \mm{} topic generation (\bass{})} gives users full control over topic definitions, allowing them to define and refine topics to fit their needs.
%
Unlike traditional topic models, which can generate uninterpretable
topics, or unsupervised \mm{} models, which may hallucinate
topics, supervised models ensure user-defined topics. %have a users control topic definitions.
%
Users appreciate \bass{} for allowing them to view \mm{}-generated topic
suggestions while also revising or defining their own topics.% to complete their tasks.
%
The active learning process reduces the necessary number of queries to the \mm{}, lowering prompting costs compared to unsupervised \mm{}-based approaches (Appendix~\ref{sec:cost}).

%A disadvantage of human supervision is that users report difficulties in determining when to stop
%the generation process, as the criteria for stopping the process is
%not defined.
A drawback of the human supervision loop in \bass{} is that users struggle to determine when to stop the generation process, as stopping criteria are undefined.
%
Reviewing documents with \mm{}-generated suggestions can be
overwhelming, as users must read through more documents than
unsupervised methods, even with \mm{} assistance.
%
On simpler datasets like \texttt{Bills}, user supervision offers limited
benefits, with users approving suggested topics 93\% of the time
compared to 62\% on the \texttt{Sci-Fi} dataset.
%
In such cases, 
%relying on 
unsupervised \mm{}-based models can significantly reduce user effort.

% In conclusion, supervised \mm{}-based topic models provide a balance between automation and user control, though they can be resource-intensive and challenging to manage for large or complex datasets.

% \jbgcomment{THis section promised qualitative analysis, but it starts with conclusions.  Save that for later.}

\paragraph{Are \mm{}-based approaches ready to replace traditional topic models for data exploration?}
%
Corpora exploration and understanding is an iterative process and is rarely a matter of
running an algorithm once and getting your desired answers.
% 
While emerging \mm{}-based approaches are exciting and new, there are
important considerations to be made in order to use them suitably: Are the predominant
metrics reliable, and what are reliable ways to evaluate emerging \mm{}-based methods?
%
Traditional topic models use automatic topic coherence, but this metric does not generalize to neural topic models.
% does not always reflect good and interpretable outputs for neural
% models.
%~\cite{aletras-stevenson-2013-evaluating, coherence}.
% \jbgcomment{some of these have been cited elsewhere, tone down self-cites}
%
%the emergence of new variations, such as
%prompt-based \mm{} topic models, makes it increasingly challenging
%to evaluate their relative performance.
Additionally, coherence is not applicable to \mm{}-based methods, making it increasingly difficult to evaluate their performance and usefulness.
%
Trade-offs between approaches are not always clear: \mm{}-based
methods are more computationally expensive and generate over generic topics.
%
%While traditional \abr{lda} has overall higher clustering metrics than
%unsupervised topic models (Table~\ref{tab:cluster_metrics}), automatic metrics
%(Figure~\ref{fig:automatic_evaluation}) and human evaluations
%(Figure~\ref{fig:bradely_terry}) show that \mm{}-based approaches are
%not always the best for every task, and \abr{lda} might still be more useful than fully unsupervised \mm{} approaches.

While traditional \lda{} outperforms unsupervised \mm{}-based methods (Table~\ref{tab:cluster_metrics}), automatic metrics (Figure~\ref{fig:automatic_evaluation}) and human evaluations (Figure~\ref{fig:bradely_terry}) indicate that \mm{}-based methods are not always the best for every task, and \abr{lda} may still be more useful than fully unsupervised \mm{} approaches.

All tools have their trade-offs and user preferences.
%
In our analysis, \mm{}s are still not a replacement of traditional
topic models.
%
Further improvements of \mm{}-based methods for large corpora understanding to reduce
hallucination, scalability, reducing human intervention, aligning
topics with user intents using \mm{}s, reducing costs are needed to
bring \mm{}-based methods to popularity and accessibility into the social science
domain.





% \subsection{Failures of \mm{}-based topic models}


% Interestingly, \lda{} topic keywords are important hints for users to explore documents and answer questions. For example, one user's response is
% \begin{quote}
%     Labels like `land', `area', `national', and `act', `wildlife', `specie' suggest that both land use and wildlife policies are designed to protect the environment. They often aim to preserve natural habitats while allowing for controlled development. These policies share the goal of balancing human use of land with the protection of wildlife and natural ecosystems.
% \end{quote}



% \subsection{Takeaway}
%


% We examine now how user gain knowledge from the different topic modeling approaches through a qualitative analysis based on annotators observations, and users' self-report feedback and answers to the post-survey questions.









% \jbgcomment{Don't use "1" as a label.  Either call this "satisfaction" or "usefulness"}

% All the questions aim to understand the usefulness of topics for each method in helping users explore and understand essential contents in a dataset.
% %
% We plot the user reported ratings of 'topic satisfaction' in Figure~\ref{fig:survey_ratings}. 
% %
% On average, users rate higher on relying on topics generated by prompt-based methods to explore documents and help them answer questions than classical models.
% %
% In addition, \bass{} has the highest rating on average comparing to other methods on the Bills dataset, which has harder questions that require more human mental reasoning to generate reasonable topics to explore the data.
% %
% On the other hand, although the median scores for TopicGPT and LLooM are higher than that of \bass{} and \abr{lda} on the synthetic news dataset, the variance of user ratings is much smaller when users have more control on generating their own topics.
% %
% We manually analyze answers from users for each of the group, and delve into the change of users' answers before and after they use each of the tool-- how using topic models can help them better understand a dataset.

% \paragraph{Baseline users report the task to be challenging}
% Users in baseline left optional feedback, stating that the task was `difficult', `starting to run out of time because I took a lot of time reading, sorting and labelling for the first couple of questions and lost track of how much time I had left.', `I found the work of sorting articles tedious but not mentally challenging.'...
%
% On the other hand, users with topic models have more positive comments such as `The labels give some idea of what types of topics are covered in the dataset, mainly as a starting point so I'd have some ideas about what to search for in the documents', `Cut down amongst of labels' (\abr{lda}), where the topics help them get a sense of the dataset at first, the number of topics generated by fully automatic methods overwhelms some of the users.
%


%We evaluate the change of user responses before/after using each method, then discuss qualitative analysis son how users learn from various methods.

%
%We finally discuss the user self-report feedback and post-survey questions.


%\abr{lda} topic keywords are important hints for users to explore documents and answer questions. 
%
%Specifically, users mention relevant keywords from the topics as important hints to generate answers. An example is ....
%



% \paragraph{Prompt-based method provide more straightforward topics...}


% \paragraph{It is challenging for users to build up a label set first before answering the questions.} 
% %
% It takes a lot of mental efforts for users to build up the label set and answer questions within an 80 minute time window. 
% %
% However, the labels are ...

% \zongxiacomment{Do a sentiment analysis on users' overall comments and analyze.}

% \subsection{Why \mm{}-based topic models cannot replace traditional topic models? What Needs to be done to improve data exploration?}

% \zongxiacomment{Most importantly, talk about the failures and user feedback from topicgpt. The topics are overly vague. There is no such one prompt fit all template for topic modeling. However, things should be human defined. For datasets that have distinct topics like the Bills or Wiki that is easy to classify, topicgpt does well and better than lda. But for Domain specific datasets, topicgpt is overly generic and cannot be used. lloom also has its restrictions, which is long context. For a few documents like 100 to 200, it can do. For longer and more documents, it fails because tries to fit too many passages into on in the prompt. Try it on 500 long documents if you don't believe it. lda is very good, but lacks human control. Using a weak human supervision can help.}






% We evaluate the change of user responses before/after using each method, then discuss qualitative analysis son how users learn from various methods.
% %
% We also calculate the coverage rate of topics generated by different user groups compared to the gold labels.
% %
% We finally discuss the user self-report feedback and post-survey questions.





% \begin{figure*}[t]
%     \jbgcomment{Can save space by making this a vertical faceted figure}
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/bills_survey2.pdf}
%         \caption{Bills}
%         \label{fig:boxplot_bills}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=\textwidth]{figures/news_survey2.pdf}
%         \caption{News}
%         \label{fig:boxplot_news}
%     \end{subfigure}
%     \caption{Users prefer using the topics they create themselves \bass{} themselves to use to answer questions on average on Bills dataset. Users depend more on prompt-based model generated topics than \abr{lda} to explore the dataset.}
%     \label{fig:survey_ratings}
% \end{figure*}




% \paragraph{Prompt-based method provide more straightforward topics...}


% \paragraph{It is challenging for users to build up a label set first before answering the questions.} 
% %
% It takes a lot of mental efforts for users to build up the label set and answer questions within an 80 minute time window. 
% %
% Although users have more control on the generated topic, within a short time window, fully automated topic models are receive more positive comments. 


% \zongxiacomment{small table}


% users rate higher on relying on topics generated by prompt-based methods to explore documents and help them answer questions than classical models.
%
% In addition, \bass{} has the highest rating on average comparing to other methods on the Bills dataset, which has harder questions that require more human mental reasoning to generate reasonable topics to explore the data.
% %
% On the other hand, although the median scores for TopicGPT and LLooM are higher than that of \bass{} and \abr{lda} on the synthetic news dataset, the variance of user ratings is much smaller when users have more control on generating their own topics.
% %
% We manually analyze answers from users for each of the group, and delve into the change of users' answers before and after they use each of the tool-- how using topic models can help them better understand a dataset.






% \jbgcomment{Do we have survey results for frustration / satisfaction / confidence?  That would help differentiate categories.}
