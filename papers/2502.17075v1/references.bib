@book{10.5555/286076,
  title = {Advanced Compiler Design and Implementation},
  author = {Muchnick, Steven S.},
  year = {1998},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  isbn = {1-55860-320-4}
}

@misc{abadiTensorFlowSystemLargescale2016,
  title = {{{TensorFlow}}: {{A}} System for Large-Scale Machine Learning},
  shorttitle = {{{TensorFlow}}},
  author = {Abadi, Mart{\'i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2016},
  month = may,
  number = {arXiv:1605.08695},
  eprint = {1605.08695},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1605.08695},
  urldate = {2024-05-22},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing},
  file = {/Users/jumerckx/Zotero/storage/PY6ITDSB/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf;/Users/jumerckx/Zotero/storage/BWTB9IZW/1605.html;/Users/jumerckx/Zotero/storage/C9WF7BG4/1605.html}
}

@misc{AcyclicEgraphsSmart2024,
  title = {Acyclic {{Egraphs}} and {{Smart Constructors}}},
  year = {2024},
  month = sep,
  journal = {Hey There Buddo!},
  urldate = {2024-10-09},
  abstract = {That there are egraphs in the Cranelift JIT is important as a proof of concept that a seriously engineered piece of production software can fruitfully use some (very intriguing!) variation of egraphs.},
  howpublished = {https://www.philipzucker.com/smart\_constructor\_aegraph/},
  langid = {english},
  keywords = {equality saturation,theory},
  file = {/Users/jumerckx/Zotero/storage/CBM4786P/smart_constructor_aegraph.html;/Users/jumerckx/Zotero/storage/DUBAHIPA/smart_constructor_aegraph.html}
}

@misc{ahrensFinchSparseStructured2024,
  title = {Finch: {{Sparse}} and {{Structured Array Programming}} with {{Control Flow}}},
  shorttitle = {Finch},
  author = {Ahrens, Willow and Collin, Teodoro Fields and Patel, Radha and Deeds, Kyle and Hong, Changwan and Amarasinghe, Saman},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2404.16730},
  urldate = {2025-01-14},
  abstract = {From FORTRAN to NumPy, arrays have revolutionized how we express computation. However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids. Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Support for structured data is fragmented and incomplete. Existing frameworks limit the array structures and program control flow they support to better simplify the problem.  In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures. Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized. Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms. Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks. Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros. We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Science - Mathematical Software,FOS: Computer and information sciences,Mathematical Software (cs.MS)},
  file = {/Users/jumerckx/Zotero/storage/NN6G4QU2/Ahrens et al. - 2024 - Finch Sparse and Structured Array Programming with Control Flow.pdf;/Users/jumerckx/Zotero/storage/J2BEFN73/2404.html;/Users/jumerckx/Zotero/storage/MXKBPY9P/2404.html}
}

@inproceedings{ahrensLoopletsLanguageStructured2023,
  title = {Looplets: A Language for Structured Coiteration},
  booktitle = {Proceedings of the 21st {{ACM}}/{{IEEE}} International Symposium on Code Generation and Optimization},
  author = {Ahrens, Willow and Donenfeld, Daniel and Kjolstad, Fredrik and Amarasinghe, Saman},
  year = {2023},
  series = {CGO '23},
  pages = {41--54},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3579990.3580020},
  abstract = {Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Specializing for structure yields significant speedups. But automatically generating efficient code for structured data is challenging, especially when arrays with different structure interact. We show how to abstract over array structures so that the compiler can generate code to coiterate over any combination of them. Our technique enables new array formats (such as 1DVBL for irregular clustered sparsity), new iteration strategies (such as galloping intersections), and new operations over structured data (such as concatenation or convolution).},
  isbn = {979-8-4007-0101-6},
  keywords = {Array,Coiteration,Compressed,Sparse,Tensor}
}

@article{arndpoetzsch-heffterProofGeneratingCompilers2005,
  title = {Towards {{Proof Generating Compilers}}},
  author = {{Arnd Poetzsch-Heffter} and {Poetzsch-Heffter}, Arnd and {Marek Gawkowski} and Gawkowski, Marek Jerzy},
  year = {2005},
  month = may,
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = {132},
  number = {1},
  pages = {37--51},
  doi = {10.1016/j.entcs.2005.03.023},
  abstract = {Correctness of compilation is important for the reliability of software. New techniques to guarantee correctness do not verify the compiler itself, but check for each compiled program whether it is correctly translated. Following these ideas, we developed an approach in which checking is realized as proof checking within a formal specification and verification framework. Based on formal specifications of source and target language and a translation predicate, compilers produce, in addition to the target program c, a proof that c is correct w.r.t. its source program. This proof can be checked independently of the compiler by the framework. Thus, it can be used as a translation certificate. The paper describes the overall approach and applies it to a simple translation scenario. Specification and verification is done within the theorem prover Isabelle/HOL. To show the flexibility of the approach, we present two different proof techniques for translation correctness.},
  annotation = {MAG ID: 2002816392},
  file = {/Users/jumerckx/Zotero/storage/HV9DBH8Q/Arnd Poetzsch-Heffter et al. - 2005 - Towards Proof Generating Compilers.pdf}
}

@article{bahmannPerfectReconstructabilityControl2015,
  title = {Perfect {{Reconstructability}} of {{Control Flow}} from {{Demand Dependence Graphs}}},
  author = {Bahmann, Helge and Reissmann, Nico and Jahre, Magnus and Meyer, Jan Christian},
  year = {2015},
  month = jan,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {11},
  number = {4},
  pages = {1--25},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/2693261},
  urldate = {2024-07-01},
  abstract = {Demand-based dependence graphs (DDGs), such as the (Regionalized) Value State Dependence Graph ((R)VSDG), are intermediate representations (IRs) well suited for a wide range of program transformations. They explicitly model the flow of data and state, and only implicitly represent a restricted form of control flow. These features make DDGs especially suitable for automatic parallelization and vectorization, but cannot be leveraged by practical compilers without efficient construction and destruction algorithms. Construction algorithms remodel the arbitrarily complex control flow of a procedure to make it amenable to DDG representation, whereas destruction algorithms reestablish control flow for generating efficient object code. Existing literature presents solutions to both problems, but these impose structural constraints on the generatable control flow, and omit qualitative evaluation.                            The key contribution of this article is to show that there is no intrinsic structural limitation in the control flow directly extractable from RVSDGs. This fundamental result originates from an interpretation of loop repetition and decision predicates as computed continuations, leading to the introduction of the               predicate continuation               normal form. We provide an algorithm for constructing RVSDGs in predicate continuation form, and propose a novel destruction algorithm for RVSDGs in this form. Our destruction algorithm can generate arbitrarily complex control flow; we show this by proving that the original CFG an RVSDG was derived from can, apart from overspecific detail, be reconstructed perfectly. Additionally, we prove termination and correctness of these algorithms. Furthermore, we empirically evaluate the performance, the representational overhead at compile time, and the reduction in branch instructions compared to existing solutions. In contrast to previous work, our algorithms impose no additional overhead on the control flow of the produced object code. To our knowledge, this is the first scheme that allows the original control flow of a procedure to be recovered from a DDG representation.},
  langid = {english},
  file = {/Users/jumerckx/Zotero/storage/E357QGVY/Bahmann et al. - 2015 - Perfect Reconstructability of Control Flow from De.pdf}
}

@article{besardEffectiveExtensibleProgramming2019,
  title = {Effective {{Extensible Programming}}: {{Unleashing Julia}} on {{GPUs}}},
  shorttitle = {Effective {{Extensible Programming}}},
  author = {Besard, Tim and Foket, Christophe and De Sutter, Bjorn},
  year = {2019},
  month = apr,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {30},
  number = {4},
  pages = {827--841},
  issn = {1045-9219, 1558-2183, 2161-9883},
  doi = {10.1109/TPDS.2018.2872064},
  urldate = {2024-02-26},
  keywords = {JuliaGPU},
  file = {/Users/jumerckx/Zotero/storage/DVYQQZZX/Besard et al. - 2019 - Effective Extensible Programming Unleashing Julia.pdf}
}

@misc{bezansonJuliaFastDynamic2012,
  title = {Julia: {{A Fast Dynamic Language}} for {{Technical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B. and Edelman, Alan},
  year = {2012},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1209.5145},
  urldate = {2025-01-14},
  abstract = {Dynamic languages have become popular for scientific computing. They are generally considered highly productive, but lacking in performance. This paper presents Julia, a new dynamic language for technical computing, designed for performance from the beginning by adapting and extending modern programming language techniques. A design based on generic functions and a rich type system simultaneously enables an expressive programming model and successful type inference, leading to good performance for a wide range of programs. This makes it possible for much of the Julia library to be written in Julia itself, while also incorporating best-of-breed C and Fortran libraries.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {and Science,Computational Engineering Finance and Science (cs.CE),Computer Science - Computational Engineering,Computer Science - Computational Engineering Finance and Science,Computer Science - Programming Languages,D.3.2,Finance,FOS: Computer and information sciences,Programming Languages (cs.PL)},
  file = {/Users/jumerckx/Zotero/storage/EZ2TM3BB/Bezanson et al. - 2012 - Julia A Fast Dynamic Language for Technical Compu.pdf;/Users/jumerckx/Zotero/storage/TVY2DHHL/1209.html}
}

@article{chandrakananandiRewriteRuleInference2021,
  title = {Rewrite Rule Inference Using Equality Saturation},
  author = {{Chandrakana Nandi} and Nandi, Chandrakana and {Max Willsey} and Willsey, Max and {Amy Zhu} and Zhu, Amy Y. X. and {Yisu Remy Wang} and Wang, Yisu Remy and {Brett Saiki} and Saiki, Brett and {Adam Anderson} and Anderson, Adam and {Adriana Schulz} and Schulz, Adriana and {Dan Grossman} and Grossman, Dan and {Zachary Tatlock} and Tatlock, Zachary},
  year = {2021},
  month = oct,
  journal = {Proc. ACM Program. Lang.},
  volume = {5},
  pages = {1--28},
  doi = {10.1145/3485496},
  abstract = {Many compilers, synthesizers, and theorem provers rely on rewrite rules to simplify expressions or prove equivalences. Developing rewrite rules can be difficult: rules may be subtly incorrect, profitable rules are easy to miss, and rulesets must be rechecked or extended whenever semantics are tweaked. Large rulesets can also be challenging to apply: redundant rules slow down rule-based search and frustrate debugging.  This paper explores how equality saturation, a promising technique that uses e-graphs to apply rewrite rules, can also be used to infer rewrite rules. E-graphs can compactly represent the exponentially large sets of enumerated terms and potential rewrite rules. We show that equality saturation efficiently shrinks both sets, leading to faster synthesis of smaller, more general rulesets.  We prototyped these strategies in a tool dubbed Ruler. Compared to a similar tool built on CVC4, Ruler synthesizes 5.8{\texttimes} smaller rulesets 25{\texttimes} faster without compromising on proving power. In an end-to-end case study, we show Ruler-synthesized rules which perform as well as those crafted by domain experts, and addressed a longstanding issue in a popular open source tool.},
  keywords = {equality saturation,theory},
  annotation = {ARXIV\_ID: 2108.10436\\
MAG ID: 3207460439\\
S2ID: 55b91d4be522c2726ec2852febbec5537081672f},
  file = {/Users/jumerckx/Zotero/storage/G6STCYUS/Chandrakana Nandi et al. - 2021 - Rewrite rule inference using equality saturation.pdf}
}

@misc{cheliAutomatedCodeOptimization2021,
  title = {Automated Code Optimization with E-Graphs},
  author = {Cheli, Alessandro},
  year = {2021},
  eprint = {2112.14714},
  primaryclass = {cs.PL},
  archiveprefix = {arXiv},
  keywords = {application,equality saturation},
  file = {/Users/jumerckx/Zotero/storage/7ZJSESNX/Cheli - 2021 - Automated Code Optimization with E-Graphs.pdf}
}

@article{cheliMetatheoryjlFastElegant2021,
  title = {Metatheory.Jl: {{Fast}} and {{Elegant Algebraic Computation}} in {{Julia}} with {{Extensible Equality Saturation}}},
  author = {Cheli, Alessandro},
  year = {2021},
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {59},
  pages = {3078},
  doi = {10.21105/joss.03078},
  abstract = {We introduce Metatheory.jl: a lightweight and performant general purpose symbolics and metaprogramming framework meant to simplify the act of writing complex Julia metaprograms and to significantly enhance Julia with a native term rewriting system, based on state-of-the-art equality saturation techniques, and a dynamic first class Abstract Syntax Tree (AST) pattern matching system that is dynamically composable in an algebraic fashion, taking full advantage of the language's powerful reflection capabilities. Our contribution allows to perform general purpose symbolic mathematics, manipulation, optimization, synthesis or analysis of syntactically valid Julia expressions with a clean and concise programming interface, both during compilation or execution of programs.},
  keywords = {equality saturation,theory},
  annotation = {ARXIV\_ID: 2102.07888\\
MAG ID: 3145384168\\
S2ID: 46b544baa83079f1a59bdafc13e63a2583e27f57},
  file = {/Users/jumerckx/Zotero/storage/7BJ2Q4KF/Cheli - 2021 - Metatheory.jl Fast and Elegant Algebraic Computation in Julia with Extensible Equality Saturation.pdf}
}


@INPROCEEDINGS{chrislattnerMLIRScalingCompiler2021,
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={{MLIR}: Scaling Compiler Infrastructure for Domain Specific Computation}, 
  year={2021},
  volume={},
  number={},
  pages={2--14},
  publisher={IEEE},
  address = {New York City, NY, USA},
  keywords={Program processors;Buildings;Semantics;Hardware;Software;Generators;Optimization},
  doi={10.1109/CGO51591.2021.9370308}
}


@inproceedings{cowardAutomatingConstraintAwareDatapath2023,
  title = {Automating {{Constraint-Aware Datapath Optimization}} Using {{E-Graphs}}},
  booktitle = {2023 60th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Coward, Samuel and Constantinides, George A. and Drane, Theo},
  year = {2023},
  month = jul,
  pages = {1--6},
  doi = {10.1109/DAC56929.2023.10247797},
  urldate = {2025-01-02},
  abstract = {Numerical hardware design requires aggressive optimization, where designers exploit branch constraints, creating optimization opportunities that are valid only on a sub-domain of input space. We developed an RTL optimization tool that automatically learns the consequences of conditional branches and exploits that knowledge to enable deep optimization. The tool deploys custom built program analysis based on abstract interpretation theory, which when combined with a data-structure known as an e-graph simplifies complex reasoning about program properties. Our tool fully-automatically discovers known floating-point architectures from the computer arithmetic literature and out-performs baseline EDA tools, generating up to 33\% faster and 41\% smaller circuits.},
  keywords = {application,Codes,Cognition,Computer architecture,Delays,Design automation,Digital arithmetic,equality saturation,Hardware},
  file = {/Users/jumerckx/Zotero/storage/42MUA742/Coward et al. - 2023 - Automating Constraint-Aware Datapath Optimization using E-Graphs.pdf;/Users/jumerckx/Zotero/storage/3JYANHEJ/10247797.html;/Users/jumerckx/Zotero/storage/7TCV3749/10247797.html}
}

@inproceedings{cowardCombiningEGraphsAbstract2023,
  title = {Combining {{E-Graphs}} with {{Abstract Interpretation}}},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Workshop}} on the {{State Of}} the {{Art}} in {{Program Analysis}}},
  author = {Coward, Samuel and Constantinides, George A. and Drane, Theo},
  year = {2023},
  month = jun,
  pages = {1--7},
  publisher = {ACM},
  address = {Orlando FL USA},
  doi = {10.1145/3589250.3596144},
  urldate = {2025-01-14},
  isbn = {979-8-4007-0170-2},
  langid = {english},
  keywords = {equality saturation,theory},
  file = {/Users/jumerckx/Zotero/storage/V6MR8UZQ/Coward et al. - 2023 - Combining E-Graphs with Abstract Interpretation.pdf}
}

@inproceedings{demouraEfficientEMatchingSMT2007,
  title = {Efficient {{E-Matching}} for {{SMT Solvers}}},
  booktitle = {Automated {{Deduction}} -- {{CADE-21}}},
  author = {{de Moura}, Leonardo and Bj{\o}rner, Nikolaj},
  editor = {Pfenning, Frank},
  year = {2007},
  pages = {183--198},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-73595-3_13},
  abstract = {Satisfiability Modulo Theories (SMT) solvers have proven highly scalable, efficient and suitable for integrating theory reasoning. However, for numerous applications from program analysis and verification, the ground fragment is insufficient, as proof obligations often include quantifiers. A well known approach for quantifier reasoning uses a matching algorithm that works against an E-graph to instantiate quantified variables. This paper introduces algorithms that identify matches on E-graphs incrementally and efficiently. In particular, we introduce an index that works on E-graphs, called E-matching code trees that combine features of substitution and code trees, used in saturation based theorem provers. E-matching code trees allow performing matching against several patterns simultaneously. The code trees are combined with an additional index, called the inverted path index, which filters E-graph terms that may potentially match patterns when the E-graph is updated. Experimental results show substantial performance improvements over existing state-of-the-art SMT solvers.},
  isbn = {978-3-540-73595-3},
  langid = {english},
  keywords = {theory},
  file = {/Users/jumerckx/Zotero/storage/LSDUUSZM/Leonardo de Moura et al. - 2007 - Efficient E-Matching for SMT Solvers.pdf}
}

@incollection{demouraZ3EfficientSMT2008,
  title = {Z3: {{An Efficient SMT Solver}}},
  shorttitle = {Z3},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {De Moura, Leonardo and Bj{\o}rner, Nikolaj},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ramakrishnan, C. R. and Rehof, Jakob},
  year = {2008},
  volume = {4963},
  pages = {337--340},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-78800-3_24},
  urldate = {2025-01-14},
  isbn = {978-3-540-78799-0 978-3-540-78800-3},
  file = {/Users/jumerckx/Zotero/storage/EEAA9KQP/De Moura and Bjørner - 2008 - Z3 An Efficient SMT Solver.pdf}
}

@misc{dettmersLlmint88bitMatrix2022,
  title = {Llm.Int8(): 8-Bit Matrix Multiplication for Transformers at Scale},
  author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  year = {2022},
  eprint = {2208.07339},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@article{elshawiDLBenchComprehensiveExperimental2021,
  title = {{{DLBench}}: A Comprehensive Experimental Evaluation of Deep Learning Frameworks},
  author = {Elshawi, Radwa and Wahab, Abdul and Barnawi, Ahmed and Sakr, Sherif},
  year = {2021},
  month = sep,
  journal = {Cluster Computing},
  volume = {24},
  number = {3},
  pages = {2017--2038},
  issn = {1573-7543},
  doi = {10.1007/s10586-021-03240-4},
  abstract = {Deep Learning (DL) has achieved remarkable progress over the last decade on various tasks such as image recognition, speech recognition, and natural language processing. In general, three main crucial aspects fueled this progress: the increasing availability of large amount of digitized data, the increasing availability of affordable parallel and powerful computing resources (e.g., GPU) and the growing number of open source deep learning frameworks that facilitate and ease the development process of deep learning architectures. In practice, the increasing popularity of deep learning frameworks calls for benchmarking studies that can effectively evaluate and understand the performance characteristics of these systems. In this paper, we conduct an extensive experimental evaluation and analysis of six popular deep learning frameworks, namely, TensorFlow, MXNet, PyTorch, Theano, Chainer, and Keras, using three types of DL architectures Convolutional Neural Networks (CNN), Faster Region-based Convolutional Neural Networks (Faster R-CNN), and Long Short Term Memory (LSTM). Our experimental evaluation considers different aspects for its comparison including accuracy, training time, convergence and resource consumption patterns. Our experiments have been conducted on both CPU and GPU environments using different datasets. We report and analyze the performance characteristics of the studied frameworks. In addition, we report a set of insights and important lessons that we have learned from conducting our experiments.},
  file = {/Users/jumerckx/Zotero/storage/8D5DNQN7/Elshawi et al. - 2021 - DLBench a comprehensive experimental evaluation of deep learning frameworks.pdf}
}

@article{faingnaertFlexiblePerformantGEMM2022,
  title = {Flexible {{Performant GEMM Kernels}} on {{GPUs}}},
  author = {Faingnaert, Thomas and Besard, Tim and De Sutter, Bjorn},
  year = {2022},
  month = sep,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {33},
  number = {9},
  pages = {2230--2248},
  issn = {1045-9219, 1558-2183, 2161-9883},
  doi = {10.1109/TPDS.2021.3136457},
  urldate = {2024-02-26},
  keywords = {JuliaGPU},
  file = {/Users/jumerckx/Zotero/storage/7ZL9MP29/Faingnaert et al. - 2022 - Flexible Performant GEMM Kernels on GPUs.pdf}
}

@misc{fallinAegraphsAcyclicEgraphs2023,
  title = {Aegraphs: {{Acyclic}} e-Graphs for Efficient Optimization in a Production Compiler},
  author = {Fallin, Chris},
  year = {2023},
  howpublished = {EGRAPHS 2023 keynote},
  keywords = {application,equality saturation,theory}
}

@inproceedings{flattSmallProofsCongruence2022,
  title = {Small {{Proofs}} from {{Congruence Closure}}},
  booktitle = {2022 {{Formal Methods}} in {{Computer-Aided Design}} ({{FMCAD}})},
  author = {Flatt, Oliver and Coward, Samuel and Willsey, Max and Tatlock, Zachary and Panchekha, Pavel},
  year = {2022},
  month = oct,
  pages = {75--83},
  issn = {2708-7824},
  doi = {10.34727/2022/isbn.978-3-85448-053-2_13},
  urldate = {2025-01-14},
  abstract = {Satisfiability Modulo Theory (SMT) solvers and equality saturation engines must generate proof certificates from e-graph-based congruence closure procedures to enable verification and conflict clause generation. Smaller proof certificates speed up these activities. Though the problem of generating proofs of minimal size is known to be NP-complete, existing proof minimization algorithms for congruence closure generate unnecessarily large proofs and introduce asymptotic overhead over the core congruence closure procedure. In this paper, we introduce an O(n5) time algorithm which generates optimal proofs under a new relaxed "proof tree size" metric that directly bounds proof size. We then relax this approach further to a practical O(n log(n)) greedy algorithm which generates small proofs with no asymptotic overhead. We implemented our techniques in the egg equality saturation toolkit, yielding the first certifying equality saturation engine. We show that our greedy approach in egg quickly generates substantially smaller proofs than the state-of-the-art Z3 SMT solver on a corpus of 3760 benchmarks.},
  keywords = {application,Benchmark testing,Design automation,Engines,equality saturation,Greedy algorithms,Measurement,Minimization,theory},
  file = {/Users/jumerckx/Zotero/storage/DLQWEWGE/Flatt et al. - 2022 - Small Proofs from Congruence Closure.pdf;/Users/jumerckx/Zotero/storage/B6DBMIAM/10026585.html;/Users/jumerckx/Zotero/storage/HYYV9FS7/10026585.html}
}

@article{futamuraPartialEvaluationComputation1999,
  title = {Partial {{Evaluation}} of {{Computation Process}}--{{An Approach}} to a {{Compiler-Compiler}}},
  author = {Futamura, Yoshihiko},
  year = {1999},
  month = dec,
  journal = {Higher-Order and Symbolic Computation},
  volume = {12},
  number = {4},
  pages = {381--391},
  issn = {1573-0557},
  doi = {10.1023/A:1010095604496},
  urldate = {2024-05-19},
  abstract = {This paper reports the relationship between formal description of semantics (i.e., interpreter) of a programming language and an actual compiler. The paper also describes a method to automatically generate an actual compiler from a formal description which is, in some sense, the partial evaluation of a computation process. The compiler-compiler inspired by this method differs from conventional ones in that the compiler-compiler based on our method can describe an evaluation procedure (interpreter) in defining the semantics of a programming language, while the conventional one describes a translation process.},
  langid = {english},
  keywords = {compiler,Futamura projections,interpreter,partial evaluation,program transformation},
  file = {/Users/jumerckx/Zotero/storage/JXE2273S/Futamura - 1999 - Partial Evaluation of Computation Process--An Appr.pdf}
}

@phdthesis{georgiadisLineartimeAlgorithmsDominators2005,
  title = {Linear-Time Algorithms for Dominators and Related Problems},
  author = {Georgiadis, Loukas},
  year = {2005},
  address = {USA},
  abstract = {This dissertation deals with several topics related to the problem of finding dominators in flowgraphs. The concept of dominators has applications in various fields, including program optimization, circuit testing and theoretical biology. We are interested both in asymptotically fast algorithms and in algorithms that are practical. We begin with an experimental study of various algorithms that compute dominators efficiently in practice. We describe two practical algorithms that have been proposed in the related literature: an iterative algorithm initially presented by Allen and Cocke and later refined by Cooper, Harvey and Kennedy, and the well-known algorithm of Lengauer and Tarjan. We discuss how to achieve efficient implementations, and furthermore, introduce a new practical algorithm. We present a thorough empirical analysis using real as well as artificial data. Then we present a linear-time algorithm for dominators implementable on the pointer machine model of computation. Previously, Alstrup, Harel, Lauridsen and Thorup gave a complicated linear-time algorithm for the random-access model. Buchsbaum, Kaplan, Rogers and Westbrook presented a simpler dominators algorithm, implementable on a pointer machine and claimed linear running time. However, as we show, one of their techniques cannot be applied to the dominators problem and, consequently, their algorithm does not run in linear time. Nonetheless, based on this algorithm, we show how to achieve linear running time on a pointer machine. Next we address the question of how to verify dominators. We derive a linear-time verification algorithm, which is much simpler than the known algorithms that compute dominators in linear time. Still, this algorithm is non-trivial and we believe it provides some new intuition and ideas towards a simpler dominators algorithm. Finally we study the relation of dominators to spanning trees. Our central result is a linear-time algorithm that constructs two spanning trees of any input flowgraph G , such that corresponding paths in the two trees satisfy a vertex-disjointness property we call ancestor-dominance . This result is related to the concepts of independent spanning trees and directed st-numberings , previously studied by various authors, and implies linear-time algorithms for these constructions.},
  school = {Princeton University},
  annotation = {AAI3188624\\
ISBN-10: 0542306964}
}

@article{godboltOptimizationsCompilersPractical2019,
  title = {Optimizations in {{C}}++ {{Compilers}}: {{A}} Practical Journey},
  author = {Godbolt, Matt},
  year = {2019},
  month = oct,
  journal = {Queue},
  volume = {17},
  number = {5},
  pages = {69--100},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1542-7730},
  doi = {10.1145/3371595.3372264},
  abstract = {There's a tradeoff to be made in giving the compiler more information: it can make compilation slower. Technologies such as link time optimization can give you the best of both worlds. Optimizations in compilers continue to improve, and upcoming improvements in indirect calls and virtual function dispatch might soon lead to even faster polymorphism.},
  issue_date = {September-October 2019}
}

@article{goharshadyFastOptimalExtraction2024,
  title = {Fast and Optimal Extraction for Sparse Equality Graphs},
  author = {Goharshady, Amir Kafshdar and Lam, Chun Kit and Parreaux, Lionel},
  year = {2024},
  month = oct,
  journal = {Proc. ACM Program. Lang.},
  volume = {8},
  number = {OOPSLA2},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3689801},
  abstract = {Equality graphs (e-graphs) are used to compactly represent equivalence classes of terms in symbolic reasoning systems. Beyond their original roots in automated theorem proving, e-graphs have been used in a variety of applications. They have become particularly important as the key ingredient in the popular technique of equality saturation, which has notable applications in compiler optimization, program synthesis, program verification, and symbolic execution, among others. In a typical equality saturation workflow, an e-graph is used to store a large number of equalities that are generated by local rewrites during a saturation phase, after which an optimal term is extracted from the e-graph as the output of the technique. However, despite its crucial role in equality saturation, e-graph extraction has received relatively little attention in the literature, which we seek to start addressing in this paper. Extraction is a challenging problem and is notably known to be NP-hard in general, so current equality saturation tools rely either on slow optimal extraction algorithms based on integer linear programming (ILP) or on heuristics that may not always produce the optimal result. In fact, in this paper, we show that e-graph extraction is hard to approximate within any constant ratio. Thus, any such heuristic will produce wildly suboptimal results in the worst case. Fortunately, we show that the problem becomes tractable when the e-graph is sparse, which is the case in many practical applications. We present a novel parameterized algorithm for extracting optimal terms from e-graphs with low treewidth, a measure of how ``tree-like'' a graph is, and prove its correctness. We also present an efficient Rust implementation of our algorithm and evaluate it against ILP on a number of benchmarks extracted from the Cranelift benchmark suite, a real-world compiler optimization library based on equality saturation. Our algorithm optimally extracts e-graphs with treewidths of up to 10 in a fraction of the time taken by ILP. These results suggest that our algorithm can be a valuable tool for equality saturation users who need to extract optimal terms from sparse e-graphs.},
  articleno = {361},
  issue_date = {October 2024},
  keywords = {e-graphs,equality saturation,extraction,theory,treewidth},
  file = {/Users/jumerckx/Zotero/storage/T6LHUG9R/Goharshady et al. - 2024 - Fast and optimal extraction for sparse equality graphs.pdf},
  numpages = {27}
}

@article{gowdaHighperformanceSymbolicnumericsMultiple2022,
  title = {High-Performance Symbolic-Numerics via Multiple Dispatch},
  author = {Gowda, Shashi and Ma, Yingbo and Cheli, Alessandro and Gw{\'o}{\'z}zd{\'z}, Maja and Shah, Viral B. and Edelman, Alan and Rackauckas, Christopher},
  year = {2022},
  month = jan,
  journal = {ACM Communications in Computer Algebra},
  volume = {55},
  number = {3},
  pages = {92--96},
  issn = {1932-2240},
  doi = {10.1145/3511528.3511535},
  urldate = {2024-10-10},
  abstract = {As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. Exploiting this feature, we demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We illustrate how this symbolic system improves numerical computing tasks by showcasing an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.},
  keywords = {application,equality saturation},
  file = {/Users/jumerckx/Zotero/storage/8B5CMKWM/Gowda et al. - 2022 - High-performance symbolic-numerics via multiple dispatch.pdf}
}

@misc{heImprovingTermExtraction2017,
  title = {Improving {{Term Extraction}} with {{Acyclic Constraints}}},
  author = {He, Mike and Dong, Haichen and Malik, Sharad and Gupta, Aarti},
  year = {2017},
  abstract = {Term extraction is a crucial workload in egg for determining the desired terms to be extracted. Some prior works have formulated term extraction as integer linear programming (ILP) problems in order to cope with common sub-expressions for optimality that could not be handled by greedy algorithms. Although ILP-based extraction algorithms ensure optimality, these formulations offload a topological sorting problem to the ILP solver to avoid extracting cyclic terms, which does not scale well with the complexity of cycles in the e-graph. Instead of enforcing topological orders with constraints, we propose to explicitly identify the cycles and encode them to Acyclic constraints. This approach enables us to formulate term extraction problems in terms of Weighted Partial MAXSAT problems and improve the solving speed of the current ILP formulation. Our evaluation of term extraction for equality saturation on real-world Deep Learning (DL) workloads shows that using the improved formulation yields up to {$\sim$}3x speed-up on the total extraction time compared with using the state-of-the-art ILP encoding.},
  langid = {english},
  keywords = {equality saturation,theory},
  file = {/Users/jumerckx/Zotero/storage/J8NREGXN/He et al. - 2017 - Improving Term Extraction with Acyclic Constraints.pdf}
}

@inproceedings{holewinskiDynamicTracebasedAnalysis2012,
  title = {Dynamic Trace-Based Analysis of Vectorization Potential of Applications},
  booktitle = {Proceedings of the 33rd {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {Holewinski, Justin and Ramamurthi, Ragavendar and Ravishankar, Mahesh and Fauzia, Naznin and Pouchet, Louis-No{\"e}l and Rountev, Atanas and Sadayappan, P.},
  year = {2012},
  series = {Pldi '12},
  pages = {371--382},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2254064.2254108},
  abstract = {Recent hardware trends with GPUs and the increasing vector lengths of SSE-like ISA extensions for multicore CPUs imply that effective exploitation of SIMD parallelism is critical for achieving high performance on emerging and future architectures. A vast majority of existing applications were developed without any attention by their developers towards effective vectorizability of the codes. While developers of production compilers such as GNU gcc, Intel icc, PGI pgcc, and IBM xlc have invested considerable effort and made significant advances in enhancing automatic vectorization capabilities, these compilers still cannot effectively vectorize many existing scientific and engineering codes. It is therefore of considerable interest to analyze existing applications to assess the inherent latent potential for SIMD parallelism, exploitable through further compiler advances and/or via manual code changes.In this paper we develop an approach to infer a program's SIMD parallelization potential by analyzing the dynamic data-dependence graph derived from a sequential execution trace. By considering only the observed run-time data dependences for the trace, and by relaxing the execution order of operations to allow any dependence-preserving reordering, we can detect potential SIMD parallelism that may otherwise be missed by more conservative compile-time analyses. We show that for several benchmarks our tool discovers regions of code within computationally-intensive loops that exhibit high potential for SIMD parallelism but are not vectorized by state-of-the-art compilers. We present several case studies of the use of the tool, both in identifying opportunities to enhance the transformation capabilities of vectorizing compilers, as well as in pointing to code regions to manually modify in order to enable auto-vectorization and performance improvement by existing compilers.},
  isbn = {978-1-4503-1205-9},
  keywords = {dynamic analysis,performance analysis,vectorization}
}

@inproceedings{hosteColeCompilerOptimization2008,
  title = {Cole: Compiler Optimization Level Exploration},
  shorttitle = {Cole},
  booktitle = {Proceedings of the 6th Annual {{IEEE}}/{{ACM}} International Symposium on {{Code}} Generation and Optimization},
  author = {Hoste, Kenneth and Eeckhout, Lieven},
  year = {2008},
  month = apr,
  pages = {165--174},
  publisher = {ACM},
  address = {Boston MA USA},
  doi = {10.1145/1356058.1356080},
  urldate = {2024-10-08},
  abstract = {Modern compilers implement a large number of optimizations which all interact in complex ways, and which all have a different impact on code quality, compilation time, code size, energy consumption, etc. For this reason, compilers typically provide a limited number of standard optimization levels, such as -O1, -O2, -O3 and -Os, that combine various optimizations providing a number of trade-offs between multiple objective functions (such as code quality, compilation time and code size). The construction of these optimization levels, i.e., choosing which optimizations to activate at each level, is a manual process typically done using high-level heuristics based on the compiler developer's experience.},
  isbn = {978-1-59593-978-4},
  langid = {english},
  file = {/Users/jumerckx/Zotero/storage/PQSTIBAE/Hoste and Eeckhout - 2008 - Cole compiler optimization level exploration.pdf}
}

@article{huangfuParallelizingDualRevised2018,
  title = {Parallelizing the Dual Revised Simplex Method},
  author = {Huangfu, Q. and Hall, J. A. J.},
  year = {2018},
  month = mar,
  journal = {Mathematical Programming Computation},
  volume = {10},
  number = {1},
  pages = {119--142},
  issn = {1867-2957},
  doi = {10.1007/s12532-017-0130-5},
  abstract = {This paper introduces the design and implementation of two parallel dual simplex solvers for general large scale sparse linear programming problems. One approach, called PAMI, extends a relatively unknown pivoting strategy called suboptimization and exploits parallelism across multiple iterations. The other, called SIP, exploits purely single iteration parallelism by overlapping computational components when possible. Computational results show that the performance of PAMI is superior to that of the leading open-source simplex solver, and that SIP complements PAMI in achieving speedup when PAMI results in slowdown. One of the authors has implemented the techniques underlying PAMI within the FICO Xpress simplex solver and this paper presents computational results demonstrating their value. In developing the first parallel revised simplex solver of general utility, this work represents a significant achievement in computational optimization.}
}

@misc{innesFashionableModellingFlux2018,
      title={Fashionable Modelling with Flux}, 
      author={Michael Innes and Elliot Saba and Keno Fischer and Dhairya Gandhi and Marco Concetto Rudilosso and Neethu Mariya Joy and Tejan Karmali and Avik Pal and Viral Shah},
      year={2018},
      eprint={1811.01457},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/1811.01457}, 
}

@article{innesFluxElegantMachine2018,
  title = {Flux: {{Elegant}} Machine Learning with {{Julia}}},
  shorttitle = {Flux},
  author = {Innes, Mike},
  year = {2018},
  month = may,
  journal = {Journal of Open Source Software},
  volume = {3},
  number = {25},
  pages = {602},
  issn = {2475-9066},
  doi = {10.21105/joss.00602},
  urldate = {2025-01-14},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  file = {/Users/jumerckx/Zotero/storage/7KM7FHSL/Innes - 2018 - Flux Elegant machine learning with Julia.pdf}
}

@article{jamesc.hoeHardwareSynthesisTerm1999,
  title = {Hardware {{Synthesis}} from {{Term Rewriting Systems}}},
  author = {{James C. Hoe} and Hoe, James C. and {. Arvind} and {Arvind}},
  year = {1999},
  month = dec,
  journal = {IEEE Transactions on Very Large Scale Integration Systems},
  pages = {595--619},
  doi = {10.1007/978-0-387-35498-9_52},
  abstract = {Term Rewriting System (TRS) is a good formalism for describing concurrent systems that embody asynchronous and nondeterministic behavior in their specifications. Elsewhere, we have used TRS's to describe speculative micro-architectures and complex cache-coherence protocols, and proven the correctness of these systems. In this paper, we describe the compilation of TRS's into a subset of Verilog that can be simulated and synthesized using commercial tools. TRAC, Term Rewriting Architecture Compiler, enables a new hardware development framework that can match the ease of today's software programming environment. TRAC reduces the time and effort in developing and debugging hardware. For several examples, we compare TRAC-generated RTL's with hand-coded RTL's after they are both compiled for Field Programmable Gate Arrays by Xilinx tools. The circuits generated from TRS are competitive with those described using Verilog RTL, especially for larger designs.},
  annotation = {MAG ID: 1683919907},
  file = {/Users/jumerckx/Zotero/storage/M3FYAS5T/James C. Hoe et al. - 1999 - Hardware Synthesis from Term Rewriting Systems.pdf}
}

@inproceedings{jiaTASOOptimizingDeep2019,
  title = {{{TASO}}: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions},
  shorttitle = {{{TASO}}},
  booktitle = {Proceedings of the 27th {{ACM Symposium}} on {{Operating Systems Principles}}},
  author = {Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  year = {2019},
  month = oct,
  series = {{{SOSP}} '19},
  pages = {47--62},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3341301.3359630},
  urldate = {2024-10-21},
  abstract = {Existing deep neural network (DNN) frameworks optimize the computation graph of a DNN by applying graph transformations manually designed by human experts. This approach misses possible graph optimizations and is difficult to scale, as new DNN operators are introduced on a regular basis.We propose TASO, the first DNN computation graph optimizer that automatically generates graph substitutions. TASO takes as input a list of operator specifications and generates candidate substitutions using the given operators as basic building blocks. All generated substitutions are formally verified against the operator specifications using an automated theorem prover. To optimize a given DNN computation graph, TASO performs a cost-based backtracking search, applying the substitutions to find an optimized graph, which can be directly used by existing DNN frameworks.Our evaluation on five real-world DNN architectures shows that TASO outperforms existing DNN frameworks by up to 2.8X, while requiring significantly less human effort. For example, TensorFlow currently contains approximately 53,000 lines of manual optimization rules, while the operator specifications needed by TASO are only 1,400 lines of code.},
  isbn = {978-1-4503-6873-5},
  file = {/Users/jumerckx/Zotero/storage/8MYI8I3Z/Jia et al. - 2019 - TASO optimizing deep learning computation with automatic generation of graph substitutions.pdf}
}

@inproceedings{kourtaCaviarEgraphBased2022,
  title = {Caviar: An e-Graph Based {{TRS}} for Automatic Code Optimization},
  shorttitle = {Caviar},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN International Conference}} on {{Compiler Construction}}},
  author = {Kourta, Smail and Namani, Adel Abderahmane and {Benbouzid-Si Tayeb}, Fatima and Hazelwood, Kim and Cummins, Chris and Leather, Hugh and Baghdadi, Riyadh},
  year = {2022},
  month = mar,
  series = {{{CC}} 2022},
  pages = {54--64},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3497776.3517781},
  urldate = {2024-10-28},
  abstract = {Term Rewriting Systems (TRSs) are used in compilers to simplify and prove expressions. State-of-the-art TRSs in compilers use a greedy algorithm that applies a set of rewriting rules in a predefined order (where some of the rules are not axiomatic). This leads to a loss of the ability to simplify certain expressions. E-graphs and equality saturation sidestep this issue by representing the different equivalent expressions in a compact manner from which the optimal expression can be extracted. While an e-graph-based TRS can be more powerful than a TRS that uses a greedy algorithm, it is slower because expressions may have a large or sometimes infinite number of equivalent expressions. Accelerating e-graph construction is crucial for making the use of e-graphs practical in compilers. In this paper, we present Caviar, an e-graph-based TRS for proving expressions within compilers. The main advantage of Caviar is its speed. It can prove expressions much faster than base e-graph TRSs. It relies on three techniques: 1) a technique that stops e-graphs from growing when the goal is reached, called Iteration Level Check; 2) a mechanism that balances exploration and exploitation in the equality saturation algorithm, called Pulsing Caviar; 3) a technique to stop e-graph construction before reaching saturation when a non-provable pattern is detected, called Non-Provable Patterns Detection (NPPD). We evaluate caviar on Halide, an optimizing compiler that relies on a greedy-algorithm-based TRS to simplify and prove its expressions. The proposed techniques allow Caviar to accelerate e-graph expansion for the task of proving expressions. They also allow Caviar to prove expressions that Halide's TRS cannot prove while being only 0.68x slower. Caviar is publicly available at: \&lt;a\&gt;https://github.com/caviar-trs/caviar\&lt;/a\&gt;.},
  isbn = {978-1-4503-9183-2},
  keywords = {application,equality saturation,verification},
  file = {/Users/jumerckx/Zotero/storage/59CDFKZF/Kourta et al. - 2022 - Caviar an e-graph based TRS for automatic code optimization.pdf}
}

@inproceedings{lattnerLLVMCompilationFramework2004,
  title = {{{LLVM}}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  booktitle = {Proceedings of the International Symposium on Code Generation and Optimization: {{Feedback-directed}} and Runtime Optimization},
  author = {Lattner, Chris and Adve, Vikram},
  year = {2004},
  series = {CGO '04},
  pages = {75},
  publisher = {IEEE Computer Society},
  address = {USA},
  abstract = {This paper describes LLVM (Low Level Virtual Machine),a compiler framework designed to support transparent, lifelongprogram analysis and transformation for arbitrary programs,by providing high-level information to compilertransformations at compile-time, link-time, run-time, and inidle time between runs.LLVM defines a common, low-levelcode representation in Static Single Assignment (SSA) form,with several novel features: a simple, language-independenttype-system that exposes the primitives commonly used toimplement high-level language features; an instruction fortyped address arithmetic; and a simple mechanism that canbe used to implement the exception handling features ofhigh-level languages (and setjmp/longjmp in C) uniformlyand efficiently.The LLVM compiler framework and coderepresentation together provide a combination of key capabilitiesthat are important for practical, lifelong analysis andtransformation of programs.To our knowledge, no existingcompilation approach provides all these capabilities.We describethe design of the LLVM representation and compilerframework, and evaluate the design in three ways: (a) thesize and effectiveness of the representation, including thetype information it provides; (b) compiler performance forseveral interprocedural problems; and (c) illustrative examplesof the benefits LLVM provides for several challengingcompiler problems.},
  isbn = {0-7695-2102-9}
}


@misc{lattnerMLIRCompilerInfrastructure2020,
      title={MLIR: A Compiler Infrastructure for the End of Moore's Law}, 
      author={Chris Lattner and Mehdi Amini and Uday Bondhugula and Albert Cohen and Andy Davis and Jacques Pienaar and River Riddle and Tatiana Shpeisman and Nicolas Vasilache and Oleksandr Zinenko},
      year={2020},
      eprint={2002.11054},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2002.11054}, 
}

@inproceedings{lernerAutomaticallyProvingCorrectness2003,
  title = {Automatically Proving the Correctness of Compiler Optimizations},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2003 Conference on {{Programming}} Language Design and Implementation},
  author = {Lerner, Sorin and Millstein, Todd and Chambers, Craig},
  year = {2003},
  month = may,
  pages = {220--231},
  publisher = {ACM},
  address = {San Diego California USA},
  doi = {10.1145/781131.781156},
  urldate = {2025-01-14},
  isbn = {978-1-58113-662-3},
  langid = {english},
  file = {/Users/jumerckx/Zotero/storage/UFA6BFRC/Sorin Lerner et al. - 2003 - Automatically proving the correctness of compiler optimizations.pdf}
}

@inproceedings{liMirCheckerDetectingBugs2021,
  title = {{{MirChecker}}: {{Detecting}} Bugs in Rust Programs via Static Analysis},
  booktitle = {Proceedings of the 2021 {{ACM SIGSAC}} Conference on Computer and Communications Security},
  author = {Li, Zhuohua and Wang, Jincheng and Sun, Mingshen and Lui, John C.S.},
  year = {2021},
  series = {Ccs '21},
  pages = {2183--2196},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3460120.3484541},
  abstract = {Safe system programming is often a crucial requirement due to its critical role in system software engineering. Conventional low-level programming languages such as C and assembly are efficient, but their inherent unsafe nature makes it undesirable for security-critical scenarios. Recently, Rust has become a promising alternative for safe system-level programming. While giving programmers fine-grained hardware control, its strong type system enforces many security properties including memory safety. However, Rust's security guarantee is not a silver bullet. Runtime crashes and memory-safety errors still harass Rust developers, causing damaging exploitable vulnerabilities, as reported by numerous studies.In this paper, we present and evaluate MirChecker, a fully automated bug detection framework for Rust programs by performing static analysis on Rust's Mid-level Intermediate Representation (MIR). Based on the observation of existing bugs found in Rust codebases, our approach keeps track of both numerical and symbolic information, detects potential runtime crashes and memory-safety errors by using constraint solving techniques, and outputs informative diagnostics to users. We evaluate MirChecker on both buggy code snippets extracted from existing Common Vulnerabilities and Exposures (CVE) and real-world Rust codebases. Our experiments show that MirChecker can detect all the issues in our code snippets, and is capable of performing bug finding in real-world scenarios, where it detected a total of 33 previously unknown bugs including 16 memory-safety issues from 12 Rust packages (crates) with an acceptable false-positive rate.},
  isbn = {978-1-4503-8454-4},
  keywords = {abstract interpretation,rust,static analysis}
}

@article{lubinJuMP10Recent2023,
  title = {{{JuMP}} 1.0: Recent Improvements to a Modeling Language for Mathematical Optimization},
  shorttitle = {{{JuMP}} 1.0},
  author = {Lubin, Miles and Dowson, Oscar and Garcia, Joaquim Dias and Huchette, Joey and Legat, Beno{\^i}t and Vielma, Juan Pablo},
  year = {2023},
  month = sep,
  journal = {Mathematical Programming Computation},
  volume = {15},
  number = {3},
  pages = {581--589},
  issn = {1867-2949, 1867-2957},
  doi = {10.1007/s12532-023-00239-3},
  urldate = {2025-01-14},
  langid = {english}
}

@article{massalinSuperoptimizerLookSmallest1987,
  title = {Superoptimizer: A Look at the Smallest Program},
  shorttitle = {Superoptimizer},
  author = {Massalin, Henry},
  year = {1987},
  month = oct,
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {21},
  number = {4},
  pages = {122--126},
  issn = {0163-5980},
  doi = {10.1145/36204.36194},
  urldate = {2025-01-14},
  abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
  langid = {english},
  file = {/Users/jumerckx/Zotero/storage/QFF4BVZ2/Henry Massalin et al. - 1987 - Superoptimizer a look at the smallest program.pdf}
}

@inproceedings{matsumuraSymbolicEmulatorShuffle2023,
  title = {A {{Symbolic Emulator}} for {{Shuffle Synthesis}} on the {{NVIDIA PTX Code}}},
  booktitle = {Proceedings of the 32nd {{ACM SIGPLAN International Conference}} on {{Compiler Construction}}},
  author = {Matsumura, Kazuaki and De Gonzalo, Simon Garcia and Pe{\~n}a, Antonio J.},
  year = {2023},
  month = feb,
  series = {{{CC}} 2023},
  pages = {110--121},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3578360.3580253},
  urldate = {2024-10-28},
  abstract = {Various kinds of applications take advantage of GPUs through automation tools that attempt to automatically exploit the available performance of the GPU's parallel architecture. Directive-based programming models, such as OpenACC, are one such method that easily enables parallel computing by just adhering code annotations to code loops. Such abstract models, however, often prevent programmers from making additional low-level optimizations to take advantage of the advanced architectural features of GPUs because the actual generated computation is hidden from the application developer. This paper describes and implements a novel flexible optimization technique that operates by inserting a code emulator phase to the tail-end of the compilation pipeline. Our tool emulates the generated code using symbolic analysis by substituting dynamic information and thus allowing for further low-level code optimizations to be applied. We implement our tool to support both CUDA and OpenACC directives as the frontend of the compilation pipeline, thus enabling low-level GPU optimizations for OpenACC that were not previously possible. We demonstrate the capabilities of our tool by automating warp-level shuffle instructions that are difficult to use by even advanced GPU programmers. Lastly, evaluating our tool with a benchmark suite and complex application code, we provide a detailed study to assess the benefits of shuffle instructions across four generations of GPU architectures.},
  isbn = {979-8-4007-0088-0},
  keywords = {application,equality saturation,hardware},
  file = {/Users/jumerckx/Zotero/storage/GEZS39U3/Matsumura et al. - 2023 - A Symbolic Emulator for Shuffle Synthesis on the NVIDIA PTX Code.pdf}
}

@misc{MLIRRationale,
  title = {{{MLIR}} Rationale},
  author = {{MLIR}},
  year = {2019},
  urldate = {2024-11-09}
}

@inproceedings{MLSYS2021_cc427d93,
  title = {Equality {{Saturation}} for {{Tensor Graph Superoptimization}}},
  booktitle = {Proceedings of {{Machine Learning}} and {{Systems}}},
  author = {Yang, Yichen and Phothilimthana, Phitchaya and Wang, Yisu and Willsey, Max and Roy, Sudip and Pienaar, Jacques},
  editor = {Smola, A. and Dimakis, A. and Stoica, I.},
  year = {2021},
  volume = {3},
  pages = {255--268},
  keywords = {application,Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,equality saturation},
  file = {/Users/jumerckx/Zotero/storage/LC3YEDX7/Yang et al. - Equality Saturation for Tensor Graph Superoptimization.pdf;/Users/jumerckx/Zotero/storage/NDGHWQ5S/2101.html}
}

@inproceedings{nandiSynthesizingStructuredCAD2020,
  title = {Synthesizing Structured {{CAD}} Models with Equality Saturation and Inverse Transformations},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, James R. and Darulova, Eva and Grossman, Dan and Tatlock, Zachary},
  year = {2020},
  series = {Pldi 2020},
  pages = {31--44},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3385412.3386012},
  abstract = {Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly "shrink" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.},
  isbn = {978-1-4503-7613-6},
  keywords = {application,Computer-Aided Design,Decompilation,equality saturation,Equality Saturation,Program Synthesis}
}

@inproceedings{neculaTranslationValidationOptimizing2000,
  title = {Translation Validation for an Optimizing Compiler},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2000 Conference on {{Programming}} Language Design and Implementation},
  author = {Necula, George C.},
  year = {2000},
  month = may,
  pages = {83--94},
  publisher = {ACM},
  address = {Vancouver British Columbia Canada},
  doi = {10.1145/349299.349314},
  urldate = {2025-01-14},
  isbn = {978-1-58113-199-4},
  langid = {english},
  file = {/Users/jumerckx/Zotero/storage/PSR25EZI/Necula - 2000 - Translation validation for an optimizing compiler.pdf;/Users/jumerckx/Zotero/storage/Y7DKI5UZ/George C. Necula and Necula - 2000 - Translation validation for an optimizing compiler.pdf}
}

@phdthesis{nelsonTechniquesProgramVerification1979,
  title = {Techniques for Program Verification},
  author = {Nelson, Charles Gregory},
  year = {1980},
  month = jan,
  school = {Stanford University},
  keywords = {equality saturation,theory},
  annotation = {MAG ID: 2130175237\\
S2ID: 7c099e7df7e2325e906c6ecb73628e2317016359},
  file = {/Users/jumerckx/Zotero/storage/7LHIXHL8/NelsonThesis.pdf}
}

@article{newcombVerifyingImprovingHalides2020,
  title = {Verifying and Improving {{Halide}}'s Term Rewriting System with Program Synthesis},
  author = {Newcomb, Julie L. and Adams, Andrew and Johnson, Steven and Bodik, Rastislav and Kamil, Shoaib},
  year = {2020},
  month = nov,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {4},
  number = {OOPSLA},
  pages = {1--28},
  issn = {2475-1421},
  doi = {10.1145/3428234},
  urldate = {2025-01-14},
  abstract = {Halide is a domain-specific language for high-performance image processing and tensor computations, widely adopted in industry. Internally, the Halide compiler relies on a term rewriting system to prove properties of code required for efficient and correct compilation. This rewrite system is a collection of handwritten transformation rules that incrementally rewrite expressions into simpler forms; the system requires high performance in both time and memory usage to keep compile times low, while operating over the undecidable theory of integers. In this work, we apply formal techniques to prove the correctness of existing rewrite rules and provide a guarantee of termination. Then, we build an automatic program synthesis system in order to craft new, provably correct rules from failure cases where the compiler was unable to prove properties. We identify and fix 4 incorrect rules as well as 8 rules which could give rise to infinite rewriting loops. We demonstrate that the synthesizer can produce better rules than hand-authored ones in five bug fixes, and describe four cases in which it has served as an assistant to a human compiler engineer. We further show that it can proactively improve weaknesses in the compiler by synthesizing a large number of rules without human supervision and showing that the enhanced ruleset lowers peak memory usage of compiled code without appreciably increasing compilation times.},
  langid = {english},
  file = {/Users/jumerckx/Zotero/storage/MVX7RQ8S/Newcomb et al. - 2020 - Verifying and improving Halide’s term rewriting system with program synthesis.pdf}
}

@misc{optir2022,
  title = {Optir},
  author = {Sharp, Jamey},
  year = {2022},
  publisher = {GitHub},
  keywords = {application,equality saturation}
}

@misc{palLuxExplicitParameterization2023,
  title = {Lux: {{Explicit Parameterization}} of {{Deep Neural Networks}} in {{Julia}}},
  shorttitle = {Lux},
  author = {Pal, Avik},
  year = {2023},
  month = apr,
  doi = {10.5281/ZENODO.7808904},
  urldate = {2025-01-14},
  abstract = {{$<$}strong{$>$}Full Changelog{$<$}/strong{$>$}: https://github.com/LuxDL/Lux.jl/compare/v0.4.49...v0.4.50},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@inproceedings{panchekhaAutomaticallyImprovingAccuracy2015,
  title = {Automatically Improving Accuracy for Floating Point Expressions},
  booktitle = {Proceedings of the 36th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Panchekha, Pavel and {Sanchez-Stern}, Alex and Wilcox, James R. and Tatlock, Zachary},
  year = {2015},
  month = jun,
  pages = {1--11},
  publisher = {ACM},
  address = {Portland OR USA},
  doi = {10.1145/2737924.2737959},
  urldate = {2025-01-14},
  isbn = {978-1-4503-3468-6},
  langid = {english},
  keywords = {application,equality saturation},
  file = {/Users/jumerckx/Zotero/storage/6RBWCL4U/Pavel Panchekha et al. - 2015 - Automatically improving accuracy for floating point expressions.pdf}
}

@inproceedings{parvatSurveyDeeplearningFrameworks2017,
  title = {A Survey of Deep-Learning Frameworks},
  booktitle = {2017 {{International Conference}} on {{Inventive Systems}} and {{Control}} ({{ICISC}})},
  author = {Parvat, Aniruddha and Chavan, Jai and Kadam, Siddhesh and Dev, Souradeep and Pathak, Vidhi},
  year = {2017},
  month = jan,
  pages = {1--7},
  publisher = {IEEE},
  address = {Coimbatore, India},
  doi = {10.1109/ICISC.2017.8068684},
  urldate = {2025-01-14},
  isbn = {978-1-5090-4715-4},
  keywords = {Artificial neural networks,Computational modeling,Deep learning,Graphics processing units,Libraries,Machine learning,Mathematical model,Neural networks,Software libraries,Training},
  file = {/Users/jumerckx/Zotero/storage/34S3G444/Parvat et al. - 2017 - A survey of deep-learning frameworks.pdf}
}

@misc{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  number = {arXiv:1912.01703},
  eprint = {1912.01703},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.01703},
  urldate = {2024-05-22},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  file = {/Users/jumerckx/Zotero/storage/MMWWIZFP/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf;/Users/jumerckx/Zotero/storage/FVTM5MQC/1912.html;/Users/jumerckx/Zotero/storage/LXE7WXGH/1912.html}
}

@article{peterdowneyVariationsCommonSubexpression1980,
  title = {Variations on the {{Common Subexpression Problem}}},
  author = {{Peter Downey} and Downey, Peter J. and {Ravi Sethi} and Sethi, Ravi and {Robert E. Tarjan} and Tarjan, Robert E.},
  year = {1980},
  month = oct,
  journal = {Journal of the ACM},
  volume = {27},
  number = {4},
  pages = {758--771},
  doi = {10.1145/322217.322228},
  abstract = {article Free Access Share on Variations on the Common Subexpression Problem Authors: Peter J. Downey Department of Computer Science, The University of Arizona, Tucson, Arizona Department of Computer Science, The University of Arizona, Tucson, ArizonaView Profile , Ravi Sethi Bell Laboratories, Murray Hill, New Jersey Bell Laboratories, Murray Hill, New JerseyView Profile , Robert Endre Tarjan Bell Laboratories, Murray Hill, New Jersey and Stanford University, Stanford, California Bell Laboratories, Murray Hill, New Jersey and Stanford University, Stanford, CaliforniaView Profile Authors Info \& Claims Journal of the ACMVolume 27Issue 4pp 758--771https://doi.org/10.1145/322217.322228Published:01 October 1980Publication History 349citation1,641DownloadsMetricsTotal Citations349Total Downloads1,641Last 12 Months128Last 6 weeks14 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF},
  annotation = {MAG ID: 2000346568},
  file = {/Users/jumerckx/Zotero/storage/PIZRDFBQ/Peter Downey et al. - 1980 - Variations on the Common Subexpression Problem.pdf}
}

@inproceedings{premtoonSemanticCodeSearch2020,
  title = {Semantic Code Search via Equational Reasoning},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {Premtoon, Varot and Koppel, James and {Solar-Lezama}, Armando},
  year = {2020},
  series = {Pldi 2020},
  pages = {1066--1082},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3385412.3386001},
  abstract = {We present a new approach to semantic code search based on equational reasoning, and the Yogo tool implementing this approach. Our approach works by considering not only the dataflow graph of a function, but also the dataflow graphs of all equivalent functions reachable via a set of rewrite rules. In doing so, it can recognize an operation even if it uses alternate APIs, is in a different but mathematically-equivalent form, is split apart with temporary variables, or is interleaved with other code. Furthermore, it can recognize when code is an instance of some higher-level concept such as iterating through a file. Because of this, from a single query, Yogo can find equivalent code in multiple languages. Our evaluation further shows the utility of Yogo beyond code search: encoding a buggy pattern as a Yogo query, we found a bug in Oracle's Graal compiler which had been missed by a hand-written static analyzer designed for that exact kind of bug. Yogo is built on the Cubix multi-language infrastructure, and currently supports Java and Python.},
  isbn = {978-1-4503-7613-6},
  keywords = {application,code search,equality saturation,equational reasoning},
  file = {/Users/jumerckx/Zotero/storage/QD4Y87NK/Premtoon et al. - 2020 - Semantic code search via equational reasoning.pdf}
}

@article{ragan-kelleyHalideLanguageCompiler2013,
  title = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
  author = {{Ragan-Kelley}, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'e}do and Amarasinghe, Saman},
  year = {2013},
  month = jun,
  journal = {SIGPLAN Not.},
  volume = {48},
  number = {6},
  pages = {519--530},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/2499370.2462176},
  abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
  issue_date = {June 2013},
  keywords = {autotuning,compiler,domain specific language,gpu,image processing,locality,optimization,parallelism,redundant computation,vectorization},
  file = {/Users/jumerckx/Zotero/storage/LHZ7W9A4/Ragan-Kelley et al. - 2013 - Halide a language and compiler for optimizing parallelism, locality, and recomputation in image pro.pdf}
}

@inproceedings{ragan-kelleyHalideLanguageCompiler2013a,
  title = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
  booktitle = {Proceedings of the 34th {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {{Ragan-Kelley}, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'e}do and Amarasinghe, Saman},
  year = {2013},
  series = {Pldi '13},
  pages = {519--530},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2491956.2462176},
  abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
  isbn = {978-1-4503-2014-6},
  keywords = {autotuning,compiler,domain specific language,gpu,image processing,locality,optimization,parallelism,redundant computation,vectorization}
}

@article{reissmannRVSDGIntermediateRepresentation2020,
  title = {{{RVSDG}}: {{An Intermediate Representation}} for {{Optimizing Compilers}}},
  shorttitle = {{{RVSDG}}},
  author = {Reissmann, Nico and Meyer, Jan Christian and Bahmann, Helge and Sj{\"a}lander, Magnus},
  year = {2020},
  month = nov,
  journal = {ACM Transactions on Embedded Computing Systems},
  volume = {19},
  number = {6},
  eprint = {1912.05036},
  primaryclass = {cs},
  pages = {1--28},
  issn = {1539-9087, 1558-3465},
  doi = {10.1145/3391902},
  urldate = {2024-07-01},
  abstract = {Intermediate Representations (IRs) are central to optimizing compilers as the way the program is represented may enhance or limit analyses and transformations. Suitable IRs focus on exposing the most relevant information and establish invariants that different compiler passes can rely on. While control-flow centric IRs appear to be a natural fit for imperative programming languages, analyses required by compilers have increasingly shifted to understand data dependencies and work at multiple abstraction layers at the same time. This is partially evidenced in recent developments such as the MLIR proposed by Google. However, rigorous use of data flow centric IRs in general purpose compilers has not been evaluated for feasibility and usability as previous works provide no practical implementations. We present the Regionalized Value State Dependence Graph (RVSDG) IR for optimizing compilers. The RVSDG is a data flow centric IR where nodes represent computations, edges represent computational dependencies, and regions capture the hierarchical structure of programs. It represents programs in demand-dependence form, implicitly supports structured control flow, and models entire programs within a single IR. We provide a complete specification of the RVSDG, construction and destruction methods, as well as exemplify its utility by presenting Dead Node and Common Node Elimination optimizations. We implemented a prototype compiler and evaluate it in terms of performance, code size, compilation time, and representational overhead. Our results indicate that the RVSDG can serve as a competitive IR in optimizing compilers while reducing complexity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/jumerckx/Zotero/storage/PVX3VKTT/Reissmann et al. - 2020 - RVSDG An Intermediate Representation for Optimizi.pdf;/Users/jumerckx/Zotero/storage/BR9AGDCG/1912.html;/Users/jumerckx/Zotero/storage/KJ5HSZZG/1912.html}
}

@article{rushikeshk.joshiDenaliGoaldirectedSuperoptimizer2002,
  title = {Denali: A Goal-Directed Superoptimizer},
  author = {{Rushikesh K. Joshi} and Joshi, Rajeev and {Greg Nelson} and Nelson, Greg and {Keith H. Randall} and Randall, Keith H.},
  year = {2002},
  month = may,
  journal = {ACM-SIGPLAN Symposium on Programming Language Design and Implementation},
  volume = {37},
  number = {5},
  pages = {304--314},
  doi = {10.1145/512529.512566},
  abstract = {This paper provides a preliminary report on a new research project that aims to construct a code generator that uses an automatic theorem prover to produce very high-quality (in fact, nearly mathematically optimal) machine code for modern architectures. The code generator is not intended for use in an ordinary compiler, but is intended to be used for inner loops and critical subroutines in those cases where peak performance is required, no available compiler generates adequately efficient code, and where current engineering practice is to use hand-coded machine language. The paper describes the design of the superoptimizer, and presents some encouraging preliminary results.},
  annotation = {MAG ID: 2150871888\\
S2ID: 5090ed315d3ab9f0135c83f287c5021d61929760},
  file = {/Users/jumerckx/Zotero/storage/YS8X76TX/Joshi et al. - 2002 - Denali a goal-directed superoptimizer.pdf}
}

@inproceedings{schkufzaStochasticSuperoptimization2013,
  title = {Stochastic Superoptimization},
  booktitle = {Proceedings of the Eighteenth International Conference on {{Architectural}} Support for Programming Languages and Operating Systems},
  author = {Schkufza, Eric and Sharma, Rahul and Aiken, Alex},
  year = {2013},
  month = mar,
  pages = {305--316},
  publisher = {ACM},
  address = {Houston Texas USA},
  doi = {10.1145/2451116.2451150},
  urldate = {2025-01-14},
  isbn = {978-1-4503-1870-9},
  langid = {english},
  file = {/Users/jumerckx/Zotero/storage/92C8KNRW/Schkufza et al. - 2013 - Stochastic superoptimization.pdf}
}

@inproceedings{smithPureTensorProgram2021,
  title = {Pure Tensor Program Rewriting via Access Patterns (Representation Pearl)},
  booktitle = {Proceedings of the 5th {{ACM SIGPLAN International Symposium}} on {{Machine Programming}}},
  author = {Smith, Gus Henry and Liu, Andrew and Lyubomirsky, Steven and Davidson, Scott and McMahan, Joseph and Taylor, Michael and Ceze, Luis and Tatlock, Zachary},
  year = {2021},
  month = jun,
  pages = {21--31},
  publisher = {ACM},
  address = {Virtual Canada},
  doi = {10.1145/3460945.3464953},
  urldate = {2025-01-14},
  isbn = {978-1-4503-8467-4},
  langid = {english},
  keywords = {application,Computer Science - Programming Languages,equality saturation,tensor compiler},
  file = {/Users/jumerckx/Zotero/storage/4IJNGHAI/Smith et al. - 2021 - Pure tensor program rewriting via access patterns (representation pearl).pdf}
}

@misc{sniderOperatorFusionXLA2023,
  title = {Operator Fusion in {{XLA}}: {{Analysis}} and Evaluation},
  author = {Snider, Daniel and Liang, Ruofan},
  year = {2023},
  eprint = {2301.13062},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@inproceedings{steppEqualityBasedTranslationValidator2011,
  title = {Equality-{{Based Translation Validator}} for {{LLVM}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Stepp, Michael and Tate, Ross and Lerner, Sorin},
  editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
  year = {2011},
  pages = {737--742},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-22110-1_59},
  abstract = {We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks.},
  isbn = {978-3-642-22110-1},
  langid = {english},
  keywords = {application,Compiler Optimization,Congruence Closure,equality saturation,Equality Saturation,Original Code,Translation Validation,verification},
  file = {/Users/jumerckx/Zotero/storage/JICNVZE6/Stepp et al. - 2011 - Equality-Based Translation Validator for LLVM.pdf}
}

@misc{suciuSemanticFoundationsEquality2025,
  title = {Semantic Foundations of Equality Saturation},
  author = {Suciu, Dan and Wang, Yisu Remy and Zhang, Yihong},
  year = {2025},
  month = jan,
  number = {arXiv:2501.02413},
  eprint = {2501.02413},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.02413},
  urldate = {2025-01-14},
  abstract = {Equality saturation is an emerging technique for program and query optimization developed in the programming language community. It performs term rewriting over an E-graph, a data structure that compactly represents a program space. Despite its popularity, the theory of equality saturation lags behind the practice. In this paper, we define a fixpoint semantics of equality saturation based on tree automata and uncover deep connections between equality saturation and the chase. We characterize the class of chase sequences that correspond to equality saturation. We study the complexities of terminations of equality saturation in three cases: single-instance, all-term-instance, and all-E-graph-instance. Finally, we define a syntactic criterion based on acyclicity that implies equality saturation termination.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Programming Languages,equality saturation,theory},
  file = {/Users/jumerckx/Zotero/storage/XI6H9T78/Suciu et al. - 2025 - Semantic foundations of equality saturation.pdf;/Users/jumerckx/Zotero/storage/JPPJ5J3R/2501.html;/Users/jumerckx/Zotero/storage/VVUXR2DC/2501.html}
}

@misc{sunEgraphsCircuitsOptimal2024,
  title = {E-Graphs as Circuits, and Optimal Extraction via Treewidth},
  author = {Sun, Glenn and Zhang, Yihong and Ni, Haobin},
  year = {2024},
  eprint = {2408.17042},
  primaryclass = {cs.DS},
  abstract = {We solve the optimal extraction problem for e-graphs by first showing a connection between e-graphs and cyclic monotone Boolean circuits, then solving the weighted satisfiability problem for such circuits. The solution is a parameterized algorithm based on treewidth. Additionally, we show how the circuit view of e-graphs allows us to apply simplification techniques that are not possible when operating directly on e-graphs. While the core parameterized algorithm may be adapted to work directly on e-graphs, the simplification results show why the circuit view is helpful.},
  archiveprefix = {arXiv},
  keywords = {equality saturation,extraction,theory},
  file = {/Users/jumerckx/Zotero/storage/S3QUTQZA/Sun et al. - 2024 - E-graphs as circuits, and optimal extraction via treewidth.pdf}
}

@misc{swiftSIL,
  title = {Swift Intermediate Language ({{SIL}})},
  author = {{swiftlang}},
  year = {2012},
  publisher = {GitHub},
  urldate = {2024-09-11}
}

@inproceedings{tateEqualitySaturationNew2009,
  title = {Equality Saturation: A New Approach to Optimization},
  shorttitle = {Equality Saturation},
  booktitle = {Proceedings of the 36th Annual {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
  year = {2009},
  month = jan,
  series = {{{POPL}} '09},
  pages = {264--276},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1480881.1480915},
  urldate = {2024-10-08},
  abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
  isbn = {978-1-60558-379-2},
  keywords = {application,equality saturation,theory},
  file = {/Users/jumerckx/Zotero/storage/T28X4HK5/Tate et al. - 2009 - Equality saturation a new approach to optimization.pdf}
}

@inproceedings{ustunIMpressLargeInteger2022,
  title = {{{IMpress}}: {{Large Integer Multiplication Expression Rewriting}} for {{FPGA HLS}}},
  shorttitle = {{{IMpress}}},
  booktitle = {2022 {{IEEE}} 30th {{Annual International Symposium}} on {{Field-Programmable Custom Computing Machines}} ({{FCCM}})},
  author = {Ustun, Ecenur and San, Ismail and Yin, Jiaqi and Yu, Cunxi and Zhang, Zhiru},
  year = {2022},
  month = may,
  pages = {1--10},
  publisher = {IEEE},
  address = {New York City, NY, USA},
  doi = {10.1109/FCCM53951.2022.9786123},
  urldate = {2025-01-14},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-8332-2},
  keywords = {application,equality saturation,hardware},
  file = {/Users/jumerckx/Zotero/storage/GH96SCJB/Ecenur Ustun et al. - 2022 - IMpress Large Integer Multiplication Expression Rewriting for FPGA HLS.pdf}
}

@inproceedings{vanhattumVectorizationDigitalSignal2021,
  title = {Vectorization for Digital Signal Processors via Equality Saturation},
  booktitle = {Proceedings of the 26th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {VanHattum, Alexa and Nigam, Rachit and Lee, Vincent T. and Bornholt, James and Sampson, Adrian},
  year = {2021},
  month = apr,
  pages = {874--886},
  publisher = {ACM},
  address = {Virtual USA},
  doi = {10.1145/3445814.3446707},
  urldate = {2025-01-14},
  isbn = {978-1-4503-8317-2},
  langid = {english},
  keywords = {application,equality saturation,hardware},
  file = {/Users/jumerckx/Zotero/storage/EZUXGCNH/Alexa VanHattum et al. - 2021 - Vectorization for digital signal processors via equality saturation.pdf}
}

@article{wangSPORESSumproductOptimization2020,
  title = {{{SPORES}}: Sum-Product Optimization via Relational Equality Saturation for Large Scale Linear Algebra},
  shorttitle = {{{SPORES}}},
  author = {Wang, Yisu Remy and Hutchison, Shana and Leang, Jonathan and Howe, Bill and Suciu, Dan},
  year = {2020},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {1919--1932},
  issn = {2150-8097},
  doi = {10.14778/3407790.3407799},
  urldate = {2025-01-14},
  abstract = {Machine learning algorithms are commonly specified in linear algebra (LA). LA expressions can be rewritten into more efficient forms, by taking advantage of input properties such as               sparsity               , as well as program properties such as               common subexpressions               and               fusible operators.               The complex interaction among these properties' impact on the execution cost poses a challenge to optimizing compilers. Existing compilers resort to intricate heuristics that complicate the codebase and add maintenance cost, but fail to search through the large space of equivalent LA expressions to find the cheapest one. We introduce a general optimization technique for LA expressions, by converting the LA expressions into Relational Algebra (RA) expressions, optimizing the latter, then converting the result back to (optimized) LA expressions. The rewrite rules we design in this approach are complete, meaning that any equivalent LA expression is covered in the search space. The challenge is the major size of the search space, and we address this by adopting and extending a technique used in compilers, called               equality saturation.               Our optimizer, SPORES, uses rule sampling to quickly cover vast portions of the search space; it then uses a constraint solver to extract the optimal plan from the covered space, or alternatively uses a greedy algorithm to shorten compile time. We integrate SPORES into SystemML and validate it empirically across a spectrum of machine learning tasks; SPORES can derive all existing hand-coded optimizations in SystemML, and perform new optimizations that lead to up to 10X speedup.},
  langid = {english},
  keywords = {application,equality saturation},
  file = {/Users/jumerckx/Zotero/storage/6VYJVJ2R/Yisu Remy Wang et al. - 2020 - SPORES sum-product optimization via relational equality saturation for large scale linear algebra.pdf}
}

@article{wegmanConstantPropagationConditional1991,
  title = {Constant Propagation with Conditional Branches},
  author = {Wegman, Mark N. and Zadeck, F. Kenneth},
  year = {1991},
  month = apr,
  journal = {ACM Trans. Program. Lang. Syst.},
  volume = {13},
  number = {2},
  pages = {181--210},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {0164-0925},
  doi = {10.1145/103135.103136},
  abstract = {Constant propagation is a well-known global flow analysis problem. The goal of constant propagation is to discover values that are constant on all possible executions of a program and to propagate these constant values as far foward through the program as possible. Expressions whose operands are all constants can be evaluated at compile time and the results propagated further. Using the algorithms presented in this paper can produce smaller and faster compiled programs. The same algorithms can be used for other kinds of analyses (e.g., type of determination). We present four algorithms in this paper, all conservitive in the sense that all constants may not be found, but each constant found is constant over all possible executions of the program. These algorithms are among the simplest, fastest, and most powerful global constant propagation algorithms known. We also present a new algorithm that performs a form of interprocedural data flow analysis in which aliasing information is gathered in conjunction with constant progagation. Several variants of this algorithm are considered.},
  issue_date = {April 1991},
  keywords = {abstract interpretation,code optimization,constant propagation,control flow graph,interprocedural analysis,procedure integration,static single assignment form,type determination}
}

@article{willseyEggFastExtensible2021,
  title = {Egg: {{Fast}} and Extensible Equality Saturation},
  shorttitle = {Egg},
  author = {Willsey, Max and Nandi, Chandrakana and Wang, Yisu Remy and Flatt, Oliver and Tatlock, Zachary and Panchekha, Pavel},
  year = {2021},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {5},
  number = {POPL},
  pages = {1--29},
  issn = {2475-1421},
  doi = {10.1145/3434304},
  urldate = {2024-02-26},
  abstract = {An e-graph efficiently represents a congruence relation over many expressions. Although they were originally developed in the late 1970s for use in automated theorem provers, a more recent technique known as equality saturation repurposes e-graphs to implement state-of-the-art, rewrite-driven compiler optimizations and program synthesizers. However, e-graphs remain unspecialized for this newer use case. Equality saturation workloads exhibit distinct characteristics and often require ad-hoc e-graph extensions to incorporate transformations beyond purely syntactic rewrites.             This work contributes two techniques that make e-graphs fast and extensible, specializing them to equality saturation. A new amortized invariant restoration technique called rebuilding takes advantage of equality saturation's distinct workload, providing asymptotic speedups over current techniques in practice. A general mechanism called e-class analyses integrates domain-specific analyses into the e-graph, reducing the need for ad hoc manipulation.             We implemented these techniques in a new open-source library called egg. Our case studies on three previously published applications of equality saturation highlight how egg's performance and flexibility enable state-of-the-art results across diverse domains.},
  langid = {english},
  keywords = {egraphs,equality saturation,theory},
  file = {/Users/jumerckx/Zotero/storage/T23525DR/Willsey et al. - 2021 - egg Fast and extensible equality saturation.pdf}
}

@misc{y.wangEGraphsVSAsTree2022,
  title = {E-{{Graphs}}, {{VSAs}}, and {{Tree Automata}}: A {{Rosetta Stone}}},
  author = {{Y. Wang} and {James Koppel} and {Altan Haan} and {Josh Pollock}},
  year = {2022},
  abstract = {Many tasks in programming languages involve representing and manipulating sets of programs. In program synthesis, the goal is to find a program satisfying a given specification from a set of programs possibly generated by a given grammar. In program optimization, the goal is to find an efficient program from the set of programs equivalent to the input. Programming languages research has considered various abstractions to represent sets of programs. Two examples are the version space algebra (VSA) [Lau et al. 2003; Mitchell 1982], popularized by FlashFill [Gulwani 2011] for enumerative program synthesis, and the e-graph [Nelson 1980; Nieuwenhuis and Oliveras 2005] that lies at the heart of an array of new program optimizers [Willsey et al. 2021]. In this talk we show that VSAs and e-graphs are but special cases of the well-studied finite-state (tree) automata from formal language theory. This new perspective allows us to place VSAs and e-graphs on a firm theoretical foundation, and also enables us to leverage powerful tools from formal language theory to perform tasks in programming languages. In the converse, bridging the concepts can also contribute to tree automata research with techniques developed for e-graphs and VSAs.},
  keywords = {equality saturation,extraction,theory},
  annotation = {S2ID: be732ff9716f6f22ed1fdbbc0199c10dad6c9130}
}
