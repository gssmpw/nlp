\documentclass[conference]{IEEEtran}

\usepackage{amsmath, amssymb, bm}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{{.}{../figs/model-compare-pdf}{./figs}}
\usepackage[backend=biber,style=authoryear,maxnames=2,sorting=none]{biblatex}
\addbibresource{ref.bib}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage[%hyperref
  hypertexnames,%
  citecolor=blue,%
  colorlinks=true,%
  linkcolor=black,%
  bookmarks=true
]{hyperref}


\newcommand{\SR}[1]{[SR:{\color{red}\textsc{#1}}]}
\newcommand{\RH}[1]{[RH:{\color{blue}\textsc{#1}}]}
\newcommand{\T}{\mathsf{T}}

%
% \usepackage{draftwatermark}
% \SetWatermarkText{DRAFT}
% \SetWatermarkScale{1.2}
% \SetWatermarkLightness{.9}



\begin{document}

\newcommand{\groupderiv}[2][]{\accentset{\scriptstyle\circ}{#2}}
\newcommand{\bodyvel}{v_b}
\newcommand{\group}{\mathfrak{G}}
\newcommand{\SE}{\mathsf{SE}}
\newcommand{\pos}{g}
\newcommand{\conn}{A}
\newcommand{\metric}{M}
\newcommand{\base}{R}
\newcommand{\shape}{r}
\newcommand{\shapevel}{\dot{r}}
\newcommand{\torq}{\tau}
\newcommand{\ctrl}{u}
\newcommand{\pass}{p}
\newcommand{\template}{\theta}
\newcommand{\templatevel}{\dot{\theta}}
\newcommand{\pert}{\delta}
\newcommand{\pertvel}{\dot{\delta}}

\title{Learning the Geometric Mechanics of Robot Motion Using Gaussian Mixtures}
\author{\IEEEauthorblockN{Ruizhen Hu}
	\IEEEauthorblockA{\textit{Robotics} \\
		\textit{University of Michigan}\\
		Ann Arbor, MI, USA \\
		\href{mailto:ruizhen@umich.edu}{ruizhen@umich.edu}}
	\and
	\IEEEauthorblockN{Shai Revzen$^*$}
	\IEEEauthorblockA{\textit{Electrical Engineering and Computer Science} \\
	\textit{University of Michigan}\\
	Ann Arbor, MI, USA \\
	\href{mailto:shrevzen@umich.edu}{shrevzen@umich.edu}}
}
\maketitle

\tableofcontents

\newcommand{\myCite}[1]{{[{\color{red}\cite{#1}}]}}
\section*{Abstract}
Data-driven models of robot motion constructed using principles from Geometric Mechanics have been shown \myCite{bittner2018geometrically, zhao2022walking, hatton2013geometric} to produce useful predictions of robot motion for a variety of robots.
For robots with a useful number of DoF, these geometric mechanics models can only be constructed in the neighborhood of a gait.
Here we show how Gaussian Mixture Models (GMM) can be used as a form of manifold learning that learns the structure of the Geometric Mechanics ``motility map\footnote{In the geometric mechanics literature, what we refer to as the ``motility map'' here is often called the ``(negative of the) local connection''. 
This terminology reflects its mathematical provenance from the fiber bundle literature, and not its application. 
We have therefore chosen a more readable name for use here.}'' and demonstrate: [i] a sizable improvement in prediction quality when compared to the previously published methods; [ii] a method that can be applied to any motion dataset and not only periodic gait data; [iii] a way to pre-process the data-set to facilitate extrapolation in places where the motility map is known to be linear.
Our results can be applied anywhere a data-driven geometric motion model might be useful.

\section{Introduction}

\subsection{A Brief Review of Classical Geometric Mechanics}

Consider a collection of inter-connected bodies whose configuration $q$ is subject to forces $F(q,
\dot q)$ that are equivariant under changes of position and orientation due to a Lie group $\group$, such as $\group=\mathsf{SE}(3)$.
The equivariance means that $F( g(q), ad_g(\dot q) ) = ad^*_g F(q, \dot q)$ i.e. for any group element $g \in \group$, i.e. changes in coordinate frame transform forces to the forces that would have arisen from the transformed positions and velocities.
Typically this arises by having multiple body parts whose positions relative to the body frame are defined by its ``shape'', and having the forces arise through contact interactions of those body parts with a homogeneous environment.
From a mathematical perspective, it is ``shape'' that arises from the equivalence of body configurations under the Lie group action, i.e. shape is the equivalence class $r := [q]_\group$.
The configuration thus partitions into a changing ``(body) shape'' $r$ and a moving body frame $g$, with associated body velocity in the relevant Lie algebra, $v_b \in \mathcal{A}_\group$ (e.g. $\mathsf{se}(3)$ for $\SE(3)$).

The resulting dynamics lead to the ``reconstruction equation''
\begin{align}
	v_b = A(r) \dot r + \mathbb{I}^{-1}(r) p
\end{align}
for some momentum $p$ associated with the moving body frame, and expressed through a shape dependent ``frozen inertia tensor'' $\mathbb{I}(r)$.
For many systems, $p=0$, or $p \to 0$ so quickly that the $p = 0$ assumption is reasonable, leaving only $A(r)$ to govern the motion; such systems are called ``principally kinematic''.
This $A(r)$ term is known as the ``local connection'' or ``motility map''.

In previous work our group has shown methods that can learn the motility map model in the neighborhood of a periodic motion \myCite{bittner2018geometrically}, and extended these learning methods to systems with non-zero momentum \myCite{kvalheim2019gait}, and under-actuated systems with passive degrees of freedom \myCite{bittner2022data}.
We have also shown that data driven motility map models are highly predictive for multilegged locomotion with or without slipping in both animals and robots \myCite{zhao2022walking}.
Follow up work by Bittner also showed that these models could be updated with online learning \myCite{deng2024adaptive}.

However, data driven motility map models remained either ``easy'' to identify near a periodic gait, or several related gaits \myCite{bittner2021optimizing}, or ``exponentially hard'' to identify, requiring an amount of data exponential in the number of DoF for an exhaustive model \myCite{dai2016geometric} throughout the phase space.

\subsection{Contribution}

In a principally kinematic system, the entire physics of body-environment interactions is captured by the motility map model $A(r)$ of the equation: $v_b = A(r) \dot{r}$.
It is a mathematical truism that any function can be represented by its ``graph'' -- the set of points that comprise its input-output pairing in the cartesian product of the domain and co-domain -- and that graph is a manifold for smooth functions.
The graph of the motility map can be represented by the constraint equation that is its indicator:
\begin{align}
  f(v_b, r, \dot{r}) = v_b - A(r) \dot{r} = 0. \label{eqn:graph}
\end{align}

In general, one should exercise caution when learning manifolds, since without additional prior knowledge the local structure of a manifold at a point provides no information (other than dimension) about the local structure of the manifold at other points.
Thus when learning a manifold it is typical to interpolate between known points of the manifold, but be cautious and conservative when extrapolating.
However, the graph of a motility map is a ``ruled surface'' -- it is a union of affine subspaces, because of its linearity in $\dot r$.
This special structure suggests that at least these affine subspaces can be learned faster, e.g. by the use of an additional meta-parameter governing how much linear extrapolation can be allowed in the $\dot r$ direction(s).

Our work here provides several contributions:
\begin{enumerate}
  \item We reformulate the problem of learning a motility map as a manifold learning problem, allowing for non-linearities in $\dot r$ such as those predicted by \myCite{kvalheim2019gait}, other non-linear structures in $r$ and $\dot r$, and hysteretic multi-valued motility maps to be represented.
  \item We provide a data transformation that allows the degree of linear extrapolation in the $\dot r$ direction to be controlled.
  \item We present an adaptation of well-known Gaussian Mixture Model fitting techniques to this machine learning problem.
  \item We demonstrate that when applied to the datasets of \myCite{zhao2020multi} (available at \myCite{bigant6dataset}), consisting of hexapedal robot data, this new approach noticeably improves the predictive ability relative to the prior state of the art. %SR: better have something quantitative
\end{enumerate}

\section{Methods}

\subsection{Manifold Learning}

Manifold learning is the process of estimating the structure of a manifold from a set of noisy observations taken from that manifold. 
It operates on the assumption that high-dimensional data often lies on or near a lower-dimensional manifold embedded within the higher-dimensional space.
Since every smooth function defines a manifold which is its graph, methods of estimating manifolds from the input-output pairs of a function are themselves methods of function regession. 

The most commonly familiar form of this relationship between estimating functions and fitting their graphs is the relationship between (Ordinary) Linear Least Squares Regression, which is a form of function estimation, and Total Least Squares (TLS) Regression which is a method for estimating the best fit hyperplane through the data -- a form of manifold regression restricting the class of manifolds to hyperplanes.
Because all $k$-dimensional manifolds have the property that at any point $p$ they are locally close to a $k$-dimensional affine subspace -- the manifold's tangent at $p$ -- one can approximate any manifold as a mixture of local TLS regressions estimating these tangent spaces.

From a practical standpoint, many functions modeling physical systems exhibit hysteretic behavior -- they can follow different branches of values for the same input, but once on a branch, they remain on that branch.
This form of hysteresis can be represented computationally by looking up the value of a function based on both its parameters and the most recent value it returned, with the latter value serving to select the appropriate branch.

Here we suggest that the motility map's graph be considered a manifold for such a manifold learning problem, as described by eqn.\ref{eqn:graph}.

\subsection{Gaussian Mixture Models as a form of Manifold Learning}

This section describes why GMM modeling of sample distribution of the graph of a nonlinear function is a natural extension of ``Total Least Squares'' regression to manifolds.

Consider now a linear model $y = A x$, $x \in X$, and $y \in Y$, applied to a sample of $x$ points coming from a Gaussian distribution $N(\mu,\sigma)$.
The graph of this linear model is a point set in the direct sum vector space $W := X \oplus Y$.

\newcommand{\Aut}{\text{Aut}}
We use the direct sum operator $\oplus$ to remind the reader that the vector space structure of the spaces $X$ and $Y$ extends naturally to $W$ as $(x \oplus y) + \alpha (v \oplus u) := (x +\alpha v) \oplus (y + \alpha u)$.
Additionally, any linear map $M$ from $W$ to itself, i.e. $M\in\Aut(W)$ has four parts: $M_{xx} \in\Aut(Y)$, $M_{yy}\in\Aut(Y)$, $M_{xy}\in L(Y,X)$ and $M_{yx} \in L(X,Y)$, such that $M (x \oplus y) := (M_{xx} x + M_{xy} y) \oplus (M_{yx} x + M_{yy} y)$ following the usual block structure of matrix operations.
When $M_{xy}$ and $M_{yx}$ are both zero, we write the block diagonal $M$ as $M_{xx} \oplus M_{yy}$, noting that in this case $(M_{xx} \oplus M_{yy}) (x \oplus y) = (M_{xx} x) \oplus (M_{yy} y)$.

Thus the graph of points $(x,Ax)$ is itself a sample from a (degenerate) Gaussian $N( \mu \oplus A \mu, S)$, where $S$ is a rank $\dim X$ symmetric non-negative matrix whose $S_{xx}$ block is $\sigma$ and whose $S_{yy}$ block is $A \sigma A^T$.
Assuming the measurements of $x$ (resp. $y$) are corrupted by (independent) Gaussian measurement noise with covariance $C_x$ (resp. $C_y$) the noisy data of the graph is still a sample from a Gaussian, now $N( \mu \oplus A \mu,~ S+(C_x \oplus C_y))$, and no longer degenerate. 

Assuming $\sigma$ is sufficiently larger than $C_x$ and $C_y$, the matrix $A$ can be recovered from $S$ by using Total Least Squares regression.
The TLS computation splits $W$ space along the eigenspaces of $S$, by decomposing $S = U^T d U$ with $U$ orthogonal and $d$ a non-negative diagonal matrix with non-increasing elements along the diagonal.
The TLS estimate of the graph is the kernel of the $\dim Y$ last rows of $U$, i.e. the space $U_{yx} x + U_{yy} y = 0$, or 
\begin{align}
  y &:= A x = - U_{yy}^{-1} U_{yx} x. \label{eqn:TLS}
\end{align}

\subsubsection{Example}

%\newcommand{\Pr}{\text{Pr}}
Consider the algebraic variety $y^2 = (x - 1)^2 (x + 1)^2$ shown in Fig.~\ref{figXgmm}.
It consists of two branches: $y = (x - 1)(x + 1)$ and $y=-(x - 1)(x + 1)$, each of which is a manifold, and provides us with a toy example of a multi-valued function.
The variety itself, as a point set in the $(x,y)$ plane, is of measure 0, but a sampling process sampling $x$ uniformly in $[-1.5, 1.5]$, then taking a corresponding $y$ from either branch at equal probability, and finally corrupting both $x$ and $y$ with $N(0,0.15)$ Gaussian measurement noise, 
We built a GMM modeling these data, described in terms of a list of Gaussians $N(\mu_k, S_k)$ with their respective weights $w_k$.

Our predicted value for the function at $x$ with prior measurement $y_p$ is computed using as following.
Define 
\begin{align}
   p_k(x,y) &:= p_k := \Pr((x,y) | N(\mu_k, S_k)),
\end{align}
the probability of the point $(x,y)$ belonging to the $k$-th Gaussian in the mixture model.

The predicted value $\hat y(x)$ given the previous value of $y_p$ is:
\begin{align}
  \hat y (x) := \frac{
    \sum_k w_k p_k(x,y_p)  \left( A (x - \mu_{x,k}) + \mu_{y,k} \right) }
  {  \sum_k w_k p_k(x,y_p) }
\end{align}

\begin{figure}[h]
  \centering
  \includegraphics[width=.48\textwidth]{/figs/stashed-PDFs/GMM-manifold-learning.pdf}
  \caption{%
    Example of Gaussian Mixture Model (GMM) based Manifold Regression. %
    We considered the algebraic variety $y^2 = (x-1)^2(x+1)^2$, which has symmetric positive (blue line) and negative (red line) branches. %
    We simulated 1000 noisy measurements (blue circles) with measurement noise $N(0,0.15)$ in both $x$ and $y$ with $x$ taken uniformly from $[-1.5, 1.5]$, and the $y$ branch chosen at random with equal probability. %graph
    Using \texttt{scikit.learn.gmm}, we represented the sample points as a GMM and plotted the covariance ellipses of the constituent Gaussians (black ellipses). %
    We then computed and plotted the predicted value of the model for a point traveling from $(-1,1)$ to the right, thus taking the positive (purple) branch, and similarly for a point traveling to the right from $(-1,-1)$ on the negative (dark red) branch. %  
    \label{figXgmm}
  }
\end{figure}

\subsection{Pre-processing to Produce a Ruled Surface}

As noted above, the GMM modeling approach makes no \textit{a-priori} assumptions about the structure of the manifold being modeled, but the graph of the motility map is a ruled surface which is affine in $\dot r$.

To enhance our algorithm's ability to produce the desired linear relationship, we first construct a GMM with the sample data, and then reprocess the data to create extrapolations of the linear relationship, as follows.
\begin{enumerate}
	\item Define $n_r$ to be the dimension of $r$, $n_v$ the dimension of $v_b$, and $n_{tot} := n_v + 2 n_r$ the total dimension of the space containing the graph. 
	Pick meta-parameters $\alpha>0$, $\beta>0$, and integer $c>0$
	\item Produce a GMM model $\{(w_k, \mu_k, S_k)\}_{k=1}^N$ consisting of $N$ Gaussians, where the $\mu_k$ is the mean of the $k$-th constituent Gaussian, whose variance is $S_k$ and whose relative weight is $w_k$. 
	For each of these Gaussians, compute $A_k$ such that $v_b = {\mu_{v_b}} + A_k (\dot r - \mu_{\dot r})$ the TLS estimate of the motility map $A(\mu_r)$.
	\item Define the Mahalanobis distance of the point $x = (v_b,r,\dot r)$ from Gaussian $k$ is $m_k(x) := (x-\mu_k)^\T S_k^{-1} (x-\mu_k) / n_{tot}$.
	A value of $m_k(x)<1$ implies the point is ``typical'' for that Gaussian, i.e. within one standard deviation from the mean.
	We will use $C_k := \{ x | m_k(x) \leq 1 \}$ as the points ``associated'' with the Gaussian $k$. 
	\item For each $C_k$ that contains $N_k$ data points, with $N_k \geq c $ do the following $\lceil \alpha N_k \rceil$ times to create an augmenting dataset ${\hat C}_k$:
	\begin{enumerate}
		\item Choose at random with replacement data points $(v_i,r_i,{\dot r}_i) \in C_k$, $i = 1\ldots c$
		\item Choose at random from ${\tilde b}_i \sim N(0,1)$ for $i = 1\ldots c$.
		  Define renormalized weights $b_i := {\tilde b}_i \left( \sum_{i=1}^c {\tilde b}^2_i \right)^{-1/2}$.
		  These correspond to a point uniformly sampled from the unit sphere in $c$ dimensions.
		\item Add the artificial point $\hat x := ({\hat v}_b,{\hat r},{\hat {\dot r}})$ to ${\hat C}_k$, given by:
			\begin{align}
				{\hat v}_b & := \mu_{k,v_b} + \beta \sum_{i=1}^c b_i (v_i-\mu_{k,v_b}) \nonumber\\
				{\hat r} & := \mu_{k,r} + \sum_{i=1}^c b_i (r_i-\mu_{k,r}) \nonumber\\
				{\hat {\dot r}} & := \mu_{k,\dot r} + \beta \sum_{i=1}^c b_i ({\dot r}_i-\mu_{k,\dot r})
			\end{align}
	\end{enumerate}
	\item Add the augmenting dataset to the original dataset and recompute the GMM to produce an ``Augmented GMM''
\end{enumerate}

Assuming the Gaussians each have $c$ data points, this process creates an additional factor of $\alpha$ data points, each of which is the linear combination of $c$ points extrapolated by a factor of $\beta$.
If the original data was affine in its relationship between $\dot r$ and $v_b$, the augmentation with a $\beta>1$ extrapolates this relationship to a larger distance along the $\dot r$ directions, which would result in an Augmented GMM that respects this affine relationship over a larger range of $\dot r$.

Sensible choices for the meta-parameters require choosing $c$ large enough to sufficiently reduce the noise of the affine model by averaging nearby points so that an extrapolation by factor $\beta$ will still be accurate enough.

For our example here we chose the number of Gaussians as $60$, $\alpha = 1, \beta = 1.2, c = 5$.

\subsection{Model types}

Since our goal is to improve the ability to construct data-driven motility map models, we chose to compare robot motion tracking data (``ground truth'') to our new GMM based approach, and to two additional model types derived from the approach of \myCite{bittner2018geometrically} and \myCite{zhao2022walking}.
We challenged our models with extrapolation, similar to \myCite{zhao2022walking}, by building a model using training data from an intermediate turning gait, and testing against both sharper and more moderate turns.
This is because only models that can correctly extrapolate motions outside of training data are usable for the optimization framework shown in \myCite{bittner2018geometrically} and \myCite{bittner2021optimizing}.

For each training dataset, we computed a motility map model following the construction in \myCite{bittner2018geometrically}, namely first separating the data into bins by ``phase'', i.e. where the robot was within its cycle of motion.
For each bin we computed the average body velocity, and built a first order Taylor series expansion of the motility map around this phase-averaged behavior.
Finally, we interpolated the bin averages and the tensors representing the Taylor series using a Fourier series to produce a body velocity estimation function of the form $v_b(\phi,r,\dot r) := v_0(\phi) + \Delta(\phi,r) \dot r$ giving body velocity as a function of phase, shape, and shape velocity.
Here the average body velocity as a function of phase was $v_0(\phi)$, and we added a first order correction $\Delta(\phi,\cdot)\cdot$.

The two additional model types we compared with are the ``Geometric'' model given by $v_b(\phi,r,\dot r)$, and the ``Phase'' model given by $v_0(\phi)$ which does not utilize any information about how the shape changes away from the average training shape at that phase.

\subsection{Loss Function}

To select a good set of meta-parameters, we chose a loss function representing a non-dimensionalized modeling error.
For each stride, i.e. full gait cycle, we consider the forward (initial body frame $x$), sideways (initial body frame $y$), and heading (initial body frame $\theta$) motion produced.
Ignoring any possible correlation of these variables, we took the loss to be the total $z$-score of the $x$, $y$, and $\theta$ predictions with respect to the corresponding variable in the ground truth data.
This means that with our choice of loss function, the loss of a random permutation of the ground truth data against itself was set to approach $3$, and the loss of a prefect prediction is zero.
\section{Results}

\begin{figure*}[h]
	\centering
	\includegraphics[height=5.5cm, trim={0 0 3.3cm 0},clip]{/figs/stashed-PDFs/trail0_s0d25__traj.pdf}
	\includegraphics[height=5.5cm, trim={2.1cm 0 3.3cm 0},clip]{/figs/stashed-PDFs/trail0_s0d75__traj.pdf}
	\hspace{2mm}
    \includegraphics[height=5cm]{/figs/BigANT-turn}
	\caption{%
		Comparison of model extrapolation to less and more turning. %
		We compared the predictions of the models for a recording of $6$ strides (cycles) of motion, after training on data from an intermediate turning rate (steering parameter $s=0.50$ of \myCite{zhao2020multi}). %
		We attempted to predict both noticeably lesser ($s=0.25$; left) and noticeably greater ($s=0.75$; middle) turning rates; the different turning radii are demonstrated by a circle fitted to the motion tracking data (dashed green line and circular markers for centers; right plot; reproduced from Fig. 3.9 of \myCite{Zhao-2021-PhD}). %
		In each case we plotted the ground truth motion capture body centroid (thick red lines, triangle markers), the Geometric model (blue lines, circle markers), the Phase model (green lines, circle markers), and the new GMM model (dark gold lines, diamond markers)  %
		We plotted the markers at the same phase in each cycle of motion, to make it easier to understand how much the robot moved in a cycle. %
		Results demonstrate that while the Geometric model is noticeably better than the Phase model, the GMM model is noticeably superior to both.
		\label{figXtraj}
	}
\end{figure*}

To get a general sense of the difference between the three model types, we provided Fig.~\ref{figXtraj} which shows two examples of observed body centroid trajectories and the predicted motion based on the various models.

We obtained better insight into the predictive ability of these models with Fig.~\ref{figXpos2d}, which shows the motion after a full cycle -- either observed or predicted -- and also the prediction errors in comparison with the variability in the ground truth data.
We plotted this as a kernel smoothed density plot to give a (non-parametric) sense of how well these predicted distributions overlap.

\begin{figure*}[h]
  \centering
  \includegraphics[width=.4\textwidth]{/figs/stashed-PDFs/s=0.25_terminalDistribution}
  \hspace{3mm}
  \includegraphics[width=.4\textwidth]{/figs/stashed-PDFs/s=0.25_terminalErrorDistribution} \\
  \includegraphics[width=.4\textwidth]{/figs/stashed-PDFs/s=0.75_terminalDistribution}
  \hspace{3mm}
  \includegraphics[width=.4\textwidth]{/figs/stashed-PDFs/s=0.75_terminalErrorDistribution}\\
  \includegraphics[width=.4\textwidth]{/figs/stashed-PDFs/s=0.25_terminalThetaDistribution}
  \hspace{3mm}
  \includegraphics[width=.4\textwidth]{/figs/stashed-PDFs/s=0.75_terminalThetaDistribution}
  \caption{Distribution of final positions after a cycle of motion. %
  	We plot a kernel smoothed density estimate (KSDE) of the forward ($x$), sideways ($y$), and turn ($\theta$) motion produced by each stride (i.e. full gait cycle), and their error distributions. %
  	To provide a reference variance allowing the relative size of the modeling error to be understood by the reader, we plotted the ground truth data minus its mean in the error distributions. %
  	We plotted ground truth (green), the \myCite{bittner2018geometrically} gait-based geometric model prediction (orange), and the GMM model prediction (purple). %
  	We trained the models on intermediate turning data (``turn parameter'' $s=0.50$) and plotted the extrapolated results for a slower turn ($s=0.25$; $x$, $y$ top row, $\theta$ left) and a faster turn ($s=0.75$; $x$, $y$ middle row, $\theta$ right). %
  	For the $\theta$ distribution we plotted the observed values (vertical tics at the bottom of the plots), the distribution medians (thick vertical lines), and a scale bar for the width of the smoothing kernel (thick black line).\label{figXpos2d}
  }
\end{figure*}

The results show that the GMM error distribution is much closer to zero in every coordinate, and contains zero error close to its mode.
The variability in GMM model prediction is of a similar order to the variability in the data itself.
Because the BigANT robot was commanded to produce identical motions each cycle, this suggests the possibility that the model variability may be due to system noise that cannot be captured by the variables we measure, rather than modeling error.
\section{Discussion}

Our results show that using a GMM to model the motility map produced predictions that were substantially more accurate than the previous data-driven geometric mechanics modeling approaches cited.
We have shown this for an \emph{extrapolation} problem --- for motions substantially different from the training data (see Fig.~\ref{figXtraj} right), suggesting that these models can be useful for planning and for iterative gait optimization similar to that shown in \myCite{bittner2018geometrically, bittner2021optimizing}.

The method we used here has significant advantages beyond the improved performance: 
[1] it does not rely on phase estimation or on having nearly periodic data as an input, allowing any motion data available to be incorporated;
[2] it can handle hysteresis effects in its input data;
[3] it provides meta-parameters to control how much linear extrapolation it applies to the parts of the motility map that are expected to be linear.

Obviously, the results here are preliminary and much work remains.
Our modeling approach needs to be tested with a variety of datasets, including testing with data that contain some hysteresis effects, and data that are less linear in the $\dot r$ dependence of the motility map.
It would be natural to extend the models to the Shape Underactuated Dissipative Systems of \myCite{bittner2022data},
and use methods for online update of GMM-s to produce an online adaptive version of our method.
Another natural direction is to automate and improve the selection of meta-parameters, and to try other established manifold learning approaches for constructing models.

\textbf{Acknowledgments:} funding for this work was provided by NSF CPS 2038432; dataset was collected with funding from ARO MURI W911NF-17-1-0306 using equipment funded by ARO DURIP W911NF-17-1-0243.  

\printbibliography
\end{document}
