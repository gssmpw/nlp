@article{abcrown,
  author = {Huan Zhang},
  title = {$\alpha,\beta$-CROWN (alpha-beta-CROWN): A Fast and Scalable Neural Network Verifier with Efficient Bound Propagation},
  year = {2018},
  url={https://github.com/Verified-Intelligence/alpha-beta-CROWN},
  journal={GitHub}
}

@article{brix2023fourthinternationalverificationneural,
      title={The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results}, 
      author={Christopher Brix and Stanley Bak and Changliu Liu and Taylor T. Johnson},
      year={2023},
      doi={10.48550/arXiv.2312.16760},
      journal={arXiv preprint arXiv:2312.16760},

}

@article{duong2023dpll,
  title={A dpll (t) framework for verifying deep neural networks},
  author={Duong, Hai and Nguyen, ThanhVu and Dwyer, Matthew},
  journal={arXiv preprint arXiv:2307.10266},
  year={2023},
  doi={10.48550/arXiv.2307.10266}
}

@inproceedings{lundberg2017unifiedapproachinterpretingmodel,
author = {Lundberg, Scott M. and Lee, Su-In},
title = {A unified approach to interpreting model predictions},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4768â€“4777},
numpages = {10},
series = {NIPS'17},
doi = {10.5555/3295222.3295230}
}

@InProceedings{pmlr-v216-li23c,
  title = 	 {{G}aussian Process Surrogate Models for Neural Networks},
  author =       {Li, Michael Y. and Grant, Erin and Griffiths, Thomas L.},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1241--1252},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/li23c/li23c.pdf},
  url = 	 {https://proceedings.mlr.press/v216/li23c.html},
  abstract = 	 {Not being able to understand and predict the behavior of deep learning systems makes it hard to decide what architecture and algorithm to use for a given problem. In science and engineering, modeling is a methodology used to understand complex systems whose internal processes are opaque. Modeling replaces a complex system with a simpler, more interpretable surrogate. Drawing inspiration from this, we construct a class of surrogate models for neural networks using Gaussian processes. Rather than deriving kernels for infinite neural networks, we learn kernels empirically from the naturalistic behavior of finite neural networks. We demonstrate our approach captures existing phenomena related to the spectral bias of neural networks, and then show that our surrogate models can be used to solve practical problems such as identifying which points most influence the behavior of specific neural networks and predicting which architectures and algorithms will generalize well for specific datasets.}
}

@inproceedings{ribeiro2016should,
  title={"Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  doi={10.1145/2939672.2939778},
  year={2016}
}

@inproceedings{wu2024marabou,
  title={Marabou 2.0: a versatile formal analyzer of neural networks},
  author={Wu, Haoze and Isac, Omri and Zelji{\'c}, Aleksandar and Tagomori, Teruhiro and Daggitt, Matthew and Kokke, Wen and Refaeli, Idan and Amir, Guy and Julian, Kyle and Bassan, Shahaf and others},
  booktitle={International Conference on Computer Aided Verification},
  pages={249--264},
  year={2024},
  organization={Springer},
  doi={10.1007/978-3-031-65630-9_13}
}

@article{yang2018deepneuraldecisiontrees,
      title={Deep Neural Decision Trees}, 
      author={Yongxin Yang and Irene Garcia Morillo and Timothy M. Hospedales},
      year={2018},
      doi={10.48550/arXiv.1806.06988},
      journal={arXiv preprint arXiv:1806.06988},
}

@article{zhang2021survey,
  title={A survey on neural network interpretability},
  author={Zhang, Yu and Ti{\v{n}}o, Peter and Leonardis, Ale{\v{s}} and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={5},
  number={5},
  pages={726--742},
  year={2021},
  publisher={IEEE},
  doi={10.1109/TETCI.2021.3100641}
}

