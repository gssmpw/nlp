\section{Related Work}
Neural networks, while powerful, are inherently complex structures that can be regarded as multi-input multi-output (MIMO) black boxes. As such, interpreting them becomes very difficult. With their heavy deployment in a wide variety of safety-critical domains such as healthcare and autonomous navigation, it is becoming increasingly necessary to build trust and accountability in their use ____.

One approach is to leverage surrogate models such as decision trees ____ and Gaussian processes ____ to increase interpretability on a global level, or use sophisticated model-agnostic methods such as LIME ____ and SHAP ____ to generate local explanations for a given prediction. Another promising approach is neural network verification, which generates mathematical guarantees that a neural network respects its safety specifications, such as input-output bounds.

With the advent of friendly competitions such as International Verification of Neural Networks Competition (VNN-COMP) ____, the problem of safety verification of neural networks is becoming more standardized, and we are seeing a shift from theoretical approaches to practical, measurable efforts. This tool, much like current state-of-the-art such as Marabou ____, $\alpha,\beta$-crown ____ and NeuralSAT ____, is a step in this direction.