@inproceedings{FoMLAS2023:Supporting_Standardization_Neural_Networks,
  author    = {Stefano Demarchi and Dario Guidotti and Luca Pulina and Armando Tacchella},
  title     = {Supporting Standardization of Neural Networks Verification with VNNLIB and CoCoNet},
  booktitle = {Proceedings of the 6th Workshop on Formal Methods for ML-Enabled Autonomous Systems},
  editor    = {Nina Narodytska and Guy Amir and Guy Katz and Omri Isac},
  series    = {Kalpa Publications in Computing},
  volume    = {16},
  publisher = {EasyChair},
  bibsource = {EasyChair, https://easychair.org},
  issn      = {2515-1762},
  doi       = {10.29007/5pdh},
  pages     = {47-58},
  year      = {2023}}

@article{yang2018deepneuraldecisiontrees,
      title={Deep Neural Decision Trees}, 
      author={Yongxin Yang and Irene Garcia Morillo and Timothy M. Hospedales},
      year={2018},
      doi={10.48550/arXiv.1806.06988},
      journal={arXiv preprint arXiv:1806.06988},
}

@InProceedings{pmlr-v216-li23c,
  title = 	 {{G}aussian Process Surrogate Models for Neural Networks},
  author =       {Li, Michael Y. and Grant, Erin and Griffiths, Thomas L.},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1241--1252},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/li23c/li23c.pdf},
  url = 	 {https://proceedings.mlr.press/v216/li23c.html},
  abstract = 	 {Not being able to understand and predict the behavior of deep learning systems makes it hard to decide what architecture and algorithm to use for a given problem. In science and engineering, modeling is a methodology used to understand complex systems whose internal processes are opaque. Modeling replaces a complex system with a simpler, more interpretable surrogate. Drawing inspiration from this, we construct a class of surrogate models for neural networks using Gaussian processes. Rather than deriving kernels for infinite neural networks, we learn kernels empirically from the naturalistic behavior of finite neural networks. We demonstrate our approach captures existing phenomena related to the spectral bias of neural networks, and then show that our surrogate models can be used to solve practical problems such as identifying which points most influence the behavior of specific neural networks and predicting which architectures and algorithms will generalize well for specific datasets.}
}

@inproceedings{ribeiro2016should,
  title={"Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  doi={10.1145/2939672.2939778},
  year={2016}
}

@inproceedings{lundberg2017unifiedapproachinterpretingmodel,
author = {Lundberg, Scott M. and Lee, Su-In},
title = {A unified approach to interpreting model predictions},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4768â€“4777},
numpages = {10},
series = {NIPS'17},
doi = {10.5555/3295222.3295230}
}


@article{zhang2021survey,
  title={A survey on neural network interpretability},
  author={Zhang, Yu and Ti{\v{n}}o, Peter and Leonardis, Ale{\v{s}} and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={5},
  number={5},
  pages={726--742},
  year={2021},
  publisher={IEEE},
  doi={10.1109/TETCI.2021.3100641}
}

@article{brix2023fourthinternationalverificationneural,
      title={The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results}, 
      author={Christopher Brix and Stanley Bak and Changliu Liu and Taylor T. Johnson},
      year={2023},
      doi={10.48550/arXiv.2312.16760},
      journal={arXiv preprint arXiv:2312.16760},

}

@inproceedings{wu2024marabou,
  title={Marabou 2.0: a versatile formal analyzer of neural networks},
  author={Wu, Haoze and Isac, Omri and Zelji{\'c}, Aleksandar and Tagomori, Teruhiro and Daggitt, Matthew and Kokke, Wen and Refaeli, Idan and Amir, Guy and Julian, Kyle and Bassan, Shahaf and others},
  booktitle={International Conference on Computer Aided Verification},
  pages={249--264},
  year={2024},
  organization={Springer},
  doi={10.1007/978-3-031-65630-9_13}
}

@article{abcrown,
  author = {Huan Zhang},
  title = {$\alpha,\beta$-CROWN (alpha-beta-CROWN): A Fast and Scalable Neural Network Verifier with Efficient Bound Propagation},
  year = {2018},
  url={https://github.com/Verified-Intelligence/alpha-beta-CROWN},
  journal={GitHub}
}

@article{BoxRL-NNV,
  author = {Sarthak Das},
  title = {\texttt{BoxRL-NNV}: Boxed Refinement of Latin Hypercube Samples for Neural Network Verification},
  year = {2025},
  url={https://github.com/dassarthak18/R2X-NNV/tree/BoxRL-NNV},
  journal={GitHub}
}

@article{duong2023dpll,
  title={A dpll (t) framework for verifying deep neural networks},
  author={Duong, Hai and Nguyen, ThanhVu and Dwyer, Matthew},
  journal={arXiv preprint arXiv:2307.10266},
  year={2023},
  doi={10.48550/arXiv.2307.10266}
}

@article{ef76b040-2f28-37ba-b0c4-02ed99573416,
 ISSN = {00401706},
 abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
 author = {M. D. McKay and R. J. Beckman and W. J. Conover},
 journal = {Technometrics},
 number = {2},
 pages = {239--245},
 publisher = {[Taylor & Francis Ltd., American Statistical Association, American Society for Quality]},
 title = {A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code},
 volume = {21},
 year = {1979},
 doi={10.2307/1271432}
}

@article{doi:10.1137/0916069,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
title = {A Limited Memory Algorithm for Bound Constrained Optimization},
journal = {SIAM Journal on Scientific Computing},
volume = {16},
number = {5},
pages = {1190-1208},
year = {1995},
doi = {10.1137/0916069},
abstract = { An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function. It is shown how to take advantage of the form of the limited memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported. }
}

@InProceedings{10.1007/978-3-540-78800-3_24,
author="de Moura, Leonardo
and Bj{\o}rner, Nikolaj",
editor="Ramakrishnan, C. R.
and Rehof, Jakob",
title="Z3: An Efficient SMT Solver",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="337--340",
abstract="Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.",
isbn="978-3-540-78800-3",
doi="10.1007/978-3-540-78800-3_24"
}

@TECHREPORT{BarFT-RR-17,
  author =	 {Clark Barrett and Pascal Fontaine and Cesare Tinelli},
  title =	 {{The SMT-LIB Standard: Version 2.6}},
  institution =	 {Department of Computer Science, The University of Iowa},
  year =	 2017,
  url ={www.SMT-LIB.org}}

@article{Biere2003BoundedMC,
  title     = {Bounded Model Checking},
  author    = {Armin Biere and Alessandro Cimatti and Edmund M. Clarke and Ofer Strichman and Yunshan Zhu},
  journal   = {Advances in Computers},
  volume    = {58},
  pages     = {117--148},
  year      = {2003},
  doi       = {10.1016/S0065-2458(03)58003-2}
}

@article{DEUTSCH2012763,
title = {Latin hypercube sampling with multidimensional uniformity},
journal = {Journal of Statistical Planning and Inference},
volume = {142},
number = {3},
pages = {763-772},
year = {2012},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2011.09.016},
author = {Jared L. Deutsch and Clayton V. Deutsch},
keywords = {Monte Carlo simulation, Maximin, Correlated variables, Simulation},
abstract = {Complex models can only be realized a limited number of times due to large computational requirements. Methods exist for generating input parameters for model realizations including Monte Carlo simulation (MCS) and Latin hypercube sampling (LHS). Recent algorithms such as maximinLHS seek to maximize the minimum distance between model inputs in the multivariate space. A novel extension of Latin hypercube sampling (LHSMDU) for multivariate models is developed here that increases the multidimensional uniformity of the input parameters through sequential realization elimination. Correlations are considered in the LHSMDU sampling matrix using a Cholesky decomposition of the correlation matrix. Computer code implementing the proposed algorithm supplements this article. A simulation study comparing MCS, LHS, maximinLHS and LHSMDU demonstrates that increased multidimensional uniformity can significantly improve realization efficiency and that LHSMDU is effective for large multivariate problems.}
}

@InProceedings{10.1007/978-3-319-63387-9_5,
author="Katz, Guy
and Barrett, Clark
and Dill, David L.
and Julian, Kyle
and Kochenderfer, Mykel J.",
editor="Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
title="Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
booktitle="Computer Aided Verification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="97--117",
abstract="Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.",
isbn="978-3-319-63387-9"
}

