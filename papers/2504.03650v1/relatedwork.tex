\section{Related Work}
Neural networks, while powerful, are inherently complex structures that can be regarded as multi-input multi-output (MIMO) black boxes. As such, interpreting them becomes very difficult. With their heavy deployment in a wide variety of safety-critical domains such as healthcare and autonomous navigation, it is becoming increasingly necessary to build trust and accountability in their use \cite{zhang2021survey}.

One approach is to leverage surrogate models such as decision trees \cite{yang2018deepneuraldecisiontrees} and Gaussian processes \cite{pmlr-v216-li23c} to increase interpretability on a global level, or use sophisticated model-agnostic methods such as LIME \cite{ribeiro2016should} and SHAP \cite{lundberg2017unifiedapproachinterpretingmodel} to generate local explanations for a given prediction. Another promising approach is neural network verification, which generates mathematical guarantees that a neural network respects its safety specifications, such as input-output bounds.

With the advent of friendly competitions such as International Verification of Neural Networks Competition (VNN-COMP) \cite{brix2023fourthinternationalverificationneural}, the problem of safety verification of neural networks is becoming more standardized, and we are seeing a shift from theoretical approaches to practical, measurable efforts. This tool, much like current state-of-the-art such as Marabou \cite{wu2024marabou}, $\alpha,\beta$-crown \cite{abcrown} and NeuralSAT \cite{duong2023dpll}, is a step in this direction.