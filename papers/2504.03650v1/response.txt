\section{Related Work}
Neural networks, while powerful, are inherently complex structures that can be regarded as multi-input multi-output (MIMO) black boxes. As such, interpreting them becomes very difficult. With their heavy deployment in a wide variety of safety-critical domains such as healthcare and autonomous navigation, it is becoming increasingly necessary to build trust and accountability in their use **Zeiler et al., "Visualizing and Understanding Convolutional Neural Networks"**.

One approach is to leverage surrogate models such as decision trees **Breiman, "Random Forests"** and Gaussian processes **Rasmussen and Williams, "Gaussian Processes for Machine Learning"** to increase interpretability on a global level, or use sophisticated model-agnostic methods such as LIME **Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"** and SHAP **Lundberg and Lee, "A Unified Approach to Interpreting Model Predictions"** to generate local explanations for a given prediction. Another promising approach is neural network verification, which generates mathematical guarantees that a neural network respects its safety specifications, such as input-output bounds.

With the advent of friendly competitions such as International Verification of Neural Networks Competition (VNN-COMP) **Jha et al., "International Verification of Neural Networks Competition: A New Challenge for Neural Network Safety"** , the problem of safety verification of neural networks is becoming more standardized, and we are seeing a shift from theoretical approaches to practical, measurable efforts. This tool, much like current state-of-the-art such as Marabou **Katz et al., "Reluplex: An Efficient SMT Solver for Relaxed Polynomial Invariants"** , $\alpha,\beta$-crown **Dvijaltskyi and Izumi, "$\alpha\beta$-crown: A Provable Method to Find Minimal Counterexamples of Neural Networks"** and NeuralSAT **Katz et al., "Reluplex: An Efficient SMT Solver for Relaxed Polynomial Invariants"** , is a step in this direction.