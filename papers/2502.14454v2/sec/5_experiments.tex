\section{Experiments}
\label{sec:experiments}
\paragraph{Datasets}
The deblurring networks of \MethodName{} are trained with the train set of \SynthDataName{}.
For evaluating \MethodName{} and other radiance field deblurring methods, we use the test set of \SynthDataName{}, the real-world dataset from Deblur-NeRF~\cite{ma2022deblurnerf}, and the \RealDataName{} dataset.
The test set of \SynthDataName{} and the real-world dataset from Deblur-NeRF each contain 10 scenes per blur type, while the \RealDataName{} dataset has 5 scenes with camera motion blur.
The real scenes of Deblur-NeRF were captured with a Canon EOS RP under the manual exposure mode.
The camera poses are estimated using COLMAP~\cite{schoenberger2016sfm}.


\paragraph{Implementation details}
\label{Implementation_Details}
We set the number of iterations $N$ to 5.
Thus, as detailed in \cref{sec:deepdeblurrf}, \MethodName{} includes one single-image deblurring network and four RF-guided deblurring networks for each blur type: camera motion blur and defocus blur.
Note that our approach requires an additional training phase for the deblurring networks, unlike previous radiance field deblurring methods.
Once trained, our framework can construct radiance fields for arbitrary scenes and render novel views, similar to conventional approaches.
To train the deblurring networks, we use the Adam optimizer~\cite{kingma2014adam} with $\beta_1 = 0.9$ and $\beta_2 = 0.9$. 
The learning rate is initially set to $10^{-3}$ and gradually reduced to $10^{-7}$ using the cosine annealing scheduler~\cite{loshchilov2016sgdr}.

For constructing radiance fields in \MethodName{}-P, we follow the forward-facing experimental setup in Plenoxels, applying pruning and upsampling to the voxel grids every 38,400 iterations. 
For \MethodName{}-G, we adopt the original Gaussian Splatting method, incorporating the depth-based pruning and additional points strategy from Deblurring-3DGS~\cite{lee2024deblurring} to address the sparse point clouds obtained from COLMAP on blurred images.

We measured the computation time for radiance field deblurring using the camera motion blur test set of \SynthDataName{} on a single NVIDIA TITAN RTX, except for DP-NeRF, which requires two GPUs due to its memory demands.
Additional implementation details, including network configurations and the training settings for each iteration of \MethodName{}-P and \MethodName{}-G, are provided in the supplementary material.

\paragraph{Comparison using BlurRF-Synth}
We compare \MethodName{} with other radiance field deblurring methods ~\cite{ma2022deblurnerf, wang2023badnerf, lee2023dpnerf, lee2023exblurf, peng2023pdrf, lee2024deblurring, peng2024bags}.
\cref{table:quantitative_comparison} shows the quantitative results of \MethodName{} and the other methods on the test set of \SynthDataName{}. 
The table demonstrates that our approach outperforms the existing methods in terms of quality, regardless of the radiance field representation. 
Moreover, it achieves substantially shorter computation times. 
Specifically, \MethodName{}-P shows the shortest computation time among the approaches based on MLPs and voxel grids, while \MethodName{}-G is the fastest among all the methods. 
This superior performance can be attributed to its DNN-based deblurring modules, which handle blur efficiently and effectively, in contrast to the other methods constrained by linear blur models.

The superior performance of our approach is further demonstrated in \cref{fig:qualitative_result1}, which shows that our method achieves sharper results on the \SynthDataName{} dataset.
Specifically, on the camera motion blur scene, all the other methods produce noisy results, whereas both \MethodName{}-P and \MethodName{}-G effectively remove the blur even in challenging conditions with noise and pixel saturation, achieving high-quality novel-view synthesis.
Similarly, in the defocus blur scene, our methods render sharper novel views by minimizing residual blur than the other methods.

\paragraph{Comparison on real datasets}
We compare the novel-view synthesis performance of \MethodName{} against other approaches using both \RealDataName{} and the real scenes from Deblur-NeRF~\cite{ma2022deblurnerf}.
As detailed in \cref{subsec:BlurRF-real}, \RealDataName{} provides an evaluation of how well each method handles camera motion blur with non-linear outliers and noise in real-world low-light conditions.
As shown in \cref{table:BlurRF-SB} and \cref{fig:qualitative_result2}, our method not only outperforms the others in non-reference metrics~\cite{mittal2012completelyblind, agnolucci2024arniqa} but also achieves high-quality qualitative results, clearly surpassing the competing methods.
Even in Deblur-NeRF's real-world scenes, which have minimal noise, our method demonstrates superior performance.
%It is worth underlining that although these scenes contain minimal noise, which differs from the assumptions of our training dataset, our method remains robust and effective.
Specifically, while other methods struggle with saturated pixels in the camera motion blur scene, our method effectively handles it.
Likewise, in the defocus blur scene, our method renders sharper novel views without residual blur.

While the real-world scenes of Deblur-NeRF provide blur-free reference images, they suffer from misalignment and exposure differences~\cite{ma2022deblurnerf, peng2023pdrf, peng2024bags}.
Similarly, \RealDataName{} also includes blur-free reference images, but they have not only similar limitations but also severe noise as they are captured under severe low-light conditions.
Thus, we report quantitative evaluations against the reference images in the supplementary material.
More experimental results are also provided in the supplementary material.



\begin{table}[t]
\centering
\scalebox{0.65}{
    \begin{tabular}{l|cc|ccc}
        \Xhline{4\arrayrulewidth}
        \multirow{2}{*}{Model} & \multicolumn{2}{c}{\RealDataName{}} & \multicolumn{3}{|c}{\SBDataName{}} \\
                               & NIQE ($\downarrow$) & ARNIQA ($\uparrow$) & PSNR ($\uparrow$) & SSIM ($\uparrow$) & LPIPS ($\downarrow$) \\ \hline\hline
        \multicolumn{6}{c}{MLP} \\ \hline 
        Deblur-NeRF~\cite{ma2022deblurnerf} & 6.341 & 0.279 & 25.43 & 0.7264 & 0.2319 \\
        BAD-NeRF~\cite{wang2023badnerf}     & 9.908 & 0.237 & 21.97 & 0.5231 & 0.4731 \\
        DP-NeRF~\cite{lee2023dpnerf}        & 6.498 & 0.266 & 27.07 & 0.7940 & 0.1795 \\ \hline
        \multicolumn{6}{c}{Voxel grid} \\ \hline 
        ExBluRF~\cite{lee2023exblurf}       & 7.479 & 0.279 & 24.16 & 0.6649 & 0.2659 \\
        PDRF-10~\cite{peng2023pdrf}         & 6.243 & 0.266 & 26.66 & 0.7730 & 0.2124 \\
        \rowcolor{skyblue}
        \cellcolor{white}\textbf{\MethodName{}-P}                              & 5.829 & 0.323 & 29.45 & 0.8472 & 0.1677 \\ \hline
        \multicolumn{6}{c}{3D Gaussians} \\ \hline 
        Deblurring-3DGS~\cite{lee2024deblurring}  & 6.086 & 0.262 & 23.87 & 0.6459 & 0.2475 \\
        BAGS~\cite{peng2024bags}            & 5.837 & 0.290 & 24.49 & 0.6748 & 0.2391 \\
        \rowcolor{red_blurry}
        \cellcolor{white}\textbf{\MethodName{}-G}                              & 5.423 & 0.329 & 29.59 & 0.8548 & 0.1399 \\ \Xhline{4\arrayrulewidth}
    \end{tabular}
    }
    \caption{Quantitative results of novel-view synthesis on \RealDataName{} and \SBDataName{} datasets. We highlight \colorbox{red_blurry}{the best metrics} and \colorbox{skyblue}{the second best metrics}.}
    \vspace{-3mm}
    \label{table:BlurRF-SB}
\end{table}

\paragraph{Input views with same blur directions}
As discussed in \cref{sec:intro}, unlike previous approaches, our method does not rely on the assumption that input blurred images contain complementary information from different blur directions.
To verify this, we generated an additional synthetic test set, named \SBDataName{}, where the blurred images of the same scene share the same blur directions, while their blur magnitudes are different.
\SBDataName{} was generated using five scenes from the \SynthDataName{} test set, following the same generation process.

\cref{fig:same_blur} shows a qualitative comparison on \SBDataName{}.
Due to the lack of complementary information in different training views, the other methods fail to restore sharp details.
Conversely, our methods successfully restore sharp details.
The quantitative comparison in \cref{table:BlurRF-SB} also demonstrates that our approach significantly outperforms the others for blurred images with the same blur directions.
These results verify the benefit of using prior knowledge on sharp images from pre-trained deblurring networks.


\begin{table}[t]
\centering
\scalebox{0.85}{
        \begin{tabular}{c|ccc|ccc}
            \Xhline{4\arrayrulewidth}
            \multirow{2}{*}{\# Iter.} & \multicolumn{3}{c|}{Camera motion} & \multicolumn{3}{c}{Defocus}  \\
                    & PSNR  & SSIM   & LPIPS  & PSNR  & SSIM   & LPIPS \\ \hline\hline
            $N = 1$ & 28.40 & 0.8175 & 0.1707 & 30.74 & 0.8862 & 0.1297 \\ 
            $N = 2$ & 29.23 & 0.8462 & 0.1281 & 31.22 & 0.8905 & 0.0955 \\ 
            $N = 3$ & 29.65 & 0.8588 & 0.1152 & 32.12 & 0.9003 & 0.0899 \\ 
            $N = 4$ & 29.83 & 0.8632 & 0.1108 & 32.30 & 0.9038 & 0.0860 \\
            $N = 5$ & 29.94 & 0.8681 & 0.1059 & 32.58 & 0.9060 & 0.0774 \\ 
            $N = 6$ & 29.97 & 0.8692 & 0.1034 & 32.75 & 0.9087 & 0.0736  \\ \Xhline{4\arrayrulewidth}
        \end{tabular}
    }
    \caption{Quantitative results of ablation study on the number of iterations $N$ using \MethodName{}-G on the test sets of \SynthDataName{}.}
    \label{table:ablation_cycle}
    \vspace{-3mm}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/fig_ablation.pdf}
    \caption{Qualitative results of ablation study on the number of iterations $N$ using \MethodName{}-G on the real-world scenes~\cite{ma2022deblurnerf}.
    }
    \vspace{-3mm}
    \label{fig:ablation_cycle}
\end{figure}


\paragraph{Number of iterations}
We investigate the impact of the hyperparameter $N$, which sets the number of iterations in our framework.
\cref{table:ablation_cycle} shows that \MethodName{}-G achieves improved novel-view synthesis as $N$ increases, highlighting the effectiveness of our iterative approach.
However, gains diminish beyond $N = 5$, suggesting that $N = 5$ is sufficient.
\cref{fig:ablation_cycle} further supports this with qualitative results on real scenes~\cite{ma2022deblurnerf} (camera motion blur and defocus blur), showing similar trends.
Similar results were observed for \MethodName{}-P, as detailed in the supplementary material.

