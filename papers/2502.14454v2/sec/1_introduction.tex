\section{Introduction}
\label{sec:intro}

\begin{figure*}[t!]
\centering
    \includegraphics[width=0.98\textwidth]{figure/fig_teaser.pdf}
    \caption{Given a set of multi-view blurry images even with non-linear outliers such as saturated pixels and noise, \MethodName{} performs high-quality novel-view synthesis with highly efficient training. \MethodName{}-P and \MethodName{}-G are the results of our framework, where radiance fields are constructed using Plenoxels~\cite{fridovich2022plenoxels} and 3D Gaussian Splatting~\cite{kerbl2023gaussiansplatting}, respectively. Note that DP-NeRF~\cite{lee2023dpnerf} was trained with two GPUs due to its memory demands, whereas other models were trained on a single NVIDIA TITAN RTX GPU.
    }
    \vspace{-3mm}
    \label{fig:teaser}
\end{figure*}

Novel-view synthesis has seen significant advancements in recent years, leading to impressive improvements in photo-realistic rendering quality. One key development in this field is the Neural Radiance Field (NeRF)~\cite{mildenhall2020nerf}, which leverages neural networks to generate highly detailed images from novel viewpoints. Building on this, various works have focused on reducing training time and enhancing rendering quality~\cite{mueller2022instantngp, chen2022tensorf, hu2023trimiprf, fridovich2022plenoxels}. More recently, 3D Gaussian Splatting (3DGS)~\cite{kerbl2023gaussiansplatting} has emerged, utilizing Gaussians as an explicit 3D representation. This technique enables high-quality scene reconstruction and real-time rendering through a differentiable rasterization method.

However, synthesizing sharp novel views from degraded training views remains a significant challenge. Capturing images in real-world conditions often leads to various degradations such as blur and noise.
Among them, blur makes it difficult to aggregate accurate 3D information from the training views, resulting in a blurry radiance field.
To address blur, several works~\cite{ma2022deblurnerf, wang2023badnerf, lee2023dpnerf, peng2023pdrf, lee2023exblurf, lee2024deblurring, peng2024bags} have been proposed to reconstruct sharp radiance fields from training views with either camera motion blur or defocus blur.
These radiance field deblurring methods jointly optimize blur kernels and the radiance field, and have shown promising results.

However, these methods still possess several limitations that hinder their effectiveness.
First, existing approaches rely on linear blur models, describing blurred pixels in training views as linear combinations of sharp pixels.
In contrast, real-world blurred images often suffer from non-linear outliers such as saturated pixels and noise, and non-linear in-camera processing~\cite{cho2011handling_original,rim_2022_ECCV_original}, which can severely degrade the performance of current radiance field deblurring techniques.
Second, existing approaches do not utilize priors on sharp images, relying solely on complementary information from different views to model blur kernels and estimate a sharp radiance field. Consequently, they tend to produce radiance fields with residual blur and may completely fail when all input views exhibit similar blur directions~\cite{ma2022deblurnerf}, akin to classical multi-frame deblurring approaches~\cite{chen1996image,rav2005two,cho2007removing}, due to the lack of sharp image priors.
Third, existing ray-based methods~\cite{ma2022deblurnerf, wang2023badnerf, lee2023dpnerf, peng2023pdrf, lee2023exblurf} employ multiple ray samples per pixel to depict a blurry image, resulting in a substantial increase in rendering time for a single view from a radiance field, and consequently, a significant computation time for radiance field construction.

Apart from the radiance field deblurring methods, image deblurring has been extensively studied for decades~\cite{cho2009fast, hirsch2011fast, zhang2013non, pan2014deblurring, whyte10nonuniform}.
Recently, a number of deep neural network (DNN)-based single-image deblurring methods have been proposed~\cite{nah2017gopro, tao2018srn, kupyn2018deblurgan, zamir2021mprnet, cho2021mimounet, chen2022nafnet}.
These methods do not rely on linear blur models, but learn image deblurring from large-scale datasets.
Thanks to this, they can effectively remove blur from a single image without the need for complementary information from other images, and also handle non-linearities such as saturated pixels~\cite{rim2020realblur}.
Moreover, their feed-forward approach allows markedly reduced processing times, unlike classical iterative optimization-based approaches.

Given the effectiveness of recent DNN-based deblurring approaches, a promising direction to construct a sharp radiance field from blurry images would be to combine a deblurring network and radiance field construction.
However, a na\"ive combination of a deblurring network and radiance field construction, which performs single-image deblurring to each input blurry image and trains a radiance field using the deblurred images, results in unsatisfactory results as reported by Ma \etal~\cite{ma2022deblurnerf}.
This is because of the limited performance of single-image deblurring, which stems from the insufficient information available in a single image.

In this paper, we propose \textit{\MethodName{}}, a novel radiance field deblurring framework that enables highly efficient training and high-quality novel-view synthesis from images blurred by motion blur or defocus blur, even in the presence of noise and saturated pixels, as shown in \cref{fig:teaser}. Unlike existing radiance field deblurring approaches, our method leverages DNN-based deblurring modules to achieve superior performance and computational efficiency. Additionally, our approach is versatile, capable of handling various 3D representations such as voxel grids~\cite{fridovich2022plenoxels} and 3D Gaussians~\cite{kerbl2023gaussiansplatting}.

However, solely relying on deblurring networks to remove blur from input blurry images may yield unsatisfactory results as discussed above.
To effectively combine DNN-based deblurring and radiance field construction, our framework adopts a novel radiance field (RF)-guided deblurring scheme, and an iterative framework that performs RF-guided deblurring and radiance field construction in an alternating manner.
Specifically, our framework first deblurs input images, and then trains a radiance field using the deblurred images.
Then, at the next iteration, it renders images corresponding to the input views from the trained radiance field.
While the deblurred images may have limited qualities with inaccurately restored details, the rendered images provide higher-quality details as they are rendered from aggregated information from multiple deblurred images.
Using the rendered images as guidance, our framework then performs RF-guided deblurring to the input blurred images.
Thanks to the guidance, we can obtain higher-quality deblurred images, which we use for radiance field construction again.
Finally, our framework iterates RF-guided deblurring and radiance field construction to gradually enhance the quality of the radiance field.

Training the deblurring modules of our framework requires a large-scale dataset of blurred images paired with ground-truth sharp images for radiance field reconstruction.
However, no such datasets are available as existing radiance field deblurring approaches adopt linear blur model-based approaches, which do not require large-scale training datasets~\cite{ma2022deblurnerf, wang2023badnerf, lee2023dpnerf, lee2023exblurf, peng2023pdrf, lee2024deblurring, peng2024bags}.
Consequently, the datasets provided by previous works, such as those from Deblur-NeRF~\cite{ma2022deblurnerf} and ExBluRF~\cite{lee2023exblurf}, are small and primarily designed for evaluation rather than training. Furthermore, these datasets often overlook crucial factors like camera noise, in-camera processing, and other non-linear outliers, which are essential for realistic deblurring tasks.

To address these gaps, we propose \textit{\SynthDataName{}}, a large-scale dataset for radiance field deblurring approaches. It includes 4,350 blurred-sharp image pairs across 150 scenes, with 2,175 pairs for each of 75 scenes, encompassing both camera motion blur and defocus blur. These images are carefully synthesized to reflect real-world camera degradations such as noise, saturated pixels, and in-camera processing pipelines.
Additionally, we introduce a real-world dataset called \textit{\RealDataName{}} for evaluation in non-ideal conditions. Unlike the Deblur-NeRF dataset, which has less noise and adequate lighting, \textit{\RealDataName{}} consists of five low-light indoor scenes captured with a machine vision camera.

We validate \textit{\MethodName{}} on both synthetic and real-world datasets with two types of blur: camera motion blur and defocus blur.
Experimental results demonstrate that our method achieves state-of-the-art novel-view synthesis performance with highly reduced processing times.
Furthermore, we demonstrate the extensibility of our framework by constructing radiance fields using different scene representations, such as voxel grids and Gaussian Splatting.
Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose \textit{\MethodName{}}, a novel radiance field deblurring framework, which is the first approach that leverages DNN-based deblurring modules to overcome the limitations of the linear blur model.
    \item To this end, we present RF-guided deblurring, and an iterative framework that performs RF-guided deblurring and radiance field construction in an alternating manner to gradually enhance the quality of the radiance field.
    \item We also present the \textit{\SynthDataName{}} dataset, the first large-scale dataset for training and evaluation of novel-view synthesis from blurry images.
\end{itemize}
