\section{BlurRF Dataset}
\label{sec:dataset}

\subsection{\SynthDataName{}}
We propose the \SynthDataName{} dataset, the first large-scale dataset for training and evaluating novel-view synthesis from blurred images.
The dataset comprises a train set and a test set for each type of blur: camera motion blur and defocus blur.
The train set includes 65 scenes, each containing 29 pairs of synthetically blurred images and their corresponding sharp images, captured from different viewpoints. 
The test set includes 10 scenes, each with 29 blurred-sharp image pairs and five additional sharp images from different viewpoints for evaluating novel-view synthesis quality.
Examples of blurred images and ground-truth sharp images are shown in \cref{fig:dataset}.
We synthetically generated the dataset using Blender models, collecting 95 models from Blendswap\footnote{\url{https://blendswap.com/}} under Creative Commons licenses and 5 models from the synthetic dataset of Deblur-NeRF~\cite{ma2022deblurnerf}.
For each model, we sampled 29 camera poses to capture different viewpoints.

\paragraph{Camera motion blur}
We simulated camera shakes by randomly sampling a camera trajectory for each pose during the exposure duration. 
To this end, we adopted B\'ezier interpolation to model 6-DOF camera motion and densely sampled 51 intermediate poses along this camera trajectory.
We then rendered the scene using Blender at each intermediate camera pose to produce a series of sharp images, which we then averaged to create a blurred image.
All rendering and averaging were performed in the linear sRGB color space to accurately replicate the real-world image formation process. 
Among the sharp images, we sampled the temporally central image (i.e., the 26th image) as the ground-truth sharp image.  

\paragraph{Defocus blur}
We also generated images with defocus blur using Blender.
We adjusted the cameraâ€™s depth-of-field (DoF) in Blender to create defocus blur.
We controlled the aperture size and randomly set the blade count between 7 and 9 to simulate realistic defocus effects.
For each camera pose, we sampled a random focal distance within a predefined range based on the scene scale, shifting the focal plane to introduce variation in the defocus blur.

\begin{table*}[t]
\centering
\scalebox{0.9}{
        \begin{tabular}{c|l|ccc|ccc|c}
            \Xhline{4\arrayrulewidth}
            \multirow{2}{*}{3D Representation} & \multirow{2}{*}{Model} & \multicolumn{3}{c|}{Camera Motion Blur} & \multicolumn{3}{c|}{Defocus Blur} & Computation \\ %\multirow{2}{*}{Time (Hr.)}  \\
                                                & & PSNR ($\uparrow$) & SSIM ($\uparrow$) & LPIPS ($\downarrow$) & PSNR ($\uparrow$) & SSIM ($\uparrow$) & LPIPS ($\downarrow$) & Time (Hr.) \\ \hline\hline 
             & Deblur-NeRF~\cite{ma2022deblurnerf} & 27.67 & 0.8340 & 0.1450 & 30.03 & 0.8727 & 0.1137 & 31.33      \\ 
            MLP & BAD-NeRF~\cite{wang2023badnerf}     & 21.74 & 0.5298 & 0.3969 & -     & -      & -      & 24.68      \\ 
             & DP-NeRF~\cite{lee2023dpnerf}        & 28.03 & 0.8412 & 0.1267 & 30.15 & 0.8763 & 0.0991 & 30.00*     \\ \hline
             & ExBluRF~\cite{lee2023exblurf}       & 26.56 & 0.7823 & 0.1955 & -     & -      & -      & 8.92       \\
            Voxel grid & PDRF-10~\cite{peng2023pdrf}         & 28.33 & 0.8435 & 0.1495 & 30.03 & 0.8750 & 0.1225 & 4.26       \\ 
            \rowcolor{skyblue}
             \cellcolor{white} & \cellcolor{white}\textbf{\MethodName{}-P}    & 29.81 & 0.8668 & 0.1142 & 32.51 & 0.9058 & 0.0961 & 1.14\cellcolor{white}              \\ \hline
             & Deblurring-3DGS~\cite{lee2024deblurring}   & 26.30 & 0.7729 & 0.1728 & 29.37 & 0.8545 & 0.1470 & 0.33\cellcolor{skyblue}       \\ 
            3D Gaussians & BAGS~\cite{peng2024bags}            & 27.41 & 0.8108 & 0.1382 & 29.90 & 0.8638 & 0.1152 & 1.25       \\  
            \rowcolor{red_blurry}
            \cellcolor{white} & \cellcolor{white}\textbf{\MethodName{}-G}    & 29.94 & 0.8681 & 0.1059 & 32.58 & 0.9060 & 0.0774 & 0.28       \\ \Xhline{4\arrayrulewidth}
        \end{tabular}
    }
    \caption{Quantitative results of novel-view synthesis on \SynthDataName{} test scenes. We highlight \colorbox{red_blurry}{the best metrics} and \colorbox{skyblue}{the second best metrics}. Note that the computation time of DP-NeRF~\cite{lee2023dpnerf} was measured with two GPUs due to its memory demands, whereas those of the other models were measured on a single GPU.}
    %\vspace{-3mm}
    \label{table:quantitative_comparison}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/fig_qual_feedback_synth_2.pdf}
    \caption{Qualitative results of novel-view synthesis on \SynthDataName{} test scenes.
    }
    \label{fig:qualitative_result1}
\end{figure*}

\paragraph{Realistic blur synthesis}
To reflect real-world image degradation, we adopted the blur synthesis pipeline of RSBlur~\cite{rim_2022_ECCV_original}. 
Specifically, we generated camera motion blur by averaging sharp images in the linear sRGB space.
Then, we synthesized saturated pixels to the blurred images and converted them to the camera RAW space.
In the camera RAW space, we synthesized shot and read noise, and converted the images back to the camera sRGB space.
For defocus blur, we rendered blurred images in the linear sRGB space and added noise in the same manner.
For a more detailed pipeline, we refer the readers to \cite{rim_2022_ECCV_original}.
The blur synthesis pipeline requires camera-specific noise parameters and a color correction matrix.
We used a Sony A7R3 camera to estimate them.
For more details about the \SynthDataName{} dataset, we refer the readers to the supplementary material.

\subsection{\RealDataName{}}
\label{subsec:BlurRF-real}
For evaluation under more realistic and challenging conditions, we propose \RealDataName{}, a real-world low-light camera motion blur dataset. The existing real datasets~\cite{ma2022deblurnerf,lee2023exblurf}  are captured in well-lit environments with minimal noise, making them less representative of camera motion blur in low-light conditions. Our dataset addresses this gap by providing data captured under challenging low-light scenarios. 
We collected five indoor scenes using a machine vision camera, each providing 20-40 multi-view blurry images, including 3-5 images for novel-view synthesis evaluation.
Examples of blurred training views are shown in \cref{fig:dataset}.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/fig_qual_feedback_real_2.pdf}
    \caption{Qualitative results of novel-view synthesis on real-world datasets.
    }
    \label{fig:qualitative_result2}
    \vspace{-2mm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/fig_qual_feedback_SB.pdf}
    \caption{Qualitative results of novel-view synthesis on \SBDataName{} dataset.
    }
    \label{fig:same_blur}
    \vspace{-2mm}
\end{figure*}