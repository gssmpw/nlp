\begin{figure*}
    \centering
    \includegraphics[width=0.98\textwidth]{figure/fig_overview.pdf}
    \caption{Overall framework and intermediate result of each step of \MethodName{}.}
    \label{fig:overview}
    \vspace{-3mm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{figure/fig_cycle.pdf}
    \caption{As iterations progress, the rendered images contain increasingly high-quality scene information, which subsequently improves the RF-guided deblurring network's performance in the next iteration.}
    \label{fig:render_iterations}
    \vspace{-3mm}
\end{figure*}

\section{\MethodName{}}
\label{sec:deepdeblurrf}
\cref{fig:overview}-(a) illustrates the overall framework of \MethodName{}.
It takes a set of $M$ blurred images $\textbf{B} = \{B_1, ..., B_M\}$ of a scene and estimates a sharp radiance field that can synthesize a sharp novel view given an arbitrary pose.
We assume that each blurred image $B_m$ is obtained from its latent sharp image $L_m$ through an unknown degradation process including blur and non-linear outliers such as saturated pixels and nonlinear in-camera processing.
Based on this assumption, \MethodName{} first performs initial deblurring to the input blurred images, and obtains initial deblurred images. % $D_m^0$.
Then, our method iteratively performs radiance field construction using deblurred images and RF-guided deblurring to gradually enhance the quality of the radiance field and the deblurred images.
At the last iteration, we perform only the radiance field construction step and obtain a final radiance field, from which we can synthesize sharp novel views.
In the following, each step of \MethodName{} is described in more detail.

\subsection{Initial Deblurring}
The initial deblurring step removes blur from each blurry training view $B_m \in \textbf{B}$ so that the following radiance field construction step can estimate the pose of each view more accurately, and more effectively aggregate information from different views.
To this end, we adopt an off-the-shelf single-image deblurring network.
Specifically, we adopt NAFNet~\cite{chen2022nafnet}, a state-of-the-art deblurring network, for its computational efficiency and performance.
We denote the deblurred image from the initial deblurring step corresponding to $B_m$ as $D_m^0$, and the set of the deblurred images as $\textbf{D}^0 = \{D_1^0, ..., D_M^0\}$.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{figure/fig_dataset.pdf}
    \caption{Examples of the \SynthDataName{} and \RealDataName{} datasets. The examples show blurred views in \RealDataName{}, while the top and bottom rows in \SynthDataName{} include blurred views and their corresponding sharp views.}
    %\vspace{-3mm}
    \label{fig:dataset}
\end{figure*}


\subsection{Radiance Field Construction}
In the radiance field construction step at the $i$-th iteration, we first estimate the camera poses of the deblurred images from the previous step, denoted as $\textbf{D}^{i-1} = \{D_1^{i-1}, ..., D_M^{i-1}\}$ using COLMAP~\cite{schoenberger2016sfm}, where $i \in [1, N]$ is the index of our iterative process.
Using the estimated poses and deblurred images, we aggregate information about the target 3D scene and construct a radiance field $\mathbf{V}^i$.
As discussed in \cref{sec:intro}, our framework can adopt diverse radiance field representations.
To demonstrate the extensibility of our framework, we adopt two different representations, Plenoxels~\cite{fridovich2022plenoxels} and 3D Gaussians~\cite{kerbl2023gaussiansplatting}, in our experiments.
We refer to our framework using Plenoxels and 3D Gaussians as \MethodName{}-P and \MethodName{}-G, respectively.

Note that deblurred images $\textbf{D}^{i-1}$ from the previous step may still contain residual blur and deblurring artifacts, which can introduce corrupted information into the aggregation process.
Nevertheless, since the input images have overlapping regions, other deblurred images can provide information for those regions that appear corrupted in some images.
By aggregating such information from different deblurred images on the same regions, the radiance field construction step can effectively suppress corrupted information and obtain a higher-quality radiance field, from which we can render higher-quality images compared to the input deblurred images at this step.


\subsection{RF-guided Deblurring}
The RF-guided deblurring step deblurs the input views $\textbf{B}$ using the aggregated information in $\textbf{V}^i$.
Specifically, for each input view $B_m$, we render $\textbf{V}^i$ to obtain a rendered image $R_m^i$.
The rendered image $R_m^i$ shares the same content as its corresponding deblurred image $D_m^{i-1}$, but contains fewer artifacts and finer details.
Motivated by this, our RF-guided deblurring leverages $R_m^i$ to obtain an updated deblurred image $D_m^i$ from $B_m$.

To perform RF-guided deblurring, we employ a novel RF-guided deblurring network that takes both the rendered image $R_m^i$ and the input blurred image $B_m$ to guide the deblurring process with the aggregated information in $R_m^i$.
For the deblurring network, we adopt NAFNet~\cite{chen2022nafnet} and modify its first layer to take a concatenation of the two images.
Once we obtain the updated deblurred images $\textbf{D}^i$, we return to the radiance field construction step and carry out the process for the $(i+1)$-th iteration, gradually enhancing the quality of the radiance field and the deblurred images.

The intermediate results of each step are visualized in \cref{fig:overview}-(b) to \cref{fig:overview}-(e).
Due to the severe blur in the input blurred image, its initial deblurring result retains some residual blur.
In contrast, thanks to the information aggregation occurring in the radiance field construction step, the rendered image contains more accurate details.
Finally, thanks to the guidance of the rendered image, the RF-guided deblurring result shows sharper and more precise details that cannot be observed in the intermediate results of previous steps.

\cref{fig:render_iterations} visualizes rendered images at different iterations.
Although the rendered image at the first iteration \cref{fig:render_iterations}-(a) contains blurry details, they are gradually refined at each iteration (\cref{fig:render_iterations} (b)-(d)) due to our iterative process, which incrementally improves the quality of the radiance fields and the deblurred images.
The effectiveness of our iterative process is also demonstrated by the increasing PSNR score as iterations progress.


