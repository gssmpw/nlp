\section{Related works}
Our study on the learnability of PRNGs for Transformers touches on several modern and classic topics. 

% \textbf{Modular Arithmetic} The study of how neural networks handle modular arithmetic gained prominence with ____'s discovery of the Grokking phenomenon, where they also show that the model acquired highly structured embeddings. Multiple studies, including ____, revealed that models tackle modular addition tasks by mapping integers to Fourier features. 
%____ provides a toy model to explain the grokking transition. ____ demonstrated the existence of multiple algorithmic approaches for decoding circular features within models.


\textbf{Interpretability and Modular Arithmetic} A growing body of work uncovers the circuits, algorithms, and emergent structures learned by Transformers ____. A particularly fruitful setting involves simple problems in modular arithmetic ____.
Our work adds to this by reverse-engineering the underlying algorithms and uncovering emergent structures in learning pseudo-random number sequences. 

% ____ showed that Transformers could learn various simple function classes in-context, while 
% ____ and ____ discovered that language models create in-context vectors that can be extracted to control model predictions. 
% In the context of optimization, ____ demonstrated that decoder-only models implement first-order optimization on emergent objective functions when solving linear regression tasks. 
% ____ further shows that larger models exhibit Bayesian estimation capabilities.
% ____ demonstrated that Transformers develop highly structured representations in attention heads and MLPs when solving modular arithmetic tasks in-context.

\textbf{Cracking PRNGs:} There is a classic duality between cryptography and learning theory ____, and cracking PRNGs is an important topic in cryptography. Nevertheless, deep learning-based attacks have received limited attention in the post-Transformer era. ____ demonstrated that a fully-connected neural network can predict the outputs of a modified LCG with fixed (irrational) parameters $(a,c,m) = (1,\pi,1)$. In comparison, we systematically analyze the harder cases of unseen parameters using Transformers, reverse-engineer the learned algorithms, and study effects of scale and complexity. 
%____ used lattice-based methods to attack the PCG64 PRNG____.

\textbf{Context-Free Grammar:} LCG can also be viewed as a formal language (Type-3 regular grammar) lying within the Chomsky hierarchy ____. Formal languages provide an interesting setting for synthetic datasets that can be used to understand the properties of neural networks in controlled settings ____. 

\looseness -1
\textbf{Chaotic time-series:} A major application of neural networks is predicting time-series for chaotic dynamics, such as weather prediction ____ and financial modeling. PRNGs provide an analog of such dynamics in the discrete setting. 

%____ shows that standard neural architectures (RNNs and Transformers) fail to generalize on context-free grammar tasks without additional structured memory components such as stacks or memory tapes.