%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\input{math_commands.tex}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}

\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
% \usepackage{caption}
% \usepackage{subcaption}

% extra packages
\usepackage{tcolorbox}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\DeclareMathOperator{\lcm}{lcm}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcounter{daggerfootnote}
\newcommand*{\daggerfootnote}[1]{%
    \setcounter{daggerfootnote}{\value{footnote}}%
    \renewcommand*{\thefootnote}{\fnsymbol{footnote}}%
    \footnote[2]{#1}%
    \setcounter{footnote}{\value{daggerfootnote}}%
    \renewcommand*{\thefootnote}{\arabic{footnote}}%
    }

\newcommand{\maissam}[1]{\textcolor{blue}{[MB: #1]}}
\newcommand{\dayal}[1]{\textcolor{teal}{[DS: #1]}}
\newcommand{\tao}[1]{{\textcolor{orange}{TT: [#1]}}}
\newcommand{\tianyu}[1]{{\textcolor[rgb]{0.505, 0, 0.921}{TH: [#1]}}}
\newcommand{\darshil}[1]{\textcolor{purple}{[DD: #1]}}

% % Add a line after authors with all emails
% \newcommand{\emails}[1]{\centerline{\texttt{#1}}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{(How) Can Transformers Predict Pseudo-Random Numbers? }

\expandafter\def\expandafter\normalsize\expandafter{%
    \normalsize%
    \setlength\abovedisplayskip{4pt}%
    \setlength\belowdisplayskip{4pt}%
    \setlength\abovedisplayshortskip{-4pt}%
    \setlength\belowdisplayshortskip{4pt}%
}

\begin{document}

% % Thanks notes for title uses \myfnsymbol
% \renewcommand{\thefootnote}{\myfnsymbol{footnote}}
% \maketitle
% % Layout the \thanks notes in the order you want
% % \footnotetext[1]{Condensed Matter Theory Center, University of Maryland, College Park}%
% \footnotetext[1]{Department of Physics, University of Maryland, College Park}%
% \footnotetext[2]{Join Quantum Institute, University of Maryland, College Park}%
% \footnotetext[3]{Corresponding author}%
% \footnotetext[4]{These authors contributed equally -- listed in psuedo-random order.}%
% % \footnotetext[5]{Source Code: \url{https://github.com/ablghtianyi/ICL_Modular_Arithmetic}}
% \setcounter{footnote}{0}% Restart footnote counter
% % Footnotes for rest of document uses \fnsymbol (or whatever you choose)
% \renewcommand{\thefootnote}{\arabic{footnote}}
% \vspace{-2.5em}
% \hspace{4em}
% \texttt{\{tianyuh, ddoshi, aritrad\}@umd.edu}
% \hspace{6.3em}
% \texttt{gromovand@meta.com}
% \vspace{2em}

\twocolumn[
\icmltitle{(How) Can Transformers Predict Pseudo-Random Numbers? }

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Tao Tao}{equal,phys}
\icmlauthor{Darshil Doshi}{equal,phys}
\icmlauthor{Dayal Singh Kalra}{equal,cs}
\icmlauthor{Tianyu He}{equal,phys}
\icmlauthor{Maissam Barkeshli}{phys,jqi}
\end{icmlauthorlist}

% Place this after your author list and before \begin{document}
% \emails{\{tao2021, ddoshi, dayal, tianyuh, maissam\}@umd.edu}
\begin{center}
    \texttt{\{tao2021, ddoshi, dayal, tianyuh, maissam\}@umd.edu}
\end{center}

\icmlaffiliation{phys}{Department of Physics, University of Maryland, College Park, USA}
\icmlaffiliation{cs}{Department of Computer Science, University of Maryland, College Park, USA}
\icmlaffiliation{jqi}{Joint Quantum Institute, University of Maryland, College Park, USA}

\icmlcorrespondingauthor{Maissam Barkeshli}{maissam@umd.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution -- authors listed in pseudo-random order.} % otherwise use the standard text.

\begin{abstract}

Transformers excel at discovering patterns in sequential data, yet their fundamental limitations and learning mechanisms remain crucial topics of investigation. In this paper, we study the ability of Transformers to learn pseudo-random number sequences from linear congruential generators (LCGs), defined by the recurrence relation $x_{t+1} = a x_t + c \;\mathrm{mod}\; m$. Our analysis reveals that with sufficient architectural capacity and training data variety, Transformers can perform in-context prediction of LCG sequences with unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding layers and attention patterns, we uncover how Transformers develop algorithmic structures to learn these sequences in two scenarios of increasing complexity. First, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but fixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. 
Our analysis reveals that models learn to factorize the modulus and utilize digit-wise number representations to make sequential predictions. In the second, more challenging scenario of unseen moduli, we show that Transformers can generalize to unseen moduli up to $m_{\text{test}} = 2^{16}$. In this case, the model employs a two-step strategy: first estimating the unknown modulus from the context, then utilizing prime factorizations to generate predictions. For this task, we observe a sharp transition in the accuracy at a critical depth $d= 3$. We also find that the number of in-context sequence elements needed to reach high accuracy scales sublinearly with the modulus.\daggerfootnote{We open source the code to reproduce our results: \url{https://github.com/dayal-kalra/transformer-prng.git}}

\end{abstract}



\section{Introduction}

Transformer-based language models have proven to be extremely powerful sequence generative models. With copious amounts of training data and computational resources, they can identify and learn complex patterns from training corpora, resulting in numerous remarkable capabilities \cite{attn_is_all_you_need, dosovitskiy2021an}. Recent research has demonstrated that these models, when provided with sufficient context and inference compute, can acquire new patterns and capabilities without additional training through techniques such as in-context learning \cite{radford2019language} and chain-of-thought reasoning \cite{wei2023chain}. While these models have achieved unprecedented success, understanding what underlying patterns are learned and how they learn them remains a significant challenge. Pseudo-Random Number Generators (PRNGs) represent an interesting test case for exploring these challenges. These algorithms, which are fundamental to modern cryptography and computer science, are designed to produce outputs that pass statistical tests for randomness, but nevertheless arise from mathematical patterns that could potentially be learned by sufficiently powerful sequence models.

This intersection between Transformer models' pattern-learning capabilities and the structured yet obfuscated nature of PRNG outputs raises intriguing questions about both the capabilities and limitations of these models. Can Transformers learn to predict PRNG outputs given sufficient training data, model capacity, and context? If so, what implications does this have for our understanding of both Transformer architectures and PRNGs? Do the Transformers learn the underlying generating algorithm or merely detect shortcuts and spurious patterns? What effect do model capacity, data variety, training methodologies, and context length have on the capabilities of Transformers? 

This work aims to answer these questions by focusing on learning sequences obtained from linear congruential generators (LCGs) using GPT-style autoregressive Transformers. We demonstrate how Transformers can successfully learn LCGs with moduli up to $m = 2^{32}$. We perform interpretability analyses, uncovering emergent structures in the embedding layers, attention heads, and underlying algorithms that the Transformer uses to learn the sequences. We also perform several systematic scaling analyses to understand the effect of architecture and sequence complexity on model performance and in-context learning ability. 

\subsection{Related works}

Our study on the learnability of PRNGs for Transformers touches on several modern and classic topics. 


\textbf{Interpretability and Modular Arithmetic:} A growing body of work uncovers the circuits, algorithms, and emergent structures learned by Transformers \cite{sharkey2025interp,olsson2022context,Ahn2023gradient,vonoswald2023Transformers,akyurek2023what,hendel2023incontext,liu2024incontextvector}. A particularly fruitful setting involves simple problems in modular arithmetic \cite{power2022grokking,gromov2022grokking,nanda2023progress,zhong2023clock,doshi2024to,he2024learning}.
Our work adds to this by reverse-engineering the underlying algorithms and uncovering emergent structures in learning pseudo-random number sequences. 


\textbf{Cracking PRNGs:} There is a classic duality between cryptography and learning theory \cite{rivest1991cryptography}, and cracking PRNGs is an important topic in cryptography. Nevertheless, deep learning-based attacks have received limited attention in the post-Transformer era. \citet{amigo2021} demonstrated that a fully-connected neural network can predict the outputs of a modified LCG with fixed (irrational) parameters $(a,c,m) = (1,\pi,1)$. In comparison, we systematically analyze the harder cases of unseen parameters using Transformers, reverse-engineer the learned algorithms, and study effects of scale and complexity. 


\textbf{Formal Grammars:} LCG can also be viewed as a formal language (Type-3 regular grammar) lying within the Chomsky hierarchy \cite{chomsky1956three}. Formal languages provide an interesting setting for synthetic datasets that can be used to understand the properties of neural networks in controlled settings \cite{deletang2023chomsky,allenzhu2024physicslanguagemodels1,cagnetta2024deep,cagnetta2024towards}. 

% \looseness -1
\textbf{Chaotic time-series:} A major application of neural networks is predicting time-series for chaotic dynamics, such as weather prediction \cite{lam2023learning} and financial modeling. PRNGs provide an analog of such dynamics in the discrete setting. 


\subsection{Linear Congruential Generators}

LCG is a simple PRNG that generates the next number in a sequence $(x_0, x_1, \dots, x_t)$ according to the map:
\begin{align}
\label{eq:lcg}
    x_{t+1} = (ax_t + c) \mod m,
\end{align}
where $m > 0$ is the modulus, $ 0 < a < m$ is the multiplier and $0 \leq  c < m$ is referred to as the increment. An LCG map is uniquely defined by the choice of $m, a, c$ and the initial seed $x_0$. An important quantity that determines the complexity of an LCG sequence is its period: $1 \leq \mathcal{T}_m(a,c) \leq m$. As we will show in the following sections, the period of a sequence plays a major role in the difficulty of prediction with Transformers. According to the Hull-Dobell Theorem \cite{hull-dobell}, the period $\mathcal{T}_m(a,c) = m$ if and only if the values of $a$ and $c$ satisfy the following criteria: (i) $m$ and $c$ are coprime, (ii) $a-1$ is divisible by all prime factors of $m$, (iii) $a-1$ is divisible by $4$ if $m$ is divisible by $4$. We evaluate (test) all our models exclusively on sequences that obey the criteria of this theorem.

LCGs are widely utilized for their speed and simplicity, often forming the core of more complex PRNGs like PCG-64, which is used in NumPy. LCGs perform poorly at small bit sizes but improve rapidly with larger state sizes. For instance, an LCG with 88 bits of state can pass the stringent BigCrush randomness test \cite{ONeill2014PCGA}.



\section{Training Setup}
\label{section:methods}

We train decoder-only Transformers to autoregressively predict the next number in LCG sequences. This means it takes as input an LCG sequence $(x_0,\cdots, x_{L-1})$, outputs a sequence $(y_0,\cdots, y_{L-1})$, and trained so $y_t$ matches $x_{t+1}$.

To predict an unknown LCG sequence, the Transformer needs to infer $m, a$, and $c$ in-context. We test the model's generalization ability in two distinct paradigms of increasing difficulty: \textbf{\texttt{FM:}} The model is trained and tested on sequences with a fixed modulus $m$. \textbf{\texttt{UM:}} Model is trained on varying moduli, and tested on unseen moduli $M_{\mathrm{test}} = \{m_{\mathrm{test}}\}$.
We highlight the key details of our experimental setups here and provide an extensive discussion in \Cref{appendix:experimental-details}.


\subsection{Dataset Generation and Evaluation}

The settings below are used in \Cref{section:train,section:interp}. In order to achieve better performance, we used a larger and higher-quality dataset in \Cref{section:scaling_laws}, which we detail later.

\textbf{Fixed Modulus (\texttt{FM}):} 
Given a modulus $m$, we apply the Hull-Dobell Theorem to determine the possible values of ($a, c$) that maximize the period. We then randomly select $64$ values of $a$ and $c$ to generate the test dataset. To generate the training dataset, we \emph{exlude} these test choices of ($a, c$) and uniformly sample $N = 100,000$ LCG sequences of length $L+1$ (where $L$ is the context length), with $n_a$ values of multipliers and $n_c$ values of increments. For each set of parameters ($a, c$), we sample an LCG sequence with a randomly selected initial seed $x_0$. Note that the training dataset includes sequences with varying periods, while the test data only contains sequences that maximize the period.


\textbf{Generalization to Unseen Modulus (\texttt{UM}):} 
In this more challenging paradigm, we first select a set of test moduli $M_{\text{test}} =  \{m_{\text{test}}\}$ that would be reserved exclusively for evaluation. For each test modulus $m_{\text{test}} \in M_{\text{test}}$, we determine the values of ($a, c$) that maximize the period. We then randomly select $64$ values of $a$ and $c$ each to generate the test dataset. These $64^2$ ($a, c$) pairs are not considered while generating the training dataset.

For the training dataset generation, we sample $n_m$ modulus values from the range $[L, m_{\text{max}}]$, with $m_{\text{max}} = \lfloor 1.2 \max(M_{\text{test}}) \rfloor$, while excluding all the values in $M_{\text{test}}$. For each modulus value $m$, we uniformly select $n_a$ multipliers and $n_c$ increments, excluding the ones reserved for testing. For each triplet ($a, c, m$), we generate a sequence of length $L+1$ using a randomly selected initial seed $x_0$. This results in a total of $N = n_m \times n_a \times n_c$ training sequences. 


We found that $n_m \gtrsim m_{\text{test}} / 4$ yields good generalization performance. Based on this relationship and our target total number of training examples $N$, we sample $n_a = n_c = \sqrt{\frac{N}{m_{\text{test}}/4}}$ values of multipliers and increments. Unless explicitly specified, we use this setting as the default configuration for all experiments.


In both paradigms, the test accuracy is measured by averaging over all sequences generated from the $(a,c)$ pairs in the test set and multiple initial random seeds $x_0$ for each $(a,c)$ pair.
We track the accuracies at all positions in the sequence $1 \leq t \leq L$.
 


\subsection{Tokenization, Architecture, and Optimizer}

In \Cref{section:train,section:interp}, we tokenize each number as a unique token, with dictionary size $m$ for the \textbf{\texttt{FM}} case and $m_{\mathrm{max}}$ for the \textbf{\texttt{UM}} case. We then use GPT-style Transformers with learnable positional embeddings and weight tying \cite{press2017tying}. 


When we scale up to larger moduli in \Cref{section:scaling_laws}, we restrict the dictionary size to $b$ by tokenizing each number in the sequence in base-$b$ (e.g. $b=2^8, 3^5$). This results in $\lceil \log_b{m} \rceil$ tokens for each number. We also apply (modified) abacus positional embeddings \cite{mcleish2024abacus}. 

\looseness -1
The model architecture is characterized by the number of blocks (depth), embedding dimension ($d_{\mathrm{model}}$), number of attention heads ($n_{\mathrm{heads}}$). Models are trained with AdamW \cite{adamwloshchilov2018} and CrossEntropy loss. 



\section{Training Results}
\label{section:train}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/phase-diagrams/depth-width_phase_diagrams.pdf}
    \caption{Accuracy of predicting the last number (token) in the sequence: phase diagrams w.r.t. various depths and $n_{\mathrm{heads}}$ values. (a) $m_{\mathrm{test}}=2048$, constant width: $d_{\mathrm{model}}=768$. (b) $m_{\mathrm{test}}=4096$, width scaled proportionally: $d_{\mathrm{model}}=128 \times n_{\mathrm{heads}}$. }
    \label{fig:phase_diagrams}
\end{figure}

We begin by investigating the minimal model
that can solve the two tasks in consideration.
Surprisingly, we found that Transformers only require one layer and one attention head to learn the \textbf{\texttt{FM}} task, as shown in \Cref{fig:training} (a) (for further results, see \Cref{appendix:fm_train}).
Conversely, the \textbf{\texttt{UM}} task requires a stronger architecture and careful hyperparameter tuning.

In \Cref{fig:phase_diagrams} we show how the performance varies with model depth and the number of attention heads. In \Cref{fig:phase_diagrams}(a) we keep the embedding dimension fixed to $d_{\text{model}} = 768$, whereas in \Cref{fig:phase_diagrams}(b) we scale it proportionally to the number of heads ($d_{\text{model}} = 128 \times n_\textrm{heads}$.). In both cases, we find that a minimum of three layers are required for effective generalization, with performance degrading sharply below this threshold.
Further analysis across multiple $m_{\mathrm{test}}$ values (see \Cref{appendix:critical_depth}) confirms that this minimal depth requirement is universal. We also observe that additional attention heads improve model performance, with substantial gain occurring when increasing from one to two heads.


Several prior studies have also observed sharp changes in model capabilities as a function of 
model depth. This includes induction head formation \cite{olsson2022context}, in-context learning of modular addition \cite{he2024learning} and various in-context generalization tasks \cite{chen2024what}. In general it is unclear to what extent these sharp depth-dependences are due to jumps in expressivity or trainability.


The \textbf{\texttt{UM}} task shows strong sensitivity to hyperparameters. As we increase $m_{\text{test}}$ while keeping the model size fixed, we observe two key phenomena: the optimal learning rate ($\eta$) and weight decay strength ($\lambda$) shift significantly, and simultaneously, the range of hyperparameter resulting in effective performance narrows (see \Cref{appendix:hparam-shrinking}).

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/training-interp/accs_final.pdf}
    \caption{Training curves for train/test accuracies for predicting the last number (token) in the sequence. (a) \textbf{\texttt{FM}:} ($m=2048, \mathrm{depth}=1, n_{\mathrm{heads}}=1, d_{\mathrm{model}}=768$) Test accuracy ``groks'' when Training accuracy reaches near-$100\%$. (b) \textbf{\texttt{UM}:} ($m_{\mathrm{test}=2048}, \mathrm{depth}=6, n_{\mathrm{heads}}=4, d_{\mathrm{model}}=768$) Test accuracy ``groks'' simultaneously with Training accuracy on sequences with period longer than context length ($\mathcal{T}_m > L=256$), indicating delayed discovery of underlying rules.}
    \label{fig:training}
\end{figure}


We then carefully examine the training dynamics in both \textbf{\texttt{FM}} and \textbf{\texttt{UM}} settings (\Cref{fig:training}).
We categorize the training sequences into two groups: i) sequences with periods shorter than the context length, which can be solved through simple copying, and ii) sequences with periods longer than the context length, which require the model to deduce underlying rules for prediction. Our analysis reveals that the model first acquires copying ability for group i) in the early stages of training, and later ``groks" the solution for group ii) \cite{power2022grokking}. Notably, the model's ability to generalize to test modulus $m_{\mathrm{test}}$ emerges simultaneously with this grokking phenomenon. These results demonstrate that the model develops different capabilities at distinct stages of training, with generalization ability emerging only after the model learns the underlying rules through solving the more challenging sequences. In \Cref{appendix:train-interp}, we present an ablation study where models are trained exclusively on either short-period or long-period sequences. Our findings indicate that training exclusively on long-period sequences enables model generalization.


\section{Interpreting how Transformers predict PRNGs}
\label{section:interp}

\looseness -1
In this section, we uncover the underlying algorithms implemented by the models for both \textbf{\texttt{FM}} and \textbf{\texttt{UM}} cases. While certain details of the algorithms differ in the two cases, they share common properties originating from the underlying LCG structure.

We first discuss properties of LCG sequences that will be useful in interpreting model behaviors.


\subsection{Residual Number System Representations}
\label{sec:radix}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/fixed_p/sequence_intro_new.pdf}
    \caption{Bit-wise periods in an example LCG sequence generated with $m=2048, a=293, c=1033$, which follows the Hull-Dobell theorem. In binary representation $w$-th lowest bit has a period of $2^w$, for $w \in \{ 1, \dots 11 \}$. Writing a new sequence by skipping every 2nd step ($r=2$) reduces the periods of all the bits by a factor of $2$, rendering the lowest bit constant. 
    $r=2^k$ reduces bit-wise periods by a factor of $2^k$, with last $k$ digits constant.
    }
    \label{fig:digit_wise_sequences}
\end{figure}
\vspace{-0.05 in}
Consider an LCG sequence with modulus $m = 2048 = 2^{11}$. Each number in this sequence can be represented as an 11-digit binary number:
\begin{equation}
\label{eq:binary_rep}
    x \;\mathrm{mod}\; 2^{11} = \alpha_0 \, 2^0 + \alpha_2 \, 2^2 + \cdots + \alpha_{10} \, 2^{10} \;,
\end{equation}
where $\{\alpha_0, \alpha_1, \dots, \alpha_{10}\}$ are the binary digits (bits), $\alpha_w \in \{0, 1\}$.

A useful property of LCGs with modulus $m=2^{(\cdot)}$ is that each digit in the binary representation has a fixed period along the sequence. As shown in \Cref{fig:digit_wise_sequences}, for a sequence of period $\mathcal{T}_m = m = 2^{11}$, the $w^{th}$ lowest digit has a period of $2^w$ \cite{art_cp}. Thus, lower (higher) digits have smaller (larger) periods along LCG sequences. (See \Cref{appendix:proof1} for a detailed derivation.) We will see later that trained Transformers have emergent structures that find these binary representations and utilize them to make systematic predictions from the context. Notably, the per-digit period plays and important role in the prediction accuracy of that digit. To understand this, consider the $r$-step iteration of \Cref{eq:lcg}:
\begin{align}
\label{eq:lcg_r}
    x_{t+r} = a^r x_t + \sum_{i=1}^{r} a^{i-1} c \mod m \;.
\end{align}

In this new sequence wherein we skip $r$ steps, the period of each digit $\alpha_w$ reduces from $2^w$ to $2^w / \gcd(r, 2^w)$. Consequently, the higher digits become relatively simpler to predict due to reduced periods while some lower digits become trivial to predict due to being constant along this new sequence.
We demonstrate this for $m=2048$ in \Cref{fig:digit_wise_sequences} (top panel). The digit-wise periods in the new sequence with $r=2$ are reduced by a factor of 2, while the last digit is simply constant. Higher values of $r=2^k$ will lead to even further simplifications of the sequence. 
Transformers can simplify the task of predicting LCG sequences by utilizing $r$-step iterations from the in-context examples -- with longer contexts leading to larger values of $r$. Consequently, the per-digit and overall accuracies improve substantially with context (see \Cref{fig:acc_vs_token}).


While moduli of the form $m=2^{(\cdot)}$ lead Transformers to find binary representations, similar simplifications in composite moduli require more general representations of the \emph{Residual Number System (RNS)} \citep{garner1959residue}. RNS represents numbers by their values modulo pairwise coprime factorizations of $m$. Specifically, consider sequences with a composite modulus $m$, which has a prime factorization $m = p_1^{w_1} p_2^{w_2}\cdots p_q^{w_q}$. In this case, we can uniquely represent each number $(x \,\mathrm{mod}\, m)$ as the tuple of residuals $( x \,\mathrm{mod}\, p_1^{w_1}, x \,\mathrm{mod}\, p_2^{w_2}, \dots, x \,\mathrm{mod}\, p_q^{w_q} )$. Analogous to \Cref{eq:binary_rep}, we can further decompose each residual,

\begin{align}
    x \;\mathrm{mod}\; p_j^{w_q} = \alpha_{j,0}\, p_j^0 + \alpha_{j,1}\, p_j^1 + \cdots + \alpha_{j,w_j-1}\, p_j^{w_j-1}
    \label{eq:period_m_composite}
\end{align}

where $\alpha_{j,w} \in \{ 0, 1, \dots, p_j - 1 \}$ are base-$p_j$ digits. We refer to $\{\alpha_{j,w}\}$ as the ``RNS representation" in the remainder of the text. When $\mathcal{T}_m = m$, each digit $\alpha_{j,w}$ has a period of $p_j^w$ (derivation in \Cref{appendix:proof2}). The $r$ step iteration \Cref{eq:lcg_r} reduces the period of each digit $\alpha_{j,w}$ from $p_j^w$ to $p_j^w / \gcd(r, p_j^w)$. This results in simplification of the prediction task whenever $r = p_1^{k_1} p_2^{k_2} \cdots p_q^{k_q}$. We will see that identifying the RNS representations is a key simplification that the Transformer discovers in learning LCG sequences.



\subsection{Interpretability: Fixed Modulus}
\label{sec:interp_fixed}


\looseness -1


\label{text:algorithm_fixed_modulus}
\begin{tcolorbox}
\underline{Qualitative Algorithm (fixed modulus):}
\begin{enumerate}
    \item[i.] Find RNS representations of inputs from the learned prime factorization of $m$
    \item[ii.] Look back $r=p_j^k$ steps in the context and copy the lowest $k$ digits, for different prime factors ($p_j$) of $m$
    \item[iii.] Using these $r$-step iterations, predict the higher digits of the simplified sequence
\end{enumerate}
\end{tcolorbox}

We now discuss the algorithm implemented by Transformers trained on LCG with a fixed modulus. We will show that, despite being provided integers as inputs, the model develops emergent structures that create and leverage the RNS representation of the inputs. We will focus on the setup with a 1-layer, single-head Transformer trained on LCG with $m=2048$. Similar results for composite moduli (e.g. $m=7776$) and different model sizes are presented in \Cref{appendix:fixed_interp}. We emphasize that this algorithm works for arbitrary $a,c,x_0$ for a given $m$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/fixed_p/acc_final_2048_d1_h1_ne768.pdf}
    \caption{\textbf{\texttt{FM:}} Test accuracy of trained model ($m=2048, \text{depth}=1, n_{\mathrm{heads}}=1, d_{\mathrm{model}}=768$), averaged over $a, c$ and initial seeds. (a) Test accuracy w.r.t. token positions. Ladder-like structure appears, with jumps occurring at $2^k$-th positions. (b) We represent numbers as an eleven-digit binary number ($2048 = 2^{11}$) and compute the per-digit test accuracy of model predictions.}
    \label{fig:acc_vs_token}
\end{figure}

We begin by analyzing the average accuracy of a trained Transformer model as a function of token position along the context, shown in \Cref{fig:acc_vs_token} (a). (Recall that in this section a token corresponds to an integer of the LCG sequence). The accuracy exhibits a ladder-like structure, with each jump occurring exactly at the $2^k$-th token position. These successive transitions can be explained using binary representations and $r$-step recurrence (\Cref{eq:lcg_r}). Specifically, to predict the token at position $t \geq 2^k$, the model can look back in the context at position $t - 2^k$ and implement $r=2^k$-step iteration. This allows the model to (i) copy the lowest $k$ bits since they remain unchanged; and (ii) simplify the higher bits, since their periods get reduced by a factor $2^k$. We note that the accuracy trend remains unchanged across different choices of $a$ and $x_0$ (see \Cref{appendix:fixed_interp} \Cref{figapp:accuracy_vs_token_ac_m=2048} ).

Next, in \Cref{fig:acc_vs_token}(b), we compute the per-digit accuracy by converting both model predictions and ground truth labels to their binary representations according to \Cref{eq:binary_rep}. We observe that the model can predict more digits correctly with more context, with sharp transitions occurring at $2^{k}$-th token positions. This is a direct result of the sequences becoming increasingly more simplified as the model can look farther back in the context and utilize $(r=2^{k})$-step iterations. Since these simplifications occur in the form of digit-wise periods, \Cref{fig:acc_vs_token}(b) serves as direct evidence that the model is internally developing and utilizing binary representations. The sequential learning of digits also explains the ladder-like structure of the accuracy in \Cref{fig:acc_vs_token}(a).
We find that the overall average accuracy (\Cref{fig:acc_vs_token}(a)) multiplicatively depends on the per-digit accuracies  (\Cref{fig:acc_vs_token}(b)) (empirical proof in \Cref{appendix:fixed_interp} \Cref{figapp:accuracy_m=2048_h1}(b)) \footnote{This holds if the per-digit accuracies are independent, which we confirm empirically.}
\begin{equation}
    \mathrm{acc}_{\mathrm{overall}} = (\mathrm{acc}_{\mathrm{digit} \, 1}) \, (\mathrm{acc}_{\mathrm{digit} \, 2}) \, \cdots \, (\mathrm{acc}_{\mathrm{digit} \, 11}) \,.
\end{equation}


Next, we investigate how various components of the model implement the algorithm outlined earlier this section. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/fixed_p/embedding_small_p2048_d1_h1_n768.pdf}
    \caption{\textbf{\texttt{FM:}} ($a=1589, c=629$)  Embedding layer. (a1) 1st principal component groups the numbers mod 2. (a2) 2nd and 3rd principal components group numbers mod 4. (b) Embedding vectors of different numbers exhibit high cosine similarity when they are spaced $2^k$ apart, with the similarity increasing with $k$.}
    \label{fig:fixed_p_embedding_pca}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/fixed_p/mask_final.pdf}
    \caption{\textbf{\texttt{FM:}} ($m=2^{11}, a=1589, c=629$) (a) Attention weights: each query attends most strongly to the tokens $2^k$ and $2^{k-1}$ distance backward, for the highest possible value of $k$, enabling copying of lowest $k$ bits. The other faint lines facilitate the prediction of higher bits. (b) Post-ReLU hidden layer MLP activations at token position $t=129$ (extracted using sequences with different $x_0$) as a function of the target number $x_{130}$ which it is supposed to predict. Each neuron gets activated only while predicting a specific $x_{130}$, exhibiting a sparse, periodic pattern. (c) Output of the MLP block projected onto the (un)embedding matrix; after masking out all but a single given hidden-layer neuron. The green dot denotes the value at the target number. Each neuron resolves the correct prediction up to a periodic structure. (d) Output of the MLP block projected onto the (un)embedding matrix; after combining the signal from multiple neurons (i.e. gradually un-masking the neurons). The per-neuron periodic patterns constructively interfere at the correct output.}
    \label{fig:fixed_p_attn_mlp}
\end{figure*}

\textbf{Step i:} We begin by conducting Principal Components Analysis (PCA) of the embedding matrix, which shows how the model performs prime factorization to develop the binary representations (RNS for general $m$). \Cref{fig:fixed_p_embedding_pca}(a1) shows the projections of all numbers $x \in \{0, \dots 2047\}$ along the first principal component of the embedding. We observe that the model groups the numbers into modulo $2$ clusters along the first principal component. Similarly, the 2nd and 3rd principal components group the numbers into modulo $4$ clusters (\Cref{fig:fixed_p_embedding_pca}(a2)). In general, we find principal directions that group the numbers into modulo $2^{(\cdot)}$ clusters (see \Cref{appendix:fixed_interp} \Cref{figapp:embedding_m=512}).
By clustering the numbers according to their remainder modulo different prime-powers, these principal directions naturally encode the digit-wise representations of the inputs. 
In \Cref{fig:fixed_p_embedding_pca}(b), we check the cosine similarity between the embedding vectors of different numbers. We see that the more digits two numbers share in the binary representation, the higher the cosine similarity of their embedding. This is a consequence of these numbers having similar components along principal directions corresponding to those digits.


\textbf{Step ii, iii:}
In \Cref{fig:fixed_p_attn_mlp}(a) we examine the attention weights and find that to predict the number at position $t$, the model attends most strongly to the position $t - 2^k$ for the highest possible value of $k$ s.t. $t \geq 2^k$ (i.e. $k = \lfloor \log_2{t} \rfloor$). This corresponds to the brightest line in \Cref{fig:fixed_p_attn_mlp}(a). Using the binary representation of the $(t - 2^k)$-th token, the model can copy the lowest $k$ bits and simplify the prediction of higher bits. The second brightest line corresponds to the tokens at position $t - 2^{k-1}$, which further strengthens the copying of the lower bits. In addition to the two bright lines in the attention weights, we observe multiple faint lines at intermediate distances, which are multiples of $2^{k'}$ for $k' < k-1$. The model combines the information from the two bright lines as well as the faint lines to predict the higher bits.

To verify that the two brightest lines facilitate the copying of lower bits and that the other faint lines enable the prediction of higher bits, we performed the following two experiments. (i) For each query in the attention head, we mask out all keys except for the one with the highest attention score (i.e. the brightest line). We then measure the performance of this ablated model, shown in \Cref{appendix:fixed_interp}, \Cref{figapp:digits_topk}(b). We observe that the model retains the ability to copy the $k$ lowest bits, but loses the ability to predict the higher bits. (ii) Next, we repeat the above ablation experiment while masking out all keys except the ones with the top two attention scores (two brightest lines). \Cref{appendix:fixed_interp}, \Cref{figapp:digits_topk}(c)) This results in an improvement in the copying performance compared to (i), but the performance on the higher bits remains poor. This confirms that the information from the other faint lines is crucial for predicting higher bits.


After the attention layer collects information about previous tokens, the MLP block\footnote{MLP block consists of a fully connected layer $\rightarrow$ ReLU $\rightarrow$ fully connected layer.} processes the information to make predictions. We find that each hidden neuron (post-ReLU) in the MLP exhibits a periodic response with a distinct period, as a function of the target prediction. Consider the MLP at token position $t$. The input to the transformer at this position is $x_t$ and the output $y_t$, should match the next number in the sequence $x_{t+1}$. In \Cref{fig:fixed_p_attn_mlp}(b) we show the activation value of selected neurons at token position $t=129$ as a function of the target $x_{130}$, for $m$ different sequences obtained by changing the seed $x_0$ for a given LCG sequence. We observe that only a sparse set of neurons are activated for a given target $x_{130}$ and that there is a strong spiked periodic structure in the response of each neuron as a function of $x_{130}$, with neuron-specific frequencies. 

The subsequent fully connected layer in the MLP block aggregates the contributions from all the activated neurons to make the correct prediction. To visualize the contributions from an individual neuron, we mask out the contribution from all other neurons and extract the MLP output for a fixed input sequence. We then project this output onto the (un)embedding matrix, which shows us the contribution of that single neuron in making the correct prediction.\footnote{We read-out MLP outputs instead of network outputs to avoid distortion from the skip connection.} In \Cref{fig:fixed_p_attn_mlp}(c) we show that each neuron resolves the target number $x_{130}$ up to a distinct periodic pattern. 
The periodic patterns from different neurons constructively interfere at the target. In \Cref{fig:fixed_p_attn_mlp}(d), we observe that gradually adding contributions from multiple neurons resolves the correct output with increasing accuracy.


\subsection{Interpretability: Generalization to Unseen Modulus}
\label{sec:interp_unseen}

\begin{tcolorbox}
\underline{Qualitative Algorithm (unseen modulus):}
\begin{enumerate}
    \item[i.] Encode information about various possible prime factorizations
    \item[ii.] Estimate the modulus using the largest number in the context
    \item[iii.] Combine steps i and ii to construct correct RNS representations, then implement steps ii and iii from the fixed modulus algorithm
\end{enumerate}
\end{tcolorbox}

Unlike the \textbf{\texttt{FM}} case, the training set for \textbf{\texttt{UM}} is generated using many different moduli $m$, unseen at test time. Since each modulus has its own RNS representation incompatible with other moduli, the model must implement a different, more general algorithm to solve \textbf{\texttt{UM}} tasks.

We analyze how the embedding layer and attention heads of a 4-layer Transformer model help implement the above algorithm to solve the \textbf{\texttt{UM}} task. The model is trained on a dataset generated with $n_m = n_a = n_c = 128$, where $m_{\max}=2457$. To avoid leakage between training and test sets, we specifically exclude the moduli $m_{\mathrm{test}} \in \{1800=2^3 \cdot 3^2 \cdot 5^2, 2048=2^{11}, 2352=2^4 \cdot 3 \cdot 7^2\}$ from the training set. Note that the choice $m_{\mathrm{max}}= \lfloor 1.2 \cdot 2048 \rfloor$ is made to maintain consistency with the setting in \Cref{fig:phase_diagrams}(a).

\begin{figure}[!t]
    \centering
    \subfloat {\includegraphics[width=1.0\linewidth]{figures/multi_interp/embd_combine_small.pdf}} 
    \caption{\textbf{\texttt{UM}}: PCA analysis of the embedding layer. 
    }
    \label{fig:multip_pca_embd}

\end{figure}


\textbf{Step i:}
We begin by analyzing the embedding layer. In \Cref{fig:multip_pca_embd}(a), PCA reveals that the model forms a semi-circular structure along the first principal component, while the second principal component segregates numbers into even and odd groups. The semi-circular structure resembles the circular patterns observed in modular arithmetic tasks \citep{power2022grokking,zhong2023clock}. However, since our model is trained with multiple moduli, it cannot determine the value that should be identified with $0$ to form a closed circle.


Further analysis of \Cref{fig:multip_pca_embd}(b) reveals that the $2$nd and $3$rd principal components of the embedding layer group numbers according to their remainder modulo $2$ and modulo $3$, respectively. Since the modulus is not fixed in this case, there is no inherent requirement for the model to encode numbers in a binary representation, unlike the case we demonstrated earlier. In fact, as we will show next, the model forms RNS representations based on the choice of modulus $m_{\mathrm{test}}$ through specialized attention heads. We believe that the embedding layer's primary focus on modulo $2$ and $3$ can be attributed to these being the most common prime factors among numbers appearing in the training set.


In the first layer, attention heads further process the embedded numbers by grouping them according to their remainder modulo various prime factors, with each head specializing in specific prime factors. This head specialization enables the model to construct RNS representations requiring different prime bases.

\begin{figure}[!h]
    \centering
    \subfloat {\includegraphics[width=1.0\linewidth]{figures/multi_interp/prune_new_small.pdf}} 
    \caption{\textbf{\texttt{UM}}: PCA analysis of attention heads specializing in different prime factorizations and their effect on per-digit accuracy. (a1, b1, c1) PCA patterns of outputs extracted from corresponding heads. The first two principal components group numbers based on their remainder modulo $14$, modulo $4$, and modulo $3$, respectively. (a2, b2, c2) Test accuracy for individual digits when numbers are represented according to \Cref{eq:period_m_composite} for given $m_{\mathrm{test}}$. (a3, b3, c3) Test accuracy for individual digits after pruning the corresponding heads shown in (a1, b1, c1). Pruning these specific heads substantially degrades performance on their corresponding digits. 
    }
    \label{fig:multip_prune}

\end{figure}

To investigate the head specializations, we first analyze the attention heads in the first layer that process different prime factors in \Cref{fig:multip_prune}, where we perform PCA for each head and depict the results in panels (a1, b1, c1). Specifically, to generate these panels, we input sequences into the model using the corresponding $m_{\mathrm{test}}$ value and randomly selected $(a, c)$ pairs according to the Hull-Dobell Theorem. We then extract outputs $\bm{H}^{(h)} \in \mathbb{R}^{m_{\mathrm{test}} \times L \times d_{\mathrm{model}}}$ from each corresponding head by zeroing out the contributions from all other heads in the first layer, where the superscript $(h)$ denotes the specific attention head of interest in \Cref{fig:multip_prune}. Subsequently, we perform PCA analysis at a specific token position $t=0$, which can be expressed in NumPy notation as $\texttt{PCA}(\bm{H}^{(h)}[:, \, 0, \, :])$, and project each number's feature vector $\bm{H}^{(h)}[x, \, t, \, :]$ onto the top two principal components. Each point in the projected space is labeled with its corresponding number $x$. 

The analysis reveals that each head naturally groups numbers according to their remainder modulo different prime factors. This head specialization enables the subsequent model layers to select the appropriate representation. The prominence of small prime numbers in the top principal components likely reflects their frequent occurrence in the prime factorization of composite numbers. Furthermore, we believe the performance gain observed in \Cref{fig:phase_diagrams} when increasing the number of heads can be partially attributed to the model's enhanced ability to capture more prime factors, enabling it to construct RNS representations for $m_{\mathrm{test}}$ with rarely occurring prime factors. More examples of this specialized behavior, including detailed analyses of heads processing other moduli and the robustness of these grouping patterns across different choices of $a$, $c$, $m_{\mathrm{test}}$, $x_0$ and $t$, are presented in \Cref{fig:pca_appendix} in \Cref{appendix:unseen_pca}.


\looseness -1
To further demonstrate that these heads directly influence the model's performance, we measured the model's per-digit accuracy in panels (a2, b2, c2) and compared it with the performance after pruning the corresponding heads, shown in panels (a3, b3, c3). For pruning, we replace all components of the outputs from the corresponding heads with a single value $\texttt{mean}(\bm{H}^{(h)}[:, \, :, \, :]) \in \mathbb{R}$. Here, the attention head output $\bm{H}^{(h)}$ is computed using $10\%$ of randomly selected training sequences. We chose this approach rather than zeroing out the outputs to avoid catastrophically degrading the model's performance, as our method preserves the scale of the signal propagating to the next layer. Comparison with the pre-pruning results reveals that for each $m_{\mathrm{test}}$, pruning the head responsible for a specific prime factorization dramatically degrades performance on the corresponding digits while having a relatively minor impact on others. For instance, in panel (a3), the model's ability to compute $7^1$ and $7^2$ digits is completely lost, while base-$2$ and base-$3$ digits, although impacted, remain above chance level. Similarly, panel (b3) shows complete performance degradation across all digits when the head responsible for computing $2^{(\cdot)}$ is pruned. In panel (c3), the model's ability to compute digits related to base-$3$ is drastically reduced, while its capacity to process base-$2$ and base-$5$ digits remains largely intact or shows only marginal degradation.

Finally, we conduct pruning experiments by removing irrelevant heads to validate the correlation between specific attention heads and digit-wise performance. For example, with $m_{\mathrm{test}}=2048$, we prune the corresponding heads shown in panels (a1) or (c1), which are mainly responsible for modulo $7$ and modulo $3$ operations. The results are shown in \Cref{fig:prune_appendix_2048} in \Cref{appendix:prune}. Interestingly, when we remove the attention head responsible for computing modulo $3$ representations, we observe improved performance at specific token positions for certain bits when $m_{\mathrm{test}}=2048$, while other bits exhibit minimal performance degradation. These results further confirm our earlier claim about head specialization. For additional details, we refer readers to \Cref{appendix:prune}.

\begin{figure}[!h]
    \centering
    \subfloat {\includegraphics[width=1.0\linewidth]{figures/multi_interp/cosine_combine_small.pdf}} 
    \caption{\textbf{\texttt{UM}}: Attention head (layer 1, head 6) for estimating $m_{\mathrm{test}}$. (a) The query consistently attends to keys corresponding to the largest values within the given sequence; (b) the head generates features with the highest cosine similarity to $m_{\mathrm{test}}$; (c) when patching the feature with sequences generated using $m_{\mathrm{patch}}=1024$ but maintaining the same $a$ and $c$, the model often makes predictions with values of $n$ smaller than $m_{\mathrm{patch}}$.}
    \label{fig:multip_cosine}
\end{figure}


\textbf{Step ii:} The observed performance variations in the previous experiments suggest that the model experiences internal uncertainty about which RNS representation is relevant. This uncertainty likely stems from the model's difficulty in accurately determining the value of $m_{\mathrm{test}}$; as we explain below, we find that the model estimates $m_{\mathrm{test}}$ greedily through in-context learning. 

In \Cref{fig:multip_cosine}(a), we identify one attention head in the first layer that implements such a greedy algorithm by attending only to the largest numbers within the sequence, visualized as a few vertical lines in the attention weight plot. The input sequence is generated with $x_0=1$, $a=5$, $c=31$, and $m_{\mathrm{test}}=2048$. In panel (b), we use the same sequence to extract outputs from this specific head $\bm{H}^{(h)} \in \mathbb{R}^{1 \times L \times d_{\mathrm{model}}}$. We then compute the cosine-similarity between these extracted outputs and the token embeddings for all numbers $x < m_{\mathrm{test}}$. The resulting $\mathbb{R}^{L \times m_{\mathrm{test}}}$ matrix is plotted in panel (b). This analysis reveals that the output from this head consistently maintains high cosine-similarity with the largest numbers it has seen within the context, representing a greedy estimation of $m_{\mathrm{test}}$.

To further confirm that this head provides an estimation of $m_{\mathrm{test}}$ for the later layers, we perform a patching experiment \citep{zhang2024actpatch}, as shown in \Cref{fig:multip_cosine}(c). We first generate a new sequence with the same $a$, $c$, and $x_0$ but with a different modulus $m_{\mathrm{patch}}=1024$. Then perform a forward pass and take the output $\bm{H}^{(h)}_{\mathrm{patch}} \in \mathbb{R}^{1 \times L \times d_{\mathrm{model}}}$ from this head. Next, we use it to overwrite the corresponding output $\bm{H}^{(h)}$ during the forward pass of the sequence generated with $m_{\mathrm{test}}=2048$. The results show that the model frequently predicts numbers with values smaller than $m_{\mathrm{patch}}$. As demonstrated in \Cref{fig:patch_appendix} in \Cref{appendix:patch}, patching other heads may disrupt the predictions but never leads to a similar qualitative change, confirming that this head uniquely contributes to the model's estimation of $m_{\mathrm{test}}$. 

Notably, one might intuitively expect that a small error in estimating $m_{\mathrm{test}}$ would completely invalidate the prediction. However, we argue that this is not the case. As demonstrated in \Cref{sec:interp_fixed}, when the correct representation is chosen, the lower bits exhibit a strong periodic coherent signal. Conversely, given that the model can prepare multiple RNS representations (as shown in \textbf{step i}) and that the estimated value is sufficiently close to $m_{\mathrm{test}}$, the coherent signal from lower bits could guide the model toward selecting the correct representation. Further discussion is provided in \Cref{appendix:estimate} and \textbf{step iii}.

\textbf{Step iii:} 
After preparing the necessary features, subsequent layers in the model implement the rest of the algorithm. As in the \textbf{\texttt{FM}} case, we observe ladder structures in digit-wise accuracy for the lower digits at early token positions. This pattern, which demonstrates the copying bias, is visible in \Cref{fig:multip_prune}(b2). 

Interestingly, through careful examination of \Cref{fig:multip_prune}(b2), we can observe that the model is copying the lowest 5 digits. Using this observation along with the estimated value of $m_{\mathrm{test}}=2033$ from \textbf{step ii}, we can make a simple yet insightful argument: The remaining factor the model needs to determine is the ratio $2033/2^{5} \approx 63.53$. Since the model deals exclusively with integers, the closest value is $64$, which yields $64 \cdot 2^{5}=2048=m_{\mathrm{test}}$. This enables the model to build an accurate algorithm for predicting higher digits without knowing the exact value of $m_{\mathrm{test}}$.

While we know there exists some algorithmic transition from lower to higher digits, the detailed mechanism for computing higher digits remains unclear. The sharp transition in per-digit performance at the $2^5$ digit in \Cref{fig:multip_prune}(b2) supports our argument. Nevertheless, we believe that the model implements an algorithm similar to that of \textbf{\texttt{FM}} for computing the higher bits in this case, which we will elaborate on in \Cref{appendix:step3}.


\section{Scaling Up the Modulus}
\label{section:scaling_laws}



In this section, we investigate Transformer training upon scaling up the modulus of LCG. For this purpose, we make the following modifications:

\textbf{Base-b tokenization:} To avoid massive dictionary sizes for large $m$, we implement a base-$b$ tokenization scheme. Each integer is decomposed into a sequence of base-$b$ digits, beginning with the least significant digit; resulting in a vocabulary size of $b$ for any $m$ (for details see \Cref{appendix:base-b}). Based on the discussion in \Cref{sec:radix}, it is beneficial to choose $b$ such that $\mathrm{gcd}(b, m) = b$.



\textbf{Abacus Embeddings:} We encode positional information using a variant of the Abacus embedding \cite{mcleish2024abacus}, as a sum of two learnable vectors. One vector encodes the position of the integer within the sequence, while the other encodes the position of each digit within the integer (for details, see \Cref{appendix:abacus}). \Cref{fig:byte-tok} provides a visualization of base-$b$ tokenization and abacus embedding.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/byte-tokenization/embeddings.pdf}
    \caption{Visualization of base-$b$ tokenization and abacus embeddings. Abacus embedding 1 is shared by all the digits within the integer, while Abacus embedding 2 varies within the digit but is shared by all integers.
    }
    \label{fig:byte-tok}
\end{figure}

\textbf{Fixed Modulus:} For each modulus $m = 2^k$, where k is an integer in the range $16 \leq k \leq 32$, we train a 2-layer model with $d_{\mathrm{model}} = 1024$ and a vocabulary size of $256$. We select training sequences via the Hull-Dobell theorem, setting $n_a = n_c = 1024$ (See \Cref{appendix:scale_up_fixed} for training details). For the test dataset, we choose 512 values of $a$ and 64 values of $c$ that differ from those in the training set. 

The quality of an LCG largely depends on its multiplier, traditionally evaluated via the spectral test \cite{art_cp}. In \Cref{fig:fixp_scale}(a), we test our model on both spectrally optimal Steele multipliers \cite{steele2021} and arbitrary multipliers for $m = 2^{32}$. While achieving $100\%$ test accuracy with equal in-context sequence lengths, the model performs consistently worse on Steele-generated sequences compared to those from arbitrary multipliers.

In \Cref{fig:fixp_scale}(b), a log-log plot reveals that the number of in-context sequence elements needed for $100\%$ test accuracy scales sublinearly with modulus $m$ as $m^\gamma$, where $\gamma \approx 1/4$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/byte-tokenization/scaling_modulus.pdf}
    \caption{\textbf{\texttt{FM}}: (a) Average test accuracy ($m = 2^{32}$) vs number of in-context sequence elements. (b) The number of in-context sequence elements required to achieve 100\% test accuracy (minimum of 5 runs). (See \Cref{appendix:scale_up_fixed} for details)}
    \label{fig:fixp_scale}
    \vspace{-0.15 in}
\end{figure}

\textbf{Unseen modulus:} For \textbf{\textbf{\texttt{UM}}} case, we train a 6-layer Transformer on a dataset with $n_m = 32,768, n_a=128, n_c=1$, $1024 < m_{\mathrm{train}} < 65536$. The sequence length is 512 and each integer is tokenized as two bytes, resulting in a context length of 1023. As before, we use unseen values of $m_\mathrm{test}$ and $a, c$ for the test dataset -- with a focus on $m_\mathrm{{test}}$ of the form $2^k$ and $3^k$. In \Cref{fig:um_scaling}, we show how the number of in-context sequence elements required to reach $60\%$ test accuracy scales with $m_\mathrm{test}$, approximately as $m_\mathrm{test}^\gamma$, where $0.24 \leq \gamma \leq 0.33$. The averaged test accuracy of each number in the sequence is shown in \Cref{appendix:scale_up_unseen}. Test performance is influenced by the tokenization base, since the tokenization base highlights the periodic structure of LCGs making it more apparent and easier for the model to leverage during training and prediction. To confirm this effect, we train a model with tokenization base = 243. In \Cref{fig:um_scaling} we observe that $m_\mathrm{test}=2^k$ ($m_\mathrm{test}=3^k$) sequences offer favorable scaling when the tokenization base is $256=2^8$ ($243=3^5$).


\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/byte-tokenization/unseen_scaling_final.pdf}
    \caption{\textbf{\texttt{UM}}: The number of in-context sequence elements required to achieve $60\%$ test accuracy shows a sublinear dependence on the modulus $m$. The scaling depends on the compatibility between $m$ and tokenization. (a) base-$2^8$ tokenization, (b) base-$3^5$ tokenization. }
    \label{fig:um_scaling}
    \vspace{-0.05 in}
\end{figure} 

\vspace{-0.5em}
\section{Conclusion}
We have investigated Transformer training on LCG sequences, focusing on fixed modulus training as well as generalization to unseen moduli. In both cases, we have uncovered the algorithm used by the model to solve these tasks and highlighted the model components that implement the steps of the algorithm. We have found that the model finds and utilizes prime factorizations of $m$ and RNS representations of numbers to simplify the sequences and make predictions. We have provided the modified training recipe for scaling up the modulus in both  \textbf{\texttt{FM}} and \textbf{\texttt{UM}} settings, and shown their scaling behaviors.

\textbf{Limitations and future work:} The results of this paper were limited to scales $m \leq 2^{32}$. It would be interesting to test our results on much larger moduli as well. We leave the exploration of PRNGs that are built upon LCGs, such as PCGs and truncated LCGs for future works. It would also be interesting to make the training even more unbiased, by training on general classes of arithmetic sequences. 


\section*{Acknowledgements}

M.B. thanks Carl Miller for discussions on PRNGs. This work is supported by NSF DMR-2345644 (D.S.K., T.T., and M.B.), and by an NSF CAREER award DMR-2045181 (T.H. and D.D.).
The authors acknowledge the University of Maryland supercomputing resources (\url{http://hpcc.umd.edu}) made available for conducting the research reported in this paper. 


% \section*{Impact Statement}

% % This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% Our work advances the understanding of how neural networks learn deterministic sequences, specifically LCGs. While this capability cannot compromise mainstream cryptographic systems, which use far more sophisticated techniques, our insights may contribute to the development of more robust cryptographic algorithms and a better understanding of neural networks' computational capabilities. 

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\newpage

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Details}
\label{appendix:experimental-details}


This section provides further details about model architecture, dataset construction, and optimization. 


\subsection{Dataset Construction} 

\textbf{Fixed Modulus (\texttt{FM}):} Given a modulus $m$, we apply the Hull-Dobell Theorem to determine the possible values of ($a, c$) that maximize the period. We then randomly select $64$ values of $a$ and $c$ to generate the test dataset. To generate the training dataset, we \emph{exlude} these test choices of ($a, c$) and uniformly sample $N = 100,000$ LCG sequences of length $L$ (context length) with $n_a$ values of multipliers and $n_c$ values of increments. For each set of parameters ($a, c$), we sample an LCG sequence with a randomly selected initial seed $x_0$. Note that the training dataset includes sequences with varying periods, while the test data only contains sequences that maximize the period.

\begin{figure*}[!h]
\centering
% Attention KQV
\begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/dataset-composition/test_last_acc_p512_sl257_pmin256_pmax640_na32_ne1_n768_h4_d4_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/dataset-composition/test_last_acc_p512_sl257_pmin256_pmax640_nc32_ne1_n768_h4_d4_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/dataset-composition/test_last_acc_p512_sl257_pmin384_pmax576_np128_ne1_n768_h4_d4_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
\end{minipage}

\caption{The impact of training dataset parameters ($n_m, n_a, n_c$) on unseen modulus task performance.}
\label{fig:dataset-composition}
\end{figure*}



\textbf{Generalization to Unseen Modulus (\texttt{UM}):} 
For the test dataset, we first select a set of test moduli $M_{\text{test}} =  \{m_{\text{test}}\}$ that would be reserved exclusively for evaluation. For each test modulus $m_{\text{test}} \in M_{\text{test}}$, we apply the Hull-Dobell Theorem to determine the values of ($a, c$) that maximize the period. We then randomly select $64$ values of $a$ and $c$ to generate the test dataset. These $64$ ($a, c$) values are not considered while generating the training dataset.

For the training dataset generation, we sample the $n_m$ modulus values from the range $[L, \lfloor 1.2 \max(M_{\text{test}}) \rfloor ]$ while excluding all the values in $M_{\text{test}}$. For each modulus value $m$, we uniformly select $n_a$ multipliers ($0 < a < m$) and $n_c$ increments ($0 \leq c < m$), excluding the ones reserved for testing. For each parameter ($a, c, m$), we generate a sequence of length $L$ using a randomly selected initial seed $x_0$. This results in a total of $N = n_m \times n_a \times n_c$ training sequences. We report that $N =400, 000$ served sufficient from the modulus values considered in this work.


Next, we examine the effect of training dataset composition ($n_m, n_a$, $n_c$) on the performance. \Cref{fig:dataset-composition} shows the test accuracy primarily depends on $n_p$ and marginally on $n_a$ and $n_c$.
Furthermore, we found that $n_m \gtrsim m_{\text{test}} / 4$ yields good generalization performance (result not shown here). Based on this relationship and our target total number of training examples $N$, we sample $n_a = n_c = \sqrt{\frac{N}{m_{\text{test}}/4}}$ values of multipliers and increments. Unless explicitly specified, we use these parameter settings as the default configuration for all experiments.


\subsection{Model Architecture Details}

We consider GPT-style Transformers \cite{radford2019language} with learnable positional embeddings and weight tying \cite{press2017tying}. The model architecture is characterized by the number of blocks (depth), embedding dimension ($d_{\text{model}}$), and number of attention heads ($n_{\text{heads}}$). For most experiments, we use GELU activations, except in \Cref{section:interp}, where we use ReLU activations for better interpretability. 


\subsection{Training Details} 
\label{appendix:optimization-details}

\begin{figure*}[!h]
\centering
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/hparam-space-shrink/test_last_acc_heatmap_np256_p1024_Tn256_N400000_ne1_n768_h6_d6_I1_Tw4096_T100000_B256.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/hparam-space-shrink/test_last_acc_heatmap_np1024_p4096_Tn256_N400000_ne1_n768_h6_d6_I1_Tw4096_T100000_B256.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/hparam-space-shrink/test_last_acc_heatmap_np4096_p16384_Tn256_N400000_ne1_n768_h6_d6_I1_Tw4096_T200000_B256.pdf}
    % \subcaption{}
\end{minipage}

\caption{Heatmap of test accuracy of a $6$ layer Transformer with learning rate and weight decay as the axes. As the modulus is increased from $1024$ to $16,384$ the range of hyperparameters resulting in reasonable accuracy becomes narrow.}
\label{fig:hparams-shrink-appendix}
\end{figure*}




We train the models with Cross-entropy loss using AdamW optimizer \cite{adamwloshchilov2018} with momentum hyperparameters $\beta_1 = 0.9 $ and $ \beta_2 = 0.99$. We implement a linear learning rate warmup over the first $2048$ steps with an initial learning rate of zero and the target learning rate $\eta$. By default, all experiments employ a batch size of $256$. Weight decay is only applied to non-bias parameters.

In the unseen modulus case, we observed that both the optimal target learning rate $\eta$ and weight decay strength $\lambda$ are highly sensitive to minute changes in training dataset properties (modulus $m_{\text{test}}$, number of LCG parameters $n_m$, $n_a$, $n_c$ and total number of examples $N$) and architectural changes (depth and embedding dimension). To determine the optimal hyperpamraters, we scan the learning rates $\eta \in \{3e\text{-}05, 1e\text{-}04, 3e\text{-}04, 1e\text{-}03\}$ and weight decay strengths $\lambda \in \{0.01, 0.1, 1.0, 3.0\}$. 

\subsection{Training Cost}
In \Cref{fig:fixp_scale}, the training of the $m = 2^{32}$ model was conducted using four NVIDIA A100 GPUs, requiring a total of 21.82 hours. The $m = 2^{16}$ model completed training in 4.83 hours under the same hardware setup. Despite having the same model size, the increased context length in the $m=2^{32}$ model led to a significantly higher computational cost.

In \Cref{fig:um_scaling}, both models were trained on a single NVIDIA H100 GPU for 85 hours.


\subsection{Hyperparameter Space Shrinking with increasing modulus}
\label{appendix:hparam-shrinking}
We also report a surprising phenomenon in the unseen modulus case, which we refer to. as the `hyperparameter space shrinking.' As we increase the test modulus $m_{\text{test}}$ while keeping the model architecture fixed, we observe that the optimal learning rate ($\eta$) and weight decay strength ($\lambda$) shift significantly. Moreover, the range of these hyperparameter values that yield reasonable performance becomes increasingly narrow. \Cref{fig:hparams-shrink-appendix} shows this result for a $6$ layer Transformer.




\section{Fixed Modules Training Results}
\label{appendix:fm_train}

For the \textbf{\texttt{FM}} case, we find that a single attention head in one layer is sufficient to solve the task. In \Cref{figapp:accuracy_vs_step_m=2048}, we present the model's training loss and performance across training steps for $m=2048$. Notably, panels (c, f) reveal a significant disparity between training and test loss, indicating a grokking transition during the training process. Similarly, we plot in \Cref{figapp:accuracy_vs_step_m=7776} for similar curves for $m=7776$, where all curves are qualitatively the same.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/fixed_p/app_acc_vs_step_final_m=2048.pdf}
    \caption{Test accuracy and train/test loss for $m=2048=2^{11}$, depth=1. (a,b,c) $n_{\mathrm{heads}}=1$ (d,e,f) $n_{\mathrm{heads}}=4$.}
    \label{figapp:accuracy_vs_step_m=2048}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/fixed_p/app_acc_vs_step_final_m=7776.pdf}
    \caption{Test accuracy and train/test loss for $m=7776=2^5 \cdot 3^5$, depth=1. (a,b,c) $n_{\mathrm{heads}}=1$ (d,e,f) $n_{\mathrm{heads}}=4$.}
    \label{figapp:accuracy_vs_step_m=7776}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Critical Depth for the Unseen Modulus Task}
\label{appendix:critical_depth}

\begin{figure*}[!h]
\centering
\begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/phase-diagrams/test_last_acc_scale_heatmap_np256_p1024_Tn256_N400000_ne1_I1_Tw4096_T100000_B256.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/phase-diagrams/test_last_acc_scale_heatmap_np1024_p4096_Tn256_N400000_ne1_I1_Tw4096_T100000_B256.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/phase-diagrams/test_last_acc_scale_heatmap_np4096_p16384_Tn256_N400000_ne1_I1_Tw4096_T200000_B256.pdf}
\end{minipage}
\caption{Test accuracy heatmaps of with depth and embedding dimensions as the two axes.}
\label{fig:critical-depth-appendix}
\end{figure*}



This section analyzes the depth and embedding dimension requirements for successfully training a Transformer on the unseen modulus task. The experimental details are the same as described in \Cref{appendix:experimental-details}.

We varied depths $ \in \{2,3,4,6,8\}$ and embedding dimensions in $d_{\text{model}} \in \{512,768,1024,1280\}$, with the head dimension fixed to $d_{\text{head}} = 128$. For each depth and width, we scanned learning rates $\eta \in \{3e\text{-}05, 1e\text{-}04, 3e\text{-}04, 1e\text{-}03\}$ and weight decay strengths $\lambda \in \{0.01, 0.1, 1.0, 3.0\}$ to identify the optimal hyperparameters. We report that the optimal learning rate and weight decay strength heavily vary with depth, embedding dimension, and training dataset.
For $m_{\text{eval}} = \{1024, 4096 \}$, the models were trained for $T=100,000$ steps, while for $m_{\text{eval}} = 16, 384$, the models required a longer training for $T = 200,000$ steps. 

\Cref{fig:critical-depth-appendix} shows the test accuracy heatmaps with depth and embedding dimensions as the two axes. These results demonstrate that a minimum depth of $3$ is required to learn the LCG sequence prediction task, with a marginal dependence on embedding dimension. This suggests the unseen modulus task requires a minimal computational depth of three to capture the underlying structure of LCGs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Training Time Interpratibility}
\label{appendix:train-interp}


\begin{figure*}[!h]
\centering
% Attention KQV
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/training-interp/train_last_acc_step_p2048_T0_T2048_Tn512_na128_nc128_ne1_n768_h4_d6_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/training-interp/train_last_acc_step_p2048_T0_T256_Tn512_na128_nc128_ne1_n768_h4_d6_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/training-interp/train_last_acc_step_p2048_T256_T2048_Tn512_na128_nc128_ne1_n768_h4_d6_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
    % \subcaption{}
\end{minipage}

\caption{(left) Comparison of the training accuracy of sequences with different periods relative to the context length $512$, (center) Accuracy when the model is only trained on sequences with period $ < 512$, (right) Accuracy when the model is trained on sequences with period $> 512$.}
\label{fig:train-interp-appendix}
\end{figure*}


In this section, we examine the order in which training sequences with different periods are learned during training. For this experiment, we consider a six-layer Transformer with $4$ heads and an embedding dimension of $768$. For this experiment, we generate sequences of length $512$ using $128$ unseen moduli and $128$ values of $a$ and $c$ each. 
The model is trained with Adam hyperparameters: $\eta = 3 \times 10^{-4}$ $\beta_1 = 0.9, \beta_2 = 0.99$ and weight decay strength $\lambda = 1.0$. 

\Cref{fig:train-interp-appendix}(left) compares the training accuracy of sequences with different periods relative to the context length $512$. We observe that sequences with period $< 512$ are memorized early in training, while the accuracy of long-period sequences coincides with the test accuracy. Next, we perform two more experiments by training the model on datasets consisting of (1) sequences with period $ < 256$, and (2) sequences with period $ > 512$. \Cref{fig:train-interp-appendix}(center, right) show the results of these experiments. The model fails to generalize when trained only on low-period sequences while training only on long-period sequences eliminates grokking.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{More Interpretability}
% \label{appendix:interp}


\section{LCG Properties}
\label{appendix:lcg_properties}

\subsection{The period of the lower $k$-th bit when $m$ is power of 2}\label{appendix:proof1}
In this section, we show that for a sequence of period $\mathcal{T}_m = m = 2^{K}$, the $k$-th lowest digit has a period of $2^k$ along the sequence.
Consider an LCG sequence:
\begin{align}
    x_{t+1} = (a x_t + c) \mod m,
    \label{appendix:lcg-def}
\end{align}
where $m$ is a power of 2, $m$ and $c$ are coprime and $a-1$ is divisible by 4. The period of sequence $x_t$ is m \cite{hull-dobell}. The lower $k$-th bits of $x_t$ is given by:
\begin{align}
    b_{t,k} = \frac{z_{t,k} - z_{t,k-1}}{2^{k-1}},
\end{align}
where $z_{t,k} = x_t \mod 2^k$. Therefore, if $z_{t, k}$ has a period of $2^k$, then the lower $k$ bits also have a period of $2^k$. Below, we show that $z_{t, k}$ has a period of $2^k$.


For an integer $M_t$, we can can re-write $x_t$ as:
\begin{align}
     x_t & = z_{t,k} + M_t 2^k.
\end{align}

Next, we substitute \Cref{appendix:lcg-def} into the definition of $z_{t+1, k}$:

\begin{align}
    z_{t+1, k} &= x_{t+1} \mod 2^k, \nonumber\\
    &=[(a x_t+c) \mod m] \mod 2^k.
    \label{eq:lcg_y}.
\end{align}

As $m$ is divisble by $2^k$, this simplifies to:

\begin{align}
    z_{t+1, k} &= (a z_{t, k} +c) \mod 2^k, \nonumber \\
    &= (a z_{t, k} + a M_t 2^k +c)  \mod 2^k, \nonumber \\
    &=(a z_{t, k} + c) \mod 2^k.
\end{align}

Therefore, $z_{t,k}$ follows its own LCG recurrence with the same $a$ and $c$ but with a reduced modulus $2^k$. Because $2^k$ and $c$ are coprime and $a-1$ is divisible by 4, the period of $z_n$ is $2^k$. Thus, the period of the lower $k$ bits is $2^k$.


Since $z_{n,k}$ has period $2^k$ and $z_{n,k-1}$ has period $2^{k-1}$, the period of $b_{n,k}$ is $2^k$.
\subsection{Derivation of \Cref{eq:period_m_composite}}\label{appendix:proof2}

We now derive \Cref{eq:period_m_composite} of the main text:

\begin{align*}
    x_t \;\mathrm{mod}\; p_i^{w_q} = \alpha_{i,0,t}\, p_i^0 + \alpha_{i,1,t}\, p_i^1 + \cdots + \alpha_{i,w_i-1,t}\, p_i^{w_i-1},
    % \begin{cases}
    % x_n \;\mathrm{mod}\; p_1^{w_1} = \alpha_{1,0,n}\, p_1^0 + \alpha_{1,1,n}\, p_1^1 + \cdots + \alpha_{1,w_1-1,n}\, p_1^{w_1-1} \\
    % x_n \;\mathrm{mod}\; p_2^{w_2} = \alpha_{2,0,n}\, p_2^0 + \alpha_{2,1,n}\, p_2^1 + \cdots + \alpha_{2,w_2-1,n}\, p_2^{w_2-1} \\
    % \qquad \vdots \\
    % x_n \;\mathrm{mod}\; p_q^{w_q} = \alpha_{q,0,n}\, p_q^0 + \alpha_{q,1,n}\, p_q^1 + \cdots + \alpha_{q,w_q-1,n}\, p_q^{w_q-1}
    % \end{cases}
\end{align*}
where $\alpha_{i,w,t} \in \{ 0, 1, \dots, p_i - 1 \}$ are base-$p_i$ digits. When the period of $x_t$ is m, each digit $\alpha_{j,w,t}$ has a period of $p_i^w$.

Consider an LCG sequence:
\begin{align}
    x_{t+1} = (a x_t + c) \mod m,
    \label{eq:appendix_lcg_3_1}
\end{align}
where $m$ has a prime factorization $m = p_1^{w_1} p_2^{w_2}\cdots p_q^{w_q}$, $m$ and $c$ are coprime, $a-1$ is divisible by all prime factors of $m$ and $a-1$ is divisible by 4 if $m$ is divisible by 4 \cite{hull-dobell}.

Consider the residulas $R_{i,t} = x_t \mod p^{w_i}_i$, where $i\in\{1\dots q\}$. We have:
\begin{align}
    x_t = R_{i,t} + M_{i,t} p^{w_i}_i,
    \label{eq:appendix_lcg_3_2}
\end{align}
where $M_{i,t}$ is an integer.

Substituting \Cref{eq:appendix_lcg_3_1,eq:appendix_lcg_3_2} into the definition of $R_{i, t+1}$:
\begin{align}
R_{i,t+1} &= x_{t+1} \mod p^{w_i}_i, \nonumber\\
&=[(a x_t + c) \mod m]\mod p^{w_i}_i,\nonumber\\
&=(a x_t + c) \mod p^{w_i}_i, \nonumber \\
&=(a R_{i, t} + a M_{i,t} p^{w_i}_i + c) \mod p^{w_i}_i, \nonumber \\
&=(a R_{i,t} +c ) \mod p^{w_i}_i.
\end{align}

Next we consider $z_{i,k,t}$, which is the lower $k$ base-$p_i$ digits of $R_{i,t}$:
\begin{align}
    z_{i,k,t} = R_{i,t} \mod  p_i^k.
\end{align}
Similarly, the recurrence of $z_{i,k,t}$ can be simplied as:
\begin{align}
    z_{i,k,t+1} &= R_{i,t+1} \mod  p_i^k, \nonumber \\
    & = [(a R_{i,t} + c) \mod p_i^{w_i}]\mod p_i^k, \nonumber \\
    & = (a R_{i,t} + c) \mod p_i^k, \nonumber \\
    & = (a z_{i,k,t} + c) \mod p_i^k.
\end{align}
$z_{i,k,t}$ follows an LCG recurrence with the same $a$ and $c$ and a reduced modulus $p^k_i$. Because $p^k_i$ and $c$ are coprime and $a-1$ is divisible by $p_i$, the period of $z_{i,k,t}$ is $p^{k}_i$.

$\alpha_{i,k,t}$, which is the lower $k$-th base-$p_i$ digits of $y_{i,t}$, can be written as:
\begin{align}
    \alpha_{i,k,t} = \frac{z_{i,k,t} - z_{i,k-1,t}}{p_i^{k-1}}.
\end{align}
Since $z_{i,k,t}$ has period $p^{k}_i$, the period of $\alpha_{i,k,t}$ is $p^{k}_i$.



\section{Fixed Modulus Interpretability}
\label{appendix:fixed_interp}
In this subsection, we show additional results on the model's behavior on the \textbf{\texttt{FM}} task.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fixed_p/digits_ablation_p2048_d1_h1_n768.pdf}
    \caption{Per-digit test accuracies in binary representation $m=2048=2^{11}$, depth=1, $n_{\mathrm{heads}}=1, d_{\mathrm{model}}=768$, averaged over $a, c$ and initial seeds. (a) Original model. (b) For every query in the attention matrix, mask out all keys except the one with the highest attention score (during inference). (c) For every query in the attention matrix, mask out all keys except ones with the top 2 attention scores.}
    \label{figapp:digits_topk}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.96\linewidth]{figures/fixed_p/app_acc_vs_token_ac_final_m=2048.pdf}
    \caption{Test accuracy vs token positions for $m=2048=2^{11}$, depth=1, various values of $a, c$. (a,b) $n_{\mathrm{heads}}=1$ (c,d) $n_{\mathrm{heads}}=4$. The accuracies for different $a$ and $c$ are exactly on top of each other.}
    \label{figapp:accuracy_vs_token_ac_m=2048}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.96\linewidth]{figures/fixed_p/app_2048_accuracy.pdf}
    \caption{Test accuracy for $m=2048=2^{11}$, depth=1, $n_{\mathrm{heads}} \in \{1, 4\}, d_{\mathrm{model}}=768$. (a,d) Test accuracy averaged over $a, c$ and initial seeds. (b,e) Multiplication of per-digit test accuracies -- matches exactly with the average test accuracy. (c,f) Per-digit accuracy in binary representation.}
    \label{figapp:accuracy_m=2048_h1}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fixed_p/app_7776_accuracy.pdf}
    \caption{Test accuracy for $m=7776=2^5\, 3^5$, depth=1, $n_{\mathrm{heads}} \in \{1, 4\}, d_{\mathrm{model}}=768$. (a,d) Test accuracy averaged over $a, c$ and initial seeds. (b,e) Multiplication of per-digit test accuracies -- matches exactly with the average test accuracy. (c,f) Per-digit accuracy in RNS representation.}
    \label{figapp:accuracy_m=7776}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/fixed_p/app_embeddings_m=512.pdf}
    \caption{Projections along Top 6 principal components of the embedding matrix, for $m=512$, depth=1, $n_{\mathrm{heads}}=1, d_{\mathrm{model}}=768$.}
    \label{figapp:embedding_m=512}
\end{figure}



\clearpage
\section{Unseen Modulus Interpretability}
\label{appendix:unseen}

\subsection{Extra PCA plots for first layer heads}
\label{appendix:unseen_pca}

We present additional PCA analyses of attention heads shown in \Cref{fig:multip_prune}, examining $\mathrm{PCA}(\bm{H}^{(h)}[:, t, :])$ for each head $h$ across various combinations of $a$, $c$, $x_0$, $m_{\mathrm{test}}$, and position $t$. The results are depicted in \Cref{fig:pca_appendix}, where we include two additional first-layer heads (heads 2 and 3) that are responsible for performing operations related to modulo $5$. Notably, with different choices of $a$, $c$, $m_{\mathrm{test}}$ and $t$ compared to \Cref{fig:multip_prune} (a1, b1, c1), the clustering behavior remains unchanged.

Note the emergence of attention heads dedicated to processing modulo $2$, $3$, $5$, and $7$ can likely be attributed to the prevalence of these prime factors in the training set.

\begin{figure}[!h]
    \centering
    \subfloat [$a=13$, $c=3$, $m_{\mathrm{test}}=2048$, $t=0$]{\includegraphics[width=0.96\linewidth] {figures/multi_interp/pcas/cps2048_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_relu_I1_lr0.000100_Tw2048_T100000_B256_wd1.0_attn_pca_comp01_a13c3_shifts0_l1.pdf}
    } \\
    \subfloat [$a=9$, $c=3$, $m_{\mathrm{test}}=2048$, $t=128$]{\includegraphics[width=0.96\linewidth] {figures/multi_interp/pcas/cps2048_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_relu_I1_lr0.000100_Tw2048_T100000_B256_wd1.0_attn_pca_comp01_a9c3_shifts128_l1.pdf}
    } \\
    \subfloat [$a=61$, $c=7$, $m_{\mathrm{test}}=1800$, $t=47$]{\includegraphics[width=0.96\linewidth] {figures/multi_interp/pcas/cps1800_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_relu_I1_lr0.000100_Tw2048_T100000_B256_wd1.0_attn_pca_comp01_a61c7_shifts47_l1.pdf}
    }
    \caption{PCA analysis of first-layer heads. Although the ordering of numbers varies, the grouping behavior discussed in \Cref{sec:interp_unseen} remains invariant to changes in $a$, $c$, $m$, $x_0$, and $t$. Note that the last head is the head that appears in \Cref{fig:multip_cosine}, which does not exhibit a strong grouping bias.
    }
    \label{fig:pca_appendix}
\end{figure}

\subsection{Pruning heads corresponding to irrelevant prime factors} 
\label{appendix:prune}

To further validate our findings from \Cref{fig:multip_prune} in \Cref{sec:interp_unseen} regarding the correlation between attention heads and digit-wise accuracy, we conduct additional pruning experiments. The results of these experiments are presented in \Cref{fig:prune_appendix_2048,fig:prune_appendix_2352}.

\begin{figure}[!h]
    \centering
    \subfloat[Original Model] {\includegraphics[width=0.32\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \subfloat[Prune modulo $3$ head] {\includegraphics[width=0.32\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_l0h4_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \subfloat[Prune modulo $14$ head] {\includegraphics[width=0.32\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_l0h0_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \caption{Per-digit accuracy for $m_{\mathrm{test}}=2048$ after pruning specific attention heads. (a) Results from the same model used in \Cref{sec:interp_unseen}, identical to \Cref{fig:multip_prune} (b2); (b) Performance after pruning the attention head responsible for grouping numbers by their values modulo $3$, which is irrelevant for solving sequences with $m_{\mathrm{test}}$ (containing only the prime factor $2$). After pruning, the model's performance shows marginal improvement for specific early bits at lower token positions; (c) Performance after pruning the attention head responsible for grouping numbers by their values modulo $14$. The model's performance on $m_{\mathrm{test}}$ decreases significantly, as this head partially contributes to processing relevant prime factors. However, since the primary head responsible for binary representation remains intact, the model maintains partial functionality.}
    \label{fig:prune_appendix_2048}
\end{figure}

\begin{figure}[!h]
    \centering
    \subfloat[Original Model] {\includegraphics[width=0.31\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_p2352_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \subfloat[Prune modulo $3$ head] {\includegraphics[width=0.31\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_l0h4_p2352_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \subfloat[Prune modulo $2$ head] {\includegraphics[width=0.31\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_l0h3_p2352_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \caption{Per-digit accuracy for $m_{\mathrm{test}}=2352$ after pruning specific attention heads. (a) Results from the same model used in \Cref{sec:interp_unseen}, identical to \Cref{fig:multip_prune} (b2); (b) Performance after pruning the attention head responsible for grouping numbers by their values modulo $3$, which is relevant for one specific digit in this case. After pruning, the model's performance shows a clear degradation, with the strongest one happening exactly at the digit corresponding to modulo $3$ (c) Performance after pruning the attention head responsible for grouping numbers by their values modulo $2$. The model's performance on $m_{\mathrm{test}}$ got obliterated, as there are many base-$2$ digits in the RNS representation of $m_{\mathrm{test}}$ in this case.}
    \label{fig:prune_appendix_2352}
\end{figure}


\subsection{Patching other heads} 
\label{appendix:patch}

In \Cref{fig:patch_appendix}, we present patching experiments for heads not previously shown in \Cref{fig:multip_cosine}. We focus only on selected heads where patching significantly affects predictions. Notably, none of these cases exhibit qualitative changes stemming from modulus alterations, further supporting our assertion that the head shown in \Cref{fig:multip_cosine} is specifically responsible for estimating $m_{\mathrm{test}}$.

\begin{figure}[!h]
    \centering
    \subfloat[layer 1, head 4] {\includegraphics[width=0.48\linewidth]{figures/multi_interp/patch_appendix/newp1024a5c31_layer1_h4_p1024_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_T100000_B256_wd1.0.pdf}}
    \subfloat[layer 2, head 4]{\includegraphics[width=0.48\linewidth]{figures/multi_interp/patch_appendix/newp1024a5c31_layer2_h4_p1024_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_T100000_B256_wd1.0.pdf}} \\
    \subfloat[layer 3, head 6]{\includegraphics[width=0.48\linewidth]{figures/multi_interp/patch_appendix/newp1024a5c31_layer3_h6_p1024_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_T100000_B256_wd1.0.pdf}} 
    \subfloat[layer 4, head 1]{\includegraphics[width=0.48\linewidth]{figures/multi_interp/patch_appendix/newp1024a5c31_layer4_h1_p1024_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_T100000_B256_wd1.0.pdf}}
    \caption{Patching experiments following the setting of \Cref{fig:multip_cosine} in the main text. None of these heads, after patching, make the model believe that the modulus is close to $m_{\mathrm{patch}}$.
    }
    \label{fig:patch_appendix}
\end{figure}

\subsection{In-accurate estimation of $m_{\mathrm{test}}$} 
\label{appendix:estimate}

From the cosine-similarity panel in \Cref{fig:multip_cosine}, we observe that the model's estimation approximates the target $m_{\mathrm{test}}=2048$. However, detailed analysis reveals that the highest cosine-similarity occurs at $m_{\mathrm{est}}=2033=19 \cdot 107$, with neighboring values exhibiting similarly high cosine-similarity values. If we assume $m_{\mathrm{est}}$ represents the model's internal belief, then the prime representation would consist solely of powers of $19$ and $107$. Such a representation can only produce periodic structures for the $k$-th bit of a binary number when $r=\lcm(19, 2^k)$. Consequently, patterns before reaching that period would appear random in this representation, providing a weaker signal compared to the correct representation. This explains why the model preferentially selects binary representation for lower bits when $m_{\mathrm{test}}=2048$.

For higher bits, the low-bit representation can be determined up to $2^k$ bits through copying. If the model utilizes this information in later stages, the precision of $m_{\mathrm{est}}$ can be drastically improved. Specifically, when $\lfloor{m_{\mathrm{est}}} / 2^k \rceil = \lfloor{m_{\mathrm{test}}} / 2^k \rceil$, the model can identify the correct representation as long as $|m_{\mathrm{test}} - m_{\mathrm{est}}| < 2^k$. This argument can be extended to any other composite $m_{\mathrm{test}}$. Note that this argument is hypothetical; further proof of this mechanism remains in future work.


\subsection{Evidence for Step iii} 
\label{appendix:step3}

In \Cref{fig:attn_stats}, we present attention patterns and token distance statistics for a selected attention head in later layers, with the same model as the one used in \Cref{sec:interp_unseen}. The token distances are measured by the spacing between keys with top-$4$ attention weights for a given query. Our analysis reveals that the model develops a head capable of dynamically adjusting their lookback distance for computations based on $m_{\mathrm{test}}$, which we interpret as evidence for \textbf{step iii} of the algorithm proposed in \Cref{sec:interp_unseen}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/multi_interp/attn_stats_compare.pdf}
    \caption{Attention patterns and token distance statistics for layer 2, head 3 of the model analyzed in \Cref{sec:interp_unseen}. (a1, b1) Results for a sequence with $m_{\mathrm{test}}=2048$, $a=5$, and $c=31$. The statistics reveal that the model consistently looks back at distances that are multiples of $4$, which divides $m_{\mathrm{test}}$ and enables the correct copy behavior. (a2, b2) Results for a sequence with $m_{\mathrm{test}}=2352$, $a=85$, and $c=5$. In contrast to panels (a1, b1), the same attention head now consistently looks back at distances that are multiples of $14$, which allows the model to copy lower digits from the context. This adaptive behavior demonstrates that the model has acquired the ability to dynamically adjust its lookback distance by $r$ iterations to copy the lower digits and leave the higher digits to the later layers for computation.}

    \label{fig:attn_stats}
\end{figure}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scaling Up the Modulus}
\label{appendix:scaleup}





\subsection{Base-b tokenization}
\label{appendix:base-b}
To convert an integer \( x \) to base \( b \) with the least significant digit (LSD) first, repeatedly divide \( x \) by \( b \), storing remainders:

\begin{equation}
    d_k = x \mod b, \quad x = \lfloor x / b \rfloor.
\end{equation}

Stop when \( x = 0 \). The sequence \( (d_0, d_1, \dots) \) is the base-\( b \) representation in LSD-first order.
% \begin{equation}
%     x = d_0b^0 + d_1b^1 + ...
% \end{equation}

Example: Converting \( x = 3,214,748,365 \) to base 256:

\[
\begin{aligned}
    3,214,748,365 \div 256 &= 12,557,610 \text{ remainder } 205, \quad d_0 = 205, \\
    12,557,610 \div 256 &= 49,053 \text{ remainder } 42, \quad d_1 = 42, \\
    49,053 \div 256 &= 191 \text{ remainder } 157, \quad d_2 = 157, \\
    191 \div 256 &= 0 \text{ remainder } 191, \quad d_3 = 191.
\end{aligned}
\]

The final LSD-first representation consists of four tokens:
\[
(205, 42, 157, 191)_{256}.
\]
Each token is then one-hot encoded into a $b$-dimensional vector, where only the index corresponding to the token value is set to 1. These one-hot vectors are then fed into a token embedding layer with an embedding dimension of $d_{model}=1024$.


\subsection{Abacas Embeddings}
\label{appendix:abacus}
The positional embedding for the \( j \)-th lower digit of the \( i \)-th number in the sequence is defined as:

\begin{align}
    \mathrm{PosEmbed}(T_{i,j}) = E_{\mathrm{int}}(i) + E_{\mathrm{digit}}(j)
\end{align}

where \( T_{i,j} \) represents the token corresponding to the \( j \)-th digit of the \( i \)-th integer.

\begin{itemize}
    \item \( E_{\mathrm{int}}(i), E_{\mathrm{digit}}(j) \in \mathbb{R}^{d_{\text{model}}} \) are learnable embeddings.
    \item \( E_{\mathrm{int}}(i) \) encodes the integer’s position in the sequence.
    \item \( E_{\mathrm{digit}}(j) \) encodes the digit’s relative position within the integer.
\end{itemize}

This embedding scheme ensures that each token captures both the integer's global position and the byte's local position.

\subsection{Fixed Modulus}\label{appendix:scale_up_fixed}

For each modulus $m = 2^k$, where  $k \in [16, 32]$, we train a 2-layer GPT model with an embedding dimension of 1024 and a vocabulary size 256. The train set consists of $n_a = 1024$ multipliers and $n_c = 1024$ increments, selected via the Hull-Dobell theorem. One LCG sequence of length 512 is included in the train set for each (a, c) pair, resulting in a total training set size of $n_a \times n_c = 1,048,576$. For each modulus, the model is trained for 200,000 steps with a batch size of 512. The context length is $512 \times $ the digit length of $m$ in the byte representation $-1$. For $m = 2^{32}$, the digit length in byte representation is 4; therefore, the context length is 2047. Training was performed using 4 A100 GPUs over a total duration of 21.82 hours. For $m = 65536$, the digit length in byte representation is 2, resulting in a context length of 1023, with training taking 4.83 hours. For each modulus, the test set includes 512 unseen $a$ values and 64 unseen $c$ values selected via the Hull-Dobell theorem.

The model may converge to different solutions depending on two random seeds: one for model initialization and batch shuffling, and another for dataset generation. \Cref{{fig:med_fixp_appendix}} shows the median performance across five runs, with the shaded region representing the range between the minimum and maximum values. For larger moduli, not all models successfully find a solution that achieves $100\%$ test accuracy.


\begin{figure}[!h]
    \centering
    \subfloat[PyTorch seed = 9, dataset seed = 71]{
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p1048576_lr0.0001_wd1_na1024_nc1024_ms9_ds71_Twarm5000_T200000.pdf}}
    \subfloat[PyTorch seed = 10, data seed = 71]{
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p1048576_lr0.0001_wd1_na1024_nc1024_ms10_ds71_Twarm5000_T200000.pdf}}
    \subfloat[PyTorch seed = 11, data seed = 71]{
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p1048576_lr0.0001_wd1_na1024_nc1024_ms11_ds71_Twarm5000_T200000.pdf}}

    \subfloat[PyTorch seed = 11, data seed = 71]{
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p1048576_lr0.0001_wd1_na1024_nc1024_ms11_ds71_Twarm5000_T200000.pdf}}
    \subfloat[PyTorch seed = 11, data seed = 72]{
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p1048576_lr0.0001_wd1_na1024_nc1024_ms11_ds72_Twarm5000_T200000.pdf}}
    \subfloat[PyTorch seed = 11, data seed = 73]{
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p1048576_lr0.0001_wd1_na1024_nc1024_ms11_ds73_Twarm5000_T200000.pdf}}
    \caption{Test accuracy vs. Number index for $m=2^{20}$. First Row: Three models trained on the same dataset, each using a unique PyTorch random seed that controls model initialization and batch shuffling. Second Row: Three models trained on different datasets, with each dataset generated using a unique random seed controlling NumPy randomness for sampling $a$, $c$, and $x_0$. All models converged to solutions that achieved and sustained 100\% test accuracy, but differed in the number of in-context examples required to reach this performance.}
    \label{appendix:seeds_20}
    
\end{figure}


\begin{figure}[!h]
    \centering
    \subfloat[PyTorch seed = 9, dataset seed = 71]{\includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p4294967296_lr0.0001_wd1_na1024_nc1024_ms9_ds71_Twarm5000_T200000.pdf}}
    \subfloat[PyTorch seed = 10, dataset seed = 71]{
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p4294967296_lr0.0001_wd1_na1024_nc1024_ms10_ds71_Twarm5000_T200000.pdf}}
    \subfloat[PyTorch seed = 11, dataset seed = 71]{ 
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p4294967296_lr0.0001_wd1_na1024_nc1024_ms11_ds71_Twarm5000_T200000.pdf}}

    \subfloat[PyTorch seed = 11, dataset seed = 71]{ 
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p4294967296_lr0.0001_wd1_na1024_nc1024_ms11_ds71_Twarm5000_T200000.pdf}}
    \subfloat[PyTorch seed = 11, dataset seed = 72]{     
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p4294967296_lr0.0001_wd1_na1024_nc1024_ms11_ds72_Twarm5000_T200000.pdf}}
    \subfloat[PyTorch seed = 11, dataset seed = 73]{         
    \includegraphics[width=0.33\linewidth]{figures/byte-tokenization/replot_p4294967296_lr0.0001_wd1_na1024_nc1024_ms11_ds73_Twarm5000_T200000.pdf}}
    \caption{Test accuracy vs Number index for $m=2^{32}$. First Row: Three models trained on the same dataset, each using a unique PyTorch random seed that controls model initialization and batch shuffling. Second Row: Three models trained on different datasets, with each generated using a unique random seed controlling NumPy randomness for sampling $a$, $c$, and $x_0$. Only one of the five models found a solution that achieved and sustained 100\% test accuracy.}
    \label{appendix:same_ds}
    
\end{figure}






\begin{figure}[!h]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/byte-tokenization/med_lcg_in_context_examples.pdf}
    \caption{Median number of in-context sequence elements required to achieve 100\% test accuracy across five runs. The shaded region represents the min-max range.}

    \label{fig:med_fixp_appendix}
\end{figure}

\subsection{Unseen Modulus}\label{appendix:scale_up_unseen}
We train a 6-layer GPT model on a dataset that comprises $n_m = 32,768$ moduli, with $n_a = 128$ training $a$ values and $n_c = 1$ training $c$ values per modulus. This results in a total of $n_m \times n_a \times n_c = 4,194,304$ sequences, each of length 512 in the training set. 
In \Cref{fig:acc_um_appendix}a, where the tokenization base is 256, $1024<m_\mathrm{train}<65536$. In \Cref{fig:acc_um_appendix}b, where the tokenization base is 243, $1024<m_\mathrm{train}<59049$.
Multipliers are selected based on the Hull-Dobell theorem when sufficient qualifying $a$ values are available; otherwise, random $a$ values are used to ensure $128$ multipliers for each modulus.
The models with approximately 76M parameters were trained on 4 million sequences over 2 million steps, using a batch size of 128 on a single H100 GPU for 85.47 hours.
Because LCGs are typically defined for moduli that are powers of prime numbers, the model is tested on moduli that are powers of the primes 2, 3, 5, and 7. The test set consists of 512 unseen $a$ values and 64 unseen $c$ values selected via the Hull-Dobell theorem for each test modulus. 

Test performance is influenced by the tokenization base, exhibiting a bias toward moduli that share the same base as the tokenization method. For instance, in \Cref{fig:acc_um_appendix} (a) when using a byte-level representation, the model achieves better performance on moduli $m_\mathrm{test} = 2^k$ compared to $m_\mathrm{test} = 3^k$, $5^k$, or $7^k$. As contrasted with \Cref{fig:acc_um_appendix} (b) where the tokenization base is $243 = 3^5$, the model performs better on moduli $m = 3^k$. This behavior is likely due to the property of LCGs, where for moduli that are powers of a prime $b$, the lowest $k$-th digit exhibits a period of $b^k$. Tokenization in such a base highlights this periodic structure, making it more apparent and easier for the model to leverage during training and prediction. 

\begin{figure}[!h]
    \centering
    \subfloat[Tokenization base = $256 = 2^8$] {\includegraphics[width=0.4\linewidth]{figures/byte-tokenization/replot_acc_curve_b256_d2_prime2357_512testa_64testc.pdf}}
    \subfloat[Tokenization base = $243 = 3^5$] {\includegraphics[width=0.4\linewidth]{figures/byte-tokenization/replot_acc_curve_b243_d2_prime2357_512testa_64testc.pdf}}
    \caption{Test accuracy vs Number index. In (a), the moduli 2048 and 16384 (blue curves) have the same root 2 as the tokenization base 256. The model performs better on these two moduli. In (b), the moduli 2178 and 19683 (orange curves) have the same root 3 as the tokenization base 243. The model performs better on these two moduli.  }
    \label{fig:acc_um_appendix}
\end{figure}





\end{document}




% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
