@inproceedings{Ahn2023gradient,
    author = {Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
    pages = {45614--45650},
    publisher = {Curran Associates, Inc.},
    title = {Transformers learn to implement preconditioned gradient descent for in-context learning},
    url={https://openreview.net/forum?id=LziniAXEI9},
    volume = {36},
    year = {2023},
}

@article{Bouillaguet_Martinez_Sauvage_2020, title={Practical seed-recovery for the PCG Pseudo-Random Number Generator}, volume={2020},  url={https://tosc.iacr.org/index.php/ToSC/article/view/8700},  DOI={10.13154/tosc.v2020.i3.175-196}, abstractNote={The Permuted Congruential Generators (PCG) are popular conventional (non-cryptographic) pseudo-random generators designed in 2014. They are used by default in the NumPy scientific computing package. Even though they are not of cryptographic strength, their designer stated that predicting their output should nevertheless be &amp;quot;challenging&amp;quot;.In this article, we present a practical algorithm that recovers all the hidden parameters and reconstructs the successive internal states of the generator. This enables us to predict the next “random” numbers, and output the seeds of the generator. We have successfully executed the reconstruction algorithm using 512 bytes of challenge input; in the worst case, the process takes 20 000 CPU hours.This reconstruction algorithm makes use of cryptanalytic techniques, both symmetric and lattice-based. In particular, the most computationally expensive part is a guessand-determine procedure that solves about 252 instances of the Closest Vector Problem on a very small lattice.}, number={3}, journal={IACR Transactions on Symmetric Cryptology}, author={Bouillaguet, Charles and Martinez, Florette and Sauvage, Julia}, year={2020}, month={Sep.}, pages={175–196} }

@inproceedings{ONeill2014PCGA,
  title={PCG : A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation},
  author={Melissa E. O'Neill},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:3489282}
}

@misc{akyurek2023what,
    title={What learning algorithm is in-context learning? Investigations with linearmodels},
    author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=0g0X4H8yN4I}
}

@article{allenzhu2024physicslanguagemodels1,
      title={Physics of Language Models: Part 1, Learning Hierarchical Language Structures}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2024},
      eprint={2305.13673},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13673}, 
}

@INPROCEEDINGS{amigo2021,
  author={Amigo, Glauco and Dong, Liang and Marks Ii, Robert J.},
  booktitle={2021 15th International Conference on Signal Processing and Communication Systems (ICSPCS)}, 
  title={Forecasting Pseudo Random Numbers Using Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  keywords={Deep learning;Training;Histograms;Software algorithms;Neural networks;Signal processing;Generators;Markov Chains;Piece-wise Continuous;Linear Congruential Generator;Pseudo Random Numbers;Deep Neural Networks;Nonparametric Forecasting},
  doi={10.1109/ICSPCS53099.2021.9660301}}

@article{cagnetta2024deep,
  title={How deep neural networks learn compositional data: The random hierarchy model},
  author={Cagnetta, Francesco and Petrini, Leonardo and Tomasini, Umberto M and Favero, Alessandro and Wyart, Matthieu},
  journal={Physical Review X},
  volume={14},
  number={3},
  pages={031001},
  year={2024},
  publisher={APS}
}

@article{cagnetta2024towards,
  title={Towards a theory of how the structure of language is acquired by deep neural networks},
  author={Cagnetta, Francesco and Wyart, Matthieu},
  journal={arXiv preprint arXiv:2406.00048},
  year={2024}
}

@article{chomsky1956three,
  title={Three models for the description of language},
  author={Chomsky, Noam},
  journal={IRE Transactions on information theory},
  volume={2},
  number={3},
  pages={113--124},
  year={1956},
  publisher={IEEE}
}

@misc{deletang2023chomsky,
      title={Neural Networks and the Chomsky Hierarchy}, 
      author={Grégoire Delétang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A. Ortega},
      year={2023},
      eprint={2207.02098},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.02098}, 
}

@inproceedings{garg2023simple,
      title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes}, 
      author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
      year={2023},
      eprint={2208.01066},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gromov2022grokking,
    doi = {10.48550/ARXIV.2301.02679},
    url = {https://arxiv.org/abs/2301.02679},
    author = {Gromov, Andrey},
    keywords = {Machine Learning (cs.LG), Disordered Systems and Neural Networks (cond-mat.dis-nn), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
    title = {Grokking modular arithmetic},
    publisher = {arXiv},
    year = {2023},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{gu2024fourier,
      title={Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic}, 
      author={Jiuxiang Gu and Chenyang Li and Yingyu Liang and Zhenmei Shi and Zhao Song and Tianyi Zhou},
      year={2024},
      eprint={2402.09469},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hendel2023incontext,
      title={In-Context Learning Creates Task Vectors}, 
      author={Roee Hendel and Mor Geva and Amir Globerson},
      year={2023},
      eprint={2310.15916},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lam2023learning,
  title={Learning skillful medium-range global weather forecasting},
  author={Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and others},
  journal={Science},
  volume={382},
  number={6677},
  pages={1416--1421},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{liu2022towards,
  title={Towards understanding grokking: An effective theory of representation learning},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34651--34663},
  year={2022}
}

@misc{liu2024incontextvector,
      title={In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering}, 
      author={Sheng Liu and Haotian Ye and Lei Xing and James Zou},
      year={2024},
      eprint={2311.06668},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{nanda2023progress,
    title={Progress measures for grokking via mechanistic interpretability},
    author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=9XFSbDPmdW}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@inproceedings{rivest1991cryptography,
  title={Cryptography and machine learning},
  author={Rivest, Ronald L},
  booktitle={International Conference on the Theory and Application of Cryptology},
  pages={427--439},
  year={1991},
  organization={Springer}
}

@article{sharkey2025interp,
      title={Open Problems in Mechanistic Interpretability}, 
      author={Lee Sharkey and Bilal Chughtai and Joshua Batson and Jack Lindsey and Jeff Wu and Lucius Bushnaq and Nicholas Goldowsky-Dill and Stefan Heimersheim and Alejandro Ortega and Joseph Bloom and Stella Biderman and Adria Garriga-Alonso and Arthur Conmy and Neel Nanda and Jessica Rumbelow and Martin Wattenberg and Nandi Schoots and Joseph Miller and Eric J. Michaud and Stephen Casper and Max Tegmark and William Saunders and David Bau and Eric Todd and Atticus Geiger and Mor Geva and Jesse Hoogland and Daniel Murfet and Tom McGrath},
      year={2025},
      eprint={2501.16496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.16496}, 
}

