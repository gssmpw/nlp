@inproceedings{attn_is_all_you_need,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{cagnetta2024towards,
  title={Towards a theory of how the structure of language is acquired by deep neural networks},
  author={Cagnetta, Francesco and Wyart, Matthieu},
  journal={arXiv preprint arXiv:2406.00048},
  year={2024}
}

@inproceedings{rivest1991cryptography,
  title={Cryptography and machine learning},
  author={Rivest, Ronald L},
  booktitle={International Conference on the Theory and Application of Cryptology},
  pages={427--439},
  year={1991},
  organization={Springer}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}
@inproceedings{
adamwloshchilov2018,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}


@article{allenzhu2024physicslanguagemodels1,
      title={Physics of Language Models: Part 1, Learning Hierarchical Language Structures}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2024},
      eprint={2305.13673},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13673}, 
}

@article{sharkey2025interp,
      title={Open Problems in Mechanistic Interpretability}, 
      author={Lee Sharkey and Bilal Chughtai and Joshua Batson and Jack Lindsey and Jeff Wu and Lucius Bushnaq and Nicholas Goldowsky-Dill and Stefan Heimersheim and Alejandro Ortega and Joseph Bloom and Stella Biderman and Adria Garriga-Alonso and Arthur Conmy and Neel Nanda and Jessica Rumbelow and Martin Wattenberg and Nandi Schoots and Joseph Miller and Eric J. Michaud and Stephen Casper and Max Tegmark and William Saunders and David Bau and Eric Todd and Atticus Geiger and Mor Geva and Jesse Hoogland and Daniel Murfet and Tom McGrath},
      year={2025},
      eprint={2501.16496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.16496}, 
}

@article{lam2023learning,
  title={Learning skillful medium-range global weather forecasting},
  author={Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and others},
  journal={Science},
  volume={382},
  number={6677},
  pages={1416--1421},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{chomsky1956three,
  title={Three models for the description of language},
  author={Chomsky, Noam},
  journal={IRE Transactions on information theory},
  volume={2},
  number={3},
  pages={113--124},
  year={1956},
  publisher={IEEE}
}

@article{cagnetta2024deep,
  title={How deep neural networks learn compositional data: The random hierarchy model},
  author={Cagnetta, Francesco and Petrini, Leonardo and Tomasini, Umberto M and Favero, Alessandro and Wyart, Matthieu},
  journal={Physical Review X},
  volume={14},
  number={3},
  pages={031001},
  year={2024},
  publisher={APS}
}

@article{hull-dobell,
author = {Hull, T.  E. and Dobell, A. R.},
title = {Random Number Generators},
journal = {SIAM Review},
volume = {4},
number = {3},
pages = {230-254},
year = {1962},
doi = {10.1137/1004061},

URL = { 
        https://doi.org/10.1137/1004061
},
eprint = { 
        https://doi.org/10.1137/1004061
}

}




@book{euler1736theorem,
  title={Commentarii Academiae scientiarum imperialis Petropolitanae},
  author={Leonhard Euler},
  url={https://books.google.com/books?id=-ssVAAAAYAAJ},
  year={1736},
  publisher={Typis Academiae}
}


@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{press2017tying,
  title={Using the Output Embedding to Improve Language Models},
  author={Press, Ofir and Wolf, Lior},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={157--163},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{wei2023chain,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@article{thomson1958modified,
    author = {Thomson, W. E.},
    title = {A Modified Congruence Method of Generating Pseudo-random Numbers},
    journal = {The Computer Journal},
    volume = {1},
    number = {2},
    pages = {83-83},
    year = {1958},
    month = {01},
    issn = {0010-4620},
    doi = {10.1093/comjnl/1.2.83},
    url = {https://doi.org/10.1093/comjnl/1.2.83},
    eprint = {https://academic.oup.com/comjnl/article-pdf/1/2/83/1175362/010083.pdf},
}

@article{rotenberg1960new,
author = {Rotenberg, A.},
title = {A New Pseudo-Random Number Generator},
year = {1960},
issue_date = {Jan. 1960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {0004-5411},
url = {https://doi.org/10.1145/321008.321019},
doi = {10.1145/321008.321019},
journal = {J. ACM},
month = jan,
pages = {75–77},
numpages = {3}
}

@article{matsumoto1998mersenne,
  title={Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator},
  author={Matsumoto, Makoto and Nishimura, Takuji},
  journal={ACM Transactions on Modeling and Computer Simulation (TOMACS)},
  volume={8},
  number={1},
  pages={3--30},
  year={1998},
  publisher={ACM New York, NY, USA}
}

@inproceedings{bernstein2008chacha,
  title={ChaCha, a variant of Salsa20},
  author={Bernstein, Daniel J and others},
  booktitle={Workshop record of SASC},
  volume={8},
  number={1},
  pages={3--5},
  year={2008},
  organization={Citeseer}
}


@misc{steele2021,
      title={Computationally easy, spectrally good multipliers for congruential pseudorandom number generators}, 
      author={Guy Steele and Sebastiano Vigna},
      year={2021},
      eprint={2001.05304},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      url={https://arxiv.org/abs/2001.05304}, 
}

@misc{mcleish2024abacus,
      title={Transformers Can Do Arithmetic with the Right Embeddings}, 
      author={Sean McLeish and Arpit Bansal and Alex Stein and Neel Jain and John Kirchenbauer and Brian R. Bartoldson and Bhavya Kailkhura and Abhinav Bhatele and Jonas Geiping and Avi Schwarzschild and Tom Goldstein},
      year={2024},
      eprint={2405.17399},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17399}, 
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@inproceedings{garg2023simple,
      title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes}, 
      author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
      year={2023},
      eprint={2208.01066},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Ahn2023gradient,
    author = {Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
    pages = {45614--45650},
    publisher = {Curran Associates, Inc.},
    title = {Transformers learn to implement preconditioned gradient descent for in-context learning},
    url={https://openreview.net/forum?id=LziniAXEI9},
    volume = {36},
    year = {2023},
}


@misc{vonoswald2023transformers,
    title={Transformers learn in-context by gradient descent}, 
    author={Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
    year={2023},
    eprint={2212.07677},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{kirsch2022iclmeta,
    title={General-Purpose In-Context Learning by Meta-Learning Transformers}, 
    author={Louis Kirsch and James Harrison and Jascha Sohl-Dickstein and Luke Metz},
    year={2024},
    eprint={2212.04458},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{akyurek2023what,
    title={What learning algorithm is in-context learning? Investigations with linearmodels},
    author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=0g0X4H8yN4I}
}

@inproceedings{guo2024how,
    title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
    author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=ikwEDva1JZ}
}

@misc{hendel2023incontext,
      title={In-Context Learning Creates Task Vectors}, 
      author={Roee Hendel and Mor Geva and Amir Globerson},
      year={2023},
      eprint={2310.15916},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2024incontextvector,
      title={In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering}, 
      author={Sheng Liu and Haotian Ye and Lei Xing and James Zou},
      year={2024},
      eprint={2311.06668},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{
    zhong2023clock,
    title={The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks},
    author={Ziqian Zhong and Ziming Liu and Max Tegmark and Jacob Andreas},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=S5wmbQc1We}
}

@inproceedings{
    zhang2024actpatch,
    title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
    author={Fred Zhang and Neel Nanda},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=Hf17y6u9BC}
}

@misc{gromov2022grokking,
    doi = {10.48550/ARXIV.2301.02679},
    url = {https://arxiv.org/abs/2301.02679},
    author = {Gromov, Andrey},
    keywords = {Machine Learning (cs.LG), Disordered Systems and Neural Networks (cond-mat.dis-nn), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
    title = {Grokking modular arithmetic},
    publisher = {arXiv},
    year = {2023},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{liu2022towards,
  title={Towards understanding grokking: An effective theory of representation learning},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34651--34663},
  year={2022}
}


@inproceedings{nanda2023progress,
    title={Progress measures for grokking via mechanistic interpretability},
    author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=9XFSbDPmdW}
}

@misc{gu2024fourier,
      title={Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic}, 
      author={Jiuxiang Gu and Chenyang Li and Yingyu Liang and Zhenmei Shi and Zhao Song and Tianyi Zhou},
      year={2024},
      eprint={2402.09469},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
    he2024learning,
    title={Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks},
    author={Tianyu He and Darshil Doshi and Aritra Das and Andrey Gromov},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=aVh9KRZdRk}
}


@INPROCEEDINGS{amigo2021,
  author={Amigo, Glauco and Dong, Liang and Marks Ii, Robert J.},
  booktitle={2021 15th International Conference on Signal Processing and Communication Systems (ICSPCS)}, 
  title={Forecasting Pseudo Random Numbers Using Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  keywords={Deep learning;Training;Histograms;Software algorithms;Neural networks;Signal processing;Generators;Markov Chains;Piece-wise Continuous;Linear Congruential Generator;Pseudo Random Numbers;Deep Neural Networks;Nonparametric Forecasting},
  doi={10.1109/ICSPCS53099.2021.9660301}}


@article{Bouillaguet_Martinez_Sauvage_2020, title={Practical seed-recovery for the PCG Pseudo-Random Number Generator}, volume={2020},  url={https://tosc.iacr.org/index.php/ToSC/article/view/8700},  DOI={10.13154/tosc.v2020.i3.175-196}, abstractNote={The Permuted Congruential Generators (PCG) are popular conventional (non-cryptographic) pseudo-random generators designed in 2014. They are used by default in the NumPy scientific computing package. Even though they are not of cryptographic strength, their designer stated that predicting their output should nevertheless be &amp;quot;challenging&amp;quot;.In this article, we present a practical algorithm that recovers all the hidden parameters and reconstructs the successive internal states of the generator. This enables us to predict the next “random” numbers, and output the seeds of the generator. We have successfully executed the reconstruction algorithm using 512 bytes of challenge input; in the worst case, the process takes 20 000 CPU hours.This reconstruction algorithm makes use of cryptanalytic techniques, both symmetric and lattice-based. In particular, the most computationally expensive part is a guessand-determine procedure that solves about 252 instances of the Closest Vector Problem on a very small lattice.}, number={3}, journal={IACR Transactions on Symmetric Cryptology}, author={Bouillaguet, Charles and Martinez, Florette and Sauvage, Julia}, year={2020}, month={Sep.}, pages={175–196} }


@misc{deletang2023chomsky,
      title={Neural Networks and the Chomsky Hierarchy}, 
      author={Grégoire Delétang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A. Ortega},
      year={2023},
      eprint={2207.02098},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.02098}, 
}


@inproceedings{ONeill2014PCGA,
  title={PCG : A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation},
  author={Melissa E. O'Neill},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:3489282}
}

@inproceedings{
doshi2024to,
title={To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets},
author={Darshil Doshi and Aritra Das and Tianyu He and Andrey Gromov},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=UHjE5v5MB7}
}


@book{art_cp,
author = {Knuth, Donald E.},
title = {The art of computer programming, volume 2 (3rd ed.): seminumerical algorithms},
year = {1997},
isbn = {0201896842},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA}
}


@inproceedings{
ye2025physics,
title={Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process},
author={Tian Ye and Zicheng Xu and Yuanzhi Li and Zeyuan Allen-Zhu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=Tn5B6Udq3E}
}


@inproceedings{
chen2024what,
title={What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks},
author={Xingwu Chen and Difan Zou},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=YNbCbcGyXE}
}

@inproceedings{garner1959residue,
author = {Garner, Harvey L.},
title = {The residue number system},
year = {1959},
isbn = {9781450378659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1457838.1457864},
doi = {10.1145/1457838.1457864},
abstract = {In this paper we develop and investigate the properrties of a novel system, called the residue code or residue number system. The residue number system is of particular interest because the arithmetic operations of addition and multiplication may be executed in the same time as required for an addition operation. The main difficulty of the residue code relative to arithmetic operations is the determination of the relative magnitude of two numbers expressed in the residue code. The residue code is probably of little utility for general-purpose computation, but the code has many characteristics which recommend its use for special-purpose computations.},
booktitle = {Papers Presented at the the March 3-5, 1959, Western Joint Computer Conference},
pages = {146–153},
numpages = {8},
location = {San Francisco, California},
series = {IRE-AIEE-ACM '59 (Western)}
}