[
  {
    "index": 0,
    "papers": [
      {
        "key": "power2022grokking",
        "author": "Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant",
        "title": "Grokking: Generalization beyond overfitting on small algorithmic datasets"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "gromov2022grokking",
        "author": "Gromov, Andrey",
        "title": "Grokking modular arithmetic"
      },
      {
        "key": "nanda2023progress",
        "author": "Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt",
        "title": "Progress measures for grokking via mechanistic interpretability"
      },
      {
        "key": "gu2024fourier",
        "author": "Jiuxiang Gu and Chenyang Li and Yingyu Liang and Zhenmei Shi and Zhao Song and Tianyi Zhou",
        "title": "Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2022towards",
        "author": "Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike",
        "title": "Towards understanding grokking: An effective theory of representation learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhong2023clock",
        "author": "Ziqian Zhong and Ziming Liu and Max Tegmark and Jacob Andreas",
        "title": "The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "sharkey2025interp",
        "author": "Lee Sharkey and Bilal Chughtai and Joshua Batson and Jack Lindsey and Jeff Wu and Lucius Bushnaq and Nicholas Goldowsky-Dill and Stefan Heimersheim and Alejandro Ortega and Joseph Bloom and Stella Biderman and Adria Garriga-Alonso and Arthur Conmy and Neel Nanda and Jessica Rumbelow and Martin Wattenberg and Nandi Schoots and Joseph Miller and Eric J. Michaud and Stephen Casper and Max Tegmark and William Saunders and David Bau and Eric Todd and Atticus Geiger and Mor Geva and Jesse Hoogland and Daniel Murfet and Tom McGrath",
        "title": "Open Problems in Mechanistic Interpretability"
      },
      {
        "key": "olsson2022context",
        "author": "Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others",
        "title": "In-context learning and induction heads"
      },
      {
        "key": "Ahn2023gradient",
        "author": "Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit",
        "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
      },
      {
        "key": "vonoswald2023Transformers",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "akyurek2023what",
        "author": "Ekin Aky{\\\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou",
        "title": "What learning algorithm is in-context learning? Investigations with linearmodels"
      },
      {
        "key": "hendel2023incontext",
        "author": "Roee Hendel and Mor Geva and Amir Globerson",
        "title": "In-Context Learning Creates Task Vectors"
      },
      {
        "key": "liu2024incontextvector",
        "author": "Sheng Liu and Haotian Ye and Lei Xing and James Zou",
        "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "power2022grokking",
        "author": "Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant",
        "title": "Grokking: Generalization beyond overfitting on small algorithmic datasets"
      },
      {
        "key": "gromov2022grokking",
        "author": "Gromov, Andrey",
        "title": "Grokking modular arithmetic"
      },
      {
        "key": "nanda2023progress",
        "author": "Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt",
        "title": "Progress measures for grokking via mechanistic interpretability"
      },
      {
        "key": "zhong2023clock",
        "author": "Ziqian Zhong and Ziming Liu and Max Tegmark and Jacob Andreas",
        "title": "The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks"
      },
      {
        "key": "doshi2024to",
        "author": "Darshil Doshi and Aritra Das and Tianyu He and Andrey Gromov",
        "title": "To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets"
      },
      {
        "key": "he2024learning",
        "author": "Tianyu He and Darshil Doshi and Aritra Das and Andrey Gromov",
        "title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "garg2023simple",
        "author": "Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant",
        "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hendel2023incontext",
        "author": "Roee Hendel and Mor Geva and Amir Globerson",
        "title": "In-Context Learning Creates Task Vectors"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2024incontextvector",
        "author": "Sheng Liu and Haotian Ye and Lei Xing and James Zou",
        "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Ahn2023gradient",
        "author": "Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit",
        "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
      },
      {
        "key": "vonoswald2023Transformers",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "akyurek2023what",
        "author": "Ekin Aky{\\\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou",
        "title": "What learning algorithm is in-context learning? Investigations with linearmodels"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "he2024learning",
        "author": "Tianyu He and Darshil Doshi and Aritra Das and Andrey Gromov",
        "title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "rivest1991cryptography",
        "author": "Rivest, Ronald L",
        "title": "Cryptography and machine learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "amigo2021",
        "author": "Amigo, Glauco and Dong, Liang and Marks Ii, Robert J.",
        "title": "Forecasting Pseudo Random Numbers Using Deep Learning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "Bouillaguet_Martinez_Sauvage_2020",
        "author": "Bouillaguet, Charles and Martinez, Florette and Sauvage, Julia",
        "title": "Practical seed-recovery for the PCG Pseudo-Random Number Generator"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ONeill2014PCGA",
        "author": "Melissa E. O'Neill",
        "title": "PCG : A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "chomsky1956three",
        "author": "Chomsky, Noam",
        "title": "Three models for the description of language"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "deletang2023chomsky",
        "author": "Gr\u00e9goire Del\u00e9tang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A. Ortega",
        "title": "Neural Networks and the Chomsky Hierarchy"
      },
      {
        "key": "allenzhu2024physicslanguagemodels1",
        "author": "Zeyuan Allen-Zhu and Yuanzhi Li",
        "title": "Physics of Language Models: Part 1, Learning Hierarchical Language Structures"
      },
      {
        "key": "cagnetta2024deep",
        "author": "Cagnetta, Francesco and Petrini, Leonardo and Tomasini, Umberto M and Favero, Alessandro and Wyart, Matthieu",
        "title": "How deep neural networks learn compositional data: The random hierarchy model"
      },
      {
        "key": "cagnetta2024towards",
        "author": "Cagnetta, Francesco and Wyart, Matthieu",
        "title": "Towards a theory of how the structure of language is acquired by deep neural networks"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "lam2023learning",
        "author": "Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and others",
        "title": "Learning skillful medium-range global weather forecasting"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "deletang2023chomsky",
        "author": "Gr\u00e9goire Del\u00e9tang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A. Ortega",
        "title": "Neural Networks and the Chomsky Hierarchy"
      }
    ]
  }
]