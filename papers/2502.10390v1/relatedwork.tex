\section{Related works}
Our study on the learnability of PRNGs for Transformers touches on several modern and classic topics. 

% \textbf{Modular Arithmetic} The study of how neural networks handle modular arithmetic gained prominence with \citet{power2022grokking}'s discovery of the Grokking phenomenon, where they also show that the model acquired highly structured embeddings. Multiple studies, including \citet{gromov2022grokking,nanda2023progress,gu2024fourier}, revealed that models tackle modular addition tasks by mapping integers to Fourier features. 
%\citet{liu2022towards} provides a toy model to explain the grokking transition. \citet{zhong2023clock} demonstrated the existence of multiple algorithmic approaches for decoding circular features within models.


\textbf{Interpretability and Modular Arithmetic} A growing body of work uncovers the circuits, algorithms, and emergent structures learned by Transformers \cite{sharkey2025interp,olsson2022context,Ahn2023gradient,vonoswald2023Transformers,akyurek2023what,hendel2023incontext,liu2024incontextvector}. A particularly fruitful setting involves simple problems in modular arithmetic \cite{power2022grokking,gromov2022grokking,nanda2023progress,zhong2023clock,doshi2024to,he2024learning}.
Our work adds to this by reverse-engineering the underlying algorithms and uncovering emergent structures in learning pseudo-random number sequences. 

% \citet{garg2023simple} showed that Transformers could learn various simple function classes in-context, while 
% \citet{hendel2023incontext} and \citet{liu2024incontextvector} discovered that language models create in-context vectors that can be extracted to control model predictions. 
% In the context of optimization, \citet{Ahn2023gradient,vonoswald2023Transformers} demonstrated that decoder-only models implement first-order optimization on emergent objective functions when solving linear regression tasks. 
% \citet{akyurek2023what} further shows that larger models exhibit Bayesian estimation capabilities.
% \cite{he2024learning} demonstrated that Transformers develop highly structured representations in attention heads and MLPs when solving modular arithmetic tasks in-context.

\textbf{Cracking PRNGs:} There is a classic duality between cryptography and learning theory \cite{rivest1991cryptography}, and cracking PRNGs is an important topic in cryptography. Nevertheless, deep learning-based attacks have received limited attention in the post-Transformer era. \citet{amigo2021} demonstrated that a fully-connected neural network can predict the outputs of a modified LCG with fixed (irrational) parameters $(a,c,m) = (1,\pi,1)$. In comparison, we systematically analyze the harder cases of unseen parameters using Transformers, reverse-engineer the learned algorithms, and study effects of scale and complexity. 
%\citet{Bouillaguet_Martinez_Sauvage_2020} used lattice-based methods to attack the PCG64 PRNG\citet{ONeill2014PCGA}.

\textbf{Context-Free Grammar:} LCG can also be viewed as a formal language (Type-3 regular grammar) lying within the Chomsky hierarchy \cite{chomsky1956three}. Formal languages provide an interesting setting for synthetic datasets that can be used to understand the properties of neural networks in controlled settings \cite{deletang2023chomsky,allenzhu2024physicslanguagemodels1,cagnetta2024deep,cagnetta2024towards}. 

\looseness -1
\textbf{Chaotic time-series:} A major application of neural networks is predicting time-series for chaotic dynamics, such as weather prediction \cite{lam2023learning} and financial modeling. PRNGs provide an analog of such dynamics in the discrete setting. 

%\citet{deletang2023chomsky} shows that standard neural architectures (RNNs and Transformers) fail to generalize on context-free grammar tasks without additional structured memory components such as stacks or memory tapes.