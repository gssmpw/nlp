%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\input{math_commands.tex}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}

\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% \usepackage{caption}
\usepackage{subcaption}

% extra packages
\usepackage{tcolorbox}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\newcommand{\maissam}[1]{\textcolor{blue}{[MB: #1]}}
\newcommand{\dayal}[1]{\textcolor{teal}{[DS: #1]}}
\newcommand{\tao}[1]{{\textcolor{orange}{TT: [#1]}}}
\newcommand{\tianyu}[1]{{\textcolor[rgb]{0.505, 0, 0.921}{TH: [#1]}}}
\newcommand{\darshil}[1]{\textcolor{purple}{[DD: #1]}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\expandafter\def\expandafter\normalsize\expandafter{%
    \normalsize%
    \setlength\abovedisplayskip{2pt}%
    \setlength\belowdisplayskip{8pt}%
    \setlength\abovedisplayshortskip{-6pt}%
    \setlength\belowdisplayshortskip{2pt}%
}

\begin{document}

\twocolumn[
\icmltitle{(How) Can Transformers Predict Pseudo-Random Numbers? }

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Transformers excel at discovering patterns in sequential data, but understanding their limits and how they learn remains a crucial topic.
In this paper, we investigate the ability of Transformers to learn pseudo-random number sequences from linear congruential generators (LCGs), defined by the recurrence relation $x_{n+1} = a x_n + c \;\mathrm{mod}\; m$. Our analysis reveals that with sufficient architectural depth, context length, and training data variety, Transformers can predict LCG sequences with unseen moduli ($m$) and parameters ($a,c$). We demonstrate successful learning up to $m = 2^{32}$, and we perform systematic analyses of the effect of dataset complexity, architecture size, and context lengths. We uncover emergent structures in the embedding layers and attention heads, which we use to reverse engineer the underlying algorithms used by the Transformer. 
We first consider training and testing on a single modulus, which reveals that models learn to factorize the modulus and subsequently utilize digit-wise number representations to make sequential predictions. When generalizing to unseen moduli, the models employ a greedy approach to estimate the unknown modulus from the context and then use prime factorizations to generate predictions. In the latter case, we observe a transition at a critical depth $d = 3$, below which the Transformer is incapable of learning. We also find that the number of in-context sequence elements needed to reach high accuracy scales sublinearly with the period. 

% Our findings explain how Transformers predict LCG sequences and provide strategies for 

% Transformers excel at identifying latent patterns in sequential data. To ensure accurate alignment, it is crucial to distinguish between meaningful patterns and spurious correlations. In this study, we trained Transformer models to predict sequences produced by the widely used Linear Congruential Generator (LCG), defined as  ($x_{n+1} = a x_n + c \; \mathrm{mod} \; m$). Our findings demonstrate that these models require minimal depth, context length, and training data diversity to accurately predict LCGs with unseen parameters ($a,c$) and moduli ($m$). Furthermore, we observed that during training, the models initially learn to predict simpler sequences by leveraging shortcuts, and subsequently generalize to more complex sequences. During inference, the models employ a greedy approach to estimate the unknown (and previously unseen) modulus from the context, and then utilize its prime factorization to solve the task. To gain a more concrete understanding of the underlying algorithm, we simplified the training task by fixing the modulus. We discovered that the model learns the prime factorization of the modulus during training, and leverages the corresponding digit-wise representation of numbers to sequentially predict digits using the context.

\end{abstract}

\section{Introduction}


Transformer-based language models have proven to be extremely powerful sequence generative models. With copious amounts of training data and computational resources, they can identify and learn complex patterns from training corpora, resulting in numerous remarkable capabilities \cite{attn_is_all_you_need, dosovitskiy2021an}. Recent research has demonstrated that these models, when provided with sufficient context and inference compute, can acquire new patterns and capabilities without additional training through techniques such as in-context learning \cite{radford2019language} and chain-of-thought reasoning \cite{wei2023chain}.
% Transformer-based models have been highly effective at tackling various sequence-based problems, such as language modeling \cite{attn_is_all_you_need}, image classification \cite{dosovitskiy2021an} 
While these models have achieved unprecedented success, understanding what underlying patterns are learned and how they learn them remains a significant challenge. 
%This challenge becomes particularly acute when models encounter structured sequences that follow precise mathematical rules rather than natural language patterns.

Pseudo-Random Number Generators (PRNGs) represent an interesting test case for exploring these challenges. 
%These algorithms, which are fundamental to modern cryptography and computer science, generate sequences of numbers that appear random but are actually determined by a deterministic algorithm and an initial seed value. 
%Common PRNGs include Linear Congruential Generators (LCGs) \cite{thomson1958modified, rotenberg1960new}, Mersenne Twisters \cite{matsumoto1998mersenne}, and cryptographically secure generators like ChaCha20 \cite{bernstein2008chacha}. 
These algorithms, which are fundamental to modern cryptography and computer science, are designed to produce outputs that pass statistical tests for randomness, but nevertheless arise from mathematical patterns that could potentially be learned by sufficiently powerful sequence models.

This intersection between Transformer models' pattern-learning capabilities and the structured nature of PRNG outputs raises intriguing questions about both the capabilities and limitations of these models. Can Transformers learn to predict PRNG outputs given sufficient training data, model capacity and context? If so, what implications does this have for our understanding of both Transformer architectures and PRNGs? Do the Transformers learn the underlying generating algorithm or merely detect shortcuts and spurious patterns? What effect do model capacity, data variety, training methodologies and context length have on the capabilities of Transformers? 

This work aims to answer these questions by focusing on learning sequences obtained from linear congruential generators (LCGs) using GPT-style autoregressive Transformers. We demonstrate how Transformers can successfully learn LCGs and training with moduli up to $m = 2^{32}$. We perform interpretability analyses, uncovering emergent structures in the embedding layers, attention heads, and underlying algorithms that the Transformer uses to learn the sequences. We also perform several systematic scaling analyses to understand the effect of architecture and sequence complexity on model performance and in-context learning ability. 

% Furthermore, how does the ability to learn such precise mathematical patterns relate to the broader challenge of preventing models from learning spurious correlations?

% \subsection{Our Contributions}


% \dayal{I don't think all results would fit into this paper as we only have eight pages. We should strategically leave out results that can be in the next paper if needed.}

% \begin{enumerate}
%     \item For the fixed modulus case, we only need one GPT block for predicting LCGs, whereas we need at least three GPT blocks for generalization to the unseen modulus case.
%     \item For the generalization to the unseen modulus case, we show that $n_m > n_a > n_c$. Moreover, we find that the $n_m \sim m /4$ scaling law ensures good performance.
%     \item For the generalization to the unseen modulus case, optimal performance is observed when $\sim 70\%$ of the training dataset consists of `hard examples.' \dayal{check for p coverage at high hard fraction}
%     \item Models trained on LCG datasets spanning multiple values of $m$ learned to predict sequences generated with unseen $m_{\mathrm{eval}}$ via the following algorithm: (i) estimate the unknown modulus $m_{\mathrm{eval}}$ by examining large values within the presented sequences; (ii) attend to elements greater than $r^{\star}$ steps back to form piecewise linear functions that predict the next token.
% \end{enumerate}


\subsection{Related works}

Our study on the learnability of PRNGs for Transformers touches on several modern and classic topics. 

% \textbf{Modular Arithmetic} The study of how neural networks handle modular arithmetic gained prominence with \citet{power2022grokking}'s discovery of the Grokking phenomenon, where they also show that the model acquired highly structured embeddings. Multiple studies, including \citet{gromov2022grokking,nanda2023progress,gu2024fourier}, revealed that models tackle modular addition tasks by mapping integers to Fourier features. 
%\citet{liu2022towards} provides a toy model to explain the grokking transition. \citet{zhong2023clock} demonstrated the existence of multiple algorithmic approaches for decoding circular features within models.


\textbf{Interpretability and Modular Arithmetic} A growing body of work uncovers the circuits, algorithms, and emergent structures learned by Transformers \cite{sharkey2025interp,olsson2022context,Ahn2023gradient,vonoswald2023Transformers,akyurek2023what,hendel2023incontext,liu2024incontextvector}. A particularly fruitful setting involves simple problems in modular arithmetic \cite{power2022grokking,gromov2022grokking,nanda2023progress,zhong2023clock,doshi2024to,he2024learning}.
Our work adds to this by reverse-engineering the underlying algorithms and uncovering emergent structures in learning pseudo-random number sequences. 

% \citet{garg2023simple} showed that Transformers could learn various simple function classes in-context, while 
% \citet{hendel2023incontext} and \citet{liu2024incontextvector} discovered that language models create in-context vectors that can be extracted to control model predictions. 
% In the context of optimization, \citet{Ahn2023gradient,vonoswald2023Transformers} demonstrated that decoder-only models implement first-order optimization on emergent objective functions when solving linear regression tasks. 
% \citet{akyurek2023what} further shows that larger models exhibit Bayesian estimation capabilities.
% \cite{he2024learning} demonstrated that Transformers develop highly structured representations in attention heads and MLPs when solving modular arithmetic tasks in-context.

\textbf{Cracking PRNGs:} There is a classic duality between cryptography and learning theory \cite{rivest1991cryptography}, and cracking PRNGs is an important topic in cryptography. Nevertheless, deep learning-based attacks have received limited attention in the post-Transformer era. \citet{amigo2021} demonstrated that a fully-connected neural network can predict the outputs of a modified LCG with fixed (irrational) parameters $(a,c,m) = (1,\pi,1)$. In comparison, we systematically analyze the harder cases of unseen parameters using Transformers, reverse-engineer the learned algorithms, and study effects of scale and complexity. 
%\citet{Bouillaguet_Martinez_Sauvage_2020} used lattice-based methods to attack the PCG64 PRNG\citet{ONeill2014PCGA}.

\textbf{Context-Free Grammar:} LCG can also be viewed as a formal language (Type-3 regular grammar) lying within the Chomsky hierarchy \cite{chomsky1956three}. Formal languages provide an interesting setting for synthetic datasets that can be used to understand the properties of neural networks in controlled settings \cite{deletang2023chomsky,allenzhu2024physicslanguagemodels1,cagnetta2024deep,cagnetta2024towards}. 

\looseness -1
\textbf{Chaotic time-series:} A major application of neural networks is predicting time-series for chaotic dynamics, such as weather prediction \cite{lam2023learning} and financial modeling. PRNGs provide an analog of such dynamics in the discrete setting. 

%\citet{deletang2023chomsky} shows that standard neural architectures (RNNs and Transformers) fail to generalize on context-free grammar tasks without additional structured memory components such as stacks or memory tapes.


\subsection{Linear Congruential Generators}

LCG is a simple PRNG that generates the next number in a sequence $\{x_n\}_{n=0}^N$ according to the linear map:
%using a discontinuous piecewise linear map defined by:
% \vspace{-0.5em}
\begin{align}
\label{eq:lcg}
    x_{n+1} = (ax_n + c) \mod m,
\end{align}
where $m > 0$ is the modulus, $ 0 < a < m$ is the multiplier and $0 \leq  c < m$ is referred to as the increment. An LCG map is uniquely defined by the choice of $m, a, c$ and the initial seed $x_0$. An important quantity that determines the complexity of an LCG sequence is its period: $1 \leq \mathcal{T}_m(a,c) \leq m$. As we will show in the following sections, the period of a sequence plays a major role in the difficulty of prediction with Transformers. According to the Hull-Dobell Theorem \cite{hull-dobell}, the period $\mathcal{T}_m(a,c) = m$ if and only if the values of $a$ and $c$ satisfy the following criteria:
%the Hull–Dobell Theorem \cite{hull-dobell} determines values of $a$ and $c$ that result in the maximum period $\mathcal{T}_m(a,c) = m$.  
% (We refer the reader to \Cref{appendix:prngs_lcg} for further details). 
\vspace{-0.5em}
\begin{enumerate}
    \item[I.] $m$ and $c$ are coprime \vspace{-0.5em}
    \item[II.] $a-1$ is divisible by all prime factors of $m$ \vspace{-0.5em}
    \item[III] $a-1$ is divisible by $4$ if $m$ is divisible by $4$.
\end{enumerate}


%   I. $p$ and $c$ are coprime

% II. $a-1$ is divisible by all prime factors of $p$

% III. $a-1$ is divisible by $4$ if $p$ is divisible by $4$

We will evaluate all our models exclusively on sequences that obey the criteria of this theorem.

LCGs are widely utilized for their speed and simplicity, often forming the core of more complex PRNGs like PCG-64, which is used in NumPy. LCGs perform poorly at small bit sizes but improve rapidly with larger state sizes. For instance, an LCG with 88 bits of state can pass the stringent BigCrush randomness test \cite{ONeill2014PCGA}.
%This combination of speed, simplicity, and extensive mathematical analysis has made LCGs a long-standing choice in programming languages and libraries.
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/fixed_p/sequence_intro_new.pdf}
%     \caption{Caption}
%     \label{fig:digit_wise_sequences}
% \end{figure}

% To get further insight into the period of these sequences, let us consider a special representation of the numbers. Specifically, each number $x \i \{ 0, 1, \dots, m-1 \}$ can be represented in a mixed radix form using the prime factorization of $m$.
% For example, if $m=2048$ then any number $x \in \mathbb{Z}_{2048}$ can be written as
% \begin{equation}
%     x = \alpha_{2^1} \, 2^1 + \alpha_{2^2} \, 2^2 + \cdots + \alpha_{2^{11}} \, 2^{11} \;.
% \end{equation}
% Note that $\{\alpha_{2^1}, \dots, \alpha_{2^5}\}$ are identical to the binary digits (bits) in binary representation of $x$, with $\alpha_{2^{j}} \in \{0, 1\}$.
% Interestingly, in this binary representation, each digit (bit) has its own period along the LCG sequence. As shown in \Cref{fig:digit_wise_sequences}, for a sequence of period $\mathcal{T}_m = m$, the $w^{th}$ lowest digit has a period of $2^w$ along the sequence. Thus, lower (higher) digits have smaller (larger) period along LCG sequences
% % In later sections, we will show that that the smaller period of lower digits make it easy for Transformers to copy from the context. On the other hand, the higher digits with larger periods are more difficult to predict.

% For a general composite modulus with the prime factorization $m = p_1^{w_1} p_2^{w_2}\cdots p_i^{w_i}$, we can write numbers $x$ in the following mixed representation:\footnote{These representations are unique, which follows from the Chinese Remainder Theorem.}
% \begin{equation}
% \label{eq:mixed_rep}
%     n = \sum_{w=1}^{w_1} \alpha_{p_1^w} \, p_1^w + \sum_{w=1}^{w_2} \alpha_{p_2^w} \, p_2^w + \cdots \sum_{w=1}^{w_i} \alpha_{p_i^w} \, p_i^w \;;
% \end{equation}
% where each digit $\alpha_{p_j^w} \in \{ 0, 1, \dots, p_j - 1 \}$.
% In this case, the period of each digit $\alpha_{p_j^w}$ has a period of $p_j^w$.


\section{Training Setup}
\label{section:methods}

% We will train and analyze Transformer models on the autoregressive task of predicting the next number in the sequences under two distinct paradigms:


% \textbf{Fixed Modulus (\texttt{FM}):} The training set consists of LCG sequences with a single fixed modulus $m$, and distinct pairs $(a,c)$. We use $n_a$ and $n_c$ to denote the number of $a$ and $c$ values used in the training dataset. The test set consists of LCG sequences with the same modulus $m$ but with $(a,c)$ that have not been seen during training. 


% \textbf{Generalization to Unseen Modulus (\texttt{UM}):} In this more challenging paradigm, the training dataset consists of sequences generated with many ($n_m$) different moduli $m$, and combinations of $(a, c)$. The test dataset is generated with modulus $m_{\text{test}}$ which is \emph{not} shown during training, along with unseen values of $(a,c)$. 

% \subsection{Dataset Generation and Evaluation}

% \textbf{\texttt{FM:}} 
% We first apply the Hull–Dobell Theorem to determine possible values of  $(a,c)$ that maximize the sequence period. From these possible values, we randomly sample $64$ values of each $a$ and $c$ to generate the test dataset. To generate the training dataset, we \emph{exclude} these choices of $a,c$ and uniformly sample 100k LCG sequences of length $L$ (context length), with different $a, c$ and $x_0$ (regardless of their period). 

% Unless otherwise stated, the test accuracy is measured by averaging over all sequences generated using the excluded values of $a$ and $c$.

% \textbf{\texttt{UM:}}
% We begin by selecting a modulus $m_{test}$ for the test dataset, which we will exclude from training. We then use the the Hull–Dobell Theorem to randomly sample $64$ values of each $a$ and $c$ to generate the test dataset.
% To generate the training dataset, we uniformly sample $n_m$ different moduli $m$ from the range $[L, m_{\max}]$, where $m_{\text{max}} = \floor{1.2 \cdot m_{\text{test}}}$ is the maximum modulus in the training dataset. For each sampled $m$, we uniformly select $n_a$ multipliers $0 < a< m$ and $n_c$ increments $0 \leq  c < m$ that are not present in the test dataset. For each unique combination of $(a, c, m)$, we generate a sequence length of $L$ -- resulting in a dataset containing a total of $N = n_m \times n_a \times n_c$ example sequences. \maissam{what about $x_0$?}

% As in the \textbf{\texttt{FM}} case, we measure test accuracy by averaging over all choices of $a$ and $c$ that have a maximum period for a given $m_{\mathrm{test}}$, with $128$ randomly selected $x_0$ values for each choice.

% Note that while the training dataset contains sequences of all periods, the test dataset only contains maximal period sequences.

\subsection{Dataset Generation and Evaluation}

We investigate two training paradigms: Fixed Modulus (\textbf{\texttt{FM}}) and Generalization to Unseen Modulus (\textbf{\texttt{UM}}).

\textbf{Fixed Modulus (\texttt{FM}):} We first apply the Hull–Dobell Theorem to determine possible values of $(a,c)$ that maximize the sequence period for a given modulus $m$. From these possible values, we randomly sample $64$ values of each $a$ and $c$ to generate the test dataset. To generate the training dataset, we \emph{exclude} these choices of $a,c$ and uniformly sample 100k LCG sequences of length $L$ (context length), with different $a, c$ and $x_0$ (regardless of their period).

\textbf{Generalization to Unseen Modulus (\texttt{UM}):} In this more challenging paradigm, we first select test modulus $m_{\text{test}}$ that are excluded from training. The training dataset incorporates $n_m$ different modulus uniformly sampled from the range $[L, m_{\text{max}}]$, where $m_{\text{max}} = \floor{1.2 \cdot m_{\text{test}}}$. For each modulus $m$, we uniformly select $n_a$ multipliers ($0 < a < m$) and $n_c$ increments ($0 \leq c < m$), generating sequences of length $L$ with random initial values $x_0$. This results in a total of $N = n_m \times n_a \times n_c$ training sequences. To construct the test dataset for each $m_{\text{test}}$, we randomly sample 64 values each of $a$ and $c$ that generate maximum-period sequences.

In both paradigms, test accuracy is measured by averaging over all sequences generated from the excluded values of $a$ and $c$ and 128 random initial values $x_0$ for each $(a,c)$ pair. 

The above settings are used in \Cref{section:train,section:interp}. In order to achieve better performance, we used a larger and slightly higher-quality dataset in \Cref{section:scaling_laws}, which we detail later. 


\subsection{Tokenization, Architecture and Optimizer}

We employ two different settings throughout this paper. (i) In \Cref{section:train,section:interp}, we tokenize each number as a unique token, with dictionary size $m$ for the \textbf{\texttt{FM}} case and $m_{\mathrm{test}}$ for the \textbf{\texttt{UM}} case. We then use GPT-style Transformers with learnable positional embeddings and weight tying \cite{press2017tying}. (ii) In \Cref{section:scaling_laws}, we tokenize each number using byte-tokenization with various base choices and apply abacus positional embeddings \cite{mcleish2024abacus}

Model architecture is characterized by number of blocks (depth), embedding dimension ($d_{\mathrm{model}}$), number of attention heads ($n_{\mathrm{heads}}$). Models are trained with AdamW optimizer.
% with weight decay applied exclusively to non-bias parameters.

% \subsection{Model and optimizer:}

% \dayal{Will update the details below after rechecking the code.}
% \paragraph{Transformer model and optimizer:} We consider GPT-style Pre-LN Transformers with learnable positional embeddings \dayal{and weight tying}. The model architecture is defined by three key parameters: $n_{\text{layers}}$, the number of blocks, $n_{\text{embd}}$, the embedding dimension, and $n_{\text{heads}}$, the number of attention heads. In our scaling experiments, we increase the embedding dimension while maintaining a fixed head dimension of $d_{\text{head}} = 128$. For optimization, we use the AdamW optimizer, applying weight decay exclusively to non-bias parameters.


% \section{Effect of LCG complexity on Training and Generalization}
% \label{section:training_dynamics}
% \tianyu{To be pruned}
% In this section, we analyze the training dynamics and generalization performance through the lens of the complexity of an LCG sequence. A naive measure of the complexity is the period of the sequence, which we denote by $T_{p}$. 
% However, sequences with identical periods can exhibit varying levels of complexity. For example, a linear sequence $x_{n+1} = a x_n$ and a `good' LCG sequence with period $m$ would have the same period. Therefore, we need a complexity measure that can distinguish between sequences with the same period. 

% Inspired by the original construction \citep{hull-dobell}, we consider an $r$-step iteration of \Cref{eq:lcg}:
% \begin{align}
% \label{eq:lcg_r}
%     x_{n+r} = a^r x_n + \sum_{i=1}^{r} a^{i-1} c \mod m.
% \end{align}
% For any given $m$ and $a$ selected according to the Hull-Dobell Theorem, there exists a minimal $r^{\star}(a, m) < m$ such that $a^{r^{\star}(a, m)} = 1 \mod m$ holds. In particular, when we take $r=r^{\star}(a, m)$ in the LCG prediction equation, the multiplier $a^r$ becomes 1, which allows one to predict any number by simply combining numbers of a given LCG without any explicit information about $a$ and $c$:
% \begin{align}
% \label{eq:lcg_pred}
%     x_{n' + r^{\star}} = x_{n'} + (x_{n + r^{\star}} - x_{n}) \mod m.
% \end{align}
% Here $n'$ can be any integer, and we have omitted $a$ and $m$ dependence of $r^{\star}$ for clarity.

% This $r^{\star}(a, m)$ value is a good proxy of the difficulty of representing the LCG with piecewise linear function without modulus. The larger $r^{\star}(a, m)$ is, the harder to represent a given LCG with simpler piecewise linear functions. Here, we show a few examples in Small Fig xx.


\section{Training Results}
\label{section:train}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/training-interp/training_unseen_modulus_swap.pdf}
    \caption{Training phase diagram and dynamics. (a) Test accuracy for last token of models trained with $m_{\mathrm{test}}=4096$, evaluated across various depths and $n_{\mathrm{head}}$ values, showing a minimum depth requirement of $3$ for generalization; (b) Training curves for last token of a model with depth $6$ and $n_{\mathrm{head}}=4$ ($m_{\mathrm{test}}=2048$), showing that test accuracy for last token ``groks'' simultaneously with performance on sequences with period longer than context length $L=256$, indicating delayed discovery of underlying rules.}
    \label{fig:training}
\end{figure}

\looseness -1
In this section, we focus on the more sophisticated behavior exhibited by models trained with \textbf{\texttt{UM}} data, leaving the discussion of \textbf{\texttt{FM}} for \Cref{appendix:fm_train}. We begin by investigating the minimal model that can solve the task. In \Cref{fig:training} (a), we present the test accuracy for the last token of models trained on a dataset with $m_{\mathrm{test}}=4096$, varying both model depth and number of attention heads ($n_{\mathrm{heads}}$) while keeping head dimensions constant. Our results indicate that a minimum depth of $3$ is necessary for good generalization performance, with a sharp transition upon lowering the depth. Further analysis across multiple $m_{\mathrm{test}}$ values (see \Cref{appendix:critical_depth}) confirms that this minimal depth requirement is universal. 

We then carefully examine the training dynamics of a model with $n_{\mathrm{head}}=4$ and depth $6$ in \Cref{fig:training} (b). We categorize the training sequences into two groups: i) sequences with periods shorter than the context length, which can be solved through simple copying, and ii) sequences with periods longer than the context length, which require the model to deduce underlying rules for prediction. Our analysis reveals that the model first acquires copying ability for group i) in the early stages of training, and later ``groks" the solution for group ii). Notably, the model's ability to generalize to test $m_{\mathrm{test}}$ emerges simultaneously with this grokking phenomenon. These results demonstrate that the model develops different capabilities at distinct stages of training, with generalization ability emerging only after the model learns the underlying rules through solving the more challenging sequences. In \Cref{appendix:train-interp}, we present an ablation study where models are trained exclusively on either short-period or long-period sequences. Our findings indicate that training exclusively on long-period sequences enables model generalization and eliminates the grokking phenomenon.

% Our findings indicate that optimal generalization performance requires a balanced combination of easy and hard examples, suggesting the emergence of an implicit curriculum during the training process.


\section{Interpreting how Transformers predict PRNGs}
\label{section:interp}

% The numbers in LCG sequences with modulus $m$ belong to the group of integers modulo $m$, denoted by $\mathbb{Z}_m$. These numbers $x \in \mathbb{Z}_m$ can be represented in a mixed radix form using the prime factorization of $m$. For example, if $m=2048$ then any number $x \in \mathbb{Z}_{2048}$ can be written as
% \begin{equation}
%     x = \alpha_{2^1} \, 2^1 + \alpha_{2^2} \, 2^2 + \cdots + \alpha_{2^{11}} \, 2^{11} \;.
% \end{equation}
% Note that $\{\alpha_{2^1}, \dots, \alpha_{2^5}\}$ is identical to the binary digits (bits) in binary representation of $n$, with $\alpha_{2^{j}} \in \{0, 1\}$.
% In general, a modulus with the prime factorization $m = p_1^{w_1} p_2^{w_2}\cdots p_i^{w_i}$, entails the following mixed representation:\footnote{These representations are unique, which follows from the Chinese Remainder Theorem.}
% \begin{equation}
% \label{eq:mixed_rep}
%     n = \sum_{w=1}^{w_1} \alpha_{p_1^w} \, p_1^w + \sum_{w=1}^{w_2} \alpha_{p_2^w} \, p_2^w + \cdots \sum_{w=1}^{w_i} \alpha_{p_i^w} \, p_i^w \;;
% \end{equation}
% where each digit $\alpha_{p_j^w} \in \{ 0, 1, \dots, p_j - 1 \}$.

% The model trained on a fixed modulus first performs prime factorization (coset decomposition) to create bit representations of numbers: It factorizes the modulus $m = p_1^{w_1} p_2^{w_2}\cdots p_i^{w_i}$ with $i$ possible prime factors, allowing the model to encode any $\mathbb{Z}_m$-valued number $x_n$ using $i$ distinct sets of $w_i$-bit, base-$p_i$ numbers.


% The relevance of these digit-wise representations stems from the fact that every digit has its own period along the LCG sequence. Specifically, an LCG sequence of period $T_p = m$ has per-digit period equal to its corresponding prime power: $T(\alpha_{p_j^w}) = p_j^w$. Thus, the lower digits have a small period, which makes it easy for Transformers to copy from the context (aka shortcut). The longer the context, the farther back in the sequence the attention can look, and the more digits can be copied. On the other hand, to predict higher digits, a more complex algorithm is required.

\looseness -1
Interpreting the underlying algorithms implemented by the models is essential for understanding the models' training dynamics and scaling capabilities. We uncover these algorithms for both \textbf{\texttt{FM}} and \textbf{\texttt{UM}} cases. While certain details of the algorithms differ in the two cases, they share some common properties originating from the underlying LCG structures.

We first discuss properties of LCG sequences that will be useful to interpret model behaviors.

\subsection{Mixed Radix Representations}
\label{sec:radix}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/fixed_p/sequence_intro_new.pdf}
    \caption{Bit-wise periods in an example LCG sequence generated with $m=2048, a=293, c=1033$, which follows the Hull-Dobell theorem. In binary representation $w$-th lowest bit has a period of $2^w$, for $w \in \{ 1, \dots 11 \}$. Writing a new sequence by skipping every 2nd step ($r=2$) reduces the periods of all the bits by a factor of $2$, rendering the lowest bit constant. 
    $r=2^k$ reduces bit-wise periods by a factor of $2^k$, with last $k$ digits constant.
    }
    \label{fig:digit_wise_sequences}
\end{figure}
\vspace{-0.05 in}
% Consider an LCG sequence with modulus $m = 2048 = 2^{11}$. Each number in this sequence ($x \in \{ 0, 1, \dots, 2047 \}$) can be represented as an 11 digit binary number using the following unique decomposition:
Consider an LCG sequence with modulus $m = 2048 = 2^{11}$. Each number in this sequence can be represented as an 11-digit binary number:
\begin{equation}
\label{eq:binary_rep}
    x = \alpha_{2^1} \, 2^1 + \alpha_{2^2} \, 2^2 + \cdots + \alpha_{2^{11}} \, 2^{11} \;,
\end{equation}
where $\{\alpha_{2^1}, \dots, \alpha_{2^{11}}\}$ are the binary digits (bits), with $\alpha_{2^{j}} \in \{0, 1\}$.

In this binary representation, each digit (bit) has its own period along the LCG sequence. As shown in \Cref{fig:digit_wise_sequences}, for a sequence of period $\mathcal{T}_m = m$, the $w^{th}$ lowest digit has a period of $2^w$ along the sequence. Thus, lower (higher) digits have smaller (larger) period along LCG sequences. %\darshil{add citation or include proof.}

% While the binary representation is natural for moduli of the form $m=2^w$ \maissam{huh? binary representation is always possible. what about the binary representation are you referring to?} \darshil{Yes, it was incorrect phrasing. Updated.}, it is possible to generalize this representation to the case of composite moduli. Consider sequences with composite modulus $m$, which has a prime factorization $m = p_1^{w_1} p_2^{w_2}\cdots p_i^{w_i}$, where $p_j$ are the prime factors of $m$ and $w_j$ are its corresponding powers. In this case, the numbers in the sequence ($x \in \{ 0, 1, \dots, m-1 \}$) can be represented in a mixed radix form, utilizing the prime factorization:\footnote{These representations are unique, which follows from the Chinese Remainder Theorem.}
While the binary representation is natural for moduli of the form $m=2^w$, it is possible to generalize this representation to the case of composite moduli. Consider sequences with composite modulus $m$, which has a prime factorization $m = p_1^{w_1} p_2^{w_2}\cdots p_i^{w_i}$. In this case, the numbers in the sequence can be represented uniquely in a mixed radix form:%\footnote{These representations are unique, which follows from the Chinese Remainder Theorem.}
%, utilizing the prime factorization:
% \vspace{-0.1 in}
\begin{equation}
\label{eq:mixed_rep}
    n = \sum_{w=1}^{w_1} \alpha_{p_1^w} \, p_1^w + \sum_{w=1}^{w_2} \alpha_{p_2^w} \, p_2^w + \cdots + \sum_{w=1}^{w_i} \alpha_{p_i^w} \, p_i^w \;;
\end{equation}
where the digits $\alpha_{p_j^w} \in \{ 0, 1, \dots, p_j - 1 \}$ are the generalization of bits. When $\mathcal{T}_m = m$, the period of each digit $\alpha_{p_j^w}$ has a period of $p_j^w$. We will see later that trained Transformers have emergent structures that find such digit-wise representations and utilize them to make systematic predictions from the context. The per-digit period will play an important role in the prediction accuracy of that digit. 

%Another interesting consequence of these representations arises when we look at sequences with skipped steps. 
%Inspired by the original construction of \citet{hull-dobell}, let us 
Consider the $r$-step iteration of \Cref{eq:lcg}:
\begin{align}
\label{eq:lcg_r}
    x_{n+r} = a^r x_n + \sum_{i=1}^{r} a^{i-1} c \mod m \;.
\end{align}
\vspace{-0.1 in}

In this new sequence, the period of each digit $\alpha_{p_j^w}$ reduces from $p_j^w$ to $p_j^w / \gcd(r, p_j^w)$. Consequently, some of the higher digits become relatively simpler to predict due to reduced periods; and some lower digits become trivial to predict due to being constant. We show an example of this for $m=2048$ in \Cref{fig:digit_wise_sequences} (top panel). The digit-wise periods in the new sequence with $r=2$ reduce by a factor of 2, while the last digit is simply constant. Higher values of $r=2^k$ will lead to even further simplifications of the sequence. We will see in the later subsections that the Transformer has emergent structure that leverages this simplification to accurately predict LCG sequences.

For general $m$, LCG sequences simplify when $r = p_1^{k_1} p_2^{k_2} \cdots$. This simplification from period-reduction is only visible when we represent the numbers in the sequence in their mixed radix representations corresponding to $m$. A different representation would only partially (or not at all) reduce the per-digit periods.
% When a number $n$ is represented in an incompatible representation (i.e., the radix representation contains some $p_i^{w'}$ such that $\gcd(p_i^{w'}, m) = 1$), the periodic structure for the lowest $k$ digits in a compatible representation only appears after examining $2^k \cdot p_i^{w'}$ steps back in this incompatible representation. Consequently, the model must use a much longer context to capture periodic patterns that would be readily apparent when using a compatible representation.



\subsection{Interpretability: Fixed Modulus}
\label{sec:interp_fixed}

\looseness -1


\label{text:algorithm_fixed_modulus}
\begin{tcolorbox}
\underline{Qualitative Algorithm (fixed modulus):}
\begin{enumerate}
    \item[i.] Find mixed-radix representations of inputs from the learned prime factorization of $m$
    \item[ii.] Look back $r=p_j^k$ steps in the context and copy the lowest $k$ digits, for different prime factors of $m$
    \item[iii.] Using these $r$-step sequences, predict the higher digits of the simplified sequence
\end{enumerate}
\end{tcolorbox}

We now discuss the algorithm implemented by Transformers trained on LCG with a fixed modulus. We will show that, despite being provided integers as inputs, the model develops emergent structures that create and leverage the mixed-radix representation of the inputs. We will focus on the setup with a 1-layer, single-head Transformer trained on LCG with $m=2048$. Similar results for composite moduli (e.g. $m=7776$) and different model sizes are presented in \Cref{appendix:fixed_interp}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/fixed_p/acc_final_2048_d1_h1_ne768.pdf}
    \caption{Test accuracy of trained model ($m=2048, l=1, n_{head}=1, d_{model}=768$), averaged over $a, c$ and initial seeds. (a) Test accuracy w.r.t. token positions. A ladder-like structure appears, with jumps occurring at $2^k$-th positions. (b) We represent numbers as an eleven-digit binary number ($2048 = 2^{11}$) and compute the per-digit test accuracy of model predictions.}
    \label{fig:acc_vs_token}
\end{figure}

We begin by analyzing the average accuracy of a trained Transformer model as a function of token position along the context, shown in \Cref{fig:acc_vs_token} (a). The accuracy exhibits a ladder-like structure, with each jump occurring exactly at the $2^k$-th token position.
These successive transitions can be explained using binary representations and $r$-step recurrence (\Cref{eq:lcg_r}).  
% suggest that the model is indeed utilizing the $r$-step simplification of the sequences and copying the lower digits from the context -- whene 
Specifically, once the token position exceeds $2^k$, the model can look back in the context and implement $r=2^k$-step iteration. This is utilized for (i) copying the lowest $k$ bits, since they become constant as well as (ii) simplifying the higher bits, since their periods get reduced by a factor $2^k$.
We note that the accuracy trend remains unchanged across different choices of $a$ and $x_0$ (see \Cref{appendix:fixed_interp} \Cref{figapp:accuracy_vs_token_ac_m=2048} ).

Next, we compute the per-digit accuracy by converting both model predictions and ground truth labels to their binary representations according \Cref{eq:binary_rep}. In \Cref{fig:acc_vs_token}(b), we observe that the model can predict more digits correctly with more context, with the transitions occurring at $2^{w}$-th token positions. This is a direct result of the sequences becoming increasingly simplified as the model can look farther back in the context and utilize $(r=2^{w})$-step iterations. Since this simplifications occurs in the form of digit-wise periods, \Cref{fig:acc_vs_token}(b) serves as direct evidence that the model is internally developing and utilizing binary representations. The sequential learning of digits also explains the ladder-like structure of the accuracy in \Cref{fig:acc_vs_token}(a).
We find that the overall average accuracy (\Cref{fig:acc_vs_token}(a)) multiplicatively depends on the per-digit accuracies  (\Cref{fig:acc_vs_token}(b)) (see \Cref{appendix:fixed_interp} \Cref{figapp:accuracy_m=2048_h1}(b))\footnote{This holds if the per-digit accuracies are independent, which we confirm empirically.}
\begin{equation}
    \mathrm{acc}_{\mathrm{overall}} = (\mathrm{acc}_{\mathrm{digit} \, 1}) \, (\mathrm{acc}_{\mathrm{digit} \, 2}) \, \cdots \, (\mathrm{acc}_{\mathrm{digit} \, w}) \,.
\end{equation}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/fixed_p/embedding+attn_head_small_p2048_d1_h1_n768.pdf}
    \caption{Fixed modulus interpretability ($m=2048$, depth=1, $n_{heads}=1, d_{model}=768, a=1589, c=629$). (a1) 1st principal component groups the numbers mod 2. (a2) 2nd and 3rd principal components group the numbers mod 4. (b) Embedding vectors of different numbers exhibit high cosine similarity when they are spaced $2^k$ distance apart -- with the similarity increasing with $k$. (c) Each query attends most strongly to the tokens $2^k$ and $2^{k-1}$ distance backward, for highest possible value of $k$, enabling copying of $k$ lower bits. (d) post-ReLU MLP activations at token position 129 with respect to different target numbers (i.e. number at the next token position). Each neuron gets activated only for specific target numbers, with a preiodic pattern.}
    \label{fig:fixed_p_embedding_pca}
    \vspace{-0.1 in}
\end{figure*}

% \darshil{Will update soon} In the remainder of the sub-section, we will show that a Transformer model trained on a fixed modulus indeed performs such a prime factorization to create digit representations of its inputs; and implements the above operations. 
% Subsequently, the model performs two distinct operations: (i) copying the lower significant digits from the context, which serves as a shortcut for LCG; and (ii) predicting the remaining higher significant digits using a simplified computation, as \Cref{eq:lcg} reduces to a smaller modulo $m / \gcd(r, m)$ in these digits.

% First, we examine the model's behavior. 
% \tianyu{Highlight/Recall a bit on the relation between copying and the performance ladder?}

% Thus, the sequential learning of digits explains the ladder-like structure in the average test accuracy. We also observe that the accuracy trends remain unchanged across different $a, c$ and initial seeds.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/fixed_p/pca_embedding_final_p2048_d1_h1_n768.pdf}
%     \caption{Caption}
%     \label{fig:fixed_p_embedding_pca}
% \end{figure}

Next, we investigate how various components of the model implement the algorithm outlined earlier this section. 

\textbf{Step i:} We begin by analyzing how the model performs prime factorization to develop the binary representations (mixed-radix for general $m$). We conduct Principal Components Analysis (PCA) of the embedding matrix. \Cref{fig:fixed_p_embedding_pca}(a,b) shows the projections of all numbers $n \in \{0, \dots 2047\}$ along the first principle component of the embedding. We observe that the model groups the numbers into modulo $2$ clusters along the first principal component. Similarly, the 2nd and 3rd principal components group the numbers into modulo $4$ clusters. In general, we find principal directions that group the numbers into modulo $2^w$ clusters (see \Cref{appendix:fixed_interp} \Cref{figapp:embedding_m=512}).
By clustering the numbers according to their remainder modulo different prime-powers, these principle directions naturally encode the digit-wise representations of the inputs. 
% Notably, the model did not just reuse the first principal component for modulo $4$ clustering but used two more independent principal components. We attribute this to some implicit bias of the training procedure. 
In \Cref{fig:fixed_p_embedding_pca}(c), we check the cosine similarity between the embedding vectors of different numbers. We see that the more digits two numbers share in the binary representation, the higer the cosine similarity of their embedding. This is a consequence of these numbers having similar components along principle directions corresponding to those digits.

\textbf{Step ii, iii:}
To validate that the model looks back $r=2^k$ steps in the context, we examine the attention weights in \Cref{fig:fixed_p_embedding_pca}(d). In order to predict the token at position $t$, the model attends to the positions $t-{2^k}$. This allows the model to copy the lowest $k$ bits and simplify the prediction of higher bits. We observe that the model attends most strongly to the positions $t-{2^k}$ and $t-{2^{k-1}}$ for the largest possible value of $k$. This corresponds to the two brightest lines in \Cref{fig:fixed_p_embedding_pca}(d) that are ${2^{k-1}}$ distance apart. By comparing the binary representations of these two numbers, the model copies the lower $k$ bits to make predictions.  

In addition to the two bright lines in the attention score, we observe multiple faint lines which are $2^{k'}$ apart, for $k' < k-1$. The model combines the information from all these tokens to predict the higher bits. 
To verify that the two bright lines at positions $t-{2^w}$ and $t-{2^{w-1}}$ copy lower bits, and the other faint lines are important for predicting higer bits, we perform the following ablation. For each query, we mask out all attention scores except for the top two key positions. We then measure the performance of this ablated model in \Cref{appendix:fixed_interp}, \Cref{figapp:accuracy_m=2048_h1}(d). In this case, we observe that the model retains the ability to copy the $k$ lowest bits, but loses the ability to predict higher bits.
% In Figure \Cref{fig:fixed_p_embedding_pca}(d), we analyze the attention heads and find that they primarily focus on tokens $8$ and $16$ positions back. These gaps in attention pattern enable the model to copy the lower $3$ and $4$ significant digits accordingly. Meanwhile, the remaining heads examine multiple in-context examples, while the computational processing occurs within the MLP layers.

After the attention layer gathers the information about previous tokens, MLPs implement the remaining computational processing to make predictions. In \Cref{fig:fixed_p_embedding_pca} (d), we plot the post-ReLU MLP activations at fixed token positions w.r.t. the number appearing at that position. We find that each neuron only fires when a specific set of numbers appear at those tokens -- often exhibiting periodic structure. We conclude that the first fully connected layer of the MLP determines which neurons to fire based on the context. The second fully connected layer aggregates the non-zero activations to form the final prediction.



\subsection{Interpretability: Generalization to Unseen Modulus}
\label{sec:interp_unseen}

\begin{tcolorbox}
\underline{Qualitative Algorithm (unseen modulus):}
\begin{enumerate}
    \item[i.] Encode information about many possible prime factorizations
    \item[ii.] Use the largest number in the context to estimate the modulus
    \item[iii.] Implement steps ii and iii from the fixed modulus algorithm with a different realization
\end{enumerate}
\end{tcolorbox}

Unlike the \textbf{\texttt{FM}} case, the training set for \textbf{\texttt{UM}} is generated using many different moduli $m$. Since each modulus has its own mixed-radix representation, which is incompatible with the other moduli, the model needs to implement a different, more general algorithm to solve \textbf{\texttt{UM}} tasks.

We analyze how various components of a 4-layer Transformer model implement the above algorithm to solve the \textbf{\texttt{UM}} task. The model is trained on a dataset generated with $n_m = n_a = n_c = 128$, where $m$ values are randomly chosen from the interval $[257, 2457]$. To avoid leakage between train and test sets, we specifically exclude the moduli $m_{\mathrm{test}} \in \{1800=2^3 \cdot 3^2 \cdot 5^2, 2048=2^{11}, 2352=2^4 \cdot 3 \cdot 7^2\}$ from the training set.

\begin{figure}[!h]
    \centering
    \subfloat {\includegraphics[width=1.0\linewidth]{figures/multi_interp/embd_combine_small.pdf}} 
    \caption{PCA analysis of the embedding layer. The first principal component resembles a circular structure, while the second and third dimensions categorize numbers based on their modulo $2$ and modulo $3$ values, respectively.}
    \label{fig:multip_pca_embd}
    % \vspace{-0.1 in}
\end{figure}

%%%%%%%%%%%% OG
% \paragraph{Step i} We begin by analyzing the embedding layer. In \Cref{fig:multip_pca_embd} (a), our PCA analysis reveals that the model attempts to form a semi-circular structure in the first principal component. This structure likely resembles the circular patterns observed in \cite{power2022grokking,zhong2023clock}, which are known to facilitate modular arithmetic tasks. However, since our model is trained with multiple moduli, it cannot determine which value should be identified with 0 to form a closed circle.
%%%%%%%%%%%%
\textbf{Step i:}
We begin by analyzing the embedding layer. In \Cref{fig:multip_pca_embd}(a), PCA reveals that the model forms a semi-circular structure in the first two principal components. This structure resembles the circular patterns observed in modular arithmetic tasks \citep{power2022grokking,zhong2023clock}. However, since our model is trained with multiple moduli, it cannot determine the value that should be identified with $0$ to form a closed circle.

% \maissam{Not sure what the last sentence means.}

\looseness -1
Further analysis of \Cref{fig:multip_pca_embd} (a, b) demonstrates that the $2$nd and $3$rd principal components of the embedding layer categorize numbers according to their values modulo $2$ and $3$, respectively. Given that the modulus is not fixed in this case, there is no inherent requirement for the model to encode numbers in binary representation. In fact, as we will demonstrate next, the model forms radix representations based on the choice of modulus $m_{\mathrm{test}}$ through specialized attention heads. We believe the embedding layer's primary focus on modulo $2$ and $3$ can be attributed to these being the most common prime factors of numbers appearing in the training set.

%Since different prime factors and their various powers each require a unique direction to embed, it is not feasible for the embedding layer to implement all of them simultaneously.\maissam{I'm skeptical of this. We are in a high dimensional space and there are not that many factors.} Consequently, the model utilizes several attention heads in the first layer to form different radix representations.\maissam{This is confusing language. What do we mean by attention heads forming radix representations?}

In the first layer, attention heads further process the embedded numbers by grouping them according to their modular values with respect to various prime factors, with each head specializing in specific prime factors. This head specialization enables the model to construct radix representations requiring different prime bases.

\begin{figure}[!h]
    \centering
    \subfloat {\includegraphics[width=1.0\linewidth]{figures/multi_interp/prune_new_small.pdf}} 
    \caption{PCA analysis of attention heads specializing in different prime factorizations and their effect on per-digit accuracy. (a1, b1, c1) PCA patterns of outputs extracted from corresponding heads. The first two principal components group numbers based on their values modulo $14$, modulo $4$, and modulo $3$, respectively. (a2, b2, c2) Test accuracy for individual digits when numbers are represented according to \Cref{eq:mixed_rep} for given $m_{\mathrm{test}}$. (a3, b3, c3) Test accuracy for individual digits after pruning the corresponding heads shown in (a1, b1, c1). Pruning these specific heads substantially degrades performance on their corresponding digits. 
    }
    \label{fig:multip_prune}
    \vspace{-0.17 in}
\end{figure}


The attention heads responsible for preparing different prime factorizations are shown in \Cref{fig:multip_prune}. To get the (a1, a2, a3) panel, we feed the model with sequences generated using the corresponding $m_{\mathrm{test}}$ value and randomly selected legitimate values of $a$ and $c$. Then, we extract outputs from each corresponding head, at the identical token positions for all $m_{\mathrm{test}}$ possible numbers to form a $m_{\mathrm{test}} \times d_{\mathrm{model}}$ matrix. Finally, we perform PCA analysis over this matrix and project each number's feature vector onto the top two principal components. The analysis reveals that each head naturally categorizes numbers according to their values modulo different prime factors. This head specialization enables the subsequent model layers to select which representation to use. The prominence of small prime numbers in the top principal components likely reflects their frequent occurrence in the prime factorization of composite numbers.

To further confirm the importance of these specialized heads, we compare the model's per-digit performance before and after pruning the corresponding heads. In \Cref{fig:multip_prune} (a2, b2, c2), we measured the model's performance before pruning, where we measured per-digit performance by representing numbers in the natural radix representation for each test modulus. The results show that the model typically struggles with calculating higher digits for a larger prime factor. This behavior aligns with what we observed in the \texttt{FM} case, where computing higher digits is more demanding than copying lower digits. Different from \texttt{FM} case, the difficulty in calculating higher digits is partly due to the model's inability to precisely determine the modulus $m_{\mathrm{test}}$ from the sequence, which we will elaborate more later in this section.

\looseness -1
In \Cref{fig:multip_prune} (a3, b3, c3), we measured the model's performance after pruning the corresponding heads. By pruning, we mean replacing all components of the outputs from the corresponding heads with a single value. This value represents the mean magnitude of the outputs, computed by averaging across all dimensions: different sequences (using $10\%$ of randomly selected training sequences), token positions, and embedding dimension. Note that zeroing out any head's contribution in the first layer would annihilate the model's performance. In contrast, our pruning method ensures that we remove the contribution of an attention head without changing the scale of the signal drastically. Comparison with the pre-pruning results reveals that for each $m_{\mathrm{test}}$, pruning the head responsible for a specific prime factorization dramatically reduces performance on the corresponding digits while having a relatively minor impact on others. For instance, in panel (a3), the model's ability to compute digits related to base-$7$ is completely lost, while digits related to base-$2$ and base-$3$, though impacted, remain above random chance. Similarly, panel (b3) shows complete performance degradation. In panel (c3), the model's ability to compute digits related to $3$ is drastically decreased, while its capacity to process digits for base-$2$ and base-$5$ digits remains intact or shows marginal degradation. 

To further validate the correlation between specific attention heads and digit-wise performance, we conduct pruning experiments as shown in \Cref{fig:prune_appendix_2048,fig:prune_appendix_2352}. Interestingly, in \Cref{fig:prune_appendix_2048} (a, b), when we remove the attention head responsible for computing modulo $3$ representations in \Cref{fig:multip_prune} (c1) and evaluate the model's performance for $m_{\mathrm{test}}=2048$, we observe improved performance at specific token positions for certain bits, while other bits exhibit minimal performance degradation. This behavior suggests that the model experiences internal uncertainty about which radix representation to employ, likely stemming from its difficulty in accurately determining the value of $m_{\mathrm{test}}$, as we will demonstrate in the following paragraph.



\begin{figure}[!h]
    \centering
    \subfloat {\includegraphics[width=1.0\linewidth]{figures/multi_interp/cosine_combine_small.pdf}} 
    \caption{Attention head (layer 1, head 6) for estimating $m_{\mathrm{test}}$. (a) The query consistently attends to keys corresponding to the largest values within the given sequence; (b) the head generates features with the highest cosine similarity to $m_{\mathrm{test}}$; (c) when patching the feature with sequences generated using $m_{\mathrm{patch}}=1024$ but maintaining the same $a$ and $c$, the model often makes predictions with values of $n$ smaller than $m_{\mathrm{patch}}$.}
    \label{fig:multip_cosine}
    \vspace{-0.2 in}
\end{figure}

\textbf{Step ii:} Next, we show how the model estimates $m_{\mathrm{test}}$ in-context in \Cref{fig:multip_cosine}, where we find one specific attention head in the first layer that is responsible for estimating $m_{\mathrm{test}}$. In panel (a), we visualize the attention weights for an input sequence generated with $a=5$, $c=31$, and $m_{\mathrm{test}}=2048$. We find that the head focuses exclusively on the few largest numbers within the context. In panel (b), using the same sequence, we extract output features from this specific head by zeroing out the contributions from all other heads in the first layer, and compute the cosine similarity between this extracted feature and the embedding of all numbers $n$. The head consistently maintains high cosine similarity with the few largest numbers observed within the context, representing a greedy estimation of $m_{\mathrm{test}}$. 

To further confirm that this head generates features corresponding to $m_{\mathrm{test}}$, we perform a patching experiment \cite{zhang2024actpatch}, as shown in panel \Cref{fig:multip_cosine} (c). Here, we generate a sequence with the same $a$ and $c$ but with a different modulus $m_{\mathrm{patch}}=1024$, then use the feature from this patching sequence to replace the feature from the same head in the forward pass of the original sequence. The results show that the model frequently predicts subsequent numbers with values smaller than $m_{\mathrm{patch}}$. We also show in \Cref{fig:patch_appendix} that patching any other heads might ruin the predictions but would never lead to a similar qualitative change, which further confirms that this head is the only one that helps the model estimate $m_{\mathrm{test}}$.


\textbf{Step iii:} After preparing the necessary features, subsequent layers in the model implement the rest of the algorithm. As in the \textbf{\texttt{FM}} case, we observe ladder structures in digit-wise accuracy for the lower digits at early token positions. This pattern, which demonstrates the copying bias, is visible in \Cref{fig:multip_prune} (b2). However, higher digits do not exhibit such ladder structures as the \textbf{\texttt{FM}} case. This discrepancy may arise because the model cannot precisely determine $m_{\mathrm{test}}$, requiring it to dynamically select the radix representation by identifying the most coherent signal at later stages. The potential competition mentioned earlier in \textbf{step i} further supports this argument. Nevertheless, we believe the model implements a similar algorithm for computing the higher bits in this case, which we will elaborate on in \Cref{appendix:unseen}.


\section{Scaling Up the Modulus}
\label{section:scaling_laws}

%%%%%%%%%%%%%%%% OG
% This section analyzes how different components of the model and dataset scale with the modulus. Specifically, we examine how (i) the number of in-context examples, (ii) model depth and width, and (iii) the number of examples $N$ scale as modulus is increased. These results quantify the resources required to predict PRNGs with extensive modulus.
%%%%%%%%%%%%%%%%


\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/byte-tokenization/embeddings.png}
    \caption{Visualization of byte tokenization and abacus embedding. Abacus embedding 1 is shared by all the bytes within the integer, while Abacus embedding 2 varies within the byte but is shared by all integers.
    % For a $b$-byte representation, the first and second Abacus embeddings are implemented as learnable matrices of dimensions $L \times d_{\mathrm{model}}$ and $b \times d_{\mathrm{model}}$, respectively.
    }
    \label{fig:byte-tok}
    \vspace{-0.15 in}
\end{figure}

%%%%%%%%%%%%%%%% OG
% \subsection{In-Context Example Scaling}

% \paragraph{Byte Tokenization:} Inspired by our interpretability results, we implement a byte-level tokenization scheme where each integer is tokenized into its radix representation using a single prime factor. We arrange the tokens such that the model encounters the least significant digit first in the context. This approach reduces the dictionary size for large modulus $m$, thereby improving computational efficiency and scalability.

% \paragraph{Abacus Positional Embeddings:} \maissam{I still don't get exactly what is being done. Maybe a formula would be helpful.}
% For experiments using byte tokenization, we employ a learnable abacus embedding scheme to encode positional information. The positional embedding for each token is constructed as a sum of two learnable abacus embeddings \cite{mcleish2024abacus}. The first embedding encodes the position of an integer within the random number sequence. The second embedding encodes the position of each byte within the integer. 
%%%%%%%%%%%%%%%%

In this section, we investigate Transformer training upon scaling up the modulus of LCG. For this purpose, we make the following modifications to our model:

\textbf{Byte Representation} Inspired by the emergent radix representations observed in the previous section and seeking to avoid massive dictionary sizes for large $m$, we implement a byte-level tokenization scheme. Each integer is decomposed into a sequence of bytes, beginning with the least significant byte. The vocabulary has a size of $256$ for any $m$.

\textbf{Abacus Embeddings} We encode positional information using a variant of the Abacus embedding \cite{mcleish2024abacus}, as a sum of two learnable vectors. One vector encodes the position of the integer within the sequence, while the other encodes the position of each byte within the integer. In \Cref{fig:byte-tok}, we show a visual representation.

% As outlined in \Cref{fig:byte-tok}, for the $j$-th lower byte of the $i$-th number in the sequence, the positional embedding is:
% \begin{align}
%     \mathrm{PosEmbed}(T_{i,j}) = E_{\mathrm{int}}(i) + E_{\mathrm{byte}}(j)
% \end{align}
% where \(T_{i,j}\) denotes the token for the \(j\)-th  byte of the \(i\)-th integer.
% Here, \(E_{\mathrm{int}}(i)\), \(E_{\mathrm{byte}}(j)\) $\in$ \(\mathbb{R}^{d_{\text{model}}}\) are learnable embeddings. \(\mathrm{PosEmbed}(T_{i,j})\) encodes both the global integer index and the local byte position.
    


%%%%%%%%%%%%%% OG
% \subsubsection{Fixed Modulus:} \label{sec:fm_ic_scale}
% \paragraph{Experiment Settings:} 
% For each modulus $p = 2^k$ , where  $k \in [16, 32]$, we train a 2-layer GPT model with an embedding dimension of 1024 and a vocabulary size of 256. The train set consists of $n_a = 1024$ multipliers and $n_c = 1024$ increments, selected via the Hull-Dobell theorem. One LCG sequence of length 512 is included in the train set for each (a, c) pair, resulting in a total training set size of $n_a \times n_c = 1,048,576$. For each modulus, the model is trained for 200,000 steps with a batch size of 512. The context length is  $511 \times $ the digit length of $p$ in the byte representation. For example, when $m = 2^{28}$, the digit length in byte representation is 4, while for $m = 2^{23}$, the digit length is 3.
% \paragraph{Results}
% Fig.~\ref{fig:fixp_scale}a shows the averaged prediction accuracy as a function of the position of the number in the sequence. A key evaluation for LCGs is the spectral test, which measures the quality of the multipliers based on how evenly they distribute points in higher-dimensional spaces.\maissam{What spaces? What does this mean?}\tao{when lcg outputs are plotted in two or more dimensions, they align along distinct lines or hyperplanes, with all possible values constrained to these 2d or higher dimensional structures.} We evaluated our model using spectrally good multipliers identified for modulus $m =  2^{32}$\cite{steele2021}, refered to as Steele multipliers. The model requires 255 in-context examples to achieve 100\% test accuracy with Steele multipliers. For the arbitrary $a$ curv2e, the accuracy is averaged over 512 randomly selected test $a$ values. The steps in the curve for arbitrary $a$ indicate that the model begins to make accurate predictions for some of the randomly selected a values after observing fewer numbers.

% The quality of an LCG improves with larger moduli, as a higher modulus enables a longer period and provides a larger search space for selecting good multipliers. As shown in Fig.~\ref{fig:fixp_scale}b, the minimum in-context examples required for 100\% test accuracy increases sublinearly with the modulus.
%%%%%%%%%%%%%%

\textbf{Fixed Modulus} For \textbf{\texttt{FM}} training, we train a 2-layer model with $d_{\mathrm{model}} = 1024$ and a vocabulary size of $256$. We select training sequences via the Hull-Dobell theorem, setting $n_a = n_c = 1024$ (See \Cref{appendix:scale_up_fixed} for training details). For the test dataset, we choose $a$ and $c$ that differ from those in the training set. 
% LCGs depend on good multipliers, and a key evaluation is the spectral test. In \Cref{fig:fixp_scale}(a), we evaluate our model using both spectrally good multipliers for $m = 2^{32}$ (Steele multipliers \cite{steele2021}) and arbitrary multipliers. 
The quality of an LCG largely depends on its multiplier, traditionally evaluated via the spectral test. In \Cref{fig:fixp_scale}(a), we test our model on both spectrally optimal Steele multipliers \cite{steele2021} and arbitrary multipliers for $m = 2^{32}$. While achieving $100\%$ test accuracy with equal in-context sequence lengths, the model performs consistently worse on Steele-generated sequences compared to those from arbitrary multipliers.
% The model requires 192 in-context examples to achieve $100\%$ test accuracy with both Steele multipliers and arbitrarily chosen multipliers. Here the number of in-context examples refers to the number of test sequence elements seen in-context.  

% In \Cref{fig:fixp_scale}(b), we plot the number of in-context examples required for the model to achieve $100\%$ accuracy over all sequences as a function of $m$. The log-log scale of the plot highlights a sublinear scaling, where the required number of required in-context examples scales like $m^\alpha$, for $\alpha < 1$ (we find $\alpha \simeq 1/4$).
In \Cref{fig:fixp_scale}(b), a log-log plot reveals that the number of in-context examples needed for $100\%$ test accuracy scales sublinearly with modulus $m$ as $m^\alpha$, where $\alpha \approx 1/4$.

% \begin{figure}[!h]
%     \centering

%     \subfloat[Test accuracy for $m=2^{32}$]{
%         \includegraphics[width=0.4\linewidth, height=0.4\linewidth]{figures/byte-tokenization/test_acc_4294967296_sl512_na1024_nc1024_ne1_n1024_h8_d2_ds71_I9_lr0.000100_Twarm5000_T200000_B512_wd1.00.pdf}
%         \label{fig:p=2^32steele}
%     }
%     \hfill
%     \subfloat[Number of in-context examples required for 100\% accuracy]{
%         \includegraphics[width=0.45\linewidth, height=0.45\linewidth]{figures/byte-tokenization/lcg_in_context_examples.pdf}
%         \label{fig:fixedp_example}
%     }    

%     \caption{\textbf{(a)} shows the test accuracy averaged over sequences for $m = 2^{32}$ as a function of number index. The model is tested on Steele multipliers (known for good spectral properties) and randomly chosen multipliers. For Steele multipliers, 100\% test accuracy is achieved with 255 in-context examples. Sequences generated with randomly selected multipliers are easier to predict, potentially due to their poorer spectral properties, such as the concentration of points on a limited number of high-dimensional planes. The test set includes 2048 test c values for each a, with 4 sequences per (a, c) pair. \textbf{(b)} demonstrates that the number of in-context examples required to achieve 100\% test accuracy scales sublinearly with increasing $m$. This was tested using 512 unseen multipliers and 64 unseen increments. Each data point in (b) represents the minimum value across five different initial seeds. The variance among runs is discussed in Appendix \ref{appendix:variance}.}

% \end{figure}

%%%%%%%%%%%% OG
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/byte-tokenization/scaling_modulus.pdf}
%     \caption{(a) Average test accuracy ($m = 2^{32}$) vs number of examples seen in context -- for both Steele and arbitrary multiplier values. A ladder-like structure appears, similar to \Cref{fig:acc_vs_token}. (b) Number of in-context examples required to achieve 100\% test accuracy. Each data point in (b) represents the minimum value across five different initial seeds. The variance among runs is discussed in Appendix \ref{appendix:variance}..}
%     \label{fig:fixp_scale}
% \end{figure}
%%%%%%%%%%%%
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/byte-tokenization/scaling_modulus.pdf}
    \caption{(a) Average test accuracy ($m = 2^{32}$) vs number of in-context examples. (b) The number of in-context examples required to achieve 100\% test accuracy (minimum of 5 runs). (See \Cref{appendix:variance} for details)}
    \label{fig:fixp_scale}
    \vspace{-0.15 in}
\end{figure}

%%%%%%%%%%%% OG
% \subsubsection{Generalization to Unseen Modulus:}\label{sec:um_ic_scale}
% \paragraph{Experiment Settings:} 
% It is more challenging to predict LCG sequences when the modulus is unknown. In this section, we address the scenario where the modulus is both unknown and absent from the training set. Since LCGs are typically defined for moduli that are powers of prime numbers, the model is tested on moduli that are powers of the primes 2, 3, 5, and 7. We train a 6-layer GPT model on a dataset that comprising $n_m = 32,768$ moduli (m), with $n_a = 128$ training multipliers (a) and $n_c = 1$ training increment (c) per modulus. This results in a total of $n_m \times n_a \times n_c = 4,194,304$ sequences of length 512 in the training set. The moduli are sampled from $m_{train} \in \{x \in \mathbb{Z} \mid 1024 \leq x < 65536\}$. The test set consists of 512 unseen $a$ values and 64 unseen $c$ values for each test modulus. Multipliers are selected based on the Hull-Dobell theorem when sufficient qualifying a values are available; otherwise, random a values are used to ensure 128 multipliers for each modulus. The models are trained on 4 million sequences for 2 million steps with a batch size of 128.
% \paragraph{Results:}
% Compared to fixed modulus experiments, generalizing to unseen moduli requires larger models and more training examples, and the test accuracy does not reach 100\%. Fig.~\ref{fig:base256_index} shows the prediction accuracy as a function of the number index for test moduli. Since LCGs are typically defined for moduli that are powers of prime numbers, the model is tested on moduli that are powers of the primes 2, 3, 5, and 7. The test set consists of 512 unseen $a$ values and 64 unseen $c$ values for each test modulus. As shown in Fig.~\ref{fig:um_scaling}a the number of in-context examples required to achieve 60\% test accuracy increases sublinearly with the modulus $m$.

% Test performance is influenced by the tokenization base, exhibiting a bias toward moduli that share the same base as the tokenization method. For instance, when using a byte-level representation, the model achieves higher performance on moduli $p = 2^k$ compared to $p = 3^k$, $5^k$, or $7^k$. As contrasted with Fig.~\ref{fig:um_scaling}b where the tokenization base is $243 = 3^5$, the model performs better on moduli $p = 3^k$. This behavior is likely due to the property of LCGs, where for moduli that are powers of a prime $b$, the lowest $k$-th digit exhibits a period of $b^k$. Tokenization in such a base highlights this periodic structure, making it more apparent and easier for the model to leverage during training and prediction. 
%%%%%%%%%%%%

\textbf{Generalization to unseen modulus} For \textbf{\textbf{\texttt{UM}}} case, we train a 6-layer Transformer on a dataset with $n_m = 32,768, n_a=128, n_c=1$ ($1024 < m_{\mathrm{train}} < 65536$), with 512 numbers in each sequence. As before, we use unseen values of $m_{test}$ and $a, c$ for the test dataset -- with a focus on $m_{test}$ of the form $2^k$ and $3^k$. In \Cref{fig:um_scaling}, we show how the number of in-context sequence elements is required to reach $60\%$ test accuracy scales with $m$. The averaged test accuracy of each number in the sequence is shown in \Cref{appendix:scale_up_unseen}. Test performance is influenced by the tokenization base, since the tokenization base highlights the periodic structure of LCGs making it more apparent and easier for the model to leverage during training and prediction. In \Cref{fig:um_scaling} we observe that $m=2^k$ ($m=3^k$) sequences offer favourable scaling when the tokenization base is $256=2^8$ ($243=3^5$). 

% \begin{figure}[ht]
%     \centering
%     \subfloat[Tokenization base = $2^8$.]{
%         \includegraphics[width=0.42\linewidth, height=0.4\linewidth]{figures/byte-tokenization/replot_acc_curve_b256_d2_prime2357_512testa_64testc.pdf}
%         \label{fig:base256_index}
%     }
%     \hfill
%     \subfloat[Tokenization base = $3^5$.]{
%         \includegraphics[width=0.42\linewidth, height=0.4\linewidth]{figures/byte-tokenization/replot_acc_curve_b243_d2_prime2357_512testa_64testc.pdf}
%         \label{fig:base243_index}
%     }


%     \caption{Generalization to Unseen Moduli \tianyu{try log-log for (c) and (d) and move (a)(b) to appendix?} \tao{i want to include at least one plot shows acc vs index for unseenp case, either here or before section5}}
%     \label{fig:unseenp_example}
% \end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/byte-tokenization/unseen_scaling_final.pdf}
    \caption{In-context eample scaling for \textbf{\texttt{UM}}: The number of in-context sequence elements required to achieve 60\% test accuracy shows a sublinear dependence on the modulus $m$. The scaling depends on the compatibility between $m$ and tokenization. (a) base-$2^8$ tokenization, (b) base-$3^5$ tokenization.}
    \label{fig:um_scaling}
    \vspace{-0.05 in}
\end{figure} 

\vspace{-0.5em}
\section{Discussion}
We have investigated Transformer training on LCG sequences, focusing on fixed modulus training as well as generalization to unseen moduli. In both cases, we have uncovered the algorithm used by the model to solve these tasks, and highlighted the model components that implement the steps of the algorithm. We have found that the model finds and utilizes prime factorizations of $m$ and mixed-radix representations of numbers to simplify the sequences and make predictions. We have provided the modified training recipe for scaling up the modulus in both  \textbf{\texttt{FM}} and \textbf{\texttt{UM}} settings, and shown their scaling behaviors.

\textbf{Limitations and future work:} 
While we scale the \textbf{\texttt{FM}} setting to $m=2^{32}$ the We only scale \textbf{\texttt{UM}} setting up to $m=2^{16}$ in this work.
We also leave the exploration of PRNGs that are built upon LCG, such as PCG and TCG for future works.


% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\newpage
\section*{Impact Statement}

% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

Our work advances the understanding of how neural networks learn deterministic sequences, specifically LCGs. While this capability cannot compromise mainstream cryptographic systems, which use far more sophisticated techniques, our insights may contribute to the development of more robust cryptographic algorithms and a better understanding of neural networks' computational capabilities. 

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{ICML2025/ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

% \section{Linear Congruential Generator (LCG)}
% \label{appendix:prngs_lcg}

% \paragraph{The Hull–Dobell Theorem:} When the modulus $p$ is a power of $2$ and $c \neq 0$, then the LCG sequence has a period of $p$ for all seed values if and only if the following conditions are met:

% \begin{enumerate}
%     \item $p$ and $c$ are coprime
%     \item $a-1$ is divisible by all prime factors of $p$
%     \item $a-1$ is divisible by $4$ if $p$ is divisible by $4$.
% \end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Fixed Modules Training Results}
\label{appendix:fm_train}

For the \textbf{\texttt{FM}} case, we find that a single attention head in one layer is sufficient to solve the task. In \Cref{figapp:accuracy_vs_step_m=2048}, we present the model's training loss and performance across training steps for $m=2048$. Notably, panels (c, f) reveal a significant disparity between training and test loss, indicating a grokking transition during the training process. Similarly, we plot in \Cref{figapp:accuracy_vs_step_m=7776} for similar curves for $m=776$, where all curves are qualitatively the same.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/fixed_p/app_acc_vs_step_final_m=2048.pdf}
    \caption{Test accuracy and train/test loss for $m=2048=2^{11}$, depth=1. (a,b,c) $n_{heads}=1$ (d,e,f) $n_{heads}=4$.}
    \label{figapp:accuracy_vs_step_m=2048}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/fixed_p/app_acc_vs_step_final_m=7776.pdf}
    \caption{Test accuracy and train/test loss for $m=7776=2^5 \cdot 3^5$, depth=1. (a,b,c) $n_{heads}=1$ (d,e,f) $n_{heads}=4$.}
    \label{figapp:accuracy_vs_step_m=7776}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Critical Depth for the Unseen Modulus Task}
\label{appendix:critical_depth}

\begin{figure*}[!h]
\centering
\begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/depth-width-scaling/test_last_acc_scale_heatmap_np256_p1024_Tn256_N400000_ne1_I1_Tw4096_T100000_B256.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/depth-width-scaling/test_last_acc_scale_heatmap_np1024_p4096_Tn256_N400000_ne1_I1_Tw4096_T100000_B256.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/depth-width-scaling/test_loss_heatmap_np4096_p16384_Tn256_N400000_ne1_I1_Tw4096_T200000_B256.pdf}
\end{minipage}
\caption{Test accuracy heatmaps of }
\label{fig:critical-depth-appendix}
\end{figure*}



This section analyzes the depth and embedding dimension requirements for successfully training a Transformer on the unseen modulus task. We generate  $N = 4 \times 10^5$ training examples covering at least $m_{\text{eval}} / 4$ moduli in the training dataset.

We varied depths $ \in \{2,3,4,6,8\}$ and embedding dimensions in $d_{\text{model}} \in \{512,768,1024,1280\}$, with the head dimension fixed to $d_{\text{head}} = 128$. For each depth and width, we scanned learning rates $\eta \in \{3\times10^{-5}, 1\times10^{-4}, 3\times10^{-4}, 1\times10^{-3}\}$ and weight decay strengths $\lambda \in \{0.01, 0.1, 1.0, 3.0\}$ to identify the optimal hyperparameters. We report that the optimal learning rate and weight decay strength heavily vary with depth and embedding dimension and training dataset.
For $m_{\text{eval}} = \{1024, 4096 \}$, the models were trained for $T=100,000$, while for $m_{\text{eval}} = 16, 384$, the models required a longer training for $T = 200,000$ steps. In both cases, we used a batch size of $256$ and a linear learning rate warmup over $4,096$ steps using AdamW with hyperparameters  $\beta_1=0.9$, $\beta_2=0.99$.

\Cref{fig:critical-depth-appendix} shows the test accuracy heatmaps with depth and embedding dimensions as the two axes. These results demonstrate that a minimum depth of $3$ is required to learn the LCG sequence prediction task, with a marginal dependence on embedding dimension. This suggests the unseen modulus task requires a minimal computational depth of three to capture the underlying structure of LCGs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Training Time Interpratibility}
\label{appendix:train-interp}


\begin{figure*}[!h]
\centering
% Attention KQV
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/training-interp/train_last_acc_step_p2048_T0_T2048_Tn512_na128_nc128_ne1_n768_h4_d6_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/training-interp/train_last_acc_step_p2048_T0_T256_Tn512_na128_nc128_ne1_n768_h4_d6_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
    % \subcaption{}
\end{minipage}
\begin{minipage}[b]{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/training-interp/train_last_acc_step_p2048_T256_T2048_Tn512_na128_nc128_ne1_n768_h4_d6_I1_lr0.000300_Tw2048_T100000_B256_wd1.0.pdf}
    % \subcaption{}
\end{minipage}

\caption{(left) Comparison of the training accuracy of sequences with different periods relative to the context length $512$, (center) Accuracy when the model is only trained on sequences with period $ < 512$, (right) Accuracy when the model is trained on sequences with period $> 512$.}
\label{fig:train-interp-appendix}
\end{figure*}


In this section, we examine the order in which training sequences with different periods are learned during training. For this experiment, we consider a six-layer Transformer with $4$ heads and an embedding dimension of $768$. For this experiment, we generate sequences of length $512$ using $128$ unseen moduli and $128$ values of $a$ and $c$ each. 
The model is trained with Adam hyperparameters: $\eta = 3 \times 10^{-4}$ $\beta_1 = 0.9, \beta_2 = 0.99$ and weight decay strength $\lambda = 1.0$. 

\Cref{fig:train-interp-appendix}(left) compares the training accuracy of sequences with different periods relative to the context length $512$. We observe that sequences with period $< 512$ are memorized early in training, while the accuracy of long-period sequences coincides with the test accuracy. Next, we perform two more experiments by training the model on datasets consisting of (1) sequences with period $ < 256$, and (2) sequences with period $ > 512$. \Cref{fig:train-interp-appendix}(center, right) show the results of these experiments. The model fails to generalize when trained only on low-period sequences while training only on long-period sequences eliminates grokking.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{More Interpretability}
\label{appendix:interp}

\subsection{Fixed Modulus}
\label{appendix:fixed_interp}
In this subsection, we show extra results on the model's behavior...

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.96\linewidth]{figures/fixed_p/app_2048_accuracy.pdf}
    \caption{Test accuracy for $m=2048=2^{11}$, depth=1, $n_{heads}=1, d_{model}=768$. (a) Test accuracy averaged over $a, c$ and initial seeds. (b) Multiplication of per-digit test accuracies -- matches exactly with the average test accuracy. (c) Per-digit accuracy in binary representation. (d) Per-digit accuracy of an ablated model: For each query, mask out all attention weights except for the top 2, which are $2^k$ and $2^{k-1}$ distance backwards in the context}
    \label{figapp:accuracy_m=2048_h1}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/fixed_p/app_2048_h4_accuracy.pdf}
    \caption{Test accuracy for $m=2048=2^{11}$, depth=1, $n_{heads}=4, d_{model}=768$. (a) Test accuracy averaged over $a, c$ and initial seeds. (b) Multiplication of per-digit test accuracies -- matches exactly with the average test accuracy. (c) Per-digit accuracy in mixed radix representation.}
    \label{figapp:accuracy_m=2048_h4}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fixed_p/app_7776_accuracy.pdf}
    \caption{Test accuracy for $m=7776=2^5\, 3^5$, depth=1, $n_{heads} \in \{1, 4\}, d_{model}=768$. (a,d) Test accuracy averaged over $a, c$ and initial seeds. (b,e) Multiplication of per-digit test accuracies -- matches exactly with the average test accuracy. (c,f) Per-digit accuracy in mixed radix representation.}
    \label{figapp:accuracy_m=7776_h1}
\end{figure}



\begin{figure}[!h]
    \centering
    \includegraphics[width=0.96\linewidth]{figures/fixed_p/app_acc_vs_token_ac_final_m=2048.pdf}
    \caption{Test accuracy vs token positions for $m=2048=2^{11}$, depth=1, various values of $a, c$. (a,b) $n_{heads}=1$ (c,d) $n_{heads}=4$. The accuracies for different $a$ and $c$ are exactly on top of each other.}
    \label{figapp:accuracy_vs_token_ac_m=2048}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/fixed_p/app_embeddings_m=512.pdf}
    \caption{Projections along Top 6 principle components of the embedding matrix, for $m=512$, depth=1, $n_{heads}=1, d_{model}=768$.}
    \label{figapp:embedding_m=512}
\end{figure}

\subsection{Unseen Modulus}
\label{appendix:unseen}

\paragraph{Pruning heads corresponding to irrelevant prime factors} To further validate our findings from \Cref{fig:multip_prune} regarding the correlation between attention heads and digit-wise accuracy, we conduct additional pruning experiments. The results of these experiments are presented in \Cref{fig:prune_appendix_2048,fig:prune_appendix_2352}.

\begin{figure}[!h]
    \centering
    \subfloat[Original Model] {\includegraphics[width=0.32\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \subfloat[Prune modulo $3$ head] {\includegraphics[width=0.32\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_l0h4_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \subfloat[Prune modulo $14$ head] {\includegraphics[width=0.32\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_l0h0_p2048_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \caption{Per-digit accuracy for $m_{\mathrm{test}}=2048$ after pruning specific attention heads. (a) Results from the same model used in \Cref{sec:interp_unseen}, identical to \Cref{fig:multip_prune} (b2); (b) Performance after pruning the attention head responsible for grouping numbers by their values modulo $3$, which is irrelevant for solving sequences with $m_{\mathrm{test}}$ (containing only the prime factor $2$). After pruning, the model's performance shows marginal improvement for specific early bits at lower token positions; (c) Performance after pruning the attention head responsible for grouping numbers by their values modulo $14$. The model's performance on $m_{\mathrm{test}}$ decreases significantly, as this head partially contributes to processing relevant prime factors. However, since the primary head responsible for binary representation remains intact, the model maintains partial functionality.}
    \label{fig:prune_appendix_2048}
\end{figure}

\begin{figure}[!h]
    \centering
    \subfloat[Original Model] {\includegraphics[width=0.31\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_p2352_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \subfloat[Prune modulo $3$ head] {\includegraphics[width=0.31\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_l0h4_p2352_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \subfloat[Prune modulo $2$ head] {\includegraphics[width=0.31\linewidth]{figures/multi_interp/prune_appendix/acc_digit_new_l0h3_p2352_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_B256_wd1.0.pdf}}
    \caption{Per-digit accuracy for $m_{\mathrm{test}}=2352$ after pruning specific attention heads. (a) Results from the same model used in \Cref{sec:interp_unseen}, identical to \Cref{fig:multip_prune} (b2); (b) Performance after pruning the attention head responsible for grouping numbers by their values modulo $3$, which is relavent for one specific digit in this case. After pruning, the model's performance shows a clear degradation, with the strongest one happening exactly at the digit corresponding to modulo $3$ (c) Performance after pruning the attention head responsible for grouping numbers by their values modulo $2$. The model's performance on $m_{\mathrm{test}}$ got obliterated, as there are many base-$2$ digits in the radix representation of $m_{\mathrm{test}}$ in this case.}
    \label{fig:prune_appendix_2352}
\end{figure}

\paragraph{Patching other heads} In \Cref{fig:patch_appendix}, we present patching experiments for heads not previously shown in \Cref{fig:multip_cosine}. We focus only on selected heads where patching significantly affects predictions. Notably, none of these cases exhibit qualitative changes stemming from modulus alterations, further supporting our assertion that the head shown in \Cref{fig:multip_cosine} is specifically responsible for estimating $m_{\mathrm{test}}$.

\begin{figure}[!h]
    \centering
    \subfloat[layer 1, head 4] {\includegraphics[width=0.48\linewidth]{figures/multi_interp/patch_appendix/newp1024a5c31_layer1_h4_p1024_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_T100000_B256_wd1.0.pdf}}
    \subfloat[layer 2, head 4]{\includegraphics[width=0.48\linewidth]{figures/multi_interp/patch_appendix/newp1024a5c31_layer2_h4_p1024_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_T100000_B256_wd1.0.pdf}} \\
    \subfloat[layer 3, head 6]{\includegraphics[width=0.48\linewidth]{figures/multi_interp/patch_appendix/newp1024a5c31_layer3_h6_p1024_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_T100000_B256_wd1.0.pdf}} 
    \subfloat[layer 4, head 1]{\includegraphics[width=0.48\linewidth]{figures/multi_interp/patch_appendix/newp1024a5c31_layer4_h1_p1024_T0_T2048_Tn256_na128_nc128_np128_ne1_n768_h6_d4_I1_lr0.000100_Tw2048_T100000_B256_wd1.0.pdf}}
    \caption{Patching experiments following the setting of \Cref{fig:multip_cosine} in the main text. None of these heads, after patching, make the model believe that the modulus is close to $m_{\mathrm{patch}}$.
    }
    \label{fig:patch_appendix}
\end{figure}

\paragraph{Evidence for Step iii} In \Cref{fig:attn_stats}, we present attention patterns and token distance statistics for a selected attention head in later layers, with the model the same as the one used in \Cref{sec:interp_unseen}. The token distances are measured by the spacing between keys with top-$4$ attention weights for a given query. Our analysis reveals that the model develops a head capable of dynamically adjusting their lookback distance for computations based on $m_{\mathrm{test}}$, which we interpret as evidence for \textbf{step iii} of the algorithm proposed in \Cref{sec:interp_unseen}.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/multi_interp/attn_stats_compare.pdf}
    \caption{Attention patterns and token distance statistics for layer 2, head 3 of the model analyzed in \Cref{sec:interp_unseen}. (a1, b1) Results for a sequence with $m_{\mathrm{test}}=2048$, $a=5$, and $c=31$. The statistics reveal that the model consistently looks back at distances that are multiples of $4$, which divides $m_{\mathrm{test}}$ and enables the correct copy behavior. (a2, b2) Results for a sequence with $m_{\mathrm{test}}=2352$, $a=85$, and $c=5$. In contrast to panels (a1, b1), the same attention head now consistently looks back at distances that are multiples of $14$, which allows the model to copy lower digits from the context. This adaptive behavior demonstrates that the model has acquired the ability to dynamically adjust its lookback distance by $r$ iterations to copy the lower digits and leave the higher digits to the later layers for computation.}

    \label{fig:attn_stats}
\end{figure}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training Variance}
\label{appendix:variance}

The model may converge to different solutions depending on the random seed used for initialization and the random seed for dataset generation. \Cref{{fig:med_fixp_appendix}} shows the median value across five runs, with the shaded region representing the range between the minimum and maximum values. For larger moduli, not all models successfully find a solution that achieves $100\%$ test accuracy.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.32\linewidth]{figures/byte-tokenization/med_lcg_in_context_examples.pdf}
    \caption{The median number of in-context sequence elements required for $100\%$ test accuracy across five runs. The min-max range is shown as a shaded region.}
    \label{fig:med_fixp_appendix}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scaling Up the Modulus}
\label{appendix:scaleup}

\subsection{Fixed Modulus}\label{appendix:scale_up_fixed}
For each modulus $m = 2^k$, where  $k \in [16, 32]$, we train a 2-layer GPT model with an embedding dimension of 1024 and a vocabulary size 256. The train set consists of $n_a = 1024$ multipliers and $n_c = 1024$ increments, selected via the Hull-Dobell theorem. One LCG sequence of length 512 is included in the train set for each (a, c) pair, resulting in a total training set size of $n_a \times n_c = 1,048,576$. For each modulus, the model is trained for 200,000 steps with a batch size of 512. The context length is  $511 \times $ the digit length of $p$ in the byte representation. For example, when $m = 2^{28}$, the digit length in byte representation is 4, while for $m = 2^{23}$, the digit length is 3.

\subsection{Unseen Modulus}\label{appendix:scale_up_unseen}
We train a 6-layer GPT model on a dataset that comprises $n_m = 32,768$ moduli, with $n_a = 128$ training multipliers and $n_c = 1$ training increment per modulus. This results in a total of $n_m \times n_a \times n_c = 4,194,304$ sequences of length 512 in the training set. The moduli are sampled from $m_{train} \in \{x \in \mathbb{Z} \mid 1024 \leq x < 65536\}$ in Fig.~\ref{fig:acc_um_appendix}a, from $m_{train} \in \{x \in \mathbb{Z} \mid 1024 \leq x < 59049\}$. Multipliers are selected based on the Hull-Dobell theorem when sufficient qualifying $a$ values are available; otherwise, random $a$ values are used to ensure $128$ multipliers for each modulus. The models are trained on 4 million sequences for 2 million steps with a batch size of $128$.
Because LCGs are typically defined for moduli that are powers of prime numbers, the model is tested on moduli that are powers of the primes 2, 3, 5, and 7. The test set consists of 512 unseen $a$ values and 64 unseen $c$ values for each test modulus. 

Test performance is influenced by the tokenization base, exhibiting a bias toward moduli that share the same base as the tokenization method. For instance, in Fig.~\ref{fig:acc_um_appendix} (a) when using a byte-level representation, the model achieves better performance on moduli $p = 2^k$ compared to $p = 3^k$, $5^k$, or $7^k$. As contrasted with Fig.~\ref{fig:acc_um_appendix} (b) where the tokenization base is $243 = 3^5$, the model performs better on moduli $p = 3^k$. This behavior is likely due to the property of LCGs, where for moduli that are powers of a prime $b$, the lowest $k$-th digit exhibits a period of $b^k$. Tokenization in such a base highlights this periodic structure, making it more apparent and easier for the model to leverage during training and prediction. 

\begin{figure}[!h]
    \centering
    \subfloat[Tokenization base = $256 = 2^8$] {\includegraphics[width=0.4\linewidth]{figures/byte-tokenization/replot_acc_curve_b256_d2_prime2357_512testa_64testc.pdf}}
    \subfloat[Tokenization base = $243 = 3^5$] {\includegraphics[width=0.4\linewidth]{figures/byte-tokenization/replot_acc_curve_b243_d2_prime2357_512testa_64testc.pdf}}
    \caption{Test accuracy vs Number index. In (a), the moduli 2048 and 16384 (blue curves) have the same root 2 as the tokenization base 256. The model performs better on these two moduli. In (b), the moduli 2178 and 19683 (orange curves) have the same root 3 as the tokenization base 243. The model performs better on these two moduli. }
    \label{fig:acc_um_appendix}
\end{figure}





\end{document}




% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
