% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

%\PassOptionsToPackage{draft}{hyperref}
\documentclass[11pt]{article}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{hyperref} 
\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{algorithmic}
%\usepackage[noend]{algorithmic}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% \usepackage[T1]{fontenc} % commented by me
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc} % I commented this line


% Added by me
\usepackage[LFE,LAE,T1]{fontenc}
\usepackage[arabic, main=english]{babel}
\usepackage[textsize=tiny]{todonotes}

% Standard package includes
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{pifont}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

%Mansour
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{subfigure}
\newcommand{\mynote}[1]{\textcolor{red}{#1}} % Red color for notes


% Added by me
\usepackage[arabic, main=english]{babel}
%\usepackage[textsize=tiny]{todonotes}

\setlength{\marginparwidth}{0.6in}
\newcommand{\allnotes}[1]{}
\renewcommand{\allnotes}[1]{#1} % Comment to turn off notes
%\newcommand{\qian}[1]{\allnotes{\todo[color=yellow!30]{QL: #1}}}


% end added by me

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{evaluateing Text Watermarking in a Cross-lingual Setting}
% \title{Your Language Has Been Disguised! Exposing a New Threat in Cross-Lingual Watermarking by Concealing the Textâ€™s Original Language}
\title{Uncovering the Hidden Threat of Text Watermarking from Users with Cross-Lingual Knowledge}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Mansour Al Ghanim \enskip Jiaqi Xue \enskip Rochana Prih Hastuti\enskip Mengxin Zheng \enskip Yan Solihin \enskip Qian Lou \\
University of Central Florida \\
\texttt{\{mansour.alghanim,jiaqi.xue,rochana,mengxin.zheng,yan.solihin,qian.lou\}@ucf.edu} \\
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
In this study, we delve into the hidden threats posed to text watermarking by users with cross-lingual knowledge. While most research focuses on watermarking methods for English, there is a significant gap in evaluating these methods in cross-lingual contexts. This oversight neglects critical adversary scenarios involving cross-lingual users, creating uncertainty regarding the effectiveness of cross-lingual watermarking. We assess four watermarking techniques across four linguistically rich languages, examining watermark resilience and text quality across various parameters and attacks. Our focus is on a realistic scenario featuring adversaries with cross-lingual expertise, evaluating the adequacy of current watermarking methods against such challenges.

% We present a study to evaluate representative state-of-the-art watermarking methods in cross-lingual settings. The current literature focuses on the evaluation of watermarking methods for the English language. However, the literature for evaluating watermarking in cross-lingual settings is scarce. This results in overlooking important adversary scenarios in which a cross-lingual adversary could be in, leading to a gray area of practicality over cross-lingual watermarking. In this paper, we evaluate four watermarking methods in four different and vocabulary rich languages. Our experiments investigate the strength of the watermarking procedure and text quality under different parameter settings and attack scenarios. Specifically, we investigate a practical scenario that an adversary with cross-lingual knowledge could take, and evaluate whether current watermarking methods are suitable for such scenario.
\end{abstract}

\section {Introduction}
The advancement of Large Language Models (LLMs) has significantly transformed text generation across various domains, producing outputs that often closely mimic human writing. This advancement has raised concerns within the research community regarding potential misuses, including academic misconduct, the spread of disinformation, and the creation of synthetic training data~\citep{xue2024badrag, bender2021dangers, xue2024trojllm, zheng2023trojfsp,xue2024badfair, cai2024wbp}. In response to these challenges, watermarking methods have been developed to differentiate between human-written and AI-generated texts~\citep{aaronson_my_2022,kirchenbauer2023watermark,kuditipudi2023robust,he2024can,dathathri2024scalable,chang2024postmark}.

Watermarking involves embedding a signal in AI-generated texts to identify the generating LLM using hypothesis testing. Specifically, watermarking offers theoretical guarantees regarding the detectability of the embedded signal by performing statistical inference on the generated text and testing against the null hypothesis. Since watermarking alters the generation of the original LLM, it is crucial to ensure that the impact on text quality is minimal while maintaining the watermark's detectability. \footnote{Although steganography and watermarking share the practice of embedding signals, steganography primarily aims to conceal information through alterations of the text's meaning, whereas watermarking serves to assert text source without changing the text's semantics.}

Much of the existing literature on watermarking~\cite{zhang2025towards} has primarily focused on the quality and detectability of watermarked texts, with a predominant emphasis on English texts. While the theoretical principles of these methods can be applied across languages, cross-lingual studies can reveal new adversarial scenarios in which watermark signal could be removed, and that have yet to be thoroughly investigated.

A recent work by \citet{he2024can} addressed adversarial scenarios within a cross-lingual context, examining the consistency of watermarking through translation attacks, and developed defenses in this specific setting. 
Other existing studies on watermarking robustness have largely regarded back translation as the primary method for examining the interplay between languages, often overlooking other potential adversarial scenarios that may emerge following a translation to non-English languages ~\citep{kuditipudi2023robust, zhao2023provable, pang2024no}. This oversight accounts for both unintentional alterations made by users for clarity or context adjustment and sophisticated adversaries intentionally refining the text to destroy watermark traces. 
% Additionally, a broader assessment of state-of-the-art (SOTA) watermarking techniques in diverse languages is necessary, particularly given the potential for more practical attacks, including translations followed by textual edits.
In this paper, we evaluate four representative watermarking methods under two high-level themes. First, syntactical watermarking, which involves syntax changes to the generated text by manipulating the log-probabilities (logits)~\cite{lou2022trojtext} before the decode stage. Second, Semantic watermarking, which involves semantics manipulation of the generated text before the decode stage as well. The syntactical methods we consider are KGW ~\citep{kirchenbauer2023watermark}, EXP ~\citep{aaronson_my_2022}, and Unigram ~\citep{zhao2023provable}. For the semantic methods, we choose XSIR ~\citep{he2024can} as it aligns with our cross-lingual investigation in this paper.
% \mynote{SHOULD WE REMOVE THIS? KGW and EXP can be regarded as representative methods of using context hashing techniques. Unigram is regarded as a robust watermarking technique that doesn't employ context hashing, and instead hash the watermark key for all generations. XSIR employs semantic context hashing, and uses slightly different method to embed the watermark. We explain the details of these methods in Section ~\ref{sec:methods}.}
% We investigate scenarios that involves not only translation or paraphrasing but a combination of translation followed by optional paraphrasing and then translating back into the original language.
Specifically, this paper addresses the following research questions.

\begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt, parsep=5pt]
\item \textbf{RQ1}: How do watermarking methods perform across languages in terms of detectability, text quality, and diversity?
\item \textbf{RQ2:} How resilient are watermarking methods against existing cross-lingual attacks, particularly translation attacks?
\item \textbf{RQ3}: How do different adversarial approaches to cross-lingual watermark evasion perform? Specifically, how do attacks fare under our novel asymmetrical threat model, where the output language differs from the pivotal language?
\end{itemize}

Our investigation reveals that current watermarking methods face significant challenges in cross-lingual settings. While methods like XSIR shows promise for cross-lingual scenarios, all current approaches demonstrate vulnerabilities to different translation-based attacks. Furthermore, we find that traditional quality metrics may not adequately capture the diversity of cross-lingual watermarked text, necessitating new evaluation approaches. The main contributions of this paper are as follows:
\begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt, parsep=5pt]
    \item We investigate the role of text watermarking across four different languages and four SOTA text watermarking methods, with a concentration on analyzing watermark detectability and quality.
    \item We propose a new text diversity metric that utilizes Self-BLEU to effectively evaluate the quality of watermarked text generated in a cross-lingual context.
    \item We evaluate the detectability of watermarks under practical attack pipeline consisting of translation, translation then paraphrase, and subsequent translation to the original language, and highlight the results on a robust cross-lingual method.
    % \item We introduce a high-level discussion of mitigation strategies to combat adversarial attacks on watermarked text.
\end{itemize}

% \mynote{ ** subject to removal ** 4) What are the key vulnerabilities of different watermarking approaches when subjected to cross-lingual attacks, and how do these vulnerabilities vary across different languages?}

\section{Background and Related Work}

\textbf{Syntactical-based Watermarking.} Syntactical-based watermarking involves editing the generated text by manipulating the log-probabilities of a certain model. Multiple studies in the literature has investigated this type by introducing hypothesis testing. Pioneering in this area, the studies in KGW~\citep{kirchenbauer2023watermark, kirchenbauer2023reliability} and EXP~\citep{aaronson_my_2022} generate the watermark as a function of hashing the previous $k-1$-gram of a prompt to generate the next $k$ token. They introduce hypothesis testing to provide theoretical guarantees of watermark detectability. This is done by performing a statistical inference on the generated text, and calculating a score that is compared against a threshold with a very low False Positive Rate (FPR). Consequently, a barrage of similar watermark studies have been conducted to investigate the robustness and quality of KGW and EXP watermarks. In Unigram~\citet{zhao2023provable}, the authors argue the robustness of the watermark to removal attacks could be mitigated by hashing a pre-determined key (uni-gram) that is used for all generations.~\citet{kuditipudi2023robust, christ2024undetectable} introduced distortion-free watermarking in which the distribution of watermarked and unwatermarked text is the same. Similarly,~\citet{dathathri2024scalable} introduces speculative sampling to generate watermarks at scale.~\citet{lu2024entropy, lee2023wrote} investigated the role of token entropy to the watermark detectability and quality and proposes methods for generating and detecting the watermark. 

\noindent\textbf{Semantic Watermarking.} Syntactical watermarking can still be compromised by paraphrasing and back-translation watermark removal attacks. In response to this, a number of studies have been conducted to evaluate watermarking methods in such scenarios. SIR~\citep{liu2024a} introduced semantic hashing of previous context instead of tokens to combat security and robustness attacks related to k-gram methods. Consequently,~\citet{he2024can} introduced XSIR, which is an extension of SIR with cross-lingual settings in which semantically cross-lingual words are clustered together. Unlike SIR, XSIR is able to mitigate translation attacks between different languages. Most recently, the work in ~\citet{chang2024postmark} introduces the use of watermarking in a blackbox setting where users can semantically mark generated output of closed-source LLMs without accessing the log-probabilities for intellectual property reasons. Similarly,~\citet{hou2023semstamp, hou2024k} introduce sentence-level clustering to semantically mark the generated output.

\noindent\textbf{Post-hoc Detection Approaches.}
In our evaluate, we evaluate proactive detection methods in which the watermark signal is embedded in the text during generation. However, 
multiple studies have investigated passive/discriminator methods for AI-generated text detection.~\citet{tian_gptzero_2023,mitchell_detectgpt_2023,gehrmann2019gltr} use some statistical patterns in AI-generated text and trained discriminator models that differentiate human-written from AI-generated texts.~\citet{alshammari2024toward} introduces a method to detect AI-generated text in Arabic language by leveraging language-specific diacritics.~\citet{abdelnabi2021adversarial} investigates adversarial text watermarking by modifying the inner workings of a transformer model to embed a hidden watermark.


\section{Threat Model}
In our threat model for text watermarking, we adopt a practical pipeline for potential watermark removal attacks, recognizing the various methods an adversary might employ. This pipeline includes stages such as simple translation, translation followed by paraphrasing, and a combination of translation, paraphrasing, and translating back again to the original language as seen in Figure \ref{fig:threat-model}. 
We consider a setting where LLMs are closed-source, in which case the model owners are the main victim of watermark removal attacks.

% \begin{figure}
%   \includegraphics[width=0.45\textwidth]{latex/images/threat model - 1 column - no flag.png}
%   \caption{Our Attack Pipeline. Translation involves translation to other languages then directly use or proceed to the next phase. In Each phase, the watermark signal weakens. Back translation restores some of the watermark signal.}
%   \label{fig:threat-model}
% \end{figure}

\begin{figure*}
  \includegraphics[width=0.98\textwidth]{latex/images/Threat_model_improved.pdf}
    % \vspace{-3mm}
  \caption{Existing symmetric cross-lingual attacks in comparison to our asymmetric attacks. (a) In Symmetrical translation attacks, a pivot language is used to \ding{182} translate the prompt. Then the original language is obtained \ding{183} without further edits are considered. (b) In Asymmetrical translation, the user \ding{182} translates to a target language. The user can use directly or \ding{183} optionally edit the text, further attenuating the watermark signal. Stage \ding{184} is also optional, which restores some of the watermark signal.}
  \label{fig:threat-model}
\end{figure*}

\noindent\textbf{Attacker Knowledge.} We consider a multilingual adversary who may operate at varying levels of sophistication. The adversary might be naive, unaware of the presence of watermarks, or advanced, intentionally attempting to evade detection. They understand how to utilize technological tools that can alter text, potentially impacting the embedded watermarks. It's crucial to consider that regular users might unintentionally remove watermarks by prompting an LLM directly in one language before translating and making textual edits as can be seen in Figure~\ref{fig:threat-model} (b) at stage \textcircled{2}. Ignoring this setting can mislead focus in the literature toward only intentional attacks utilizing pivotal languages as shown in Figure~\ref{fig:threat-model} (a) stage \textcircled{2}.

\noindent\textbf{Attacker Capability.} The adversary is equipped with the ability to access AI chat interfaces, through which they begin prompting and engaging with content. They can leverage translation APIs to translate texts into different languages, which may weaken or eliminate watermark signals. Moreover, they have the capability to use probably less sophisticated LLM to paraphrase the text, thereby further disrupting the watermark signal. 

This comprehensive threat model highlights the challenges that the watermarked text goes through in a cross-lingual setting. Our threat model underscores the necessity of developing sophisticated watermarking methods that can endure these intricate and pipelined attacks.

\section{Methodology}\label{sec:methods}
\subsection{Quality Evaluation}
% shorter version
Evaluating watermarked text quality is essential for assessing watermark methods. Most studies focus on English, so we introduce a pipeline for multi-lingual LLMs. Initially, we used perplexity (PPL), but it wasn't effective across languages. PPL results can be shown in Appendix~\ref{sec:appendix-quality}, Figures~\ref{fig:z-scores-vs-ppls-kgw-unigram} and ~\ref{fig:z-scores-vs-ppls-others}. For better assessment, we use GPT-Judger from Singh et al. (2023) with an advanced OpenAI model. Due to k-gram repetitions in watermarked text, we also apply Self-BLEU to measure diversity and repetition. We combine these methods to create a metric that adjusts diversity more effectively than relying solely on Self-BLEU.

% Longer version
% Examining the quality of watermarked text is key in evaluating watermark methods. Most studies in the literature evaluates watermarking text quality in English. Therefore, before we delve into the attack pipeline, we present a pipeline to examine the quality of watermarked text generated from multi-lingual LLMs. Initially, we examined text quality using perplexity (PPL), but due to the richness of the text in different languages, PPL wasn't very helpful. We include the results in Appendix~\ref{sec:appendix-quality} Figures~\ref{fig:z-scores-vs-ppls-kgw-unigram} and ~\ref{fig:z-scores-vs-ppls-others} to provide a useful statistical or probabilistic background for assessing text quality under watermarking. To obtain better quality assessment, we utilize the GPT-Judger from ~\citet{singh2023new} using an advanced OpenAI model to judge watermarked and unwatermarked text. Because text watermarking has been shown to have k-gram repetitions ~\citep{kirchenbauer2023watermark, dathathri2024scalable, chang2024postmark}, we also utilize Self-BLEU ~\citet{zhu2018texygen} to quantify the diversity and repetition in text. Finally, we leverage both methods to come up with a metric to adjust the diversity of the watermarked text rather than using Self-BLEU directly.

% \begin{figure}
%   \includegraphics[width=0.4\textwidth]{latex/images/adjusted diversity.png}
%   \caption{Our method for quantifying the diversity and repetition in text. $w$ is chosen to be $0.3$, SB dentoes Self-BLEU metric, and NC denotes Normalized Coherency $\text{NC}=(\text{coherency rating}-1)/(5-1)$.}
%   \label{fig:adjusted-diversity}
% \end{figure}

\noindent\textbf{GPT-Judger.} Using an external judge to examine the quality of text has proven to be useful ~\citep{singh2023new, zheng2023judging}, especially when text quality metrics like perplexity are unable to capture all aspects of text quality. GPT-Judger covers a range of criteria that reflect the essential parts of text quality. These categories include 
\begin{quote}
    Relevance to the prompt, Depth of detail, Clarity of writing, Coherence and logical flow, Originality and insight, Use of specific examples, and Accuracy of information.
\end{quote}
Additionally, from GPT-judger, we could extract soft wins and highlight which quality criterion of the text is affected the most by the watermarking method.

\noindent\textbf{Self-BLEU SB.} SB \citet{zhu2018texygen} measures the diversity in a generated text by measuring how similar sentences are to each other in the same text. SB is calculated by matching a sentence (hypothesis) with all other sentences (references) in the same text for each sentence in the text. Then an average score is calculated and returned as the final score.

\noindent\textbf{Adjusted Diversity AD.} Because SB can be misleading in measuring the diversity of text --for example mixing tokens from different languages yields very low SB score, hence more diversity in text-- we create a new diversity metric that reflects unwanted diversity or excessive repetition in text by leveraging the Judger's coherency criterion scores as follow:
\begin{equation}
\small
    \text{AD} = w \times \text{SB} + 
    (1-w) \times (1 - \text{NC})
\end{equation}
 Where NC indicates Normalized Coherency score from GPT-Judger, $w$ is a weight between 0 and 1 that can be adjusted based on how much importance we want to give to each metric. The term $(1 - \text{NC})$ inverts the coherency score from GPT-judger so that a low coherency score (indicating problematic text) contributes to a higher "unrealistic diversity" in the text. In our experiment, we choose $w$ to be $0.3$ as SB doesn't catch the semantic level of the text as GPT-Judger does, yet SB could give a subtle indication of repetition in the text.
 
\subsection{Watermark Methods}
\textbf{KGW \citep{kirchenbauer2023watermark}.} The KGW method embeds a watermark signal in generated text by manipulating log-probabilities (logits) of next token. The vocabulary $v$ is divided into green and red lists based on a split ratio $\gamma$. A secret key $S_k$ and a hash of the previous $k-1$ token ids seed a pseudorandom generator to produce the next token $k$. The generating model is either limited to the green list (hard watermark) or biased toward it (soft watermark) by adding a small value $\delta$ to the logits. Soft watermarking manages low-entropy contexts with few green list options. % Unlike Unigram, KGW creates new green and red lists for each completion by hashing the previous $k-1$ tokens.

\noindent\textbf{Unigram \citep{zhao2023provable}.} Unigram, like KGW, divides the vocabulary $v$ into green list $v_g$ and red list $v_r$. The key distinction is that in Unigram, these lists are consistent for all generated tokens over the course of the generation process, as it does not utilize hashing with previous token IDs. Instead, Unigram employs the sha256 hashing algorithm, using a secret key $S_k$ to partition the vocabulary into the green and red lists. Subsequently, it applies the soft-watermark technique to adjust the logit values. This approach is expected to reduce the effectiveness of watermark frequency counting removal attacks, since the hashing process does not incorporate previous token IDs. Both KGW and Unigram use the following equation to detect the watermark.
\begin{equation}
\small
    z = (|s|_G - \gamma |s|) / \sqrt{|s|\gamma(1-\gamma)}
\end{equation}
where $|s|_G$ is the number of green tokens in the generated text, and $\gamma=\frac{|v_G|}{|v|}$.

\noindent\textbf{EXP \cite{aaronson_my_2022}.} EXP uses exponential minimum sampling, which is a variant of the Gumbel trick ~\citet{papandreou2011perturb}, to bias the distribution of the next token generation. Specifically, like KGW, EXP uses $k-1$ context-window to seed a pseudorandom generator (PRG) to generate the next token $k$. However, instead of using soft watermark or applying a $\delta$ value to the logits, EXP uses PRG to generate random numbers $r_{t,i}$, which is the same size as the vocabulary of the generating model. Then, at position $t$, token $i$ is sampled by maximizing the following quantity. 
\[
\small
    \text{argmax}_i(r_{t,i}^{1/p_{t,i}})
\]
when $p_{t,i}$ is very small, token $i$ will only be chosen if $r_{t,i}$ is close to one, which is very unlikely to happen. In terms of randomness, this sampling method will return the same token every time the same $k-1$ context is used for the PRG. The watermark is then detected by the following equation.
\begin{equation}
\small
    \sum_{t=1}^n log(\frac{1}{1-r_{t,i}})
\end{equation}

\noindent\textbf{XSIR \cite{he2024can}.} XSIR is designed to enhance cross-lingual watermarking by ensuring that semantically similar prefix texts receive similar logit biases. Instead of directly hashing token IDs, XSIR hashes a semantic chunk of the prefix text to generate the logit bias for the next token \( k \). This approach ensures that different prefix texts with similar meanings produce comparable logit bias distributions:

\begin{equation}
\small
    \text{Sim}(\Delta(x), \Delta(y)) \approx \text{Sim}(E(x),E(y))
\end{equation}
where \( E \) is a multilingual embedding model, and \( \Delta \) is the function that determines the watermark logit bias distribution for all tokens in the vocabulary. By doing so, XSIR ensures that tokens predicted under similar semantic prefixes receive similar watermarking bias.

Besides, XSIR also enforces consistent biases across words within the same semantic cluster. Specifically, words that share the same meaning across different languages are assigned identical biases:

\begin{equation}
\small
    C(i) = C(j) \Rightarrow \Delta_{C(i)} = \Delta_{C(j)}
\end{equation}
where \( C(i) \) represents the cluster index of word \( i \). As a result, if tokens \( i \) and \( j \) are semantically equivalent, they receive the same logit bias, preserving watermark consistency across translations.

% \begin{figure*}[th]
%   \subfigure[KGW]{\includegraphics[width=0.49\columnwidth]{latex/images/KGW/gpt_judge_avgs_KGW_small_selected_criteria.pdf}\label{fig:gptjudge-kgw}}
%   \subfigure[Unigram]{\includegraphics[width=0.49\columnwidth]{latex/images/Unigram/gpt_judge_avgs_Unigram_small_selected_criteria.pdf}\label{fig:gptjudge-unigram}} \hfill
%   \subfigure[XSIR]{\includegraphics[width=0.49\columnwidth]{latex/images/XSIR/average_gpt_judge_scores_xsir_small_selected_criteria.pdf}\label{fig:gptjudge-xsir}}
%   \subfigure[EXP]{\includegraphics[width=0.49\columnwidth]{latex/images/EXP/average_gpt_judge_scores_exp_small_selected_criteria.pdf}\label{fig:gptjudge-exp}}
%   \vspace{-3mm}
%   \caption {GPT-judge results. We compute the average of watermarked and unwatermarked scores for $500$ generations to find that the criterion the judge complains the most about was coherency. The large green area in the figure indicates that unwatermarked text quality is always better on average.}
%   \label{fig:gptjudge-results}
%   \vspace{-3mm}
% \end{figure*}


\begin{figure*}[h!]
\centering
\includegraphics[width=\linewidth]{latex/images/quality.pdf}
\caption{GPT-judge results. We compute the average of watermarked and unwatermarked scores for $500$ generations to find that the criterion the judge complains the most about was coherency decrease.}
\label{fig:gptjudge-results}
\end{figure*}

\section{Experimental Setup}
In all our experiments, we assess the strength of the watermark both before and after various attacks across four watermarking methods. Following previous work~\cite{kirchenbauer2023watermark,zhao2023provable,dathathri2024scalable}, we evaluate watermark detection using Receiver Operating Characteristic (ROC) curves, which illustrate the trade-off between the false positive rate (FPR) and false negative rate (FNR). FPR indicates that the generated text is falsely flagged as human written, which is more critical in this context.
Although theoretical guarantees exist for the FPRs of the watermark methods we study, we adopt an empirical approach. Specifically, we automatically calculate a threshold based on the scores of unwatermarked text and then compare it against the scores of watermarked text. This method ensures that a fair and consistent threshold is applied, particularly in a multilingual setting. For further details, Appendix~\ref{sec:appendix-detection} reports additional performance metrics (TPR, TNR, FPR, FNR) derived from the thresholds used by the evaluated methods.
% Although the watermark methods we studied have a test statistics where FPRs could be theoretically proven, we choose an empirical approach where a threshold of the unwatermarked text scores is automatically calculated and compared with the scores of the watermarked text. This is to make sure a fair threshold is used considering multilingual setting. In Appendix ~\ref{sec:appendix-detection}, we show the performance metrics (TPR, TNR, FPR, FNR) using the threshold used by the methods we study.

% In all of our attacks, we use English as the pivot language from/to which we perform translations. We follow a practical attack pipeline in which an adversary translates the watermarked text and use it as is or further modifies it by paraphrasing. The adversary then uses the paraphrased version directly or back translates to English as a final step. We report the watermark detection roc curves to test the watermark signal after these attacks.

\noindent\textbf{LLMs.} We use several multi-billion parameter models to generate and evaluate watermarked texts. Due to the lack of availability of open-source LLMs that support all of the languages we used in our study, we used different models for different languages. For Chinese and Indonesian, we use the Sailor2 1B and 8B variants~\cite{sailor2report} for text generation and perplexity evaluation\footnote{Perplexity results are provided in the appendices.}. For Arabic, we generate text with the Jais family 6.7B model~\cite{jaisfamilymodelcard} and assess perplexity using Acegpt 7B~\cite{huang2023acegpt}. English is generated on both models. We generate watermarked text in different hyper-parameter settings and different languages with an average of 10 GPU hours for KGW and Unigram, an average of 7 hours for XSIR, and 3 hours for EXP. We also use OpenAI GPT-4o-mini-2024-07-18 as the GPT-based judger which is slower but better than GPT-3.5-turbo. Each experiment took around 20 minutes for 500 generations to assess text quality with GPT-Judger. For translation attacks, we employ OPUS-MT models~\cite{tiedemann2020opus,tiedemann2022democratizing}. To ensure consistency with watermark protocols (e.g., using the same generation vocabulary for detection), we generate English texts with both the Jais and Sailor2 models; translations (for Chinese, Indonesian, or Arabic) are derived from the original English versions generated by their respective LLMs.

\noindent\textbf{Watermark Parameters.} For KGW~\cite{kirchenbauer2023watermark} and Unigram~\cite{zhao2023provable} we use three green list ratios, $\gamma \in \{0.1, 0.5, 0.9\}$, and three bias values, $\delta \in \{2, 5, 10\}$. For KGW, we also use a lefthash with a context size of 1\footnote{Distinct from Unigram, where a key is hashed once and reused for subsequent generations.}. When applicable, we follow previous studies by using a z-score threshold of $4$.
For EXP~\cite{aaronson_my_2022}, we set a context length of $4$ for the hashing and consider text watermarked if the $p$-value falls below $10^{-4}$. For XSIR~\cite{he2024can}, approximately half the vocabulary is split into green and red lists, i.e., $\gamma=0.5$. Although the original XSIR uses a bias value of $\delta=1$, we vary the $\delta \in \{2, 5, 10\}$ to facilitate extensive evaluation; we also adopt their watermark threshold of $0.2$.
% Also, following their setting, we use a threshold of $0.2$, above which the text is considered watermarked.

Unless stated otherwise, all experiments use $\gamma=0.5$ and $\delta=2.0$ for KGW, Unigram, and XSIR, as these values empirically provide the best balance between watermark strength and text quality. Additional results with varying parameters are presented in the appendices, which confirm with previous analyses.

\noindent\textbf{Dataset.} For all experiments, we use $500$ examples from the C4 dataset~\cite{JMLR:v21:20-074}, which is available in all evaluated languages. For English, we follow previous work by using the RealNewsLike split; for other languages, we use the main training split, as RealNewsLike is available only for English\footnote{\url{https://huggingface.co}}.

\section{Results}
We perform experiments to answer the following research questions:

\noindent\textit{RQ1:} How do watermarking methods perform across languages in terms of detectability, text quality, and diversity?

\noindent\textit{RQ2:} How resilient are watermarking methods against existing cross-lingual attacks, particularly translation attacks?

\noindent\textit{RQ3:} How do different adversarial approaches to cross-lingual watermark evasion perform? Specifically, how do attacks fare under our novel asymmetrical threat model, where the output language differs from the pivotal language?



% \textbf{Challenges Beyond Repetition for Watermarked Text}
\subsection{RQ1: Watermarking Performance under Cross-lingual Setting}
\noindent\textbf{Quality.} We evaluate text quality through two complementary approaches: GPT-Judger and our proposed diversity metrics. This dual evaluation provides a comprehensive view of how watermarking affects text quality across languages.

Figure~\ref{fig:gptjudge-results} shows the result of obtaining the decreases of each criterion score after watermarking, indicating the text coherency is the most affected aspect in all languages. Moreover, XSIR and EXP showcase the largest degrade of the quality. Our analysis indicates that GPT-Judger frequently flags issues related to \emph{clarity of writing}, including repetition and the overlapping of different language characters or unknown symbols.

\begin{table}[h]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \renewcommand\arraystretch{0.9}
    \caption{GPT-Judger Final Verdict Analysis. A soft-win is recorded when the watermarked text is judged to be of equal (Tie) or superior quality (Hard-win) compared to the non-watermarked version, reflecting the method's ability to preserve text quality. Higher soft-win rates indicate better quality retention after watermarking.}
    \begin{tabular}{lcccc}\toprule
        Method & Language & Hard-Win \(\uparrow\) & Tie \(\uparrow\) & Soft-Win \(\uparrow\) \\
        \midrule
        \multirow{4}{*}{KGW} & English & 0.42 & 0.05 & 0.47 \\
        & Arabic & 0.41 & 0.15 & 0.56\\
        & Chinese & 0.31 & 0.26 & 0.57\\
        & Indonesian & 0.29 & 0.18 & 0.47\\
        \midrule
        \multirow{4}{*}{Unigram} & English & 0.38 & 0.08 & 0.46 \\
        & Arabic & 0.37 & 0.12 & 0.49\\
        & Chinese & 0.24 & 0.27 & 0.51\\
        & Indonesian & 0.27 & 0.14 & 0.41\\
        \midrule
        \multirow{4}{*}{XSIR} & English & 0.22 & 0.12 & 0.34 \\
        & Arabic & 0.20 & 0.06 & 0.26\\
        & Chinese & 0.11 & 0.24 & 0.35\\
        & Indonesian & 0.16 & 0.12 & 0.28\\
        \midrule
        \multirow{4}{*}{EXP} & English & 0.10 & 0.04 & 0.14\\
        & Arabic & 0.14 & 0.10 & 0.25\\
        & Chinese & 0.07 & 0.24 & 0.31\\
        & Indonesian & 0.15 & 0.12 & 0.26\\
        \bottomrule
    \end{tabular}
    
    \label{tab:soft-win-rates}
\end{table}


% Samll all-lang-one-figure
\begin{figure*}[th]
  \subfigure[KGW]{\includegraphics[width=0.5\columnwidth]{latex/images/KGW/roc_curves_all_languages_KGW_small.pdf}\label{fig:roc-curve-kgw-before}}
  \subfigure[Unigram]{\includegraphics[width=0.5\columnwidth]{latex/images/Unigram/roc_curves_all_languages_Unigram_small.pdf}\label{fig:roc-curve-unigram-before}} \hfill
  \subfigure[XSIR]{\includegraphics[width=0.5\columnwidth]{latex/images/XSIR/roc_curves_all_languages_XSIR_small.pdf}\label{fig:roc-curve-xsir-before}}
  \subfigure[EXP]{\includegraphics[width=0.5\columnwidth]{latex/images/EXP/roc_curves_all_languages_EXP_small.pdf}\label{fig:roc-curve-exp-before}}
  \vspace{-3mm}
  \caption {Watermark detection ROC curves with AUC before attacks. We fix $\gamma=0.5$ and $\delta=2.0$ for KGW, Unigram, and XSIR. The watermark threshold is calculated automatically by comparing unwatermarked and watermarked scores for $500$ generations.}
  \label{fig:roc-curves-all-methods-before}
  \vspace{-3mm}
\end{figure*}


To further analyze the GPT-Judger results, we calculate soft-win rates in Table~\ref{tab:soft-win-rates}. A soft-win occurs when the judge favors the watermarked text or issues a tie. Among all methods, KGW achieves the highest soft-win rate. Nonetheless, the GPT-Judger occasionally struggles to deliver definitive judgments, particularly for non-English texts, as indicated by the high tie rates compared to English. To objectively capture this impact, we introduce our new metric rooted in the Self-BLEU (SB) metric.



% \begin{table}
%     \centering
%     \small
%     \setlength{\tabcolsep}{4pt}
%     \caption{GPT-Judger final verdict analysis. $\text{soft wins} = \text{watermarked} + \text{Tie}$. The Tie rates mostly increase for languages other than English.}
%     \begin{tabular}{ccccc}
%         \toprule
%         Method & Language & Watermarked \(\uparrow\) & Tie \(\uparrow\) & Soft Win \(\uparrow\) \\
%         \midrule
%         \multirow{4}{*}{KGW} & English & 0.416 & 0.054 & 0.470 \\
%         & Arabic & 0.41 & 0.15 & 0.560\\
%         & Chinese & 0.31 & 0.256 & 0.566\\
%         & Indonesian & 0.286 & 0.182 & 0.468\\
%         \midrule
%         \multirow{4}{*}{Unigram} & English & 0.384 & 0.078 & 0.462 \\
%         & Arabic & 0.37 & 0.124 & 0.494\\
%         & Chinese & 0.244 & 0.266 & 0.510\\
%         & Indonesian & 0.27 & 0.142 & 0.412\\
%         \midrule
%         \multirow{4}{*}{XSIR} & English & 0.218 & 0.124 & 0.342 \\
%         & Arabic & 0.198 & 0.062 & 0.26\\
%         & Chinese & 0.108 & 0.242 & 0.350\\
%         & Indonesian & 0.158 & 0.118 & 0.276\\
%         \midrule
%         \multirow{4}{*}{EXP} & English & 0.104 & 0.038 & 0.142\\
%         & Arabic & 0.142 & 0.104 & 0.246\\
%         & Chinese & 0.068 & 0.242 & 0.310\\
%         & Indonesian & 0.146 & 0.116 & 0.262\\
%         \bottomrule
%     \end{tabular}
    
%     \label{tab:soft-win-rates}
% \end{table}

\begin{table*}[ht]
    \renewcommand\arraystretch{0.9}
    \centering
    \small
    \caption{SelfBleu results for all watermark methods. For KGW, Unigram and XSIR, $\gamma=0.5$ and $\delta=2.0$. We consider this setting is the best for these methods with different languages. More results with varying the $\gamma$ and $\delta$ values in are in Appendix}
    \setlength{\tabcolsep}{12pt}
    \begin{tabular}{cccccc}
        \toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{Language} & \multicolumn{2}{c}{self-bleu ($\downarrow$ more diverse)} & \multicolumn{2}{c}{AD ($\downarrow$ better)}\\
        \cmidrule(lr){3-4}\cmidrule(lr){5-6}
        & & watermarked & unwatermarked & watermarked & unwatermarked\\
        \midrule
        \multirow{4}{*}{KGW} & English & 0.16 & 0.16 & 0.38 & 0.34\\
        % \cline{2-6}
        & Arabic & 0.11 & 0.12 & 0.36 & 0.35\\
        % \cline{2-6}
        & Chinese & 0.04 & 0.04 & \textbf{0.44} & 0.40\\
        % \cline{2-6}
        & Indonesian & 0.10 & 0.10 & \textbf{0.43} & 0.38\\
        \midrule
        \multirow{4}{*}{Unigram} & English & 0.23 & 0.17 & 0.40 & 0.35\\
        % \cline{2-6}
        & Arabic & 0.16 & 0.12 & 0.41 & 0.35\\
        % \cline{2-6}
        & Chinese & 0.02 & 0.04 & \textbf{0.47} & 0.40\\
        % \cline{2-6}
        & Indonesian & 0.12 & 0.10 & \textbf{0.46} & 0.37\\
        \midrule
        \multirow{4}{*}{XSIR} & English & 0.20 & 0.17  & \textbf{0.49} & 0.32\\
        % \cline{2-6}
        & Arabic & 0.14 & 0.12 & \textbf{0.45} & 0.33\\
        % \cline{2-6}
        & Chinese & 0.03 & 0.04 & \textbf{0.55} & 0.38\\
        % \cline{2-6}
        & Indonesian & 0.12 & 0.11 & \textbf{0.53} & 0.37\\
        \midrule
        \multirow{4}{*}{EXP} & English & 0.19 & 0.17 & \textbf{0.57} & 0.29 \\
        & Arabic & 0.20 & 0.12 & \textbf{0.55} & 0.31\\
        & Chinese & 0.13 & 0.04 & \textbf{0.61} & 0.38\\
        & Indonesian & 0.21 & 0.10 & \textbf{0.57} & 0.36\\
        \bottomrule
    \end{tabular}

    \label{tab:self-bleu}
\end{table*}


% Old figure all translation
\begin{figure*}[th]
  \subfigure{\includegraphics[width=0.98\columnwidth]{latex/images/roc_curves_selected_methods_kgw_unigram_exp_translation_paraphrase_with_numbering.pdf}\label{fig:roc-curve-kgw-unigram-exp-translations}} \hfill
  \subfigure{\includegraphics[width=0.98\columnwidth]{latex/images/roc_curves_xsir_all_attacks_with_numbering.pdf}\label{fig:roc-curve-xsir-all-attacks}}
  \vspace{-3mm}
  \caption {Watermark detection ROC curves with AUC after attacks. We fix $\gamma=0.5$ and $\delta=2.0$ for KGW, Unigram, and XSIR. For all attacks, the watermark threshold is calculated automatically by comparing unwatermarked and attacked watermarked text score for $100$ generations. \textbf{Left:} we perform translation attacks on KGW, Unigram, and EXP. \textbf{Right:} Only the leftmost column represent asymmetrical translation attack setting. We apply more invasive attacks to XSIR to highlight its robustness}
  \label{fig:roc-curves-all-methods-after}
  \vspace{-3mm}
\end{figure*}

\noindent\textbf{Diversity.}
Previous research has used the Self-BLEU (SB) metric to assess the diversity of watermarked text \citep{dathathri2024scalable}. Table~\ref{tab:self-bleu} includes results from SB and the Adjusted Diversity (AD) metrics for various watermark methods. While SB suggests high diversity, especially for Chinese and Indonesian, it can be misleading; low SB scores (e.g., $0.04$ for Chinese) might indicate text degradation, as shown by higher AD scores ($\geq 0.44$). This pattern holds across other languages. When we employ larger $\delta$ values with $\gamma=0.5$, AD ratios are higher compared to SB, indicating unrealistic diversity in watermarked text. More detailed results with different hyper-parameters are in Table~\ref{tab:self-bleu-more} in  Appendix~\ref{sec:appendix-quality}\footnote{Language mixing can be due to combined-language prompts, model size, or vocabulary coverage, and watermarking exacerbate the problem}.


\noindent\textbf{Detectability.}
Our detection analysis examines both performance without attack and attack resilience across languages. Figures~\ref{fig:roc-curves-all-methods-before} and~\ref{fig:roc-curves-all-methods-after} show the ROC curves for all watermarking methods before and after attacks. 
Prior to any attacks, all methods demonstrate strong detectability. For KGW, Unigram, and XSIR, we identified $\delta=2.0$ and $\gamma=0.5$ as the optimal parameters yielding the best detection results. However, the role of employing different hyper-parameters for detection differs from that for text quality. When larger values of $\gamma$ are used with $delta=2.0$, detection scores are adversely affected, especially for Unigram method. This is counterintuitive to the green/red splitting methodology as in KGW and Unigram since using $delta=2.0$ with smaller $\gamma=0.1$ yield higher detection scores. We show detailed results in Figure~\ref{fig:roc-curves-kgw-unigram-before-zoomed} in which we illustrate the close relationship between $\gamma$ and $\delta$ for methods like KGW and Unigram in Appendix~\ref{sec:appendix-detection}.



\subsection{RQ2: Watermarking Resiliencies to Translation Attacks.} 
Regarding attack resilience, all methods vulnerable to our proposed \textit{asymmetrical} attacks. In Figure~\ref{fig:roc-curves-all-methods-after}~(left), we perform \textit{asymmetrical} translation attacks against KGW, Unigram, and EXP. The results showcase the watermark detectability under translation attacks varies by language~\cite{gha2024jailbreaking}. For instance, KGW shows the lowest AUC of 0.55 for Arabic-English, while Unigram presents a worst-case AUC of 0.57 for English-Arabic and Arabic-English. In the EXP method as shown in Figure~\ref{fig:roc-curves-all-methods-after}~(c), all attacks are notably invasive, likely due to its larger hashing window $4$ compared to KGW's recommended window of $1$.

In Figure~\ref{fig:roc-curves-all-methods-after}~(right), we evaluate all of our \textit{asymmetrical} attacks to the more robust XSIR method. Although XSIR was developed for \textit{symmetrical} attacks, it remains susceptible to \textit{asymmetrical} attacks. This vulnerability underscores the severity of attack scenarios where the pivot language differs from the output language. XSIR's semantic clusters vary across languages, weakening its watermark robustness when the pivot and output languages differ. In contrast, back-translate attacks preserve the watermark better, as translating back to the pivot language realigns the semantic clusters as shown in Figure~\ref{fig:roc-curves-all-methods-after}~(f).

Additionally, KGW and EXP have more predictable results than XSIR and Unigram where different languages affect the watermark signal differently. For instance, detection outcomes for translation and translation-paraphrase attacks remain consistently tight across languages for KGW and EXP, whereas XSIR and Unigram exhibit greater variability, with some languages deviating significantly. This behavior may stem from technical challenges such as preserving text length during translation.

% In Figure~\ref{fig:roc-curves-all-methods-after}~(right), we evaluate all of our \textit{asymmetrical} attacks to the more robust XSIR method. Although XSIR was developed for \textit{symmetrical} attacks, it remains susceptible to our \textit{asymmetrical} attacks, especially for Chinese and Arabic. Notably, the detectability of translating \textit{to English} is larger than translating \textit{from English}. This highlights the importance of the attack scenarios we proposed, in which not only non-English languages are used as the pivotal language to perform translation attacks. We found that XSIR semantic clusters have more similar English words than other languages. This also explains the better detection results for back translate attacks in Figure~\ref{fig:roc-curves-all-methods-after}~(f), in which translating back to English with probably new words doesn't go undetected in XSIR semantic clusters. To sum up, KGW and EXP methods have more predictable results than XSIR and Unigram where different languages affect the watermark signal differently. Specifically, we can see the tightness of detection results of translation and translation-paraphrase attacks for all languages in KGW and EXP, whereas the tightness broke in XSIR and Unigram with some languages deviating from similar detection rates. This behavior could be due to technical difficulties such as preservation of text length after translations. We explain about this in the next section.

% including linguistics complexity and preservation of text length after translations. We explain about these technical difficulties in the next section.

\subsection{RQ3: Asymmetrical Translation Attacks Raise a New Threat}
Despite XSIR being specifically designed for cross-lingual scenarios, our results shown in Figure~\ref{fig:roc-curve-xsir-all-attacks} demonstrate that it remains vulnerable to post-generation translation attacks. Particularly, the vulnerability is pronounced for Chinese and Arabic languages, suggesting that XSIR's effectiveness varies significantly by language. This contrasts with XSIR proposed adversarial scenario of translating prompts both before and after generation, indicating that post-generation attacks can be more effective at evading detection.

Our analysis reveals a critical asymmetry: translations \textit{TO English} yield better detection rates than translations \textit{FROM English}. This asymmetry appears to be rooted in XSIR's semantic clustering characteristics, as we found that XSIR semantic clusters have more similar English words than other languages. This disparity is likely due to XSIR's semantic clustering, which groups English words more consistently than words from other languages. Consequently, XSIR appears to perform more robustly when English is the target language rather than the source language.

These findings highlight the need for more robust cross-lingual watermarking approaches that can maintain effectiveness across different languages and resist sophisticated attack pipelines.

\section{Conclusion}
In this paper, we assess four distinct watermarking techniques across four different languages, considering realistic watermark removal attacks in a cross-lingual context. We examine the effectiveness of these watermarking methods by evaluating the quality of the watermarked text and the resilience of the watermark signal following such attacks. Our findings indicate that various watermarking techniques respond differently to language translations when the watermarked text is modified. We propose that there is a need for more robust cross-lingual watermarking solutions that integrate the predictability of KGW and EXP across languages with the more resilient approaches of Unigram and XSIR.


\section{Limitations and Discussions}
% more refined
In this study, we evaluate four distinct watermarking methods across four languages, focusing on practical removal attacks in a cross-lingual context. We believe that more research should include languages with rich syntax and structure and suggest exploring datasets beyond C4, given its generalized nature.

The attacks presented are invasive, significantly weakening the watermark signal, but can be countered through various strategies. We believe that developing more resilient cross-lingual watermarking techniques should integrate KGW and EXP predictability with the robust Unigram method. XSIR could be instrumental here, as it manipulates text semantics over syntax. We first analyze why XSIR falls short against translation attacks and then explore potential solutions.

Our investigation into XSIR revealed that its shortcomings in handling translation attacks are largely technical. XSIR clusters semantically similar words across languages by constructing a graph from a comprehensive dictionary using text transformations. It identifies connected components (CCs), but this process can be problematic for languages with diverse character sets. For instance, CCs rely on exact matches, which may prevent clustering of semantically similar words.

To enhance XSIR's robustness, we suggest modifying the detection process. Given that English handles XSIR cluster transformations effectively, translating non-English outputs to English before detection could ensure watermark signal identification. This semantic-focused approach can extend to other methods, such as PostMark ~\citep{chang2024postmark}.

Additionally, XSIR's robustness can be improved by refining the use of CCs. Changing node content to latent word representations, formulated as numerical vectors, would enable more meaningful exact matching, enhancing clustering accuracy across different languages.


\section{Ethics Consideration}
Our research reveals critical vulnerabilities in text watermarking when subjected to cross-lingual translation attacksâ€”specifically, the risk that the original language of a text can be concealed, allowing adversaries to evade detection and potentially disseminate harmful content. We acknowledge that such findings may be exploited by malicious actors, thereby posing a serious risk to digital authenticity and safety. However, the primary goal of our work is to illuminate these weaknesses so that more robust watermarking strategies can be developed and integrated into language models. In the interim, we propose simple yet effective countermeasures that can be readily incorporated by AI service providers. Our study employs publicly available data and models and is intended solely for academic research and the improvement of digital security. We strongly advocate for responsible disclosure and the continuous refinement of safeguards to ensure that AI technologies are deployed safely and ethically.

% In this study, we evaluate four different watermark methods on four languages with practical watermark removal attacks in cross-lingual setting. We believe that more languages should be investigated especially those with rich syntax and structure. We also believe that different datasets, other than C4, could be utilized, since C4 is a more generalized dataset.

% While the attacks we introduced here are invasive in terms of attenuating the watermark signal, they can be mitigated in different ways. We believe more robust cross-lingual watermarking is needed that combines KGW and EXP predictability across languages with the more robust Unigram method. We posit that XSIR can serve this purpose since it manipulates the text semantics over syntax. We first examine why XSIR was not robust against translation attacks, then we will provide fruitful discussion of what could be done to such attacks.

% Regarding the issues with XSIR, we found that the main reason it did not perform well against the translation attacks is more technical than theoretical. XSIR aims to cluster words that have the same semantics together for different languages. They do so by building a graph of edges from a large dictionary built by applying different transformations to the texts. Consequently, they find all the connected components (CCs) of all the edges in the graph. However, this pipeline of preparing the dictionary and connecting the similar words by CCs can create complications for different languages with different character sets. For example, the CC is found by performing exact-match comparisons between nodes content, which can result in many similar words not clustered together.

% To use XSIR in a more robust way, we can change the detection procedure as follows. Since English language seems to endure the transformation that XSIR's clusters go through, we could always first translate any output in non-English language to English and then preform the detection procedure as normal. We believe that this will ensure that the watermark signal is detected. Changing the detection to this manner, any watermark method that operates on the semantics of the texts can be used in this way, so for example, PostMark ~\citep{chang2024postmark} can also be used in this way.

% The other solution to XSIR robustness issue is to use CC more carefuly. This can be done by changing the nodes content to the words latent representations, which can be added as a vector of numbers. Only then, exact matching can be done in a more meaningful way.



% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl}


\newpage
\appendix
\section{More Quality Results}\label{sec:appendix-quality}
\subsection{Perplexity (PPL)}
Perplexity is a metric that, although known for its limitations, still provides a useful statistical or probabilistic background for assessing text quality. It measures how well a probability distribution predicts a sample. In the context of language models, it provides insights into how \emph{surprised} the model is by the actual sequence of words compared to its predicted probabilities. A lower perplexity score indicates that the model found the text more predictable and thus, in a broad sense, of higher quality. In our experiments, the PPL is calculated as follows:
\[
\text{Perplexity}(W) = \exp\left(\frac{1}{N} \sum_{i=1}^{N} -\log p(w_i)\right)
\]
where \( \exp \) represents the exponential function. \( N \) is the total number of words in the text. \( p(w_i) \) is the probability assigned to the word \( w_i \) by the model. The negative log likelihood, \(-\log p(w_i)\), is referred to the Cross-Entropy loss for word \( w_i \).

\textbf{PPL Analysis}:
\begin{figure*}[th]
  \subfigure[KGW]{\includegraphics[width=0.98\columnwidth]{latex/images/KGW/z_scores_ppl_all_KGW_watermarked_small.pdf}\label{fig:z-vs-ppl-kgw}} \hfill
  \subfigure[Unigram]{\includegraphics[width=0.98\columnwidth]{latex/images/Unigram/z_scores_ppl_all_Unigram_watermarked_small.pdf}\label{fig:z-vs-ppl-unigram}}
  \vspace{-3mm}
  \caption {Detection scores as a function of PPL for KGW and Unigram. For KGW, we notice that as $\delta$ grows higher, the quality of text decreases for all languages. Larger $\gamma$ values with smaller $\delta$ values greately affected the watermark strength in which it is attenuated. Unigram presents interesting graphs. When $\gamma$ is large, we see simiar trend as in KGW. However, smaller $\gamma$ values behave differently for different languages. The PPL and Z-scores are calculated on 500 generations.}
  \label{fig:z-scores-vs-ppls-kgw-unigram}
  \vspace{-3mm}
\end{figure*}

\begin{figure*}[th]
  \subfigure[XSIR]{\includegraphics[width=0.98\columnwidth]{latex/images/XSIR/z_score_ppl_scatter_xsir_mean_smaller.pdf}\label{fig:z-vs-ppl-xsir}} \hfill
  \subfigure[EXP]{\includegraphics[width=0.98\columnwidth]{latex/images/EXP/pvals_ppl_with_mean_exp_all_langs_smaller_not_tight.pdf}\label{fig:z-vs-ppl-exp}}
  \vspace{-3mm}
  \caption {Detection scores as a function of PPL for XSIR and EXP. XSIR employs $\gamma=0.5$. The smaller the $\delta$ the better the qulaity of text. EXP $p$-values show insensitivity to PPL scores with a few examples falling above the threshold or indicating False Negatives.}
  \label{fig:z-scores-vs-ppls-others}
  \vspace{-3mm}
\end{figure*}
Figures ~\ref{fig:z-scores-vs-ppls-kgw-unigram} and ~\ref{fig:z-scores-vs-ppls-others} show the strength of detection of the watermark as a function of PPL. The results in figure ~\ref{fig:z-vs-ppl-kgw} confirm the findings in ~\cite{kirchenbauer2023watermark} about the slight effect of larger $\delta$ values to the quality of the watermarked text for all languages. We see similar trends for Unigram in figure ~\ref{fig:z-vs-ppl-unigram} only when $\gamma \geq 0.5$. For smaller values of $\gamma$, the quality of text seems to slightly improve even for larger $\delta$ values. We believe that this effect is a result of low-entropy completions from the red list being repeated over and over given smaller potion of the vocabulary, which increases the z-scores and precludes PPL from catching this effect.

For the other watermark methods XSIR and EXP, we show their results in figure ~\ref{fig:z-scores-vs-ppls-others}. For EXP, we experiment with the whole data points to clearly identify the trends. In figure ~\ref{fig:z-vs-ppl-xsir}, XSIR shows similar trends to KGW in terms of the effect of larger values of $\delta$ on text quality. For EXP as shown in figure ~\ref{fig:z-vs-ppl-exp}, the quality of text has a consistent relationship with the lower $p$-values across all languages. In other words, the $p$-values remain virtually insensitive to variations in the quality of the text.

\subsection{Complete Self-BLEU Results}
Table ~\ref{tab:self-bleu-more}, we show complete results of self-BLEU and Adjusted Diversity (AD) using different hyper-parameter settings. From the results, it's clear that when the $\delta$ is large, the text diversity unrealistically increases for KGW, Unigram and XSIR. This is due to the tight relationship between $\gamma$ and $\delta$ in which lower $\gamma$ values with higher $\delta$ values causes an adverse effect on the text quality.

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{cccccccc}
        \hline
        Method & Language & $\gamma$ & $\delta$ & \multicolumn{2}{c}{self-bleu ($\downarrow$ more diverse)} & \multicolumn{2}{c}{AD ($\downarrow$ more diverse)}\\
        \cline{5-6}
        \cline{7-8}
        & & & & watermarked & unwatermarked & watermarked & unwatermarked\\
        \hline
        \multirow{4}{*}{KGW} & \multirow{3}{*}{English}
        & \multirow{3}{*}{0.5} & 2.0 & 0.16 & 0.16 & 0.38 & 0.34\\
        & & & 5.0 & 0.15 & 0.17 & 0.46 & 0.32\\
        & & & 10.0 & 0.15 & 0.17 & \textbf{0.50} & 0.31\\
        \cline{2-8}
        & \multirow{3}{*}{Arabic}
        & \multirow{3}{*}{0.5} & 2.0 & 0.11 & 0.12 & 0.36 & 0.35\\
        & & & 5.0 & 0.12 & 0.12 & 0.41 & 0.35\\
        & & & 10.0 & 0.12 & 0.13 & \textbf{0.45} & 0.33\\
        \cline{2-8}
        & \multirow{3}{*}{Chinese}
        & \multirow{3}{*}{0.5} & 2.0 & 0.04 & 0.04 & 0.44 & 0.40\\
        & & & 5.0 & 0.04 & 0.04 & 0.46 & 0.41\\
        & & & 10.0 & 0.04 & 0.04 & \textbf{0.48} & 0.40\\
        \cline{2-8}
        & \multirow{3}{*}{Indonesian}
        & \multirow{3}{*}{0.5} & 2.0 & 0.10 & 0.10 & 0.43 & 0.38\\
        & & & 5.0 & 0.08 & 0.10 & 0.53 & 0.35\\
        & & & 10.0 & 0.08 & 0.11 & \textbf{0.60} & 0.33\\
        \hline
        \multirow{4}{*}{Unigram} & \multirow{3}{*}{English}
        & \multirow{3}{*}{0.5} & 2.0 & 0.23 & 0.17 & 0.40 & 0.35\\
        & & & 5.0 & 0.26 & 0.17 & 0.54 & 0.32\\
        & & & 10.0 & 0.27 & 0.17 & \textbf{0.59} & 0.31\\
        \cline{2-8}
        & \multirow{3}{*}{Arabic}
        & \multirow{3}{*}{0.5} & 2.0 & 0.16 & 0.12 & 0.41 & 0.35\\
        & & & 5.0 & 0.19 & 0.12 & 0.48 & 0.32\\
        & & & 10.0 & 0.21 & 0.12 & \textbf{0.52} & 0.34\\
        \cline{2-8}
        & \multirow{3}{*}{Chinese}
        & \multirow{3}{*}{0.5} & 2.0 & 0.02 & 0.04 & 0.47 & 0.40\\
        & & & 5.0 & 0.02 & 0.04 & 0.52 & 0.39\\
        & & & 10.0 & 0.03 & 0.03 & \textbf{0.54} & 0.39\\
        \cline{2-8}
        & \multirow{3}{*}{Indonesian}
        & \multirow{3}{*}{0.5} & 2.0  & 0.12 & 0.10 & 0.46 & 0.37\\
        & & & 5.0 & 0.13 & 0.10 & 0.57 & 0.34\\
        & & & 10.0 & 0.13 & 0.11 & \textbf{0.63} & 0.33\\
        \hline
        \multirow{4}{*}{XSIR} & \multirow{3}{*}{English}
        & \multirow{3}{*}{0.5} & 2.0 & 0.20 & 0.17  & 0.49 & 0.32\\
        & & & 5.0 & 0.22 & 0.17 & 0.59 & 0.30\\
        & & & 10.0 & 0.22 & 0.17 & \textbf{0.63} & 0.29\\
        \cline{2-8}
        & \multirow{3}{*}{Arabic}
        & \multirow{3}{*}{0.5} & 2.0 & 0.14 & 0.12 & 0.45 & 0.33\\
        & & & 5.0 & 0.19 & 0.13 & 0.55 & 0.30\\
        & & & 10.0 & 0.19 & 0.13 & \textbf{0.57} & 0.31\\
        \cline{2-8}
        & \multirow{3}{*}{Chinese}
        & \multirow{3}{*}{0.5} & 2.0 & 0.03 & 0.04 & 0.55 & 0.38\\
        & & & 5.0 & 0.04 & 0.04 & 0.61 & 0.36\\
        & & & 10.0 & 0.03 & 0.04 & \textbf{0.62} & 0.36\\
        \cline{2-8}
        & \multirow{3}{*}{Indonesian}
        & \multirow{3}{*}{0.5} & 2.0 & 0.12 & 0.10 & 0.53 & 0.37\\
        & & & 5.0 & 0.10 & 0.10 & 0.66 & 0.32\\
        & & & 10.0 & 0.10 & 0.10 & \textbf{0.68} & 0.31\\
        \hline
        \multirow{4}{*}{EXP} & English
        & \multirow{4}{*}{-} & \multirow{4}{*}{-} & 0.19 & 0.17 & 0.57 & 0.29 \\
        & Arabic
        &  &  & 0.20 & 0.12 & 0.55 & 0.31\\
        & Chinese
        &  &  & 0.13 & 0.04 & \textbf{0.61} & 0.38\\
        & Indonesian
        &  &  & 0.21 & 0.10 & 0.57 & 0.36\\
        \hline
    \end{tabular}
    \caption{Complete SelfBleu results with the Adjusted Diversity (AD) metric for all watermark methods. For KGW, Unigram, and XSIR, we fix $\gamma=0.5$ and vary $\delta=[2.0, 5.0, 10.0]$}
    \label{tab:self-bleu-more}
\end{table*}

\subsection{Complete Soft-Win Results}
In Table ~\ref{tab:soft-win-rates-more}, we show more results across different hyper-parameter settings for KGW, Unigram and XSIR. The soft-win rates drastically decreases with lower $\gamma$ values and higher $\delta$ values.
\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{ccccccc}
        \hline
        Method & Language & $\gamma$ & $\delta$ & Soft Win Rate & Language Avg. & Method Avg. \\
        \hline
        \multirow{9}{*}{KGW} & \multirow{3}{*}{English}
        & \multirow{3}{*}{0.5} & 2.0 & 0.47 & \multirow{3}{*}{0.314} & \multirow{12}{*}{0.355} \\
        & & & 5.0 & 0.264 & & \\
        & & & 10.0 & 0.21 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Arabic}
        & \multirow{3}{*}{0.5} & 2.0 & 0.56 & \multirow{3}{*}{0.417} & \\
        & & & 5.0 & 0.418 & & \\
        & & & 10.0 & 0.314 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Chinese}
        & \multirow{3}{*}{0.5} & 2.0 & 0.566 & \multirow{3}{*}{0.502} & \\
        & & & 5.0 & 0.514 & & \\
        & & & 10.0 & 0.426 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Indonesian}
        & \multirow{3}{*}{0.5} & 2.0 & 0.468 & \multirow{3}{*}{0.267} & \\
        & & & 5.0 & 0.224 & & \\
        & & & 10.0 & 0.108 & & \\
        \hline
        \multirow{9}{*}{Unigram} & \multirow{3}{*}{English}
        & \multirow{3}{*}{0.5} & 2.0 & 0.462 & \multirow{3}{*}{0.287} & \multirow{12}{*}{0.314} \\
        & & & 5.0 & 0.232 & & \\
        & & & 10.0 & 0.166 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Arabic}
        & \multirow{3}{*}{0.5} & 2.0 & 0.494 & \multirow{3}{*}{0.352} & \\
        & & & 5.0 & 0.292 & & \\
        & & & 10.0 & 0.27 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Chinese}
        & \multirow{3}{*}{0.5} & 2.0 & 0.51 & \multirow{3}{*}{0.436} & \\
        & & & 5.0 & 0.422 & & \\
        & & & 10.0 & 0.376 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Indonesian}
        & \multirow{3}{*}{0.5} & 2.0 & 0.412 & \multirow{3}{*}{0.234} & \\
        & & & 5.0 & 0.178 & & \\
        & & & 10.0 & 0.112 & & \\
        \hline
        \multirow{9}{*}{XSIR} & \multirow{3}{*}{English}
        & \multirow{3}{*}{0.5} & 2.0 & 0.342 & \multirow{3}{*}{0.256} & \multirow{12}{*}{0.218} \\
        & & & 5.0 & 0.2 & & \\
        & & & 10.0 & 0.226 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Arabic}
        & \multirow{3}{*}{0.5} & 2.0 & 0.26 & \multirow{3}{*}{0.183} & \\
        & & & 5.0 & 0.136 & & \\
        & & & 10.0 & 0.1 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Chinese}
        & \multirow{3}{*}{0.5} & 2.0 & 0.35 & \multirow{3}{*}{0.293} & \\
        & & & 5.0 & 0.262 & & \\
        & & & 10.0 & 0.268 & & \\
        \cline{2-6}
        & \multirow{3}{*}{Indonesian}
        & \multirow{3}{*}{0.5} & 2.0 & 0.276 & \multirow{3}{*}{0.15} & \\
        & & & 5.0 & 0.108 & & \\
        & & & 10.0 & 0.066 & & \\
        \hline
        \multirow{4}{*}{EXP} & English
        & \multirow{4}{*}{-} & \multirow{4}{*}{-} & 0.142 & \multirow{4}{*}{-} & \multirow{4}{*}{0.24} \\
        & Arabic
        &  &  & 0.246 & & \\
        & Chinese
        &  &  & 0.31 & & \\
        & Indonesian
        &  &  & 0.262 & & \\
        \hline
    \end{tabular}
    \caption{Soft Win Rates for Different Methods by Language and Hyper-parameters. Columns are added for averages across languages and methods.}
    \label{tab:soft-win-rates-more}
\end{table*}


\section{Detailed Detection Results}\label{sec:appendix-detection}
\begin{figure*}[th]
  \subfigure[KGW]{\includegraphics[width=0.98\columnwidth]{latex/images/KGW/roc_curves_all_KGW_multiple_gammas_zoomed.pdf}\label{fig:roc-curve-kgw-before-zoomed}} \hfill
  \subfigure[Unigram]{\includegraphics[width=0.98\columnwidth]{latex/images/Unigram/roc_curves_all_Unigram_multiple_gammas_zoomed.pdf}\label{fig:roc-curve-unigram-before-zoomed}}
  \vspace{-3mm}
  \caption {Here we fix $\delta$ at $2.0$ and vary $\gamma$ for KGW and Unigram with $\gamma=(0.1, 0.5, 0.9)$ and with a fixed $\delta=2.0$. The larger the $\gamma$ ratio, the worse the watermark detection. We also use smaller FPRs in the range $[0.1, 0.15]$}
  \label{fig:roc-curves-kgw-unigram-before-zoomed}
  \vspace{-3mm}
\end{figure*}

\begin{figure*}[th]
  \subfigure[XSIR]{\includegraphics[width=0.98\columnwidth]{latex/images/XSIR/roc_curves_XSIR_with_legend_on_top_zoomed_in.pdf}\label{fig:roc-curve-xsir-zoomed}} \hfill
  \subfigure[EXP]{\includegraphics[width=0.98\columnwidth]{latex/images/EXP/roc_curves_EXP_with_legend_on_top_zoomed_in.pdf}\label{fig:roc-curve-exp-zoomed}}
  \vspace{-3mm}
  \caption {Watermark detection for XSIR and EXP with lower values of FPRs, which are  in the range $[0.1, 0.15]$. For XSIR, lower values of $\delta$ results in lower TPRs at very low FPRs.}
  \label{fig:roc-curves-xsir-exp-before-zoomed}
  \vspace{-3mm}
\end{figure*}

\subsection{Watermark Detection with Multiple $\gamma$ Ratios for KGW and Unigram}
KGW ~\citet{kirchenbauer2023watermark} as shown in ~\ref{fig:roc-curve-kgw-before} performs the best among all other watermark methods, while Unigram as shown in figure ~\ref{fig:roc-curve-unigram-before} performs the worst. XSIR and EXP as shown in figures ~\ref{fig:roc-curve-xsir-before} and ~\ref{fig:roc-curve-exp-before} respectively show higher TPRs compared to KGW and Unigram. In all watermark methods, English language shows stable AUC curve in all FPRs in all watermark methods. For KGW and Unigram, increasing the value of $\gamma$ and decreasing the value of $\delta$ has adverse effect on the TPR characteristic, especially for languages other than English (see subsequent sections for roc curves with zoomed-in rates). 

\citet{kirchenbauer2023watermark} has investigated the tight relationship of $\gamma$ and $\delta$ in watermark strength by creating a lower bound on $\gamma$ with the spike entropy in the picture. However, we believe more investigation is needed in cross-lingual manner. For example, beside the effect of the sampling method (greedy or multinomial), what could be done to incorporate the role of the tokenization of different languages in the $\gamma$ lower bound analysis? By doing a simple analysis of the results with $\gamma=0.9$ and $\delta=0.2$, we found that only an average of $2$ greenlist tokens are missing to raise the z-score from, say $3.52$ to $4$, with a z-score of threshold $4$. As shown in Figures \ref{fig:roc-curve-kgw-before-zoomed} and \ref{fig:roc-curve-unigram-before-zoomed}, the z-score threshold performs adequately for both methods in English but struggles with other languages. At FPR 0\%, KGW experiences the most significant TPR drop in Arabic and Indonesian, while Unigram reach worst TPR primarily in Chinese and Indonesian. This suggests that the current z-score setting may not be optimal for all languages, particularly for this $\gamma$ configuration. Further investigation into language-specific z-score adjustments is left for future work.

While this is more evident in KGW and Unigram, XSIR as shown in Figure \ref{fig:roc-curve-xsir-zoomed} has relatively minor effects for smaller values of $\delta$. This is because XSIR logically uses $50\%$ of the total vocabulary, which is high compared to a smaller $\delta$ value such as $2.0$.

For the Exponential Minimum Sampling or EXP as shown in Figure ~\ref{fig:roc-curve-exp-zoomed}, all languages are closely stable in terms of the curve characteristics.

% \begin{figure*}[th]
%   \subfigure[KGW]{\includegraphics[width=0.98\columnwidth]{latex/images/KGW/roc_curves_all_KGW_multiple_gammas.pdf}\label{fig:roc-curve-kgw-before-multiple}} \hfill
%   \subfigure[Unigram]{\includegraphics[width=0.98\columnwidth]{latex/images/Unigram/roc_curves_all_Unigram_multiple_gammas.pdf}\label{fig:roc-curve-unigram-before-multiple}}
%   \vspace{-3mm}
%   \caption {\mynote{Similar to figures 4 \& 5 except here we fix $\delta$ at $2.0$ and vary $\gamma$} KGW and Unigram with $\gamma=(0.1, 0.5, 0.9)$ and with a fixed $\delta=2.0$. The larger the $\gamma$ ratio, the worse the watermark detection.}
%   \label{fig:roc-curves-kgw-unigram-before-multiple-gammas}
%   \vspace{-3mm}
% \end{figure*}

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{ccccccccccc}
        \hline
        \multirow{2}{*}{Method} & \multirow{2}{*}{Language} & \multirow{2}{*}{$\gamma$} & \multicolumn{4}{c}{Metrics ($z=4.0$)} & \multicolumn{4}{c}{Metrics ($z=5.0$)} \\
        \cline{4-7}
        \cline{8-11}
        & & & TPR & TNR & FPR & FNR & TPR & TNR & FPR & FNR\\
        \hline
        \multirow{12}{*}{KGW} & \multirow{3}{*}{English}
        & 0.1 & 0.998 & 1.0 & 0.0 & 0.002 & 0.994 & 1.0 & 0.0 & 0.006 \\
        & & 0.5 & 0.998	& 1.0 & 0.0	& 0.002 & 0.998 & 1.0 & 0.0 & 0.002 \\
        & & 0.9 & 0.162	& 1.0 & 0.0 & 0.838 & 0.0 & 1.0 & 0.0 & 1.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Arabic}
        & 0.1 & 0.970 & 0.990 & 0.010 & 0.030 & 0.954 & 0.994 & 0.006 & 0.046 \\
        & & 0.5 & 0.974 & 0.994 & 0.006 & 0.026 & 0.958 & 0.998 & 0.002 & 0.042 \\
        & & 0.9 & 0.112 & 1.0 & 0.0 & 0.888 & 0.0 & 1.0 & 0.0 & 1.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Chinese}
        & 0.1 & 1.0 & 0.992 & 0.008 & 0.0 & 0.994 & 0.992 & 0.008 & 0.006 \\
        & & 0.5 & 1.0 & 0.998 & 0.002 & 0.0 & 0.992 & 1.0 & 0.0 & 0.008 \\
        & & 0.9 & 0.162 & 1.0 & 0.0 &0.838 & 0.0 & 1.0 & 0.0 & 1.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Indonesian}
        & 0.1 & 0.956 & 1.0 & 0.0 & 0.044 & 0.902 & 1.0 & 0.0 & 0.098 \\
        & & 0.5 & 0.966 & 0.996 & 0.004 & 0.034 & 0.862 & 1.0 & 0.0 & 0.138 \\
        & & 0.9 & 0.026 & 1.0 & 0.0 & 0.974 & 0.0 & 1.0 & 0.0 & 1.0 \\
        \hline
        \multirow{12}{*}{Unigram} & \multirow{3}{*}{English}
        & 0.1 & 0.986 & 0.998 & 0.002 & 0.014 & 0.936 & 1.0 & 0.0 & 0.064 \\
        & & 0.5 & 1.0 & 0.990 & 0.010 & 0.0 & 1.0 & 0.998 & 0.002 & 0.0 \\
        & & 0.9 & 0.244 & 1.0 & 0.0 & 0.756 & 0.0 & 1.0 & 0.0 & 1.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Arabic}
        & 0.1 & 0.976 & 0.970 & 0.030 & 0.024 & 0.948 & 0.976 & 0.024 & 0.052 \\
        & & 0.5 & 0.986 & 0.970 & 0.030 & 0.014 & 0.970 & 0.996 & 0.004 & 0.030 \\
        & & 0.9 & 0.432 & 0.998 & 0.002 & 0.568 & 0.0 & 1.0 & 0.0 & 1.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Chinese}
        & 0.1 & 1.0 & 0.856 & 0.144 & 0.0 & 1.0 & 0.926 & 0.074 & 0.0 \\
        & & 0.5 & 0.998 & 0.876 & 0.124 & 0.002 & 0.996 & 0.926 & 0.074 & 0.004 \\
        & & 0.9 & 0.500 & 0.946 & 0.054 & 0.500 & 0.0 & 1.0 & 0.0 & 1.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Indonesian}
        & 0.1 & 0.952 & 0.970 & 0.030 & 0.048 & 0.922 & 0.982 & 0.018 & 0.078 \\
        & & 0.5 & 0.984 & 0.954 & 0.046 & 0.016 & 0.962 & 0.990 & 0.010 & 0.038 \\
        & & 0.9 & 0.052 & 0.998 & 0.002 & 0.948 & 0.0 & 1.0 & 0.0 & 1.0 \\
        \hline
    \end{tabular}
    \caption{Performance Metrics for Multiple Languages and Watermarking Methods calculated by following the test statistics score with two threshold values as shown by $z$. The results show the effect of varying $\gamma$ values with a fixed $\delta=2.0$ for KGW \citet{kirchenbauer2023watermark} and Unigram \citet{zhao2023provable}.}
    \label{tab:all-error-rates-gamma}
\end{table*}

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{ccccccccccc}
        \hline
        \multirow{2}{*}{Method} & \multirow{2}{*}{Language} & \multirow{2}{*}{$\delta$} & \multicolumn{4}{c}{Metrics ($z=4.0$)} & \multicolumn{4}{c}{Metrics ($z=5.0$)} \\
        \cline{4-7}
        \cline{8-11}
        & & & TPR & TNR & FPR & FNR & TPR & TNR & FPR & FNR \\
        \hline
        \multirow{12}{*}{KGW} & \multirow{3}{*}{English}
        & 2.0 & 0.998 & 1.0 & 0.0 & 0.002 & 0.998 & 1.0 & 0.0 & 0.002 \\
        & & 5.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
        & & 10.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Arabic}
        & 2.0 & 0.974 & 0.994 & 0.006 & 0.026 & 0.958 & 0.998 & 0.002 & 0.042\\
        & & 5.0 & 1.0 & 0.998 & 0.002 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
        & & 10.0 & 1.0 & 0.998 & 0.002 & 0.0 & 1.0 & 1.0  0.0 & 0.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Chinese}
        & 2.0 & 1.0 & 0.998 & 0.002 & 0.0 & 0.992 & 1.0 & 0.0 & 0.008 \\
        & & 5.0 & 1.0 & 0.994 & 0.006 & 0.0 & 1.0 & 0.998 & 0.002 & 0.0 \\
        & & 10.0 & 1.0 & 0.998 & 0.002 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Indonesian}
        & 2.0 & 0.966 & 0.996 & 0.004 & 0.034 & 0.862 & 1.0 & 0.0 & 0.138 \\
        & & 5.0 & 0.996 & 1.0 & 0.0 & 0.004 & 0.992 & 1.0 & 0.0 & 0.008 \\
        & & 10.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
        \hline
        \multirow{12}{*}{Unigam} & \multirow{3}{*}{English}
        & 2.0 & 1.0 & 0.990 & 0.010 & 0.0 & 1.0 & 0.998 & 0.002 & 0.0 \\
        & & 5.0 & 1.0 & 0.984 & 0.016 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
        & & 10.0 & 1.0 & 0.984 & 0.016 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Arabic}
        & 2.0 & 0.986 & 0.970 & 0.030 & 0.014 & 0.97 & 0.996 & 0.004 & 0.03 \\
        & & 5.0 & 1.0 & 0.972 & 0.028 & 0.0 & 1.00 & 0.990 & 0.010 & 0.00 \\
        & & 10.0 & 1.0 & 0.960 & 0.040 & 0.0 & 1.00 & 0.990 & 0.010 & 0.00 \\
        \cline{2-11}
        & \multirow{3}{*}{Chinese}
        & 2.0 & 0.998 & 0.876 & 0.124 & 0.002 & 0.996 & 0.926 & 0.074 & 0.004 \\
        & & 5.0 & 1.0 & 0.890 & 0.110 & 0.0 & 1.0 & 0.924 & 0.076 & 0.0 \\
        & & 10.0 & 1.0 & 0.878 & 0.122 & 0.0 & 1.0 & 0.922 & 0.078 & 0.0 \\
        \cline{2-11}
        & \multirow{3}{*}{Indonesian}
        & 2.0 & 0.984 & 0.954 & 0.046 & 0.016 & 0.962 & 0.990 & 0.010 & 0.038 \\
        & & 5.0 & 1.0 & 0.962 & 0.038 & 0.0 & 1.0 & 0.994 & 0.006 & 0.0 \\
        & & 10.0 & 1.0 & 0.942 & 0.058 & 0.0 & 1.0 & 0.984 & 0.016 & 0.0 \\
        \hline
        & & & \multicolumn{4}{c}{Metric ($z=0.2$)} & \multicolumn{4}{c}{Metric ($z=0.3$)} \\
        \cline{4-7}
        \cline{8-11}
        & & & TPR & TNR & FPR & FNR & TPR & TNR & FPR & FNR \\
        \cline{2-11}
        \multirow{12}{*}{XSIR} & \multirow{3}{*}{English}
        & 2.0 & 1.0 & 0.702 & \textbf{0.298} & 0.0 & 0.994 & 0.958 & 0.042 & 0.006 \\
        & & 5.0 & 1.0 & 0.734 & \textbf{0.266} & 0.0 & 0.998 & 0.964 & 0.036 & 0.002 \\
        & & 10.0 & 0.996 & 0.708 & \textbf{0.292} & 0.004 & 0.996 & 0.960 & 0.040 & 0.004 \\
        \cline{2-11}
        & \multirow{3}{*}{Arabic}
        & 2.0 & 0.998 & 0.916 & 0.084 & 0.002 & 0.996 & 0.994 & 0.006 & 0.004 \\
        & & 5.0 & 1.0 & 0.928 & 0.072 & 0.0 & 0.998 & 0.994 & 0.006 & 0.002 \\
        & & 10.0 & 1.0 & 0.898 & \textbf{0.102} & 0.0 & 0.998 & 0.982 & 0.018 & 0.002 \\
        \cline{2-11}
        & \multirow{3}{*}{Chinese}
        & 2.0 & 1.0 & 0.966 & 0.034 & 0.0 & 0.994 & 0.994 & 0.006 & 0.006 \\
        & & 5.0 & 0.996 & 0.944 & 0.056 & 0.004 & 0.992 & 0.982 & 0.018 & 0.008 \\
        & & 10.0 & 0.992 & 0.964 & 0.036 & 0.008 & 0.992 & 0.994 & 0.006 & 0.008 \\
        \cline{2-11}
        & \multirow{3}{*}{Indonesian}
        & 2.0 & 0.988 & 0.990 & 0.010 & 0.012 & 0.976 & 0.998 & 0.002 & 0.024 \\
        & & 5.0 & 0.998 & 0.994 & 0.006 & 0.002 & 0.998 & 1.0 & 0.0 & 0.002 \\
        & & 10.0 & 0.996 & 0.984 & 0.016 & 0.004 & 0.994 & 0.998 & 0.002 & 0.006 \\
        \hline
        & & & \multicolumn{4}{c}{Metric ($p=10e-5$)} & \multicolumn{4}{c}{Metric ($p=10e-4$)} \\
        \cline{4-7}
        \cline{8-11}
        & & & TPR & TNR & FPR & FNR & TPR & TNR & FPR & FNR \\
        \cline{2-11}
        \multirow{4}{*}{EXP} & English
        & - & 0.998 & 1.0 & 0.0 & 0.002 & 0.998 & 0.996 & 0.004 & 0.002 \\
        & Arabic & - & 0.982 & 1.0 & 0.0 & 0.018 & 0.982 & 1.0 & 0.0 & 0.018 \\
        & Chinese & - & 0.994 & 1.0 & 0.0 & 0.006 & 0.994 & 1.0 & 0.0 & 0.006 \\
        & Indonesian & - & 0.990 & 1.0 & 0.0 & 0.010 & 0.992 & 1.0 & 0.0 & 0.008\\
        \hline
    \end{tabular}
    \caption{Performance Metrics for Multiple Languages and Watermarking Methods calculated by following the test statistics score. The results show the effect of varying $\delta$ values with a fixed of $\gamma=0.5$ for KGW, Unigram, and XSIR. Using a threshold of $0.2$ for XSIR resulted in higher FPRs Specifically for the English language. (see note below).}\vspace{0.5em}\footnotetext{}XSIR logically employs 50-50 split ratio. It's not a hyper-parameter that can be changed directly.
    \label{tab:all-error-rates-delta}
\end{table*}

\begin{figure*}[th]
  \subfigure[Translation->Paraphrase Attacks]{\includegraphics[width=0.98\columnwidth]{latex/images/roc_curves_selected_methods_kgw_unigram_exp_translation_paraphrase.pdf}}\label{fig:roc-curve-kgw-unigram-exp-translations_paraphrase} \hfill
  \subfigure[Translation->Paraphrase->Translation Attacks]{\includegraphics[width=0.98\columnwidth]{latex/images/roc_curves_selected_methods_kgw_unigram_exp_paraphrase_translation.pdf}\label{fig:roc-curve-kgw-unigram-exp-all-attacks}}
  \vspace{-3mm}
  \caption {Watermark detection ROC curves with AUC after attacks for syntactical methods KGW, Unigram, and EXP. We fix $\gamma=0.5$ and $\delta=2.0$ for KGW and Unigram. For each attack, the watermark threshold is calculated automatically by comparing unwatermarked and attacked watermarked text score for $100$ generations. The bottom row in all figures represent XSIR original attack setting in which non-English language is used first.}
  \label{fig:roc-curves-syntactical-methods-after}
  \vspace{-3mm}
\end{figure*}
\subsection{Watermark Detection After Attacks for Syntactical Methods}
In Figure~\ref{fig:roc-curve-kgw-unigram-exp-all-attacks} shows the completion of attack pipeline performed on KGW, Unigram, and EXP. Translation followed by paraphrase attacks are no worse than translations alone for syntactical methods. However, we notice that beginning with English in the pipeline performs better in all methods as can be seen from the top row of all the figures. Translation-paraphrase-translation performs well when English is the source language in all methods across all target languages, whereas the detection is very low when English is not the source language. 
%% Detection all methods no smaller values of FPRs.%%%
%%%% Begin %%%%%
% \begin{figure*}[th]
%   \subfigure[KGW]{\includegraphics[width=0.98\columnwidth]{latex/images/KGW/roc_curves_KGW_with_legend_on_top.pdf}\label{fig:roc-curve-kgw-lang-separate-before}}\hfill
%   \subfigure[Unigram]{\includegraphics[width=0.98\columnwidth]{latex/images/Unigram/roc_curves_Unigram_with_legend_on_top.pdf}\label{fig:roc-curve-unigram-lang-separate-before}}
%   \vspace{-3mm}
%   \caption {Watermark detection ROC curves with AUC before attacks. We show multiple $\gamma$ and $\delta$ values for KGW, Unigram. The watermark threshold is calculated automatically by comparing unwatermarked and watermarked scores for $500$ generations.}
%   \label{fig:roc-curves-kgw-unigram-separate-before}
%   \vspace{-3mm}
% \end{figure*}

% \begin{figure*}[th]
%     \subfigure[XSIR]{\includegraphics[width=0.98\columnwidth]{latex/images/XSIR/roc_curves_XSIR_with_legend_on_top.pdf}\label{fig:roc-curve-xsir-lang-separate-before}} \hfill
%   \subfigure[EXP]{\includegraphics[width=0.98\columnwidth]{latex/images/EXP/roc_curves_EXP_with_legend_on_top.pdf}\label{fig:roc-curve-exp-lang-separate-before}}
%   \vspace{-3mm}
%   \caption {Watermark detection ROC curves with AUC before attacks. We show multiple $\delta$ values for XSIR since we have one $\gamma=0.5$. The watermark threshold is calculated automatically by comparing unwatermarked and watermarked scores for $500$ generations.}
%   \label{fig:roc-curves-xsir-exp-separate-before}
%   \vspace{-3mm}
% \end{figure*}
%%%%% end %%%%%%%%%%
\end{document}
