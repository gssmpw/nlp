% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% GPT misuses
@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@inproceedings{cai2024wbp,
  title={WBP: Training-Time Backdoor Attacks Through Hardware-Based Weight Bit Poisoning},
  author={Cai, Kunbei and Zhang, Zhenkai and Lou, Qian and Yao, Fan},
  booktitle={European Conference on Computer Vision},
  pages={179--197},
  year={2024},
  organization={Springer}
}


@article{zhang2025towards,
  title={Towards Safe AI Clinicians: A Comprehensive Study on Large Language Model Jailbreaking in Healthcare},
  author={Zhang, Hang and Lou, Qian and Wang, Yanshan},
  journal={arXiv preprint arXiv:2501.18632},
  year={2025}
}


# Evaluations
% new evaluation metric
@article{singh2023new,
  title={New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking},
  author={Singh, Karanpartap and Zou, James},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

% gpt judge
@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

% bleu
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
% Rouge
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}


% Watermark methods -- proactive --
% KGW 1
@inproceedings{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17061--17084},
  year={2023},
  organization={PMLR}
}
% KGW 2
@article{kirchenbauer2023reliability,
  title={On the reliability of watermarks for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Shu, Manli and Saifullah, Khalid and Kong, Kezhi and Fernando, Kasun and Saha, Aniruddha and Goldblum, Micah and Goldstein, Tom},
  journal={arXiv preprint arXiv:2306.04634},
  year={2023}
}

% Unigram
@article{zhao2023provable,
  title={Provable robust watermarking for ai-generated text},
  author={Zhao, Xuandong and Ananth, Prabhanjan and Li, Lei and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2306.17439},
  year={2023}
}
% EXP
@misc{aaronson_my_2022,
  title = {My {{AI Safety Lecture}} for {{UT Effective Altruism}}},
  author = {Aaronson, Scott},
  year = {2022},
  month = nov,
  journal = {Shtetl-Optimized},
  url = {https://scottaaronson.blog/?p=6823},
  urldate = {2023-01-12},
  abstract = {Two weeks ago, I gave a lecture setting out my current thoughts on AI safety, halfway through my year at OpenAI. I was asked to speak by UT Austin's Effective Altruist club. You can watch the\ldots},
  langid = {american}
}
% Gumbel trick
@inproceedings{papandreou2011perturb,
  title={Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models},
  author={Papandreou, George and Yuille, Alan L},
  booktitle={2011 international conference on computer vision},
  pages={193--200},
  year={2011},
  organization={IEEE}
}

% EXP-edit
@article{kuditipudi2023robust,
  title={Robust distortion-free watermarks for language models},
  author={Kuditipudi, Rohith and Thickstun, John and Hashimoto, Tatsunori and Liang, Percy},
  journal={arXiv preprint arXiv:2307.15593},
  year={2023}
}
% Christ
@inproceedings{christ2024undetectable,
  title={Undetectable watermarks for language models},
  author={Christ, Miranda and Gunn, Sam and Zamir, Or},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={1125--1139},
  year={2024},
  organization={PMLR}
}

% EWD
@article{lu2024entropy,
  title={An Entropy-based Text Watermarking Detection Method},
  author={Lu, Yijian and Liu, Aiwei and Yu, Dianzhi and Li, Jingjing and King, Irwin},
  journal={arXiv preprint arXiv:2403.13485},
  year={2024}
}
% SWEET
@article{lee2023wrote,
  title={Who wrote this code? watermarking for code generation},
  author={Lee, Taehyun and Hong, Seokhee and Ahn, Jaewoo and Hong, Ilgee and Lee, Hwaran and Yun, Sangdoo and Shin, Jamin and Kim, Gunhee},
  journal={arXiv preprint arXiv:2305.15060},
  year={2023}
}
@inproceedings{lou2022trojtext,
  title={TrojText: Test-time Invisible Textual Trojan Insertion},
  author={Lou, Qian and Liu, Yepeng and Feng, Bo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@incollection{al2023trojbits,
  title={Trojbits: A hardware aware inference-time attack on transformer-based language models},
  author={Al Ghanim, Mansour and Santriaji, Muhammad and Lou, Qian and Solihin, Yan},
  booktitle={ECAI 2023},
  pages={60--68},
  year={2023},
  publisher={IOS Press}
}

@inproceedings{lou2024cr,
  title={CR-UTP: Certified Robustness against Universal Text Perturbations on Large Language Models},
  author={Lou, Qian and Liang, Xin and Xue, Jiaqi and Zhang, Yancheng and Xie, Rui and Zheng, Mengxin},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},
  pages={9863--9875},
  year={2024}
}

@inproceedings{zheng2024ssl,
  title={Ssl-cleanse: Trojan detection and mitigation in self-supervised learning},
  author={Zheng, Mengxin and Xue, Jiaqi and Wang, Zihao and Chen, Xun and Lou, Qian and Jiang, Lei and Wang, Xiaofeng},
  booktitle={European Conference on Computer Vision},
  pages={405--421},
  year={2024},
  organization={Springer}
}


@inproceedings{zheng2023trojfsp,
  title={TrojFSP: Trojan Insertion in Few-shot Prompt Tuning},
  author={Zheng, Mengxin and Xue, Jiaqi and Chen, Xun and Wang, Yanshan and Lou, Qian and Jiang, Lei},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={1141--1151},
  year={2024}
}

@inproceedings{xue2024badfair,
  title={BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers},
  author={Xue, Jiaqi and Lou, Qian and Zheng, Mengxin},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={8257--8270},
  year={2024}
}

@inproceedings{lou2024cr,
  title={CR-UTP: Certified Robustness against Universal Text Perturbations on Large Language Models},
  author={Lou, Qian and Liang, Xin and Xue, Jiaqi and Zhang, Yancheng and Xie, Rui and Zheng, Mengxin},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={9863--9875},
  year={2024}
}

@article{xue2024badrag,
  title={BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models},
  author={Xue, Jiaqi and Zheng, Mengxin and Hu, Yebowen and Liu, Fei and Chen, Xun and Lou, Qian},
  journal={arXiv preprint arXiv:2406.00083},
  year={2024}
}

@article{xue2024trojllm,
  title={Trojllm: A black-box trojan prompt attack on large language models},
  author={Xue, Jiaqi and Zheng, Mengxin and Hua, Ting and Shen, Yilin and Liu, Yepeng and B{\"o}l{\"o}ni, Ladislau and Lou, Qian},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{gha2024jailbreaking,
  title={Jailbreaking LLMs with Arabic Transliteration and Arabizi},
  author={Ghanim, Mansour and Almohaimeed, Saleh and Zheng, Mengxin and Solihin, Yan and Lou, Qian},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={18584--18600},
  year={2024}
}





% publicly unforgeable
@inproceedings{liu2023unforgeable,
  title={An unforgeable publicly verifiable watermark for large language models},
  author={Liu, Aiwei and Pan, Leyi and Hu, Xuming and Li, Shuang and Wen, Lijie and King, Irwin and Philip, S Yu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

% Cross-lingual watermarking and semantic watermarking
% XSIR
@article{he2024can,
  title={Can watermarks survive translation? on the cross-lingual consistency of text watermark for large language models},
  author={He, Zhiwei and Zhou, Binglin and Hao, Hongkun and Liu, Aiwei and Wang, Xing and Tu, Zhaopeng and Zhang, Zhuosheng and Wang, Rui},
  journal={arXiv preprint arXiv:2402.14007},
  year={2024}
}
% SIR
@inproceedings{
  liu2024a,
  title={A Semantic Invariant Robust Watermark for Large Language Models},
  author={Aiwei Liu and Leyi Pan and Xuming Hu and Shiao Meng and Lijie Wen},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=6p8lpe4MNf}
}

@article{alshammari2024ai,
  title={Ai-generated text detector for arabic language using encoder-based transformer architecture},
  author={Alshammari, Hamed and El-Sayed, Ahmed and Elleithy, Khaled},
  journal={Big Data and Cognitive Computing},
  volume={8},
  number={3},
  pages={32},
  year={2024},
  publisher={MDPI}
}
@article{alshammari2024toward,
  title={Toward Robust Arabic AI-Generated Text Detection: Tackling Diacritics Challenges},
  author={Alshammari, Hamed and Elleithy, Khaled},
  journal={Information},
  volume={15},
  number={7},
  pages={419},
  year={2024},
  publisher={MDPI}
}
@inproceedings{alginahi2013enhanced,
  title={An enhanced Kashida-based watermarking approach for Arabic text-documents},
  author={Alginahi, Yasser M and Kabir, Muhammad N and Tayan, Omar},
  booktitle={2013 International Conference on Electronics, Computer and Computation (ICECCO)},
  pages={301--304},
  year={2013},
  organization={IEEE}
}
% cross-lingual speech
@article{biadsy2024zero,
  title={Zero-shot cross-lingual voice transfer for tts},
  author={Biadsy, Fadi and Chen, Youzheng and Elias, Isaac and Kastner, Kyle and Wang, Gary and Rosenberg, Andrew and Ramabhadran, Bhuvana},
  journal={arXiv preprint arXiv:2409.13910},
  year={2024}
}



% synthid
@article{dathathri2024scalable,
  title={Scalable watermarking for identifying large language model outputs},
  author={Dathathri, Sumanth and See, Abigail and Ghaisas, Sumedh and Huang, Po-Sen and McAdam, Rob and Welbl, Johannes and Bachani, Vandana and Kaskasoli, Alex and Stanforth, Robert and Matejovicova, Tatiana and others},
  journal={Nature},
  volume={634},
  number={8035},
  pages={818--823},
  year={2024},
  publisher={Nature Publishing Group UK London}
}
% Postmark
@article{chang2024postmark,
  title={PostMark: A Robust Blackbox Watermark for Large Language Models},
  author={Chang, Yapei and Krishna, Kalpesh and Houmansadr, Amir and Wieting, John and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2406.14517},
  year={2024}
}
% Semstamp
@article{hou2023semstamp,
  title={Semstamp: A semantic watermark with paraphrastic robustness for text generation},
  author={Hou, Abe Bohan and Zhang, Jingyu and He, Tianxing and Wang, Yichen and Chuang, Yung-Sung and Wang, Hongwei and Shen, Lingfeng and Van Durme, Benjamin and Khashabi, Daniel and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2310.03991},
  year={2023}
}
% k-semstamp
@article{hou2024k,
  title={k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text},
  author={Hou, Abe Bohan and Zhang, Jingyu and Wang, Yichen and Khashabi, Daniel and He, Tianxing},
  journal={arXiv preprint arXiv:2402.11399},
  year={2024}
}


% Discriminator methods -- passive approaches --
% gptzero, ...
@misc{tian_gptzero_2023,
  type = {Substack Newsletter},
  title = {Gptzero Update V1},
  author = {Tian, Edward},
  year = {2023},
  month = jan,
  journal = {GPTZero},
  url = {https://gptzero.substack.com/p/gptzero-update-v1},
  urldate = {2023-01-12},
  abstract = {General Updates: Thank you sincerely for signing up for the GPTzero beta. I'm completely awestruck by the support this app has generated. In the past day, over 4000+ people have signed up for the beta (via this substack) and 10,000+ more have tried and tested it out on the Streamlit}
}
% detectgpt
@article{mitchell_detectgpt_2023,
  title = {{{DetectGPT}}: {{Zero-Shot Machine-Generated Text Detection}} Using {{Probability Curvature}}},
  shorttitle = {{{DetectGPT}}},
  author = {Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D. and Finn, Chelsea},
  year = {2023},
  month = jan,
  doi = {10.48550/arXiv.2301.11305},
  url = {https://arxiv.org/abs/2301.11305v1},
  urldate = {2023-01-27},
  abstract = {The fluency and factual knowledge of large language models (LLMs) heightens the need for corresponding systems to detect whether a piece of text is machine-written. For example, students may use LLMs to complete written assignments, leaving instructors unable to accurately assess student learning. In this paper, we first demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g, T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.},
  langid = {english}
}
%GLTR
@article{gehrmann2019gltr,
  title={Gltr: Statistical detection and visualization of generated text},
  author={Gehrmann, Sebastian and Strobelt, Hendrik and Rush, Alexander M},
  journal={arXiv preprint arXiv:1906.04043},
  year={2019}
}
% abdel nabi adv text watermarking
@inproceedings{abdelnabi2021adversarial,
  title={Adversarial watermarking transformer: Towards tracing text provenance with data hiding},
  author={Abdelnabi, Sahar and Fritz, Mario},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={121--140},
  year={2021},
  organization={IEEE}
}


%%% LLMs %%%
% Sailor 2
@misc{sailor2report,
  title={Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLM},
  author={{Sailor2 Team}},
  year={2024}
}
% ACEGPT from freedomintelligence
@misc{huang2023acegpt,
      title={AceGPT, Localizing Large Language Models in Arabic}, 
      author={Huang Huang and Fei Yu and Jianqing Zhu and Xuening Sun and Hao Cheng and Dingjie Song and Zhihong Chen and Abdulmohsen Alharthi and Bang An and Ziche Liu and Zhiyi Zhang and Junying Chen and Jianquan Li and Benyou Wang and Lian Zhang and Ruoyu Sun and Xiang Wan and Haizhou Li and Jinchao Xu},
      year={2023},
      eprint={2309.12053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
% bloomz 7b mt from bigscience
@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}
% jais family 6.7 b from inceptionai
@article{jaisfamilymodelcard,
    title={Jais Family Model Card},
    author={Inception},
    year={2024},
    url = {https://huggingface.co/inceptionai/jais-family-30b-16k-chat/blob/main/README.md}
}


% Datasets
% C4
@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

% Translation models
% Opus1
@InProceedings{tiedemann2020opus,
  author = {J{\"o}rg Tiedemann and Santhosh Thottingal},
  title = {{OPUS-MT} Ã¢ {B}uilding open translation services for the {W}orld},
  booktitle = {Proceedings of the 22nd Annual Conference of the European Association for Machine Translation},
  year = {2020}
 }
 % Opus2
 @article{tiedemann2022democratizing,
  title={Democratizing Machine Translation with OPUS-MT},
  author={Tiedemann, J{\"o}rg and Aulamo, Mikko and Bakshandaeva, Daria and Boggia, Michele and Gr{\"o}nroos, Stig-Arne and Nieminen, Tommi and Raganato, Alessandro and Scherrer, Yves and Vazquez, Raul and Virpioja, Sami},
  journal={arXiv preprint arXiv:2212.01936},
  year={2022}
}

% Self-bleu
@article{zhu2018texygen,
  title={Texygen: A Benchmarking Platform for Text Generation Models},
  author={Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  journal={SIGIR},
  year={2018}
}

% watermark attacks
@inproceedings{pang2024no,
  title={No free lunch in llm watermarking: Trade-offs in watermarking design choices},
  author={Pang, Qi and Hu, Shengyuan and Zheng, Wenting and Smith, Virginia},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

