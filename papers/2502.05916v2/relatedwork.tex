\section{RELATED WORKS}
Recent research on grasping unknown objects typically addresses cluttered and moving objects separately, but few can combine these two types of uncertainty due to the greatly increased difficulty in grasp detection and model training.

\textbf{Grasping objects in clutter.} As a representative of this research topic, GraspNet-1Billion \cite{c7} is a state-of-the-art generalized object grasping model trained on a large-scale dataset consisting of a large number of RGBD images and grasp poses. HGGD \cite{c8} is a more recent work that proposes a local grasp generator combined with grasp heatmaps as guidance to efficiently detect real-time grasps for cluttered objects. In addition to training directly on RGBD images or point clouds, VGN \cite{c9} and GIGA \cite{c10} propose to use Truncated Signed Distance Fields (TSDFs) to represent cluttered object scenes and utilize this representation to learn grasp detection. There are also reinforcement learning (RL) based approaches, such as QT-Opt \cite{c11} and VPG \cite{c12}, which collect large amounts of training data in a self-supervised manner and utilize skills such as pushing to achieve high success rates for cluttered object grasping. These methods share common drawbacks such as high-cost, time-consuming training process, and limited adaptability to varying scenes. In contrast, our method is training-free and does not require specific operating environments.

\textbf{Grasping objects in motion.} Dynamic grasping of moving objects has recently attracted attention in the field of robotic grasping. GG-CNN \cite{c13} is a well-known generative grasping network that is capable of handling dynamic scenes where object positions are changed after each grasp attempt. However, they need the objects to remain stationary during the grasp execution. Marturi et al. \cite{c14} achieve adaptive grasping for various types of moving objects by developing a local planner for object tracking and a global planner for grasp switching. However, they require prior observation of the target object from multiple viewpoints to obtain its complete surface geometry. Two recent works \cite{c15,c16} use Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM), respectively, to predict the future locations of moving objects for precise dynamic grasping. However, they require the target object to be pre-trained with robust grasps, whereas our method can handle novel objects without any prior knowledge. In addition, a few studies \cite{c17,c18} incorporate RL algorithms to achieve moving object grasping with a single camera. However, their methods have only been evaluated on single objects, not in cluttered scenes.