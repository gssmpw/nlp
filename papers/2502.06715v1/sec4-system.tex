\section{The Preprocessor} \label{sec:prep}

The preprocessor is responsible for partitioning the input relations,
and for computing the trie indices, called \indexlayout, for each
partition.  It receives the shares $P_1, P_2, \ldots, P_n$, and the
variable order $X_{\sigma(1)}, \ldots, X_{\sigma(n)}$ from the optimizer; for presentation
purposes we assume in this and the next section that the variable
order is $X_1, X_2, \ldots, X_n$, and we will revisit this in
Sec.~\ref{sec:opt}.

\begin{figure}[t]
  \centering
  \includegraphics[height=.33\textheight]{para_part}
  \caption{Parallel Partitioning}
  \label{fig:parapart}
\end{figure}

The input data is partitioned using the HyperCube method.  The
preprocessor chooses $n$ independent hash functions
$h_1, \ldots, h_n$, one for each query variable.  We will assume
w.l.o.g. that $h_i$ returns numbers in the set
$\set{0,1,\ldots,P_i-1}$ (otherwise we replace it with
$h_i(x) \mod P_i$).  Initially, each input relation
$R_j(X_{i_1},\ldots,X_{i_k})$ is stored in an array; we follow the C
convention and assume array indices start at 0.  Consider a $k$-tuple
$(x_1, \ldots, x_k)$ of $R_j$.  The quantities
$s_1 \defeq h_{i_1}(x_1), \ldots, s_k \defeq h_{i_k}(x_k)$ are called
the {\em partition identifiers} of this tuple.  The preprocessor
physically partitions $R_j$ into
$\Pi_j \defeq P_{i_1} \! \times \cdots \times  P_{i_k} \leq P$ chunks, called partitions,
where each partition $R_{j,s_1,\ldots,s_k}$ is a continuous array
holding all tuples with partition identifiers $s_1, \ldots, s_k$; all
partitions are concatenated and stored in an output array $\tilde R_j$
of the same size as $R_j$.  Unlike HyperCube, in \name there is no
need to replicate $R_j$, instead, the partitioned data $\tilde R_j$ is
an array of exactly the same size as $R_j$.  As we discuss in the next
section, during execution multiple threads will read from the same
partition, taking advantage of the shared memory.

  

We describe now the details of the parallel partitioning step.  First,
\name computes (in parallel) the partition identifiers
$(s_1,\ldots, s_k)$ of each tuple $(x_1, \ldots, x_k)$ in $R_j$, by
applying the hash functions, and storing them in an array $A_j$, of the
same size as $R_j$.  

Next, it computes the size of each partition
$R_{j,s_1,\ldots,s_k}$ and stores them in a $k$-dimensional array
$B_j$.  Next, it computes a prefix sum on $B_j$ and stores the result
in $B^{pref}_j$: notice that both $B_j$ and $B^{pref}_j$ have size $\Pi_j$ (the
number of partitions of $R_j$), which is $\leq P$ (the total number of
threads).  $B_j$ is computed by scanning the tuples
$(s_1, \ldots, s_k)$ in $A_j$ and incrementing
$B_j[s_1, \ldots, s_k]$; while $B^{pref}_j$ is the prefix sum of $B_j$,
$B^{pref}_j[s_1, \ldots, s_k] = \sum_{(t_1,\ldots,t_k)\preceq
  (s_1,\ldots,s_k)} B_j[t_1,\ldots,t_k]$, where $\preceq$ represents
lexicographic order.  We did not parallelize the computations of $B_j$
and $B^{pref}_j$ because they turned out to represent only a tiny fraction of
the total preprocessing time.  

Finally, \name allocates the output
array $\tilde R_j$ and copies the input tuples from $R_j$ into their
respective partition $R_{j,s_1,\ldots,s_k}$ in $\tilde R_j$: notice
that this partition starts at position $B^{pref}_j[s_1,\ldots,s_k]$ in
$\tilde R_j$.  Copying is done in parallel, using a number of threads
equal to the number of available cores.  Each thread is responsible
for an exclusive subset of the $\Pi_j$ partitions: it scans the entire
array $A_j$ and, if the current tuple $(s_1, \ldots, s_k)$ belongs to
one of its partitions, then it physically copies the corresponding
$R_j$-tuple to the partition $R_{j,s_1,\ldots, s_k}$, otherwise it
ignores the tuple.  

There is no write contention between threads and
the work is uniformly distributed.  Since all threads need to read the
entire vector $A_j$, in order to minimize the total amount of work, we
restrict the number of threads to the number of physical cores. At
the end of this phase, each partition $R_{j,s_1,\ldots,s_k}$ is stored
in a subarray of $\tilde R_j$, starting at the position
$B^{pref}_j[s_1, \ldots, s_k]$.

\begin{example} \label{ex:partition} For a simple illustration,
  consider the Loomis-Whitney query,
  $Q=R_1(X, Y, Z), R_2(X, Y, U),$ $R_3(X, Z, U), R_4(Y, Z, U)$.  Assume
  the variable order is $X,Y,Z,U$ and
  $P = 2 \times 2 \times 1 \times 2 = 8$.
  
  
  In Fig.~\ref{fig:parapart}, we show a simple instance of the
  relation $R(X,Y,Z)$, the arrays $B, B^{pref}$ and the output
  $\tilde R$.  For illustration, we made some arbitrary assumptions
  about the hash functions, e.g. $h_Y(1)=1$, $h_Y(4)=h_Y(5)=h_Y(3)=1$,
  shown in the relation $A$.  
  
  
  
  
  Each partition is color coded, and its tuples occur in a continuous
  subarray of $\tilde R$, e.g.  the grey partition with identifiers
  $0,1,0$ consists of $(0,4,1), (5,3,9)$, has size $B[0,1,0]=2$ and
  starts at position $B^{pref}[0,1,0]=3$ in $\tilde R$. The white
  Partition $1,0,0$ is empty: $B[1,0,0]=0$.
\end{example}

\begin{figure}[t]
  \centering
  \includegraphics[height=.33\textheight]{coco_index}
  \caption{Construction of \indexlayout}
  \label{fig:cocoindex}
\end{figure}

Next, the preprocessor computes a trie index for each partition
$R_{j,s_1, \ldots, s_k}$ of each relation $R_j$. The index is a new
and simple data structure that we call \indexlayout (Compressed Column
layout), \revB{which combines ideas from both the trie in
  LFTJ~\cite{DBLP:conf/icdt/Veldhuizen14} and the memory formats used
  by sparse tensor
  compilers~\cite{DBLP:journals/pacmpl/ChouKA18}}. \indexlayout is
defined as follows. For a $k$-ary relation $R(X_1, \ldots, X_k)$,
\indexlayout consists of $k$ arrays $C_1, ..., C_{k}$.  Each array
$C_r$ contains the entire level $r$ of a sorted trie.  Its entries are
pairs $(x_i,p_i)$, where $x_i$ is a value of $R.X_i$ and $p_i$ is an
index to the beginning of a subarray of $C_{i+1}$.  The top vector
consists of all distinct values $x_1$ in $R.X_1$, sorted in ascending
order, and, for each pair $(x_1, p_1)$ in $C_1$, the index $p_1$
represents the beginning of a subarray in $C_2$ that contains all
distinct values $x_2$ in $R[x_1].X_2$.  And so on. The last vector
$C_{k}$ has pointers in the data array $R$. \name constructs a
separate \indexlayout index for each partition, $R_{j,s_1,...,s_k}$ of
the relation $R_j$.

To build \indexlayout, we sort the partitions $R_{j,s_1,\ldots,s_k}$
lexicographically, according to the variable order provided by the
optimizer: recall that we assumed, for simplicity, that the order is
$X_1, \ldots, X_K$. For example for the triangle query in
Fig.~\ref{fig:algo-comp}, if the order is $X,Y,Z$ then the relation
$T(Z,X)$ will be sorted by $X$ first then by $Z$.  Since each
partition $R_{j,s_1,\ldots,s_k}$ is a subarrays of a larger array
$\tilde R_j$, there are two possibilities to sort it.  One is to sort
each partition independently, in parallel threads; we do this when the
number of partitions $\Pi_j$ of $R_j$ is larger than the number of
available cores. If $\Pi_j$ is too small (e.g. when all shares $P_i$
of all variables in $R_j$ are $=1$), then we use a single parallel
sorting function and sort together the pair of arrays
$A_j,\tilde R_j$, thus forcing the tuples with the same partition to
stay together.  For this purpose we use \textbf{ips4o} (In-place
Parallel Super Scalar Samplesort)~\cite{axtmann2017,
  axtmann2020engineering}, which is a highly optimized parallel
sorting function for multicores.

Next, for each sorted partition $R_{j, s_1, ..., s_k}$, the
\indexlayout is constructed by compressing the same column values with
the same prefix determined by the variable order.  For each variable
$X_r$, the array $C_r$ consists of two separate arrays $V_r, O_r$,
where $V_r$ for the values of $X_r$ and $O_r$ for offsets into
$C_{r+1}$.  

We first scan over all rows in the partition by computing
the number $l_r$ of unique values with prefix $(\ldots, s_r)$ for any
attribute $X_r$. Then, we allocate the space with $l_r$ size for $V_r$
and $l_{r}+1$ size for $O_r$ and scan again to populate them from
bottom to above. During the second pass, we bookkeep a cursor about
current offset $o_i$ in the compressed array for each attribute and
sequentially compare the adjacent two rows $(s_1, \ldots, s_{k})$ and
$(t_1, \ldots, t_{k})$ and find the index $e$ of the first differing
element $s_e \neq t_e$ but
$(s_1, \ldots, s_{e-1}) = (t_1, \ldots, t_{e-1})$. Then, we create a
new entry in $V_e$ with the value $t_e$ and the offset $o_e$ in $O_e$,
and increment $o_e$. Moreover, since the prefix is changed, we need to
create entries with value $t_d$ and offset $o_d$ for any $V_d, O_d$
with any $d > e$ and increment $o_d$ as well.  Besides, if the
relation is unique, we will ignore the offset vector of last attribute
$O_k$ and its construction.

\begin{example}
  In Fig.~\ref{fig:cocoindex}, we show the \indexlayout index for the relation $R(X, Y, Z)$.\footnote{It ought to be an index on partition, but for simplicity, just on the whole relation.} 
  The first vector $C_1 = C_X$ contains all distinct values $V_X$ of $X$ and the offsets $O_X$ in the $C_Y$ (starts with $0$). The second vector $C_2 = C_Y$ depends on the prefix of $X$ and contains all values $V_Y$ of $Y$ with different $X$, thus we can see two $4$ in the $V_Y$ since they have different $X$ values. 
  The last vector $V_3 = V_Z$ is just similar cases, but the last level offsets $O_3 = O_Z$ will be removed if the relation $R$ itself is distinct.
\end{example}

\section{Executor}

\label{sec:join}

\begin{figure}[t]
  \centering
  \includegraphics[height=.33\textheight]{coco_join}
  \caption{Join Execution on \indexlayout}
  \label{fig:cocojoin}
\end{figure}

After preprocessing, the Executor computes Generic Join, in parallel.
Recall that the query $Q$ in Eq.~\eqref{eq:full:cq} has $n$ variables
$X_1, \ldots, X_n$.  The optimizer has computed the shares
$P_1, \ldots, P_n$ for each variable, and has chosen a variable order,
which we assume w.l.o.g. is $X_1, \ldots, X_n$, while the preprocessor
has partitioned each input relation $R_j$ and moved it to a new array
$\tilde R_j$.

The executor assigns each partition to a thread with identifier
$(s_1, s_2, \ldots, s_n)$.  Each thread executes Generic Join
independently, with the only difference that, for each relation
$R_j(X_{i_1}, \ldots, X_{i_k})$, it only accesses the subarray of
$\tilde R_j$ corresponding to the partition
$R_{j,s_{i_1}, \ldots, s_{i_k}}$, which starts at offset
$C_j[s_{i_1}, \ldots, s_{i_k}]$ in $\tilde R_j$.  Otherwise, the
structure of the executor is the same as that of the standard generic
join: a sequence of nested loops, each corresponding to a variable
$X_i$.  The main work is done by the intersection
$R_{j_1}.X_i \cap R_{j_2}.X_i\cap \cdots$, which we describe in detail
next.  Notice that the total number of threads is the total number of
output partitions, which is $P_1 \times P_2 \times \cdots \times P_n= P$.  Also,
there is no read-write conflict between threads: their only
interaction is that they read from the shared memory.  Besides, due to
non-overlapping join results, each thread can store its outputs in its
local memory; at the end of the computation, these results can be
concatenated if needed.

Listing~\ref{lst:newpara} in Fig.~\ref{fig:algo-comp} shows the execution performed by the
thread with the identifier $(i,j,k)$; this thread only needs to access the
partitions $R_{ij}, S_{jk}$, and $T_{ik}$, and join them.

Next, we present the details of the intersection, which is the main
work done in WCOJ. By using \indexlayout, the intersection needs to compute common values in a set of sorted and contiguous
arrays.  For this purpose, \name uses ideas from multi-way merge.

Given a sorted array $V$ and a value $x$, the method
$\text{search}(V, x)$ (described below) that finds the position of the
first element in $V$ that is $\geq x$.  To compute the intersection of
multiple arrays $V_1 \cap V_2 \cap \cdots$ we use a cursor in each
array, and maintain the minimum $\text{min}$ and maximum $\text{max}$
values among all cursors.  If $\text{max} \neq \text{min}$, we just
advance the cursor of $\text{min}$ to
$\text{search}(V_j, \text{max})$; otherwise, we have found a common 
value $\text{min}=\text{max}$.

For the first attribute $X_1$ in variable order, the intersection
$R_{j_1}.X_1 \cap R_{j_2}.X_1 \cap \cdots$ is done by the above
procedure on the first levels $V_1$ in \indexlayout of
$R_{j_1}, R_{j_2}, \ldots$.  Once we encounter a value $X_1 = x$
present in all relation, for each relation $R_{j_k}$ we need to
restrict the range of next trie level.  Assuming that $x$ occurs on
position $\mathrm{index}$ in $V_1$, then the next level will be
restricted to the subarray
$V_2[O_1[\mathrm{index}]: O_1[\mathrm{index}+1]-1]$.  In general, each
intersection will be done by intersecting several arrays, which can be
either a full \indexlayout array $V_r$, or a subarray thereof.

\begin{example}
  In Fig.~\ref{fig:cocojoin}, The area surrounded by the brown line shows a state during the join between $R(X, Y, Z)$ and $S(X, Y, U)$ on the \indexlayout. Now, we already find that $R.X = S.X = 2$ is an intersection on the attribute $X$. To find the intersecting range of $Y$ in $R$, we need compute the $\mathrm{index}(R.X, V_X) = 1$, and obtain the range $[O_X[1], O_X[2]) = [2, 4)$ in the $V_Y$, which is the subarray $\{4, 5\}$. The similar case happened on $S.Y$ and obtained the subarray $\{0, 2, 4\}$. Then we do the intersection on two subarrays and find the intersected value $Y = 4$.
\end{example}

To further improve the intersections on sorted arrays efficiently, 
we propose three $search$ ways, which are Exponential, Quadratic, and Linear, 
and use them based on the number of attributes involved in the intersection. 
Exponential Search~\cite{DBLP:conf/birthday/Baeza-YatesS10}, also known as galloping search, 
is suitable when dealing with skewed and large datasets. 
It begins by jumping exponentially (i.e., $2^n$) through the data until it overshoots the target value, 
after which it switches to a binary search within the identified range. 
Quadratic Search is somehow used for medium-sized datasets 
and involves first a series of steps that increase by fixed size 
which is set to the multiple of cache line size, 
and then scan within identified range or recursively apply Quadratic search (but unrolling in implementation). 
Finally, Linear Search is employed for small datasets, 
as it is the most efficient search method for a small number of elements which is fitted the cache size.
By selecting the appropriate search method based on the size of the intersection, 
we ensure that the join operation is optimized for each scenario, maximizing efficiency and performance.

