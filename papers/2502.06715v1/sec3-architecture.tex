\section{Architecture of \name}

\begin{figure}[t]
	\includegraphics[width=.75\linewidth]{architecture}
	\Description[Arch]{The Architecture of \name}
	\caption{The Architecture of \name} 
	\label{fig:arch}
\end{figure}

\name takes as input a conjunctive query (see Eq.~\eqref{eq:full:cq})
and evaluates it in parallel, using a fixed number of threads $P$; we
use by default $P=1024$ threads.  The threads are executed in parallel
by all available cores on the system.  Since \name assumes that the
system has a large number of cores, e.g.  dozens, it aims to avoid any
synchronization between threads, because these can lead to significant
slowdown when the number of cores is large.  

\name uses the same
partitioning method as HyperCube, by computing a number of shares
$P_i$ for each query variable $X_i$, such that
$P_1 \times P_2 \times P_n=P$, but, unlike HyperCube, it does not
replicate the data, instead leverages the shared memory and allows
threads to read concurrently.  It optimizes both the variable order
and the shares together.  To avoid read-write conflicts, it
precomputes all trie indices eagerly, using a novel index
called~\indexlayout, which is optimized for modern hardware
architecture by minimizing pointer chasing and leveraging vector
processing capabilities.  

\name has four parts, see Fig.~\ref{fig:arch}: the Optimizer, the
Preprocessor, the Executor, and the Allocator.

The \textbf{optimizer}, described in Sec.~\ref{sec:opt}, uses a cost
model (Sec.~\ref{subsec:cost:model}) to determine both the variable
order and the shares for each variable (Sec.~\ref{subsec:plan}).  The
optimizer also performs query rewriting to remove some redundant
computations, described in Sec.~\ref{subsec:rewrite}.

Once the shares for each variable are computed, the
\textbf{Preprocessor} (Sec.~\ref{sec:prep}) sorts the input relations,
physically partitions them, and builds the~\indexlayout index for each
partition.  The partitioner is fully parallelized and its threads do
not require coordination.

Finally, the \textbf{Executor} (Sec.~\ref{sec:join}) performs the
actual work, by computing the query on each partition.  The algorithm
is similar to the standard Generic Join, with two exceptions: it uses
our sort-based index~\indexlayout instead of a hash-trie, and it may
compute and store some intermediate results, as introduced by the
optimizer during source rewriting. The executor is fully parallelized,
and its threads do not require coordination; as we show in
Sec.~\ref{sec:exp}, this allows it to scale almost linearly up to 60
physical cores.

The \textbf{Allocator} uses use Intel Thread Building Block (TBB) to
implement the scheduling framework~\cite{inteltbb}.  This is a
standard task-stealing technique to execute threads on the available
cores.  The framework dynamically allocates computational tasks to
cores, ensuring that all cores are engaged in meaningful work
throughout the execution process. Moreover, the Allocator is designed
to provide isolated partitioned data and cache for both the Executor
and Preprocessor, minimizing contention and maximizing efficiency.
Since this is a standard technique, we will not discuss the allocator
in this paper any further.

