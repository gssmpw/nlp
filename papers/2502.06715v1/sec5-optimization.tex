\section{Optimization}\label{sec:opt}

The optimizer in \name has three goals: it chooses shares
$P_1, \ldots, P_n$ for the variables $X_1, \ldots, X_n$, it chooses a
variable order $X_{\sigma(1)}, \ldots, X_{\sigma(n)}$ to be used by
the query executor, and, finally, it performs a rewriting of the code
of Generic Join in order to remove redundant computations inherent in
this algorithm.  All decisions are done jointly, and are informed by a
cost model described in Sec.~\ref{subsec:cost:model}.
Although it happens last, we start our presentation of the optimizer
by describing query rewriting.

\subsection{Query Rewriting}
\label{subsec:rewrite}

Recall that Generic Join consists of $n$ nested loops, one loop for
each variable $X_i$.  The actual work in GJ consists of computing the
intersection of all columns of the variables $X_i$.  For complex
queries, some of these intersections computed in the inner loops are
independent of outer loop and, thus, are repeated unnecessarily for
each iteration of the outer loop.  In this case our optimizer rewrites
the query such as to compute the intersection early, and stores this
as a temporary result.  We illustrate query rewriting with an example;
the general case follows immediately.

\begin{figure}[t]
  
{
  \begin{minipage}{0.495\textwidth}
\begin{lstlisting}[style=BashInputStyle,
  label=lst:beforeopt]
Q(X,Y,Z,U) = R1(X,Y), R2(X,Z), R3(X,U),
              $\,$R4(Y,Z), R5(Y,U), R6(Z,U).

// $\textbf{BEFORE REWRITING}$
For x $\in$ R1.X $\cap$ R2.X $\cap$ R3.X
  For y $\in$ R1[x].Y $\cap$ !\colorbox{green}{R4.Y $\cap$ R5.Y}!
    For z $\in$ !\colorbox{yellow}{R2[x].Z}! $\cap$ R4[y].Z $\cap$ !\colorbox{yellow}{R6.Z}!
      For u $\in$ !\colorbox{pink}{R3[x].U $\cap$ R5[y].U}! $\cap$ R6[z].U
        Q += (x, y, z, u)
\end{lstlisting}
  \end{minipage}
  
  \begin{minipage}{0.495\textwidth}
\begin{lstlisting}[style=BashInputStyle,
  label=lst:afteropt]  
// $\textbf{AFTER REWRITING}$ 
tmp_Y := !\colorbox{green}{R4.Y $\cap$ R5.Y}! // Lift Up Y
For x $\in$ R1.X $\cap$ R2.X $\cap$ R3.X
  tmp_Z := !\colorbox{yellow}{R2[x].Z $\cap$ R6.Z}! // Lift Up Z
  For y $\in$ R1[x].Y $\cap$ !\colorbox{green}{tmp\_Y}!
  tmp_U := !\colorbox{pink}{R3[x].U $\cap$ R5[y].U}! // Lift Up U
    For z $\in$ R4[y].Z $\cap$ !\colorbox{yellow}{tmp\_Z}!
      For u $\in$ R6[z].U $\cap$ !\colorbox{pink}{tmp\_U}!
        Q += (x, y, z, u)
\end{lstlisting}
  \end{minipage}
}
\caption{Example of Query Rewriting}
\label{fig:rewrtie}
\end{figure}

\begin{example}[Duplicated Intersections] \label{ex:duplicate:intersections}
Consider the following query which computes a 4-clique $Q$ on the variables $(X,Y,Z,U)$. Generic Join for $Q$ is shown in Fig~\ref{fig:rewrtie} (left), with three pieces of redundant
work highlighted. They do not depend on the current loop variable, and
therefore are computed repeatedly:

\revB{Here $R4[y].Z$ represents the column  $Z$ of the subset of $R4$ where $Y=y$.}

For example, R4.Y $\cap$ R5.Y is computed repeatedly, for every value
$x$, although this expression does not depend on $x$.

\end{example}

\revB{Given the variable order, our optimizer identifies 
  intersections  that can be  cached, then}

proceeds to "lift" these intersections, 
treating them as independent computational entities. 
These independent intersections are then computed only once before the iteration over the current loop variable and cached within auxiliary data structures. 
This process not only reduces the need for repeated calculations 
but also ensures that these pre-computed intersections 
can be efficiently accessed and reused across different parts of the join operation.

\begin{example}  \label{ex:duplicate:intersections:2}
  The optimizer rewrites the code above as follows:

\revB{ Instead of computing the intersection
  $R1[x].Y \cap R4.Y \cap R5.Y$ for each $x$, we compute
  $R4.Y \cap R5.Y$ before starting the iteration on $x$ and cache the
  result in a temporary sorted array \texttt{tmp\_Y}, as shown in Fig~\ref{fig:rewrtie} (right).  Then, during
  the iteration over $x$, we intersect $R1[x].Y$ with \texttt{tmp\_Y}.
  Importantly, the temporary array \texttt{tmp\_Y} also stores offsets
  corresponding to the value $y$ in the \indexlayout of $R4$ and $R5$.
  
  While this optimization does not improve the asymptotic runtime of
  WCOJ, it can improve the actual runtime.  To see this, consider how
  WCOJ computes the intersection $R1[x].Y \cap R4.Y \cap R5.Y$.
  Denoting the three sorted arrays $R1[x].Y$, $R4.Y$, $R5.Y$ by
  $A,B,C$ respectively, the simplified pseudocode is shown in
  Fig.~\ref{fig:multiwayjoin}: notice that \name uses exponential search
  instead of the linear search, see Sec.~\ref{sec:join}.  By
  precomputing the intersection of $B \cap C$, we replace a 3-way
  merge with two 2-way joins, which avoids repeating the same
  iterations over $B, C$ for every value of $x$, and also have a
  simpler code and better cache locality.  }

  

\begin{figure}[t]
  
{
  
  
  \begin{minipage}{0.495\textwidth}
    \begin{lstlisting}[style=BashInputStyle, label=lst:multijoin]
i = j = k = 0;
while not done:
  if A[i] = B[j] = C[k]: 
    output(A[i]); 
    i++, j++, k++;
  while (A[i] < min(B[j], C[k])):
    i++;
  while (B[j] < min(A[i], C[k])):
    j++;
  while (C[k] < min(A[i], B[j])):
    k++;
\end{lstlisting}
    \end{minipage}
    
    \begin{minipage}{0.495\textwidth}
\begin{lstlisting}[style=BashInputStyle, label=lst:multijoin]
j = k = p = 0;
while not done:
  if B[j] = C[k]: 
    Tmp[p++] = B[j++]; j++, k++;
  while (B[j] < C[k]): j++;
  while (C[k] < B[j]): k++;
 . . . .
i = p = 0
while not done:
  if A[i] = Tmp[p]: 
    output(A[i]); i++, p++;
  while (A[i] < Tmp[p]): i++;
  while (Tmp[[p] < Ai]): p++;
\end{lstlisting}
    \end{minipage}}

  \caption{\revB{Simplified Pseudocode for computing
      $A \cap B \cap C$, and same code that caches $B \cap C$ before
      computing the intersection.  \name uses exponential search
      (Sec.~\ref{sec:join}) instead of the linear search shown here.
      Notice that these two code fragments are equivalent only if
      $A,B,C$ are sorted arrays.}}
    \label{fig:multiwayjoin}
\end{figure}

\revB{ We note that this query rewriting is a logical optimization,
  and is not an optimization that can be done by a compiler, e.g. by
  using LLVM Loop Invariant Code Motion (-licm) pass. A simple reason
  is that there is no piece of code in Fig.~\ref{fig:multiwayjoin}
  (left) that represents $B \cap C$, which the compiler could attempt
  to move.  A deeper reason is that the optimization in
  Fig.~\ref{fig:multiwayjoin} is sound only if the arrays $A,B,C$ are
  sorted, which is an invariant that compilers usually cannot infer.

  
}
\end{example}

\name stores each temporary intersection locally in each thread.
There are no read-write or write-write conflicts between threads
during the parallel execution.  Each thread only operates on its
locally cached data, and there is no need for a lock-based
synchronization.  Therefore this optimization reduces duplicated work,
without creating additional bottlenecks.

\revB{ Next, we discuss the complexity of the optimized algorithm.  A
  \emph{fractional edge cover} of the full conjunctive query in
  Eq.~\eqref{eq:full:cq} is a tuple of non-negative numbers
  $\bm w = (w_1, \ldots, w_m)$ such that ``every variable $X$ is
  covered'', meaning $\sum_{j: X \in \text{Vars}(R_j)} w_j \geq 1$.
  It is known that, for any fractional edge cover $\bm w$, Generic
  Join runs in time $\tilde O(\prod_j |R_j|^{w_j})$.  Consider now an
  optimized algorithm $P$, where some intersections have been cached
  early, and let $Q_P$ the query obtained from $Q$ as follows: $Q_P$
  has the same atoms $R_j$ as $Q$, and each variable $X$ occurs only
  in those atoms $R_j$ that are used in the first intersection of the
  $X$ domains. We prove in the full version of the paper:\footnote{By
    a simple adaptation of the optimality proof of generic
    join~\cite{DBLP:journals/sigmod/NgoRR13}.}

\begin{theorem}
  For any fractional edge cover $\bm w$ of $Q_P$, algorithm $P$ runs in time
  $\tilde O(\prod_j |R_j|^{w_j})$.
\end{theorem}

To illustrate, consider the query $Q$ in
Example~\ref{ex:duplicate:intersections}.  If $R_2, R_5$ are the
smallest of the 6 relations, then standard Generic Join runs in time
$\tilde O(|R_2|\cdot |R_5|)$, because of the fractional edge cover
$\bm w = (0, 1, 0, 0, 1, 0)$.  The query $Q_p$ associated to the
program $P$ in Example~\ref{ex:duplicate:intersections:2} is
$Q_P = R_1(X) \wedge R_2(X,Z) \wedge R_3(X,U) \wedge R_4(Y) \wedge
R_5(Y,U) \wedge R_6(Z)$ ($Y$ only occurs in $R_4, R_5$ because of
$\text{tmp}_Y := R_4.Y\cap R_5.Y$).  Since $\bm w$ is also a
fractional edge cover of $Q_P$, algorithm $P$ runs in optimal time
$\tilde O(|R_2|\cdot |R_5|)$; in particular, $P$ is optimal when
$|R_1|=\cdots = |R_6|=N$.  However, if $R_1, R_6$ are strictly smaller
than all other relations, then $P$ is no longer optimal.

If theoretical optimality is required, it is possible to check for
each candidate rewriting $P$ if its complexity is the same as that of
standard generic join.  In \name we use a cost model instead, as we
describe next.}

\subsection{Cost Model}

\label{subsec:cost:model}

We describe here our model for estimating the cost of Generic Join on the
query $Q$ in~\eqref{eq:full:cq}, assuming a variable order
$\bm X_\sigma$ given by a candidate permutation $\sigma$, i.e.
$X_{\sigma(1)}, X_{\sigma(2)}, \ldots, X_{\sigma(n)}$; refer to the
notations in Table~\ref{tab:freq}.  

Consider some level $i=1,n$.  The outer iterations have bound their
variables to the prefix
$x_{\sigma(1:i-1)}\defeq
(x_{\sigma(1)},x_{\sigma(2)},\ldots,x_{\sigma(i-1)})$.  Let
$\SetS[X_{\sigma(i)}]=\set{R_{j_1}, R_{j_2}, \ldots}$ (or just $\SetS$
when the variable $X_{\sigma(i)}$ is clear from the context) be the
set of relation names that contain the variable $X_{\sigma(i)}$, and
$|\SetS|$ be its size. Then the $i$'th loop needs to compute
$R_{j_1}[x_{\sigma(1:i-1)}].X_{\sigma(i)} \cap
R_{j_2}[x_{\sigma(1:i-1)}].X_{\sigma(i)}\cap \cdots$.  We denote by
$\SetN[x_{\sigma(1:i-1)}\oplus X_{\sigma(i)}]$ the set of
cardinalities of the relations in $\SetS$, i.e.
$\set{|R_{j_1}[x_{\sigma(1:i-1)}]|, |R_{j_2}[x_{\sigma(1:i-1)}]|,
  \ldots}$, and by $\min \SetN$, $\max \SetN$ the smallest/largest
cardinality.  
For example, consider the triangle query in
Fig.~\ref{fig:algo-comp}, where the variable order is $X,Y,Z$.  In the
second loop ($\texttt{for } y \texttt{ in} \ldots$), we have
$\SetS = \SetS[x\oplus Y] = \set{R[x], S}$, $|\SetS|=2$, and
$\SetN[x\oplus Y]$ is $\set{|R[x]|, |S|}$. If $|R[x]| < |S|$, then $\min \SetN = |R[x]|$ and $\max \SetN = |S|$.

Next, we describe the cost of computing the intersection at level $i$,
which we denote by
$\Restrict{\Cost}{\Concat{\ctxpp{1:i-1}}{\VTXPP{i}}}$.  Assume that
the intersection is computed using galloping search: iterate over the
smallest relation $R_{j_{\min}}$ with the cardinality $\min \SetN[x_{\sigma(1:i-1)}\oplus X_{\sigma(i)}]$,  and probe using exponential search
in each of the remaining relations $R_j$.  If the values in
$R_{j_{\min}}$ are uniformly distributed in $R_j$, then the cost is
$\log\frac{|R_j|}{|R_{j_{\min}}|}$, which leads to:

\begin{equation*}
\Restrict{\Cost}{\Concat{\ctxpp{1:i-1}}{\VTXPP{i}}} = |\Restrict{\SetS}{\VTXPP{i}}|\cdot \min \Restrict{\SetN}{\Concat{\ctxpp{1:i-1}}{\VTXPP{i}}} \cdot \log_2\Big(1 + \frac{\max \Restrict{\SetN}{\Concat{\ctxpp{1:i-1}}{\VTXPP{i}}}}{\min \Restrict{\SetN}{\Concat{\ctxpp{1:i-1}}{\VTXPP{i}}}}\Big)
\end{equation*}

So far the cost is for a single binding $\bm x_{\sigma(1:i-1)}$ of the
prefix $\bm X_{\sigma(1:i-1)}$ of the variables.  To add up their
cost, we need a notation.  Consider our definition of the conjunctive
query in Eq.~\eqref{eq:full:cq}: the head variables are
$X_1, \ldots, X_n$.  We denote by $Q(\bm X_{\sigma(1:i-1)})$ the query
where the head variables are restricted to $\bm X_{\sigma(1:i-1)}$.
The bindings  $\bm x_{\sigma(1:i-1)}$ will iterate over all outputs of
this query, hence the total cost at the level $i$ is:

\begin{equation*}
  \Restrict{\Cost}{\VTXPP{1:i}} = \qquad \smashoperator[lr]{\sum_{\ctxpp{1:i-1} \in Q(\bm X_{\sigma(1:i-1)})}} \qquad \Restrict{\Cost}{\Concat{\ctxpp{1:i-1}}{\VTXPP{i}}} 
\end{equation*}

Ultimately, we define the total cost $\TotalCost[\VTXP]$ of our join algorithm by summing up the intersection costs for all variables in the variable order $\VTXP$. 
\begin{equation*}
  \TotalCost[\VTXP] = \TotalCost[\VTXPP{1:n}] = \smashoperator[lr]{\sum_{i \in [n]}} \Restrict{\Cost}{\VTXPP{1:i}} 
\end{equation*}

This cost cannot be computed exactly at optimization time, instead we
compute an upper bound.  For that purpose we use a pessimistic
cardinality
estimator~\cite{DBLP:conf/sigmod/CaiBS19,DBLP:journals/pacmmod/KhamisNOS24,10.1145/3651597}
to upper bound the output size of the
queries $Q(\bm X_{\sigma(1:i-1)})$, and use the following inequality,
which we prove in the full version of the paper:

\begin{lemma}\label{lem:cost-ub}
The following inequality holds, 
where the summation is over $\bm x_{\sigma(1:i-1)} \in Q(\bm
X_{\sigma(1:i-1)})$, and we drop the argument
$\Restrict{\cdot}{\Concat{\ctxpp{1:i-1}}{\VTXPP{i}}}$ from $\SetN$:
  \begin{equation}
    \Restrict{\Cost}{\VTXPP{1:i}} \leq |\SetS(X_{\sigma(i)}| \cdot (\sum \min \SetN) \cdot \log_2\Big(1 + \frac{\sum \max \SetN}{\sum \min \SetN}\Big)\label{eq:final:cost}
  \end{equation}
\end{lemma}

\begin{example}
  Referring again to the 2nd loop of the triangle query,
  \revB{$R[x].Y\cap S.Y$}, we have $\SetN = \set{|R[x]|, |S|}$.  We
  compute Eq.~\eqref{eq:final:cost} as follows.  \revB{First,
    $\sum \min \SetN$ is $\sum_x \min(|R[x].Y|,|S.Y|)$, which is the
    same as the output size of the query $Q(X,Y)=R(X,Y),S(Y),T(X)$
    (where $S(Y),T(X)$ represent $\Pi_Y(S)$ and $\Pi_X(T)$
    respectively). We upper bound it using a pessimistic
    cardinality estimator: \name uses~\cite{10.1145/3651597} for this
    purpose.  For the expression inside the logarithm 
    $\dfrac{\sum_x\max(|R[x].Y|,|S.Y|)}{\sum_x
      \min(|R[x].Y|,|S.Y|)} = \dfrac{\avg_x\max(|R[x].Y|,|S.Y|)}{\avg_x
      \min(|R[x].Y|,|S.Y|)}$.  Since this does not have a
    closed form, we estimate it as
    $\dfrac{\max(\avg_x(|R[x].Y|),\avg_x(|S.Y|))}{\min(\avg_x(|R[x].Y|),\avg_x(|S.Y|))}$.
    Next, we compute $\avg_x(|R[x].Y|) = |R|/|R.X|$, and similarly for
    the other terms. 
    The 1st and the 3rd loops of the triangle query are handled similarly.
    }

\end{example}

If an intersection at level $i$ is rewritten to be computed at an
earlier level $i_0<i$ (Sec.~\ref{subsec:rewrite}), then those
relations will be included in the set $\SetS(X_{\sigma(i_0)})$, while
the temporary relation will be added to $\SetS(X_{\sigma(i)})$.

\subsection{Plan Decision} \label{subsec:plan}

We describe now how the optimizer chooses the optimal variable order,
$\bm X_\sigma$ and shares $P_1 \times P_2 \times \cdots \times P_n = P$, 
where $P_i$ represents the share of the variable $X_{\sigma(i)}$.

The HyperCube partition leads implicitly to some replicated work,
which we capture by the following expressions:

\begin{equation}
  \Restrict{\TotalCost}{\VTXP, \PartSharePerm} = \smashoperator[lr]{\sum_{i \in [n]}} \Big((\prod_{j > i} P_{\sigma(j)}) \Restrict{\Cost}{\VTXPP{1:i}} \Big)\label{eq:total:cost}
\end{equation}

For example, in the triangle query, for a fixed $x$, every value
$y \in R[x].Y\cap S.Y$ is discovered redundantly by all threads with
partition identifiers the form $(i,j,*)$ where $i=h_X(x)$ and
$j=h_Y(y)$: this accounts for the product $\prod_{j>i}$ above.

A naive way to compute the optimal variable order $\sigma$ and shares
$P_i$ is to try all combinations and return the cheapest
cost~\eqref{eq:total:cost}.  There are $n!$ variable orders and
${10+n-1\choose n-1}$ total possible share allocations (since we use by
default $P=1024=2^{10}$ threads).  A brute force search only works for
small values of $n$.  Instead, we apply some pruning heuristics, as follows.

We prune partition shares that increase the cost
$\Restrict{\TotalCost}{\VTXP, \PartSharePerm} > 2 \cdot
\Restrict{\TotalCost}{\VTXP}$.  We will also prune small partitions.
Recall that if we throw randomly $B$ balls into $P$ bins and
$B = O(N)$, then the expected size of the largest bin is not
$O(B/P)=O(1)$, but it is $O(\log P)$, which means that the data is
non-uniformly distributed.  To expect uniform distribution, $O(B/P)$,
we need $B > 3 P \log P$ balls~\cite{DBLP:journals/sigmod/KoutrisS16}.
Therefore, we prune partitions containing some share $P_i$ where
$|\dom(X_i)| < 3 \cdot P_i \cdot \log P_i$

Finally, we assign an \emph{eveness} score $E$ to a set of shares,
which favors evenly distributed, with a bias towards have more shares
for last variables $X_{\sigma(n)}, X_{\sigma(n-1)}, \ldots$ than for
the first variables.  $E$ is defined as:

\begin{equation}
  E(\PartSharePerm) = \sum_{i \in [n]} P_{\sigma(i)} \cdot w(i)
\end{equation}
where $w(i) = \max\{1 - \frac{i}{100}, \frac{3}{4}\}$ is a smooth weight function on $i$ to distinguish the importance of the variable. Finally, we choose the partition share with the minimum evenness among all feasible partitions, to balance the task computations and avoid data skewness.