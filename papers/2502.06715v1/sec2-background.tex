\section{Background and Problem Statement} \label{sec:bg}

{
\begin{table}[t]
	\centering
	\caption{Basic Notations for Cost Model}
	\label{tab:freq}
	\begin{tabular}{cc}
          \toprule
          Notations & Definition \\
          \midrule
          $\ctx$ / $\VTX$ & tuple of constants / variables \\
          $\ctxp$ / $\VTXP$ & permuted tuple of constants /
                              variables \\
          \ \ \ $\ConstTupleProj{x}{i:j}$  /
          $\VarTupleProj{X}{i:j}$
                    & projected tuple of constants /
                      variables\tablefootnote{$\ctxp$ means
                      $(\VarTupleProj{x}{\sigma(1)},\VarTupleProj{x}{\sigma(2)},\ldots\VarTupleProj{x}{\sigma(n)})$
                      and $\VarTupleProj{x}{i:j}$
                      means $(\VarTupleProj{x}{i},\VarTupleProj{x}{i+1},\ldots\VarTupleProj{x}{j})$.} \\
          $\Concat{\ctx}{\VTX}$ & concatenation of constants or variables\\
          $\SetS$ & set of relations, e.g. $\{R_{j_1}, \cdots, R_{j_k}\}$\\
          $|\SetS|$ & the size of $\SetS$, e.g. $k$ above \\

          $\SetN$ &  cardinalities of relations in $\SetS$, e.g. $\set{|R_{j_1}|,\ldots,|R_{j_k}|}$ \\
		\bottomrule 
\end{tabular}
\end{table}
}
{
  \setlength{\abovedisplayskip}{0.25em}  
  \setlength{\belowdisplayskip}{0.25em} 
\noindent A \textbf{full Conjunctive Query} with variables $X_1,
\ldots, X_n$ is defined as:
\begin{equation}
  Q(X_1, \ldots, X_n) \leftarrow R_1(\bm{X}_1)\wedge R_2(\bm{X}_2)\wedge \dots\wedge R_m(\bm{X}_m)\label{eq:full:cq}
\end{equation}
Each term \( R(\bm{X}_i) \) is called an \emph{atom}, where
\( \bm{X}_i \subseteq \set{X_1, \ldots, X_n}\) is a tuple of
variables, and every variables $X_i$ occurs in some atom
$R_j(\bm X_j)$. }We use $m$ to denote the number of atoms, and $n$ for
the number of variables; we also use normal letters $X_i$ to denote
single variables, and boldface $\bm X_j$ to denote a tuple of
variables.

We will always drop the head variables, since they are clear from the
rest of the query, for example we abbreviate a 2-way join
$Q(X,Y,Z)=R(X,Y)\wedge S(Y,Z)$ as $Q = R(X,Y)\wedge S(Y,Z)$.
Predicates are pre-processed by pushing them down to the atoms, for
example we treat the query $Q=R(X,Y)\wedge S(Y,Z) \wedge Z>5$ as
$Q = R(X,Y)\wedge S_0(Y,Z)$ where $S_0=\sigma_{Z>5}(S)$.  We do not
discuss predicates in this paper.

\addvspace{\smallskipamount}
\noindent\textbf{Generic Join Algorithm} Traditional query engines compute the
query~\eqref{eq:full:cq} one join at a time.  The problem with this
approach is that the size of intermediate relations can be larger than
the final output.  For example, if each of the three relations
$R, S, T$ of the triangle query in Fig.~\ref{fig:algo-comp} has
size $N$, then the final output is $\leq N^{3/2}$, while any 2-way
join can have size $N^2$.  \emph{Generic Join}, GJ, which is a special
case of a \emph{Worst Case Optimal Join}
(WCOJ)~\cite{DBLP:journals/sigmod/NgoRR13}, computes the query in time
that is guaranteed to be no larger than the largest possible output to
$Q$.  GJ chooses some variables order, say $X_1, X_2, \ldots$, then
performs $n$ nested loops, one for each variable $X_i$. At each loop
it first computes the intersection of all columns $R_j.X_i$, the binds
the variable $X_i$ to each value $x_i$ in the intersection and
computes the residual query, where each $R_j$ containing $X_i$ is
restricted to $R_j[x_i]$:

\centerline{
  \begin{minipage}{0.7\linewidth}
  \begin{tabbing}
    \texttt{for} \=$x_1$ \texttt{in} $R_{j_1}.X_1 \cap R_{j_2}.X_1 \cap \ldots$ \\
    \>\texttt{for} \=$x_2$ \texttt{in} $\ldots$\\
    \> \>\texttt{for} \=$x_3$ \texttt{in} $\ldots$\\
    \> \> \>$\ldots$
  \end{tabbing}
\end{minipage}
}

Referring to the triangle query in Fig.~\ref{fig:algo-comp}, the
top loop binds $X$ to each value of the intersection $R.X\cap T.X$,
then restricts $R, T$ to $R[x], T[x]$ (their subsets where $X=x$) and
computes the residual query $R[x](Y) \wedge S(Y,Z) \wedge T[x](Z)$.
The runtime of GJ is proven to be no larger than the maximum output
size.  The theoretical guarantee holds for any variable order, but, in
practice, the choice of variable order can affect the runtime
significantly~\cite{DBLP:journals/pvldb/WangT0O23}.

An important aspect of GJ is that, in order to guarantee the
theoretical runtime, it must compute the intersection
$R_{j_1}.X_i \cap R_{j_2}.X_i\cap \cdots$ in time proportional to the
smallest of the sets.  This is achieved by representing each relation
as a hash trie with a depth equal to the number of its attributes,
where each level corresponds one attribute, see
Fig.~\ref{fig:hashtrie}.  To compute the intersection, GJ selects the
smallest hash table, iterates over the values stored there, and probes
each of them in all the other hash tables.  Since pre-computing all
hash tries for all relations are expensive, a common technique is to
use a \emph{lazy trie}, where the trie is constructed on demand, as
needed during the
join~\cite{DBLP:journals/pvldb/FreitagBSKN20,DBLP:journals/pacmmod/WangWS23}.
Thus, only relevant parts of the trie are built.

\begin{figure}[t]
	\includegraphics[width=.9\linewidth]{hashtrie_journal}
	
	\caption{(Adapted from~\cite[Fig.6]{umbratechreport})
          Conflicts in Traditional Parallel WCOJ Algorithm. The blue
          thread tries to access the sub-trie $h_2(1)$ in $S.Y$, while
          the red thread is constructing it. }
  \label{fig:hashtrie}
\end{figure}

\addvspace{\smallskipamount}
\noindent\textbf{Parallel GJ} Logicblox\cite{DBLP:conf/sigmod/ArefCGKOPVW15}
and Umbra~\cite{DBLP:journals/pvldb/FreitagBSKN20} parallelilze GJ by
partitioning the domain of the first variable $X_1$ into subsets of
equal size, then computing the query on each subset in a separate
thread, see Listing~\ref{lst:tradpara} in Fig.~\ref{fig:algo-comp}.  The domains of
$X_2, X_3, \ldots$ are not partitioned, and they will be scanned
entirely by every thread, which leads to two important drawbacks.
First, even if the domain of $X_1$ is uniformly distributed, the total
work may be skewed: this can be seen for example in
Fig.~\ref{fig:skew-eg}.  Second, as shown in Fig.~\ref{fig:hashtrie}, since multiple threads access the
same values of $X_2, X_3, \ldots$ they may attempt to construct
concurrently the same fragment of the lazy trie, requiring the use of
locks for synchronization (to avoid read/write conflicts), which
introduces significant overhead. Those conflicting overheads are revealed by the scalability experiments of Umbra in Sec.~\ref{subsec:exp:scale}.

\begin{figure}[t]
	\includegraphics[width=.65\linewidth,trim={0 0 0 0},clip]{hypercube}
	\Description[HyperCube]{HyperCube Partition}
	\caption{\revA{The HyperCube Partition}}
  \label{fig:hc}
\end{figure}

\addvspace{\smallskipamount}
\noindent\textbf{HyperCube Algorithm:} The HyperCube algorithm~\cite{DBLP:conf/edbt/AfratiU10,DBLP:journals/jacm/BeameKS17,DBLP:conf/icdt/KoutrisBS16}
computes the query~\eqref{eq:full:cq} on a shared-nothing architecture
with $P$ servers by partitioning \emph{all} domains of \emph{all}
attributes. It organizes the $P$ servers into a hypercube with $n$
dimensions, by writing $P$ as a product
$P = P_1 \times P_2 \times P_n$, such that each server is uniquely
identified by $n$ coordinates, $(s_1, \ldots, s_n)$, with
$s_i \in \set{0,1,\ldots,P_i-1}$.  In the first step, HyperCube hash-partitions the
domain of each variable $X_i$ into $P_i$ buckets then, in a single
global communication step, it sends every tuple $R_j(\ldots)$ to all
servers whose coordinates agree with the hash values of the tuple. In
the second step, each of the $P$ servers computes the query on the
fragment of relations it has received.  The quantity $P_i$ is called
the \emph{share} of the variable $X_i$.

For example, consider the query $Q=R(X,Y),S(Y,Z),T(Z,X)$ and $P=4096$
severs.  Hypercube writes $P=16 \times 16 \times 16$ and assigns to each
server a unique combination of 3 coordinates
$(s_1, s_2, s_3) \in
\set{0,\ldots,15}\times\set{0,\ldots,15}\times\set{0,\ldots,15}$, \revA{see
  Fig.~\ref{fig:hc}.} 
In the first step, it sends each tuple $R(x,y)$ to all of servers with the
coordinates\footnote{HyperCube uses independent hash functions
  $h_X, h_Y, h_Z$ for each coordinate.}  $(h_X(x),h_Y(y), *)$; each
tuple $(y,z)$ in the relation $S$ is also sent to all servers $(*,h_Y(y),h_Z(z))$, and
similarly for $T$. In the second step, each server computes the query
on the relation fragments that it has received.  Notice that each
relation is replicated 16 times, for example each tuple $R(x,y)$ is
sent to 16 servers, $(s_1,s_2,0), \ldots, (s_1,s_2,15)$.

A direct adaptation of HyperCube from the shared-nothing to a
multicore, shared-memory architecture will have poor performance.
The theoretical analysis in~\cite{DBLP:journals/sigmod/KoutrisS16}
addresses \emph{only} the communication cost, i.e. the total number of
tuples received by any server, and optimizes the shares
$P_1, P_2, \ldots, P_n$ such as to minimize the communication cost.
No algorithm is prescribed for the second step, and its computational
cost is not analyzed.  As we show in our paper, for multicores the
choice of the variable order and the associated shares must be
considered together.

\addvspace{\smallskipamount}
\noindent\textbf{Sparse Matrix Format}:

The high-performance, and the sparse tensor compiler communities have
developed as a suite of very efficient main memory representations for
sparse tensors.  Examples include Compressed Sparse Row (CSR),
Compressed Sparse Column (CSC),
Compressed Sparse Block (CSB)
the Coordinate List (COO), Blocked
Compressed Sparse Row (BCSR), Double Compressed Sparse Row (DCSR),
see~\cite{DBLP:conf/spaa/BulucFFGL09},
\cite[Fig.5]{DBLP:journals/pacmpl/KjolstadKCLA17} and
\cite[Fig.2]{DBLP:journals/pacmpl/ChouKA18}.  Our
representation~\indexlayout adapts ideas from sparse tensor
representations for parallel evaluation of Generic Join.

\addvspace{\baselineskip}

\noindent\textbf{Problem Statement} The goal of this paper is to
design efficient and scalable parallel variants of Generic Join; we
will use the terms Generic Join and Worst Case Optimal Join
interchangeably.  As we saw, the traditional approaches to
parallelization has several shortcomings. It is sensitive to
\textbf{data skew}, causing some processors to be overloaded while
others remain underutilized, thus degrading parallelization
efficiency.  The lazy trie construction \textbf{read-write conflicts}
when executed in multi-threaded environments, which requires the use
of locks for each index access.  A third shortcoming (described in
Sec.~\ref{sec:opt}) is that some duplicated work that is inherent in
Generic Join impacts the runtime, and this is more apparent in
parallel implementations.

Our paper proposes, \name, designed to address these problems by
ensuring better workload balance and avoiding conflicts during
parallel execution.  Additionally, \name is optimized for modern
hardware architectures, by minimizing pointer chasing to improve
memory access patterns and leveraging vector processing capabilities
to accelerate operations.

