\section{Experiments} \label{sec:exp}

We implemented \name as a standalone C++ program. It reads the query
and the data path from a JSON file, and loads data from a binary file,
which was previously converted from CSV format. After running the
query, the program can either output all result tuples, or output only
the count.  We compared \name against some state-of-the-art systems
that support cyclic join queries, and conducted some scalability and
ablation studies. We ask four research questions:

\begin{enumerate}
	\item How does the absolute performance of \name compare to that of related systems on both real and synthetic datasets?
	\item How does \name scale up with an progressively increased number of cores?
	\item How important is it to optimize the partition shares and the variable order?
	\item How effective is the query rewriting method and the \indexlayout data structure?
\end{enumerate}

\subsection{Setup}

{
\begin{table}[t]
	\caption{Dataset Characteristics}
	\label{tab:data}
	\begin{tabular}{ccll}
		\toprule
		Name & \# Node & \# Edge & Feature \\
		\midrule
		WGPB~\cite{wgpb-dataset, DBLP:conf/semweb/HoganRRS19} & 54.0M & 81.4M & sparse, skew   \\
		Orkut~\cite{DBLP:conf/imc/MisloveMGDB07} & $3.07M$ & $117M$ & partial dense, uniform  \\
		GPlus~\cite{DBLP:conf/nips/McAuleyL12} & 107K & 13.6M & dense, skew \\
		USPatent~\cite{DBLP:conf/kdd/LeskovecKF05} & 3.77M & 16.5M & sparse, uniform \\
		Skitter~\cite{DBLP:conf/kdd/LeskovecKF05} & 1.69M & 11.1M & sparse, partial skew \\
		Topcats~\cite{DBLP:conf/kdd/YinBLG17} & 1.79M & 28.5M & partial dense, skew \\
		\bottomrule
	\end{tabular}
\end{table}
}

\noindent\textbf{Datasets:} We provide the information for our graph datasets in Table~\ref{tab:data}. These datasets include a variety of types, sparsity, and skewness, which cover the common characteristics of real-world datasets.

We transform those datasets into CSV format and either load them directly or use the provided loader to fetch them to the main memory. 

In addition, we also use two synthetic datasets for tensor kernels, with different sparsity. We generate the synthetic datasets with the same number of nodes and hyperedges where the number of attributes is set to 3. The first dataset (named ST-Dense) is generated uniformly densely distributed. The dataset consists of 7 relations, each with three attributes. The first four relations follow the form\footnote{$[N]$ represents all positive integers $\leq N$, $\times$ denotes the Cartesian product.} $[512] \times [512] \times [512]$. The last three relations have attribute domains $(\{0, 1\} | \{0, 2\} | \{1, 2\}) \times [4096] \times [4096]$. The second dataset (named ST-Sparse) is similar but just distributed sparsely. The first four relation randonly selected $512^3$ tuples from $[5120] \times [5120] \times [5120]$, the sparsity is $0.001$. The last three relations are selected $4096^2$ tuples from $[40960] \times [40960]$ while keeping the first attribute, thus the sparsity is $0.01$.

\addvspace{\smallskipamount}
\noindent\textbf{Baselines:} We compare our method with five state-of-the-art baselines: Umbra~\cite{DBLP:journals/pvldb/FreitagBSKN20}, Diamond~\cite{DBLP:journals/pvldb/BirlerKN24}, DuckDb~\cite{DBLP:conf/cidr/RaasveldtM20}, Soufflé~\cite{DBLP:conf/lopstr/ArchHZSS22}, and Kùzu~\cite{DBLP:conf/cidr/JinFCLS23}. 

Umbra (2022-11-03) is a high-performance database management system designed for modern hardware architectures. It nicely supports the worst-case optimal join (multiway join) algorithms in parallel through morsel-driven execution.
\revB{Diamond is a variant of Umbra, which is optimized to solve the
  diamond\footnote{Intermediate results are larger than the
  inputs and the output.} problem. It splits join operators into Lookup and
  Expand suboperators, and allows them to be freely reordered. It
  also uses a novel ternary operator  Expand3 that implements the
  triangle query.}
Kùzu (v0.6.0) is a graph database engine that excels at handling
complex queries involving joins across graph-structured data. Its most
recent version it supports  worst-case optimal join through ``join hints''~\cite{kuzujoinhint}.
Soufflé (v2.4.1) is a logic-based database engine that compiles queries into optimized C++ code. It supports efficient execution of Datalog queries, particularly for complex join operations, but does not implement worst-case optimal join.
DuckDB (v1.1.1) is an in-process analytical database management system optimized for fast OLAP workloads. It is heavily optimized for complex join queries, but does not support worst-case optimal join.

Umbra is provided in binary format and was obtained directly from the author, while the other baselines are open-source and were downloaded directly from their official GitHub repositories.

For Umbra, we enabled paged storage optimization, and forced it to use worst-case optimal join, which improved its performance. For Soufflé, we enabled brie representation for dense input relation, which also improved its performance. For Kùzu, we provided join order hints in order to facilitate the worst-case optimal join plan. All these non-default settings were chosen to improve the systems' overall performance. 

{
\begin{table*}[t]
	\centering
	\caption{Table of Queries}
	\label{tab:query}
	\begin{tabular}{ll}  
	\toprule
	\textbf{Name} & \textbf{Queries} \\ 
	\midrule
	Q1 (Triangle) & \texttt{Q(X,Y,Z) := R(X,Y), S(Y,Z), T(X,Z).}
	 \\ 
	\hline
	Q2 (4-Loop) & \texttt{Q(X,Y,Z,U) := R1(X,Y), R2(X,Z), R3(Y,U), R4(Z,U).} \\ 
	\hline
	Q4 (4-Diamond) & \texttt{Q(X,Y,Z,U) := R1(X,Y), R2(X,Z), R3(Y,U), R4(Z,U), R5(Y,Z).} \\ 
	\hline
	Q6 (4-Clique) & \texttt{Q(X,Y,Z,U) := R1(X,Y), R2(X,Z), R3(Y,U), R4(Z,U), R5(Y,Z), R6(X,U).} \\ 
	\hline
	Q8 (2-Triangle) & \texttt{Q(X,Y,Z,U,V) := R1(X,Y), R2(X,Z), R3(Y,Z), R4(Z,U), R5(Z,V), R6(U,V).}\\ 
	\hline
	\hline
	LW(Loomis-Whitney) \!\!\!& \texttt{Q(X,Y,Z,U) := R1(X,Y,Z), R2(X,Y,U), R3(X,Z,U), R4(Y,Z,U).}\\
	\hline
	CT (Clover-Triangle) & \texttt{Q(U,X,Y,Z) := R5(U,X,Y), R6(U,X,Z), R7(U,Y,Z).} \\
	\bottomrule
	\end{tabular}
\end{table*}
}

\addvspace{\smallskipamount}
\noindent\textbf{Queries:} We evaluate the performance of our method and baselines on the queries listed\footnote{We evalaute on more queries, but, to save space, we report only the seven queries in the table; this also explains the weird numbering.} in Table~\ref{tab:query}.  The first five query patterns are derived from the paper~\cite{DBLP:journals/pvldb/MhedhbiS19}, and are used for graph datasets, while the latter are typical sparse tensor kernels~\cite{DBLP:journals/pacmpl/KjolstadKCLA17}. For the triangle query, Q1, we used two variations, Q1d and Q1u, representing triangles on a directed and undirected graph respectively.  We run all other queries only on the directed graphs.

\addvspace{\smallskipamount}
\noindent\textbf{Metrics:} We evaluate the performance of \name and other systems by measuring their overall execution time. Specifically, we track the wall-clock time taken by each system to complete each query from start to finish. This timing excludes the duration spent on data loading, result output, and data statistics collection, but it does account for the time used to create indexes.

We conduct each measurement three times and report the average runtime. 
For Soufflé, we also exclude the compilation time and report only the runtime of the generated programs after compilation. 
For Umbra and Kùzu, we run the query one extra time initially, to avoid the cold start issue, such as additional query compilation or data loading time.  
We set a timeout of 10,000 seconds for each individual experiment repetition.  
Any timeout is marked by X in our graphs.

\addvspace{\smallskipamount}
\noindent\textbf{Environment:} We conduct our experiments on Intel Xeon Phi Server, which has 60 cores and 120 hyperthreads on 4 sockets, as well as 1TB main memory. Except Umbra, \name and the open-source baselines are all compiled using GCC 13.3.0 with Release mode on Rocky Linux 9.4. 

\subsection{Performance Comparison}\label{subsec:exp:perf}

\begin{figure*}[t]
	\includegraphics[width=.95\linewidth, trim={0 0 0 0}, clip]{baseline_journal}
  \caption{\revB{Performance Comparison on Graph Datasets. An X means ``timeout after 10,000 seconds''}}
  \label{fig:perf}
\end{figure*}

Our first set of experiments compares the runtime of \name and the
other baselines on graph datasets, using all the 60 available
cores. The results,  in Fig.~\ref{fig:perf},  show that \name
consistently outperforms the other systems across all queries and
datasets, with significant runtime reductions observed in most cases,
\revB{with the exception of Diamond, which it outperforms on most but
  not all queries (more on this below).}

On dense datasets, such as GPlus and Orkut, \name benefits from
rewriting and \indexlayout while it efficiently supports lookup and
scan over the sorted dense data. Umbra incurs an overhead during the
construction of the hash trie since most of the sub-tries are touched
and need to be built. \revC{Kùzu, despite using hints for generic join
  order, is slower than Umbra.} DuckDB, although it does not support
the worst-case optimal join and  generally lags behind the others, it performed quite well on the 4-loop query Q2, because Q2 has a small tree-width~\cite{DBLP:conf/pods/KhamisNR16}, where the advantage of the worst-case optimal join over traditional plans diminishes.  Soufflé is the slowest, where almost all queries are timeout, mainly due to the lack of optimizations for both binary and generic joins.

For sparse datasets, such as WGPB, \name continues to outperform the
other systems, but the gap between \name and Umbra
narrows. \revC{Based on the description
  of~\cite{DBLP:journals/pvldb/FreitagBSKN20} we believe that Umbra’s
  lazy trie construction is more effective for sparse data than for
  dense data, because there are fewer contentions between threads and
  fewer sub-trie structures to be constructed on demand.} On sparse
data, \name benefits from the partitioning strategy, which helps
reduce the computational skew. In contrast, both Umbra and Kùzu
partition only on the first attribute and are more likely to suffer
from computation skew. \revB{Diamond behaves slightly better on Q1u
  and Q2 due to its customized operator Expand3 and special
  optimization for queries with small tree-width. However, these
  optimizations are not universally beneficial: Diamond performed
  worse than Umbra on some queries.} DuckDB and Soufflé,
which are not specifically designed for the worst-case optimal join,
perform worse on sparse datasets. Interestingly, Soufflé differs from
DuckDB in that it performs well on queries with large tree-width, such
as the Q6 (4-clique) and Q8 (4-diamond) queries. This is mainly
because Soufflé is optimized for Datalog queries, which are inherently
more cyclic and thus more efficient for these types of queries.

\revB{Umbra and Diamond outperformed \name on several queries on
  USPatent and Skitter: the reason is that the intermediate results on
  these datasets are small, which makes Umbra's lazy trie construction
  and Diamond's hash join very effective.  In contrast, \name computes
  all indices eagerly, and its preprocessing time spent in
  constructing these indices (around 0.5s to 1s) affects more
  significantly the runtime of these relatively cheap queries.

  
  
  
  
  
  
  
  
}

{
\begin{table}[t]
	\caption{\revB{Optimizations on Q8 (Orkut)}}
	\label{tab:opt_comp}
	\begin{tabular}{lr}
		\toprule
		Approach & Runtime (s) \\
		\midrule
		\name & 5,680.988 \\
		\name + Tree Decomposition~\cite{DBLP:conf/sigmod/AbergerTOR16} & 1,473.199 \\
		\name + Variable Elimination~\cite{DBLP:conf/pods/KhamisNR16} & 26.956 \\
		\name + Eager Agg~\cite{DBLP:conf/pods/KhamisNR16} & 7.749 \\
		Umbra & Timeout\\
		Diamond (= Umbra + TD) & 436.257 \\
		Diamond + Lookup & 51.513 \\
		Diamond + Loopup + Eager Agg & 21.624 \\
		\bottomrule
	\end{tabular}
\end{table}
}

\revB{For Query Q8, Diamond timed out on WGPB, but outperformed \name
  on all other datasets, due to optimizations that benefit Q8
  specifically.  While \name currently does not support similar
  optimizations, we wanted to get a sense of how much they could
  improve \name's performance.  We hand-optimized Q8 using existing
  optimization techniques, similar to those used in Diamond, and
  present the results in Table~\ref{tab:opt_comp}, using the Orkut
  dataset. Notably, these trends remain consistent across other
  datasets. The results suggest that \name could outperform Diamond
  on queries with good tree decompositions (like Q8), by incorporating
  these orthogonal optimizations.

}

\begin{figure}[t]
	\centering
      \includegraphics[width=0.65\linewidth, trim={0 0 0 0}, clip]{baseline_tensor}
			\caption{\revB{Performance on Tensor Dataset}}
      \label{fig:perf_tensor}
\end{figure}

Next, we compared \name with the baseline systems on tensor datasets using the queries LW (Loomis-Whitney) and CT (clover-triangle), which have  more complex hypergraphs, typical for tensor kernels. The results, shown in Fig.~\ref{fig:perf_tensor}, demonstrate that \name consistently outperforms most baselines across all queries and datasets, with significant runtime reductions in most cases. We do not include Kùzu, because it doesn't support hypergraph queries. Thus, \name continues to outperform other systems on complex cyclic joins on $n$-ary ($n > 2$) relations.

\subsection{Scalability}\label{subsec:exp:scale}

\begin{figure}[t]
	\centering
	\includegraphics[width=.85\textwidth, trim={0 0 0 0}, clip]{scale_journal}
	\caption{\revC{Speedup of query execution on  $P=1,\ldots,120$
    virtual cores (there are 60 hyperthreaded physical cores).}}
	\label{fig:scale}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
\end{figure}

\revC{We evaluated \name's scalability, by measuring the runtime of
  the system as a function of the number of virtual cores $P$.  We report the
  findings only for \name, Umbra and Diamond, since other systems are
  generally too slow to show their scalability. }

We tested the Q1u (triangle) and Q6 (4-clique) queries on the WGPB and Orkut dataset, as well as the LW (Loomis-Whitney) and CT (clover-triangle) queries on the ST-Dense Dataset.

The results are shown in Fig.~\ref{fig:scale}. We find that \name
consistently demonstrated almost linear scalability up to the maximum
number of physical cores, which was 60 on our system.  Increasing the number of
virtual cores up to 120 led to no additional performance improvements at most cases, showing
that the cores were saturated and the use of hyperthreads was not
effective; \revB{we also confirmed that the CPUs were at almost 100\
utilization at 60 virtual cores.}
\revC{In a few cases (Orkut), \name's performance continued to improve a little beyond 60 cores: this is because of a higher contribution to the runtime of exponential search in middle loops, which is less memory efficient. Manually replacing exponential search with linear search restored full CPU utilization at 60 cores.}

Umbra also shows an almost linear speedup, but slightly below that of \name. \revC{For instance, consider Q6 on WGPB, \name is $1.12\times$ faster than Umbra on 1
core, but $2.52\times$ faster on 60 cores, demonstrating that our techniques improve scalability as we increase the number of cores. This can be attributed to several factors.} On sparse datasets, suboptimal partitioning of skewed data can lead to long-tail latencies, negatively impacting performance. On dense datasets, read-write conflicts caused by lazy trie building further hinder speedup when executing queries in parallel, as these conflicts introduce additional overhead that reduces the efficiency of parallel execution.

\revB{Diamond demonstrates similar scalability to Umbra up to 60 cores in most cases. However, Diamond maintains good scalability beyond 60 cores, indicating that hyperthreading benefits its performance. This is mainly due to the use of secondary hash tables over entire relations in Expand3, which are memory-latency bound due to random hash probes over large tables. Hyperthreading, with its separate register stacks and shared execution engine, helps mitigate this bottleneck. Nonetheless, while Diamond achieves speedups beyond 60 cores, its design does not fully address memory access latency, resulting in overall slower performance compared to our system. By leveraging the \indexlayout index and Adaptive Searching, \name achieves better cache locality and effectively utilizes the computational power of cores.}

Also, \name does not achieve optimal speedup on sparse and skew datasets, even though it has better scalability. This is because the partitioning strategy of \name is not only targeted to relieve the skewness problem but also to avoid huge duplicated computation, as mentioned in Sec.~\ref{sec:intro}, which may hurt the scalability. 

Besides, the preprocessing time is not fully parallelized, and the overhead of preprocessing is not negligible in sparse cases.

			

		

\revC{These experiments also point to the importance of
  query rewriting for scalability.  Still consider Q6 on WGPB. If we remove
  the query rewriting optimization, then \name is $0.91\times$ faster
  than Umbra (i.e. slower) on 1 core, and $1.15\times$ faster on 60
  cores.  Thus without  rewriting \name gains very little over
  Umbra as $P$ increases (due to its mitigation of skew), but the gain
  becomes more pronounced when we enable the query rewriting optimization.

}

\subsection{Variable Ordering}

\begin{figure}[t]
	\centering
	\begin{minipage}[b]{0.49\textwidth}
		\centering
			  \includegraphics[width=\linewidth, trim={0 0 0 0}, clip]{order}
			  \caption{Effect of Variable Ordering}
			  \label{fig:order}
	\end{minipage}
	~
	\begin{minipage}[b]{0.49\textwidth}
		  \includegraphics[width=\linewidth, trim={0 0 0 0}, clip]{part}
		  \caption{Effects of Partition Shares}
		  \label{fig:partition}
	\end{minipage}
\end{figure}

Recall that the theoretical analysis of the worst-case optimal join holds for any choice of variable order, however, in practice, the choice of variable order may affect the runtime significantly.  Next, we evaluated how much the variable order can affect the runtime of \name.  

Fig.~\ref{fig:order} shows the runtime for executing the Q6 (4-clique) query on WGPB, for all $24 = 4!$ possible variable orderings.  

The results show that optimizing the variable order can have a significant impact on the query performance. This is because the order in which joins are executed directly influences the size of intermediate results, which can lead to substantial differences in both memory usage and computational effort.

Efficient ordering can minimize the number and length of intersections, reducing the overall complexity of the query execution. In contrast, poor ordering can result in intersecting large sets, which increases memory overhead and slows down execution.

To demonstrate the feasibility of our optimizer, we use a red bar to represent the ordering chosen by our optimizer in Fig.~\ref{fig:order}. In this particular Q6 (4-clique) query, our optimizer chooses the best variable order.  This did not happen for all queries, but it is important to note that the optimizer does not need to select the absolute best ordering, as long as is not significantly slower than the optimal.  

\subsection{Partition}

Next, we conducted a similar analysis on the choice of the partition shares.  We ran the Q1u (triangle) query using $P=1024$ threads, and considered all possible ways to factorize it into $P = P_X \times P_Y \times P_Z$.  The results are shown in Fig.~\ref{fig:partition}: the $X$ and $Y$ axes show $P_X, P_Y$ (the value of $P_Z$ can be inferred, and is also shown by the color of each dot), and the size of each dot represents the runtime (smaller is better).  The optimal shares are $32 \times 32 \times 1$, and these are, indeed, the shares chosen by our optimizer.  The traditional parallelization method, which allocates all shares to one variable, corresponds to the factorization $1024 \times 1 \times 1$, and is the right-most orange dot, which is visibly worse than the optimal choice of shares.  

Recall that the HyperCube algorithm tries to minimize the communication cost; when all three relations of the query have the same size, then the optimal shares are equal, which, in our case corresponds to $8 \times 8 \times 16$ or some permutation thereof.  As one can see from the graph, these choices of shares are far from optimal: our optimizer adjusts the partition shares based on not only the data distribution, but also on query characteristics that we care about. As noted in Sec.~\ref{subsec:plan}, the optimizer will rule out these partition shares by detecting the heavy duplicated intersection computations across different threads.

On the other hand, if we keep $P_Z =1$ then it doesn't matter too much how many shares we allocate to $X$ or $Y$ (the line of red dots, which also contains the optimal configuration).  This is because the data itself is relatively evenly distributed, and the difference in allocating shares between X and Y has little impact on performance.

Overall, the graph shows a large variation in the total runtime, proving the need for an optimized allocation of shares.

\subsection{Query Rewriting and Indexes}

\begin{figure}[t]
	\centering
		  \includegraphics[width=.5\linewidth, trim={0 0 0 0}, clip]{other}
		  \caption{\revC{Effects of Rewriting and \indexlayout}}
		  \label{fig:other}
\end{figure}

In this section, we present additional experimental results that highlight key aspects of our optimization techniques.

We first evaluate the effectiveness of our query rewriting optimization by measuring the runtime with and without rewriting enabled. The results, summarized in Fig.~\ref{fig:other}, show that enabling query rewriting consistently improves performance across all queries. This demonstrates that our rewriting strategies optimize execution plans, leading to more efficient query processing. \revC{For the the queries LW on the ST-Dense dataset, the rewriting is not effective, due to no duplicated intersections in its plan.}

Next, we assess the impact of our \indexlayout index on query performance. We compare the runtime with the \indexlayout index enabled and disabled, and the results indicate a significant performance improvement when the index is utilized. Also, we report the size of the \indexlayout index, which ranges from $1$GB to $5$GB for each table and is comparable with datasets. Thus, the \indexlayout does not introduce significant storage overhead. This ensures that the index remains practical for optimizing joins without imposing a large memory footprint.

In addition, we measure the total time spent in the preprocessing stage, which includes the creation of the \indexlayout index. The preprocessing time ranges from $0.2$s to $5$s, depending on the number of tables and the size of datasets. For long-running queries, the preprocessing time is generally minimal compared to the join execution time. For short-running queries, even sometimes the preprocessing time is larger than the join time, but the total runtime is still less than or comparable to other baselines. This indicates that the overhead introduced by preprocessing is acceptable, making our approach efficient and suitable for real-time query processing. 

