\documentclass[11pt]{scrartcl}
\addtokomafont{disposition}{\rmfamily}

%load any additional packages
\usepackage{amsmath, amssymb, amsthm}
\usepackage{quiver}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,intersections}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{authblk} % affiliations etc
\usepackage{hyperref} % makes references hyperlinks
\usepackage{cleveref} % knows if it's theorem, lemma, etc
\usepackage[nottoc]{tocbibind} % references appear in toc
\usepackage{comment}

\usepackage{todonotes}
\newcommand{\pao}[1]{\todo[color=blue!12,inline,caption={}]{\textbf{P:} #1}} % comments etc
\newcommand{\mika}[1]{\todo[color=red!12,inline,caption={}]{\textbf{M:} #1}} % comments etc

% Tikz packages
%\usetikzlibrary{braids}
\usetikzlibrary{calc}
\usetikzlibrary{trees}
\input{diagrams.tex}
\input{markov.tikzstyles}
\usepackage{graphicx}
\usepackage{tikzit}

% Adjunction symbol
\usepackage{pict2e}

\usepackage{pifont} % tick and cross
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\makeatletter
\newcommand{\adjunction}[2]{%
	% #2 <arrows> #3
	#1%
	\mathrel{\vcenter{%
			\offinterlineskip\m@th
			\ialign{%
				\hfil$##$\hfil\cr
				\longrightharpoonup\cr
				\noalign{\kern-.3ex}
				\smallbot\cr
				\longleftharpoondown\cr
			}%
	}}%
	#2
}
\newcommand{\longrightharpoonup}{\relbar\joinrel\rightharpoonup}
\newcommand{\longleftharpoondown}{\leftharpoondown\joinrel\relbar}
\newcommand{\smallbot}{%
	\begingroup\setlength\unitlength{.15em}%
	\begin{picture}(1,1)
		\roundcap
		\polyline(0,0)(1,0)
		\polyline(0.5,0)(0.5,1)
	\end{picture}%
	\endgroup
}
\makeatother

% Add theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{problem}[theorem]{Problem}
\theoremstyle{definition}				% the following ones will not be italicized
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}


% CATEGORY THEORY
\newcommand{\cat}[1]{{\mathbf{#1}}} % font for categories
\newcommand{\samp}{\texttt{samp}} % sampling map
\renewcommand{\det}{\text{det}} % deterministic

\title{Categorical algebra of\\conditional probability}

\author{Mika Bohinen and Paolo Perrone}
\affil{University of Oxford}
\date{2025}


\begin{document}
\maketitle


\begin{abstract}
	In the field of categorical probability, one uses concepts and techniques from category theory, such as monads and monoidal categories, to study the structures of probability and statistics.
	In this paper, we connect some ideas from categorical algebra, namely weakly cartesian functors and natural transformations, to the idea of conditioning in probability theory, using Markov categories and probability monads. 
	
	First of all, we show that under some conditions, the monad associated to a Markov category with conditionals has a weakly cartesian functor and weakly cartesian multiplication (a condition known as Beck-Chevalley, or BC).
	In particular, we show that this is the case for the Giry monad on standard Borel spaces.
	
	We then connect this theory to existing results on statistical experiments. We show that for deterministic statistical experiments, the so-called standard measure construction (which can be seen as a generalization of the ``hypernormalizations'' introduced by Jacobs) satisfies a universal property, allowing an equivalent definition which does not rely on the existence of conditionals. 
\end{abstract}


\tableofcontents

\section{Introduction}

Categorical probability is an emerging field that applies category theory to
probability and statistics. This approach seeks to:
\begin{itemize}
    \item Reformulate probabilistic concepts using abstract categorical methods,
        particularly through diagrammatic reasoning, rather than relying on
        specific analytical properties like cardinality or separability;
        
    \item Simplify complex probabilistic proofs by working at a higher level
        of abstraction, making existing results more transparent and enabling
        the discovery of new theorems that were previously intractable;
        
    \item Bridge probability theory with other mathematical disciplines,
        particularly theoretical computer science, through the shared language
        of category theory.
\end{itemize}

Markov categories~\cite{Cho_2019,Fritz-2020-Asyntheticapproachto} provide the
primary mathematical framework for this approach (see
\cite[Remark~2.2]{fritz2022free} for historical context). This framework has
enabled categorical reformulations and generalizations of fundamental results in
probability theory, including de Finetti's theorem~\cite{fritz2021definetti},
the Kolmogorov and Hewitt-Savage zero-one laws~\cite{fritzrischel2019zeroone},
and the ergodic decomposition theorem~\cite{ensarguet2023ergodic}. In
statistics, Markov categories have formalized concepts like sufficient
statistics~\cite{Fritz-2020-Asyntheticapproachto} and, through their connection
with probability monads~\cite{jacobs2018probabilitymonads}, led to new insights
into Blackwell's theorem on statistical experiments~\cite{Fritzetal-2023-Representablemarkovc}.

A central concept in Markov categories is \emph{conditioning} (see
\Cref{conditional}). It seeks to capture the traditional operations of
conditioning---inlcuding disintegrations, conditional expectations, and
Bayesian inverses---using string diagrams, but without having to worry about
measure-theoretic subtleties.

This work develops a categorical-algebraic perspective on conditionals in
Markov categories. We establish that, given suitable compatibility conditions
between a Markov category and a probability monad (see \Cref{main_results}),
conditionals arise from two key properties of the monad:
\begin{itemize}
    \item The underlying functor is weakly cartesian;
    \item The multiplication of the monad is weakly cartesian.
\end{itemize}
These properties, collectively known as \emph{Beck-Chevalley} or
\emph{BC}~\cite{clementino2014monads}, were previously established for the
distribution monad on sets~\cite{Constantinetal-2020-Partialevaluationsan}. Our
results extend this characterization to the Giry monad on standard Borel spaces
(\Cref{giryBC}).

To understand intuitively how conditionals relate to weak pullback
preservation, consider a weak pullback diagram in a category $\cat{C}$ (such
as $\cat{Set}$ or $\cat{Meas}$) and its image under a functor
$P:\cat{C}\to\cat{C}$:
\[
\begin{tikzcd}
	A & B \\
	C & D
	\arrow["f", from=1-1, to=1-2]
	\arrow["g"', from=1-1, to=2-1]
	\arrow["m", from=1-2, to=2-2]
	\arrow["n"', from=2-1, to=2-2]
\end{tikzcd}
\qquad\qquad
\begin{tikzcd}
	PA & PB \\
	PC & PD
	\arrow["Pf", from=1-1, to=1-2]
	\arrow["Pg"', from=1-1, to=2-1]
	\arrow["Pm", from=1-2, to=2-2]
	\arrow["Pn"', from=2-1, to=2-2]
\end{tikzcd}
\]
For the right diagram to be a weak pullback, we must establish that for any
elements $p\in PB$ and $q\in PC$ with $Pm(p)=Pn(q)$, there exists some
(not necessarily unique) element $r\in PA$ satisfying $Pf(r)=p$ and
$Pg(r)=q$. When $P$ is a probability monad, $p$ and $q$ represent probability
distributions, and the existence of conditionals provides a canonical choice
for $r\in PA$: the conditional product of $p$ and $q$ given $D$. This
construction yields a measure under which the observations $f$ and $g$ are
conditionally independent given their common coarse-graining $m\circ f=n\circ
g$. Under suitable conditions, this measure can be shown to have support in
$A$. For the complete technical development, see \Cref{main_results}.

These results lead to new insights in the theory of statistical experiments. In
\cite{Constantinetal-2020-Partialevaluationsan}, as well as in
\cite{FritzPerrone-2018-Monadspartialevaluat} and \cite{constantin2023weak}, it
was shown that monads satisfying the BC conditions have a particularly
interesting \emph{bar construction}, a simplicial set (or more generally
simplicial object) formed by the algebra of a monad $(A,a)$ as follows:
\[
\begin{tikzcd}[sep=large]
	\cdots \ar{r} & TTTA \ar[shift left=0.8cm]{r}{\mu} \ar[shift left=0.4cm]{r}{T\mu} \ar["TTa"{description}]{r} & TTA \ar[shift left=0.4cm]{r}{\mu} \ar["Ta"{description}]{r}  \ar[shift left=0.4cm]{l}{T\eta} \ar[shift left=0.8cm]{l}{TT\eta} & TA \ar[shift left=0.4cm]{l}{T\eta}
\end{tikzcd}
\]
The (0-1)-truncation of this simplicial set is called the \emph{partial
evaluation relation}, and can be seen as the relation connecting a formal
expression to its result or partial results (see the references above and our
\Cref{pev}). For BC monads, this relation is transitive.

The partial evaluation relation for the case of probability monads was shown to
be equivalent to what in probability and statistics is known as
\emph{second-order stochastic dominance}
\cite{FritzPerrone-2018-Monadspartialevaluat,Perrone18}, which measures how
``spread'' probability measures over the real line (or a vector
space) are. In \cite{Fritzetal-2023-Representablemarkovc}, partial evaluations
were connected to Markov categories. There, a synthetic and more general
version of Blackwell's theorem on statistical experiments was proven, showing
equivalence between stochastic dominance of certain measures (the
\emph{standard measures}, see \Cref{stat_exp}) and an order on statistical
experiments measuring their ``informativeness''. 

In the final section, we establish a universal property for the standard
measures of deterministic statistical experiments (which generalize Jacobs'
``hypernormalizations'', see \Cref{hypernorm}). Namely, they can be seen as the
\emph{coarsest decomposition of a measure which is still finer or equal than
the ``partition'' induced by a statistical experiment}. (This idea of
``partition induced by a function'' may remind the reader of \emph{descent},
but we leave a more thorough investigation of this analogy to future work.)

The structures and techniques of categorical algebra reach far beyond weakly
cartesian and BC monads, and we hope that this work is just the first one of a
fruitful thread of research, paving the way for an even deeper structural
understanding of probability, possibly connecting it to fibrations and descent
theory.

\paragraph{Outline.}
In \Cref{background} we establish the main concepts that are used in the rest
of the work. In particular, in \Cref{markov} we give an overview of Markov
categories and their relationship with probability monads. In \Cref{pev} we
explain the main ideas behind partial evaluations, especially in the context of
probability monads. We then turn to statistical experiments in \Cref{stat_exp},
where we also give our definition of ``hypernormalization'' as a special case
of a standard measure (\Cref{def:hypernormalization}).

Our main results are stated and proven in \Cref{main_results}. In
\Cref{wcartmu} we prove that for an a.s.~compatibly representable Markov
category with conditionals, the multiplication of the monad is weakly cartesian
(\Cref{thm:pullbacks}). This in particular applies to the Giry monad on
standard Borel spaces (\Cref{pullbacks_giry}). In \Cref{wcartp} we prove that
for a representable Markov category with conditionals and satisfying the
so-called ``equalizer principle'' (\Cref{def:equalizer_principle}), the functor
underlying the monad is weakly cartesian (\Cref{thm:main}). Once again, this in
particular applies to the Giry monad, which is then Beck-Chevalley
(\Cref{giryBC}). In \Cref{hypernorm} we show that hypernormalization satisfy a
universal property, namely that they are the coarsest measure (in the
stochastic dominance order) which is compatible with the experiment
(\Cref{thm:uni-std}).

Finally, in \Cref{appendix} we give some technical results used in our proofs.

\paragraph{Acknowledgements.}
Research for Mika Bohinen was part of his MSc thesis at the University of
Oxford, supported in part by Aker Scholarship.

Research for Paolo Perrone was funded at the time of writing by Sam Staton's
ERC Consolidator Grant ``BLaSt -- a Better Language for Statistics''.


\section{Markov categories and partial evaluations}\label{background}

\subsection{Markov categories and probability monads}\label{markov}

Let's briefly recall the main definitions of Markov categories and their relationship with probability monads.
For more details, see the original papers \cite{Cho_2019,Fritz-2020-Asyntheticapproachto,Fritzetal-2023-Representablemarkovc}.

\begin{definition}
	A \textbf{copy-discard} (\textbf{CD}) \textbf{category} is a symmetric monoidal category $(\cat{C},\otimes,I)$ where every object is equipped with a distinguished commutative comonoid structure, compatible with the tensor product. We denote the comonoid structure maps as follows:
	\begin{equation*}
		\texttt{copy} \;=
		\tikzfig{copy}
		\qquad\mbox{and}\qquad
		\texttt{del} \;=
		\tikzfig{discard}
	\end{equation*}

	A \textbf{Markov category} is a CD category where for every $f:X \to Y$ in $\cat{C}$ the following equality holds,
	\begin{equation}
		\tikzfig{norm}
		\label{eq:main-def}
	\end{equation}
	or equivalently, where the monoidal unit $I$ is terminal.
\end{definition}

Canonical examples of Markov categories are \emph{categories of Markov kernels}, hence the name.
These are also the most relevant examples for the purposes of the present paper.

\begin{example}
	The category \( \cat{Stoch} \) is specified via the following data.
	\begin{itemize}
		\item Objects are measurable spaces, i.e., pairs \( (X,
		      \mathcal{A}) \) where \( X \in \cat{Set} \) and \( \mathcal{A} \)
		      is a sigma-algebra on \( X \);
		\item Morphisms \( (X, \mathcal{A}) \to  (Y, \mathcal{B}) \) are Markov
		      kernels of entries \( k(B|x) \), for \( x \in  X \) and \( B \in
		      \mathcal{B} \). That is to say,, $k(B|-)$ is a measurable function $X\to\mathbb{R}$ for all $B\in\mathcal{B}$, and \( k(-|x) \) is a probability	measure on \( (Y,\mathcal{B}) \) for all \( x \in  X \).
		\item The identity \( (X, \mathcal{A}) \to  (X, \mathcal{A}) \) is given by the Dirac delta, i.e.,
		      \begin{equation}
			      \delta(A|x) = \begin{cases}
				      1, & \text{ if }x \in  A \\
				      0, & \text{ otherwise.}
			      \end{cases}
			      \label{eq:diraciden}
		      \end{equation}
		\item The composition of two kernels \( k: (X, \mathcal{A}) \to (Y,
		      \mathcal{B}) \) and \( h:(Y, \mathcal{B}) \to  (Z, \mathcal{C})
		      \) is given by taking the Lebesgue integral, i.e.,
		      \begin{equation}
			      (h\circ k)(C|x) = \int_Y h(C|y)k(dy|x)
			      \label{eq:complebeg}
		      \end{equation}
		      for all \( x \in  X \) and \( C \in  \mathcal{C} \).
		\item The monoidal structure is given by the usual product of measurable spaces.
		\item The ``copy'' map $\texttt{copy}:X\to X\otimes X$ is defined by
		      \[
			      \texttt{copy}(A\times B| x) \;=\; \begin{cases}
				      1 & \text{ if }x\in A\cap B ; \\
				      0 & \text{ otherwise.}
			      \end{cases}
		      \]
		\item The ``delete'' map $\texttt{del}:X\to I$ is the unique kernel to the one-point space.
	\end{itemize}

	The Markov category \( \cat{BorelStoch} \) is defined to be the full
	subcategory of \( \cat{Stoch} \) where the objects are standard
	Borel spaces.
\end{example}


\begin{definition}[\cite{Fritzetal-2023-Representablemarkovc}]
	Let \( \cat{C} \) be a Markov category. A morphism \( f: A \to X \) in \( \cat{C} \) is said to be \textbf{deterministic} if the following equality holds
	\begin{equation}
		\tikzfig{det}
		\label{eq:deterministic}
	\end{equation}
	The subcategory of \( \cat{C} \) that consists of only deterministic morphisms is denoted by \( \cat{C}_{\text{det}} \).
\end{definition}

\begin{example}
	The deterministic morphisms of $\cat{BorelStoch}$ are exactly those Markov kernels $k_f:X\to Y$ which are induced by a measurable function as follows:
	\[
		k_f(B|x) \;=\; \begin{cases}
			1 & \text{ if } f(x)\in B ; \\
			0 & \text{ otherwise. }
		\end{cases}
	\]
\end{example}

One of the most important pieces of structures for this paper is the idea of \emph{conditioning}.

\begin{definition}\label{conditional}
	Let \( f: A \to  X \otimes Y \) be a morphism in a Markov category \(
	\cat{C} \). A map \( f|_X: X \otimes A \to Y \) is called a
	\textbf{conditional} of \( f \) with respect to \( X \) if the following equation holds.
	\begin{equation}
		\tikzfig{conditional}
		\label{eq:conditional}
	\end{equation}

	A Markov category is said to \textbf{have all conditionals} (or more briefly, just \textbf{have conditionals}) if every morphism admits a conditional with respect to any of its outputs.
\end{definition}

\begin{example}
	The category $\cat{BorelStoch}$ has all conditionals, and they correspond to \textbf{regular conditional probability distributions} \cite[Example~11.3]{Fritz-2020-Asyntheticapproachto}.
	For example, given a probability measure $\psi: I \to X \otimes Y$, a
	conditional distribution is a kernel $\psi|_X: X \to Y$ which has to to satisfy
	\begin{equation*}
		\psi(S \times T|) = \int_{s \in S}\psi|_X (T|x) \psi(dx) .
	\end{equation*}
	Such a kernel always exists if $X$ and $Y$ are standard Borel.
\end{example}

\begin{definition}
	\label{def:almost}
	Let $p : A \to X $ be a morphism and $f,g: X \to Y $ two parallel
	morphisms. We say that $f$ and $g$ are $p$-almost surely equal, denoted
	$f=_{p\text{-a.s.}} g$, if we have
	\begin{equation}
		\label{eq:aes}
		\tikzfig{aes}
	\end{equation}
\end{definition}

\begin{example}
	\label{ex:almostsure}
	In $\cat{BorelStoch}$, Definition~\ref{def:almost} gives the standard
	notion of almost surely equality. More specifically, given Markov kernels
	$f,g: (X, \Sigma_{A}) \to (Y, \Sigma_{B})$ and a probability measure $\nu:
		I \to X$, the relation $f =_{\nu\text{-a.s.}} g$ Equation~\ref{eq:aes}
	becomes the condition
	\begin{equation*}
		\int_S f(T|x)\nu(dx) = \int_S g(T|x)\nu(dx),
	\end{equation*}
	for all $S \in \Sigma_X$ and $T \in \Sigma_Y$. This is the same as saying that
	$f(T|-)$ and $g(T|-)$ are $\nu$-almost everywhere equal for all $T$.
\end{example}

An important example of Markov categories are those arising as Kleisli categories of a monad (sometimes called a \emph{probability monad}~\cite{jacobs2018probabilitymonads}).
The following phenomenon is well known.

\begin{proposition}
	\label{prop:importantproperty}
	Let \( (P, \mu, \delta) \) be a symmetric monoidal monad on some symmetric
	monoidal category $(D, \otimes, I)$. Then \( \cat{D}_P \) is a symmetric
	monoidal category, with:
	\begin{itemize}
		\item the same monoidal product as the one in \( \cat{D} \);

		\item the tensor product of morphisms represented by \( f:A \to PX \)
		      and \( g:B \to PY \) being represented by the composite
		      \begin{equation}
			      A\otimes B \xrightarrow{f\otimes g} PX\otimes PY \xrightarrow{\nabla} P(X\otimes Y).
			      \label{eq:repr}
		      \end{equation}
	\end{itemize}
	Moreover, the inclusion \( \cat{D} \to  \cat{D}_P \) is strict symmetric monoidal.
\end{proposition}

The final statement about the inclusion implies that if $ X \in \cat{D} $ has a
distinguished comonoid structure, then so does $ X \in \cat{D}_P $.

\begin{definition}
	A monad $(P,\mu, \delta)$ on $\cat{D}$ is said to be
	\textbf{affine} if $PI \cong I$.
\end{definition}

Thus, if $P$ is affine and $I$ is terminal, then $PI \in \cat{D}_P$ is also
terminal and we get the following result.
\begin{corollary}
	Let \( (P,\mu ,\delta ) \) be a symmetric monoidal affine monad on a Markov
	category \( \cat{D} \). Then the Kleisli category \( \cat{D}_P \) is again a
	Markov category in a canonical way.
	\label{cor:affine}
\end{corollary}

\begin{example}
	$\cat{BorelStoch}$ can be seen as the Kleisli category of the Giry monad on standard Borel spaces.
\end{example}

Markov categories of this kind have a particular structure, very convenient for
the purposes of probability theory. This abstracts the idea that a Markov
kernel $X\to Y$ is equivalently specified by a measurable function $X\to PY$,
where $PY$ is the space of probability measures over $Y$, equipped with the
canonical sigma-algebra given by the Giry monad~\cite{giry}.

\begin{definition}
	Let $\cat{C} $ be a Markov category and $X\in \cat{C} $ an object. A
	\textbf{distribution object} for $X$ is an object $PX$ together with a
	morphism $\samp_X: PX \to X $ so that the induced map
	\begin{equation*}
		\samp_X \circ -: \cat{C}_{\text{det}}(A, PX) \to C(A, X)
	\end{equation*}
	is a bijection for all $A \in \cat{C} $. We denote the inverse of this map by
	\begin{equation*}
		(-)^\#: \cat{C}(A, X) \to \cat{C}_{\text{det}}(A, PX) ,
	\end{equation*}
	and we set $\delta_X=(1_X)^\#:X\to PX$.
\end{definition}

\begin{definition}
	\label{def:distfunctor}
	Let $\cat{C}$ be a Markov category. We say that $\cat{C}$ is
	\textbf{representable} if every object has a distribution object.
\end{definition}

Distribution objects, if they exist for all $X$, assemble together to give a
right adjoint $P:\cat{C} \to \cat{C}_{\text{det}} $ to the inclusion functor
$\cat{C}_{\text{det}} \hookrightarrow \cat{C} $. The unit of the adjunction is
given by the maps $\delta:X\to PX$, and the counit by $\samp:PX\to X$. The
resulting monad on $\cat{C}_{\text{det}}$ is given by $(P,P\samp,\delta)$.

\begin{example}
	The Giry monad on $\cat{BorelStoch}$ is in this form. In particular,
	\begin{itemize}
		\item The unit $\delta:X\to PX$ assigns to each point $x\in X$ the corresponding Dirac measure $\delta_x$;
		\item The counit of the adjunction $\samp:PX\to X$ is the kernel
		      \[
			      \samp(A|p) \;=\; p(A)
		      \]
		      for all $p\in PX$ and all measurable subsets $A\subseteq X$.
	\end{itemize}
\end{example}

\begin{definition}
	Let $\cat{C}$ be a Markov category. We say that $\cat{C}$ is
	\textbf{a.s.-compatibly representable} if it is representable and for any
	morphism $p: \Theta \to A$, the natural bijection
	\[
		\cat{C}_{\text{det}}(A,X) \;\cong\; \cat{C}(A,X)
	\]
	respects almost sure equality. That is to say, for
	all $f,g: A \to X$, we have
	\begin{equation}
		\label{eq:almostsurely}
		f^\# =_{p\text{-a.s}} g^\# \iff f =_{p\text{-a.s.}} g.
	\end{equation}
\end{definition}

In later proofs we shall make use of another equivalent characterization of a.s.-compatibly representable Markov categories.
\begin{definition}
	Let $\cat{C}$ be a representable Markov category. It is said to
	satisfy the \textbf{sampling cancellation property} if, for any three
	morphisms $f,g:X\otimes A \to Y$ and $ p:A \to X$, the following
	implication holds
	\begin{equation}
		\label{eq:samp-canc}
		\tikzfig{samp-canc}
	\end{equation}
\end{definition}
\begin{remark}
	Remembering that $f = \samp\, \circ f^\# $ for all morphisms in
	$\cat{C}$, the condition above is named so because it amounts to canceling
	the $\samp$ from $f$ and $g$.
\end{remark}

\begin{proposition}[{\cite[Proposition~3.24]{Fritzetal-2023-Representablemarkovc}}]
	Let $\cat{C}$ be a representable Markov category. Then $\cat{C}$ is
	a.s.-compatibly representable if and only it satisfies the sampling cancellation property.
\end{proposition}


\subsection{Partial evaluations}\label{pev}

We now turn our attention to partial evaluations. The main idea, quite simple, is that for example, a
formal expression like $1 + 2 + 3$ can be totally evaluated to $6$, but it can also
be partially evaluated to $3 + 3$ or $1 + 5$.

It is well known that partial evaluations for probability monads correspond exactly to the second-order stochastic dominance~\cite{FritzPerrone-2018-Monadspartialevaluat}, and their relationship with Markov categories was explored in \cite{Fritzetal-2023-Representablemarkovc}.
This paper extends that relationship, but before we do that, let's recall some of the main known ideas. For more details, see ~\cite{FritzPerrone-2018-Monadspartialevaluat,Constantinetal-2020-Partialevaluationsan}.

\begin{definition}
	Let $\cat{C}$ be a category and $X \in \cat{C}$. An $S$-shaped
	\textbf{generalized element} of $X$ is a morphism $p:S \to X$ for some $S \in \cat{C}$.
	By abuse of notation we will also write $p \in X$ when $p$ is a generalized element.
\end{definition}

Putting $\cat{C} = \cat{Set}$ and $S = \{*\}$ recovers the usual notion of elements in a set.
Indeed, we are particularly interested in the case where $\cat{C}$ is a Markov category and $p$ is a state.

\begin{definition}
	Let $(T, \mu, \eta)$ be a monad on some category $\cat{C}$, and
	$A\in\cat{C}$. An $S$-shaped \textbf{generalized formal expression} on $A$
	is an $S$-shaped generalized element $p \in TA$ for some $S\in \cat{C}$.
\end{definition}

\begin{definition}
	Let $p,q \in TA$ be $S$-shaped generalized formal expressions on a $T$-algebra
	$(A,e)$. A \textbf{partial evaluation} from $p$ into $q$ is an $S$-shaped generalized
	element $k \in TTA$ such that the following diagram commutes
	% https://q.uiver.app/#q=WzAsNCxbMSwwLCJTIl0sWzAsMSwiVEEiXSxbMSwxLCJUVEEiXSxbMiwxLCJUQSJdLFswLDMsInEiXSxbMCwxLCJwIiwyXSxbMiwxLCJcXG11Il0sWzAsMiwiayJdLFsyLDMsIlRlIiwyXV0=
	\[
		\begin{tikzcd} & S \\ TA & TTA & TA. \arrow["p"', from=1-2, to=2-1]
			\arrow["k", from=1-2, to=2-2] \arrow["q", from=1-2, to=2-3]
			\arrow["\mu", from=2-2, to=2-1] \arrow["Te"', from=2-2, to=2-3]
		\end{tikzcd}
	\]
\end{definition}

\begin{definition}
	The \textbf{partial evaluation relation} on $TA$ is defined as follows: given $p,q\in TA$, we say that $p\to q$ or $p\le q$ if and only if there exists a partial evaluation from $p$ to $q$.
\end{definition}

This relation is always reflexive.
Here is a sufficient condition for it to be transitive.

Recall that a commutative diagram
% https://q.uiver.app/#q=WzAsNCxbMCwwLCJBIl0sWzEsMCwiQiJdLFswLDEsIkMiXSxbMSwxLCJEIl0sWzAsMSwiZiJdLFsxLDMsIm0iXSxbMCwyLCJnIiwyXSxbMiwzLCJuIiwyXV0=
\[\begin{tikzcd}
		A & B \\
		C & D
		\arrow["f", from=1-1, to=1-2]
		\arrow["g"', from=1-1, to=2-1]
		\arrow["m", from=1-2, to=2-2]
		\arrow["n"', from=2-1, to=2-2]
	\end{tikzcd}\]
is a \textbf{weak pullback} if, given $p: S \to C$ and $q:S \to B$ such that $n \circ p = m \circ q$ then there
exists $r: S \to A$ such that
% https://q.uiver.app/#q=WzAsNSxbMSwxLCJBIl0sWzIsMSwiQiJdLFsxLDIsIkMiXSxbMiwyLCJEIl0sWzAsMCwiUyJdLFswLDEsImYiXSxbMSwzLCJtIl0sWzAsMiwiZyIsMl0sWzIsMywibiIsMl0sWzQsMSwicSIsMCx7ImN1cnZlIjotM31dLFs0LDIsInAiLDIseyJjdXJ2ZSI6M31dLFs0LDAsInIiLDEseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJkYXNoZWQifX19XV0=
\[\begin{tikzcd}
		S \\
		& A & B \\
		& C & D
		\arrow["r"{description}, dashed, from=1-1, to=2-2]
		\arrow["q", curve={height=-18pt}, from=1-1, to=2-3]
		\arrow["p"', curve={height=18pt}, from=1-1, to=3-2]
		\arrow["f", from=2-2, to=2-3]
		\arrow["g"', from=2-2, to=3-2]
		\arrow["m", from=2-3, to=3-3]
		\arrow["n"', from=3-2, to=3-3]
	\end{tikzcd}\]
is commutative.

(This is almost the same as a pullback except that we have dropped the uniqueness condition.)


\begin{definition}
	Let $(T, \mu, \eta)$ be a monad on some category $\cat{C}$.
	\begin{itemize}
		\item We say that the functor $T$ is \textbf{weakly Cartesian} if and only if it preserves weak pullbacks.
		\item We say that $\mu$
		      is \textbf{weakly Cartesian} if the diagram
		      % https://q.uiver.app/#q=WzAsNCxbMCwwLCJUVFgiXSxbMSwwLCJUVFkiXSxbMCwxLCJUWCJdLFsxLDEsIlRZIl0sWzAsMSwiVFRmIl0sWzIsMywiVGYiLDJdLFswLDIsIlxcbXVfWCIsMl0sWzEsMywiXFxtdV9ZIl1d
		      \[\begin{tikzcd}
				      TTX & TTY \\
				      TX & TY
				      \arrow["TTf", from=1-1, to=1-2]
				      \arrow["{\mu_X}"', from=1-1, to=2-1]
				      \arrow["{\mu_Y}", from=1-2, to=2-2]
				      \arrow["Tf"', from=2-1, to=2-2]
			      \end{tikzcd}\]
		      is a weak pullback for all $X,Y \in \cat{C}$ and $f:X \to Y$.
		\item We say that the monad $(T, \mu, \eta)$ is \textbf{Beck-Chevalley} (BC for short) if $\mu$ is weakly Cartesian and $T$ preserves weak pullbacks.
	\end{itemize}
\end{definition}


\begin{proposition}[{\cite[Proposition 4.1]{FritzPerrone-2018-Monadspartialevaluat}}]
	\label{prop:partialrel}
	Let $(T, \mu, \eta)$ be a monad on a category $\cat{C}$, $S \in
	\cat{C}$, and $A \in \cat{C}^T$ a $T$-algebra with $e: TA \to A$ the algebra
	map. The partial evaluation relation on $\cat{C}(S, TA)$ is always
	reflexive, and if the multiplication $\mu$ is weakly cartesian, the
	relation is also transitive.
\end{proposition}
\begin{proof}
	To see reflexivity, given $p:S\to TA$, take $T\eta\circ p:S\to TTA$. We
	have that 
	\[
	\mu\circ T\eta\circ p \;=\; p \;=\; Te\circ T\eta\circ p 
	\]
	using the monad and the algebra unit condition, and that shows that
	$T\eta\circ p$ is a partial evaluation from $p$ to itself.
	
	To see transitivity, suppose that $\mu$ is weakly cartesian, consider
	consider ``composable'' partial evaluations $r$ and $s$. That is, let
	$p,q,t:S\to TA$, and $r,s:S\to TTA$ be such that $\mu\circ r=p$,
	$Te\circ s=t$, and $Te\circ r= \mu\circ s=q$. We then have the solid
	arrows in the following commutative diagram:
	\[
	\begin{tikzcd}[sep=large]
		&& TTA \ar{r}{\mu} \ar{dr}[very near end,swap,inner sep=0.5mm]{Te} & TA \\
		S \ar[dashed]{r}{n} \ar[bend left=15]{urr}{r} \ar[bend right=15]{drr}[swap]{s} & TTTA \ar{ur}{\mu} \ar{r}{T\mu} \ar{dr}[swap,inner sep=0.5mm]{TTe} & TTA \ar{ur}[swap,near end]{\mu} \ar{dr}[very near end,inner sep=0.5mm]{Te} & TA \\
		&& TTA \ar{r}[swap]{Te} \ar{ur}[near end,inner sep=0.5mm]{\mu} & TA
	\end{tikzcd}
	\] 
	Now notice that, by hypothesis, the diamond diagram involving $S$
	commutes ($Te\circ r= \mu\circ s$). Since the diamond involving $TTTA$
	is weakly cartesian (it is a naturality square for $\mu$), there exists
	a (possibly non-unique) arrow $n:S\to TTTA$ making the entire diagram
	commute. Forming now the partial evaluation $T\mu\circ n:S\to TTA$, we
	have that $\mu\circ T\mu\circ n = \mu\circ\mu\circ n=\mu\circ r=p$, and
	$Te\circ T\mu\circ n=Te\circ TTe\circ n = Te\circ s = t$. 
\end{proof}

\begin{remark}
	While in the case above the partial evaluation relation is a preorder,
	which can be seen as a truncation of a category, partial evaluations
	and their composition hardly ever form a category. The situation is
	more interesting when $T$ preserves weak pullbacks (and so we have a
	Beck-Chevalley monad), in that case, while we still do not have a
	category (or a quasi-category) in general, we have an interesting
	higher compositional structure. See
	\cite{Constantinetal-2020-Partialevaluationsan,constantin2023weak} for
	the details.
\end{remark}

From the point of view of the Kleisli category, these weak cartesian conditions
look as follows.

\begin{proposition}\label{prop:BC_Kleisli}
	Let $(T, \mu, \eta)$ be a monad on a category $\cat{C}$. A commutative
	square in the Kleisli category $\cat{C}_T$ is a weak pullback if and
	only if its image under the right-adjoint $R:\cat{C}_T\to\cat{C}$ is.
\end{proposition}
\begin{proof}
	Consider the bijection given by the Kleisli adjunction,
	\[
		\begin{tikzcd}[row sep=0]
			\cat{C}_T(X,A) = \cat{C}_T(LX,A) \ar{r}{\cong} & \cat{C}(X,RA) = \cat{C}(X,TA) \\
			f \ar[mapsto]{r} & f^\#
		\end{tikzcd}
	\]
	where $L$ and $R$ denote the left- and right-adjoints. (Note that on
	objects, $LX=X$ and $RA=A$.) By naturality of the bijection above in
	$A$ (against morphisms of $\cat{C}_T$), we have that for all $p:X\to
	A$, $f:A\to B$ and $q:X\to B$ of $\cat{C}_T$, the triangle of
	$\cat{C}_T$ on the left commutes if and only if the triangle of
	$\cat{C}$ on the right does.
	\begin{equation}\label{eq:triangles}
		\begin{tikzcd}[row sep=small]
			& A \ar{dd}{f} \\
			X \ar{ur}{p} \ar{dr}[swap]{q} \\
			& B
		\end{tikzcd}
		\qquad\longleftrightarrow\qquad
		\begin{tikzcd}[row sep=small]
			& TA \ar{dd}{Rf} \\
			X \ar{ur}{p^\#} \ar{dr}[swap]{q^\#} \\
			& TB
		\end{tikzcd}
	\end{equation}
	Consider now a commutative square of $\cat{C}_T$ as the one on the left
	below.
	\[
		\begin{tikzcd}
			S \\
			& A & B \\
			& C & D
			\arrow["r"{description}, dashed, from=1-1, to=2-2]
			\arrow["q", curve={height=-18pt}, from=1-1, to=2-3]
			\arrow["p"', curve={height=18pt}, from=1-1, to=3-2]
			\arrow["f", from=2-2, to=2-3]
			\arrow["g"', from=2-2, to=3-2]
			\arrow["m", from=2-3, to=3-3]
			\arrow["n"', from=3-2, to=3-3]
		\end{tikzcd}
		\qquad\longleftrightarrow\qquad
		\begin{tikzcd}
			S \\
			& TA & TB \\
			& TC & TD
			\arrow["r^\#"{description}, dashed, from=1-1, to=2-2]
			\arrow["q^\#", curve={height=-18pt}, from=1-1, to=2-3]
			\arrow["p^\#"', curve={height=18pt}, from=1-1, to=3-2]
			\arrow["Rf", from=2-2, to=2-3]
			\arrow["Rg"', from=2-2, to=3-2]
			\arrow["Rm", from=2-3, to=3-3]
			\arrow["Rn"', from=3-2, to=3-3]
		\end{tikzcd}
	\]
	We have to prove that the square on the left is a weak pullback in
	$\cat{C}_T$ if and only if the one on the right is a weak pullback in
	$\cat{C}$.

	Let $S$ be an object of $\cat{C}$ (equivalently, of $\cat{C}_T$), and
	let $p:S\to C$ and $q:S\to B$ be morphisms of $\cat{C}_T$ such that
	$m\circ q=n\circ p$ (equivalently, by \eqref{eq:triangles}, let
	$p^\#:S\to TC$ and $q^\#:S\to TB$ be morphisms of $\cat{C}$ such that
	$Rm\circ q^\#=Rn\circ p^\#$). Again by \eqref{eq:triangles}, there
	exists $r:S\to A$ such that $f\circ r=q$ and $g\circ r= p$ if and only
	if there exists $r^\#:S\to TA$ such that $Rf\circ r^\#=q^\#$ and
	$Rg\circ r^\# = p^\#$.
\end{proof}

\begin{corollary}\label{cor:BC_Kleisli}
	Let $(T, \mu, \eta)$ be a monad on a category $\cat{C}$. Then
	\begin{enumerate}
		\item $T$ is weakly cartesian if and only if the left-adjoint $\cat{C}\to\cat{C}_T$ preserves weak pullbacks;
		\item $\mu$ is weakly cartesian if and only if the naturality diagram of the counit $\varepsilon$ in $\cat{C}_T$
		      % https://q.uiver.app/#q=WzAsNCxbMCwwLCJUVFgiXSxbMSwwLCJUVFkiXSxbMCwxLCJUWCJdLFsxLDEsIlRZIl0sWzAsMSwiVFRmIl0sWzIsMywiVGYiLDJdLFswLDIsIlxcbXVfWCIsMl0sWzEsMywiXFxtdV9ZIl1d
		      \[\begin{tikzcd}
				      TX & TY \\
				      X & Y
				      \arrow["Tf", from=1-1, to=1-2]
				      \arrow["{\varepsilon_X}"', from=1-1, to=2-1]
				      \arrow["{\varepsilon_Y}", from=1-2, to=2-2]
				      \arrow["f"', from=2-1, to=2-2]
			      \end{tikzcd}\]
		      is a weak pullback for all $f:X \to Y$ of $\cat{C}$, recalling that $\mu=R\varepsilon$. (In Markov categories, $\varepsilon$ is the map $\samp$.)
	\end{enumerate}
\end{corollary}

Let's now look at partial evaluations from the point of view of Markov categories. (See \cite{Fritzetal-2023-Representablemarkovc} for all the details.)

Consider a $P$-algebra $e:PA \to A$ in $\cat{C}_{\text{det}}$. Using
that
\begin{equation*}
	\cat{C}_{\text{det}}(\Theta, PA) \;\cong\; \cat{C}(\Theta, A)
\end{equation*}
we see that the partial evaluation relation correspond to a certain relation on
$\cat{C}(\Theta, A)$. More specifically, if $p,q:\Theta \to A$ in $\cat{C}$ then
$p \leq q $ in the relation on $\cat{C}(\Theta, A)$ means that there exists
$k:\Theta \to PA$ such that the diagram
\begin{equation*}
	% https://q.uiver.app/#q=WzAsNCxbMSwwLCJcXFRoZXRhIl0sWzAsMSwiQSJdLFsxLDEsIlBBIl0sWzIsMSwiQSJdLFswLDEsInAiLDJdLFswLDIsImsiXSxbMiwxLCJlIl0sWzIsMywiXFx0ZXh0dHR7c2FtcH0iLDJdLFswLDMsInEiXV0=
	\begin{tikzcd}
		& \Theta \\
		A & PA & A
		\arrow["p"', from=1-2, to=2-1]
		\arrow["k", from=1-2, to=2-2]
		\arrow["q", from=1-2, to=2-3]
		\arrow["e"', from=2-2, to=2-3]
		\arrow["{\samp_A}", from=2-2, to=2-1]
	\end{tikzcd}
\end{equation*}
is commutative in $\cat{C}$. Here is the precise result.

\begin{proposition}
	\label{prop:monotone}
	The isomorphism
	\begin{equation*}
		\cat{C}_{\text{det}}(\Theta, PA) \;\cong\; \cat{C}(\Theta, A)
	\end{equation*}
	is monotone in both directions where the order on $\cat{C}_{\text{det}}(\Theta, PA) $ is given by the
	partial evaluation order and the order on $\cat{C}(\Theta, A)$ is the one
	described above.
\end{proposition}
\begin{proof}
	Consider $p,q:\Theta\to A$ in $\cat{C}$ (equivalently, consider $p^\#,q^\#:\Theta\to PA$ in $\cat{C}_{\text{det}}$).
	Similarly to the proof of the proposition above, there exists $k^\#:\Theta\to PPA$ such that $\mu\circ k^\# = p^\#$ and $Pe\circ k^\#= q^\#$ if and only if there exists $k:\Theta\to PA$ such that $\samp\circ k = p$ and $e\circ k= q$.
\end{proof}

\begin{definition}
	The relation on $\cat{C}(\Theta, X)$ described above is called the \textbf{second-order dominance relation}.
\end{definition}

Setting $\Theta = I$ in $\cat{BorelStoch}$ one recovers usual notions of second-order stochastic dominance for random variables.
Indeed, we can interpret a partial evaluation for the case of probability monads (say, with a line segment as algebra) as ``subdividing a probability measure and replacing the parts by their centers of mass''.
For free algebras (i.e.~\emph{probability measure}-valued random variables), we can similarly interpret partial evaluations as partial mixtures of probability measures. 
Recall for example that a mixture of Gaussians is a probability measure that looks (for instance) as follows, and a random variable following such a distribution is called a mixture model:
\begin{center}
	\begin{tikzpicture}[x={(0:1.8cm)},y={(90:1.8cm)},z={(180:0.5cm)},%
		bullet/.style={circle, fill, minimum size=2pt, inner sep=1.5pt, outer sep=0pt},%
		]
		\draw [color=gray,domain=-3:3,samples=100,name path=normal] plot (\x,{exp(-(\x+1)*(\x+1)/1)+exp(-(\x-1)*(\x-1)/1)}) ;
		\draw [color=gray,domain=-3:3,samples=100,name path=normal] plot (\x,{exp(-(\x+1)*(\x+1)/1)}) ;
		\draw (-3,0) -- (3,0) ;
	\end{tikzpicture}
\end{center}
Partial evaluations in this case can be considered ``coarse-grainings'' of these mixtures of measures:
\begin{center}
	\begin{tikzcd}[row sep=0]
		\begin{tikzpicture}[x={(0:0.8cm)},y={(90:1cm)},z={(180:0.5cm)},%
			bullet/.style={circle, fill, minimum size=2pt, inner sep=1.5pt, outer sep=0pt},%
			]
			\draw [color=gray,domain=-4:4,samples=100,name path=normal] plot (\x,{exp(-(\x+2)*(\x+2)/1)+exp(-(\x)*(\x)/1)+exp(-(\x-2)*(\x-2)/1)}) ;
			\draw [color=gray,domain=-4:4,samples=100,name path=normal] plot (\x,{exp(-(\x+2)*(\x+2)/1)+exp(-(\x)*(\x)/1)}) ;
			\draw [color=gray,domain=-4:4,samples=100,name path=normal] plot (\x,{exp(-(\x+2)*(\x+2)/1)}) ;
			\draw (-4,0) -- (4,0) ;
		\end{tikzpicture}
		&
		\begin{tikzpicture}[x={(0:0.8cm)},y={(90:1cm)},z={(180:0.5cm)},%
			bullet/.style={circle, fill, minimum size=2pt, inner sep=1.5pt, outer sep=0pt},%
			]
			\draw [color=gray,domain=-4:4,samples=100,name path=normal] plot (\x,{exp(-(\x+2)*(\x+2)/1)+exp(-(\x)*(\x)/1)+exp(-(\x-2)*(\x-2)/1)}) ;
			\draw [color=gray,domain=-4:4,samples=100,name path=normal] plot (\x,{exp(-(\x+2)*(\x+2)/1)}) ;
			\draw (-4,0) -- (4,0) ;
		\end{tikzpicture}
		\\
		\dfrac{1}{3} [N(-2,1)] + \dfrac{1}{3} [N(0,1)] + \dfrac{1}{3} [N(2,1)]
		\ar{r} 
		& 
		\dfrac{1}{3} [N(-2,1)] + \dfrac{2}{3} \Bigg[ \dfrac{1}{2} N(0,1) + \dfrac{1}{2} N(2,1) \Bigg]
		 \\
		\in PPX & \in PPX
	\end{tikzcd}
\end{center}
In particular, given a morphism $p:S\to PX$ (or a Kleisli morphism $p^\flat:S\to X$), we can view a morphism $d:S\to PPX$ such that $\mu\circ d=p$ (or equivalently a Kleisli morphism $d^\flat:S\to PX$ such that $\samp\circ d^\flat=p^\flat$) as a ``decomposition'' of $p$.

For more details we refer the reader to \cite{Fritzetal-2023-Representablemarkovc} and also \cite[Chapter 4]{Perrone18}.


\subsection{Statistical experiments and hypernormalizations}\label{stat_exp}

We can model a probability space in a Markov category as a pair $(\Theta,p)$ where $\Theta$ is an object on our category. and $p:I\to\Theta$.
A \textbf{statistical experiment} on $(\Theta,p)$ is a morphism $f:\Theta\to X$ up to $p$-almost sure equality. The idea is that
\begin{itemize}
	\item The space $\Theta$ contains states of the world that we cannot access or observe directly;
	\item The probabilistic state $p:I\to\Theta$ represents our incomplete knowledge on where we are in $\Theta$. From the Bayesian point of view, it is our prior distribution;
	\item The space $X$ encodes something we can observe, and is hence more ``coarse-grained'' than $\Theta$;
	\item The map $f:\Theta\to X$ performs this coarse-graining, or this observation, possibly in a noisy way, so that we allow it to be non-deterministic. (In $\cat{BorelStoch}$, this is a Markov kernel or a stochastic map.)
\end{itemize}

\begin{example}\label{ex:gaussian}
	Consider the following situation in $\cat{BorelStoch}$, where $(\Theta,p)$ is the real line with a normal distribution, and $X=\{1,2,3\}$ is a finite set.
	We now take a deterministic experiment $f:\Theta\to X$ which partitions $\Theta$ into regions labeled by the elements of $X$.
	\begin{center}
		\begin{tikzpicture}[x={(20:1.2cm)},y={(90:1.2cm)},z={(-20:4cm)},%
				bullet/.style={circle, fill, minimum size=2pt, inner sep=1.5pt, outer sep=0pt},%
			]
			\fill [gray!35, domain=-3:3, variable=\x]
			(-3, 0)
			-- plot (\x,{exp(-\x*\x/2)})
			-- (3, 0)
			-- cycle ;
			\draw [color=gray,domain=-3:3,samples=100] plot (\x,{exp(-\x*\x/2)}) node[midway,yshift=1.5cm] {$p$};
			\draw (-3,0) -- (3,0) ;

			\draw [black,dashed] (-1,-0.5) -- (-1,1) ;
			\draw [black,dashed] (1,-0.5) -- (1,1) ;

			\node [label=right:$1$] (1) at (-2.05,-0.2,1) {};
			\node [label=right:$2$] (2) at (0,0,1) {};
			\node [label=right:$3$] (3) at (1.95,0.2,1) {};

			\draw [->, shorten <=5mm, shorten >=5mm] (-2,0,0) -- (1) node[midway,below left] {$f$};
			\draw [->, shorten <=4.8mm, shorten >=4.8mm] (0,0,0) -- (2) ;
			\draw [->, shorten <=4.6mm, shorten >=4.6mm] (2,0,0) -- (3) ;

			\draw [draw=gray,fill=gray!35] (1) -- ++(0.1,0) -- ++(0,0.5) -- ++(-0.1,0) -- ++(0,-0.5) --cycle ;
			\draw [draw=gray,fill=gray!35] (2) -- ++(0.1,0) -- ++(0,1) -- ++(-0.1,0) -- ++(0,-1) --cycle ;
			\draw [draw=gray,fill=gray!35] (3) -- ++(0.1,0) -- ++(0,0.5) -- ++(-0.1,0) -- ++(0,-0.5) --cycle node[above right,yshift=0.5cm,color=gray] {$f\circ p$};

			\node [bullet] at (1) {};
			\node [bullet] at (2) {};
			\node [bullet] at (3) {};
		\end{tikzpicture}
	\end{center}
	We can interpret it in a Bayesian way as follows: we have a point on the real line, but we don't know where exactly, and the measure $p$ represent our belief about its position. $f$ is now an experiment that will tell us, deterministically, in which one of the three regions the point is, therefore improving our knowledge.

	In the picture above we have also drawn the pushforward measure $f\circ p$, which is the probability, according to our belief, that each point is chosen. (It is the same number as the measure, according to $p$, of the corresponding region of $\Theta$.)

	Note that, in $\cat{Stoch}$, $X$ is isomorphic to the real line equipped with the sigma-algebra induced by the partition (\cite[Proposition~2.7]{ensarguet2023ergodic}). The same can be said in the category of \emph{probability} spaces (see the same reference).
	We can therefore see a deterministic experiment equivalently as a coarse-graining of the sigma-algebra.
\end{example}

\begin{example}
	Similarly to the example above, we can view a non-deterministic
	experiment as a situation where, instead of partitioning the
	\emph{domain}, we more generally partition the \emph{mass} over the
	domain:
	\begin{center}
		\begin{tikzpicture}[x={(0:1.8cm)},y={(90:1.8cm)},z={(180:0.5cm)},%
				bullet/.style={circle, fill, minimum size=2pt, inner sep=1.5pt, outer sep=0pt},%
			]
			\fill [gray!35, domain=-3:3, variable=\x]
			(-3, 0)
			-- plot (\x,{exp(-\x*\x/2)})
			-- (3, 0)
			-- cycle;
			\draw [color=gray,domain=-3:3,samples=100,name path=normal] plot (\x,{exp(-\x*\x/2)}) ;
			\draw (-3,0) -- (3,0) ;

			\node [bullet,label=below:$\theta$] (theta) at (-1,0) {};
			\draw [draw=none,name path=up] (theta) -- ++(0,1) ;
			\draw [line width=0.8mm,gray!72,name intersections={of=normal and up,by={i}}] (theta) -- (i);

			\draw [black,dashed] (-1.5,-0.25) -- (0.,1.25) ;
			\draw [black,dashed] (0.5,-0.25) -- (1.5,0.75) ;

			\node [label={[gray]below:$(1)$}] (1) at (-2,0,1) {};
			\node [label={[gray]below:$(2)$}] (2) at (0,0,1) {};
			\node [label={[gray]below:$(3)$}] (3) at (2,0,1) {};
		\end{tikzpicture}
	\end{center}
	The map $f$ this time takes each point $\theta$ and splits its output across the elements of $X$,
	with the probability of each output being proportional to the height of
	that element's region in the vertical column above $\theta$.
\end{example}


Now following again \Cref{ex:gaussian}, suppose that the result of the
experiment is the point $1$. Then we know that our point lies in the left
region of the curve. But we still don't know where exactly, within that region.
We can therefore update our belief to the following distribution,
$p(\theta|x=1)$:
\begin{center}
	\begin{tikzpicture}[x={(20:1.2cm)},y={(90:1.2cm)},z={(-20:4cm)},%
			bullet/.style={circle, fill, minimum size=2pt, inner sep=1.5pt, outer sep=0pt},%
		]
		\fill [gray!35, domain=-3:-1, variable=\x]
		(-3, 0)
		-- plot (\x,{4*exp(-\x*\x/2)})
		-- (-1, 0)
		-- cycle ;
		\draw [color=gray!72,domain=-3:3,samples=100] plot (\x,{exp(-\x*\x/2)}) node[midway,yshift=1.5cm] {$p$};
		\draw [color=gray,domain=-3:-1,samples=100] plot (\x,{4*exp(-\x*\x/2)}) node[left] {$p(-|x=1)$};
		\draw (-3,0) -- (3,0) ;

		\draw [black,dashed] (-1,-0.5) -- (-1,3) ;
		\draw [black,dashed] (1,-0.5) -- (1,1) ;

		\node [bullet,label=right:$1\;$\cmark] (1) at (-2.05,-0.2,1) {};
		\node [gray!72,bullet,label={[gray!72]right:$2\;$\xmark}] (2) at (0,0,1) {};
		\node [gray!72,bullet,label={[gray!72]right:$3\;$\xmark}] (3) at (1.95,0.2,1) {};

		\draw [->, shorten <=5mm, shorten >=5mm] (-2,0,0) -- (1) node[midway,below left] {$f$};
		\draw [gray!72,->, shorten <=4.8mm, shorten >=4.8mm] (0,0,0) -- (2) ;
		\draw [gray!72,->, shorten <=4.6mm, shorten >=4.6mm] (2,0,0) -- (3) ;
	\end{tikzpicture}
\end{center}
\begin{itemize}
	\item The excluded regions $(2)$ and $(3)$ have now probability zero;
	\item The confirmed region $(1)$ has now probability one;
	\item Within region $(1)$, and only within there, we keep the probability proportional to the old measure $p$ -- but we need to renormalize it so that the total mass is one:
	      \[
		      p(A|x=1) \;=\; \dfrac{p(A\cap f^{-1}(1))}{p(f^{-1}(1))} .
	      \]
\end{itemize}
(A similar measure can also be constructed if the experiment is noisy.)

The measure $p(-|x=1)$ is called the \textbf{posterior distribution}, and the
process of moving from $p$ to $p(-|x=1)$ is called \textbf{Bayesian updating}.
Depending on the observed $x$, we get a different posterior distribution
$p(-|x)$. We can then form a probabilistic mapping $X\to\Theta$ assigning to
each $x$ its corresponding posterior distribution (or at least, a random
element distributed according to it). This morphism, when it exists, is called
a Bayesian inverse of $f$, and it is a special case of a conditional:

\begin{definition}
	Let $p:I \to \Theta $ and $ f: \Theta \to X $ be a statistical experiment in a Markov category. A
	\textbf{Bayesian inverse} of $ f $ with respect to $ p $ is a morphism $
		f^\dag_p: X \to \Theta $ such that the following equation holds,
	\begin{equation}
		\label{eq:bayesian0}
		\tikzfig{bayesian0}
	\end{equation}
	where $ q = p \circ f $.

	More generally, when $p$ depends on a parameter $A$, a
	\textbf{Bayesian inverse} of $ f $ with respect to $ p:A\to\Theta $ is a morphism $
		f^\dag_p: A \otimes X \to \Theta $ such that the following equation holds.
	\begin{equation}
		\label{eq:bayesian}
		\tikzfig{bayesian}
	\end{equation}\\
	where again $ q = p \circ f $.

	(When $p$ is clear from the context we will just write $f^\dag$.)
\end{definition}

Note that a Bayesian inverse of $f$ only depends on the a.s.\ equality class of
$f$, and if it exists, it is also unique almost surely.

In a representable Markov category we have access to the actual distribution
(via distribution objects), and so we can interpret the morphism
$(f^\dag_p)^\#:X\to P\Theta$ as assigning from a point $x$ its corresponding
posterior distribution (as a point of $P\Theta$). Mind the difference between:
\begin{itemize}
	\item The Bayesian inverse $f^\dag_p:X\to \Theta$: is a stochastic map,
		which from $X$ gives us a probabilistic output distributed
		according to the posterior distribution;
	\item The deterministic morphism $(f^\dag_p)^\#:X\to P\Theta$, which
		from $X$ gives us the actual posterior distribution
		(deterministically depending on $X$).
\end{itemize}
As usual, we can obtain the former by applying the map $\samp$ to the latter.
(But the former is also defined outside the representable case.)

As each posterior distribution can be seen as a (renormalized) ``piece'' of the
prior, conversely we can view the prior as a mixture of the posteriors. The
measure on $P\Theta$ which gives the mixing is called the \emph{standard
measure}:

\begin{definition}
	Let $\cat{C}$ be a representable Markov category. Let $(\Theta,p)$ be a
	probability space, and let $f:\Theta\to X$ be a statistical experiment
	The \textbf{standard measure} of $f$ is the state $\hat{f}_p$ on
	$P\Theta$ given by
	\[
		\begin{tikzcd}
			I \ar{r}{p} & \Theta \ar{r}{f} & X \ar{r}{(f^\dag_p)^\#} & P\Theta .
		\end{tikzcd}
	\]
\end{definition}

For discrete $\Theta$ and deterministic $f$, we can equivalently view the
standard measure as follows:
\begin{itemize}
	\item $f$ is equivalently a (finite) partition of $\Theta$;
	\item The standard measure is a measure on $P\Theta$ such that each of
		the elements (i.e.~measures) on its support are each supported
		exactly on a cell of the partition. 
\end{itemize}
Such a construction, for the special case of discrete product projections was
called ``hypernormalization'' (up to different spelling conventions) in
\cite{hypernormalization}. Let's give here a partial generalization, with a
similar intuition.

\begin{definition}\label{def:hypernormalization}
	Let $(\Theta,p)$ be a probability space in a representable Markov
	category, and let $f:\Theta\to X$ be an a.s.~deterministic statistical
	experiment. The \textbf{hypernormalization} of $p$ with respect to $f$,
	if it exists, is the standard measure $\hat{f}_p$ on $P\Theta$. 
\end{definition}

The construction given in \cite{hypernormalization}, or at least its
instantiation for the case of the distribution monad, can be considered a
special case of the definition above. Let's see this:
\begin{example}
	In $\cat{Stoch}$, let's consider the special case of finite sets $X$
	and $Y$ with a probability distribution $p$ on $X\times Y$. Let now
	$\pi:X\times Y\to X$ be the product projection, which we can see as
	partitioning the product into $X$-many copies of $Y$, the fibers
	$\pi^{-1}(x)\subseteq X\times Y$. Instantiating our notion of
	hypernormalization, we are decomposing $p$ into a convex combination of
	measures $p_x\in P(X\times Y)$ as follows,
	\[
	p= \sum_{x\in X} q(x) \, p_x \qquad\Longleftrightarrow\qquad p(x',y)= \sum_{x\in X} q(x) \, p_x(x',y)
	\] 
	where 
	\begin{itemize}
		\item $q$ is the marginal of $p$ on $X$;
		\item $p_x$ is really only appearing for those $x$ for which $q(x)>0$;
		\item Each measure $p_x$ (where $q(x)>0$) is supported on the fiber $\pi^{-1}(x)$;
		\item Each measure $p_x$ (where $q(x)>0$) is proportional to the restriction of $p$ to the fiber:  
		\[
		p_x(x',y) \;=\;\begin{cases}
			\frac{p(x',y)}{q(x)} & x'=x, \\
			0 & x'\ne x .
		\end{cases}
		\]
	\end{itemize}
	This is equivalent to the construction given in
	\cite[Section~3]{hypernormalization} for the distribution monad.
\end{example}

Note that, according to our definitions, in order for the hypernormalization to
exist, we need the Bayesian inverse $f^\dag_p$ to exist. (In $\cat{Stoch}$,
this is guaranteed if $X$ is standard Borel.) We will see in \Cref{hypernorm}
that hypernormalizations satisfy a universal property. Therefore, one could
define them more generally outside the case where the necessary conditionals
exist (for example, in categories of topological spaces).

Let's now define an order on statistical experiments, deterministic or not.
Experiments are naturally ordered in terms of ``how informative they are'':

\begin{definition}
	Let $f:\Theta\to X$ and $g:\Theta\to Y$ be statistical experiments on
	$(\Theta,p)$. We say that $g\le f$ in the \textbf{Blackwell order} if
	and only if there exists a morphism $h:X\to Y$ such that $h\circ f=_p
	g$.
\end{definition}

In some sense, $f$ is ``more informative'' than $g$ if we can recover the
results of the experiment $g$ by processing the results of $f$ without having
any further access to $\Theta$.

\begin{theorem}[Blackwell-Sherman-Stein theorem for Markov categories, {\cite[Theorem~5.13]{Fritzetal-2023-Representablemarkovc}}]\label{thm:bss}
	Let $\cat{C}$ be an a.s.\ compatibly representable Markov category. Let
	$f:\Theta\to X$ and $g:\Theta\to Y$ be statistical experiments on
	$(\Theta,p)$. Then $g\le f$ in the Blackwell order if and only if for
	their standard measures, $\hat{g}_p\ge\hat{f}_p$ in the stochastic
	dominance order of $\cat{C}(I,P\Theta)$.
\end{theorem}


\begin{example}
	As in \Cref{ex:gaussian}, let $(\Theta,p)$ be the real line with the
	normal distribution, and let $f:\Theta\to X$ with $X=\{1,2,3\}$.
	Consider now $Y=\{a,b\}$, and the (deterministic) function $h:X\to Y$
	given by $h(1)=a$, $h(2)=h(3)=b$. Then the function $g=h\circ
	f:\Theta\to Y$ can be seen as partitioning $\Theta$, but putting
	together the second and third region:
	\begin{center}
		\begin{minipage}{0.45\textwidth}
			\begin{center}
				\begin{tikzpicture}[x={(20:0.9cm)},y={(90:0.9cm)},z={(-20:1.5cm)},%
						bullet/.style={circle, fill, minimum size=2pt, inner sep=1.5pt, outer sep=0pt},%
					]
					\fill [gray!35, domain=-3:3, variable=\x]
					(-3, 0)
					-- plot (\x,{exp(-\x*\x/2)})
					-- (3, 0)
					-- cycle ;
					\draw [color=gray,domain=-3:3,samples=100] plot (\x,{exp(-\x*\x/2)}) ;
					\draw (-3,0) -- (3,0) ;

					\draw [black,dashed] (-1,-0.5) -- (-1,1) ;
					\draw [black,dashed] (1,-0.5) -- (1,1) ;

					\node [label=right:$1$] (1) at (-2.05,-0.2,1) {};
					\node [label=right:$2$] (2) at (0,0,1) {};
					\node [label=right:$3$] (3) at (1.95,0.2,1) {};

					\draw [->, shorten <=3.6mm, shorten >=3.6mm] (-2,0,0) -- (1) node[midway,below left] {$f$};
					\draw [->, shorten <=3.6mm, shorten >=3.6mm] (0,0,0) -- (2) ;
					\draw [->, shorten <=3.6mm, shorten >=3.6mm] (2,0,0) -- (3) ;

					\draw [draw=gray,fill=gray!35] (1) -- ++(0.1,0) -- ++(0,0.5) -- ++(-0.1,0) -- ++(0,-0.5) --cycle ;
					\draw [draw=gray,fill=gray!35] (2) -- ++(0.1,0) -- ++(0,1) -- ++(-0.1,0) -- ++(0,-1) --cycle ;
					\draw [draw=gray,fill=gray!35] (3) -- ++(0.1,0) -- ++(0,0.5) -- ++(-0.1,0) -- ++(0,-0.5) --cycle ;

					\node [bullet] at (1) {};
					\node [bullet] at (2) {};
					\node [bullet] at (3) {};
				\end{tikzpicture}
			\end{center}
		\end{minipage}
		\begin{minipage}{0.45\textwidth}
			\begin{center}
				\begin{tikzpicture}[x={(20:0.9cm)},y={(90:0.9cm)},z={(-20:1.5cm)},%
						bullet/.style={circle, fill, minimum size=2pt, inner sep=1.5pt, outer sep=0pt},%
					]
					\fill [gray!35, domain=-3:3, variable=\x]
					(-3, 0)
					-- plot (\x,{exp(-\x*\x/2)})
					-- (3, 0)
					-- cycle ;
					\draw [color=gray,domain=-3:3,samples=100] plot (\x,{exp(-\x*\x/2)}) ;
					\draw (-3,0) -- (3,0) ;

					\draw [black,dashed] (-1,-0.5) -- (-1,1) ;

					\node [label=right:$a$] (1) at (-2.05,-0.2,1) {};
					\node [label=right:$b$] (2) at (0.49,0.05,1) {};

					\draw [->, shorten <=3.6mm, shorten >=3.6mm] (-2,0,0) -- (1) node[midway,below left] {$g$};
					\draw [->, shorten <=3.6mm, shorten >=3.6mm] (0.5,0,0) -- (2) ;

					\draw [draw=gray,fill=gray!35] (1) -- ++(0.1,0) -- ++(0,0.5) -- ++(-0.1,0) -- ++(0,-0.5) --cycle ;
					\draw [draw=gray,fill=gray!35] (2) -- ++(0.1,0) -- ++(0,1.5) -- ++(-0.1,0) -- ++(0,-1.5) --cycle ;

					\node [bullet] at (1) {};
					\node [bullet] at (2) {};
				\end{tikzpicture}
			\end{center}
		\end{minipage}
	\end{center}
	Therefore the decomposition of $p$ is coarser than the one given by $f$.
\end{example}

The idea illustrated in this example will be made precise in \Cref{hypernorm}.

We conclude with a sort of converse to \Cref{thm:bss}, which seems to be new.

\begin{proposition}\label{blackwell2}
	Let $\cat{C}$ be an a.s.\ compatibly representable Markov category.
	Let $\pi,\tau:I\to P\Theta$. Then $\pi\le\tau$ in the order of stochastic dominance on $\cat{C}(I,P\Theta)$ if and only if both of the following conditions are satisfied:
	\begin{enumerate}
		\item $\samp\circ\pi=\samp\circ\tau$. (That is, they are decompositions of the same state, denote it by $p:I\to\Theta$.)
		\item There exist statistical experiments $f:\Theta\to X$ and $g:\Theta\to Y$ on $(\Theta,p)$ with $\hat{f}_p=\pi$, $\hat{g}_p=\tau$, and $f\ge g$ in the Blackwell order.
	\end{enumerate}
\end{proposition}

We will use the following auxiliary statement.
\begin{lemma}\label{stdmeas}
	Let $\pi:I\to P\Theta$, denote $\samp\circ\pi:I\to\Theta$ by $p$, and consider the Bayesian inverse $\samp^\dag_\pi:\Theta\to P\Theta$ as a statistical experiment on $(\Theta,p)$.
	Then its standard measure is exactly $\pi$.
\end{lemma}
\begin{proof}[Proof of \Cref{stdmeas}]
	Using the usual definition of standard measure,
	\begin{align*}
	\big((\samp^\dag_\pi)^\dag\big)^\sharp \circ \samp^\dag_p \circ p \;&=\; \big((\samp^\dag_\pi)^\dag\big)^\sharp \circ\pi \\ &=\; \samp^\sharp \circ\pi \\
	&=\; 1\circ\pi \;=\; \pi . \qedhere
	\end{align*}
\end{proof}

\begin{proof}[Proof of \Cref{blackwell2}]
	One side of the implication is given by \Cref{thm:bss}. More in detail, suppose that $\samp\circ\pi=\samp\circ\tau$, and denote either side of the equation by $p:I\to\Theta$. Suppose now that there exist experiments $f:\Theta\to X$ and $g:\Theta\to Y$ on $(\Theta,p)$ with $\hat{f}_p=\pi$, $\hat{g}_p=\tau$, and $f\ge g$ in the Blackwell order. Then by \Cref{thm:bss}, $\pi\le\tau$.
	
	Conversely, suppose that $\pi\le\tau$ in the order of stochastic dominance. Then by definition there exists $\kappa:I\to PP\Theta$ such that $\samp\circ\kappa=\pi$ and $\mu\circ\kappa=\tau$. 
	By naturality of $\samp$ (and recalling that $\mu=P\samp$), we now have
	\[
	\samp\circ\pi\;=\;\samp\circ\samp\circ\kappa \;=\; \samp\circ\mu\circ\kappa \;=\; \samp\circ\tau .
	\]
	Denote now by $p:I\to\Theta$ either side of the equation above, and
	consider the statistical experiments $f=\samp^\dag_\pi$ and
	$g=\samp^\dag_\tau:\Theta\to P\Theta$ on $(X,p)$. By \Cref{stdmeas},
	their standard measures are respectively $\pi$ and $\tau$. As
	$\pi\le\tau$, by \Cref{thm:bss} we have that $f\ge g$ in the Blackwell
	order.
\end{proof}


\section{Main results}\label{main_results}

\subsection{The multiplication map is weakly cartesian}\label{wcartmu}

\begin{theorem}
	\label{thm:pullbacks}
	Let $\cat{C}$ be an a.s.-compatibly representable Markov category with
	monad $(P,\mu,\delta)$. If $\cat{C}$ has conditionals, then $\mu$ is
	weakly Cartesian.
\end{theorem}

\begin{corollary}\label{pullbacks_giry}
	The Giry monad on standard Borel spaces has weakly cartesian multiplication.
\end{corollary}

In order to prove the theorem we use the following technical statement, which
can be seen as an equivalent characterization of a.s.-compatible
representability for the case when conditionals exist.

\begin{lemma}\label{lem:alt-asc}
	Let $\cat{C}$ be a representable Markov category.
	Then
	\begin{enumerate}
		\item If $p:I\to X$ and $f,g:X\to Y$ such that
			$Pf=_\pi Pg$ for every $\pi:I\to PX$ with
			$\samp\circ\pi=p$, then $f=_p g$:
			\begin{equation}\label{eq:alt-asc0}
			    \tikzfig{alt-asc0}
			\end{equation}
		\item If $p:A\to X$ and $f,g:X\otimes A\to Y$ such that
			$Pf\circ\sigma_{X,A}=_\pi Pg\circ\sigma_{X,A}$ for
			every $\pi:A\to PX$ with $\samp\circ\pi=p$, then $f=_p
			g$: 
			\begin{equation}\label{eq:alt-asc}
			    \tikzfig{alt-asc}
			\end{equation}
			where $\sigma_{X,A}:PX\otimes A\to P(X\otimes A)$ is
			the right strength of the monad $P$.

		\item Suppose moreover that $\cat{C}$ has conditionals. Then
			the converse to the implication above holds if and only
			if $\cat{C}$ is a.s.-compatibly representable.
	\end{enumerate}
\end{lemma}


\begin{proof}[Proof of \Cref{lem:alt-asc}]
	\begin{enumerate}
		\item Note that this statement is implied by 2., setting $A=I$.
			So let's prove 2.\ directly.
		
		
		\item Take $\pi=\delta\circ p$, and notice that
			$\samp\circ\pi=\samp\circ\delta\circ p= p $. Applying
			$\samp$ to both outputs of the first term in
			\eqref{eq:alt-asc} we get:
		\[
		\tikzfig{alt-asc-proof1}
		\]
		using determinism of $\delta$, naturality of $\samp$, monoidality of $\samp$ (see \cite[Remark 3.16]{Fritzetal-2023-Representablemarkovc}), and $\samp\circ\delta=1$. The same can be done with $g$, and so, by the left equality in \eqref{eq:alt-asc} we get the right equality.
		
		
		\item Suppose now that $\cat{C}$ has conditionals.
		Suppose first of all that for every $p:A\to X$, $f,g:X\otimes A\to Y$ and $\pi:A\to PX$ we have the reverse implication to \eqref{eq:alt-asc}. Take again $\pi=\delta\circ p$, and applying $\samp$ to the first output of the first term in \eqref{eq:alt-asc}, we get
		\[
		\tikzfig{alt-asc-proof2}
		\]
		using determinism of $\delta$, $\samp\circ\delta=1$, monoidality of $\delta$, and the definition of $f^\sharp$. The same can be done with $g$, and so, by the right equality in \eqref{eq:alt-asc}, we get exactly the sampling cancellation property \eqref{eq:samp-canc}.
		
		
		Conversely, suppose that $\cat{C}$ is a.s.-compatibly representable (and has conditionals).
		Let $p:A\to X$, $f,g:X\otimes A\to Y$, $\pi:A\to PX$ such that $\samp\circ\pi=p$, and suppose that the right equality in \eqref{eq:alt-asc} holds.
		To prove the left equality, by the sampling cancellation property it suffices to show that
		\begin{equation}\label{eq:alt-asc-proof3}
			\tikzfig{alt-asc-proof3}
		\end{equation}
		Consider now the Bayesian inverse $(\samp_X)^\dag_\pi$ appearing in the following equation.
		\begin{equation}\label{eq:bayesian-samp}
			\tikzfig{bayesian-samp}
		\end{equation}
		The left-hand side of \eqref{eq:alt-asc-proof3} can be transformed as follows,
		\[
		\tikzfig{alt-asc-proof4}
		\]
		using naturality of $\samp$, monoidality of $\samp$, \eqref{eq:bayesian-samp}, the fact that $\samp\circ\pi=p$, and associativity of copying.
		The same can be done for $g$. Therefore in the last term above we can substitute $f$ by $g$ by the right equality in \eqref{eq:alt-asc}, and we obtain \eqref{eq:alt-asc-proof3}, which implies the left equality in \eqref{eq:alt-asc} by the sampling cancellation property. \qedhere
	\end{enumerate}
\end{proof}


Let's now prove the main statement.

\begin{proof}[Proof of \Cref{thm:pullbacks}]
	Let \(f:X\to Y\) be deterministic.
	Using \Cref{cor:BC_Kleisli}, let \( p:A \to X \), and \( q: A \to PY \) be
	morphisms in $\cat{C}$ such that the following diagram is
	commutative.
	\begin{equation}\label{eq:outer}
		\begin{tikzcd}
			A \\
			& PX & PY \\
			& X & Y
			\arrow["q", curve={height=-18pt}, from=1-1, to=2-3]
			\arrow["p"', curve={height=18pt}, from=1-1, to=3-2]
			\arrow["Pf", from=2-2, to=2-3]
			\arrow["\samp"', from=2-2, to=3-2]
			\arrow["\samp", from=2-3, to=3-3]
			\arrow["f"', from=3-2, to=3-3]
		\end{tikzcd}
	\end{equation}
	We need to find a map \( r: A \to PX \) in $\cat{C}$ such that the following diagram commutes.
	\[
		\begin{tikzcd}
			A \\
			& PX & PY \\
			& X & Y
			\arrow["r"{description}, dashed, from=1-1, to=2-2]
			\arrow["q", curve={height=-18pt}, from=1-1, to=2-3]
			\arrow["p"', curve={height=18pt}, from=1-1, to=3-2]
			\arrow["Pf", from=2-2, to=2-3]
			\arrow["\samp"', from=2-2, to=3-2]
			\arrow["\samp", from=2-3, to=3-3]
			\arrow["f"', from=3-2, to=3-3]
		\end{tikzcd}
	\]
	Consider therefore the following morphism in $\cat{C}$,
	\begin{equation}
		\tikzfig{r-def}
		\label{eq:r-def}
	\end{equation}
	where $f^\dag:Y\otimes A\to X$ is the Bayesian inverse of $f$ with respect to $p$,
	\begin{equation}
		\label{eq:bayesian2}
		\tikzfig{bayesian2}
	\end{equation}
	and
	$\sigma_{Y,A}:PY \otimes A \to P(A \otimes Y)$ is the right strength of the monad $P$.

	To show that the left triangle commutes, i.e.\ that $\samp_X\circ\mu\circ r^\sharp = p $ we have that
	\begin{equation*}
		\tikzfig{lem-weak-2}
	\end{equation*}
	using the definition of $r$, naturality of $\samp$, monoidality of $\samp$, commutativity of the outer diagram~\eqref{eq:outer}, and the second marginal of \eqref{eq:bayesian2}.

	Let's now show that the top triangle commutes, i.e.\ $Pf\circ r = q$.
	Using \Cref{lem:f-as-det} and \eqref{eq:bayesian2} we have that $f\circ f_p^+$ is almost surely equal to the marginalization $1_Y\otimes\texttt{del}_A$. Since now (by commutativity of the outer diagram~\eqref{eq:outer}) $\samp\circ q=f\circ p$, we can apply \Cref{lem:alt-asc} to \eqref{eq:f-as-det}, and we get the following,
	\[
		\tikzfig{lem-weak-stronger}
	\]
	using also naturality of the strength. Taking the second marginal, we get exactly $Pf\circ r = q$.
\end{proof}

In light of Proposition~\ref{prop:partialrel} we also have the following property on the
second-order stochastic dominance relation.
\begin{corollary}
	\label{cor:dominance}
	Let $\cat{C}$ be an a.s.-compatibly representable Markov category with
	conditionals. Then the second-order dominance
	relation on $\cat{C}(\Theta, A)$ is transitive for all $\Theta\in\cat{C}$ and all $P$-algebras $A$.
\end{corollary}


\subsection{The functor preserves weak pullbacks}\label{wcartp}

Let's now give a condition for when probability monads preserve weak pullbacks.
Before we begin, we need a condition relating equalizers in the category
$\cat{C}_\det$ (and hence finite limits, since $\cat{C}_\det$ has finite
products) and almost sure equality in the Markov category sense (which is
important for conditionals). 

Recall first of all that in traditional probability theory, two random
variables $f$ and $g$ on a probability space $(X,p)$ are almost surely equal if
and only if they agree on a set of probability one. This condition is
equivalent to say that the equalizer of $f$ and $g$ has full measure, or again
equivalently, that the measure $p$, seen as a kernel $I\to X$, factors through
the equalizer of $f$ and $g$.

With this example in mind,let's define the following property of compatibility
between equalizers and almost sure equality in a Markov category.

\begin{definition}[{\cite[Definition~3.5.1]{Fritzetal-2023-Absolutecontinuitysu}}]\label{def:equalizer_principle}
	A Markov category $\cat{C}$ is said to satisfy the \textbf{equalizer principle} if:
	\begin{enumerate}[label=(\roman*)]
		\item Equalizers in $\cat{C}_{\text{det}}$ exist;
		\item For every equalizer diagram
		      \begin{equation*}
			      % https://q.uiver.app/#q=WzAsMyxbMSwwLCJYIl0sWzIsMCwiWSJdLFswLDAsIkUiXSxbMCwxLCJmIiwwLHsib2Zmc2V0IjotMX1dLFswLDEsImciLDIseyJvZmZzZXQiOjF9XSxbMiwwLCJcXHRleHR0dHtlcX0iXV0=
			      \begin{tikzcd}
				      E & X & Y
				      \arrow["{\cat{eq}}", from=1-1, to=1-2]
				      \arrow["f", shift left, from=1-2, to=1-3]
				      \arrow["g"', shift right, from=1-2, to=1-3]
			      \end{tikzcd}
		      \end{equation*}
		      in $\cat{C}_{\text{det}}$, every $p:A \to X$ in $\cat{C}$
		      satisfying $f =_p g$ factors uniquely across $\cat{eq}$.
	\end{enumerate}
\end{definition}

\begin{example}[{\cite[Proposition~3.5.4]{Fritzetal-2023-Absolutecontinuitysu}}]
	\label{lem:boreleq}
	$\cat{BorelStoch}$ satisfies the equalizer principle.
\end{example}

Here is now our main statement.

\begin{theorem}
	\label{thm:main}
	Let $\cat{C}$ be a representable Markov category with monad $(P,\mu,\delta)$.
	Suppose moreover that
	\begin{itemize}
		\item $\cat{C}$ has conditionals;
		\item $\cat{C}$ satisfies the equalizer principle.
	\end{itemize}
	Then the functor $P$ preserves weak pullbacks.
\end{theorem}

\begin{corollary}
	In the hypotheses of the theorem above, suppose $\cat{C}$ is not just representable, but also a.s.-compatibly so.
	Then, together with \Cref{thm:pullbacks}, the monad $(P,\mu,\delta)$ is Beck-Chevalley.
\end{corollary}

\begin{corollary}\label{giryBC}
	The Giry monad on standard Borel spaces is Beck-Chevalley.
\end{corollary}

We will use the following auxiliary statement, which is well known (see for example \cite{Constantinetal-2020-Partialevaluationsan}).

\begin{lemma}
	\label{lem:weakpullback}
	Let $\cat{C}$ be a category with pullbacks and $(P, \mu, \delta)$ a monad on $\cat{C}$.
	If $P$ turns pullbacks into weak pullbacks then $P$ preserves weak
	pullbacks.
\end{lemma}

\begin{proof}[Proof of \Cref{thm:main}]
	First of all, since $\cat{C}_{\text{det}}$ has finite products (it is cartesian monoidal) and equalizers (by the equalizer principle), then it has all finite limits; in particular, it has all pullbacks.
	By \Cref{lem:weakpullback} it then suffices to show that $P$ turns
	pullbacks into weak pullbacks. Thus, suppose we have a pullback
	\[\begin{tikzcd}
			{X\times_Z Y} & Y \\
			X & Z.
			\arrow["{g^*f}", from=1-1, to=1-2]
			\arrow["{f^*g}"', from=1-1, to=2-1]
			\arrow["g", from=1-2, to=2-2]
			\arrow["f"', from=2-1, to=2-2]
		\end{tikzcd}\]
	in $\cat{C}_{\text{det}}$. Using \Cref{prop:BC_Kleisli}, we equivalently want to show that the diagram above is also a weak pullback in the Kleisli category. Suppose therefore that we have maps \( p:A \to X \)
	and \( q: A \to Y \), not necessarily deterministic, such that the following diagram commutes.
	\[\begin{tikzcd}
			A \\
			& {X\times_Z Y} & Y \\
			& X & Z.
			\arrow["{q}", curve={height=-18pt}, from=1-1, to=2-3]
			\arrow["{p}"', curve={height=18pt}, from=1-1, to=3-2]
			\arrow["{g^*f}", from=2-2, to=2-3]
			\arrow["{f^*g}"', from=2-2, to=3-2]
			\arrow["g", from=2-3, to=3-3]
			\arrow["f"', from=3-2, to=3-3]
		\end{tikzcd}\]
	Now form $\rho: A \to X \otimes Y$ as the conditional product
	\begin{equation*}
		\tikzfig{rho-def}
	\end{equation*}
	where \( s \) is the common composition \( f \circ p = g \circ q \).
	As one can readily check, the marginals of $\rho$ are $p$ and $q$ respectively.
	In order to prove the theorem, it then suffices to show that it factors through $X\times_Z Y$ (which, in $\cat{C}_{\text{det}}$, is a subobject of $X\times Y=X\otimes Y$).

	Recall now that the pullback $X\times_Z Y$ in$\cat{C}_{\text{det}}$ can be expressed as the following equalizer, also in $\cat{C}_{\text{det}}$:
	\[
		\begin{tikzcd}
			X\times_Z Y \ar[hook]{r} & X\times Y \ar[shift left]{rr}{f\circ\pi_X} \ar[shift right]{rr}[swap]{g\circ\pi_Y} && Z
		\end{tikzcd}
	\]
	The equalizer principle tells us that a sufficient condition for $\rho$ to factor through $X\times_Z Y$ is that the maps $f\circ\pi_X$ and $g\circ\pi_Y$ are $\rho$-almost surely equal, i.e. that the following equation holds.
	\begin{equation}
		\label{eq:eq-prin-1}
		\tikzfig{eq-prin-1}
	\end{equation}
	The left-hand side of \eqref{eq:eq-prin-1} is now equal to the following,
	\begin{equation*}
		\tikzfig{eq-prin-3}
	\end{equation*}
	using the definition of $\rho$, relative positivity of \( \mathcal{C} \) (see \cite[Section~2.5]{Fritzetal-2023-Dilationsandinformat}) together with the fact that \( f
	\circ f^\dag \) is $(s,1_A)$-a.s. deterministic (\Cref{lem:f-as-det}), and associativity of copying.
	The same procedure can be done for the right-hand side of \eqref{eq:eq-prin-1}, and so the two sides are equal.
	Thus, by the equalizer principle, there must exist a map \( r: A \to X \times_Z Y \) in \(
	\mathcal{C} \) such that
	\begin{equation*}
		\rho = (f^*g, g^*f) \circ r ,
	\end{equation*}
	i.e.\ such $(f^*g)\circ r=p$ and $(g^*f)\circ r= q$.
\end{proof}



\subsection{The universal property of hypernormalizations}\label{hypernorm}

The purpose of this section is to make the following intuition precise:
\begin{itemize}
	\item Consider a deterministic experiment $f$ on $(\Theta,p)$, which we can consider a ``partition'' or ``coarse-graining'';
	\item Form the standard measure on $P\Theta$, or ``hypernormalization'' (see \Cref{stat_exp});
	\item We can view this standard measure as the ``coarsest'' decomposition of $p$ which is still finer than the ``partition'' $f$.
\end{itemize}
From a somewhat different point of view, we can view the standard measure as \emph{the} decomposition of $p$ (in the sense of partial evaluations) induced by the partitioning $f$. 

Here is the precise statement. 

\begin{theorem}\label{thm:uni-std}
	Let $\cat{C}$ be an a.s.-compatibly representable Markov category.
	Let	$(\Theta,p)$ be a probability space, let $f:\Theta\to X$ $p$-a.s.\ deterministic, suppose its Bayesian inversion exists, and consider the standard measure $\hat{f}_p$.
	Denote also by $q$ the pushforward measure $f\circ p$ on $X$.
	
	Then for every $\pi:I \to P\Theta$ such that
	\begin{itemize}
		\item $\texttt{samp}_\Theta \circ \pi = p$ (i.e.~$\pi$ is a decomposition of $p$);
		\item $ Pf\circ \pi = \delta \circ q$ (i.e.~the pushforward of $\pi$ is the finest possible decomposition of $q$ which $X$ allows, or equivalently, $\pi$ is not ``coarser'' than the ``partition'' $f$),
	\end{itemize}
	we have a partial evaluation from $\pi$ to the standard measure $\hat{f}_p$.
\end{theorem}

\begin{comment}
\mika{This lemma needs to be reformulated}
\begin{lemma}\label{lem:samp-dag-com}
 	In the hypotheses of \Cref{thm:uni-std}, the following diagram  commutes $p$-almost surely.
	\[
		\begin{tikzcd}
			PX & PY \\
			X & Y
			\arrow["Pf", from=1-1, to=1-2]
			\arrow["{\texttt{samp}_\pi^\dag}", from=2-1, to=1-1]
			\arrow["f"', from=2-1, to=2-2]
			\arrow["{\texttt{samp}_{\delta\circ q}^\dag}"', from=2-2, to=1-2]
		\end{tikzcd}
	\]
\end{lemma}
\begin{proof}
	We have to show that
	\begin{equation}
		\tikzfig{samp-dag-com1}
	\end{equation}
	Using the sampling cancellation property it thus suffices to show that
	\begin{equation}
		\tikzfig{samp-dag-com2}
	\end{equation}
	which again is equivalent to showing
	\begin{equation}\label{eq:samp-dag-com3}
		\tikzfig{samp-dag-com3}
	\end{equation}
	Now, using \Cref{cor:samp-det} we have that
	\begin{equation}
		\tikzfig{samp-dag-com4}
	\end{equation}
	For the right hand side of \Cref{eq:samp-dag-com3} we will use the fact that $f \circ \texttt{samp}$ is
	$\pi$-a.s. deterministic which implies that $(f\circ \texttt{samp}) \circ
		(f \circ \texttt{samp})^\dag =_q 1_Y$. We thus have
	\begin{equation}
		\tikzfig{samp-dag-com5}
	\end{equation}
	where the second equality follows from relative positivity. This shows that
	\Cref{eq:samp-dag-com3} holds as desired.
\end{proof}
\end{comment}

\begin{proof}%[{Proof of \Cref{thm:uni-std}}]
	Showing that there exists a partial evaluation from $\pi$ to
	$P(f^\dag_p)\circ  \delta\circ q$ is the same as showing that
	\begin{equation}\label{eq:unit}
		\pi \;\le\; P(f^\dag_p)\circ \delta\circ q
	\end{equation}
	in the order of stochastic dominance. Thus consider the experiment
	$\samp^\dag_\pi:X\to PX$, which, by	\Cref{stdmeas}, has $\pi$ as standard measure. Let's compose it with the
	map $Pf:PX\to PY$ to obtain a new experiment $Pf\circ\samp^\dag_\pi:X\to PY$, so that by definition of Blackwell order, $\samp^\dag_\pi\le
		Pf\circ\samp^\dag_\pi$. By Blackwell's theorem, to prove \eqref{eq:unit}
	it suffices to show that the standard measure of the composite experiment
	$Pf\circ\samp^\dag_\pi$ is the right-hand side of \eqref{eq:unit}. 
	We thus want to show that
	\[
		\widehat{(\texttt{samp}_{\delta \circ q}^\dag \circ f)}_p = P(f^\dag_p) \circ \delta\circ q
	\]
	which explicitly says
	\[
		\left( \left( \texttt{samp}_{\delta \circ q}^\dag \circ f \right)^\dag_p \right)^\# \circ \texttt{samp}^\dag_{\delta \circ q} \circ f \circ p = P(f^\dag_p) \circ \delta\circ q .
	\]
	We can now rewrite the left-hand side of the equation above as follows.
	\begin{align*}
		\left( \left( \texttt{samp}_{\delta \circ q}^\dag \circ f \right)^\dag_p \right)^\# \circ \texttt{samp}^\dag_{\delta \circ q} \circ f \circ p
		&= \left( f^\dag_p\circ\samp \right)^\sharp \circ \texttt{samp}^\dag_{\delta \circ q} \circ f \circ p \\
		&= \left( f^\dag_p\circ\samp \right)^\sharp \circ \texttt{samp}^\dag_{\delta \circ q} \circ q \\
		&= \left( f^\dag_p\circ\samp \right)^\sharp \circ \delta\circ q \\
		&= P\left( f^\dag_p\circ\samp \right)\circ\delta \circ \delta\circ q \\
		&= P(f^\dag_p)\circ P(\samp) \circ\delta \circ \delta\circ q \\
		&= P(f^\dag_p)\circ \mu \circ\delta \circ \delta\circ q \\
		&= P(f^\dag_p)\circ \delta\circ q ,
	\end{align*}
	which concludes the proof.
\end{proof}

Therefore, one could redefine hypernormalizations as follows, without having to rely on conditionals or Bayesian inverses:

\begin{definition}[alternative]\label{alt-def}
	Let $(\Theta,p)$ be a probability space in an a.s.-compatibly representable Markov category, and let $f:\Theta\to X$ be almost surely deterministic.
	The \textbf{hypernormalization} of $p$ with respect to $f$, if it exists, is the state $\pi:I\to P\Theta$ which
	\begin{itemize}
		\item satisfies $\samp\circ\pi=p$ (i.e.~it is a decomposition of $p$);
		\item satisfies $Pf\circ\pi=\delta\circ q$ (i.e.\ it is coarser or equal than the partition induced by $f$);
		\item is maximal in the stochastic dominance order (or minimal in the Blackwell order) among those states satisfying the two conditions above. 
	\end{itemize} 
\end{definition}

\appendix


\section{Some results about Markov categories}\label{appendix}

\begin{lemma}\label{lem:f-as-det}
	Consider the following Bayesian inversion.
	\begin{equation}\label{eq:bayesian22}
		\tikzfig{bayesian2}
	\end{equation}
	If $f$ is almost deterministic, we have that $f\circ f^\dagger_p$ is almost surely equal to the marginalization $1_Y\otimes\texttt{del}_A$ for the measure $(f\circ p,1_A)$, i.e.\ the following equation holds.
	\begin{equation}\label{eq:f-as-det}
		\tikzfig{f-as-det}
	\end{equation}
	and so, in particular, $f\circ f^\dag_p$ is almost surely deterministic.
\end{lemma}
\begin{proof}
	We have
	\[
		\tikzfig{f-as-det-proof}
	\]
	using associativity of copying, \eqref{eq:bayesian22}, almost sure determinism of $f$, and explicitly copying and discarding.
\end{proof}

\begin{lemma}\label{lem:delta-q-as-det}
	Let $\cat{C}$ be an a.s.-compatibly representable Markov category with
	conditionals. If  $p \in \cat{C}(I, X)$,
	then $\texttt{samp}_X$ is $(\delta_X \circ p)$-a.s.
	deterministic.
\end{lemma}
\begin{proof}
	We want to show that the following equality holds
	\begin{equation}\label{eq:delta-q-as-det1}
		\tikzfig{delta-q-as-det1}
	\end{equation}
	Using that $\delta_X$ is deterministic we get that the left hand side
	of \Cref{eq:delta-q-as-det1} can be written as
	\begin{equation}\label{eq:delta-q-as-det2}
		\tikzfig{delta-q-as-det2}
	\end{equation}
	Similarly, the right hand side of \Cref{eq:delta-q-as-det1} can be
	written as
	\begin{equation}\label{eq:delta-q-as-det3}
		\tikzfig{delta-q-as-det3}
	\end{equation}
	Hence \Cref{eq:delta-q-as-det1} holds and we are done.
\end{proof}

\begin{corollary}\label{cor:samp-det}
	Let $\cat{C}$ be an a.s.-compatibly representable Markov category with
	conditionals. Then, for all $p \in \cat{C}(I, X)$ we have
	\begin{equation}
		\texttt{samp} \circ \texttt{samp}_{\delta \circ p}^\dag =_p 1_X.
	\end{equation}
\end{corollary}

\begin{comment}
A Markov category is called \textbf{causal} if for all $f,g,h_1$ and $h_2$ as below, the following implication holds,
\begin{equation}\label{eq:causality}
	\tikzfig{causality}
\end{equation}
which can be seen as a strengthening of almost sure equality.
Every Markov category with conditionals is causal \cite[Proposition 11.34]{Fritz-2020-Asyntheticapproachto}.
\end{comment}

\bibliographystyle{alpha}
\bibliography{./refs-paper}

\end{document}
