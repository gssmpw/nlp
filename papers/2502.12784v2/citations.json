[
  {
    "index": 0,
    "papers": [
      {
        "key": "kitaev2020reformer",
        "author": "Kitaev, Nikita and Kaiser, {\\L}ukasz and Levskaya, Anselm",
        "title": "Reformer: The efficient transformer"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "xFormers2022",
        "author": "Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza and Luca Wehrstedt and Jeremy Reizenstein and Grigory Sizov",
        "title": "xFormers: A modular and hackable Transformer modelling library"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zaheer2020big",
        "author": "Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others",
        "title": "Big bird: Transformers for longer sequences"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wolf-etal-2020-transformers",
        "author": "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
        "title": "Transformers: State-of-the-Art Natural Language Processing"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ying2021lazyformer",
        "author": "Ying, Chengxuan and Ke, Guolin and He, Di and Liu, Tie-Yan",
        "title": "Lazyformer: Self attention with lazy update"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "fan2019reducing",
        "author": "Fan, Angela and Grave, Edouard and Joulin, Armand",
        "title": "Reducing transformer depth on demand with structured dropout"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2020accelerating",
        "author": "Zhang, Minjia and He, Yuxiong",
        "title": "Accelerating training of transformer-based language models with progressive layer dropping"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "choromanski2020rethinking",
        "author": "Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others",
        "title": "Rethinking attention with performers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2019variance",
        "author": "Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei",
        "title": "On the variance of the adaptive learning rate and beyond"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gong2019efficient",
        "author": "Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan",
        "title": "Efficient training of bert by progressively stacking"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2020shallow",
        "author": "Li, Bei and Wang, Ziyang and Liu, Hui and Jiang, Yufan and Du, Quan and Xiao, Tong and Wang, Huizhen and Zhu, Jingbo",
        "title": "Shallow-to-deep training for neural machine translation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yao2021adahessian",
        "author": "Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael",
        "title": "Adahessian: An adaptive second order optimizer for machine learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhang2020fixed",
        "author": "Zhang, Xishan and Liu, Shaoli and Zhang, Rui and Liu, Chang and Huang, Di and Zhou, Shiyi and Guo, Jiaming and Guo, Qi and Du, Zidong and Zhi, Tian and others",
        "title": "Fixed-point back-propagation training"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "sun2019hybrid",
        "author": "Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi Viji and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash",
        "title": "Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "nvidia2020apex",
        "author": "NVIDIA",
        "title": "NVIDIA Apex"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "rasley2020deepspeed",
        "author": "Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong",
        "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "fang2021turbotransformers",
        "author": "Fang, Jiarui and Yu, Yang and Zhao, Chengduo and Zhou, Jie",
        "title": "Turbotransformers: an efficient gpu serving system for transformer models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "yu2022orca",
        "author": "Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon",
        "title": "Orca: A distributed serving system for $\\{$Transformer-Based$\\}$ generative models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "kwon2023efficient",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "Efficient memory management for large language model serving with pagedattention"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "chen2018tvm",
        "author": "Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others",
        "title": "$\\{$TVM$\\}$: An automated $\\{$End-to-End$\\}$ optimizing compiler for deep learning"
      }
    ]
  }
]