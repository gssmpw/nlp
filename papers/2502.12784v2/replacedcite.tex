\section{Related Work}
\label{sec6}

\textbf{Model architecture.} Works such as Reformer____, Longformer____, xformers____, and Big Bird____ utilize sparse attention to transform the training computation complexity from the previous $O(N^2)$ to $O(N)$. 
Without compromising model accuracy, this approach alleviates the issue of excessively long training times to some extent.  
These methods transform two matrix multiplications into one Sampled-Dense-Dense Matrix Multiplication (SDDMM) and one Sparse Matrix-Matrix Multiplication (SpMM) with high sparsity, aiming to achieve shorter computation times. 
These works have experimentally demonstrated that approximate attention does not significantly impact the accuracy of the models.
Sparse attention methods only consider computational efficiency. 
However, in most training processes, I/O is the bottleneck for overall runtime speed. 
Besides, in some implementations (Big Bird in Hugging Face____) of Sparse attention models, Sparse attention computation is achieved by calling multiple GeMM operations instead of using a single SDDMM operator. 
This inefficient implementation often results in less noticeable improvements in training time.

In addition, Ying et al.____ proposed Lazyformer that improves computational efficiency by reducing the frequency of computing self-attention distributions. 
In each Lazy block, only the first layer computes the self-attention distribution, which is then reused by subsequent layers, significantly reducing computational costs. 
Fan et al.____ proposed using structured dropout techniques to dynamically reduce the depth of Transformer models. 
Zhang et al.____ proposed using progressive layer dropping techniques to gradually reduce the number of layers in the model during training, thereby reducing computational costs and accelerating training speed without compromising model performance and accuracy. 
Choromanski et al.____ use low-rank approximation methods to replace traditional self-attention mechanisms.

\noindent\textbf{Training strategy.} Liu et al.____ proposed RAdam and demonstrated that the warm-up strategy for learning rates can effectively reduce variance, thereby stabilizing the training process, accelerating convergence, and improving generalization performance. 
Gong et al.____ and Li et al.____ proposed accelerating training by transferring knowledge between shallow and deep models, and progressively applying stacking.

\noindent\textbf{Optimization algorithms.} Yao et al.____ proposed the adaptive second-order optimization algorithm AdaHessian, which improves the optimization process by dynamically estimating the Hessian matrix of the loss function.

\noindent\textbf{Data precision.} Zhang et al.____ proposed a novel training approach that addresses significant accuracy loss issues caused by direct quantization in deep neural networks, by applying layer-wise precision-adaptive quantization. 
Sun et al.____ use a hybrid 8-bit floating point for training and inference of deep neural networks.

In addition to the above, Nvidia Apex____ supports automatic mixed precision training and distributed training without compromising computational accuracy;
DeepSpeed____ integrates small kernels such as \textit{LayerNorm} and \textit{Softmax} into the encoder; 
TurboTransformers____, Orca____, and pagedattention____ are mainly used to serve the inference phase of Transformers;
TVM____ automatically generates kernels through compilation and achieves graph-level and operator-level optimization in deep learning computations.