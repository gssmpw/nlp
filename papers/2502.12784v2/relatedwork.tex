\section{Related Work}
\label{sec6}

\textbf{Model architecture.} Works such as Reformer\cite{kitaev2020reformer}, Longformer\cite{beltagy2020longformer}, xformers\cite{xFormers2022}, and Big Bird\cite{zaheer2020big} utilize sparse attention to transform the training computation complexity from the previous $O(N^2)$ to $O(N)$. 
Without compromising model accuracy, this approach alleviates the issue of excessively long training times to some extent.  
These methods transform two matrix multiplications into one Sampled-Dense-Dense Matrix Multiplication (SDDMM) and one Sparse Matrix-Matrix Multiplication (SpMM) with high sparsity, aiming to achieve shorter computation times. 
These works have experimentally demonstrated that approximate attention does not significantly impact the accuracy of the models.
Sparse attention methods only consider computational efficiency. 
However, in most training processes, I/O is the bottleneck for overall runtime speed. 
Besides, in some implementations (Big Bird in Hugging Face\cite{wolf-etal-2020-transformers}) of Sparse attention models, Sparse attention computation is achieved by calling multiple GeMM operations instead of using a single SDDMM operator. 
This inefficient implementation often results in less noticeable improvements in training time.

In addition, Ying et al.\cite{ying2021lazyformer} proposed Lazyformer that improves computational efficiency by reducing the frequency of computing self-attention distributions. 
In each Lazy block, only the first layer computes the self-attention distribution, which is then reused by subsequent layers, significantly reducing computational costs. 
Fan et al.\cite{fan2019reducing} proposed using structured dropout techniques to dynamically reduce the depth of Transformer models. 
Zhang et al.\cite{zhang2020accelerating} proposed using progressive layer dropping techniques to gradually reduce the number of layers in the model during training, thereby reducing computational costs and accelerating training speed without compromising model performance and accuracy. 
Choromanski et al.\cite{choromanski2020rethinking} use low-rank approximation methods to replace traditional self-attention mechanisms.

\noindent\textbf{Training strategy.} Liu et al.\cite{liu2019variance} proposed RAdam and demonstrated that the warm-up strategy for learning rates can effectively reduce variance, thereby stabilizing the training process, accelerating convergence, and improving generalization performance. 
Gong et al.\cite{gong2019efficient} and Li et al.\cite{li2020shallow} proposed accelerating training by transferring knowledge between shallow and deep models, and progressively applying stacking.

\noindent\textbf{Optimization algorithms.} Yao et al.\cite{yao2021adahessian} proposed the adaptive second-order optimization algorithm AdaHessian, which improves the optimization process by dynamically estimating the Hessian matrix of the loss function.

\noindent\textbf{Data precision.} Zhang et al.\cite{zhang2020fixed} proposed a novel training approach that addresses significant accuracy loss issues caused by direct quantization in deep neural networks, by applying layer-wise precision-adaptive quantization. 
Sun et al.\cite{sun2019hybrid} use a hybrid 8-bit floating point for training and inference of deep neural networks.

In addition to the above, Nvidia Apex\cite{nvidia2020apex} supports automatic mixed precision training and distributed training without compromising computational accuracy;
DeepSpeed\cite{rasley2020deepspeed} integrates small kernels such as \textit{LayerNorm} and \textit{Softmax} into the encoder; 
TurboTransformers\cite{fang2021turbotransformers}, Orca\cite{yu2022orca}, and pagedattention\cite{kwon2023efficient} are mainly used to serve the inference phase of Transformers;
TVM\cite{chen2018tvm} automatically generates kernels through compilation and achieves graph-level and operator-level optimization in deep learning computations.