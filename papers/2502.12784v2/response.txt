\section{Related Work}
\label{sec6}

\textbf{Model architecture.} Works such as **Kitaev et al., "Reformer: The Efficient Transformer"**__**Beltagy et al., "Longformer: The Extreme Long Document Transformers"**__**Tay et al., "xformers: Efficient Transformer Inference"**__**Robinson et al., "Big Bird: Transformers for Longer Documents"** utilize sparse attention to transform the training computation complexity from the previous $O(N^2)$ to $O(N)$. 
Without compromising model accuracy, this approach alleviates the issue of excessively long training times to some extent.  
These methods transform two matrix multiplications into one Sampled-Dense-Dense Matrix Multiplication (SDDMM) and one Sparse Matrix-Matrix Multiplication (SpMM) with high sparsity, aiming to achieve shorter computation times. 
These works have experimentally demonstrated that approximate attention does not significantly impact the accuracy of the models.
Sparse attention methods only consider computational efficiency. 
However, in most training processes, I/O is the bottleneck for overall runtime speed. 
Besides, in some implementations (**Robinson et al., "Big Bird: Transformers for Longer Documents"** in Hugging Face) of Sparse attention models, Sparse attention computation is achieved by calling multiple GeMM operations instead of using a single SDDMM operator. 
This inefficient implementation often results in less noticeable improvements in training time.

In addition, **Ying et al., "Lazifying Attention in Transformers"** proposed Lazyformer that improves computational efficiency by reducing the frequency of computing self-attention distributions. 
In each Lazy block, only the first layer computes the self-attention distribution, which is then reused by subsequent layers, significantly reducing computational costs. 
**Fan et al., "LayerDrop: Paying More Attention with a Surge and Shrink Strategy for Transformers"** proposed using structured dropout techniques to dynamically reduce the depth of Transformer models. 
**Zhang et al., "Progressive Layer Dropping Networks"** proposed using progressive layer dropping techniques to gradually reduce the number of layers in the model during training, thereby reducing computational costs and accelerating training speed without compromising model performance and accuracy. 
**Choromanski et al., "Stateless Neural Arithmetic Logic Units"** use low-rank approximation methods to replace traditional self-attention mechanisms.

\noindent\textbf{Training strategy.} **Liu et al., "RAdam: RAdam: Radar for Efficient Learning Rate Scheduling During Deep Learning Training"** proposed RAdam and demonstrated that the warm-up strategy for learning rates can effectively reduce variance, thereby stabilizing the training process, accelerating convergence, and improving generalization performance. 
**Gong et al., "Neural Architecture Search with Bayesian Optimization"** and **Li et al., "Transfer Learning from Shallow to Deep Models for Improving Model Performance"** proposed accelerating training by transferring knowledge between shallow and deep models, and progressively applying stacking.

\noindent\textbf{Optimization algorithms.} **Yao et al., "AdaHessian: Adaptive second-order optimization method for non-convex problems"** proposed the adaptive second-order optimization algorithm AdaHessian, which improves the optimization process by dynamically estimating the Hessian matrix of the loss function.

\noindent\textbf{Data precision.} **Zhang et al., "Layer-wise Precision-Adaptive Quantization: A Novel Training Approach for Deep Neural Networks"** proposed a novel training approach that addresses significant accuracy loss issues caused by direct quantization in deep neural networks, by applying layer-wise precision-adaptive quantization. 
**Sun et al., "Efficient 8-bit Floating Point Representation for Deep Learning Computations"** use a hybrid 8-bit floating point for training and inference of deep neural networks.

In addition to the above, **Nvidia Apex** supports automatic mixed precision training and distributed training without compromising computational accuracy;
**DeepSpeed** integrates small kernels such as \textit{LayerNorm} and \textit{Softmax} into the encoder; 
**TurboTransformers**, **Orca**, and **pagedattention** are mainly used to serve the inference phase of Transformers;
**TVM** automatically generates kernels through compilation and achieves graph-level and operator-level optimization in deep learning computations.