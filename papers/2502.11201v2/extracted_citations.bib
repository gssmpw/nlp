@article{10.1007/s10796-022-10295-0,
author = {Ni, Pin and Okhrati, Ramin and Guan, Steven and Chang, Victor},
title = {Knowledge Graph and Deep Learning-based Text-to-GraphQL Model for Intelligent Medical Consultation Chatbot},
year = {2022},
issue_date = {Feb 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-022-10295-0},
doi = {10.1007/s10796-022-10295-0},
abstract = {Text-to-GraphQL (Text2GraphQL) is a task that converts the user's questions into Graph + QL (Query Language) when a graph database is given. That is a task of semantic parsing that transforms natural language problems into logical expressions, which will bring more efficient direct communication between humans and machines. The existing related work mainly focuses on Text-to-SQL tasks, and there is no available semantic parsing method and data set for the graph database. In order to fill the gaps in this field to serve the medical Human–Robot Interactions (HRI) better, we propose this task and a pipeline solution for the Text2GraphQL task. This solution uses the Adapter pre-trained by “the linking of GraphQL schemas and the corresponding utterances” as an external knowledge introduction plug-in. By inserting the Adapter into the language model, the mapping between logical language and natural language can be introduced faster and more directly to better realize the end-to-end human–machine language translation task. In the study, the proposed Text2GraphQL task model is mainly constructed based on an improved pipeline composed of a Language Model, Pre-trained Adapter plug-in, and Pointer Network. This enables the model to copy objects' tokens from utterances, generate corresponding GraphQL statements for graph database retrieval, and builds an adjustment mechanism to improve the final output. And the experiments have proved that our proposed method has certain competitiveness on the counterpart datasets (Spider, ATIS, GeoQuery, and 39.net) converted from the Text2SQL task, and the proposed method is also practical in medical scenarios.},
journal = {Information Systems Frontiers},
month = {jul},
pages = {137–156},
numpages = {20},
keywords = {Text-to-GraphQL, Semantic parsing, Knowledge graph, Natural language processing, Deep learning, Health informatics}
}

@inproceedings{10.1109/TrustCom.2011.70,
author = {Okman, Lior and Gal-Oz, Nurit and Gonen, Yaron and Gudes, Ehud and Abramov, Jenny},
title = {Security Issues in NoSQL Databases},
year = {2011},
isbn = {9780769546001},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/TrustCom.2011.70},
doi = {10.1109/TrustCom.2011.70},
abstract = {applications has created the need to store large amount of data in distributed databases that provide high availability and scalability. In recent years, a growing number of companies have adopted various types of non-relational databases, commonly referred to as NoSQL databases, and as the applications they serve emerge, they gain extensive market interest. These new database systems are not relational by definition and therefore they do not support full SQL functionality. Moreover, as opposed to relational databases they trade consistency and security for performance and scalability. As increasingly sensitive data is being stored in NoSQL databases, security issues become growing concerns. This paper reviews two of the most popular NoSQL databases (Cassandra and MongoDB) and outlines their main security features and problems.},
booktitle = {Proceedings of the 2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications},
pages = {541–547},
numpages = {7},
keywords = {Security, NoSQL, MongoDB, Cassandra},
series = {TRUSTCOM '11}
}

@article{10.1145/1978915.1978919,
author = {Cattell, Rick},
title = {Scalable SQL and NoSQL data stores},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/1978915.1978919},
doi = {10.1145/1978915.1978919},
abstract = {In this paper, we examine a number of SQL and socalled "NoSQL" data stores designed to scale simple OLTP-style application loads over many servers. Originally motivated by Web 2.0 applications, these systems are designed to scale to thousands or millions of users doing updates as well as reads, in contrast to traditional DBMSs and data warehouses. We contrast the new systems on their data model, consistency mechanisms, storage mechanisms, durability guarantees, availability, query support, and other dimensions. These systems typically sacrifice some of these dimensions, e.g. database-wide transaction consistency, in order to achieve others, e.g. higher availability and scalability.},
journal = {SIGMOD Rec.},
month = {may},
pages = {12–27},
numpages = {16}
}

@inproceedings{10.1145/3318464.3389776,
author = {Baik, Christopher and Jin, Zhongjun and Cafarella, Michael and Jagadish, H. V.},
title = {Duoquest: A Dual-Specification System for Expressive SQL Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389776},
doi = {10.1145/3318464.3389776},
abstract = {Querying a relational database is difficult because it requires users to be familiar with both the SQL language and the schema. However, many users possess enough domain expertise to describe their desired queries by alternative means. For such users, two major alternatives to writing SQL are natural language interfaces (NLIs) and programming-by-example (PBE). Both of these alternatives face certain pitfalls: natural language queries (NLQs) are often ambiguous, even for human interpreters, while current PBE approaches limit functionality to be tractable. Consequently, we propose dual-specification query synthesis, which consumes both a NLQ and an optional PBE-like table sketch query that enables users to express varied levels of domain knowledge. We introduce the novel dual-specification Duoquest system, which leverages guided partial query enumeration to efficiently explore the space of possible queries. We present results from user studies in which Duoquest demonstrates a 62.5\% absolute increase in query construction accuracy over a state-of-the-art NLI and comparable accuracy to a PBE system on a limited workload supported by the PBE system. In a simulation study on the Spider benchmark, Duoquest demonstrates a >2x increase in top-1 accuracy over both NLI and PBE.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2319–2329},
numpages = {11},
keywords = {query by example, program synthesis, dual-specification interface, database usability, SQL},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3534678.3539305,
author = {Wang, Lihan and Qin, Bowen and Hui, Binyuan and Li, Bowen and Yang, Min and Wang, Bailin and Li, Binhua and Sun, Jian and Huang, Fei and Si, Luo and Li, Yongbin},
title = {Proton: Probing Schema Linking Information from Pre-trained Language Models for Text-to-SQL Parsing},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539305},
doi = {10.1145/3534678.3539305},
abstract = {The importance of building text-to-SQL parsers which can be applied to new databases has long been acknowledged, and a critical step to achieve this goal is schema linking, i.e., properly recognizing mentions of unseen columns or tables when generating SQLs. In this work, we propose a novel framework to elicit relational structures from large-scale pre-trained language models (PLMs) via a probing procedure based on Poincar\'{e} distance metric, and use the induced relations to augment current graph-based parsers for better schema linking. Compared with commonly-used rule-based methods for schema linking, we found that probing relations can robustly capture semantic correspondences, even when surface forms of mentions and entities differ. Moreover, our probing procedure is entirely unsupervised and requires no additional parameters. Extensive experiments show that our framework sets new state-of-the-art performance on three benchmarks. We empirically verify that our probing procedure can indeed find desired relational structures through qualitative analysis.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1889–1898},
numpages = {10},
keywords = {knowledge probing, semantic parsing, text-to-sql parsing},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3589292,
author = {Gu, Zihui and Fan, Ju and Tang, Nan and Cao, Lei and Jia, Bowen and Madden, Sam and Du, Xiaoyong},
title = {Few-shot Text-to-SQL Translation using Structure and Content Prompt Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589292},
doi = {10.1145/3589292},
abstract = {A common problem with adopting Text-to-SQL translation in database systems is poor generalization. Specifically, when there is limited training data on new datasets, existing few-shot Text-to-SQL techniques, even with carefully designed textual prompts on pre-trained language models (PLMs), tend to be ineffective. In this paper, we present a divide-and-conquer framework to better support few-shot Text-to-SQL translation, which divides Text-to-SQL translation into two stages (or sub-tasks), such that each sub-task is simpler to be tackled. The first stage, called the structure stage, steers a PLM to generate an SQL structure (including SQL commands such as SELECT, FROM, WHERE and SQL operators such as <", ?>") with placeholders for missing identifiers. The second stage, called the content stage, guides a PLM to populate the placeholders in the generated SQL structure with concrete values (including SQL identifies such as table names, column names, and constant values). We propose a hybrid prompt strategy that combines learnable vectors and fixed vectors (i.e., word embeddings of textual prompts), such that the hybrid prompt can learn contextual information to better guide PLMs for prediction in both stages. In addition, we design keyword constrained decoding to ensure the validity of generated SQL structures, and structure guided decoding to guarantee the model to fill correct content. Extensive experiments, by comparing with ten state-of-the-art Text-to-SQL solutions at the time of writing, show that SC-Prompt significantly outperforms them in the few-shot scenario. In particular, on the widely-adopted Spider dataset, given less than 500 labeled training examples (5\% of the official training set), SC-Prompt outperforms the previous SOTA methods by around 5\% on accuracy.},
journal = {Proc. ACM Manag. Data},
month = {jun},
articleno = {147},
numpages = {28},
keywords = {pre-trained language model, prompt learning, text-to-SQL}
}

@article{10.1145/3654930,
author = {Li, Haoyang and Zhang, Jing and Liu, Hanbing and Fan, Ju and Zhang, Xiaokang and Zhu, Jun and Wei, Renjie and Pan, Hongyan and Li, Cuiping and Chen, Hong},
title = {CodeS: Towards Building Open-source Language Models for Text-to-SQL},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654930},
doi = {10.1145/3654930},
abstract = {Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid domain adaptation through strategic prompt construction and a bi-directional data augmentation technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging text-to-SQL benchmarks.},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {127},
numpages = {28},
keywords = {language model, natural language interface for databases, text-to-SQL}
}

@article{10.14778/3407790.3407858,
author = {Sen, Jaydeep and Lei, Chuan and Quamar, Abdul and \"{O}zcan, Fatma and Efthymiou, Vasilis and Dalmia, Ayushi and Stager, Greg and Mittal, Ashish and Saha, Diptikalyan and Sankaranarayanan, Karthik},
title = {ATHENA++: natural language querying for complex nested SQL queries},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407858},
doi = {10.14778/3407790.3407858},
abstract = {Natural Language Interfaces to Databases (NLIDB) systems eliminate the requirement for an end user to use complex query languages like SQL, by translating the input natural language (NL) queries to SQL automatically. Although a significant volume of research has focused on this space, most state-of-the-art systems can at best handle simple select-project-join queries. There has been little to no research on extending the capabilities of NLIDB systems to handle complex business intelligence (BI) queries that often involve nesting as well as aggregation. In this paper, we present Athena++, an end-to-end system that can answer such complex queries in natural language by translating them into nested SQL queries. In particular, Athena++ combines linguistic patterns from NL queries with deep domain reasoning using ontologies to enable nested query detection and generation. We also introduce a new benchmark data set (FIBEN), which consists of 300 NL queries, corresponding to 237 distinct complex SQL queries on a database with 152 tables, conforming to an ontology derived from standard financial ontologies (FIBO and FRO). We conducted extensive experiments comparing Athena++ with two state-of-the-art NLIDB systems, using both FIBEN and the prominent Spider benchmark. Athena++ consistently outperforms both systems across all benchmark data sets with a wide variety of complex queries, achieving 88.33\% accuracy on FIBEN benchmark, and 78.89\% accuracy on Spider benchmark, beating the best reported accuracy results on the dev set by 8\%.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2747–2759},
numpages = {13}
}

@article{10.14778/3641204.3641221,
author = {Gao, Dawei and Wang, Haibin and Li, Yaliang and Sun, Xiuyu and Qian, Yichen and Ding, Bolin and Zhou, Jingren},
title = {Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation},
year = {2024},
issue_date = {January 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3641204.3641221},
doi = {10.14778/3641204.3641221},
abstract = {Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6\% execution accuracy and sets a new bar.To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.},
journal = {Proc. VLDB Endow.},
month = {may},
pages = {1132–1145},
numpages = {14}
}

@inproceedings{10.1609/aaai.v37i11.26535,
author = {Li, Haoyang and Zhang, Jing and Li, Cuiping and Chen, Hong},
title = {RESDSQL: decoupling schema linking and skeleton parsing for text-to-SQL},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i11.26535},
doi = {10.1609/aaai.v37i11.26535},
abstract = {One of the recent best attempts at Text-to-SQL is the pre-trained language model. Due to the structural property of the SQL queries, the seq2seq model takes the responsibility of parsing both the schema items (i.e., tables and columns) and the skeleton (i.e., SQL keywords). Such coupled targets increase the difficulty of parsing the correct SQL queries especially when they involve many schema items and logic operators. This paper proposes a ranking-enhanced encoding and skeleton-aware decoding framework to decouple the schema linking and the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its encoder is injected by the most relevant schema items instead of the whole unordered ones, which could alleviate the schema linking effort during SQL parsing, and its decoder first generates the skeleton and then the actual SQL query, which could implicitly constrain the SQL parsing. We evaluate our proposed framework on Spider and its three robustness variants: Spider-DK, Spider-Syn, and Spider-Realistic. The experimental results show that our framework delivers promising performance and robustness.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1466},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{10.1609/aaai.v37i11.26536,
author = {Li, Jinyang and Hui, Binyuan and Cheng, Reynold and Qin, Bowen and Ma, Chenhao and Huo, Nan and Huang, Fei and Du, Wenyu and Si, Luo and Li, Yongbin},
title = {Graphix-T5: mixing pre-trained transformers with graph-aware layers for text-to-SQL parsing},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i11.26536},
doi = {10.1609/aaai.v37i11.26536},
abstract = {The task of text-to-SQL parsing, which aims at converting natural language questions into executable SQL queries, has garnered increasing attention in recent years. One of the major challenges in text-to-SQL parsing is domain generalization, i.e., how to generalize well to unseen databases. Recently, the pre-trained text-to-text transformer model, namely T5, though not specialized for text-to-SQL parsing, has achieved state-of-the-art performance on standard benchmarks targeting domain generalization. In this work, we explore ways to further augment the pre-trained T5 model with specialized components for text-to-SQL parsing. Such components are expected to introduce structural inductive bias into text-to-SQL parsers thus improving model's capacity on (potentially multi-hop) reasoning, which is critical for generating structure-rich SQLs. To this end, we propose a new architecture GRAPHIX-T5, a mixed model with the standard pre-trained transformer model augmented by specially-designed graph-aware layers. Extensive experiments and analysis demonstrate the effectiveness of GRAPHIX-T5 across four text-to-SQL benchmarks: SPIDER, SYN, REALISTIC and DK. GRAPHIX-T5 surpass all other T5-based parsers with a significant margin, achieving new state-of-the-art performance. Notably, GRAPHIX-T5-large reaches performance superior to the original T5-large by 5.7\% on exact match (EM) accuracy and 6.6\% on execution accuracy (EX). This even outperforms the T5-3B by 1.2\% on EM and 1.5\% on EX.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1467},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{10.5555/3304222.3304323,
author = {Cai, Ruichu and Xu, Boyan and Zhang, Zhenjie and Yang, Xiaoyan and Li, Zijian and Liang, Zhihao},
title = {An encoder-decoder framework translating natural language to database queries},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Machine translation is going through a radical revolution, driven by the explosive development of deep learning techniques using Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). In this paper, we consider a special case in machine translation problems, targeting to convert natural language into Structured Query Language (SQL) for data retrieval over relational database. Although generic CNN and RNN learn the grammar structure of SQL when trained with sufficient samples, the accuracy and training efficiency of the model could be dramatically improved, when the translation model is deeply integrated with the grammar rules of SQL. We present a new encoder-decoder framework, with a suite of new approaches, including new semantic features fed into the encoder, grammar-aware states injected into the memory of decoder, as well as recursive state management for sub-queries. These techniques help the neural network better focus on understanding semantics of operations in natural language and save the efforts on SQL grammar learning. The empirical evaluation on real world database and queries show that our approach outperform state-of-the-art solution by a significant margin.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {3977–3983},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@INPROCEEDINGS{10191914,
  author={Liu, Hu and Shi, Yuliang and Zhang, Jianlin and Wang, Xinjun and Li, Hui and Kong, Fanyu},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Multi-hop Relational Graph Attention Network for Text-to-SQL Parsing}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  keywords={Structured Query Language;Databases;Network topology;Neural networks;Natural languages;Semantics;Spread spectrum communication;Natural language processing;Text-to-SQL;Semantic parsing;SQL;Graph neural network},
  doi={10.1109/IJCNN54540.2023.10191914}}

@article{DBS-078,
url = {http://dx.doi.org/10.1561/1900000078},
year = {2022},
volume = {11},
journal = {Foundations and Trends® in Databases},
title = {Natural Language Interfaces to Data},
doi = {10.1561/1900000078},
issn = {1931-7883},
number = {4},
pages = {319-414},
author = {Abdul Quamar and Vasilis Efthymiou and Chuan Lei and Fatma Özcan}
}

@article{MAHAJAN2019120,
title = {Improving the energy efficiency of relational and NoSQL databases via query optimizations},
journal = {Sustainable Computing: Informatics and Systems},
volume = {22},
pages = {120-133},
year = {2019},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S2210537918301112},
author = {Divya Mahajan and Cody Blakeney and Ziliang Zong},
keywords = {Energy-efficiency, Relational databases, NoSQL databases, MySQL, MongoDB, Cassandra, DVFS},
abstract = {As big data becomes the norm of various industrial applications, the complexity of database workloads and database system design has increased significantly. To address these challenges, conventional relational databases have been constantly improved and NoSQL databases such as MongoDB and Cassandra have been proposed and implemented to compete with SQL databases. In addition to traditional metrics such as response time, throughput, and capacity, modern database systems are posing higher requirements on energy efficiency due to the large volume of data that need to be stored, queried, updated, and analyzed. While decades of research in the database and data processing communities has produced a wealth of literature that optimize for performance, research on optimizations for energy efficiency has been historically overlooked and only a few studies have investigated the energy efficiency of database systems. To the best our knowledge, there are currently no comprehensive studies that analyze the impact of query optimizations on performance and energy efficiency across both relational and NoSQL databases. In fact, the energy behavior of many basic database operations (e.g. insertion, deletion, searching, update, indexing, etc) remains largely unknown due to the lack of accurate power measurement methodologies for various databases and queries. In this paper, we investigate a series of query optimization techniques for improving the energy-efficiency of relational databases and NoSQL databases. We use both widely acceptable benchmarks (e.g. Yahoo! Cloud Server Benchmark) and customized datasets (converted from ˜100GB of Twitter data) in our experiments to evaluate the effectiveness of various optimization techniques. We conduct cross database analysis on relational database (MySQL) and NoSQL based databases (MongoDB and Cassandra) to compare their performance and energy efficiency. Additionally, we study a variety of optimization techniques that can improve energy efficiency without compromising performance on the databases derived from the Twitter data. Using these techniques, we are able to achieve significant energy savings without performance degradation. Moreover, we investigate the impact of Dynamic Voltage and Frequency Scaling (DVFS) on the performance and energy efficiency of MySQL, MongoDB and Cassandra.}
}

@inproceedings{NEURIPS2023_72223cc6,
 author = {Pourreza, Mohammadreza and Rafiei, Davood},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {36339--36348},
 publisher = {Curran Associates, Inc.},
 title = {DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/72223cc66f63ca1aa59edaec1b3670e6-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{NEURIPS2023_83fc8fab,
 author = {Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Geng, Ruiying and Huo, Nan and Zhou, Xuanhe and Chenhao, Ma and Li, Guoliang and Chang, Kevin and Huang, Fei and Cheng, Reynold and Li, Yongbin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {42330--42357},
 publisher = {Curran Associates, Inc.},
 title = {Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/83fc8fab1710363050bbd1d4b8cc0021-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@misc{dong2023c3zeroshottexttosqlchatgpt,
      title={C3: Zero-shot Text-to-SQL with ChatGPT}, 
      author={Xuemei Dong and Chao Zhang and Yuhang Ge and Yuren Mao and Yunjun Gao and lu Chen and Jinshu Lin and Dongfang Lou},
      year={2023},
      eprint={2307.07306},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.07306}, 
}

@inproceedings{guo-etal-2019-towards,
    title = "Towards Complex Text-to-{SQL} in Cross-Domain Database with Intermediate Representation",
    author = "Guo, Jiaqi  and
      Zhan, Zecheng  and
      Gao, Yan  and
      Xiao, Yan  and
      Lou, Jian-Guang  and
      Liu, Ting  and
      Zhang, Dongmei",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1444",
    doi = "10.18653/v1/P19-1444",
    pages = "4524--4535",
    abstract = "We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) and the implementation details in SQL; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a SQL query, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a database schema. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL. Finally, IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7{\%} accuracy, obtaining 19.5{\%} absolute improvement over previous state-of-the-art approaches. At the time of writing, IRNet achieves the first position on the Spider leaderboard.",
}

@inproceedings{hui-etal-2022-s2sql,
    title = "{S}$^2${SQL}: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-{SQL} Parsers",
    author = "Hui, Binyuan  and
      Geng, Ruiying  and
      Wang, Lihan  and
      Qin, Bowen  and
      Li, Yanyang  and
      Li, Bowen  and
      Sun, Jian  and
      Li, Yongbin",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.99",
    doi = "10.18653/v1/2022.findings-acl.99",
    pages = "1254--1262",
    abstract = "The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing. The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well. In this paper, we propose S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectively leverages the syntactic dependency information of questions in text-to-SQL to improve the performance. We also employ the decoupling constraint to induce diverse relational edge embedding, which further improves the network{'}s performance. Experiments on the Spider and robustness setting Spider-Syn demonstrate that the proposed approach outperforms all existing methods when pre-training models are used, resulting in a performance ranks first on the Spider leaderboard.",
}

@inproceedings{lee-etal-2021-kaggledbqa,
    title = "{K}aggle{DBQA}: Realistic Evaluation of Text-to-{SQL} Parsers",
    author = "Lee, Chia-Hsuan  and
      Polozov, Oleksandr  and
      Richardson, Matthew",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.176",
    doi = "10.18653/v1/2021.acl-long.176",
    pages = "2261--2273",
    abstract = "The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2{\%}, doubling their performance.",
}

@inproceedings{nan-etal-2023-enhancing,
    title = "Enhancing Text-to-{SQL} Capabilities of Large Language Models: A Study on Prompt Design Strategies",
    author = "Nan, Linyong  and
      Zhao, Yilun  and
      Zou, Weijin  and
      Ri, Narutatsu  and
      Tae, Jaesung  and
      Zhang, Ellen  and
      Cohan, Arman  and
      Radev, Dragomir",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.996",
    doi = "10.18653/v1/2023.findings-emnlp.996",
    pages = "14935--14956",
    abstract = "In-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method to question answering tasks that utilize structured knowledge sources, and improve Text-to-SQL systems by exploring various prompt design strategies for employing LLMs. We conduct a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task. Our approach involves leveraging the syntactic structure of an example{'}s SQL query to retrieve demonstrations, and we demonstrate that pursuing both diversity and similarity in demonstration selection leads to enhanced performance. Furthermore, we show that LLMs benefit from database-related knowledge augmentations. Our most effective strategy outperforms the state-of-the-art system by 2.5 points (Execution Accuracy) and the best fine-tuned system by 5.1 points on the Spider dataset. These results highlight the effectiveness of our approach in adapting LLMs to the Text-to-SQL task, and we present an analysis of the factors contributing to the success of our strategy.",
}

@inproceedings{popescu-etal-2022-addressing,
    title = "Addressing Limitations of Encoder-Decoder Based Approach to Text-to-{SQL}",
    author = "Popescu, Octavian  and
      Manotas, Irene  and
      Vo, Ngoc Phuoc An  and
      Yeo, Hangu  and
      Khorashani, Elahe  and
      Sheinin, Vadim",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.137",
    pages = "1593--1603",
    abstract = "Most attempts on Text-to-SQL task using encoder-decoder approach show a big problem of dramatic decline in performance for new databases. For the popular Spider dataset, despite models achieving 70{\%} accuracy on its development or test sets, the same models show a huge decline below 20{\%} accuracy for unseen databases. The root causes for this problem are complex and they cannot be easily fixed by adding more manually created training. In this paper we address the problem and propose a solution that is a hybrid system using automated training-data augmentation technique. Our system consists of a rule-based and a deep learning components that interact to understand crucial information in a given query and produce correct SQL as a result. It achieves double-digit percentage improvement for databases that are not part of the Spider corpus.",
}

@inproceedings{qi-etal-2022-rasat,
    title = "{RASAT}: Integrating Relational Structures into Pretrained {S}eq2{S}eq Model for Text-to-{SQL}",
    author = "Qi, Jiexing  and
      Tang, Jingyao  and
      He, Ziwei  and
      Wan, Xiangpeng  and
      Cheng, Yu  and
      Zhou, Chenghu  and
      Wang, Xinbing  and
      Zhang, Quanshi  and
      Lin, Zhouhan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.211",
    doi = "10.18653/v1/2022.emnlp-main.211",
    pages = "3215--3229",
    abstract = "Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into SQL queries. However, introducing these structural relations comes with prices: they often result in a specialized model structure, which largely prohibits using large pretrained models in text-to-SQL. To address this problem, we propose RASAT: a Transformer seq2seq architecture augmented with relation-aware self-attention that could leverage a variety of relational structures while inheriting the pretrained parameters from the T5 model effectively. Our model can incorporate almost all types of existing relations in the literature, and in addition, we propose introducing co-reference relations for the multi-turn scenario. Experimental results on three widely used text-to-SQL datasets, covering both single-turn and multi-turn scenarios, have shown that RASAT could achieve competitive results in all three benchmarks, achieving state-of-the-art execution accuracy (75.5{\%} EX on Spider, 52.6{\%} IEX on SParC, and 37.4{\%} IEX on CoSQL).",
}

@misc{ren2024purplemakinglargelanguage,
      title={PURPLE: Making a Large Language Model a Better SQL Writer}, 
      author={Tonghui Ren and Yuankai Fan and Zhenying He and Ren Huang and Jiaqi Dai and Can Huang and Yinan Jing and Kai Zhang and Yifan Yang and X. Sean Wang},
      year={2024},
      eprint={2403.20014},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2403.20014}, 
}

@inproceedings{scholak-etal-2021-picard,
    title = "{PICARD}: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
    author = "Scholak, Torsten  and
      Schucher, Nathan  and
      Bahdanau, Dzmitry",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.779",
    doi = "10.18653/v1/2021.emnlp-main.779",
    pages = "9895--9901",
    abstract = "Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at \url{https://github.com/ElementAI/picard}), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.",
}

@inproceedings{song2022voicequerysystem,
  title={VoiceQuerySystem: A voice-driven database querying system using natural language questions},
  author={Song, Yuanfeng and Wong, Raymond Chi-Wing and Zhao, Xuefang and Jiang, Di},
  booktitle={Proceedings of the 2022 International Conference on Management of Data},
  pages={2385--2388},
  year={2022}
}

@article{song2024speech,
  title={Speech-to-SQL: toward speech-driven SQL query generation from natural language question},
  author={Song, Yuanfeng and Wong, Raymond Chi-Wing and Zhao, Xuefang},
  journal={The VLDB Journal},
  pages={1--23},
  year={2024},
  publisher={Springer}
}

@article{staniek-etal-2024-text,
    title = "Text-to-{O}verpass{QL}: A Natural Language Interface for Complex Geodata Querying of {O}pen{S}treet{M}ap",
    author = {Staniek, Michael  and
      Schumann, Raphael  and
      Z{\"u}fle, Maike  and
      Riezler, Stefan},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.31",
    doi = "10.1162/tacl_a_00654",
    pages = "562--575",
    abstract = "We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL,1 a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models and adapting large language models with in-context examples. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task.",
}

@inproceedings{wang-etal-2020-rat,
    title = "{RAT-SQL}: Relation-Aware Schema Encoding and Linking for Text-to-{SQL} Parsers",
    author = "Wang, Bailin  and
      Shin, Richard  and
      Liu, Xiaodong  and
      Polozov, Oleksandr  and
      Richardson, Matthew",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.677",
    doi = "10.18653/v1/2020.acl-main.677",
    pages = "7567--7578",
    abstract = "When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2{\%}, surpassing its best counterparts by 8.7{\%} absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6{\%} on the Spider leaderboard. In addition, we observe qualitative improvements in the model{'}s understanding of schema linking and alignment. Our implementation will be open-sourced at \url{https://github.com/Microsoft/rat-sql}.",
}

@inproceedings{xu-etal-2018-sql,
    title = "{SQL}-to-Text Generation with Graph-to-Sequence Model",
    author = "Xu, Kun  and
      Wu, Lingfei  and
      Wang, Zhiguo  and
      Feng, Yansong  and
      Sheinin, Vadim",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1112",
    doi = "10.18653/v1/D18-1112",
    pages = "931--936",
    abstract = "Previous work approaches the SQL-to-text generation task using vanilla Seq2Seq models, which may not fully capture the inherent graph-structured information in SQL query. In this paper, we propose a graph-to-sequence model to encode the global structure information into node embeddings. This model can effectively learn the correlation between the SQL query pattern and its interpretation. Experimental results on the WikiSQL dataset and Stackoverflow dataset show that our model outperforms the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance.",
}

@inproceedings{yu-etal-2018-spider,
    title = "{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task",
    author = "Yu, Tao  and
      Zhang, Rui  and
      Yang, Kai  and
      Yasunaga, Michihiro  and
      Wang, Dongxu  and
      Li, Zifan  and
      Ma, James  and
      Li, Irene  and
      Yao, Qingning  and
      Roman, Shanelle  and
      Zhang, Zilin  and
      Radev, Dragomir",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1425",
    doi = "10.18653/v1/D18-1425",
    pages = "3911--3921",
    abstract = "We present \textit{Spider}, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7{\%} exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at \url{https://yale-lily.github.io/seq2sql/spider}.",
}

@article{zcan2020StateOT,
  title={State of the Art and Open Challenges in Natural Language Interfaces to Data},
  author={Fatma {\"O}zcan and Abdul Quamar and Jaydeep Sen and Chuan Lei and Vasilis Efthymiou},
  journal={Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:218881987}
}

@inproceedings{zheng-etal-2022-hie,
    title = "{HIE}-{SQL}: History Information Enhanced Network for Context-Dependent Text-to-{SQL} Semantic Parsing",
    author = "Zheng, Yanzhao  and
      Wang, Haibin  and
      Dong, Baohua  and
      Wang, Xingjun  and
      Li, Changshan",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.236",
    doi = "10.18653/v1/2022.findings-acl.236",
    pages = "2997--3007",
    abstract = "Recently, context-dependent text-to-SQL semantic parsing which translates natural language into SQL in an interaction process has attracted a lot of attentions. Previous works leverage context dependence information either from interaction history utterances or previous predicted queries but fail in taking advantage of both of them since of the mismatch between the natural language and logic-form SQL. In this work, we propose a History Information Enhanced text-to-SQL model (HIE-SQL) to exploit context dependence information from both history utterances and the last predicted SQL query. In view of the mismatch, we treat natural language and SQL as two modalities and propose a bimodal pre-trained model to bridge the gap between them. Besides, we design a schema-linking graph to enhance connections from utterances and the SQL query to database schema. We show our history information enhanced methods improve the performance of HIE-SQL by a significant margin, which achieves new state-of-the-art results on two context-dependent text-to-SQL benchmarks, the SparC and CoSQL datasets, at the writing time.",
}

@article{zhong2017seq2sql,
  title={Seq2sql: Generating structured queries from natural language using reinforcement learning},
  author={Zhong, Victor and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1709.00103},
  year={2017}
}

@misc{zhong2017seq2sqlgeneratingstructuredqueries,
      title={Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning}, 
      author={Victor Zhong and Caiming Xiong and Richard Socher},
      year={2017},
      eprint={1709.00103},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1709.00103}, 
}

