
% VLDB template version of 2020-08-03 enhances the ACM template, version 1.7.0:
% https://www.acm.org/publications/proceedings-template
% The ACM Latex guide provides further information about the ACM template

\documentclass[sigconf, nonacm]{acmart}

%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{URL_TO_YOUR_ARTIFACTS}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 

\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{tcolorbox}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage{url}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{array}
\usepackage{listings, xcolor}

\input{meta/command}
\newtcolorbox{bluequotation}{
  colback=blue!5,  %
  sharp corners,  %
  boxrule=0.5pt,  %
  left=0pt,  %
  right=0pt,  %
}

\definecolor{c1}{HTML}{0c8918}
\definecolor{c2}{HTML}{dc3023}
\definecolor{darkblue}{rgb}{0, 0.0, 0.78}

\definecolor{tabletext}{RGB}{15, 23, 42}        % 文本色
\definecolor{resultcolor}{RGB}{0, 120, 90}    % 成功结果色
\definecolor{errorcolor}{RGB}{220, 38, 38}      % 错误结果色加深
\definecolor{syntaxcolor}{RGB}{30, 41, 59}     % MongoDB语法颜色加深
\definecolor{keywordcolor}{RGB}{67, 56, 202}   % MongoDB关键字颜色加深
\definecolor{codebg}{RGB}{243, 244, 246}        % 代码背景色
\definecolor{commentcolor}{RGB}{51, 65, 85}  % 注释颜色加深

\begin{document}
\title{Bridging the Gap: Enabling Natural Language Queries for NoSQL Databases through Text-to-NoSQL Translation}

%%
%% The "author" command and its associated commands are used to define the authors and their affiliations.

\author{%
  Jinwei Lu$^1$,
  Yuanfeng Song$^2$,
  Zhiqian Qin$^1$,
  Haodi Zhang$^3$,
  Chen Zhang$^1$,
  Raymond Chi-Wing Wong$^4$
}

\affiliation{%
  \institution{%
    $^1$The Hong Kong Polytechnic University, Hong Kong, China\\
    $^2$WeBank Co., Ltd, Shenzhen, China
    $^3$Shenzhen University, Shenzhen, China\\
    $^4$The Hong Kong University of Science and Technology, Hong Kong, China%
  }
}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
NoSQL databases have become increasingly popular due to their outstanding performance in handling large-scale, unstructured, and semi-structured data, highlighting the need for user-friendly interfaces to bridge the gap between non-technical users and complex database queries. In this paper, we introduce the Text-to-NoSQL task, which aims to convert natural language queries into NoSQL queries, thereby lowering the technical barrier for non-expert users. To promote research in this area, we developed a novel automated dataset construction process and released a large-scale and open-source dataset for this task, named TEND (short for \underline{Te}xt-to-\underline{N}oSQL \underline{D}ataset). Additionally, we designed a \underline{S}L\underline{M} (Small Language Model)-\underline{a}ssisted and \underline{R}AG (Retrieval-augmented Generation)-assisted multi-s\underline{t}ep framework called SMART, which is specifically designed for Text-to-NoSQL conversion. To ensure comprehensive evaluation of the models, we also introduced a detailed set of metrics that assess the model's performance from both the query itself and its execution results. Our experimental results demonstrate the effectiveness of our approach and establish a benchmark for future research in this emerging field. We believe that our contributions will pave the way for more accessible and intuitive interactions with NoSQL databases.
\end{abstract}

\maketitle

\renewcommand\thefootnote{}\footnote{\noindent A preprint.}

\section{Introduction}
\label{sec:intro}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figure/Text2NoSQL.pdf}
    \caption{An example of Text-to-NoSQL involves a Text-to-NoSQL model converting a user's natural language query into a NoSQL query, which is then executed in the corresponding NoSQL database to obtain the desired result.}
    \label{fig:text2nosql}
\end{figure*}

In recent years, with the rapid development of Internet and big data technologies, NoSQL databases have garnered increasing attention due to their outstanding performance in handling large-scale, unstructured, and semi-structured data. NoSQL databases offer high scalability, high performance, and flexible data models, effectively addressing the challenges posed by massive data volumes and high-concurrency access. Within the database and data mining community, NoSQL databases have also been the subject of extensive research and have gained widespread recognition \cite{vera2021data,gessert2016scalable,davoudian2018survey,6106531,10.1016/j.future.2015.05.003,6625441,6106531,10.1145/1978915.1978919}.


Despite the numerous advantages of NoSQL databases, the need for specialized knowledge to write query statements and manage data effectively presents a high technical barrier for users. The query languages and operational methods of NoSQL databases differ significantly from traditional SQL, posing considerable difficulties for non-expert users. Therefore, bridging the gap between natural language (NL) and NoSQL query syntax is a crucial step towards further promoting these powerful database systems.


To address this challenge, we introduce the task of \textbf{Text-to-NoSQL} for the first time in this paper. This new paradigm aims to translate natural language queries (NLQs) into corresponding NoSQL queries, thereby significantly reducing the technical difficulty of using NoSQL databases (as shown in Figure~\ref{fig:text2nosql}). Its goal is to facilitate more intuitive interactions between users and NoSQL databases, enabling a broader audience, including those without extensive programming backgrounds, to benefit from these systems.


Furthermore, recognizing the critical role of benchmark datasets in driving research and development, we present \textbf{TEND} \footnote{The partial TEND dataset and online technical report with more experimental results and analysis are available at \url{https://1drv.ms/f/c/80e862c5ba9a0a46/EiLzN6bnOyhFnd66dl0tEOQB9T_YTVzy9l_7Bp8kHpb02g?e=ntl9eE}}, a large-scale and open-source \underline{Te}xt-to-\underline{N}oSQL task \underline{d}ataset. TEND is designed to provide a comprehensive resource for researchers and developers working on natural language to query translation systems, particularly those focused on NoSQL environments. It offers a standardized evaluation platform, promoting comparison between different approaches, accelerating innovation, and fostering competition within the Text-to-NoSQL community.

To address the significant human and time costs associated with constructing large-scale datasets, we propose a novel semi-automated process for constructing a large-scale, high-quality Text-to-NoSQL dataset. Specifically, the Text-to-NoSQL dataset consists of three components: databases, NoSQL queries, and corresponding natural language queries (NLQs). First, as described in Section~\ref{sec:db_trans}, we employ a graph traversal algorithm to organize relational database tables into distinct clusters based on foreign key relationships. Using the Depth-First Search algorithm, we transform these foreign key relationships into nested structures suitable for NoSQL databases, ultimately constructing a comprehensive NoSQL database. Next, a sophisticated process combining large language models (LLMs) and code programs is utilized to generate and refine NoSQL queries. Finally, multiple LLMs are employed to expand the NLQs in the dataset. All LLM-based processes in the pipeline leverage advanced Chain-of-Thoughts (CoT) techniques, generated by state-of-the-art LLMs such as GPT-4o, to enhance the reasoning capabilities of second-tier LLMs (e.g., GPT-4o-mini). For additional details, please refer to Section~\ref{sec:tend}. This dataset construction approach not only significantly reduces manual intervention but also ensures data consistency and accuracy.

To advance research on the Text-to-NoSQL task within the data mining and database communities, we designed a \underline{S}L\underline{M} (Small Language Model)-\underline{a}ssisted and \underline{R}AG (Retrieval-augmented Generation)-assisted multi-s\underline{t}ep framework called \textbf{SMART}. SMART is divided into four main processes: (1) \emph{SLM-based Query Generation}, (2) \emph{SLM-based Query Generation}, (3) \emph{Predicted Schema-driven and Retrieved Example-driven Query Refinement}, and (4) \emph{Execution Result-based Query Optimization}. Specifically, we have the following. (i) Establishing the mapping relationship between NLQ and database schemas is a complex task. If the model is required to complete the conversion from NLQ to NoSQL query directly, it becomes more challenging for the model to establish this mapping relationship effectively. To address this issue, we adopted a step-by-step approach, fine-tuning schema predictors to predict this mapping relationship. For example, We fine-tuned various SLMs to predict the database fields used in queries, the non-database fields named in queries, as well as the fields in the final execution result documents. (ii) After fine-tuning, SLMs are generally better aligned with the preferences of the dataset without the need for manual summarization of these preferences and additional declarations through prompt engineering. Leveraging this characteristic, we fine-tuned a Text-to-NoSQL SLM to directly transform NLQs into initial NoSQL queries based on the database schema. (iii) After obtaining the schemas predicted by the SLM and the initial NoSQL query, we retrieve examples from the TEND training set based on this predicted information and the current NLQ. Specifically, we calculate the cosine similarity of the predicted schemas, NoSQL query, and NLQ with various elements in the training set examples, obtaining the top-k most relevant examples as reference examples in the RAG prompt. The LLM then refines the NoSQL query predicted by the SLM based on these examples. (iv) Similar to the query refiner, the query optimizer adopts the same retrieval strategy to retrieve the top-k most relevant examples. The difference is that in the query optimizer, the retrieved queries and the adjusted queries are executed to obtain result documents. These documents serve as the basis for the LLM to further optimize the query.


To provide a comprehensive evaluation of the performance of Text-to-NoSQL models, we specifically designed evaluation metrics for this task. The metrics are divided into two main categories, focusing on the query itself and its execution results. Metrics for the query itself can assess the model's understanding of the logical structure of the query statements, while metrics for the execution results can better evaluate the model's performance in real-world scenarios. Specifically, we use \textbf{Exact Match} (EM, which focuses on the exact match between the generated query and the gold query) as the primary evaluation metric for the query itself. Additionally, we refined two further metrics: \textbf{Query Stages Match} (QSM, which focuses on the match between the generated query and the gold query in the query stage) and \textbf{Query Fields Coverage} (QFC, which focuses on the match between the fields used in the generated query and the gold query) to provide a more detailed evaluation of the query itself. For the evaluation of execution results, we use \textbf{Execution Accuracy} (EX, which focuses on the exact match of the execution results) as the primary evaluation metric. We also designed \textbf{Execution Fields Match} (EFM, which focuses on the fields in the execution results) and \textbf{Execution Value Match} (EVM, which focuses on the values in the execution results) as further detailed evaluation metrics.

Finally, we validated the feasibility and complexity of the Text-to-NoSQL task through extensive experiments. Basic models such as Seq2Seq and Transformer, due to their simplistic architectures, were unable to learn the complex syntax of NoSQL during training and thus failed to achieve Text-to-NoSQL predictions, highlighting the intricacy of the task. In contrast, LLMs were augmented solely with the RAG technology and the Fine-tuned Llama, which was directly fine-tuned on the TEND training set, achieved execution accuracies of 52.76\% and 53.12\% on the TEND test set, respectively. Moreover, SMART attained an execution accuracy of 65.08\%, which amply demonstrates the feasibility of the Text-to-NoSQL task.

To sum up, the primary contributions of this paper can be summarized as follows:
\begin{itemize}
\item We are the first to propose the Text-to-NoSQL task, which aims to reduce the technical difficulty of using NoSQL databases by automatically translating NLQs into corresponding NoSQL queries, thereby bridging the gap between NL and NoSQL queries.
\item We introduced TEND, a large-scale and open-source benchmark dataset for the Text-to-NoSQL task, with the aim of providing a standardized evaluation platform to attract more researchers to contribute to the development of the Text-to-NoSQL community.
\item We propose a novel large-scale dataset construction process and aim at reducing the time and labor costs for researchers while ensuring the consistency and accuracy of the data in the dataset.
\item We designed a sophisticated multi-step framework, SMART, which cleverly combines SLM and RAG technologies to effectively achieve Text-to-NoSQL task predictions, providing a model baseline for future work on this task.
\item We designed specialized evaluation metrics for the Text-to-NoSQL model. By refining the evaluation of the query itself and its execution results, we can comprehensively analyze the model's ability to understand the logic of the query and its generalization capability in real-world scenarios. The aim is to foster healthy competition among models within the Text-to-NoSQL community, thereby promoting the development of this community.
\item We validated the complexity and feasibility of the Text-to-NoSQL task through extensive experiments.
\end{itemize}

The rest of this paper is structured as follows. Section~\ref{sec:tend} details the process of constructing the Text-to-NoSQL dataset. Section~\ref{sec:tend_setup_stat} explains the specific setup for dataset creation and provides statistics about the dataset. Section~\ref{sec:smart} discusses the design and implementation of SMART. Section~\ref{sec:exp_analysis} outlines the experimental setup and analyzes the results. Section~\ref{sec:related_work} reviews related studies in the field. Finally, Section~\ref{sec:conclusion} summarizes the key contributions of this paper.

\section{The Proposed Pipeline for Constructing the Text-to-NoSQL Benchmark}
\label{sec:tend}
In this section, we provide a detailed explanation of the construction pipeline of TEND.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figure/TEND_construct.pdf}
    \caption{Construction pipeline for the TEND dataset. Through an automated process, the conversion from a Text-to-SQL dataset to a Text-to-NoSQL dataset can be achieved. Specifically, (i) the conversion from SQL databases to NoSQL (e.g., MongoDB) databases is accomplished through algorithmically programmed software; (ii) by inputting examples into the most advanced LLM (such as GPT-4) to obtain examples of Chain-of-Thought, namely Advanced COT, which assists a second-tier LLM (such as GPT-3.5) in reasoning, thus achieving automated generation of NoSQL queries, feedback generation, and query debug; (iii) the expansion of dataset questions is implemented using Multi-LLM.}
    \label{fig:tend}
\end{figure*}

\subsection{Overview}

The construction process of TEND is a semi-automatic pipeline that can convert a Text-to-SQL dataset into a Text-to-NoSQL dataset. Its automation is mainly divided into three steps: Database Transformation (Section~\ref{sec:db_trans}), Advanced CoT-driven Query Transformation (Section~\ref{subsec:AdvancedCoT}), and Question Extension by Multi-LLM (Section~\ref{sec:q_extension}) (as shown in Figure~\ref{fig:tend}). Finally, we manually reviewed and adjusted the entire dataset, removing examples that did not meet expectations and standardizing programming conventions across the dataset. Below, we explain this process in detail.


\subsection{Database Transformation}
\label{sec:db_trans}

\begin{algorithm}[t!]
    \SetKwFunction{GetFKTableFrom}{\textsc{GetFKTableFrom}}
    \SetKwFunction{GetFKTableTo}{\textsc{GetFKTableTo}}
    \SetKwFunction{ExistT}{\textsc{ExistTable}}
    \SetKwFunction{GetCollection}{\textsc{GetC}}
    \SetKwFunction{MergeCollection}{\textsc{MergeC}}
    \SetKwFunction{AddTable}{\textsc{AddTable}}
    \SetKwFunction{GetMainTable}{\textsc{GetMainTable}}
    \SetKwFunction{GetData}{\textsc{GetData}}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \textbf{Input:} SQL Database $\texttt{$D_{\text{SQL}}$}$\\ 
    \textbf{Output:} NoSQL Database $\texttt{$D_{\text{NoSQL}}$}$ \\ 
    \Begin{
        \tcp{\color{blue} Relational Table Grouping}
        $\texttt{$C_{\text{SQL}}$} \leftarrow \texttt{\{\}}$ 

        \For{\textup{\textbf{each} $t \in \texttt{$D_{\text{SQL}}$}$}}{ 
            \tcp{\color{purple} Grouping by Foreign Key Relationships}
            \For{\textup{\textbf{each} $t' \in \texttt{\GetFKTableFrom{$t$}}$}}{ 
                \eIf{\ExistT{$t'$, $\texttt{$C_{\text{SQL}}$}$}}{ 
                    \MergeCollection{\GetCollection{$t'$, $\texttt{$C_{\text{SQL}}$}$}, \GetCollection{$t$, $\texttt{$C_{\text{SQL}}$}$}} 
                }{
                    \AddTable{$t'$, \GetCollection{$t$, $\texttt{$C_{\text{SQL}}$}$}}\\ 
                }
            }
        }

        \tcp{\color{blue} Main Table Obtaining}
        \For{\textup{\textbf{each} $c \in \texttt{$C_{\text{SQL}}$}$}}{ 
            $c[\texttt{$T_{\text{main}}$}] \leftarrow \GetMainTable{c}$ 

        }

        \tcp{\color{blue} Recursive Data Transformation}
        $\texttt{$D_{\text{NoSQL}}$} \leftarrow \texttt{[]}$ 

        \For{\textup{\textbf{each} $C \in \texttt{$C_{\text{SQL}}$}$}}{ 
            \tcp{\color{purple} Embed Main Table Data as Top-Level NoSQL Documents}
            $c \leftarrow \GetData{C[\texttt{$T_{\text{main}}$}]}$ 

            \tcp{\color{purple} Recursively Join Tables via Foreign Keys}
            $Q \leftarrow \texttt{$T_{\text{main}}$}$ 

            \While{
                $Q \neq \texttt{[]}$
            }{
                $t \leftarrow Q$.pop()

                \For{\textup{\textbf{each} $t' \in \texttt{\GetFKTableTo{$t$}}$}}{ 
                    $c[t] \leftarrow \GetData{$t'$, $t[fk] \equiv t'[fk]$}$ 

                    $Q$.push($t'$)

                }
            }
            $\texttt{$D_{\text{NoSQL}}$}$.append($c$) 
        }

        return $\texttt{$D_{\text{NoSQL}}$}$ 
    }

\caption{The Pipeline for Database Transformation}
\label{alg:db_trans}
\end{algorithm}
TEND consists of three parts: databases, NoSQL queries, and NLQs. The construction of databases is a labor-intensive and time-consuming task, involving extensive data collection and filtering. To reduce the substantial manpower and time required for database construction, we designed an algorithm to convert relational databases into NoSQL (MongoDB) databases, where we transformed the foreign key relationships between tables in the relational database into nested relationships in the NoSQL database. Below is a detailed explanation of the database transformation algorithm along with specific examples.

The algorithm for database transformation is shown in Algorithm~\ref{alg:db_trans}, with a specific example illustrated in Figure~\ref{fig:tend}(i). First, we traverse all tables and their potential foreign keys, grouping two tables connected through a foreign key into the same MongoDB collection. For instance, in Figure~\ref{fig:tend}(i), the ``\texttt{student}'' and ``\texttt{tutor}'' tables are connected by the foreign key ``\texttt{tutor\_id}'', indicating that these two tables belong to the same relation set, whereas the ``staff'' table does not have a foreign key connection with other tables, meaning the ``staff'' table forms its own relation set. For tables within the same relation set (e.g., ``\texttt{student}'' and ``\texttt{tutor}'' tables), we identify the table at the far end of the foreign key relationship (e.g., the ``\texttt{tutor}'' table) and use its column names (e.g., ``\texttt{tutor\_id}'' and ``\texttt{gender}'' from the ``\texttt{tutor}'' table) as the outermost keys of the collection. We then begin to traverse all the records in this table, with each record corresponding to a document in the NoSQL database. For each record, we add a key-value pair to store data from other tables linked by foreign keys pointing to this record (e.g., adding the key ``\texttt{student}'' to store all records from the ``\texttt{student}'' table where ``\texttt{tutor\_id}'' equals the current ``\texttt{tutor}'' table record). This process is recursively continued until no more foreign keys point to the currently recursive table. For example, the ``\texttt{student}'' table is linked to the ``\texttt{tutor}'' table through the foreign key ``\texttt{tutor\_id}''. After transformation to a non-relational database, the outermost key-value pairs of the collection will be the records from the ``\texttt{tutor}'' table, while the records of the ``\texttt{student}'' table will be stored in the second layer of the collection after being filtered by ``\texttt{tutor\_id}'' in multi-table join queries. Tables connected to the ``\texttt{student}'' table through foreign keys will also undergo this recursive process, with their records stored in the third layer of key-value pairs in the collection. It is noteworthy that if two tables mutually point to each other through foreign keys, such a database will be discarded because this situation can lead to an infinite loop in the database transformation process.



\subsection{Advanced CoT-driven Query Transformation}
\label{subsec:AdvancedCoT}
After completing the database conversion, we need to generate the corresponding NoSQL queries based on the NoSQL database. Manually writing all the NoSQL queries for a large-scale dataset is also a labor-intensive and time-consuming task. To automate this process, we designed a specialized LLM-based pipeline that leverages the execution results of SQL queries (rather than the queries themselves) to autonomously generate NoSQL queries. These queries are then executed to obtain feedback, which is subsequently used to refine and debug the queries. Below, we provide a detailed introduction to this Query Transformation pipeline (shown in Figure~\ref{fig:tend}(ii)).

\subsubsection{Advanced CoT-driven Query Generation}
\label{sec:cot-driven_QG}
Using state-of-the-art LLMs (like GPT-4o) for inference is very expensive. To fully leverage the performance of the second-tier LLMs (such as GPT-4o-mini), we adopted an Advanced CoT-driven approach for query generation. Specifically, we provided the LLM with input and output examples as the first round of dialogue, where the input includes an introduction for query generation, NoSQL database schemas, and user questions (the prompt is written in markdown format), and the output contains a chain of thoughts generated by the most advanced LLM (GPT-4o), inspired by the following prompt: ``\textit{A: Let's think step by step!}''. Notably, during query generation, we also provide the schemas obtained from executing the corresponding SQL queries of the user questions as a reference, effectively assisting the LLM in pre-completing schema-linking.

\subsubsection{Execution Feedback Generation}
\label{sec:feedback_gen}
After the Advanced CoT-driven Query Generation, we obtain many unverified NoSQL Queries, and manually checking all queries for correctness would require a considerable amount of time. To screen all queries automatically, we employ the LLM as an inspector. Specifically, we execute the NoSQL queries and their reference SQL queries obtained from Section~\ref{sec:cot-driven_QG} on MongoDB and SQLite databases respectively, and organize the results into JSON format for direct comparison. If the two results are identical, then the NoSQL query is considered correct; if they differ, then LLM feedback is required. For LLM feedback, the execution results of the NoSQL and reference SQL queries (in JSON format), user questions, and the schemas required are combined into prompts, instructing the LLM to perform the following two steps: (i) Examine the differences between
the data obtained from executing a NoSQL query and the data obtained from executing the corresponding reference SQL query.
(ii) Analyze where these differences may have originated from, with no solutions. It is worth noting that the Feedback Generation step also uses the Advanced CoT-driven method, similar to that mentioned in Section~\ref{sec:cot-driven_QG}.


\subsubsection{Advanced CoT-driven and Feedback-driven Query Debug}
\label{sec:debug}
For those NoSQL queries whose execution results are inconsistent with the reference SQL queries, we need to correct them. To automate NoSQL debugging, we let the LLM act as an inspector in Section~\ref{sec:feedback_gen}, identifying potential issues with the problematic NoSQL queries. We then composed these factors that could cause discrepancies between the execution results of the NoSQL queries and the reference SQL queries into debug prompts, and used the most advanced LLM to generate a debug thought chain as a demonstration dialogue. Subsequently, we let a second-tier LLM correct these errors while continuously checking if the debugged NoSQL query's execution results are correct. If correct, we exit the debug program and output the NoSQL query; if incorrect, we re-try. After two accumulated errors, we opt to use the highest-level LLM for debugging. If the highest-level LLM still fails to correct the NoSQL query, we abandon that case. It is noteworthy that the execution results of SQL queries serve as a critical dependency in this process, which makes it inherently unsuitable for direct application in Text-to-NoSQL tasks.

\begin{table}[t]
  \centering

        \begin{tabular}{|>{\centering\arraybackslash}p{1.8cm}|>{\centering\arraybackslash}p{2.0cm}|>{\centering\arraybackslash}p{1.6cm}|}
            \hline
            \textbf{\#-Databases} & \textbf{\#-Collections} & \textbf{\#-Domains} \\
            \hline
            154 & 347 & 105 \\
            \hline
        \end{tabular}
        \vspace{4pt}

        \begin{tabular}{|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{1.4cm}|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{0.8cm}|>{\centering\arraybackslash}p{1.0cm}|}
            \hline
            \multicolumn{5}{|c|}{\textbf{Top-5 Domains}} \\
            \hline
            Sport & Customer & School & Shop & Student\\
            \hline
        \end{tabular}
        \vspace{4pt}

        \begin{tabular}{|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{2.1cm}|>{\centering\arraybackslash}p{2.1cm}|>{\centering\arraybackslash}p{2.1cm}|}
            \hline
            \multicolumn{4}{|c|}{\textbf{The number of Fields and Documents}} \\
            \hline
            \#-Fields & \#-Avg(\#-Fields) & \#-Max(\#-Fields) & \#-Min(\#-Fields) \\
            \hline
            5960 & 38.70 & 331 & 7 \\
            \hline
            \#-Docs & \#-Avg(\#-Docs) & \#-Max(\#-Docs) & \#-Min(\#-Docs) \\
            \hline
            32979 & 214.15 & 13694 & 3 \\
            \hline
        \end{tabular}
        \caption*{(a) Database Statistics}
        \vspace{-4pt}


        \begin{tabular}{|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{1.8cm}|}
            \hline
            \multicolumn{3}{|c|}{\textbf{Query Method}} \\
            \hline
            \textbf{\#-Total} & \textbf{\#-Find} & \textbf{\#-Aggregate} \\
            \hline
            17020 & 2770 & 14250 \\
            \hline
        \end{tabular}
        \vspace{4pt}

        \begin{tabular}{|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{1.6cm}|>{\centering\arraybackslash}p{1.0cm}|>{\centering\arraybackslash}p{1.0cm}|}
            \hline
            \multicolumn{4}{|c|}{\textbf{Operations in Find Method}} \\
            \hline
            \#-Filter & \#-Projection & \#-Sort & \#-Limit\\
            \hline
            2690 & 2005 & 600 & 10 \\
            \hline
        \end{tabular}
        \vspace{4pt}

          \begin{tabular}{|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.3cm}|>{\centering\arraybackslash}p{1.3cm}|}
              \hline
              \multicolumn{5}{|c|}{\textbf{Stages in Aggregate Method}} \\
              \hline
              \#-Project & \#-Unwind & \#-Group & \#-Match & \#-Sort \\
              \hline
              13235 & 8575 & 7740 & 7305 & 3325 \\
              \hline
              \#-Limit & \#-Lookup & \#-Count & \#-Others & \\
              \hline
              2390 & 2155 & 955 & 25 & \\
              \hline
          \end{tabular}
          \caption*{(b) (NLQ, Query) pairs Statistics}
          \vspace{-4pt}

  \caption{TEND Dataset Statistics}
  \label{tab:tend_stat}
  \vspace{-30pt}
\end{table}

\subsection{Question Extension by Multi-LLM}
\label{sec:q_extension}
Figure~\ref{fig:tend}(iii) illustrates the process of expanding NLQs based on Multi-LLM. To construct a dataset of sufficient size for model training, we need to expand the original NLQs to create enough (NLQ, NoSQL) pairs. In this process, we use LLMs to automatically expand the NLQs from the original Text-to-SQL dataset. We provide the LLMs with NoSQL database schemas, NoSQL queries, reference question examples, and the target schemas for the questions. A single LLM often generates the same questions repeatedly, failing to effectively expand the question set. To address this issue, we employed a Multi-LLM approach, using multiple LLMs {\color{black}(e.g., \texttt{gpt-3.5-turbo-16k-0613}, \texttt{gpt-4o-2024-05-13} and \texttt{claude-3-sonnet-20240229})} to expand the questions, ultimately resulting in a large-scale Text-to-NoSQL dataset. 


\subsection{Manual Review and Refinement}
Finally, we conducted a comprehensive manual review and refinement of the Text-to-NoSQL dataset TEND generated by the automated pipeline. Specifically, we carefully modified the renamed fields in the NoSQL queries within TEND. For example, in the group stage, which uses aggregation operations similar to those in SQL, such as ``\texttt{sum\_Population: \{ \$sum: "\$Population" \}}'', the LLM might use different names for ``\texttt{sum\_Population}'', such as ``\texttt{total\_population}''. If the renamed fields in the NoSQL queries are not standardized, it could lead to poor performance of the models trained on this dataset in query-based metrics (Section~\ref{sec:metric}). Therefore, we standardized the naming of aggregation operations, renaming fields to ``\texttt{[operation]\_[object]}'' {\color{black}(e.g., ``sum\_population'')}. Similar to the renaming in the group stage, we also standardized the renaming of the new document collections after joins in the lookup stage, using names like ``\texttt{Docs1}'' and ``\texttt{Docs2}''.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figure/SMART_v2.pdf}
    \caption{The working pipeline of our proposed SMART framework, where (i) \textit{SLM-based Schema Prediction} predicts the required NoSQL schemas by fine-tuning the SLM. (ii) \textit{SLM-based Query Generation} generates initial NoSQL queries by fine-tuning the SLM. (iii) \textit{Predicted Schema-driven and Retrieved Example-driven Query Refinement} is responsible for refining the NoSQL queries generated by the SLM. The RAG technique used here is detailed in Section~\ref{sec:query_refine}. (iv) \textit{Execution Result-based Query Optimization} optimizes the refined NoSQL queries based on their execution results.}
    \label{fig:model}
\end{figure*}

\section{The TEND Dataset: Setup And Statistics}
\label{sec:tend_setup_stat}

\subsection{Setup}
We applied the dataset construction process introduced in Section~\ref{sec:tend} to the training set of Spider \cite{yu-etal-2018-spider}, a popular and complex large-scale benchmark dataset for the Text-to-SQL task. The training set includes 166 databases (with an average of 5.28 tables per database) and 7,000 (NLQ, SQL) pairs on these databases. These (NLQ, SQL) pairs are designed to comprehensively cover different databases and various levels of difficulty in Text-to-SQL tasks.

After applying the construction process introduced in Section~\ref{sec:tend} to Spider, we obtained a high-quality Text-to-NoSQL dataset. However, during manual inspection, we found that some NoSQL queries contained special operations such as ``\texttt{\$isArray}'', ``\texttt{\$concatArrays}'', and ``\texttt{\$arrayElemAt}''. These special operations often introduce unnecessary difficulties for model prediction, similar to special operations like ``\texttt{strftime()}'' in Text-to-SQL tasks, which should be avoided. To address this issue, we developed a NoSQL query parser using Python, which is responsible for parsing operations in NoSQL queries into JSON data. For example, it can parse the query stages in the NoSQL query ``\texttt{db.people.aggregate([\{\$group: \{ \_id:"\$Country", count:\{\$sum: 1\}\} \}, \{\$project:\{ Country: "\$\_id", count:1, \_id:0  \}\}]);}'' into a list of dictionaries, where each dictionary represents a stage in the query, such as \texttt{[\{"\$group": \{\dots\}\}, \{"\$project":\{\dots\}\}]}. To simplify the implementation of this process, we used the demjson library in Python to loosely convert strings into JSON data without requiring the strings to strictly follow JSON format. For instance, ``\texttt{\{\$project \{\_id:0\}\}}'' cannot be directly decoded into JSON because it lacks necessary quotation marks, but demjson can automatically complete it and parse it into JSON data \texttt{\{"\$project":\{"\_id":0\}\}}. Using this parser, we can write Python programs to analyze the parsed JSON data of NoSQL queries and filter them based on their operation keywords, directly discarding examples that contain special operations, thereby obtaining the final TEND.


\subsection{Statistics of TEND}
Table~\ref{tab:tend_stat} presents the statistical overview of the TEND dataset, encompassing both database and NoSQL query statistics (each NoSQL query corresponds to 5 NLQs, forming 5 pairs of (NLQ, NoSQL)). %(each NoSQL query comprises 5 pairs of (NLQ, NoSQL)).
As illustrated in Table~\ref{tab:tend_stat}(a), the TEND dataset includes 154 databases, which collectively store 347 collections and span 105 domains. The top five domains are sport, customer, school, shop, and student. In total, the databases contain 5,960 fields (corresponding to 32,979 documents), with an average of 38.70 fields (214.15 documents) per database. The database with the highest number of fields (documents) contains 331 fields (13,694 documents), whereas the one with the fewest has only 7 fields (3 documents).

Table~\ref{tab:tend_stat}(b) presents the query statistics for the TEND dataset, which includes a total of 17,020 (NLQ, NoSQL) pairs. Within this dataset, 2,770 pairs utilize the find method, while 14,250 pairs employ the aggregate method. Among the find method pairs, specific operations are distributed as follows: 2,690 pairs incorporate the filter operation, 2,005 utilize the projection operation, 600 employ the sort operation, and 10 pairs apply the limit operation.

For the aggregate method pairs, operations are executed through pipelines, where each operation stage is defined within the pipeline. The data reveal that 13,235 of these aggregate pipelines incorporate the Project stage. Additional operations are utilized across the following stages: Unwind (8,575 pairs), Group (7,740 pairs), Match (7,305 pairs), Sort (3,325 pairs), Limit (2,390 pairs), Lookup (2,155 pairs), Count (955 pairs), and other operations (25 pairs).



\section{Multi-step Framework for Text-to-NoSQL: SLM and RAG Assistance}
\label{sec:smart}
To provide a high-performance solution for the novel task of Text-to-NoSQL, we have designed a Multi-step framework called SMART (an abbreviation for \underline{S}L\underline{M}-\underline{a}ssisted and \underline{R}AG-assisted Mul\underline{t}i-step framework) based on RAG techniques and fine-tuned SLM. This chapter will provide a detailed introduction to SMART.

\begin{algorithm}[t]
\small
 \SetAlgoLined
 \SetKwFunction{TrainSLM}{\textsc{TrainSLM}}
 \SetKwFunction{BuildVecLib}{\textsc{BuildVecLib}}
 \SetKwFunction{SLMPredict}{\textsc{SLMPredict}}
 \SetKwFunction{Retrieve}{\textsc{Retrieve}}
 \SetKwFunction{Integrate}{\textsc{Integrate}}
 \SetKwFunction{QueryAdjust}{\textsc{QueryAdjust}}
 \SetKwFunction{Execute}{\textsc{Execute}}
 \SetKwFunction{RetrieveExec}{\textsc{RetrieveExec}}
 \SetKwFunction{QueryOptimize}{\textsc{QueryOptimize}}
 \SetKwFunction{SMART}{\textsc{SMART}}
 
 \SetKwInOut{KwIn}{Inputs}
 \SetKwInOut{KwOut}{Output}
 \KwIn{\small{NLQ list in Test Set $\mathcal{Q}$}; \\
 Database list in Test Set $\mathcal{D}$; \\
 NLQ list in Training Set $\mathcal{Q}'$; \\
 Database list in Training Set $\mathcal{D}'$; \\
 NoSQL list in Training Set $\mathcal{N}'$; \\}
 \KwOut{\small{NoSQL list $\mathcal{N}$}}
\SetKwProg{Fn}{Procedure}{:}{}
 \Fn{\SMART{$\mathcal{Q}$, $\mathcal{D}$}}{
    \tcp{\color{blue} SLM Fine-tuning}
    
    $\mathcal{M\textsubscript{schema}}, \mathcal{M\textsubscript{nosql}} \gets $ \TrainSLM{$\texttt{SLM}$, $\mathcal{Q}'$, $\mathcal{D}'$, $\mathcal{N}'$}

    \tcp{\color{blue} Build Vector Library}

    $\mathcal{V} \gets $ \BuildVecLib{$\mathcal{Q}'$, $\mathcal{D}'$, $\mathcal{N}'$}

    \tcp{\color{blue} Pipeline of SMART}
    
    $\mathcal{N} \gets \texttt{[]}$
    
    \For{\textup{\textbf{each} $(q, d) \in (\mathcal{Q}, \mathcal{D})$}}{
        \tcp{\color{purple} SLM-based Schemas Prediction}
        $\mathcal{S}$ $\gets$ \SLMPredict{$\mathcal{M\textsubscript{schema}}$, $q$, $d$}

        \tcp{\color{purple} SLM-based Query Generation}
        $n\textsubscript{gen}$ $\gets$ \SLMPredict{$\mathcal{M\textsubscript{nosql}}$, $q$, $d$}

        \tcp{\color{purple} Query Adjustment}

        $\mathcal{E}\textsubscript{adj}$ $\gets$ \Retrieve{$q$, $\mathcal{S}$, $n\textsubscript{gen}$, $\mathcal{V}$}
         
        $n\textsubscript{adj}$ $\gets$ \QueryAdjust{$q$, $d$, $\mathcal{S}$, $\mathcal{E}\textsubscript{adj}$}

        \tcp{\color{purple} Query Optimization}

        $e$ $\gets$ \Execute{$n\textsubscript{adj}$, $d$}

        $\mathcal{E}\textsubscript{opt}$ $\gets$ \Retrieve{$q$, $\mathcal{S}$, $n\textsubscript{adj}$, $\mathcal{V}$}
        
        $n\textsubscript{opt}$ $\gets$ \QueryOptimize{$n\textsubscript{adj}$, $d$, $e$, $\mathcal{E}\textsubscript{opt}$}

        $\mathcal{N}$.append($n\textsubscript{opt}$)
    }
    \KwRet $\mathcal{N}$
  }
 \caption{The SMART Algorithm}
 \label{alg:smart}
\end{algorithm}

\smallskip
\noindent
\textbf{Overview:}
As shown in Figure~\ref{fig:model} and Algorithm~\ref{alg:smart}, SMART is mainly divided into four processes: SLM-based Schema Prediction (Section~\ref{sec:pref_predict}), SLM-based Query Generation (Section~\ref{sec:self_qa}), Predicted Schema-driven and Retrieved Example-driven Query Refinement (Section~\ref{sec:query_refine}), and Execution Result-based Query Optimization (Section~\ref{subsec:executionResultBasedQueryOptimization}). Compared to LLM, the fine-tuning cost of SLM is much lower, making SLM a promising auxiliary tool. Especially when we need to predict code or details that are easily influenced by preferences (e.g. the renamed fields in NoSQL queries), LLM often struggles to align with these preferences. This means that directly using prompting methods makes it difficult for LLM to achieve high accuracy. However, with the assistance of SLM, the entire framework can achieve this alignment. RAG technology is a mainstream auxiliary technology for LLM, but its drawback lies in its high requirements for retrieval technology. It requires retrieving high-quality examples and adding them to the prompt context to be effective. If the quality of the context examples is poor, it can significantly interfere with LLM's predictions. To maximize the relevance of the retrieved examples, the retrieval method used in the SMART framework does not solely rely on the similarity of NLQs between examples. Instead, it calculates the final example similarity by weighting and summing the cosine similarities of NLQs, NoSQL queries, and schemas (such as fields in the queries and fields displayed in the execution result documents) to retrieve examples. Below, we  introduce the four main processes of SMART in detail.








 \subsection{SLM-based Schema Prediction}
\label{sec:pref_predict}
As shown in Figure~\ref{fig:model}(i), the implementation of schema prediction based on SLM is accomplished through model fine-tuning. Specifically, we parse each example in the training set of TEND, extracting various schema information from the NoSQL queries, such as the collections queried, the database fields queried, the renamed fields in the query, and the fields included in the resulting documents after query execution. These schema details are then paired with the corresponding NLQ to form prompts and outputs, thereby constructing a fine-tuning corpus for predicting various schemas. We then use this fine-tuning corpus to fine-tune SLMs specialized in predicting different schemas. Finally, we use these SLMs to perform schema prediction on examples in the TEND test set.

\begin{lstlisting}[backgroundcolor=\color{gray!10},frame=lines]
You are now the MongoDB natural language interface, responsible for converting user input natural language queries into MongoDB query statements based on the MongoDB Collection and their Fields, and parsing the features according to user requirements.
Human: # Given the natural language query, please predict the fields used in the query.
## Natural Language Query: `<NLQ>`
## MongoDB Collection and their Fields
<MongoDB_Schemas>
Assistant:
\end{lstlisting}

The specific prompt template in the corpus is as shown above. It consists of four main parts: the system prompt, the instruction, the NLQ, and the complete database schemas. The output is the serialized schema information. For example, in the NoSQL query ``\texttt{db.train.find(\{\},\{"Name":1,"Time":1,"Service":1,"\_id": 0\});}'', the database fields used are ``\texttt{Name,Service,Time,\_id}''.

\subsection{SLM-based Query Generation}
\label{sec:self_qa}
Figure~\ref{fig:model}(ii) illustrates the SLM-based Query Generation process. This process involves directly constructing the TEND training set into a text-to-NoSQL corpus and using this corpus to fine-tune a specialized text-to-NoSQL SLM. Subsequently, this SLM is used to directly predict examples in the test set, generating initial NoSQL queries.

The following is the prompt template used for fine-tuning the SLM in the SLM-based Query Generation process. Similar to the template for SLM-based Query Generation, it consists of the system prompt, the instruction, the NLQ, and the database schemas. A specific example is as follows: the SLM needs to convert the NLQ ``\texttt{Count the number of products.}'' into the NoSQL query ``\texttt{db.Ref\_Colors.aggregate([\{\$unwind:"\$Products"\},\{\$group: \{\_id:null,count:\{\$sum:1\}\}\},\{\$project:\{\_id:0, count:1\}\}]);}''

\begin{lstlisting}[backgroundcolor=\color{gray!10},frame=lines]
You are now the MongoDB natural language interface, responsible for converting user input natural language queries into MongoDB query statements based on the MongoDB collections and their fields.
Human: # Given the MongoDB collections and their fields and natural language query, please generate final MongoDB query.
## Natural Language Query: `Count the number of schools that have had basketball matches.`
## MongoDB Collection and their Fields
<MongoDB_Schemas>
Assistant:
\end{lstlisting}



\subsection{Predicted Schema-driven and Retrieved Example-driven Query Refinement}
\label{sec:query_refine}
Figure~\ref{fig:model}(iii) illustrates the process of query refinement based on predicted schemas and queries. Specifically, we first compute the cosine similarity of each element (natural language query (NLQ), schema predicted by the SLM-based schema predictor, and initial NoSQL query generated by the SLM-based query generator) with examples in the training set. We then perform a weighted sum to retrieve the top $k$ most relevant reference examples. The formula for calculating sample similarity is as follows:\\
\makecell[c]{\texttt{Sim = Sim\textsubscript{nlq} $\times$ w1 + Sim\textsubscript{nosql} $\times$ w2 + ...}}\\
where \texttt{Sim} represents the cosine similarity, \texttt{w} represents the weights, with ``\texttt{w1=1.0, w2=0.3}''. The reason for this weighting is that the NLQ is an actual value, whereas the NoSQL query and other elements are predicted values. It is worth noting that the sum of the weights for all similarity measures does not necessarily have to be 1. This is because the final ranking of similarities is based on the magnitude of \texttt{Sim}. When all \texttt{Sim} values are proportionally scaled by a factor of \texttt{w}, the results remain unaffected. For example, a ratio of $1.0:0.5:0.5$ (where the sum of weights is not equal to 1) is equivalent to $0.5:0.25:0.25$ (where the sum of weights equals 1). The ``\texttt{...}'' represents the weighted sum of the similarities of database fields and the fields displayed in the document, which are also multiplied by the weight w2 but are omitted due to space constraints. The retrieved reference examples are subsequently used to form the context for the RAG prompt. The LLM first evaluates whether the NoSQL query generated by the SLM is reasonable given the current NLQ and database schema. If it is not reasonable, the LLM adjusts the query based on the retrieved examples; if it is reasonable, the original NoSQL query is retained. The reason for adopting this retrieval strategy is to maximize the relevance of the retrieved examples to the current example, thereby enhancing the LLM's ability of refining the NoSQL query. 
\begin{lstlisting}[backgroundcolor=\color{gray!10},frame=lines]
## Query Transformation Reference Examples
[RAG_examples]
## MongoDB collections and their fields
[MongoDB_Schemas]
## Natural Language Query
   - `[NLQ]`
## Original MongoDB Query
[NoSQL]
[Predicted_Schemas]
[instruction]
A: Let's think step by step!
\end{lstlisting}
The above is the prompt template for query refinement based on predicted schemas and queries.

\subsection{Execution Result-based Query Optimization}
\label{subsec:executionResultBasedQueryOptimization}
After the query adjustment in Section~\ref{sec:query_refine}, we also designed a module for query optimization based on execution results (Figure~\ref{fig:model}(iv)). This module primarily relies on RAG technology. Consistent with the retrieval strategy introduced in Section~\ref{sec:query_refine}, the query optimizer also relies on the cosine similarity of elements such as NLQ and schemas to retrieve examples. For each retrieved example, we execute the NoSQL query to obtain the execution result document, allowing the LLM to reference the mapping relationships between the NLQ, NoSQL query, and execution results to further optimize the NoSQL query. The following is the prompt template for this step:
\begin{lstlisting}[backgroundcolor=\color{gray!10},frame=lines]
## Reference Exampels:
[RAG_Examples]
##  MongoDB collections and their fields
[MongoDB_Schemas]
## Natural Language Query
   - `[NLQ]`
## Fields Shown in the Execution Results
   - `[target_fields]`
## Original MongoDB Query
[NoSQL]
### Execution Results
[Exec_Results]
[instruction]
A: Let's think step by step! 
\end{lstlisting}



\section{Experiments and Analysis}
\label{sec:exp_analysis}
This section shows extensive experimental results on a new dataset (TEND). We  provide a detailed description of the experimental setup, evaluation metrics, and baseline models, followed by a comparative analysis of the performance between our proposed model (SMART) and the baseline models. Subsequently, we perform ablation studies and parameter experiments to validate the effectiveness of each component in SMART and explore the optimal hyperparameter settings. Finally, we present detailed case studies through specific examples to illustrate the concrete performance of each method.

\subsection{Experimental Setup}
\label{sec:exp_setup}
\subsubsection{Dataset}
% 我们已根据跨域（cross-domain）标准将TEND数据集划分为训练集和测试集，比例约为8:2。最终得到的训练集和测试集分别包含14,245和2,775对（NLQ, NoSQL）数据。这样的数据集划分可以分析各方法在面对未知数据时的表现，从而更全面地评估模型性能。
We have partitioned the TEND dataset into training and testing sets according to the cross-domain standard, with a ratio of approximately 8:2. The resulting training and testing sets contain 14,245 and 2,775 pairs of (NLQ, NoSQL) instance, respectively. This division of the dataset allows for an analysis of how each method performs when confronted with unseen data, thereby providing a more comprehensive evaluation of the model's performance.

\subsubsection{Models}
We utilized a variety of popular neural network models and LLM-based prompting methods as baseline models for a comprehensive performance comparison with SMART. The models are as follows:
\begin{itemize}[leftmargin=5pt]
    \item \textbf{Seq2Seq}: The Seq2Seq \cite{bahdanau2015neural} model transforms natural language queries and database schemas into hidden states via an encoder, and then generates NoSQL queries through a decoder.
    \item \textbf{Transformer}: Transformer \cite{vaswani2017attention} demonstrated excellent performance in tasks such as machine translation \cite{currey2019incorporating}, dialogue systems \cite{zhao2020multiple}, and speech recognition \cite{zeyer2019comparison}, and it is also a mainstream model in neural network models.
    \item \textbf{Instructing LLM}: The Instructing LLM is an experimental setup designed to guide the LLM in performing Text-to-NoSQL predictions by crafting clear and precise instructional prompts.
    \item \textbf{Few-shot LLM}: The few-shot prompting method is an important way to implement in-context learning (ICL), by enhancing a certain number of examples in the context to teach LLMs how to perform tasks in specific domains.
    \item \textbf{RAG for LLM}: RAG technology can adaptively select examples from the knowledge base as references based on the input, which can further mitigate model hallucinations.
    \item \textbf{Fine-tuned Llama}: The Fine-tuned Llama is an experimental setup that involves supervised fine-tuning of \texttt{Llama-3.2-1B} using the training set of TEND, representing the performance of model training-based approaches on the Text-to-NoSQL task.
    \item \textbf{SMART}: SMART is the framework proposed in this paper, which, with the assistance of SLM and RAG technologies, constructs four main processes: SLM-based Query Generation, SLM-based Query Generation, query refinement based on predicted schema and retrieved examples, and execution results-based query optimization. To validate the versatility of SMART, we conducted experiments on both \texttt{gpt-4o-mini} and \texttt{deepseek-v3}.
\end{itemize}

\subsubsection{Evaluation Metrics}
\label{sec:metric}
To evaluate the performance of models on the Text-to-NoSQL task, we introduce several metrics:

\begin{itemize}[leftmargin=0pt]
    \item \textbf{Exact Match (EM)}: This metric evaluates whether the generated query exactly matches the gold query in both structure and content. It is calculated as:\\
    \makecell[c]{\texttt{EM = \text{N}\textsubscript{em} / N}}\\
    where \text{N}\textsubscript{em} is the number of queries that fully match the gold query, and \text{N} is the total number of queries in the test set. EM provides a strict measure of syntactic and semantic alignment.
    \begin{itemize}[leftmargin=5pt]
        \item \textbf{Query Stages Match (QSM)}: QSM assesses whether the key stages (e.g., \texttt{match}, \texttt{group}, \texttt{lookup}) in the generated query match the gold query in terms of order and keywords. It is computed as:\\
        \makecell[c]{\texttt{QSM = \text{N}\textsubscript{qsm} / N}}\\
        where \text{N}\textsubscript{qsm} is the number of queries with matching stages.
        \item \textbf{Query Fields Coverage (QFC)}: QFC measures whether the fields in the generated query cover all fields in the gold query, including database fields and query-defined fields. It is defined as:\\
        \makecell[c]{\texttt{QFC = \text{N}\textsubscript{qfc} / N}}\\
        where \text{N}\textsubscript{qfc} is the number of queries with complete field coverage.
    \end{itemize}

    \item \textbf{Execution Accuracy (EX)}: This metric evaluates the correctness of the results obtained by executing the generated query on the database. It is calculated as:\\
    \makecell[c]{\texttt{EX = \text{N}\textsubscript{ex} / N}}\\
    where \text{N}\textsubscript{ex} is the number of queries whose execution results match those of the gold query. EX is the most critical performance metric for evaluating Text-to-NoSQL models.
    \begin{itemize}[leftmargin=5pt]
        \item \textbf{Execution Fields Match (EFM)}: EFM checks whether the field names in the execution results of the generated query match those of the gold query. It is computed as:\\
        \makecell[c]{\texttt{EFM = \text{N}\textsubscript{efm} / N}}\\
        where \text{N}\textsubscript{efm} is the number of queries with matching field names in the results.
        \item \textbf{Execution Value Match (EVM)}: EVM measures whether the values in the execution results of the generated query match those of the gold query. It is defined as:\\
        \makecell[c]{\texttt{EVM = \text{N}\textsubscript{evm} / N}}\\
        where \text{N}\textsubscript{evm} is the number of queries with matching values in the results.
    \end{itemize}
\end{itemize}

\subsubsection{Implementation Details}
The SLM used in SMART and Fine-tuned Llama is \texttt{Llama-3.2-1B}, which is fine-tuned using a full-parameter fine-tuning strategy with a batch size set to \texttt{4}. The LLM used in Instructing LLM, Few-shot LLM, RAG for LLM, and SMART is \texttt{deepseek-v3}, with the parameter configured as ``\texttt{temperature=0.0}''. The text-to-embedding model employed is ``\texttt{text-embedding-ada-002}''. The number of examples provided by Few-shot LLM, RAG for LLM, and SMART is \texttt{20}.


\subsection{Performance Comparison}
\begin{table}[t!]
    \begin{minipage}{\linewidth}
        \centering
        \scalebox{1.05}{
            \begin{tabular}{l|ccc}
            \toprule
            & \multicolumn{3}{c}{\textbf{Query-based Metric}} \\
            \midrule
            \textbf{Method} & EM & QSM & QFC \\
            \midrule
            Seq2Seq & 0.00\% & 0.00\% & 0.00\% \\
            Transformer & 0.00\% & 0.00\% & 0.00\% \\
            Instructing LLM & 5.91\% & 50.77\% & 64.32\% \\
            Few-shot LLM & 10.41\% & 48.40\% & 55.46\% \\
            RAG for LLM & 16.32\% & 63.06\% & 70.05\% \\
            Fine-tuned Llama & 20.54\% & 56.50\% & 67.68\% \\
            \textbf{SMART (gpt-4o-mini)} & 21.77\% & 60.65\% & 72.86\% \\
            \textbf{SMART (deepseek-v3)} & \textbf{23.82\%} & \textbf{63.21\%} & \textbf{75.60\%} \\
            \bottomrule
          \end{tabular}
        }
    \caption*{(a) Query-based Metric Results}
    \end{minipage}

    \vspace{-10pt}

    \begin{minipage}{\linewidth}
        \centering
        \scalebox{1.05}{
            \begin{tabular}{l|ccc}
            \toprule
            & \multicolumn{3}{c}{\textbf{Execution-based Metric}} \\
            \midrule
            \textbf{Method} & EX & EFM & EVM \\
            \midrule
            Seq2Seq & 0.00\% & 0.00\% & 0.00\% \\
            Transformer & 0.00\% & 0.00\% & 0.00\% \\
            Instructing LLM & 35.06\% & 53.05\% & 58.23\% \\
            Few-shot LLM & 35.82\% & 66.99\% & 68.25\% \\
            RAG for LLM & 53.26\% & 73.95\% & 62.02\% \\
            Fine-tuned Llama & 53.12\% & 84.36\% & 68.11\% \\
            \textbf{SMART (gpt-4o-mini)} & 59.57\% & 84.90\% & 69.73\% \\
            \textbf{SMART (deepseek-v3)} & \textbf{65.08\%} & \textbf{87.21\%} & \textbf{72.79\%} \\
            \bottomrule
          \end{tabular}
        }
    \caption*{(b) Execution-based Metric Results}
    \end{minipage}
\vspace{-10pt}
\caption{Performance Comparison}
\vspace{-30pt}
\label{tab:results}
\end{table}

As shown in Table~\ref{tab:results}, the Seq2Seq and Transformer models were completely unable to achieve Text-to-NoSQL predictions. By analyzing the prediction results of these two models, we found that the model outputs failed to grasp the code logic of special symbols such as '\texttt{()}', '\texttt{{}}', '\texttt{[]}', '\texttt{.}', and '\texttt{\$}'. This led to a large amount of special symbol redundancy in the model outputs, such as ``\texttt{db $\dots$ aggregate ( [ \{ \$ : \{ \$ : \{}''. This issue is caused by the NoSQL syntax structure and represents a significant challenge in the Text-to-NoSQL task.


In addition, the three baseline methods based on the prompting method-Instructing LLM, Few-shot LLM, and RAG for LLM-achieved notable performance in the most critical metric, EX, with RAG for LLM particularly reaching an execution accuracy of 52.76\%. The Fine-tuned Llama, obtained through supervised fine-tuning, delivered comparable performance to RAG for LLM, achieving an execution accuracy of 53.12\%. These results indicate that even simple methods can yield satisfactory outcomes in the Text-to-NoSQL task, validating the feasibility of the task. The framework we designed, SMART, excelled in this task, achieving an accuracy of 65.08\% in Execution Accuracy. This fully demonstrates the advanced nature of SMART. Moreover, SMART achieved the best performance across all detailed metrics, indicating that it outperforms all baseline methods in terms of field usage and operation stages, understanding of field mapping, and data processing.

Text-to-SQL is a well-developed field, and we need to explore the cascading approach of Text-to-SQL and SQL-to-NoSQL to validate the value of the Text-to-NoSQL task. However, due to limited space in the main text, we have placed the discussion and analysis of SQL-to-NoSQL technology in the technical report mentioned in the footnote of Section~\ref{sec:intro}.



\subsection{Parameter Study}

To explore the performance of SMART under different parameters, we conducted parameter experiments on the test set of the TEND dataset, varying the number of retrieval examples. As shown in Figure~\ref{fig:hyper_parameter_study}, we found that as the number of retrieval examples increased, SMART exhibited different performance curves across various metrics. Specifically, the accuracy of SMART on query-based metrics initially decreased and then increased. In contrast, the execution-based metrics showed a fluctuating trend. Overall, EX is the metric that best reflects the model's performance in real-world scenarios. When focusing on the EX metric, SMART achieved the highest execution accuracy of 65.08\% with 20 retrieval examples.

\begin{figure}[t!]
    \centering
    \subfigure[{Query-based Metric}]{
        \includegraphics[width=0.22\textwidth]{figure/para_query.png}
        \label{fig:para_query}
    }
    \subfigure[{Execution-based Metric}]{
        \includegraphics[width=0.22\textwidth]{figure/para_exec.png}
        \label{fig:para_exec}
    }
    \vspace{-10pt}
    \caption{Parameter study. These are the variation curves of SMART under different numbers of retrieval examples and different metrics. The vertical axis represents the model's accuracy under a specific metric, and the horizontal axis represents the number of retrieval examples.}
    \label{fig:hyper_parameter_study}
\vspace{-10pt}
\end{figure}

\subsection{Ablation Study}
In this section, we conduct ablation experiments to examine the effectiveness and contribution of each major process and component in SMART. Specifically, we first evaluate SMART with all major processes included. Then, we remove or replace some key processes in SMART and assess its performance under the following configurations: (i) removing SLM-based Query Generation (\textbf{w/o SP}); (ii) removing Predicted Schema-driven and Retrieved Example-driven Query Refinement (\textbf{w/o RF}); (iii) removing execution results-based query optimization (\textbf{w/o OPT}); (iv) using only SLM-based Query Generation (\textbf{only GEN}). Additionally, we include the experimental results of \texttt{Instructing LLM} and \texttt{RAG for LLM} from the table as part of the ablation study. The purpose is to demonstrate that the high accuracy performance of SMART primarily stems from our designed complex framework rather than solely from the inherent strong performance of the LLM itself.


The results of the ablation experiments are shown in Table~\ref{tab:ablation_study}. The SMART configuration with all processes included performs slightly lower on all metrics except for the \texttt{Query-based} ones compared to other configurations, but it leads in the \texttt{Execution-based} metrics across all experimental setups. The \texttt{w/o RF} configuration performs best on the EM and QSM metrics, indicating that providing the LLM with the query and its execution results during the execution results-based query optimization process allows the LLM to better understand the structure of the query itself. However, on the EX metric, which best reflects the model's performance in real-world scenarios, SMART with all processes included outperforms all other configurations. This validates that all components in SMART effectively contribute to the overall framework. Furthermore, SMART with all major processes included significantly outperforms the \texttt{Instructing LLM}, \texttt{RAG for LLM} and \texttt{only GEN} configurations on the important EX and EM metrics. This indicates that the high accuracy of SMART primarily stems from the framework itself rather than the understanding and execution capabilities of the LLM or fine-tuned SLM in the Text-to-NoSQL task.

\begin{table}[t!]
    \begin{minipage}{\linewidth}
        \centering
        \scalebox{1.05}{
            \begin{tabular}{l|ccc}
            \toprule
            & \multicolumn{3}{c}{\textbf{Query-based Metric}} \\
            \midrule
            \textbf{Method} & EM & QSM & QFC \\
            \midrule
            Instructing LLM & 5.91\% & 50.77\% & 64.32\% \\
            RAG for LLM & 37.74\% & 62.10\% & 51.57\% \\
            \textbf{SMART (Ours)} & 23.82\% & 63.21\% & 75.60\% \\
            $\quad$ - w/o SP & 23.78\% & 63.53\% & \textbf{76.65\%} \\
            $\quad$ - w/o RF & \textbf{24.07\%} & \textbf{63.64\%} & 75.28\% \\
            $\quad$ - w/o OPT & 23.06\% & 63.21\% & 75.71\% \\
            $\quad$ - only GEN & 20.54\% & 56.50\% & 67.68\% \\
            \bottomrule
          \end{tabular}
        }
    \caption*{(a) Query-based Metric Results}
    \end{minipage}

    \vspace{-10pt}

    \begin{minipage}{\linewidth}
        \centering
        \scalebox{1.05}{
            \begin{tabular}{l|ccc}
            \toprule
            & \multicolumn{3}{c}{\textbf{Execution-based Metric}} \\
            \midrule
            \textbf{Method} & EX & EFM & EVM \\
            \midrule
            Instructing LLM & 35.06\% & 53.05\% & 58.23\% \\
            RAG for LLM & 57.02\% & 74.61\% & 71.29\% \\
            \textbf{SMART (Ours)} & \textbf{65.08\%} & \textbf{87.21\%} & \textbf{72.79\%} \\
            $\quad$ - w/o SP & 63.78\% & 85.80\% & 71.24\% \\
            $\quad$ - w/o RF & 62.85\% & 87.17\% & 70.88\% \\
            $\quad$ - w/o OPT & 63.78\% & 86.85\% & 72.47\% \\
            $\quad$ - only GEN & 53.12\% & 84.36\% & 68.11\% \\
            \bottomrule
          \end{tabular}
        }
    \caption*{(b) Execution-based Metric Results}
    \end{minipage}
\vspace{-15pt}
\caption{Ablation study. This table shows the performance of SMART on various metrics after removing each component.}
\label{tab:ablation_study}
\vspace{-25pt}
\end{table}

\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{6pt}
    
    % 字体颜色配置
    \definecolor{tabletext}{RGB}{0, 0, 0}        % 文本色
    \definecolor{resultcolor}{RGB}{0, 130, 0}    % 成功结果色
    \definecolor{errorcolor}{RGB}{220, 0, 0}      % 错误结果色加深
    % \definecolor{tabletext}{RGB}{30, 41, 59}     % MongoDB语法
    \definecolor{codebg}{RGB}{243, 244, 246}        % 代码背景色
    
    \resizebox{\textwidth}{!}{
    \begin{tabular}{>{\raggedright\arraybackslash}p{0.06\linewidth}|>{\raggedright\arraybackslash}p{0.82\linewidth}}
        \toprule[1pt]        
        \midrule[0.4pt]
        {\color{tabletext}\large\textbf{NLQ}} & {\color{tabletext}\large\textbf{What are the dates of completion for tests that have received a failing result}} \\
        
        \midrule[0.4pt]
        {\color{tabletext}\large\textbf{Target NoSQL}} & 
        \begin{minipage}[t]{\linewidth}
        \ttfamily\small\colorbox{codebg}{%
        \begin{minipage}[t]{\dimexpr\linewidth-2\fboxsep\relax}
        \raggedright
        \textcolor{tabletext}{db.Subjects.}\textcolor{tabletext}{aggregate}\textcolor{tabletext}{([} \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses.Student\_Course\_Enrolment" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{}  \textcolor{tabletext} {\$unwind}: "\$Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$match}: \{ "Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken.test\_result": "Fail" \} \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$project}: \{ date\_of\_completion: "\$Courses.Student\_Course\_Enrolment.date\_of\_completion", \_id: 0 \} \textcolor{tabletext}{\}} \textcolor{tabletext}{]);}
        \end{minipage}}\\[4pt]
        \textcolor{tabletext}{\small Standard query: Correct usage of nested field paths, returns 7 results} \\[2pt]
        {\color{resultcolor}\small Result: [\{'date\_of\_completion':'2018-03-04 01:24:56'\},\dots]}
        \end{minipage} \\[6pt]
        
        \midrule[0.4pt]
        {\color{tabletext}\large\textbf{RAG for LLM}} & 
        \begin{minipage}[t]{\linewidth}
        \ttfamily\small\colorbox{codebg}{%
        \begin{minipage}[t]{\dimexpr\linewidth-2\fboxsep\relax}
        \raggedright
        \textcolor{tabletext}{db.Subjects.}\textcolor{tabletext}{aggregate}\textcolor{tabletext}{([} \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses.Student\_Course\_Enrolment" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$match}: \{ "Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken.test\_result": "Fail" \} \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$project}: \{ \_id: 0, \textcolor{errorcolor}{date\_test\_taken: "\$Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken.date\_test\_taken"} \} \textcolor{tabletext}{\}} \textcolor{tabletext}{]);}
        \end{minipage}}\\[4pt]
        \textcolor{tabletext}{\small Error: Incorrect field name (date\_test\_taken) used, leading to unexpected results} \\[2pt]
        {\color{errorcolor}\small Result: [\{'date\_test\_taken':'2018-03-22 13:10:06'\},\dots]}
        \end{minipage} \\[6pt]
        
        \midrule[0.4pt]
        {\color{tabletext}\large\textbf{Fine-tuned Llama}} & 
        \begin{minipage}[t]{\linewidth}
        \ttfamily\small\colorbox{codebg}{%
        \begin{minipage}[t]{\dimexpr\linewidth-2\fboxsep\relax}
        \raggedright
        \textcolor{tabletext}{db.}\textcolor{errorcolor}{Courses}\textcolor{tabletext}{.}\textcolor{tabletext}{aggregate}\textcolor{tabletext}{([} \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses.Student\_Course\_Enrolment" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$match}: \{ "Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken.test\_result": "Fail" \} \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$project}: \{ date\_of\_completion: "\$Courses.Student\_Course\_Enrolment.date\_of\_completion", \_id: 0 \} \textcolor{tabletext}{\}} \textcolor{tabletext}{]);}
        \end{minipage}}\\[4pt]
        \textcolor{tabletext}{\small Error: Wrong collection name (Courses instead of Subjects) used, resulting in empty output} \\[2pt]
        {\color{errorcolor}\small Result: []}
        \end{minipage} \\[6pt]
        
        \midrule[0.4pt]
        {\color{tabletext}\large\textbf{SMART (Ours)}} & 
        \begin{minipage}[t]{\linewidth}
        \ttfamily\small\colorbox{codebg}{%
        \begin{minipage}[t]{\dimexpr\linewidth-2\fboxsep\relax}
        \raggedright
        \textcolor{tabletext}{db.Subjects.}\textcolor{tabletext}{aggregate}\textcolor{tabletext}{([} \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses.Student\_Course\_Enrolment" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$unwind}: "\$Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken" \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$match}: \{ "Courses.Student\_Course\_Enrolment.Student\_Tests\_Taken.test\_result": "Fail" \} \textcolor{tabletext}{\}}, \textcolor{tabletext}{\{} \textcolor{tabletext}{\$project}: \{ date\_of\_completion: "\$Courses.Student\_Course\_Enrolment.date\_of\_completion", \_id: 0 \} \textcolor{tabletext}{\}} \textcolor{tabletext}{]);}
        \end{minipage}}\\[4pt]
        \textcolor{tabletext}{\small Success: Correct query structure and field usage, demonstrating accurate understanding of schema and requirements} \\[2pt]
        {\color{resultcolor}\small Result: [\{'date\_of\_completion':'2018-03-04 01:24:56'\},\dots]}
        \end{minipage} \\[6pt]
        
        \bottomrule[1.2pt]
    \end{tabular}
    }
    \caption{Case Study. Listed are the NoSQL queries generated by SMART alongside two representative baselines—RAG for LLM and Fine-tuned—using the Prompting method and fine-tuning method, respectively, along with their corresponding execution results (errors are marked in \textcolor{errorcolor}{red}, and correct results are marked in \textcolor{resultcolor}{green}).}
    \label{tab:case_study}
\vspace{-20pt}
\end{table*}

\subsection{Case Study}

Table~\ref{tab:case_study} presents a case study comparing the NoSQL queries generated by various baseline methods and SMART, along with their execution results. Due to space limitations, we only showcase the performance of the most representative baseline models, including RAG for LLM, which represents prompting methods, and Fine-tuned Llama, which represents model training methods. In the example from Table~\ref{tab:case_study}, RAG for LLM can correctly generate query stages and the most operations but fails to accurately establish the mapping between the nested structure of the database and the NLQ, leading to the inference of the wrong field ``date\_test\_taken''. On the other hand, Fine-tuned Llama predicted the correct phase and all data processing steps, but it made an error in the simplest part by inferring a collection name that does not exist in the database. Such errors are common in end-to-end models, as they tend to rely heavily on training data. When faced with unfamiliar databases, they are prone to model hallucinations influenced by the training data. In contrast, SMART is able to accurately generate the correct query, demonstrating that SMART can effectively understand and execute the Text-to-NoSQL task and can predict test set examples well after learning from the training set.

\section{Related Work}
\label{sec:related_work}

Our work intersects with three key research areas: NoSQL databases, text-to-SQL systems, and natural language interfaces for data systems. In this section, we review relevant studies from each field.

\subsection{NoSQL Databases}
NoSQL databases have become a crucial component of modern data management systems, primarily due to their flexibility, scalability, and high performance. These attributes are essential for handling the large volumes of unstructured and semi-structured data commonly encountered in contemporary applications. NoSQL databases allow for efficient horizontal scaling and offer robust options for distributed data storage, making them an ideal choice for large-scale applications that require high availability and fault tolerance.

In the fields of databases and data mining, current research focuses on several key areas within NoSQL databases. These include enhancing scalability and reliability in distributed environments \cite{10.1145/1978915.1978919}, improving query capabilities for efficient large-scale data processing \cite{MAHAJAN2019120}, and ensuring data security and privacy in the context of increasing cloud deployments \cite{10.1109/TrustCom.2011.70}. Additionally, many scholars are exploring the differences between NoSQL and SQL, aiming to uncover the development potential and future directions of NoSQL technologies.

Despite the maturation of NoSQL-related technologies, challenges remain in terms of accessibility and usability, particularly for users unfamiliar with specific query languages. This paper introduces the Text-to-NoSQL task, which aims to enable users to interact with databases using intuitive NLQs. This technology not only broadens the user base but also enhances the efficiency and productivity of data management operations by bridging the gap between technical and user language capabilities.

\vspace{-5pt}
\subsection{Text-to-SQL}
Text-to-SQL aims to automatically convert NLQs into SQL queries, effectively bridging the gap between non-expert users and traditional relational database systems. This task has been extensively studied within both the database and natural language processing communities. Early research efforts relied on predefined rules or query enumeration to tackle the Text-to-SQL task \cite{10.14778/3407790.3407858,DBS-078,10.1145/3318464.3389776}, or treated it as a sequence-to-sequence problem, leveraging encoder-decoder architectures to predict the final SQL queries \cite{qi-etal-2022-rasat,popescu-etal-2022-addressing,10.5555/3304222.3304323}. With the rapid advancement of deep learning, numerous techniques such as attention mechanisms \cite{10191914}, graph representations \cite{hui-etal-2022-s2sql,10.1609/aaai.v37i11.26536,qi-etal-2022-rasat,wang-etal-2020-rat,xu-etal-2018-sql,zheng-etal-2022-hie}, and syntax parsing \cite{guo-etal-2019-towards,10.1609/aaai.v37i11.26535,scholak-etal-2021-picard,10.1145/3534678.3539305} have been employed to aid the Text-to-SQL task, resulting in significant improvements. To evaluate the performance of Text-to-SQL models in real-world scenarios, several large-scale benchmark datasets have been constructed and released, including WikiSQL \cite{zhong2017seq2sqlgeneratingstructuredqueries}, Spider \cite{yu-etal-2018-spider}, KaggleDBQA \cite{lee-etal-2021-kaggledbqa}, and BIRD \cite{NEURIPS2023_83fc8fab}. 

With the advent of large language models (LLMs), the Text-to-SQL task has entered a new era of development \cite{10.14778/3641204.3641221,NEURIPS2023_72223cc6,dong2023c3zeroshottexttosqlchatgpt,10.1145/3654930,nan-etal-2023-enhancing,ren2024purplemakinglargelanguage,10.1145/3589292}. By pre-training on massive text corpora, LLMs have developed strong natural language understanding capabilities, enabling them to accomplish various downstream tasks with input prompts. The paradigm of leveraging LLMs for Text-to-SQL tasks has emerged as the new mainstream approach. For instance, Ren \texttt{et al.} \cite{ren2024purplemakinglargelanguage} designed a more intricate pipeline involving Schema Pruning, Skeleton Prediction, and Demonstration Selection to generate SQL queries from LLM prompts. They further refined the SQL output by adjusting it to fit specific database schemas and SQL dialects and addressed hallucination issues.

Although significant progress has been made in Text-to-SQL conversion technology, research in the field of Text-to-NoSQL is still in its infancy. Moreover, due to substantial differences in database structures, query languages, and application scenarios, existing Text-to-SQL research cannot be directly applied to Text-to-NoSQL. Specifically, NoSQL databases typically feature more complex schemas, and query syntax varies significantly across different systems, posing substantial challenges for SQL-to-NoSQL conversion. As a result, achieving Text-to-NoSQL through a cascaded approach of Text-to-SQL and SQL-to-NoSQL is not only technically demanding but also resource-intensive. This conclusion has been experimentally validated in the technical report cited in the footnote of Section~\ref{sec:intro}.

\subsection{Natural Language Interfaces for Data Systems}
The application of Natural Language Interfaces to Data Systems (NLIDS) in the industry has been evolving for a considerable period. Numerous applications have demonstrated the practicality of natural language interfaces in data interaction. For instance, Salesforce Einstein integrates natural language processing, enabling users to query Customer Relationship Management (CRM) data using simple language. Amazon QuickSight allows users to pose questions about their datasets without complex query syntax. Google Cloud's Natural Language API assists businesses in classifying and understanding text data. IBM Watson Assistant provides a conversational interface, allowing users to interact with backend systems via natural language. Microsoft Power BI features a Q\&A function that translates NLQs into visual data insights. These applications are designed to enhance data accessibility for non-technical users.

In the academic realm, research on NLIDS is similarly well-explored. Studies such as WikiSQL \cite{zhong2017seq2sql} and Spider \cite{yu-etal-2018-spider} have provided large-scale datasets to evaluate models that convert NLQs into SQL queries, assisting the examination of this conversion process in real-world systems. Ni \texttt{et al.} \cite{10.1007/s10796-022-10295-0} and Staniek \texttt{et al} \cite{staniek-etal-2024-text}. have explored the conversion of NLQs into query languages for graph databases and the Overpass geographic database, respectively, further expanding the research boundaries of NLIDS. Furthermore, Song \texttt{et al.}'s work \cite{song2024speech,song2022voicequerysystem} extends NLIDS model inputs from text to audio, highlighting further innovation in the field. A survey on NLIDS reviewed the current state of this technology and the open challenges it faces \cite{zcan2020StateOT}.

However, these studies have not directly addressed research on NLIDS for NoSQL databases. Therefore, in this work, we introduce a large-scale dataset, TEND, aimed at evaluating the ability of models to translate NLQs into query languages specific to NoSQL databases. This initiative is intended to foster further research on NLIDS for NoSQL databases within the database and data mining communities. In addition, we have developed SMART, a novel system, and have conducted extensive experiments to demonstrate the feasibility of this new task.

\section{Conclusion and Future Work} 
\label{sec:conclusion}
This study introduces the Text-to-NoSQL task for the first time, aiming to generate NoSQL queries from natural language queries to lower the barrier for non-technical users. To this end, we developed a novel dataset construction process and released a large-scale, open-source benchmark dataset called TEND. We also designed a multi-step framework based SLM and RAG, and established a set of evaluation metrics to measure model performance based on both the query and its execution results. Experimental results demonstrate the feasibility of the Text-to-NoSQL task and highlight the significant progress achieved by our proposed SMART model.

Although TEND provides a competitive benchmark platform for Text-to-NoSQL research and propels the advancement in this field, it is worth noting that its databases are converted from relational databases through graph algorithms. Therefore, constructing new Text-to-NoSQL datasets based on NoSQL databases from real-world application scenarios represents a promising and practically significant direction for future research.

In addition, SMART has demonstrated robust performance when dealing with unknown databases, significantly outperforming various end-to-end models such as Fine-tuned Llama. However, efficiency is also a crucial factor in practical applications. Exploring ways to enhance efficiency without compromising accuracy is a highly promising direction.

\section*{Acknowledge}
Yuanfeng Song is the corresponding author.


\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}

\section{Analysis of SQL-to-NoSQL Approaches}
Text-to-SQL, a classic task in the field of natural language processing, aims to automatically convert natural language queries into structured query language (SQL). It has a long research history and has accumulated a wealth of achievements \cite{yu-etal-2018-spider, finegan-dollak-etal-2018-improving, NEURIPS2023_72223cc6, wang-etal-2020-rat}. However, with the widespread adoption of NoSQL databases in various application scenarios, exploring how to extend Text-to-SQL technology to the NoSQL domain through a cascading approach is of significant importance. Specifically, this process can be divided into multiple stages: First, existing Text-to-SQL models (such as SQLNet, TypeSQL, RAT-SQL, etc.) are used to convert natural language queries into SQL queries. For example, given the natural language query "\textit{Find all users older than 30}", the model generates the corresponding SQL query: ``\texttt{SELECT * FROM users WHERE age > 30}''. Subsequently, based on the syntactic mapping relationship between SQL and NoSQL, the generated SQL query is converted into the query language of the target NoSQL database. This process requires in-depth analysis of the syntax and semantics of both query languages. For instance, for the document-oriented database MongoDB, the SQL ``SELECT'' statement can be mapped to the ``find'' operation, and the `WHERE` condition can be mapped to a query filter, thereby transforming the aforementioned SQL query into a MongoDB query: ``\texttt{db.users.find(\{age:\{\$gt:30\}\})}''.

% Text-to-SQL is a long-standing task in the field of natural language processing, and there has been a significant amount of outstanding research in this area \cite{yu-etal-2018-spider, finegan-dollak-etal-2018-improving, NEURIPS2023_72223cc6, wang-etal-2020-rat}. It is essential to explore whether this technology can be directly applied to the NoSQL domain through a cascading approach. Specifically, this involves using Text-to-SQL technology to first convert natural language queries into SQL queries, and then leveraging the correspondence between SQL and NoSQL queries, such as syntactic mapping relationships, to directly transform SQL queries into NoSQL queries. 

% 目前已有几个基于SQL和NoSQL之间的语法映射关系专门实现SQL-to-NoSQL的网站\cite{site24x7, javainuse}，以及GitHub项目，我们将TEND数据集的测试集(包含NoSQL查询及其对应的SQL查询)利用GitHub上的开源项目进行转换，得到Table 1和Table2中的SQL-to-NoSQL by Grammar，其中Table1是一些具有代表性的样例，Table2中展示的是转换后的nosql的准确率。
Currently, there are several websites specifically designed for SQL-to-NoSQL conversion based on the syntax mapping between SQL and NoSQL \cite{site24x7, javainuse}, as well as GitHub projects \cite{RusselSQLtoMongoDB}. We utilized an open-source project from GitHub to transform the test set of the TEND dataset (which includes NoSQL queries and their corresponding SQL queries), resulting in the SQL-to-NoSQL by Grammar presented in Table~\ref{tab:results_sql_to_nosql} (\textbf{Grammar Converter}) and Table~\ref{tab:sql_to_nosql_cases}. Table~\ref{tab:results_sql_to_nosql} displays the accuracy of the converted NoSQL queries, while Table~\ref{tab:sql_to_nosql_cases} showcases some representative examples.

\begin{table}[htbp]
  \begin{minipage}{\linewidth}
      \centering
      \scalebox{0.9}{
          \begin{tabular}{l|ccc}
          \toprule
          & \multicolumn{3}{c}{\textbf{Query-based}} \\
          \midrule
          \textbf{Method} & EM & QSM & QFC \\
          \midrule
          \textbf{SMART (Ours)} & 23.82\% & 63.21\% & 75.60\% \\
          \textbf{Grammar Converter} & 0.00\% & 17.12\% & 23.24\% \\
          \textbf{LLM Converter w. SQL Schema} & 10.09\% & 58.05\% & 64.90\% \\
          \textbf{LLM Converter w/o. SQL Schema} & 7.71\% & 57.80\% & 63.35\% \\

          \bottomrule
        \end{tabular}
      }
  \caption*{(a) Query-based Results}
  \end{minipage}

  \vspace{-10pt}

  \begin{minipage}{\linewidth}
      \centering
      \scalebox{0.9}{
          \begin{tabular}{l|ccc}
          \toprule
          & \multicolumn{3}{c}{\textbf{Execution-based}} \\
          \midrule
          \textbf{Method} & EX & EFM & EVM \\
          \midrule
          \textbf{SMART (Ours)} & 65.08\% & 87.21\% & 72.29\% \\
          \textbf{Grammar Converter} & 10.81\% & 70.45\% & 67.57\% \\
          \textbf{LLM Converter w. SQL Schema} & 44.76\% & 71.57\% & 73.55\% \\
          \textbf{LLM Converter w/o. SQL Schema} & 47.68\% & 69.19\% & 73.80\% \\
          \bottomrule
        \end{tabular}
      }
  \caption*{(b) Execution-based Results}
  \end{minipage}
\vspace{-15pt}
\caption{Experimental Results of SQL-to-NoSQL.}
\label{tab:results_sql_to_nosql}
\vspace{-25pt}
\end{table}

\begin{table*}[htbp]
    \centering
    \begin{tabular}{p{0.2\linewidth}|p{0.75\linewidth}}
        \toprule 
        \multicolumn{2}{c}{\textbf{\textcolor{darkblue}{Easy Find Query}}} \\
        \midrule
        \textbf{SQL} & SELECT Address FROM Restaurant WHERE ResName = "Subway"; \\
        \textbf{Target NoSQL} & db.Restaurant.find(\{ "ResName": "Subway" \}, \{ "Address": 1, "\_id": 0 \});\\
        \textbf{Converted NoSQL} & db.Restaurant.find(\{ "ResName" : "Subway" \},\{ "Address": 1 \} );\\
        \midrule

        \multicolumn{2}{c}{\textbf{\textcolor{darkblue}{Medium Find Query}}} \\
        \midrule
        \textbf{SQL} & SELECT last\_name FROM staff WHERE email\_address LIKE "\%wrau\%" \\
        \textbf{Target NoSQL} & db.Staff.find( \{ email\_address: \{ \$regex: "wrau", \$options: "i" \} \}, \{ last\_name: 1, \_id: 0 \} );\\
        \textbf{Converted NoSQL} &  db.staff.find(\{ "email\_address": \textcolor{c2}{ \{ "\$regex": "\textasciicircum.*wrau.*\$" \} \}} , \{ "\_id": 0, "last\_name": 1 \})\\
        \midrule

        \multicolumn{2}{c}{\textbf{\textcolor{darkblue}{Hard Find Query}}} \\
        \midrule
        \textbf{SQL} & SELECT T2.Fname , T2.Lname FROM COURSE AS T1 JOIN FACULTY AS T2 ON T1.Instructor = T2.FacID WHERE T1.CName = "COMPUTER LITERACY"; \\
        \textbf{Target NoSQL} & db.Faculty.find( \{ "Course.CName": "COMPUTER LITERACY" \}, \{ "Fname": 1, "Lname": 1, "\_id": 0 \} ); \\
        \textbf{Converted NoSQL} & \textcolor{c2}{db.COURSE.aggregate([\{ "\$match": \{ "CName": "COMPUTER LITERACY" \} \},\{ "\$lookup": \{ "from": "FACULTY", "let": \{ "instructor": "\$Instructor" \}, "pipeline": [ \{ "\$match": \{ "\$expr": \{ "\$eq": [ "\$\$instructor", "\$FacID" ] \} \} \} ], "as": "T2" \} \},\{ "\$unwind": \{ "path": "\$T2", "preserveNullAndEmptyArrays": false \} \},\{ "\$project": \{ "\_id": 0, "T2.Fname": 1, "T2.Lname": 1 \} \}])}\\
        \midrule
        
        \multicolumn{2}{c}{\textbf{\textcolor{darkblue}{Easy Aggregate Query}}} \\
        \midrule
        \textbf{SQL} & SELECT Nationality FROM pilot GROUP BY Nationality ORDER BY COUNT(*) DESC LIMIT 1 \\
        \textbf{Target NoSQL} & db.pilot.aggregate([ \{ \$group: \{ \_id: "\$Nationality", count: \{ \$sum: 1 \} \} \}, \{ \$sort: \{ count: -1 \} \}, \{ \$limit: 1 \}, \{ \$project: \{ \_id: 0, Nationality: "\$\_id" \} \} ]);\\
        \textbf{Converted NoSQL} & db.pilot.aggregate([\{ "\$group": \{ "\_id": "\$Nationality" \} \textcolor{c2}{\},\{ "\$sort": \{ "count": -1 \} \},\{ "\$limit": 1 \},\{ "\$project": \{ "Nationality": "\$\_id", "\_id": 0 \} \}])}\\

        \midrule
        
        \multicolumn{2}{c}{\textbf{\textcolor{darkblue}{Medium Aggregate Query}}} \\
        \midrule
        \textbf{SQL} & SELECT company , main\_industry FROM company WHERE company\_id NOT IN (SELECT company\_id FROM station\_company) \\
        \textbf{Target NoSQL} & db.company.aggregate([ \{ \$lookup: \{ from: "gas\_station", localField: "Company\_ID", foreignField: "station\_company.Company\_ID", as: "Docs1" \} \}, \{ \$match: \{ Docs1: \{ \$size: 0 \} \} \}, \{ \$project: \{ Company: 1, Main\_Industry: 1, \_id: 0 \} \} ]);\\
        \textbf{Converted NoSQL} & \textcolor{c2}{Converting Error}\\
        \midrule
        
        \multicolumn{2}{c}{\textbf{\textcolor{darkblue}{Hard Aggregate Query}}} \\
        \midrule
        \textbf{SQL} &  SELECT T1.first\_name , T1.last\_name , T1.employee\_id , T4.country\_name FROM employees AS T1 JOIN departments AS T2 ON T1.department\_id = T2.department\_id JOIN locations AS T3 ON T2.location\_id = T3.location\_id JOIN countries AS T4 ON T3.country\_id = T4.country\_id \\
        \textbf{Target NoSQL} & db.departments.aggregate([ \{ \$unwind: "\$employees" \}, \{ \$lookup: \{ from: "regions", let: \{ location\_id: "\$LOCATION\_ID" \}, pipeline: [ \{ \$unwind: "\$countries" \}, \{ \$unwind: "\$countries.locations" \}, \{ \$match: \{ \$expr: \{ \$eq: ["\$countries.locations.LOCATION\_ID", "\$\$location\_id"] \} \} \}, \{ \$project: \{ COUNTRY\_NAME: "\$countries.COUNTRY\_NAME" \} \} ], as: "Docs1" \} \}, \{ \$unwind: "\$Docs1" \}, \{ \$project: \{ FIRST\_NAME: "\$employees.FIRST\_NAME", LAST\_NAME: "\$employees.LAST\_NAME", EMPLOYEE\_ID: "\$employees.EMPLOYEE\_ID", COUNTRY\_NAME: "\$Docs1.COUNTRY\_NAME", \_id: 0 \} \} ]);\\
        \textbf{Converted NoSQL} & \textcolor{c2}{db.employees.aggregate([\{ "\$lookup": \{ "from": "departments", "let": \{ "department\_id": "\$department\_id" \}, "pipeline": [ \{ "\$match": \{ "\$expr": \{ "\$eq": [ "\$\$department\_id", "\$department\_id" ] \} \} \} ], "as": "T2" \} \},\{ "\$unwind": \{ "path": "\$T2", "preserveNullAndEmptyArrays": false \} \},\{ "\$lookup": \{ "from": "locations", "let": \{ "t2\_location\_id": "\$T2.location\_id" \}, "pipeline": [ \{ "\$match": \{ "\$expr": \{ "\$eq": [ "\$\$t2\_location\_id", "\$location\_id" ] \} \} \} ], "as": "T3" \} \},\{ "\$unwind": \{ "path": "\$T3", "preserveNullAndEmptyArrays": false \} \},\{ "\$lookup": \{ "from": "countries", "let": \{ "t3\_country\_id": "\$T3.country\_id" \}, "pipeline": [ \{ "\$match": \{ "\$expr": \{ "\$eq": [ "\$\$t3\_country\_id", "\$country\_id" ] \} \} \} ], "as": "T4" \} \},\{ "\$unwind": \{ "path": "\$T4", "preserveNullAndEmptyArrays": false \} \},\{ "\$project": \{ "\_id": 0, "first\_name": 1, "last\_name": 1, "employee\_id": 1, "T4.country\_name": 1 \} \}])}\\
        \bottomrule  
    \end{tabular}

    \caption{Samples of converting SQL to NoSQL through grammatical mapping relationships using SQL-to-NoSQL converter \cite{RusselSQLtoMongoDB}. (Errors are marked with \textcolor{c2}{red} colors)}
    \label{tab:sql_to_nosql_cases}
\end{table*}

From the experimental results presented in Table~\ref{tab:results_sql_to_nosql}, we have observed that direct conversion based on grammatical rules is not feasible. This is fundamentally because such an approach entirely overlooks the significant differences between relational databases and NoSQL databases, culminating in a mere 10.09\% accuracy rate on the most crucial metric — EX. This level of performance is far from sufficient to meet the demands of practical applications. Concurrently, we have analyzed the error samples from the Grammar Converter, with several representative examples showcased in Tablee~\ref{tab:sql_to_nosql_cases}. Based on the displayed examples, it is evident that the method of directly converting SQL queries into NoSQL queries according to grammatical mapping relationships can only be correct when the converted NoSQL query is a simple ``find'' query. However, this method becomes unfeasible when the NoSQL query to be converted becomes complex. Currently, large language models (LLMs) are revolutionizing and optimizing solutions across various research domains. The SQL-to-NoSQL conversion process might benefit from leveraging LLMs' understanding of both programming languages. We attempted to provide an LLM (such as \texttt{Deepseek-v3}) with ground truth SQL queries and a MongoDB database schema, instructing it to translate the SQL queries into corresponding MongoDB queries. The experimental setup was as follows: (i) Providing an SQL query, the complete database schema corresponding to the SQL query, and the complete MongoDB database schema (LLM Converter w. SQL Schema); (ii) Not providing the SQL database schema (LLM Converter w/o. SQL Schema). The experimental results obtained are also presented in Table~\ref{tab:results_sql_to_nosql}.


Based on the results presented in Table~\ref{tab:results_sql_to_nosql}, we have found that leveraging LLMs for SQL-to-NoSQL conversion demonstrates a certain level of feasibility. By furnishing the LLM with precise SQL queries, the corresponding database schema for the SQL queries, and the NoSQL database schema, we achieved an execution accuracy of 44.76\%. This figure significantly surpasses the 10.81\% accuracy rate attained by the Grammar Converter. However, it is important to note that in practical applications, obtaining accurate SQL queries is not always accurate. Incorrect SQL queries may lead to error accumulation, potentially resulting in a significant decline in accuracy. Therefore, to pursue higher accuracy, it may be necessary to design a more complex LLM framework, which represents a promising direction for further research.


\end{document}
\endinput
