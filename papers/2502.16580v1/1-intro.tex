\section{Introduction}
With rapidly advancing technologies, large language models (LLMs) have demonstrated remarkable performance across various NLP tasks \cite{Chen2021EvaluatingLL, Kojima2022LargeLM, zhou2023leasttomost}. However, their intrinsic instruction-following capabilities render them susceptible to prompt injection attacks, which manipulate LLMs into deviating from the \textbf{original input instructions} and executing malicious instructions injected in the data content.
Prompt injection attacks can be broadly categorized into direct attacks \cite{perez2022ignore, chen2024struq} and indirect attacks \cite{greshake2023not,li2023evaluating, zhan2024injecagent}. 
For direct prompt injection attacks, the attackers, who are also the users, directly inject instructions into the prompt for malicious purposes, such as application prompt extraction \cite{perez2022ignore} as shown in Figure \ref{fig:intro} (a). Because of their inability to distinguish the instructions to execute, the LLMs execute the \textbf{injected instructions} and give undesired responses.  
On the other hand, for indirect prompt injection attacks which have more application scenarios, the users are the victims. Attackers inject malicious instructions within external data sources, such as web documents, which are later retrieved by LLMs using external tools. For example, as shown in Figure \ref{fig:intro} (b), when an LLM processes these \textbf{injected documents}, it identifies the injected instructions and executes them, resulting in unintended responses. The indirect prompt injection attacks are much more practical because they can be employed to achieve different purposes \cite{liu2024automatic, shu2023exploitability} and to target a wide range of applications \cite{greshake2023not}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/intro.pdf}
    \caption{(a) represents a direct prompt injection attack example and (b) illustrates an indirect prompt injection attack example.}
    \label{fig:intro}
    \vspace{-5pt}
\end{figure*}

% Most of current defense methods fall into two categories: fine-tuning-based approaches \cite{chen2024struq,wallace2024instruction,suo2024signed,piet2023jatmo} and prompt-engineering-based strategies \cite{hines2024defending,sandwich_defense_2023,instruction_defense_2023,willison_2023}. Fine-tuning-based methods, such as StruQ \cite{chen2024struq}, deliberately introduce the attack into the input and fine-tune the LLMs to respond solely to the original input instruction, effectively training the models to ignore injected instructions. While these methods are effective, their high computational resource requirements make them sometimes impractical for developers.
% In contrast, prompt-engineering-based approaches focus on guiding LLMs to treat injected instructions as part of the data content. These methods avoid the need for fine-tuning but rely on carefully crafted prompts to achieve effectiveness, making them labor-intensive to design.
% To defend against such attacks, one approach involves instructing LLMs not to execute injected instructions within the data content \cite{hines2024defending,sandwich_defense_2023,instruction_defense_2023,willison_2023,chen2024struq,wallace2024instruction}. For example, \citet{chen2024struq} and \citet{wallace2024instruction} fine-tune the LLMs to respond solely to the original input instruction while ignoring the injected instructions. Although these methods are effective, their high computational resource requirements make them sometimes impractical for developers. As an alternative, other works \cite{hines2024defending,sandwich_defense_2023,instruction_defense_2023,willison_2023} leverage prompt engineering to guide LLMs in treating injected instructions as part of the data content. These methods avoid the need for fine-tuning but rely on carefully crafted prompts to achieve effectiveness, making them labor-intensive to design.

To defend against such attacks, one approach involves instructing LLMs not to execute injected instructions within the data content \cite{hines2024defending,sandwich_defense_2023,instruction_defense_2023,willison_2023,chen2024struq,wallace2024instruction}, including fine-tuning methods and prompt-engineering-based approaches. However, fine-tuning methods demand substantial computing resources, and prompt-engineering methods rely on carefully crafted prompts to achieve effectiveness. 
Another category of approaches relies on \textbf{filtering} \cite{deberta-v3-base-prompt-injection-v2, meta2024prompt,gorman2022jailbreaking}, i.e., detecting injected documents and removing injected instructions, as shown in Figure \ref{fig:pipeline}. For instance, \citet{deberta-v3-base-prompt-injection-v2} and \citet{meta2024prompt} train much smaller detection models to identify prompt injection attacks. These detection methods are less computationally demanding and do not require meticulous prompt crafting. However, they focus only on identifying prompt injection attacks, leaving the removal of malicious instructions largely unexplored. Moreover, most of them are solely evaluated on direct prompt injection attacks,  without considering indirect scenarios, which are often more practical. 

Recognizing this research gap, this paper focuses on exploring both the detection of indirect prompt injection attacks and the removal of injected instructions.
To achieve this objective, we first construct an evaluation benchmark consisting of documents from QA datasets \cite{rajpurkar-etal-2016-squad, 2017arXivtriviaqa} and manually crafted injected instructions. Using this benchmark, we assess the performance of current LLMs and detection models in identifying indirect prompt injection attacks. To further investigate the challenges associated with training detection models, we create additional training data specifically designed for indirect prompt injection detection and use it to train new detection models. After exploring the detection, we investigate the removal process with the previously crafted benchmark. We evaluate two intuitive removal methods: (1) \textit{Segmentation removal method}: This method divides injected documents into multiple segments, employs a detection model to classify each segment, and discards those identified as containing injected instructions. (2) \textit{Extraction removal method}: This approach involves training extraction models to identify and remove injected content directly from the documents. Finally, we combine detection and removal methods together as unified \textit{filtering methods} to evaluate the defense performance against indirect prompt injection attacks.

Our investigation yields several main observations: (1) Both instructed LLMs \cite{gorman2022jailbreaking} and open-source detection models \cite{deberta-v3-base-prompt-injection-v2, meta2024prompt} struggle to effectively detect indirect prompt injection attacks, whereas specifically trained models show satisfactory performance. (2) The over-defense problem (where the model misclassifies clean documents as injected documents) rarely occurs with in-domain documents. However, it occurs on out-of-domain documents. 
Moreover, stronger models and more fluent documents are less prone to this issue. (3) Both the segmentation and extraction removal methods can remove some of the injected instructions, but the segmentation method demonstrates better overall performance. However, the extraction method excels at removing injected instructions from the tail, which is the most effective attack position. (4) Combining the detection and removal methods as filtering methods is effective for defending against indirect prompt injection attacks.