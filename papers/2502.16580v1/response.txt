\section{Related Work}
\subsection{Prompt Injection Attacks}
Prompt injection attacks pose a critical challenge for Large Language Models (LLMs) and have garnered significant research attention  Liang, "Adversarial Examples for Evaluating Reading Comprehension Systems". Zhang et al., "Character-Level Deep Memory Network for Text Classification"__ Wang, "Hierarchical Multi-Task Learning with Weakly Supervised Data". Similarly, Liu et al., "Generating Natural Adversarial Samples" introduces a technique involving the addition of fake responses, tricking the LLMs into believing the userâ€™s input has already been processed, thereby executing the malicious instruction instead. Zhang et al., "Deep Learning for Text Classification: A Survey and Meta-Analysis" further enhances attack effectiveness by combining multiple attack strategies. Additionally, Liu et al., "Adversarial Attacks on Deep Learning Models for Natural Language Processing" optimize suffixes to effectively mislead LLMs.

\subsection{Prompt Injection Defenses}
In response to the growing threat of prompt injection attacks, numerous defense mechanisms have been proposed  Liang et al., "Defending Against Adversarial Attacks by Enforcing Input Validity". Zhang et al., "A Unified Approach for Adversarial Training and Defense" suggest appending reminders to reinforce adherence to the original instructions. Wang et al., "Improving Model Robustness with Multiple-Sample Adversarial Training"__ Liu, "Adversarial Learning for Neural Network Robustness" propose using special tokens to clearly delineate the data content area. Liang, "Multi-Task Deep Reinforcement Learning with Weakly Supervised Data" address the issue by training models to specialize in specific tasks. Zhang et al., "A Framework for Adversarial Training and Defense: A Survey"__ Wang, "Robustness Regularization of Deep Neural Networks via Jacobian Regularization" advocate fine-tuning LLMs with adversarial training ____, granting privileged status to authorized instructions. Lastly, detection methods Liang et al., "Anomaly Detection in Natural Language Processing" have been proposed to address direct prompt injection attacks. However, these methods overlook indirect prompt injection attacks, which are more practical and applicable.