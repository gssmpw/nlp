\section{Conclusion}
In this paper, we investigate the detection and removal of indirect prompt injection attacks. We construct two evaluation benchmarks containing injected instructions designed for different purposes, and craft training datasets to explore the training challenges. Through comprehensive experiments, we provide valuable insights into the effectiveness and limitations of current models and methods.
Our results reveal that existing models struggle to reliably detect indirect prompt injection attacks, and training the models faces challenges such as over-defense and position generalization issues. Additionally, there remains room for improvement in the effectiveness of removal methods.
% We hope our work inspires further research about defenses against indirect prompt injection attacks. 

\section*{Limitations}
In this paper, we conduct an empirical study on the detection and removal of indirect prompt injection attacks. Given the limited attention to removing injected instructions, we propose and evaluate two intuitive removal methods. These two methods are simple and easy to implement, but their performance is not entirely satisfactory, leaving room for future exploration of more effective and robust approaches. Moreover, our assessment does not consider direct prompt injection, because in real-world applications, direct and indirect prompt injection detection models are not mutually exclusive but coexist.
We hope our work inspires further research about defenses against indirect prompt injection attacks. 

\section*{Ethical Considerations}
All authors of this paper affirm their adherence to the ACM Code of Ethics and the ACL Code of Conduct. This work is primarily aimed at conducting empirical studies about defending against prompt injection attacks. The source code will be made publicly available. Additionally, we construct our benchmark and training data with existing datasets and the crafted injected instructions are not harmful or poisonous. This ensures that no new safety risks are introduced concerning unsafe data samples.