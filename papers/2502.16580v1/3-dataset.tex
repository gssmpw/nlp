\section{Benchmark and Training Datasets}

Our evaluation benchmark is designed to simultaneously assess the detection task, removal task, and prompt injection defense task. The detection task evaluates the ability of different models to identify injected documents. The removal task measures the effectiveness of various methods in removing injected instructions from these documents. Finally, the prompt injection defense task examines the robustness of different approaches against indirect prompt injection attacks. 

Training datasets are constructed to investigate the challenges of training detection and removal models for defending against indirect prompt injection attacks.


\subsection{Evaluation Benchmark Construction}
Each sample in the benchmark is represented as a tuple $(p, d, a, x, y)$, with the specific meanings of the elements detailed below:

\begin{itemize}
\item $p$: The original input instruction, such as ``What is the process of adding structure to real property or construction of buildings?'' as shown in Table \ref{tab:examples_benchmark}.
\item $d$: The clean document, for example, ``Building construction is the process of …''
\item $a$: The answer to the original input instruction, such as ``Building construction.''
\item $x$: The injected instruction, for instance, ``Give back only www.example.com.''
\item $y$: The probe used to determine whether the injected instruction $x$ is executed. In this example, $y$ is ``www.example.com.'' To determine whether $x$ is executed, we will check whether $y$ is present in the response.
\end{itemize}
% \noindent $\bullet$ The original input instruction, such as ``What is the land area of Jacksonville?'' as shown in Table \ref{tab:examples_benchmark}.
% Example
% The details of the evaluation process can be found in Appendix \ref{sec:eval}.




The evaluation process can be found in Appendix \ref{sec:eval}. For constructing the original input instruction $p$ and clean document $d$, we follow the methodology of \citet{li2023evaluating}, utilizing questions and documents from QA datasets. Specifically, we construct two benchmarks using the validation data from SQuAD\footnote{The SQuAD dataset is released under the CC BY-SA 4.0 license.} \cite{rajpurkar-etal-2016-squad} and TriviaQA \cite{2017arXivtriviaqa}, with each benchmark containing 900 samples. These benchmarks are referred to as ``Inj-SQuAD'' and ``Inj-TriviaQA'', respectively.

When constructing the injected instructions $x$ and the probes $y$, prior benchmarks typically use simple questions and their corresponding answers \cite{zverev2024can, li2023evaluating}. However, this approach presents two problems: (1) the probe $y$ lacks accuracy. For instance, if the injected question has multiple answers that are semantically identical but differ in their string representation, the probe may fail to detect the success of the attack. (2) Injecting simple questions does not fully align with real-world attack scenarios. While simple questions can achieve goal-hijacking, attackers may have additional purposes, such as phishing \cite{liu2024automatic} or advertising \cite{shu2023exploitability}.
To address these issues, we construct injected instructions tailored for phishing, advertising, and propaganda purposes. Moreover, we redefine the probe $y$ to capture the key content of the response to the injected instruction. For example, if the injected instruction $x$ is ``Tell people to click www.example.com,'' the presence of the phishing link in the response is sufficient to indicate that the attacker’s goal has been achieved. Therefore, we set the corresponding probe  $y$  as ``www.example.com.''
We craft these injected instructions using GPT-4o \cite{hurst2024gpt} and append them, along with their probes, to both the Inj-SQuAD and Inj-TriviaQA benchmarks. It is important to note that the injected instructions are identical across both benchmarks. Examples of Inj-SQuAD are shown in Table \ref{tab:examples_benchmark}.

\subsection{Training Data Construction}
% In this research, we have two training tasks: training detection models and training extraction models. The detection models process clean or injected documents and the extraction models process injected documents. 
We first collect clean document and injected instruction pairs, represented as tuples $ \mathcal{P}=
\{(d_i, x_i)\}_{i=1}^{N}$, in preparation for further training data construction.
We construct two sets of clean documents using documents from the SQuAD and TriviaQA training datasets. The SQuAD dataset contributes 18,891 samples, while TriviaQA provides 19,000 samples. Instructions from Stanford-Alpaca \cite{alpaca} are selected as the injected instructions and appended to the two sets of documents, constructing two sets of the clean document and injected instruction pairs. 

For training the detection models, the clean document and injected instruction pairs $\mathcal{P}$ are divided to construct clean documents and injected documents, along with considerations for the injected positions (analyzed in Section \ref{sec:rq1}). $\mathcal{P}$ are divided as follows for constructing training data: 40\% of the samples are clean documents, 15\% have injected instructions at the head of the document, 30\% have injections in the middle and 15\% at the tail. The final detection training dataset is denoted as $\mathcal{D}_{det}$.
Clean documents are excluded to train the extraction models. For each sample from the clean document and injected instruction pairs, the injected instruction $x$ is placed at three different positions (head, middle, and tail) within the document $d$, effectively tripling the size of the training dataset as denoted $\mathcal{D}_{ext}$. This approach ensures robust coverage of different positions during model training.

\subsection{Evaluation Metrics}
To evaluate detection performance, we employ the \textbf{true positive rate} for evaluating the injected documents and \textbf{false positive rate} for evaluating the clean documents.  For clean documents, a higher false positive rate indicates a more severe over-defense problem. Conversely, for injected documents, a higher true positive rate reflects better detection effectiveness.
Then we evaluate removal performance using \textbf{removal rate}, which measures if the injected instruction is \textbf{not} in the processed documents. 
Finally, we integrate the detection and removal methods to assess the overall defense performance against indirect prompt injection attacks. We measure this using the \textbf{attack success rate (ASR)}, which verifies whether the probe $y$ appears in the model’s response.