

\section{Experiments}


\subsection{Baselines}

Before presenting our insights, we first introduce the attack baselines used to evaluate detection, removal, and prompt injection defense performance. We then describe the prompt injection defense baselines for comparison when combining detection and removal methods into filtering methods.

\paragraph{Attack Baselines.}
We select widely-used attack methods to assess the effectiveness of detection and removal techniques. Specifically,  we select the following  attack methods for evaluation: \textbf{Naive attack} (abbreviated as ``Naive''), \textbf{Ignore attack} (``Ignore'') proposed by \citet{perez2022ignore}, \textbf{Escape-Character attack} (``Escape'') introduced by \citet{breitenbach2023dont} and \citet{liu2024formalizing}, \textbf{Fake completion attack} (``Fakecom'') proposed by \citet{willison_2023} and \textbf{Combined attack} (``Combined'') further formalized by \cite{liu2024formalizing}. More details can be found in Appendix \ref{app:attack}. Notably, when the model training process, such as training detection models, requires to incorporate of attack methods, we only consider the ``Naive attack''.

\paragraph{Defense Baselines.}
To demonstrate the effectiveness of the filtering method, we compare it with the prompt-engineering-based defenses \textbf{Sandwich} \cite{sandwich_defense_2023} and \textbf{Instructional} \cite{instruction_defense_2023}. Additionally, we compare it with the fine-tuning strategy \textbf{StruQ} \cite{chen2024struq}. Further details are available in Appendix \ref{app:defense}.



\subsection{RQ1: How well the indirect prompt injection attack can be detected and what influences the detection performance?}
\label{sec:rq1}
First, we evaluate the ability of existing instructed LLMs such as Llama3-8B-Instruct to detect indirect prompt injection attacks. Additionally, we assess open-source detection models such as Llama-Guard3-8B, Protect-AI-detection, and Prompt-Guard.
We also train detection models on our specifically crafted training data and evaluate the performance, with the results presented in Table \ref{tab:detection-general}. Following this, we investigate the issue of over-defense when training models, with results illustrated in Figure \ref{fig:over-defense} and Table \ref{tab:over-defense}. Finally, we analyze the impact of the injection position in the training data on overall detection performance, as shown in Figure \ref{fig:detection_position}. These evaluations reveal several intriguing insights:
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figs/over-defense.pdf}
    \caption{The defense performance and over-defense problem are a trade-off for models trained on crafted SQuAD training dataset and evaluated on Inj-TriviaQA documents. We report the minimum performance across different attacks and positions. The evaluation metric is the false positive rate for over-defense and true positive rate for normal defense.}
    \label{fig:over-defense}
    \vspace{-10pt}
\end{figure}

\vspace{5pt}
(1) \textit{The existing instructed LLMs and open-source detection models struggle to effectively detect indirect prompt injection attacks, whereas the specifically trained models demonstrate satisfactory performance.}
As shown in Table \ref{tab:detection-general}, instructed LLMs, such as Llama3-8B-Instruct and Qwen2-7B-Instruct, fail to perform well on the detection task. Due to the severe over-defense problem (false positive in clean documents) observed with Inj-TriviaQA documents, our analysis focuses on Inj-SQuAD documents, where the average accuracy of Llama is only 78.74\%, and Qwen achieves a mere 42.54\%.

As for the open-source detection models, a safety-focused model like Llama-Guard is not suitable for detecting injected documents, achieving a maximum accuracy of only 39.11\% across all types of injected documents. 
Although Protect-AI-detector is trained specifically on direct prompt injection attack datasets, it is only effective against attacks containing ``Ignore attack''. The detection performance of Prompt-Guard varies depending on the type of attack. For instance, it detects just 39.55\% of Inj-TriviaQA documents with ``Fakecom attack'' injected at the head but achieves 86.00\% accuracy when the same attack is injected in the middle of the document.

In contrast, models trained on our crafted datasets deliver better results. For example, the generative model Qwen2-1.5B achieves an average accuracy of 97.20\% on Inj-TriviaQA documents, while the classification model DeBERTa reaches an impressive 99.12\%, accounting for the over-defense problem. Although these models are trained specifically to defend against ``Naive attack'', they generalize well to other types of attacks. However, despite their strong detection performance, the over-defense problem arises.



\begin{table}[!h]
\centering
\small
% \setlength{\tabcolsep}{5pt} % Adjust the space between columns
\renewcommand{\arraystretch}{1.5} % Adjust line spacing
\begin{tabular}{!{\color{white}\vrule}l!{\color{white}\vrule}c!{\color{white}\vrule}c!{\color{white}\vrule}}
\toprule
{\textbf{Models}} & {\textbf{Inj-SQuAD}} & {\textbf{Inj-TriviaQA}}  \\
 
\midrule

{{DeBERTa-SQuAD}}
     & \cellcolor{blue!30}0.0 & \cellcolor{orange!30}12.44    \\



{{DeBERTa-TriviaQA}}
     & \cellcolor{orange!30}0.44 & \cellcolor{blue!30}0.22      \\

\midrule

{{Qwen2-0.5B-SQuAD}}
      & \cellcolor{blue!30}0.0 & \cellcolor{orange!30}27.33     \\



{{Qwen2-0.5B-TriviaQA}}
   & \cellcolor{orange!30}18.88 & \cellcolor{blue!30}0.0  \\


\midrule

{{Qwen2-1.5B-SQuAD}}
      & \cellcolor{blue!30}0.0 & \cellcolor{orange!30}11.11     \\



{{Qwen2-1.5B-TriviaQA}}
    & \cellcolor{orange!30}6.11 & \cellcolor{blue!30}0.11     \\


\bottomrule
\end{tabular}
\caption{Over-defense occurrence rate for in-domain and out-of-domain evaluation. The figure with the blue background such as ``\colorbox{blue!30}{0.0}'' means over-defense occurrence rate in the in-domain scenario. The figure with the orange background such as ``\colorbox{orange!30}{12.44}'' means over-defense occurrence rate in the out-of-domain scenario. ``DeBERTa-SQuAD'' means model DeBERTa is trained  on crafted SQuAD training data. Evaluation metric is false positive rate. All results are reported in \%.}
\label{tab:over-defense}
\vspace{-10pt}
\end{table}

\vspace{5pt}
(2) \textit{ The over-defense problem rarely occurs with in-domain documents. However, for out-of-domain documents, stronger models and more fluent documents are less prone to this issue.}
As shown in Table \ref{tab:detection-general}, the over-defense issue is obvious in Llama3-8B-Instruct, which frequently misclassifies clean TriviaQA documents as injected documents. For models trained on crafted SQuAD training dataset, although they can accurately detect most attacked cases, the over-defense problem is unavoidable when applied to out-of-domain TriviaQA documents.
One potential solution is to reduce the injection rate (the ratio of injected documents to training documents). However, as shown in Figure \ref{fig:over-defense}, there is a trade-off between over-defense problem and defense performance. While lowering the injection rate reduces the over-defense rate, it also leads to a more significant drop in overall model defense performance. Notably, for the Qwen2-0.5B model, reducing the injection rate from 0.2 to 0.1 even results in an increased over-defense rate. Thus, simply reducing the injection rate is not an optimal solution.

We further investigate the influence of model and document factors on the over-defense problem. As shown in Table \ref{tab:over-defense}, in-domain documents exhibit minimal over-defense issued with the maximum over-defense rate of only 0.22\%. However, for out-of-domain documents, the occurrence of over-defense depends on both the model and the document characteristics.
First, stronger models with greater learning capacity exhibit less severe over-defense issues. For example, the Qwen2-1.5B model shows fewer over-defense problems compared to the Qwen2-0.5B. Additionally, document fluency is also critical. SQuAD documents, which are more fluent than TriviaQA documents, as examples shown in Table \ref{tab:example_doc}, are less likely to trigger over-defense when used as out-of-domain data. 

\vspace{5pt}
(3) \textit{Detection models trained on data with a single injection position struggle to effectively detect attacks injected at other positions.}
When crafting training data, we consider all possible injection positions and the rate of each injection position, which seems to be cumbersome. Therefore, we further investigate the relationship between the injection position in training data and detection performance. Specifically, we train models on crafted SQuAD data with injection positions only at the head, middle, or tail, and evaluate their ability to generalize across different positions.
As shown in Figure \ref{fig:detection_position}, models perform well when detecting attacks at the same injection position as in their training data. However, their performance drops significantly when tasked with detecting attacks injected at different positions, particularly for models trained on head or tail injection positions. While training with middle-position injections has better generalization, it remains necessary to consider all positions during training to achieve robust detection performance.



\subsection{RQ2: Can injected instructions be removed from the data documents?}
After investigating the detection, we now turn to the removal of injected instructions from detected documents. First, we evaluate the two intuitive approaches: the segmentation removal method and the extraction removal method. All the models are trained on crafted SQuAD training dataset and evaluated on Inj-TriviaQA documents. The results of these methods are presented in Table \ref{tab:removal}. Subsequently, we combine the detection and removal methods to evaluate the final defense performance, and compare the performance with previous effective baselines, as results presented in Table \ref{tab:defense_indirect}. Finally, we explore the over-defense impact on the original QA task performance as shown in Table \ref{tab:defense_utility}.

\vspace{5pt}
(4) \textit{Both the segmentation and extraction removal methods can remove some of the injected instructions, but the segmentation method demonstrates better overall performance. However, the extraction method excels at removing injected instructions from the tail, which is the most effective attack position.}
As shown in Table \ref{tab:removal}, both methods are capable of removing some injected instructions. However, the extraction method struggles with instructions injected at the head and middle positions, particularly for ``Fakecom attack'' and ``Combined attack'', achieving a maximum removal rate of only 67.77\%. Interestingly, for instructions injected at the tail, the extraction method performs exceptionally well, achieving removal rates no lower than 94.66\% with the Qwen2-1.5B extraction model.
Furthermore, the Qwen2-1.5B detection model exhibits strong sentence-level detection abilities, despite being trained for document-level detection tasks. In contrast, the DeBERTa detection model has weaker sentence-level detection capabilities,  although one interesting fact is that DeBERTa detection model has stronger document-level detection ability. Another noteworthy finding is the influence of ``ignoring prompts'', which appear to increase the risk of exposing injected instructions. For instance, in the DeBERTa model, ``ignoring prompts'' improve the removal rate by an average of 9.48\% compared to the original ``Naive attack''.

\vspace{5pt}
(5) \textit{Combining the detection and removing methods as filtering methods is effective for defending against the indirect prompt injection attack.}
Building on our investigation of detection and removal techniques, we integrate these methods as unified filtering approaches to defend against indirect prompt injection attacks, comparing the performance to previous strategies such as prompt-engineering-based methods (``Sandwich'' and ``Instructional'') and fine-tuning-based approaches (``StruQ'') as shown in Table \ref{tab:defense_indirect}. For optimal effectiveness and efficiency, we employ the DeBERTa model as the detection model. We utilize DeBERTa model for segmentation removal and Qwen2-1.5B model for extraction removal. The performance is evaluated on Inj-TriviaQA benchmark.
Our findings reveal that while the filtering methods may occasionally fail on some documents, not all injected instructions in these failed cases are executed by the LLMs. Moreover, the filtering approach generally outperforms prior prompt-engineering and fine-tuning methods (For StruQ, we incorporate the ``Naive attack'' for defense but exclude the ``Fakecom attack''). However, both StruQ and our filtering methods, trained solely on ``Naive attack'' incorporated data, exhibit limited generalization capabilities against the ``Fakecom attack''. 

\vspace{5pt}
(6) \textit{The removal methods will not eliminate the key information in the clean data content, despite the over-defense of detection models.} 
After investigating the defense performance of the filtering approach, we further explore the damage of the filtering method to clean document data quality. The results, presented in Table \ref{tab:defense_utility}, indicate that although the detection methods may exhibit over-defense problem for the clean documents and potentially affect document quality, the subsequent removal methods rarely eliminate essential information, ensuring that the document remains useful.




\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{1.3} % Adjust line spacing
\setlength{\tabcolsep}{1.5pt} % Adjust column spacing
\resizebox{\columnwidth}{!}{ % Adjust to fit within a single column
\begin{tabular}{lcccccc}
\toprule
\textbf{Models} & \textbf{\makecell{Injected \\ Position}} & \textbf{Naive} & \textbf{Ignore} & \textbf{Escape} & \textbf{Fakecom} & \textbf{Combined} \\
\midrule
\multirow{3}{*}{\textbf{\makecell{Segment- \\ DeBERTa}}} 
    & Head     & 84.66 &100.00	&84.66&	79.88&	100.00 \\
    & Middle & 97.22 &100.00	&98.66&	98.44&	100.00 \\
    & Tail & 89.66 &100.00	&89.66&	82.66&	100.00 \\
\cline{2-7}
\multirow{3}{*}{\textbf{\makecell{Segment- \\ Qwen2-1.5B}}} 
    & Head     & 96.33 &98.22	&96.33&	96.66&	98.11 \\
    & Middle & 94.66 &98.11	&97.33&	97.66&	97.66 \\
    & Tail & 96.66 &98.66	&96.66&	97.00&	98.88 \\
\midrule
\multirow{3}{*}{\textbf{\makecell{Extraction- \\ Qwen2-1.5B}}} 
    & Head     & 94.33 &75.33	&93.44&	61.66&	67.77 \\
    & Middle & 91.11 &71.55	&85.55&	42.33&	56.11 \\
    & Tail & 100.00 &98.44	&99.88&	97.66&	94.66 \\
\cline{2-7}
\multirow{3}{*}{\textbf{\makecell{Extraction- \\ Llama3.2-3B}}} 
    & Head     & 87.77 &73.77	&73.00&	42.88&	60.22 \\
    & Middle & 87.55 &78.22	&76.44&	42.22&	66.77 \\
    & Tail & 96.88 &93.00	&95.66&	74.55&	91.00 \\
\bottomrule
\end{tabular}
}
\caption{The removal performance of different methods. We evaluate the performance by verifying if the injected instructions are \textbf{not} in the processed documents. ``Segment-DeBERTa'' means use the trained ``DeBERTa'' model to detect each segment. ``Extraction-Qwen2-1.5B'' means we train ``Qwen2-1.5B'' for extraction task. The evaluation metric is removal rate. All results are reported in \%.}
\label{tab:removal}
\vspace{-15pt}
\end{table}









