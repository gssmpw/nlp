\section{Related Work}
\subsection{Prompt Injection Attacks}
Prompt injection attacks pose a critical challenge for Large Language Models (LLMs) and have garnered significant research attention  \cite{perez2022ignore, willison_2023, liu2023prompt, li2023evaluating, liu2024formalizing, zhan2024injecagent, shi2024optimization, liu2024automatic, shafran2024machine, huang2024semantic, breitenbach2023dont}. \citet{perez2022ignore} explore the use of an ``ignoring prompt'' which is prepended to the injected instruction to manipulate the models. Similarly, \citet{willison_2023} introduces a technique involving the addition of fake responses, tricking the LLMs into believing the userâ€™s input has already been processed, thereby executing the malicious instruction instead. \citet{yi2023benchmarking} further enhances attack effectiveness by combining multiple attack strategies. Additionally, \citet{liu2024automatic} optimize suffixes to effectively mislead LLMs.

\subsection{Prompt Injection Defenses}
In response to the growing threat of prompt injection attacks, numerous defense mechanisms have been proposed  \cite{sandwich_defense_2023, hines2024defending, willison_2023, chen2024struq, wallace2024instruction, yi2023benchmarking, piet2023jatmo, suo2024signed}. \citet{sandwich_defense_2023} and \citet{yi2023benchmarking} suggest appending reminders to reinforce adherence to the original instructions.  \citet{hines2024defending} and \citet{ willison_2023} propose using special tokens to clearly delineate the data content area. \citet{piet2023jatmo} address the issue by training models to specialize in specific tasks. \citet{chen2024struq} and \citet{ wallace2024instruction} advocate fine-tuning LLMs with adversarial training \cite{mkadry2017towards}, granting privileged status to authorized instructions. Lastly, detection methods \cite{deberta-v3-base-prompt-injection-v2, meta2024prompt, gorman2022jailbreaking} have been proposed to address direct prompt injection attacks. However, these methods overlook indirect prompt injection attacks, which are more practical and applicable.