[
  {
    "index": 0,
    "papers": [
      {
        "key": "perez2022ignore",
        "author": "Perez, F{\\'a}bio and Ribeiro, Ian",
        "title": "Ignore previous prompt: Attack techniques for language models"
      },
      {
        "key": "willison_2023",
        "author": "Simon Willison",
        "title": "Delimiters won\u2019t save you from prompt injection"
      },
      {
        "key": "liu2023prompt",
        "author": "Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and others",
        "title": "Prompt Injection attack against LLM-integrated Applications"
      },
      {
        "key": "li2023evaluating",
        "author": "Li, Zekun and Peng, Baolin and He, Pengcheng and Yan, Xifeng",
        "title": "Evaluating the instruction-following robustness of large language models to prompt injection"
      },
      {
        "key": "liu2024formalizing",
        "author": "Liu, Yupei and Jia, Yuqi and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang",
        "title": "Formalizing and benchmarking prompt injection attacks and defenses"
      },
      {
        "key": "zhan2024injecagent",
        "author": "Zhan, Qiusi and Liang, Zhixiang and Ying, Zifan and Kang, Daniel",
        "title": "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents"
      },
      {
        "key": "shi2024optimization",
        "author": "Shi, Jiawen and Yuan, Zenghui and Liu, Yinuo and Huang, Yue and Zhou, Pan and Sun, Lichao and Gong, Neil Zhenqiang",
        "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge"
      },
      {
        "key": "liu2024automatic",
        "author": "Liu, Xiaogeng and Yu, Zhiyuan and Zhang, Yizhe and Zhang, Ning and Xiao, Chaowei",
        "title": "Automatic and universal prompt injection attacks against large language models"
      },
      {
        "key": "shafran2024machine",
        "author": "Shafran, Avital and Schuster, Roei and Shmatikov, Vitaly",
        "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents"
      },
      {
        "key": "huang2024semantic",
        "author": "Huang, Yihao and Wang, Chong and Jia, Xiaojun and Guo, Qing and Juefei-Xu, Felix and Zhang, Jian and Pu, Geguang and Liu, Yang",
        "title": "Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs"
      },
      {
        "key": "breitenbach2023dont",
        "author": "Mark Breitenbach and Adrian Wood and Win Suen and Po-Ning Tseng",
        "title": "Don't You (Forget NLP): Prompt Injection with Control Characters in ChatGPT"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "perez2022ignore",
        "author": "Perez, F{\\'a}bio and Ribeiro, Ian",
        "title": "Ignore previous prompt: Attack techniques for language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "willison_2023",
        "author": "Simon Willison",
        "title": "Delimiters won\u2019t save you from prompt injection"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yi2023benchmarking",
        "author": "Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Hines, Keegan and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao",
        "title": "Benchmarking and defending against indirect prompt injection attacks on large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2024automatic",
        "author": "Liu, Xiaogeng and Yu, Zhiyuan and Zhang, Yizhe and Zhang, Ning and Xiao, Chaowei",
        "title": "Automatic and universal prompt injection attacks against large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sandwich_defense_2023",
        "author": "Unknown",
        "title": "Sandwich Defense"
      },
      {
        "key": "hines2024defending",
        "author": "Hines, Keegan and Lopez, Gary and Hall, Matthew and Zarfati, Federico and Zunger, Yonatan and Kiciman, Emre",
        "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting"
      },
      {
        "key": "willison_2023",
        "author": "Simon Willison",
        "title": "Delimiters won\u2019t save you from prompt injection"
      },
      {
        "key": "chen2024struq",
        "author": "Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David",
        "title": "StruQ: Defending against prompt injection with structured queries"
      },
      {
        "key": "wallace2024instruction",
        "author": "Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex",
        "title": "The instruction hierarchy: Training llms to prioritize privileged instructions"
      },
      {
        "key": "yi2023benchmarking",
        "author": "Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Hines, Keegan and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao",
        "title": "Benchmarking and defending against indirect prompt injection attacks on large language models"
      },
      {
        "key": "piet2023jatmo",
        "author": "Piet, Julien and Alrashed, Maha and Sitawarin, Chawin and Chen, Sizhe and Wei, Zeming and Sun, Elizabeth and Alomair, Basel and Wagner, David",
        "title": "Jatmo: Prompt injection defense by task-specific finetuning"
      },
      {
        "key": "suo2024signed",
        "author": "Suo, Xuchen",
        "title": "Signed-Prompt: A new approach to prevent prompt injection attacks against LLM-integrated applications"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "sandwich_defense_2023",
        "author": "Unknown",
        "title": "Sandwich Defense"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yi2023benchmarking",
        "author": "Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Hines, Keegan and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao",
        "title": "Benchmarking and defending against indirect prompt injection attacks on large language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hines2024defending",
        "author": "Hines, Keegan and Lopez, Gary and Hall, Matthew and Zarfati, Federico and Zunger, Yonatan and Kiciman, Emre",
        "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "willison_2023",
        "author": "Simon Willison",
        "title": "Delimiters won\u2019t save you from prompt injection"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "piet2023jatmo",
        "author": "Piet, Julien and Alrashed, Maha and Sitawarin, Chawin and Chen, Sizhe and Wei, Zeming and Sun, Elizabeth and Alomair, Basel and Wagner, David",
        "title": "Jatmo: Prompt injection defense by task-specific finetuning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2024struq",
        "author": "Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David",
        "title": "StruQ: Defending against prompt injection with structured queries"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wallace2024instruction",
        "author": "Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex",
        "title": "The instruction hierarchy: Training llms to prioritize privileged instructions"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "mkadry2017towards",
        "author": "M{\\k{a}}dry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian",
        "title": "Towards deep learning models resistant to adversarial attacks"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "deberta-v3-base-prompt-injection-v2",
        "author": "ProtectAI.com",
        "title": "Fine-Tuned DeBERTa-v3-base for Prompt Injection Detection"
      },
      {
        "key": "meta2024prompt",
        "author": "Meta",
        "title": "Prompt Guard-86M | Model Cards and Prompt formats"
      },
      {
        "key": "gorman2022jailbreaking",
        "author": "Stuart Armstrong, rgorman",
        "title": "Using GPT-Eliezer against ChatGPT Jailbreaking"
      }
    ]
  }
]