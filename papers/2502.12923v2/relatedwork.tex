\section{Related Work}
\paragraph{Slot and Intent Detection.}
Traditional approaches to spoken language understanding (SLU) often treat SID separately using domain-specific classification or sequence tagging approaches \citep{zhang2016joint, wang-etal-2018-bi, weld2022survey, Survey, pham-etal-2023-misca}. More recent transformer-based solutions unify both tasks, leveraging contextual embeddings to improve performance~\citep{castellucci2019multi, van-der-goot-etal-2021-masked, stoica2021intent, arora2024intent} with models like BERT~\citep{devlin-etal-2019-bert}. However, many of these solutions still presume tailored sequence labeling datasets or full-size transformer backends. Our work aligns with the shift to more expressive transformer models for SLU, but we push inference to a local environment while also adding dynamic text generation.

\paragraph{Running LLMs on Edge Devices.}
While training large-scale LLMs remains computationally expensive, numerous works explore strategies for \emph{deploying} them on edge hardware. \citet{arxiv2408} propose FPGA-based accelerators to reduce memory overhead for LLM inference. \citet{EdgeShard} distribute an LLM across multiple low-power devices to increase throughput. An empirical footprint study by \citet{Resource_Footprint} shows that even 7B-parameter models can strain embedded hardware if not sufficiently compressed. Our approach uses a much smaller LLM (0.5B--1.5B parameters) plus weight quantization, showing that near-commodity devices with 8GB RAM can handle both intent classification and text generation if the domain is sufficiently specialized.