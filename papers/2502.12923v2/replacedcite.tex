\section{Related Work}
\paragraph{Slot and Intent Detection.}
Traditional approaches to spoken language understanding (SLU) often treat SID separately using domain-specific classification or sequence tagging approaches ____. More recent transformer-based solutions unify both tasks, leveraging contextual embeddings to improve performance____ with models like BERT____. However, many of these solutions still presume tailored sequence labeling datasets or full-size transformer backends. Our work aligns with the shift to more expressive transformer models for SLU, but we push inference to a local environment while also adding dynamic text generation.

\paragraph{Running LLMs on Edge Devices.}
While training large-scale LLMs remains computationally expensive, numerous works explore strategies for \emph{deploying} them on edge hardware. ____ propose FPGA-based accelerators to reduce memory overhead for LLM inference. ____ distribute an LLM across multiple low-power devices to increase throughput. An empirical footprint study by ____ shows that even 7B-parameter models can strain embedded hardware if not sufficiently compressed. Our approach uses a much smaller LLM (0.5B--1.5B parameters) plus weight quantization, showing that near-commodity devices with 8GB RAM can handle both intent classification and text generation if the domain is sufficiently specialized.