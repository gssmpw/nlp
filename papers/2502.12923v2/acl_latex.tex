% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{listings}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[justification=justified,singlelinecheck=false]{caption}
\usepackage{ragged2e}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{fontawesome5}
\usepackage{subcaption}
\usepackage[draft]{minted}
\usepackage{hyperref}
\usepackage[noabbrev,capitalize,nameinlink]{cleveref}
\usepackage{tikz}
\usepackage{tikz-dependency}
\usepackage{adjustbox}
\usepackage{todonotes}
\usepackage[multiple]{footmisc}
\usepackage{pgfplots}
\usepackage{rotating}
\usetikzlibrary{positioning, fadings}
\usepackage{transparent}
\usepackage{cclicenses}
\usepackage[ragged]{sidecap}
\usepackage{bbm}
\usepackage{colortbl}
\usepackage{adjustbox}
\usepackage{mdframed}
\usepackage{listings}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{lscape, float} 
\usepackage[most]{tcolorbox}
\usepackage{subcaption}

\tcbset{
    promptstyle/.style={
        enhanced,
        width=\linewidth,
        colback=white,
        colframe=black,
        colbacktitle=gray!20,
        coltitle=black,
        rounded corners,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        attach boxed title to top left={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=gray!20
        }
    },
    replystyleg/.style={
        enhanced,
        width=\linewidth,
        colback=green!15,
        colframe=black,
        colbacktitle=green!30,
        coltitle=black,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        rounded corners,
        sharp corners=north,
        attach boxed title to top right={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=green!40
        }
    },
    replystyler/.style={
        enhanced,
        width=\linewidth,
        colback=red!15,
        colframe=black,
        colbacktitle=red!40,
        coltitle=black,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        rounded corners,
        sharp corners=north,
        attach boxed title to top right={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=red!40
        }
    }
}

\newtcolorbox{promptbox}[1][]{
    promptstyle,
    title=Prompt,
    #1
}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=.6pt] (char) {#1};}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{On-Device LLMs for Home Assistant: \\Dual Role in Intent Detection and Response Generation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Rune Birkmose\textsuperscript{*} \hspace{1em} Nathan Mørkeberg Reece\textsuperscript{*} \hspace{1em} Esben Hofstedt Norvin\textsuperscript{*} \\ \textbf{Johannes Bjerva} \hspace{1em} \textbf{Mike Zhang}\textsuperscript{\textdagger} \\
  Aalborg University, Denmark \\
  \textsuperscript{*}\texttt{\{rbirkm20, nreece20, enorvi20\}@student.aau.dk}\hspace{1em}\textsuperscript{\textdagger}\texttt{jjz@cs.aau.dk} \\
}


\begin{document}
\maketitle
\begingroup\renewcommand\thefootnote{*}
\footnotetext{The authors contributed equally to this work.}
\endgroup
\begin{abstract}
This paper investigates whether Large Language Models (LLMs), fine-tuned on synthetic but domain-representative data, can perform the twofold task of (i) slot and intent detection and (ii) natural language response generation for a smart home assistant, while running solely on resource-limited, CPU-only edge hardware. We fine-tune LLMs to produce both JSON action calls and text responses. Our experiments show that 16-bit and 8-bit quantized variants preserve high accuracy on slot and intent detection and maintain strong semantic coherence in generated text, while the 4-bit model, while retaining generative fluency, suffers a noticeable drop in device-service classification accuracy. Further evaluations on noisy human (non-synthetic) prompts and out-of-domain intents confirm the models' generalization ability, obtaining around 80--86\% accuracy. While the average inference time is 5--6 seconds per query---acceptable for one-shot commands but suboptimal for multi-turn dialogue---our results affirm that an on-device LLM can effectively unify command interpretation and flexible response generation for home automation without relying on specialized hardware.
\end{abstract}

\section{Introduction}
Smart home technologies and IoT devices have proliferated in recent years, with an expected rise from 16.6~billion to 18.8~billion connected devices by the end of 2024 \citep{iotanalytics}. Major providers like Amazon, Google, and Apple typically handle speech recognition and intent detection on cloud servers, which raises user concerns about privacy, data ownership, and reliance on proprietary ecosystems \citep{bbc2025siri}. Conventional solutions for home assistants often rely on specialized, domain-specific classifiers for slot and intent detection (SID), paired with templated system responses. While these approaches can be efficient, they can also be rigid, sometimes requiring precisely phrased user inputs and yielding repetitive or unpersonalized answers.

Recent developments in on-device computing—coupled with improvements in model compression and quantization~\cite{liang2021pruning,gholami2022survey,lang2024comprehensive}—have paved the way for smaller yet still capable language models to run on commodity hardware. These models offer privacy benefits and allow customizable local inference with reduced latency. However, deploying a capable model under strict memory and computational constraints remains challenging. Large-scale Transformer-based language models~\citep{vaswani2017attention}, and especially LLMs~\citep{touvron2023llama, dubey2024llama, bai2023qwen, Qwen, groeneveld-etal-2024-olmo}, have demonstrated remarkable proficiency in tasks ranging from question answering to text generation~\citep{arora2024intent, yin2024large}, yet typically demand substantial hardware resources, restricting them to cloud-based services or large compute clusters.

This paper explores whether a smaller, fine-tuned LLM can provide two capabilities essential to a home assistant---accurate recognition of \emph{what} users want (i.e., slot and intent detection), and \emph{natural} textual responses---while running entirely on an edge device with limited CPU and memory. By unifying these tasks into one end-to-end system, we eliminate the need for separate domain-specific classification modules and templated responses, focusing on efficiency, robust language understanding, and strict correctness in JSON action output.

Additionally, we move away from classic SID datasets and other general spoken language understanding benchmarks. Instead, we investigate whether LLMs can be directly applied to digital assistant software. To this end, we take the open-source Home Assistant software\footnote{\url{https://github.com/home-assistant/core}} as our gold standard for evaluation, targeting real-world device-service pairs and actionable JSON outputs.

\paragraph{Contributions.} Our contributions can be summarized as follows:\footnote{We release all our code and models at~\url{https://github.com/Run396/P9}.}
\circled{1} We show that a 0.5B LLM can be fine-tuned to already jointly handle SID \textit{and} response generation with high accuracy. \circled{2} By quantizing the model (from 16-bit to 8-bit and 4-bit), we quantify trade-offs between memory usage, accuracy, and generative fluency on CPU-only edge hardware. \circled {3} We evaluate the approach on synthetic data, human queries, and out-of-domain tasks, confirming robust generalization.

\section{Related Work}

\paragraph{Slot and Intent Detection.}
Traditional approaches to spoken language understanding (SLU) often treat SID separately using domain-specific classification or sequence tagging approaches \citep{zhang2016joint, wang-etal-2018-bi, weld2022survey, Survey, pham-etal-2023-misca}. More recent transformer-based solutions unify both tasks, leveraging contextual embeddings to improve performance~\citep{castellucci2019multi, van-der-goot-etal-2021-masked, stoica2021intent, arora2024intent} with models like BERT~\citep{devlin-etal-2019-bert}. However, many of these solutions still presume tailored sequence labeling datasets or full-size transformer backends. Our work aligns with the shift to more expressive transformer models for SLU, but we push inference to a local environment while also adding dynamic text generation.

\paragraph{Running LLMs on Edge Devices.}
While training large-scale LLMs remains computationally expensive, numerous works explore strategies for \emph{deploying} them on edge hardware. \citet{arxiv2408} propose FPGA-based accelerators to reduce memory overhead for LLM inference. \citet{EdgeShard} distribute an LLM across multiple low-power devices to increase throughput. An empirical footprint study by \citet{Resource_Footprint} shows that even 7B-parameter models can strain embedded hardware if not sufficiently compressed. Our approach uses a much smaller LLM (0.5B--1.5B parameters) plus weight quantization, showing that near-commodity devices with 8GB RAM can handle both intent classification and text generation if the domain is sufficiently specialized.

\section{Methodology}

% \subsection{Task Overview}
Our goal is to integrate two core functionalities of a home assistant into a single model: 

\begin{itemize}
    \item \textbf{Slot and Intent Detection:} The model outputs a valid JSON object that maps to a desired service (intent) and device (slot) pair:
\begin{minted}[frame=single,
               framesep=2mm,
               xleftmargin=0pt,
               tabsize=2]{js}
{"service": "light.turn_on",
  "device": "light.living_room",
  "assistant": "Sure, turning on 
    on the living room light."}
\end{minted}
    \item \textbf{Natural Language Generation:} The model also produces a textual response confirming or elaborating on its action, as can be seen in the example above. The text can then be propagated to, e.g., a text-to-speech model.
\end{itemize}

\noindent
Traditional classifiers only handle device-service classification and do not produce any text. For user-facing text, the baseline approach would rely on templated responses.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Partition} & \textbf{Train} & \textbf{Test} & \textbf{Total}\\
\midrule
Classification & 23,372 & 5,843 & 29,215 \\
LLM & 33,361 & 2,435 & 35,796 \\
\bottomrule
\end{tabular}
\caption{\textbf{Aggregated Train/Test Splits.} For the classification baseline, 20\% of the original training set was used as test data (after removing multi-intent samples). The LLM used the full synthetic data; 2,435 remain as test.}
\label{tab:data}
\end{table}

\subsection{Data and Pre-processing}
\label{sec:data}

To the best of our knowledge, there is no existing human-curated dataset specifically for the Home Assistant software. Thus, we rely on synthetic data. We use a publicly available synthetic dataset~\citep{acon96}, which consists of 35,840 synthetic examples designed to mimic Home Assistant commands. Each instance consists of:

\begin{itemize}
    \itemsep0em
    \item A \textbf{User Prompt:} e.g., \emph{``Turn on the kitchen light''}, \emph{``Set the thermostat to 22 degrees''}.
    \item One \textbf{Valid JSON Action}, containing the \texttt{service} and \texttt{device} fields corresponding to Home Assistant calls.
    \item A \textbf{Natural Language Response:} e.g., a paraphrase or affirmation of the action taken.
\end{itemize}

A full example can be found in~\cref{fig:data_example} (\cref{app:sec-data-example}).
We stratify the dataset, maintaining the inherent imbalance (some device types and services appear more frequently). There are 38 service labels and 858 device labels. We split into training and test sets as shown in~\cref{tab:data}. The final training set for the LLM includes $\sim$33k examples, and we set aside 2,435 synthetic samples for evaluation. Note that for the classification-based baselines, we split up the train and test set to separately predict \texttt{service} and \texttt{device} instead of as one prediction, ending up with double the test data (5,843 samples; excluding multi-intent examples). The input consists of only the user message and leave the system message out. A more detailed distribution of the data can be found in~\cref{tab:detailed} (\cref{app:distribution}).

\begin{table}[t]
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{BERTScore}\\
\midrule
\textbf{Baselines}           & & \\
SVC Classifier               &  & \\
 \hspace{1em} Service        & 76.6  & ---\\
 \hspace{1em} Device         & 45.4  & ---\\
DistilBERT                   &        &    \\
 \hspace{1em} Service        & 98.8  & ---\\
 \hspace{1em} Device         & 47.9  & ---\\
\midrule
Qwen2.5-0.5B (16-bit)        & 98.8 & 0.84\\
Qwen2.5-0.5B (8-bit)         & 98.4 & 0.79\\
Qwen2.5-0.5B (4-bit)         & 81.7 & 0.88\\
\midrule
Qwen2.5-1.5B (16-bit)        & 96.9 & 0.84\\
Qwen2.5-1.5B (8-bit)         & 96.5 & 0.83\\
Qwen2.5-1.5B (4-bit)         & 90.7 & 0.82\\
\bottomrule
\end{tabular}
\caption{\textbf{Slot/Intent Detection and NLG Results on Synthetic Test Data.} Accuracy is based on exact JSON match. BERTScore measures semantic similarity of the generated text vs.\ gold reference.}
\label{tab:results}
\end{table}

\subsection{Models}

\paragraph{Baseline Classifiers.}
We train a Linear SVC from Scikit-Learn~\citep{pedregosa2011scikit} on TF-IDF features of the user prompt. The classifier outputs a concatenated device-service pair, which is then wrapped in JSON. Additionally, we fine-tune DistilBERT~\cite{sanh2019distilbert} for classification. We use the transformers library~\citep{wolf-etal-2020-transformers} for fine-tuning. We train for 1 epoch using a learning rate of 3$\times 10^{-4}$ with the AdamW optimizer, and a batch size of 64 on a NVIDIA A10 (24GB) GPUs. Both models have no generative capability, so user-facing text is templated.

\paragraph{Small Large Language Models.}
We train using a chat-style format with user--assistant pairs. We primarily use the Qwen2.5-0.5B-Instruct model and the Qwen2.5-1.5B-Instruct~\citep{Qwen}. We fine-tune both models for one epoch with a batch size of 4, using the AdamW optimizer at a learning rate of $2\times10^{-5}$ with a cosine scheduler. The maximum sequence length is set to 2,048 tokens. We use the HuggingFace Transformers library~\citep{wolf-etal-2020-transformers} for training on NVIDIA L4 (24\,GB) GPUs.

\paragraph{Quantization.}
After fine-tuning and having the original 16-bit model, we produce two quantized versions of each model: NF8 and NF4~\cite{dettmers2024qlora}, using bitsandbytes.\footnote{See~\url{https://github.com/bitsandbytes-foundation/bitsandbytes}. We also use double quantization.} 
%\footnote{We quantize using the \url{https://github.com/ggerganov/llama.cpp} library.} 
This allows us to compare accuracy, generative quality, and inference speed under varying memory constraints.

\begin{table}[t]
\centering
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Model}  & \textbf{CPU} & \textbf{T/Q (s)} & \textbf{Load (s)}\\
\midrule
\textbf{Baselines}    & & & \\
SVC Classifier          & 4 & $<$1 & ---\\
DistilBERT              & 4 & $<$1 & ---\\
\midrule
Qwen2.5-0.5B (16-bit)   & 4 & 6.25 & $\pm$3.2 \\
Qwen2.5-1.5B (16-bit)   & 4 & 10.81 & $\pm$5.6 \\
Qwen2.5-0.5B (8-bit)    & 4 & 5.50 & $\pm$3.2 \\
Qwen2.5-1.5B (8-bit)    & 4 & 10.32 & $\pm$5.6 \\
\midrule
Qwen2.5-0.5B (16-bit)   & 2 & 8.49 & $\pm$5.6 \\
Qwen2.5-1.5B (16-bit)   & 2 & 17.72 & $\pm$5.6 \\
Qwen2.5-0.5B (8-bit)    & 2 & 7.89 & $\pm$5.6 \\
Qwen2.5-1.5B (8-bit)    & 2 & 16.11 & $\pm$5.6 \\
\bottomrule
\end{tabular}
\caption{\textbf{Computation Time.} Mean time per query (T/Q) across 500 samples under different CPU core counts and quantization levels. Load time is model initialization.}
\label{tab:performance}
\end{table}

\subsection{Evaluation}
\label{sec:evaluation}
\paragraph{Slot-Intent Detection Accuracy.}
SID must be correct with near-exact string matching, as JSON calls are consumed downstream by the home automation system. We thus parse the model output for the \texttt{service} and \texttt{device} fields; if they match the gold annotation exactly, it is counted as correct. Any mismatch or invalid JSON results in an error.

For the classification task, instead, we separately predict \texttt{service} and \texttt{device} using the same classification model and take the average accuracy.

\paragraph{Text Generation Quality.}
For the natural language responses using the LLMs, we compare each generated response to the reference using BERTScore~\citep{bert-score}.

\paragraph{Inference Environment.}
We simulate a CPU-only setup on an 8\,GB RAM device with up to four CPU cores. We measure average inference time on a 500-sample subset, varying both quantization level and the number of CPU cores.

\begin{table}[t]
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{BERTScore}\\
\midrule
Qwen2.5-0.5B    &   80.0    &   0.76 \\
Qwen2.5-1.5B    &   86.7    &   0.74 \\
\bottomrule
\end{tabular}
\caption{\textbf{Results Out-of-Domain Queries.} Accuracy and BERTScore over 60 OOD samples.}
\label{tab:ood_eval}
\end{table}

\section{Results}

\subsection{Slot and Intent Detection}
\cref{tab:results} shows the SID performance of both the 0.5B and 1.5B LLMs under various quantization levels, alongside the baseline SVC and DistilBERT. For the 0.5B model, the 16-bit and 8-bit variants reach near-perfect accuracy ($\sim 99\%$). The 4-bit version drops to 81.7\%, which is still better than the the SVC baseline (average 61.0\% accuracy) and DistilBERT baseline (average 73.4\% accuracy).

Interestingly, for the larger 1.5B model, the 16-bit and 8-bit variants achieve 96.9\% and 96.5\% accuracy, respectively, while the 4-bit version gets 90.7\%. Thus, while the smaller 0.5B model actually yields higher raw accuracy in-domain, the 1.5B model remains competitive and in some out-of-domain tests (next section) performs better.

\subsection{Natural Language Generation}
Although the 4-bit models suffer in SID accuracy, \cref{tab:results} shows that the 0.5B 4-bit variant has the highest BERTScore (0.88). This indicates that while it may misclassify device/service fields, the generative text can still be fluent and semantically close to the target. Meanwhile, the 8-bit versions drop in BERTScore for the 0.5B model (0.79) and remain steady for the 1.5B model (0.83). Qualitative samples show that small changes in quantization can shift the style and lexical choices of the generated text.

\subsection{Inference Time and Memory}

\cref{tab:performance} summarizes the inference speed across model size, quantization, and CPU core settings. The 8-bit model is only slightly faster than the 16-bit model (5.5\,s vs.\ 6.25\,s on 4 cores for the 0.5B). Doubling CPU cores from 2 to 4 reduces latency roughly by half. The 1.5B model takes longer (up to 10--17\,s per query), which may be borderline for real-time usage in multi-turn dialogues.

\begin{table}[t]
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{BERTScore}\\
\midrule
Qwen2.5-0.5B    &   84.0    &   0.68 \\
Qwen2.5-1.5B    &   86.4    &   0.66 \\
\bottomrule
\end{tabular}
\caption{\textbf{Results Human-Generated Queries.} Accuracy and BERTScore over 81 real-user queries.}
\label{tab:human_eval}
\end{table}

\subsection{Out-of-Domain Intents}

In \cref{tab:ood_eval}, we evaluate 60 OOD queries that mention either novel device types or services not appearing in the training set. The 0.5B model scores 80.0\% accuracy vs.\ 86.7\% for the 1.5B model, with BERTScores of 0.76 and 0.74 respectively. The results suggest that the 1.5B model generalizes somewhat better to unfamiliar domains, though both degrade compared to in-domain performance.

\subsection{Human Prompts}
Finally, we tested each model on 81 human-written prompts. Ten participants (ages 23--69) contributed typical commands they would issue to a home assistant, including incomplete or ambiguous phrasing. \cref{tab:human_eval} shows that the 0.5B model achieves 84.0\% accuracy, whereas the 1.5B model is slightly higher at 86.4\%. BERTScores are around 0.66--0.68. The gap vs.\ synthetic data reflects real-user queries with more variation and noisy data.

\section{Discussion}
Despite near-perfect performance on the synthetic test set,~\cref{tab:ood_eval} and~\ref{tab:human_eval} reveal a drop to 80--86\% accuracy in real or out-of-domain queries. This discrepancy likely stems from the difficulty of handling spontaneous human phrasing, missing location or device details, and genuinely novel device types. Still, the results surpass the SVC and DistilBERT baseline.

Interestingly, while the 4-bit model can generate fluent natural language responses (often scoring the highest BERTScore in the 0.5B case), its classification accuracy suffers. This underscores that quantizing a model to extreme levels can degrade structured predictions more than open-ended text generation.

Regarding speed, the 1.5B model yields consistent accuracy gains on OOD data but also increases inference time by up to 2--3$\times$. For single-turn commands, 5--6 seconds per query might be acceptable, but multi-turn dialogue would require faster or more efficient strategies. Future work may explore parameter-efficient fine-tuning, context truncation, or advanced quantization (e.g., 8-bit + partial 4-bit layering) to reduce inference times.

\section{Conclusion}
We present that LLMs can simultaneously perform SID and natural language response generation for a home automation domain. Experiments on an 8GB RAM, CPU-only environment show that 8-bit quantization largely preserves in-domain accuracy (up to 99\%) and strong text fluency, while 4-bit introduces significant classification errors despite retaining good generative capability. We further demonstrate promising generalization to human-written prompts and out-of-domain tasks, with accuracy around 80--86\%. However, per-query inference times of 5--6 seconds indicate that LLM-based assistants, as implemented here, are not yet ideal for fast multi-turn dialogues on edge devices. Future work can refine these models for faster, more memory-efficient inference, enabling privacy-preserving yet flexible home automation assistants.

\section*{Limitations} 
Our use of synthetic data may limit the diversity of user prompts; while we partially mitigated this with human-written queries, data coverage remains a challenge. The model also relies on structurally valid JSON output. Real-world usage may need fallback logic to handle malformed or incomplete responses. Moreover, we focus on a single domain (home automation); scaling to broader or open-ended tasks likely requires larger models and may degrade performance under CPU-only constraints.

\section*{Ethical Considerations}
We do not foresee any major ethical issues with this work. The primary domain is home automation, and the dataset is synthetic or user-provided under informed consent. Nonetheless, deploying generative models in user-facing applications requires caution regarding hallucinated or incorrect responses, as well as user data privacy.

\section*{Acknowledgments}
MZ and JB are supported by the research grant (VIL57392) from VILLUM FONDEN.

\bibliography{custom, anthology}

\clearpage

\appendix

\section{Data Examples}
\label{app:sec-data-example}
We show a data example in~\cref{fig:data_example}.

\begin{figure*}
\begin{minted}[frame=single,framesep=2mm,xleftmargin=0pt,tabsize=2]{js}
[ 
    { 
      "from": "system", 
      "value": "You are 'Al', a helpful AI Assistant that controls the devices in 
      a house. Complete the following task as instructed or answer the following 
      question with the information provided only.\nServices: cover.close_cover(), 
      cover.open_cover(), cover.stop_cover(), cover.toggle(), lock.lock(), 
      lock.unlock(), media_player.media_next_track(), media_player.media_pause(), 
      media_player.media_play(), media_player.media_play_pause(), 
      media_player.media_previous_track(), media_player.media_stop(), 
      media_player.toggle(), media_player.turn_off(), media_player.turn_on(), 
      media_player.volume_down(), media_player.volume_mute(), 
      media_player.volume_up(), switch.toggle(), switch.turn_off(), 
      switch.turn_on(), timer.cancel(), timer.pause(), timer.start(duration), 
      vacuum.pause(), vacuum.return_to_base(), vacuum.start(), vacuum.stop()
      Devices: media_player.harman_kardon_aura 'Harman Kardon Glass Speaker' 
      = standby; vol=0.88\ntimer.kitchen_oven 'Kitchen oven timer' = 
      active\nlock.office_cabinet 'Office cabinet lock' = unlocked
      cover.master_bedroom 'Master Bedroom' = closed
      vacuum.hallway_neato 'Hallway path cleaner' = docked
      switch.basement_lights 'Basement Lights Switch' = off" 
    }, 
    { 
      "from": "user", 
      "value": "reverse the master bedroom blinds" 
    }, 
    { 
      "from": "assistant", 
      "value": "switching Master Bedroom state as requested\n
      ```homeassistant
         {
          \"service\": \"cover.toggle\", 
          \"target_device\": \"cover.master_bedroom\"
        }
      ```" 
    }
]
\end{minted}
\caption{\textbf{Data Example.} In the figure, we show a data example from the~\citet{acon96} dataset.}
\label{fig:data_example}
\end{figure*}

\clearpage
\section{Data Distribution Detailed}
\label{app:distribution}

We show a more detailed distribution of the dataset in~\cref{tab:detailed}.

\begin{table*}[t]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Class} & \textbf{Total Dataset} & \textbf{Test} \\
\midrule
climate.set\_fan\_mode        & 1080  & 0    \\
climate.set\_humidity         & 1080  & 0    \\
climate.set\_hvac\_mode       & 1080  & 0    \\
climate.set\_temperature      & 1000  & 0    \\
cover.close                   & 385   & 35   \\
cover.open                    & 395   & 40   \\
cover.stop                    & 320   & 25   \\
cover.toggle                  & 365   & 25   \\
fan.decrease\_speed           & 360   & 60   \\
fan.increase\_speed           & 300   & 40   \\
fan.toggle                    & 390   & 85   \\
fan.turn\_off                 & 390   & 70   \\
fan.turn\_on                  & 405   & 60   \\
light.toggle                  & 450   & 90   \\
light.turn\_off               & 2535  & 600  \\
light.turn\_on                & 11940 & 150  \\
lock.lock                     & 200   & 125  \\
lock.unlock                   & 185   & 125  \\
media\_player.media\_next\_track & 55    & 25   \\
media\_player.media\_pause       & 55    & 25   \\
media\_player.media\_play        & 70    & 25   \\
media\_player.media\_previous\_track & 55 & 25 \\
media\_player.media\_stop        & 55    & 25   \\
media\_player.turn\_off          & 25    & 25   \\
media\_player.turn\_on           & 40    & 40   \\
media\_player.volume\_down       & 65    & 35   \\
media\_player.volume\_mute       & 60    & 30   \\
media\_player.volume\_up         & 85    & 40   \\
switch.toggle                 & 250   & 50   \\
switch.turn\_off              & 500   & 175  \\
switch.turn\_on               & 540   & 165  \\
timer.cancel                  & 600   & 0    \\
timer.start                   & 600   & 0    \\
todo.add\_item                & 1560  & 0    \\
vacuum.pause                  & 15    & 0    \\
vacuum.return\_to\_base       & 150   & 0    \\
vacuum.start                  & 370   & 220  \\
vacuum.stop                   & 15    & 0    \\
\bottomrule
\end{tabular}
\caption{\textbf{Detailed Class Distribution Service.} Total Dataset vs.\ LLM Test Subset}
\label{tab:detailed}
\end{table*}


% \section{Hyperparameters}

% Additional training details and hyperparameters can be found in Table~\ref{tab:training-hyperparameters}.

% \begin{table}[h!]
%     \centering
%     \small
%     \begin{tabular}{ll}
%         \toprule
%         \textbf{Parameter}         & \textbf{Value}       \\
%         \midrule
%         Batch size                & 4                    \\
%         Learning rate             & \(2 \times 10^{-5}\) \\
%         Max sequence length       & 2048                 \\
%         Epochs                    & 1                    \\
%         Learning rate schedule    & Cosine               \\
%         \bottomrule
%     \end{tabular}
%     \caption{Training hyperparameters.}
%     \label{tab:training-hyperparameters}
% \end{table}

% \section{Additional Metrics}

% \begin{table*}[ht]
% \centering
% % \small % Reduce font size for the table
% \setlength{\tabcolsep}{8pt} % Adjust column spacing
% \begin{tabular}{lcccc}
% \hline
% \textbf{Action} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
% \hline
% climate.set\_fan\_mode               & 0.92 & 1.00 & 0.96 & 209 \\
% climate.set\_humidity                & 0.94 & 0.98 & 0.96 & 202 \\
% climate.set\_hvac\_mode              & 0.98 & 1.00 & 0.99 & 241 \\
% climate.set\_temperature             & 0.97 & 1.00 & 0.98 & 344 \\
% cover.close                          & 0.90 & 0.49 & 0.63 & 88  \\
% cover.open                           & 0.83 & 0.66 & 0.74 & 74  \\
% cover.stop                           & 0.81 & 0.53 & 0.64 & 64  \\
% cover.toggle                         & 0.64 & 0.58  & 0.60 & 73 \\

% fan.decrease\_speed                  & 0.80 & 0.69 & 0.74 & 74 \\

% fan.increase\_speed                  & 0.75 & 0.83 & 0.79 & 58 \\
% fan.toggle                           & 0.50 & 0.27 & 0.35 & 74 \\
% fan.turn\_off                        & 0.60 & 0.04 & 0.07 & 81 \\

% fan.turn\_on                         & 0.80 & 0.14 & 0.24 & 86 \\
% light.toggle                         & 0.57 & 0.26 & 0.36 & 80 \\
% light.turn\_off                      & 0.70 & 0.71 & 0.71 & 477 \\
% light.turn\_on                       & 0.82 & 0.97 & 0.89 & 2443 \\
% lock.lock                            & 0.74 & 0.41 & 0.53 & 41 \\
% lock.unlock                          & 0.81 & 0.53 & 0.64 & 40 \\
% media\_player.media\_next\_track     & 1.00 & 0.91 & 0.95 & 11 \\
% media\_player.media\_pause           & 0.83 & 0.38 & 0.53 & 13 \\
% media\_player.media\_play            & 1.00 & 0.24 & 0.38 & 17 \\
% media\_player.media\_previous\_track & 1.00 & 0.94 & 0.97 & 16 \\
% media\_player.media\_stop            & 0.43 & 0.50 & 0.46 & 6 \\
% media\_player.turn\_off              & 1.00 & 0.43 & 0.60 & 7 \\
% media\_player.turn\_on               & 1.00 & 0.29 & 0.44 & 7 \\
% media\_player.volume\_down           & 0.83 & 0.45 & 0.59 & 11\\
% media\_player.volume\_mute           & 0.67 & 0.50 & 0.57 & 8 \\
% media\_player.volume\_up             & 1.00 & 0.37 & 0.54 & 19 \\
% switch.toggle                        & 0.93 & 0.60 & 0.73 & 47 \\
% switch.turn\_off                     & 0.87 & 0.69 & 0.77 & 87 \\
% switch.turn\_on                      & 0.83 & 0.60 & 0.70 & 107 \\
% timer.cancel                         & 0.83 & 0.69 & 0.75 & 132 \\
% timer.start                          & 0.90 & 1.00 & 0.94 & 120 \\
% todo.add\_item                       & 0.95 & 1.00 & 0.97 & 300 \\
% vacuum.pause                         & 0.74 & 0.53 & 0.62 & 32 \\
% vacuum.return\_to\_base              & 0.97 & 0.88 & 0.92 & 33 \\
% vacuum.start                         & 0.94 & 0.56 & 0.70 & 78 \\
% vacuum.stop                          & 0.85 & 0.58 & 0.69 & 19 \\
% \_\_\_                               & \_\_\_  & \_\_\_  & \_\_\_  & \_\_\_ \\
%                                                      \\
% Accuracy                  &      &      & 0.84 & 5819  \\
% Macro average             & 0.83 & 0.61 & 0.67 & 5819  \\
% Weighted average          & 0.83 & 0.84 & 0.82 & 5819  \\
% \hline
% \end{tabular}
% \caption{Precision, recall, F1-score, and support for each action.}
% \label{f1-baseline}
% \end{table*}

\end{document}

