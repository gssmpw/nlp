\section{Related Works}
\textbf{Diffusion-based Image and Video Generation.} Diffusion models have gained significant traction in image and video generation \cite{nichol2021glide, rombach2022high, croitoru2023diffusion, guo2023zero, blattmann2023align, esser2024scaling, wang2024videocomposer, peng2024conditionvideo}. In image generation, DDPM \cite{ho2020denoising} and its variants \cite{song2020denoising, dhariwal2021diffusion, nichol2021improved, rombach2022high, croitoru2023diffusion, guo2023zero} have demonstrated impressive results in producing detailed and realistic images. They iteratively refine noisy images, progressively improving quality and coherence. In addition, recent advances \cite{ho2022imagen, ho2022video, wu2023tune, blattmann2023align, wang2024videocomposer} have extended diffusion models to video generation, where temporal consistency is crucial. These methods build upon the success of image-based diffusion models by incorporating temporal attention mechanisms to ensure consistency across frames. However, challenges persist, particularly with long video generation, due to the computational and memory demands of processing hundreds of frames.

\textbf{Text-driven Video Editing.} Recently, an increasing number of works have applied pre-trained text-to-image diffusion models to video editing \cite{wang2023zero, wu2023tune, cohen2024slicedit, ma2024follow, liu2024video}, with the primary challenge being maintaining temporal consistency across frames. Zero-shot video editing methods have gained attention for addressing this issue. 
FateZero \cite{qi2023fatezero} introduced an attention blending module, combining attention maps from the source and edited videos during the denoising process to improve consistency. 
TokenFlow \cite{geyer2023tokenflow} computes frame feature correspondences via nearest neighbors, which is similar to optical flow, enhancing coherence. 
Similarly, Flatten \cite{cong2023flatten} proposed flow-guided attention that uses optical flow to guide attention for smoother editing. 
Video-P2P \cite{liu2024video} adapted classic image editing methods to video, but editing even an 8-frame video takes over ten minutes, making it impractical for real-world applications. Although these methods offer effective solutions for video editing, they struggle with long videos having thousands of frames.
InsV2V \cite{cheng2023consistent} directly trains a video-to-video model and proposes a method for long video editing, but it only edits about 20-30 frames ($\sim1s$) at a time and stitches them together, resulting in cumulative errors and quality decline after several iterations. In addition to processing long videos, great content modification is also a research focus \cite{cong2023flatten, geyer2023tokenflow}. In particular, this challenge often requires large-scale training or test-time tuning \cite{wu2023tune, qi2023fatezero, gu2024videoswap}, such as FateZero \cite{qi2023fatezero} that performs structural editing with test-time tuning, which is orthogonal to the contribution of this paper. In this paper, we mainly focus on training-free solutions for extremely long video editing.

\begin{figure*}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{fig/framework.pdf}}
\vskip -0.1in
\caption{The framework of the proposed AdaFlow. (a) The pipeline of AdaFlow for long video editing. Given a source video and the text editing prompt, AdaFlow first applies \emph{Adaptive Keyframe Selection} (AKS) (b) to adaptively divide the video into clips according to its content and then sample frames for keyframe translation. Afterwards, \emph{Adaptive Attention Slimming} (AAS) (c) is applied to reduce the redundant tokens in \emph{Extended Self-Attention} for keyframe translation, thereby increasing the number of frames edited. Finally, the editing information of the keyframes is propagated throughout the entire video.}
\label{fig:framework}
\end{center}
\vskip -0.3in
\end{figure*}