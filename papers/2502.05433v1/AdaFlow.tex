%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{xcolor}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\hypersetup{
    colorlinks=true,    % 开启链接彩色显示
    urlcolor=blue        % 设置URL链接的颜色为红色
}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming And Keyframe Selection}
% \let\oldtwocolumn\twocolumn
% \renewcommand\twocolumn[1][]{%
%     \oldtwocolumn[{#1}{
%     \begin{center}
%     \vspace{-6mm}
%            \includegraphics[width=.98\textwidth]{fig/opening.pdf}
%            \vspace{-2mm}
%            \captionof{\textit{Figure 1: }}{Our StoryWeaver can achieve high-quality story visualization based on the given characters within a unified model.}
%            % \vspace{-.5mm}
%            \label{fig:FIG1}
%         \end{center}
%     }]
% }
\begin{document}

\twocolumn[
\icmltitle{AdaFlow: Efficient Long Video Editing via\\Adaptive Attention Slimming And Keyframe Selection}
% \begin{center}
% \centerline{\includegraphics[width=1.8\columnwidth]{fig/opening.pdf}}
% \captionof{figure}{The proposed AdaFlow can support the text-driven video editing of more than 1$k$ frames in one inference, which can be the change of the primary subjects, background, or overall style of the video. Meanwhile, AdaFlow can also adaptively select the representative frames in different video clips for keyframe translation, ensuring the continuity and quality of long video editing.}
% \label{fig:opening}
% \end{center}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shuheng Zhang}{xxx,yyy}
\icmlauthor{Yuqi Liu}{xxx}
\icmlauthor{Hongbo Zhou}{xxx}
\icmlauthor{Jun Peng}{zzz}
\icmlauthor{Yiyi Zhou}{xxx,yyy}
\icmlauthor{Xiaoshuai Sun}{xxx,yyy}
\icmlauthor{Rongrong Ji}{xxx,yyy}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{xxx}{Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China}
\icmlaffiliation{yyy}{Institute of Artificial Intelligence, Xiamen University, 361005, P.R. China}
\icmlaffiliation{zzz}{Peng Cheng Laboratory, Shenzhen, 518000, P.R. China}

\icmlcorrespondingauthor{Yiyi Zhou}{zhouyiyi@xmu.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{figure*}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{fig/opening.pdf}}
\vskip -0.1in
\caption{The proposed AdaFlow can support the text-driven video editing of more than 1$k$ frames in one inference. Meanwhile, AdaFlow can adaptively select the representative frames for keyframe translation, ensuring the continuity and quality of long video editing.}
\label{fig:opening}
\end{center}
\vskip -0.3in
\end{figure*}

\begin{abstract}
Despite great progress, text-driven long video editing is still notoriously challenging mainly due to excessive memory overhead. Although recent efforts have simplified this task into a two-step process of keyframe translation and interpolation generation, the token-wise keyframe translation still plagues the upper limit of video length. In this paper, we propose a novel and training-free approach towards efficient and effective long video editing, termed \textbf{\emph{AdaFlow}}. We first reveal that not all tokens of video frames hold equal importance for keyframe translation, based on which we propose an \emph{Adaptive Attention Slimming} scheme for AdaFlow to squeeze the $KV$ sequence, thus increasing the number of keyframes for translations by an order of magnitude. In addition, an \emph{Adaptive Keyframe Selection} scheme is also equipped to select the representative frames for joint editing, further improving generation quality. With these innovative designs, AdaFlow achieves high-quality long video editing of minutes in one inference, \emph{i.e.}, more than 1$k$ frames on one A800 GPU, which is about ten times longer than the compared methods, \emph{e.g.}, TokenFlow. To validate AdaFlow, we also build a new benchmark for long video editing with high-quality annotations, termed \textbf{\emph{LongV-EVAL}}. Our code is released at: \href{https://github.com/jidantang55/AdaFlow}{https://github.com/jidantang55/AdaFlow}.
\end{abstract}

\section{Introduction}

% text-guided video editing 的重要性以及挑战
Recent years have witnessed the great success of diffusion-based models in high-quality text-driven image generation and editing \cite{ho2020denoising, hertz2022prompt, couairon2022diffedit, tumanyan2023plug, brooks2023instructpix2pix, tewel2024training}. More recently, the rapid development of image diffusion models also sparks an influx of attention to text-driven video editing \cite{geyer2023tokenflow, cong2023flatten, qi2023fatezero}. As a milestone in the research of \emph{AI Generated Content} (AIGC), text-driven video editing can well broaden the application scope of diffusion models, such as \emph{animation creation}, \emph{virtual try-on}, and \emph{video effects enhancement}. However, compared with the well-studied image editing, text-driven video editing is still far from satisfactory due to its high requirement of frame-wise consistency \cite{wu2023tune, qi2023fatezero, yang2023rerender, yang2024fresco}. Meanwhile, its extremely high demand for computation resources also greatly hinders development \cite{cong2023flatten, wu2023tune, kara2024rave}.

% 现有方法及不足
Most existing methods \cite{cong2023flatten, wu2023tune, kara2024rave, liu2024video} can only support video editing of a few seconds, and long video editing is still notoriously challenging. In particular, current research often resorts to the well-trained image diffusion models for video editing via test-time tuning \cite{wu2023tune, liu2024video} or training-free paradigms \cite{ceylan2023pix2video, cong2023flatten, kara2024rave}.  
To maintain the smoothness and consistency of edited videos, these methods primarily extend the self-attention module in diffusion models to all video frames, commonly referred to as \emph{extended self-attention} \cite{geyer2023tokenflow, wu2023tune}. 
Despite its effectiveness, this solution will lead to a quadratic increase in computation as the number of video frames grows, and the token-based representations of these video frames further greatly exacerbate the memory footprint. For instance, the editing of ten video frames needs to compute extended self-attention on up to 40$k$ visual tokens in the diffusion model \cite{geyer2023tokenflow}.
As a result, processing only a few video frames will require a prohibitive GPU memory footprint,  making existing approaches can only conduct video editing of several seconds.

% 更新的现有方法
To alleviate this issue, recent endeavors focus on factorizing video editing into a two-step generation task \cite{geyer2023tokenflow, yang2023rerender, yang2024fresco}.
The first step is \emph{keyframe translation}, which samples the video keyframes to perform extended self-attention. 
Afterwards, all frames are fed to the diffusion model for editing based on the translated keyframe information, often termed \emph{interpolation generation} \cite{geyer2023tokenflow}. 
Compared to the direct editing on all video frames, this two-step solution only needs to perform the quadratic computation of extended self-attention for the keyframes, thus improving the number of overall editing frames from a dozen to nearly one hundred frames \cite{geyer2023tokenflow}. 
However, the basic mechanism of extended self-attention is still left unexplored, making these approaches \cite{geyer2023tokenflow, yang2023rerender, yang2024fresco} still hard to achieve minute-long video editing in one inference. 
Moreover, the naive uniform sampling of keyframes \cite{geyer2023tokenflow} also does not consider the change of video content, \emph{e.g.}, the motion of objects or the transitions of the scene, and a large sampling interval will inevitably undermine video quality.

%  我们的方法
In this paper, we propose a novel and training-free method called \textbf{\emph{AdaFlow}} for high-quality long video editing.
In particular, we first observe that during extended self-attention, not all visual tokens of a video frame are equally important for maintaining frame consistency and video continuity. Only the tokens of the frame correspond to the \emph{query} matter.
In this case, \emph{Adaptive Attention Slimming} is proposed to squeeze the less important ones in the $KV$ sequence of extended self-attention, thereby greatly alleviating the computation burden. 
Meanwhile, we also introduce an \emph{Adaptive Keyframe Selection} for AdaFlow to pick up the frames that can well represent the edited video content, thus avoiding the translation of redundant keyframes and improving the utilization of computation resources. 
With these innovative designs, AdaFlow can improve the number of video frames edited by an order of magnitude.

% 数据集介绍
To well validate the proposed AdaFlow, we also propose a new long video editing benchmark to complement the existing evaluation system, termed \textbf{\emph{LongV-EVAL}}.
This benchmark consists of 75 videos,  and they are about one minute long and cover various scenes, such as \emph{humans}, \emph{landscapes}, \emph{indoor settings} and \emph{animals}. 
For LongV-EVAL, we meticulously design a data annotation process, which applies multimodal large language models \cite{achiam2023gpt, lin2023video} to generate three high-quality editing prompts for each video. 
These prompts focus on different aspects of the video, such as \emph{primary subjects}, \emph{background}, \emph{overall style}, and \emph{so on}. In terms of evaluation metrics, we follow \cite{sun2024diffusion} to evaluate the edited videos from the aspects of \emph{frame quality}, \emph{video quality}, \emph{object consistency}, and \emph{semantic consistency} on LongV-EVAL.

% 数据集比较
To validate AdaFlow, we conduct extensive experiments on the proposed LongV-EVAL benchmark, and also compare AdaFlow with a set of advanced video editing methods \cite{yang2023rerender, geyer2023tokenflow, cong2023flatten, yang2024fresco, kara2024rave}. 
Both qualitative and quantitative results show that our AdaFlow has obvious advantages over the compared methods in terms of the efficiency and quality of long video editing. 
More importantly, AdaFlow can effectively conduct various high-quality edits for videos of more than 1$k$ frames on a single GPU\footnote{In our appendix, we also achieve one editing of 10k frames.}, \emph{e.g.}, changing the main object, background or overall style.

% 总结
Conclusively, the contribution of this paper is threefold: 
\begin{itemize}
    \item We propose a novel and training-free video editing method called \textbf{\emph{AdaFlow}} with two innovative designs, namely \emph{Adaptive Attention Slimming} and \emph{Adaptive Keyframe Selection}. 
    \item The proposed AdaFlow is capable of long video editing of more than 1$k$ frames in one inference on a single GPU, and it also supports various editing tasks, such as the changes of background, foreground, and styles.
    \item We also build a high-quality benchmark to complement the lack of long video editing evaluation, termed \textbf{\emph{LongV-EVAL}}. On this benchmark, our AdaFlow shows obvious advantages over the compared methods in terms of efficiency and quality.
\end{itemize} 

\section{Related Works}

\textbf{Diffusion-based Image and Video Generation.} Diffusion models have gained significant traction in image and video generation \cite{nichol2021glide, rombach2022high, croitoru2023diffusion, guo2023zero, blattmann2023align, esser2024scaling, wang2024videocomposer, peng2024conditionvideo}. In image generation, DDPM \cite{ho2020denoising} and its variants \cite{song2020denoising, dhariwal2021diffusion, nichol2021improved, rombach2022high, croitoru2023diffusion, guo2023zero} have demonstrated impressive results in producing detailed and realistic images. They iteratively refine noisy images, progressively improving quality and coherence. In addition, recent advances \cite{ho2022imagen, ho2022video, wu2023tune, blattmann2023align, wang2024videocomposer} have extended diffusion models to video generation, where temporal consistency is crucial. These methods build upon the success of image-based diffusion models by incorporating temporal attention mechanisms to ensure consistency across frames. However, challenges persist, particularly with long video generation, due to the computational and memory demands of processing hundreds of frames.

\textbf{Text-driven Video Editing.} Recently, an increasing number of works have applied pre-trained text-to-image diffusion models to video editing \cite{wang2023zero, wu2023tune, cohen2024slicedit, ma2024follow, liu2024video}, with the primary challenge being maintaining temporal consistency across frames. Zero-shot video editing methods have gained attention for addressing this issue. 
FateZero \cite{qi2023fatezero} introduced an attention blending module, combining attention maps from the source and edited videos during the denoising process to improve consistency. 
TokenFlow \cite{geyer2023tokenflow} computes frame feature correspondences via nearest neighbors, which is similar to optical flow, enhancing coherence. 
Similarly, Flatten \cite{cong2023flatten} proposed flow-guided attention that uses optical flow to guide attention for smoother editing. 
Video-P2P \cite{liu2024video} adapted classic image editing methods to video, but editing even an 8-frame video takes over ten minutes, making it impractical for real-world applications. Although these methods offer effective solutions for video editing, they struggle with long videos having thousands of frames.
InsV2V \cite{cheng2023consistent} directly trains a video-to-video model and proposes a method for long video editing, but it only edits about 20-30 frames ($\sim1s$) at a time and stitches them together, resulting in cumulative errors and quality decline after several iterations. In addition to processing long videos, great content modification is also a research focus \cite{cong2023flatten, geyer2023tokenflow}. In particular, this challenge often requires large-scale training or test-time tuning \cite{wu2023tune, qi2023fatezero, gu2024videoswap}, such as FateZero \cite{qi2023fatezero} that performs structural editing with test-time tuning, which is orthogonal to the contribution of this paper. In this paper, we mainly focus on training-free solutions for extremely long video editing.

\begin{figure*}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{fig/framework.pdf}}
\vskip -0.1in
\caption{The framework of the proposed AdaFlow. (a) The pipeline of AdaFlow for long video editing. Given a source video and the text editing prompt, AdaFlow first applies \emph{Adaptive Keyframe Selection} (AKS) (b) to adaptively divide the video into clips according to its content and then sample frames for keyframe translation. Afterwards, \emph{Adaptive Attention Slimming} (AAS) (c) is applied to reduce the redundant tokens in \emph{Extended Self-Attention} for keyframe translation, thereby increasing the number of frames edited. Finally, the editing information of the keyframes is propagated throughout the entire video.}
\label{fig:framework}
\end{center}
\vskip -0.3in
\end{figure*}

\section{Preliminary}
\label{preliminary}

\textbf{Diffusion Models.} \emph{Denoising diffusion probabilistic model} (DDPM) \cite{ho2020denoising} is a generative network that aims at reconstructing a forward Markov chain $\{x_1, \ldots, x_T\}$. For a data distribution $x_0 \sim q(x_0)$, the Markov transition $q(x_t | x_{t-1})$ follows a Gaussian distribution with a variance schedule $\beta_t \in (0, 1)$:
\begin{equation}
q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}\right)=\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{1-\beta_t} \boldsymbol{x}_{t-1}, \beta_t \mathbf{I}\right).\label{eq:dm1}
\end{equation}
To generate the Markov chain $\{x_0, \ldots, x_T\}$, DDPM employs a reverse mechanism with an initial distribution $p(x_T) = \mathcal{N}(x_T; 0, I)$ and Gaussian transitions. A neural network $\epsilon_\theta$ is trained to estimate the noise, ensuring that the reverse mechanism approximates the forward process:
\begin{equation}
    p_\theta\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)=\mathcal{N}\left(\boldsymbol{x}_{t-1} ; \mu_\theta\left(\boldsymbol{x}_t, \boldsymbol{\tau}, t\right), \Sigma_\theta\left(\boldsymbol{x}_t, \boldsymbol{\tau}, t\right)\right), 
\end{equation}
where $\tau$ denotes the text prompt. The parameters $\mu_\theta$ and $\Sigma_\theta$ are inferred by the denoising model $\epsilon_\theta$. Latent diffusion \cite{rombach2022high} alleviates the computational demands by executing these processes within the latent space of a \emph{variational autoencoder} \cite{kingma2013auto}.

\textbf{Diffusion Features.} \emph{Diffusion Features} (DIFT) can extract the correspondence of images from the diffusion network $\epsilon_\theta$ without explicit supervision \cite{tang2023emergent}. Starting from noise $z$, a series of images $x_t$ are generated by gradual denoising through a reverse diffusion process. At each timestep $t$, the output of each layer of $\epsilon_\theta$ can be used as a feature. Larger $t$ and earlier network layers produce more semantically aware features, while smaller $t$ and later layers focus more on low-level details. To extract DIFT from an existing image, \citet{tang2023emergent} propose adding noise of timestep $t$ to the real image, then inputting it into the network $\epsilon_\theta$ along with $t$ to extract the latent of the intermediate layer as DIFT. This method predicts corresponding points between two images, and can even generate correct correspondences across different domains.

\textbf{Extended Self-Attention.} 
To ensure video smoothness and coherence, the self-attention block of an image diffusion model must edit all frames simultaneously \cite{wu2023tune, geyer2023tokenflow}. In this case, \emph{Extended Self-Attention} (ESA) is introduced to maintain the coherence and temporal consistency of the video. For the latent of the $i$-th frame at timestep $t$, denoted as $z_t^i$, the attention score is computed between the $i$-th frame and all other $n$ frames. Mathematically, the extended self-attention can be formulated as
\begin{equation}
\text{Attention}(Q_i,K_{1:n},V_{1:n}) = \text{Softmax}\left(\frac{Q_i K_{1:n}^T}{\sqrt{d}}\right)\cdot V_{1:n},
\end{equation}
where $Q_i = W^Qz_t^i, K_{1:n} = W^Kz_t^{1:n}, V_{1:n} = W^Vz_t^{1:n}$.
Here, $W^Q$, $W^K$, and $W^V$ are the weighted matrices identical to those used in the self-attention layers of the image diffusion model.

\begin{algorithm}[ht]
   \caption{Adaptive Video Partitioning}
   \label{algo:segment}
   \begin{algorithmic}[1] % [1] 添加行号
    \REQUIRE \ \\
    $\mathcal{F}$: DIFT for each frame, \\ 
    $n$: Number of frames, \\
    $l$: Sliding window size, \\
    $s$: Step size, \\
    $ms$: Mean threshold, \\
    $ws$: Window threshold.
   \ENSURE \ \\
   \textbf{segment\_starts}: List of segment start indices.
   \STATE \textbf{initialize} segment\_starts $\gets$ [ ]
   \STATE $i \gets 1$, $j \gets 2$
   \WHILE{$j < n$}
        \STATE \textbf{calculate} $H_{i, j}$ with $F_i$, $F_j$
        \IF{\textbf{mean}($H_{i, j}$) $<$ $ms$ \textbf{or not} \texttt{window\_check}($H_{i, j}$, $l$, $s$, $ws$)}
            \STATE \textbf{append} $i$ to segment\_starts
            \STATE $i \gets j + 1$
            \STATE $j \gets i + 1$
        \ELSE
            \STATE $j \gets j + 1$
        \ENDIF
   \ENDWHILE
   \STATE \textbf{return} segment\_starts
   \end{algorithmic}
\end{algorithm}

\section{Method}
Given a source video of $n$ frames, $\mathcal{I}=[\boldsymbol{I}_1,...,\boldsymbol{I}_n]$, $\boldsymbol{I}_{i} \in \mathbb{R} ^{H\times W} $, where $H\times W$ denotes the resolution, and a text prompt $\mathcal{P}$ describing the editing task, we first use a pre-trained text-to-image diffusion model $\epsilon_\theta$ to extract its diffusion features, denoted as $\mathcal{F}=[\boldsymbol{F}_1,...,\boldsymbol{F}_n]$, $\boldsymbol{F}_{i} \in \mathbb{R} ^{h\times w\times d} $. Based on the obtained diffusion features $\mathcal{F}$, AdaFlow employs \emph{Adaptive Keyframe Selection} (Sec.\ref{method1}) to divide the video into multiple clips based on the content. For each clip that consists of consecutive frames with similar content, one frame is then sampled as a keyframe at each timestep, and all keyframes are edited simultaneously using $\epsilon_\theta$. To edit videos as long as possible, AdaFlow then applies \emph{Adaptive Attention Slimming} to reduce the length of $KV$ sequences in extended self-attention for keyframe translation (Sec. \ref{method2}). Finally, the information from translated keyframes is propagated to the remaining frames to ensure smoothness and continuity throughout the edited video, which is denoted as $\mathcal{J}=[\boldsymbol{J}^1,...,\boldsymbol{J}^n]$ (Sec. \ref{method3}).

\textbf{Pre-processing.} Given the source video $\mathcal{I}$, we first use a pre-trained text-to-image diffusion model $\epsilon_\theta$ to extract the diffusion features of each frame $\boldsymbol{I}_i$, resulting in $\mathcal{F}=[\boldsymbol{F}_1,...,\boldsymbol{F}_n]$. Afterwards, we use the diffusion model $\epsilon_\theta$ to perform DDIM inversion \cite{song2020denoising} on each frame $\boldsymbol{I}_i$ to obtain a sequence of latents, which will be used in the subsequent editing. 

\subsection{Adaptive Keyframe Selection}
\label{method1}

Keyframe selection is critical for long video editing, which however is often ignored in previous research \cite{wu2023tune, cong2023flatten, liu2024video}. When the visual content of a given video changes rapidly, keyframe samplings at shorter intervals are usually required to ensure the editing quality \cite{geyer2023tokenflow}, but it will result in a large number of redundant frames for editing. To address this issue, we propose \emph{Adaptive Keyframe Selection} (AKS) based on the video content. 

In particular, consecutive and similar frames are grouped into clips allowing for more informed keyframe sampling. In periods where the visual content changes rapidly, keyframes can be selected more densely, whereas fewer frames are required for less dynamic content. In this case, AKS can retain editing quality while reducing the computational burden, particularly for videos with little variation. As shown in Fig.\ref{fig:ap_10000} of Appendix, our AdaFlow can even process hour-long videos with fewer variations.

In practice, \emph{Adaptive Keyframe Selection} (AKS) resorts to DIFT features for frame-wise similarity. DIFT can effectively match corresponding points between images \cite{tang2023emergent}. It is shown that when two images are not very similar, the confidence level of the matching decreases significantly. Based on this principle,  AKS uses DIFT to quickly assess the degree of change in a video. As shown in Fig.\ref{fig:framework} (b), we can obtain a heatmap to represent the temporal dynamics \cite{brooks2022generating} between frames using DIFT. When there is a noticeable shift in the angle of objects in the frame or a sudden appearance of new objects, these regions will show brighter colors in the heatmap. 

Concretely, to compute the heatmap $H_{i, j}\in \mathbb{R} ^{h\times w}$ of the temporal dynamics between the $i$-th frame and the $j$-th frame, we compute the token-wise cosine similarity using their DIFT features. For a token $p$ in the $i$-th frame and a token $q$ in the $j$-th frame, whose feature vectors are $f_i^p \in \mathbf{F}_i$ and $f_j^q \in \mathbf{F}_j$, the cosine similarity $CS(\cdot)$ is computed by 
\begin{equation}
    CS(f_i^p, f_j^q) = \frac{f_i^p \cdot f_j^q}{\|f_i^p\| \|f_j^q\|}.
\end{equation}
Then the token $q^*$ most similar to the token $p$ is obtained by 
\begin{equation}
\label{eq:max_position}
    q^* = \arg\max_{q \in \boldsymbol{T}_j} CS(f_i^p, f_j^q),
\end{equation}
where $\boldsymbol{T}_j$ denotes all tokens corresponding to the $j$-th frame.

Finally, the value of token $p$ in the heatmap is
\begin{equation}
\label{eq:heatmap}
    H_{i, j}^p = CS(f_i^p, f_j^{q^*}).
\end{equation}
After obtaining the heatmaps of a video, we can use them to segment clips that consist of consecutive frames with similar content, of which procedure is described in Algorithm \ref{algo:segment}. In principle, we determine the partition points of the video by calculating the similarity between video frames. Specifically, we traverse the sequence of video frames and calculate the similarity heatmap for the frame pair. If the mean value of the heatmap between a pair of frames is smaller than a defined threshold, or if the sliding window finds the mean value below the threshold at any point, the current frame will be marked as the start of a new clip. Then, we continue traversing from the next possible starting point until the entire video is processed. Finally, we obtain the starting indices of all clips $\mathcal{S}=\{\boldsymbol{s}_1,...,\boldsymbol{s}_M\}$, where $M$ represents the total number of clips.

In Appendix \ref{ap:keyframe}, we visualize the content-aware video partitioning with a $y-t$ plot. As shown in Fig.\ref{fig:yt}, the adaptively partitioned video clips are similar within each part, but the partitioning points are accurately positioned where the video content undergoes rapid changes.

After partitioning, we can directly select a frame from each partition at each timestep, obtaining a total of $M$ keyframes, denoted as $\mathcal{K}=[\boldsymbol{I}_{k_1},...,\boldsymbol{I}_{k_M}]$, where $\boldsymbol{s}_i \le k_{i} < \boldsymbol{s}_{i+1}$.

\subsection{Adaptive Attention Slimming}
\label{method2}

As mentioned in Section \ref{preliminary}, we use extended self-attention for keyframe translation \cite{wu2023tune},  thereby ensuring the smoothness and continuity of edited videos. However, extended self-attention involves the concatenation of $KV$ tokens of all frames, resulting in a quadratic increase in computation.  Moreover, the extremely high GPU memory footprint becomes a bottleneck for long video editing. Besides, if the number of keyframes is severely limited, it will significantly hinder the length of the editable video and adversely affect the editing quality. To address this issue, we propose a novel \emph{Adaptive Attention Slimming} (AAS) method to reduce the $KV$ sequence of extended self-attention, which can significantly improve computational efficiency without affecting video editing quality.

Concretely, given one keyframe $I_{k_i}$, similar to Eq.\ref{eq:heatmap}, we use DIFT to calculate $M$ cosine similarity heatmaps between this keyframe and all other keyframes, denoted as $H=\{H_{k_{1},  k_{i}}, H_{k_{2},  k_{i}}, \dots, H_{k_{M},  k_{i}}\}$. From these heatmaps, we select the $m$ pixel positions with the highest values. For $K$ and $V$ in extended self-attention, we retain only the tokens corresponding to these $m$ positions and obtain new $\widetilde{K}_{k_{1}:k_{M}}$ and $\widetilde{V}_{k_{1}:k_{M}}$, of which length is much shorter than the default ones. Afterwards, the slimmed Extended Self-attention is defined by 
% \begin{equation}
%     \text{Attention}(Q_i,\widetilde{K}_{k_{1}:k_{M}},\widetilde{V}_{k_{1}:k_{M}})=\text{Softmax}\left(\frac{Q_i \widetilde{K}_{k_{1}:k_{M}}^T}{\sqrt{d}}\right)\cdot \widetilde{V}_{k_{1}:k_{M}}.
% \end{equation}
\begin{equation}
\begin{aligned}
    &\text{Attention}(Q_i,\widetilde{K}_{k_{1}:k_{M}},\widetilde{V}_{k_{1}:k_{M}}) = \\
    &\quad \text{Softmax}\left(\frac{Q_i \widetilde{K}_{k_{1}:k_{M}}^T}{\sqrt{d}}\right) \cdot \widetilde{V}_{k_{1}:k_{M}}.
\end{aligned}
\end{equation}
For ease of subsequent calculations, we abbreviate $\text{Attention}(Q_i,\widetilde{K}_{k_{1}:k_{M}},\widetilde{V}_{k_{1}:k_{M}})$ as $\mathcal{A}_{i}$.

In Appendix \ref{ap:pruning}, we visualize the relationship between the retained tokens in the \emph{key/value} pairs and the \emph{query}. It can be intuitively observed that the KV tokens more related to the \emph{query} frames are retained more, while the ones different from the \emph{query} are often discarded. It is because over longer time spans, more content becomes dissimilar to the \emph{query}, and attending to these contents does not significantly improve the generation quality and consistency of the \emph{query} frames. Conversely, frames closer to the \emph{query} are crucial for maintaining the video’s coherence. Therefore, the proposed AAS can save computational resources and minimize the impact on video editing quality.

\subsection{Feature-Matched Latent Propagation}
\label{method3}

Similar to TokenFlow \cite{geyer2023tokenflow}, we propagate the generation of keyframes to non-keyframes based on the token correspondences obtained from the source video, thus generating a continuous and smooth video. However, unlike TokenFlow \cite{geyer2023tokenflow}, which requires the calculations of token correspondences at each timestep and every self-attention operation, our method only needs to compute the correspondences once before editing, and saves them for the use in following timesteps. This setting greatly simplifies the computational process. 

Specifically, given the source video and the obtained video clips, we compute token correspondences between every two frames within the same clip. The formula for calculating the spatial position $p$ of the $i$-th frame corresponding to the $j$-th frame is the same as Eq.\ref{eq:max_position}. For convenience, we express the correspondence between the position $p$ in the $i$-th frame and the position $q^*$ in the $j$-th frame as
\begin{equation}
    \phi_{ij}(p) = q^*.
\end{equation}
For each non-keyframe $i$, there is a keyframe $j$ within the same video clip. Through the calculation above, we can map each token in $\mathcal{A}_{i}$ to a corresponding token in $\mathcal{A}_{j}$, which can be expressed as
\begin{equation}
    \mathcal{A}_{i}[p] = \mathcal{A}_{j}[\phi_{ij}(p)].
\end{equation}
For cases where there may be an inconsistent size between $F_{i}$ and the output latent of self-attention $\mathcal{A}_{i}$, a simple resize operation is sufficient and will not affect the generation.

\begin{table*}[ht]
\caption{Comparisons between AdaFlow and SOTA methods on LongV-EVAL. Here, \emph{Mins/Video} denotes the average minutes for video editing. \emph{FQ}, \emph{VQ}, \emph{OC}, and \emph{SC} denote \emph{frame quality}, \emph{video quality}, \emph{object consistency}, and \emph{semantic consistency}, respectively.}
\label{tab:quantitative}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|cccc|c}
\toprule
                                        \textbf{Method}  & \textbf{FQ↑}           & \textbf{VQ↑}            & \textbf{OC↑}            & \textbf{SC↑}            & \textbf{Mins/Video↓} \\ \midrule
Rerender\cite{yang2023rerender} & 5.36          & 0.638          & 0.942          & 0.961          & 52          \\
TokenFlow\cite{geyer2023tokenflow} & 5.30 & 0.808 & \underline{0.947} & \underline{0.966} & \underline{40} \\
FLATTEN\cite{cong2023flatten}   & 5.05          & 0.637          & 0.882          & 0.931          & 80          \\
RAVE\cite{kara2024rave}         & 5.17          & 0.677          & 0.861          & 0.909          & 83          \\
FRESCO\cite{yang2024fresco}     & \textbf{5.65} & \underline{0.820}          & 0.930          & 0.954          & 47          \\ \midrule
AdaFlow (ours)      & \underline{5.43}    & \textbf{0.839} & \textbf{0.953} & \textbf{0.969} & \textbf{24} \\ \bottomrule
\end{tabular}%
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}

\begin{table*}[ht]
\caption{User study. 18 participants are asked to evaluate the edited videos of different methods in terms of video quality and temporal consistency. The values are the percentages of choices.}
\label{tab:user}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|cccccc}
\hline
       \textbf{Metrics}         & Rerender & TokenFlow & FLATTEN & FRESCO & RAVE &  AdaFlow (Ours)          \\ \hline
\textbf{Video Quality}        & 0.0\%      & 12.5\%      & 1.8\%     & 4.5\%    & 3.6\%  & \textbf{77.7\%} \\
\textbf{Temporal Consistency} & 0.0\%      & 10.7\%      & 1.8\%     & 11.6\%   & 0.0\%  & \textbf{75.9\%} \\ \hline
\end{tabular}%
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}

\begin{figure*}[ht]
% \vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{fig/qualitative.pdf}}
\vskip -0.1in
\caption{Comparisons of AdaFlow with a set of advanced video editing methods (a) and ablation study for Adaptive Keyframe Selection (AKS) (b). (a) The red box refers to the failed editing of advanced video editing methods, \emph{e.g.}, the changes of objects or background, or the inconsistency between frames. Compared with the other methods, our AdaFlow can not only process videos of up to 1$k$ frames in one inference but also can well keep the quality and continuity of edited videos. (b) The ablation shows that AKS can capture the abrupt changes of edited videos to ensure the editing quality, \emph{e.g.}, the appearance of the car (above), or the girl dancing (below). Without AKS, the rapidly changing parts of the video are often blurry.}
\label{fig:qualitative}
\end{center}
\vskip -0.3in
\end{figure*}

% \begin{figure*}[ht]
% \vskip 0.1in
% \begin{center}
% \centerline{\includegraphics[width=2\columnwidth]{fig/ablation.pdf}}
% \caption{Ablation Study for Adaptive Keyframe Selection (AKS). AKS can capture the abrupt changes of edited videos to ensure the editing quality, \emph{e.g.}, the appearance of the car (left), or the cat yawning suddenly (right). Without AKS, the rapidly changing parts of the video are often blurry.}
% \label{fig:ablation}
% \end{center}
% \vskip -0.2in
% \end{figure*}

\section{Experiments}
\subsection{Long Video Editing Evaluation Benchmark}

In this paper, we also propose a new long video editing benchmark considering the lack of specific evaluation of text-driven long video editing, termed \textbf{\emph{LongV-EVAL}}. Concretely, we collected 75 videos of approximately 1 minute in length from websites that provides royalty-free and freely usable media content, which cover various domains such as landscapes, people, and animals. We then annotate the videos using Video-LLaVA \cite{lin2023video} and GPT-4 \cite{achiam2023gpt}, generating three high-quality video editing prompts for each video. These three prompts focus on different aspects of editing, \emph{i.e.}, the change to foreground, background or overall style. More details of this benchmark are described in Appendix \ref{ap:dataset}.

In terms of evaluation, we follow \cite{sun2024diffusion} to use four quantitative evaluation metrics. (1) \textbf{Frames Quality} (FQ): We use the LAION aesthetic predictor \cite{schuhmann2021laion}, which is aligned with human rankings, for image-level quality assessment. This predictor estimates aspects such as composition, richness, artistry, and visual appeal of the images. We take the average aesthetic score of all frames as the overall quality score of the video. (2) \textbf{Video Quality} (VQ): We use the DOVER score \cite{wu2023exploring} for video-level quality assessment. DOVER is the most advanced video evaluation method trained on a large-scale human-ranked video dataset. It can evaluate aspects such as artifacts, distortions, blurriness, and incoherence. (3) \textbf{Object Consistency} (OC): In addition to evaluating overall video quality, maintaining object consistency in long video editing is also important. We use DINO \cite{caron2021emerging}, a self-supervised pre-trained image embedding model, to calculate frame-to-frame similarity at the object level. (4) \textbf{Semantic Consistency} (SC): CLIP \cite{radford2021learning} visual embeddings are widely used to capture the semantic information of images. The cosine similarity of CLIP embeddings between adjacent frames is a standard metric for evaluating the frame-to-frame consistency and overall smoothness of a video.

\subsection{Experimental Setups}

In our experiments, we use the official pre-trained weights of Stable Diffusion (SD) 2.1 \cite{rombach2022high} as the text-to-image model. We employ DDIM Inversion with 50 timesteps and denoising with 50 timesteps. For image editing, we adopt PnP-Diffusion \cite{tumanyan2023plug}. When extracting DIFT, we select the features corresponding to t=0 for each frame of the source video \cite{tang2023emergent}, which are extracted from the intermediate layer of the 2D Unet Decoder. During editing, the video resolution is set to 384x672. For keyframe selection, the average similarity threshold is set to 0.75, and the similarity threshold within the sliding window is set to 0.6. The sliding window has a side length of 42 pixels, with a step size of 21 pixels per slide. For joint editing of keyframes, if the number of keyframes exceeds 14, pruning is initiated. We consistently retain the token count corresponding to 14 frames, with the degree of pruning increasing as the number of keyframes increases. All our experiments are conducted on an NVIDIA A800 80GB GPU. 
The main compared methods include Rerender \cite{yang2023rerender}, TokenFlow \cite{geyer2023tokenflow}, FLATTEN \cite{cong2023flatten}, FRESCO \cite{yang2024fresco}, and RAVE \cite{kara2024rave}. For these baselines, we use the default settings provided in their official GitHub repositories. Since TokenFlow, FLATTEN, and RAVE are unable to edit long videos in a single inference, we segment the long videos for editing. Based on their computational resource usage, we edit 128, 32, and 16 frames at a time.

\subsection{Quantitative Analysis}

In Tab.\ref{tab:quantitative}, we first quantitatively compare the proposed AdaFlow with a set of the latest video editing methods \cite{yang2023rerender, geyer2023tokenflow, cong2023flatten, yang2024fresco, kara2024rave} on LongV-EVAL. In particular, we accomplish the long video editing of the compared methods in multiple inferences due to the limit of GPU memory. As can be seen, our AdaFlow achieves better performance than the compared methods in terms of video quality, object consistency, and semantic consistency. Although it is slightly inferior to FRESCO \cite{yang2024fresco} in frame quality, FRESCO has a large gap between the edited video and the source video, according to the visualization of Fig.\ref{fig:qualitative} (a). In addition to excellent editing quality, our AdaFlow not only enables the editing of longer videos but also achieves much higher efficiency through its innovative designs. As shown in the last column of Tab.\ref{tab:quantitative}, our method takes an average of 24 minutes to edit a video, while the baselines take at least 40 minutes, almost twice as long as ours.

In addition to the measurable metrics of LongV-EVAL, we also conduct a comprehensive user study to compare our AdaFlow with other methods in Tab.\ref{tab:user}. In practice, we invited 18 participants to choose their preferred videos edited by different methods based on two metrics, \emph{i.e.}, video quality and temporal consistency. We randomly selected 20 sets of video-text data for the user study. Each set contains 6 videos for comparison, so each participant needs to view 120 long videos and make 40 choices. The specific evaluation criteria are given in Appendix \ref{ap:user}. Considering the participants’ attention span, we believe this is an appropriate amount of data. As shown in Tab.\ref{tab:user}, it is evident that our method is the most favored in terms of two metrics. Overall, these results well validate the efficiency and effectiveness of our AdaFlow for long video editing.

\subsection{Qualitative Results}

To better evaluate the effectiveness of our AdaFlow, we visualize its key steps in Fig.\ref{fig:opening} and also compare its results with a set of the latest video editing methods in Fig.\ref{fig:qualitative} (a). As shown in Fig.\ref{fig:opening}, for a video approximately 1000 frames long, AdaFlow adaptively segments the video clips based on content, and then selects keyframes (Row 2) accurately and effectively perform text-guided keyframe translation. For instance, turn the girl playing with the tablet in the source video into Cinderella from a cartoon to get a surreal video (Row 3). The edit strictly follows the text prompt and maintains the consistency with the source video for the parts that do not require changing. Furthermore, our method can support video editing of up to ten thousand frames in a single inference while maintaining high editing quality and temporal consistency. More visualization can be found in Fig.\ref{fig:ap_vis} and Fig.\ref{fig:ap_10000} of Appendix \ref{ap:vis}. 

In terms of the compared methods, as shown in  Fig.\ref{fig:qualitative} (a), Rerender \cite{yang2023rerender} can sometimes over-edit or even fail, resulting in strange bright spots or objects that are not in the source video. FRESCO \cite{yang2024fresco} demonstrates better temporal consistency, but it always alters the background even though the prompt doesn't mention it. This case significantly hinders the controllability of video editing. The editing results of TokenFlow \cite{geyer2023tokenflow}, which also follows a two-step editing, are inferior in frame quality and temporal consistency when editing long videos. As marked by the red boxes, the editing also shows the lack of temporal consistency and defective editing quality by TokenFlow. It can be observed that the bird’s beak often changes in the first editing results, indicating temporal inconsistency. In the second editing result, both the face of the princess and the background appear blurred, indicating lower editing quality. Compared to TokenFlow and the other two baselines, our proposed AdaFlow can maintain consistency in long video editing tasks while achieving high-quality edits. Conclusively, these results show that our AdaFlow can not only achieve long video editing of more than 1$k$ frames in one inference but also can obtain better video quality and consistency than existing methods. We additionally compare our method with TokenFlow \cite{geyer2023tokenflow} more on its official examples, as shown in Fig.\ref{fig:ap_tokenflow} of Appendix \ref{ap:vis}.

In Fig.\ref{fig:qualitative} (b), we also ablate the effect of the \emph{Adaptive Keyframe Selection} (AKS) in AdaFlow. It can be seen that the example on the left figure shows a car quickly entering the video frame. With AKS, AdaFlow can automatically select more keyframes of this content, significantly improving image quality. The example on the right shows a dancing girl that is constantly moving. Since the uniform keyframe selection is difficult to deal with such a motion scene, the girl's face in the generated results is always blurred and distorted. Instead, AKS can automatically recognize such rapid changes and sample keyframes at this point, resulting in significantly better generation quality. Overall, these results confirm the effectiveness of our AdaFlow for editing videos with obvious variations.

\section{Conclusion}

In this paper, we present a novel and training-free method for high-quality long video editing, termed \emph{AdaFlow}, which can effectively edit more than 1$k$ video frames in one inference. By introducing the innovative designs of \emph{Adaptive Attention Slimming} and \emph{Adaptive Keyframe Selection}, AdaFlow significantly reduces computational resource consumption while enhancing the number of keyframes that can be edited simultaneously. We also build a new benchmark called \emph{LongV-EVAL} to complement the evaluation of text-driven long video editing. Extensive experiments are conducted and show that AdaFlow is more effective and efficient than the compared methods in long video editing.

\section*{Acknowledgements}

This work was supported by the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. U23A20383, No. U21A20472, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No. 2021J06003, No. 2022J06001).

% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.1in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{zhou2019plenty, luo2024moil, luo2024towards, zhangfast, zou2024towards}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/ap_data.pdf}}
\caption{Examples of results for dataset annotation. Each source video is accompanied by three different prompts that focus on three aspects: foreground, background, and style.}
\label{fig:ap_data}
\end{center}
\vskip -0.2in
\end{figure}

\section{Dataset Annotating Details}
\label{ap:dataset}
We collected 75 videos, each approximately one minute long with a frame rate of 20-30 fps, from \emph{https://mixkit.co/, https://www.pexels.com, and https://pixabay.com}. The video content spans various subjects, including people, animals, and landscapes. To annotate these data with high-quality editing prompts, we first input the video $V$ and prompt $P_1$ into Video-Llava \cite{lin2023video}, where $P_1$ is \emph{“Please add a caption to the video in great detail.”} This generates a detailed textual description $C$ of the video.

Next, we input prompt $P_2$ into GPT-4 \cite{achiam2023gpt}, where $P_2$ has three different forms to generate three distinct editing prompts for the same video. The forms of $P_2$ are as follows:
\begin{itemize}
    \item \emph{``I have a video caption: $C$. Imagine that you have modified the \textbf{main object} of the video content (such as color change, similar object replacement, etc.). After editing, add a concise one-sentence caption of the edited video (with emphasis on the edited part, no more than 15 words), not the original video content. The answer should contain only the caption, without any additional content.''}
    \item \emph{``I have a video caption: $C$. Imagine that you have modified the \textbf{background} of the video content (such as background tone replacement, similar background replacement, etc.). After editing, add a concise one-sentence caption of the edited video (with emphasis on the edited part, no more than 15 words), not the original video content. The answer should contain only the caption, without any additional content.''}
    \item \emph{``I have a video caption: $C$. Imagine that you have applied Van Gogh, Picasso, Da Vinci, Mondrian, watercolors, comics, or \textbf{drawings style transfer} to the video. After editing, add a concise one-sentence caption of the edited video (with emphasis on the style, no more than 15 words), not the original video content. The answer should contain only the caption, without any additional content.''}
\end{itemize}

This process eventually generates three editing prompts for each video, as shown in Fig.\ref{fig:ap_data}.

\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{fig/ap_vis.pdf}}
\caption{Additional Qualitative Results. Our method supports a wide variety of text-driven video edits and maintains high editing quality and temporal consistency even for videos exceeding a thousand frames.}
\label{fig:ap_vis}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{fig/ap_10000.pdf}}
\caption{Additional Qualitative Results. Our method can support processing videos up to 10k frames in a single inference while maintaining high editing quality and temporal consistency.}
\label{fig:ap_10000}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{fig/ap_tokenflow.pdf}}
\caption{Additional Qualitative Comparison. We compare with TokenFlow on the official examples used by TokenFlow and find that our method can better preserve details (fingers under the basketball) and more realistically preserve the background content (background behind the sculpture man).}
\label{fig:ap_tokenflow}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{fig/yt.pdf}}
\caption{y-t plot. We extracted a vertical column of pixels from the center of each video frame and then sequentially stitched these columns together from left to right to get the y-t plot. The blue lines in the figure indicate the points where the video is segmented.}
\label{fig:yt}
\end{center}
\vskip -0.2in
\end{figure}

\section{Additional Qualitative Results}
\label{ap:vis}
As shown in Fig.\ref{fig:ap_vis} and Fig.\ref{fig:ap_10000}, our method can edit over a thousand video frames (even 10k frames) on a single NVIDIA A800 (80GB), while maintaining temporal consistency and achieving high editing quality.

In addition, we also compare with TokenFlow \cite{geyer2023tokenflow} on the official examples used by TokenFlow, as shown in Fig.\ref{fig:ap_tokenflow}, and find that our method can better preserve details (fingers under the basketball), as well as more realistically preserve the background content (background behind the sculpture man).

\section{User Study Details}
\label{ap:user}
We randomly selected 20 video-text pairs from our dataset for a user study, comparing them with the five baselines mentioned in the main text. For each pair, 50 participants were asked to evaluate and select the best video from the six options based on the following criteria:
\begin{itemize}
    \item \textbf{Video Quality}: The edited video should appear realistic and not easily identifiable as AI-generated. Only the parts specified by the prompt should be edited, while the content not mentioned in the prompt should remain consistent with the source video.
    \item \textbf{Temporal Consistency}: The same object should remain consistent at any point in the long video, and the transitions between frames should be as smooth as in the source video.
\end{itemize}

\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{fig/kv.png}}
\caption{We retain only the tokens corresponding to the regions shown in the figure for $K$ and $V$ during the self-attention computation. In the scenario illustrated here, the eighth frame serves as the query. It can be observed that the content closer to the query frame is automatically retained more, while the content further away from the query frame is discarded more. This automatic selection can save substantial computational resources while maintaining the continuity and consistency of video generation.}
\label{fig:kv}
\end{center}
\vskip -0.2in
\end{figure}

\section{Visualization of Adaptive Attention Slimming}
\label{ap:pruning}
As shown in Fig.\ref{fig:kv}, the eighth frame serves as the \emph{query} in this attention operation. By employing our proposed method, a portion of the tokens can be automatically discarded to save computational resources. The content closer to the \emph{query} frame is retained more, while the content further away from the \emph{query} frame is discarded more. This is because, with a larger period, a significant amount of content dissimilar to the \emph{query} appears in the frames, and attending to this content does not contribute to the continuity and consistency of the video. Conversely, the content closer to the query is crucial for maintaining the smoothness of the video. Therefore, using our proposed method not only saves memory but also minimally impacts the quality of video generation.

\section{Visualization of Keyframe Selection}
\label{ap:keyframe}
To visualize the \emph{Adaptive Keyframe Selection}, we extracted a vertical column of pixels from the center of each video frame. We then sequentially stitched these columns together from left to right to create a y-t diagram, as shown in Fig.\ref{fig:yt}. The blue dashed lines in the figure indicate the points where we segmented the video. It can be observed that each segmentation point corresponds to a significant change in the video content. Moreover, the keyframes obtained from each segment always contain different content. This demonstrates the effectiveness of our method.

\section{Limitations}
\label{ap:limit}
Our method adopts the motion information from the source video as a reference to generate non-key frames. Therefore, our approach performs exceptionally well when the image structure remains unchanged. However, it often produces unsatisfactory results when changes in object shapes are required. Additionally, since our method is training-free and directly employs image editing techniques, it primarily addresses the issue of temporal consistency. Consequently, the editing capability of our method may be influenced by the performance of the image editing techniques used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
