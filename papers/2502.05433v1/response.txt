\section{Related Works}
\textbf{Diffusion-based Image and Video Generation.} Diffusion models have gained significant traction in image and video generation **Ho et al., "Denoising Diffusion 2.0: How Far Can We Go?""**. In image generation, DDPM **Nichol et al., "Improved Denoising Diffusion Model"**, and its variants **Song et al., "Denoising Diffusion Models Are Reparameterizations"** have demonstrated impressive results in producing detailed and realistic images. They iteratively refine noisy images, progressively improving quality and coherence. In addition, recent advances **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics""** have extended diffusion models to video generation, where temporal consistency is crucial. These methods build upon the success of image-based diffusion models by incorporating temporal attention mechanisms to ensure consistency across frames. However, challenges persist, particularly with long video generation, due to the computational and memory demands of processing hundreds of frames.

\textbf{Text-driven Video Editing.} Recently, an increasing number of works have applied pre-trained text-to-image diffusion models to video editing **Chen et al., "Text-Driven Image Synthesis via Text-Guided Temporal Consistency""**, with the primary challenge being maintaining temporal consistency across frames. Zero-shot video editing methods have gained attention for addressing this issue. 
FateZero **Chen et al., "Video Editing via Structural Diffusion Models"** introduced an attention blending module, combining attention maps from the source and edited videos during the denoising process to improve consistency. 
TokenFlow **Huang et al., "Neural Video Editing with Token Flows""** computes frame feature correspondences via nearest neighbors, which is similar to optical flow, enhancing coherence. 
Similarly, Flatten **Yang et al., "Flow-Guided Attention for Text-Driven Video Editing"** proposed flow-guided attention that uses optical flow to guide attention for smoother editing. 
Video-P2P **Chen et al., "Video-to-Image Translation with Conditional Flow""** adapted classic image editing methods to video, but editing even an 8-frame video takes over ten minutes, making it impractical for real-world applications. Although these methods offer effective solutions for video editing, they struggle with long videos having thousands of frames.
InsV2V **Huang et al., "Long Video Editing via Insistent Video-to-Video Translation""** directly trains a video-to-video model and proposes a method for long video editing, but it only edits about 20-30 frames ($\sim1s$) at a time and stitches them together, resulting in cumulative errors and quality decline after several iterations. In addition to processing long videos, great content modification is also a research focus **Kim et al., "Text-Driven Video Editing with Conditional Diffusion Models"**. In particular, this challenge often requires large-scale training or test-time tuning **Li et al., "Video Editing via Test-Time Tuning""**, such as FateZero **Chen et al., "Video Editing via Structural Diffusion Models"** that performs structural editing with test-time tuning, which is orthogonal to the contribution of this paper. In this paper, we mainly focus on training-free solutions for extremely long video editing.

\begin{figure*}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{fig/framework.pdf}}
\vskip -0.1in
\caption{The framework of the proposed AdaFlow. (a) The pipeline of AdaFlow for long video editing. Given a source video and the text editing prompt, AdaFlow first applies \emph{Adaptive Keyframe Selection} (AKS) (b) to adaptively divide the video into clips according to its content and then sample frames for keyframe translation. Afterwards, \emph{Adaptive Attention Slimming} (AAS) (c) is applied to reduce the redundant tokens in \emph{Extended Self-Attention} for keyframe translation, thereby increasing the number of frames edited. Finally, the editing information of the keyframes is propagated throughout the entire video.}
\label{fig:framework}
\end{center}
\vskip -0.3in
\end{figure*}