\section{Related Work}
\textbf{Document Understanding Models} Models like LayoutLMv3 \cite{layoutlm3} excel in parsing complex documents for tasks such as layout analysis and visual question answering. However, they rely heavily on pre-extracted text, images, and bounding boxes, forming a brittle pipeline that can be error-prone due to its dependence on external systems. SwinDocSegmenter \cite{swindocsegmenter} and specialized variants of YOLO \cite{yolov5} have been trained for document-specific detection tasks without requiring additional inputs. While they effectively detect objects within documents, they generally do not output any text associated with these objects, lacking integrated OCR capabilities.

\textbf{Object Detection in Documents} is crucial for identifying and localizing elements within documents, aiding tasks like OCR and determining reading order. Traditional models such as Faster R-CNN \cite{ren2016faster} and Mask R-CNN \cite{he2017mask} have been adapted for document analysis, effectively detecting and segmenting components like text blocks, images, and tables. Despite their success, these models typically do not provide textual content alongside the detected objects, limiting their usefulness for comprehensive document understanding.

\textbf{End-to-End OCR-Free Models} that do not depend on external OCR systems have gained attention. Donut \cite{donut} introduced a transformer-based encoder-decoder architecture pre-trained on general text documents. Building on this, Nougat \cite{Nougat} extended training to scientific documents, outputting structured markdown with \LaTeX\ tables and equations. GOT \cite{GOT} focused on enhancing the recognition of specialized documents containing molecular formulas, sheet music, and charts. Kosmos-2.5 \cite{Kosmos} incorporated both markdown and plain text data with bounding boxes, introducing a prompt structure that allows users to choose between different output formats. However, these models may require compromises in prompt structures or may not handle a wide variety of document layouts effectively. Our proposed model, \eclair, is specifically trained to handle a greater variety of document layouts without requiring compromises in the prompt structure.

\textbf{Multimodal Large Language Models} like QwenVL~\cite{Qwen-VL}, GPT-4O~\cite{openai2024gpt4o} and Claude~\cite{anthropic2024claude} have demonstrated impressive OCR and document understanding capabilities, including the extraction of complex equations and tables in structured formats. While powerful, these models are large and computationally expensive, making them impractical for scaling to millions of pages. In contrast, \eclair\ is a sub-1B parameter model optimized for inference speed with multi-token decoding.