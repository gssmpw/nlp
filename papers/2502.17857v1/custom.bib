% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{lee2023chain,
      title={Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models}, 
      author={Yoon Kyung Lee and Inju Lee and Minjung Shin and Seoyeon Bae and Sowon Hahn},
      year={2023},
      eprint={2311.04915},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}



@article{Girotra2023IdeasAD,
  title={Ideas are Dimes a Dozen: Large Language Models for Idea Generation in Innovation},
  author={Karan Girotra and Lennart Meincke and Christian Terwiesch and Karl T. Ulrich},
  journal={SSRN Electronic Journal},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:260467886}
}


@article{elliott2011empathy,
  title={Empathy.},
  author={Elliott, Robert and Bohart, Arthur C and Watson, Jeanne C and Greenberg, Leslie S},
  journal={Psychotherapy},
  volume={48},
  number={1},
  pages={43},
  year={2011},
  publisher={Educational Publishing Foundation}
}


@misc{welivita2020finegrained,
      title={Fine-grained Emotion and Intent Learning in Movie Dialogues}, 
      author={Anuradha Welivita and Yubo Xie and Pearl Pu},
      year={2020},
      eprint={2012.13624},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{goel2021emotion,
  title={Emotion-aware transformer encoder for empathetic dialogue generation},
  author={Goel, Raman and Susan, Seba and Vashisht, Sachin and Dhanda, Armaan},
  booktitle={2021 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}


@inproceedings{lee2022improving,
  title={Improving contextual coherence in variational personalized and empathetic dialogue agents},
  author={Lee, Jing Yang and Lee, Kong Aik and Gan, Woon Seng},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7052--7056},
  year={2022},
  organization={IEEE}
}


@inproceedings{10.1145/3411763.3451799,
    author = {Mauriello, Matthew Louis and Lincoln, Thierry and Hon, Grace and Simon, Dorien and Jurafsky, Dan and Paredes, Pablo},
    title = {SAD: A Stress Annotated Dataset for Recognizing Everyday Stressors in SMS-like Conversational Systems},
    year = {2021},
    isbn = {9781450380959},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3411763.3451799},
    doi = {10.1145/3411763.3451799},
    abstract = {There is limited infrastructure for providing stress management services to those in need. To address this problem, chatbots are viewed as a scalable solution. However, one limiting factor is having clear definitions and examples of daily stress on which to build models and methods for routing appropriate advice during conversations. We developed a dataset of 6850 SMS-like sentences that can be used to classify input using a scheme of 9 stressor categories derived from: stress management literature, live conversations from a prototype chatbot system, crowdsourcing, and targeted web scraping from an online repository. In addition to releasing this dataset, we show results that are promising for classification purposes. Our contributions include: (i) a categorization of daily stressors, (ii) a dataset of SMS-like sentences, (iii) an analysis of this dataset that demonstrates its potential efficacy, and (iv) a demonstration of its utility for implementation via a simulation of model response times.},
    booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
    articleno = {399},
    numpages = {7},
    keywords = {Stressors, Stress Management, Datasets, Daily Stress, Conversational Agents, Classification},
    location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
    series = {CHI EA '21}
}


@inproceedings{chen2024detecting,
    title={Detecting Empathy in Speech},
    author={Chen, Run and Chen, Haozhe and Kulkarni, Anushka and Lin, Eleanor and Pang, Linda and Tadimeti, Divya and Shin, Jun and Hirschberg, Julia},
   booktitle={Proc. INTERSPEECH 2024},
   year ={2024},
   pages={},
   note = {},
   doi={},
   issn={},
}

@InProceedings{C2,
  author = 	 "Jones, C.D. and Smith, A.B. and Roberts, E.F.",
  title =        "Article Title",
  booktitle =        "Proceedings Title",
  organization = "IEEE",
  year = 	 "2003",
  volume = 	 "II",
  pages = 	 "803-806"
}

%jh: probably just take this one out -- not published
@article{speechlab,
author = {Hirschberg, Julia},
title = {Conveying Empathy in Spoken Language},
year = {2021},
publisher = {Amazon Gift},
}



@article{10.1145/365153.365168,
author = {Weizenbaum, Joseph},
title = {ELIZA—a Computer Program for the Study of Natural Language Communication between Man and Machine},
year = {1966},
issue_date = {Jan. 1966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/365153.365168},
doi = {10.1145/365153.365168},
journal = {Commun. ACM},
month = {jan},
pages = {36–45},
numpages = {10}
}

@article{10.5555/567363.567368,
author = {Cassell, Justine},
title = {Embodied Conversational Agents: Representation and Intelligence in User Interfaces},
year = {2001},
issue_date = {Winter 2001},
publisher = {American Association for Artificial Intelligence},
address = {USA},
volume = {22},
number = {4},
issn = {0738-4602},
journal = {AI Mag.},
month = {oct},
pages = {67–83},
numpages = {17}
}

@article{10.1145/1067860.1067867,
author = {Bickmore, Timothy W. and Picard, Rosalind W.},
title = {Establishing and Maintaining Long-Term Human-Computer Relationships},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/1067860.1067867},
doi = {10.1145/1067860.1067867},
abstract = {This research investigates the meaning of “human-computer relationship” and presents techniques for constructing, maintaining, and evaluating such relationships, based on research in social psychology, sociolinguistics, communication and other social sciences. Contexts in which relationships are particularly important are described, together with specific benefits (like trust) and task outcomes (like improved learning) known to be associated with relationship quality. We especially consider the problem of designing for long-term interaction, and define relational agents as computational artifacts designed to establish and maintain long-term social-emotional relationships with their users. We construct the first such agent, and evaluate it in a controlled experiment with 101 users who were asked to interact daily with an exercise adoption system for a month. Compared to an equivalent task-oriented agent without any deliberate social-emotional or relationship-building skills, the relational agent was respected more, liked more, and trusted more, even after four weeks of interaction. Additionally, users expressed a significantly greater desire to continue working with the relational agent after the termination of the study. We conclude by discussing future directions for this research together with ethical and other ramifications of this work for HCI designers.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {jun},
pages = {293–327},
numpages = {35},
keywords = {embodied conversational agent, relational agent, social interface, Human-computer interaction}
}
@article{10.1080/08839514.2010.492259,
author = {Bickmore, Timothy and Schulman, Daniel and Yin, Langxuan},
year = {2010},
month = {07},
pages = {648-666},
title = {Maintaining Engagement in Long-term Interventions with Relational Agents},
volume = {24},
journal = {Applied artificial intelligence : AAI},
doi = {10.1080/08839514.2010.492259}
}

@inproceedings{10.1007/978-3-540-74997-4_12,
author="Gratch, Jonathan and Wang, Ning and Gerten, Jillian and Fast, Edward and Duffy, Robin",
editor="Pelachaud, Catherine and Martin, Jean-Claude and Andr{\'e}, Elisabeth and Chollet, G{\'e}rard and Karpouzis, Kostas and Pel{\'e}, Danielle",
title="Creating Rapport with Virtual Agents",
booktitle="Intelligent Virtual Agents",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="125--138",
abstract="Recent research has established the potential for virtual characters to establish rapport with humans through simple contingent nonverbal behaviors. We hypothesized that the contingency, not just the frequency of positive feedback is crucial when it comes to creating rapport. The primary goal in this study was evaluative: can an agent generate behavior that engenders feelings of rapport in human speakers and how does this compare to human generated feedback? A secondary goal was to answer the question: Is contingency (as opposed to frequency) of agent feedback crucial when it comes to creating feelings of rapport? Results suggest that contingency matters when it comes to creating rapport and that agent generated behavior was as good as human listeners in creating rapport. A ``virtual human listener'' condition performed worse than other conditions.",
isbn="978-3-540-74997-4"
}

@article{10.1007/978-3-642-23974-8_8,
author="Huang, Lixing
and Morency, Louis-Philippe
and Gratch, Jonathan",
editor="Vilhj{\'a}lmsson, Hannes H{\"o}gni
and Kopp, Stefan
and Marsella, Stacy
and Th{\'o}risson, Kristinn R.",
title="Virtual Rapport 2.0",
journal="Intelligent Virtual Agents",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="68--79",
abstract="Rapport, the feeling of being ``in sync'' with your conversational partners, is argued to underlie many desirable social effects. By generating proper verbal and nonverbal behaviors, virtual humans have been seen to create rapport during interactions with human users. In this paper, we introduce our approach to creating rapport following Tickle-Degnen and Rosenberg's threefactor (positivity, mutual attention and coordination) theory of rapport. By comparing with a previously published virtual agent, the Rapport Agent, we show that our virtual human predicts the timing of backchannel feedback and end-of-turn more precisely, performs more natural behaviors and, thereby creates much stronger feelings of rapport between users and virtual agents.",
isbn="978-3-642-23974-8"
}
@inproceedings{10.5555/1558109.1558314,
author = {Niewiadomski, Radoslaw and Bevacqua, Elisabetta and Mancini, Maurizio and Pelachaud, Catherine},
title = {Greta: An Interactive Expressive ECA System},
year = {2009},
isbn = {9780981738178},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We have developed a general purpose use and modular architecture of an Embodied Conversational Agent (ECA) called Greta. Our 3D agent is able to communicate using verbal and nonverbal channels like gaze, head and torso movements, facial expressions and gestures. It follows the SAIBA framework [10] and the MPEG4 [6] standards. Our system is optimized to be used in interactive applications.},
booktitle = {Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {1399–1400},
numpages = {2},
keywords = {ECA architecture, FML, BML, multimodal behavior, SAIBA},
location = {Budapest, Hungary},
series = {AAMAS '09}
}

@inproceedings{10.5555/1402383.1402401,
author = {Ochs, Magalie and Pelachaud, Catherine and Sadek, David},
title = {An Empathic Virtual Dialog Agent to Improve Human-Machine Interaction},
year = {2008},
isbn = {9780981738109},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent research has shown that virtual agents expressing empathic emotions toward users have the potentiality to enhance human-machine interaction. To identify under which circumstances a virtual agent should express empathic emotions, we have analyzed real human-machine dialog situations that have led users to express emotion. The results of this empirical study have been combined with theoretical descriptions of emotions to construct a model of empathic emotions. Based on this model, a module of emotions has been implemented as a plug-in for JSA agents. It determines the empathic emotions (their types and intensity) of such agents in real time. It has been used to develop a demonstrator where users can interact with an empathic dialog agent to obtain information on their emails. An evaluation of this agent has enabled us to both validate the proposed model of empathic emotions and highlight the positive user's perception of the virtual agent.},
booktitle = {Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {89–96},
numpages = {8},
location = {Estoril, Portugal},
series = {AAMAS '08}
}

@inproceedings{10.1007/978-3-319-47665-0_20,
author = {Zhao, Ran and Sinha, Tanmay and Black, Alan and Cassell, Justine},
year = {2016},
title = {Socially-Aware Virtual Agents: Automatically Assessing Dyadic Rapport from Temporal Patterns of Behavior},
booktitle={Intelligent Virtual Agents}
}

@inproceedings{10.5555/2615731.2617415,
author = {DeVault, David and Artstein, Ron and Benn, Grace and Dey, Teresa and Fast, Ed and Gainer, Alesia and Georgila, Kallirroi and Gratch, Jon and Hartholt, Arno and Lhommet, Margaux and Lucas, Gale and Marsella, Stacy and Morbini, Fabrizio and Nazarian, Angela and Scherer, Stefan and Stratou, Giota and Suri, Apar and Traum, David and Wood, Rachel and Xu, Yuyu and Rizzo, Albert and Morency, Louis-Philippe},
title = {SimSensei Kiosk: A Virtual Human Interviewer for Healthcare Decision Support},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We present SimSensei Kiosk, an implemented virtual human interviewer designed to create an engaging face-to-face interaction where the user feels comfortable talking and sharing information. SimSensei Kiosk is also designed to create interactional situations favorable to the automatic assessment of distress indicators, defined as verbal and nonverbal behaviors correlated with depression, anxiety or post-traumatic stress disorder (PTSD). In this paper, we summarize the design methodology, performed over the past two years, which is based on three main development cycles: (1) analysis of face-to-face human interactions to identify potential distress indicators, dialogue policies and virtual human gestures, (2) development and analysis of a Wizard-of-Oz prototype system where two human operators were deciding the spoken and gestural responses, and (3) development of a fully automatic virtual interviewer able to engage users in 15-25 minute interactions. We show the potential of our fully automatic virtual human interviewer in a user study, and situate its performance in relation to the Wizard-of-Oz prototype.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1061–1068},
numpages = {8},
keywords = {virtual humans, nonverbal behavior, dialogue systems},
location = {Paris, France},
series = {AAMAS '14}
}
@INPROCEEDINGS{9473685,
  author={Lucas, Gale M. and Boberg, Jill and Traum, David and Artstein, Ron and Gratch, Jonathan and Gainer, Alesia and Johnson, Emmanuel and Leuski, Anton and Nakano, Mikio},
  booktitle={2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Getting to Know Each Other: The Role of Social Dialogue in Recovery from Errors in Social Robots}, 
  year={2018},
  volume={},
  number={},
  pages={344-351},
  doi={}}


@inproceedings{Winata2017NoraTE,
  title={Nora the Empathetic Psychologist},
  author={Genta Indra Winata and Onno P. Kampman and Yang Yang and Anik Dey and Pascale Fung},
  booktitle={INTERSPEECH},
  year={2017}
}


@article{Fitzpatrick2017DeliveringCB,
  title={Delivering Cognitive Behavior Therapy to Young Adults With Symptoms of Depression and Anxiety Using a Fully Automated Conversational Agent (Woebot): A Randomized Controlled Trial},
  author={Kathleen Kara Fitzpatrick and Alison M Darcy and Molly Vierhile},
  journal={JMIR Mental Health},
  year={2017},
  volume={4}
}

@inproceedings{ghandeharioun2019emma,
author = {Ghandeharioun, Asma and McDuff, Daniel and Czerwinski, Mary and Rowan, Kael},
title = {EMMA: An Emotion-Aware Wellbeing Chatbot},
booktitle = {International Conference on Affective Computing and Intelligent Interaction},
year = {2019},
month = {September},
abstract = {The delivery of mental health interventions via ubiquitous devices has shown much promise. A conversational chatbot is a promising oracle for delivering appropriate just-in-time interventions. However, designing emotionally-aware agents, specially in this context, is under-explored. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this paper, we present the design and evaluation of EMMA (EMotion-Aware mHealth Agent) through a two-week long human-subject experiment with N=39 participants. EMMA provides emotionally appropriate micro-activities in an empathetic manner. We show that the system can be extended to detect a user's mood purely from smartphone sensor data. Our results show that our personalized machine learning model was perceived as likable via self-reports of emotion from users. Finally, we provide a set of guidelines for the design of emotion-aware bots for mHealth.},
url = {https://www.microsoft.com/en-us/research/publication/emma-an-emotion-aware-wellbeing-chatbot/},
}

@article{10.1016/j.csl.2017.12.003,
author = {Alam, Firoj and Danieli, Morena and Riccardi, Giuseppe},
title = {Annotating and Modeling Empathy in Spoken Conversations},
year = {2018},
issue_date = {July 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {50},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2017.12.003},
doi = {10.1016/j.csl.2017.12.003},
abstract = {We address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from spoken conversations.We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions.We investigated features derived from the lexical and acoustic spaces.We evaluated automatic classification system on call center conversations, where it showed significantly better performance than the baseline. Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from humanhuman dyadic spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from a high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline.},
journal = {Comput. Speech Lang.},
month = {jul},
pages = {40–61},
numpages = {22},
keywords = {Call center, Affective scene, Emotion, HumanHuman conversation, Behavior analysis, Spoken conversation, Affect, Empathy}
}

@article{Lotfian_2019_3,
	author = {R. Lotfian and C. Busso},
	title = {Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech From Existing Podcast Recordings},
	journal = {IEEE Transactions on Affective Computing},
	volume = {10},
	number = {4},
	year = {2019},
	pages = {471-483},
	month = {October-December},
	doi={10.1109/TAFFC.2017.2736999},
}

@article{parselmouth,
    author = "Yannick Jadoul and Bill Thompson and Bart de Boer",
    title = "Introducing {P}arselmouth: A {P}ython interface to {P}raat",
    journal = "Journal of Phonetics",
    volume = "71",
    pages = "1--15",
    year = "2018",
    doi = "https://doi.org/10.1016/j.wocn.2018.07.001"
}

@inproceedings{10.1145/1873951.1874246,
author = {Eyben, Florian and W\"{o}llmer, Martin and Schuller, Bj\"{o}rn},
title = {Opensmile: The Munich Versatile and Fast Open-Source Audio Feature Extractor},
booktitle={Interspeech 2010},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874246},
doi = {10.1145/1873951.1874246},
abstract = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1459–1462},
numpages = {4},
keywords = {signal processing, speech, emotion, music, audio feature extraction, statistical functionals},
location = {Firenze, Italy},
series = {MM '10}
}

@book{pennebaker01,
  added-at = {2006-03-08T19:52:29.000+0100},
  address = {Mahwah, NJ},
  author = {Pennebaker, James W. and Francis, Martha E. and Booth, Roger J.},
  biburl = {https://www.bibsonomy.org/bibtex/258596235376f026ca492242dcec89033/vanatteveldt},
  interhash = {25dedc21b73ca5887fd00d00697addc4},
  intrahash = {58596235376f026ca492242dcec89033},
  keywords = {imported},
  publisher = {Lawerence Erlbaum Associates},
  timestamp = {2006-03-08T19:52:29.000+0100},
  title = {Linguistic Inquiry and Word Count},
  year = 2001
}




@article{Davis80-COP,
	Author = {Steven B. Davis and Paul Mermelstein},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Number = {4},
	Pages = {357--366},
	Title = {Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences},
	Volume = {28},
  Month = aug,
	Year = {1980}}

@article{Rabiner89-ATO,
	Author = {Lawrence R. Rabiner},
	Journal = {Proceedings of the IEEE},
	Number = {2},
	Pages = {257--286},
	Title = {A Tutorial on Hidden {Markov} Models and Selected Applications in Speech Recognition},
	Volume = {77},
  Month = feb,
	Year = {1989}}

@book{Hastie09-TEO,
	Address = {New York},
	Author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	Publisher = {Springer},
	Title = {The Elements of Statistical Learning -- Data Mining, Inference, and Prediction},
	Year = {2009}}

@inproceedings{Smith22-XXX,
	Author = {Jane Smith and Firstname2 Lastname2 and Firstname3 Lastname3},
	Pages = {100--104},
	Title = {A really good paper about {D}ynamic {T}ime {W}arping},
 	Booktitle = {Proc. {INTERSPEECH} 2022 -- 23\textsuperscript{rd} Annual Conference of the International Speech Communication Association},
    Address = {{Incheon, Korea}},
    Month = {{Sep.}},
	Year = {2022}}

 % use the Crossref field to copy any unspecified fields (such as booktitle) from another entry, 
@inproceedings{Jones22-XXX,
	Author = {Robert Jones and Firstname2 Lastname2 and Firstname3 Lastname3},
	Crossref = {Smith22-XXX},
	Pages = {105--109},
	Title = {An excellent paper introducing the {ABC} toolkit}}

@inproceedings{moore19_interspeech,
  author={Roger K. Moore and Lucy Skidmore},
  title={On the Use/Misuse of the Term {`Phoneme'}},
    Address = {{Graz, Austria}},
    Month = {{Sep.}},
  year=2019,
 	Booktitle = {Proc. {INTERSPEECH} 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association},
  pages={2340--2344},
  doi={10.21437/Interspeech.2019-2711}
}

@article{msp,
	author = {R. Lotfian and C. Busso},
	title = {Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech From Existing Podcast Recordings},
	journal = {IEEE Transactions on Affective Computing},
	volume = {10},
	number = {4},
	year = {2019},
	pages = {471-483},
	month = {October-December},
	doi={10.1109/TAFFC.2017.2736999},
}

@misc{praat,
    author = "Paul Boersma and David Weenink",
    title = "{P}raat: doing phonetics by computer [{C}omputer program]",
    howpublished = "Version 6.2.14, retrieved 24 May 2022 \url{http://www.praat.org/}",
    year = "1992-2022"
}

@inproceedings{eyben2010opensmile,
  title={Opensmile: the munich versatile and fast open-source audio feature extractor},
  author={Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn},
  booktitle={Proceedings of the 18th ACM international conference on Multimedia},
  pages={1459--1462},
  year={2010}
}
@inproceedings{schuller2009interspeech,
  title={The interspeech 2009 emotion challenge},
  author={Schuller, Bj{\"o}rn and Steidl, Stefan and Batliner, Anton},
  booktitle={Interspeech 2009},
  year={2009}
}

@manual{liwc2015,
author = {Pennebaker, James W and Booth, Roger and Boyd, Ryan L and Francis, Martha},
year = {2015},
month = sep,
title = {Linguistic Inquiry and Word Count: LIWC2015},
address = {Austin, TX},
institution = {Pennebaker Conglomerates (www.LIWC.net)},
}

@inproceedings{speciteller,
  title={Fast and accurate prediction of sentence specificity},
  author={Li, Junyi and Nenkova, Ani},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={29},
  number={1},
  year={2015}
}

@techreport{readability,
  title={Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel},
  author={Kincaid, J Peter and Fishburne Jr, Robert P and Rogers, Richard L and Chissom, Brad S},
  year={1975},
  institution={Naval Technical Training Command Millington TN Research Branch}
}

@article{dale1948formula,
  title={A formula for predicting readability: Instructions},
  author={Dale, Edgar and Chall, Jeanne S},
  journal={Educational research bulletin},
  pages={37--54},
  year={1948},
  publisher={JSTOR}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{fuller2021conceptualizing,
  title={Conceptualizing empathy competence: a professional communication perspective},
  author={Fuller, Melissa and Kamans, Elanor and van Vuuren, Mark and Wolfensberger, Marca and de Jong, Menno DT},
  journal={Journal of business and technical communication},
  volume={35},
  number={3},
  pages={333--368},
  year={2021},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{jani2012role,
  title={The role of empathy in therapy and the physician-patient relationship},
  author={Jani, Bhautesh Dinesh and Blane, David N and Mercer, Stewart W},
  journal={Complementary Medicine Research},
  volume={19},
  number={5},
  pages={252--257},
  year={2012},
  publisher={Karger Publishers}
}


@article{goetz2010compassion,
  title={Compassion: an evolutionary analysis and empirical review.},
  author={Goetz, Jennifer L and Keltner, Dacher and Simon-Thomas, Emiliana},
  journal={Psychological bulletin},
  volume={136},
  number={3},
  pages={351},
  year={2010},
  publisher={American Psychological Association}
}

@article{healey2018cognitive,
  title={Cognitive and affective perspective-taking: evidence for shared and dissociable anatomical substrates},
  author={Healey, Meghan L and Grossman, Murray},
  journal={Frontiers in neurology},
  volume={9},
  pages={491},
  year={2018},
  publisher={Frontiers Media SA}
}

@book{baumeister2007encyclopedia,
  title={Encyclopedia of social psychology},
  author={Baumeister, Roy F and Vohs, Kathleen D.},
  volume={1},
  year={2007},
  publisher={Sage}
}

@article{prince1982hedging,
  title={On hedging in physician-physician discourse},
  author={Prince, Ellen F and Frader, Joel and Bosk, Charles and others},
  journal={Linguistics and the Professions},
  volume={8},
  number={1},
  pages={83--97},
  year={1982},
  publisher={Ablex Norwood, NJ}
}

@inproceedings{prokofieva2014hedging,
  title={Hedging and speaker commitment},
  author={Prokofieva, Anna and Hirschberg, Julia},
  booktitle={5th Intl. Workshop on Emotion, Social Signals, Sentiment \& Linked Open Data, Reykjavik, Iceland},
  year={2014}
}

@inproceedings{tao22_interspeech,
  author={Dehua Tao and Tan Lee and Harold Chui and Sarah Luk},
  title={{Characterizing Therapist's Speaking Style in Relation to Empathy in Psychotherapy}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={2003--2007},
  doi={10.21437/Interspeech.2022-10416}
}
@inproceedings{lee22f_interspeech,
  author={Jonathan Him Nok Lee and Dehua Tao and Harold Chui and Tan Lee and Sarah Luk and Nicolette Wing Tung Lee and Koonkan Fung},
  title={{Durational Patterning at Discourse Boundaries in Relation to Therapist Empathy in Psychotherapy}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={5248--5252},
  doi={10.21437/Interspeech.2022-722}
}

@inproceedings{saito22_interspeech,
  author={Yuki Saito and Yuto Nishimura and Shinnosuke Takamichi and Kentaro Tachibana and Hiroshi Saruwatari},
  title={{STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={5155--5159},
  doi={10.21437/Interspeech.2022-300}
}

@inproceedings{xiao14_interspeech,
  author={Bo Xiao and Daniel Bone and Maarten Van Segbroeck and Zac E. Imel and David C. Atkins and Panayiotis G. Georgiou and Shrikanth S. Narayanan},
  title={{Modeling therapist empathy through prosody in drug addiction counseling}},
  year=2014,
  booktitle={Proc. Interspeech 2014},
  pages={213--217},
  doi={10.21437/Interspeech.2014-55}
}

@article{brysbaert2014concreteness,
  title={Concreteness ratings for 40 thousand generally known English word lemmas},
  author={Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  journal={Behavior research methods},
  volume={46},
  pages={904--911},
  year={2014},
  publisher={Springer}
}

@article{mccarthy2010mtld,
  title={MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment},
  author={McCarthy, Philip M and Jarvis, Scott},
  journal={Behavior research methods},
  volume={42},
  number={2},
  pages={381--392},
  year={2010},
  publisher={Springer}
}

@inproceedings{10.1145/3577190.3614105,
author = {Tran, Trang and Yin, Yufeng and Tavabi, Leili and Delacruz, Joannalyn and Borsari, Brian and Woolley, Joshua D and Scherer, Stefan and Soleymani, Mohammad},
title = {Multimodal Analysis and Assessment of Therapist Empathy in Motivational Interviews},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614105},
doi = {10.1145/3577190.3614105},
abstract = {The quality and effectiveness of psychotherapy sessions are highly influenced by the therapists’ ability to meaningfully connect with clients. Automated assessment of therapist empathy provides cost-effective and systematic means of assessing the quality of therapy sessions. In this work, we propose to assess therapist empathy using multimodal behavioral data, i.e.&nbsp;spoken language (text) and audio in real-world motivational interviewing (MI) sessions for alcohol abuse intervention. We first study each modality (text vs. audio) individually and then evaluate a multimodal approach using different fusion strategies for automated recognition of empathy levels (high vs. low). Leveraging recent pre-trained models both for text (DistilRoBERTa) and speech (HuBERT) as strong unimodal baselines, we obtain consistent 2-3 point improvements in F1 scores with early and late fusion, and the highest absolute improvement of 6–12 points over unimodal baselines. Our models obtain F1 scores of 68\% when only looking at an early segment of the sessions and up to 72\% in a therapist-dependent setting. In addition, our results show that a relatively small portion of sessions, specifically the second quartile, is most important in empathy prediction, outperforming predictions on later segments and on the full sessions. Our analyses in late fusion results show that fusion models rely more on the audio modality in limited-data settings, such as in individual quartiles and when using only therapist turns. Further, we observe the highest misclassification rates for parts of the sessions with MI inconsistent utterances (20\% misclassified by all models), likely due to the complex nature of these types of intents in relation to perceived empathy.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {406–415},
numpages = {10},
keywords = {Motivational interview, empathy, language, multimodal learning, speech},
location = {<conf-loc>, <city>Paris</city>, <country>France</country>, </conf-loc>},
series = {ICMI '23}
}

@inproceedings{nishimura22_interspeech,
  author={Yuto Nishimura and Yuki Saito and Shinnosuke Takamichi and Kentaro Tachibana and Hiroshi Saruwatari},
  title={{Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={3373--3377},
  doi={10.21437/Interspeech.2022-403},
  issn={2308-457X}
}

@inproceedings{gibson16_interspeech,
  author={James Gibson and Doğan Can and Bo Xiao and Zac E. Imel and David C. Atkins and Panayiotis Georgiou and Shrikanth S. Narayanan},
  title={{A Deep Learning Approach to Modeling Empathy in Addiction Counseling}},
  year=2016,
  booktitle={Proc. Interspeech 2016},
  pages={1447--1451},
  doi={10.21437/Interspeech.2016-554},
  issn={2308-457X}
}

@article{moss2023ethical,
  title={Is it ethical to use Mechanical Turk for behavioral research? Relevant data from a representative survey of MTurk participants and wages},
  author={Moss, Aaron J and Rosenzweig, Cheskie and Robinson, Jonathan and Jaffe, Shalom N and Litman, Leib},
  journal={Behavior Research Methods},
  volume={55},
  number={8},
  pages={4048--4067},
  year={2023},
  publisher={Springer}
}

@article{webb2022too,
  title={Too good to be true: Bots and bad data from Mechanical Turk},
  author={Webb, Margaret A and Tangney, June P},
  journal={Perspectives on Psychological Science},
  pages={17456916221120027},
  year={2022},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}