\section{Introduction}\label{sec:intro}
Constrained decoding guides the output of Large Language Models (LLMs) by greedily enforcing user-given constraints in highly structured settings.
%
\textit{Grammar-constrained decoding} (GCD)~\cite{geng2024grammarconstrained}  refers to the specific case where the constraint is given as a formal grammar that the LLM's output must conform to.
%
This is done by applying parsing algorithms to build an automaton that interfaces with the LLM's decoding algorithm to mask away \textit{all tokens that will provably lead to outputs not in the grammar}.
For example, GCD can be used to ensure that an LLM generates programs that only use a specific set of function names.

Parsing algorithms for Context-Free Grammars (CFG) achieve efficiency by processing input strings in two phases. Terminals---e.g., variable names or string literals---are recognized by a lexer in a preprocessing phase, while the grammatical structure of how terminals can be arranged---e.g., that the body of a loop should be a sequence of statements---is enforced by an automaton operating over lexed terminals.
% 
A key challenge with implementing GCD algorithms is that \textit{the tokens used by subword tokenizers in LLMs do not align with the terminals used by parsers}.

Because of this misalignment, GCD approaches either incur high \textit{online token-masking overhead (>1 second per token)}, or high \textit{offline preprocessing costs (>30 minutes)} to precompute a lookup table that relates LLM tokens to terminals \cite{beurer2024domino, ugare2024syncode}. Thus, existing GCD algorithms are impractical for domains where the constraining grammar frequently changes.
% 
Examples of such domains include \textit{program synthesis}~\cite{alur2019syguscomp}, where a grammar is provided for every program one may try to synthesize, and \textit{grammar prompting} \cite{wang2024grammar}, where  grammars are predicted from a given prompt to then guide the LLM towards a particular output structure. 

In this paper, we introduce a new approach for grammar-constrained decoding that is both \emph{flexible}---i.e., handles diverse grammars without incurring prohibitive offline preprocessing costs---and \emph{efficient}---i.e., maintains state-of-the-art efficiency in online token masking.
%
The key innovation is a combined analysis of the LLM token vocabulary and set of CFG terminals. 
% 
The analysis efficiently precomputes a lexer-state-dependent mapping between sequences of CFG tokens and individual LLM tokens. 
This precomputation allows the decoder to efficiently identify valid LLM tokens at decoding time while avoiding wasteful processing of token combinations that are not \emph{realizable} within the given LLM vocabulary.


% \loris{I think next part mixes related work and what you are doing.
% % 
% It should sound more like.
% Our algorithm proceeds in N steps. First it... Second...
% % 
% The following should go in related.
% This approach combines two idea from existing works: fusing lexing and parsing~ CITE and using PDAs over bytes CITE. However, by doing X, it overcomes both of the limitations of using these two approaches in isolation.
% }
% 


We implement our approach in a tool, \name, which compares favorably to existing GCD approaches.
\name achieves an average \nx speedup in offline preprocessing compared to related approaches~\cite{ugare2024syncode} while maintaining state-of-the-art online masking efficiency (5-32ms per token).
% 
Furthermore, our evaluation reveals that existing GCD implementations contain soundness bugs (e.g., they mask tokens that should not be masked or do not terminate when preprocessing simple grammars), whereas our approach lends itself to an elegant implementation consisting of simple modules.
% (we describe all modules to make our approach reproducible).

This paper makes two contributions:
\begin{itemize}
\item A flexible (low offline overhead) and efficient (low online token-masking overhead) algorithm for grammar-constrained decoding~(\autoref{sec:offline}-\ref{sec:online}).
\item A tool implementing our algorithm, called \name, that is up to \nx faster than other tools in offline preprocessing while retaining low online token-masking overhead (\autoref{sec:eval}).
\end{itemize}
% }

% \kh{TODO: A few sentences about performance improvement, from experiment data.}


