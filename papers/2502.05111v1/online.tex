\section{Online Token Masking}
\label{sec:online}

With a preprocessed inverse token spanner table $\inversetable$, along with the table of always-accepted tokens $A$ and context-dependent sequences $D$, we are now ready to describe the online part of our grammar-constrained decoder (\autoref{alg:token-mask}).

At each decoding step, \autoref{alg:token-mask} analyzes the lexer state $q^\automaton$, parser state $q^\pushdown$, and the current stack configuration $\gamma$ to produce the set of exactly all LLM tokens $\vocab_{\textit{allowed}}$ that  can lead to a sequence accepted by the the grammar.

% Although in principle one could compute at decoding time the set of possible LLM tokens without ever consulting the set of always accepted tokens $A$, this operation would be inefficient due to the following reason.
% 
Although we construct the combined PDA $\pushdown \circ \transducer_{\realizable{\vocab}{\automaton}}$ for preprocessing what terminal sequences a token can result in, using the same automaton for checking at decoding time whether LLM tokens can lead to valid parsing sequences would be inefficient. 
PDAs cannot always be determinized and one would have to compute reachability for nodes in 
$\pushdown \circ \transducer_{\realizable{\vocab}{\automaton}}$ under different stack configurations to determine valid language token sequences. 
% 
This computation would effectively involve testing all sequences in $\realizable{\vocab}{\automaton}$ at decoding time, making the preprocessing of the parser meaningless.
% 
Therefore, \autoref{alg:token-mask} uses the set of always-accepted tokens $A$ whenever possible to avoid traversing the PDA and only analyzes what terminal the PDA can accept at decoding time for context-dependent sequences described by $D$.
The set of next legal tokens is then obtained by looking up, for each sequence of terminals accepted by the grammar, the set of tokens that can lead to that sequence. This is done by consulting the inverse token spanner table $\inversetable$.
% \loris{say exactly what you do, also I feel this could be preprocessed too if one wanted. It's really just dynamic prog of possible stack sequnces. Maybe future work}
% \kh{You are correct, I exactly have thought about similar. I see }
% \loris{no need to do now! You can think about it on Tuesday :)} 

% \loris{ideally show example? Maybe provide a simple case with open/closed parens and explain earlier that this is context dep}

\begin{algorithm}[!t]
\caption{\tname{ComputeValidTokenMask}}
\label{alg:token-mask}
\begin{algorithmic}
    \STATE {\bfseries Input:}  PDA $\pushdown$, FSA $\automaton$, Inverse token spanner table $\inversetable$ \\
    Always-accepted token table $A$,\\
    Context-dependent sequence table $D$, \\
    Lexer state $q^\automaton$, Parser state $q^\pushdown$, Stack configuration $\gamma$
    \STATE {\bfseries Output:} Set of allowed tokens $\vocab_{\textit{allowed}}$
    \STATE $\vocab_{\textit{allowed}} := A(q^\automaton, q^\pushdown)$
    \FOR{$\alpha \in D(q^\automaton, q^\pushdown)$}
        \IF{$\alpha$ is accepted by $\pushdown$ in state $q^\pushdown$ with stack $\gamma$}
            \STATE $\vocab_{\textit{allowed}} := \vocab_{\textit{allowed}} \,\cup\, \inversetable(q^\automaton, \alpha)$
        \ENDIF
    \ENDFOR
    \RETURN $\vocab_{\textit{allowed}}$
\end{algorithmic}
\end{algorithm}

% \paragraph{$\epsilon$-input Transitions}

% While lexing transducers operate without $\epsilon$-input transitions, PDAs can incorporate $\epsilon$-input transitions. 
% Even deterministic PDAs---including widely used LR or LALR parsers \cite{knuth1965translation}---eliminate nondeterminism on input symbols but allow $\epsilon$-transitions guided by stack symbols, such as reducing a grammar rule during parsing.
% % For example, a reduce action in an LR parser corresponds to an $\epsilon$-transition that modifies the stack without advancing the input.

% %In such case, $\epsilon$-filter can still be used for efficient composition of them, 
% The existence of $\epsilon$-input transition complicates determining the next available token.
% Before consuming a new input symbol, the system must account for all possible $\epsilon$-transition paths the PDA might traverse. 
% This requires enumerating all reachable states and stack configurations through $\epsilon$-transition, and analyzing the subsequent input-dependent transitions from each of these states. 

% \begin{definition}[$\epsilon$-closure]
% For a PDA configuration $(q, \gamma) \in Q \times \Pi^\ast$, its $\epsilon$-closure, denoted as $\epscl(q, \gamma)$, is the minimal set satisfying:
% \rone $(q, \gamma) \in \epscl(q, \gamma)$;
% \rtwo if $(q', \gamma') \in \epscl(q, \gamma)$ and there exists $(q', \epsilon, \alpha, q'', \beta)$ where $\gamma' = \gamma'' \alpha$ for some $\gamma'' \in \Pi^\ast$, then $(q'', \gamma'' \beta) \in \epscl(q, \gamma)$.
% \end{definition}

% In PDA-based parsing, particularly in bottom-up approaches like LR parsing, $\epsilon$-transitions represent reduce actions, where a sequence of symbols on the stack matching a productions's right-hand side is replaced by its corresponding nonterminal. 
% These reductions consume stack symbols by popping them and push the nonterminal, resulting in a net decrease in stack size. Consequently, the $\epsilon$-closure remains finite, and this boundedness allows efficient computation of closures.



% \kh{Explain even deterministic PDA can have $\epsilon$-input transitions (ex. reduce-goto in LR parser). 
% Explain $\epsilon$-filter and define $\epsilon$-input reachable nodes (for both version of stack dependent and independent, need to think about better naming) }


% To verify the compatibility of a specific lexer and parser states combination, we check whether the intersection of the set of terminals producible from the lexer state (i.e., terminals generated from reachable final states) and the set of acceptable terminals from the parser state is nonempty.


% \kh{Formalization of of \texttt{producible} and \texttt{acceptable}}

% \begin{algorithm}[H]
% \caption{Token level parse table from FST $\mathcal{T} = (\Sigma, \Gamma, Q, q_0, \delta, F)$, and parse table $\mathfrak{T}$}\label{alg:cap}
% \begin{algorithmic}
% \State Initialize $\mathfrak{T}_{tok} := \{ \}$, 
% $\mathcal{I} := \{ (s_{lex}, s_{parse}) \mid \mathtt{producible}(s_{lex}) \cap \mathtt{acceptable}(s_{parse}) = \emptyset\}$
% \For{$s_{lex} \in Q$} 
%     \For{$s_{parse} \in \mathfrak{T}$}

%     \EndFor
% \EndFor\\
% \Return $\mathfrak{T}_{tok}$
% \end{algorithmic}
% \end{algorithm}

% \subsection{Fused Parser as Hidden Markov Model}