% \section{Related Work}
% \label{sec:related}

% \kh{TODO: Rewrite (regex-based)}

% \khchanged{
% One example of constrained decoding is generating text that conforms to a regular expression \cite{guidance, willard2023efficient}. In this approach, the decoding is guided by a finite-state automaton that recognizes the same set of strings as the given regular expression \cite{chomsky1956three}.
% }

\section{Conclusion}
\label{sec:conclusion}

We present \name, a tool for grammar-constrained decoding that is both flexible (low offline overhead) and efficient (low online overhead).
% 
\name precomputes a succinct and easy-to-generate data structure that only captures terminal sequences that are \textit{realizable} by LLM tokens in a given lexer state.
% 
This data structure also speeds up online masking compared to similar approaches by further reducing the number of inputs the parser has to check during decoding. 


\name is built with simple primitives that are already supported by existing parsing libraries and lends itself to an easy implementation consisting of just 800 lines of Python.
% 
This simple implementation is easier to test (we have identified that other tools often crash or produce incorrect outputs) and opens the door to many future possible research directions---e.g., finding ways to leverage the PDA stack to perform more aggressive precomputation or implementing decoding directly on GPUs.


\section*{Impact Statement}

This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.