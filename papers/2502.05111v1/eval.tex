\section{Experiments}\label{sec:eval}



We implemented our algorithms for computing token masks in a tool called \name. 
% 
A testament to the simplicity of our approach is that \name is implemented in just \textit{900 lines of Python code} built on top of the LALR parser provided by the \tname{Lark} library~\cite{lark}.
% 
Specifically, we used \tname{Lark} to parse regular expressions and context-free grammars, and used \tname{Lark}'s LALR parser to generate a parse table representing a deterministic PDA for parsing sequences of tokens in a given context-free grammar. 
% \loris{Specifically, we used the LALR parser to generate...}

In this section, we evaluate \name by answering the following
two questions:
\begin{itemize}
    \item \textbf{RQ1:} How does the \textit{offline preprocessing overhead} of \name compares to existing GCD approaches?
    \item \textbf{RQ2:} How does the \textit{online per-token decoding overhead} of \name compares to existing GCD approaches?
\end{itemize}
Because all GCD approaches in theory produce the same token masks and only differ in execution time, we do not need to evaluate \name's performance on specific downstream tasks; the effectiveness of GCD has already been evaluated in prior work~\cite{geng2024grammarconstrained,beurer2024domino,gad2024}.

\paragraph{Models and Grammars.}
We conduct our experiments using
three different tokenizers: Llama (32K tokens), Llama-3 (128K), and Qwen-2 (151K).
% 
We consider simplified Go, Java, and Python grammars from prior work on GCD~\cite{ugare2024syncode}.
% 
We choose these grammars for three reasons: they are large grammars (87-99 terminals, 109-145 nonterminals, 353-520 production rules, 7,441-9,319 bytes), they capture real programming languages that are used in existing applications of constrained decoding, and they illustrate well the trade-off between offline preprocessing and online masking times.
% 
While other smaller grammars appear in existing work on GCD, the tools we compare against all take slightly different grammar formats with various restrictions (e.g., no left recursion was allowed in \xgrammar), and we had to manually translate all grammars we considered between these formats to perform our evaluation (a very time-consuming task).

\paragraph{Baselines.}
We compare \name against \outlines~\cite{willard2023efficient},\syncode~\cite{ugare2024syncode},  and \xgrammar~\cite{dong2024xgrammar}, the three state-of-the-art GCD tools that can nominally handle grammars of practical sizes.


\paragraph{Measures.}
For each combination of tokenizer and grammar, we measured the time taken to preprocess the grammar for that tokenizer and the average time taken by each GCD implementation to produce a token at inference time.
% 
To fairly evaluate per-token computation time, we wanted to ensure that all three tools followed the same token selection process and recorded the time required to compute the mask at each step---i.e., the online overhead.
% 
We manually built 5 programs in each grammar and used them to decide what sequence of tokens the decoder was going to produce---i.e., for each example program, the decoder produced each token in the program in order and computed the token masks at each step.
% 
Following this setup, we could exactly measure the same online average per-token overhead across all GCD approaches.

% \begin{itemize}
%     \item Explain every tool computes the same mask but only different in running time
%     \item Preprocessing time
%     \item Preprocessed table size (memory usage)
%     \item Time per token
% \end{itemize}

\paragraph{Results.}

\autoref{tab:benchmarks} reports the results.
We do not include \xgrammar in \autoref{tab:benchmarks} because it encountered errors during either preprocessing or decoding  for all the grammars we considered---it failed to compile the Java grammar, and entered an infinite loop or encountered a segmentation fault during decoding for Python and Go grammars. 

% \outlines' offline preprocessing is the fastest (\loris{min-max}s, \loris{k}s avg), followed by 
% \name's (\loris{min-max}s, \loris{k}s avg), and then by \syncode's (\loris{min-max}s, \loris{k}s avg).
% 
On average (geomean), \name's \textit{offline preprocessing} is \nx  faster than \syncode, but 30.01x slower than \outlines.

For certain terminal sequences, \syncode incorrectly masked a valid token (we suspect the source of this imprecision is that \syncode only unrolls future terminal sequences up to a fixed bound $k$). 
% 
When considering the remaining data points, \name's \textit{online masking} is on average (geomean) 2.73x faster than \syncode and 550.57x faster than \outlines.


\input{figs/table}
% \input{figs/table-online}

\paragraph{Discussion}
In summary, \name emerges as the new state-of-the-art GCD approach.
\name is \nx faster at offline preprocessing than \syncode, the only other GCD tool with acceptable online overhead (\textbf{RQ1}), and exhibits the lowest online masking overhead than any other GCD tool (\textbf{RQ2}).

While \outlines has the lowest preprocessing offline overhead, its seconds-per-token online overhead does not meet the needs of most practical applications of LLMs.
% 
\outlines' online overhead is due to the fact that its CFG module verifies the acceptability of each token at runtime.
% \loris{explain why?}

The improvement in offline pre-processing over \syncode is expected as our new approach targets the inefficiency of \syncode's algorithm for token alignment.
% 
The slight improvement in online token-mask computation over \syncode is likely due to how \name only consults the PDA for context-dependent sequences, while \syncode does so for all terminal sequences up to a bound.


% 
% \loris{doesn't it also produce string outsoide the grammar?}
% \kh{It produces but that is not observed in this particular experiment, so better not to mention?}
While of course we cannot guarantee that \name is free of bugs, the simplicity of our approach makes its implementation easier (just 900 lines of code).

Furthermore, \name's  offline preprocessing times have allowed us to heavily test our implementation without incurring in multi-hour testing times.

\paragraph{Limitations}

The current implementation of \name works under the lexing assumptions described in \autoref{sec:lexing}: maximal munch principle and 1-lookahead lexing. 
% 
It therefore does not support languages for which lexing requires more than 1-lookahead or instances where the same input sequence must be lexed differently depending on the parsing context.

For example, in Java, the end of the nested generic \texttt{List<List<T>{}>} is erroneously tokenized by a lexer operating under maximal munch as $\texttt{>{}>}$---i.e., the bitwise  right-shift operator. However, in this context the parser instead expects two consecutive \texttt{>} terminals denoting the closure of a generic type. (The Java grammar we used in our evaluation, sourced from the \syncode benchmark, does not support generic types.)
% \loris{can you say other tools have the current same limitation?}
% \kh{XGrammar may not have such problem because it uses character NPDA, but not scale. Syncode also assumes 1-lookahead. Not sure about Domino because they don't mention. }

To resolve this type of lexing ambiguity, the lexer must consider the current state of the parser, a task that for arbitrary regular expressions requires the lexer to perform unbounded lookahead.
% 
It is possible to modify our approach to handle practically occuring ambiguities by allowing the lexing transducer to nondeterministically generate both possible tokenizations (e.g., a single \texttt{>{}>} terminal or two separate \texttt{>} terminals), specifically for such cases that do not require arbitrary lookahead, and enabling the parser to select the valid interpretation based on grammatical constraints.