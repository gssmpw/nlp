\section{Related Work}
\label{sec:related}

There are many different implementations of grammar constrained decoding~\cite{geng2024grammarconstrained, willard2023efficient, guidance, beurer2024domino, ugare2024syncode, dong2024xgrammar, llamacpp}, but we focus our comparison on the ones that can handle realistic grammars (e.g., those of modern programming languages).

\syncode \cite{ugare2024syncode} is the only that can provide low online overhead for the large grammars in our evaluation. \syncode speculatively unrolls future terminal sequences up to a fixed bound (i.e., every terminal sequence in $\terms^k$) and precomputes a table similar to our inverse token spanner table but \textit{for all} sequences. Our evaluation shows that our table can be computed significantly faster since it only contains the set of realizable terminal sequences.

\domino \cite{beurer2024domino} precomputes a tree of terminal sequences similar to our lexing transducer. 
However, at decoding time, \domino must traverse the entire tree because the set of realizable terminal sequences varies \emph{for each} lexer state. 
In contrast, our approach computes a single \emph{global} set of realizable terminal sequences $\realizable{\vocab}{\automaton}$, which we then use to preprocess the parser.  
\domino's implementation is not publicly available, but their paper reports 20s for offline preprocessing and 22\% online overhead for an extremely simplified C grammar with approximately 70 rules when using the smaller Llama tokenizer. 
For the same tokenizer, \name can preprocess \textit{much larger grammars} (353-520 rules) with similar times (25-35s).



\xgrammar \cite{dong2024xgrammar} reduces runtime checks by preprocessing context-independent tokens for character-based nondeterministic PDAs, but mentions the misalignment problem in their related work. 
By preprocessing the set of realizable terminal sequences and the inverse token spanner table, 
we efficiently integrate their context-independent token caching approach with a parser that uses a separate lexer. Furthermore, our overapproximation via FSA can identify more always-rejected sequences compared to their expanded suffix analysis, as our method can also follow rule-reduction edges by treating them as $\epsilon$-transitions.



% \loris{full one here, draw inspiration from SynCode related}

% Make clear domino no open so can't compare, copy quotes from syncode about being slow