\subsection{Cultural Image VQA}
\label{sec:sivqa}
%
In the Cultural Image VQA (\sivqa) task, models are presented with an image depicting a CEF and a question that relates to a particular CEF aspect (see \S\ref{appendix:sec:sivqa:examples} for examples).
%
Models are evaluated based on answer correctness.
%
To create the data for \sivqa, we couple synthetic data generation with a two-stage annotation process.
%

\rparagraph{Synthetic Data Generation}
\label{sec:sivqa:collection}
%
Building on the high-quality UNESCO ICH data, we applied synthetic data generation by prompting GPT-4o\footnote{\texttt{gpt-4o-2024-08-06}} to construct the basis for our dataset.%, a large number of ``silver'' VQA pairs.
%
Each VQA pair is related to a CEF and consists of an image depicting one aspect of the CEF, a question related to the CEF and the image, and an answer.
%
Maximizing the quality of the generated silver data, we applied extensive prompt engineering combining techniques such as Few-Shot, Chain-of-Thought, ReAct~\cite{wei2022cot,zhang2023autocot,zheng2024react,sahoo2024promptsurvey} to craft the prompt.
%
Key aspects of the prompt are a role description, a general task description, detailed annotation guidelines, a step-by-step strategy, an expected output format, few-shot examples, and the information of the target CEF (see \S\ref{appendix:sec:sivqa:synth} for the full prompt).
%
We then generated silver VQA pairs for each of the 6,827 images contained in the ICH data source, which resulted in 17,369 pairs.
%
Afterward, we automatically removed pairs where 1) the question contained words that introduce subjectiveness or ambiguity (``\textit{could}'', ``\textit{should}'', ``\textit{maybe}'', etc.); 2) the answer contained abstract words that are hard to depict visually; and 3) where the answer is not a substring of the description of the related CEF.
%
This way, we obtained 9,900 silver VQA samples related to 5,517 images from all 728 CEFs.
%

\rparagraph{Annotation Process}
\label{sec:sivqa:collection:annotation}
%
Opting for high-quality VQA pairs as well as cultural diversity, we devised a two-stage annotation process with 18 trained experts from various cultural backgrounds covering all six regions (see Table~\ref{tab:sivqa:anno:demographics} in \S\ref{appendix:sec:sivqa:anno}).
%
Each silver pair was evaluated using two questionnaires---one with seven question-related requirements and another with four answer-related requirements.
%
Questions had to target the CEF and image content directly, require cultural knowledge, and depend on visual evidence \cite{chen2024mmstar}.
%
Answers needed to be clear, objective, concise, and depictable.
%
For details on the annotation process, see \S\ref{appendix:sec:sivqa:anno}.
%

In the first round, we annotated each sample once, resulting in 4,114 samples, of which 2,826 (68.69\%) met all criteria.
%
In the second round, five annotators re-evaluated these, retaining only samples with concordant approval.
%
This process finally yielded 2,233 samples for 1,928 images from 728 CEFs across 144 countries in six global regions.% (see Table~\ref{tab:benchmark:datasets:samples}).
%
