\section{Experimental Setup}
\label{sec:exsetup}
%
\rparagraph{Models and Inference}
\label{sec:benchmark:models}
%
We evaluate a total of 31 models, including five proprietary LVLMs, 15 open-weight LVLMs, and 11 open-weight LLMs---the backbones of the respective LVLMs---covering 9 LVLM and 4 LLM model families.
%
The sizes of the open-weight models vary, categorized as small, medium, large, and extra-large (see Table~\ref{tab:benchmark:model_groups}).
%
A comprehensive list of models is provided in Table~\ref{tab:benchmark:models} in \S\ref{appendix:sec:benchmark:models}.
%
\begin{table}[t]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{.8}
    \begin{tabular}{l l r r}
        \toprule
        \textsc{Group} & \textsc{Parameters (B)}  & \textsc{LLMs} & \textsc{LVLMs} \\
        \midrule
        S  & 0.5 -- 4 & 5 & 5 \\
        M  & 7 -- 11 & 3 & 6 \\
        L  & 26 -- 38 & 2 & 2 \\
        XL & 72 -- 78 & 1 & 2 \\
        Closed  & unkown & 0 & 5 \\
        \midrule
        \multicolumn{2}{l}{\textit{Total}} & 11 & 20 \\
        \bottomrule
    \end{tabular}
    \caption{The size groups we define for result aggregation according to models' number of parameters.}
    \label{tab:benchmark:model_groups}
\end{table}
%
%\rparagraph{Model Inference}
%
For our experiments, we download open weights from the respective Huggingface~\cite{wolf2019hftransformers} repositories (see Table~\ref{tab:benchmark:models}) and generate responses employing greedy decoding.
%
For proprietary models, we use the official Python SDKs.
%
More details are reported in \S\ref{appendix:sec:setup}.

\rparagraph{Metrics}
%
%Depending on the task, we employ different evaluation metrics.
%
For the \sivqa, \vvqa, and \coqa tasks, we report relaxed answer accuracy, for which we consider a generated answer correct if it starts with the ground truth answer.
%
For \ckqad and \ckqan, due to their generative nature, we use GPT-4o\footnote{\texttt{gpt-4o-2024-11-20}} in an ``LVLM-as-a-Judge''~\cite{zheng2023llm-as-a-judge, xiong2024llava-critic} setup to judge responses with a score $s \in [0, 100]$.
%
Where $s=0$, $s=50$, and $s=100$ indicate \textit{completely incorrect or irrelevant}, \textit{partially correct or relevant}, and \textit{perfectly correct and complete} answers, respectively.
%
%We show the judge prompt and examples for different scores in \S\ref{appendix:sec:ckqa:judge}.
%

\rparagraph{Video Processing}
%
The 10-second video clips from \vvqa do not contain an audio stream, and we only use the visual information.
%
Following established praxis~\cite[e.g.,][]{wang2024qwen2vl}, we extract one frame per second from the videos and provide them to the models as input alongside the textual prompt.
%
Specifics about the image and video processing of the individual models are documented in the code.
%

\section{Results and Analyses}
\label{sec:analyses}
%
In this section, we present a series of in-depth analyses based on the outcomes of our benchmark.
%
We show aggregated results: open-weight models are grouped and averaged by parameter size, and proprietary models are averaged together (see Table~\ref{tab:benchmark:model_groups}).
%
We provide the complete numerical results for all tasks and models in \S~\ref{appendix:sec:analyses}.
%
In the following, we use abbreviations for regionsâ€š as defined in Table~\ref{tab:benchmark:regions}.

%
\input{src/041_analyses_a1}
%
\input{src/042_analyses_a2}
%
\input{src/043_analyses_a3}
%
\input{src/044_analyses_a4}
%

% \subsection{Key Findings}
% \label{sec:analyses:findings}
% %
% In summary, we find that models generally perform relatively low on tasks requiring nuanced cultural knowledge across all regions with overall averages of $16.67$ and $21.13$ accuracy and $13.75$ and $30.14$ Judge-Score for \sivqa, \vvqa, \ckqan, \ckqad, respectively. 
% %
% Further, we assess a significant Western bias on these tasks across all models, with, e.g., the average accuracy on \RegW of \sivqa ($17.63$) being significantly lower than on \RegSA ($6.93$). 
% %
% In tasks where only coarse cultural knowledge, i.e., the regional or country origin of a CEF, is required, the models are significantly better, with overall averages of $81.17$ for \coqar and $85.02$ \coqac.
% %
% Interestingly, models perform much better on non-Western cultures, with, e.g., \RegW having the lowest average accuracy of $74.64$ and \RegAP having the highest with $92.94$ followed by \RegSA with $90.08$ for the \coqar task.
% %
% This suggests more distinct visual and linguistic features of the mentioned regions compared to relatively 
% %
