\subsection{Cultural Video VQA}
\label{sec:vvqa}
%
In this task, models are evaluated on questions relating to videos instead of single images, again employing accuracy as the metric.
%
To this end, we extend \sivqa in two steps: synthetic data generation and quality annotation.
%

\rparagraph{Synthetic Data Generation}
\label{sec:vvqa:collection}
%
First, we adjusted the \sivqa questions by replacing the term  \emph{``image''} with \emph{``video''}. We then coupled the question with a short video clip, for which we started from the CEF's associated YouTube video. We ensured that the shortened clip contains relevant information for answering the question as follows:
%
From each video, we extracted one frame per second, and computed image embeddings for both the frames and the \sivqa image, using DINOv2\footnote{\texttt{facebook/dinov2-with-registers-large}}~\cite{oquab2024dinov2,darcet2024dinov2registers}.
%
We then identified the frame that best matches the original image by calculating Cosine similarity. We selected this frame as the center (at $t=0$) for a 10-second clip\footnote{
We do not include the audio stream in our clips.} (from $t=-5$ to $t=5$).
%
We only include clips with a best-matching frame similarity $>0.5$, which we found to yield high-quality instances based on a manual inspection of random samples.
%
Overall, this procedure resulted in 2,001 silver samples.
%

\rparagraph{Annotation Process}
\label{sec:vvqa:collection:annotation}
%
For additional quality control, a trained expert annotated 20\% of the silver data (400 samples).
%
Each sample was evaluated using a three-item questionnaire\footnote{cf. \S\ref{appendix:sec:vvqa} for details.} assessing whether (1) the video contained frames resembling the CEF image, (2) it clearly answered the question, or (3) neither condition was met.
%
Overall, 95\% of the annotated samples were accepted.
%
For closer inspection, we stratified the annotated samples into four similarity bins, revealing that roughly 10\% of those in the lower bins ($[0.5, 0.75[)$ were rejected, while nearly all, i.e., 99\% and 100\%, in the higher bins ($[0.75, 1.0]$) were retained.
%
The residual 5\% label noise was considered acceptable based on further manual analysis.
%
Notably, we found that of the 20 rejected samples, only 9 were unanswerable based on the video, while the remaining 11 exhibited only a suboptimal frame match w.r.t. the \sivqa image.
%
The final \dsname \vvqa dataset contains 1,809 samples (see \S\ref{appendix:sec:vvqa:examples} for examples) linked to 553 CEFs from 139 countries.
%