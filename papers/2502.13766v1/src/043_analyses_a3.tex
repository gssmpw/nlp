\subsection{Influence of Modalities}
\label{sec:analyses:a3_modality}
%
We explore how input modality---text-only, image-only, or text+image---affects performance on \coqac, \coqar, and \ckqad.
%
Further, we compare LVLMs to their LLM backbones to assess potential losses in cultural knowledge during multimodal training.
%

\begin{figure*}[t]
    \centering
     \begin{subfigure}{1.\linewidth}
        \centering
        \includegraphics[width=1.\linewidth]{gfx/coqa_countries_avg_scores.pdf}
        \caption{\coqac}
        \label{fig:analyses:a3_modality:coqa-countries}
    \end{subfigure}
    
     \begin{subfigure}{1.\linewidth}
        \centering
        \includegraphics[width=1.\linewidth, trim=0 0 0 25, clip]{gfx/coqa_regions_avg_scores.pdf}
        \caption{\coqar}
        \label{fig:analyses:a3_modality:coqa-regions}
    \end{subfigure}

    \begin{subfigure}{1.\linewidth}
        \centering
        \includegraphics[width=1.\linewidth, trim=0 0 0 25, clip]{gfx/ckt_desc_avg_scores.pdf}
        \caption{\ckqad}
        \label{fig:analyses:a3_modality:ckqa-desc}
    \end{subfigure}
    \caption{Aggregated results including multimodal input variations: \textbf{T}ext-only, \textbf{I}mage-only, \textbf{T}ext+\textbf{I}mage.}
    \label{fig:analyses:a3_modality:scores}
\end{figure*}
%
%
\rparagraph{Input Modalities}
Figure~\ref{fig:analyses:a3_modality:scores} shows that text+image (\texttt{I+T}) inputs consistently yield the highest performance across all tasks, confirming that textual and visual data provide complementary cultural cues.
%
The gap between \texttt{I+T} and text-only (\texttt{T}) is slightly more prominent for \coqac than \coqar, suggesting that visual information aids in inferring fine-grained, country-level details.
%
In contrast, image-only (\texttt{I}) inputs perform poorly, indicating that textual information, such as CEF titles, carries more cultural context.
%
The high variance in \texttt{T} results for the \coqa tasks stems from the performance disparity between \m{Gemini Pro} and \m{Claude 3.5 Sonnet} (e.g., $59.38$ vs. $83.75$ for \RegW).
%

\rparagraph{LVLM vs. LLM-Backbone}
%
Comparing LVLMs with their LLM backbones reveals that multimodal training can impair the acquisition of detailed cultural knowledge (notably in \ckqad) while having minimal impact on coarse-grained cultural understanding (\coqa).
%
For large models, significant performance gaps---$50.62$ for \m{Qwen2.5 72B} vs. $40.02$ for \m{Qwen2VL 72B} on \RegAP---on the \ckqad task between the LVLMS and their LLM backbones can be observed, whereas, for smaller models, the effect is subtle.
%
Overall, our findings highlight that while images complement text for culturally grounded tasks, it is ultimately the synergy between both modalities that leads to robust and broad cultural understanding.
%
