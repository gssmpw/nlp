\subsection{General Trends and Cultural Bias}
\label{sec:analyses:a1_bias}
%
We discuss general trends and investigate cultural bias across regions (Figures~\ref{fig:analyses:a1_bias:scores} and~\ref{fig:analyses:a1_bias:ppl}). %and analyzing ground-truth answer perplexity for the \m{QwenVL} model family on \sivqa (Figures~\ref{fig:analyses:a1_bias:scores} and~\ref{fig:analyses:a1_bias:ppl}).
%

\begin{figure}[t]
     \begin{subfigure}{1.\linewidth}
        \centering
        \includegraphics[width=1.\linewidth]{gfx/sivqa_avg_scores.pdf}
        \caption{\sivqa Accuracy}
        \label{fig:analyses:a1_bias:scores:sivqa}
    \end{subfigure}

    \begin{subfigure}{1.\linewidth} 
        \centering
        \includegraphics[width=1.\linewidth, trim=0 0 0 31.5, clip]{gfx/vvqa_avg_scores.pdf}
        \caption{\vvqa Accuracy}
        \label{fig:analyses:a1_bias:scores:vvqa}
    \end{subfigure}

    \begin{subfigure}{1.\linewidth}
        \centering
        \includegraphics[width=1.\linewidth, trim=0 0 0 25, clip]{gfx/ckt_naming_avg_scores.pdf}
        \caption{\ckqan Accuracy}
        \label{fig:analyses:a1_bias:ckqa-name}
    \end{subfigure}
    \caption{Aggregated results of the VQA tasks.}
    \label{fig:analyses:a1_bias:scores}
\end{figure}
%
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{gfx/sivqa_perplexity_qwen.pdf}
    \caption{\sivqa ground-truth answer perplexity.}
    \label{fig:analyses:a1_bias:ppl}
\end{figure}

\rrparagraph{\sivqa \& \vvqa}
%
Figures~\ref{fig:analyses:a1_bias:scores}a–c show clear regional performance disparities.
%
Across all models---proprietary and open-weight, regardless of size---scores are highest for Western and Asian targets (\RegW, \RegE, and \RegAP) and lowest for \RegSA.
%
XL models, e.g., reach $24.04$ on \RegW and $9.32$ on \RegSA on average.
%
\RegA and \RegLAC fall in between, with model performance varying by size.
%
Since \sivqa is an open-answer task, often with rare culturally specific terms, we also evaluated the task with GPT-4o as LVLM-as-a-Judge to account for imperfect naming or spelling.
%
While this method yields higher scores, it confirms the same trend: models exhibit a strong bias toward Western contexts.
%
However, even the best model (\m{GPT-4o}) scores only $31.58$\% on \RegW and $25.44$\% on average, highlighting \dsname as a challenging benchmark and the lack of fine-grained cultural knowledge in current models.
%
We supplement our analysis with a more fine-grained investigation of how well models ``know'' the cultural concepts discussed.
%
Here, we focus on the \m{QwenVL} models on \sivqa and the compute perplexity of ground truth answers (conditioned on the input context) as a proxy of model cultural knowledge (details in \S\ref{appendix:sec:analyses:sivqa:results:ppl}). 
%
Figure~\ref{fig:analyses:a1_bias:ppl} shows that for the 7B and 72B models, perplexity is consistently lower for \RegW, \RegE, and \RegAP compared to \RegA and \RegSA, aligning with our performance findings.
%
For the 2B model, however, \RegE and \RegSA yield the highest perplexities, which we attribute to the overall brittleness of the model.
%
Moreover, we revisit the performance on questions about the prevalent cultural aspects in \sivqa (details in \S\ref{sec:appendix:sec:analyses:sivqa:results:aspects}) and find that models perform notably better on tangible cultural aspects than on intangible ones.
%
For instance, closed models achieve an accuracy of 30\% for food-related questions and only 8\% and 10\% for questions concerning rituals or festivals.
%
This highlights biases along the cultural dimension, which are particularly pronounced in non-Western contexts.
%This highlights not only regional but also biases along the cultural dimension, the latter being particularly pronounced in non-Western contexts.
%

%Figure~\ref{fig:analyses:a1_bias:ppl} shows that for the 7B and 72B models, perplexity is consistently lower for \RegW, \RegE, and \RegAP compared to \RegA and \RegSA, aligning with our performance findings.
%
%For the smallest model, however, \RegE and \RegSA yield the highest perplexities.
%

\rrparagraph{\ckqan \& \ckqad}
%
For \ckqan, regional differences are minor, though proprietary models significantly outperform open-weight ones (see Figure~\ref{fig:analyses:a1_bias:ckqa-name}).
%
The large error bars for closed models indicate inconsistent performance---particularly from \m{GPT-4o Mini} and \m{Gemini Flash} models, which perform similarly to large open-weight models.
%
XL and L models perform worst on \RegSA and \RegLAC and best on \RegA and \RegAP with minor differences to \RegW and \RegE.
%
For \ckqad (Figure~\ref{fig:analyses:a3_modality:ckqa-desc}), performance is $10–20\%$ higher than on \ckqan, likely because describing a CEF is easier than exactly naming it.
%
However, regional biases are larger, with consistently higher scores on \RegW than on \RegSA, primarily for closed models like \m{GPT-4o}, which reaches $53.66$ for \RegW and $43.70$ on \RegSA.
%

\rrparagraph{\coqac \& \coqar}
%
Figure~\ref{fig:analyses:a3_modality:coqa-countries} shows minimal regional differences for \coqac.%, with all models performing well across regions.
%
Average accuracies range from close to or above $90\%$ for closed, XL, and L models to $77.42$\% for S models.
%
However, performance on \coqar is lower than on \coqac---$85.02\%$ vs. $81.17\%$ on average over all models and regions--- with models achieving the highest scores in \RegAP.
%
Notably, the regional ranking is mostly inverted compared to other tasks---\RegSA, \RegA, \RegLAC, \RegE, and \RegAP score higher than \RegW---suggesting more distinct visual and linguistic features in non-Western regions.
%


%\rparagraph{Perplexity Analysis}
%
%We also examined regional biases in ground-truth answer perplexity on \sivqa for the Qwen2 VL family.
%
%The perplexity for every sample is computed as:
%
%{
%\footnotesize
%\begin{equation*}
%\mathrm{PPL}(y \mid x) = \exp\left(-\frac{1}{N} \sum_{t=0}^{N} \log p\left(y_t \mid y_{t-1},\, x\right)\right)
%\end{equation*}
%}
%where $x = \{s, v\}$ are the textual ($s$) and visual ($v$) prompt (prefix) tokens and $y$ are the $N$ ground-truth answer tokens.
%
%Figure~\ref{fig:analyses:a1_bias:ppl} shows that for the 7B and 72B models, perplexity is consistently lower for \RegW, \RegE, and \RegAP compared to \RegA and \RegSA, aligning with our performance findings.
%
%For the smallest model, however, \RegE and \RegSA yield the highest perplexities.
%
%This reflects similar findings as previous black-box probing approaches.
%
