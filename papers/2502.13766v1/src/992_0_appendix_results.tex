\onecolumn
\section{Experimental Setup}
\label{appendix:sec:setup}
%
For inference, we load all models using the \textit{transformers} library (\texttt{v.4.48.0}) in 16-bit with Flash Attention 2~\cite{dao2022flashattention,dao2023flashattention2} (\texttt{v.2.7.3}), PyTorch (\texttt{v.2.4.0}), and CUDA (\texttt{v12.1}).
%
We used A40 (46GB) GPUs for models up to 26B parameters, A100 (80GB) GPUs for models up to 38B parameters, and two H100 (96GB) GPUs for 70B+ models in a multi-GPU setup.
%
To generate responses, we use greedy decoding, i.e., we use the following arguments for the generation method:
%
\begin{listing}[H]
\begin{minted}[fontsize=\footnotesize]{python}
generation_kwargs = {
    "max_new_tokens": 512,
    "do_sample": False,
    "temperature": None,
    "top_p": None,
    "top_k": None,
}
\end{minted}
\end{listing}
%

More details and exact hyperparameters are documented in the code base: \href{https://github.com/floschne/gimmick}{https://github.com/floschne/gimmick}.

\section{Results and Analyses}
\label{appendix:sec:analyses}
%
\input{src/992_1_appendix_results_sivqa}
%
\input{src/992_2_appendix_results_vvqa}
%
\input{src/992_3_appendix_results_coqa}
%
\input{src/992_4_appendix_results_ckqa}
%