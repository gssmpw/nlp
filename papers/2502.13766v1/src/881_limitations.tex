\newpage
\section*{Limitations}
\label{sec:limitations}
%
\paragraph{English-Only Benchmark}
%
Although we consider the performance on tasks requiring cultural understanding in English as an upper bound for the majority of models, it is yet to be tested if that hypothesis generally holds across tasks, model size, and model family.
%
Especially for models like \m{QwenVL} and \m{InternVL}, which were pretrained on large portions of Chinese textual data, Chinese could be pivotal instead of English.
%
Moreover, some cultural nuances might not be translatable to other languages.
%

\rparagraph{Open-Ended VQA}
%
%
\sivqa and \vvqa comprise open-ended answers to their questions, imposing challenges for adequate evaluation, especially when employing binary metrics like accuracy.
%
This is especially true for rare, culturally specific answer terms, such as in our tasks, which are prone to spelling inaccuracies or might have different names in different cultures or languages.
%
Although we alleviate this issue by computing scores using GPT-4o in an LVLM-as-a-Judge setting and thereby confirm our findings, this requires additional computational and financial resources.
%
A typical solution for this is transforming the questions into multiple choice questions, which, however, requires culturally expert annotators, which are complicated to find or train and expensive if hired via professional annotation companies.
%

\rparagraph{Small Number of Samples}
%
With a total of 7239 unique samples across all tasks in \dsname---2233 (\sivqa), 1809 (\vvqa), 982 (\coqac), 759 (\coqar), 728 (\ckqad), and 728 (\ckqan)---, the benchmark itself as the third most samples compared to other recent benchmarks.
%
However, the per-task number falls relatively low, leading to even fewer counts per country or culture, making judgments about single countries not informative.
%
