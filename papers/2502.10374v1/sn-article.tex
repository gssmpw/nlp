%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  

\PassOptionsToPackage{usenames,dvipsnames}{xcolor}

\documentclass[pdflatex,sn-nature,iicol]{sn-jnl}% Style for submissions to Nature Portfolio journals
%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
% \documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
% \documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{caption}
\usepackage{enumitem}
\usepackage[many]{tcolorbox}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{multicol}

% \usepackage{fdsymbol}
\usepackage{helvet}
\renewcommand\familydefault\sfdefault
\titleformat{\section}
  {\normalfont\sffamily\large\bfseries}
  {\thesection}{1em}{}


%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{\bf
    Robustness tests for biomedical foundation models should tailor to specification \vspace{1em}
}
%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1,*]{\fnm{R. Patrick} \sur{Xian}}

\author[2]{\fnm{Noah R.} \sur{Baker}}

\author[3]{\fnm{Tom} \sur{David}}

\author[4,1]{\fnm{Qiming} \sur{Cui}}

\author[5]{\fnm{A. Jay} \sur{Holmgren}}

\author[6]{\fnm{Stefan} \sur{Bauer}}

\author[7]{\fnm{Madhumita} \sur{Sushil}}

\author[1,7,*]{\fnm{Reza} \sur{Abbasi-Asl}}

\affil[1]{\orgdiv{Weill Institute for Neurosciences}, \orgname{University of California, San Francisco}, \orgaddress{\street{1651 4th Street}, \city{San Francisco}, \state{CA} \postcode{94158}, \country{USA}}}

\affil[2]{\orgdiv{Biological and Medical Informatics Graduate Program}, \orgname{University of California, San Francisco}, \orgaddress{\street{550 16th Street, 3rd Floor}, \city{San Francisco}, \state{CA} \postcode{94158}, \country{USA}}}

\affil[3]{\orgdiv{PRISM Eval}, \orgaddress{10 Rue de Penthièvre, \postcode{75008} \city{Paris}, \country{France}}}

\affil[4]{\orgdiv{Department of Bioengineering}, \orgname{University of California, Berkeley}, \orgaddress{\street{306 Stanley Hall, University Drive}, \city{Berkeley}, \state{CA} \postcode{94720}, \country{USA}}}

\affil[5]{\orgdiv{Division of Clinical Informatics and Digital Transformation}, \orgname{University of California, San Francisco}, \orgaddress{\street{10 Koret Way}, \city{San Francisco}, \state{CA} \postcode{94117}, \country{USA}}}

\affil[6]{School of Computation, Information and Technology, Technical University of Munich \& Helmholtz AI, \orgaddress{\street{Arcisstrasse 21}, \postcode{80333} \city{Munich}, \country{Germany}}}

\affil[7]{\orgdiv{Bakar Computational Health Sciences Institute}, \orgname{University of California, San Francisco}, \orgaddress{\street{490 Illinois Street}, \city{San Francisco}, \state{CA} \postcode{94158}, \country{USA}}}

\affil[*]{Corresponding authors: \texttt{xrpatrick AT gmail.com, reza.abbasiasl AT ucsf.edu}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%
% \abstract{Existing regulatory frameworks on AI in medicine emphasize robustness but lack detailed implementational guidance. The recent rise of foundation models in the biomedical domains creates new hurdles given their broad capabilities. We provide a narrative distinction between different forms of robustness that may contribute to model failures in biomedical foundation models. We ground the robustness issues with available examples and discuss potential scenarios that may emerge imminently. To tackle robustness evaluation more concretely, we advocate a granular categorization of robustness that motivates risk stratification and tailoring the design of robustness tests to the utility in application settings. We suggest combining domain-agnostic and domain-specific robustness tests in benchmarks and post-deployment monitoring along with human oversight to mitigate potential robustness-associated failures.}
% Existing regulatory frameworks on AI in medicine emphasize robustness but lack detailed implementational guidance. The recent rise of foundation models in the biomedical domains creates new hurdles given their broad capabilities. We provide a narrative distinction between different forms of robustness using available exam- ples grounded in the biomedical domain and discuss potential scenarios that may emerge imminently. To tackle robustness evaluation more concretely, we suggest a granular categorization that motivates risk stratification. We advocate tailoring the design of robustness tests to different goals in monolithic and compound systems to address domain-agnostic and domain-specific needs.

% Existing regulatory frameworks on AI in medicine emphasize robustness but lack detailed implementational guidance. The recent rise of foundation models in the biomedical domains creates new hurdles given their broad capabilities. We discuss the distinction between different forms of robustness using available examples grounded in the biomedical domain and discuss potential scenarios that may emerge imminently. To tackle robustness evaluation more concretely, we suggest a granular categorization that motivates risk stratification. We advocate tailoring the design of robustness tests according to the development-deployment distribution mismatch, domain specificity, and model architecture.

% Existing regulatory frameworks on AI in medicine include robustness as a key component but lack detailed implementational guidance. The recent rise of biomedical foundation models creates new hurdles given their broad capabilities and exposure to complex distribution shifts. We highlight the development-deployment environment mismatch, domain specificity, and model architecture as the three main aspects to tailor the design of robustness tests. Concrete policy should adopt a granular categorization of robustness concepts to facilitate risk specification and stratification and foster continuous user engagements for risk mitigation.

\abstract{
\setstretch{1.4}\bfseries
Existing regulatory frameworks for biomedical AI include robustness as a key component but lack detailed implementational guidance. The recent rise of biomedical foundation models creates new hurdles in testing and certification given their broad capabilities and susceptibility to complex distribution shifts. To balance test feasibility and effectiveness, we suggest a priority-based, task-oriented approach to tailor robustness evaluation objectives to a predefined specification. We urge concrete policies to adopt a granular categorization of robustness concepts in the specification. Our approach promotes the standardization of risk assessment and monitoring, which guides technical developments and mitigation efforts.
\vspace{-1.5em}
}

% featuring a natural language interface
%%================================%%
%% Sample for structured abstract %%
%%================================%%

% Deep neural networks trained on large biomedical datasets of varying modalities are becoming more popular for applications.
% To tackle robustness evaluation more concretely,
% We contextualize robustness issues using available examples in the biomedical domains and discuss emerging scenarios.

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

% \keywords{keyword1, Keyword2, Keyword3, Keyword4}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

% \section{Introduction}\label{sec1}
% We is on robustness failures and the evaluation through domain-specific tests 
% We highlight here two directions that require further attention from the communities developing and deploying foundation models in the biomedical domain.
% However, a growing number of cases show that the inputs created with subtle changes using domain-constrained rules can cause models to fail.
% that are wildly incorrect yet subtly noticeable delivered at unexpected times
% Generative foundation models also suffers from that lead.

% Robustness is an equally important topic, and this comment intends to raise awareness of it by discussing concrete and potential failure cases.
% because models are often deployed in heterogeneous environments that are distributionally different from the datasets used in their development and biomedical data are generally limited in their coverage. 
% large vision models (LVMs)
% \setstretch{1}
% \hspace*{1em}
\setstretch{1.55}
\hspace*{1em}The growing presence of biomedical foundation models (BFMs), including large language models (LLMs), vision-language models (VLMs), and others, trained using biomedical or de-identified healthcare data suggests they will eventually become integral to healthcare automation. Discussions on the risks of deploying algorithmic decision-making and generative AI in medicine have focused on bias and fairness. Robustness \cite{tocchetti2025} is an equally important topic. It generally refers to the consistency of model prediction to distribution shift and is measured using either averaged or worst-case metrics. Robustness issues manifest in every stage of the model lifecycle (see Fig. \ref{fig:cases}a-b), leading to performance degradation, and more alarmingly, the generation of misleading or harmful content by imperfect users or bad actors \cite{imperfect_users_2022}. The robustness of software also carries legal responsibilities \cite{Ladkin2020}, especially when it requires formal licensing or deployment or is classified as a medical device when used in high-stakes environments \cite{warraich_fda_2024,freyer_future_2024}. 

We examined over 50 existing BFMs (\href{https://docs.google.com/spreadsheets/d/1i2Nj4xRwwnwCti14fFUopJvHaZQyDi6ycnmAAHAYTbA/edit?usp=sharing}{link to data}) in preprints and publications covering different biomedical domains, about 32\% of them contain no and about 32\% use very limited robustness assessments (see also the \href{https://github.com/RealPolitiX/bfm-robust}{repo}). The most commonly presented evidence of model robustness is consistent performance across multiple datasets. Although a convenient proxy, consistent performance is not equivalent to a rigorous robustness guarantee because the relationships between datasets are generally unknown. To ensure constructive and beneficial use of BFMs in the future, we need to consider robustness evaluation in the model lifecycle and safety bottlenecks in the intended application settings \cite{lyell_more_2023}. Box 1 provides a structured list of robustness concepts that warrant consideration in biomedical domains. Inspired by test case prioritization in software engineering \citep{rothermel_prioritizing_2001}, we suggest designing effective robustness tests according to task-dependent robustness specifications constructed from priority scenarios (or priorities, see Fig. \ref{fig:cases}c) to facilitate test standardization while extending upon existing customized tests as building blocks. We introduce the background and motivation before our proposal using examples.
\begin{figure*}[hbt!]
\centering
\includegraphics[width=0.95\textwidth]{Fig1_RobConsiderations_v8.pdf}
\vspace{4pt}
\caption{\textbf{Settings and designs of robustness tests.} Visualization in \textbf{a} illustrates the potential settings of development and deployment mismatches, which are represented in \textbf{b} according to the types of distribution shifts. Setting 1 indicates adversarial contribution shift. Setting 2 refers to the natural distribution shift. In setting 3, adversarial perturbations are introduced in deployment, while in setting 4, they are applied to the training data. Setting 5 contains adversarial perturbations both during model development and deployment, such as in backdoor attacks. \textbf{c}, Specification of robustness by a simplified threat model (defined by a distance bound) or priority (defined by realistic artifacts) in the task domain, shown using two examples. The threat-based robustness tests use the error bound from edit distance for the EHR foundation model (left) and the Euclidean distance for the MRI foundation model (right). An overlap exists between these two approaches to generating test examples.}
\label{fig:cases}
\vspace{-0.5em}
\end{figure*}
% While these concepts have necessary overlaps, their applications vary according to distinct areas of emphasis.
% While conceptual overlaps exist, their distinct emphasis defines the respective application setting.

% due either to natural occurrence or adversarial perturbation
% continuous monitoring of model behavior through tests designed for application purposes.
% and robustness benchmarks for healthcare AI have recently been identified as a bottleneck for next-generation adoption and regulatory action \cite{mincu_developing_2022}

% such as the knowledge acquisition process during development stages and the exposure to heterogeneous deployment environments considered out-of-distribution (OOD) from training data \cite{karunanayake_ood_2024}. Moreover, imperfect users and bad actors can subject the models to adversarial examples.
% \textbf{Complex distribution shifts.}
% \textbf{Model versatility.}

% The fresh challenges that the robustness evaluation of foundation models faces are chiefly due to the versatility of use cases and the complex distribution shifts the models are subject to, among others.

% For example, the changing symptomatology of a disease, such as during the rapidly evolving COVID pandemic can undermine the use of AI tools for diagnostic screening \cite{wynants_prediction_2020}.
% \textbf{Manifestations of complex distribution shifts}.

% In the development stage, malicious training data modification can result in poisoning and backdoor attacks.

\section*{The robustness evaluation challenges}
\hspace*{1em}\textbf{Foundation model characteristics}. The versatility of use cases and the exposure to complex distribution shifts are two major challenges of robustness evaluation (or testing) \cite{chen_foundational_2022} for foundation models that differentiate from prior generations of predictive algorithms. The versatility comes from foundation models' increased capabilities at inference time with knowledge injection through in-context learning, instruction following, the use of external tools (e.g. function calling) and data sources (e.g. retrieval augmentation), and with user steering of model behavior using specially designed prompts. These new learning paradigms blur the line between development and deployment stages and open up more avenues where models are exploited for their design imperfections.
\begin{figure*}[hbt!]
\vspace{-0.5em}
% \tcbsidebyside[enhanced, segmentation style={solid}]
\begin{tcolorbox}[colback=Gray!40, colframe=Black, left=.5mm, right=.5mm, top=.5mm, bottom=.5mm, sharp corners, label=concepts, segmentation style=solid, equal height group=bottombox]
% \label{fig:concepts}
\setstretch{1.05}
\textbf{\large{Box 1}: Varieties of robustness concepts}\tcbline
\vspace{-10pt}
\setlength\columnsep{2pt}
\begin{multicols}{2}
\begin{tcolorbox}[colback=White, colframe=Gray!40, left=1mm, right=1mm, top=1mm, bottom=1mm, sharp corners, title=Theoretical perspective, coltitle=Black, fonttitle=\bfseries]
\small{
\textbf{Natural robustness} refers to the resistance to natural variation of data distribution, also called non-adversarial robustness to emphasize the distinction.
\vspace{3pt}\\
\textbf{Adversarial robustness} defines model resistance to accidental or deliberate malicious data manipulations such as adversarial attacks and jailbreaks.
\vspace{3pt}\\
\textbf{Interventional robustness} refers to the invariance of model behavior to causally-defined interventions (e.g. image background or contrast shift, patient age shift) that characterize distribution shifts.
\vspace{3pt}\\
\textbf{Out-of-distribution robustness} defines model resistance to semantic or covariate shift of test examples from training data distribution.
\vspace{3pt}\\
\textbf{Average-case \& Worst-case robustness} define the robustness concepts based on either the averaged or worst-case performance metric of a model.
\vspace{3pt}\\
\textbf{Feature \& Data robustness} define the robustness concepts based on the model performance change by removing or adding specific features or data in model development.
}
\end{tcolorbox}
\begin{tcolorbox}[colback=white, colframe=Gray!40, left=1mm, right=1mm, top=1mm, bottom=1mm, sharp corners, title=Engineering perspective, coltitle=Black, fonttitle=\bfseries]
\small{
\textbf{Development-stage robustness} resists corruption during model development by noise in labels or data, or by adversarial attacks with poisoning and backdoor attacks as the most common examples.
\vspace{3pt}\\
\textbf{Deployment-stage robustness} resists corruption at inference time due to transformed inputs, distracting information, or ill-intentioned behavior manipulations such as prompt injection and jailbreaks.
\vspace{3pt}\\
\textbf{Component \& Aggregated robustness} refers to
}
\end{tcolorbox}
\begin{tcolorbox}[colback=white, colframe=Gray!40, left=1mm, right=1mm, top=1mm, bottom=1mm, sharp corners, coltitle=Black, fonttitle=\bfseries]
\small{
the individual-level robustness or overall combined robustness of the addressable components in a multi-expert or multiagent system.
}
\end{tcolorbox}
\begin{tcolorbox}[colback=white, colframe=Gray!40, left=1mm, right=1mm, top=1mm, bottom=1mm, sharp corners, title=Application perspective, coltitle=Black, fonttitle=\bfseries]
\small{
\textbf{Group robustness} refers to the resistance of model performance against subpopulation distribution shifts. It is also called subgroup robustness.
\vspace{3pt}\\
\textbf{Instance-wise robustness} refers to the consistency of model performance to label-preserving changes at the level of a particular or the worst-case instance.
\vspace{-1em}\\
\textbf{Longitudinal robustness} refers to the consistency of model performance against temporal shifts or nonstationary effects in the data distribution.
\vspace{3pt}\\
\textbf{Knowledge robustness} refers to the consistency of model knowledge against perturbations at the level of entities and their relations, against missing or nonstationary elements in a knowledge graph used for development or inference.
\vspace{3pt}\\
\textbf{Vendor robustness} refers to the consistency of model prediction to instrument vendors or the associated acquisition protocols that generate data. It is also known as acquisition-shift robustness.
\vspace{3pt}\\
\textbf{Uncertainty-aware robustness} defines model's ability to account for uncertainties in the data distribution or in the model's internal mechanisms.
\vspace{3pt}\\
\textbf{Prevalence-shift robustness} refers to the consistency of model performance to changes in disease prevalence from epidemiological transitions.
\vspace{3pt}\\
\textbf{Behavioral robustness} refers to the consistency of model behavior in continuous interactions with humans or other models in a changing environment.
}
\end{tcolorbox}
\end{multicols}
\end{tcolorbox}
\end{figure*}

Distribution shifts come from natural changes in the data distribution or intentional and sometimes malicious data manipulation (i.e. adversarial distribution shift) \cite{chen_foundational_2022}. However, their distinction is becoming increasingly nuanced in the era of foundation models \cite{qi_safe_secure_2024} due to the growing diversity of use cases. In the biomedical domains, natural distribution shifts manifest in changing disease symptomatology, divergent population structure, etc. Inadvertent text deletion or image cropping result in data manipulations, potentially leading to adversarial examples that alter model behavior. More elaborate attacks have been designed that involve targeted manipulation in model development and deployment \cite{yang_poisoning_2024,jin_backdoor_2024}. Poisoning attacks involve stealthy modification of training data, while in backdoor attacks, a specific token sequence (called trigger) is inserted during model training and activate during inference time \cite{chowdhury_breaking_2024}. Distribution shifts in the deployment stage result in the majority of failure modes, including input transforms applied to text (deletion, substitution, and addition including prompt injection, jailbreaks, etc) or images (noising, rotation, cropping, etc). Both natural distribution shifts and data manipulation yield out-of-distribution data \cite{karunanayake_ood_2024} and they can have high domain-specificity or be created to target specific steps in the model lifecycle, creating complex deployment settings.

\textbf{Robustness framework limitations}. Aside from the challenges in scope, generating appropriate test examples for robustness evaluation are not often discussed. Two important robustness frameworks in ML, adversarial and interventional robustness, come from the security and causality viewpoints, respectively. The adversarial framework typically requires a guided search of test examples within a distance-bounded constraint such as the bounds by edit distance for text and Euclidean distance for image in Fig. \ref{fig:cases}c, yet there is no guarantee the test examples are sufficiently naturalistic to reflect reality. The interventional framework requires predefined interventions and a corresponding causal graph, which is often not immediately available for all tasks. Theoretical guarantees provided by these frameworks generally require justifications in the asymptotic limit and don't necessarily translate into effective robustness in diverse yet highly contextualized deployment settings characteristic of the specialized domains \cite{hager_evaluation_2024,johri_evaluation_2025}. Because robustness testing (and hence certification) is critically dependent on the robustness framework of choice, we should design robustness tests more aligned with naturalistic settings that reflect the priorities in particular domains.

% These factors illustrate the challenges in reliable assessments of robustness.
% A standard approach to evaluating robustness is to construct potential OOD samples using known data manipulation. Adversarial perturbations typically use a norm-bounded constraint to construct OOD samples, while in the causal intervention framework,  is Existing frameworks such as adversarial perturbation or causal intervention offer extensive computational leeway to construct perturbed data instances that models may encounter in the real world. Current regulatory frameworks on AI in medicine provide no specific categorization or recommendation on robustness tests \cite{tocchetti2025} beyond the general definition.

% In the adversarial setting, data manipulations (called attacks) may come from imperfect users or bad actors. Identifying concrete instances for BFMs to inform deployment protocol is an ongoing challenge.

% Traditionally, there has been a clear distinction between non-adversarial (also called natural) robustness and adversarial robustness during model development (including training, testing, and fine-tuning) and deployment. However, the distinction has become increasingly finer in the era of foundation models \cite{qi_safe_secure_2024} because These additional characteristics of BFMs, compared to prior generations of algorithms and models, blur the line between development and deployment stages, thereby exposing more avenues where the models may be exploited for their design imperfections.

% Understanding the origin of robustness-associated failures of BFMs is an ongoing challenge that requires the vigilance of and continuous engagements among diverse stakeholders in model development and deployment.

% Without external information and by examining only the outcome, it is generally difficult to determine whether the creator of adversarial examples is an intentional bad actor or an unsuspecting user.

% include the integrity of domain and commonsense knowledge, population structures present in the evaluation data, and the uncertainties of the data or the model.

% Essentially, priority-based design cater to the specific needs of the domain(s) the model serves, while

% However, it remains to be shown whether threat-based tests alone can satisfy all robustness specifications in an application domain. In comparison, the outcomes of priority-based tests directly inform model quality.

% Fig. \ref{fig:cases}b illustrates two examples for text and image inputs.

% \section*{Goal-driven test design}
% \section*{Specifying priorities for robustness}
\section*{Specifying robustness by priorities}
\hspace*{1em}Effective robustness evaluations requires a pragmatic robustness framework. We considers two aspects central to its specification: (i) the degradation mechanism behind the distribution shift, and (ii) the task performance metric which requires protection against the shift. The mechanistic understanding of a robustness failure mode requires establishing a connection between (i) and (ii), which is costly to account for every type of user interaction, or impractical when the users have insufficient information on development history or blackbox access. Moreover, multiple degradation mechanisms can simultaneously affect a particular downstream task.

Technical robustness evaluations in ML have focused on tackling a simplified threat (threat-based) for obtaining certification, where a specific degradation mechanism guides the creation of test examples. Most adversarial and interventional robustness tests fit into this category \cite{qi_safe_secure_2024}, yet these approaches often target a considerably broader set of scenarios than meaningful in reality. From the efficiency perspective, taking a priority-based viewpoint \cite{rothermel_prioritizing_2001} and focusing on retaining task performance under common degradation mechanisms anticipated in deployment settings is sufficient. Robustness tests based on simplified threat models and priorities are not mutually exclusive because accounting for realistic and meaningful perturbations (priority-based) overlaps with distance-bounded perturbations (threat-based), while the outcomes of priority-based tests should directly inform model quality. Fig. \ref{fig:cases}c contains two examples comparing threat- and priority-based robustness tests for text and image inputs, which illustrate the relationship between these two approaches for designing robustness tests.

We refer to the collection of priorities that demand testing for an individual task as a robustness specification. To contextualize robustness specification in naturalistic settings, we constructed two examples in Box 2 for an LLM-based pharmacy chatbot for over-the-counter (OTC) medicine and a VLM-based radiology report copilot for magnetic resonance imaging (MRI), both of which are attainable with existing research in BFM development. The specification contains a mixture of domain-specific (e.g. drug interaction, scanner information) and general aspects (e.g. paraphrasing, off-topic requests) that can induce model failures. The specification breaks down robustness evaluation into operationalizable units such that each can be converted into a small number of quantitative tests with guarantees. In reality, the test examples may come from augmenting or modifying the specified information in an existing data record \cite{hager_evaluation_2024,johri_evaluation_2025}, such as a clinical vignette or case report. The specification can accommodate future capability expansion of models and updated risk assessments accordingly. We discuss below the feasibility of our proposal using existing and potential realizations of robustness tests of BFMs for major types of robustness in application settings (see Box 1).
\begin{figure}[hbtp!]
\vspace{-0.5em}
\begin{tcolorbox}[colback=Gray!40, colframe=Black, left=.5mm, right=.5mm, top=.5mm, bottom=.5mm, sharp corners, label=concepts, segmentation style=solid]
% \label{fig:concepts}
\setstretch{1.05}
\textbf{\large{Box 2}: Robustness specification}\tcbline
\vspace{-10pt}
\begin{tcolorbox}[colback=White, colframe=Gray!40, left=1mm, right=1mm, top=1mm, bottom=1mm, sharp corners, title=Pharmacy chatbot for OTC medication, coltitle=Black, fonttitle=\bfseries]
\small{
The chatbot which can generate a medication request for patients has been extensively tested for robustness under the following scenarios:
\begin{enumerate}[wide, labelindent=0pt]
\item Up to 12 turns of conversation on topics related to primary care medication
\item Knowledge of specialized vocabulary, including products of major pharmaceutical vendors
\item Partly missing patient information (may refuse the task if patient is unwilling to provide sufficient information)
\item Drug combinations, recommended intake quantities, and dosage limits
\item Adverse drug interactions and patient history
\item Stock availability and provider preference
\item Limited prescription authority (will refuse the task if asked for non-OTC or prescription drugs)
\item Paraphrasing and typo in instruction prompt
\item Off-topic requests and non-existent drugs
\end{enumerate}
}
\end{tcolorbox}
\begin{tcolorbox}[colback=White, colframe=Gray!40, left=1mm, right=1mm, top=1mm, bottom=1mm, sharp corners, title=MRI radiology report copilot, coltitle=Black, fonttitle=\bfseries]
\small{
The copilot that generates a report from one or a series of images has been extensively tested for robustness under the following scenarios:
\begin{enumerate}[wide, labelindent=0pt]
\item Up to 15 turns of conversation including comparison of up to five images
\item Data from top-ten vendors of MRI scanners
\item Variability of MRI acquisition mode, image contrast, cropping or masking, and resolution
\item Common imaging artifacts (may refuse the task if image quality is too low)
\item Major human organs and body sections (may refuse the task if images given don't resemble human anatomy)
\item Knowledge of specialized vocabulary
\item Spatial relations of human anatomy
\item Temporal ordering of and distinguish disease manifestation
\item Paraphrasing and typo in instruction prompt
\item Off-topic requests and non-MRI images
\end{enumerate}
}
\end{tcolorbox}
\end{tcolorbox}
\end{figure}

\textbf{Knowledge integrity.} BFMs are knowledge models and the knowledge acquisition process in the model lifecycle can be tempered to compromise knowledge robustness. Demonstrated examples for BFMs include a poisoning attack on biomedical entities, which have been shown to affect an entire knowledge graph in LLM-based biomedical reasoning \citep{yang_poisoning_2024} and a backdoor attack using noise as the trigger for model failures in MedCLIP \cite{jin_backdoor_2024}. Testing knowledge robustness should focus on knowledge integrity checks using realistic transforms. For text inputs, one may prioritize typos and distracting domain-specific information involving biomedical entities over random string perturbation under an edit-distance limit (see Fig. \ref{fig:cases}b). Existing examples include deliberately misinforming the model about the patient history \cite{han_medical_2024}, negating scientific findings \cite{yan_worse_2024}, and substituting biomedical entities \cite{xian_assessing_2024} to induce erroneous model behaviors. For image inputs, one may prioritize the effects of common imaging and scanner artifacts \cite{boone_rood-mri_2023} and alterations in organ morphology and orientation on model performance (see Fig. \ref{fig:cases}b).

\textbf{Population structure.} Explicit or implicit group structures are often present in biomedical and healthcare data, including prominent examples such as subpopulations organized by age group, ethnicity, or socioeconomic strata, medical study cohorts with specific phenotypic traits, etc. BFM-enabled cross-sectional or longitudinal studies for patient similarity analysis or health trajectory simulation may lead to group or longitudinal robustness issues when evaluating on incompatible populations. Group robustness assesses the model performance gap between the best- and the worst-performing groups, either identifiable through the label or hidden in the dataset. Testing group robustness may modify subpopulation labels in patient descriptions to gauge the change in model performance \cite{yang_subpop_2023}. At a finer granularity, instance-wise robustness represents the performance gap between instances that are more prone to robustness failures than others, which are likely corner cases. It is important when the model deployment setting requires a minimum robustness threshold for every instance. Robustness testing in this context may use a balanced metric to reflect the impact of input modifications across individual instances.

\textbf{Uncertainty awareness.} The machine learning community typically distinguishes between aleatoric uncertainty, which comes from inherent data variability, and epistemic uncertainty, which arises from insufficient knowledge of the model in the specific problem context. Robustness tests against aleatoric uncertainty may assess the sensitivity of model output to prompt formatting and paraphrasing, while assessing robustness to epistemic uncertainty may use out-of-context examples \cite{chandu2025} to examine if a model acknowledges the significant missing contextual information in domain-specific cases (e.g. providing the model with a chest X-ray and asking for a knee injury diagnosis). Additionally, uncertain information may also be directly verbalized in text prompts, a fitting scenario in biomedicine, to examine its influence on model behavior. Overall, the current generation of robustness evaluations haven't yet included realistic uncertain scenarios often encountered in medical decision-making, although robustness against uncertainty is an important topic.

\section*{Embracing emerging complexities}
\hspace*{1em}Previous scenarios primarily consider assessing a monolithic model using single-criterion robustness tests. Specifying and testing robustness for more complex systems should also account for performance tradeoffs, model architecture, and user interactions. From the testing perspective, evaluating tradeoffs between various robustness criteria offers a more realistic view of a model's aggregated robustness. Thus, multi-criteria robustness tests are essential to assess whether the model's behavior reaches an optimal balance across these criteria. From the pipeline perspective, models integrated into a healthcare workflow can affect downstream biomedical outcomes. For example, using LLMs to summarize or VLMs to generate case reports may influence clinician decisions by emphasizing certain conditions or sentiments, affecting diagnoses or procedures. This highlights the need for considering robustness tests with humans in the loop and behavioral robustness in diverse interaction settings to assess the model's impact on the care journey.

From the model perspective, as modularity and maintainability become increasingly important, decision-making will be delegated to specialized subunits in a multi-expert (such as a mixture of experts) or multiagent system with a centralized coordinating unit. In these compound AI systems, each addressable subsystem is subject to testing and maintenance according to capability demand and regulatory compliance (see Fig. \ref{fig:cases}c). For example, Polaris \cite{mukherjee_polaris_2024} from Hippocratic AI features a multiagent medical foundation model which writes medical reports and notes as well as engages in low-risk patient interactions. Future systems with specialized units can mimic the group decision-making process in healthcare \cite{radcliffe_collective_2019} to manage real-world complexities through enhanced reasoning and cooperative performance gain. Robustness tests for compound AI systems may consider different specifications for subsystems depending on the part-part and part-whole relationship in identifying bottlenecks and cascading effects associated with robustness failures.
\vspace{-0.2em}

% While zero-risk applications are unattainable, strategies must be in place for evidence collection and judicious risk management in deployment \cite{freyer_future_2024}. Our prior analysis leads to the following recommendations. it is advisable to expect that not all potential robustness issues in BFMs are fully predictable prior to adoption. 

% Tailoring robustness requirements to specification also provides a way to align policy objectives with practical implementation and facilitates evidence collection and judicious risk management in deployment

% BFMs with addressable and specialized units where more comprehensive tests are appropriate (see Fig. \ref{fig:cases}c).
% : biomedical researchers, clinicians, and patients
% The primary user groups for BFMs—biomedical researchers, clinicians, and patients—require systems of varying robustness to ensure safety while maximizing the benefits derived from the model capabilities. Despite institutional policies advocating for robustness, gaps remain in exploring this from the perspective of model integrity.
% often without fully informing all end-users—from clinicians to patients—about the potential robustness issues when these models are applied at scale. 
% In aligning with pioneering frameworks such as the University of California’s AI policy, which underscores the necessity of robust risk assessment to embed principles of accuracy, reliability, safety, fairness, non-discrimination, and privacy into AI tools, our comment further delineates specific policy implications. The UC policy articulates that more stringent accountability and review processes are imperative for high-risk AI applications, advocating for multidisciplinary teams to evaluate and mitigate risks effectively.
% Earlier discussions on robustness set the stage for an educated look into existing regulatory frameworks
% It also connects seamlessly with the reporting mechanisms currently implemented for incidences \cite{lyell_more_2023}.
\section*{Advancing concrete policies\\ to guide implementation}
\hspace*{1em} Ensuring robustness for BFMs requires advancing regulatory policies for both AI and health information technology. Currently, the details for implementing robustness standards in AI regulations, such as the EU AI Act and the US Federal Artificial Intelligence Risk Management Act, are incomplete or insufficiently detailed. Existing health information technology regulations such as US-based HTI-1 final rule by Office of the National Coordinator focuses primarily on transparency and disclosures of the use of predictive decision support models yet lack detailed robustness requirements. The situation is in part due to the lack of a safety bare minimum for specific biomedical applications \citep{freyer_future_2024} and the evolving technological landscape, which can exacerbate the challenges laid out at the beginning of this Comment. Mandating a robustness specification proposed here provides a means to align policy objectives with practical implementation strategies, supporting evidence collection and enabling effective risk management throughout the model lifecycle. We advocate that robustness specifications (i) should seek community endorsement to gain a broad adoption; (ii) should consider the permissible tasks and user group characteristics due to the difference in user journeys; (iii) should inform regulatory standards such as in the construction of quantitative risk thresholds \cite{koessler_risk_2024} or safety cases by enriching the failure mode taxonomy and improving their informativeness.

Establishing robustness specifications with consensus from the research community will incentivize systematic evaluations by both model developers, research institutions, and independent third parties. These evaluations can inform proactive community engagement efforts, such as training end-users to recognize potential failures and apply mitigation strategies, which are critical to calibrating trust in human-AI interactions in high-stakes contexts. Concise, inclusive failure-reporting mechanisms should be established to allow users to provide timely feedback to the deployment team. Cross-referencing the specification with existing incident reporting mechanisms \cite{lyell_more_2023} will help pinpoint model vulnerabilities and guide targeted improvements or inform post-hoc adjustments to model behavior.

% Local evaluations may compare a fraction of the processed cases with human expert judgement to estimate discrepancy, which plays an indispensable role in ensuring consistent compatibility between the model development history and the heterogeneous environment.
% \textbf{Encourage novel approaches to AI interpretability.} 
% , which should be carried out regularly to provide insurance and inform maintenance

% \section*{Acknowledgements}
% RPX thanks S. Bauer from Technical University of Munich for helpful discussions. RA-A acknowledges support from the Sandler Program for Breakthrough Biomedical Research, which is partially funded by the Sandler Foundation.

% \section*{Acknowledgements}
% R.P.X., Q.C., R.A.-A. acknowledge support from the National Institute of Health. R.A.-A. acknowledges support from the Sandler Program for Breakthrough Biomedical Research, which is partially funded by the Sandler Foundation.

\section*{Acknowledgements}
R.P.X. thanks N. Rethmeier for helpful discussions. R.A.-A. would like to acknowledge funding from Weill Neurohub. M.S. is partially funded by the National Cancer Institute of the National Institutes of Health under Award Number P30CA082103. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

\section*{Author contributions}
R.P.X. conceptualized the main idea of the work and prepared the figures and tables. R.P.X. wrote the majority of text with significant contributions from N.R.B., T.D., Q.C., and A.J.H.. S.B. provided theoretical support, M.S., and R.A.-A. provided background information in different application settings. All authors edited the text and contributed to the discussions to finalize the manuscript.

\section*{Competing interests}
T.D. is a co-founder and director of governance \& standardization at PRISM Eval. Other authors declare no competing interest.

% \end{enumerate}

% Even well-meaning, holistic, and equitable models may be susceptible to perturbations resulting from failures in robustness.
% For patient-facing applications, a substantial portion of the robustness budget should be allocated to improving prompt sensitivity and knowledge robustness. Risk stratification should also guide the monitoring system for its prioritization. 
% Security-oriented robustness failures, for instance, could produce harmful content, impacting patient safety or the perceived trustworthiness of the model. This, in turn, could bias the user’s perception of the model system, the biomedical context, or the institution, depending on the severity of the robustness failure. The inherent risk of topic divergence in publicly-facing BFMs necessitates more stringent topic governance than in clinical settings, where interactions are typically shorter and more informationally relevant.

% This requires comprehensive documentation and clear communication tailored to the user’s level of interaction with the model and the potential risks that perturbations in robustness might pose to individual safety, data security, or organizational integrity.
% Appropriate policies for model behavior monitoring and algorithmic recourse should serve as additional layers of defense.

% Documentation should be developed to clearly explain how BFMs might experience perturbations in robustness, with language tailored to be accessible and address the challenges users might face when interacting with large language models. Providing users with feedback options beyond simple binary indicators—such as more detailed response assessments—could furnish BFM developers with richer information to enhance robustness across various domains or to automate corrective procedures in the BFM responses.

% Regardless of the user group, it is advisable to employ clearer language and tools for case-based robustness assessment and to expand existing auditing frameworks to include more comprehensive robustness test procedures combining domain-agnostic and domain-specific approaches.

% Uncertainty-aware robustness may pose lower risks in environments with controlled informational inputs to BFMs but could be higher in settings prone to stochastic behavior or variable information.

% , such as employing tested prompt templates or model behavior steering mechanisms

% \item \textbf{Determine Robustness Specifications, Data Sensitivity, and Budgeting to Match Application Settings and User Groups}\\


% Understanding the distinction between different types of robustness is a basis for compensating for their issues. 
% We advocate systematic risk monitoring for model updates, employing risk stratification to prioritize robustness testing. Logging incidents in relevant databases for suggestions in future updates in model and guardrail systems. The involvement of biomedical professionals in proofing the outcome can be determined based on the complexity, risk, and uncertainty of the tasks executed by the model. Failures from other types of robustness could be simulated using synthetic data \cite{bartolo2021} as proxy. It is advised to use clearer language and tools for case-based robustness assessment and expand on existing auditing frameworks to include more robustness test procedures that combine domain-agnostic and domain-specific approaches.

% Mitigation mechanisms, synthetic data \cite{bartolo2021} may be used for improving the robustness of LLMs and VLMs.
% Existing regulatory and auditing framework focuses on bias and fairness \cite{liu_medaudit_2022,pfohl_toolbox_2024}.

% Discussions should be brief and focused. In some disciplines use of Discussion or `Conclusion' is interchangeable. It is not mandatory to use both. Some journals prefer a section `Results and Discussion' followed by a section `Conclusion'. Please refer to Journal-level guidance for any specific requirements. 

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%
% \bigskip
% \begin{minipage}{\hsize}%
% \lstset{frame=single,framexleftmargin=-1pt,framexrightmargin=-17pt,framesep=12pt,linewidth=0.98\textwidth,language=pascal}% Set your language (you can change the language for each code-block optionally)
% %%% Start your code-block
% \begin{lstlisting}
% for i:=maxint to 0 do
% begin
% { do nothing }
% end;
% Write('Case insensitive ');
% Write('Pascal keywords.');
% \end{lstlisting}
% \end{minipage}

% \section{Methods}\label{sec11}

% \textbf{Ethical approval declarations} (only required where applicable) Any article reporting experiment/s carried out on (i)~live vertebrate (or higher invertebrates), (ii)~humans or (iii)~human samples must include an unambiguous statement within the methods section that meets the following requirements: 

% \backmatter

% \bmhead{Supplementary information}

% If your article has accompanying supplementary file/s please state so here. 

% Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

% Please refer to Journal-level guidance for any specific requirements.

% \bmhead{Acknowledgements}

% Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

% Please refer to Journal-level guidance for any specific requirements.

% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
% \item Ethics approval and consent to participate
% \item Consent for publication
% \item Data availability 
% \item Materials availability
% \item Code availability 
% \item Author contribution
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

% \bibliographystyle{sn-basic}
\setstretch{1.3}
\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

\end{document}
