\subsection{Choice of weight sequence}
\label{apdx-sec:weight-seq}
In all our experiments we draw the index $s$, at time $t_i$, from $\mbox{Uniform}\intset{\tau}{t_{i-1}}$ with $\tau = 10$. The main motivation behind setting $\tau = 10$ and not $\tau = 1$, which is more natural, is that we have found that otherwise it may lead to instabilities. This arises typically when an index $s$ is sampled very close to $0$ when $t \approx T$. To avoid such behavior we use a smaller learning rate in \Cref{algo:gauss_vi} for the first few iterations and set $\tau > 1$. For the last $25\%$ diffusion steps we set $s$ deterministically to $t_{i-1}$ as we have found that this slightly improves the reconstructions quality. We also ramp up the number of gradient steps as this significantly sharpens the details in the images. 

While it is more intuitive to sample $s$ close to $0$ as it provides the best approximation error for the likelihood, we have found that this can significantly slow the mixing of the Gibbs sampler in very large dimensions and provides rather poor results when used with a small number of Gibbs steps. Practically speaking, significant artifacts arise during the initial iterations of the algorithm due to the optimization procedure, and they tend to persist in subsequent iterations when $s$ is sampled close to 0. To see why this is the case consider the following empirical discussion on a simplified scenario. We write $\bx = [\bar\bx, \underline\bx]$  where $\bar\bx \in \rset^\dimobs$ and $\underline\bx \in \rset^{\dimx - \dimobs}$. 
We assume that $\pot{0}{\bx} = \normpdf(\obs; \bar\bx, \stdobs^2 \Id_\dimobs)$, \emph{i.e.}, we observe only the first $\dimobs$ coordinates of the hidden state. Since $s$ is sampled near $0$ we may assume that $\hpot{s}{} = \pot{0}{}$. Then, sampling $Z \sim \epost{s|0, t}{\bx_0, \bx_t}{}$  is equivalent to sampling 
\begin{align*}
    \bar{Z} & \sim \gauss\left(\frac{\std^2 _{s|0, t}}{\stdobs^2 + \std^2 _{s|0,t}} \obs + \frac{\stdobs^2}{\stdobs^2 + \std^2 _{s|0, t}} \big[ \gamma_{t|s} \a_{s|0} \bar\bx_0 + (1 - \gamma_{t|s}) \a^{-1} _{t|s} \bar\bx_t\big], \frac{\stdobs^2 \std^2 _{s|0,t}}{\stdobs^2 + \std^2 _{s|0,t}} \Id_\dimobs\right) \eqsp, \\
    \underline{Z} & \sim \gauss(\gamma_{t|s} \a_{s|0} \underline\bx_0 + (1 - \gamma_{t|s}) \a^{-1} _{t|s} \underline\bx_t, \sigma^2 _{s|0,t} \Id_{\dimx - \dimobs}) \eqsp,
\end{align*}
setting $Z = [\bar{Z}, \underline{Z}]$ and then concatenating both vectors. It is thus seen that the observed part of the state is updated with the observation whereas the bottom part is simply drawn from the prior. Moreover, if $\std^2 _{\smash{s|0, t}} \approx 0$ then $\gamma_{t|s} \a_{s|0} \approx 1$ and $\underline{Z}$ is almost the same as $\bx_0$.  In \Cref{algo:midpoint-gibbs}, once we have sampled $\vX_s \sim \epost{\smash{s|0,t}}{\vX_0, \vX_t}{}$, we first denoise it to obtain the new $\vX_0$ and then noise it to obtain the new $\vX_t$. As $s$ is sampled near 0, the denoising step will merely modify $\vX_s$ whereas the noising step will add significant noise to $\vX_s$ and may help with removing the artifacts. This noised sampled has however only a small impact on the next samples $\vX_0, \vX_s$ since $(1 - \gamma_{t|s}) \a^{-1} _{t|s} \approx 0$. In short, the first $\dimobs$ coordinates of the running state $\vX^* _0$ will be quickly replaced by the observation whereas the last $\dimx - \dimobs$ coordinates will be stuck at their initialization and will evolve only by a small amount throughout the iterations of the algorithm. We illustrate this situation on a concrete example in \Cref{fig:sampling-comparison} where we consider a half mask inpainting task. The first and second rows show the evolution of the running state $\vX^* _0$ with the time-sampling distributions  
\begin{align}
    \label{eq:sampling-dist-mix}
  & \mu^* _{i} = \begin{cases} \mbox{Uniform}\intset{\tau}{t_{i-1}} \, & \quad \text{if} \quad i > \lfloor  K / 4 \rfloor \\
                    t_{i-1} \, & \quad \text{else} 
\end{cases},\\
\label{eq:sampling-dist-zero}
& \mu^0 _i = \mbox{Uniform}\intset{1}{\lfloor t_i / 5 \rfloor} \eqsp, 
\end{align}
\emph{i.e.}, the time-sampling distribution we use in all our experiments, where $K$ is the number of diffusion steps, and the one that we use to sample only close to $0$, respectively. In \Cref{table:sampling-comparison} we compute the LPIPS for both distributions on a subset of the tasks we consider in the main paper. It is clear that $\mu^* _i$ outperforms $\mu^0 _i$, even when we increase the number of Gibbs steps (see phase retrieval task). 
% The unobserved part of the image contains significant errors during the first few steps due to the initialization and do not vanish afterwards whereas after a few iterations the unmasked part is recovered almost perfectly. 
% This is to be expected when $s$ is small. Indeed, consider the following empirical analysis with the simplified scenario where we assume that we obs. we may assume in this case that $\hpot{s}{\bx_s} = \pot{s}{\bx_s}$ and then, sampling perfectly from $\epost{s|0, t}{\bx_0, \bx_t}{}$ is equivalent to sampling 


%  and these are usually forgotten throughout the subsequent iterations when we sample $s$ uniformly on $\intset{1}{t_{i-1}}$. 
% We hypothesize that this is due to the combination of random timestep sampling, noising and denoising steps of the algorithm. Indeed, assume that in the first few iterations of the algorithm, the running state $\vX^* _0$ (see \Cref{algo:midpoint-gibbs}) contains significant artifacts. Then, once an index $s$ 

% However, when $s$ is sampled too close to $0$ they are not forgotten fast enough. 
\begin{table} 
    \centering 
    \caption{LPIPS on the \ffhq\ dataset for the two time-sampling distributions given in \eqref{eq:sampling-dist-mix} and \eqref{eq:sampling-dist-zero}. We use $R = 4$ Gibbs steps for the phase retrieval task.}
    \resizebox{0.60\textwidth}{!}{
    \begin{tabular}{l cccc}
        \toprule
        Distribution & Phase retrieval ($R = 4$) & JPEG2 & Gaussian deblurring & Motion deblurring \\
        \midrule
        $\mu^* _t$  & \textbf{0.10} & \textbf{0.14} & \textbf{0.12} & \textbf{0.09} \\
        $\mu^0 _t$ & 0.53 & 0.19 & 0.16 & 0.19\\
        \bottomrule
    \end{tabular} 
    }
    \label{table:sampling-comparison}
\end{table}
\begin{figure}
\centering 
\includegraphics[width=\textwidth]{figures/tau_sampling.jpg}
\caption{Evolution of the running state $\vX^* _0$ in \Cref{algo:midpoint-gibbs} for the two time-sampling distributions given in \eqref{eq:sampling-dist-mix} and \eqref{eq:sampling-dist-zero}. }
\label{fig:sampling-comparison}
\end{figure}

\subsection{Hyperparameters setup of \algo}
\label{apdx-sec:hyperparameters}
The details about the hyperparameters of \algo\ are reported in \Cref{table:hyperparams-algo}.
We adjust the optimization of the Gaussian Variational approximation in \Cref{algo:gauss_vi} during the first and last diffusion steps.
We ramp up the number of gradient steps during the final diffusion steps.
This allows us to substantially improve the fine grained details of the reconstructions. 
Similarly, we reduce the learning rate in the early step to alleviate potential instabilities.


\begin{table}[ht]
    \centering
    \caption{The hyperparameters used in \algo\ for the considered datasets. The index $i$ of the timesteps $\{t_i\}_{i=K}^0$ is taken in reverse order. The symbol \# stands for \emph{``number of''}.}
    \vspace{-0.2cm}
    \renewcommand{\arraystretch}{1.3} % Adjust row spacing if needed
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l cccccc}
        \toprule
        & \# Gibbs repetitions $R$ & \# Diffusion steps $K$ & \# Denoising steps $M$ & Time-sampling distribution & Learning rate $\eta$ & \# Gradient steps $G$ \\
        \midrule
        \ffhq & $R=1$ & $K=100$ & $M=20$ & $\mu^* _{i}$ as in \eqref{eq:sampling-dist-mix} & $\eta=\begin{cases}
            0.01  & \text{ if } i \geq \lfloor 3K/4 \rfloor \\
            0.03  & \text{ otherwise} \\
            \end{cases}$ 
            & 
            $ G = \begin{cases}
            20 & \text{ if } i \leq \lfloor K/4 \rfloor \\
            5  & \text{ otherwise} \\
            \end{cases}$
        \\
        \midrule
        \ffhq\ LDM & $R=1$ & $K=100$ & $M=20$ & $\mu^* _{i}$ as in \eqref{eq:sampling-dist-mix}  & $\eta=\begin{cases}
            0.01  & \text{ if } i \geq \lfloor 3K/4 \rfloor \\
            0.03  & \text{ otherwise} \\
            \end{cases}$ 
            &
            $ G = \begin{cases}
            20 & \text{ if } i \leq \lfloor K/4 \rfloor \\
            20 & \text{ if } i \mod 10 = 0   \\
            3  & \text{ otherwise} \\
            \end{cases}$
        \\
        \midrule
        \imagenet & $R=1$ & $K=100$ & $M=20$ & $\mu^* _{i}$ as in \eqref{eq:sampling-dist-mix} & $\eta=\begin{cases}
            0.01  & \text{ if } i \geq \lfloor 3K/4 \rfloor \\
            0.03  & \text{ otherwise} \\
            \end{cases}$ 
            &
            $ G = \begin{cases}
            20 & \text{ if } i \leq \lfloor K/4 \rfloor \\
            5  & \text{ otherwise} \\
            \end{cases}$
        \\
        \midrule
        Audio-source separation & $R=6$ & $K=20$ & $M=15$ & $\mu^* _{i}$ as in \eqref{eq:sampling-dist-mix} & $\eta=0.005$ &  $ G = \begin{cases}
            20 & \text{ if } i \leq \lfloor K/4 \rfloor \\
            3  & \text{ otherwise} \\
            \end{cases}$
        \\
        \midrule
        %
        \parbox[m]{10em}{Audio-source separation\\ (Best result in \Cref{table:si-snri})}
        & $R=1$ & $K=20$ & $M=15$ & $\mu^* _{i}$ as in \eqref{eq:sampling-dist-mix} & $\eta=0.005$ &  $ G = 90$
        \\
        \bottomrule
    \end{tabular}
    \label{table:hyperparams-algo}
    }
\end{table}


\subsection{Audio source separation}
% \paragraph*{Audio diffusion model.}

% For audio data, we consider a setup where the input consists of multiple audio tracks of varying lengths, each containing $N$ distinct source waveforms $\{\mathbf{x}_1, \dots, \mathbf{x}_N\}$, which coherently sum to form a mixture $\mathbf{y} = \sum_{i=1}^N \mathbf{x}_i$. A diffusion model is trained to learn the prior for this setup.

In our experiment, the diffusion model employed provided by \cite{mariani2023multi} is trained on the \slakh\ training dataset\footnote{\url{http://www.slakh.com/}},  using only the four abundant instruments (bass, drums, guitar and piano) downsampled to 22 kHz. The denoiser network is based on a non-latent, time-domain unconditional variant of \citep{schneider2023musai}.

Its architecture follows a U-Net design, comprising an encoder, bottleneck, and decoder. The encoder consists of six layers with channel numbers $[256, 512, 1024, 1024, 1024, 1024]$, where each layer includes two convolutional ResNet blocks, and multihead attention is applied in the last three layers. The decoder mirrors the encoder structure in reverse. The bottleneck contains a ResNet block, followed by a self-attention mechanism, and then another ResNet block. Training is performed on the four stacked instruments using the publicly available trainer from repository\footnote{\url{https://github.com/archinetai/audio-diffusion-pytorch-trainer}}.

\subsection{Implementation of the competitors}
\label{apdx:competitors}
In this section, we provide implementation details of the competitors.
We adopt the hyperparameters recommended by the authors tune them on each dataset if they are not provided.
The complete set of hyperparameters and there values for both image experiments and audio-sound separation can be found in the supplementary material under the folders \texttt{configs\slash experiments/sampler} and \texttt{configs\slash exp\_sound/sampler}.


\paragraph*{DPS.}
We implemented \citet[Algorithm~1]{chung2023diffusion} and selected the hyperparameters of each considered task  based on \citet[App.~D]{chung2023diffusion}.
We tuned the algorithm for the other tasks, namely, we use $\gamma = 0.2$ for JPEG $2\%$, $\gamma = 0.07$ for High Dynamic Range tasks, and $\gamma = 1$ for audio-source separation.

\paragraph*{DiffPIR.}
We implemented \citet[Algorithm 1]{zhu2023denoising} to make it compatible with our existing code base.
We adopt the hyperparameters recommended in the official, released version\footnote{\url{https://github.com/yuanzhi-zhu/DiffPIR}}.
We followed the guidelines in \citep[Eqn. (13)]{zhu2023denoising} to extend the algorithm to nonlinear problems.
However, we noticed that the algorithm diverges in these cases and we could not follow up as the paper and the released code lack examples of nonlinear problems.
\citet{zhu2023denoising} provides an FFT-based solution for the motion blur tasks which is only valid in the case of circular convolution.
Hence, and since we adapted the experimental setup of \citet{chung2023diffusion}, we do not run the algorithm on motion blur task as it uses convolution with reflect padding. For audio-source separation, we found that $\lambda = \mu = 1$ works best.

\paragraph*{DDNM.}
We adapted the implementation provided in the released code\footnote{\url{https://github.com/wyhuai/DDNM}}.
Namely, the authors provide classes, in the module \texttt{functions\slash svd\_operators.py} that implement the logic of the algorithm on each degradation operator separately.
The adaptation includes factorizing these classes to a single class to support all SVD linear degradation operators.
On the other hand, we notice \ddnm\ is unstable for operators whose SVD decomposition is prone to numerical errors, such as Gaussian Blur with wide convolution kernel. This results from the algorithm using the pseudo-inverse of the operator.

\paragraph*{RedDiff.}
We used the implementation of \reddiff\ available in the released code\footnote{\url{https://github.com/NVlabs/RED-diff}}.
For linear problems, we use the pseudo-inverse of the observation as an initialization of the variational optimization problem. 
On nonlinear problems, for which the pseudo-inverse of the observation is not available, we initialized the optimization with a sample from the standard Gaussian distribution. 

\paragraph*{PGDM.}
We opted for the implementation available in the \reddiff\'s repository as some the authors are co-authors of \pgdm as well.
Notably, the implementation introduces a subtle deviation from \citet[Algorithm 1]{song2022pseudoinverse}: in the algorithm's final step, the guidance term $g$ is scaled by $\a_t$ ($\sqrt{\a_t}$ in their notation) whereas the implementation scales it by $\a_{t-1}\a_t$.
This adjustment improves the algorithm for most tasks except for JPEG dequantization. We found that the original scaling by $\a_t$ is better in this case.

\paragraph*{PSLD.}
We implemented the \psld\ algorithm provided in \citet[Algorithm 2]{rout2024solving} and referred to the publicly available implementation\footnote{\url{https://github.com/LituRout/PSLD}} to set the hyperparameters of the algorithm for the different tasks.

\paragraph*{ReSample.}
We modified the original code\footnote{\url{https://github.com/soominkwon/resample}} provided by the authors to make its hyperparameters directly adjustable, namely, the tolerance $\varepsilon$ and the maximum number of iterations $N$ for solving the optimization problems related to hard data consistency, and the scaling factor for the variance of the stochastic resampling distribution $\gamma$.
We found the algorithm to be sensitive to $\varepsilon$ and that setting it to the noise level of the inverse problem yields the best reconstructions across tasks and noise levels.
On the other hand, we noticed that $\gamma$ has less impact on the quality of the reconstructions.
Finally, we set a threshold $N=200$ on the maximum number of gradient iterations to make the algorithm less computationally intensive.

\paragraph*{DAPS.}
We have the official codebase \footnote{\url{https://github.com/zhangbingliang2019/DAPS}}.
We referred to \citet[Table.~7]{zhang2024daps} to set the hyperparameters.
% For audio-source separation, we modified the code to pass in direclty the diffusion model as it was built and trained following the Variance Exploding setup of \citet{karras2022elucidating}.
For audio-source separation, we set $\sigma_{\max}$ and $\sigma_{\min}$ to match those of the sound model and adapted the Langevin stepsize \texttt{lr} and the standard deviation \texttt{tau} to the audio-separation task.

\paragraph*{PNP-DM.}
We adapted the implementation provided in the released code\footnote{\url{https://github.com/zihuiwu/PnP-DM-public/}}.
Specifically, we exposed the coupling parameter $\rho$ including its initial value, minimum value, and decay rate, as well as the number of Langevin steps and its step size.
The hyperparameters were set based on \citet[Table 3 and Table 4]{wu2024pnpdm}.
For inpainting tasks, while it is theoretically possible to perform the likelihood steps using Gaussian conjugacy \citep[Sec.~3.1]{wu2024pnpdm}, we found that using Langevin produced better results in practice. For example, the reconstructions in the left figure of \Cref{fig:pnpdm-conjugacy} are obtained by sampling exactly from the posterior whereas on the \rhs\ we use Langevin dynamics. 
% For audio source separation, we pass in the model directly to the algorithm, following the approach that we used in \daps.
Although the audio separation task is linear and hence the likelihood steps can be implemented exactly, we encountered similar challenges as in inpainting and therefore we used Langevin here as well.


\subsection{Experiments reproducibility}
Our code will be made available upon acceptange of the paper. In the anonymous codebase provided as companion of the paper we use $\sqrt{\a_t}$ instead of $\a_t$ to match the conventions of existing codebases.  All experiments were conducted on Nvidia Tesla V100 SXM2 GPUs. 
For the image experiments, we used $300$ images from the validation sets of \ffhq\ and \imagenet\ $256 \times 256$ that we numbered from $0$ to $299$.
The image number was used to seed the randomness of the experiments on that image.
For the audio source separation experiments, the \slakh\ test dataset has tracks named following the pattern \texttt{Track0XXXX}, where \texttt{X} represents a digit in $0-9$.
The number \texttt{XXXX} was used as the seed for the experiments conducted on each track.


% \subsection{Runtime and memory requirement comparison}
% We evaluate the runtime and GPU memory consumption for image experiments on the three considered diffusion model priors.
% Since not all algorithms support every task, we restrict the evaluation to commune tasks.
% \Cref{fig:runtime-gpu} presents the average runtime and GPU memory requirement over both samples and tasks.

% \begin{figure}[htb]
%     \centering
%     \subfigure{
%         \includegraphics[height=0.2\textwidth]{figures/runtime-gpu/ffhq-ldm.pdf}   
%     }
%     \subfigure{
%         \includegraphics[height=.2\textwidth]{figures/runtime-gpu/ffhq.pdf}
%         }
%     \subfigure{
%             \includegraphics[height=.2\textwidth]{figures/runtime-gpu/imagenet.pdf}
%         }
%     \caption{Comparison of the runtime (red bars -- left axis) and memory requirement (blue bars -- right axis) between the considered algorithms on \ffhq\ latent space (1\textsuperscript{st} row), \ffhq\ pixel space (2\textsuperscript{nd} row), and \imagenet\ (3\textsuperscript{rd} row).}
%     \label{fig:runtime-gpu}
% \end{figure}


\subsection{Extended results}
\label{apdx:extended-results}
We present the complete table with LPIPS, PSNR, and SSIM metrics for the image inverse problems experiment in \Cref{table:extended-ffhq-imagenet} for the \ffhq\ and \imagenet\ datasets, and in \Cref{table:extended-ffhq-ldm} for \ffhq\ LDM. Similarly, the complete results for the audio source separation experiments that include all competitors are provided in \Cref{table:extended-si-snri}.

From \Cref{table:extended-ffhq-imagenet}, one can note that \ddnm, \diffpir\ and \daps\ score better in PSNR and SSIM compared to \algo\; but score lower in LPIPS.
For most of the tasks we considered, one does not expect to recover an image very close to the reference and thus, metrics that perform pixel-wise comparisons are less relevant and favor images that are overly smooth.
We provide evidence for this in the gallery of images below where we compare qualitatively the outputs of our algorithm with those of the competitors.
It can be seen that our method provides reconstructions with ine-grained details that more coherent with the reference image.
Note for example that \ddnm, \diffpir\ and \daps\ outperform \algo\ in terms of PSNR and SSIM on the half mask task on \imagenet\ while failing to reconstruct the missing \rhs\ of the images. 



\input{appendices/_extended_px.tex}
\input{appendices/_extended_ldm.tex}
\input{appendices/_extended_sound.tex}
