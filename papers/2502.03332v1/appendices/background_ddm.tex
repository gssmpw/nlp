\section{Methodology details}
\subsection{Primer on Gibbs sampling}
\label{apdx-sec:gibbs}
In this section we lay out the basic properties of Gibbs sampling. We use measure-theoretic notation for conciseness. 

Let $\probmeas{0,1}{}{\rmd (\bx_0, \bx_1)}$ be a probability measure on $\rset^\dimx \times \rset^\dimx$. We denote by $\probmeas{0|1}{\bx_1}{\rmd \bx_0}$ and $\probmeas{1|0}{\bx_0}{\rmd \bx_1}$ the associated full conditionals and we write $\probmeas{0}{}{}$, $\probmeas{1}{}{}$ for its marginals. Define the %following 
transition kernels 
\begin{align*} 
    P_0(\rmd (\bx^\prime _0, \bx^\prime _1) | \bx_0, \bx_1) & \eqdef \probmeas{0|1}{\bx_1}{\rmd \bx^\prime _0} \delta_{\bx_1}(\rmd \bx^\prime _1),\\
    P_1(\rmd (\bx^\prime _0, \bx^\prime _1) | \bx_0, \bx_1) & \eqdef \probmeas{1|0}{\bx_0}{\rmd \bx^\prime _1} \delta_{\bx_0}(\rmd \bx^\prime _0).
\end{align*}
Each transition kernel updates only one coordinate at a time. A full update of the coordinates is obtained by composition of the kernels, \emph{i.e.}
$$ 
    P_0 P_1(\rmd (\bx^\prime _0, \bx^\prime _1) | \bx_0, \bx_1) \eqdef \int P_1(\rmd (\bx^\prime _0, \bx^\prime _1) | \tilde\bx_0, \tilde\bx_1) P_0(\rmd (\tilde\bx _0, \tilde\bx _1) | \bx_0, \bx_1) \eqsp.
$$  
 Each transition admits the joint distribution $\probmeas{0, 1}{}{}$ as stationary distribution, meaning that $\probmeas{0, 1}{}{\rmd (\bx_0, \bx_1)} = \int P_0(\rmd (\bx _0, \bx _1) | \bx^\prime _0, \bx^\prime _1)\, \probmeas{0, 1}{}{\rmd (\bx^\prime _0, \bx^\prime _1)}$. Indeed, this is seen by noting that 
\begin{align*}
    P_0(\rmd (\bx _0, \bx _1) | \bx^\prime _0, \bx^\prime _1) \probmeas{0, 1}{}{\rmd (\bx^\prime _0, \bx^\prime _1)}  & =  \probmeas{0|1}{\bx^\prime _1}{\rmd \bx _0} \delta_{\bx^\prime _1}(\rmd \bx _1) \probmeas{0, 1}{}{\rmd (\bx^\prime _0, \bx^\prime _1)}\\
    & =   \probmeas{0|1}{\bx^\prime _1}{\rmd \bx _0} \delta_{\bx^\prime _1}(\rmd \bx _1) \probmeas{0|1}{\bx^\prime _1}{\rmd \bx^\prime _0} \probmeas{1}{}{\rmd \bx^\prime _1}\\ 
    & = \probmeas{0|1}{\bx _1}{\rmd \bx _0} \probmeas{1}{}{\rmd \bx _1}  \probmeas{0|1}{\bx^\prime _1}{\rmd \bx^\prime _0}\delta_{\bx _1}(\rmd \bx^\prime _1),
\end{align*}
and then integrating both sides \wrt\ $(\bx^\prime _0, \bx^\prime _1)$. It then follows immediately that also $P_0 P_1$ admits $\probmeas{0,1}{}{}$ as stationary distribution. Letting $\big( (X^k _0, X^k _1) \big)_{k \in \nset}$ be a Markov chain with transition kernel $P_0 P_1$, the law of $(X^k _0, X^k _1)$ converges to $\probmeas{0,1}{}{}$ as $k \to \infty$ under mild conditions; see \cite{roberts1994simple}. 
\subsection{Full Gibbs conditionals}
\label{apdx-sec:conditionals}
In the main paper we consider the following data augmentation of the mixture $\hpost{t}{}{}$ \eqref{eq:posterior-approximation}
\begin{equation}
    \label{eq:extended-distr-normalized}
    \epost{0, s, t}{}{\bx_0, \bx_s, \bx_t} \\ = \pdata{0|s}{\bx_s}{\bx_0} \frac{\hpot{s}{\bx_s} \pdata{s|t}{\bx_t}{\bx_s} \pdata{t}{}{\bx_t}}{\int  \hpot{s}{\bx^\prime _s} \pdata{s|t}{\bx^\prime _t}{\bx^\prime _s} \pdata{t}{}{\bx^\prime _t} \, \rmd \bx_{s, t}}  \eqsp.
\end{equation}
From this definition it is straightforward to see that $\epost{0|s, t}{\bx_s, \bx_t}{\bx_0} = \pdata{0|s}{\bx_s}{\bx_0}$. In order to compute the full conditional $\epost{s|0, t}{\bx_0, \bx_t}{\bx_s}$ we use the identity 
\begin{equation} 
    \label{eq:bw-fw}
    \pdata{0|s}{\bx_s}{\bx_0} \pdata{s|t}{\bx_t}{\bx_s} \pdata{t}{}{\bx_t}= \pdata{0}{}{\bx_0} \fw{s|0}{\bx_0}{\bx_s} \fw{t|s}{\bx_s}{\bx_t},
\end{equation} 
from which it follows that 
\begin{align*} 
    \epost{s|0, t}{\bx_0, \bx_t}{\bx_s} & = \frac{\pdata{0|s}{\bx_s}{\bx_0} \hpot{s}{\bx_s} \pdata{s|t}{\bx_t}{\bx_s}}{\int \pdata{0|s}{\bx^\prime _s}{\bx_0} \hpot{s}{\bx^\prime _s} \pdata{s|t}{\bx_t}{\bx^\prime _s}\, \rmd \bx^\prime _s } \\
    & = \frac{ \fw{s|0}{\bx_0}{\bx_s} \hpot{s}{\bx_s} \fw{t|s}{\bx_s}{\bx_t}}{\int \fw{s|0}{\bx _0}{\bx^\prime _s} \hpot{s}{\bx^\prime _s} \fw{t|s}{\bx^\prime _s}{\bx _t}\, \rmd \bx^\prime _s } \\
    & = \frac{ \fw{s|0}{\bx_0}{\bx_s} \hpot{s}{\bx_s} \fw{t|s}{\bx_s}{\bx_t} \big/ \fw{t|0}{\bx_0}{\bx_t}}{\int \fw{s|0}{\bx _0}{\bx^\prime _s} \hpot{s}{\bx^\prime _s} \fw{t|s}{\bx^\prime _s}{\bx _t} \big/ \fw{t|0}{\bx_0}{\bx_t} \, \rmd \bx^\prime _s } \eqsp.
\end{align*}
Then, by noting that the bridge transition \eqref{eq:bridge} satisfies $\fw{s|0, t}{\bx_0, \bx_t}{\bx_s} = \fw{s|0}{\bx_0}{\bx_s} \fw{t|s}{\bx_s}{\bx_t} / \fw{t|0}{\bx_0}{\bx_t}$, we find that 
$$ 
\epost{s|0, t}{\bx_0, \bx_t}{\bx_s} = \frac{\hpot{s}{\bx_s} \fw{s|0, t}{\bx_0, \bx_t}{\bx_s}}{\int \hpot{s}{\bx^\prime _s} \fw{s|0, t}{\bx_0, \bx_t}{\bx^\prime _s} \, \rmd \bx^\prime _s}
$$
Finally, for the third conditional, using again the identity \eqref{eq:bw-fw}, we find that 
\begin{align*} 
    \epost{t|0, s}{\bx_0, \bx_s}{\bx_t} & = \frac{\pdata{0|s}{\bx_s}{\bx_0} \hpot{s}{\bx_s} \pdata{s|t}{\bx_t}{\bx_s} \pdata{t}{}{\bx_t}}{\int \pdata{0|s}{\bx _s}{\bx_0} \hpot{s}{\bx _s} \pdata{s|t}{\bx^\prime _t}{\bx _s} \pdata{t}{}{\bx^\prime _t}\, \rmd \bx^\prime _t } \\
    & = \fw{t|s}{\bx_s}{\bx_t} \eqsp.
\end{align*}
\subsection{Variational approximation}
\label{apdx-sec:vi-approx}
In this section we describe the variational approach of \citet{moufad2024variational}, which we use to fit a Gaussian variational approximation to $\post{s|0, t}{\bx_0, \bx_t}{}$ for fixed $(\bx_0, \bx_t)$. Similarly to the main paper we consider the variational approximation 
\begin{equation} 
    \smash{\vi{s|0, t}{} \eqdef \gauss\big(\vmu_{s|0, t}, \diag(\rme^{\vlstd_{s|0,t}})\big)}, 
    %\quad \mbox{and} \quad \vparam_{s|0, t} \eqdef (\vmu_{s|0, t}, \vlstd_{s|0,t}) \in \rset^{\dimx} \times \rset^{\dimx}. 
\end{equation}
and let $\vparam_{s|0, t} \eqdef (\vmu_{s|0, t}, \vlstd_{s|0,t}) \in \rset^{\dimx} \times \rset^{\dimx}$ denote the variational parameters. 
The reverse KL divergence writes, following definition \eqref{eq:bridge}, 
\begin{align} 
    \lefteqn{\kldivergence{\vi{s|0, t}{}}{\post{s|0, t}{\bx_0, \bx_t}{}}} \nonumber \\
    & \hspace{.7cm} = \int \log \frac{\vi{s|0, t}{\bx_s}}{\hpot{s}{\bx_s}{} \fw{s|0, t}{\bx_0, \bx_t}{\bx_s}} \, \vi{s|0, t}{\bx_s} \, \rmd \bx_s + \mathrm{C} \nonumber \\
    & \hspace{.7cm} = \pE _{\vi{s|0, t}{}} \left[ - \log \hpot{s}{\vX^\vparam _s} + \frac{\| \vX^\vparam _s - \big( \gamma_{t|s} \a_{s|0} \bx_0 + (1 - \gamma_{t|s}) \a^{-1} _{t|s} \bx_t \big) \|^2}{2 \std^2 _{s|0, t}} \right] - \frac{1}{2} \vlstd_{s|0, t}^T \mathbf{1}_d + \mathrm{C}^\prime. \label{eq:gradient-estimator} 
\end{align}
Using the reparameterization trick \cite{kingma2013auto} and plugging-in the neural network approximation $\hpot{s}{}[\param]$ of $\hpot{s}{}$, we obtain the gradient estimator  
\begin{multline*}
    \nabla_\vparam \mathcal{L}^s _t(\vparam; \bx_0, \bx_t, Z) \eqdef - \nabla_\vparam \log \hpot{s}{\vmu_{s|0, t} + \diag(\rme^{\vlstd_{s|0, t} })^{1/2} Z}[\param] \\
     + \nabla_\vparam \bigg[ \frac{\|\vmu_{s|0, t} + \diag(\rme^{\vlstd_{s|0, t} })^{1/2} Z - \big( \gamma_{t|s} \a_{s|0} \bx_0 + (1 - \gamma_{t|s}) \a^{-1} _{t|s} \bx_t \big)\|^2 }{2 \std^2 _{s|0, t}} - \frac{1}{2} \vlstd_{s|0, t}^T \mathbf{1}_d \bigg], 
\end{multline*}
where $Z \sim \gauss(\zero_\dimx, \Id_\dimx)$. We initialize the variational parameters with the mean and covariance of the bridge kernel \eqref{eq:bridge},  \emph{i.e.}, at initialization, $\vmu^0 _{s|0, t} \eqdef \gamma_{t|s} \a_{s|0} \bx_0 + (1 - \gamma_{t|s}) \a^{-1} _{t|s} \bx_t$ and $\vlstd^0 _{s|0, t} = \log \std^2 _{s|0, t} \Id_\dimx$.
The $\vifn$ routine is summarized in \Cref{algo:gauss_vi}. 
\begin{algorithm}
    \caption{$\vifn$ routine}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} vectors $(\bx_0, \bx_t)$, timesteps $(s, t)$, gradient steps $G$
        \STATE $\vmu \gets \gamma_{t|s} \a_{s|0} \bx_0 + (1 - \gamma_{t|s}) \a^{-1} _{t|s} \bx_t$
        \STATE $\vlstd \gets \log \std^2 _{s|0, t}$

        \FOR{$g=1$ to $G$}
            \STATE $Z \sim \gauss(\zero_\dimx, \Id_\dimx)$
            \STATE $(\vmu, \vlstd) \gets \mathsf{OptimizerStep}(\nabla _\vparam \mathcal{L}^s _t(\cdot, \bx_0, \bx_t, Z))$
        \ENDFOR
        \STATE $Z \sim \gauss(\zero_\dimx, \Id_\dimx)$
        \STATE {\bfseries Output:} $\vmu + \diag(\rme^{\vlstd / 2}) Z$
    \end{algorithmic}
    \label{algo:gauss_vi}
\end{algorithm}
\begin{remark} 
    While the expectation of the squared norm in \eqref{eq:gradient-estimator} can be computed exactly, we found that, in practice, doing so degraded the algorithmâ€™s performance, producing blurrier images compared to simply using a Monte Carlo estimator for the full expectation.
\end{remark} 
\begin{remark} 
    \label{rem:metropolis}
    The fact that the density of our target distribution can be computed approximately by plugging the denoiser approximation allows us to add a Metropolis--Hastings (MH) correction with approximate acceptance ratio. Indeed, once we fit the Gaussian approximation, we can improve the accuracy of our sampler by simulating a Markov chain $(\vX^k _s)_k$ where, given $\vX^k _s$, 
    $$ 
    \vX^{k+1} _s \sim M_s(\rmd \bx_s | \vX^k _s) \eqdef \int \vi{s|0,t}{z} \bigg[ r_s(\vX^k _s, z) \delta_{z}(\rmd \bx_s) + (1 - r_s(\vX^k _s, z) ) \delta_{\vX^k _s}(\rmd \bx_s)\bigg]\, \rmd z \eqsp,
    $$ 
    with
    $$ 
        r_s(\bx_s, \bx^* _s) = \mbox{min}\left(1, \frac{\hpot{s}{\bx^* _s} \fw{s|0, t}{\bx_0, \bx_t}{\bx^* _s} \vi{s|0,t}{\bx_s}}{\hpot{s}{\bx _s} \fw{s|0, t}{\bx_0, \bx_t}{\bx _s} \vi{s|0,t}{\bx^* _s}} \right) \eqsp.
    $$ 
\end{remark}
\subsection{Alternative data augmentation and sequence}
\label{apdx-sec:data-aug}
\paragraph{Data augmentation.} Our algorithm is based on one data-augmentation approach, but alternative augmentations could also be considered. 
Let $s \in \intset{1}{t-1}$. Then the most obvious and natural data augmentation involves simply marginalizing out the $\bx_0$ variable in \eqref{eq:extended-distr-normalized}, yielding 
$$ 
    \epost{s, t}{}{\bx_s, \bx_t} \propto \hpot{s}{\bx_s} \pdata{s|t}{\bx_t}{\bx_s} \pdata{t}{}{\bx_t} \eqsp.
$$ 
Its full conditionals are $\epost{s|t}{\bx_t}{\bx_s} \propto \hpot{s}{\bx_s} \pdata{s|t}{\bx_t}{\bx_s}$ and $\epost{t|s}{\bx_s}{\bx_t} = \fw{t|s}{\bx_s}{\bx_t}$. The first conditional is intractable for sampling, and we could approximate it with a Gaussian variational distribution, similar to our approach for $\epost{s|0, t}{\bx_0, \bx_t}{}$. Indeed, this is possible since $\nabla_{\bx_s} \log \epost{s|t}{\bx_t}{\bx_s} = \nabla_{\bx_s} \log \hpot{s}{\bx_s} + \nabla_{\bx_s} \log \pdata{s}{}{\bx_s} + \nabla_{\bx_s} \log \fw{t|s}{\bx_s}{\bx_t}$, which can then be approximated using the parametric approximations $\nabla \log \hpot{s}{\bx_s}[\param]$ and $\nabla \log \pdata{s}{}{\bx_s} \approx (- \bx_s + \a_s \denoiser{s}{}{\bx_s}[\param]) / (1 - \a^2 _s)$. 

The first drawback of this approach is that, in practice, it tends to degrade reconstruction quality---\emph{e.g.}, introducing blurriness---as $t$ tends to $0$, due to the poor approximation of the score near the data distribution. Additionally, beyond the loss of quality, we observe that it produces more incoherent reconstructions with noticeable artifacts. We hypothesize that this issue arises because the distribution we aim to approximately sample involves the prior transition $\pdata{s|t}{}{}$,  which can be highly multi-modal when $s \ll t$. This multi-modality may make the posterior $\epost{s|t}{\bx_t}{}$ more challenging to approximately sample from. On the other hand, when further conditioning on $\bx_0$, the sampling problem becomes more well-behaved, as we then target the posterior of a Gaussian distribution. Finally, while the score of $\epost{\smash{s|t}}{\bx_t}{\bx_s}$ can be easily approximated, its density cannot, preventing the use of a Metropolis--Hastings correction, unless we use the independent proposal $\pdata{s|t}{\bx_t}{}$. However, this approach is suboptimal, as it does not incorporate any information from the observation. This is not the case of the data-augmentation approach we use in \algo\ as we highlight in \Cref{rem:metropolis}. 
\paragraph{Alternative sequence.} An alternative to the mixture of posterior approximations \eqref{eq:posterior-approximation}, on which \algo\ is based, is the posterior formed as a mixture of likelihoods: 
$$ 
    \hpost{t}{}{\bx_t} = \frac{ \sum_{s = 1}^{t-1} \wght^s _t \hpot{t}{\bx_t}[s] \pdata{t}{}{\bx_t} }{\int \sum_{s = 1}^{t-1} \wght^s _t \hpot{t}{\bx^\prime _t}[s] \pdata{t}{}{\bx^\prime _t} \, \rmd \bx^\prime _t} \eqsp, 
$$ 
being the $\bx_t$-marginal of the extended distribution 
\begin{equation}
    \label{eq:mixture-pot-extended}
    \epost{0, \smbs, t}{}{s, \bx_0, \bz, \bx_t} \propto  \wght^s _t \pdata{0|s}{\bz}{\bx_0} \hpot{s}{\bz} \pdata{s|t}{\bx_t}{\bz} \pdata{t}{}{\bx_t} \eqsp.
\end{equation}
Now, let $(s, \bXy_0, \bZy, \bXy_t) \sim \epost{0, \smbs, t}{}{}$; then, conditionally on $s$, the distribution of $(\bXy_0, \bZy, \bXy_t)$ is $\epost{0, s, t}{}{}$, whereas 
$$
s | \bXy_0, \bZy, \bXy_t \, \sim \mbox{Categorical}\left( \left\{ \frac{\wght^\ell _t \hpot{\ell}{\bZy} \fw{\ell|0, t}{\bXy_0, \bXy_t}{\bZy}}{\sum_{k = 1}^{t-1} \wght^k _t \hpot{k}{\bZy} \fw{k|0, t}{\bXy_0, \bXy_t}{\bZy}}\right\}_{\ell = 1} ^{t-1} \right) \eqsp.
$$

A Gibbs sampler targeting \eqref{eq:mixture-pot-extended} is described in \Cref{algo:mixturepot-extended-gibbs}. It allows updating the index $s$ in an observation-driven fashion, but is unfortunately computationally expensive as we need to evaluate the denoiser at $\bZy$ in parallel for $t-1$ timesteps. A cheaper alternative could be to block the variables $(s, \bZy)$ and use an independent MH step to target their joint conditional distribution. Denoting by $\lambda$ the joint proposal distribution on $\intset{1}{t-1} \times \rset^\dimx$ used in this independent MH step, the probability of accepting a candidate $(s^*, \bz^*)$ is 
$$ 
    r_t\big( (s, \bz), (s^*, \bz^*)\big) = \mbox{min}\left( 1, \frac{\wght^{s^*} _{t} \hpot{s^*}{\bz^*}\fw{s^* | 0, t}{\bx_0, \bx_t}{\bz^*} \lambda(s, \bz)}{\wght^{s} _{t} \hpot{s}{\bz}\fw{s | 0, t}{\bx_0, \bx_t}{\bz} \lambda(s^{*}, \bz^{*})} \right) \eqsp.
$$ 
\begin{algorithm}[h]
    \caption{Gibbs sampler targeting \eqref{eq:extended}}
    % \small 
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} $(s^r, \bXy^r _0, \bZy^r, \bXy^r _t)$
        \STATE draw $s^{r+1} \sim \mbox{Categorical}\left( \left\{ \frac{\wght^\ell _t \hpot{\ell}{\bZy^r} \fw{\ell|0, t}{\bXy^r _0, \bXy^r _t}{\bZy^r}}{\sum_{k = 1}^{t-1} \wght^k _t \hpot{k}{\bZy^r} \fw{k|0, t}{\bXy^r _0, \bXy^r _t}{\bZy^r}}\right\}_{k = 1} ^{t-1} \right)$ 
        \STATE draw $\bZy^{r+1} \sim \epost{s^{r+1} |0, t}{\bXy^r _0, \bXy^r _t}{}$
        \STATE draw $\bXy^{r+1} _t \sim \fw{t|s^{r+1}}{\bZy^{r+1}}{}$ 
        \STATE draw $\bXy^{r+1} _0 \sim \pdata{0|s^{r+1}}{\bZy^{r+1}}{}$
    \end{algorithmic}
    \label{algo:mixturepot-extended-gibbs}
\end{algorithm}
\begin{remark} 
    \label{rem:mgdm-weight}
    Note that we could have used a similar data augmentation \eqref{eq:mixture-pot-extended} for the mixture used in \algo. This would yield the full conditional 
$$
    s | \bXy_0, \bZy, \bXy_t \, \sim \mbox{Categorical}\left( \left\{ \frac{\wght^\ell _t \epost{\ell|0, t}{\bXy_0, \bXy_t}{\bZy}}{\sum_{k = 1}^{t-1} \wght^k _t \epost{\ell|0, t}{\bXy_0, \bXy_t}{\bZy}}\right\}_{k = 1} ^{t-1} \right) \eqsp, 
$$
which is, however, intractable due to the normalizing constant involved in each $\epost{\ell|0, t}{}{}$. 
\end{remark}
\subsection{Related algorithms}
\label{apdx-sec:comparisons}
\paragraph{Comparison with \citet{zhang2024daps}} In this section we clarify the difference between \algo\ and the \daps\ algorithm \cite{zhang2024daps}, which shares some similarities with our approach. The sampling procedure in \daps\ relies on sequential approximate sampling from the joint distribution 
$$ 
    \tilde\pi^\obs _{0:T}(\bx_{0:T}) \eqdef \post{T}{}{\bx_T} \prod_{t = 0}^{T-1} \tilde{\pi}_{t|t+1}(\bx_t | \bx_{t+1}),  
$$ 
where 
\begin{equation}
    \label{eq:daps-bw}
    \tilde\pi^\obs _{t|t+1}(\bx_t | \bx_{t+1}) \eqdef \int \fw{t|0}{\bx_0}{\bx_t} \post{0|t+1}{\bx_{t+1}}{\bx_0} \, \rmd \bx_0
\end{equation}
and $\post{0|t+1}{\bx_{t+1}}{\bx_0} = \post{0}{}{\bx_0} \fw{t+1|0}{\bx_0}{\bx_{t+1}} \big/ \post{t+1}{}{\bx_{t+1}}$. From this definition it follows that 
$$ 
 \post{t}{}{\bx_t} = \int \tilde\pi^\obs _{t|t+1}(\bx_t | \bx_{t+1}) \post{t+1}{}{\bx_{t+1}} \, \rmd \bx_{t+1} \eqsp, 
$$ 
and hence that the marginals of the joint distribution $\tilde\pi^\obs _{0:T}$ are $(\post{t}{}{})_{t = 0}^T$. The canonical backward transition $\post{t|t+1}{\bx_{t+1}}{\bx_t} \propto \post{t}{}{\bx_t} \fw{t+1|t}{\bx_t}{\bx_{t+1}}$ has the alternative form 
$$ 
    \post{t|t+1}{\bx_{t+1}}{\bx_t} = \int \fw{t|0, t+1}{\bx_0, \bx_{t+1}}{\bx_t} \post{0|t+1}{\bx_{t+1}}{\bx_t} \, \rmd \bx_0 \eqsp,
$$
which differs from \eqref{eq:daps-bw} in the use of the bridge transition $q_{t|0, t+1}$ instead of the forward transition $q_{t|0}$. 
%and \eqref{eq:daps-bw} differs from it by the use of the forward transition $q_{t|0}$ in place of the bridge transition $q_{t|0, t+1}$. 

In order to sample from $\tilde\pi_{t|t+1}(\cdot | \bx_{t+1})$, one needs to first sample $X_0 \sim \post{0|t+1}{\bx_{t+1}}{}$ and then $X_t \sim \fw{t|0}{X_0}{}$. \daps\ performs the former step using Langevin dynamics on an approximation of $\post{0|t+1}{\bx_{t+1}}{}$. More specifically, the authors use the approximation 
$$ 
\post{0|t+1}{\bx_{t+1}}{\bx_0} \approx \frac{\pot{0}{\bx_0} \normpdf(\bx_0; \denoiser{t+1}{}{\bx_{t+1}}, r^2 _{t+1} \Id_\dimx)}{\int \pot{0}{\bx^\prime _0} \normpdf(\bx^\prime _0; \denoiser{t+1}{}{\bx_{t+1}}, r^2 _{t+1} \Id_\dimx) \, \rmd \bx^\prime _0} \eqsp,
$$ 
where $r^2 _{t+1}$ is a hyperparameter. This approximation follows by noting that $\post{0|t+1}{\bx_{t+1}}{\bx_0} \propto \pot{0}{\bx_0} \pdata{0|t+1}{\bx_{t+1}}{\bx_0}$ and using the Gaussian approximation of $\pdata{0|t+1}{\bx_{t+1}}{}$ proposed by \citet{song2022pseudoinverse}. The Langevin step is initialized with a sample obtained by discretizing the probability flow ODE \cite{song2021score} between $t+1$ and $0$. 

Both \algo\ and \daps\ perform full noising and denoising steps and operate in a similar manner in this respect (with the distinction that we use DDPM instead of the probability flow ODE). The first fundamental difference is that we sample, conditionally on \(\obs\) and at a random timestep $s$, by drawing from \(\epost{s|0, t}{\bx_0, \bx_t}{} \propto \hpot{s}{\bx_s} \fw{s|0, t}{\bx_0, \bx_t}{\bx_s}\). Unlike \daps, our method does not rely on a density approximation prior to applying an approximate sampler. The second main difference is the fact that within each denoising step, we can increase the number of Gibbs iterations to improve the overall performance, as demonstrated in \Cref{fig:scaling}. This is on top of the number of gradient steps that we use to fit the variational approximation and which enhance the performance when we increase them.  

On the other hand, \daps\ does not require the computation of vector-Jacobian products of the denoiser and is thus more efficient in terms of memory. However it requires many calls to the likelihood function, which can substantially increase the runtime if it is expensive to evaluate. For example, with a latent diffusion model, the runtime of DAPS is at least three times larger than that of \algo, \resample, and \psld. 
\paragraph{Comparison with \citet{moufad2024variational}}
The more recent {\sc{MGPS}} algorithm of \citet{moufad2024variational} is also related to \algo. Similarly to DAPS \cite{zhang2024daps}, their methodology relies on sampling approximately from the posterior transition $\post{t|t+1}{\bx_{t+1}}{}$ at each step of the backward denoising process. It builds on the following decomposition, which holds for all $s \in \intset{0}{t-1}$:
$$ 
    \post{t|t+1}{\bx_{t+1}}{\bx_t} = \int \fw{t|s, t+1}{\bx_s, \bx_{t+1}}{\bx_t} \post{s|t+1}{\bx_{t+1}}{\bx_s} \, \rmd \bx_s \eqsp.
$$ 
One step of {\sc{MGPS}} proceeds by first sampling from an approximation of the posterior transition $\post{s|t+1}{\bx_{t+1}}{}$ and then sampling from the bridge transition to return back to time $t$. The approximation of the posterior transition used in the {\sc{MGPS}} is 
\begin{equation} 
    \label{eq:mgps-approx}
    \post{s|t+1}{\bx_{t+1}}{\bx_s} \approx \frac{\hpot{s}{\bx_s}[\param] \pdata{s|t+1}{\bx_{t+1}}{\bx_s}[\param]}{\int \hpot{s}{\bx^\prime _s} \pdata{s|t+1}{\bx_{t+1}}{\bx^\prime _s}[\param] \, \rmd \bx^\prime _s} \eqsp.
\end{equation}
Here one can then choose $s$ to be sufficiently small to enhance the likelihood approximation, while still having an accurate Gaussian approximation of the transition $\pdata{s|t+1}{\bx_{t+1}}{}$. The authors demonstrate, using a solvable toy example, that this trade-off indeed exists; see \citep[Example 3.2]{moufad2024variational}. The approximate sampling step is then performed by fitting a Gaussian variational approximation to the approximation on the \rhs\ of \eqref{eq:mgps-approx}, similarly to what we do in \Cref{algo:midpoint-gibbs}.  

Both \algo\ and {\sc{MGPS}} leverage the same idea of using, at step time $t$, likelihood approximations at earlier steps $s < t$. While in {\sc{MGPS}} the time $s$ is set deterministically as a function of $t$, we sample it randomly. However, the main difference lies in the step where we sample conditionally on the observation $\obs$. Once the index $s$ is sampled we proceed with $R$ rounds of reverse KL minimization \wrt\ to a \emph{different} target distribution. Indeed, following \Cref{algo:midpoint-gibbs}, in the first round we seek to fit a distribution with density proportional to $\bx_s \mapsto \hpot{s}{\bx_s}[\param] \fw{s|0, t}{\textcolor{purple}{\vX^* _0}, \vX_t}{\bx_s}$, where $\vX^* _0$ is an output from the previous step of the algorithm. 
At step $r$, we fit $\bx_s \mapsto \hpot{s}{\bx_s}[\param] \fw{s|0, t}{\textcolor{purple}{\vX^{r-1} _0}, \vX^{r-1} _t}{\bx_s}$, where $\vX^{r-1} _0$ is sampled using a few DDPM steps starting from $\vX^{r-1} _s$ at time $s$ and $\vX^{r-1} _t \sim \fw{t|s}{\vX^{r-1} _s}{}$. On the other hand, {\sc{MGPS}} fits in a single round the distribution with density proportional to $\bx_s \mapsto \hpot{s}{\bx_s}[\param] \fw{s|0, t+1}{\textcolor{purple}{\denoiser{t+1}{}{\vX_{t+1}}[\param]}, \vX_{t+1}}{\bx_s}$, where $\vX_{t+1}$ is the output of the previous step. Finally, the authors report that the performance of {\sc{MGPS}} improves when the number of gradient steps is increased. In our case, we have two axes, Gibbs iterations $R$ and gradient steps, that allow us to improve the performance when more compute is available. 
