%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\definecolor{babypink}{rgb}{0.96, 0.76, 0.76} 
\definecolor{burntsienna}{rgb}{0.91, 0.45, 0.32}     % colors
\definecolor{crimson}{rgb}{0.86, 0.08, 0.24}
\definecolor{darkspringgreen}{rgb}{0.09, 0.45, 0.27}
\definecolor{deepcarrotorange}{rgb}{0.91, 0.41, 0.17}
%%%%%%%%%%%%%
% additional pacakge
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{nccmath}
\hypersetup{
     colorlinks=true,
     linkcolor=blue,
     filecolor=blue,
     citecolor=blue,
     urlcolor=blue,
    }
\usepackage{url}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{transparent}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{mathtools}
\usepackage[export]{adjustbox}
\usepackage{stmaryrd}
% \include{header.tex}
\usepackage{xifthen}
\usepackage{xargs}
\usepackage{amsmath,amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{titlesec}
\usepackage{multirow}
% \usepackage[hypertexnames=false]{hyperref}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%
% custom notations & commands
\input{def.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand\plabel[1]{\phantomsection\label{#1}}


\begin{document}

\twocolumn[
\icmltitle{A Mixture-Based Framework for Guiding Diffusion Models}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
    \icmlauthor{Yazid Janati}{equal,yyy}
    \icmlauthor{Badr Moufad}{equal,yyy}
    \icmlauthor{Mehdi Abou El Qassime}{yyy}\\
    \icmlauthor{Alain Durmus}{yyy}
    \icmlauthor{Eric Moulines}{yyy}
    \icmlauthor{Jimmy Olsson}{sch}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Ecole polytechnique}
% \icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{KTH University}

\icmlcorrespondingauthor{Yazid Janati, Badr Moufad}{first.last@polytechnique.edu}
% \icmlcorrespondingauthor{Badr Moufad}{}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
  Denoising diffusion models have driven significant progress in the field of Bayesian inverse problems. Recent approaches use pre-trained diffusion models as priors to solve a wide range of such problems, only leveraging inference-time compute and thereby eliminating the need to retrain task-specific models on the same dataset. To approximate the posterior of a Bayesian inverse problem, a diffusion model samples from a sequence of intermediate posterior distributions, each with an intractable likelihood function. This work proposes a novel mixture approximation of these intermediate distributions. Since direct gradient-based sampling of these mixtures is infeasible due to intractable terms, we propose a practical method based on Gibbs sampling. We validate our approach through extensive experiments on image inverse problems, utilizing both pixel- and latent-space diffusion priors, as well as on source separation with an audio diffusion model. The code is available at \url{https://www.github.com/badr-moufad/mgdm}.
\end{abstract}


%%%%%%%%%%%%
%%%%%%%%%%%%
\section{Introduction}
Inverse problems occur when a signal $X$ of interest must be inferred from an incomplete and noisy observation $Y$, a challenge frequently encountered in diverse fields such as weather forecasting, image reconstruction (\emph{e.g.}, tomography or black-hole imaging), and speech processing.  Such problems are typically ill-posed, 
%as the observations can correspond to infinitely many possible signals. However, most of these solutions are either physically implausible or lack practical relevance, 
making it essential to incorporate additional constraints, regularization techniques, or prior knowledge to arrive at meaningful and realistic solutions. 

The Bayesian framework, in conjunction with generative modeling, offers a systematic approach to the challenges associated with inverse problems. Prior knowledge about the signal of interest, often represented through samples from its underlying distribution $\pdata{0}{}{}$, can be leveraged to train a generative model $\pdata{0}{}{}[\param]$ that acts as a prior. 
%The a priori knowledge about the signal of interest, usually materialized with samples from its underlying distribution $\pdata{0}{}{}$, can be used to learn a generative model $\pdata{0}{}{}[\param]$ that serves as prior. 
By combining it with the conditional density $\pot{0}{\bx}$ of the observation given the signal, deduced from the form of the inverse problem at hand, we can compute the posterior distribution. 
Samples drawn from this posterior encapsulate plausible solutions that harmonize prior knowledge with the observed data.
%Samples drawn from this posterior represent plausible solutions that reconcile both the prior knowledge and the observed data. 
One straightforward approach to approximate sampling from the posterior distribution involves constructing a paired dataset of i.i.d. signals and observations, $(\bX_i, Y_i)_{i = 1}^N$, where $\bX_i \sim \pdata{0}{}{}$ and $Y_i \sim g_0(\cdot | \bX_i)$, and learning a direct mapping \cite{dong2015image} or generative model \cite{ledig2017photo,isola2017image}. The latter, when queried with multiple independent noise samples alongside an observation, %$\obs$, 
generates a diverse set of potential reconstructions. However, this approach is inherently \emph{task-specific}, delivering reliable reconstructions only when the conditional distribution of the observation remains unchanged at test time. As a result, it cannot straightforwardly adapt to unseen tasks with the same prior. Adaptation to a new task can only be achieved by retraining a new generative model. 

An increasingly popular approach consists in learning a generative model only for the prior $\pdata{0}{}{}$, and then leveraging inference-time compute to solve any inverse problem for which the likelihood function $\bx \mapsto \pot{0}{\bx}$ is provided in a closed form. This strategy eliminates the need for expensive and inefficient task-specific training. Initially explored with generative models such as variational autoencoders and generative adversarial networks \cite{xia2022gan}, this framework has recently been extended to denoising diffusion models (DDMs) \cite{song2021score,kadkhodaie2020solving,kawar2021snips,kawar2022denoising,chung2023diffusion,song2022pseudoinverse,daras2024survey}, which are the focus of the present paper.

DDMs \cite{sohl2015deep,song2019generative,ho2020denoising} achieve state-of-the-art generative performance across a wide range of domains. At their core is a forward noising process that transforms the data distribution $\pdata{0}{}{}$ into a Gaussian distribution. A generative model is then learned by  reversing this noising process. With a specific parameterization of the backward process, which converts noise into data samples, training the generative model reduces to approximating denoisers for each noise level introduced during the forward process. Recent methods for training-free posterior sampling aim to approximate the denoisers for the posterior distribution, enabling the use of diffusion models for sampling \cite{ho2022video,chung2023diffusion,song2022pseudoinverse}. A posterior distribution denoiser can be decomposed into two terms: the prior denoiser at the same noise level (provided by a pre-trained diffusion model) and the gradient of the log-likelihood of the observation conditioned on the current noisy sample. The latter term, which is intractable, is what guides the samples during the denoising process towards the posterior distribution. Various approximations for this gradient term have been proposed. However, they are often crude and require significant adjustments and heuristics to ensure stability and satisfactory performance. When applied to latent diffusion models, they often demand additional, model-specific adjustments 
\cite{rout2024solving}.\\


\textbf{Our contribution.}\, In this paper, we present a principled method that circumvents these issues by introducing a new approximation of the likelihood term, paired with a sampling scheme based on Gibbs sampling \cite{geman1984stochastic}. 
Our key observation is that multiple approximations can be derived for each likelihood term at a fixed noise level using a simple identity that it satisfies.
However, the scores of these new likelihood approximations are not available in closed form, preventing us from deriving a direct posterior denoiser approximation by combining, through a mixture, the different likelihood approximations. We overcome this limitation by constructing a mixture approximation of the intermediate posterior distributions defined by the diffusion model for the original posterior. 
Our algorithm,  \algoname\ (\algo), proceeds by sequentially sampling from these mixtures using Gibbs sampling. This is enabled by a carefully designed data augmentation scheme that ensures straightforward Gibbs updates. A key advantage of our approach is its adaptability to available computational resources. Specifically, the number of Gibbs iterations acts as a tunable parameter, allowing substantial improvements with increased inference-time compute. \algo\ demonstrates strong empirical performance across 10 image-restoration tasks involving both pixel-space and latent-space diffusion models, as well as in musical source separation, even matching the performance of supervised methods. 
%A key advantage of our approach is its flexibility in scaling with available computational resources. Specifically, the number of Gibbs iterations serves as a tunable parameter: by increasing the number of iterations, the algorithm can improve its performance when more inference-time compute is available. 
%The strong empirical performance of \algo\ is demonstrated on 10 image-restoration tasks, with both pixel-space and latent-space diffusion models, as well as musical source separation, and is further validated through comparisons with 10 competing methods.

\section{Background}
\input{sections/background.tex}
\input{sections/method.tex}
\section{Experiments}
\label{sec:experiments}
\input{sections/experiments.tex}

\section{Conclusion}
We have developed a novel posterior sampling scheme for denoising diffusion priors. The proposed algorithm proceeds by sequentially sampling, using a Gibbs sampler, from a sequence of mixture approximations of the smoothed posteriors. Our experiments show that \algo\ not only matches but often surpasses state-of-the-art performance and reconstruction quality across various tasks.
Furthermore, we have demonstrated that the Gibbs sampling perspective allows favorable performance improvement with inference-time compute scaling.

This work has certain limitations that open avenues for further exploration. While we outperform the state-of-the-art on most tasks and remains competitive overall on latent diffusion,  we still fall short of what we achieve with pixel-space diffusion. We believe that bridging this gap requires a more careful selection of the weight sequence. More broadly, an observation-driven approach to sampling the index could further enhance \algo. A second limitation is that our methodology does not extend to ODE-based samplers or DDIM, and adapting related ideas to these methods is an interesting research direction. Finally, like all existing methods relying on \eqref{eq:dps}, our approach incurs a higher memory cost compared to unconditional diffusion. It remains an open question whether the vector-Jacobian product can be eliminated without compromising performance.
% \clearpage


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\clearpage
\newpage
\section*{Acknowledgements}
The work of Y.J. and B.M. has been supported by Technology Innovation Institute (TII), project Fed2Learn. The work of Eric Moulines has been partly funded by the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. This work was granted access to the HPC resources of IDRIS under the allocation 2025-AD011015980 made by GENCI.
\bibliography{bibliography.bib}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn
\input{appendices/background_ddm.tex}
% \input{appendices/transition_kernels.tex}
% \input{appendices/gauss_case.tex}
\section{Experiments details}
\input{appendices/exp_details.tex}
\input{appendices/reconstructions.tex}
\label{apdx-sec:visual-reconstructions}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
