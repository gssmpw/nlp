\section{Guidance with mixtures}
We now present our main contribution: a novel density approximation of the smoothed posteriors $\post{t}{}{}$. Since their scores are intractable, gradient-based samplers cannot be directly applied. Thus, we develop a Gibbs sampling scheme targeting a data augmentation of our smoothed posterior approximation, marking our second key contribution. 

In the next two sections we develop an algorithm for the ideal generative model, \emph{i.e.}, we assume that we have at hand the true marginals $\pdata{t}{}{}$ and backward transitions $\pdata{s|t}{}{}$; then, in \Cref{sec:implementation}, we provide a practical implementation involving the learned model. 

\subsection{Guidance approximation} 
We begin by extending the likelihood approximation in \eqref{eq:dps} introduced by \citet{ho2022video, chung2023diffusion}. 
First, note that by combining \eqref{eq:back_chapman} with \eqref{eq:pot-defn}, we find that $\pot{t}{}$ satisfies
\[
\pot{t}{\bx_t} = \int \pot{s}{\bx_s} \pdata{s|t}{\bx_t}{\bx_s} \, \rmd \bx_s, 
\]
for all $t\in \intset{1}{T}$ and $s \in \intset{0}{t-1}$. Thus, we obtain $t-1$ different approximations of $\pot{t}{}$ by simply setting,  for $s \in \intset{1}{t-1}$,
\begin{equation}
    \label{eq:hpot-def}
%\textstyle 
\hpot{t}{\bx_t}[s] \eqdef \int \hpot{s}{\bx_s}  \pdata{s|t}{\bx_t}{\bx_s} \, \rmd \bx_s \eqsp, \quad t \geq 2 \eqsp,
\end{equation}
where $\hpot{s}{}$ denotes the counterpart of \eqref{eq:dps}, with the learned denoiser $\denoiser{s}{}{}[\param]$ replaced by the true denoiser $\denoiser{s}{}{}$. 
In contrast to $\hpot{t}{\cdot}[\param]$ in \eqref{eq:dps}, the scores of these approximations remain intractable even when the approximate model is used, as they involve an intractable integral.
%over $\bx_s$.
Instead, we take a different approach and use $ \hpot{t}{\cdot}[s] $ to define density approximations 
\begin{equation}
    \label{eq:mixture-component}
\hpost{t}{}{\bx_t}[s] \eqdef \frac{\hpot{t}{\bx_t}[s] \pdata{t}{}{\bx_t}}{\int \hpot{t}{\bx^\prime_t}[s] \pdata{t}{}{\bx^\prime_t} \, \rmd \bx^\prime_t} 
\end{equation}
of the smoothed posteriors $\post{t}{}{}$. Since we have $t-1$ such approximations, we consider a weighted mixture approximation of $\post{t}{}{}$ defined, for $t \geq 1$, as 
\begin{equation}
    \label{eq:posterior-approximation}
    \hpost{t}{}{\bx_t}  \eqdef \sum_{s = 1}^{t-1} \wght^s _t \hpost{t}{}{\bx_t}[s] \eqsp, 
\end{equation}
where $(\wght^s _t)_{s = 1} ^{t-1}$ are time-dependent weights and $\sum_{s = 1}^{t-1} \wght^s _t = 1$ with $\wght^s _t \geq 0$. 
%defined for $t \geq 1$. 
Then, to sample approximately from $\post{0}{}{}$ we can use a sequential sampling procedure that runs through the intermediate distributions $\hpost{T}{}{}, \dotsc, \hpost{1}{}{}$. Similar sequential sampling procedures from posterior sequences different from $(\post{t}{}{})_t$ have also been utilized in previous works. For instance, \citet{wu2023practical,rozet2023score} use $\hpost{t}{}{\bx_t} \propto \hpot{t}{\bx_t} \pdata{t}{}{\bx_t}$. Our approach differs from these prior works by employing a mixture-based formulation with non-standard approximations of $\post{t}{}{}$. 
However, sampling from $\post{t}{}$ remains a non-trivial challenge. Indeed, a naive procedure would consist in sampling an index $s \sim \mbox{Categorical}(\{ \wght^\ell _t \}_{\ell = 1}^{t-1})$ and then use an approximate sampler only for $\hpost{t}{}{}[s]$. However, we must address the intractability of both $\hpost{t}{}{\cdot}[s]$ and its score. In the next section, we propose a method that fully overcomes these challenges. The discussion on selecting the weight sequence $(\wght^s_t)_{s=1}^{t-1}$ is postponed until after presenting the algorithm, more specifically at the beginning of \Cref{sec:experiments}.

\subsection{Data augmentation and Gibbs sampling}
% \jimmy{Maybe `Data augmentation and Gibbs sampling'?}

We first detail how to sample from a single component $\hpost{t}{}{}[s]$ of the mixture \eqref{eq:posterior-approximation} for given $t \in \intset{2}{T}$ and $s \in \intset{1}{t-1}$. 
Consider first the extended distribution
\begin{multline}
    \label{eq:extended}
    \epost{0, s, t}{}{\bx_0, \bx_s, \bx_t} \\ \propto \pdata{0|s}{\bx_s}{\bx_0} \hpot{s}{\bx_s} \pdata{s|t}{\bx_t}{\bx_s} \pdata{t}{}{\bx_t} \eqsp.
\end{multline}
From the definitions in \eqref{eq:posterior-approximation} and \eqref{eq:hpot-def} it follows that $\hpost{t}{}{}[s]$ is the $\bx_t$-marginal of \eqref{eq:extended}, \emph{i.e.}, 
\[
\hpost{t}{}{\bx_t}[s] = \int \epost{0, s, t}{}{\bx_0, \bx_s, \bx_t} \, \rmd \bx_0 \, \rmd \bx_s.
\]
To sample approximately from $\hpost{t}{}{}[s]$, we employ a sampler targeting $\epost{0, s, t}{}{}$ and retain only the $\bx_t$-coordinate of its output. 

Specifically, we use a Gibbs sampler (GS) \cite{geman1984stochastic, casella1992explaining,gelfand2000gibbs},  which, in this context, constructs a Markov chain $(\bXy^r _0, \bXy^r _s, \bXy^r _t)_{r\in\nset}$ having $\epost{0, s, t}{}{}$ as its stationary distribution. Denote by $\epost{\smash{s|0, t}}{}{}$, $\epost{t|0, s}{}{}$, and $\epost{0|s, t}{}{}$ its three full conditionals given by 
 \begin{equation*}
    \label{eq:conditionals}
    \begin{cases}
        \begin{aligned}
            \epost{\smash{s|0, t}}{\bx_0, \bx_t}{\bx_s} & \txts = \frac{\hpot{s}{\bx_s} \fw{s|0, t}{\bx_0, \bx_t}{\bx _s}}{\int \hpot{s}{\bx^\prime _s} \fw{s|0, t}{\bx_0, \bx_t}{\bx^\prime _s} \, \rmd \bx^\prime _s} \eqsp,\\
            \epost{t|0, s}{\bx_0, \bx_s}{\bx_t} & = \fw{t|s}{\bx_s}{\bx_t}, \\
            \epost{0|s, t}{\bx_s, \bx_t}{\bx_0} & = \pdata{0|s}{\bx_s}{\bx_0}.
        \end{aligned}
    \end{cases}
\end{equation*}
The proof of this fact is postponed to \Cref{apdx-sec:conditionals}.
%single-state conditionals. 
Then, one step of the associated (deterministic scan) GS 
%, given $(\bXy^r _0, \bXy^r_s, \bXy^r _t)$, 
is described in
\Cref{algo:extended-gibbs}.

\begin{algorithm}[h]
    \caption{Gibbs sampler targeting \eqref{eq:extended}}
    % \small 
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} $(\bXy^r _0, \bXy^r _s, \bXy^r _t)$
        \STATE draw $\bXy^{r+1} _s \sim \epost{\smash{s|0, t}}{\bXy^r _0, \bXy^r _t}{}$ \label{step:bridge}
        \STATE draw $\bXy^{r+1} _t \sim \fw{t|s}{\bXy^{r+1} _s}{}$ \hfill {\small\texttt{//noising}}
        \STATE draw $\bXy^{r+1} _0 \sim \pdata{0|s}{\bXy^{r+1} _s}{}$ \label{step:denoising} \hfill {\small\texttt{//denoising}}  
    \end{algorithmic}
    \label{algo:extended-gibbs}
\end{algorithm}

Since \eqref{eq:extended} admits $\hpost{t}{}{}[s]$ as marginal, the process $(\bXy^r _t)_{r\in\nset}$ will, at stationarity of $(\bXy^r _0, \bXy^r _s, \bXy^r _t)_{r\in\nset}$, have $\hpost{t}{}{}$ as a marginal distribution. We provide basic background on Gibbs sampling in \Cref{apdx-sec:gibbs} and refer the reader to \cite{casella1992explaining}. 

It is clear from \Cref{algo:extended-gibbs} that only the update of $\bXy_s^r$ depends on the observation $\obs$, while the updates of the remaining components are sampled via (i) a noising step involving the forward transition \eqref{eq:def_gauss_transition}, which can be performed exactly, and (ii) a denoising step involving the prior diffusion model, which can be approximated using the pre-trained model. 

Finally, to target the mixture \eqref{eq:posterior-approximation}, we first sample the mixture index $s \sim \mbox{Categorical}\big(\{ \wght^\ell _t \}_{\ell=1}^{t-1}\big)$, which determines the component of the mixture $\hpost{t}{}{\bx_t}$. %$\hpost{t}{}{}[s]$. 
Next, we apply \Cref{algo:extended-gibbs} $R$ times to update the remaining coordinates, treating $s$ a fixed, and output the result $\bXy^R _t$. 
Note that an alternative to our method would be to consider a Gibbs sampler for which one of its marginal is directly the mixture 
\eqref{eq:posterior-approximation} incorporating also the mixture index $s$ as a state. However, this would then require sweeping over all states $(\bXy_0, \dotsc, \bXy_t)$, rendering it computationally expensive and impractical. We discuss other possible data augmentations and their limitations in \Cref{apdx-sec:data-aug}.
%This contrasts with the Gibbs sampler applied to the mixture itself by including the mixture index in the data augmention, which requires updating the mixture index $s$ at every iteration. This, in turn, involves sweeping over all states $(\bXy_0, \dotsc, \bXy_t)$, making the approach computationally expensive and impractical.

\begin{algorithm}[t]
    \caption{\algoname}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} Timesteps $(t_i)_{i = 1} ^K$ with $t_1 > 1$ and $t_K = T$, Gibbs repetitions $\gibbsReps$, DDPM steps $M$, gradient steps $G$, probabilities $\{\wght^\ell _{t_i}\}^{2, t_{i-1}} _{i=K, \ell=1}$
        \STATE $\vX_{t_K} \sim \gauss(\zero_d, \Id_d)$
        \STATE $\vX_{0} \gets \denoiser{t_K}{}{\vX_{t_K}}[\param]$, $\, \vX^* _0 \gets \vX_0$

        \FOR{$i=K$ to $2$}
            \STATE $s \sim \mbox{Categorical}(\{\wght^\ell _{t_i}\}_{\ell = 1}^{t_{i-1}})$
            \STATE $\vX _0 \gets \vX^* _0$ \label{line:x0init}
            \STATE $\vX _{t_i} \sim \fw{t_i|0, t_{i+1}}{\vX^{*} _0, \vX _{t_{i+1}}}{}$ \label{line:xtinit}
            \FOR{$r = 1$ {\bfseries to} $R$}
                \STATE $\vX_s \gets \vifn\big(\vX_0, \vX_{t_i}, s, G\big)$ \label{line:gaussvi} \hfill {\small\texttt{//see \ref{apdx-sec:vi-approx}}}
                \STATE $\vX _0 \gets \ddimfn_{}(\vX _s, s, M)$ \label{line:ddpm}
                \STATE $\vX _{t_i} \sim \fw{t_i|s}{\vX _s}{}$
            \ENDFOR
            \STATE $\vX^* _{0} \gets \vX _0$
        \ENDFOR
        \STATE {\bfseries Output:} $\bX^* _0$
    \end{algorithmic}
    \label{algo:midpoint-gibbs}
\end{algorithm}

\begin{figure} 
    \centering
    \includegraphics[width=.45\textwidth]{figures/x0_plot.pdf}
    \captionsetup{font=small}
    \caption{Evolution of $\vX^* _0$ throughout the iterations for \algo\ and \daps\ \cite{zhang2024daps}. }
    \label{fig:running_x0}
\end{figure}
\label{sec:implementation}
\subsection{Practical implementation}
For simplicity, we present the algorithm in the case where we progressively sample from each $\hpost{t}{}{}$ for $t \in \intset{2}{T}$. In practice, however, we subsample a small number $K$ of timesteps $(t_i)_{i = K}^1$, with $t_1 > 1$ and $t_K = T$, and apply the algorithm only to $( \hpost{t_i}{}{} )_{i = K}^i$. 


The denoising step in \Cref{algo:extended-gibbs} can be approximated by sampling from the learned diffusion model. To reduce runtime, we again subsample a small number of timesteps $\{s_i\}_{i = 0}^M \subset \intset{0}{s-1}$, ensuring that  $s_0 = 0$ and $s_M = s$. We then generate  $(X_{s_i})_{i = 0}^M$ by sampling iteratively $X_{s_i} \sim \pdata{s_i | s_{i+1}}{X_{s_{i+1}}}{\cdot}[\param]$ and retaining only $X_{s_0}$.
This operation is referred to as $\ddimfn(\cdot, s, M)$ on Line~\ref{line:ddpm} in \Cref{algo:midpoint-gibbs}.
As for the step involving $\hpost{s|0, t}{}{}$,
we follow \citet{moufad2024variational} and sample approximately by fitting a Gaussian variational approximation.
More specifically, given $(\bx_0, \bx_t)$, we draw from the Gaussian variational approximation $\smash{\vi{s|0, t}{} \eqdef \gauss\big(\vmu_{s|0, t}, \diag(\rme^{\vlstd_{s|0,t}})\big)}$ where the parameters $\vparam_{s|0, t} \eqdef (\vmu_{s|0, t}, \vlstd_{s|0,t}) \in \rset^{\dimx} \times \rset^{\dimx}$ are obtained by optimizing the right-hand side of 
%\rhs\ of 
\begin{multline*}
    \kldivergence{\vi{s|0,t}{}}{\post{s|0,t}{\bx_0, \bx_t}{}} \\ \approx - \pE \big[ \log \hpot{s}{\vX^{\vparam} _s}[\param] \big] + 
    \kldivergence{\vi{s|0,t}{}}{\fw{s|0,t}{\bx_0, \bx_t}{}},  
\end{multline*}
where $\vX^\vparam _s \sim \vi{s|0, t}{}$. The gradient of this quantity can be estimated straightforwardly using the reparameterization trick \cite{kingma2013auto}. The initial parameters $\vparam_{s|0, t}$ are set to the mean and covariance of $\fw{s|0,t}{\bx_0, \bx_t}{\cdot}$ defined in \eqref{eq:bridge}. 
This step corresponds to the $\vifn$ routine in \Cref{algo:midpoint-gibbs} and is detailed in \Cref{apdx-sec:vi-approx}. Regarding the initialization of the GS for $\hpost{t}{}{}$, we use the output of the previous GS targeting $\hpost{t+1}{}{}$; see Lines \ref{line:x0init} and \ref{line:xtinit} in \Cref{algo:midpoint-gibbs}. We maintain a running variable $\vX^* _0$, which is iteratively updated and serves as the initialization for the other variables at the beginning of each loop iteration. It is also the output of the algorithm. Indeed, note that the last distribution to which we apply the GS is $\epost{0, 1, 2}{}{\bx_0, \bx_1, \bx_2}$ of which the $\bx_0$-marginal is proportional to $\pdata{0}{}{\bx_0} \int \hpot{1}{\bx_1} \fw{1|0}{\bx_0}{\bx_1} \, \rmd \bx_1$. 
Since the Gaussian density $\fw{1|0}{\bx_0}{\cdot}$ has a very small variance and $\hpot{1}{} \approx \pot{0}{}$, we may assume that $ \int \hpot{1}{\bx_1} \, \fw{1|0}{\bx_0}{\bx_1} \rmd \bx_1 \approx \pot{0}{\bx_0}$ and hence that the posterior $\post{0}{}{}$ of interest is approximately the $\bx_0$-marginal of the last extended distribution. As a result, we can take the $\bx_0$-coordinate of the output of the last GS, which is $\vX_0$ and hence $\vX^* _0$, as an approximate sample from $\post{0}{}{}$. In the first row of \Cref{fig:running_x0} we display the evolution of $\vX^* _0$ throughout the iterations with a DDM pre-trained on the \ffhq\ dataset. It is seen that the algorithm reaches a plausible reconstruction of $\obs$ rather fast, at $t = 800$ with $T = 1000$, and then spends the remaining iterations refining the details.  As a comparison, the DAPS algorithm proposed by \citet{zhang2024daps}, which displayed in the second row, also maintains a running variable at time $0$ that serves as output to the algorithm.
%  \jimmy{Maybe just: `As a comparison, the output of the DAPS algorithm proposed by \citet{zhang2024daps} is displayed in the second row.'?}

% The choice of weight sequence $\{\wght^\ell _t\}^{2, t-1} _{t=T, \ell=1}$ plays an important role in the algorithm's performance. Intuitively, we have that $\hpot{t}{}[s] \approx \pot{t}{}$ when $s \approx 0$ which suggests that the weights, for all $t \in \intset{2}{T}$, should be set to $0$ after a certain threshold so that $s$ is sampled near $0$. We found however that this strategy does not translate to good performance for our algorithm. On the other hand, sampling the index uniformly on $\intset{1}{t-1}$ provides a faster mixing. We provide empirical evidence by implementing exactly \Cref{algo:midpoint-gibbs} on a Gaussian toy example, \emph{i.e.} we sample exactly from $\epost{s|0, t}{}{}{}$ in Line~\ref{line:gaussvi}. We then estimate the sliced Wasserstein distance to the posterior. We compare the extreme case where the weight sequence is such that $\wght^1 _t = 1$ for all $t \in \intset{2}{T}$ against uniform sampling, \emph{i.e} $\wght^\ell _t = (t-1)^{-1}$ for all $\ell \in \intset{1}{t-1}$. The results are displayed in \Cref{fig:gaussian-example}. Specifically, it is seen that on this specific example, \algo\ with uniform sampling approaches the posterior faster and with fewer Gibbs steps. We provide more details about this experiment in \Cref{apdx-sec:gibbs}. 
% On high-dimensional image datasets, we observe that when $s$ is sampled near $0$ at all iterations, the algorithm struggles to overcome the errors that accumulate at initialization and thus provides suboptimal reconstructions. We provide an illustration in Appendix~XX. 
\section{Related works}
\begin{figure*}[!t]
    \centering 
    \includegraphics[width=\textwidth]{figures/fig_reconstruction.pdf}
    \captionsetup{font=small}
    \caption{\algo\ sample images for various tasks on \imagenet\ (left) and \ffhq\ (right) datasets.}
    \label{fig:main-reconstructions}
\end{figure*}
\paragraph{Alternative likelihood approximations.} In addition to this work, several other papers introduce alternative approximations of $\pot{t}{}$. \cite{song2022pseudoinverse} proposes a Gaussian approximation of $\pdata{0|t}{}{}$ with mean given by the denoiser $\denoiser{t}{}{}[\param]$ and covariance being left as a hyperparameter. For linear inverse problems with Gaussian noise, the likelihood $\pot{0}{}$ can be integrated exactly against this Gaussian approximation, providing an alternative approximation of $\pot{t}{}$. \citet{finzi2023user, stevens2023removing, boys2023tweedie} use that the covariance of $\pdata{0|t}{\bx_t}{\cdot}$ is proportional to the Jacobian of the denoiser \cite{meng2021estimating}. Computing the score of the resulting likelihood approximation, for linear inverse problems, is prohibitively expensive. To mitigate this, these works and subsequent ones assume that the Jacobian of the denoiser is constant with respect to $\bx_t$. Despite this simplification, the score approximation still involves an expensive matrix inversion. \citet{boys2023tweedie} use diagonal approximation of the covariance based on its row sums. \citet{rozet2024learning} use conjugate gradient to perform the matrix inversion efficiently. For general likelihoods $\pot{0}{}$, \citet{song2023loss} use Gaussian approximations of \citet{song2022pseudoinverse} to estimate $\pot{t}{}$ using a standard Monte Carlo approach. For latent diffusion models, \citet{rout2024solving} apply the approximation in \eqref{eq:dps} together with a regularization term that penalizes latent variables deviating from fixed points of the decoder-encoder composition. \citet{moufad2024variational} propose a general method for both vanilla and latent space diffusion models. At step $t$ of the diffusion process they first sample, at an intermediate timestep $s < t$, a state conditionally on $\obs$ with the approximation \eqref{eq:dps}, before returning back to the timestep $t$. In \Cref{apdx-sec:comparisons} we explain in more details how the present work differs from this method. 

\paragraph{Asymptotically exact methods.} \citet{trippe2023diffusion,wu2023practical, cardoso2024monte, dou2024diffusion, corenflos2024conditioning,li2024derivative} use the sequential Monte Carlo (SMC) framework to construct an empirical approximation of the posterior distribution represented by $N$ samples. The samples undergo transitions guided by user-defined updates, are reweighted using an appropriate importance weight, and are subsequently resampled to focus computational effort on the most promising candidates. The performance of these methods improves by scaling the number of samples $N$, which impacts both the memory requirement and compute time. As evidenced by the experiments in the next section, our method improves by increasing the number of Gibbs steps, which impacts only the runtime. 

\paragraph{Gibbs sampling approaches.} The recent works \cite{wu2024principled, xu2024provably} on \pnpdm\ also propose a Gibbs sampling-inspired algorithm. They consider (within the variance exploding framework) the distribution sequence $(\tilde{\pi}^\obs _t)_{t = 0} ^T$, where each distribution $\tilde{\pi}^\obs _t(\bx_t) \propto \pot{\textcolor{purple}{0}}{\bx_t} \pdata{t}{}{\bx_t}$ is the $\bx_t$-marginal of the extended distribution 
$$\tilde{\pi}^\obs _{0, t}(\bx_0, \bx_t) \propto \pot{0}{\bx_t} \pdata{0}{}{\bx_0} \fw{t|0}{\bx_0}{\bx_t}\eqsp.$$ 
As its full conditionals are $\smash{\tilde\pi^\obs _{0|t}(\bx_0|\bx_t)} = \pdata{0|t}{\bx_t}{\bx_0}$ and $\tilde\pi^\obs _{t|0}(\bx_t|\bx_0) \propto \pot{0}{\bx_t} \fw{t|0}{\bx_0}{\bx_t}$, the GS targeting this joint distribution also proceeds with a prior denoising step. On the other hand, sampling from $\tilde\pi^\obs _{t|0}(\cdot|\bx_0)$ can be performed exactly when $\pot{0}{}$ is the likelihood of a linear inverse problem with Gaussian noise, since $\fw{t|0}{\bx_0}{\cdot}$ is a Gaussian distribution. For more general problems, this step can be implemented using MCMC methods; see \emph{e.g.} \citep[Algorithms 3 \& 4]{xu2024provably}. Compared to our algorithm, \pnpdm\ has a lower memory footprint because it does not require a vector-Jacobian product as it uses the likelihood $\pot{0}{}$ instead of $\hpot{t}{}$.
However, as we show in the next section, this comes at the cost of performance, especially when using latent diffusion models. The \textsc{RePaint} algorithm \cite{lugmayr2022repaint}, which applies to noiseless linear inverse problems, uses noising and denoising steps repeatidly and can also be viewed as a variant of a specific Gibbs sampler. Finally, the recently proposed \daps\, \cite{zhang2024improving} can also be related to a Gibbs sampler targeting a specific sequence of distributions. Further details and comparisons to {\algo} are provided in \Cref{apdx-sec:comparisons}.

%This is discussed in further details in the extended related works and compared to \algo; see \Cref{apdx-sec:comparisons}. 

% \paragraph{Jacobian-free methods.} Finally, beyond the gradient-step (GS)-based approaches we have discussed, numerous alternative methods have been proposed in the literature that do not rely on vector-Jacobian-based approximations. Among these, replacement-based methods \cite{song2019generative, kadkhodaie2020solving, song2021score, kawar2021snips, kawar2022denoising, wang2023zeroshot, zhu2023denoising}, which are specifically designed for linear inverse problems, introduce modifications to the backward diffusion process. These methods incorporate a linear transformation of the observations directly into the denoising step, effectively guiding the diffusion process toward reconstructions that are more consistent with the observed data. This approach ensures plausible and data-aligned solutions in challenging linear inverse settings.

% Finally, \citet{mardani2024a}... (expand further as needed).% Diffusion model \citep{ho2020denoising} defined trained to generate samples of $\prior$.
% \begin{equation}
%     \label{eq:prior_forward}
%     \begin{aligned}
%         \fwtrans{0:n}{}{}(\bx_{0:n})
%             & = \prior(\bx_0) \prod_{k = 0}^{n-1} \fwtrans{k+1|k}{\bx_k}{\bx_{k+1}} \\
%             & \approx \bwp{n}{x}{}(\bx_n) \prod_{k = 0}^{n-1} \bwp{k|k+1}{\bx_{k+1}}{\bx_k}
%     \end{aligned}
%     \eqsp.
% \end{equation}
% with $n$ being large enough that $\bwp{n}{x}{}(\bx_n)$ can be confused with a standard Gaussian.
% Letting $(\alpha_k)_{k=1}^n$ being the noise scheduler of the diffusion model, we denote for $i < j$, $\acp{j|i} = \acp{j} / \acp{i}$.


% \paragraph{Problem setup.}
% We focus on sampling from the unnormalized density
% \begin{equation}
%     \label{eq:posterior_def}
%     \post{}{\bx} \propto \potn{}{\bx} \prior(\bx) % / \normconst
%     \eqsp,
% \end{equation}
% where $\potn{}{}: \rset^{\dimx} \to \rset$ is a \emph{non-negative} and \emph{differentiable} potential that \emph{can be evaluated pointwise}, $\prior$ is a prior distribution.
% We assume access to a pre-trained diffusion model $\bwp{}{}{}$ for the prior $\prior$.

% We abuse the notation and drop the parameter $\theta$ and denote $\bwp{0:n}{}{}$, the approximate reverse, by $\bw{0:n}{}{}$.


% \paragraph{The target posterior as a DDPM.}
% Applying DDPM forward process to the target distribution $\post{}{}$, we can define its associated joint distribution
% \begin{equation}
%     \label{eq:posterior_forward}
%     \post{0:n}{\bx_{0:n}} \eqdef \post{}{\bx_0} \prod_{k = 0}^{n-1} \fwtrans{k+1|k}{\bx_k}{\bx_{k+1}}
%     \eqsp.
% \end{equation}
% The time-reversed decomposition writes
% \begin{equation}
%     \label{eq:pi-backward}
%     \post{0:n}{\bx_{0:n}} \eqdef \post{n}{\bx_n} \prod_{k = 0}^{n-1} \pibw{k|k+1}{\bx_{k+1}}{\bx_{k}} \eqsp,
% \end{equation}
% where the transition kernels and the marginals are defined, for $i < j \in \nset$ and $k \in \nset$, by
% \begin{equation}
%     \label{eq:pi-backward-transition}
%     \begin{aligned}
%         \pibw{i|j}{\bx_{j}}{\bx_{i}} = \post{i}{\bx_i}\fwtrans{j|i}{\bx_i}{\bx_{j}} \big/ \post{j}{\bx_{j}},
%         \quad \\
%         \post{k}{\bx_{k}} \propto \int \potn{}{\bx_0} \fwtrans{k|0}{\bx_0}{\bx_k} \prior(\bx_0) \rmd \bx_0
%         \eqsp.
%     \end{aligned}
% \end{equation}
% Using the expression of the backward kernels, namely, $\fwmarg{k}{\bx_k} \bw{0|k}{\bx_k}{\bx_0} \eqdef \prior(\bx_0) \fwtrans{k|0}{\bx_0}{\bx_k}$, we can rewrite the marginals of $\post{0:n}{}$ as
% \begin{equation}
%     \begin{aligned}
%         \post{k}{\bx_{k}} & \propto \potn{k}{\bx_k} \fwmarg{k}{\bx_k},
%         \\
%         \potn{k}{\bx_k} & \eqdef \int \potn{}{\bx_0} \bw{0|k}{\bx_k}{\rmd \bx_0}
%         \eqsp.
%     \end{aligned}
% \end{equation}
% The potentials $\{ \potn{k}{} \}$ hence defined foresee the value of the potential given $\bx_k$ if we were to follow the backward of the prior.

% Notice that for a $\midpoint \in \intset{0}{k}$, we can decompose $\bw{0|k}{\bx_{k}}{\bx_0}$ as $\int \bw{0|\midpoint}{\bx_{\midpoint}}{\bx_0} \bw{\midpoint|k}{\bx_{k}}{\rmd \bx_\midpoint}$.
% This enables to link past and future potentials thought the denoising density.
% \begin{equation}
%     \label{eq:potn-recursion}
%     \potn{k}{\bx_k} = \int \potn{\midpoint}{\bx_{\midpoint}} \bw{\midpoint|k}{\bx_{k}}{\rmd \bx_\midpoint}
%     \eqsp.
% \end{equation}

% % \begin{proposition}
% %     Let $k \in \nset$.
% %     Pick a time $0 \leq \midpoint < k$, then the potential $\potn{k}{}$ can be decomposed as
% %     \begin{equation}
% %         \potn{k}{\bx_k} = \int \potn{\midpoint}{\bx_{\midpoint}} \bw{\midpoint|k}{\bx_{k}}{\rmd \bx_\midpoint}
% %         \eqsp.
% %     \end{equation}
% % \end{proposition}
% % \begin{proof}
% %     Notice that $\bw{0|k}{\bx_{k}}{\bx_0}$ can be decomposed as $\int \bw{0|\midpoint}{\bx_{\midpoint}}{\bx_0} \bw{\midpoint|k}{\bx_{k}}{\rmd \bx_\midpoint}$.
% % \end{proof}

% \paragraph{Multistage Gibbs Sampler.}
% For given $k \in \nset$, let us pick $\initpoint \leq \midpoint \leq k$, then using \Cref{eq:potn-recursion}, we can write
% \begin{equation}
%     \begin{aligned}
%         \post{k}{}{}(\bx_{k})
%             & \propto \potn{k}{\bx_k} \fwtrans{k}{}{}(\bx_k) \\
%             & \txts \propto  \big[ \int \potn{\midpoint}{\bx_{\midpoint}} \bw{\midpoint|k}{\bx_{k}}{\rmd \bx_\midpoint} \big] \ \fwtrans{k}{}{}(\bx_k)
%             % \\
%             % & \txts \propto \big[ \int \potn{\midpoint}{\bx_{\midpoint}} \fwtrans{\midpoint|\initpoint, k}{\bx_\initpoint, \bx_k}{\rmd \bx_\midpoint} \bw{\initpoint|k}{\bx_{k}}{\rmd \bx_\initpoint} \big] \ \fwtrans{k}{}{}(\bx_k) \\
%     \end{aligned}
%     \eqsp.
% \end{equation}
% Using the fact that
% $$
% \txts \fwtrans{\midpoint|k}{\bx_{k}}{\bx_\midpoint} = \int \fwtrans{\midpoint|\initpoint, k}{\bx_\initpoint, \bx_k}{\bx_\midpoint} \fwtrans{\initpoint|k}{\bx_{k}}{\rmd \bx_\initpoint}
% $$
% the marginal $\post{k}{}{}$ can be further decomposed as
% \begin{equation}
%     \label{eq:marginal-unrolled}
%     \txts \big[ \int \potn{\midpoint}{\bx_{\midpoint}} \fwtrans{\midpoint|\initpoint, k}{\bx_\initpoint, \bx_k}{\rmd \bx_\midpoint} \bw{\initpoint|k}{\bx_{k}}{\rmd \bx_\initpoint} \big] \ \fwtrans{k}{}{}(\bx_k)
% \end{equation}
% % where we used the fact that $\fwtrans{\midpoint|k}{\bx_{k}}{\bx_\midpoint} = \int \fwtrans{\midpoint|\initpoint, k}{\bx_\initpoint, \bx_k}{\bx_\midpoint} \fwtrans{\initpoint|k}{\bx_{k}}{\rmd \bx_\initpoint}$.
% Based on that, define the following distribution whose marginal is $\post{k}{}$ after integrating over $\bx_{\initpoint}, \bx_{\midpoint}$ thanks to \eqref{eq:marginal-unrolled}
% \begin{multline}
%     \mgibbs{\initpoint, \midpoint, k}{}{}(\bx_\initpoint, \bx_\midpoint, \bx_k)
%         \propto \potn{\midpoint}{\bx_{\midpoint}} \\
%             \fwtrans{\midpoint|\initpoint, k}{\bx_\initpoint, \bx_k}{\bx_\midpoint}
%             \fwtrans{\initpoint|k}{\bx_{k}}{\bx_\initpoint} \fwtrans{k}{}{}(\bx_k)
%     \eqsp.
% \end{multline}
% The conditionals of the distribution hence defined are
% \begin{align}
%     \label{eq:gibbs-step-1}
%     \mgibbs{\midpoint |\initpoint, k}{\bx_\initpoint, \bx_k}{\bx_{\midpoint}}
%         & \propto \potn{\midpoint}{\bx_{\midpoint}} \fwtrans{\midpoint|\initpoint, k}{\bx_\initpoint, \bx_k}{\bx_\midpoint}
%         \eqsp, \\
%     \label{eq:gibbs-step-2}
%     \mgibbs{\initpoint |\midpoint, k}{\bx_{\midpoint}, \bx_k}{\bx_\initpoint}
%         & = \mgibbs{\initpoint |\midpoint}{\bx_{\midpoint}}{\bx_\initpoint}
%         = \fwtrans{\initpoint|\midpoint}{\bx_{\midpoint}}{\bx_\initpoint}
%         \eqsp, \\
%     \label{eq:gibbs-step-3}
%     \mgibbs{k |\initpoint, \midpoint}{\bx_\initpoint, \bx_{\midpoint}}{\bx_k}
%         & = \mgibbs{k |\midpoint}{\bx_{\midpoint}}{\bx_k}
%         = \fwtrans{k|\midpoint}{\bx_{\midpoint}}{\bx_k}
%     \eqsp.
% \end{align}
% Refer to XXX for the details of the derivation.
% Notice that, \eqref{eq:gibbs-step-1} is a Gaussian kernel, namely the bridge kernel, weighted by the potential, \eqref{eq:gibbs-step-2} is the denoising step, and \eqref{eq:gibbs-step-3} is the noising step.
% In particular, the conditionals of the distribution hence defined do not involve simultaneously the potential and the denoising density and hence are amenable to Gibbs Sampling \citep[Chapter 10]{robert1999mc-stats}.

% \input{sections/_algo.tex}


% To sample from $\post{k}{}$, we proceed by recursively sampling from the conditionals,
% \begin{align}
%     \label{eq:gibbs-step-1}
%     \mgibbs{\midpoint |\initpoint, k}{\bx_\initpoint, \bx_k}{\bx_{\midpoint}}
%         & \propto \potn{\midpoint}{\bx_{\midpoint}} \fwtrans{\midpoint|\initpoint, k}{\bx_\initpoint, \bx_k}{\bx_\midpoint}
%         \eqsp, \\
%     \label{eq:gibbs-step-2}
%     \mgibbs{\initpoint |\midpoint, k}{\bx_{\midpoint}, \bx_k}{\bx_\initpoint}
%         & =  \fwtrans{\initpoint|\midpoint}{\bx_{\midpoint}}{\bx_\initpoint}
%         \eqsp, \\
%     \label{eq:gibbs-step-3}
%     \mgibbs{k |\initpoint, \midpoint}{\bx_\initpoint, \bx_{\midpoint}}{\bx_k}
%         & = \fwtrans{k|\midpoint}{\bx_{\midpoint}}{\bx_k}
%     \eqsp.
% \end{align}
% Notice that, \eqref{eq:gibbs-step-1} is a Gaussian kernel, namely the bridge kernel, weighted by the potential, \eqref{eq:gibbs-step-2} is the denoising density, and \eqref{eq:gibbs-step-3} is the noising process.

% \paragraph{Slipt Gibbs Sampling.}
% We use a Split Gibbs Sampling (SGS) scheme to target the transition kernels $\pibw{k|k+1}{\bx_{k+1}}{\bx_{k}}$, in particular, we target,
% \begin{equation}
%     \pibw{\midpoint, k|k+1}{\bx_{k+1}}{\bx_{\midpoint}, \bx_{k}}
%         \propto \potn{\midpoint}{\bx_{\midpoint}} \fwtrans{\midpoint|k}{\bx_{k}}{\bx_\midpoint} \fwtrans{{k}|k+1}{\bx_{k+1}}{\bx_{k}}
%     \eqsp.
% \end{equation}
% For that, we define the augmented distribution $\augpibw^z$ with the coupling term $\normpdf(z; \bx_\midpoint, \mu \Id)$ where $\mu > 0$
% \begin{equation}
%     \augpibw^z_{\midpoint, k|k+1}(\bx_\midpoint, \bx_k, z| \bx_{k+1})
%         \propto \potn{\midpoint}{z} \fwtrans{\midpoint|k}{\bx_{k}}{\bx_\midpoint} \fwtrans{{k}|k+1}{\bx_{k+1}}{\bx_{k}}
%         \exp( -\frac{1}{2\mu} \| \bx_{\midpoint} - z \|^2 )
%     \eqsp.
% \end{equation}
% Then, we proceed by recursively drawing each variable given the others
% \begin{align}
%     \augpibw^z_{\midpoint|k, k+1}(\bx_\midpoint| z, \bx_k, \bx_{k+1})
%         & \propto \fwtrans{\midpoint|k}{\bx_{k}}{\bx_\midpoint} \exp( -\frac{1}{2\mu} \| \bx_{\midpoint} - z \|^2 ) \eqsp,\\
%     \augpibw^z_{k|\midpoint, k+1}(\bx_k| z, \bx_\midpoint, \bx_{k+1})
%         & = \fwtrans{{k}|\midpoint, k+1}{\bx_\midpoint, \bx_{k+1}}{\bx_{k}} \eqsp, \\
%     \augpibw^z_{|\midpoint, k, k+1}(z| \bx_k, \bx_\midpoint, \bx_{k+1})
%         & \propto \potn{\midpoint}{z} \exp( -\frac{1}{2\mu} \| z - \bx_{\midpoint} \|^2 )
%     \eqsp.
% \end{align}
% Now, we dive into the details of each setup.