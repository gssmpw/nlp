@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-04-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\M9K7WXNX\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{xai_2_0_survey,
  title={Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions},
  author={Longo, Luca and Brcic, Mario and Cabitza, Federico and Choi, Jaesik and Confalonieri, Roberto and Del Ser, Javier and Guidotti, Riccardo and Hayashi, Yoichi and Herrera, Francisco and Holzinger, Andreas and others},
  journal={Information Fusion},
  volume={106},
  pages={102301},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{lundberg_unified_2017,
	OPTaddress = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {A unified approach to interpreting model predictions},
	isbn = {978-1-5108-6096-4},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 31st {Int.} {Conf.} on {Neural} {Information} {Processing} {Systems}},
    url = {https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
	OPTpublisher = {Curran Associates Inc.},
	author = {Lundberg, Scott M. and Lee, Su-In},
	month = dec,
	year = {2017},
	keywords = {notion},
	pages = {4768--4777},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\WQVUN6D6\\Lundberg et Lee - 2017 - A unified approach to interpreting model predictio.pdf:application/pdf},
}

@inproceedings{ribeiro_why_2016,
	OPTaddress = {New York, NY, USA},
	series = {{KDD} '16},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://doi.org/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 22nd {ACM} {SIGKDD} {Int.} {Conf.} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	keywords = {notion, black box classifier, explaining machine learning, interpretability, interpretable machine learning},
	pages = {1135--1144},
}

@online{modernBERT,
  author = { Answer.AI, LightOn},
  title = {Finally, a Replacement for BERT},
  year = 2024}

@online{gemma,
  author = {Gemma Team, Google DeepMind},
  title = {Gemma: Open Models Based on Gemini
Research and Technology},
  year = 2024}

@inproceedings{SIQA,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}

@inproceedings{CQA,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    abstract = "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.",
}

@inproceedings{ross_explaining_2021,
	address = {Online},
	title = {Explaining {NLP} {Models} via {Minimal} {Contrastive} {Editing} ({MiCE})},
	url = {https://aclanthology.org/2021.findings-acl.336},
	doi = {10.18653/v1/2021.findings-acl.336},
	urldate = {2023-04-27},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Ross, Alexis and Marasović, Ana and Peters, Matthew},
	month = aug,
	year = {2021},
	pages = {3840--3852},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\XDK3J8F2\\Ross et al. - 2021 - Explaining NLP Models via Minimal Contrastive Edit.pdf:application/pdf},
}

@inproceedings{wu_polyjuice_2021,
	address = {Online},
	title = {Polyjuice: {Generating} {Counterfactuals} for {Explaining}, {Evaluating}, and {Improving} {Models}},
	shorttitle = {Polyjuice},
	url = {https://aclanthology.org/2021.acl-long.523},
	doi = {10.18653/v1/2021.acl-long.523},
	abstract = {While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70\% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {Int.} {Joint} {Conf.} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel},
	month = aug,
	year = {2021},
	keywords = {notion},
	pages = {6707--6723},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\HH4P89BU\\Wu et al. - 2021 - Polyjuice Generating Counterfactuals for Explaini.pdf:application/pdf},
}

@inproceedings{fern_text_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Text {Counterfactuals} via {Latent} {Optimization} and {Shapley}-{Guided} {Search}},
	url = {https://aclanthology.org/2021.emnlp-main.452},
	doi = {10.18653/v1/2021.emnlp-main.452},
	abstract = {We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model's prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent white-box and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2021 {Conf.} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Fern, Xiaoli and Pope, Quintin},
	month = nov,
	year = {2021},
	keywords = {notion},
	pages = {5578--5593},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\I3QB8V95\\Fern et Pope - 2021 - Text Counterfactuals via Latent Optimization and S.pdf:application/pdf},
}

@misc{noauthor_optuna_nodate,
	title = {Optuna {\textbar} {Proc.} of the 25th {ACM} {SIGKDD} {Int.} {Conf.} on {Knowledge} {Discovery} \& {Data} {Mining}},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330701},
	urldate = {2023-04-27},
	keywords = {notion},
}

@inproceedings{akiba_optuna_2019,
	address = {Anchorage AK USA},
	title = {Optuna: {A} {Next}-generation {Hyperparameter} {Optimization} {Framework}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Optuna},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330701},
	doi = {10.1145/3292500.3330701},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 25th {ACM} {SIGKDD} {Int.} {Conf.} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	month = jul,
	year = {2019},
	keywords = {notion},
	pages = {2623--2631},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\YXX2RKZM\\Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimizat.pdf:application/pdf},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\DV4LUEKK\\1409.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\HIUKZ88E\\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf},
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	volume = {1409},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.},
	journal = {ArXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Y.},
	month = sep,
	year = {2014},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\3NFI94NV\\Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf},
}

@article{barredo_arrieta_explainable_2020,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {1566-2535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = jun,
	year = {2020},
	keywords = {Fairness, Data Fusion, Deep Learning, Explainable Artificial Intelligence, notion, Accountability, Comprehensibility, Interpretability, Machine Learning, Privacy, Responsible Artificial Intelligence, Transparency},
	pages = {82--115},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\ESJTSE69\\Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\ILPU8UW7\\S1566253519308103.html:text/html},
}

@inproceedings{bibal_is_2022,
	address = {Dublin, Ireland},
	title = {Is {Attention} {Explanation}? {An} {Introduction} to the {Debate}},
	shorttitle = {Is {Attention} {Explanation}?},
	url = {https://aclanthology.org/2022.acl-long.269},
	doi = {10.18653/v1/2022.acl-long.269},
	abstract = {The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bibal, Adrien and Cardon, Rémi and Alfter, David and Wilkens, Rodrigo and Wang, Xiaoou and François, Thomas and Watrin, Patrick},
	month = may,
	year = {2022},
	keywords = {notion},
	pages = {3889--3900},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\5VZ5HZ29\\Bibal et al. - 2022 - Is Attention Explanation An Introduction to the D.pdf:application/pdf},
}

@article{guidotti_counterfactual_2022,
	title = {Counterfactual explanations and how to find them: literature review and benchmarking},
	issn = {1573-756X},
	shorttitle = {Counterfactual explanations and how to find them},
	url = {https://doi.org/10.1007/s10618-022-00831-6},
	doi = {10.1007/s10618-022-00831-6},
	abstract = {Interpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classifiers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Data Mining and Knowledge Discovery},
	author = {Guidotti, Riccardo},
	month = apr,
	year = {2022},
	keywords = {Interpretable machine learning, notion, Contrastive explanations, Counterfactual explanations, Explainable AI},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\XXUNZR2W\\Guidotti - 2022 - Counterfactual explanations and how to find them .pdf:application/pdf},
}

@article{jelinek_perplexitymeasure_2005,
	title = {Perplexity—a measure of the difficulty of speech recognition tasks},
	volume = {62},
	issn = {0001-4966},
	url = {https://doi.org/10.1121/1.2016299},
	doi = {10.1121/1.2016299},
	abstract = {Using counterexamples, we show that vocabulary size and static and dynamic branching factors are all inadequate as measures of speech recognition complexity of finite state grammars. Information theoretic arguments show that perplexity (the logarithm of which is the familiar entropy) is a more appropriate measure of equivalent choice. It too has certain weaknesses which we discuss. We show that perplexity can also be applied to languages having no obvious statistical description, since an entropy‐maximizing probability assignment can be found for any finite‐state grammar. Table I shows perplexity values for some well‐known speech recognition tasks. Perplexity Vocabulary Dynamic Phone Word size branching factorIBM‐Lasers 2.14 21.11 1000 1000IBM‐Raleigh 1.69 7.74 250 7.32CMU‐AIX05 1.52 6.41 1011 35},
	number = {S1},
	urldate = {2023-04-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Jelinek, F. and Mercer, R. L. and Bahl, L. R. and Baker, J. K.},
	month = aug,
	year = {2005},
	keywords = {notion},
	pages = {S63},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\IBB3WM6U\\Jelinek et al. - 2005 - Perplexity—a measure of the difficulty of speech r.pdf:application/pdf;Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\Z3I8KZMD\\Perplexity-a-measure-of-the-difficulty-of-speech.html:text/html},
}

@inproceedings{laugel_dangers_2019,
	address = {Macao, China},
	title = {The {Dangers} of {Post}-hoc {Interpretability}: {Unjustified} {Counterfactual} {Explanations}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {The {Dangers} of {Post}-hoc {Interpretability}},
	url = {https://www.ijcai.org/proceedings/2019/388},
	doi = {10.24963/ijcai.2019/388},
	abstract = {Post-hoc interpretability approaches have been proven to be powerful tools to generate explanations for the predictions made by a trained blackbox model. However, they create the risk of having explanations that are a result of some artifacts learned by the model instead of actual knowledge from the data. This paper focuses on the case of counterfactual explanations and asks whether the generated instances can be justiﬁed, i.e. continuously connected to some ground-truth data. We evaluate the risk of generating unjustiﬁed counterfactual examples by investigating the local neighborhoods of instances whose predictions are to be explained and show that this risk is quite high for several datasets. Furthermore, we show that most state of the art approaches do not differentiate justiﬁed from unjustiﬁed counterfactual examples, leading to less useful explanations.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Proc. of the {Twenty}-{Eighth} {Int.} {Joint} {Conf.} on {Artificial} {Intelligence}},
	publisher = {Int. Joint Conf. on Artificial Intelligence Organization},
	author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
	month = aug,
	year = {2019},
	keywords = {notion},
	pages = {2801--2807},
	file = {Laugel et al. - 2019 - The Dangers of Post-hoc Interpretability Unjustif.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\K6ELCVF9\\Laugel et al. - 2019 - The Dangers of Post-hoc Interpretability Unjustif.pdf:application/pdf},
}

@inproceedings{laugel2023achieving,
  title={Achieving diversity in counterfactual explanations: a review and discussion},
  author={Laugel, Thibault and Jeyasothy, Adulam and Lesot, Marie-Jeanne and Marsala, Christophe and Detyniecki, Marcin},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1859--1869},
  year={2023}
}

@inproceedings{lewis_bart_2020,
	address = {Online},
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {https://aclanthology.org/2020.acl-main.703},
	doi = {10.18653/v1/2020.acl-main.703},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
	month = jul,
	year = {2020},
	keywords = {notion},
	pages = {7871--7880},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\PRAGLEBE\\Lewis et al. - 2020 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf},
}

@inproceedings{maas_learning_2011,
	address = {Portland, Oregon, USA},
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},
	url = {https://aclanthology.org/P11-1015},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	month = jun,
	year = {2011},
	pages = {142--150},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\US4ITWT5\\Maas et al. - 2011 - Learning Word Vectors for Sentiment Analysis.pdf:application/pdf},
}

@misc{madaan_plug_2022,
	title = {Plug and {Play} {Counterfactual} {Text} {Generation} for {Model} {Robustness}},
	url = {http://arxiv.org/abs/2206.10429},
	doi = {10.48550/arXiv.2206.10429},
	abstract = {Generating counterfactual test-cases is an important backbone for testing NLP models and making them as robust and reliable as traditional software. In generating the test-cases, a desired property is the ability to control the test-case generation in a flexible manner to test for a large variety of failure cases and to explain and repair them in a targeted manner. In this direction, significant progress has been made in the prior works by manually writing rules for generating controlled counterfactuals. However, this approach requires heavy manual supervision and lacks the flexibility to easily introduce new controls. Motivated by the impressive flexibility of the plug-and-play approach of PPLM, we propose bringing the framework of plug-and-play to counterfactual test case generation task. We introduce CASPer, a plug-and-play counterfactual generation framework to generate test cases that satisfy goal attributes on demand. Our plug-and-play model can steer the test case generation process given any attribute model without requiring attribute-specific training of the model. In experiments, we show that CASPer effectively generates counterfactual text that follow the steering provided by an attribute model while also being fluent, diverse and preserving the original content. We also show that the generated counterfactuals from CASPer can be used for augmenting the training data and thereby fixing and making the test model more robust.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Madaan, Nishtha and Bedathur, Srikanta and Saha, Diptikalyan},
	month = jun,
	year = {2022},
	note = {arXiv:2206.10429 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\RMNIWJRQ\\Madaan et al. - 2022 - Plug and Play Counterfactual Text Generation for M.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\QMRV5B5K\\2206.html:text/html},
}

@article{de_oliveira_framework_2021,
	title = {A {Framework} and {Benchmarking} {Study} for {Counterfactual} {Generating} {Methods} on {Tabular} {Data}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/16/7274},
	doi = {10.3390/app11167274},
	abstract = {Counterfactual explanations are viewed as an effective way to explain machine learning predictions. This interest is reflected by a relatively young literature with already dozens of algorithms aiming to generate such explanations. These algorithms are focused on finding how features can be modified to change the output classification. However, this rather general objective can be achieved in different ways, which brings about the need for a methodology to test and benchmark these algorithms. The contributions of this work are manifold: First, a large benchmarking study of 10 algorithmic approaches on 22 tabular datasets is performed, using nine relevant evaluation metrics; second, the introduction of a novel, first of its kind, framework to test counterfactual generation algorithms; third, a set of objective metrics to evaluate and compare counterfactual results; and, finally, insight from the benchmarking results that indicate which approaches obtain the best performance on what type of dataset. This benchmarking study and framework can help practitioners in determining which technique and building blocks most suit their context, and can help researchers in the design and evaluation of current and future counterfactual generation algorithms. Our findings show that, overall, there’s no single best algorithm to generate counterfactual explanations as the performance highly depends on properties related to the dataset, model, score, and factual point specificities.},
	language = {en},
	number = {16},
	urldate = {2023-04-27},
	journal = {Applied Sciences},
	author = {de Oliveira, Raphael Mazzine Barbosa and Martens, David},
	month = jan,
	year = {2021},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {notion, algorithm benchmarking, artificial explainable intelligence, artificial neuronal networks, black box models, counterfactual explanations},
	pages = {7274},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\YMAIW98E\\de Oliveira et Martens - 2021 - A Framework and Benchmarking Study for Counterfact.pdf:application/pdf},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {0004-3702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	keywords = {notion, Interpretability, Transparency, Explainable AI, Explainability, Explanation},
	pages = {1--38},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\6W95LWVQ\\Miller - 2019 - Explanation in artificial intelligence Insights f.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\5VIJ9F84\\S0004370218305988.html:text/html},
}

@inproceedings{morris_textattack_2020,
	address = {Online},
	title = {{TextAttack}: {A} {Framework} for {Adversarial} {Attacks}, {Data} {Augmentation}, and {Adversarial} {Training} in {NLP}},
	shorttitle = {{TextAttack}},
	url = {https://aclanthology.org/2020.emnlp-demos.16},
	doi = {10.18653/v1/2020.emnlp-demos.16},
	abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2020 {Conf.} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
	month = oct,
	year = {2020},
	keywords = {notion},
	pages = {119--126},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\3VV9ULFT\\Morris et al. - 2020 - TextAttack A Framework for Adversarial Attacks, D.pdf:application/pdf},
}

@inproceedings{morris2020textattack,
  title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},
  author={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={119--126},
  year={2020}
}

@article{XAI_nlp_survey,
  title={Towards faithful model explanation in nlp: A survey},
  author={Lyu, Qing and Apidianaki, Marianna and Callison-Burch, Chris},
  journal={Computational Linguistics},
  pages={1--67},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{mothilal_explaining_2020,
	title = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/1905.07697},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	language = {en},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2020 {Conf.} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	note = {arXiv:1905.07697 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society, notion},
	pages = {607--617},
	file = {10179945.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\E4CIJP85\\10179945.pdf:application/pdf},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	month = jul,
	year = {2002},
	pages = {311--318},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\FLTWK6B9\\Papineni et al. - 2002 - Bleu a Method for Automatic Evaluation of Machine.pdf:application/pdf},
}

@inproceedings{poyiadzi_face_2020,
	title = {{FACE}: {Feasible} and {Actionable} {Counterfactual} {Explanations}},
	shorttitle = {{FACE}},
	url = {http://arxiv.org/abs/1909.09369},
	doi = {10.1145/3375627.3375850},
	abstract = {Work in Counterfactual Explanations tends to focus on the principle of "the closest possible world" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals(e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a "feasible path" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these "feasible paths" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the "feasible paths" of change, which are achievable and can be tailored to the problem at hand.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the {AAAI}/{ACM} {Conf.} on {AI}, {Ethics}, and {Society}},
	author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter},
	month = feb,
	year = {2020},
	note = {arXiv:1909.09369 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {344--350},
	file = {arXiv Fulltext PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\I5VNN4RI\\Poyiadzi et al. - 2020 - FACE Feasible and Actionable Counterfactual Expla.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\KRQVKEPX\\1909.html:text/html},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	keywords = {notion},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\JR6D3V4R\\Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@article{rael_exploring_nodate,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Uniﬁed} {Text}-to-{Text} {Transformer}},
	language = {en},
	author = {Raﬀel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	file = {Raﬀel et al. - Exploring the Limits of Transfer Learning with a U.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\N6PB4F2U\\Raﬀel et al. - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-074.html},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {140},
	urldate = {2023-04-27},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	pages = {1--67},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\7JYJGKKL\\Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;Source Code:C\:\\Users\\milan.bhan\\Zotero\\storage\\LIDK2SU4\\text-to-text-transfer-transformer.html:text/html},
}

@article{radford_language_nodate-1,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\TM75P5XM\\Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@article{arc,
  title={Think you have solved question answering? try {ARC}, the {AI}2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018},
  url={https://arxiv.org/abs/1803.05457}
}

@article{slm_survey,
  title={Small language models: Survey, measurements, and insights},
  author={Lu, Zhenyan and Li, Xiang and Cai, Dongqi and Yi, Rongjie and Liu, Fangming and Zhang, Xiwen and Lane, Nicholas D and Xu, Mengwei},
  journal={arXiv preprint arXiv:2409.15790},
  year={2024}
}

@inproceedings{reimers_sentence-bert_2019,
	OPTaddress = {Hong Kong, China},
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {https://aclanthology.org/D19-1410},
	doi = {10.18653/v1/D19-1410},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textbackslash}textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2019 {Conf.} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {Int.} {Joint} {Conf.} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = nov,
	year = {2019},
	keywords = {notion},
	pages = {3982--3992},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\I7CRVEWR\\Reimers et Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf:application/pdf},
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	doi = {10.48550/arXiv.1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01108 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\AABVU2B8\\Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\KEVHDTN9\\1910.html:text/html},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? ��},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2021 {ACM} {Conf.} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	keywords = {notion},
	pages = {610--623},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\HNR9M9GJ\\Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf:application/pdf},
}

@article{hdbscan,
  title={hdbscan: Hierarchical density based clustering},
  author={McInnes, Leland and Healy, John and Astels, Steve},
  journal={Journal of Open Source Software},
  volume={2},
  number={11},
  pages={205},
  year={2017}
}

@article{umap,
  title={UMAP: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Gro{\ss}berger, Lukas},
  journal={Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}

@inproceedings{yang_generating_2020,
	address = {Barcelona, Spain (Online)},
	title = {Generating {Plausible} {Counterfactual} {Explanations} for {Deep} {Transformers} in {Financial} {Text} {Classification}},
	url = {https://aclanthology.org/2020.coling-main.541},
	doi = {10.18653/v1/2020.coling-main.541},
	abstract = {Corporate mergers and acquisitions (M\&A) account for billions of dollars of investment globally every year and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust/accurate model, but be able to generate useful explanations to garner a user's trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user's trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 28th {Int.} {Conf.} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Yang, Linyi and Kenny, Eoin and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
	month = dec,
	year = {2020},
	keywords = {notion},
	pages = {6150--6160},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\82LXQLYD\\Yang et al. - 2020 - Generating Plausible Counterfactual Explanations f.pdf:application/pdf},
}

@article{jin_is_2020,
	title = {Is {BERT} {Really} {Robust}? {A} {Strong} {Baseline} for {Natural} {Language} {Attack} on {Text} {Classification} and {Entailment}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Is {BERT} {Really} {Robust}?},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6311},
	doi = {10.1609/aaai.v34i05.6311},
	abstract = {Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.1},
	language = {en},
	number = {05},
	urldate = {2023-04-27},
	journal = {Proc. of the AAAI Conf. on Artificial Intelligence},
	author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
	month = apr,
	year = {2020},
	note = {Number: 05},
	keywords = {notion},
	pages = {8018--8025},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\LD6YLHXJ\\Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf:application/pdf},
}

@inproceedings{li_bert-attack_2020,
	address = {Online},
	title = {{BERT}-{ATTACK}: {Adversarial} {Attack} {Against} {BERT} {Using} {BERT}},
	shorttitle = {{BERT}-{ATTACK}},
	url = {https://aclanthology.org/2020.emnlp-main.500},
	doi = {10.18653/v1/2020.emnlp-main.500},
	abstract = {Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2020 {Conf.} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
	month = nov,
	year = {2020},
	keywords = {notion},
	pages = {6193--6202},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\6WXDG8D8\\Li et al. - 2020 - BERT-ATTACK Adversarial Attack Against BERT Using.pdf:application/pdf},
}

@inproceedings{ebrahimi_hotflip_2018,
	address = {Melbourne, Australia},
	title = {{HotFlip}: {White}-{Box} {Adversarial} {Examples} for {Text} {Classification}},
	shorttitle = {{HotFlip}},
	url = {https://aclanthology.org/P18-2006},
	doi = {10.18653/v1/P18-2006},
	abstract = {We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ebrahimi, Javid and Rao, Anyi and Lowd, Daniel and Dou, Dejing},
	month = jul,
	year = {2018},
	keywords = {notion},
	pages = {31--36},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\8KDE2UXG\\Ebrahimi et al. - 2018 - HotFlip White-Box Adversarial Examples for Text C.pdf:application/pdf},
}

@inproceedings{gao_black-box_2018,
	title = {Black-{Box} {Generation} of {Adversarial} {Text} {Sequences} to {Evade} {Deep} {Learning} {Classifiers}},
	doi = {10.1109/SPW.2018.00016},
	abstract = {Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to a black-box attack, which is a more realistic scenario. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We develop novel scoring strategies to find the most important words to modify such that the deep classifier makes a wrong prediction. Simple character-level transformations are applied to the highest-ranked words in order to minimize the edit distance of the perturbation. We evaluated DeepWordBug on two real-world text datasets: Enron spam emails and IMDB movie reviews. Our experimental results indicate that DeepWordBug can reduce the classification accuracy from 99\% to 40\% on Enron and from 87\% to 26\% on IMDB. Our results strongly demonstrate that the generated adversarial sequences from a deep-learning model can similarly evade other deep models.},
	booktitle = {2018 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
	month = may,
	year = {2018},
	keywords = {Sentiment analysis, adversarial samples, black box attack, deep learning, Machine learning, misclassification, Perturbation methods, Prediction algorithms, Recurrent neural networks, Task analysis, text classification, word embedding},
	pages = {50--56},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\milan.bhan\\Zotero\\storage\\LRFNIG9Z\\8424632.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\L5FD9486\\Gao et al. - 2018 - Black-Box Generation of Adversarial Text Sequences.pdf:application/pdf},
}

@misc{noauthor_post-hoc_nodate,
	title = {Post-hoc {Interpretability} for {Neural} {NLP}: {A} {Survey} {\textbar} {ACM} {Computing} {Surveys}},
	url = {https://dl.acm.org/doi/10.1145/3546577},
	urldate = {2023-04-27},
	file = {Post-hoc Interpretability for Neural NLP\: A Survey | ACM Computing Surveys:C\:\\Users\\milan.bhan\\Zotero\\storage\\VTAF3THE\\3546577.html:text/html},
}

@article{ali_explainable_2023,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {What} we know and what is left to attain {Trustworthy} {Artificial} {Intelligence}},
	issn = {1566-2535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
	doi = {10.1016/j.inffus.2023.101805},
	abstract = {Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model’s decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and so on. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The review starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.},
	language = {en},
	urldate = {2023-04-27},
	journal = {Information Fusion},
	author = {Ali, Sajid and Abuhmed, Tamer and El-Sappagh, Shaker and Muhammad, Khan and Alonso-Moral, Jose M. and Confalonieri, Roberto and Guidotti, Riccardo and Ser, Javier Del and Díaz-Rodríguez, Natalia and Herrera, Francisco},
	month = apr,
	year = {2023},
	keywords = {AI principles, Data Fusion, Deep Learning, Explainable Artificial Intelligence, Interpretable machine learning, Post-hoc explainability, Trustworthy AI, XAI assessment, notion},
	pages = {101805},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\ICCGGFTW\\Ali et al. - 2023 - Explainable Artificial Intelligence (XAI) What we.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\IWUM484V\\S1566253523001148.html:text/html},
}

@inproceedings{abnar_quantifying_2020,
	address = {Online},
	title = {Quantifying {Attention} {Flow} in {Transformers}},
	url = {https://aclanthology.org/2020.acl-main.385},
	doi = {10.18653/v1/2020.acl-main.385},
	abstract = {In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Abnar, Samira and Zuidema, Willem},
	month = jul,
	year = {2020},
	pages = {4190--4197},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\SGJQENFT\\Abnar et Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf:application/pdf},
}

@inproceedings{bell_its_2022,
	address = {New York, NY, USA},
	series = {{FAccT} '22},
	title = {It’s {Just} {Not} {That} {Simple}: {An} {Empirical} {Study} of the {Accuracy}-{Explainability} {Trade}-off in {Machine} {Learning} for {Public} {Policy}},
	isbn = {978-1-4503-9352-2},
	shorttitle = {It’s {Just} {Not} {That} {Simple}},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533090},
	doi = {10.1145/3531146.3533090},
	abstract = {To achieve high accuracy in machine learning (ML) systems, practitioners often use complex “black-box” models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature — and even the existence — of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human’s ability to anticipate a model’s output or identify the most important feature of a model, and subjective measures, such as a human’s perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It’s just not that simple!},
	urldate = {2023-04-27},
	booktitle = {2022 {ACM} {Conf.} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bell, Andrew and Solano-Kamaiko, Ian and Nov, Oded and Stoyanovich, Julia},
	month = jun,
	year = {2022},
	keywords = {notion, explainability, machine learning, public policy, responsible AI},
	pages = {248--266},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\L9VFAFZF\\Bell et al. - 2022 - It’s Just Not That Simple An Empirical Study of t.pdf:application/pdf},
}

@inproceedings{clark_what_2019,
	address = {Florence, Italy},
	title = {What {Does} {BERT} {Look} at? {An} {Analysis} of {BERT}'s {Attention}},
	shorttitle = {What {Does} {BERT} {Look} at?},
	url = {https://aclanthology.org/W19-4828},
	doi = {10.18653/v1/W19-4828},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	month = aug,
	year = {2019},
	pages = {276--286},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\HLMDE2IS\\Clark et al. - 2019 - What Does BERT Look at An Analysis of BERT's Atte.pdf:application/pdf},
}

@inproceedings{lai_why_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {"{Why} is '{Chicago}' deceptive?" {Towards} {Building} {Model}-{Driven} {Tutorials} for {Humans}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {"{Why} is '{Chicago}' deceptive?},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376873},
	doi = {10.1145/3313831.3376873},
	abstract = {To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a train- ing phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically selected examples from training data with explanations. We use deceptive review detection as a testbed and conduct large-scale, randomized human-subject experiments to examine the effectiveness of such tutorials. We find that tutorials indeed improve human performance, with and without real-time assistance. In particular, although deep learning provides superior predictive performance than simple models, tutorials and explanations from simple models are more useful to humans. Our work suggests future directions for human-centered tutorials and explanations towards a synergy between humans and AI.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2020 {CHI} {Conf.} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lai, Vivian and Liu, Han and Tan, Chenhao},
	month = apr,
	year = {2020},
	keywords = {notion, interpretable machine learning, deception detection, explanations, tutorials},
	pages = {1--13},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\CKQHYQUK\\Lai et al. - 2020 - Why is 'Chicago' deceptive Towards Building Mod.pdf:application/pdf},
}

@inproceedings{schemmer_meta-analysis_2022,
	address = {New York, NY, USA},
	series = {{AIES} '22},
	title = {A {Meta}-{Analysis} of the {Utility} of {Explainable} {Artificial} {Intelligence} in {Human}-{AI} {Decision}-{Making}},
	isbn = {978-1-4503-9247-1},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534128},
	doi = {10.1145/3514094.3534128},
	abstract = {Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysis to derive implications across existing research. We observe a statistically positive impact of XAI on users' performance. Additionally, the first results indicate that human-AI decision-making tends to yield better task performance on text data. However, we find no effect of explanations on users' performance compared to sole AI predictions. Our initial synthesis gives rise to future research investigating the underlying causes and contributes to further developing algorithms that effectively benefit human decision-makers by providing meaningful explanations.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2022 {AAAI}/{ACM} {Conf.} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Schemmer, Max and Hemmer, Patrick and Nitsche, Maximilian and Kühl, Niklas and Vössing, Michael},
	month = jul,
	year = {2022},
	keywords = {notion, decision-making, empirical studies, explainable artificial intelligence, meta-analysis},
	pages = {617--626},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\TNGRELDX\\Schemmer et al. - 2022 - A Meta-Analysis of the Utility of Explainable Arti.pdf:application/pdf},
}

@inproceedings{wang_glue_2018,
	address = {Brussels, Belgium},
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {https://aclanthology.org/W18-5446},
	doi = {10.18653/v1/W18-5446},
	abstract = {Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2018 {EMNLP} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	month = nov,
	year = {2018},
	keywords = {notion},
	pages = {353--355},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\LBR3MSYM\\Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:application/pdf},
}

@inproceedings{wang_are_2021,
	address = {New York, NY, USA},
	series = {{IUI} '21},
	title = {Are {Explanations} {Helpful}? {A} {Comparative} {Study} of the {Effects} of {Explanations} in {AI}-{Assisted} {Decision}-{Making}},
	isbn = {978-1-4503-8017-1},
	shorttitle = {Are {Explanations} {Helpful}?},
	url = {https://dl.acm.org/doi/10.1145/3397481.3450650},
	doi = {10.1145/3397481.3450650},
	abstract = {This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy—improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.},
	urldate = {2023-04-27},
	booktitle = {26th {Int.} {Conf.} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Xinru and Yin, Ming},
	month = apr,
	year = {2021},
	keywords = {notion, interpretable machine learning, explainable AI, human-subject experiments, trust, trust calibration},
	pages = {318--328},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\PRGRLKX3\\Wang et Yin - 2021 - Are Explanations Helpful A Comparative Study of t.pdf:application/pdf},
}

@book{molnar_interpretable_2020,
	title = {Interpretable {Machine} {Learning}},
	isbn = {978-0-244-76852-2},
	abstract = {This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.},
	language = {en},
	publisher = {Lulu.com},
	author = {Molnar, Christoph},
	year = {2020},
    url = {https://christophm.github.io/interpretable-ml-book/},
	OPTnote = {Google-Books-ID: jBm3DwAAQBAJ},
	keywords = {notion},
}

@inproceedings{bhan_evaluating_2023,
  author ={Bhan, Milan and Achache, Nina and Legrand, Victor
           and Blangero, Annabelle and Chesneau, Nicolas}, 
  title = {Evaluating self-attention interpretability through
                  human-grounded experimental protocol}, 
	url = {http://arxiv.org/abs/2303.15190},
  booktitle = {Proc. of the First World Conf. on Explainable Artificial
                  Intelligence xAI}, 
  year = 	 {2023},
  pages = 	 {26-46}
}

@misc{bhan_evaluating_2023_old,
	title = {Evaluating self-attention interpretability through human-grounded experimental protocol},
	url = {http://arxiv.org/abs/2303.15190},
	abstract = {Attention mechanisms have played a crucial role in the development of complex architectures such as Transformers in natural language processing. However, Transformers remain hard to interpret and are considered as black-boxes. This paper aims to assess how attention coefficients from Transformers can help in providing interpretability. A new attention-based interpretability method called CLaSsification-Attention (CLS-A) is proposed. CLS-A computes an interpretability score for each word based on the attention coefficient distribution related to the part specific to the classification task within the Transformer architecture. A human-grounded experiment is conducted to evaluate and compare CLS-A to other interpretability methods. The experimental protocol relies on the capacity of an interpretability method to provide explanation in line with human reasoning. Experiment design includes measuring reaction times and correct response rates by human subjects. CLS-A performs comparably to usual interpretability methods regarding average participant reaction time and accuracy. The lower computational cost of CLS-A compared to other interpretability methods and its availability by design within the classifier make it particularly interesting. Data analysis also highlights the link between the probability score of a classifier prediction and adequate explanations. Finally, our work confirms the relevancy of the use of CLS-A and shows to which extent self-attention contains rich information to explain Transformer classifiers.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Bhan, Milan and Achache, Nina and Legrand, Victor and Blangero, Annabelle and Chesneau, Nicolas},
	month = mar,
	year = {2023},
	note = {arXiv:2303.15190 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\D4M5VJZ5\\2303.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\FYKGBBPD\\Bhan et al. - 2023 - Evaluating self-attention interpretability through.pdf:application/pdf},
}

@inproceedings{bibal_is_2022-1,
	address = {Dublin, Ireland},
	title = {Is {Attention} {Explanation}? {An} {Introduction} to the {Debate}},
	shorttitle = {Is {Attention} {Explanation}?},
	url = {https://aclanthology.org/2022.acl-long.269},
	doi = {10.18653/v1/2022.acl-long.269},
	abstract = {The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.},
	urldate = {2023-05-03},
	booktitle = {Proc. of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bibal, Adrien and Cardon, Rémi and Alfter, David and Wilkens, Rodrigo and Wang, Xiaoou and François, Thomas and Watrin, Patrick},
	month = may,
	year = {2022},
	pages = {3889--3900},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\9I6D5C83\\Bibal et al. - 2022 - Is Attention Explanation An Introduction to the D.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-05-03},
	booktitle = {Proc. of the 2019 {Conf.} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	keywords = {notion},
	pages = {4171--4186},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\ANUUGG77\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{shrikumar_learning_2017,
	OPTaddress = {Sydney, NSW, Australia},
	series = {{ICML}'17},
	title = {Learning important features through propagating activation differences},
	abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.},
	urldate = {2023-05-03},
	booktitle = {Proc. of the 34th {Int.} {Conf.} on {Machine} {Learning}, ICML},
    url = {https://dl.acm.org/doi/10.5555/3305890.3306006},
    volume = 70,
	publisher = {JMLR.org},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	month = aug,
	year = {2017},
	keywords = {notion},
	pages = {3145--3153},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\IYRKSNWW\\Shrikumar et al. - 2017 - Learning important features through propagating ac.pdf:application/pdf},
}

@inproceedings{sundararajan_axiomatic_2017,
	OPTaddress = {Sydney, NSW, Australia},
	series = {{ICML}'17},
	title = {Axiomatic attribution for deep networks},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	urldate = {2023-05-03},
	booktitle = {Proc. of the 34th {Int.} {Conf.} on {Machine} {Learning}, ICML},
    volume = 70,
    url = {https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
	publisher = {JMLR.org},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = aug,
	year = {2017},
	keywords = {notion},
	pages = {3319--3328},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\S3QJB223\\Sundararajan et al. - 2017 - Axiomatic attribution for deep networks.pdf:application/pdf},
}

@inproceedings{bhatt_explainable_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Explainable machine learning in deployment},
	isbn = {978-1-4503-6936-7},
	url = {https://dl.acm.org/doi/10.1145/3351095.3375624},
	doi = {10.1145/3351095.3375624},
	abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
	urldate = {2023-05-03},
	booktitle = {Proc. of the 2020 {Conf.} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, José M. F. and Eckersley, Peter},
	month = jan,
	year = {2020},
	keywords = {notion, explainability, machine learning, deployed systems, qualitative study, transparency},
	pages = {648--657},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\KWTCYBNJ\\Bhatt et al. - 2020 - Explainable machine learning in deployment.pdf:application/pdf},
}

@inproceedings{abnar_quantifying_2020-1,
	address = {Online},
	title = {Quantifying {Attention} {Flow} in {Transformers}},
	url = {https://aclanthology.org/2020.acl-main.385},
	doi = {10.18653/v1/2020.acl-main.385},
	abstract = {In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
	urldate = {2023-05-03},
	booktitle = {Proc. of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Abnar, Samira and Zuidema, Willem},
	month = jul,
	year = {2020},
	pages = {4190--4197},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\WAIV5HIC\\Abnar et Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf:application/pdf},
}

@inproceedings{carraro_courge_2023,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {{CouRGe}: {Counterfactual} {Reviews} {Generator} for {Sentiment} {Analysis}},
	isbn = {978-3-031-26438-2},
	shorttitle = {{CouRGe}},
	doi = {10.1007/978-3-031-26438-2_24},
	abstract = {Past literature in Natural Language Processing (NLP) has demonstrated that counterfactual data points are useful, for example, for increasing model generalisation, enhancing model interpretability, and as a data augmentation approach. However, obtaining counterfactual examples often requires human annotation effort, which is an expensive and highly skilled process. For these reasons, solutions that resort to transformer-based language models have been recently proposed to generate counterfactuals automatically, but such solutions show limitations.},
	language = {en},
	booktitle = {Artificial {Intelligence} and {Cognitive} {Science}},
	publisher = {Springer Nature Switzerland},
	author = {Carraro, Diego and Brown, Kenneth N.},
	editor = {Longo, Luca and O’Reilly, Ruairi},
	year = {2023},
	keywords = {Counterfactual reasoning, Data augmentation, Language models, Natural language processing, Sentiment analysis, notion},
	pages = {305--317},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\K4FRUVRU\\Carraro et Brown - 2023 - CouRGe Counterfactual Reviews Generator for Senti.pdf:application/pdf},
}

@misc{ziegler_fine-tuning_2020,
	title = {Fine-{Tuning} {Language} {Models} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/1909.08593},
	abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	month = jan,
	year = {2020},
	note = {arXiv:1909.08593 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\4B96QIHT\\1909.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\BV4KCKQG\\Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf:application/pdf},
}

@inproceedings{sanyal_discretized_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Discretized {Integrated} {Gradients} for {Explaining} {Language} {Models}},
	url = {https://aclanthology.org/2021.emnlp-main.805},
	doi = {10.18653/v1/2021.emnlp-main.805},
	abstract = {As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model's output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the gradients computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research.},
	urldate = {2023-05-05},
	booktitle = {Proc. of the 2021 {Conf.} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sanyal, Soumya and Ren, Xiang},
	month = nov,
	year = {2021},
	keywords = {notion},
	pages = {10285--10299},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\A7L58Z7F\\Sanyal et Ren - 2021 - Discretized Integrated Gradients for Explaining La.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\GBSRNESX\\2302.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\H4NUQKW7\\Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\ADV292W4\\1707.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\2JLS6XQT\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@misc{chollet_measure_2019,
	title = {On the {Measure} of {Intelligence}},
	url = {http://arxiv.org/abs/1911.01547},
	doi = {10.48550/arXiv.1911.01547},
	abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
	urldate = {2023-05-09},
	publisher = {arXiv},
	author = {Chollet, François},
	month = nov,
	year = {2019},
	note = {arXiv:1911.01547 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\HTRVMLM3\\Chollet - 2019 - On the Measure of Intelligence.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\2JXEKIQN\\1911.html:text/html},
}

@misc{noauthor_tackling_nodate,
	title = {Tackling {Climate} {Change} with {Machine} {Learning} {\textbar} {ACM} {Computing} {Surveys}},
	url = {https://dl.acm.org/doi/full/10.1145/3485128},
	urldate = {2023-05-09},
	keywords = {notion},
}

@misc{kaushik_learning_2020,
	title = {Learning the {Difference} that {Makes} a {Difference} with {Counterfactually}-{Augmented} {Data}},
	url = {http://arxiv.org/abs/1909.12434},
	abstract = {Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C.},
	month = feb,
	year = {2020},
	note = {arXiv:1909.12434 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\6X4FZ7LG\\1909.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\CZNMFNKK\\Kaushik et al. - 2020 - Learning the Difference that Makes a Difference wi.pdf:application/pdf},
}

@misc{agarwal_openxai_2023,
	title = {{OpenXAI}: {Towards} a {Transparent} {Evaluation} of {Model} {Explanations}},
	shorttitle = {{OpenXAI}},
	url = {http://arxiv.org/abs/2206.11104},
	abstract = {While several types of post hoc explanation methods (e.g., feature attribution methods) have been proposed in recent literature, there is little to no work on systematically benchmarking these methods in an efficient and transparent manner. Here, we introduce OpenXAI, a comprehensive and extensible open source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to benchmark explanations. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/.},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Agarwal, Chirag and Krishna, Satyapriya and Saxena, Eshika and Pawelczyk, Martin and Johnson, Nari and Puri, Isha and Zitnik, Marinka and Lakkaraju, Himabindu},
	month = jan,
	year = {2023},
	note = {arXiv:2206.11104 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\2VY73EU2\\2206.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\GDWLKJIJ\\Agarwal et al. - 2023 - OpenXAI Towards a Transparent Evaluation of Model.pdf:application/pdf},
}

@misc{krishna_disagreement_2022,
	title = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}: {A} {Practitioner}'s {Perspective}},
	shorttitle = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2202.01602},
	abstract = {As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Our findings also underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations.},
	urldate = {2023-05-24},
	publisher = {arXiv},
	author = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
	month = feb,
	year = {2022},
	note = {arXiv:2202.01602 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\CV47YH3R\\2202.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\Y5HW9KMV\\Krishna et al. - 2022 - The Disagreement Problem in Explainable Machine Le.pdf:application/pdf},
}

@misc{filandrianos_counterfactuals_2023,
	title = {Counterfactuals of {Counterfactuals}: a back-translation-inspired approach to analyse counterfactual editors},
	shorttitle = {Counterfactuals of {Counterfactuals}},
	url = {http://arxiv.org/abs/2305.17055},
	abstract = {In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus specifically on the analysis of counterfactual, contrastive explanations. We note that while there have been several explainers proposed to produce counterfactual explanations, their behaviour can vary significantly and the lack of a universal ground truth for the counterfactual edits imposes an insuperable barrier on their evaluation. We propose a new back translation-inspired evaluation methodology that utilises earlier outputs of the explainer as ground truth proxies to investigate the consistency of explainers. We show that by iteratively feeding the counterfactual to the explainer we can obtain valuable insights into the behaviour of both the predictor and the explainer models, and infer patterns that would be otherwise obscured. Using this methodology, we conduct a thorough analysis and propose a novel metric to evaluate the consistency of counterfactual generation approaches with different characteristics across available performance indicators.},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Filandrianos, Giorgos and Dervakos, Edmund and Menis-Mastromichalakis, Orfeas and Zerva, Chrysoula and Stamou, Giorgos},
	month = may,
	year = {2023},
	note = {arXiv:2305.17055 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\PH6MPIKB\\2305.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\W9APZXCM\\Filandrianos et al. - 2023 - Counterfactuals of Counterfactuals a back-transla.pdf:application/pdf},
}
@article{beyond_imitation_game,
  title={Beyond the Imitation Game: Quantifying and extrapolatingthe capabilities of language models},
  author={Srivastava, Aarohi and Kleyjo, Denis and Wu, Ziyi},
  journal={Transactions on Machine Learning Research},
  number={5},
  year={2023}
}

@misc{treviso_crest_2023,
	title = {{CREST}: {A} {Joint} {Framework} for {Rationalization} and {Counterfactual} {Text} {Generation}},
	shorttitle = {{CREST}},
	url = {http://arxiv.org/abs/2305.17075},
	abstract = {Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, CREST generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages CREST counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage CREST counterfactuals. Our results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model's predictions.},
	urldate = {2023-06-01},
	publisher = {arXiv},
	author = {Treviso, Marcos and Ross, Alexis and Guerreiro, Nuno M. and Martins, André F. T.},
	month = may,
	year = {2023},
	note = {arXiv:2305.17075 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\F92H4YL5\\2305.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\LNJJCRGY\\Treviso et al. - 2023 - CREST A Joint Framework for Rationalization and C.pdf:application/pdf},
}

@misc{chen_disco_2023,
	title = {{DISCO}: {Distilling} {Counterfactuals} with {Large} {Language} {Models}},
	shorttitle = {{DISCO}},
	url = {http://arxiv.org/abs/2212.10534},
	abstract = {Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCO generated counterfactuals are more robust (6\% absolute) and generalize better across distributions (2\%) compared to models trained without data augmentation. Furthermore, DISCO augmented models are 10\% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO augmentation enables models to more reliably learn causal representations. Our repository is available at: https://github.com/eric11eca/disco},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Chen, Zeming and Gao, Qiyue and Bosselut, Antoine and Sabharwal, Ashish and Richardson, Kyle},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10534 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\8AR38VNJ\\2212.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\JTTZ9G4Q\\Chen et al. - 2023 - DISCO Distilling Counterfactuals with Large Langu.pdf:application/pdf},
}

@inproceedings{bertrand_how_2022,
	address = {Oxford United Kingdom},
	title = {How {Cognitive} {Biases} {Affect} {XAI}-assisted {Decision}-making: {A} {Systematic} {Review}},
	isbn = {978-1-4503-9247-1},
	shorttitle = {How {Cognitive} {Biases} {Affect} {XAI}-assisted {Decision}-making},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534164},
	doi = {10.1145/3514094.3534164},
	language = {en},
	urldate = {2023-06-13},
	booktitle = {Proc. of the 2022 {AAAI}/{ACM} {Conf.} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Bertrand, Astrid and Belloum, Rafik and Eagan, James R. and Maxwell, Winston},
	month = jul,
	year = {2022},
	keywords = {notion},
	pages = {78--91},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\ZDR8BMCV\\Bertrand et al. - 2022 - How Cognitive Biases Affect XAI-assisted Decision-.pdf:application/pdf},
}

@article{meng_interpretability_2022,
	title = {Interpretability and fairness evaluation of deep learning models on {MIMIC}-{IV} dataset},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-11012-2},
	doi = {10.1038/s41598-022-11012-2},
	abstract = {The recent release of large-scale healthcare datasets has greatly propelled the research of data-driven deep learning models for healthcare applications. However, due to the nature of such deep black-boxed models, concerns about interpretability, fairness, and biases in healthcare scenarios where human lives are at stake call for a careful and thorough examination of both datasets and models. In this work, we focus on MIMIC-IV (Medical Information Mart for Intensive Care, version IV), the largest publicly available healthcare dataset, and conduct comprehensive analyses of interpretability as well as dataset representation bias and prediction fairness of deep learning models for in-hospital mortality prediction. First, we analyze the interpretability of deep learning mortality prediction models and observe that (1) the best-performing interpretability method successfully identifies critical features for mortality prediction on various prediction models as well as recognizing new important features that domain knowledge does not consider; (2) prediction models rely on demographic features, raising concerns in fairness. Therefore, we then evaluate the fairness of models and do observe the unfairness: (1) there exists disparate treatment in prescribing mechanical ventilation among patient groups across ethnicity, gender and age; (2) models often rely on racial attributes unequally across subgroups to generate their predictions. We further draw concrete connections between interpretability methods and fairness metrics by showing how feature importance from interpretability methods can be beneficial in quantifying potential disparities in mortality predictors. Our analysis demonstrates that the prediction performance is not the only factor to consider when evaluating models for healthcare applications, since high prediction performance might be the result of unfair utilization of demographic features. Our findings suggest that future research in AI models for healthcare applications can benefit from utilizing the analysis workflow of interpretability and fairness as well as verifying if models achieve superior performance at the cost of introducing bias.},
	language = {en},
	number = {1},
	urldate = {2023-06-13},
	journal = {Scientific Reports},
	author = {Meng, Chuizheng and Trinh, Loc and Xu, Nan and Enouen, James and Liu, Yan},
	month = may,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {notion, Computer science, Medical ethics, Medical research, Outcomes research},
	pages = {7166},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\4ZBIZZZR\\Meng et al. - 2022 - Interpretability and fairness evaluation of deep l.pdf:application/pdf},
}

@article{bodria_benchmarking_2023,
	title = {Benchmarking and survey of explanation methods for black box models},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-023-00933-9},
	doi = {10.1007/s10618-023-00933-9},
	abstract = {The rise of sophisticated black-box machine learning models in Artificial Intelligence systems has prompted the need for explanation methods that reveal how these models work in an understandable way to users and decision makers. Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.},
	language = {en},
	urldate = {2023-06-14},
	journal = {Data Mining and Knowledge Discovery},
	author = {Bodria, Francesco and Giannotti, Fosca and Guidotti, Riccardo and Naretto, Francesca and Pedreschi, Dino and Rinzivillo, Salvatore},
	month = jun,
	year = {2023},
	keywords = {Interpretable machine learning, notion, Benchmarking, Explainable artificial intelligence, Transparent models},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\87T9DKQW\\Bodria et al. - 2023 - Benchmarking and survey of explanation methods for.pdf:application/pdf},
}

@misc{rafailov_direct_2023,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = may,
	year = {2023},
	note = {arXiv:2305.18290 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\TURQKXZL\\2305.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\RQEGP4D5\\Rafailov et al. - 2023 - Direct Preference Optimization Your Language Mode.pdf:application/pdf},
}

@Article{gurrapu_rationalization_2023,
  author =  {Gurrapu, Sai and Kulkarni, Ajay and Huang, Lifu and
                  Lourentzou, Ismini and Batarseh,
                  Feras A.}, 
  title =   {Rationalization for {Explainable} {NLP}: {A} {Survey}},
  journal = {Frontiers in Artificial Intelligence},
  year =    {2023},
  volume =  {6},
  url =     {https://www.frontiersin.org/articles/10.3389/frai.2023.1225093/full}
}



@misc{gurrapu_rationalization_2023_old,
	title = {Rationalization for {Explainable} {NLP}: {A} {Survey}},
	shorttitle = {Rationalization for {Explainable} {NLP}},
	url = {http://arxiv.org/abs/2301.08912},
	abstract = {Recent advances in deep learning have improved the performance of many Natural Language Processing (NLP) tasks such as translation, question-answering, and text classification. However, this improvement comes at the expense of model explainability. Black-box models make it difficult to understand the internals of a system and the process it takes to arrive at an output. Numerical (LIME, Shapley) and visualization (saliency heatmap) explainability techniques are helpful; however, they are insufficient because they require specialized knowledge. These factors led rationalization to emerge as a more accessible explainable technique in NLP. Rationalization justifies a model's output by providing a natural language explanation (rationale). Recent improvements in natural language generation have made rationalization an attractive technique because it is intuitive, human-comprehensible, and accessible to non-technical users. Since rationalization is a relatively new field, it is disorganized. As the first survey, rationalization literature in NLP from 2007-2022 is analyzed. This survey presents available methods, explainable evaluations, code, and datasets used across various NLP tasks that use rationalization. Further, a new subfield in Explainable AI (XAI), namely, Rational AI (RAI), is introduced to advance the current state of rationalization. A discussion on observed insights, challenges, and future directions is provided to point to promising research opportunities.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Gurrapu, Sai and Kulkarni, Ajay and Huang, Lifu and Lourentzou, Ismini and Freeman, Laura and Batarseh, Feras A.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.08912 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\EBBA38IH\\2301.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\IDB8Y3NB\\Gurrapu et al. - 2023 - Rationalization for Explainable NLP A Survey.pdf:application/pdf},
}

@misc{balkir_challenges_2022,
	title = {Challenges in {Applying} {Explainability} {Methods} to {Improve} the {Fairness} of {NLP} {Models}},
	url = {http://arxiv.org/abs/2206.03945},
	abstract = {Motivations for methods in explainable artificial intelligence (XAI) often include detecting, quantifying and mitigating bias, and contributing to making machine learning models fairer. However, exactly how an XAI method can help in combating biases is often left unspecified. In this paper, we briefly review trends in explainability and fairness in NLP research, identify the current practices in which explainability methods are applied to detect and mitigate bias, and investigate the barriers preventing XAI methods from being used more widely in tackling fairness issues.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Balkir, Esma and Kiritchenko, Svetlana and Nejadgholi, Isar and Fraser, Kathleen C.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03945 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\GVGRKYZL\\2206.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\D8I4EISP\\Balkir et al. - 2022 - Challenges in Applying Explainability Methods to I.pdf:application/pdf},
}

@misc{sun_retentive_2023,
	title = {Retentive {Network}: {A} {Successor} to {Transformer} for {Large} {Language} {Models}},
	shorttitle = {Retentive {Network}},
	url = {http://arxiv.org/abs/2307.08621},
	abstract = {In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost \$O(1)\$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.08621 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, notion},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\6IQA8SX6\\2307.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\IE5XRFVE\\Sun et al. - 2023 - Retentive Network A Successor to Transformer for .pdf:application/pdf},
}

@inproceedings{abdalla_elephant_2023,
	address = {Toronto, Canada},
	title = {The {Elephant} in the {Room}: {Analyzing} the {Presence} of {Big} {Tech} in {Natural} {Language} {Processing} {Research}},
	shorttitle = {The {Elephant} in the {Room}},
	url = {https://aclanthology.org/2023.acl-long.734},
	abstract = {Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP publication authors, we explore the industry presence in the field since the early 90s. We find that industry presence among NLP authors has been steady before a steep increase over the past five years (180\% growth from 2017 to 2022). A few companies account for most of the publications and provide funding to academic researchers through grants and internships. Our study shows that the presence and impact of the industry on natural language processing research are significant and fast-growing. This work calls for increased transparency of industry influence in the field.},
	urldate = {2023-07-20},
	booktitle = {Proc. of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Abdalla, Mohamed and Wahle, Jan Philip and Lima Ruas, Terry and Névéol, Aurélie and Ducel, Fanny and Mohammad, Saif and Fort, Karen},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {13141--13160},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\IY65QB6L\\Abdalla et al. - 2023 - The Elephant in the Room Analyzing the Presence o.pdf:application/pdf},
}

@misc{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv:1702.08608 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\9V6GAB66\\1702.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\UFKBMVVU\\Doshi-Velez et Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf},
}

@inproceedings{hallinan_detoxifying_2023,
	address = {Toronto, Canada},
	title = {Detoxifying {Text} with {MaRCo}: {Controllable} {Revision} with {Experts} and {Anti}-{Experts}},
	shorttitle = {Detoxifying {Text} with {MaRCo}},
	url = {https://aclanthology.org/2023.acl-short.21},
	abstract = {Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo's rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.},
	urldate = {2023-07-20},
	booktitle = {Proc. of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hallinan, Skyler and Liu, Alisa and Choi, Yejin and Sap, Maarten},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {228--242},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\D79RRVLU\\Hallinan et al. - 2023 - Detoxifying Text with MaRCo Controllable Revision.pdf:application/pdf},
}

@inproceedings{feng_pretraining_2023,
	address = {Toronto, Canada},
	title = {From {Pretraining} {Data} to {Language} {Models} to {Downstream} {Tasks}: {Tracking} the {Trails} of {Political} {Biases} {Leading} to {Unfair} {NLP} {Models}},
	shorttitle = {From {Pretraining} {Data} to {Language} {Models} to {Downstream} {Tasks}},
	url = {https://aclanthology.org/2023.acl-long.656},
	abstract = {Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.},
	urldate = {2023-07-20},
	booktitle = {Proc. of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {11737--11762},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\3UQDIDL4\\Feng et al. - 2023 - From Pretraining Data to Language Models to Downst.pdf:application/pdf},
}

@InProceedings{wang_self-consistency_2023,
  author =    {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  title =     {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
  url =       {http://arxiv.org/abs/2203.11171},
  booktitle = {Proc. of the 11th Int. Conf. on Learning Representations, ICLR23},
  year =      {2023}
}

@misc{wang_self-consistency_2023_old,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\CQLZAJ7V\\2203.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\PJ93PQJY\\Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf},
}

@inproceedings{huang_towards_2023,
	OPTaddress = {Toronto, Canada},
	title = {Towards {Reasoning} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Towards {Reasoning} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-acl.67},
	abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
	urldate = {2023-07-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Jie and Chang, Kevin Chen-Chuan},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {1049--1065},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\2VRNDT2G\\Huang et Chang - 2023 - Towards Reasoning in Large Language Models A Surv.pdf:application/pdf},
}

@misc{gurrapu_rationalization_2023-1,
	title = {Rationalization for {Explainable} {NLP}: {A} {Survey}},
	shorttitle = {Rationalization for {Explainable} {NLP}},
	url = {http://arxiv.org/abs/2301.08912},
	abstract = {Recent advances in deep learning have improved the performance of many Natural Language Processing (NLP) tasks such as translation, question-answering, and text classification. However, this improvement comes at the expense of model explainability. Black-box models make it difficult to understand the internals of a system and the process it takes to arrive at an output. Numerical (LIME, Shapley) and visualization (saliency heatmap) explainability techniques are helpful; however, they are insufficient because they require specialized knowledge. These factors led rationalization to emerge as a more accessible explainable technique in NLP. Rationalization justifies a model's output by providing a natural language explanation (rationale). Recent improvements in natural language generation have made rationalization an attractive technique because it is intuitive, human-comprehensible, and accessible to non-technical users. Since rationalization is a relatively new field, it is disorganized. As the first survey, rationalization literature in NLP from 2007-2022 is analyzed. This survey presents available methods, explainable evaluations, code, and datasets used across various NLP tasks that use rationalization. Further, a new subfield in Explainable AI (XAI), namely, Rational AI (RAI), is introduced to advance the current state of rationalization. A discussion on observed insights, challenges, and future directions is provided to point to promising research opportunities.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Gurrapu, Sai and Kulkarni, Ajay and Huang, Lifu and Lourentzou, Ismini and Freeman, Laura and Batarseh, Feras A.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.08912 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\4WSQF4NA\\2301.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\WFVFUUVN\\Gurrapu et al. - 2023 - Rationalization for Explainable NLP A Survey.pdf:application/pdf},
}

@inproceedings{joshi_are_2023,
	address = {Toronto, Canada},
	title = {Are {Machine} {Rationales} ({Not}) {Useful} to {Humans}? {Measuring} and {Improving} {Human} {Utility} of {Free}-text {Rationales}},
	shorttitle = {Are {Machine} {Rationales} ({Not}) {Useful} to {Humans}?},
	url = {https://aclanthology.org/2023.acl-long.392},
	abstract = {Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale's helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs' ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project.},
	urldate = {2023-07-20},
	booktitle = {Proc. of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Brihi and Liu, Ziyi and Ramnath, Sahana and Chan, Aaron and Tong, Zhewei and Nie, Shaoliang and Wang, Qifan and Choi, Yejin and Ren, Xiang},
	month = jul,
	year = {2023},
	pages = {7103--7128},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\X2V9A3XE\\Joshi et al. - 2023 - Are Machine Rationales (Not) Useful to Humans Mea.pdf:application/pdf},
}

@article{noy_experimental_nodate,
	title = {Experimental {Evidence} on the {Productivity} {Effects} of {Generative} {Artificial} {Intelligence}},
	abstract = {We examine the productivity effects of a generative artificial intelligence technology—the assistive chatbot ChatGPT—in the context of mid-level professional writing tasks. In a preregistered online experiment, we assign occupation-specific, incentivized writing tasks to 444 college-educated professionals, and randomly expose half of them to ChatGPT. Our results show that ChatGPT substantially raises average productivity: time taken decreases by 0.8 SDs and output quality rises by 0.4 SDs. Inequality between workers decreases, as ChatGPT compresses the productivity distribution by benefiting low-ability workers more. ChatGPT mostly substitutes for worker effort rather than complementing worker skills, and restructures tasks towards idea-generation and editing and away from rough-drafting. Exposure to ChatGPT increases job satisfaction and self-efficacy and heightens both concern and excitement about automation technologies.},
	language = {en},
	author = {Noy, Shakked and Zhang, Whitney},
	keywords = {notion},
	file = {Noy et Zhang - Experimental Evidence on the Productivity Effects .pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\LWL44D55\\Noy et Zhang - Experimental Evidence on the Productivity Effects .pdf:application/pdf},
}

@misc{lengerich_llms_2023,
	title = {{LLMs} {Understand} {Glass}-{Box} {Models}, {Discover} {Surprises}, and {Suggest} {Repairs}},
	url = {http://arxiv.org/abs/2308.01157},
	abstract = {We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package \${\textbackslash}texttt\{TalkToEBM\}\$ as an open-source LLM-GAM interface.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Lengerich, Benjamin J. and Bordt, Sebastian and Nori, Harsha and Nunnally, Mark E. and Aphinyanaphongs, Yin and Kellis, Manolis and Caruana, Rich},
	month = aug,
	year = {2023},
	note = {arXiv:2308.01157 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\5LZLQMHN\\2308.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\F57KSLAD\\Lengerich et al. - 2023 - LLMs Understand Glass-Box Models, Discover Surpris.pdf:application/pdf},
}

@misc{wang_self-consistency_2023-1,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\DQEYI5U7\\2203.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\Y4Q5KKSE\\Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf},
}

@misc{doshi-velez_towards_2017-1,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv:1702.08608 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\CHTIFV8X\\1702.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\7KSS23IZ\\Doshi-Velez et Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf},
}

@misc{karimi_relationship_2023,
	title = {On the {Relationship} {Between} {Explanation} and {Prediction}: {A} {Causal} {View}},
	shorttitle = {On the {Relationship} {Between} {Explanation} and {Prediction}},
	url = {http://arxiv.org/abs/2212.06925},
	abstract = {Being able to provide explanations for a model's decision has become a central requirement for the development, deployment, and adoption of machine learning models. However, we are yet to understand what explanation methods can and cannot do. How do upstream factors such as data, model prediction, hyperparameters, and random initialization influence downstream explanations? While previous work raised concerns that explanations (E) may have little relationship with the prediction (Y), there is a lack of conclusive study to quantify this relationship. Our work borrows tools from causal inference to systematically assay this relationship. More specifically, we study the relationship between E and Y by measuring the treatment effect when intervening on their causal ancestors, i.e., on hyperparameters and inputs used to generate saliency-based Es or Ys. Our results suggest that the relationships between E and Y is far from ideal. In fact, the gap between 'ideal' case only increase in higher-performing models -- models that are likely to be deployed. Our work is a promising first step towards providing a quantitative measure of the relationship between E and Y, which could also inform the future development of methods for E with a quantitative metric.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Karimi, Amir-Hossein and Muandet, Krikamol and Kornblith, Simon and Schölkopf, Bernhard and Kim, Been},
	month = may,
	year = {2023},
	note = {arXiv:2212.06925 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\8BLPS66M\\2212.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\IH4647QE\\Karimi et al. - 2023 - On the Relationship Between Explanation and Predic.pdf:application/pdf},
}

@misc{karimi_relationship_2023-1,
	title = {On the {Relationship} {Between} {Explanation} and {Prediction}: {A} {Causal} {View}},
	shorttitle = {On the {Relationship} {Between} {Explanation} and {Prediction}},
	url = {http://arxiv.org/abs/2212.06925},
	abstract = {Being able to provide explanations for a model's decision has become a central requirement for the development, deployment, and adoption of machine learning models. However, we are yet to understand what explanation methods can and cannot do. How do upstream factors such as data, model prediction, hyperparameters, and random initialization influence downstream explanations? While previous work raised concerns that explanations (E) may have little relationship with the prediction (Y), there is a lack of conclusive study to quantify this relationship. Our work borrows tools from causal inference to systematically assay this relationship. More specifically, we study the relationship between E and Y by measuring the treatment effect when intervening on their causal ancestors, i.e., on hyperparameters and inputs used to generate saliency-based Es or Ys. Our results suggest that the relationships between E and Y is far from ideal. In fact, the gap between 'ideal' case only increase in higher-performing models -- models that are likely to be deployed. Our work is a promising first step towards providing a quantitative measure of the relationship between E and Y, which could also inform the future development of methods for E with a quantitative metric.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Karimi, Amir-Hossein and Muandet, Krikamol and Kornblith, Simon and Schölkopf, Bernhard and Kim, Been},
	month = may,
	year = {2023},
	note = {arXiv:2212.06925 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\NBKXIZRM\\2212.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\R76J49QR\\Karimi et al. - 2023 - On the Relationship Between Explanation and Predic.pdf:application/pdf},
}

@misc{tang_what_2022,
	title = {What the {DAAM}: {Interpreting} {Stable} {Diffusion} {Using} {Cross} {Attention}},
	shorttitle = {What the {DAAM}},
	url = {http://arxiv.org/abs/2210.04885},
	abstract = {Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce pixel-level attribution maps, we upscale and aggregate cross-attention word-pixel scores in the denoising subnetwork, naming our method DAAM. We evaluate its correctness by testing its semantic segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. We then apply DAAM to study the role of syntax in the pixel space, characterizing head--dependent heat map interaction patterns for ten common dependency relations. Finally, we study several semantic phenomena using DAAM, with a focus on feature entanglement, where we find that cohyponyms worsen generation quality and descriptive adjectives attend too broadly. To our knowledge, we are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future lines of research. Our code is at https://github.com/castorini/daam.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Tang, Raphael and Liu, Linqing and Pandey, Akshat and Jiang, Zhiying and Yang, Gefei and Kumar, Karun and Stenetorp, Pontus and Lin, Jimmy and Ture, Ferhan},
	month = dec,
	year = {2022},
	note = {arXiv:2210.04885 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\4QM227PC\\2210.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\9FCCR8J3\\Tang et al. - 2022 - What the DAAM Interpreting Stable Diffusion Using.pdf:application/pdf},
}

@inproceedings{lampinen_can_2022,
	OPTaddress = {Abu Dhabi, United Arab Emirates},
	title = {Can language models learn from explanations in context?},
	url = {https://aclanthology.org/2022.findings-emnlp.38},
	doi = {10.18653/v1/2022.findings-emnlp.38},
	abstract = {Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations, and various matched control explanations. We evaluate how different types of explanations, instructions, and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We find that explanations can improve performance—even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features. However, only large models benefit. In summary, explanations can support the in-context learning of large LMs on challenging tasks.},
	urldate = {2023-08-16},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Lampinen, Andrew and Dasgupta, Ishita and Chan, Stephanie and Mathewson, Kory and Tessler, Mh and Creswell, Antonia and McClelland, James and Wang, Jane and Hill, Felix},
	month = dec,
	year = {2022},
	keywords = {notion},
	pages = {537--563},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\KTKZSHYK\\Lampinen et al. - 2022 - Can language models learn from explanations in con.pdf:application/pdf},
}

@misc{corbett-davies_measure_2023,
	title = {The {Measure} and {Mismeasure} of {Fairness}},
	url = {http://arxiv.org/abs/1808.00023},
	abstract = {The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Corbett-Davies, Sam and Gaebler, Johann D. and Nilforoshan, Hamed and Shroff, Ravi and Goel, Sharad},
	month = aug,
	year = {2023},
	note = {arXiv:1808.00023 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\4WC9J52E\\1808.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\65RWA5NR\\Corbett-Davies et al. - 2023 - The Measure and Mismeasure of Fairness.pdf:application/pdf},
}

@misc{liu_trustworthy_2023,
	title = {Trustworthy {LLMs}: a {Survey} and {Guideline} for {Evaluating} {Large} {Language} {Models}' {Alignment}},
	shorttitle = {Trustworthy {LLMs}},
	url = {http://arxiv.org/abs/2308.05374},
	abstract = {Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.},
	urldate = {2023-08-16},
	publisher = {arXiv},
	author = {Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Cheng, Ruocheng Guo Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05374 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\HE9N4W3M\\2308.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\63UUF2TM\\Liu et al. - 2023 - Trustworthy LLMs a Survey and Guideline for Evalu.pdf:application/pdf},
}

@misc{gulcehre_reinforced_2023,
	title = {Reinforced {Self}-{Training} ({ReST}) for {Language} {Modeling}},
	url = {http://arxiv.org/abs/2308.08998},
	abstract = {Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.},
	urldate = {2023-08-25},
	publisher = {arXiv},
	author = {Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and Macherey, Wolfgang and Doucet, Arnaud and Firat, Orhan and de Freitas, Nando},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08998 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\JQS2BRML\\2308.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\TE3PR7G6\\Gulcehre et al. - 2023 - Reinforced Self-Training (ReST) for Language Model.pdf:application/pdf},
}
@article{yao_tree_2023,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@misc{yang_auto-gpt_2023,
	title = {Auto-{GPT} for {Online} {Decision} {Making}: {Benchmarks} and {Additional} {Opinions}},
	shorttitle = {Auto-{GPT} for {Online} {Decision} {Making}},
	url = {http://arxiv.org/abs/2306.02224},
	abstract = {Auto-GPT is an autonomous agent that leverages recent advancements in adapting Large Language Models (LLMs) for decision-making tasks. While there has been a growing interest in Auto-GPT stypled agents, questions remain regarding the effectiveness and flexibility of Auto-GPT in solving real-world decision-making tasks. Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties. In this paper, we present a comprehensive benchmark study of Auto-GPT styled agents in decision-making tasks that simulate realworld scenarios. Our aim is to gain deeper insights into this problem and understand the adaptability of GPT-based agents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5, Claude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we introduce the Additional Opinions algorithm, an easy and effective method that incorporates supervised/imitation-based learners into the Auto-GPT scheme. This approach enables lightweight supervised learning without requiring fine-tuning of the foundational LLMs. We demonstrate through careful baseline comparisons and ablation studies that the Additional Opinions algorithm significantly enhances performance in online decision-making benchmarks, including WebShop and ALFWorld.},
	language = {en},
	urldate = {2023-08-25},
	publisher = {arXiv},
	author = {Yang, Hui and Yue, Sifu and He, Yunzhong},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02224 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, notion},
	file = {Yang et al. - 2023 - Auto-GPT for Online Decision Making Benchmarks an.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\WG93YHAT\\Yang et al. - 2023 - Auto-GPT for Online Decision Making Benchmarks an.pdf:application/pdf},
}

@misc{zhou_large_2023,
	title = {Large {Language} {Models} {Are} {Human}-{Level} {Prompt} {Engineers}},
	url = {http://arxiv.org/abs/2211.01910},
	abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
	language = {en},
	urldate = {2023-08-25},
	publisher = {arXiv},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	month = mar,
	year = {2023},
	note = {arXiv:2211.01910 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
	file = {Zhou et al. - 2023 - Large Language Models Are Human-Level Prompt Engin.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\EC4FCGTF\\Zhou et al. - 2023 - Large Language Models Are Human-Level Prompt Engin.pdf:application/pdf},
}

@article{sajjad_neuron-level_2022,
	title = {Neuron-level {Interpretation} of {Deep} {NLP} {Models}: {A} {Survey}},
	volume = {10},
	shorttitle = {Neuron-level {Interpretation} of {Deep} {NLP} {Models}},
	url = {https://aclanthology.org/2022.tacl-1.74},
	doi = {10.1162/tacl_a_00519},
	abstract = {The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions.},
	urldate = {2023-08-28},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim},
	year = {2022},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	keywords = {notion},
	pages = {1285--1303},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\V73IUB2L\\Sajjad et al. - 2022 - Neuron-level Interpretation of Deep NLP Models A .pdf:application/pdf},
}

@article{freiesleben_intriguing_2022,
	title = {The {Intriguing} {Relation} {Between} {Counterfactual} {Explanations} and {Adversarial} {Examples}},
	volume = {32},
	issn = {0924-6495, 1572-8641},
	url = {http://arxiv.org/abs/2009.05487},
	doi = {10.1007/s11023-021-09580-9},
	abstract = {The same method that creates adversarial examples (AEs) to fool image-classifiers can be used to generate counterfactual explanations (CEs) that explain algorithmic decisions. This observation has led researchers to consider CEs as AEs by another name. We argue that the relationship to the true label and the tolerance with respect to proximity are two properties that formally distinguish CEs and AEs. Based on these arguments, we introduce CEs, AEs, and related concepts mathematically in a common framework. Furthermore, we show connections between current methods for generating CEs and AEs, and estimate that the fields will merge more and more as the number of common use-cases grows.},
	number = {1},
	urldate = {2023-08-28},
	journal = {Minds and Machines},
	author = {Freiesleben, Timo},
	month = mar,
	year = {2022},
	note = {arXiv:2009.05487 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {77--109},
	file = {arXiv Fulltext PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\N6CLLTQZ\\Freiesleben - 2022 - The Intriguing Relation Between Counterfactual Exp.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\G359IY4M\\2009.html:text/html},
}

@inproceedings{cot,
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
title = {Chain-of-thought prompting elicits reasoning in large language models},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proc. of the 36th Int. Conf. on Neural Information Processing Systems},
numpages = {14},
series = {NIPS '22}
}

@misc{dong_survey_2023,
	title = {A {Survey} on {In}-context {Learning}},
	url = {http://arxiv.org/abs/2301.00234},
	abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
	month = jun,
	year = {2023},
	note = {arXiv:2301.00234 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\DJJUIVX2\\2301.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\7AJZQUWW\\Dong et al. - 2023 - A Survey on In-context Learning.pdf:application/pdf},
}

@misc{holtzman_generative_2023,
	title = {Generative {Models} as a {Complex} {Systems} {Science}: {How} can we make sense of large language model behavior?},
	shorttitle = {Generative {Models} as a {Complex} {Systems} {Science}},
	url = {http://arxiv.org/abs/2308.00189},
	abstract = {Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases. Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Holtzman, Ari and West, Peter and Zettlemoyer, Luke},
	month = jul,
	year = {2023},
	note = {arXiv:2308.00189 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\LNFK67VL\\2308.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\3GXBFSFQ\\Holtzman et al. - 2023 - Generative Models as a Complex Systems Science Ho.pdf:application/pdf},
}

@misc{madaan_text_2022,
	title = {Text and {Patterns}: {For} {Effective} {Chain} of {Thought}, {It} {Takes} {Two} to {Tango}},
	shorttitle = {Text and {Patterns}},
	url = {http://arxiv.org/abs/2209.07686},
	abstract = {The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.},
	urldate = {2023-08-28},
	publisher = {arXiv},
	author = {Madaan, Aman and Yazdanbakhsh, Amir},
	month = oct,
	year = {2022},
	note = {arXiv:2209.07686 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\5Z84RLWE\\2209.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\KRXY6U8X\\Madaan et Yazdanbakhsh - 2022 - Text and Patterns For Effective Chain of Thought,.pdf:application/pdf},
}

@inproceedings{wang_towards_2023,
	address = {Toronto, Canada},
	title = {Towards {Understanding} {Chain}-of-{Thought} {Prompting}: {An} {Empirical} {Study} of {What} {Matters}},
	shorttitle = {Towards {Understanding} {Chain}-of-{Thought} {Prompting}},
	url = {https://aclanthology.org/2023.acl-long.153},
	doi = {10.18653/v1/2023.acl-long.153},
	abstract = {Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90\% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs' capability to learn to reason in context.},
	urldate = {2023-08-28},
	booktitle = {Proc. of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {2717--2739},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\HQUEXS2L\\Wang et al. - 2023 - Towards Understanding Chain-of-Thought Prompting .pdf:application/pdf},
}

@article{de_oliveira_model-agnostic_2023,
	title = {A model-agnostic and data-independent tabu search algorithm to generate counterfactuals for tabular, image, and text data},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221723006598},
	doi = {10.1016/j.ejor.2023.08.031},
	abstract = {The growing prevalence of artificial decision systems has prompted a keen interest in their efficiency, yet this progress is accompanied by their inherent complexity. This poses a significant challenge for various domains, including operational research, where decisions hold crucial influence over outcomes and thus must not remain undisclosed. Counterfactual explanations are greatly remarked as a simple (to understand) yet efficient way to explain the decisions made by a machine learning model by finding a minimal set of changes required to change the prediction outcome for a specific instance. We, then, present a novel algorithmic approach, called CFNOW, which implements a modular, fast, two-step process using tabu search, a well-known metaheuristic framework, to find counterfactuals for multiple data types (tabular, image, and text) with high efficiency. We run an extensive benchmark study with more than 5,000 factual points from 25 datasets to demonstrate that CFNOW can generate high-quality counterfactual results in terms of metrics such as speed, coverage, distance, and sparsity, surpassing the state-of-the-art. These characteristics, associated with the simple code implementation, may aid embedding explainability to complex models which are often necessary for compliance requirements.},
	urldate = {2023-08-29},
	journal = {European Journal of Operational Research},
	author = {de Oliveira, Raphael Mazzine Barbosa and Sörensen, Kenneth and Martens, David},
	month = aug,
	year = {2023},
	keywords = {Deep Learning, notion, Counterfactual Explanations, Decision Support, eXplainable Artificial Intelligence},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\TINDLEZ4\\de Oliveira et al. - 2023 - A model-agnostic and data-independent tabu search .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\LMD8JVEY\\S0377221723006598.html:text/html},
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in Neural Information Processing Systems, NeurIPS22},
  volume={35},
  pages={22199--22213},
  year={2022},
url= {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conf..pdf}
}

@InProceedings{krishna_post_2023,
  author =    {Krishna, Satyapriya and Ma, Jiaqi and Slack, Dylan and Ghandeharioun, Asma and Singh, Sameer and Lakkaraju, Himabindu},
  title =     {Post {Hoc} {Explanations} of {Language} {Models} {Can} {Improve} {Language} {Models}},
  booktitle = {Advances in Neural Information Processing Systems, NeurIPS23},
  year =      {2023},
  OPTvolume = {},
  OPTpages =  {},
  OPTpublisher = {},
	url = {http://arxiv.org/abs/2305.11426},
}

@inproceedings{faithful_xai,
  title={Fairness via explanation quality: Evaluating disparities in the quality of post hoc explanations},
  author={Dai, Jessica and Upadhyay, Sohini and Aivodji, Ulrich and Bach, Stephen H and Lakkaraju, Himabindu},
  booktitle={Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={203--214},
  year={2022}
}

@inproceedings{atanasova-etal-2023-faithfulness,
    title = "Faithfulness Tests for Natural Language Explanations",
    author = "Atanasova, Pepa  and
      Camburu, Oana-Maria  and
      Lioma, Christina  and
      Lukasiewicz, Thomas  and
      Simonsen, Jakob Grue  and
      Augenstein, Isabelle",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Procs. of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.25",
    doi = "10.18653/v1/2023.acl-short.25",
    pages = "283--294"
}

@inproceedings{self_consistency_madsen,
  title={Are self-explanations from large language models faithful},
  author={Madsen, Andreas and Chandar, Sarath and Reddy, Siva},
  booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
  year={2024}
}

@inproceedings{eraser,
    title = "{ERASER}: {A} Benchmark to Evaluate Rationalized {NLP} Models",
    author = "DeYoung, Jay  and
      Jain, Sarthak  and
      Rajani, Nazneen Fatema  and
      Lehman, Eric  and
      Xiong, Caiming  and
      Socher, Richard  and
      Wallace, Byron C.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Procs. of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.408",
    doi = "10.18653/v1/2020.acl-main.408",
    pages = "4443--4458"
}

@article{esnli,
  title={e-snli: Natural language inference with natural language explanations},
  author={Camburu, Oana-Maria and Rockt{\"a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{trust_xai,
  title={How explainability contributes to trust in AI},
  author={Ferrario, Andrea and Loi, Michele},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1457--1466},
  year={2022}
}

@inproceedings{wiegreffe2021measuring,
  title={Measuring Association Between Labels and Free-Text Rationales},
  author={Wiegreffe, Sarah and Marasovi{\'c}, Ana and Smith, Noah A},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={10266--10284},
  year={2021}
}

@misc{krishna_post_2023_old,
	title = {Post {Hoc} {Explanations} of {Language} {Models} {Can} {Improve} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.11426},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25\% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.},
	urldate = {2023-08-29},
	publisher = {arXiv},
	author = {Krishna, Satyapriya and Ma, Jiaqi and Slack, Dylan and Ghandeharioun, Asma and Singh, Sameer and Lakkaraju, Himabindu},
	month = may,
	year = {2023},
	note = {arXiv:2305.11426 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\PQ6ALVHZ\\2305.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\RAFAHCD8\\Krishna et al. - 2023 - Post Hoc Explanations of Language Models Can Impro.pdf:application/pdf},
}

@misc{ho_international_2023,
	title = {International {Institutions} for {Advanced} {AI}},
	url = {http://arxiv.org/abs/2307.04699},
	abstract = {International institutions may have an important role to play in ensuring advanced AI systems benefit humanity. International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits. Conversely, the potential dangerous capabilities of powerful and general-purpose AI systems create global externalities in their development and deployment, and international efforts to further responsible AI practices could help manage the risks they pose. This paper identifies a set of governance functions that could be performed at an international level to address these challenges, ranging from supporting access to frontier AI systems to setting international safety standards. It groups these functions into four institutional models that exhibit internal synergies and have precedents in existing organizations: 1) a Commission on Frontier AI that facilitates expert consensus on opportunities and risks from advanced AI, 2) an Advanced AI Governance Organization that sets international standards to manage global threats from advanced models, supports their implementation, and possibly monitors compliance with a future governance regime, 3) a Frontier AI Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety Project that brings together leading researchers and engineers to further AI safety research. We explore the utility of these models and identify open questions about their viability.},
	urldate = {2023-08-29},
	publisher = {arXiv},
	author = {Ho, Lewis and Barnhart, Joslyn and Trager, Robert and Bengio, Yoshua and Brundage, Miles and Carnegie, Allison and Chowdhury, Rumman and Dafoe, Allan and Hadfield, Gillian and Levi, Margaret and Snidal, Duncan},
	month = jul,
	year = {2023},
	note = {arXiv:2307.04699 [cs]},
	keywords = {Computer Science - Computers and Society, notion, K.4.1},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\ZESHYEZP\\2307.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\9K26HZBR\\Ho et al. - 2023 - International Institutions for Advanced AI.pdf:application/pdf},
}

@inproceedings{plyler_making_2021,
	title = {Making a ({Counterfactual}) {Difference} {One} {Rationale} at a {Time}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/f0f800c92d191d736c4411f3b3f8ef4a-Abstract.html},
	abstract = {Rationales, snippets of extracted text that explain an inference, have emerged as a popular framework for interpretable natural language processing (NLP). Rationale models typically consist of two cooperating modules: a selector and a classifier with the goal of maximizing the mutual information (MMI) between the "selected" text and the document label. Despite their promises, MMI-based methods often pick up on spurious text patterns and result in models with nonsensical behaviors. In this work, we investigate whether counterfactual data augmentation (CDA), without human assistance, can improve the performance of the selector by lowering the mutual information between spurious signals and the document label. Our counterfactuals are produced in an unsupervised fashion using class-dependent generative models. From an information theoretic lens, we derive properties of the unaugmented dataset for which our CDA approach would succeed. The effectiveness of CDA is empirically evaluated by comparing against several baselines including an improved MMI-based rationale schema on two multi-aspect datasets. Our results show that CDA produces rationales that better capture the signal of interest.},
	urldate = {2023-08-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Plyler, Mitchell and Green, Michael and Chi, Min},
	year = {2021},
	keywords = {notion},
	pages = {28701--28713},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\U53J4ACU\\Plyler et al. - 2021 - Making a (Counterfactual) Difference One Rationale.pdf:application/pdf},
}

@misc{li_large_2023,
	title = {Large {Language} {Models} as {Counterfactual} {Generator}: {Strengths} and {Weaknesses}},
	shorttitle = {Large {Language} {Models} as {Counterfactual} {Generator}},
	url = {http://arxiv.org/abs/2305.14791},
	abstract = {Large language models (LLMs) have demonstrated remarkable performance in a range of natural language understanding and generation tasks. Yet, their ability to generate counterfactuals, which can be used for areas like data augmentation, remains under-explored. This study aims to investigate the counterfactual generation capabilities of LLMs and analysis factors that influence this ability. First, we evaluate how effective are LLMs in counterfactual generation through data augmentation experiments for small language models (SLMs) across four tasks: sentiment analysis, natural language inference, named entity recognition, and relation extraction. While LLMs show promising enhancements in various settings, they struggle in complex tasks due to their self-limitations and the lack of logical guidance to produce counterfactuals that align with commonsense. Second, our analysis reveals the pivotal role of providing accurate task definitions and detailed step-by-step instructions to LLMs in generating counterfactuals. Interestingly, we also find that LLMs can generate reasonable counterfactuals even with unreasonable demonstrations, which illustrates that demonstrations are primarily to regulate the output format.This study provides the first comprehensive insight into counterfactual generation abilities of LLMs, and offers a novel perspective on utilizing LLMs for data augmentation to enhance SLMs.},
	urldate = {2023-08-29},
	publisher = {arXiv},
	author = {Li, Yongqi and Xu, Mayi and Miao, Xin and Zhou, Shen and Qian, Tieyun},
	month = may,
	year = {2023},
	note = {arXiv:2305.14791 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\BIR5BLMR\\2305.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\H6N8M3CY\\Li et al. - 2023 - Large Language Models as Counterfactual Generator.pdf:application/pdf},
}

@misc{madaan_self-refine_2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2023-08-29},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\VXJ3JU9K\\2303.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\6UVGK9HW\\Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf:application/pdf},
}

@misc{mehrabi_survey_2022,
	title = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1908.09635},
	abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	urldate = {2023-08-30},
	publisher = {arXiv},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	month = jan,
	year = {2022},
	note = {arXiv:1908.09635 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\TFL54PLE\\1908.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\DQGVW4HB\\Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf:application/pdf},
}

@misc{besta_graph_2023,
	title = {Graph of {Thoughts}: {Solving} {Elaborate} {Problems} with {Large} {Language} {Models}},
	shorttitle = {Graph of {Thoughts}},
	url = {http://arxiv.org/abs/2308.09687},
	abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {\textgreater}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
	urldate = {2023-08-30},
	publisher = {arXiv},
	author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	month = aug,
	year = {2023},
	note = {arXiv:2308.09687 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\CPZWXFDP\\2308.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\4V9NRPRU\\Besta et al. - 2023 - Graph of Thoughts Solving Elaborate Problems with.pdf:application/pdf},
}

@inproceedings{kenthapadi_generative_2023,
	address = {New York, NY, USA},
	series = {{KDD} '23},
	title = {Generative {AI} meets {Responsible} {AI}: {Practical} {Challenges} and {Opportunities}},
	isbn = {9798400701030},
	shorttitle = {Generative {AI} meets {Responsible} {AI}},
	url = {https://dl.acm.org/doi/10.1145/3580305.3599557},
	doi = {10.1145/3580305.3599557},
	abstract = {Generative AI models and applications are being rapidly developed and deployed across a wide spectrum of industries and applications ranging from writing and email assistants to graphic design and art generation to educational assistants to coding to drug discovery. However, there are several ethical and social considerations associated with generative AI models and applications. These concerns include lack of interpretability, bias and discrimination, privacy, lack of model robustness, fake and misleading content, copyright implications, plagiarism, and environmental impact associated with training and inference of generative AI models. In this tutorial, we first motivate the need for adopting responsible AI principles when developing and deploying large language models (LLMs) and other generative AI models, as part of a broader AI model governance and responsible AI framework, from societal, legal, user, and model developer perspectives, and provide a roadmap for thinking about responsible AI for generative AI in practice. We provide a brief technical overview of text and image generation models, and highlight the key responsible AI desiderata associated with these models. We then describe the technical considerations and challenges associated with realizing the above desiderata in practice. We focus on real-world generative AI use cases spanning domains such as media generation, writing assistants, copywriting, code generation, and conversational assistants, present practical solution approaches / guidelines for applying responsible AI techniques effectively, discuss lessons learned from deploying responsible AI approaches for generative AI applications in practice, and highlight the key open research problems. We hope that our tutorial will inform both researchers and practitioners, stimulate further research on responsible AI in the context of generative AI, and pave the way for building more reliable and trustworthy generative AI applications in the future.},
	urldate = {2023-08-31},
	booktitle = {Proc. of the 29th {ACM} {SIGKDD} {Conf.} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Kenthapadi, Krishnaram and Lakkaraju, Himabindu and Rajani, Nazneen},
	month = aug,
	year = {2023},
	keywords = {case studies from industry, ethics in ai, generative ai models and applications, large language models, responsible ai},
	pages = {5805--5806},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\8FL9W4DA\\Kenthapadi et al. - 2023 - Generative AI meets Responsible AI Practical Chal.pdf:application/pdf},
}

@misc{wu_analyzing_2023,
	title = {Analyzing {Chain}-of-{Thought} {Prompting} in {Large} {Language} {Models} via {Gradient}-based {Feature} {Attributions}},
	url = {http://arxiv.org/abs/2307.13339},
	abstract = {Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Wu, Skyler and Shen, Eric Meng and Badrinath, Charumathi and Ma, Jiaqi and Lakkaraju, Himabindu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.13339 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\SJX7XWP8\\2307.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\J9FYJA7H\\Wu et al. - 2023 - Analyzing Chain-of-Thought Prompting in Large Lang.pdf:application/pdf},
}

@misc{kanamori_learning_2023,
	title = {Learning {Locally} {Interpretable} {Rule} {Ensemble}},
	url = {http://arxiv.org/abs/2306.11481},
	abstract = {This paper proposes a new framework for learning a rule ensemble model that is both accurate and interpretable. A rule ensemble is an interpretable model based on the linear combination of weighted rules. In practice, we often face the trade-off between the accuracy and interpretability of rule ensembles. That is, a rule ensemble needs to include a sufficiently large number of weighted rules to maintain its accuracy, which harms its interpretability for human users. To avoid this trade-off and learn an interpretable rule ensemble without degrading accuracy, we introduce a new concept of interpretability, named local interpretability, which is evaluated by the total number of rules necessary to express individual predictions made by the model, rather than to express the model itself. Then, we propose a regularizer that promotes local interpretability and develop an efficient algorithm for learning a rule ensemble with the proposed regularizer by coordinate descent with local search. Experimental results demonstrated that our method learns rule ensembles that can explain individual predictions with fewer rules than the existing methods, including RuleFit, while maintaining comparable accuracy.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Kanamori, Kentaro},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11481 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\74VY95AY\\2306.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\Y33KRM7T\\Kanamori - 2023 - Learning Locally Interpretable Rule Ensemble.pdf:application/pdf},
}

@misc{janizek_explaining_2020,
	title = {Explaining {Explanations}: {Axiomatic} {Feature} {Interactions} for {Deep} {Networks}},
	shorttitle = {Explaining {Explanations}},
	url = {http://arxiv.org/abs/2002.04138},
	abstract = {Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain which features were most important to a model's prediction on a given input. However, for many tasks, simply knowing which features were important to a model's prediction may not provide enough insight to understand model behavior. The interactions between features within the model may better help us understand not only the model, but also why certain features are more important than others. In this work, we present Integrated Hessians, an extension of Integrated Gradients that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods to explain interactions, and unlike such previous methods is not limited to a specific architecture or class of neural network. Additionally, we find that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks. Code available at https://github.com/suinleelab/path\_explain},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Janizek, Joseph D. and Sturmfels, Pascal and Lee, Su-In},
	month = jun,
	year = {2020},
	note = {arXiv:2002.04138 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\M99I9NWG\\2002.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\WS9JFMED\\Janizek et al. - 2020 - Explaining Explanations Axiomatic Feature Interac.pdf:application/pdf},
}

@inproceedings{tsang_how_2020,
	title = {How does {This} {Interaction} {Affect} {Me}? {Interpretable} {Attribution} for {Feature} {Interactions}},
	volume = {33},
	shorttitle = {How does {This} {Interaction} {Affect} {Me}?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/443dec3062d0286986e21dc0631734c9-Abstract.html},
	abstract = {Machine learning transparency calls for interpretable explanations of how inputs relate to predictions. Feature attribution is a way to analyze the impact of features on predictions. Feature interactions are the contextual dependence between features that jointly impact predictions. There are a number of methods that extract feature interactions in prediction models; however, the methods that assign attributions to interactions are either uninterpretable, model-specific, or non-axiomatic. We propose an interaction attribution and detection framework called Archipelago which addresses these problems and is also scalable in real-world settings. Our experiments on standard annotation labels indicate our approach provides significantly more interpretable explanations than comparable methods, which is important for analyzing the impact of interactions on predictions. We also provide accompanying visualizations of our approach that give new insights into deep neural networks.},
	urldate = {2023-09-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tsang, Michael and Rambhatla, Sirisha and Liu, Yan},
	year = {2020},
	keywords = {notion},
	pages = {6147--6159},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\Q4W9LTT2\\Tsang et al. - 2020 - How does This Interaction Affect Me Interpretable.pdf:application/pdf},
}

@inproceedings{pistilli_stronger_2023,
	address = {Chicago IL USA},
	title = {Stronger {Together}: on the {Articulation} of {Ethical} {Charters}, {Legal} {Tools}, and {Technical} {Documentation} in {ML}},
	isbn = {9798400701924},
	shorttitle = {Stronger {Together}},
	url = {https://dl.acm.org/doi/10.1145/3593013.3594002},
	doi = {10.1145/3593013.3594002},
	language = {en},
	urldate = {2023-09-04},
	booktitle = {2023 {ACM} {Conf.} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Pistilli, Giada and Muñoz Ferrandis, Carlos and Jernite, Yacine and Mitchell, Margaret},
	month = jun,
	year = {2023},
	keywords = {notion},
	pages = {343--354},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\5MSZ882F\\Pistilli et al. - 2023 - Stronger Together on the Articulation of Ethical .pdf:application/pdf},
}

@article{browning_ai_2022,
	title = {{AI} {And} {The} {Limits} {Of} {Language}},
	url = {https://www.noemamag.com/ai-and-the-limits-of-language},
	abstract = {An artificial intelligence system trained on words and sentences alone will never approximate human understanding.},
	language = {en-US},
	urldate = {2023-09-04},
	author = {Browning, Jacob},
	month = aug,
	year = {2022},
	keywords = {notion},
	file = {Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\UKJQTPDW\\ai-and-the-limits-of-language.html:text/html},
}

@misc{liu_trustworthy_2023-1,
	title = {Trustworthy {LLMs}: a {Survey} and {Guideline} for {Evaluating} {Large} {Language} {Models}' {Alignment}},
	shorttitle = {Trustworthy {LLMs}},
	url = {http://arxiv.org/abs/2308.05374},
	abstract = {Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Guo, Ruocheng and Cheng, Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05374 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\SERPQZ47\\2308.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\5NNSZRTW\\Liu et al. - 2023 - Trustworthy LLMs a Survey and Guideline for Evalu.pdf:application/pdf},
}

@article{olah_feature_2017,
	title = {Feature {Visualization}},
	volume = {2},
	issn = {2476-0757},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	number = {11},
	urldate = {2023-09-05},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	month = nov,
	year = {2017},
	pages = {10.23915/distill.00007},
}

@article{olah_zoom_2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	volume = {5},
	issn = {2476-0757},
	shorttitle = {Zoom {In}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	number = {3},
	urldate = {2023-09-05},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	month = mar,
	year = {2020},
	keywords = {notion},
	pages = {10.23915/distill.00024.001},
}

@article{cammarata_curve_2020,
	title = {Curve {Detectors}},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	number = {6},
	urldate = {2023-09-05},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	month = jun,
	year = {2020},
	keywords = {notion},
	pages = {10.23915/distill.00024.003},
}

@article{olah_building_2018,
	title = {The {Building} {Blocks} of {Interpretability}},
	volume = {3},
	issn = {2476-0757},
	url = {https://distill.pub/2018/building-blocks},
	doi = {10.23915/distill.00010},
	number = {3},
	urldate = {2023-09-05},
	journal = {Distill},
	author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
	month = mar,
	year = {2018},
	keywords = {notion},
	pages = {10.23915/distill.00010},
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{gpt3,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	OPTpublisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	keywords = {notion},
	pages = {1877--1901},
	}

@article{zecevic_causal_nodate,
	title = {Causal {Parrots}: {Large} {Language} {Models} {May} {Talk} {Causality} {But} {Are} {Not} {Causal}},
	language = {en},
	author = {Zečević, Matej and Willig, Moritz and Dhami, Devendra Singh and Kersting, Kristian},
	keywords = {notion},
	file = {Zečević et al. - Causal Parrots Large Language Models May Talk Cau.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\MKJLSXWK\\Zečević et al. - Causal Parrots Large Language Models May Talk Cau.pdf:application/pdf},
}

@misc{gallegos_bias_2023,
	title = {Bias and {Fairness} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Bias and {Fairness} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.00770},
	abstract = {Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.},
	urldate = {2023-09-11},
	publisher = {arXiv},
	author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.00770 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\KBV9UFLG\\2309.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\C4LSL4ZT\\Gallegos et al. - 2023 - Bias and Fairness in Large Language Models A Surv.pdf:application/pdf},
}

@misc{krishna_towards_2023,
	title = {Towards {Bridging} the {Gaps} between the {Right} to {Explanation} and the {Right} to be {Forgotten}},
	url = {http://arxiv.org/abs/2302.04288},
	abstract = {The Right to Explanation and the Right to be Forgotten are two important principles outlined to regulate algorithmic decision making and data usage in real-world applications. While the right to explanation allows individuals to request an actionable explanation for an algorithmic decision, the right to be forgotten grants them the right to ask for their data to be deleted from all the databases and models of an organization. Intuitively, enforcing the right to be forgotten may trigger model updates which in turn invalidate previously provided explanations, thus violating the right to explanation. In this work, we investigate the technical implications arising due to the interference between the two aforementioned regulatory principles, and propose the first algorithmic framework to resolve the tension between them. To this end, we formulate a novel optimization problem to generate explanations that are robust to model updates due to the removal of training data instances by data deletion requests. We then derive an efficient approximation algorithm to handle the combinatorial complexity of this optimization problem. We theoretically demonstrate that our method generates explanations that are provably robust to worst-case data deletion requests with bounded costs in case of linear models and certain classes of non-linear models. Extensive experimentation with real-world datasets demonstrates the efficacy of the proposed framework.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Krishna, Satyapriya and Ma, Jiaqi and Lakkaraju, Himabindu},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04288 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\3IVJJBID\\2302.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\FAU6WRV3\\Krishna et al. - 2023 - Towards Bridging the Gaps between the Right to Exp.pdf:application/pdf},
}

@misc{chen_what_2022,
	title = {What {Makes} a {Good} {Explanation}?: {A} {Harmonized} {View} of {Properties} of {Explanations}},
	shorttitle = {What {Makes} a {Good} {Explanation}?},
	url = {http://arxiv.org/abs/2211.05667},
	abstract = {Interpretability provides a means for humans to verify aspects of machine learning (ML) models and empower human+ML teaming in situations where the task cannot be fully automated. Different contexts require explanations with different properties. For example, the kind of explanation required to determine if an early cardiac arrest warning system is ready to be integrated into a care setting is very different from the type of explanation required for a loan applicant to help determine the actions they might need to take to make their application successful. Unfortunately, there is a lack of standardization when it comes to properties of explanations: different papers may use the same term to mean different quantities, and different terms to mean the same quantity. This lack of a standardized terminology and categorization of the properties of ML explanations prevents us from both rigorously comparing interpretable machine learning methods and identifying what properties are needed in what contexts. In this work, we survey properties defined in interpretable machine learning papers, synthesize them based on what they actually measure, and describe the trade-offs between different formulations of these properties. In doing so, we enable more informed selection of task-appropriate formulations of explanation properties as well as standardization for future work in interpretable machine learning.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Chen, Zixi and Subhash, Varshini and Havasi, Marton and Pan, Weiwei and Doshi-Velez, Finale},
	month = dec,
	year = {2022},
	note = {arXiv:2211.05667 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\N7HGK9YD\\2211.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\D6WPFZYC\\Chen et al. - 2022 - What Makes a Good Explanation A Harmonized View .pdf:application/pdf},
}

@article{ali_explainable_2023-1,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {What} we know and what is left to attain {Trustworthy} {Artificial} {Intelligence}},
	volume = {99},
	issn = {15662535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253523001148},
	doi = {10.1016/j.inffus.2023.101805},
	abstract = {Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model’s decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and other related aspects. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The study starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) posthoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.},
	language = {en},
	urldate = {2023-09-12},
	journal = {Information Fusion},
	author = {Ali, Sajid and Abuhmed, Tamer and El-Sappagh, Shaker and Muhammad, Khan and Alonso-Moral, Jose M. and Confalonieri, Roberto and Guidotti, Riccardo and Del Ser, Javier and Díaz-Rodríguez, Natalia and Herrera, Francisco},
	month = nov,
	year = {2023},
	keywords = {notion},
	pages = {101805},
	file = {Ali et al. - 2023 - Explainable Artificial Intelligence (XAI) What we.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\6YI3G478\\Ali et al. - 2023 - Explainable Artificial Intelligence (XAI) What we.pdf:application/pdf},
}

@inproceedings{dai_fairness_2022,
	address = {New York, NY, USA},
	series = {{AIES} '22},
	title = {Fairness via {Explanation} {Quality}: {Evaluating} {Disparities} in the {Quality} of {Post} hoc {Explanations}},
	isbn = {978-1-4503-9247-1},
	shorttitle = {Fairness via {Explanation} {Quality}},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534159},
	doi = {10.1145/3514094.3534159},
	abstract = {As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across all subgroups of a population. For instance, it should not be the case that explanations associated with instances belonging to, e.g., women, are less accurate than those associated with other genders. In this work, we initiate the study of identifying group-based disparities in explanation quality. To this end, we first outline several key properties that contribute to explanation quality-namely, fidelity (accuracy), stability, consistency, and sparsity-and discuss why and how disparities in these properties can be particularly problematic. We then propose an evaluation framework which can quantitatively measure disparities in the quality of explanations. Using this framework, we carry out an empirical analysis with three datasets, six post hoc explanation methods, and different model classes to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and non-linear. We also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit disparities. Our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.},
	urldate = {2023-10-06},
	booktitle = {Proc. of the 2022 {AAAI}/{ACM} {Conf.} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Dai, Jessica and Upadhyay, Sohini and Aivodji, Ulrich and Bach, Stephen H. and Lakkaraju, Himabindu},
	month = jul,
	year = {2022},
	keywords = {fairness, notion, interpretability, explainable machine learning, robustness},
	pages = {203--214},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\ZQRNWICX\\Dai et al. - 2022 - Fairness via Explanation Quality Evaluating Dispa.pdf:application/pdf},
}

@inproceedings{chen_neural_2018,
	title = {Neural {Ordinary} {Differential} {Equations}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2023-10-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	year = {2018},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\X99E3KRK\\Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf:application/pdf},
}

@misc{gat_faithful_2023,
	title = {Faithful {Explanations} of {Black}-box {NLP} {Models} {Using} {LLM}-generated {Counterfactuals}},
	url = {http://arxiv.org/abs/2310.00603},
	abstract = {Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.},
	language = {en},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Gat, Yair and Calderon, Nitay and Feder, Amir and Chapanin, Alexander and Sharma, Amit and Reichart, Roi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.00603 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
	file = {Gat et al. - 2023 - Faithful Explanations of Black-box NLP Models Usin.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\A8LSZ9VW\\Gat et al. - 2023 - Faithful Explanations of Black-box NLP Models Usin.pdf:application/pdf},
}


@Article{zhao_explainability_2023,
  author =  {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  title =   {Explainability for {Large} {Language} {Models}: {A} {Survey}},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  year =    {2024},
url={https://dl.acm.org/doi/10.1145/3639372}
}

@misc{zhao_explainability_2023_old,
	title = {Explainability for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Explainability for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.01029},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.},
	language = {en},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01029 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
	file = {Zhao et al. - 2023 - Explainability for Large Language Models A Survey.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\R5BBI7Y6\\Zhao et al. - 2023 - Explainability for Large Language Models A Survey.pdf:application/pdf},
}

@misc{stammer_learning_2023,
	title = {Learning by {Self}-{Explaining}},
	url = {http://arxiv.org/abs/2309.08395},
	abstract = {Artificial intelligence (AI) research has a long track record of drawing inspirations from findings from biology, in particular human intelligence. In contrast to current AI research that mainly treats explanations as a means for model inspection, a somewhat neglected finding from human psychology is the benefit of self-explaining in an agents’ learning process. Motivated by this, we introduce a novel learning paradigm, termed Learning by Self-Explaining (LSX). The underlying idea is that a learning module (learner) performs a base task, e.g. image classification, and provides explanations to its decisions. An internal critic module next evaluates the quality of these explanations given the original task. Finally, the learner is refined with the critic’s feedback and the loop is repeated as required. The intuition behind this is that an explanation is considered “good” if the critic can perform the same task given the respective explanation. Despite many implementation possibilities the structure of any LSX instantiation can be taxonomized based on four learning modules which we identify as: FIT, EXPLAIN, REFLECT and REVISE. In our work, we provide distinct instantiations of LSX for two different learner models, each illustrating different choices for the various LSX components. We broadly evaluate these on several datasets and show that Learning by Self-Explaining not only boosts the generalization abilities of AI models, particularly in small-data regimes, but also aids in mitigating the influence of confounding factors, as well as leading to more task-specific and faithful model explanations. Overall, our results provide experimental evidence of the potential of self-explaining within the learning phase of an AI model.},
	language = {en},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Stammer, Wolfgang and Friedrich, Felix and Steinmann, David and Shindo, Hikaru and Kersting, Kristian},
	month = sep,
	year = {2023},
	note = {arXiv:2309.08395 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Stammer et al. - 2023 - Learning by Self-Explaining.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\5TRCWBVD\\Stammer et al. - 2023 - Learning by Self-Explaining.pdf:application/pdf},
}

@article{meng_interpretability_2022-1,
	title = {Interpretability and fairness evaluation of deep learning models on {MIMIC}-{IV} dataset},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-11012-2},
	doi = {10.1038/s41598-022-11012-2},
	abstract = {The recent release of large-scale healthcare datasets has greatly propelled the research of data-driven deep learning models for healthcare applications. However, due to the nature of such deep black-boxed models, concerns about interpretability, fairness, and biases in healthcare scenarios where human lives are at stake call for a careful and thorough examination of both datasets and models. In this work, we focus on MIMIC-IV (Medical Information Mart for Intensive Care, version IV), the largest publicly available healthcare dataset, and conduct comprehensive analyses of interpretability as well as dataset representation bias and prediction fairness of deep learning models for in-hospital mortality prediction. First, we analyze the interpretability of deep learning mortality prediction models and observe that (1) the best-performing interpretability method successfully identifies critical features for mortality prediction on various prediction models as well as recognizing new important features that domain knowledge does not consider; (2) prediction models rely on demographic features, raising concerns in fairness. Therefore, we then evaluate the fairness of models and do observe the unfairness: (1) there exists disparate treatment in prescribing mechanical ventilation among patient groups across ethnicity, gender and age; (2) models often rely on racial attributes unequally across subgroups to generate their predictions. We further draw concrete connections between interpretability methods and fairness metrics by showing how feature importance from interpretability methods can be beneficial in quantifying potential disparities in mortality predictors. Our analysis demonstrates that the prediction performance is not the only factor to consider when evaluating models for healthcare applications, since high prediction performance might be the result of unfair utilization of demographic features. Our findings suggest that future research in AI models for healthcare applications can benefit from utilizing the analysis workflow of interpretability and fairness as well as verifying if models achieve superior performance at the cost of introducing bias.},
	language = {en},
	number = {1},
	urldate = {2023-10-12},
	journal = {Scientific Reports},
	author = {Meng, Chuizheng and Trinh, Loc and Xu, Nan and Enouen, James and Liu, Yan},
	month = may,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {notion, Computer science, Medical ethics, Medical research, Outcomes research},
	pages = {7166},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\ZEU744AM\\Meng et al. - 2022 - Interpretability and fairness evaluation of deep l.pdf:application/pdf},
}

@misc{janizek_explaining_2020-1,
	title = {Explaining {Explanations}: {Axiomatic} {Feature} {Interactions} for {Deep} {Networks}},
	shorttitle = {Explaining {Explanations}},
	url = {http://arxiv.org/abs/2002.04138},
	abstract = {Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain which features were most important to a model's prediction on a given input. However, for many tasks, simply knowing which features were important to a model's prediction may not provide enough insight to understand model behavior. The interactions between features within the model may better help us understand not only the model, but also why certain features are more important than others. In this work, we present Integrated Hessians, an extension of Integrated Gradients that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods to explain interactions, and unlike such previous methods is not limited to a specific architecture or class of neural network. Additionally, we find that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks. Code available at https://github.com/suinleelab/path\_explain},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Janizek, Joseph D. and Sturmfels, Pascal and Lee, Su-In},
	month = jun,
	year = {2020},
	note = {arXiv:2002.04138 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\X2WGUKDK\\2002.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\S4DZPDPU\\Janizek et al. - 2020 - Explaining Explanations Axiomatic Feature Interac.pdf:application/pdf},
}

@inproceedings{tsang_how_2020-1,
	title = {How does {This} {Interaction} {Affect} {Me}? {Interpretable} {Attribution} for {Feature} {Interactions}},
	volume = {33},
	shorttitle = {How does {This} {Interaction} {Affect} {Me}?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/443dec3062d0286986e21dc0631734c9-Abstract.html},
	abstract = {Machine learning transparency calls for interpretable explanations of how inputs relate to predictions. Feature attribution is a way to analyze the impact of features on predictions. Feature interactions are the contextual dependence between features that jointly impact predictions. There are a number of methods that extract feature interactions in prediction models; however, the methods that assign attributions to interactions are either uninterpretable, model-specific, or non-axiomatic. We propose an interaction attribution and detection framework called Archipelago which addresses these problems and is also scalable in real-world settings. Our experiments on standard annotation labels indicate our approach provides significantly more interpretable explanations than comparable methods, which is important for analyzing the impact of interactions on predictions. We also provide accompanying visualizations of our approach that give new insights into deep neural networks.},
	urldate = {2023-10-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tsang, Michael and Rambhatla, Sirisha and Liu, Yan},
	year = {2020},
	pages = {6147--6159},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\ZYKL6P9K\\Tsang et al. - 2020 - How does This Interaction Affect Me Interpretable.pdf:application/pdf},
}
@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}


@article{chowdhery2023palm,
  title={Pa{LM}: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023},
url={https://jmlr.org/papers/v24/22-1144.html}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022},
    url = {https://arxiv.org/abs/2201.08239}
}

@misc{bhattacharjee_llms_2023,
	title = {{LLMs} as {Counterfactual} {Explanation} {Modules}: {Can} {ChatGPT} {Explain} {Black}-box {Text} {Classifiers}?},
	shorttitle = {{LLMs} as {Counterfactual} {Explanation} {Modules}},
	url = {http://arxiv.org/abs/2309.13340},
	abstract = {Large language models (LLMs) are increasingly being used for tasks beyond text generation, including complex tasks such as data labeling, information extraction, etc. With the recent surge in research efforts to comprehend the full extent of LLM capabilities, in this work, we investigate the role of LLMs as counterfactual explanation modules, to explain decisions of black-box text classifiers. Inspired by causal thinking, we propose a pipeline for using LLMs to generate post-hoc, model-agnostic counterfactual explanations in a principled way via (i) leveraging the textual understanding capabilities of the LLM to identify and extract latent features, and (ii) leveraging the perturbation and generation capabilities of the same LLM to generate a counterfactual explanation by perturbing input features derived from the extracted latent features. We evaluate three variants of our framework, with varying degrees of specificity, on a suite of state-of-the-art LLMs, including ChatGPT and LLaMA 2. We evaluate the effectiveness and quality of the generated counterfactual explanations, over a variety of text classification benchmarks. Our results show varied performance of these models in different settings, with a full two-step feature extraction based variant outperforming others in most cases. Our pipeline can be used in automated explanation systems, potentially reducing human effort.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Bhattacharjee, Amrita and Moraffah, Raha and Garland, Joshua and Liu, Huan},
	month = sep,
	year = {2023},
	note = {arXiv:2309.13340 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\H73FAL8N\\2309.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\YVH8D7I5\\Bhattacharjee et al. - 2023 - LLMs as Counterfactual Explanation Modules Can Ch.pdf:application/pdf},
}

@misc{li_towards_2023,
	title = {Towards {Understanding} {In}-{Context} {Learning} with {Contrastive} {Demonstrations} and {Saliency} {Maps}},
	url = {http://arxiv.org/abs/2307.05052},
	abstract = {We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boosting ICL performance is task-dependent, with limited benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks. These insights are critical for understanding the functionality of LLMs and guiding the development of effective demonstrations, which is increasingly relevant in light of the growing use of LLMs in applications such as ChatGPT. Our research code is publicly available at https://github.com/paihengxu/XICL.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Li, Zongxia and Xu, Paiheng and Liu, Fuxiao and Song, Hyemi},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05052 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\F92A5QLE\\2307.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\5B7K73UZ\\Li et al. - 2023 - Towards Understanding In-Context Learning with Con.pdf:application/pdf},
}

@article{wang_style-transfer_2023,
	title = {Style-transfer counterfactual explanations: {An} application to mortality prevention of {ICU} patients},
	volume = {135},
	issn = {09333657},
	shorttitle = {Style-transfer counterfactual explanations},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365722002093},
	doi = {10.1016/j.artmed.2022.102457},
	abstract = {In recent years, machine learning methods have been rapidly adopted in the medical domain. However, current state-of-the-art medical mining methods usually produce opaque, black-box models. To address the lack of model transparency, substantial attention has been given to developing interpretable machine learning models. In the medical domain, counterfactuals can provide example-based explanations for predictions, and show practitioners the modifications required to change a prediction from an undesired to a desired state. In this paper, we propose a counterfactual solution MedSeqCF for preventing the mortality of three cohorts of ICU patients, by representing their electronic health records as medical event sequences, and generating counterfactuals by adopting and employing a text style-transfer technique. We propose three model augmentations for MedSeqCF to integrate additional medical knowledge for generating more trustworthy counterfactuals. Experimental results on the MIMIC-III dataset strongly suggest that augmented style-transfer methods can be effectively adapted for the problem of counterfactual explanations in healthcare applications and can further improve the model performance in terms of validity, BLEU-4, local outlier factor, and edit distance. In addition, our qualitative analysis of the results by consultation with medical experts suggests that our style-transfer solutions can generate clinically relevant and actionable counterfactual explanations.},
	language = {en},
	urldate = {2023-10-12},
	journal = {Artificial Intelligence in Medicine},
	author = {Wang, Zhendong and Samsten, Isak and Kougia, Vasiliki and Papapetrou, Panagiotis},
	month = jan,
	year = {2023},
	keywords = {notion},
	pages = {102457},
	file = {Wang et al. - 2023 - Style-transfer counterfactual explanations An app.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\KY6GSGW8\\Wang et al. - 2023 - Style-transfer counterfactual explanations An app.pdf:application/pdf},
}

@misc{chen_models_2023,
	title = {Do {Models} {Explain} {Themselves}? {Counterfactual} {Simulatability} of {Natural} {Language} {Explanations}},
	shorttitle = {Do {Models} {Explain} {Themselves}?},
	url = {http://arxiv.org/abs/2307.08678},
	abstract = {Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate \${\textbackslash}textbf\{counterfactual simulatability\}\$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers "yes" to the input question "Can eagles fly?" with the explanation "all birds can fly", then humans would infer from the explanation that it would also answer "yes" to the counterfactual input "Can penguins fly?". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Chen, Yanda and Zhong, Ruiqi and Ri, Narutatsu and Zhao, Chen and He, He and Steinhardt, Jacob and Yu, Zhou and McKeown, Kathleen},
	month = jul,
	year = {2023},
	note = {arXiv:2307.08678 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\46JXXWI2\\2307.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\4YKGPKAW\\Chen et al. - 2023 - Do Models Explain Themselves Counterfactual Simul.pdf:application/pdf},
}

@misc{singh_explaining_2023,
	title = {Explaining black box text modules in natural language with language models},
	url = {http://arxiv.org/abs/2305.09863},
	abstract = {Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs. We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the model's internals. Finally, we show that SASC can generate explanations for the response of individual fMRI voxels to language stimuli, with potential applications to fine-grained brain mapping. All code for using SASC and reproducing results is made available on Github.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Singh, Chandan and Hsu, Aliyah R. and Antonello, Richard and Jain, Shailee and Huth, Alexander G. and Yu, Bin and Gao, Jianfeng},
	month = may,
	year = {2023},
	note = {arXiv:2305.09863 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\I4MXTG8P\\2305.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\NIQG55N2\\Singh et al. - 2023 - Explaining black box text modules in natural langu.pdf:application/pdf},
}

@misc{kroeger_are_2023,
	title = {Are {Large} {Language} {Models} {Post} {Hoc} {Explainers}?},
	url = {http://arxiv.org/abs/2310.05797},
	abstract = {Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based ICL, with varying levels of information about the underlying ML model and the local neighborhood of the test sample. We conduct extensive experiments with real-world benchmark datasets to demonstrate that LLM-generated explanations perform on par with state-of-the-art post hoc explainers using their ability to leverage ICL examples and their internal knowledge in generating model explanations. On average, across four datasets and two ML models, we observe that LLMs identify the most important feature with 72.19\% accuracy, opening up new frontiers in explainable artificial intelligence (XAI) to explore LLM-based explanation frameworks.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Kroeger, Nicholas and Ley, Dan and Krishna, Satyapriya and Agarwal, Chirag and Lakkaraju, Himabindu},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05797 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\SGIY5NHS\\2310.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\UR9Z4AHC\\Kroeger et al. - 2023 - Are Large Language Models Post Hoc Explainers.pdf:application/pdf},
}

@misc{touvron_llama_2023-1,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\4NBTFKA6\\2307.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\B9TYW2WY\\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf},
}

@inproceedings{bhan-etal-2024-self,
    title = "Self-{AMPLIFY}: Improving Small Language Models with Self Post Hoc Explanations",
    author = {Bhan, Milan  and
      Vittaut, Jean-No{\"e}l  and
      Chesneau, Nicolas  and
      Lesot, Marie-Jeanne},
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.615/",
    doi = "10.18653/v1/2024.emnlp-main.615",
    pages = "10974--10991",
    abstract = "Incorporating natural language rationales in the prompt and In-Context Learning (ICL) have led to a significant improvement of Large Language Models (LLMs) performance. However, generating high-quality rationales require human-annotation or the use of auxiliary proxy models. In this work, we propose Self-AMPLIFY to automatically generate rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on four SLMs and five datasets requiring strong reasoning abilities. Self-AMPLIFY achieves good results against competitors, leading to strong accuracy improvement. Self-AMPLIFY is the first method to apply post hoc explanation methods to autoregressive language models to generate rationales to improve their own performance in a fully automated manner."
}

@article{concept_activation_region,
  title={Concept activation regions: A generalized framework for concept-based explanations},
  author={Crabb{\'e}, Jonathan and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2590--2607},
  year={2022}
}

@article{fel2023holistic,
  title={A holistic approach to unifying automatic concept extraction and concept importance estimation},
  author={Fel, Thomas and Boutin, Victor and B{\'e}thune, Louis and Cad{\`e}ne, R{\'e}mi and Moayeri, Mazda and And{\'e}ol, L{\'e}o and Chalvidal, Mathieu and Serre, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={54805--54818},
  year={2023}
}

@inproceedings{tigtec,
author = {Bhan, Milan and Vittaut, Jean-No\"{e}l and Chesneau, Nicolas and Lesot, Marie-Jeanne},
title = {TIGTEC: Token Importance Guided TExt Counterfactuals},
year = {2023},
publisher = {Springer},
booktitle = {Proc. of the European Conf. on Machine Learning ECML-PKDD},
pages = {496–512},
url = {https://doi.org/10.1007/978-3-031-43418-1_30},
}

@inproceedings{tigtec_old,
author = {Bhan, Milan and Vittaut, Jean-No\"{e}l and Chesneau, Nicolas and Lesot, Marie-Jeanne},
title = {TIGTEC: Token Importance Guided TExt Counterfactuals},
year = {2023},
isbn = {978-3-031-43417-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43418-1_30},
doi = {10.1007/978-3-031-43418-1_30},
booktitle = {Machine Learning and Knowledge Discovery in Databases: Research Track: European Conf., ECML PKDD 2023, Turin, Italy, September 18–22, 2023, Proc., Part III},
pages = {496–512},
numpages = {17},
keywords = {Attention, Local Feature Importance, Counterfactual examples, NLP, XAI},
location = {Turin, Italy}
}



@misc{tunstall_zephyr_2023,
	title = {Zephyr: {Direct} {Distillation} of {LM} {Alignment}},
	shorttitle = {Zephyr},
	url = {http://arxiv.org/abs/2310.16944},
	abstract = {We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Clémentine and Habib, Nathan and Sarrazin, Nathan and Sanseviero, Omar and Rush, Alexander M. and Wolf, Thomas},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16944 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\9SDWY99D\\2310.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\N7J5CRVW\\Tunstall et al. - 2023 - Zephyr Direct Distillation of LM Alignment.pdf:application/pdf},
}

@misc{guo_evaluating_2023,
	title = {Evaluating {Large} {Language} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Evaluating {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.19736},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs. This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability. We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.},
	urldate = {2023-11-02},
	publisher = {arXiv},
	author = {Guo, Zishan and Jin, Renren and Liu, Chuang and Huang, Yufei and Shi, Dan and Supryadi and Yu, Linhao and Liu, Yan and Li, Jiaxuan and Xiong, Bojian and Xiong, Deyi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.19736 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\BILSP5PD\\2310.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\QLVQIBQ5\\Guo et al. - 2023 - Evaluating Large Language Models A Comprehensive .pdf:application/pdf},
}

@inproceedings{boggust_saliency_2023,
	address = {Chicago IL USA},
	title = {Saliency {Cards}: {A} {Framework} to {Characterize} and {Compare} {Saliency} {Methods}},
	isbn = {9798400701924},
	shorttitle = {Saliency {Cards}},
	url = {https://dl.acm.org/doi/10.1145/3593013.3593997},
	doi = {10.1145/3593013.3593997},
	language = {en},
	urldate = {2023-11-02},
	booktitle = {2023 {ACM} {Conf.} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Boggust, Angie and Suresh, Harini and Strobelt, Hendrik and Guttag, John and Satyanarayan, Arvind},
	month = jun,
	year = {2023},
	keywords = {notion},
	pages = {285--296},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\Q7Y9259V\\Boggust et al. - 2023 - Saliency Cards A Framework to Characterize and Co.pdf:application/pdf},
}

@misc{hallinan_detoxifying_2023-1,
	title = {Detoxifying {Text} with {MaRCo}: {Controllable} {Revision} with {Experts} and {Anti}-{Experts}},
	shorttitle = {Detoxifying {Text} with {MaRCo}},
	url = {http://arxiv.org/abs/2212.10543},
	abstract = {Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo's rewrites are preferred 2.1 \${\textbackslash}times\$ more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Hallinan, Skyler and Liu, Alisa and Choi, Yejin and Sap, Maarten},
	month = may,
	year = {2023},
	note = {arXiv:2212.10543 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\UXLG57HW\\2212.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\DN9BW8B6\\Hallinan et al. - 2023 - Detoxifying Text with MaRCo Controllable Revision.pdf:application/pdf},
}

@inproceedings{dale_text_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Text {Detoxification} using {Large} {Pre}-trained {Neural} {Models}},
	url = {https://aclanthology.org/2021.emnlp-main.629},
	doi = {10.18653/v1/2021.emnlp-main.629},
	abstract = {We present two novel unsupervised methods for eliminating toxicity in text. Our first method combines two recent ideas: (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing paraphraser guided by style-trained language models to keep the text content and remove toxicity. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the method more flexible by enabling BERT to replace mask tokens with a variable number of words. Finally, we present the first large-scale comparative study of style transfer models on the task of toxicity removal. We compare our models with a number of methods for style transfer. The models are evaluated in a reference-free way using a combination of unsupervised style transfer metrics. Both methods we suggest yield new SOTA results.},
	urldate = {2023-11-03},
	booktitle = {Proc. of the 2021 {Conf.} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Dale, David and Voronov, Anton and Dementieva, Daryna and Logacheva, Varvara and Kozlova, Olga and Semenov, Nikita and Panchenko, Alexander},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	keywords = {notion},
	pages = {7979--7996},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\ILTY5VNG\\Dale et al. - 2021 - Text Detoxification using Large Pre-trained Neural.pdf:application/pdf},
}

@inproceedings{dale_text_2021-1,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Text {Detoxification} using {Large} {Pre}-trained {Neural} {Models}},
	url = {https://aclanthology.org/2021.emnlp-main.629},
	doi = {10.18653/v1/2021.emnlp-main.629},
	language = {en},
	urldate = {2023-11-03},
	booktitle = {Proc. of the 2021 {Conf.} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Dale, David and Voronov, Anton and Dementieva, Daryna and Logacheva, Varvara and Kozlova, Olga and Semenov, Nikita and Panchenko, Alexander},
	year = {2021},
	pages = {7979--7996},
	file = {Dale et al. - 2021 - Text Detoxification using Large Pre-trained Neural.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\3XKIH247\\Dale et al. - 2021 - Text Detoxification using Large Pre-trained Neural.pdf:application/pdf},
}

@article{wu_mask_2019,
	title = {Mask and {Infill}: {Applying} {Masked} {Language} {Model} for {Sentiment} {Transfer}},
	shorttitle = {Mask and {Infill}},
	url = {https://www.ijcai.org/proceedings/2019/732},
	abstract = {Electronic proceedings of IJCAI 2019},
	urldate = {2023-11-03},
	author = {Wu, Xing and Zhang, Tao and Zang, Liangjun and Han, Jizhong and Hu, Songlin},
	year = {2019},
	keywords = {notion},
	pages = {5271--5277},
	file = {Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\87NEQJ9N\\732.html:text/html},
}

@misc{li_towards_2023-1,
	title = {Towards {Understanding} {In}-{Context} {Learning} with {Contrastive} {Demonstrations} and {Saliency} {Maps}},
	url = {http://arxiv.org/abs/2307.05052},
	abstract = {We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boosting ICL performance is task-dependent, with limited benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks. These insights are critical for understanding the functionality of LLMs and guiding the development of effective demonstrations, which is increasingly relevant in light of the growing use of LLMs in applications such as ChatGPT. Our research code is publicly available at https://github.com/paihengxu/XICL.},
	urldate = {2023-11-08},
	publisher = {arXiv},
	author = {Li, Zongxia and Xu, Paiheng and Liu, Fuxiao and Song, Hyemi},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05052 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\AWFU7B95\\2307.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\A635JVG9\\Li et al. - 2023 - Towards Understanding In-Context Learning with Con.pdf:application/pdf},
}

@misc{huang_can_2023,
	title = {Can {Large} {Language} {Models} {Explain} {Themselves}? {A} {Study} of {LLM}-{Generated} {Self}-{Explanations}},
	shorttitle = {Can {Large} {Language} {Models} {Explain} {Themselves}?},
	url = {http://arxiv.org/abs/2310.11207},
	abstract = {Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11207 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\N95K8NZZ\\2310.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\XFDXQXFC\\Huang et al. - 2023 - Can Large Language Models Explain Themselves A St.pdf:application/pdf},
}

@misc{lakkaraju_rethinking_2022,
	title = {Rethinking {Explainability} as a {Dialogue}: {A} {Practitioner}'s {Perspective}},
	shorttitle = {Rethinking {Explainability} as a {Dialogue}},
	url = {http://arxiv.org/abs/2202.01875},
	abstract = {As practitioners increasingly deploy machine learning models in critical domains such as health care, finance, and policy, it becomes vital to ensure that domain experts function effectively alongside these models. Explainability is one way to bridge the gap between human decision-makers and machine learning models. However, most of the existing work on explainability focuses on one-off, static explanations like feature importances or rule lists. These sorts of explanations may not be sufficient for many use cases that require dynamic, continuous discovery from stakeholders. In the literature, few works ask decision-makers about the utility of existing explanations and other desiderata they would like to see in an explanation going forward. In this work, we address this gap and carry out a study where we interview doctors, healthcare professionals, and policymakers about their needs and desires for explanations. Our study indicates that decision-makers would strongly prefer interactive explanations in the form of natural language dialogues. Domain experts wish to treat machine learning models as "another colleague", i.e., one who can be held accountable by asking why they made a particular decision through expressive and accessible natural language interactions. Considering these needs, we outline a set of five principles researchers should follow when designing interactive explanations as a starting place for future work. Further, we show why natural language dialogues satisfy these principles and are a desirable way to build interactive explanations. Next, we provide a design of a dialogue system for explainability and discuss the risks, trade-offs, and research opportunities of building these systems. Overall, we hope our work serves as a starting place for researchers and engineers to design interactive explainability systems.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Lakkaraju, Himabindu and Slack, Dylan and Chen, Yuxin and Tan, Chenhao and Singh, Sameer},
	month = feb,
	year = {2022},
	note = {arXiv:2202.01875 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\AMNTA675\\2202.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\AP4RH6ZV\\Lakkaraju et al. - 2022 - Rethinking Explainability as a Dialogue A Practit.pdf:application/pdf},
}

@misc{menon_mantle_2023,
	title = {{MaNtLE}: {Model}-agnostic {Natural} {Language} {Explainer}},
	shorttitle = {{MaNtLE}},
	url = {http://arxiv.org/abs/2305.12995},
	abstract = {Understanding the internal reasoning behind the predictions of machine learning systems is increasingly vital, given their rising adoption and acceptance. While previous approaches, such as LIME, generate algorithmic explanations by attributing importance to input features for individual examples, recent research indicates that practitioners prefer examining language explanations that explain sub-groups of examples. In this paper, we introduce MaNtLE, a model-agnostic natural language explainer that analyzes multiple classifier predictions and generates faithful natural language explanations of classifier rationale for structured classification tasks. MaNtLE uses multi-task training on thousands of synthetic classification tasks to generate faithful explanations. Simulated user studies indicate that, on average, MaNtLE-generated explanations are at least 11\% more faithful compared to LIME and Anchors explanations across three tasks. Human evaluations demonstrate that users can better predict model behavior using explanations from MaNtLE compared to other techniques},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Menon, Rakesh R. and Zaman, Kerem and Srivastava, Shashank},
	month = may,
	year = {2023},
	note = {arXiv:2305.12995 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\BRC5TLVC\\2305.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\IJQQ8JSY\\Menon et al. - 2023 - MaNtLE Model-agnostic Natural Language Explainer.pdf:application/pdf},
}

@misc{kim_self-generated_2022,
	title = {Self-{Generated} {In}-{Context} {Learning}: {Leveraging} {Auto}-regressive {Language} {Models} as a {Demonstration} {Generator}},
	shorttitle = {Self-{Generated} {In}-{Context} {Learning}},
	url = {http://arxiv.org/abs/2206.08082},
	abstract = {Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. We conduct experiments on four different text classification tasks and show SG-ICL significantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover, our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Kim, Hyuhng Joon and Cho, Hyunsoo and Kim, Junyeob and Kim, Taeuk and Yoo, Kang Min and Lee, Sang-goo},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08082 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\2RR34MC6\\2206.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\KPPAEXNX\\Kim et al. - 2022 - Self-Generated In-Context Learning Leveraging Aut.pdf:application/pdf},
}

@misc{wang_self-instruct_2023,
	title = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = may,
	year = {2023},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\7LFM6I4A\\2212.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\HG547M7V\\Wang et al. - 2023 - Self-Instruct Aligning Language Models with Self-.pdf:application/pdf},
}

@misc{press_measuring_2023,
	title = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03350},
	abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
	month = oct,
	year = {2023},
	note = {arXiv:2210.03350 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\FRP9S5ZG\\2210.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\MAVNM44A\\Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap i.pdf:application/pdf},
}

@InProceedings{cb_llm,
  author={Sun, Chung-En and Oikarinen, Tuomas and Ustun, Berk and Weng, Tsui-Wei},
  title={Concept Bottleneck Large Language Models},
 booktitle = {Proc. of the 13th Int. Conf. on Learning Representations, ICLR23},
  year =      {2025}
}

@InProceedings{zhang_automatic_2022,
  author =    {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  title =     {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
 booktitle = {Proc. of the 11th Int. Conf. on Learning Representations, ICLR23},
  year =      {2023},
	url = {http://arxiv.org/abs/2210.03493}
}

 

@misc{zhang_automatic_2022_old,
	title = {Automatic {Chain} of {Thought} {Prompting} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03493},
	abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\FI8TM4VG\\2210.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\68I866KH\\Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf:application/pdf},
}

@misc{ehsan_who_2021,
	title = {The {Who} in {Explainable} {AI}: {How} {AI} {Background} {Shapes} {Perceptions} of {AI} {Explanations}},
	shorttitle = {The {Who} in {Explainable} {AI}},
	url = {http://arxiv.org/abs/2107.13509},
	abstract = {Explainability of AI systems is critical for users to take informed actions and hold systems accountable. While "opening the opaque box" is important, understanding who opens the box can govern if the Human-AI interaction is effective. In this paper, we conduct a mixed-methods study of how two different groups of whos--people with and without a background in AI--perceive different types of AI explanations. These groups were chosen to look at how disparities in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively share what the perceptions are along five dimensions: confidence, intelligence, understandability, second chance, and friendliness. Qualitatively, we highlight how the AI background influences each group's interpretations and elucidate why the differences might exist through the lenses of appropriation and cognitive heuristics. We find that (1) both groups had unwarranted faith in numbers, to different extents and for different reasons, (2) each group found explanatory values in different explanations that went beyond the usage we designed them for, and (3) each group had different requirements of what counts as humanlike explanations. Using our findings, we discuss potential negative consequences such as harmful manipulation of user trust and propose design interventions to mitigate them. By bringing conscious awareness to how and why AI backgrounds shape perceptions of potential creators and consumers in XAI, our work takes a formative step in advancing a pluralistic Human-centered Explainable AI discourse.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Ehsan, Upol and Passi, Samir and Liao, Q. Vera and Chan, Larry and Lee, I.-Hsiang and Muller, Michael and Riedl, Mark O.},
	month = jul,
	year = {2021},
	note = {arXiv:2107.13509 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\IEZ8UUVF\\2107.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\9FTCCUHD\\Ehsan et al. - 2021 - The Who in Explainable AI How AI Background Shape.pdf:application/pdf},
}

@misc{adebayo_post_2022,
	title = {Post hoc {Explanations} may be {Ineffective} for {Detecting} {Unknown} {Spurious} {Correlation}},
	url = {http://arxiv.org/abs/2212.04629},
	abstract = {We investigate whether three types of post hoc model explanations--feature attribution, concept activation, and training point ranking--are effective for detecting a model's reliance on spurious signals in the training data. Specifically, we consider the scenario where the spurious signal to be detected is unknown, at test-time, to the user of the explanation method. We design an empirical methodology that uses semi-synthetic datasets along with pre-specified spurious artifacts to obtain models that verifiably rely on these spurious training signals. We then provide a suite of metrics that assess an explanation method's reliability for spurious signal detection under various conditions. We find that the post hoc explanation methods tested are ineffective when the spurious artifact is unknown at test-time especially for non-visible artifacts like a background blur. Further, we find that feature attribution methods are susceptible to erroneously indicating dependence on spurious signals even when the model being explained does not rely on spurious artifacts. This finding casts doubt on the utility of these approaches, in the hands of a practitioner, for detecting a model's reliance on spurious signals.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Adebayo, Julius and Muelly, Michael and Abelson, Hal and Kim, Been},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04629 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\3DULHZG9\\2212.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\74ZS3BG4\\Adebayo et al. - 2022 - Post hoc Explanations may be Ineffective for Detec.pdf:application/pdf},
}

@misc{chen_towards_2022,
	title = {Towards {Understanding} {Mixture} of {Experts} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2208.02813},
	abstract = {The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has achieved great success in deep learning. However, the understanding of such architecture remains elusive. In this paper, we formally study how the MoE layer improves the performance of neural network learning and why the mixture model will not collapse into a single model. Our empirical results suggest that the cluster structure of the underlying problem and the non-linearity of the expert are pivotal to the success of MoE. To further understand this, we consider a challenging classification problem with intrinsic cluster structures, which is hard to learn using a single expert. Yet with the MoE layer, by choosing the experts as two-layer nonlinear convolutional neural networks (CNNs), we show that the problem can be learned successfully. Furthermore, our theory shows that the router can learn the cluster-center features, which helps divide the input complex problem into simpler linear classification sub-problems that individual experts can conquer. To our knowledge, this is the first result towards formally understanding the mechanism of the MoE layer for deep learning.},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Chen, Zixiang and Deng, Yihe and Wu, Yue and Gu, Quanquan and Li, Yuanzhi},
	month = aug,
	year = {2022},
	note = {arXiv:2208.02813 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\68FMWPUH\\2208.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\I9QVNGSP\\Chen et al. - 2022 - Towards Understanding Mixture of Experts in Deep L.pdf:application/pdf},
}
@article{schaeffer_are_2024,
  title={Are emergent abilities of large language models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@misc{conmy_towards_2023,
	title = {Towards {Automated} {Circuit} {Discovery} for {Mechanistic} {Interpretability}},
	url = {http://arxiv.org/abs/2304.14997},
	abstract = {Through considerable effort and intuition, several recent works have reverseengineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component.},
	language = {en},
	urldate = {2024-01-02},
	publisher = {arXiv},
	author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
	month = oct,
	year = {2023},
	note = {arXiv:2304.14997 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\WYD3SETB\\Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf:application/pdf},
}

@misc{lindner_tracr_2023,
	title = {Tracr: {Compiled} {Transformers} as a {Laboratory} for {Interpretability}},
	shorttitle = {Tracr},
	url = {http://arxiv.org/abs/2301.05062},
	abstract = {We show how to “compile” human-readable programs into standard decoderonly transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study “superposition” in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the “programs” learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/google-deepmind/tracr.},
	language = {en},
	urldate = {2024-01-02},
	publisher = {arXiv},
	author = {Lindner, David and Kramár, János and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
	month = nov,
	year = {2023},
	note = {arXiv:2301.05062 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Lindner et al. - 2023 - Tracr Compiled Transformers as a Laboratory for I.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\MEQYSXIK\\Lindner et al. - 2023 - Tracr Compiled Transformers as a Laboratory for I.pdf:application/pdf},
}

@article{zarlenga_tabcbm_nodate,
	title = {{TabCBM}: {Concept}-based {Interpretable} {Neural} {Networks} for {Tabular} {Data}},
	abstract = {Concept-based interpretability addresses the opacity of deep neural networks by constructing an explanation for a model’s prediction using high-level units of information referred to as concepts. Research in this area, however, has been mainly focused on image and graphstructured data, leaving high-stakes tasks whose data is tabular out of reach of existing methods. In this paper, we address this gap by introducing the first definition of what a high-level concept may entail in tabular data. We use this definition to propose Tabular Concept Bottleneck Models (TabCBMs), a family of interpretable self-explaining neural architectures capable of learning high-level concept explanations for tabular tasks. As our method produces concept-based explanations both when partial concept supervision or no concept supervision is available at training time, it is adaptable to settings where concept annotations are missing. We evaluate our method in both synthetic and real-world tabular tasks and show that TabCBM outperforms or performs competitively compared to state-ofthe-art methods, while providing a high level of interpretability as measured by its ability to discover known high-level concepts. Finally, we show that TabCBM can discover important high-level concepts in synthetic datasets inspired by critical tabular tasks (e.g., single-cell RNAseq) and allows for human-in-the-loop concept interventions in which an expert can identify and correct mispredicted concepts to boost the model’s performance.},
	language = {en},
	author = {Zarlenga, Mateo Espinosa and Shams, Zohreh and Nelson, Michael Edward},
	keywords = {notion},
	file = {Zarlenga et al. - TabCBM Concept-based Interpretable Neural Network.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\J6QBT29J\\Zarlenga et al. - TabCBM Concept-based Interpretable Neural Network.pdf:application/pdf},
}

@article{geometry_of_truth,
  title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
  year = {2024},
  booktitle = "Proceedings of the first Conference on Language Modeling (COLM)",
  author={Marks, Samuel and Tegmark, Max}
}

@article{cav_axbench,
  title={AXBENCH: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders},
  author={Wu, Zhengxuan and Arora, Aryaman and Geiger, Atticus and Wang, Zheng and Huang, Jing and Jurafsky, Dan and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:2501.17148},
  year={2025}
}

@inproceedings{cav_text_abusive,
    title = "Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors",
    author = "Nejadgholi, Isar  and
      Fraser, Kathleen  and
      Kiritchenko, Svetlana",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.378/",
    doi = "10.18653/v1/2022.acl-long.378",
    pages = "5517--5529",
    abstract = "Robustness of machine learning models on ever-changing real-world data is critical, especially for applications affecting human well-being such as content moderation. New kinds of abusive language continually emerge in online discussions in response to current events (e.g., COVID-19), and the deployed abuse detection systems should be updated regularly to remain accurate. In this paper, we show that general abusive language classifiers tend to be fairly reliable in detecting out-of-domain explicitly abusive utterances but fail to detect new types of more subtle, implicit abuse. Next, we propose an interpretability technique, based on the Testing Concept Activation Vector (TCAV) method from computer vision, to quantify the sensitivity of a trained model to the human-defined concepts of explicit and implicit abusive language, and use that to explain the generalizability of the model on new data, in this case, COVID-related anti-Asian hate speech. Extending this technique, we introduce a novel metric, Degree of Explicitness, for a single instance and show that the new metric is beneficial in suggesting out-of-domain unlabeled examples to effectively enrich the training data with informative, implicitly abusive texts."
}

@misc{schaeffer_are_2023-1,
	title = {Are {Emergent} {Abilities} of {Large} {Language} {Models} a {Mirage}?},
	url = {http://arxiv.org/abs/2304.15004},
	abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
	urldate = {2024-01-03},
	publisher = {arXiv},
	author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
	month = may,
	year = {2023},
	note = {arXiv:2304.15004 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\VWWYXP6G\\2304.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\DDQQC2IW\\Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf:application/pdf},
}

@article{li_m4_nodate,
	title = {M4: {A} {Unified} {XAI} {Benchmark} for {Faithfulness} {Evaluation} of {Feature} {Attribution} {Methods} across {Metrics}, {Modalities} and {Models}},
	abstract = {While Explainable Artificial Intelligence (XAI) techniques have been widely studied to explain predictions made by deep neural networks, the way to evaluate the faithfulness of explanation results remains challenging, due to the heterogeneity of explanations for various models and the lack of ground-truth explanations. This paper introduces an XAI benchmark named M4, which allows evaluating various input feature attribution methods using the same set of faithfulness metrics across multiple data modalities (images and texts) and network structures (ResNets, MobileNets, Transformers). A taxonomy for the metrics has been proposed as well. We first categorize commonly used XAI evaluation metrics into three groups based on the ground truth they require. We then implement classic and state-of-the-art feature attribution methods using InterpretDL and conduct extensive experiments to compare methods and gain insights. Extensive experiments have been conducted to provide holistic evaluations as benchmark baselines. Several interesting observations are made for designing attribution algorithms. The implementation of state-of-the-art explanation methods and evaluation metrics of M4 is publicly available at https://github.com/PaddlePaddle/InterpretDL.},
	language = {en},
	author = {Li, Xuhong and Du, Mengnan and Chen, Jiamin},
	keywords = {notion},
	file = {Li et al. - M4 A Unified XAI Benchmark for Faithfulness Evalu.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\H4C5P23L\\Li et al. - M4 A Unified XAI Benchmark for Faithfulness Evalu.pdf:application/pdf},
}

@article{bhalla_discriminative_nodate,
	title = {Discriminative {Feature} {Attributions}: {Bridging} {Post} {Hoc} {Explainability} and {Inherent} {Interpretability}},
	abstract = {With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by identifying features critical to model predictions; however, prior work has shown that these explanations may not be faithful, in that they incorrectly attribute high importance to features that are unimportant or non-discriminative for the underlying task. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we identify a key reason for the lack of faithfulness of feature attributions: the lack of robustness of the underlying blackbox models, especially to the erasure of unimportant distractor features in the input. To address this issue, we propose Distractor Erasure Tuning (DiET), a method that adapts black-box models to be robust to distractor erasure, thus providing discriminative and faithful attributions. This strategy naturally combines the ease of use of post hoc explanations with the faithfulness of inherently interpretable models. We perform extensive experiments on semi-synthetic and real-world datasets, and show that DiET produces models that (1) closely approximate the original black-box models they are intended to explain, and (2) yield explanations that match approximate ground truths available by construction. Our code is made public here.},
	language = {en},
	author = {Bhalla, Usha and Srinivas, Suraj and Lakkaraju, Himabindu},
	keywords = {notion},
	file = {Bhalla et al. - Discriminative Feature Attributions Bridging Post.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\Y7EBQDCM\\Bhalla et al. - Discriminative Feature Attributions Bridging Post.pdf:application/pdf},
}

@misc{bouchaud_statistical_2023,
	title = {From {Statistical} {Physics} to {Social} {Sciences}: {The} {Pitfalls} of {Multi}-disciplinarity},
	shorttitle = {From {Statistical} {Physics} to {Social} {Sciences}},
	url = {http://arxiv.org/abs/2308.02895},
	abstract = {This is the English version of my inaugural lecture at Coll{\textbackslash}`ege de France in 2021, available at https://www.youtube.com/watch?v=bxktplKMhKU. I reflect on the difficulty of multi-disciplinary research, which often hinges of unexpected epistemological and methodological differences, for example about the scientific status of models. What is the purpose of a model? What are we ultimately trying to establish: rigorous theorems or ad-hoc calculation recipes; absolute truth, or heuristic representations of the world? I argue that the main contribution of statistical physics to social and economic sciences is to make us realise that unexpected behaviour can emerge at the aggregate level, that isolated individuals would never experience. Crises, panics, opinion reversals, the spread of rumours or beliefs, fashion effects and the zeitgeist, but also the existence of money, lasting institutions, social norms and stable societies, must be understood in terms of collective belief and/or trust, self-sustained by interactions, or on the contrary, the rapid collapse of this belief or trust. The Appendix contains my opening remarks to the workshop ``More is Different'', as a tribute to Phil Anderson.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Bouchaud, Jean-Philippe},
	month = aug,
	year = {2023},
	note = {arXiv:2308.02895 [cond-mat, physics:physics, q-fin]},
	keywords = {Condensed Matter - Statistical Mechanics, Economics - General Economics, Physics - Physics and Society},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\9PPHSEF7\\2308.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\FC4UULX5\\Bouchaud - 2023 - From Statistical Physics to Social Sciences The P.pdf:application/pdf},
}

@inproceedings{miglani_using_2023,
	OPTaddress = {Singapore},
	title = {Using {Captum} to {Explain} {Generative} {Language} {Models}},
	url = {https://aclanthology.org/2023.nlposs-1.19},
	doi = {10.18653/v1/2023.nlposs-1.19},
	abstract = {Captum is a comprehensive library for model explainability in PyTorch, offering a range of methods from the interpretability literature to enhance users' understanding of PyTorch models. In this paper, we introduce new features in Captum that are specifically designed to analyze the behavior of generative language models. We provide an overview of the available functionalities and example applications of their potential for understanding learned associations within generative language models.},
	urldate = {2024-01-09},
	booktitle = {Proc. of the 3rd {Workshop} for {Natural} {Language} {Processing} {Open} {Source} {Software} ({NLP}-{OSS} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Miglani, Vivek and Yang, Aobo and Markosyan, Aram and Garcia-Olano, Diego and Kokhlikyan, Narine},
	editor = {Tan, Liling and Milajevs, Dmitrijs and Chauhan, Geeticka and Gwinnup, Jeremy and Rippeth, Elijah},
	month = dec,
	year = {2023},
	keywords = {notion},
	pages = {165--173},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\C28IH46B\\Miglani et al. - 2023 - Using Captum to Explain Generative Language Models.pdf:application/pdf},
}

@inproceedings{wang_interpretability_2022,
	address = {Washington DC USA},
	title = {Interpretability, {Then} {What}? {Editing} {Machine} {Learning} {Models} to {Reflect} {Human} {Knowledge} and {Values}},
	isbn = {978-1-4503-9385-0},
	shorttitle = {Interpretability, {Then} {What}?},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539074},
	doi = {10.1145/3534678.3539074},
	language = {en},
	urldate = {2024-01-10},
	booktitle = {Proc. of the 28th {ACM} {SIGKDD} {Conf.} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Wang, Zijie J. and Kale, Alex and Nori, Harsha and Stella, Peter and Nunnally, Mark E. and Chau, Duen Horng and Vorvoreanu, Mihaela and Wortman Vaughan, Jennifer and Caruana, Rich},
	month = aug,
	year = {2022},
	keywords = {notion},
	pages = {4132--4142},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\W242Q75A\\Wang et al. - 2022 - Interpretability, Then What Editing Machine Learn.pdf:application/pdf},
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org},
  url = {https://www.jmlr.org/papers/v12/pedregosa11a.html}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proc. of the Conf. on Empirical Methods in Natural language Processing: system demonstrations, EMNLP},
  pages={38--45},
  year={2020},
  url = {https://aclanthology.org/2020.emnlp-demos.6/}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  OPTvolume={32},
  year={2019},
url    = {https://dl.acm.org/doi/10.5555/3454287.3455008}
}

@inproceedings{wang_interpretability_2022-1,
	address = {Washington DC USA},
	title = {Interpretability, {Then} {What}? {Editing} {Machine} {Learning} {Models} to {Reflect} {Human} {Knowledge} and {Values}},
	isbn = {978-1-4503-9385-0},
	shorttitle = {Interpretability, {Then} {What}?},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539074},
	doi = {10.1145/3534678.3539074},
	language = {en},
	urldate = {2024-01-10},
	booktitle = {Proc. of the 28th {ACM} {SIGKDD} {Conf.} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Wang, Zijie J. and Kale, Alex and Nori, Harsha and Stella, Peter and Nunnally, Mark E. and Chau, Duen Horng and Vorvoreanu, Mihaela and Wortman Vaughan, Jennifer and Caruana, Rich},
	month = aug,
	year = {2022},
	keywords = {notion},
	pages = {4132--4142},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\KAC8354E\\Wang et al. - 2022 - Interpretability, Then What Editing Machine Learn.pdf:application/pdf},
}

@inproceedings{vo_feature-based_2023,
	address = {Long Beach CA USA},
	title = {Feature-based {Learning} for {Diverse} and {Privacy}-{Preserving} {Counterfactual} {Explanations}},
	isbn = {9798400701030},
	url = {https://dl.acm.org/doi/10.1145/3580305.3599343},
	doi = {10.1145/3580305.3599343},
	language = {en},
	urldate = {2024-01-10},
	booktitle = {Proc. of the 29th {ACM} {SIGKDD} {Conf.} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Vo, Vy and Le, Trung and Nguyen, Van and Zhao, He and Bonilla, Edwin V. and Haffari, Gholamreza and Phung, Dinh},
	month = aug,
	year = {2023},
	keywords = {notion},
	pages = {2211--2222},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\AICT3CL2\\Vo et al. - 2023 - Feature-based Learning for Diverse and Privacy-Pre.pdf:application/pdf},
}

@inproceedings{tcav_abusive_language,
    title = "Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors",
    author = "Nejadgholi, Isar  and
      Fraser, Kathleen  and
      Kiritchenko, Svetlana",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.378",
    doi = "10.18653/v1/2022.acl-long.378",
    pages = "5517--5529",
    abstract = "Robustness of machine learning models on ever-changing real-world data is critical, especially for applications affecting human well-being such as content moderation. New kinds of abusive language continually emerge in online discussions in response to current events (e.g., COVID-19), and the deployed abuse detection systems should be updated regularly to remain accurate. In this paper, we show that general abusive language classifiers tend to be fairly reliable in detecting out-of-domain explicitly abusive utterances but fail to detect new types of more subtle, implicit abuse. Next, we propose an interpretability technique, based on the Testing Concept Activation Vector (TCAV) method from computer vision, to quantify the sensitivity of a trained model to the human-defined concepts of explicit and implicit abusive language, and use that to explain the generalizability of the model on new data, in this case, COVID-related anti-Asian hate speech. Extending this technique, we introduce a novel metric, Degree of Explicitness, for a single instance and show that the new metric is beneficial in suggesting out-of-domain unlabeled examples to effectively enrich the training data with informative, implicitly abusive texts.",
}

@article{zhang2024tinyllama,
  title={Tinyllama: An open-source small language model},
  author={Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  journal={arXiv preprint arXiv:2401.02385},
  year={2024},
  url = {https://arxiv.org/abs/2401.02385}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023},
url= {https://arxiv.org/abs/2310.06825}
}

@article{mitra2023orca,
  title={Orca 2: Teaching small language models how to reason},
  author={Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes, Clarisse and Agarwal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and others},
  journal={arXiv preprint arXiv:2311.11045},
  year={2023}
}

@article{madsen2022survey,
  title={Post-hoc interpretability for neural nlp: A survey},
  author={Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  journal={ACM Computing Surveys},
  volume={55},
  number={8},
  pages={1--42},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{tcav,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  booktitle={International conference on machine learning},
  pages={2668--2677},
  year={2018},
  organization={PMLR}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{cbm_by_design,
  title={Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck},
  author={Ludan, Josh Magnus and Lyu, Qing and Yang, Yue and Dugan, Liam and Yatskar, Mark and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:2310.19660},
  year={2023}
}

@inproceedings{cbm_sparsity_guided,
  title={Sparsity-guided holistic explanation for llms with interpretable inference-time intervention},
  author={Tan, Zhen and Chen, Tianlong and Zhang, Zhenyu and Liu, Huan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={19},
  pages={21619--21627},
  year={2024}
}

@article{cbm_crafting,
  title={Crafting large language models for enhanced interpretability},
  author={Sun, Chung-En and Oikarinen, Tuomas and Weng, Tsui-Wei},
  journal={Proceedings of the Mechanistic Interpretability Workshop@ICML},
  year={2024}
}

@article{gemma_2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{addressing_leakage_cbm,
  title={Addressing leakage in concept bottleneck models},
  author={Havasi, Marton and Parbhoo, Sonali and Doshi-Velez, Finale},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23386--23397},
  year={2022}
}

@article{concept_xai_survey,
  title={Concept-based explainable artificial intelligence: A survey},
  author={Poeta, Eleonora and Ciravegna, Gabriele and Pastor, Eliana and Cerquitelli, Tania and Baralis, Elena},
  journal={arXiv preprint arXiv:2312.12936},
  year={2023}
}

@inproceedings{cbm_plm,
  title={Interpreting pretrained language models via concept bottlenecks},
  author={Tan, Zhen and Cheng, Lu and Wang, Song and Yuan, Bo and Li, Jundong and Liu, Huan},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={56--74},
  year={2024},
  organization={Springer}
}

@inproceedings{cbm_incremental,
  title={Incremental residual concept bottleneck models},
  author={Shang, Chenming and Zhou, Shiji and Zhang, Hengyuan and Ni, Xinzhe and Yang, Yujiu and Wang, Yuwang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11030--11040},
  year={2024}
}

@inproceedings{health_dataset_nlp,
  title={Evaluating unsupervised text classification: zero-shot and similarity-based approaches},
  author={Schopf, Tim and Braun, Daniel and Matthes, Florian},
  booktitle={Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
  pages={6--15},
  year={2022}
}

@inproceedings{linear_rep_hypothesis,
  title={The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}

@article{concept_survey,
  title={Concept-based explainable artificial intelligence: A survey},
  author={Poeta, Eleonora and Ciravegna, Gabriele and Pastor, Eliana and Cerquitelli, Tania and Baralis, Elena},
  journal={arXiv preprint arXiv:2312.12936},
  year={2023}
}

@article{elhage_linear_representation,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy\_model/index.html}
}

@inproceedings{steering_mean_embedding,
    title = "Steering Llama 2 via Contrastive Activation Addition",
    author = "Rimsky, Nina  and
      Gabrieli, Nick  and
      Schulz, Julian  and
      Tong, Meg  and
      Hubinger, Evan  and
      Turner, Alexander",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.828/",
    doi = "10.18653/v1/2024.acl-long.828",
    pages = "15504--15522",
    abstract = "We introduce Contrastive Activation Addition (CAA), a method for steering language models by modifying their activations during forward passes. CAA computes {\textquotedblleft}steering vectors{\textquotedblright} by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user`s prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA`s effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA`s mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs)."
}

@inproceedings{cbm_learning_intervene,
  title={Learning to Intervene on Concept Bottlenecks},
  author={Steinmann, David and Stammer, Wolfgang and Friedrich, Felix and Kersting, Kristian},
  booktitle={Proc. of the 41st Int. Conf. on Machine Learning, ICML},
year=2024
}

@article{scaling_monosemanticity,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@inproceedings{concepf_embedding_models,
  title={Concept embedding models},
  author={Zarlenga, Mateo Espinosa and Barbiero, Pietro and Ciravegna, Gabriele and Marra, Giuseppe and Giannini, Francesco and Diligenti, Michelangelo and Precioso, Frederic and Melacci, Stefano and Weller, Adrian and Lio, Pietro and others},
  booktitle={NeurIPS 2022-36th Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{label_free_cbm,
  title={Label-free Concept Bottleneck Models},
  author={Oikarinen, Tuomas and Das, Subhro and Nguyen, Lam M and Weng, Tsui-Wei},
  booktitle={Proc. of the 11th Int. Conf. on Learning Representations, ICLR},
year={2023}
}

@inproceedings{cbm_posthoc,
  title={Post-hoc Concept Bottleneck Models},
  author={Yuksekgonul, Mert and Wang, Maggie and Zou, James},
  booktitle={The Eleventh International Conference on Learning Representations},
year={2023}
}

@inproceedings{cbm_intro,
  title={Concept bottleneck models},
  author={Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={5338--5348},
  year={2020},
  organization={PMLR}
}

@article{AG_news,
  title={Ag’s corpus of news articles},
  author={Gulli, Antonio},
  journal={Dipartimento di Informatica, University of Pisa, Nov},
  year={2005}
}

@inproceedings{hedeberta,
  title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{dbpedia,
  title={Dbpedia--a large-scale, multilingual knowledge base extracted from wikipedia},
  author={Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, S{\"o}ren and others},
  journal={Semantic web},
  volume={6},
  number={2},
  pages={167--195},
  year={2015},
  publisher={IOS Press}
}

@article{madsen2024interpretability,
  title={Interpretability Needs a New Paradigm},
  author={Madsen, Andreas and Lakkaraju, Himabindu and Reddy, Siva and Chandar, Sarath},
  journal={arXiv preprint arXiv:2405.05386},
  year={2024}
}

@inproceedings{jacovi2020towards,
  title={Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?},
  author={Jacovi, Alon and Goldberg, Yoav},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020},
  organization={Association for Computational Linguistics}
}

@misc{madsen_are_2024,
	title = {Are self-explanations from {Large} {Language} {Models} faithful?},
	url = {http://arxiv.org/abs/2401.07927},
	abstract = {Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to three types of self-explanations: counterfactuals, importance measures, and redactions. Our work demonstrate that faithfulness is both task and model dependent, e.g., for sentiment classification, counterfactual explanations are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B. Finally, our findings are robust to prompt-variations.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Madsen, Andreas and Chandar, Sarath and Reddy, Siva},
	month = jan,
	year = {2024},
	note = {arXiv:2401.07927 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\EGMSAJKW\\2401.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\PHJYG69V\\Madsen et al. - 2024 - Are self-explanations from Large Language Models f.pdf:application/pdf},
}

@misc{colombo_toward_2023,
	title = {Toward {Stronger} {Textual} {Attack} {Detectors}},
	url = {http://arxiv.org/abs/2310.14001},
	abstract = {The landscape of available textual adversarial attacks keeps growing, posing severe threats and raising concerns regarding the deep NLP system's integrity. However, the crucial problem of defending against malicious attacks has only drawn the attention of the NLP community. The latter is nonetheless instrumental in developing robust and trustworthy systems. This paper makes two important contributions in this line of search: (i) we introduce LAROUSSE, a new framework to detect textual adversarial attacks and (ii) we introduce STAKEOUT, a new benchmark composed of nine popular attack methods, three datasets, and two pre-trained models. LAROUSSE is ready-to-use in production as it is unsupervised, hyperparameter-free, and non-differentiable, protecting it against gradient-based methods. Our new benchmark STAKEOUT allows for a robust evaluation framework: we conduct extensive numerical experiments which demonstrate that LAROUSSE outperforms previous methods, and which allows to identify interesting factors of detection rate variations.},
	urldate = {2024-01-26},
	publisher = {arXiv},
	author = {Colombo, Pierre and Picot, Marine and Noiry, Nathan and Staerman, Guillaume and Piantanida, Pablo},
	month = oct,
	year = {2023},
	note = {arXiv:2310.14001 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\WCRZQX34\\2310.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\DCR26ZQA\\Colombo et al. - 2023 - Toward Stronger Textual Attack Detectors.pdf:application/pdf},
}

@article{yuksekgonul_post-hoc_2023,
	title = {{POST}-{HOC} {CONCEPT} {BOTTLENECK} {MODELS}},
	abstract = {Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (“the bottleneck”) and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model ”sees” in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via conceptlevel feedback can provide significant performance gains without using data from the target domain or model retraining. The code for our paper can be found in https://github.com/mertyg/post-hoc-cbm.},
	language = {en},
	author = {Yuksekgonul, Mert and Wang, Maggie and Zou, James},
	year = {2023},
	keywords = {notion},
	file = {Yuksekgonul et al. - 2023 - POST-HOC CONCEPT BOTTLENECK MODELS.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\ZZUAEJ8S\\Yuksekgonul et al. - 2023 - POST-HOC CONCEPT BOTTLENECK MODELS.pdf:application/pdf},
}

@inproceedings{atanasova_faithfulness_2023,
	address = {Toronto, Canada},
	title = {Faithfulness {Tests} for {Natural} {Language} {Explanations}},
	url = {https://aclanthology.org/2023.acl-short.25},
	doi = {10.18653/v1/2023.acl-short.25},
	abstract = {Explanations of neural models aim to reveal a model's decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model's inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a counterfactual input editor for inserting reasons that lead to counterfactual predictions but are not reflected by the NLEs. Second, we reconstruct inputs from the reasons stated in the generated NLEs and check how often they lead to the same predictions. Our tests can evaluate emerging NLE models, proving a fundamental tool in the development of faithful NLEs.},
	urldate = {2024-01-31},
	booktitle = {Proc. of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Atanasova, Pepa and Camburu, Oana-Maria and Lioma, Christina and Lukasiewicz, Thomas and Simonsen, Jakob Grue and Augenstein, Isabelle},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {283--294},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\TPQG4L7S\\Atanasova et al. - 2023 - Faithfulness Tests for Natural Language Explanatio.pdf:application/pdf},
}

@misc{diaz-rodriguez_connecting_2023,
	title = {Connecting the {Dots} in {Trustworthy} {Artificial} {Intelligence}: {From} {AI} {Principles}, {Ethics}, and {Key} {Requirements} to {Responsible} {AI} {Systems} and {Regulation}},
	shorttitle = {Connecting the {Dots} in {Trustworthy} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2305.02231},
	abstract = {Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a given auditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized through auditing processes, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.},
	urldate = {2024-01-31},
	publisher = {arXiv},
	author = {Díaz-Rodríguez, Natalia and Del Ser, Javier and Coeckelbergh, Mark and de Prado, Marcos López and Herrera-Viedma, Enrique and Herrera, Francisco},
	month = jun,
	year = {2023},
	note = {arXiv:2305.02231 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, 68T01, I.2, K.4, K.5},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\FMI26HAL\\2305.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\7RHFH2DG\\Díaz-Rodríguez et al. - 2023 - Connecting the Dots in Trustworthy Artificial Inte.pdf:application/pdf},
}

@article{ye_unreliability_nodate,
	title = {The {Unreliability} of {Explanations} in {Few}-shot {Prompting} for {Textual} {Reasoning}},
	abstract = {Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially.},
	language = {en},
	author = {Ye, Xi and Durrett, Greg},
	keywords = {notion},
	file = {Ye et Durrett - The Unreliability of Explanations in Few-shot Prom.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\AC73BRMU\\Ye et Durrett - The Unreliability of Explanations in Few-shot Prom.pdf:application/pdf},
}

@misc{madaan_self-refine_2023-1,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\8MAZ4LRS\\2303.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\QZH77KRL\\Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf:application/pdf},
}

@inproceedings{wang_gam_2023,
	title = {{GAM} {Coach}: {Towards} {Interactive} and {User}-centered {Algorithmic} {Recourse}},
	shorttitle = {{GAM} {Coach}},
	url = {http://arxiv.org/abs/2302.14165},
	doi = {10.1145/3544548.3580816},
	abstract = {Machine learning (ML) recourse techniques are increasingly used in high-stakes domains, providing end users with actions to alter ML predictions, but they assume ML developers understand what input variables can be changed. However, a recourse plan's actionability is subjective and unlikely to match developers' expectations completely. We present GAM Coach, a novel open-source system that adapts integer linear programming to generate customizable counterfactual explanations for Generalized Additive Models (GAMs), and leverages interactive visualizations to enable end users to iteratively generate recourse plans meeting their needs. A quantitative user study with 41 participants shows our tool is usable and useful, and users prefer personalized recourse plans over generic plans. Through a log analysis, we explore how users discover satisfactory recourse plans, and provide empirical evidence that transparency can lead to more opportunities for everyday users to discover counterintuitive patterns in ML models. GAM Coach is available at: https://poloclub.github.io/gam-coach/.},
	urldate = {2024-02-05},
	booktitle = {Proc. of the 2023 {CHI} {Conf.} on {Human} {Factors} in {Computing} {Systems}},
	author = {Wang, Zijie J. and Vaughan, Jennifer Wortman and Caruana, Rich and Chau, Duen Horng},
	month = apr,
	year = {2023},
	note = {arXiv:2302.14165 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	pages = {1--20},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\FAILF7T8\\2302.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\92UP2RHF\\Wang et al. - 2023 - GAM Coach Towards Interactive and User-centered A.pdf:application/pdf},
}

@article{yuksekgonul_post-hoc_2023-1,
	title = {{POST}-{HOC} {CONCEPT} {BOTTLENECK} {MODELS}},
	abstract = {Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (“the bottleneck”) and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model ”sees” in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via conceptlevel feedback can provide significant performance gains without using data from the target domain or model retraining. The code for our paper can be found in https://github.com/mertyg/post-hoc-cbm.},
	language = {en},
	author = {Yuksekgonul, Mert and Wang, Maggie and Zou, James},
	year = {2023},
	keywords = {notion},
	file = {Yuksekgonul et al. - 2023 - POST-HOC CONCEPT BOTTLENECK MODELS.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\QQGP52KX\\Yuksekgonul et al. - 2023 - POST-HOC CONCEPT BOTTLENECK MODELS.pdf:application/pdf},
}

@inproceedings{balagopalan_road_2022,
	address = {Seoul Republic of Korea},
	title = {The {Road} to {Explainability} is {Paved} with {Bias}: {Measuring} the {Fairness} of {Explanations}},
	isbn = {978-1-4503-9352-2},
	shorttitle = {The {Road} to {Explainability} is {Paved} with {Bias}},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533179},
	doi = {10.1145/3531146.3533179},
	language = {en},
	urldate = {2024-02-08},
	booktitle = {2022 {ACM} {Conf.} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Balagopalan, Aparna and Zhang, Haoran and Hamidieh, Kimia and Hartvigsen, Thomas and Rudzicz, Frank and Ghassemi, Marzyeh},
	month = jun,
	year = {2022},
	keywords = {notion},
	pages = {1194--1206},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\G7V9YICG\\Balagopalan et al. - 2022 - The Road to Explainability is Paved with Bias Mea.pdf:application/pdf},
}

@inproceedings{bordt_post-hoc_2022,
	address = {Seoul Republic of Korea},
	title = {Post-{Hoc} {Explanations} {Fail} to {Achieve} their {Purpose} in {Adversarial} {Contexts}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533153},
	doi = {10.1145/3531146.3533153},
	language = {en},
	urldate = {2024-02-08},
	booktitle = {2022 {ACM} {Conf.} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Bordt, Sebastian and Finck, Michèle and Raidl, Eric and Von Luxburg, Ulrike},
	month = jun,
	year = {2022},
	keywords = {notion},
	pages = {891--905},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\8YQZX5TC\\Bordt et al. - 2022 - Post-Hoc Explanations Fail to Achieve their Purpos.pdf:application/pdf},
}

@misc{yang_psychological_2022,
	title = {A {Psychological} {Theory} of {Explainability}},
	url = {http://arxiv.org/abs/2205.08452},
	abstract = {The goal of explainable Artificial Intelligence (XAI) is to generate human-interpretable explanations, but there are no computationally precise theories of how humans interpret AI generated explanations. The lack of theory means that validation of XAI must be done empirically, on a case-by-case basis, which prevents systematic theory-building in XAI. We propose a psychological theory of how humans draw conclusions from saliency maps, the most common form of XAI explanation, which for the first time allows for precise prediction of explainee inference conditioned on explanation. Our theory posits that absent explanation humans expect the AI to make similar decisions to themselves, and that they interpret an explanation by comparison to the explanations they themselves would give. Comparison is formalized via Shepard's universal law of generalization in a similarity space, a classic theory from cognitive science. A pre-registered user study on AI image classifications with saliency map explanations demonstrate that our theory quantitatively matches participants' predictions of the AI.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Yang, Scott Cheng-Hsin and Folke, Tomas and Shafto, Patrick},
	month = jun,
	year = {2022},
	note = {arXiv:2205.08452 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\milan.bhan\\Zotero\\storage\\M876XY8W\\2205.html:text/html;Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\MUMRTCF7\\Yang et al. - 2022 - A Psychological Theory of Explainability.pdf:application/pdf},
}

@article{covert_explaining_nodate,
	title = {Explaining by {Removing}: {A} {Uniﬁed} {Framework} for {Model} {Explanation}},
	abstract = {Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new uniﬁed class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature’s inﬂuence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature’s inﬂuence. Our framework uniﬁes 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-oﬀs among diﬀerent methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a uniﬁed framework that helps practitioners better understand model explanation tools, and that oﬀers a strong theoretical foundation upon which future explainability research can build.},
	language = {en},
	author = {Covert, Ian C},
	keywords = {notion},
	file = {Covert - Explaining by Removing A Uniﬁed Framework for Mod.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\PMIENY5M\\Covert - Explaining by Removing A Uniﬁed Framework for Mod.pdf:application/pdf},
}

@inproceedings{sharma_feamoe_2023,
	address = {Macau, SAR China},
	title = {{FEAMOE}: {Fair}, {Explainable} and {Adaptive} {Mixture} of {Experts}},
	isbn = {978-1-956792-03-4},
	shorttitle = {{FEAMOE}},
	url = {https://www.ijcai.org/proceedings/2023/55},
	doi = {10.24963/ijcai.2023/55},
	abstract = {Three key properties that are desired of trustworthy machine learning models deployed in highstakes environments are fairness, explainability, and an ability to account for various kinds of "drift". While drifts in model accuracy have been widely investigated, drifts in fairness metrics over time remain largely unexplored. In this paper, we propose FEAMOE, a novel "mixture-of-experts" inspired framework aimed at learning fairer, more interpretable models that can also rapidly adjust to drifts in both the accuracy and the fairness of a classifier. We illustrate our framework for three popular fairness measures and demonstrate how drift can be handled with respect to these fairness constraints. Experiments on multiple datasets show that our framework as applied to a mixture of linear experts is able to perform comparably to neural networks in terms of accuracy while producing fairer models. We then use the large-scale HMDA dataset and show that various models trained on HMDA demonstrate drift and FEAMOE can ably handle these drifts with respect to all the considered fairness measures and maintain model accuracy. We also prove that the proposed framework allows for producing fast Shapley value explanations, which makes computationally efficient feature attribution based explanations of model decisions readily available via FEAMOE.},
	language = {en},
	urldate = {2024-02-08},
	booktitle = {Proc. of the {Thirty}-{Second} {Int.} {Joint} {Conf.} on {Artificial} {Intelligence}},
	publisher = {Int. Joint Conf. on Artificial Intelligence Organization},
	author = {Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
	month = aug,
	year = {2023},
	keywords = {notion},
	pages = {492--500},
	file = {Sharma et al. - 2023 - FEAMOE Fair, Explainable and Adaptive Mixture of .pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\7JAFQFNW\\Sharma et al. - 2023 - FEAMOE Fair, Explainable and Adaptive Mixture of .pdf:application/pdf},
}

@article{oikarinen_label-free_2023,
	title = {{LABEL}-{FREE} {CONCEPT} {BOTTLENECK} {MODELS}},
	abstract = {Concept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to humanunderstandable concepts. However, existing CBMs and their variants have two crucial limitations: first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable - we present the first CBM scaled to ImageNet, efficient - creating a CBM takes only a few hours even for very large datasets, and automated - training it for a new dataset requires minimal human effort. Our code is available at https://github.com/TrustworthyML-Lab/Label-free-CBM.},
	language = {en},
	author = {Oikarinen, Tuomas and Nguyen, Lam M},
	year = {2023},
	keywords = {notion},
	file = {Oikarinen et Nguyen - 2023 - LABEL-FREE CONCEPT BOTTLENECK MODELS.pdf:C\:\\Users\\milan.bhan\\Zotero\\storage\\6PNC7SAV\\Oikarinen et Nguyen - 2023 - LABEL-FREE CONCEPT BOTTLENECK MODELS.pdf:application/pdf},
}