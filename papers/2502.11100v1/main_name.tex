% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

\usepackage{authblk}
% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
\usepackage[]{ACL2023}
\usepackage{lipsum}
\usepackage{multirow}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{algorithm,algorithmic}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{xcolor,colortbl}
\usepackage{cleveref}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\method}{\texttt{CT-CBM}}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{bbm}
% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

       
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Towards Achieving Concept Completeness\\ for Unsupervised Textual Concept Bottleneck Models}

% \author{First Author \\
%   test / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author[1,2]{Milan Bhan$^*$}
\author[2]{Yann Choho$^*$}
\author[2]{Pierre Moreau}
\author[1]{Jean-Noël Vittaut}
\author[2]{Nicolas Chesneau}
\author[1]{Marie-Jeanne Lesot}
\affil[1]{Sorbonne Université, CNRS, LIP6, F-75005 Paris, France}
\affil[2]{Ekimetrics, Paris, France}
\affil[ ]{\texttt{\{milan.bhan, nicolas.chesneau\}@ekimetrics.com}}
\affil[ ]{\texttt{\{jean-noel.vittaut, marie-jeanne.lesot\}@lip6.fr}}
\renewcommand\Authands{ and }


\begin{document}
\maketitle
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work}
\begin{abstract}
Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models for text classification that predict a set of salient concepts before making the final prediction. 
%However, existing TCBM face significant limitations, including reliance on computationally expensive large language models, dependence on predefined human-labeled concepts, vulnerability to classification leakage, and unreliable concept detection. 
This paper proposes Complete Textual Concept Bottleneck Model (\method), 
%to address these limitations. \method\ 
a novel TCBM generator building concept labels in a fully unsupervised manner using a small language model, eliminating both the need for predefined human labeled concepts and LLM annotations. \method\ iteratively targets and adds important concepts in the bottleneck layer to create a complete concept basis and addresses downstream classification leakage through a parallel residual connection. 
\method\ achieves good results against competitors, offering a promising solution to enhance interpretability of NLP classifiers without sacrificing performance.




% a novel approach to enhance interpretability in natural language processing (NLP) classifiers. Existing Textual Concept Bottleneck Models (TCBM) for NLP face significant limitations, including reliance on computationally expensive large language models, dependence on predefined human-labeled concepts, overcomplete concept layer, vulnerability to classification leakage, and unreliable concept activations. \method\ addresses the limitations of existing TCBM for text classification by generating concept labels in a fully unsupervised manner using a small language model, eliminating the need for predefined labeled concepts and the use of LLM for annotation. \method\ iteratively adds important concepts to create a complete concept basis, addresses downstream classification leakage through a parallel residual connection, and implements strategies to build the TCBM along with heuristics to detect important concepts. Experimental results demonstrate that \method\ consistently achieves performance comparable to fine-tuned black-box neural NLP models while accurately detecting concept activations in the complete concept bottleneck layer. This approach offers a promising solution to enhance interpretability in NLP without sacrificing performance, bridging the gap between high-performing black-box models and the need for explainable AI in NLP.
\end{abstract}


% \vspace{0.5cm}
\section{Introduction} 
\label{intro}
% \begin{itemize}
%     \item Def CBM
%     \item Mainly for CV
%     \item Need label and put to many concepts
%     \item Our approach: label free, iterative, sufficient set of concepts, based on SLMs
%     \item eval on 4 models and 4 datasets, better than SOTA
% \end{itemize}

The striking level of performance in natural language processing (NLP) achieved by black-box neural language models~\cite{vaswani_attention_2017, gpt3, chowdhery2023palm} comes along with a lack of interpretability~\cite{madsen2022survey}. The field of eXplainable Artificial Intelligence (XAI)~\cite{xai_2_0_survey} intends to make the behavior of such models more interpretable. A common distinction of XAI is to define interpretability methods either (1) by applying post hoc explanation methods to interpret black box models,  or (2) by constructing interpretable models by-design~\cite{jacovi2020towards, madsen2024interpretability}. 

\begin{table}[t]
\small
\begin{tabular}{cccc}
\hline
\rowcolor[HTML]{ECF4FF} 
                                                                           & \textbf{\texttt{C\textsuperscript{3}M}} & \textbf{\texttt{CB-LLM}} & \textbf{\method} \\
\rowcolor[HTML]{ECF4FF} 
                                                                           &                       &                          & \textbf{(ours)}  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\begin{tabular}[c]{@{}c@{}}Need for predefined\\  concepts\end{tabular}        & Yes                   & Yes                      & \textbf{No}      \\ \hline
\rowcolor[HTML]{EFEFEF} 
\begin{tabular}[c]{@{}c@{}}Classification leakage \\ tackling\end{tabular} & No                    & No                       & \textbf{Yes}     \\ \hline
\rowcolor[HTML]{FFFFFF} 
\begin{tabular}[c]{@{}c@{}}Concept base\\  complenetess\end{tabular}       & No                    & No                       & \textbf{Yes}     \\ \hline
\rowcolor[HTML]{EFEFEF} 
\begin{tabular}[c]{@{}c@{}}Accurate concept\\  detection\end{tabular}      & No                    & No                       & \textbf{Yes}     \\ \hline
\rowcolor[HTML]{FFFFFF} 
\begin{tabular}[c]{@{}c@{}}Black-box \\ performance reached\end{tabular}   & \textbf{Yes}                    & \textbf{Yes}             & \textbf{Yes}     \\ \hline
\rowcolor[HTML]{EFEFEF} 
Use of ChatGPT                  & Yes                   & Yes                      & \textbf{No}      \\ \hline
\rowcolor[HTML]{FFFFFF} 
Scalability                                                                & No                    & \textbf{Yes}             & \textbf{Yes}     \\ \hline
\end{tabular}
\caption{\label{tab:intro} 
Qualitative comparison of \method\ to competitors. Desired modalities are highlighted in bold.}
\end{table}


One promising approach to designing models that are more interpretable is Concept Bottleneck Models (CBM)~\cite{cbm_intro}. CBM are models that first map the input representations to a set of human-interpretable high-level attributes, called \emph{concepts}. The latter are then used to make the final prediction with a linear layer,  improving the interpretability of black box models. While CBM have been widely used in computer vision~\cite{cbm_posthoc, label_free_cbm, cbm_incremental, concepf_embedding_models}, they have been much less explored for NLP~\cite{concept_xai_survey}. Existing Textual Concept Bottleneck Models (TCBM) have limitations: (i) they mainly rely on the use of large language models (LLM)~\cite{cbm_sparsity_guided,cbm_plm, cbm_crafting,  cbm_by_design} whose computational cost is very high, (ii) they often require access to a set of predefined human-labeled concepts~\cite{cbm_plm, cbm_sparsity_guided, cbm_by_design}, (iii) their concept layers can be non complete, missing  concepts relevant for the downstream classification and potentially leading to performance loss~\cite{cbm_plm, cbm_crafting, cbm_sparsity_guided}, (iv) they do not address classification leakage~\cite{cbm_sparsity_guided, cbm_plm, cbm_crafting}, leading to the use of unintended information from the concept predictor, (v) they do not systematically guarantee the reliability of concept activations, actually challenging the interpretability faithfulness of CBM.


% \begin{figure}[t]{\centering}
% \begin{center}
% \includegraphics[scale=0.45]{image/intro.png}
% \caption{BLABALLBABAL}
% \label{intro_fig}
% \end{center}
% \end{figure}

In this paper, we propose Complete Textual Concept Bottleneck Model (\method), a novel approach to transform any fine-tuned NLP classifier into an interpretable-by-design TCBM. As summarised in Table~\ref{tab:intro}, the main contributions of \method\ are as follows: \begin{enumerate}
    \item Concept labels are computed in a fully unsupervised manner without the need of predefined labelled concepts, based only a small language model (SLM).
    \item Concept completeness is achieved through iterative addition of important concepts in the concept layer.
    \item Downstream classification leakage is addressed through a parallel residual connection.
\end{enumerate}


% In this paper, we propose Complete Textual Concept Bottleneck Model (\method), a novel approach to transform any fine-tuned NLP classifier into an interpretable-by-design TCBM. As summarised in Table~\ref{tab:intro}, the main contributions of \method\ are as follows:\\
% (1)  Concept labels are computed in a fully unsupervised manner without the need of predefined labelled concepts, based only a small language model (SLM).\\
%     %Important concepts are iteratively added to the concept layer, enabling to achieve concept completeness.
% (2)    Concept completeness is achieved through iterative addition of important concepts in the concept layer. \\
%  (3) Downstream classification leakage is addressed through a parallel residual connection.



% To enhance any fine tuned NLP classifier with a concept bottleneck layer to transform it into an interpretable-by-design CBM, we propose in this paper COMPlete Iterative Textual Concept Bottleneck Model (\method). The main contributions of \method\ are as follows: 
%\begin{itemize}
%    \item Concept labels are computed in a fully unsupervised manner without the need of predefined labelled concepts, based only a small language model (SLM).
%    \item %Important concepts are iteratively added to the concept layer, enabling to achieve concept completeness.
%    Concept completeness is achieved through iterative addition of important concepts in the concept layer. 
%    \item Downstream classification leakage is addressed through a parallel residual connection.
%\end{itemize}
	
% As an illustration, Figure~\ref{intro_fig} illustrates the outcome of an NLP classifier enriched with a concept bottleneck layer within the \method\ framework, making the prediction more interpretable than a simple classification. The concepts X and Y are activated to make the final predictions. Important tokens explaining the concept activations are highlighted in different shades of blue (X) and red (Y).

This paper is organized as follows:  Section~\ref{bk_rw} recalls some basic principles of XAI and related work. Section~\ref{method} describes in details the proposed \method. Section~\ref{xp} discusses the conducted experiments, that show that \method\ systematically succeeds in reaching the performance of a fine-tuned black box neural NLP model, while accurately detecting the activations of the concepts contained in the complete concept bottleneck layer.




\section{Background and Related Work}
\label{bk_rw}
This section first recalls some principles of XAI methods used later in the papers and presents existing methods generating Concept Bottleneck Model for NLP.
% \begin{itemize}
%     \item Def concepts and TCAV
%     \item Def CBM
%     \item Strategies: projection, and training (joint, sequential and independant)
%     \item With and without label, iteratively, learning to intervene
%     \item No label: use of LLM (chatgpt...) and mainly for CV
% \end{itemize}


\subsection{XAI Background}
% \paragraph{Neural Post Hoc Interpretability.} 
\paragraph{Post Hoc Interpretability.}
Post hoc methods explain the behavior of a model after its training. These include post hoc attribution methods that attribute importance scores to inputs to explain the model outcome~\cite{zhao_explainability_2023}. In particular, gradient-based approaches such as \texttt{Integrated Gradients}~\cite{sundararajan_axiomatic_2017} compute these scores by back-propagating the gradients through the model.

Post hoc concept-based approaches generate explanations at a higher level of abstraction, by focusing on human interpretable attributes, called \emph{concepts}. %For instance, 
%A popular way of generating post hoc concept-based explanations is 
\texttt{TCAV}~\cite{tcav}
%. This approach consists in assessing 
assesses the model's sensitivity to a concept by back-propagating the gradients with respect to a linear representation of a candidate concept in the activation space, called concept activation vector (CAV). In the original paper, 
\texttt{TCAV} relies on human-labeled concepts, whose annotation can be time-consuming and expensive.  


\paragraph{Concept Bottleneck Models.}

Another way to improve interpretability
%the interpretability of AI systems 
consists in constructing 
%more interpretable 
so-called Concept Bottleneck Models (CBM)~\cite{cbm_intro}. 
These models sequentially (1) detect concepts and (2) linearly make the final prediction from concept activations, thereby significantly improving the understanding of the decision-making process. 

CBM have some limitations, such as requiring a predefined set of human-labeled concepts, generating incomplete concept bottleneck layers (CBL), or doing downstream task leakage. Concept incompleteness can have consequences either on the model accuracy (under-complete concept base) or the intelligibility (over-complete concept base) of the provided explanations~\cite{cbm_incremental}. Downstream task leakage~\cite{addressing_leakage_cbm} occurs when the final prediction uses unintended additional information from the concept predictor scores. 
%Under such a leakage regime, 
The concept predictor then no longer needs to  detect faithfully the concepts to be accurate on the classification task, thus compromising the interpretability faithfulness of the CBM. 

Among the vast literature on CBM~\cite{concept_survey}, numerous variants have been proposed to address one by one the aforementioned limitations. Notably, \texttt{Label-Free CBM}~\cite{label_free_cbm} prompts GPT-3~\cite{gpt3} to list the most important concepts for recognizing a specific class, freeing the approach from dependency on predefined labeled concepts. However, \texttt{Label-Free CBM} structurally depends on the parametric knowledge of GPT-3 and does not generate data driven concepts. In order to reach the accuracy of a black box NLP classifier while avoiding leakage, a non-interpretable connection parallel to the concept layer can be added to fit the residuals between the raw CBM outcome and the ground truth~\cite{cbm_posthoc, addressing_leakage_cbm}. However, adding such a residual connection decreases the CBM interpretability. \texttt{Res-CBM}~\cite{cbm_incremental} develops a method to derive new concepts from the residual layer to build a more complete CBL. Yet, it 
%\texttt{Res-CBM}~\cite{cbm_incremental} 
requires access to a set of candidate concepts to add to the CBL before probing the residual connection. Although these methods overcome some of the limitations inherent in CBM, their application has so far been restricted to computer vision. 



\subsection{Textual Concept Bottleneck Models}
\label{related_work}
% \begin{itemize}
%     \item SOTA \begin{itemize}
%         \item Interpreting Pretrained Language Models via Concept Bottlenecks
%     \item Sparsity-Guided Holistic Explanation for LLMs with
% Interpretable Inference-Time Intervention
%     \item Interpretable-by-Design Text Understanding
% with Iteratively Generated Concept Bottleneck
% \item Crafting Large Language Models for Enhanced Interpretability
% \end{itemize}
%     \item Limitations \begin{itemize}
%         \item Mostly need labels
%         \item when label-free -> based on LLM prior knowledge ("what are the important concepts?)"
%         \item Use LLM (chatgpt) -> very costly
%         \item Put many concepts (over-sufficiency)
%         \item Performance on concept detection
%     \end{itemize}
% \end{itemize}
This section presents recent works on generating Concept Bottleneck Models for NLP, referred to  as Textual Concept Bottleneck Models (TCBM).

\texttt{C\textsuperscript{3}M}~\cite{cbm_plm} enriches a set of predefined human-labeled concepts with additional concepts obtained from ChatGPT. While it approximately reaches the performance of an unrestricted black-box NLP classifier, it is trained without addressing the completeness of the CBL and the downstream classification leakage. Besides, its relying on ChatGPT and human-labeled concepts prevents reproducibility and scalability.

% \texttt{C\textsuperscript{3}M}~\cite{cbm_plm} consists in enriching a set of predefined human-labeled concepts with additional concepts obtained from ChatGPT\footnote{\url{https://openai.com/chatgpt/overview/}}. \texttt{Sparse-CBM}~\cite{cbm_sparsity_guided} extended \texttt{C\textsuperscript{3}M} by making the language model backbone sparser and thus more interpretable. While \texttt{C\textsuperscript{3}M} and \texttt{Sparse-CBM} approximately reach the performance of an unrestricted black-box NLP classifier, they are trained without addressing the completeness of the concept bottleneck layer and the downstream classification leakage. Besides, they rely on ChatGPT and human-labeled concepts, which poses problems in terms of reproducibility and scalability.

\texttt{CB-LLM}~\cite{cbm_crafting, cb_llm} also uses ChatGPT to generate a set of concepts that are scored with a sentence embedding model~\cite{reimers_sentence-bert_2019} to perform concept labeling. This way, concept are represented with numerical values, unlike \texttt{C\textsuperscript{3}M}. The concepts are then added to the CBL to train the TCBM. While \texttt{CB-LLM} approximately reaches the performance of a black-box NLP classifier, downstream classification leakage is not addressed. Moreover, it overlooks the completeness of its CBL, possibly resulting in an excessive number of concepts in the bottleneck layer and unintelligible explanations.

\texttt{TBM}~\cite{cbm_by_design} iteratively discovers concepts by leveraging GPT-4~\cite{gpt4} and focusing on examples misclassified by a separately trained linear layer. \texttt{TBM} is not strictly a CBM, since concept detection is performed with GPT-4 during inference, making also the approach non scalable and computationally expensive.
% \texttt{TBM}~\cite{cbm_by_design} iteratively discovers concepts by leveraging GPT-4~\cite{gpt4} and focusing on examples misclassified by a separately trained linear layer. While this approach does not depend on a predefined set of human-labeled concepts, \texttt{TBM} is not strictly a CBM, since concept detection is performed with GPT-4 during inference, making the approach non scalable and computationally expensive.


\begin{figure*}[t]{\centering}
\begin{center}
\includegraphics[scale=0.55]{image/overall_pipeline.png}
\caption{\method\ overview illustrated in the example of film synopsis classification. \method\ is a 4-step approach to build a TCBM from a $f$ black box NLP classifier. (1) A concept bank is created from the text corpus of interest. (2) Concepts are scored with respect to their importance to explain $f$ predictions. (3) The TCBM is trained through 3 layers: $\Phi^{\text{C}}$, $\Phi^{\text{cls}}$ and $\Phi^{\text{r}}$. (4) The TCBM training stops when the importance of $\Phi^{\text{r}}$ stops decreasing.}  
\label{fig:global_flow}
\end{center}
\end{figure*}

\section{Proposed approach: \method} 
\label{method}
This section describes the architecture of the proposed Complete Textual Concept Bottleneck Model (\method). As summarized in Table~\ref{tab:intro}, \method\  addresses classification leakage and only uses SLM to build TCBM with a complete concept bottleneck basis without the need for a predefined human-labeled set of concepts. 

\subsection{\method\ Overview}
We consider a corpus of text-label pairs $\mathcal{T} = \left\{ (x,y)\right\}$ where $x \in \mathcal{X}$ denotes the text and ${y \in \mathcal{Y}}$ the label. $f : \mathcal{X} \rightarrow \mathbb{R}^d$ is the backbone of a language model classifier fine-tuned on $\mathcal{T}$, with $d$ the dimension of the $f$ embedding space. The TCBM is constructed by iteratively adding concepts into the CBL until a stopping criterion, which validates concept completeness, is met. Concepts are added progressively based on their importance scores, starting with the highest. As shown in Figure~\ref{fig:global_flow}, \method\ is a 4-step framework 
%summarised below and 
detailed in the following subsections: \\
 \textbf{1. Concept Bank Construction}. A set of concept candidates is generated from $\mathcal{X}$. \method\ prompts an auto-regressive SLM to enrich $\mathcal{T}$ with micro concepts defined as topics. Micro concepts are clustered to construct a set of meaningful high level macro concept candidates.\\
 \textbf{2. Concept Scoring}. 
The importance of the candidate concepts is assessed, in order to select the one to be added in the concept bottleneck layer.\\
% We define how to compute concept importance to select the macro concept to be added in the concept bottleneck layer.\\
 \textbf{3. TCBM Training}. Given a set of concepts, we train a \textit{simple} TCBM and a \textit{residual} TCBM with an additional parallel residual connection.\\
\textbf{4. Stopping Criterion}. Training stops when the importance of the residual connection is low and stable, indicating a complete concept bottleneck basis. The residual layer is finally removed to obtain a \textit{simple} TCBM.

% \begin{enumerate}
%      \item \textbf{Concept Bank Construction}. A set of concept candidates is generated from $\mathcal{X}$. \method\ prompts an autoregressive SLM to enrich $\mathcal{T}$ with micro concepts defined as topics. Micro concepts are clustered to construct a set of meaningful high level macro concept candidates.

%     \item \textbf{Concept Scoring}. We define how to compute concept importance to select the macro concept to be added in the concept bottleneck layer.
     
%      \item \textbf{TCBM Training}. Given a set of concepts, we define how to train a \textit{simple} TCBM and a \textit{residual} TCBM with an additional parallel residual connection.

%      \item \textbf{Stopping Criterion}. Training stops when the importance of the residual connection is low and stable, indicating a complete concept bottleneck basis. The residual layer is finally removed to obtain a \textit{simple} TCBM.
% \end{enumerate}

% \paragraph{Step 1: Concept Bank Construction.} A set of concept candidates is generated from $\mathcal{X}$. \method\ prompts an autoregressive SLM to enrich $\mathcal{T}$ with micro concepts defined as topics. Micro concepts are clustered to construct a set of meaningful high level macro concept candidates.

% \paragraph{Step 2: TCBM Training.}
% Given a set of concepts, we define how to train a \textit{simple} TCBM and a \textit{residual} TCBM with an additional parallel residual connection.

% \paragraph{Step 3: Concept Scoring.} We define how to compute concept importance to select the macro concept to be added in the concept bottleneck layer. These importance scores can be either \texttt{static} or \texttt{dynamic}.

% \paragraph{Step 4: Stopping Criterion.} Training stops when the importance of the residual connection is low and stable, indicating a complete concept bottleneck basis. The residual layer is finally removed to obtain a \textit{simple} TCBM.

\subsection{Concept Bank Construction}


The first step aims at constructing a set of concept candidates $\mathcal{C}$ without human annotation and in an automated manner for potential inclusion in the final CBL of the TCBM.

\paragraph{Micro Concept Bank Creation.}
We first prompt an auto-regressive language model to annotate each input text with several topics that we call micro concepts, representing features at a higher level of abstraction than simple tokens.
%(e.g. \textit{demoniac monster}). 
We then define the micro concept bank $\widetilde{\mathcal{C}}$ as the set of micro concepts (topics) associated with the text corpus $\mathcal{X}$ by the auto-regressive language model.

To be scalable and computationally affordable, \method\ uses an SLM, defined as having less than 9B parameters
%$\leqslant$ 9B parameters following
~\cite{slm_survey}, to perform the micro concept annotation. This choice is justified by the ability of recent SLM to follow precisely instructions at lower cost than LLM~\cite{gemma_2}. We give more information about the prompt used to generate micro concepts in Appendix~\ref{sec:prompt_micro_concept}.

% The text corpus is then enriched and defined as $\mathcal{T}_{m} = \left\{ (x,y, \widetilde{c})\right\}$, where $\widetilde{c} \in  \left\{ 0,1\right\}^{\widetilde{p}}$ is a vector of absence or presence of the $\widetilde{p}$ found micro concepts. 


\paragraph{Macro Concept Bank Creation.}
Secondly, we construct a set of macro concepts $\mathcal{C}$ by decomposing the set of micro concepts $\widetilde{\mathcal{C}}$ into $p$ clusters, with $p \ll \lvert \widetilde{\mathcal{C}} \rvert$. This choice is justified by the variability of the micro concepts generated by the SLM whose labels may differ but whose semantics may be very similar (e.g. \textit{demoniac monster} vs. \textit{diabolic creature} clustered as \textit{supernatural entities}). 
We sequentially use a sentence embedding model, UMAP~\cite{umap} and HDBSCAN~\cite{hdbscan} to perform macro concept clustering. By deriving candidate concepts from the analysis of the entire text corpus, \method\ concept bank creation stage is \textit{data-driven}, unlike its competitors which directly ask ChatGPT to provide the most promising concepts in general. 

The text corpus is finally formally defined as $\mathcal{T}_{M} = \left\{ (x,y, c)\right\}$, where $c = [c_{1},...,c_{p}] \in  \left\{ 0,1\right\}^{p}$ is a vector of absence or presence of the $p$ macro concepts found in $\mathcal{X}$. As detailed below, each concept $c_{k}$ is associated with a numerical representation $\overrightarrow{\gamma_{k}}$ and a textual label $l_{k}$.

\paragraph{Concept Activations Vectors Computing.} 
The "Linear Representation Hypothesis" states that high-level concepts are represented linearly in the embedding space of language models~\cite{elhage_linear_representation,linear_rep_hypothesis}. Motivated by this hypothesis, we assign to each macro concept a linear representation from $f$ embedding space, called Concept Activation Vector (CAV). These CAVs are later  used to build the CBL. 

For each concept $c_{k}$, we define its CAV $\overrightarrow{\gamma_{k}}$ as the mean difference (MD) of embeddings~\cite{steering_mean_embedding}:
\begin{equation}
  \overrightarrow{\gamma_{k}} = \frac{1}{\left|\mathcal{X}^{+}_{k}\right|} \sum_{x \in \mathcal{X}^{+}_{k}} f(x) - \frac{1}{\left|\mathcal{X}^{-}_{k}\right|} \sum_{x \in \mathcal{X}^{-}_{k}} f(x)  
\end{equation}
where $\mathcal{X}^{+}_{k}$ and $\mathcal{X}^{-}_{k}$ respectively represent the corpora of texts where $c_{k}$ is present or absent.
Among the different ways to compute a CAV~\cite{cav_axbench}, MD leads to the best compromise is terms of concept detection accuracy and computational cost~\cite{geometry_of_truth}.


% To compute a CAV, we follow the approach of ~\citet{tcav} and train a linear SVM to detect a concept in the embedding space retrieving the vector normal to the linear classification boundary. For a given a concept $c_{k}$, we denote as $\overrightarrow{\gamma_{k}}$ its related CAV.

\paragraph{Macro Concept Labeling.}
Finally, the %previously identified 
macro concepts are assigned to a textual label $l_{k}$ using the SLM used for micro concept annotation. Due to the potential various micro concepts related to a macro concept, we sample the 15 micro concepts closest to the macro concept centroid and prompt the SLM to define the superclass of these micro concepts. More details about the prompt used to do macro concept labeling are given in Appendix~\ref{sec:prompt_macro_concept}.

% A MODIFIER. This way, \method\ neither depends on predefined human-labeled concepts nor on a LLM such as GPT-4. Moreover, the set of concept candidates is generated given the training set, being less influenced than existing TCBM by the prior knowledge of the language model used to generate the concepts.

\subsection{Concept Scoring}
The objective  of the concept scoring step is to find $\mathcal{C^{*}} \subset \mathcal{C}$ to be included in the final CBL. \method\  assigns a fixed importance score to each concept throughout the iterative TCBM generation. \method\ implements 2 ways of computing score importance.

Firstly, based on the previously computed CAVs, we apply \texttt{TCAV} as in~\citet{tcav_abusive_language} on the last layer of $f$ to compute the fraction of inputs positively influenced by each concept summed over all target classes. This way, we know which concepts are most important for $f$ to make its predictions. Secondly, motivated by the fact that the most represented concepts in the training dataset are often among the most detectable ones in the latent representations of language models~\cite{scaling_monosemanticity}, \method\ also implements the concept \texttt{frequency} as a simple way of targeting concepts.

The concepts are then sorted in descending order of importance and iteratively added to the CBL.

\subsection{TCBM Training}
Given a subset of concepts $\text{C} \subset \mathcal{C}$, we introduce the protocol followed by \method\ to train a TCBM. To avoid classification leakage, we propose to guide the evolution of the TCBM training by adding a residual connection parallel to the CBL as done with CBM in computer vision~\cite{cbm_incremental}. Generating a \textit{simple} TCBM consists in training the two following layers: $\Phi^{\text{C}}: \mathbb{R}^d \rightarrow \mathbb{R}^{\lvert \text{C} \rvert}$ and $\Phi^{\text{cls}}: \mathbb{R}^{\lvert \text{C} \rvert} \rightarrow \mathcal{Y}$, where $\Phi^{\text{C}}$ is the layer detecting concepts from $f$ embedding and $\Phi^{\text{cls}}$ is the sparse linear concept-based classification layer. The \textit{simple} TCBM is then defined by $\Phi^{\text{cls}}~\circ~\Phi^{\text{C}}~\circ~f$. A \textit{residual} TCBM contains an additional non interpretable residual layer $\Phi^{r}: \mathbb{R}^d \rightarrow \mathcal{Y}$ using unknown residual concepts to enhance the downstream classification accuracy of the TCBM and avoid leakage. This way, the \textit{residual} TCBM is defined as $((\Phi^{\text{cls}}~\circ~\Phi^{\text{C}})+\Phi^{r})~\circ f$.

 % \method\ employs two methods for constructing phi $\Phi^{\text{C}}$: one based on concept embedding projection, and the other on supervised learning. These methods differ in terms of computational cost and concept detection accuracy.

% \paragraph{Concept Embedding Projection.} The \texttt{projection} approach to build  $\Phi^{\text{C}}$ consists in projecting the CAVs into the concept space. We formally define $\Phi^{\text{c}_{k}}(f(x)) = \frac{\langle f(x), \overrightarrow{\gamma_{k}}\rangle}{||f(x)||.||\overrightarrow{\gamma_{k}}||}$ as the linear projection of the embedding of $x$ from $f$ on the concept space associated to the concept $\text{c}_{k}$. This way, the concept embedding projection consists in computing the cosine similarity (à confirmer) between the CAV and $f$ output. $\Phi^{\text{C}}$ is then constructed by concatenating linear projections corresponding to each concept $\text{c}_{i}$ and the final layer. Finally, $\Phi^{\text{cls}}$ and $\Phi^{\text{r}}$ are trained to perform the classification by minimizing the following loss function:
% \begin{equation}
%  \label{eqn:loss_function}
% \mathcal{L}_{TCBM} = \mathcal{L}(\Phi^{\text{cls}}(\Phi^{\text{C}}(f(x)) + \Phi^{r}(f(x)), y)
% \end{equation}
% where $\mathcal{L}$ is the cross-entropy loss.

% \paragraph{Concept Detection Training.} 
% The \texttt{training} way of generating $\Phi^{\text{C}}$ consists in constructing the concept bottleneck layer through supervised learning. 

\method\ constructs $\Phi^{\text{C}}$ based on supervised learning,
%. The aim is to minimise 
minimising the following loss function: 
\begin{eqnarray}
\label{eqn:argmin2}
  \mathcal{L}_{TCBM} = \lambda \mathcal{L}(\Phi^{\text{C}}(f(x)), \texttt{c})
\end{eqnarray}
$$
  +  \mathcal{L}(\Phi^{\text{cls}}(\Phi^{\text{C}}(f(x)) + \Phi^{r}(f(x)), y)
$$
where $\mathcal{L}$ is the cross-entropy loss, $\lambda$ is a hyperparameter and $\texttt{c}$ is the vector of absence or presence of the concepts included in \text{C}. $\Phi^{\text{r}}$ is trained with a Ridge penalty constraint as in \citet{cbm_posthoc} and $\Phi^{\text{cls}}$ is trained with an elastic net penalty constraint to foster sparsity. The supervised training can be done \textit{jointly} (concept detection and downstream classification performed at the same time) or \textit{sequentially} (concept detection learned firstly and classification training performed afterwards). 

\method\ also implements a TCBM  building method based on the CAVs projection in the concept layer. We present this \texttt{projection} methodology and give more details about training strategies, Ridge and elastic net hyperparameters in Appendix~\ref{sec:appendix_tcbm_implementation_details}.

% While the \texttt{training} approach is computationally greedy as compared to the \texttt{projection} one, we show in SectionXXX that training leads to more accurate concept detection.



% We filter out concepts misclassified by the SVMs (accuracy higher than XX) during CAV computation in order to promote their correct detection during the construction of $\Phi^{\text{C}}$. 

% \paragraph{Dynamic Scoring.} VA CERTAINEMENT BEAUCOUP EVOLUER. The \texttt{dynamic} scoring approach updates importance scores at each iteration during the TCBM generation. This time, the concept importance is computed by applying \texttt{Integrated gradients} to the residual connection $\Phi^{\text{r}}$. Focusing on $\Phi^{\text{r}}$ relies on the idea that relevant concept to be added in the concept bottleneck layer can be inferred by explaining $\Phi^{\text{r}}$ activity. At each TCBM training step, we sample XX\% of the training set and apply \texttt{Integrated gradients} to explain $\Phi^{\text{r}}$ output. For each sampled text $x_{i}$, we retrieve the most important word $w_{i}$ and assign it to a macro cluster by applying $g$. For a given concept $c_{k}$, we define the concept importance during one iteration as $\text{Imp}(c_{i}) = \sum_{x_{k}\in\widetilde{\mathcal{X}}}\mathbbm{1}_{g(w_{k})=c_{i}}$. At each iteration, the concept with the highest score is added to the concept bottleneck layer, and the scores related to the other concepts are kept in memory to be updated at the next iteration. 

\subsection{Stopping Criterion}
\method\ stops adding concepts in the CBL when the concept basis is deemed complete. The residual connection is then removed in order to obtain a TCBM without non-interpretable layer.

Since the residual layer uses unknown residual concepts, we measure its importance in the classification decision process as a proxy of concept completeness. A low residual connection importance  indicates that most of the necessary concepts have been added to the CBL, pinpointing a complete concept base. Thus, we define the stopping criterion based on the importance of $\Phi^{r}$ in the TCBM decision making process. For a given input text $x \in \mathcal{X}$ and a given target class $k \in \mathcal{Y}$, we formally define the importance of $\Phi^{r}$ as follows: 
\begin{equation}
 \label{eqn:distance}
\texttt{I}^{k}_{r}(x) = \frac{\lvert \langle w_{k}, f(x)\rangle \rvert} {\lvert \langle w_{k}, f(x)\rangle \rvert + \langle \lvert a_{k}  \rvert , \lvert \Phi^{\text{C}}(f(x)) \rvert\rangle }
\end{equation}
% \texttt{I}^{k}_{r}(x) = \frac{\lvert \sum_{i=1}^d  w_{i,k}f_{i}(x) \rvert}{\lvert \sum_{i=1}^d  w_{i,k}f_{i}(x) \rvert+\sum_{i=1}^{\lvert \text{C} \rvert} \lvert a_{i,k}\Phi_{i}^{\text{C}}(f(x)) \rvert}
where $w_{k}$ and $a_{k}$ are respectively the weights of $\Phi^{r}$ and $\Phi^{\text{cls}}$ associated to class $k$: %. This way, 
$\texttt{I}^{k}_{r}(x)$ measures the importance of the residual connection relative to the sum of the importance of each concept in the decision process related to class $k$.   We finally compute the global importance of $\Phi^{r}$ 
denoted as $\texttt{I}_{r}$ by averaging $\texttt{I}^{k}_{r}(x)$ over all  target classes~$k$ and all inputs~$x$.  
%over the training set. 

\method\ training stops when the moving average of $\texttt{I}_{r}$ of order 4 over iterations stops decreasing. We then select the iteration that minimizes the importance of $\Phi^{r}$ on the training set. The residual layer $\Phi^{r}$ is finally removed to end with a fully interpretable TCBM. We show an example of the evolution of the residual connection importance over the \method\ training in Appendix~\ref{sec:appendix_examples}


% \paragraph{TCBM Training at One Iteration.} At each iteration, the 3 following layers are trained: $\Phi^{\mathcal{C}'}: \mathbb{R}^d \rightarrow \mathbb{R}^{\lvert \mathcal{C}' \rvert}$, $\Phi^{r}: \mathbb{R}^d \rightarrow \mathcal{Y}$ and $\Phi^{\text{cls}}: \mathbb{R}^{\lvert \mathcal{C}' \rvert} \rightarrow \mathcal{Y}$, where $\Phi^{\mathcal{C}'}$ is the layer extracting concept from $f$ embedding, $\mathcal{C}'$ is the set of concepts to be added in the concept bottleneck layer at one iteration, $\Phi^{r}: \mathbb{R}^d \rightarrow \mathcal{Y}$ is the residual classification layer using unknown residual concepts and $\Phi^{\text{cls}}$ is the linear concept-based classification layer. Thus, the TCBM without residual layer is defined by $\Phi^{\text{cls}} \circ \Phi^{\mathcal{C}'} \circ f$, whereas the CBM with additional residual layer is defined by $(\Phi^{\text{cls}} \circ \Phi^{\mathcal{C}'}+\Phi^{r})\circ f$

% We propose to ways of constructing the concept bottleneck layer, either based on concept embedding projection or common supervised learning. Concept embedding projection consists in projecting concept linear representation from the embedding space called CAVs (Concept Activation Vectors) into the concept space. To compute a CAV, we follow~\citet{tcav} and train a linear SVM to detect the presence of a concept in the embedding space and retrieve the vector normal to the linear classification boundary. Given an input text $x$ and by denoting $\epsilon(c_{i}) \in \mathbb{R}^{d}$ as the CAV related to the concept $c_{i}$, we formally define $\Phi^{c_{i}}(f(x)) = \frac{\langle f(x), \epsilon(c_{i})\rangle}{||f(x)||.||\epsilon(c_{i})||}$ as the linear projection of the embedding of $x$ from $f$ on the concept space associated to the concept $c_{i}$. This way, the concept embedding projection consists in computing the cosine similarity (à confirmer) between the CAV of a concept and the $f$ output. $\Phi^{\mathcal{C}'}$ is then constructed by concatenating linear projections corresponding to each concept $c_{i}$ and the final layer. Finally, $\Phi^{\text{cls}}$ and $\Phi^{\text{r}}$ are trained to perform the classification by minimizing the following loss function:
% \begin{equation}
%  \label{eqn:loss_function}
% \mathcal{L}_{CBM} = \mathcal{L}(\Phi^{\text{cls}}(\Phi^{\mathcal{C}'}(f(x)) + \Phi^{r}(f(x)), y)
% \end{equation}
% where $\mathcal{L}$ is the cross-entropy loss and $c' \in  \left\{ 0,1\right\}^{p'}$ the vector containing the set of $p'$ wanted concepts in the concept bottleneck layer during the iteration. 

% We implement another way of generating $\Phi^{\mathcal{C}'}$ through supervised learning where the concept bottleneck layer is also trained. It consists in minimizing the following loss function: 
% \begin{eqnarray}
% \label{eqn:argmin2}
%   \mathcal{L}_{CBM} = \mathcal{L}(\Phi^{\mathcal{C}'}(f(x)), c')
% \end{eqnarray}
% $$
%   + \gamma \mathcal{L}(\Phi^{\text{cls}}(\Phi^{\mathcal{C}'}(f(x)) + \Phi^{r}(f(x)), y)
% $$
% % \begin{equation}
% %  \label{eqn:loss_function}
% % \mathcal{L}_{CBM} = \mathcal{L}(\Phi^{\mathcal{C}'}(f(x)), c') + \gamma \mathcal{L}(\Phi^{\text{cls}}(\Phi^{\mathcal{C}'}(f(x))), y)
% % \end{equation}
% The supervised training can be done \textit{jointly} or \textit{sequentially}, we give more information about training strategies in AppendixXXX.

% While the \textit{training} approach is computationally greedy as compared to the \textit{projection} one, we show in SectionXXX that it leads to a better concept detection performance.
\begin{table*}[]
% \footnotesize
\scriptsize
\begin{center}
\begin{tabular}{cc|cccccc|ccccc}
\hline
\multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Model backbone \\ (size)\end{tabular}}}                                                                                     & \multicolumn{6}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}\texttt{BERT-base} \\ (110M)\end{tabular}}}                                                                                                           & \multicolumn{5}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}\texttt{DeBERTa-large} \\ (395M)\end{tabular}}}                                                                                  \\ \hline
\multicolumn{1}{c|}{\textbf{Dataset}}                                                                      & \textbf{Method}                                                       & \texttt{Black-box} & \multicolumn{2}{c}{\texttt{C\textsuperscript{3}M}}                            & \texttt{CB-LLM} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\texttt{CT-CBM}\\ (ours)\end{tabular}} & \texttt{Black-box} & \multicolumn{2}{c}{\texttt{C\textsuperscript{3}M}}                         & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{CT-CBM}\\ (ours)\end{tabular}} \\ \cline{2-13} 
\multicolumn{1}{c|}{}                                                                                      & \textbf{\begin{tabular}[c]{@{}c@{}}Concept\\ Annotation\end{tabular}} &                    & -                                     & \tiny \texttt{CT-CBM}                 &                 & -                                         & \tiny \texttt{C\textsuperscript{3}M}      &                    & -                                     & \tiny \texttt{CT-CBM}              & -                                         & \tiny \texttt{C\textsuperscript{3}M}     \\ \hline
\multicolumn{1}{c|}{}                                                                                      & \%ACC \tiny $\uparrow$                                                & 91.0               & \cellcolor[HTML]{EFEFEF}\textbf{91.5} & \cellcolor[HTML]{EFEFEF}{\ul 91.1}    & 90.0            & \cellcolor[HTML]{EFEFEF}90.6              & \cellcolor[HTML]{EFEFEF}{\ul 91.1}        & 92.0               & \cellcolor[HTML]{EFEFEF}{\ul 91.8}    & \cellcolor[HTML]{EFEFEF}91.2       & \cellcolor[HTML]{EFEFEF}91.4              & \cellcolor[HTML]{EFEFEF}\textbf{92.5}    \\
\multicolumn{1}{c|}{}                                                                                      & \%c \tiny $\uparrow$                                                  & -                  & \cellcolor[HTML]{EFEFEF}\textbf{76.2} & \cellcolor[HTML]{EFEFEF}54.8          & 56.0            & \cellcolor[HTML]{EFEFEF}62.8              & \cellcolor[HTML]{EFEFEF}{\ul 70.2}        & -                  & \cellcolor[HTML]{EFEFEF}{\ul 81.5}    & \cellcolor[HTML]{EFEFEF}55.0       & \cellcolor[HTML]{EFEFEF}58.9              & \cellcolor[HTML]{EFEFEF}\textbf{84.5}    \\
\multicolumn{1}{c|}{}                                                                                      & \#c \tiny$\downarrow$                                                 & -                  & \cellcolor[HTML]{EFEFEF}41            & \cellcolor[HTML]{EFEFEF}100           & 41              & \cellcolor[HTML]{EFEFEF}\textbf{11}       & \cellcolor[HTML]{EFEFEF}{\ul 12}          & -                  & \cellcolor[HTML]{EFEFEF}41            & \cellcolor[HTML]{EFEFEF}100        & \cellcolor[HTML]{EFEFEF}\textbf{12}       & \cellcolor[HTML]{EFEFEF}{\ul 16}         \\
\multicolumn{1}{c|}{\multirow{-4}{*}{\textbf{AG News}}}                                                    & \%D \tiny $\uparrow$                                                  & \textbf{-}         & \cellcolor[HTML]{EFEFEF}76.5          & \cellcolor[HTML]{EFEFEF}78.5          & 76.5            & \cellcolor[HTML]{EFEFEF}\textbf{80.8}     & \cellcolor[HTML]{EFEFEF}{\ul 81.2}        & \textbf{-}         & \cellcolor[HTML]{EFEFEF}76.5          & \cellcolor[HTML]{EFEFEF}78.5       & \cellcolor[HTML]{EFEFEF}\textbf{87.2}     & \cellcolor[HTML]{EFEFEF}{\ul 85.1}       \\ \hline
\multicolumn{1}{c|}{}                                                                                      & \%ACC \tiny $\uparrow$                                                & 99.4               & \cellcolor[HTML]{EFEFEF}\textbf{99.5} & \cellcolor[HTML]{EFEFEF}\textbf{99.5} & 99.3            & \cellcolor[HTML]{EFEFEF}99.3              & \cellcolor[HTML]{EFEFEF}{\ul 99.4}        & 99.4               & \cellcolor[HTML]{EFEFEF}\textbf{99.5} & \cellcolor[HTML]{EFEFEF}99.4       & \cellcolor[HTML]{EFEFEF}99.2              & \cellcolor[HTML]{EFEFEF}\textbf{99.5}    \\
\multicolumn{1}{c|}{}                                                                                      & \%c \tiny $\uparrow$                                                  & -                  & \cellcolor[HTML]{EFEFEF}56.1          & \cellcolor[HTML]{EFEFEF}{\ul 59.0}    & 56.0            & \cellcolor[HTML]{EFEFEF}\textbf{60.8}     & \cellcolor[HTML]{EFEFEF}52.9              & -                  & \cellcolor[HTML]{EFEFEF}{\ul 68.1}    & \cellcolor[HTML]{EFEFEF}62.2       & \cellcolor[HTML]{EFEFEF}64.0              & \cellcolor[HTML]{EFEFEF}\textbf{70.5}    \\
\multicolumn{1}{c|}{}                                                                                      & \#c \tiny $\downarrow$                                                & -                  & \cellcolor[HTML]{EFEFEF}63            & \cellcolor[HTML]{EFEFEF}100           & 63              & \cellcolor[HTML]{EFEFEF}\textbf{10}       & \cellcolor[HTML]{EFEFEF}{\ul 16}          & -                  & \cellcolor[HTML]{EFEFEF}63            & \cellcolor[HTML]{EFEFEF}100        & \cellcolor[HTML]{EFEFEF}\textbf{9}        & \cellcolor[HTML]{EFEFEF}{\ul 12}         \\
\multicolumn{1}{c|}{\multirow{-4}{*}{\textbf{DBpedia}}}                                                    & \%D \tiny $\uparrow$                                                  & \textbf{-}         & \cellcolor[HTML]{EFEFEF}82.2          & \cellcolor[HTML]{EFEFEF}80.3          & 82.2            & \cellcolor[HTML]{EFEFEF}\textbf{85.1}     & \cellcolor[HTML]{EFEFEF}{\ul 84.5}        & \textbf{-}         & \cellcolor[HTML]{EFEFEF}82.2          & \cellcolor[HTML]{EFEFEF}80.3       & \cellcolor[HTML]{EFEFEF}\textbf{85.0}     & \cellcolor[HTML]{EFEFEF}{\ul 83.5}       \\ \hline
\multicolumn{1}{c|}{}                                                                                      & \%ACC \tiny $\uparrow$                                                & 62.7               & \cellcolor[HTML]{EFEFEF}\textbf{61.4} & \cellcolor[HTML]{EFEFEF}{\ul 60.5}    & 57.9            & \cellcolor[HTML]{EFEFEF}57.5              & \cellcolor[HTML]{EFEFEF}59.2              & 62.6               & \cellcolor[HTML]{EFEFEF}64.7          & \cellcolor[HTML]{EFEFEF}62.6       & \cellcolor[HTML]{EFEFEF}58.6              & \cellcolor[HTML]{EFEFEF}60.1             \\
\multicolumn{1}{c|}{}                                                                                      & \%c \tiny $\uparrow$                                                  & -                  & \cellcolor[HTML]{EFEFEF}50.6          & \cellcolor[HTML]{EFEFEF}50.2          & 25.2            & \cellcolor[HTML]{EFEFEF}{\ul 51.1}        & \cellcolor[HTML]{EFEFEF}\textbf{54.0}     & -                  & \cellcolor[HTML]{EFEFEF}54.7          & \cellcolor[HTML]{EFEFEF}51.6       & \cellcolor[HTML]{EFEFEF}{\ul 58.5}        & \cellcolor[HTML]{EFEFEF}\textbf{61.2}    \\
\multicolumn{1}{c|}{}                                                                                      & \#c \tiny $\downarrow$                                                & -                  & \cellcolor[HTML]{EFEFEF}57            & \cellcolor[HTML]{EFEFEF}100           & 57              & \cellcolor[HTML]{EFEFEF}{\ul 13}          & \cellcolor[HTML]{EFEFEF}\textbf{10}       & -                  & \cellcolor[HTML]{EFEFEF}57            & \cellcolor[HTML]{EFEFEF}100        & \cellcolor[HTML]{EFEFEF}{\ul 12}          & \cellcolor[HTML]{EFEFEF}\textbf{9}       \\
\multicolumn{1}{c|}{\multirow{-4}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Medical\\ Abstract\end{tabular}}}} & \%D \tiny $\uparrow$                                                  & \textbf{-}         & \cellcolor[HTML]{EFEFEF}{\ul 77.2}    & \cellcolor[HTML]{EFEFEF}76.4          & {\ul 77.2}      & \cellcolor[HTML]{EFEFEF}\textbf{79.2}     & \cellcolor[HTML]{EFEFEF}77.1              & \textbf{-}         & \cellcolor[HTML]{EFEFEF}\textbf{77.2} & \cellcolor[HTML]{EFEFEF}{\ul 76.4} & \cellcolor[HTML]{EFEFEF}77.1              & \cellcolor[HTML]{EFEFEF}76.9             \\ \hline
\multicolumn{1}{c|}{}                                                                                      & \%ACC \tiny $\uparrow$                                                & 91.7               & \cellcolor[HTML]{EFEFEF}{\ul 92.0}    & \cellcolor[HTML]{EFEFEF}\textbf{92.6} & 91.4            & \cellcolor[HTML]{EFEFEF}90.6              & \cellcolor[HTML]{EFEFEF}91.4              & 93.8               & \cellcolor[HTML]{EFEFEF}93.3          & \cellcolor[HTML]{EFEFEF}92.7       & \cellcolor[HTML]{EFEFEF}92.2              & \cellcolor[HTML]{EFEFEF}91.9             \\
\multicolumn{1}{c|}{}                                                                                      & \%c \tiny $\uparrow$                                                  & -                  & \cellcolor[HTML]{EFEFEF}{\ul 70.4}    & \cellcolor[HTML]{EFEFEF}45.3          & 29.8            & \cellcolor[HTML]{EFEFEF}52.5              & \cellcolor[HTML]{EFEFEF}\textbf{72.0}     & -                  & \cellcolor[HTML]{EFEFEF}\textbf{77.3} & \cellcolor[HTML]{EFEFEF}51.5       & \cellcolor[HTML]{EFEFEF}55.2              & \cellcolor[HTML]{EFEFEF}{\ul 62.1}       \\
\multicolumn{1}{c|}{}                                                                                      & \#c \tiny $\downarrow$                                                & -                  & \cellcolor[HTML]{EFEFEF}68            & \cellcolor[HTML]{EFEFEF}100           & 68              & \cellcolor[HTML]{EFEFEF}\textbf{11}       & \cellcolor[HTML]{EFEFEF}{\ul 12}          & -                  & \cellcolor[HTML]{EFEFEF}68            & \cellcolor[HTML]{EFEFEF}100        & \cellcolor[HTML]{EFEFEF}\textbf{10}       & \cellcolor[HTML]{EFEFEF}{\ul 14}         \\
\multicolumn{1}{c|}{\multirow{-4}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Movie\\ Genre\end{tabular}}}}      & \%D \tiny $\uparrow$                                                  & \textbf{-}         & \cellcolor[HTML]{EFEFEF}78.1          & \cellcolor[HTML]{EFEFEF}78.8          & 78.1            & \cellcolor[HTML]{EFEFEF}\textbf{81.6}     & \cellcolor[HTML]{EFEFEF}{\ul 80.2}        & \textbf{-}         & \cellcolor[HTML]{EFEFEF}78.1          & \cellcolor[HTML]{EFEFEF}78.8       & \cellcolor[HTML]{EFEFEF}\textbf{81.3}     & \cellcolor[HTML]{EFEFEF}{\ul 80.5}       \\ \hline
\end{tabular}
\caption{\label{tab:results} 
\method\ and competitors evaluation on four test sets and two NLP classifiers. Except the black box baseline, the best results are highlighted in bold and the second best ones are underlined.}
\end{center}
\end{table*}

\section{Experimental Settings}
\label{xp}

This section presents the experimental study conducted across four datasets and two NLP classifiers of different sizes, first comparing
%. We start by comparing 
\method\ to several competitors. Next, we assess the impact of the method used to target important concepts. Then we illustrate several TCBM applications, such as the better understanding of counterfactual explanations and adversarial attacks~\cite{XAI_nlp_survey}. Finally, we show how global explanations can be derived from a TCBM.


\subsection{Experimental Protocol}

\paragraph{Datasets and models.} \method\ is tested on four  multi-class text classification datasets: AG News~\cite{AG_news}, DBpedia~~\cite{dbpedia}, Movie Genre\footnote{\url{https://www.kaggle.com/competitions/movie-genre-classification/overview}} and the more challenging classification dataset Medical Abstracts~\cite{health_dataset_nlp}. We apply \method\ on two fine-tuned NLP classifiers of different sizes: \texttt{BERT}~\cite{devlin_bert_2019} and  \texttt{DeBERTa-large}~\cite{hedeberta}.  More information about the content of the classification datasets and the language models are provided in Appendix~\ref{sec:appendix_classifiers_dataset_details}. 


\paragraph{\method\ and Competitors.}
We run the proposed \method\ with a \texttt{Gemma-2-9B} SLM to generate concept candidates. The CBL is constructed by \textit{joint} training and important concepts are targeted and added in the bottleneck layer with \texttt{TCAV} and \texttt{frequency}. We compare \method\ to both \texttt{C\textsuperscript{3}M}~\cite{cbm_plm} and \texttt{CB-LLM}~\cite{cb_llm}. Given that \texttt{TBM}~\cite{cbm_by_design} does not enhance an NLP classifier but rather performs concept detection with GPT4 during inference, we do not include it in the comparative study. We only show the results of \texttt{CB-LLM} for BERT, as we were unable to make TCBM converge for DeBERTa, despite using a grid search on the hyperparameters.
%compare ourselves to it. 
We also run \method\ based on the concept annotation from \texttt{C\textsuperscript{3}M} and, conversely, we utilize \texttt{C\textsuperscript{3}M} based on \method\ concept annotation. To ensure comparability and address the ChatGPT annotation non scalability of \texttt{C\textsuperscript{3}M} (complexity proportional to size of the dataset $\times$ number of targeted concepts), we run \texttt{C\textsuperscript{3}M} with \texttt{Gemma-2-9B} as concept annotator. \texttt{CB-LLM} concept evaluation is done by discretizing its concept prediction since concept detection learning is done with numeric values. The hyperparameters of the experiments are detailed in Appendices~\ref{sec:tcbm_training} and~\ref{sec:appendix_competitors_implementation_details}.

\paragraph{Evaluation Criteria.}
We propose a 4-metric evaluation, with the first metric being the final classification task accuracy (\textbf{\%ACC}). We evaluate concept detection accuracy (\textbf{\%c}) based on the F1 score related to concept detection due to the strong imbalance in concept labels.  
%in the test set. 
Given the prohibitive computational cost of \texttt{C\textsuperscript{3}M} despite the use of \texttt{Gemma-2-9B} instead of ChatGPT for concept annotation, the \texttt{C\textsuperscript{3}M} evaluation of concept detection accuracy is only carried out on 1000 texts. We also report the number of concepts (\textbf{\#c}) in the CBL and the concept diversity (\textbf{D}) in the TCBM. 
The latter  
%Evaluating diversity 
is motivated by the extensive XAI literature emphasising the significance of diversity in the components of an explanation~\cite{laugel2023achieving, mothilal_explaining_2020}. Moreover, greater diversity in the case of TCBM tends to reduce concept redundancy, therefore promoting the objective of a more complete concept base. We formally measure diversity as $D = 1 - \frac{2}{k(k-1)} \sum_{i=1}^k \sum_{j=i+1}^k \frac{\langle \mathbf{e_i} \cdot \mathbf{e_j} \rangle}{\|\mathbf{e_i}\| \|\mathbf{e_j}\|}$ where $\mathbf{e_i}$ is the embedding representation from a sentence embedding model of the textual label $l_{k}$ of concept~$c_{k}$.





\subsection{Results}
\label{result}

\paragraph{Global Results.} Table~\ref{tab:results} shows the experimental results obtained from \method\ and its competitors on \texttt{BERT} and \texttt{DeBERTa} and the same training sets. \texttt{C\textsuperscript{3}M} with \texttt{CT-CBM} concept annotation consists in training the TCBM with the  \texttt{C\textsuperscript{3}M} training methodology based on the dataset enriched with concepts dervied from the \texttt{CT-CBM} methodology. Conversely, \texttt{CT-CBM} with \texttt{C\textsuperscript{3}M} concept annotation applies the \texttt{CT-CBM} training approach subsequent to the generation of concepts following \texttt{C\textsuperscript{3}M}. 

Overall, \method\ achieves a performance very similar to that of the original black box models, even surpassing it for AG news and DBpedia when the concept annotation was previously done with \texttt{C\textsuperscript{3}M}. For the majority of datasets and models, \method\ achieves the best results in terms of both quantity and diversity of concepts in the CBL. The accuracy of the classification task is comparable between \method\, \texttt{C\textsuperscript{3}M} and \texttt{CB-LLM}, except for the Medical Abstract dataset, where our approach performs marginally lower.

\begin{table}[t]
% \footnotesize
\scriptsize
\begin{center}
\begin{tabular}{c|c|cccc}
\hline
Metric                                  & \begin{tabular}[c]{@{}c@{}}Concept\\ scoring\end{tabular} & \begin{tabular}[c]{@{}c@{}}AG \\ News\end{tabular} & DBPedia       & \begin{tabular}[c]{@{}c@{}}Medical\\ Abstract\end{tabular} & \begin{tabular}[c]{@{}c@{}}Movie \\ Genre\end{tabular} \\ \hline
\multirow{2}{*}{\%ACC \tiny $\uparrow$} & \tiny \texttt{TCAV}                                       & \textbf{90.6}                                      & 99.3          & \textbf{57.5}                                              & 90.6                                                   \\
                                        & \texttt{frequency}                                        & 88.5                                               & \textbf{99.4} & 57.6                                                       & \textbf{91.7}                                          \\ \hline
\multirow{2}{*}{\%c \tiny $\uparrow$}   & \tiny \texttt{TCAV}                                       & 62.8                                               & 60.8          & \textbf{52.5}                                              & \textbf{52.5}                                          \\
                                        & \texttt{frequency}                                        & \textbf{63.3}                                      & \textbf{61.0} & 49.7                                                       & 51.2                                                   \\ \hline
\multirow{2}{*}{\#c \tiny$\downarrow$}  & \tiny \texttt{TCAV}                                       & \textbf{11}                                        & \textbf{10}   & \textbf{13}                                                & \textbf{11}                                            \\
                                        & \texttt{frequency}                                        & 19                                                 & 13            & 14                                                         & 19                                                     \\ \hline
\end{tabular}
\caption{\label{tab:concet_score} 
Downstream task accuracy, concept accuracy and number of concepts of \method\ applied to \texttt{BERT} per concept scoring method.}
\end{center}
\end{table}

\paragraph{Influence of Concept Annotation on Concept Detection Accuracy.}
Concept detection accuracies of \method\ and \texttt{C\textsuperscript{3}M} are highly dependent on the method used to do concept annotation upstream. On average, the \texttt{C\textsuperscript{3}M} annotation leads to higher concept accuracy as compared to the \method\ annotation. Annotation with \texttt{C\textsuperscript{3}M} is extremely costly, with a complexity equal to the product of the number of concepts in the database and the size of the dataset, whereas annotation with \method\ has a complexity proportional to the size of the dataset of interest. For a given annotation method, our \method\ achieves on average a higher concept accuracy than \texttt{C\textsuperscript{3}M}. \texttt{CB-LLM} has overall a lower concept accuracy than its competitors.

Consequently, \method\ appears to offer a balancd compromise between downstream task accuracy, concept detection performance and concept diversity, while structurally avoiding classification leakage in a fully unsupervised manner. However, a heavier but higher-quality annotation method such as \texttt{C\textsuperscript{3}M} does improve results, at the expense of scalability. 

\paragraph{Impact of the Concept Scoring Method.}
Table~\ref{tab:concet_score} shows the results of the evaluation of the impact of the concept scoring method of \method\ on its performance. It turns out that \texttt{TCAV} and the \texttt{frequency} approach give similar results in terms of downstream task and concept detection accuracy. However, \texttt{TCAV} converges much faster than \texttt{frequency} with less concepts in its CBL, which underlines the benefits of using TCAV to target important concepts to be added to the CBL. However, the \texttt{frequency}-based targeting concepts remains a noteworthy compromise in terms of its implementation simplicity and computational cost.




\begin{figure}[t]{\centering}
\begin{center}
\includegraphics[scale=0.35]{image/adv_attack_cf.png}
\caption{Example of an adversarial attack ($x_{adv}$) and a counterfactual explanation ($x_{cf}$) obtained from \method\ and the AGnews dataset. TCBM enable to understand the label change by explaining it in terms of concept change.}
\label{fig:cf_detox}
\end{center}
\end{figure} 

\begin{figure*}[t]{\centering}
\begin{center}
\includegraphics[scale=0.28]{image/sankey_x_c_y.png}
\caption{Global explanation of a TCBM trained on the AGnews dataset with \method. The 7 most important concepts of the bottleneck layer and the 8 most important tokens to explain each concept are shown. }  
\label{fig:sankey_t_c_y}
\end{center}
\end{figure*}

\subsection{Practical Applications of TCBM}
\paragraph{Better Understanding Adversarial Attacks and Counterfactual Explanations.}
A common application of CBMs is to allow domain experts to modify predicted concepts at test time to improve the accuracy of final task predictions~\cite{cbm_learning_intervene}. We propose here another application of TCBM by showing how TCBM can provide a better understanding of adversarial attacks effectiveness and counterfactual explanation expressivity. To this end, we apply \texttt{TextAttack}~\cite{morris2020textattack} and use \texttt{Claude 3.5 Sonnet} to generate adversarial attacks and counterfactual explanations to a TCBM trained via the proposed \method\ on the AGnews dataset, thereby achieveing a switch in the outcome of its prediction. 

Figure~\ref{fig:cf_detox} gives a salient example of how TCBM can provide an understanding of adversarial attacks and counterfactual explanations at the concept level. The adversarial attack succeeds in flipping the label of the TCBM from \textit{Business} to \textit{Sport} by performing the following change at token level: \textbf{{stock}} $\rightarrow$ \textbf{{man}}. The change in concepts \textbf{{Financial terms related to money}} $\rightarrow$ \textbf{{Acronyms/initials}} clearly highlights the model's poor understanding of the token change induced by the adversarial attack, offering avenues for troubleshooting the model. In the same way, the label flipping from \textit{Business} to \textit{Sci/Tech} induced by the counterfactual token changes \textbf{{Pfizer}} $\rightarrow$ \textbf{{NVIDIA}} and \textbf{{Celebrex}} $\rightarrow$ \textbf{{AI}} can be understood at a higher level of abstraction than the token level with the \textbf{{Financial terms related to money}} $\rightarrow$ \textbf{{Security and identification methods}}. This way, we believe that focusing on the conceptual level can significantly improve the understanding of a counterfactual explanation.



\paragraph{Global TCBM Interpretability.} We illustrate how to interpret TCBM at a global scale by representing the relationship between tokens, concepts and target classes. Regarding the relationship between important tokens and concepts, we propose to apply an attribution method, here \texttt{Integrated gradients}~\cite{sundararajan_axiomatic_2017} to explain TCBM concept activations for each concept at a global scale. The resulting local feature importance scores are then averaged by concept. Finally, the weights of the~$\Phi^{\text{cls}}$ layers are directly used to represent the intensity of the relationship between  concepts and target labels. Figure~\ref{fig:sankey_t_c_y} gives an example of a global explanation of the proposed \method\ trained on DBPedia. The "Financial terms related to money" concept is important to predict that a press article is related to "Science and Technology". Moreover, "internet" is identified as important to activate the "Financial terms related to money" concept.

\section{Conclusion}
\label{conclusion}
We introduced \method, a novel unsupervised approach to transform a fine-tuned NLP classifier into a TCBM. \method\ automatically generates, scores and targets concepts to build a complete CBL. \method\ demonstrates a good level of performance as compared to its competitors for the two involved classification tasks, concept detection and class prediction, especially regarding the former. Moreover, \method\ leads to a more diverse concept basis, reducing the risk of redundant explanations at concept level. We have highlighted several advantages of TCBM, such as increased adversarial attacks and counterfactual intelligibility and the ability to produce global explanations. 

\section{Limitations}
\paragraph{Datasets and models.}
This work tested \method\  on 4 datasets and 2 language models. It would be interesting to include other models in the study, such as recent decoder-only architectures e.g. \texttt{Gemma-2B}.

\paragraph{Concept Interactions.}
We have not considered possible relationships between concepts. This could highlight a better understanding of the impact of concepts on the classes to be predicted. We see this as a promising way of improving our approach.

\paragraph{Concept Importance.}
There are other approaches for assessing the importance of a concept in explaining the behavior of a model~\cite{fel2023holistic,concept_activation_region}, especially when concepts do not necessarily appear to be represented linearly in the latent spaces of models. Using these approaches would enable \method\ to better target important concepts to be added to the CBL.

\paragraph{Text generation.}
Recent work has proposed having generative models generate explanations before answering the question in the same way as TCBM~\cite{bhan-etal-2024-self, cb_llm}. For the time being, our work has focused on text classification.

\label{limitations}

\section*{Ethics Considerations}
Since NLP training data can be biased, there is a risk of generating harmful concepts to be added in the CBL. One using \method\ to enhance a NLP classifier must be aware of these biases in order to stand back and analyze the produced concepts and the manipulated texts. Moreover, the use of \texttt{Gemma-9B} for concept annotation is computationally costly and consumes energy, potentially emitting greenhouse gases. \method\ must be  used with caution.


% Entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}
\bibliography{compit}
\bibliographystyle{acl_natbib}
\appendix
\section{Appendix}
\label{sec:appendix}
\subsection{Scientific Libraries}
We used several open-source libraries in this work: pytorch~\cite{paszke2019pytorch}, HuggingFace transformers~\cite{wolf2020transformers} sklearn~\cite{pedregosa2011scikit} and Captum~\cite{miglani_using_2023}. 

\subsection{Autoregressive language models implementation details}
\label{sec:appendix_slm_implementation_details}
\paragraph{Language Models.} The library used to import the pretrained autoregressive language models is Hugging-Face. In particular, the backbone version of Gemma-2-9B is \texttt{gemma-2-9B-it}.


\paragraph{Gemma-2 instruction special tokens.}
The special tokens to use Gemma in instruction mode were the following:
\begin{itemize}
    \item \texttt{Gemma-2}: \begin{itemize}
        \item \texttt{user\_token= '<start\_of\_turn>user'}
        \item \texttt{assistant\_token= '<start\_of\_turn>model'}
        \item \texttt{stop\_token='<eos>'}
    \end{itemize}
\end{itemize}
\paragraph{Text generation.}
Text generation was performed using the native functions of the Hugging Face library: \texttt{generate}. The \texttt{generate} function has been used with the following parameters:
\begin{itemize}
    \item \texttt{max\_new\_tokens = 50}
    \item \texttt{do\_sample = True}
    \item \texttt{num\_beams = 2}
    \item \texttt{no\_repeat\_ngram\_size = 2}
    \item \texttt{early\_stopping = True}
    \item \texttt{temperature = 1}
\end{itemize}


\subsection{Prompting format}
\label{sec:appendix_prompt}
Here we provide some details of different prompts used to give instructions to Gemma-2-9B for micro concept annotation and macro concept labeling. We mainly leverage the In-context Learning (ICL)~\cite{dong_survey_2023}  capabilities of Gemma-2-9B. 
\\

\subsubsection{Preprompt for micro concept generation}
\label{sec:prompt_micro_concept}
\textbf{user}\\
\textit{You are presented with several parts of speech.
        Identify only the main topics in this text. Respond with topic in list format like the examples in a very concise way using as few words as possible. Example: 'As cities expand and populations grow, there is a growing tension between development and the need to preserve historical landmarks. Citizens and authorities often clash over the balance between progress and cultural heritage.'}
\\
\textbf{assistant} \\
\textit{Topics: ['urban development', 'cultural heritage', 'conflict']<eos>}
\\
\textbf{user} \\
\textit{'Recent breakthroughs in neuroscience are shedding light on the complexities of human cognition. Researchers are particularly excited about the potential to better understand decision-making processes and emotional regulation in the brain.'} \\
\textbf{assistant} \\
\textit{Topics: ['neuroscience', 'human cognition', 'decision-making', 'emotional regulation']<eos>}




\subsubsection{Preprompt for macro concept labeling}
\label{sec:prompt_macro_concept}
\textbf{user}\\
\textit{You are presented with several parts of speech.
        Summarise what these parts of speech have in common in a very concise way using as few words as possible. Example: ["piano", "guitar", "saxophone", "violin", "cheyenne", "drum"]}
\\
\textbf{assistant} \\
\textit{Summarization: 'musical instrument'<eos>}
\\
\textbf{user} \\
\textit{["football", "basketball", "baseball", "tennis", "badmington", "soccer"]} \\
\textbf{assistant} \\
\textit{Summarization: 'sport'<eos>} \\
\textbf{user} \\
\textit{["lion", "tiger", "cat", "pumas", "panther", "leopard"]}\\
\textbf{assistant} \\
\textit{Summarization: 'feline-type animal'<eos>}


% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\linewidth]{image/generated_text_example.png}
% \caption{ARC Challenge answers conditioned by different ICL prompt built from different rationale generators.}
% \label{fig:text_example}
% \end{figure*}

\subsection{TCBM implementation details}
\label{sec:appendix_tcbm_implementation_details}

\subsubsection{Micro concept clustering settings}
\label{sec:micro_concept_clustering}
In order to perform micro concept clustering to build macro concepts, we use the \texttt{umap} library to perform dimension reduction with UMAP with \texttt{n\_components} = 5. Text embeddings are initially obtained with the \texttt{all-mpnet-base-v2} backbone from the \texttt{sentence\_transformers} library. Finally, clustering is performed with \texttt{HDBSCAN} with the basic settings from the \texttt{hdbscan} library.

\subsubsection{TCBM training strategies}
\label{sec:joint_sequential}
\method\ implements two strategies for TCBM training: \textit{joint} and \textit{sequential}. The \textit{sequential} strategy first predicts concepts from input texts and then uses these predicted concepts to make the final target prediction. In this approach, the output of the concept prediction stage is directly used as input for the target prediction stage. This way, the concept loss $\mathcal{L}(\Phi^{\text{C}}(f(x)), \texttt{c})$ is firstly minimized before minimizing the target one $\mathcal{L}(\Phi^{\text{cls}}(\Phi^{\text{C}}(f(x)) + \Phi^{r}(f(x)), y)$. On the other hand, the joint strategy predicts concepts and the final target simultaneously. It optimizes both concept prediction and target prediction losses during training. This enables the model to consider the relationship between concept and target predictions. This way, the loss of Equation~\ref{eqn:argmin2} is directly optimized. In our experiments, TCBMs are trained \textit{jointly} and the $f$ parameters are frozen during the TCBM training.

\subsubsection{Implementation of $\Phi^{r}$ and $\Phi^{\text{cls}}$}
\label{sec:phi_ridge_elasticnet}
$\Phi^{r}$ and $\Phi^{\text{cls}}$ are respectively trained with Ridge and elastic net penalties during the TCBM training. The Ridge penalization $R$ can be written as follows:
\begin{equation}
\begin{aligned}
    R(W) = \lambda_{R} \|W\|^{2}_2
\end{aligned}
\end{equation}
with $W \in \mathbb{R}^{d \times k}$ the weight matrix of the  $\Phi^{r}$ layer, $\lambda_{R}$ an hyperparameter and $\|\cdot\|^{2}_2$ the $L_2$ norm. On the other hand, the elastic net penalization $EN$ can be written as follows:
\begin{equation}
\begin{aligned}
    EN(A) = \lambda_{EN} \left( \alpha \|A\|_1 + (1-\alpha) \|A\|^{2}_2 \right)
\end{aligned}
\end{equation}
with $A  \in \mathbb{R}^{\lvert \text{C} \rvert \times k}$ the weight matrix of the  $\Phi^{\text{cls}}$ layer, $\lambda_{EN}$ and $\alpha$ two hyperparameters and $\|\cdot\|_1$ the $L_1$ norm.


\subsubsection{Other TCBM training hyperparameters}
\label{sec:tcbm_training}
In our experiments, language model and TCBM training is done with the following hyperparameters: \begin{itemize}
    \item \texttt{batch\_size} = 8
    \item \texttt{num\_epochs} = 15
    \item \texttt{max\_len} = 128 for AGNews and DBPedia, 256 for Movie Genre and 512 for Medical Abstracts. 
    \item \texttt{learning rate} = 0.001
    \item \texttt{optimizer} = \texttt{Adam }
    \item $\lambda_{R}$ = 0.01
    \item $\lambda_{EN}$ = 0.5
    \item $\alpha$ = 0.01
    \item $\lambda$ = 0.5
\end{itemize}


\subsubsection{TCBM construction with CAV projection}
\label{sec:cav_projection}
The \texttt{projection} approach to build  $\Phi^{\text{C}}$ consists in projecting the CAVs into the concept space. We formally define $\Phi^{\text{c}_{k}}(f(x)) = \frac{\langle f(x), \overrightarrow{\gamma_{k}}\rangle}{||f(x)||.||\overrightarrow{\gamma_{k}}||}$ as the linear projection of the embedding of $x$ from $f$ on the concept space associated to concept~$\text{c}_{k}$. This way, the concept embedding projection consists in computing the cosine similarity between the CAV and $f$ output. $\Phi^{\text{C}}$ is then constructed by concatenating linear projections corresponding to each concept $\text{c}_{i}$ and the final layer. Finally, $\Phi^{\text{cls}}$ and $\Phi^{\text{r}}$ are trained to perform the classification by minimizing the following loss function:
\begin{equation}
 \label{eqn:loss_function}
\mathcal{L}_{TCBM} = \mathcal{L}(\Phi^{\text{cls}}(\Phi^{\text{C}}(f(x)) + \Phi^{r}(f(x)), y)
\end{equation}
where $\mathcal{L}$ is the cross-entropy loss, $\Phi^{r}$ is trained with a Ridge penalty constraint and $\Phi^{cls}$ is trained with an elastic net penalty constraint.

\subsection{Language model classifiers and classification datasets details}
\label{sec:appendix_classifiers_dataset_details}
\paragraph{Language model classifiers.} The library used to import the pretrained language models is Hugging-Face. In particular, the backbone version of BERT is \texttt{bert-base-uncased} and the one of DeBERTa is \texttt{deberta-large}.

\paragraph{Classification datasets.}
The size of the training sets for AG News, DBpedia, Movie Genre and Medical Abstracts are respectively 4000, 6000, 4000 and 5000. The size of the test sets for AG News, DBpedia, Movie Genre and Medical Abstracts are respectively 23778, 30000, 7600 and 2888. \texttt{C\textsuperscript{3}M} concept evaluation is done on 1000 randomly selected rows on each dataset.

\subsection{Competitors implementation details}
\label{sec:appendix_competitors_implementation_details}
In our experiments, \texttt{C\textsuperscript{3}M}~\cite{cbm_plm} training is done with the following hyperparameters: \begin{itemize}
    \item \texttt{batch\_size} = 8
    \item \texttt{num\_epochs} = 15
    \item \texttt{max\_len} = 128 for AGNews and DBPedia, 256 for Movie Genre and 512 for Medical Abstracts. 
    \item \texttt{learning rate} = 0.001
    \item \texttt{optimizer} = \texttt{Adam }
    \item $\lambda_{R}$ = 0.01
    \item $\lambda_{EN}$ = 0.5
    \item $\alpha$ = 0.01
    \item $\lambda$ = 0.5
\end{itemize}

The training of \texttt{CB-LLM} is a two-stage process: (1) CBL training and (2) classification layer training. The CBL training is done with the following hyperparameters:  \begin{itemize}
    \item \texttt{batch\_size} = 16
    \item \texttt{num\_epochs} = 4
    \item \texttt{max\_len} = 128 for AGNews and DBPedia, 256 for Movie Genre and 512 for Medical Abstracts. 
    \item \texttt{learning rate} = 0.001
    \item \texttt{optimizer} = \texttt{Adam}
    \item \texttt{loss\_function} = \textit{cos cubed} as in~\citet{label_free_cbm} and ~\citet{cb_llm} 
\end{itemize}

The training of the classification layer of \texttt{CB-LLM} is done with the following hyperparameters: \begin{itemize}
    \item \texttt{batch\_size} = 64
    \item \texttt{num\_epochs} = 50
    \item \texttt{max\_len} = 128 for AGNews and DBPedia, 256 for Movie Genre and 512 for Medical Abstracts. 
    \item \texttt{learning rate} = 0.001
    \item \texttt{optimizer} = \texttt{Adam}
\end{itemize}

\subsection{Post hoc attribution explanation methods}
\label{sec:appendix_post_hoc}
\paragraph{Captum library.}
Post hoc attribution has been computed using the Captum~\cite{miglani_using_2023} library.
In particular, \texttt{Integrated gradients} has been computed with respect to language models' embedding layer with Captum's default settings. The embedding layers of BERT and DeBERTa are specified as follows: \texttt{model.model.embed\_tokens}.

\subsection{\method\ output examples}
\label{sec:appendix_examples}
\subsubsection{Residual connection importance evolution}

\begin{figure}[H]{\centering}
\begin{center}
\includegraphics[scale=0.15]{image/bert_gemma_2b2_20iteration_movies.png}
\caption{\label{fig:concept_add} Residual connection importance evolution during \method\ BERT training for the movie genre dataset.}
\end{center}
\end{figure}

\subsubsection{Examples of macro concept compositions}
\begin{figure}[H]{\centering}
\begin{center}
\includegraphics[scale=0.30]{image/wordcloud_postpon.png}
\caption{\label{fig:concept_add} Cloud of micro concepts composing the macro concept "Postponements or interputions" from the AGNews dataset.}
\end{center}
\end{figure}

\begin{figure}[H]{\centering}
\begin{center}
\includegraphics[scale=0.30]{image/wordcloud_accountability.png}
\caption{\label{fig:concept_add} Cloud of micro concepts composing the macro concept "Instances of accountability or public discourse" from the AGNews dataset.}
\end{center}
\end{figure}

\begin{figure}[H]{\centering}
\begin{center}
\includegraphics[scale=0.30]{image/wordcloud_acronym.png}
\caption{\label{fig:concept_add} Cloud of micro concepts composing the macro concept "Acronyms and initials" from the AGNews dataset.}
\end{center}
\end{figure}

\begin{figure}[H]{\centering}
\begin{center}
\includegraphics[scale=0.30]{image/wordcloud_cyber.png}
\caption{\label{fig:concept_add} Cloud of micro concepts composing the macro concept "Cybersecurity and information protection" from the AGNews dataset.}
\end{center}
\end{figure}

\begin{figure}[H]{\centering}
\begin{center}
\includegraphics[scale=0.30]{image/wordcloud_financial.png}
\caption{\label{fig:concept_add} Cloud of micro concepts composing the macro concept "Financial terms related to money" from the AGNews dataset.}
\end{center}
\end{figure}

\begin{figure}[H]{\centering}
\begin{center}
\includegraphics[scale=0.30]{image/wordcloud_onu.png}
\caption{\label{fig:concept_add} Cloud of micro concepts composing the macro concept "United-Nations-related" from the AGNews dataset.}
\end{center}
\end{figure}




\end{document}

