\section{Background and Related Work}
\label{bk_rw}
This section first recalls some principles of XAI methods used later in the papers and presents existing methods generating Concept Bottleneck Model for NLP.
% \begin{itemize}
%     \item Def concepts and TCAV
%     \item Def CBM
%     \item Strategies: projection, and training (joint, sequential and independant)
%     \item With and without label, iteratively, learning to intervene
%     \item No label: use of LLM (chatgpt...) and mainly for CV
% \end{itemize}


\subsection{XAI Background}
% \paragraph{Neural Post Hoc Interpretability.} 
\paragraph{Post Hoc Interpretability.}
Post hoc methods explain the behavior of a model after its training. These include post hoc attribution methods that attribute importance scores to inputs to explain the model outcome~\cite{zhao_explainability_2023}. In particular, gradient-based approaches such as \texttt{Integrated Gradients}~\cite{sundararajan_axiomatic_2017} compute these scores by back-propagating the gradients through the model.

Post hoc concept-based approaches generate explanations at a higher level of abstraction, by focusing on human interpretable attributes, called \emph{concepts}. %For instance, 
%A popular way of generating post hoc concept-based explanations is 
\texttt{TCAV}~\cite{tcav}
%. This approach consists in assessing 
assesses the model's sensitivity to a concept by back-propagating the gradients with respect to a linear representation of a candidate concept in the activation space, called concept activation vector (CAV). In the original paper, 
\texttt{TCAV} relies on human-labeled concepts, whose annotation can be time-consuming and expensive.  


\paragraph{Concept Bottleneck Models.}

Another way to improve interpretability
%the interpretability of AI systems 
consists in constructing 
%more interpretable 
so-called Concept Bottleneck Models (CBM)~\cite{cbm_intro}. 
These models sequentially (1) detect concepts and (2) linearly make the final prediction from concept activations, thereby significantly improving the understanding of the decision-making process. 

CBM have some limitations, such as requiring a predefined set of human-labeled concepts, generating incomplete concept bottleneck layers (CBL), or doing downstream task leakage. Concept incompleteness can have consequences either on the model accuracy (under-complete concept base) or the intelligibility (over-complete concept base) of the provided explanations~\cite{cbm_incremental}. Downstream task leakage~\cite{addressing_leakage_cbm} occurs when the final prediction uses unintended additional information from the concept predictor scores. 
%Under such a leakage regime, 
The concept predictor then no longer needs to  detect faithfully the concepts to be accurate on the classification task, thus compromising the interpretability faithfulness of the CBM. 

Among the vast literature on CBM~\cite{concept_survey}, numerous variants have been proposed to address one by one the aforementioned limitations. Notably, \texttt{Label-Free CBM}~\cite{label_free_cbm} prompts GPT-3~\cite{gpt3} to list the most important concepts for recognizing a specific class, freeing the approach from dependency on predefined labeled concepts. However, \texttt{Label-Free CBM} structurally depends on the parametric knowledge of GPT-3 and does not generate data driven concepts. In order to reach the accuracy of a black box NLP classifier while avoiding leakage, a non-interpretable connection parallel to the concept layer can be added to fit the residuals between the raw CBM outcome and the ground truth~\cite{cbm_posthoc, addressing_leakage_cbm}. However, adding such a residual connection decreases the CBM interpretability. \texttt{Res-CBM}~\cite{cbm_incremental} develops a method to derive new concepts from the residual layer to build a more complete CBL. Yet, it 
%\texttt{Res-CBM}~\cite{cbm_incremental} 
requires access to a set of candidate concepts to add to the CBL before probing the residual connection. Although these methods overcome some of the limitations inherent in CBM, their application has so far been restricted to computer vision. 



\subsection{Textual Concept Bottleneck Models}
\label{related_work}
% \begin{itemize}
%     \item SOTA \begin{itemize}
%         \item Interpreting Pretrained Language Models via Concept Bottlenecks
%     \item Sparsity-Guided Holistic Explanation for LLMs with
% Interpretable Inference-Time Intervention
%     \item Interpretable-by-Design Text Understanding
% with Iteratively Generated Concept Bottleneck
% \item Crafting Large Language Models for Enhanced Interpretability
% \end{itemize}
%     \item Limitations \begin{itemize}
%         \item Mostly need labels
%         \item when label-free -> based on LLM prior knowledge ("what are the important concepts?)"
%         \item Use LLM (chatgpt) -> very costly
%         \item Put many concepts (over-sufficiency)
%         \item Performance on concept detection
%     \end{itemize}
% \end{itemize}
This section presents recent works on generating Concept Bottleneck Models for NLP, referred to  as Textual Concept Bottleneck Models (TCBM).

\texttt{C\textsuperscript{3}M}~\cite{cbm_plm} enriches a set of predefined human-labeled concepts with additional concepts obtained from ChatGPT. While it approximately reaches the performance of an unrestricted black-box NLP classifier, it is trained without addressing the completeness of the CBL and the downstream classification leakage. Besides, its relying on ChatGPT and human-labeled concepts prevents reproducibility and scalability.

% \texttt{C\textsuperscript{3}M}~\cite{cbm_plm} consists in enriching a set of predefined human-labeled concepts with additional concepts obtained from ChatGPT\footnote{\url{https://openai.com/chatgpt/overview/}}. \texttt{Sparse-CBM}~\cite{cbm_sparsity_guided} extended \texttt{C\textsuperscript{3}M} by making the language model backbone sparser and thus more interpretable. While \texttt{C\textsuperscript{3}M} and \texttt{Sparse-CBM} approximately reach the performance of an unrestricted black-box NLP classifier, they are trained without addressing the completeness of the concept bottleneck layer and the downstream classification leakage. Besides, they rely on ChatGPT and human-labeled concepts, which poses problems in terms of reproducibility and scalability.

\texttt{CB-LLM}~\cite{cbm_crafting, cb_llm} also uses ChatGPT to generate a set of concepts that are scored with a sentence embedding model~\cite{reimers_sentence-bert_2019} to perform concept labeling. This way, concept are represented with numerical values, unlike \texttt{C\textsuperscript{3}M}. The concepts are then added to the CBL to train the TCBM. While \texttt{CB-LLM} approximately reaches the performance of a black-box NLP classifier, downstream classification leakage is not addressed. Moreover, it overlooks the completeness of its CBL, possibly resulting in an excessive number of concepts in the bottleneck layer and unintelligible explanations.

\texttt{TBM}~\cite{cbm_by_design} iteratively discovers concepts by leveraging GPT-4~\cite{gpt4} and focusing on examples misclassified by a separately trained linear layer. \texttt{TBM} is not strictly a CBM, since concept detection is performed with GPT-4 during inference, making also the approach non scalable and computationally expensive.
% \texttt{TBM}~\cite{cbm_by_design} iteratively discovers concepts by leveraging GPT-4~\cite{gpt4} and focusing on examples misclassified by a separately trained linear layer. While this approach does not depend on a predefined set of human-labeled concepts, \texttt{TBM} is not strictly a CBM, since concept detection is performed with GPT-4 during inference, making the approach non scalable and computationally expensive.


\begin{figure*}[t]{\centering}
\begin{center}
\includegraphics[scale=0.55]{image/overall_pipeline.png}
\caption{\method\ overview illustrated in the example of film synopsis classification. \method\ is a 4-step approach to build a TCBM from a $f$ black box NLP classifier. (1) A concept bank is created from the text corpus of interest. (2) Concepts are scored with respect to their importance to explain $f$ predictions. (3) The TCBM is trained through 3 layers: $\Phi^{\text{C}}$, $\Phi^{\text{cls}}$ and $\Phi^{\text{r}}$. (4) The TCBM training stops when the importance of $\Phi^{\text{r}}$ stops decreasing.}  
\label{fig:global_flow}
\end{center}
\end{figure*}