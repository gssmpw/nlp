@article{addressing_leakage_cbm,
  title={Addressing leakage in concept bottleneck models},
  author={Havasi, Marton and Parbhoo, Sonali and Doshi-Velez, Finale},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23386--23397},
  year={2022}
}

@InProceedings{cb_llm,
  author={Sun, Chung-En and Oikarinen, Tuomas and Ustun, Berk and Weng, Tsui-Wei},
  title={Concept Bottleneck Large Language Models},
 booktitle = {Proc. of the 13th Int. Conf. on Learning Representations, ICLR23},
  year =      {2025}
}

@article{cbm_by_design,
  title={Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck},
  author={Ludan, Josh Magnus and Lyu, Qing and Yang, Yue and Dugan, Liam and Yatskar, Mark and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:2310.19660},
  year={2023}
}

@article{cbm_crafting,
  title={Crafting large language models for enhanced interpretability},
  author={Sun, Chung-En and Oikarinen, Tuomas and Weng, Tsui-Wei}

@inproceedings{cbm_incremental,
  title={Incremental residual concept bottleneck models},
  author={Shang, Chenming and Zhou, Shiji and Zhang, Hengyuan and Ni, Xinzhe and Yang, Yujiu and Wang, Yuwang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11030--11040},
  year={2024}
}

@inproceedings{cbm_intro,
  title={Concept bottleneck models},
  author={Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={5338--5348},
  year={2020},
  organization={PMLR}
}

@inproceedings{cbm_plm,
  title={Interpreting pretrained language models via concept bottlenecks},
  author={Tan, Zhen and Cheng, Lu and Wang, Song and Yuan, Bo and Li, Jundong and Liu, Huan},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  pages={56--74},
  year={2024},
  organization={Springer}
}

@inproceedings{cbm_posthoc,
  title={Post-hoc Concept Bottleneck Models},
  author={Yuksekgonul, Mert and Wang, Maggie and Zou, James},
  booktitle={The Eleventh International Conference on Learning Representations},
year={2023}
}

@inproceedings{cbm_sparsity_guided,
  title={Sparsity-guided holistic explanation for llms with interpretable inference-time intervention},
  author={Tan, Zhen and Chen, Tianlong and Zhang, Zhenyu and Liu, Huan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={19},
  pages={21619--21627},
  year={2024}
}

@article{concept_survey,
  title={Concept-based explainable artificial intelligence: A survey},
  author={Poeta, Eleonora and Ciravegna, Gabriele and Pastor, Eliana and Cerquitelli, Tania and Baralis, Elena},
  journal={arXiv preprint arXiv:2312.12936},
  year={2023}
}

@inproceedings{gpt3,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	OPTpublisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	keywords = {notion},
	pages = {1877--1901},
	}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{label_free_cbm,
  title={Label-free Concept Bottleneck Models},
  author={Oikarinen, Tuomas and Das, Subhro and Nguyen, Lam M and Weng, Tsui-Wei},
  booktitle={Proc. of the 11th Int. Conf. on Learning Representations, ICLR},
year={2023}
}

@inproceedings{reimers_sentence-bert_2019,
	OPTaddress = {Hong Kong, China},
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {https://aclanthology.org/D19-1410},
	doi = {10.18653/v1/D19-1410},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textbackslash}textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2023-04-27},
	booktitle = {Proc. of the 2019 {Conf.} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {Int.} {Joint} {Conf.} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = nov,
	year = {2019},
	keywords = {notion},
	pages = {3982--3992},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\I7CRVEWR\\Reimers et Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf:application/pdf},
}

@inproceedings{sundararajan_axiomatic_2017,
	OPTaddress = {Sydney, NSW, Australia},
	series = {{ICML}'17},
	title = {Axiomatic attribution for deep networks},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axiomsâ€” Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	urldate = {2023-05-03},
	booktitle = {Proc. of the 34th {Int.} {Conf.} on {Machine} {Learning}, ICML},
    volume = 70,
    url = {https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
	publisher = {JMLR.org},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = aug,
	year = {2017},
	keywords = {notion},
	pages = {3319--3328},
	file = {Full Text PDF:C\:\\Users\\milan.bhan\\Zotero\\storage\\S3QJB223\\Sundararajan et al. - 2017 - Axiomatic attribution for deep networks.pdf:application/pdf},
}

@inproceedings{tcav,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  booktitle={International conference on machine learning},
  pages={2668--2677},
  year={2018},
  organization={PMLR}
}

@Article{zhao_explainability_2023,
  author =  {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  title =   {Explainability for {Large} {Language} {Models}: {A} {Survey}},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  year =    {2024},
url={https://dl.acm.org/doi/10.1145/3639372}
}

