\subsection{K-Paths improves LLM Reasoning in a Zero-Shot Setting} \label{result-zero-shot}
We evaluate LLMs' reasoning capabilities for interaction prediction across three zero-shot scenarios with varying context: (1) Base, where the LLM relies solely on its internal knowledge; (2) Definitions, where textual definitions of drugs and diseases are provided; and (3) K-Paths, where the LLM is provided with diverse paths retrieved by our K-Paths framework.

Table ~\ref{tab:zero_shot_results} presents results for Llama 3.1 8B and 70B on the DDInter, PharmacotherapyDB, and DrugBank datasets.
Across all datasets, LLMs significantly benefit from the diverse reasoning paths, outperforming both the Base and Definitions settings. 
On DDInter, K-Paths boosts the F1-score by 13.42 for Llama 3.1 8B and 6.18 for Llama 3.1 70B, compared to the Definitions setting.
Similarly, on PharmacoTherapyDB, inference with K-Paths leads to F1-score improvements of 12.45 (Llama 3.1 8B) and 8.46 (Llama 3.1 70B).
In some cases, Definitions slightly improve performance over Base or even degrade it, but they consistently fall short of the substantial gains achieved with K-Paths.
This difference likely stems from the nature of the information provided.
K-paths offer structured, contextualized knowledge, showing how entities are related.
Definitions, while informative, provide declarative knowledge without this crucial relational context, making it harder for LLMs to reason about interactions effectively.
Performance on the DrugBank task, which involves 86 interaction types, also highlights the necessity of structured reasoning. 
For both models, DrugBank's Base F1-scores hover around 1\%, but with K-Paths, they improve to approximately 40\%, emphasizing the importance of structured external knowledge in such complex tasks.

Model scaling (70B vs.\ 8B) demonstrates better reasoning ability with consistent performance improvements, but the gains diminish when external knowledge is provided. 
This suggests that high-quality external knowledge may reduce the reliance on larger parameter models. 
For example, on DDInter, the 8B model's F1-Score was slightly higher.
These findings highlight that LLMs benefit significantly from structured reasoning paths in zero-shot settings.

\begin{table*}[t!]
\small %
    \centering
    
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{llccc|ccc|ccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Setting}} & \multicolumn{3}{c|}{\textbf{DDInter}} & \multicolumn{3}{c|}{\textbf{PharmacotherapyDB}} & \multicolumn{3}{c}{\textbf{DrugBank}} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
        & & Accuracy & F1 & Kappa & Accuracy & F1 & Kappa & Accuracy & F1 & Kappa \\
        \midrule
        \multirow{4}{*}{Llama 3.1 8B Instruct} 
        & Base            & 69.09  & 33.34  & 4.37   & \underline{57.94}  & \underline{51.91}  & \underline{35.18}  & 31.36  & 0.65   & 0.87  \\
            & Defintions    & \underline{70.43}  & \underline{33.46}  & \underline{5.51}   & 57.54  & 51.70  & 34.38  & \underline{31.63}  & \underline{0.68}   & \underline{0.95}  \\
        & \sys              & \textbf{75.93}  & \textbf{46.76}  & \textbf{36.99}  & \textbf{69.44} & \textbf{64.36}  & \textbf{51.34}  & \textbf{55.54}  & \textbf{40.46}  & \textbf{45.58} \\

        
        \midrule
        \multirow{4}{*}{Llama 3.1 70B Instruct} 
        & Base            & 70.51  & \underline{40.01}  & \underline{19.07}  & 60.71  & 59.00  & 39.38  & \underline{31.32}  & \underline{1.35}   & \underline{3.88}  \\
        & Definitions   & \underline{71.61}  & 37.07  & 12.98  & \underline{62.30}  & \underline{61.14}  & \underline{42.30}  & 30.48  & 1.03   & 1.84   \\  
        & \sys & \textbf{78.01}  & \textbf{46.19}  & \textbf{35.33}  & \textbf{71.03}  & \textbf{67.46}  & \textbf{54.66}  & \textbf{57.72}  & \textbf{46.91}  & \textbf{49.19} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Performance comparison of Llama 3.1 models on zero-shot reasoning tasks. \textbf{Bold} indicates the best performance, while \underline{underlined} denotes the second-best performance. K-Paths improves domain-specific reasoning in zero-shot setting.}
    \label{tab:zero_shot_results}

\end{table*}

\begin{table*}
    \centering
    \scriptsize %
    \renewcommand{\arraystretch}{1.2} %
    \begin{tabular}{p{1.5cm} | p{7cm} | p{7cm}} %
    \toprule
             & \textbf{PharmacotherapyDB} & \textbf{DDinter} \\
            \midrule
    
        \textbf{Query} & \begin{minipage}[t]{6.5cm}Determine the possible effect of using \textbf{\textit{Vincristine}} (Drug) for \textbf{\textit{Muscle cancer}} (Disease).\end{minipage} & \begin{minipage}[t]{6.5cm}Determine the severity of interaction when \textbf{\textit{Ritonavir}} (Drug 1) and  \textbf{\textit{Leflunomide}} (Drug 2) are taken together.\end{minipage} \\
        \midrule
        \textbf{Answer} & \begin{minipage}[t]{6.5cm}Disease Modifying\end{minipage} & \begin{minipage}[t]{6.5cm}Major\end{minipage} \\
        \midrule
        \textbf{Definitions} & 
        \begin{minipage}[t]{6.5cm}
        \textbf{\textit{Vincristine}} is an antitumor that treats leukemia, lymphoma, Hodgkin's disease, and other blood disorders.\par
        \textbf{\textit{Muscle cancer}} is a musculoskeletal system cancer located in the muscle.
        \end{minipage} & 
        \begin{minipage}[t]{6.5cm}
        \textbf{\textit{Ritonavir}}, an HIV protease inhibitor, boosts other protease inhibitors' effectiveness and is used in some HCV therapies. As a CYP3A inhibitor, it increases drug concentrations. \par
        \textbf{\textit{Leflunomide}} is a pyrimidine synthesis inhibitor belonging to the disease-modifying antirheumatic drugs chemically and pharmacologically very heterogeneous.
        \end{minipage} \\
        \midrule
        \textbf{\sys} &  
        \begin{minipage}[t]{6.5cm}
        \textbf{\textit{Vincristine}} treats Kidney Cancer (Disease) and Kidney Cancer (Disease) resembles \textbf{\textit{Muscle Cancer}}.\par  
        \textbf{\textit{Vincristine}} downregulates \textit{TP53} (Gene) and \textit{TP53} (Gene) is associated with Muscle Cancer.
        \end{minipage} & 
        \begin{minipage}[t]{6.5cm}
        \textbf{\textit{Ritonavir}} binds CYP2C9 (Gene) and CYP2C9 (Gene) is bound by \textbf{\textit{Leflunomide}}\par
        \textbf{\textit{Ritonavir}} causes Neutropenia (Side Effect), and Neutropenia (Side Effect) is caused by \textbf{\textit{Leflunomide}}
        \end{minipage} \\
        \midrule
        \textbf{LLM Only} & \begin{minipage}[t]{6.5cm}Non Indications\end{minipage} & \begin{minipage}[t]{6.5cm}Moderate\end{minipage} \\
        \midrule
        
        \textbf{LLM+Definitions} & \begin{minipage}[t]{6.5cm}Non Indications\end{minipage} & \begin{minipage}[t]{6.5cm}Moderate\end{minipage} \\
        \midrule
        
        \textbf{LLM+\sys } & \begin{minipage}[t]{6.5cm}Disease Modifying\end{minipage} & \begin{minipage}[t]{6.5cm}Major\end{minipage} \\
        
        \bottomrule
    \end{tabular}
    \caption{Comparison of LLM responses based on external knowledge type. K-Paths allows for explainable inference.}
    \label{tab:llm_kg_comparison}
\end{table*}

\subsection{\sys Improves LLM Responses}
To complement our quantitative evaluation in Section \ref{result-zero-shot}, we conducted a qualitative analysis examining how the different forms of external knowledgeâ€”definitions and K-paths influence LLM responses. 
Table \ref{tab:llm_kg_comparison} presents two case studies from PharmacotherapyDB and DDInter, comparing the model's predictions under different knowledge augmentation conditions.
We make two key observations.
(1) Without external knowledge, the Base model often predicts incorrectly, demonstrating insufficient domain knowledge.
In the PharmacotherapyDB example, although the Definition of \textit{Vincristine} mentions its use in cancer treatments, the model failed to infer its applicability to \textit{Muscle Cancer}.
However, introducing diverse reasoning paths from K-Paths improved the LLM's predictions.
This suggests that simply providing factual information (definitions) without explicit relational information is insufficient for correct reasoning in interaction prediction. 

(2) Diverse retrieved paths enable the model to connect relevant entities and their relationships, facilitating more accurate and contextually relevant answers. 
For instance, in the DDInter example, when K-Paths was incorporated, the model correctly predicted a ``Major'' interaction between \textit{Ritonavir} and \textit{Leflunomide}, identifying \textit{Neutropenia} as a possible side effect. This is crucial, as \textit{Neutropenia} is a potentially life-threatening condition.
A ``Moderate'' misclassification could have severe safety implications, stressing the importance of high-quality reasoning paths for model reliability, especially in high-stakes medical contexts where inadequate therapy selection may lead to patient harm.
In summary, the LLM responses in the LLM+K-Paths setting demonstrate that while Definitions provide some information about query entities, structured, diverse KG paths are essential for effective contextualization and improved zero-shot performance. 


\subsection{Influence of Path Selection Strategies on LLM Performance}
We conduct an ablation study using Llama 3.1 8B on the validation set to investigate the impact of different path selection strategies and the number of retrieved paths ($K$) on LLM performance in interaction prediction.  
Specifically, we compared LLM performance using the ``Base'' setting ($K=0$), diverse paths ($K=1, 5, 10, 15, 20$ from our K-Paths framework), and shortest paths (without diversity filtering).

Results in Table~\ref{fig:RQ2-results} show that adding diverse reasoning paths significantly improves performance.
On DDInter, F1-scores increase from 12.66\% ($K=0$) to 46.73\% ($K=10$), and on DrugBank, they increase from 0.59\% ($K=0$) to 43.35\% ($K=10$).  
However, performance gains diminish beyond $K=10$, suggesting that excessively long paths introduce redundancy or noise.  
Furthermore, \sys outperforms shortest-path selection (without diversity filtering).  
Removing our filtering algorithm results in the degradation of the F1-score by 6.99\% on DDInter and 4.32\% on DrugBank, demonstrating the importance of diverse path retrieval for improved performance.

\begin{figure*}[t]
  \centering
  \begin{minipage}{0.4\textwidth} %
    \centering
    \includegraphics[width=\linewidth]{images/K_paths_vs_performance.png}
  \end{minipage}
  \hspace{0.05\textwidth} %
  \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/Path_diversity.png}
  \end{minipage}
  
  \caption{Influence of path selection strategies on Llama 3.1 8B. Diverse paths are essential for performance improvement.}
  \label{fig:RQ2-results}
\end{figure*}



\begin{table*}[t]
    \centering
    \small %
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{llccc|ccc|ccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Setting}} & \multicolumn{3}{c|}{\textbf{DDInter}} & \multicolumn{3}{c|}{\textbf{PharmacotherapyDB}} & \multicolumn{3}{c}{\textbf{DrugBank}} \\
        \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
        & & Accuracy & F1 & Kappa & Accuracy & F1 & Kappa & Accuracy & F1 & Kappa \\
        \midrule
        
        \multicolumn{11}{l}{\textbf{LLM-Based Models}} \\
        QLoRA-Llama & Definitions & 82.36 & 67.96 & 53.64 & \textbf{81.75} & \textbf{79.91} & \textbf{71.84} & \textbf{73.22} & \textbf{68.15} & \textbf{67.26} \\
        QLoRA-Llama & K-Paths & 80.55 & \underline{68.63} & 57.85 & \underline{78.57} & \underline{76.71} & \underline{66.68} & \underline{71.83} & \underline{65.57} & \underline{65.63} \\
        
        \midrule
        
        \multicolumn{11}{l}{\textbf{Graph-Based Models (GNNs)}} \\
        EmerGNN & Complete KG & \underline{84.26} & 68.00 & \underline{58.92} & 71.43 & 68.41 & 55.38 & 71.04 & 59.42 & 65.14 \\
        EmerGNN & K-Paths & \textbf{84.53} & \textbf{68.85} & \textbf{58.91} & 71.03 & 68.12 & 54.10 & 68.98 & 59.06 & 62.54 \\
        RGCN & Complete KG & 72.01 & 51.47 & 31.38 & 61.11 & 60.08 & 41.12 & 29.98 & 15.49 & 20.88 \\
        RGCN & K-Paths & 73.32 & 52.12 & 32.70 & 66.82 & 61.74 & 39.25 & 31.70 & 17.51 & 23.14 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Performance of various models across datasets in a supervised setting. Bold indicates the best performance, and \underline{underlined} denotes the second-best. LLMs leverage both knowledge types effectively with supervision, and K-Paths enhance GNN efficiency without significant performance loss.}
    \label{tab:results-3}
\end{table*}

\begin{table*}[t]
    \centering
    \begin{adjustbox}{max width=0.9\textwidth}
    \begin{tabular}{lcccccccc}
        \toprule
        \multirow{2}{*}{Dataset} & \multicolumn{4}{c}{Before Retrieval (Complete KG)} & \multicolumn{4}{c}{After Retrieval (K-Paths)} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9}
         & \#Nodes & \#Relations & \#Triplets & Sec/Epoch \{EmerGNN, RGCN\} & \#Nodes & \#Relations & \#Triplets & Sec/Epoch \{EmerGNN, RGCN\} \\
        \midrule
        \textbf{DDInter}  & 35,107 & 26 & 1,763,596 & \{606.24s, 5.78s\} & 4,723 & 18 & 113,933 & \{102.21s, 1.18s\} \\
        \textbf{PharmacotherapyDB} & 34,412 & 26 & 1,691,829 & \{20.83s, 2.41s\} & 3,307 & 23 & 11,905 & \{2.95s, 0.12s\} \\
        \textbf{DrugBank} & 35,103 & 109 & 1,789,976 & \{1011.86s, 11.41s\} & 6,335 & 101 & 184,273 & \{146.98s, 3.56s\} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
        \caption{Augmented KG Statistics: K-Paths Improves Training Efficiency.}
        \label{results-efficiency}
\end{table*}

\subsection{Impact of K-Paths Derived Subgraphs on Supervised LLM and GNN Performance}
We evaluate the influence of subgraphs derived from retrieved paths on supervised learning for LLMs and GNNs.

\textbf{LLM Performance:} We fine-tuned Llama 3.1 8B Instruct using \sys and textual definitions.
As shown in Table~\ref{tab:results-3}, fine-tuned LLMs perform comparably regardless of the training source, indicating their ability to integrate both structured and unstructured knowledge when supervision is provided.
However, we further observe that definitions often outperform K-Paths, likely because they provide a more direct and semantically rich signal that is easier to learn; in contrast, in the zero-shot setting (Section \ref{result-zero-shot}), K-Paths provide more useful relational cues.


\textbf{GNN Performance:} We compared GNN models trained on the full knowledge graph (Complete KG) against those trained on the subgraphs constructed from the K-Paths framework.  Table~\ref{tab:results-3} shows that with K-Paths, we achieve comparable accuracy to using the Complete KG despite being approximately 90\% smaller.
This reinforces that smaller, task-specific graphs enhance efficiency without significant performance loss.
For example, on DDInter, EmerGNN achieves nearly identical performance using K-Paths (F1: 68.85\%) compared to Complete KG (F1: 68.00\%), suggesting that a targeted subgraph retains essential knowledge while significantly improving efficiency.
RGCN benefits from \sys, with F1 increasing from 51.47\% (Complete KG) to 52.12\% (K-Paths) on DDInter and from 15.49\% to 17.51\% on DrugBank.
This highlights the advantage of a more focused graph structure for such models. This result is consistent in the transductive setting shown in the Appendix \ref{sec-transd}.


\textbf{Efficiency Analysis:}  Table~\ref{results-efficiency} analyzes the augmented network's statistics before and after K-Paths subgraph retrieval.
The results clearly demonstrate that this retrieval process significantly reduces the number of nodes, relations, and triplets, leading to faster training times and reduced memory usage.
For instance, on DrugBank, the number of nodes reduces from 35,103 to 6,335, the number of triplets decreases from 1,789,976 to 184,273, and the time per epoch for EmerGNN drops from 1011.86s to 146.98s.
Similarly, for DDInter, the reduction in graph size leads to a considerable speedup, with EmerGNN's time per epoch decreasing from 606.24s to 102.21s. We also show the important relations retained by K-Paths in the Appendix \ref{app:hetio}.
These results highlight that K-Paths improves training efficiency without compromising model performance, reinforcing the scalability of our approach. 

\textbf{LLM vs. GNN Performance:} Finally, we observe that supervised fine-tuning enables LLMs to outperform GNNs on most datasets, suggesting that supervised LLMs can effectively leverage multiple structured and unstructured modalities and sometimes even surpass GNNs trained solely on relational graphs.
