In this section, we aim to answer the following research questions:
\textbf{RQ1:} Can LLMs accurately predict interaction \textit{types}, \textit{severities}, or \textit{indications} by reasoning over multi-hop knowledge graph paths provided as context in a zero-shot setting? 

\noindent
\textbf{RQ2:} How do path selection strategies (e.g., shortest path, diverse path selection) and the number of reasoning paths \textit{(K)} influence LLM performance in interaction prediction tasks? 

\noindent
\textbf{RQ3:} How do subgraphs derived from the K-Paths framework impact the performance of LLMs and GNNs in a supervised setting?

\subsection{Datasets} \label{sec:datasets}  
\heading {Evaluation Datasets:} Following previous work, we conducted experiments in an inductive setting to assess the model's generalization to unseen entities.
In this setting, the training set includes interactions only between entities in the training set, the validation set contains interactions where at least one entity appears only in the validation set or interactions exclusively within it, and the test set follows the same rule for test entities.
This setup enables evaluation of the model's ability to predict unobserved interactions involving both known and emerging entities, as well as interactions between emerging entities.

We used three datasets with different task objectives: DDInter \cite{xiong2022ddinter}, PharmacotherapyDB (v1.0) \cite{himmelstein2016pharmacotherapydb}, and DrugBank \cite{wishart2018drugbank}.
For DrugBank, we applied the inductive split from \cite{du2024customized}, while for DDInter, we created train, validation, and test sets following the described inductive split. Due to its smaller size, PharmacotherapyDB was divided into training and test sets in a similar inductive setting. 
Table 1 provides detailed statistics, including the number of specific interaction types.

\heading {External Knowledge Base:} Hetionet is a heterogeneous biomedical network curated from 29 databases. It includes various biomedical entities such as compounds, genes, diseases, etc. Following \cite{yu2021sumgnn}, the processed version used in this work includes 33,765 nodes across 11 types and 1,690,693 edges spanning 23 relation types.


\subsection{Baselines}
We compare the K-Paths framework  with the following baselines:

\subsubsection{LLM-based Baselines:}
Here, we evaluated the impact of K-Paths on LLM reasoning in zero-shot and supervised settings. 
\begin{itemize}
    \item \textbf{Reasoning based on internal knowledge (Base):} The LLM uses only its internalized knowledge to infer interactions and relationships.
    \item \textbf{Reasoning based on textual definitions (Definitions):} The LLM leverages textual definitions of drugs and diseases from external resources, such as DrugBank\footnote{\url{https://go.drugbank.com/}} and the Disease Ontology\footnote{\url{https://disease-ontology.org/}}, to infer interactions and relationships independent of the knowledge graph's structure.

\end{itemize}

\subsubsection{GNN-based Baselines:}
Here, we evaluated the impact of K-Paths on GNNs in a supervised setting only.
\begin{itemize}
    \item \textbf{Complete Graph-based Prediction (Complete KG):} The Graph Neural Network (GNN) uses the entire augmented KG to predict interactions, considering all entities and relationships.
\end{itemize}

\subsection{Implementation details}
We provide the details for \sys implementation for all our experiments with different LLMs and GNNs. 
For further details on prompting strategies, training configurations, and hyperparameters, see Appendix \ref{app:prompting}.
During path retrieval, we limit the maximum path length to 3 as longer paths tend to be more susceptible to noise and less likely to represent meaningful, interpretable relationships.
We assume there is no direct link between the query entities whose interaction we aim to predict, and the model must infer the interaction type by reasoning over existing facts in the knowledge graph. 
To prevent data leakage, we explicitly check and remove direct interaction links between entities during training and verify their absence in the test set.
For all experiments across datasets, we set  \(K = 10\) retrieved paths per query.

For LLM reasoning, we employ Llama-3.1-8B-Instruct \cite{dubey2024llama} and Llama-3.1-70B-Instruct \cite{dubey2024llama} as our primary inference models, leveraging their instruction-following capabilities.
In zero-shot settings, we utilize direct prompting to evaluate their reasoning ability.
In supervised settings, we fine-tuned Llama-3.1-8B-Instruct using QLoRA, a lightweight and efficient fine-tuning approach.
In both these experiments, we generate the output with a greedy decoding.
For GNN architectures, we use Relational Graph Convolutional Networks (RGCN) because of their ability to handle relational data, which aligns with our task. Additionally, we adopt EmerGNNâ€™s backbone architecture, as it is the current state-of-the-art inductive graph-based model for drug-drug interaction prediction tasks.

\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}} %
\newcolumntype{Q}[1]{>{\centering\arraybackslash}p{#1}} %

\begin{table}
\centering
\setlength{\tabcolsep}{2pt} %
\renewcommand{\arraystretch}{1.1} %
\scriptsize %
\begin{tabular}{Q{2.1cm} Q{1.5cm} Q{2.0cm} P{2.6cm}}
\toprule
\makecell{\textbf{Dataset}} & 
\makecell{\textbf{Entities}} & 
\makecell{\textbf{Categories}} & 
\makecell{\textbf{Example}} \\ 
\midrule

DDInter & \makecell{1,689 \\ drugs} & \makecell{3 interaction\\ severity levels} & \makecell{Severity: Major \\ (DrugX + DrugY)} \\ 
\addlinespace[2pt]

PharmacotherapyDB & \makecell{601 drugs \\ 97 diseases} & \makecell{3 indications \\ } & \makecell{DrugX treats \\ DiseaseY} \\ 
\addlinespace[2pt]

Drugbank & \makecell{1,710 \\ drugs} & \makecell{86 metabolic\\ interaction levels} & \makecell{DrugX decreases \\ excretion rate of DrugY} \\ 

\bottomrule
\end{tabular}
\caption{Overview of the datasets and tasks studied.}
\label{tab:tasks_overview}
\end{table}


\subsection{Evaluation}
Following ~\citep{zhang2023emergingdruginteractionprediction}, for all the experiments, we report the accuracy, macro averaged F1-score, and Cohen's Kappa for the multi-class classification task. 
Since LLMs generate open-ended responses, we first parse the response using regular expression and map it to one of the labels in the dataset. 
If we cannot map the response to one of the labels in the dataset, we assign the prediction to the majority class from the training set. 
We also report the same evaluation metrics for GNN experiments. 
