\section{Detaching Safety Mechanism from The Template Region}
\label{sec:rq3}

Since an anchored safety mechanism likely causes vulnerabilities, it is worth exploring whether a detached safety mechanism during generation could, conversely, improve the model's overall safety robustness. This would involve detaching its safety functions from two aspects: (\romannumeral 1) the process of identifying harmful content and (\romannumeral 2) the way this processed information is utilized during generation.


\paragraph{Transferability of Probes.} 
Regarding the first aspect, we inspect whether the harmfulness processing functions in the template region can transfer effectively to response generation. 
To investigate this, we collect harmful responses from successful jailbreaking attempts and harmless responses using instructions in \(\gD_{\text{anlz}} \). We then evaluate whether the harmfulness probes derived from the template region in \Cref{subsec:prob_attack} can still distinguish if a response is harmful.
Specifically, we collect the residual streams from all layers at the first 50 positions of each response and measure the probes' accuracy in classifying harmfulness.

\input{Figures/fig7}

\looseness=-1 As shown in \Cref{fig:probe_in_resp} (see others in \Cref{appendix:probes}), our analysis of Llama-3-8B-Instruct reveals that harmfulness probes from the middle layers achieve relatively high accuracy and remain consistent across response positions. This result suggests that harmfulness probes from specific layers in the template region can be effectively transferred to identify harmful content in generated responses. 





\paragraph{Detaching Safety Mechanism.} 

To address the harmfulness-to-generation aspect, we need to examine how harmfulness features evolve during the generation process. The right-most plot in \Cref{fig:prob_in_temp} highlights distinct patterns between successful and failed attacks when generating the first response token. In failed attacks, the harmfulness feature quickly peaks and sustains that level throughout the generation process, whereas in successful attacks, it decreases and remains at a low level.
This observation suggests that additional harmfulness features should be injected during generation to counteract their decline in effective attacks. 

Based on this finding, we propose a simple straightforward method to detach the safety mechanism: use the probe to monitor whether the model is generating harmful content during response generation and, if detected, inject harmfulness features to trigger refusal behavior.
Formally, for a harmful probe \(\vd^{\ell,-}_{\tau}\) obtained from position \(\tau\) and layer \(\ell\), the representation at position \(i\) during generation is steered as follows:
\begin{equation}
\label{eq:detach}
    \vx_i^\ell \leftarrow \begin{cases} 
    \vx_i^\ell + \alpha\vd^{\ell,-}_{\tau} & \text{if } \vx_i^\ell\vd^{\ell,-}_{\tau} > \lambda \\
    \vx_i^\ell & \text{otherwise}
\end{cases},
\end{equation}
where \(\alpha\) is a factor controlling the strength of injection and \(\lambda\) is a decision threshold (See \Cref{appendix:detaching} for further details).

We evaluate this approach against AIM, AmpleGCG, and PAIR attacks.
We compare ASRs for response generations with and without detaching the safety mechanism, as shown in \Cref{tab:steering}. The results demonstrate that detaching the safety mechanism from the template and applying it directly to response generation effectively reduces ASRs, strengthening the model's safety robustness.


\input{table1}