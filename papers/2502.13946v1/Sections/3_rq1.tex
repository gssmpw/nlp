\section{The Template-Anchored Safety
Alignment in Aligned LLMs}
\label{sec:rq1}

% In this section, we provide evidence that (1) aligned LLMs shift their attention from the instruction region to the template region (\Cref{subsec:attn_shift}), and (2) their refusal capability causally relies on the information processed in the template region (\Cref{subsec:temp_patching}).
%In this section, we demonstrate that the template-anchored safety alignment issue is widespread across various aligned LLMs.

\input{Figures/fig2}

\subsection{Preliminaries}

\paragraph{Datasets.}
We construct two datasets, $\gD_{\text{anlz}}$ and $\gD_{\text{eval}}$, designed to analyze the behavioral differences of LLMs when handling harmless versus harmful inputs and to evaluate their refusal capabilities, respectively. 
Each dataset consists of paired \textit{harmful} and \textit{harmless} instructions. For $\gD_{\text{anlz}}$, harmful instructions are sourced from JailbreakBench \cite{chao2024jailbreakbench}, while for $\gD_{\text{anlz}}$, they are drawn from HarmBench's standard behavior test set \cite{mazeika2024harmbench}. The harmless counterparts in both datasets are sampled from Alpaca-Cleaned,
\footnote{https://huggingface.co/datasets/yahma/alpaca-cleaned}
a filtered version of Alpaca \cite{alpaca} that excludes refusal-triggering content.
To ensure a precise comparative analysis, each harmless instruction matches its harmful counterpart in token length. Since tokenization methods vary across models, we maintained separate versions of $\gD_{\text{anlz}}$ and $\gD_{\text{eval}}$ for each model.


\paragraph{Models.}
To validate the generality of our findings, we study a diverse set of safety fine-tuned models: Gemma-2 (2b-it, 9b-it) \cite{team2024gemma}, Llama-2-7b-Chat \cite{touvron2023llama}, Llama-3 (3.2-3b-Instruct, 8B-Instruct) \cite{dubey2024llama}, and Mistral-7B-Instruct \cite{jiang2023mistral}.



\subsection{Attention Shifts to The Template Region}
\label{subsec:attn_shift}
In modern LLMs based on attention mechanisms, the distribution of attention weights across different heads reflects which regions of information collectively influence the model's next token predictions \cite{bibal2022attention}. A notable observation is that when the model refuses harmful requests, its response often exhibits distinct patterns from the outset, for instance, initiating with the token `\texttt{Sorry}' as the first output \cite{zou2023universal, qi2024safety}. 
This suggests that if the model's safety function primarily depends on the template region, then when processing harmful inputs, the attention weights at the final input position should focus more on the template region, while exhibiting comparatively less focus on the instruction region.

\input{Figures/fig3}
\paragraph{Method.}
To investigate whether the attention weights exhibit increased focus on the template region when processing harmful inputs, we analyze attention weight distributions across all heads for both the instruction and template regions. More importantly, we examine how these distributions differ between harmless and harmful inputs.

Formally, for $h$-th attention head in layer $\ell$, we compute the average attention weight accumulation over regions of interest. Let $\mathbf{A}^{\ell,h,j}_{T,i}$ denote the attention weight at the final position $T$ of the input that attends to the position $i$ in $j$-example, we define the regional attention accumulation for harmless (\(+\)) and harmful (\(-\)) inputs as:
\vspace{-0.5em}
\begin{equation}
\alpha^{\pm}_R(\ell,h) = \frac{1}{|\gD_{\text{anlz}}|} \sum_{j=1}^{|\gD_{\text{anlz}}|} \sum_{i \in \gI_R} \mathbf{A}^{\ell,h,j,\pm}_{T,i},
\end{equation}
where $R \in \{\text{inst}, \text{temp}\}$ indicates the region, with $\gI_{\text{inst}} = \{1,\dots,S\}$ and $\gI_{\text{temp}} = \{S+1,\dots,T\}$ being the position indices for the instruction and template region, respectively.

When processing harmful inputs compared to harmless ones, the attention shift is computed as:
\begin{equation}
\delta_R(\ell,h) = \alpha^{-}_R(\ell,h) - \alpha^{+}_R(\ell,h),
\end{equation}
where a positive \( \delta_R(\ell,h) \) indicates that region $R$ receives more attention from the given head when processing harmful inputs relative to harmless ones, whereas a negative value suggests the opposite.


\paragraph{Results.}

\Cref{fig:attn_shift} shows the distribution histograms of \( \delta_R \) from all heads across the compared LLMs. We observe that the template distributions exhibit longer and more pronounced tails on the positive side compared to the negative side, while the instruction distributions show the opposite trend. This consistent phenomenon observed across various safety-tuned LLMs suggests that \subconc{these models tend to focus more on the template region when processing harmful inputs, providing strong evidence for the existence of TASA}.

To illustrate this phenomenon more concretely, we showcase the behavior of a specific attention head (17th-layer, 21st-head) from Llama-3-8B-Instruct on the right side of \Cref{fig:attn_shift}. This example demonstrates how an individual head behaves differently when processing harmless versus harmful inputs. We observe that the attention weights at the final input position (i.e., `\texttt{\textbackslash n\textbackslash n}') show a clear focus shift from a concrete noun `\texttt{tea}' in the instruction to a role-indicating token `\texttt{assistant}' in the template region when the input is harmful.


\subsection{Causal Role of The Template Region}
\label{subsec:temp_patching}
While safety-tuned LLMs shift their attention toward the template region when processing harmful inputs, \textit{does this shift indicate a reliance on template information for safety-related decisions?} To confirm this, we verify whether intermediate states from the template region exert a greater influence on models' safety capabilities than those from the instruction region.


\paragraph{Evaluation Metric.}

Quantifying the influence of intermediate states typically involves causal effects, such as IE (see \Cref{para:patching}).
However, evaluating an LLM's safety capability by analyzing complete responses for each of its numerous internal states would be highly inefficient.
To address this, we adopt a lightweight surrogate metric following prior work \cite{lee2024mechanistic, arditi2024refusal}. This approach uses a linear probe on the last hidden states to estimate a model's likelihood of complying with harmful inputs.
The predicted logits for harmful inputs serve as an efficient proxy to measure the causal effects of intermediate states on safety capability, where higher logits for harmful inputs indicate weaker safety capability. Following difference-in-mean method \cite{arditi2024refusal, marks2024geometry}, we obtain the probe \(\vd^{+}\in \sR^d\) as follows:
\vspace{-0.6em}
\begin{equation}
    \vd^+ = \frac{1}{|\gD_{\text{anlz}}|}\sum_{j=1}^{|\gD_{\text{anlz}}|} \vx^{L,j,+}_{T} - \frac{1}{|\gD_{\text{anlz}}|}\sum_{j=1}^{|\gD_{\text{anlz}}|} \vx^{L,j,-}_{T},
\label{eq:diff_prob}
\end{equation}
where \( \vx^{L,j,\pm}_{T} \) is the residual stream from example \(j\) of either harmless (\(+\)) or harmful (\(-\)).  We then compute \(m(x) = \vx_{T}^{L} \vd^+\) and refer to it as the \textit{compliance metric}. 

A 5-fold cross-validation of the probe achieves an average accuracy of $98.7 \pm 0.7\%$ across models, demonstrating its effectiveness in distinguishing between safe and unsafe model behaviors.

\paragraph{Method.}

Consider a scenario where we input the last token in the template and aim to obtain whether the model intends to comply the input, as measured by the compliance probe. 
In this forward pass, the residual stream of the last token aggregates context information by fusing the previous value states \( \vv^{\ell,h}_{<T}\coloneqq\vx_{<T}^\ell\mW^{\ell,h}_V\) in every attention head.
To compute the causal effects of intermediate states from different regions, we calculate the IE when patching the value states of harmful input with those of harmless input for one region, while leaving the states unchanged for the other region. Specifically, we compute the IE as:
\vspace{-0.3em}
\begin{align}
     &\mathrm{IE}^{\ell,h}_{R^\prime}\left(m;\gD_{\text{anlz}}\right) = \nonumber\\ &\resizebox{\linewidth}{!}{$\underset{\left(x^{+},x^{-}\right)\sim\gD_{\text{anlz}}}{\E}\left[m\left(x^{-}|\mathrm{do}\left(\vv_{\gI_{R^\prime}}^{\ell,h}=\vv_{\gI_{R^\prime}}^{\ell,h,+}\right)\right) - m(x^{+})\right],$}
\end{align}
where \(R^\prime \in \{\text{inst}, \text{temp}^\prime, \text{all}\}\) indicates a specific region, with $\gI_{\text{inst}} = \{1,\dots,K\}$, $\gI_{\text{temp}^\prime} = \{K+1,\dots,T-1\}$ and $\gI_{\text{all}} = \{1,\dots,T-1\}$. Notably, we exclude the last position $T$ from patching to avoid direct impact on the compliance probe.


Given that different heads have varying influences on safety capability, we first patch two regions together to quantify the importance of each head by \(\mathrm{IE}^{\ell,h}_{\text{all}}\left(m;\gD_{\text{anlz}}\right)\). Then we cumulatively patch the value states of heads for each region, starting from the most important head to the least, to obtain \(\mathrm{IE}^\gH_{R^\prime}\left(m;\gD_{\text{anlz}}\right)\). Here, \(\gH=\{(\ell_1,h_1),\dots\}\) represents the head indexes sorted by their importance scores. A higher \( \mathrm{IE}^\gH_{R^\prime} \) indicates the information from region \( R^\prime \) has a greater causal effect on the model's compliance decision, and vice versa. For a fair cross-model comparison, we use the \textit{normalized indirect effect} (NIE) by dividing the IE of each pair by \( (m(x^-)-m(x^+)) \). 

\paragraph{Results.}

\Cref{fig:region_patching} shows the trend of NIE in different regions as the number of patched heads increases. We have these key observations: (1) When patching the template region, a substantial increase in NIE is achieved by patching only a small number of heads that are critical to safety capabilities. In contrast, patching the instruction region does not bring significant improvement. This indicates that \subconc{the core computation of safety functions primarily occurs in heads processing information from the template region}. (2) For most models, even as the number of patched heads increases steadily, the NIE of the instruction region remains a remarkable gap compared to that of the template region. This indicates that \subconc{safety-tuned LLMs tend to rely on information from the template region rather than the instruction region when making initial compliance decisions.} Even when reversed instruction information is forcibly injected, it has limited influence on the prediction results. 

Overall, these results confirm that the safety alignment of LLMs is indeed anchored: \mainconc{current safety alignment mechanisms primarily rely on information aggregated from the template region to make initial safety-related decisions}.

