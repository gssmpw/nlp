\section{Background}

\paragraph{Generation Process of LLMs.}
\label{para:forward}
Following prior works \cite{elhage2021mathematical, geva2023dissecting}, we demonstrate how a Transformer \cite{vaswani2017attention} decoder-based LLM computes new tokens autoregressively.
Given a prompt with tokens \(t_1,\dots,t_T\), tokens are first embedded into vectors \(\vx_1,\dots,\vx_T\).
Each vector at position \(i\) forms an initial residual stream \(\vx^{0}_i\).
Through each layer \(\ell\in[1,L]\), the residual stream is updated according to \(\vx^{\ell}_i = \vx^{\ell-1}_i + \va^{\ell}_i + \vm^{\ell}_i\), where \(\va^{\ell}_i\) and \(\vm^{\ell}_i\) represent the attention and MLP outputs, respectively.
For simplicity, we omit the layer normalization and position embedding calculations.

Each attention head \(h\) employs four projection matrices: \(\mW^{\ell,h}_Q, \mW^{\ell,h}_K, \mW^{\ell,h}_V \in \mathbb{R}^{d \times \frac{d}{H}}\) and \(\mW^{\ell,h}_O \in \mathbb{R}^{\frac{d}{H} \times d}\). The attention map \( \mA \in \mathbb{R}^{T \times T}\) for each head is computed as: 
\(
\mA^{\ell,h} = \varphi\left( \frac{ (\vx^\ell\mW^{\ell,h}_Q)(\vx^\ell\mW^{\ell,h}_K)^{T} }{\sqrt{d/H}} + \mM \right)
\), \(\varphi\) denotes row-wise softmax normalization, and \(\mM\) is a lower triangular matrix for causal masking.
The final outputs from the attention module is competed as \(\va^\ell = \sum_{h=1}^H (\mA^{\ell,h}\vx^\ell\mW^{\ell,h}_V)\mW^{\ell,h}_O\).
The MLP then independently applies non-linear transformations on each token's representation.

Finally, the model unembeds the final position's representation into logits, applies softmax to obtain next-token probabilities, and samples tokens autoregressively until the generation is complete.


\paragraph{Activation Patching.}
\label{para:patching}

\looseness=-1 Consider a metric \( m \in \sR \) evaluated via a computation graph (e.g., an LLM), \( \rvr \in \sR^d \) represent a node (e.g., an intermediate activation\footnote{We use these terms \textit{activation}, \textit{representation} and \textit{hidden state} interchangeably throughout this paper.}) in this graph. Following prior work \cite{vig2020investigating,finlayson2021causal, marks2024sparse}, we assess the importance of \( \rvr \) for a pair of inputs \(\left( x_\text{clean}, x_\text{patch} \right)\) by measuring its \textit{indirect effect} (IE) \cite{pearl2001direct}) with respect to \( m \):
\vspace{-0.6pt}
\begin{align}
&\mathrm{IE}\left(m; \rvr; x_\text{clean}, x_\text{patch}\right) =\nonumber\\ &m\left(x_\text{clean}|\mathrm{do}(\rvr = \rvr_\text{patch})\right) - m(x_\text{clean}).
\end{align}

In this formulation, \( \rvr_\text{patch} \) represents the value that \( \rvr \) is given in the computation of \( m(x_\text{patch}) \), and \( m(x_\text{clean}|\text{do}(\rvr = \rvr_\text{patch})) \) represents the metric's value when computing \( m(x_\text{clean}) \) with an \textit{intervention} that explicitly sets \( \rvr \) to \( \rvr_\text{patch} \) . We illustrate this patching process at left side of \Cref{fig:region_patching}. As an example, consider the inputs \( x_\text{clean} = \) `\texttt{How to make a bomb}' and \( x_\text{patch}\) =  `\texttt{How to read a book}', with metric \( m(x) = P\left(\textit{model complies}|x\right) \) representing the model's compliance probability. When \( \rvr \) is an intermediate activation from a specific input position, larger values of \( \text{IE}(m; \rvr; x_\text{clean}, x_\text{patch}) \) suggest that the activation from this position is highly influential on the model's compliance (equivalently, refusal) decision on this pair of inputs \cite{wang2023interpretability, heimersheim2024use}.

\paragraph{Chat Template.}
\input{Figures/fig1}

To encode necessary information about roles and interaction turns in the input, existing LLMs employ a predefined chat template to format user inputs and model outputs.
\Cref{fig:chat_template} shows an example chat template, where a user's instruction (spanning positions \(1\) to \(S\)) is enclosed between special tokens - one indicating the beginning of user input, and another indicating both its end and the start of the LLM's response (positions \(S+1\) to \(T\)).
Due to the causal attention mechanism of LLMs, the beginning of the template positioned before the user's instruction does not incorporate any information from the instruction. Therefore, our analysis focuses on the ending part of the template, which we refer to as the \textit{template region}.
