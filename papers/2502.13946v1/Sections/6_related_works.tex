\section{Related Works}

\paragraph{Safety Vulnerabilities of Aligned LLMs.} 

Although significant research has focused on aligning LLMs to develop safety mechanisms enabling them to reject harmful requests \cite{bai2022training, dai2024safe, ji2024beavertails, bai2022constitutional}, recent studies show these safety mechanisms remain vulnerable \cite{wei2023jailbroken, qi2024finetuning, wei2024assessing}.
%
These vulnerabilities enable attacks on aligned LLMs during inference through jailbreak prompts, which are typically crafted through manual design \cite{wei2023jailbroken}, iterative refinement with LLM feedback \cite{chao2023jailbreaking, mehrotra2024tree}, and optimization via gradient or heuristic methods \cite{zou2023universal, liu2024autodan, liao2024amplegcg}
%
Such attacks exploit two key characteristics of aligned LLMs - the competition between helpfulness and harmlessness objectives \citep{wei2023jailbroken,ortu2024competition, anil2024manyshot}, and superficial alignment \cite{zhang2024dissecting, lin2024unlocking, li2024superficial, qi2024safety}.
%
Compared to previous studies, our work identifies an underexplored characteristic of aligned LLMs: their over-reliance on the template region for safety-related decisions. This dependency introduces a new attack surface, exposing the limitations of current alignment strategies.
%While prior research has focused on these vulnerabilities, our work identifies a previously overlooked characteristic of aligned LLMs: their over-reliance on the template region for safety-related decisions. This dependency introduces a new attack surface, further exposing the limitations of current alignment strategies.
%When helpfulness overcomes safety constraints, safety mechanisms fail, while superficial alignment concentrates distribution adjustments in response beginnings, making them vulnerable to manipulation.
%
%Our work reveals a new vulnerability characteristic of aligned LLMs - their general over-reliance on information from the template region for safety decisions.


\paragraph{Mechanistic Interpretability for LLM Safety.} 

Mechanistic Interpretability (MI) aims to reverse-engineer specific model functions or behaviors to make their internal workings human-interpretable. 
This research examines various components like individual neurons~\citep{gurnee2023finding, stolfo2024confidence}, representations~\citep{marks2024geometry,gurnee2024language}, and larger functional units such as MLPs~\citep{geva2021transformer, geva2022transformer} and attention heads~\citep{mcdougall2023copy, gould2024successor}.
Building on this foundation, recent research has leveraged MI to understand and enhance LLM safety \cite{bereska2024mechanistic}.
One line of research analyzes safety behaviors at the representation level and explores ways to manipulate safety-related representations \cite{leong2023self, zou2023representation, arditi2024refusal, cao2024nothing, lee2024programming, li2024rethinking, shen2024jailbreak}. Another investigates components directly connected to safety, such as neurons \cite{chen2024finding}, attention heads \cite{zhu2024locking, zhou2024role}, or MLPs \cite{lee2024mechanistic, luo2024jailbreak}. 
Some researchers examine specific aspects like safety-related parameters \cite{wei2024assessing, yi2024nlsr} or the risks to safety mechanisms during fine-tuning \cite{li2024safety, leong2024no}. 
Decomposing representations into interpretable sparse features enables automated explanations of safety mechanisms \cite{kirch2024features, templeton2024scaling} and suggests promising directions for achieving more effective safety alignment at representation levels \cite{liu2024aligning, yin2024direct, zou2024improving, rosati2024representation}.
% Additionally, decomposing representations into interpretable sparse features enables automated explanations of safety mechanisms \cite{kirch2024features, templeton2024scaling}.


