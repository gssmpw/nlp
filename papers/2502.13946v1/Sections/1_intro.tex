\section{Introduction}


\input{Figures/fig0}

Large language models (LLMs) are trained using safety alignment techniques and guided by ethical principles to ensure their interactions with users remain safe and helpful \cite{bai2022training, dai2024safe, ji2024beavertails, bai2022constitutional}.
These alignment methods enable LLMs to identify and decline potentially harmful or unethical queries.
Recent studies \cite{zhang2024dissecting, lin2024unlocking, li2024superficial} have revealed that safety alignment in LLMs is often superficial, where the alignment adapts a model's generative distribution primarily over its beginning output tokens \cite{qi2024safety}.
This excessive focus on specific regions introduces vulnerabilities: adversarially optimized inputs \cite{zou2023universal, chao2023jailbreaking, liao2024amplegcg} or carefully crafted jailbreak prompts \cite{wei2023jailbroken, shen2024anything} targeting a model's initial behavior can easily bypass safety mechanisms, undermining the model's ability to maintain safety. 
However, the root causes of these vulnerabilities remain unclear, making it difficult to develop effective alignment strategies to address them.


Existing aligned LLMs commonly incorporate a specific template inserted between the user's input instruction and the model's initial output \cite{touvron2023llama,jiang2023mistral,team2024gemma}, encoding essential role information in structuring interactions with users.
As illustrated in \Cref{fig:intro}, the template for a safety-tuned LLM remains fixed, regardless of the input instruction. 
Positioned immediately before the model's initial output, this template region aggregates information from the input and facilitates the critical transition from understanding instructions to generating responses. Due to its pivotal position, the template region serves as a potential anchor point for safety-related decision-making.
We hypothesize that LLMs' safety mechanisms may inadvertently take shortcuts to the tokens in the template region, relying too heavily on its aggregated information to assess the harmfulness of the input. We refer to this issue as \textbf{Template-Anchored Safety Alignment (TASA)}, which leads to safety-related vulnerabilities. Specifically, jailbreak attacks that simply manipulate the model's interpretation of the input via instructions can exploit this reliance to bypass safeguards and generate harmful responses.
To thoroughly analyze TASA and its implications, our work is divided into the following three phases.





First, we conduct comprehensive experiments to verify that TASA is widespread across various safety-tuned LLMs (\Cref{sec:rq1}).
Our findings reveal that these models tend to shift their attention from the \textit{instruction} region to the \textit{template} region when processing harmful requests. Further analysis confirms that this shift is systematic rather than coincidental: models consistently rely more on the information from the template region when making safety-related decisions. Specifically, we observe that interventions in intermediate states derived from the template region, compared to the instruction region, significantly increase the likelihood of initial compliance decisions.


Second, we establish a strong connection between TASA and inference-time vulnerabilities (\Cref{sec:rq2}). To investigate this, we perform interventions exclusively in the template region during the model's response generation to harmful inputs. Notably, these interventions prove highly effective at inducing LLMs to comply with harmful requests, even without altering instructions. Furthermore, by probing harmfulness features across layers and positions within the template region, we observe that common inference-time attacks cause significant interferences in these positions. This finding explains how such attacks exploit TASA to compromise model safety.



Third, we demonstrate that safety mechanisms anchored in the template region can be detached during response generation, enhancing the robustness of a model's safety (\Cref{sec:rq3}). This approach stems from our observation that harmfulness probes trained on template positions in specific layers can be directly transferred to identify harmful outputs during response generation. By leveraging these probes, we can detect harmful content in inference and steer activations to mitigate interference from attacks. Our experiments validate that this method is both simple and effective, showing a significant reduction in attack success rates.

In summary, this work investigates template-anchored safety alignment (TASA), a pervasive yet under-explored phenomenon in LLMs. We uncover its connection to inference-time vulnerabilities and propose initial strategies to alleviate this issue. Our findings highlight the importance of future safety alignment in developing more robust techniques that reduce models' reliance on potential shortcuts.

