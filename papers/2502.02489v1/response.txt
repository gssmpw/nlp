\section{Related Work}
\subsection{Deep Learning for Segmentation in US B-mode Imaging}
AI techniques in US B-mode imaging are typically centred around DL model development within a supervised learning framework, often tailored to a specific clinical application. SOTA approaches for segmentation in US imaging have developed from pure convolutional approaches using CNNs. CNN-based approaches often use variants of U-Net**Ronneberger et al., "U-Net: Deep Learning for Biological Image Segmentation"**. **Shareef et al., "Efficient Boundary Detection in Ultrasound Images with a Novel Encoder-Decoder Network"** introduced the ESTAN network aimed at improving small breast tumour segmentation. ESTAN uses two encoder branches with different kernel shapes and sizes and three skip connections to improve multi-scale contextual information. **Banerjee et al., "Segmentation of Lumbar and Thoracic Bony Features Using Inception Blocks in Convolutional Neural Networks"** proposed SIU-Net for segmenting lumbar and thoracic bony features using their proposed inception block. This block uses multiple filter sizes with improved computational efficiency through combined $1\times1$ and $3\times3$ convolutions. They also combined features of multiple scales through dense skip connections. **Meshram et al., "Dilated Convolutional Layers in U-Net for Carotid Plaque Segmentation"** improved carotid plaque segmentation using dilated convolutional layers in a U-Net model and **Qiao et al., "Fetal Skull Segmentation Using Dilated Convolution and Squeeze Excitation Blocks"** used dilated convolution and squeeze excitation blocks on skip connections to improve fetal skull segmentation.

With advancements in transformer-based architectures, the latest SOTA approaches for US segmentation combine transformer and convolutional methods leveraging complementary global and local feature information, respectively. **Zhang et al., "A CNN-Transformer Combination in a U-Net Framework for Breast Ultrasound Image Segmentation"** used a CNN-Transformer combination in a U-Net framework. The authors used a ResNet backbone and a novel local-global transformer block nested into skip connections to capture long-range feature information efficiently for breast US segmentation. **Jiang et al., "US-Seg: A CNN-Transformer U-Net Model for Ultrasound Image Segmentation"** also explored a CNN-Transformer U-Net model to improve US segmentation of the breast, thyroid and left ventricle. The authors used a coordinate residual block to extract local feature information with absolute position information and enhanced channel self-attention blocks to extract global features. **Wu et al., "BUSSeg: A Parallel Bi-Encoder in a U-Net Style Architecture for Breast Ultrasound Image Segmentation"** introduced a BUSSeg model for breast US segmentation by using a parallel bi-encoder in a U-Net style architecture consisting of a transformer and CNN blocks. In addition, the authors use a cross-image dependency module to capture cross-image long-range dependencies utilising feature memory banks. 
% All these approaches were trained on 100\% training samples, i.e., requiring larger labelled training data. Also, they often require higher computational training and inference time due to the combined transformer and convolutional approach.
% This module provides additional contextual information from stored image features in the memory bank and used to augment current image features through feature aggregation, in turn improving segmentation performance. 


% A novel cross-attention block was used to capture multi-scale features from different layers.A spatial and channel-based attention module was also used in parallel to strengthen skip connections.
% Delete--
% This approach provides an extension to CNN-Transformer architectures by combining intra and inter-image feature representations. This additional contextual information is valuable for improving segmentation performance. --
%
These supervised learning models have demonstrated promising segmentation results across various US clinical domains. However, due to the limited availability of US data, self-supervised learning (SSL) offers a more robust approach, enabling high segmentation performance while minimizing performance degradation when applied to out-of-distribution data. Also, combined transformer and convolutional approaches**Zhang et al., "A CNN-Transformer Combination in a U-Net Framework for Breast Ultrasound Image Segmentation"**, often require higher computational training and inference time that hinders clinical translation, whereas SSL techniques are independent of the model choice and can boost performance significantly.

\subsection{Self-Supervised Learning}
Self-supervised learning often follows a two-stage training strategy. Firstly, pretext learning is focused on learning representations from unlabelled data. Secondly, these learnt weights are then used in the fine-tuning downstream supervised learning tasks, such as segmentation. The objective is to learn semantically meaningful feature representations without requiring labels, thereby improving the performance of a downstream task on limited labelled datasets. Since the labels are not required during the pretext task, a large number of available unlabelled samples can be used which makes the SSL approach more generalisable to out-of-distribution samples. 

The pretext learning task is critical for developing meaningful representations of the target domain in SSL**Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**. Often a combination of image augmentation benefits contrastive SSL pretext learning, with this unsupervised learning stage also benefiting from stronger augmentation than supervised learning**He et al., "Contrastive Multiview Coding for Lossy Image Compression"**. For example, geometric rotation transformation was applied to an image in**Meng et al., "Geometric Transformation Based Contrastive Learning for Medical Image Analysis"** while the Jigsaw pretext task with a set of shuffled patches within an image was used in**Caron et al., "Unsupervised Learning of Visual Features by Culling Unreliable Outputs"**. The Jigsaw approach provides a strong geometric transformation to an image, a common strategy used in contrastive SSL**Misra et al., "Self-Supervised Learning of Pretext-Invariant Representations"**. The traditional Jigsaw approach learns a representation that is covariant to the perturbation, the pretext-invariant representation learning (PIRL)**Beery et al., "Learning by Forgetting for Deep Memory Augmented Neural Networks"** approach adopts the Jigsaw task in an invariant learning strategy. 
% Our work builds upon the PIRL approach.

Several SSL approaches have been established involving generative, contrastive and generative-contrastive techniques applied to medical image analysis**Gidaris et al., "Self-Supervised Representation Learning for Medical Image Analysis"**. The pretext learning strategy differs for each approach with a generative task focused on reconstruction, for example, recovering masked areas of an image**Anoosheh et al., "Reconstruction-based Contrastive Learning for Unsupervised Image-to-Image Translation"**. However, the contrastive approach aims to discriminate similar and dissimilar samples.

Contrastive approaches are often preferred to generative approaches for downstream discriminative applications**Tian et al., "Contrastive Multiview Coding for Lossy Image Compression"**. By avoiding low-level abstraction objectives, such as pixel-level reconstruction, contrastive learning tends to be more lightweight, as it does not require complex network architecture and is suitable to capture subtle differences in pathological and normal. 

Our work focuses on contrastive learning because it excels in discriminative downstream applications, is more lightweight during pretext learning, and favours high-level abstract feature learning compared to generative approaches**Zhang et al., "A Simple Framework for Contrastive Learning of Visual Representations"**. With contrastive learning dependent upon data transformations in the pretext learning task, we utilise a combination of data-specific augmentations, shown to improve SSL feature learning**Meng et al., "Geometric Transformation Based Contrastive Learning for Medical Image Analysis"**. In this work, we explore novel combined spatial and frequency-based augmentation strategies aimed at US images to improve representation learning in US data. Furthermore, inspired by relation networks**Gidaris et al., "Self-Supervised Representation Learning for Medical Image Analysis"** as a metric learning technique, we utilise relation networks and propose a novel relation contrastive loss (RCL) in a contrastive learning setting. To further guide representation learning, 
we propose to combine RCL with perceptual loss to weight feature learning with high-level features tackling high noise and poor contrast of US images. 
%
% With US images susceptible to high noise and poor contrast, low level pixel information is less informative, compared to higher level structural information within the image. Our improvements within a contrastive learning framework are targeted to maximize learning meaningful feature representations for US B-mode images.
% Therefore, enabling sample similarity to be determined in a data-driven manner rather than using a fixed distance-based similarity metric, like cosine similarity, commonly used in contrastive learning. 
%
%
% We propose a novel self-supervised framework aimed at improved segmentation performance in US B-mode image data.