\documentclass{article} %

%
\def\preprint{}

\ifdefined\submission
  \usepackage{icml2025}
\else
  \ifdefined\preprint
    \usepackage[accepted]{icml_preprint}
  \else
    \usepackage[accepted]{icml2025}
  \fi
\fi



\usepackage{times}
%
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{bbm}

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{microtype}
\usepackage{graphicx}
%
\usepackage{booktabs, makecell, multirow} %
\usepackage{hhline}
\usepackage{ulem}
\usepackage{xspace}
\usepackage{subcaption}
\usepackage{soul}

%
\usepackage[most]{tcolorbox}
\tcbuselibrary{skins, breakable}
%
\usepackage{etoolbox}
\AtBeginEnvironment{tcolorbox}{\small}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    filecolor=blue,
    urlcolor=blue,
}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}
%

\newcommand{\rmref}{\mathrm{ref}}
\newcommand{\rmpt}{\mathrm{pt}}
\newcommand{\rmft}{\mathrm{ft}}
\newcommand{\rmtrue}{\mathrm{true}}
\newcommand{\eft}{\mathrm{EFT}}
\newcommand{\bc}{\mathrm{BC}}
\newcommand{\TV}{\mathrm{TV}}
\newcommand{\crossentropy}{\mathrm{CE}}
\newcommand{\WIP}[1]{\textcolor[HTML]{00a000}{{#1}}}
\newcommand{\TODO}[1]{\textcolor{red}{(TODO: {#1})}}
\newcommand{\COMMENTOUT}[1]{}
%
%
\newcommand{\textbad}[1]{\textcolor{red}{#1}}
\newcommand{\textgood}[1]{\textcolor{green!60!black}{#1}}
\newcommand{\texteven}[1]{\textcolor{blue!90!black}{#1}}
\newcommand{\wt}{\widetilde}

\definecolor{ptcolor}{HTML}{ff7f0e}
\definecolor{prtcolor}{HTML}{1f77b4}
\definecolor{eftcolor}{HTML}{2ca02c}
\definecolor{ftcolor}{HTML}{9467bd}

\allowdisplaybreaks

\begin{comment}
\author{
   {\authorsize\bf
      Daiki Chijiwa$^{*}$$^{\dag}$ \quad
      Taku Hasegawa$^{*}$$^{\ddag}$ \quad
      Kyosuke Nishida$^{\ddag}$ \quad
   }\vspace{6pt}\\
   {\authorsize\bf
      Kuniko Saito$^{\ddag}$ \quad
      Susumu Takeuchi$^{\dag}$
   }\vspace{9pt}\\
   \raisebox{\ht\strutbox}{\hypertarget{affil1}{}}{\normalsize
       $\dag$ NTT Computer and Data Science Laboratories, NTT Corporation
   }\vspace{1.5pt}\\
   \raisebox{\ht\strutbox}{\hypertarget{affil2}{}}{\normalsize
      $\ddag$ NTT Human Informatics Laboratories, NTT Corporation
   }
}
\end{comment}

%
%
%
%
%
%
%
\date{}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\model}{\textsc{Code~Llama}\xspace}
\newcommand{\codellamavtwo}{\textsc{Code~Llama~2}\xspace}
\newcommand{\codellama}{\textsc{Code~Llama}\xspace}
\newcommand{\codellamainstruct}{\textsc{Code~Llama~-~Instruct}\xspace}
\newcommand{\codellamapython}{\textsc{Code~Llama~-~Python}\xspace}
\newcommand{\starcoder}{\textsc{StarCoder}\xspace}
\newcommand{\llama}{\textsc{Llama}\xspace}
\newcommand{\llamavtwo}{\textsc{Llama~2}\xspace}
\newcommand{\chatllama}{\textsc{Llama~2~Chat}\xspace}
\newcommand{\llamavone}{\textsc{Llama~1}\xspace}



\icmltitlerunning{Portable Reward Tuning}

\begin{document}

\twocolumn[
%
%
\icmltitle{Portable Reward Tuning: \\ Towards Reusable Fine-Tuning across Different Pretrained Models}

%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Daiki Chijiwa}{equal,cdlab}
\icmlauthor{Taku Hasegawa}{equal,hulab}
\icmlauthor{Kyosuke Nishida}{hulab}
\icmlauthor{Kuniko Saito}{hulab}
\icmlauthor{Susumu Takeuchi}{cdlab}
\end{icmlauthorlist}

\icmlaffiliation{cdlab}{NTT Computer and Data Science Laboratories, NTT Corporation}
\icmlaffiliation{hulab}{NTT Human Informatics Laboratories, NTT Corporation}

\icmlcorrespondingauthor{Daiki Chijiwa}{daiki.chijiwa@ntt.com}
\icmlcorrespondingauthor{Taku Hasegawa}{taku.hasegawa@ntt.com}

%
%
%
%

\vskip 0.3in
]

%
\printAffiliationsAndNotice{\icmlEqualContribution} %


\begin{abstract}
While foundation models have been exploited for various expert tasks through fine-tuning, any foundation model will become outdated due to its old knowledge or limited capability.
Thus the underlying foundation model should be eventually replaced by new ones, which leads to repeated cost of fine-tuning these new models.
Existing work addresses this problem by inference-time tuning, i.e., modifying the output probabilities from the new foundation model with the outputs from the old foundation model and its fine-tuned model, which involves an additional overhead in inference by the latter two models.
In this paper, we propose a new fine-tuning principle, Portable Reward Tuning (PRT), that reduces the inference overhead by its nature, based on the reformulation of fine-tuning as the reward maximization.
Specifically, instead of fine-tuning parameters of the foundation models, PRT trains the reward model explicitly through the same loss function as in fine-tuning.
During inference, the reward model can be used with any foundation model (with the same set of vocabularies or labels) through the formulation of reward maximization.
Experimental results, covering both vision and language models, demonstrate that the PRT-trained model can achieve comparable accuracy to the existing work of inference-time tuning, with less inference cost.
\end{abstract}


\section{Introduction}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.78\linewidth,height=4cm]{figs/portable_reward_tuning_cropped.pdf}
    \caption{An overview of our approach of portable reward tuning (PRT) compared with the previous work of inference-time tuning, emulated fine-tuning (EFT). In training phase, we tune the reward model $r_\theta(x,y)$ instead of tuning a given pretrained model, through the same loss and dataset, which leads to the reduced cost in inference with another pretrained model.}
\end{figure*}


Foundation models, or simply pretrained models, play a central role in the recent development of artificial intelligence~\citep{bommasani2021opportunities}.
They are typically large-scale neural networks pretrained on massive amounts of data from the Internet, which makes them generalizable to various downstream tasks, such as CLIP~\citep{radford2021learning} for visual recognition and \llama-series~\citep{touvron2023llama,touvron2023llama2} for language generation.
Even though foundation models are somewhat already capable of handling downstream tasks, we can further bring out their potential ability by {\it fine-tuning} for each task, i.e., additional optimization of the foundation models on supervised data.
Furthermore, fine-tuning can also be used for aligning the behavior of foundation models to follow human instructions or preferences, by instruction tuning or reinforcement learning from human feedback (RLHF;~\citet{christiano2017deep}).

Although fine-tuning has been the standard principle for tuning foundation models, there still remains an overlooked problem in the long term: the underlying foundation models should be eventually replaced by other (often newer) foundation models for various reasons, such as their outdated knowledge or limited capability, which requires the new foundation models to be fine-tuned on every task again.
For example, (i) in the case of open-source CLIP models~\citep{schuhmann2021laion,schuhmann2022laion}, an early model was initially trained on a dataset with 400 million image-text pairs, but later it was replaced by new versions trained on 2 billion and 5 billion pairs; (ii) in the case of the \llama series, both major updates (e.g., \llama-2 to \llama-3) and minor updates (e.g., \llama-3.1 to \llama-3.2) have occurred.
Every time such an update occurs, one may have to fine-tune the new foundation model again, which poses an important issue of the repeated training cost.

One promising approach for this challenge is {\it inference-time tuning}~\citep{mitchell2024emulator,liu2024tuning}, which emulates fine-tuning of the new foundation model even with a possibly different architecture, by mixing three output probabilities from the old foundation model, its fine-tuned model, and the new foundation model.
More specifically, the approach first defines an {\it implicit reward} for each task as the logarithmic density ratio of the output probabilities from the (old) foundation and its fine-tuned models.
Then, it reinterprets fine-tuning as maximization of the implicit reward with penalizing the deviation from the foundation model in Kullback-Leibler (KL) divergence, whose closed-form solution~\citep{ziebart2010modeling} can be computed efficiently and coincides with the fine-tuned model itself in this formulation.
Based on this viewpoint, inference-time tuning is performed by solving the reward maximization, with the foundation model in the KL penalty replaced by the new foundation model.
This approach is promising since it can avoid the repeated fine-tuning associated with replacement of foundation models,
but instead it introduces another issue of inference cost, i.e., requiring three models to run at each inference step.

%
%
%
In this paper, we propose a new principle as an alternative to fine-tuning, called {\it Portable Reward Tuning} (PRT),  that is more suitable for inference-time tuning.
The proposed principle is straightforward from the above viewpoint of fine-tuning as reward maximization: we introduce an auxiliary model as an {\it explicit reward}, and redefine a fine-tuned model, which we call a PRT model, as the closed-form solution that maximizes the explicit reward with KL regularization to a given foundation model.
For training, instead of directly optimizing the foundation model itself, the explicit reward model is trained so that the corresponding PRT model minimizes the same cross-entropy loss as in usual fine-tuning.
For inference, the PRT model is (re-)constructed from a pair of the explicit reward model and a given (possibly different) foundation model to serve as the corresponding fine-tuned model.
By its nature, PRT enables us to freely update the underlying foundation model only with a single model overhead, while the existing methods based on standard fine-tuning still require additional two models to compute the implicit reward for inference-time tuning.

Our contributions are as follows:
\begin{itemize}
    \item We derive portable reward tuning (PRT) as an alternative to conventional fine-tuning, and established its basic theoretical properties including (i) a natural interpretation of its training objective as reward learning, (ii) evaluation of how the PRT model changes its behavior by replacement of the underlying foundation model in terms of KL divergence, (iii) generalization analysis from the PAC-Bayesian perspective.
    \item Using both vision and language models, we conduct experiments on inference-time tuning in realistic scenarios like updating the pretrained knowledge and up-scaling the size of foundation models. We confirmed that PRT models can achieve comparable accuracy to the baseline of emulated fine-tuning,
    even with less inference cost in terms of both speed and memory usage.
    %
\end{itemize}




%
%
%
%


\section{Background}

In this section, we summarize the background of this work.
In Sec.~\ref{sec:kl-regularized reward maximization}, we review the basics of KL-regularized Reward Maximization from the literature of Decision-Making Theory and Reinforcement Learning.
In Sec.~\ref{sec:emulated fine tuning}, we briefly explain previous work on inference-time tuning with its formulation based on reward maximization.


\subsection{KL-Regularized Reward Maximization}\label{sec:kl-regularized reward maximization}

Let $\mathcal{S}$ be a state space, $\mathcal{A}$ be an action space, and a policy model $\pi(a|s)$ be the probability that the action $a\in\mathcal{A}$ is chosen by some probabilistic mapping $\mathcal{S}\to\mathcal{A}$ given the state $s\in\mathcal{S}$.
Furthermore, we consider a reward function $r(s,a):\mathcal{S}\times\mathcal{A}\to \mathbb{R}$ and a reference model $\pi_\mathrm{ref}(a|s)$.
Then the goal of KL-regularized reward maximization is to maximize the expected reward $\mathbb{E}_{a\sim \pi(a|s)}[r(s,a)]$ with a soft constraint that keeps $\pi(a|s)$ close to the reference model $\pi_\rmref(a|s)$, in the sense of KL divergence, as follows:
\begin{align}
\max_{\pi(\text{-}|s)} \  &\mathbb{E}_{a\sim\pi(a|s)}\left[r\left(s,a\right)\right] \nonumber\\
&- \lambda \KL\left(\pi(\text{-}|s) \parallel \pi_\rmref(\text{-}|s)\right),\label{eq:objective in KL-regularized reward maximization}
\end{align}
where $\KL(\pi(\text{-}|s) \parallel \pi_\rmref(\text{-}|s)) := \sum_a \pi(a|s)\log(\pi(a|s)/\allowbreak\pi_\rmref(a|s))$.
The closed-form solution of this problem is well-known~\citep{ziebart2010modeling,korbak2022rl} as an extension of maximum entropy principle~\citep{ziebart2008maximum}, given by
\begin{equation}\label{eq:closed solution in kl-regularized reward maximization}
    \pi(a|s) = \frac{1}{Z(s)} \pi_\rmref(a|s) \exp\left( \frac{1}{\lambda} r(s,a) \right),
\end{equation}
%
where $Z(s) := \sum_{a\in\mathcal{A}} \pi_\rmref(a|s) \exp\left( \frac{1}{\lambda} r(s,a) \right)$ is the normalization factor.

\subsection{Inference-Time Tuning}\label{sec:emulated fine tuning}

%
%
%
%


Let $\pi_\theta(y|x)$ be a classification model defined as $\pi_\theta(y|x) := \softmax(f(x;\theta))$, where $x\in\mathcal{X}$ is an input, $y\in\mathcal{Y}=\{c
_1,\cdots,c_L\}$ a classification label,  $f(x;\theta) \in \mathbb{R}^L$ a neural network parameterized by $\theta\in\mathbb{R}^N$, and the $\softmax$ function\footnote{$\softmax(y_1, \cdots, y_L) := (e^{y_1} / \sum_i e^{y_i}, \cdots, e^{y_L} / \sum_i e^{y_i})$}.
Let $\pi_\mathrm{pt}(y|x) := \pi_{\theta^\mathrm{pt}}(y|x)$ be a pretrained model, and $\pi_\mathrm{ft}(y|x) := \pi_{\theta^\mathrm{ft}}(y|x)$ its fine-tuned model on some specific task.
Typical examples  include (i) in image classification, $x$ is an image, $y$ is its corresponding label and $f(x;\theta)$ is a CNN~\cite{bengio2017deep} or Vision Transformer~\cite{dosovitskiy2021an};
and (ii) in language generation, $x$ is a sequence of tokens $t_1\cdots t_{k}$, $y$ is the next token $t_{k+1}$ and $f_\theta(x)$ is a decoder-only Transformer~\cite{radford2018gpt1}.

Based on the theory of KL-regularized reward maximization, \citet{mitchell2024emulator} proposed an emulated fine-tuning (EFT) which views the fine-tuned model $\pi^\mathrm{ft}(y|x)$ as the solution of the following problem:
\begin{align}
    \max_{\pi(\text{-}|x)} \  & \mathbb{E}_{y\sim\pi(y|x)} \left[  \log\left( \frac{\pi_\mathrm{ft}(y|x)}{\pi_\mathrm{pt}(y|x)} \right)  \right] \nonumber\\
    &- \KL \big( \pi(\text{-}|x) \parallel \pi_\mathrm{pt}(\text{-}|x) \big),\label{eq:emulated fine-tuning}
\end{align}
where $\log\left( \pi_\mathrm{ft}(y|x) / \pi_\mathrm{pt}(y|x) \right)$ is called an {\it implicit reward}, expected to be the reward function that reflects the task-specific preference for $y\in\mathcal{Y}$.
Indeed,  by applying (\ref{eq:closed solution in kl-regularized reward maximization}), the closed-form solution $\pi(y|x)$ of (\ref{eq:emulated fine-tuning}) is given by 
\begin{align}
    \pi(y|x) &= \frac{1}{Z(x)} \pi_\mathrm{pt}(y|x)\exp\left( \log\left( \frac{\pi_\mathrm{ft}(y|x)}{\pi_\mathrm{pt}(y|x)} \right) \right) \nonumber\\
    &= \pi_\mathrm{ft}(y|x) \nonumber
\end{align}
Building on this fact, \citet{mitchell2024emulator} also proposed  what they call scale decoupling, which replaces the pretrained model in the KL constraint by different pretrained model from the one appeared in the implicit reward. In other words, they consider the following problem:
\begin{align}
    \max_{\pi(\text{-}|x)} \  & \mathbb{E}_{y\sim\pi(y|x)}  \left[ \log\left( \frac{\pi_\mathrm{ft}(y|x)}{\pi_\mathrm{pt}(y|x)} \right)  \right] \nonumber\\
    &- \KL \big( \pi(\text{-}|x) \parallel \widetilde{\pi}_\mathrm{pt}(\text{-}|x) \big),\label{eq:scale decoupling}
\end{align}
where $\widetilde{\pi}_\mathrm{pt}(\text{-}|x)$ is another pretrained model that is different from $\pi_\mathrm{pt}(\text{-}|x)$, possibly with different network architecture.
The closed-form solution of  (\ref{eq:scale decoupling}) can be considered as the emulation result of fine-tuning the new pretrained model $\widetilde{\pi}_\mathrm{pt}(y|x)$ through the implicit reward on the specific task.
Also, \citet{liu2024tuning} proposed almost the same approach called proxy-tuning.

\section{Portable Reward Tuning}

In this section, we develop a new fine-tuning framework, called portable reward tuning (PRT), with both training and inference algorithms based on KL-regularized reward maximization.
Throughout this section, we follow the same classification setting as in Sec.~\ref{sec:emulated fine tuning}, which includes both image classification and language generation tasks.


\subsection{Training of PRT}\label{section:training of PRT}

\paragraph{Setup.}
Let $r(x;\theta) = (r_1(x; \theta), \cdots, r_L(x; \theta)) \in \mathbb{R}^L$ be a neural network with $L$-dimensional outputs. 
We refer to the $i$-th component of $r(x;\theta)$ as the reward value, denoting $r_\theta(x, c_i) := r_i(x;\theta)$ for an input $x$ and the $i$-th label $c_i\in\mathcal{Y}$.
We also assume that a pretrained model $\pi_\mathrm{pt}$ and a dataset of input-label pairs $S = \{(x_1, y_1),\cdots,(x_{|S|}, y_{|S|}) \}$ for some specific task are given.

\paragraph{Formulation.}
In our PRT framework, we optimize the reward model $r_\theta(x,y)$, instead of directly optimizing the given pretrained model $\pi_\mathrm{pt}(y|x)$.
For a little while, assume that we already have the learned reward model $r_\theta(x,y)$ for the given specific task.
Then, the desired classification model $\pi_\theta(y|x)$ is defined as the solution of reward maximization with KL-constraint to the pretrained model:
\begin{align}\label{eq:reward maximization in portable-tuning}
    \max_{\pi(\text{-}|x)} \ &\mathbb{E}_{a\sim\pi(y|x)}\left[r_\theta(x,y)\right] \nonumber\\
    &- \lambda \KL\left(\pi(\text{-}|x) \parallel \pi_\mathrm{pt}(\text{-}|x)\right).
\end{align}
As we already discussed in Sec.~\ref{sec:kl-regularized reward maximization},  the closed-form solution for this maximization problem is provided by
\begin{align}\label{eq:closed solution in portable-tuning}
    \pi_\theta(y|x) = \frac{1}{Z_\theta(x)} \pi_\mathrm{pt}(y|x) \exp \left( \frac{1}{\lambda}r_\theta(x, y) \right),
\end{align}
where $Z_\theta(x) := \sum_y \pi_\mathrm{pt}(y|x) \exp(r_\theta(x,y) / \lambda)$ is the normalization factor.
We call this $\pi_\theta(y|x)$ a PRT model (for training) with the reward $r_\theta(x,y)$ and the pretrained model $\pi_\mathrm{pt}(y|x)$.

%
Although it is somewhat obvious, the following proposition guarantees that the above PRT models are equivalent to standard fine-tuned models in terms of their expressiveness:
\begin{proposition}
    There is a one-to-one correspondence between fine-tuned models and rewards, which preserves their accuracy:
    \begin{align}
        & \  \{ \pi_{\mathrm{ft}}(y|x): \text{ fine-tuned models} \} \nonumber\\
        &\to \{ r(y|x): \text{rewards satisfying } \mathbb{E}_{y\sim\pi_\mathrm{pt}(y|x)}[r(x,y)]=1  \} \nonumber
    \end{align}
    where $\pi_{\mathrm{ft}}(y|x)$ is mapped to the implicit reward $\log(\pi_{\mathrm{ft}}(y|x) / \pi_{\mathrm{pt}}(y|x))$.
\end{proposition}
\begin{proof}
The mapping preserves accuracy since the PRT model (\ref{eq:closed solution in portable-tuning}) with the implicit reward recovers the given model $\pi_\mathrm{ft}(y|x)$.
The invertibility also holds since the implicit reward of the PRT model (\ref{eq:closed solution in portable-tuning}) recovers the reward itself.
\end{proof}
In other words, if there is a fine-tuned model that achieves some accuracy, there is also the corresponding reward whose PRT model achieves the same accuracy.
Thus, the reparameterization in PRT (\ref{eq:closed solution in portable-tuning}) does not restrict its expressiveness, even compared to standard fine-tuning.

%
\paragraph{Training objective.}

The reward model $r_\theta(x,y)$ is trained by simply optimizing the same loss function $\mathcal{L}(\mathbf{p}, y^*)$ as in standard fine-tuning, with the true label $y^*$ for the input $x$ and the output distribution $\mathbf{p}:=\pi_\theta(\text{-}|x)$ of the PRT model.
In particular, throughout this paper, we minimize the cross-entropy loss $\mathcal{L}(\mathbf{p}, y^*) := \crossentropy(\mathbf{p}, y^*) := -\log\pi_\theta(y^*|x)$ over a given dataset $S \subset \mathcal{X}\times\mathcal{Y}$ to train the reward model:
%
\begin{align}
    &\argmin_\theta \frac{1}{|S|} \sum_{(x,y^*)\in S} \mathcal{L}(\mathbf{p}, y^*) \label{eq:loss for reward model}\\
    &= \argmax_\theta \sum_{(x,y^*)\in S} \log \pi_\theta(y^*|x) \nonumber\\
    &= \argmax_\theta \sum_{(x,y^*)\in S} r_\theta(x,y^*) -  V_\theta(x), \label{eq:objective for reward model}
\end{align}
where $V_\theta(x) := \lambda\log Z_\theta(x)$.
Notably, this maximization can be reinterpreted in terms of reward training.
Indeed, by applying Jensen's inequality, we obtain
\begin{align}
    &r_\theta(x,y^*) -  V_\theta(x) \nonumber\\
    &= r_\theta(x,y^*) - \lambda \log \mathbb{E}_{y\sim\pi_\mathrm{pt}(y|x)} \exp(r_\theta(y|x)/\lambda) \nonumber\\
    &\leq r_\theta(x,y^*) - \lambda  \mathbb{E}_{y\sim\pi_\mathrm{pt}(y|x)} \log \exp(r_\theta(y|x)/\lambda) \nonumber\\
    &= r_\theta(x,y^*) - \mathbb{E}_{y\sim\pi_\mathrm{pt}(y|x)} r_\theta(y|x).
\end{align}
Therefore, {\it maximization of (\ref{eq:objective for reward model}), i.e., training the reward model using the cross-entropy loss, leads to an increase in the reward for the ground-truth  $y^*$ while decreasing rewards for the average outcomes $y$  from the pretrained model $\pi_\mathrm{pt}(y|x)$.}
This interpretation can be seen as analogous to the training of the Bradley-Terry reward model in RLHF~\citep{christiano2017deep} with pairs of preferred-dispreferred sentences, where the reward model learns to evaluate the preferred one higher than the dispreferred one.


\begin{algorithm}[tb]
    \caption{Pseudocode for Training of PRT}
    \label{alg:Training of PRT}
 \begin{algorithmic}[1]
    \STATE {\bfseries Given:} training data $S=\{(x_1, y_1), \cdots, (x_m, y_m)\}$,\\
    \STATE \quad a reward model $r(x;\theta)$,
    \STATE \quad a pretrained model $\pi_\rmpt(\text{-}|x)=\softmax(f(x;\theta_\rmpt))$.
    \STATE Initialize $\theta$.
    \FOR{$i=1,...,m$}
    \STATE $\mathbf{v}_\theta \leftarrow \log\softmax(f(x_i;\theta_\rmpt)) + r(x_i; \theta)$.
    \STATE $\mathbf{p}_\theta \leftarrow \softmax(\mathbf{v}_\theta)$
    \STATE $\mathcal{L}_\theta \leftarrow \crossentropy(\mathbf{p}_\theta, y_i)$
    \STATE Update $\theta$ with the gradient of $\mathcal{L}_\theta$.
    %
    %
    %
    %
    \ENDFOR
    \STATE {\bfseries return} $\theta$.
 \end{algorithmic}
 \end{algorithm}
 \begin{algorithm}[tb]
    \caption{Pseudocode for Inference of PRT}
    \label{alg:Inference of PRT}
 \begin{algorithmic}[1]
    \STATE {\bfseries Given:} an input $x$, the trained reward $r(x;\theta)$,
    \STATE \quad a pretrained model $\wt{\pi}_\rmpt(y|x)=\softmax(\wt{f}(x;\wt{\theta}_\rmpt))$,
    \STATE $\wt{\mathbf{v}} \leftarrow \log\softmax(\wt{f}(x;\wt{\theta}_\rmpt)) + r(x; \theta)$.
    \STATE $\wt{\mathbf{p}} \leftarrow \softmax(\wt{\mathbf{v}})$
    \STATE {\bfseries return} $\wt{\mathbf{p}}$ as the output probability conditioned by $x$.
 \end{algorithmic}
 \end{algorithm}


\paragraph{Implementation Details}
Algorithm~\ref{alg:Training of PRT} presents the pseudocode for training PRT models.
The training data is a set of input-label pairs just like in standard training.
For the reward model $r(x;\theta)$, although it can be an arbitrary neural network model, we assume that it is modeled using the same network architecture as the pretrained model $f(x;\theta_\rmpt)$ and that $\theta$ initialized with $\theta_\rmpt$ throughout this paper.
Lines 6-7 compute the PRT model (\ref{eq:closed solution in portable-tuning}) in the logit space to avoid numerical instability.
Here, we set the coefficient $\lambda=1$ in (\ref{eq:closed solution in portable-tuning}) since the reward model can  automatically learn the scaling factor.
%

\subsection{Inference of PRT}

Let $r_\theta(x,y)$ and  $\pi_\rmpt(y|x)$ be the reward and pretrained model introduced in Section~\ref{section:training of PRT}.
Let $\wt{\pi}_\rmpt(y|x)$ be another pretrained model whose label space $\mathcal{Y}$ (or vocabularies for language models) is the same as the one for $\pi_\rmpt(y|x)$.
Examples of $\wt{\pi}_\rmpt(y|x)$ include a model pretrained on a larger or more recent dataset than that of $\pi_\rmpt(y|x)$, and a pretrained model with more parameters.

The inference model $\wt{\pi}_\theta(y|x)$ for the reward $r_\theta(x,y)$ and the specified pretrained model $\wt{\pi}_\rmpt(y|x)$ can be derived by replacing $\pi_\rmpt(y|x)$ in (\ref{eq:closed solution in portable-tuning}) with $\wt{\pi}_\rmpt(y|x)$.
Specifically, given an input $x \in \mathcal{X}$, the prediction for its label $y$ is performed by the following model that maximizes the reward $r_\theta(x,y)$ while minimizing the deviation from the specified pretrained model $\wt{\pi}_\rmpt(y|x)$:
\begin{align}\label{eq:PRT model for inference}
    \wt{\pi}_\theta(y|x) &:= \argmax_{\pi(\text{-}|x)} \ \mathbb{E}_{a\sim\pi(y|x)}\left[r_\theta(x,y)\right] \nonumber\\
    & \qquad\qquad\qquad - \lambda \KL\left(\pi(\text{-}|x) \parallel \wt{\pi}_\mathrm{pt}(\text{-}|x)\right) \nonumber\\
    &=\frac{1}{\wt{Z}_\theta(x)}\wt{\pi}_\mathrm{pt}(y|x)  \exp \left( \frac{1}{\lambda} r_\theta(x, y) \right), 
\end{align}
where $\wt{Z}_\theta(x) := \sum_y \wt{\pi}_\mathrm{pt}(y|x) \exp(r_\theta(x,y) / \lambda)$.
The implementation of inference by this PRT model (\ref{eq:PRT model for inference}) is straightforward as described in Algorithm~\ref{alg:Inference of PRT}, with $\lambda=1$ as in training.

Now the following question naturally arises: How does the choice of $\wt{\pi}_\rmpt(y|x)$ affect the behavior of the inference model $\wt{\pi}(y|x)$?
Intuitively, if $\wt{\pi}_\rmpt(y|x)$ does not deviate from the original $\pi_\rmpt(y|x)$, the inference model $\wt{\pi}_\rmpt(y|x)$ also keeps to behave similarly to the training time.
This intuition can be formalized as follows:
\begin{proposition}\label{prop:evaluation of KL divergence}
    Suppose that $\wt{\pi}_\rmpt(y|x)$ is close to $\pi_\rmpt(y|x)$, i.e.,  $\KL\left(\pi_\rmpt(\text{-}|x) \parallel \wt{\pi}_\rmpt(\text{-}|x) \right) \leq \varepsilon$.
    Additionally, we assume that the maximum and mean value ratio of the exponential reward, i.e., $\max_y \exp r_\theta(x,y)/\mathbb{E}_y \exp r_\theta(x,y)$, is bounded by some constant $C$.
    Then, the PRT models $\pi_\theta(y|x)$  and  $\wt{\pi}_\theta(y|x)$ are also close as distributions:
    \begin{align*}
        \KL\left( \pi_\theta(y|x) \parallel \wt{\pi}_\theta(y|x) \right) \leq O(\sqrt{\varepsilon}).    
    \end{align*}
\end{proposition}
\begin{proof}
    See Appendix~\ref{app:section:proof of Proposition 3.2}.
\end{proof}



\subsection{A PAC-Bayesian Perspective}\label{sec:pac-bayesian perspective}

We can suppose that the pretrained models for both training and inference, i.e., $\pi_\rmpt(y|x)$ and $\wt{\pi}_\rmpt(y|x)$, are chosen from some distribution $\mathcal{P}$ over the set of pretrained models.
Then the PRT models for inference, $\wt{\pi}_\theta(y|x)$ combined with the sampled pretrained model $\wt{\pi}_\rmpt(y|x)\sim \mathcal{P}$, form a new distribution $\mathcal{Q}_\theta$.

%
This formulation of PRT models naturally fits into the PAC-Bayes framework established in~\citet{mcallester1999pac}, which enables us to analyze the generalization error of posterior distributions over  classifiers, in comparison to a fixed prior distribution.
Specifically in our setting, the pretrained distribution $\mathcal{P}$ can be seen as a prior distribution, and the PRT distribution $\mathcal{Q}_\theta$ as a posterior distribution.

Let $l(x, \pi)$ be a finitely bounded loss function, e.g., one that returns the error rate, for a given input $x$ and classifier $\pi$.
Assume that the input $x$ follows some distribution $\mathcal{D}$.
The following generalization bound can be obtained as a direct consequence of Theorem 1 in \citet{mcallester1999pac}:
\begin{proposition}
    Let $S=(x_1, \cdots, x_m)\sim \mathcal{D}^m$ be i.i.d. $m$ training samples from the data distribution $\mathcal{D}$.
    Then, with probability at least $1-\delta$, we have
    \begin{align}
        &\mathbb{E}_{\pi\sim\mathcal{Q}_\theta} \mathbb{E}_{x\sim\mathcal{D}}\big[l(x, \pi)\big] \leq \mathbb{E}_{\pi\sim\mathcal{Q}_\theta} \left[\frac{1}{m}\sum_i^m l(x_i, \pi)\right] \nonumber\\
        & \qquad + \sqrt{\frac{\KL(\mathcal{Q}_\theta\parallel \mathcal{P}) + \log(\frac{1}{\delta}) + \frac{5}{2}\log m + 8}{2m-1}},
    \end{align}
    for the posterior distribution $\mathcal{Q}_\theta$ with the reward $r_\theta$ trained on $S$.
\end{proposition}
The KL divergence $\KL(\mathcal{Q}_\theta\parallel \mathcal{P})$ is not computationally tractable because the pretrained distribution $\mathcal{P}$ itself is not tractable and also the underlying space of pretrained models is too vast.
Nevertheless, the result implies that the generalization capability of PRT models can be captured by the closeness of the PRT model $\wt{\pi}_\theta(y|x)$ compared to the underlying pretrained model $\wt{\pi}_\rmpt(y|x)$.

In particular, we can easily see that the KL divergence term vanishes if the reward $r_\theta(x, y)$ is a constant value given each $x$ and then the equality $\wt{\pi}_\theta(y|x)=\wt{\pi}_\rmpt(y|x)$ holds.
This can be seen as the case that the exponential distribution $\rho_\theta(y|x) := \exp(r_\theta(x,y)) / \sum_y \exp(r_\theta(x,y))$ maximizes its entropy.
Thus, the generalization capability can be further improved by regularizing the exponential distribution of the reward $r_\theta(x,y)$ during training of $r_\theta(x,y)$, while it may hurt the optimization quality instead.

The above analysis motivates us to consider the following regularization, which we call Entropy Maximization (EM),  to optionally enhance the generalization capability across various pretrained models:
\begin{align}
\wt{\mathcal{L}}(\theta) := \mathcal{L}(\theta) - \alpha \frac{1}{|S|} \sum_{(x,y)\in S} H(\rho_\theta(\text{-}|x)),
\end{align}
where $\mathcal{L}(\theta)$ is the original loss (\ref{eq:loss for reward model}) for training the reward model,  $H(\rho_\theta(\text{-}|x)) := -\sum_{y\in\mathcal{Y}} \rho_\theta(y|x) \log\rho_\theta(y|x)$ is the entropy of the exponential distribution $\rho_\theta(y|x)$, and $\alpha\in\mathbb{R}_{\geq 0}$ is the hyperparameter to control the regularization.
As we can see larger $\alpha$ enforces the reward $r_\theta(x,y)$ to be closer to some constant for each input $x\in\mathcal{X}$.


\section{Experiments}\label{sec:experiments}

In this section, we evaluate the performance of portable reward tuning (PRT) for inference-time tuning, with various pretrained models including both vision and language models.
The main comparison is between PRT and its corresponding baseline, emulated fine-tuning (EFT; \citet{mitchell2024emulator}), which differ only in whether they maximize an explicit or implicit reward during inference-time tuning.

\begin{figure*}[!t]
    \centering
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_CUB_nolegend.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_Flowers_nolegend.pdf}
        \end{minipage}
        \caption{\scriptsize ResNet-50 (OpenAI)}
        \label{fig:resnet50_openai}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_CUB_nolegend.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_Flowers_nolegend.pdf}
        \end{minipage}
        \caption{{\scriptsize ConvNext (LAION-400M)}}
        \label{fig:convnext_laion400m}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_CUB_nolegend.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_Flowers_nolegend.pdf}
        \end{minipage}
    \caption{\scriptsize ViT-B-16 (OpenAI)}
    \label{fig:vit-b-16_openai}
    \end{subfigure}
    \hfill    
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_CUB_nolegend.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_Flowers_nolegend.pdf}
        \end{minipage}
    \caption{\scriptsize ViT-B-16 (LAION-400M)}
    \label{fig:vit-b-16_laion400m}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_CUB_nolegend.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_Flowers_nolegend.pdf}
            \end{minipage}
    \caption{\scriptsize ViT-B-16 (LAION-2B)}
    \label{fig:vit-b-16_laion2b}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{Evaluations of inference-time tuned models for vision tasks. Each subcaption refers to the source pretrained model, and the labels in x-axis are target pretrained models. \textcolor{ptcolor}{\bf Pretrained} means the zero-shot classification by each target model as a baseline, and \textcolor{ftcolor}{\bf FT} means the fine-tuned target model as an oracle result.}
    \label{fig:vision_results}
\end{figure*}

%
\begin{figure*}[!t]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Llama-2-7b-hf.pdf}
            \includegraphics[width=\linewidth]{results_final/ifeval_average_Llama-2-7b-hf.pdf}
        \end{minipage}
    \caption{\scriptsize Llama2-7B}
    \label{fig:llama2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Llama-3.2-1B.pdf}
            \includegraphics[width=\linewidth]{results_final/ifeval_average_Llama-3.2-1B.pdf}
        \end{minipage}
    \caption{\scriptsize Llama3.2-1B}
    \label{fig:llama3}
    \vspace{-0.1cm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Qwen2.5-0.5B.pdf}
            \includegraphics[width=\linewidth]{results_final/ifeval_average_Qwen2.5-0.5B.pdf}
        \end{minipage}
    \caption{\scriptsize Qwen2.5-0.5B}
    \label{fig:qwen2.5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Falcon3-1B-Base.pdf}
            \includegraphics[width=\linewidth]{results_final/ifeval_average_Falcon3-1B-Base.pdf}
        \end{minipage}
    \caption{\scriptsize Falcon3-1B}
    \label{fig:falcon3}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{Evaluations of inference-time instruction-tuned models on GSM8k and IFEval benchmarks. Each subcaption refers to the source pretrained model, and the labels in x-axis are target pretrained models. \textcolor{ptcolor}{\bf Pretrained} means the zero-shot inference by each target model as a baseline, and \textcolor{ftcolor}{\bf Instruct} means the instruct-tuned target model as an oracle result.}
    \label{fig:llm_results}
    \vspace{-0.1cm}
\end{figure*}


\vspace{-0.2cm}
\paragraph{Setups for vision experiments.} 
(1) Pretrained models: We employed CLIP models~\citep{ilharco2021openclip} pretrained on various datasets including OpenAI’s proprietary dataset~\citep{radford2021learning}, LAION-400M~\citep{schuhmann2021laion}, LAION-2B~\citep{schuhmann2022laion}, and DataComp-1B~\citep{gadre2024datacomp}.
(2) Fine-tuning: For each fine-grained dataset, such as Cars~\citep{krause20133d} and CUB~\citep{WahCUB_200_2011}, we first constructed and fixed the classification layer of each pretrained model for zero-shot classification, and then fine-tuned (or reward-tuned) its feature extractor on the train set.
(3) Evaluation: We evaluated models on the test set of each dataset where the training set was used for tuning the models.

\vspace{-0.2cm}
\paragraph{Setups for language experiments.} 
(1) Pretrained models: We employed pretrained language models of the decoder-only Transformers such as \llama series~\citep{touvron2023llama2}, Qwen series~\citep{yang2024qwen2.5,yang2024qwen2} and Falcon series~\citep{Falcon3}. (2) Fine-tuning: We performed instruction tuning on these pretrained models with Tulu v2 dataset~\citep{ivison2023camels}, a large-scale dataset consisting of demonstrations for following given instructions.
(3) Evaluation: We evaluated models on downstream benchmarks, particularly on the GSM8k benchmark~\citep{cobbe2021training} for reasoning ability and IFEval benchmark~\citep{zhou2023instruction} for instruction-following ability.
%
%


\vspace{-0.1cm}
\subsection{Results}
\vspace{-0.1cm}

Figures~\ref{fig:vision_results}, \ref{fig:llm_results} and \ref{fig:qwen2-to-qwen2.5} show the  results of inference-time tuning using PRT and EFT, from a {\it source} pretrained model, i.e., the one used in tuning the (either explicit or implicit) reward, to a {\it target} pretrained model, i.e., the one never used in tuning the reward.
Here PRT refers to the vanilla one without regularization for fair comparison.
We also compare them with the zero-shot performance of pretrained models themselves as baselines, and with their fine-tuned performance as oracles.
Note that, in the case that the source and target are the same pretrained model (i.e., the leftmost one in x-axis), the results correspond to the ones without inference-time tuning.
Overall, these results support our main claim in this paper, i.e., PRT achieves comparable accuracy to EFT with less inference cost. (See also Section~\ref{sec:memory and speed analysis})

Particularly, the pairs of the source/target pretrained models can be categorized into either of the following two scenarios: (i) upscaling the source model to larger target models, and (ii) updating the pretrained knowledge of the source model to the target model with better pretraining data.
The results for the former scenario (i) are shown in Figures~\ref{fig:resnet50_openai}-\ref{fig:convnext_laion400m},~\ref{fig:llama2}-\ref{fig:falcon3}, where the source and target models are pretrained on the same dataset, and thus the only difference between them is the network architecture.
The results for the latter scenario (ii) are shown in Figures~\ref{fig:vit-b-16_openai}-\ref{fig:vit-b-16_laion2b} and Figure~\ref{fig:qwen2-to-qwen2.5}.
In both scenarios, we observe that PRT successfully leverages the improved capabilities of target pretrained models, by reusing the fixed reward model trained with the source pretrained model.

\COMMENTOUT{ %
\WIP{As shown in Figure~\ref{fig:llm_results}, PRT outperformed the Pretrained in all cases. 
Furthermore, even when compared with EFT or Instruct, PRT demonstrated performance comparable to or exceeding those models, thereby confirming its effectiveness.    
}
}

\COMMENTOUT{ %
\paragraph{Scenario (i): Upscaling the model size only.}

The results for this scenario are shown in Figures~\ref{fig:resnet50_openai}-\ref{fig:convnext_laion400m},~\ref{fig:llama2}-\ref{fig:falcon3}.
In these experiments, the source and target models are pretrained on the same dataset, and thus the only difference between them is the network architecture.
In general, the larger architecture leads to the higher capability of the pretrained models.
The overall results show that the inference-time tuning benefits from this fact, i.e., 

%
%
%
%
%
As shown in Figures~\ref{fig:llama2} $\sim$~\ref{fig:falcon3}, 
PRT outperforms the Pretrained on each task, indicating that the PRT model can effectively leverage its learned capabilities during inference, 
even when the Target Model is larger than the model used during training.

\WIP{Furthermore, in several tasks, PRT achieves higher scores than the Instruct Model. 
This does not necessarily mean that PRT inherently learns better than standard instruction tuning; 
rather, under the specific tasks and model configurations used in this study, PRT happened to yield higher scores.
Theoretically, there should be no difference between the optimal models obtained through PRT and instruction tuning, but differences in how the model parameters are updated during training may have led to the observed outcomes. 
Meanwhile, a more in-depth analysis of the distinctions among tuning methods, as well as the unique characteristics of PRT during training, remains a topic for future work.}


\paragraph{Scenario (ii): Updating the pretrained knowledge.}
%
\WIP{In this scenario, we evaluate the effectiveness of PRT when upgrading model versions using the Qwen2 and Qwen2.5 families.
We evaluate the effectiveness of PRT when upgrading model versions using the Qwen2 and Qwen2.5 families.}

During PRT training, we used Qwen2-0.5B as the Pretrained Model and initialized the reward model with Qwen2-0.5B.
For inference, we evaluated Qwen2.5 models with 0.5B, 1.5B, 3B, 7B, and 14B parameters as the target model.
As shown in Figure~\ref{fig:qwen2_gsm} and \ref{fig:qwen2_ifeval}, PRT outperformed the Pretrained in all settings and achieved performance comparable to or better than EFT and Instruct.
This indicates that training PRT on an older version of the model allows for immediate deployment of the PRT model when a new version is released, enabling inference results similar to those of a instruct model.
}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Qwen2-0.5B.pdf}
        \includegraphics[width=\linewidth]{results_final/ifeval_average_Qwen2-0.5B.pdf}
    \end{minipage}
    \vspace{-0.2cm}
    \caption{\footnotesize Inference-time tuning from Qwen2-0.5B to the Qwen2.5 models with various sizes.}\label{fig:qwen2-to-qwen2.5}
    \vspace{-0.2cm}
\end{figure}
\begin{figure}[t!]
    \centering

    \begin{tcolorbox}[
        width=0.98\linewidth,    %
        colback=white,
        colframe=black,
        colbacktitle=black,
        coltitle=white,
        fonttitle=\bfseries,
        title=Prompt,           %
        %
    ]

    %
    Question: James has 6 more candies than Robert. John has twice as many candies as Robert. If John has 54 candies, how many more candies does John have than James?
    
    Answer:
      
    \tcbsubtitle{Model Response}

    %
    Let R be the number of candies Robert has.
    \sethlcolor{yellow!40}\hl{John has 2R candies.} 
    \sethlcolor{yellow!40}\hl{James has 6} + R candies. We know that John has 54 candies, so 2R = 54.
    \sethlcolor{cyan!40} \hl{R = 27. James has }6 + 27 = 33 candies. John has 54 - 33 = 21 more candies than James.
    
    \tcblower
    %
    \sethlcolor{yellow!40} 
    %
    %
        \begin{minipage}{0.48\linewidth} %
            \centering
            \includegraphics[width=\linewidth]{results_final/prob_1.pdf}
            \subcaption{Next Token Candidates Following "... \hl{John has 2R candies. James has 6}"}
            \label{fig:model_res_prob1}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\linewidth} %
            \centering
            \includegraphics[width=\linewidth]{results_final/prob_2.pdf}
            \subcaption{Next Token Candidates Following "... \sethlcolor{cyan!40} \hl{R = 27. James has }"}
            \label{fig:model_res_prob2}
        \end{minipage}
    %
    \end{tcolorbox}
    \vspace{-0.2cm}
    \caption{\footnotesize Changes in next-token probabilities by PRT.}
    \label{fig:model_response}
    \vspace{-0.4cm}
\end{figure}

\vspace{-0.1cm}
\subsection{Qualitative Analysis}
\vspace{-0.1cm}
To analyze the behavior of PRT in more detail, we examine the tokens generated by the model. 
%
In this analysis, we used Llama3-8B as the target model, while Llama-3.2-1B was used for training and reward-model initialization.
Figure~\ref{fig:model_response} presents the output obtained under these settings. 
We observe that the PRT output correctly produces the chain of reasoning that leads to the final answer of 21.\footnote{By contrast, the pretrained Llama3-8B output was only “18.” For details, see Appendix \ref{appendix:qualitative_analysis}.}
Additionally, Figures~\ref{fig:model_res_prob1} and \ref{fig:model_res_prob2} show the top-5 prediction probabilities for the tokens that follow each highlighted sentence.
From these distributions, we see that the target pretrained model’s predictions are altered by the PRT model. 
In Figure~\ref{fig:model_res_prob2}, for instance, the pretrained model  attempts to output the answer "33" directly, whereas the PRT outputs "6" which reflects the step-by-step reasoning capability acquired through inference-time instruction-tuning.


\begin{figure}[t!]
    \centering
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/plot_train_valaccs_for_reg_factors/vitb_400m__FlowersVal.pdf}
        \vspace{-0.5cm}
        \subcaption{\scriptsize Accuracy of PRT (solid) and reward-only (dotted) during training.}\label{subfig:reward only accuracy}
    \end{minipage}
    \hfill
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/plot_transfer_val_for_reg_factors/vitb_400m_FlowersVal.pdf}
        \vspace{-0.5cm}
        \subcaption{\scriptsize Inference-time tuning with various regularization coefficients $\alpha$.}\label{subfig:vision results with varying alpha}
    \end{minipage}
    \vspace{-0.2cm}
    \caption{\footnotesize Analysis on how the EM regularization affects on performance of PRT with various coefficient $\alpha$. Reward-only refers to the exponential distribution $\rho_\theta(y|x)$ in Sec~\ref{sec:pac-bayesian perspective}. }
    \label{fig:vision reg factors}
    \vspace{-0.3cm}
\end{figure}
\begin{figure}[t!]
    \centering
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/llama3_REG.pdf}
        \vspace{-0.6cm}
        \subcaption{Llama3}
        \label{fig:reg_llama3}
    \end{minipage}
    \hfill
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/qwen2_REG.pdf}
        \vspace{-0.6cm}
        \subcaption{Qwen2.5}
        \label{fig:reg_qwen2.5}
    \end{minipage}
    \vspace{-0.2cm}
    \caption{\footnotesize Performance of PRT on GSM8k with various regularization coefficients $\alpha$.}
    \label{fig:ablation_reg}
\end{figure}

\vspace{-0.1cm}
\subsection{Effects of Entropy Regularization}
\vspace{-0.1cm}

Here we employ Entropy Maximization (EM) regularization in PRT training, with its coefficient $\alpha \in \mathbb{R}_{\geq 0}$.
%
In Section~\ref{sec:pac-bayesian perspective}, the PAC-Bayesian analysis indicated that the generalization capability of PRT models is affected by the entropy of the reward distribution $\rho_\theta(y|x)$ defined by the reward model $r_\theta(x,y)$.
In Figure~\ref{fig:vision reg factors} and \ref{fig:ablation_reg}, we empirically analyze its effect by varying $\alpha$.
First of all, we observed that the EM regularization generally suppresses the accuracy of the reward itself (Fig.~\ref{subfig:reward only accuracy}) as $\alpha$ increases, which can be naturally expected.
However, interestingly, the performance of PRT (Fig.~\ref{subfig:vision results with varying alpha}, \ref{fig:ablation_reg}) is not degraded even with such rewards, but also sometimes boosted regardless of the accuracy of rewards themselves.
The regularization itself may not be yet practical since the optimal $\alpha$ tends to be dependent on tasks and pretrained models, but these analyses provide valuable insights for generalization capability of PRT models, which may lead to future exploration of more practical regularization.
%

\begin{comment}
To investigate the effect of entropy regularization during training of the reward model, we conduct an ablation study on the language modeling task using the Llama3 and Qwen2.5.
We evaluate the performance of PRT with and without entropy regularization on the GSM task.
Figure~\ref{fig:ablation_reg} shows the results for the GSM task for each regularized factor.
As shown in the figure, both models achieved better scores when Entropy regularization was applied during training compared to when it was not used.
The results presented in Figure 1 and Table 1 do not incorporate Entropy regularization, suggesting that introducing Entropy regularization during training could further improve performance.
Meanwhile, although an Entropy regularization coefficient around 1e-3 generally yielded favorable results on GSM8k, further investigation is needed to determine the optimal coefficient selection method.
\end{comment}

\vspace{-0.1cm}
\subsection{Memory and Speed Analysis}\label{sec:memory and speed analysis}
\vspace{-0.1cm}

\COMMENTOUT{
We investigated the impact of EFT and PRT on inference speed.
The evaluation was conducted using Llama2 with a reward model of size 7B and a target model of size 13B, as well as Llama3 with a reward model of size 1B and a target model of size 8B.
For each setting, six samples were generated, and the average and standard deviation of the generation time per token were computed.    
}

Since PRT introduces an auxiliary model as the reward for both training and inference, we investigate how the memory usage and speed increase or decrease, compared to standard fine-tuning in training and EFT in inference.
For training, Table~\ref{tab:vision training stats} in Appendix shows that, while training time slightly increases due to the auxiliary model, the increase in memory usage is relatively negligible because the pretrained model in PRT does not require back-propagation.
For inference, Tables~\ref{tab:vision inference stats} and \ref{tab:llm inference_speeds} show that PRT successfully reduces both inference speed and memory usage compared to EFT, which highlights the benefit of employing explicit reward models.


\section{Related Work}

%

\paragraph{Tuning by Refining Predictions.}
In this paper we focused on inference-time tuning accomplished by refining predictions from the underlying pretrained model.
In particular, our baseline is the emulated fine-tuning (EFT;~\citet{mitchell2024emulator}) which established the interpretation of inference-time tuning based on KL-regularized reward maximization.
Parallel work by \citet{liu2024tuning} also proposed an essentially same method called proxy-tuning.
While these previous work focused on inference-time tuning with pretrained models that only differ in their model scale, we examined a more general setting that the pretrained models may differ even in their architectures or pretraining datasets.
Also, the previous work assumed the fine-tuned model was prepared in advance, and explored how to exploit it for a new pretrained model.
In contrast, we reexamined the assumption  and explored an alternative to fine-tuning that is more suitable for inference-time tuning.
As a consequence, at the cost of a little overhead in training, our approach successfully halves the overhead in inference-time tuning.
%

The literature of controlled text generation~\citep{krause2021gedi,yang2021fudge,pascual2021plug,li2023contrastive,deng2023reward} is also related to our work, but they have explored specific methods for attribute-conditioned text generation, which requires a classifier for some attributes, rather than fine-tuning for general tasks as in our work.
In particular, \citet{deng2023reward} proposed to control language models with reward models similarly to our work, but which are trained with a manually-designed loss specific to text classification tasks, while our method employs  standard loss for fine-tuning and thus can be naturally applied to broader domains and tasks such as vision classification and instruction tuning.

\vspace{-0.3cm}
\paragraph{Tuning by Editing Parameters or Activations.}
Another possible approach for inference-time tuning would be directly editing the parameters or activations of pretrained models, instead of their predictions.
A bunch of research on parameter editing, including \citep{ilharco2023editing,gueta2023knowledge,ortiz2024task,yadav2024ties,chijiwa2024transferring,daheim2024model}, addresses inference-time tuning by leveraging existing fine-tuned results.
However, most of these work aimed to tune a given pretrained model for multiple tasks, and thus cannot be used to tune a newly provided pretrained model.
Although \citet{chijiwa2024transferring} tackled the challenge of inference-time tuning with different pretrained models, it still requires the pretrained models to share the same architecture, which may be a fundamental limitation of the approach by editing parameters.
Similarly, there is a line of research~\citep{dathathri2020plug,hernandez2023inspecting,chuang2024dola,li2024inference} on tuning by editing the activations, but they also require the model architecture to have the same dimension for activations.
In contrast to these approaches, the approach by refining predictions in this paper would be more promising since it is completely free from the choice of the model architectures.


\vspace{-0.3cm}
\paragraph{Reward for Language Models.}
For language models, the notion of rewards has been exploited mainly in two lines of research: (i) Reinforcement Learning from Human Feedback (RLHF;~\citet{christiano2017deep,jaques2017sequence,ouyang2022training}), and (ii) multi-step reasoning~\citep{cobbe2021training,uesato2022solving}.
In RLHF, reward models are first trained on a dataset of human feedbacks, such as pairs of prefered-disprefered responses, and then used for training LLMs by reinforcement learning.
Although recent methods~\citep{rafailov2024direct,calandriello24humanalignment,ethayarajh24modelalignment,rafailov2024from} successfully bypass the explicit use of reward models, they are still learning human preferences through implicit rewards.
Several work~\cite{liu2024decoding,chakraborty2024transferqstar,khanov2024args} also integrate the idea of inference-time decoding into RLHF, but they assume the reward model is already prepared by RLHF.
In multi-step reasoning, reward models are trained to evaluate the intermediate process of reasoning by LLMs, from datasets with process or outcome supervision, and then used as verifiers for sampling such as Best-of-N sampling~\citep{lightman2024lets_verify,wang2024math} or self-consistency decoding~\citep{wang2023selfconsistency,luo2024improve}.
In contrast to these lines of work, where rewards are used for either training LLMs or verifying inference, the reward model in our approach is directly trained for {\it classification or generation} from ground-truth labels or tokens through the same loss and dataset for standard fine-tuning, which may lead to a broad range of applications in various domains including language generation.


\section{Conclusion}

In this paper, we introduced a new fine-tuning principle called portable reward tuning (PRT) as an alternative to standard fine-tuning, based on the interpretation of fine-tuning as reward maximization with KL regularization.
%
PRT naturally fits into the framework of inference-time tuning as the reward maximization.
We theoretically analyzed its basic properties of both training and inference, revealing how the reward evolves in training and how the choice of pretrained models affects inference-time tuning.
Also we empirically confirmed that PRT can achieve comparable accuracy to previous work of inference-time tuning, even with less computational overhead.

\ifdefined\preprint
\else
    \section*{Impact Statement}
    This paper investigates basic properties of a proposed principle of fine-tuning, which aims to advance the field of Machine Learning.
    There may be potential societal consequences of our work, but discussing about them is out of scope of this work.
\fi


%
\bibliography{main}
\bibliographystyle{icml2025}



\appendix
\onecolumn


\section{Proof of Proposition~\ref{prop:evaluation of KL divergence}}\label{app:section:proof of Proposition 3.2}
\begin{proof}
    Let $Z_\theta(x) := \mathbb{E}_{y\sim\pi_\rmpt(y|x)} [ \exp(r_\theta(x, y)) ]$ and similarly $\wt{Z}_\theta(x) := \mathbb{E}_{y\sim\wt{\pi}_\rmpt(y|x)}  [ \exp(r_\theta(x, y)) ]$ for convenience.
    \begin{align}
        &\KL(\pi_\theta(\text{-}|x) \parallel \wt{\pi}_\theta(\text{-}|x)) \nonumber\\
        &= \sum_y \pi_\theta(y|x) \log\left( \frac{\pi_\theta(y|x)}{\wt{\pi}_\theta(y|x)} \right) \nonumber\\
        &= \sum_y \frac{\pi_\rmpt(y|x)\exp(r_\theta(x,y))}{Z_{\theta}(x)} \log\left( \frac{\pi_\theta(y|x)}{\wt{\pi}_\theta(y|x)} \right) \nonumber\\
        &= \sum_y \frac{\pi_\rmpt(y|x)\exp(r_\theta(x,y))}{Z_{\theta}(x)}  \nonumber\\
        & \quad \ \times \left\{ \log\left( \frac{\pi_\rmpt(y|x)}{\wt{\pi}_\rmpt(y|x)} \right) + \log\left( \frac{\wt{Z}_\theta(x)}{Z_\theta(x)} \right) \right\} \nonumber\\
        & = \frac{1}{Z_\theta(x)}\sum_y \exp\left(r_\theta(x,y)\right) \pi_\rmpt(y|x)  \log\left( \frac{\pi_\rmpt(y|x)}{\wt{\pi}_\rmpt(y|x)} \right) \nonumber\\
        & \quad \ + \log\left( \frac{\wt{Z}_\theta(x)}{Z_\theta(x)} \right) \nonumber\\
        & \leq \frac{\max_y \exp r_\theta(x,y)}{\mathbb{E}_{y\sim\pi_\rmpt(y|x)} \exp r_\theta(x,y)} \KL(\pi_\rmpt(\text{-}|x) \parallel \wt{\pi}_\rmpt(\text{-}|x)) \nonumber\\
        & \quad \ + \log\left( \frac{\wt{Z}_\theta(x)}{Z_\theta(x)} \right) \nonumber.
    \end{align}
    The first term can be bounded by $C\varepsilon$ by combining the assumptions.
    The second term can be evaluated as follows:
    \begin{align*}
        & \frac{\wt{Z}_\theta(x)}{Z_\theta(x)} \\
        & = \frac{\sum_y \wt{\pi}_\rmpt(y|x) \exp r_\theta(x,y)}{\sum_y \pi_\rmpt(y|x) \exp r_\theta(x,y)} \\
        & = 1 + \frac{\sum_y (\wt{\pi}_\rmpt(y|x) - \pi_\rmpt(y|x)) \exp r_\theta(x,y)}{\sum_y \pi_\rmpt(y|x) \exp r_\theta(x,y)} \\
        &\leq 1 + \frac{\max_y \exp r_\theta(x,y)}{\mathbb{E}_{y\sim\pi_\rmpt(y|x)} \exp r_\theta(x,y)} \sum_y \big|\wt{\pi}_\rmpt(y|x) - \pi_\rmpt(y|x)\big| \\
        &\leq 1 + C \sum_y \big|\wt{\pi}_\rmpt(y|x) - \pi_\rmpt(y|x)\big| \\
        & \quad (\text{by the assumption}) \\
        &\leq 1 + C'\sqrt{\KL(\wt{\pi}_\rmpt(\text{-}|x) \parallel \pi_\rmpt(\text{-}|x))} \\
        & \quad (\text{by Pinsker's inequality}) \\
        &\leq 1 + C'\sqrt{\varepsilon}.
    \end{align*}
    Since $\log(1+C'\sqrt{\varepsilon})=O(\sqrt{\varepsilon})$ holds asymptotically, finally we  have $\KL(\pi_\theta(y|x) \parallel \wt{\pi}_\theta(y|x))) \leq O(\sqrt{\varepsilon})$.
\end{proof}

\section{Experimental Setup}
\label{app:section:exp_setup}

\subsection{Image Classification Tasks.}

\begin{comment}
\paragraph{Pretraining Dataset}
We use the zero-shot classification models pretrained on the following datasets:
\begin{itemize}
    \item \textbf{LAION-400m}~\cite{schuhmann2021laion}: A large-scale image classification dataset with 400 million images, focusing on diverse and high-quality image-text pairs.
    \item \textbf{LAION-2B}~\cite{schuhmann2022laion}: An extended version of LAION-400m with 2 billion images, providing a broader range of image-text pairs for more comprehensive training.
    \item \textbf{DC-1B}~\cite{gadre2024datacomp}: A dataset with 1 billion images, specifically curated for domain-specific tasks, offering targeted data for specialized applications.
\end{itemize}
\end{comment}

\paragraph{Training Setups}
In training of either standard fine-tuning or PRT, we used the same hyperparameters following existing work~\citep{ilharco2023editing} as follows: learning rate $= 1\times 10^{-5}$, batch size $= 128$, number of iterations $=2000$, optimizer $=$ Adam, warmup iterations $=500$, learning rate scheduler $=$ cosine annealing. We conducted all training on a single A100 GPU.

\paragraph{Fine-Tuning and Evaluation Dataset}
We consider the following image classification tasks:

\begin{itemize}
    \item \textbf{Aircraft}~\citep{maji13fine-grained}: A dataset with 100 classes of aircrafts, 100 images per class.
    \item \textbf{Caltech101}~\citep{li2022caltech101}: A dataset with 101 classes of objects,  40 to 800 images per class.
    \item \textbf{Cars}~\cite{krause20133d}: A dataset with 196 classes of various cars.
    \item \textbf{CIFAR-100}~\citep{krizhevsky2009learning}: A dataset with 100 classes of 32x32 color images.
    \item \textbf{Country211}~\citep{radford2021learning}: A dataset of photos taken in 211 different countries.
    \item \textbf{CUB}~\cite{WahCUB_200_2011}: A dataset of images with 200 bird species for fine-grained classification.
    \item \textbf{Flowers}~\citep{nilsback2008automated}: The Oxford 102 Flower Dataset, containing images of 102 flower categories for fine-grained classification.
    \item \textbf{RESISC45}~\citep{cheng2017remote}: A dataset of images with 45 scene classes.
\end{itemize}

\paragraph{Models}
We employed the following model architectures for the vision feature extractors of CLIP models:

\begin{itemize}
    \item \textbf{ResNets (ResNet-50, ResNet-101;~\citet{he2016resnet})}
    \item \textbf{ConvNext~\citep{liu2022convnet}}
    \item \textbf{Vision Transformers (ViT-B-16, ViT-L-14;~\citet{dosovitskiy2021an})}
\end{itemize}


\subsection{Language Modeling Tasks.}

\paragraph{Training Dataset and Settings}
For instruction tuning, we used the Tulu v2 dataset~\cite{ivison2023camels}, which is a large-scale dataset designed to improve the instruction-following capabilities of language models. 
The dataset includes a diverse set of instructions and corresponding responses, covering a wide range of topics and tasks.
The training conditions are as follows: learning rate = $2 \times 10^{-5}$, batch size = 128, number of epochs = 2, optimizer = Adam, warmup ratio = 0.03, and learning rate scheduler = linear.
We conducted all training on 8 NVIDIA A100 GPUs.

\paragraph{Evaluation Dataset}
We consider the following language modeling tasks: GSM8k and IFEval.
\begin{itemize}
    \item \textbf{GSM8K}~\citep{cobbe2021training}: A dataset for evaluating the ability of models to solve grade-school math problems. Evaluation is based on the exact match metric, which measures whether the model's final answer exactly matches the correct answer.
    \item \textbf{IFEval}~\citep{zhou2023instruction}: A dataset for evaluating the ability of models to perform information extraction tasks. It employs four evaluation metrics: instruction-level strict accuracy, instruction-level relaxed accuracy, prompt-level strict accuracy, and prompt-level relaxed accuracy, which comprehensively assess the model’s ability to follow complex instructions. In this paper, we report the average of these four scores as the evaluation metric.
\end{itemize}

\paragraph{Models}
We evaluated the proposed PRT method on the following models:

\begin{itemize}
    \item \textbf{\llama 2 Series}~\citep{touvron2023llama}: A family of large-scale language models developed by Meta AI, available in various sizes, including 7B, 13B, and 70B parameters, designed for a wide range of natural language processing tasks.
    \item \textbf{\llama 3 Series}~\citep{dubey2024llama}: A family of large-scale language models developed by Meta AI, introduced in April 2024 with 8B and 70B parameter variants. Compared to Llama 2, it features improvements in tokenizer efficiency, training data scale, and overall model optimization.  
    The subsequent update, {\bf \llama 3.1}, released in July 2024, further enhanced performance by refining pretraining methodologies while maintaining the same parameter sizes.  
    In September 2024, {\bf \llama 3.2} introduced lightweight text models with 1B and 3B parameters, optimized for efficiency in resource-constrained environments such as mobile and edge devices.  
    \item \textbf{Qwen 2 Series}~\citep{yang2024qwen2.5,yang2024qwen2}: A series of large-scale language models developed by Alibaba Cloud's Qwen team, designed for various natural language understanding and generation tasks. The initial {\bf Qwen 2} models were released with parameter sizes such as 1.5B and 3B, focusing on high-quality training data and diverse applications.  
    The subsequent {\bf Qwen 2.5} series expanded the model range, introducing sizes from 0.5B to 72B parameters, with both base and instruction-tuned variants, further improving performance and efficiency.  
    \item \textbf{Falcon 3 Series}~\citep{Falcon3}: A series of open-source large language models developed by the Technology Innovation Institute (TII) in Abu Dhabi, designed to provide accessible and efficient AI solutions. Released in December 2024, Falcon 3 models are available in 1B, 3B, 7B, and 10B parameter sizes, each offered in both Base and Instruct variants. The Base models are tailored for general-purpose text generation, while the Instruct models are fine-tuned for conversational applications.  

\end{itemize}

\section{Memory and Speed Benchmarks}

See Table~\ref{tab:vision training stats} for training-time benchmarks, and Tables~\ref{tab:vision inference stats} and \ref{tab:llm inference_speeds} for inference-time benchmarks.

\begin{table}[h!]
    \centering
        \begin{tabular}{cc|cc}
            \toprule
            Models & & FT & PRT \\
            \midrule
            \multirow{2}{*}{ResNet-50} & Peak memory & $12.84$ GB & $13.08$ GB \\
            & Speed per batch & $38.26_{\pm 3.62}$ ms & $46.36_{\pm 3.34}$ ms \\
            \midrule
            \multirow{2}{*}{ViT-B-16} & Peak memory & $20.17$ GB & $20.58$ GB \\
            & Speed per batch & $116.10_{\pm 0.43}$ ms & $151.13_{\pm 0.48}$ ms \\ 
            \bottomrule
        \end{tabular}
    \caption{Memory usage and average time  per batch in training with batch size $128$.}\label{tab:vision training stats}
\end{table}
\begin{table}[h!]
    \centering
    \resizebox{0.95\linewidth}{!}{%
        \begin{tabular}{ccc|ccc}
            \toprule
            Source Models & Target Models &  & Target FT (Oracle) & EFT & PRT \\
            \midrule
            \multirow{2}{*}{ResNet-50} & \multirow{2}{*}{ViT-B-16} & Peak memory & $1.46$ GB & $2.42$ GB & $2.18$ GB  \\
            & & Speed per batch & $3.52 \pm 0.17$ ms & $10.27 \pm 0.22$ ms & $7.00 \pm 0.12$ ms  \\
            \midrule
            \multirow{2}{*}{ResNet-50} & \multirow{2}{*}{ViT-L-14} & Peak memory & $2.96$ GB & $3.44$ GB & $3.20$ GB \\
            & & Speed per batch & $5.85 \pm 0.11$ ms & $12.52 \pm 0.21$ ms & $9.43 \pm 0.19$ ms \\ 
            \midrule
            \multirow{2}{*}{ViT-B-16 (LAION-400M)} & \multirow{2}{*}{ViT-B-16 (LAION-2B)} & Peak memory & $1.46$ GB & $2.29$ GB  & $1.88$ GB  \\
            & & Speed per batch & $3.52 \pm 0.17$ ms & $9.18 \pm 0.12$ ms & $6.33 \pm 0.07$ ms \\
            \midrule
            \multirow{2}{*}{ViT-B-16 (LAION-2B)} & \multirow{2}{*}{ViT-L-14 (LAION-2B)} & Peak memory & $2.96$ GB & $3.79$ GB & $3.38$ GB \\
            & & Speed per batch & $5.85 \pm 0.11$ ms & $11.73 \pm 0.15$ ms & $8.89 \pm 0.11$ ms \\
            \bottomrule
        \end{tabular}
    }
    \caption{Memory usage and average time  per batch in inference with batch size $128$.}\label{tab:vision inference stats}
\end{table}
\begin{table}[h!]
    \centering
    \begin{tabular}{lcc}
        \toprule
        & \multicolumn{2}{c}{Average Time per Token (ms)} \\
       & \textbf{Llama2-7B} $\to$ \textbf{Llama2-13B} & \textbf{Llama3.2-1B} $\to$ \textbf{Llama3-8B} \\
        \midrule
        Pretrained  & 26.0$_{\pm0.2}$ & 17.1$_{\pm1.1}$ \\
        EFT & 39.8$_{\pm 0.1}$ ($\times$ 0.65) & 24.4$_{\pm 0.2} (\times 0.7)$ \\
        PRT & 27.8$_{\pm 0.5}$ ($\times$ 0.93) & 22.7$_{\pm 1.8} (\times 0.75)$ \\
        \bottomrule
    \end{tabular}
    \caption{Inference speed of Pretrained, EFT and PRT. The number following “$\times$” in brackets indicates each method’s token generation speed relative to the Pretrained model, whose speed is set to 1.0.}
    \label{tab:llm inference_speeds}
\end{table}

\section{Qualitative Analysis}
\label{appendix:qualitative_analysis}
To analyze the behavior of PRT in more detail, we examine the tokens generated by the model. 
In this analysis, we adopt Llama3-8B as the target model. 
In this analysis, we used Llama3-8B as the target model, while Llama-3.2-1B was used for training and reward-model initialization.
Figures~\ref{fig:model_response_gsm_1} and ~\ref{fig:model_response_gsm_2} present the output obtained under these settings.

\begin{figure}[h!]
    \centering

    \begin{tcolorbox}[
        width=0.9\linewidth,    %
        colback=white,
        colframe=black,
        colbacktitle=black,
        coltitle=white,
        fonttitle=\bfseries,
        title=Prompt,           %
        %
    ]

    %
    Question: James has 6 more candies than Robert. John has twice as many candies as Robert. If John has 54 candies, how many more candies does John have than James?
    
    Answer:
      
    \tcbsubtitle{Response of Pretrained}

     18

    \tcbsubtitle{Response of PRT}

    %
    Let R be the number of candies Robert has.
    \sethlcolor{yellow!40}\hl{John has 2R candies.} 
    \sethlcolor{yellow!40}\hl{James has 6} + R candies. We know that John has 54 candies, so 2R = 54.
    \sethlcolor{cyan!40} \hl{R = 27. James has }6 + 27 = 33 candies. John has 54 - 33 = 21 more candies than James.
    
    \tcblower
    %
    \sethlcolor{yellow!40} 
    %
    %
        \begin{minipage}{0.48\linewidth} %
            \centering
            \includegraphics[width=\linewidth]{results_final/prob_1.pdf}
            \subcaption{Next Token Candidates Following "... \hl{John has 2R candies. James has 6}"}
            \label{fig:model_res2_prob1}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\linewidth} %
            \centering
            \includegraphics[width=\linewidth]{results_final/prob_2.pdf}
            \subcaption{Next Token Candidates Following "... \sethlcolor{cyan!40} \hl{R = 27. James has }"}
            \label{fig:model_res2_prob2}
        \end{minipage}
    %

    \end{tcolorbox}

    \caption{Example of model response.}
    \label{fig:model_response_gsm_1}
\end{figure}
\begin{figure}[h!]
    \centering

    \begin{tcolorbox}[
        width=0.9\linewidth,    %
        colback=white,
        colframe=black,
        colbacktitle=black,
        coltitle=white,
        fonttitle=\bfseries,
        title=Prompt,           %
        %
    ]

    %
    Question: Gerald and Julia divided \$100 in the ratio 3:2. If Gerald spent \$10 on a book, how much money did he have left?
    
    Answer:
      
    \tcbsubtitle{Response of Pretrained}

     \$40

    \tcbsubtitle{Response of PRT}

    %
    Let x be the amount of money Julia had. Then 3x / 2 - 10 = x. So x = 20. \sethlcolor{yellow!40}\hl{Therefore, Gerald had }60 - 10 = \$50 left.
    
    \tcblower
    %
    \sethlcolor{yellow!40} 
    %
    \begin{center}
        \begin{minipage}{0.5\linewidth} %
            \centering
            \includegraphics[width=\linewidth]{results_final/prob_ex2_1.pdf}
            \subcaption{Next Token Candidates Following "... \hl{Therefore, Gerald had }"}
            \label{fig:model_res3_prob1}
        \end{minipage}
    \end{center}

    \end{tcolorbox}

    \caption{Example of model response.}
    \label{fig:model_response_gsm_2}
\end{figure}

\section{Evaluation on Various Vision Tasks}

Figure~\ref{app:fig:vision_results} shows an extensive evaluation of inference-time tuning on various vision datasets.

\begin{figure*}[!t]
    \centering
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_CUB.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_Flowers.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_Aircraft.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_Caltech101.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_CIFAR100.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_Country211.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/RN50/openai_RESISC45.pdf}
        \end{minipage}
        \caption{\scriptsize ResNet-50 (OpenAI)}
        \label{app:fig:resnet50_openai}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_CUB.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_Flowers.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_Aircraft.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_Caltech101.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_CIFAR100.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_Country211.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/convnext/laion400m_s13b_b51k_RESISC45.pdf}
        \end{minipage}
        \caption{{\scriptsize ConvNext (LAION-400M)}}
        \label{app:fig:convnext_laion400m}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_CUB.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_Flowers.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_Aircraft.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_Caltech101.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_CIFAR100.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_Country211.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_openai/openai_RESISC45.pdf}
        \end{minipage}
    \caption{\scriptsize ViT-B-16 (OpenAI)}
    \label{app:fig:vit-b-16_openai}
    \end{subfigure}
    \hfill    
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_CUB.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_Flowers.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_Aircraft.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_Caltech101.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_CIFAR100.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_Country211.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_400m_to_2b_n_vitl/laion400m_e32_RESISC45.pdf}
        \end{minipage}
    \caption{\scriptsize ViT-B-16 (LAION-400M)}
    \label{app:fig:vit-b-16_laion400m}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.19\textwidth}
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_Cars.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_CUB.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_Flowers.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_Aircraft.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_Caltech101.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_CIFAR100.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_Country211.pdf}
            \includegraphics[width=\linewidth]{results_final/plot_test/vitb_2b_to_dc/laion2b_s34b_b88k_RESISC45.pdf}
            \end{minipage}
    \caption{\scriptsize ViT-B-16 (LAION-2B)}
    \label{app:fig:vit-b-16_laion2b}
    \end{subfigure}
    \caption{Evaluations of inference-time tuned models for vision tasks. Each subcaption refers to the source pretrained model, and the labels in x-axis are target pretrained models. {\bf Pretrained} means the zero-shot classification by each target model as a baseline, and {\bf FT} means the fine-tuned target model as an oracle result.}
    \label{app:fig:vision_results}
\end{figure*}

\section{Evaluation on Code Generation Tasks}
To evaluate the code generation capabilities of PRT, we conducted a comparison using HumanEval~\cite{chen2021evaluating}.

\paragraph{HumanEval}: A dataset for evaluating the ability of models to generate correct code based on natural language descriptions.

\paragraph{Result} As shown in Figure~\ref{fig:llm_results_humaneval}, the effectiveness of PRT was confirmed for Llama2, Llama3, and Qwen2.5.
On the other hand, for Falcon3 with target model sizes of 7B and 10B, the score actually declined compared to the Pretrained model. 
We believe this is because, during training, the same 1B model was used as the target, which exhibited little improvement over the Pretrained model in code generation. 
As a result, the PRT model did not acquire code generation capabilities during training; consequently, when applied to larger model sizes, it acts as noise rather than providing a benefit.

%
\begin{figure*}[t!]
    \centering
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/humaneval_pass_at_1_Llama-2-7b-hf.pdf}
        \subcaption{Llama2}
        \label{fig:llama2_humaneval}
    \end{minipage}
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/humaneval_pass_at_1_Llama-3.2-1B.pdf}
        \subcaption{Llama3}
        \label{fig:llama3_humaneval}
    \end{minipage} \\
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/humaneval_pass_at_1_Qwen2.5-0.5B.pdf}
        \subcaption{Qwen2.5}
        \label{fig:qwen2.5_humaneval}
    \end{minipage}
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/humaneval_pass_at_1_Falcon3-1B-Base.pdf}
        \subcaption{Falcon3}
        \label{fig:falcon3_humaneval}
    \end{minipage}
    \caption{Comparison of PRT and EFT on HumanEval.}
    \label{fig:llm_results_humaneval}
\end{figure*}


\section{Evaluation on Various NLP Tasks}
To investigate the effectiveness of PRT, we conducted comparisons using a diverse set of evaluation datasets.
In addition to GSM8k, IFEval, and HumanEval, we used the following datasets for evaluation.

\begin{itemize}
    \item \textbf{Academic / General Knowledge MCQ}
    \begin{itemize}
        \item \textbf{ARC}~\cite{clark2018think}: The AI2 Reasoning Challenge (ARC) dataset is designed to test a model's ability to answer grade-school level science questions.
        \item \textbf{MMLU}~\cite{hendrycks2020measuring}: The Massive Multitask Language Understanding (MMLU) benchmark tests a model's ability to perform a wide range of language understanding tasks.
        \item \textbf{MMLU Pro}~\cite{wang2024mmlu}: An extension of the MMLU benchmark with more challenging tasks.
    \end{itemize}

    \item \textbf{Code Generation / Programming}
    \begin{itemize}
        \item \textbf{MBPP}~\cite{austin2021program}: The Mostly Basic Programming Problems (MBPP) dataset is designed to test a model's ability to solve basic programming problems.
    \end{itemize}

    \item \textbf{Commonsense Reasoning}
    \begin{itemize}
        \item \textbf{Hellaswag}~\cite{zellers2019hellaswag}: A dataset for evaluating commonsense reasoning and natural language inference.
        \item \textbf{GPQA}~\cite{rein2023gpqa}: A dataset for evaluating general-purpose question answering capabilities of models.
    \end{itemize}

    \item \textbf{Truthfulness}
    \begin{itemize}
        \item \textbf{TruthfulQA}~\cite{lin2021truthfulqa}: A dataset for evaluating the truthfulness of answers generated by models.
    \end{itemize}
\end{itemize}


In this experiment, we compared PRT with the baseline using the Llama2 series.
Each method employs the same models used in the NLP experiments in Section~\ref{sec:experiments}, which were trained with Tulu v2.


%
\begin{figure*}[t!]
    \centering
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/arc_easy_acc,none_Llama-2-7b-hf.pdf}
        \subcaption{ARC Easy}
        \label{fig:llama2_arc_easy}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/arc_challenge_acc,none_Llama-2-7b-hf.pdf}
        \subcaption{ARC Challenge}
        \label{fig:llama2_arc_challenge}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/gpqa_diamond_zeroshot_acc,none_Llama-2-7b-hf.pdf}
        \subcaption{GPQA Diamond}
        \label{fig:llama2_gpqa}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Llama-2-7b-hf.pdf}
        \subcaption{GSM8k}
        \label{fig:llama2_gsm8k_appendix}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/hellaswag_acc,none_Llama-2-7b-hf.pdf}
        \subcaption{Hellaswag}
        \label{fig:llama2_hellaswag_appendix}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/humaneval_pass_at_1_Llama-2-7b-hf.pdf}
        \subcaption{HumanEval}
        \label{fig:llama2_humaneval_full}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/ifeval_average_Llama-2-7b-hf.pdf}
        \subcaption{IFEval}
        \label{fig:llama2_ifeval_appendix}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/mbpp_average_Llama-2-7b-hf.pdf}
        \subcaption{MBPP}
        \label{fig:llama2_mbpp}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/mmlu_acc,none_Llama-2-7b-hf.pdf}
        \subcaption{MMLU}
        \label{fig:llama2_mmlu}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/mmlu_pro_exact_match,custom-extract_Llama-2-7b-hf.pdf}
        \subcaption{MMLU PRO}
        \label{fig:llama2_mmlu_ro}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/truthfulqa_mc1_acc,none_Llama-2-7b-hf.pdf}
        \subcaption{TruthfulQA}
        \label{fig:llama2_truthfulqa}
    \end{minipage}
    \caption{Evaluations of inference-time instruction-tuned models on Llama 2 Series. Each subcaption refers to the task name, and the labels in x-axis are target pretrained models. {\bf Pretrained} means the zero-shot inference by each target model as a baseline, and {\bf Instruct} means the instruct-tuned target model as an oracle result.}
    \label{fig:llm_results_llama2_appendix}
\end{figure*}


\section{Impact of Source Model Choice on PRT}
In this experiment, we compared PRT with the baseline using the Llama2 series.
In this section, we investigate the impact of source model differences on PRT by comparing GSM8K and IFEval scores when using Llama 3.2-1B and 3B, Qwen 2.5-0.5B and 1.5B, and Falcon 3-1B and 3B as source models.
All models were trained under the same conditions using the training data and hyperparameters specified in Appendix \ref{app:section:exp_setup}.
The results for each case are shown in Figures~\ref{fig:llm_results_appendix_source_model1} and Figures~\ref{fig:llm_results_appendix_source_model2}.


%
\begin{figure*}[!t]
    \centering
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Llama-3.2-1B.pdf}
        \subcaption{Llama-3.2-1B}
        \label{fig:llama3.2-1B_appendix1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Qwen2.5-0.5B.pdf}
        \subcaption{Qwen2.5-0.5B}
        \label{fig:qwen2.5-0.5B_appendix1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Falcon3-1B-Base.pdf}
        \subcaption{Falcon3-1B}
        \label{fig:falcon3-1B_appendix1}
    \end{minipage}
        \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Llama-3.2-3B.pdf}
        \subcaption{Llama-3.2-3B}
        \label{fig:llama3.2-3B_appendix1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Qwen2.5-1.5B.pdf}
        \subcaption{Qwen2.5-1.5B}
        \label{fig:qwen2.5-1.5B_appendix1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/gsm8k_exact_match,flexible-extract_Falcon3-3B-Base.pdf}
        \subcaption{Falcon3-3B}
        \label{fig:falcon3-3B_appendix1}
    \end{minipage}
    \caption{Evaluations of inference-time instruction-tuned models on the GSM8k benchmark. Each subcaption refers to the source pretrained model, and the labels in x-axis are target pretrained models. {\bf Pretrained} means the zero-shot inference by each target model as a baseline, and {\bf Instruct} means the instruct-tuned target model as an oracle result.}
    \label{fig:llm_results_appendix_source_model1}
\end{figure*}

%
\begin{figure*}[!t]
    \centering
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/ifeval_average_Llama-3.2-1B.pdf}
        \subcaption{Llama-3.2-1B}
        \label{fig:llama3.2-1B_appendix2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/ifeval_average_Qwen2.5-0.5B.pdf}
        \subcaption{Qwen2.5-0.5B}
        \label{fig:qwen2.5-0.5B_appendix2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/ifeval_average_Falcon3-1B-Base.pdf}
        \subcaption{Falcon3-1B}
        \label{fig:falcon3-1B_appendix2}
    \end{minipage}
        \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/ifeval_average_Llama-3.2-3B.pdf}
        \subcaption{Llama-3.2-3B}
        \label{fig:llama3.2-3B_appendix2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/ifeval_average_Qwen2.5-1.5B.pdf}
        \subcaption{Qwen2.5-1.5B}
        \label{fig:qwen2.5-1.5B_appendix2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_final/ifeval_average_Falcon3-3B-Base.pdf}
        \subcaption{Falcon3-3B}
        \label{fig:falcon3-3B_appendix2}
    \end{minipage}

    
    \caption{Evaluations of inference-time instruction-tuned models on the IFEval benchmark. Each subcaption refers to the source pretrained model, and the labels in x-axis are target pretrained models. {\bf Pretrained} means the zero-shot inference by each target model as a baseline, and {\bf Instruct} means the instruct-tuned target model as an oracle result.}
    \label{fig:llm_results_appendix_source_model2}
\end{figure*}

\begin{comment}
\begin{table}[t!]
    \centering
    \small %
    \begin{tabular}{cccccc}
    \hline
    Size & Task & \textbf{PT}  & \textbf{EFT} & \textbf{PRT} & \textbf{IT} \\ \hline
    \multicolumn{6}{c}{\textbf{Llama2}} \\ \hline
    \multirow{2}{*}{13B} & Math & 9. $_{\pm 1.}$ & 20. $_{\pm 1.}$\textgood{$\uparrow$} & \bf{27. $_{\pm 1.}$}\textgood{$\uparrow$} & \bf{27. $_{\pm 1.}$}\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & 17.$_{\pm}$ & 21. $_{\pm}$\textgood{$\uparrow$} & \bf{22. $_{\pm}$}\textgood{$\uparrow$} & \bf{22. $_{\pm}$}\textgood{$\uparrow$} \\ \hline
    \multicolumn{6}{c}{\textbf{Llama3}} \\ \hline
    \multirow{2}{*}{3B} & Math & 12. $_{\pm 1.}$ & 10. $_{\pm 1.}$\textbad{$\downarrow$} & \bf{18. $_{\pm 1.}$}\textgood{$\uparrow$} & 16. $_{\pm 1.}$\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & 17. $_\pm$ & - & - & - \\ \hline
    \multirow{2}{*}{3-8B} & Math & 15. $_{\pm 1.}$ & 11. $_{\pm 1.}$\textbad{$\downarrow$} & \bf{27. $_{\pm 1.}$}\textgood{$\uparrow$} & 22. $_{\pm 1.}$\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & 29. $_\pm$ & 2. $_\pm$ & - & 24.\textbad{$\downarrow$} \\ \hline
    \multirow{2}{*}{3.1-8B} & Math & 26. $_{\pm 1.}$ & 15. $_{\pm 1.}$\textbad{$\downarrow$} & \bf{28. $_{\pm 1.}$}\textgood{$\uparrow$} & 11. $_{\pm 1.}$\textbad{$\downarrow$} \\ \cline{2-6}
    & Code & 25. $_\pm$ & - & - & 30. $_\pm$\textgood{$\uparrow$} \\ \hline
    \multicolumn{6}{c}{\textbf{Qwen2.5}} \\ \hline
    \multirow{2}{*}{1.5B} & Math & 9. $_{\pm 1.}$ & 46. $_{\pm 1.}$\textgood{$\uparrow$} & 49. $_{\pm 1.}$\textgood{$\uparrow$} & \bf{51. $_{\pm 1.}$}\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & 4. $_\pm$ & - & 14. $_\pm$\textgood{$\uparrow$} & 1. $_\pm$\textbad{$\downarrow$} \\ \hline
    \multirow{2}{*}{3B} & Math & 7. $_{\pm 1.}$ & 57. $_{\pm 1.}$\textgood{$\uparrow$} & \bf{60. $_{\pm 1.}$}\textgood{$\uparrow$}  & 35. $_{\pm 1.}$\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & 10. $_\pm$ & 11.$_\pm$\textgood{$\uparrow$} & \bf{22.}$_\pm$\textgood{$\uparrow$} & 14 $_\pm$\textgood{$\uparrow$} \\ \hline
    \multirow{2}{*}{7B} & Math & 9. $\pm$ 1 & 68. $_{\pm 1.}$\textgood{$\uparrow$} &  \bf{71. $_{\pm 1.}$}\textgood{$\uparrow$} & 55. $_{\pm 1.}$\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & 11. $_\pm$  & - & \bf{33.$_\pm$}\textgood{$\uparrow$} & 13. $_\pm$\textgood{$\uparrow$} \\ \hline
    \multirow{2}{*}{14B} & Math & 15. $_{\pm 1.}$ & 72. $_{\pm 1.}$\textgood{$\uparrow$} & \bf{77. $_{\pm 1.}$}\textgood{$\uparrow$} & 69. $_{\pm 1.}$\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & 6. $_\pm$  & - & 26.\textgood{$\uparrow$} & 15.\textgood{$\uparrow$}  \\ \hline
    \multicolumn{6}{c}{\textbf{Falcon3}} \\ \hline
    \multirow{2}{*}{3B} & Math & 34. $_{\pm 1.}$ & 43. $_{\pm 1.}$\textgood{$\uparrow$} & \bf{53. $_{\pm 1.}$}\textgood{$\uparrow$} & 41. $_{\pm 1.}$\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & - & - & - & - \\ \hline
    \multirow{2}{*}{7B} & Math & 54. $_{\pm 1.}$ & 39. $_{\pm 1.}$\textbad{$\downarrow$} & \bf{63. $_{\pm 1.}$}\textgood{$\uparrow$}& 58. $_{\pm 1.}$\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & - & - & - & - \\ \hline
    \multirow{2}{*}{10B} & Math & 14. $_{\pm 1.}$ & 52.$_{\pm 1.}$\textgood{$\uparrow$} & \bf{68. $_{\pm 1.}$}\textgood{$\uparrow$} & 56. $_{\pm 1.}$\textgood{$\uparrow$} \\ \cline{2-6}
    & Code & - & - & - & - \\ \hline
    \end{tabular}
    \caption{Scenario 1: Performance of PRT on models of various sizes.
    PT: Pretrained model. EFT: Emulated fine-tuning. PRT: Portable reward tuning. IT: Instruction tuning.
    \textgood{$\uparrow$}: Higher than PT. \textbad{$\downarrow$} Lower than PT. \textbf{Bold}: Best score.}
    \label{details}
\end{table}

\begin{table}[t!]
    \centering
    \small %
    \begin{tabular}{cccccc}
    \hline
    Size & Task & \textbf{PT} & \textbf{EFT} & \textbf{PRT} & \textbf{IT} \\ \hline
    \multirow{2}{*}{0.5B} & Math & 4.4 $_{\pm 0.6}$ & 7.9 $_{\pm 0.7.}$\textgood{$\uparrow$} & 24.3 $_{\pm 1.2.}$\textgood{$\uparrow$} & \bf{28.9} $_{\pm 1.2.}$\textgood{$\uparrow$} \\ %
    & Code & \bf{7.4} & 3.1 \textbad{$\downarrow$} & 0.1 \textbad{$\downarrow$} & 6.5 \textbad{$\downarrow$} \\ \hline
    \multirow{2}{*}{1.5B} & Math & 9.1$_{\pm 0.8}$ & 13.1 $_{\pm 0.9}$\textgood{$\uparrow$} & 48.7.$_{\pm 1.4.}$\textgood{$\uparrow$} & \bf{51.5} $_{\pm 1.4.}$ \textgood{$\uparrow$} \\
    & Code & \bf{3.8} & 0.0 \textbad{$\downarrow$} & 0.7 \textbad{$\downarrow$} & 0.7 \textbad{$\downarrow$} \\ \hline
    \multirow{2}{*}{3B} & Math & 6.7 $_{\pm 0.7}$ & 14.5 $_{\pm 1.0}$ \textgood{$\uparrow$} & \bf{61.0} $_{\pm 1.3}$ \textgood{$\uparrow$} & 35.0 $_{\pm 1.3}$ \textgood{$\uparrow$} \\  %
    & Code & 9.7 & 0.0 \textbad{$\downarrow$} & 2.3 \textbad{$\downarrow$} & \bf{13.5} \textgood{$\uparrow$} \\ \hline
    \multirow{2}{*}{7B} & Math & 9.5 $_{\pm 0.8}$ & 24.4 $_{\pm 1.2}$ \textgood{$\uparrow$} & \bf{71.7} $_{\pm 1.2}$ \textgood{$\uparrow$} & 55.1 $_{\pm 1.4}$ \textgood{$\uparrow$} \\ %
    & Code & 11.2 & 7.1 \textbad{$\downarrow$} & 0.9 \textbad{$\downarrow$} & \bf{13.4} \textgood{$\uparrow$} \\ \hline
    \multirow{2}{*}{14B} & Math & 14.9 $_{\pm 1.0}$ & 27.4 $_{\pm 1.2}$ \textgood{$\uparrow$} & \bf{76.2} $_{\pm 1.2}$ \textgood{$\uparrow$} & 69.1 $_{\pm 1.3}$ \textgood{$\uparrow$} \\ %
    & Code & 6.3 & 2.4 \textbad{$\downarrow$} & 0.7 \textbad{$\downarrow$} & bf{14.6} \textgood{$\uparrow$} \\ \hline
    \end{tabular}
    \caption{Scenario 2: Performance of PRT on upgraded model versions.
    PT: Pretrained model. EFT: Emulated fine-tuning. PRT: Portable reward tuning. IT: Instruction tuning.
    \textgood{$\uparrow$}: Higher than PT. \textbad{$\downarrow$} Lower than PT. \textbf{Bold}: Best score.}
    \label{tab:scenario2}
\end{table}

\begin{table*}[t!]
    \centering
    \small
    \begin{tabular}{cc|cccc|cccc}
    \hline
    \multicolumn{2}{c|}{Target Model} & \multicolumn{4}{c|}{\textbf{GSM8k}} & \multicolumn{4}{c}{\textbf{HumanEval}} \\
    \multicolumn{2}{c|}{Size} & \textbf{PT} & \textbf{EFT} & \textbf{PRT} & \textbf{IT} & \textbf{PT} & \textbf{EFT} & \textbf{PRT} & \textbf{IT} \\
    \hline
    \multirow{4}{*}{Qwen2.5} &1.5B & 9.1 & 25.2 \textgood{$\uparrow$} & 50.1 \textgood{$\uparrow$} & \bf{51.5} \textgood{$\uparrow$} 
         & 3.8 & 2.8 \textbad{$\downarrow$} & \bf{10.2} \textgood{$\uparrow$} & 0.7 \textbad{$\downarrow$} \\
    & 3B   & 6.7  & 26.8  \textgood{$\uparrow$} & \bf{57.6} \textgood{$\uparrow$} & 35.0 \textgood{$\uparrow$}
         & 9.7 & 6.6 \textbad{$\downarrow$} & \bf{20.3} \textgood{$\uparrow$} & 13.5 \textgood{$\uparrow$} \\
    & 7B   & 9.5 & 41.8 \textgood{$\uparrow$} & \bf{70.7} \textgood{$\uparrow$} & 55.1 \textgood{$\uparrow$}
         & 11.2 & 4.1 \textbad{$\downarrow$} & \bf{25.1} \textgood{$\uparrow$} & 13.4 \textgood{$\uparrow$} \\
    & 14B  & 14.9  & 43.3 \textgood{$\uparrow$} & \bf{77.2} \textgood{$\uparrow$} & 69.1 \textgood{$\uparrow$}
         & 6.3 & 2.7 \textbad{$\downarrow$} & \bf{19.2} \textgood{$\uparrow$} & 14.6 \textgood{$\uparrow$} \\
    \hline
    \end{tabular}
    \caption{Scenario 2: Performance of PRT on upgraded model versions.
    PT: Pretrained model. EFT: Emulated fine-tuning. PRT: Portable reward tuning. IT: Instruction tuning.
    \textgood{$\uparrow$}: Higher than PT. \textbad{$\downarrow$}: Lower than PT. \textbf{Bold}: Best score.}
    \label{tab:scenario2}
\end{table*}

\begin{table}[t!]
    \centering
    \small %
    \begin{tabular}{cccc}
    \hline
    Target Model & & REG Factor & GSM8k \\ \hline
    \multirow{5}{*}{Llama2-7B} & wo/ EM & - & 21.6 \\ \cline{2-4}
    & \multirow{4}{*}{w/ EM} & 1e-5 & 22.5 \\ \cline{3-4}
    & & 1e-4 & 22.7 \\ \cline{3-4}
    & & 1e-3 & 23.0 \\ \cline{3-4}
    & & 1e-2 & \bf{25.2} \\ \hline
    \multirow{5}{*}{Llama2-13B} & wo/EM & - & 27.3 \\ \cline{2-4}
    & \multirow{4}{*}{w/ EM} & 1e-5 & \bf{29.6} \\ \cline{3-4}
    & & 1e-4 & 28.7 \\ \cline{3-4}
    & & 1e-3 & 28.1 \\ \cline{3-4}
    & & 1e-2 & 28.4 \\ \hline
    \end{tabular}
    \caption{Performance of PRT on various models and tasks with different regularization factors.}
    \label{tab:ablation2}
\end{table}
\end{comment}


\end{document}
