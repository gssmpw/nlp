\section{Related Work}
%

\paragraph{Tuning by Refining Predictions.}
In this paper we focused on inference-time tuning accomplished by refining predictions from the underlying pretrained model.
In particular, our baseline is the emulated fine-tuning (EFT;~\citet{mitchell2024emulator}) which established the interpretation of inference-time tuning based on KL-regularized reward maximization.
Parallel work by \citet{liu2024tuning} also proposed an essentially same method called proxy-tuning.
While these previous work focused on inference-time tuning with pretrained models that only differ in their model scale, we examined a more general setting that the pretrained models may differ even in their architectures or pretraining datasets.
Also, the previous work assumed the fine-tuned model was prepared in advance, and explored how to exploit it for a new pretrained model.
In contrast, we reexamined the assumption  and explored an alternative to fine-tuning that is more suitable for inference-time tuning.
As a consequence, at the cost of a little overhead in training, our approach successfully halves the overhead in inference-time tuning.
%

The literature of controlled text generation~\citep{krause2021gedi,yang2021fudge,pascual2021plug,li2023contrastive,deng2023reward} is also related to our work, but they have explored specific methods for attribute-conditioned text generation, which requires a classifier for some attributes, rather than fine-tuning for general tasks as in our work.
In particular, \citet{deng2023reward} proposed to control language models with reward models similarly to our work, but which are trained with a manually-designed loss specific to text classification tasks, while our method employs  standard loss for fine-tuning and thus can be naturally applied to broader domains and tasks such as vision classification and instruction tuning.

\vspace{-0.3cm}
\paragraph{Tuning by Editing Parameters or Activations.}
Another possible approach for inference-time tuning would be directly editing the parameters or activations of pretrained models, instead of their predictions.
A bunch of research on parameter editing, including \citep{ilharco2023editing,gueta2023knowledge,ortiz2024task,yadav2024ties,chijiwa2024transferring,daheim2024model}, addresses inference-time tuning by leveraging existing fine-tuned results.
However, most of these work aimed to tune a given pretrained model for multiple tasks, and thus cannot be used to tune a newly provided pretrained model.
Although \citet{chijiwa2024transferring} tackled the challenge of inference-time tuning with different pretrained models, it still requires the pretrained models to share the same architecture, which may be a fundamental limitation of the approach by editing parameters.
Similarly, there is a line of research~\citep{dathathri2020plug,hernandez2023inspecting,chuang2024dola,li2024inference} on tuning by editing the activations, but they also require the model architecture to have the same dimension for activations.
In contrast to these approaches, the approach by refining predictions in this paper would be more promising since it is completely free from the choice of the model architectures.


\vspace{-0.3cm}
\paragraph{Reward for Language Models.}
For language models, the notion of rewards has been exploited mainly in two lines of research: (i) Reinforcement Learning from Human Feedback (RLHF;~\citet{christiano2017deep,jaques2017sequence,ouyang2022training}), and (ii) multi-step reasoning~\citep{cobbe2021training,uesato2022solving}.
In RLHF, reward models are first trained on a dataset of human feedbacks, such as pairs of prefered-disprefered responses, and then used for training LLMs by reinforcement learning.
Although recent methods~\citep{rafailov2024direct,calandriello24humanalignment,ethayarajh24modelalignment,rafailov2024from} successfully bypass the explicit use of reward models, they are still learning human preferences through implicit rewards.
Several work~\cite{liu2024decoding,chakraborty2024transferqstar,khanov2024args} also integrate the idea of inference-time decoding into RLHF, but they assume the reward model is already prepared by RLHF.
In multi-step reasoning, reward models are trained to evaluate the intermediate process of reasoning by LLMs, from datasets with process or outcome supervision, and then used as verifiers for sampling such as Best-of-N sampling~\citep{lightman2024lets_verify,wang2024math} or self-consistency decoding~\citep{wang2023selfconsistency,luo2024improve}.
In contrast to these lines of work, where rewards are used for either training LLMs or verifying inference, the reward model in our approach is directly trained for {\it classification or generation} from ground-truth labels or tokens through the same loss and dataset for standard fine-tuning, which may lead to a broad range of applications in various domains including language generation.