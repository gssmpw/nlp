\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{multirow} 
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{color}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{.8in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \title{\LARGE A Comparison of DeepSeek and Other LLMs}
  \author{
    Tianchen Gao, Jiashun Jin, Zheng Tracy Ke, and Gabriel Moryoussef \footnote{For contact, use jiashun@stat.cmu.edu or zke@fas.harvard.edu} 
  }
  \maketitle
  \medskip
} \fi


\if1\blind
{
  \title{A comparison of DeepSeek and other LLMs}
  \author{Tianchen Gao \hspace{.2cm}\\ 
    Department of Statistics, Harvard University \\  
    Jiashun Jin   \hspace{.2cm} \\
    Department of Statistics $\&$ Data Science, Carnegie Mellon University  \\ 
    Zheng Tracy Ke \hspace{.2cm} \\ 
    Department of Statistics, Harvard University  \\ 
    Gabriel Moryoussef \hspace{.2cm}  \\ 
    Department of Statistics $\&$ Data Science, Carnegie Mellon University}
  \maketitle
} \fi


\bigskip
\begin{abstract}
Recently,  DeepSeek has been the focus of attention in and beyond  the AI community. 
An interesting problem is how DeepSeek compares to other large language models (LLMs).   There are many tasks an LLM can do, and in this paper, 
we use the {\it task  of predicting an outcome 
using a short text} for comparison.  
We consider two settings, an authorship classification setting and a citation 
classification setting. In the first one, the goal is 
to determine whether a short text   is written by human or AI. 
In the second one, the goal is to classify a  citation  to one of  four types  
using the textual content.  
For each experiment, we compare DeepSeek with $4$ popular LLMs: Claude, Gemini, GPT, and Llama. 

\vspace{0.35 em} 

We find that,  in terms of classification accuracy,  DeepSeek outperforms Gemini, GPT, and Llama in most cases,  but underperforms Claude.  We also find that  DeepSeek is comparably slower than others but with a low cost to use, while 
Claude is much more expensive than all the others. 
Finally, we find that  in terms of similarity, the output  of DeepSeek is most similar to those of Gemini and Claude (and among all  $5$ LLMs, Claude and Gemini have the most similar outputs).  

\vspace{0.35 em} 


In this paper,   we also present a fully-labeled dataset   collected by ourselves,   
and propose a recipe where we can use the LLMs and a  recent data set,  MADStat,  
to generate new data sets.   
The datasets in our paper can  be used as benchmarks for future study on LLMs. 
\end{abstract}

\noindent%
{\it Keywords:}  Citation classification,  AI-generated text detection,   MADStat, prompt,  text analysis, textual content.  
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!


\section{Introduction} 
In the past two weeks, DeepSeek (DS),  a recent large language model (LLM) \citep{DeepSeek,deepseekai2025deepseekr1incentivizingreasoningcapability},   has shaken up the AI industry.  Since its latest version was released on January 20, 2025,  DS  has made the headlines of news and social media,  shot to the top of Apple Store's downloads, stunning investors and sinking some tech stocks including Nvidia.  

What makes DS so special is that in some benchmark tasks it achieved  the same or even better results as the big players in the AI industry (e.g., OpenAI's ChatGPT),  but with only a fraction of the training cost.  
For example, 
\begin{itemize} 
\item In \cite{DS-math}, the author showed that over $30$ challenging mathematical problems derived from the MATH dataset 
\citep{MATHdata}, DeepSeek-R1 achieves superior accuracy on these complex problems, compared with 
ChatGPT and Gemini, among others.  
\item  In a LinkedIn post on January 28, 2025, 
Javier Aguirre (a researcher specialized in medicine and AI,  South Korea) wrote:  ``I am quite impressed with Deepseek.  ....  
Today I had a really tricky and complex (coding) problem. Even chatGPT-o1 was not able to reason enough to solve it. I gave a try to Deepseek and it solved it at once and straight to the point".  This was echoed by several other researchers in AI. 
\end{itemize} 
See more comparison in \cite{DeepSeek, deepseekai2025deepseekr1incentivizingreasoningcapability,DeepSeek1, DeepSeek2}. 
 Of course, a sophisticated LLM has many aspects (e.g., Infrastructure,  Architecture,   Performances, Costs) and can achieve many tasks. The tasks discussed  above are only a small part of what an LLM can deliver.  It is desirable to have a more comprehensive and in-depth comparison. Seemingly, such a comparison may take a lot of time and efforts,  
but some interesting discussions  have  already appeared on the internet and social media (e.g.,  \cite{DS-comparison}).    

We are especially interested in how accurate an LLM is  in prediction.  
Despite that there are a long list of literature on this topic (see, for example, \cite{ESL}), using LLM for prediction still has advantages: while a classical approach may need a reasonable large training sample,  an LLM can work with only a prompt. 
Along this line, a  problem of major interest is  how DS compares to  other LLMs in terms of prediction accuracy.     
In this paper, we consider the two classification settings as follows. 
\begin{itemize} 
\item {\it Authorship classification (AC)}.   Determine whether a document is human generated (hum),   or  AI-generated (AI), or human-generated but with AI-editing (humAI). 
\item {\it Citation classification (CC)}.  Given an (academic) citation and the small piece of text surrounding it, 
determine which type  the citation is  (see below for the $4$ types of citations). 
\end{itemize}  
For each of the two settings,  we compare the classification results of DeepSeek-R1 (DS)  with those of  $4$ representative LLMs: OpenAI's  GPT-4o-mini (GPT),  Google's Gemini-1.5-flash (Gemini), 
Meta's Llama-3.1-8b (Llama)  and Anthropic's Claude-3.5-sonnet (Claude).  We now discuss each of the two settings with more details. 


\subsection{Authorship classification} 
\label{subsec:intro-AC}  
In the past two years,  AI-generated text content started to spread quickly, influencing the internet, workplace, and daily life.  This raises a problem: 
how to differentiate AI-authored content from human-authored content \citep{brown2020language,danilevsky2020survey}.  

The problem is interesting for at least two reasons. First, the  AI-generated content may contain harmful misinformation in areas such as health care, news, and finance \citep{brown2020language},  and the spread of fake and misleading information may threaten the integrity of online resources.  Second,  understanding the main differences between human-generated and AI-written content can significantly help  improve  AI language models  \citep{danilevsky2020survey}.

We approach the problem by considering  two classification settings, AC1 and AC2. 
\begin{itemize} 
\item {\it (AC1)}.  In the first setting, we focus on distinguishing human-generated text  and AI-generated text (i.e., hum vs. AI).  
\item {\it (AC2)}. In the second setting, we consider the more subtle setting of 
distinguishing text generated by human and text that are generated by human  but with AI editing (i.e., hum vs. humAI). 
\end{itemize} 
For experiments, we  propose to  use the recent MADStat data set  \citep{JBES, TextReview}.  MADStat is a large-scale data set on statistical publications, consisting of  the bibtex and citation information of 83,331 papers published in 36 journal in statistics and related field, spanning 1975-2015.  The data set is available for free download (see Section \ref{sec:main} for the download links). 

We propose a general recipe where we use the LLMs and MADStat to generate new data sets for our study. 
We start by selecting a few authors and collecting all papers authored by thesm in the MADStat. For each paper, the MADStat contains a title and an abstract. 
\begin{itemize} 
\item {\it (hum)}.  We use all the   abstracts as the  human-generated text.  
\item {\it (AI)}.   For each paper, we feed in the title to GPT-4o-mini and ask it to generate 
an abstract. We treat these abstracts as the AI-generated text. 
\item {\it (humAI)}.  For each paper, we also ask  GPT-4o-mini to edit the abstract. 
We treat  these abstracts as the humAI text.  
\end{itemize} 
Seemingly, using this recipe, we can generate many different data sets.   
These data sets provide a useful platform where we can compare 
different classification approaches, especially the $5$ LLMs. 

{\bf Remark 1}. {\it (The MadStatAI data set)}.  In Section \ref{subsec:CC},  we fix $15$ authors in the MADStat data set  (see Table \ref{tb:15-author} for the list) and generate a data set containing 582 abstract triplets  (each triplet contains three abstracts: hum, AI, and humAI)  following the recipe above.  For simplicity, we call this  data set the {\it MadStatAI}.    


 

Once the data set is ready, we apply each of the $5$ LLMs above for classification, 
with the same prompt. See Section \ref{subsec:AC} for details. 
Note that aside from LLMs,  we may apply other algorithms to this problem  \citep{solaiman2019release,zellers2019defending,gehrmann2019gltr,ippolito2020automatic,fagni2021tweepfake,adelani2020generating,alonAI}. However, as our focus in this paper is to compare DeepSeek with other LLMs, 
we only consider the $5$ LLM classifiers mentioned above.    




 
 


\subsection{Citation classification} 
\label{subsec:intro-CC}  
%How to evaluate the impact of a paper is an important problem in academic research and social science \cite{Hir2005, JBES, %TextReview, WSB2013,wang2022statistics}.  A popular evaluation metric is the number of (raw) citation counts. 
When a paper is being cited, the citation could be significant or insignificant. 
Therefore, to evaluate the impact of a paper, we are   interested in not only  how many times it is being cited, 
but also how many significant citations it has.  The challenge is, while it is comparably easier to 
count the raw citations of a paper (i.e., by Google Scholar, Web of Science), it is unclear how to 
count the number of `significant' citations of a paper. 

To address the issue, note that surrounding a citation instance, there is usually a short piece of text. The text contains important information for the citation, and we can use it to predict the type of this  citation.  
This gives rise to the problem of {\it Citation Classification}, where the goal is to use the short text surrounding a 
citation to predict the citation type. 

Here, we have two challenges. First, it is unclear how many different types 
academic citations may have and what these types are.  
Second, we do not have a ready-to-use data set. 

To address these challenges,  first,  after reviewing many literature works and empirical results,  we propose to divide all academic citations into four different types:  
\begin{itemize} 
\item  ``Fundamental ideas (FI)"
\item   ``Technical basis (TB)", 
\item  ``Background (BG)", 
\item  ``Comparison (CP)".
\end{itemize} 
Below for simplicity,  we encode the four types as ``1", ``2", ``3", ``4".  Note that 
the first two types are considered as significant, and the other two  are considered as  
comparably less significant.  See Section \ref{subsec:CC} for details. 
%{\color{red}  See Section \ref{subsec:CC}, where we explain why 
%these are all the types we consider  for academic citations, at least in  our study}.   
 



Second, with substantial efforts, we have {\it collected from scratch}  a new data set  by ourselves, which we call the {\it CitaStat}. In this data set, we downloaded all papers in $4$ representative journals in statistics between 1996 and 2020 in PDF format. 
These papers contain about $360K$ citation instances. For our study, we selected $n = 3000$ citation instances. 
For each citation,  
\begin{itemize} 
\item we write a code  to select the small piece of text surrounding the citation in the PDF file and convert it to usable text files. 
\item we manually label each citation to each of the $4$ citation types above. 
\end{itemize} 
See Section \ref{subsec:CC} for details. 
As a result, CitaStat is a fully labeled data set with $n = 3000$ samples, 
where each $y$-variable takes values in $\{1, 2, 3, 4\}$ (see above), and each $x$-variable  
is a short text which we call the {\it textual content} of the corresponding citation. 

We can now use the data set to compare the $5$ LLMs above for their 
performances in citation classification.  We consider two experiments. 
\begin{itemize} 
\item {\it (CC1)}. A $4$-class classification experiment where we use the CitaStat without any modification.  
\item {\it (CC2)}. A $2$-class classification experiment where we merge class ``1" and ``2" (`FI' and `TB') to a new class 
of ``S" (significant), and we merge class ``3" and ``4" (`BG' and `CP') to a new class of `I' (incidental). 
\end{itemize}   

 
\subsection{Results and contributions} \label{subsec:intro-results} 
We have applied all $5$ LLMs to the four experiments (AC1, AC2, CC1, CC2), and we have the following observations:  
\begin{itemize} 
\item In terms of classification errors,   Claude consistently outperforms all other LLM approaches.   DeepSeek-R1 underperforms Claude but outperforms Gemini, GPT, and Llama in most of the cases.  
GPT performs unsatisfactorily for AC1 and AC2,  with an error rate similar to that of random guessing, but it performs much better than random guessing for CC1 and CC2.  Llama performs unsatisfactorily: its error rates are either comparable to those of random 
guessing or even larger.   
\item In terms of computing time,  Gemini and GPT are much faster than the other three approaches, and DeepSeek-R1 
is the slowest (an older version of DeepSeek, DeepSeek-V3,   is faster but does not perform as well as DeepSeek-R1). 
\item In terms of cost, Claude is  much more expensive for a customer than other approaches.  For example, for CC1 and CC2 altogether, Claude costs $\$12.30$, Llama costs $\$1.2$, and the other three methods (DeepSeek, Gemini and GPT) cost no more than $\$0.3$. 
\item In terms of output similarity,   DeepSeek is most similar to Gemini and Claude (GPT and Llama are highly similar in AC1 and AC2, but both perform relatively unsatisfactorily). 
\end{itemize} 
Table \ref{table:ranking} presents  the ranks of different approaches in terms of error rates (the method with the lowest error rate is assigned a rank of $1$).   The average ranks suggest that DeepSeek outperforms Gemini, GPT, and Llama, but 
underperforms Claude (note that for CC1 and CC2, we have used two versions of DeepSeek, R1 and V3; 
the results in Table \ref{table:ranking} are based on R1. If we use V3, then 
DeepSeek ties with Gemini in average rank; it still  outperforms GPT and Llama). See Section 
\ref{sec:main} for details.  


\spacingset{1.2} 
%%%%%%%%%
%%%%%%%%%
%%%%%%%%%
\begin{table}[htb!]
\centering
\scalebox{1}{
\begin{tabular}{lccccc}
\toprule
  &Claude  & DeepSeek & Gemini & GPT & Llama \\   
  \midrule
Experiment AC1 (hum vs. AI)   &1 &2  &3  &5 &4    \\ 
Experiment AC2 (hum vs. humAI) & 2 & 1& 3 & 5 & 4 \\ 
Experiment CC1 (4-class)  & 1 &4 & 2& 3 & 5 \\ 
Experiment CC2 (2-calss)  & 1 &2 & 3& 4 & 5  \\  
\midrule
Average Ranking & 1.25  &2.25  &2.75  & 4.25  & 4.50  \\ 
\bottomrule
\end{tabular}}
\caption{The rankings of all $5$ LLM approaches in terms of error rates.}
\label{table:ranking}
\end{table}
\spacingset{1.45}



Overall, we find that Claude and DeepSeek have the lowest error rates, but 
Claude is relatively expensive and DeepSeek is relatively slow. 


 
We have made the following contributions.  First, as DeepSeek 
has been the focus of attention in and beyond the  AI community,   there is a timely need to understand how 
it  compares  to other popular LLMs.  Using two interesting classification problems, 
we demonstrate that DeepSeek is competitive in the task of predicting an outcome using a short piece of text.  
Second, we  propose citation classification as an interesting new problem, the understanding of which 
will help evaluate the impact of academic research. 
Last but not the least,  we provide CitaStat as a new data set which can be used for evaluating academic research. We also propose a general recipe for generating new data sets (with the MadStatAI as an example) for studying AI-generated text. 
These data sets can be used as benchmarks to compare 
different algorithms, and  to learn the differences between 
human-generated text and AI-generated text. 

 
%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%    
\section{Main results} 
\label{sec:main} 
In this section, we describe our numerical results on the two problems, authorship classification and citation classification, and report the performances of all 5 LLMs. 

\subsection{Authorship classification} 
\label{subsec:AC} 

The MADStat  \citep{JBES,TextReview} contains over 83K  abstracts, but it is time-consuming to process all  of them.\footnote{MADStat stands for Multi-Attributed Dataset on Statisticians.  MADStat is available 
for free download at \url{http://zke.fas.harvard.edu/MADStat.html} or in the \href{https://doi.org/10.1080/07350015.2021.1978469}{supplementary material} of \cite{JBES}.}
 We selected a small subset as follows: First, we restricted to authors who had over 30 papers in MADStat. Second, we randomly drew 15 authors from this pool by sampling without replacement. Each time a new author was selected, we checked if he/she had co-authored papers with previously drawn authors; if so, we deleted this author and drew a new one, until the total number of authors reached 15. Finally, we collected all 15 authors' abstracts in MADStat. This gave rise to a data set with 582 abstracts in total (see Table~\ref{tb:15-author}).    



\spacingset{1.2}
\begin{table}[htb!]
\centering
\scalebox{.9}{
\begin{tabular}{lc|lc|lc}
\toprule
Name & \#abstracts & Name & \#abstracts & Name & \#abstracts\\
\midrule
Andrew Gelman & 40 & Anthony Pettitt & 60 & Damaraju Raghavarao &31\\
David Nott & 35 & Frank Proschan &39 & Ishwar Basawa &53\\
Ngai Hang Chan &32 & Nicholas I. Fisher & 32  & Peter X. K. Song & 32\\
Philippe Vieu & 31 &Piet Groeneboom & 30 & Richard Simon & 45\\
Sanat K. Sarkar & 44  & Stephane Girard & 33 & Yuehua Wu & 45 \\
\bottomrule
\end{tabular}}
\caption{The 15 selected authors and their numbers of abstracts in MADStat.} \label{tb:15-author}
\end{table} 
\spacingset{1.45}

For each original human-written abstract, we used GPT-4o-mini to get two variants. 
\begin{itemize}
\item The AI version. We provided the paper title and requested for a new abstract. The prompt is \textit{``Write an abstract for a statistical paper with this title: [paper title]."} 

\item The humAI version. We provided the original abstract and requested for an edited version. The prompt is \textit{``Given the following abstract, make some revisions. Make sure not to change the length too much. [original abstract]."}
\end{itemize}
Both variants are authored by AI, but they look differently. The AI version is usually significantly different from the original abstract, so the `human versus AI' classification problem is comparably easier. 
For example, the left panel of Figure~\ref{fig:length} is a comparison of the length of the original abstract with that of its AI version. The length of human-written abstracts varies widely, while the length of AI-generated ones is mostly in the range of 100-200 words. The humAI version is much closer to the original abstract, typically only having local word replacements and mild sentence re-structuring. In particular, its length is highly correlated with the original length, which can be seen in the right panel of Figure~\ref{fig:length}.   


\begin{figure}[h!]
  \centering
    \includegraphics[width=.49\textwidth]{PDF/abstract_lenght_generated.pdf}
    \includegraphics[width=.49\textwidth]{PDF/abstract_lenght_edited.pdf}
  \caption{Comparison of the lengths of  human-generated and AI-generated abstracts}
  \label{fig:length}
\end{figure}


%Figure \ref{fig:length} illustrates the relationship between abstract lengths across three categories: original human-authored texts, their AI-generated counterparts, and humAI versions. Two distinct patterns emerge: (1) Edited abstracts exhibit strong length conservation as expected, with their original version, while (2) purely AI-generated abstracts demonstrate minimal length correlation ($r \approx Y$) with human originals, suggesting fundamentally different composition approaches.


As mentioned,  we consider two classification problems:
\begin{itemize} 
\item (AC1).   A $2$-class classification problem of `human versus AI', 
\item (AC2).   A $2$-class classification problem of  `human versus humAI'. 
\end{itemize}  
For each problem, there are $582\times2 = 1164$ testing samples, half from each class.  We input them into each of the 5 LLMs using the same prompt: \textit{``You are a classifier that determines whether text is human-written or AI-edited. Respond with exactly one word: either `human' for human-written text or `ChatGPT' for AI-written text. Be as accurate as possible."}

Note that comparing with classification approaches (e.g., SVM, Random Forest \citep{ESL}), 
an advantage of using an LLM for classification is that, we do not need to provide any training sample.  
All we need is to input the LLM with a prompt. 




Table~\ref{table:accuracy_llms} summarizes the performances of 5 LLMs. 
For `human vs AI' (AC1), Claude-3.5-sonnet has the best error rate 0.218, and DeepSeek-R1 has the second best one 0.286. %The other methods are significantly worse. 
%Llama-3.1-8b always predicts `human-written' (AC2), so its error rate is exactly 0.5.
%Gemini-1.5-flash is slightly better than random guessing. Gpt-4o-mini and Llama-3.1-8b are even worse than random guessing. 
The remaining three methods almost always predict `human-written', which explains why their error rates are close to 0.5. 
For `human vs humAI' (AC2), since the problem is harder, the best achievable error rate is much higher than that of `human vs AI' (AC1).  DeepSeek-R1 has the lowest error rate 0.405, and Claude-3.5-sonnet has the second best one 0.435. The error rates of the other three methods are nearly 0.5. In conclusion, Claude-3.5-sonnet and DeepSeek-R1 are the winners in terms of error rate. If we also take the running time into account, Claude-3.5-sonnet has the best overall performance.  On the other hand, the cost of Claude-3.5-sonnet is the highest. 
 
\spacingset{1.2}
\begin{table}[h!]
\centering
\scalebox{1}{
\begin{tabular}{l | c r c | c r c}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{human vs. AI} & \multicolumn{3}{c}{human vs. humAI}\\
\cline{2-7}
& \textbf{Error} & \textbf{Runtime} & \textbf{Cost }& \textbf{Error} & \textbf{Runtime} & \textbf{Cost}\\
\hline
Claude-3.5-sonnet & 0.218  & 7 min & \$ 0.5 USD & 0.435 & 7 min &\$ 0.3 USD\\
DeepSeek-R1 & 0.286  & 235 min & \$ 0.05 USD &  0.405 & 183 min & \$ 0.04 USD\\
Gemini-1.5-flash & 0.468 & 6 min &\$ 0.1 USD & 0.500 & 6 min &\$ 0.09 USD \\
GPT-4o-mini & 0.511 & 7 min &\$ 0.1 USD & 0.502 & 8 min &\$ 0.12 USD\\
Llama-3.1-8b &  0.511 & 11 min & \$ 0.2 USD& 0.501 & 12 min &\$ 0.17 USD\\
\bottomrule
\end{tabular}}
\caption{The classification errors, runtime, and costs of 5 LLMs for authorship classification. (In `human vs AI' (AC1), if we report four digits after the decimal point, Llama-3.1-8b has a lower error than GPT-4o-mini. This is why they have different ranks for AC1 in Table~\ref{table:ranking}.)}
\label{table:accuracy_llms}
\end{table}
\spacingset{1.45}


Since the 1164 testing abstracts come from 15 authors, we also report the classification error for each author (i.e., the testing documents only include this author's human-written abstracts and the AI-generated variants). 
Figure~\ref{fig:boxplot} shows the boxplots of per-author errors for each of 5 LLMs. 
Since authors have different writing styles, these plots give more information than Table~\ref{table:accuracy_llms}. For `human vs AI' (AC1), Claude-3.5-sonnet is still the clear winner. For `human vs humAI' (AC2),  DeepSeek-R1 still has the best performance. Furthermore, its advantage over Claude-3.5-sonnet is more visible in these boxplots: Although the overall error rates of two methods are only mildly different, DeepSeek-R1 does have a significantly better performance for some authors. 


\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.85\textwidth]{PDF/boxplot_llm.pdf}
    %\caption{Error rates for different LLM models in authorship classification tasks. Left: Performance on distinguishing between human-written and AI-generated abstracts. Right: Performance on distinguishing between human-written and human-AI hybrid abstracts. The horizontal dashed line represents random guessing performance ($0.5$ error rate).}
    \caption{The boxplots of per-author classification errors.}
    \label{fig:boxplot}
\end{figure}

We also investigate the similarity of predictions made by different LLMs.  
For each pair of LLMs, we calculate the percent of agreement on predicted labels, in both the `human versus AI' (AC1)  setting and `human versus humAI' (AC2) settings. The results are in Figure~\ref{fig:agreement-humanai}. 
For both settings,  Gemini-1.5-flash, GPT-4o-mini, and Llama-3.1-8b have extremely high agreements with each other. 
This is because all three models predict `human-written' for the majority of samples. DeepSeek-R1 and Claude are different from the other three, and they have 64\% and 70\% agreements with each other in two settings, respectively. 
%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!ht]
	\centering
	\includegraphics[height=.439\textwidth, trim=0 0 100 0, clip=true]{PDF/llm_agreement_heatmap_c1.pdf} 
	\includegraphics[height=.439\textwidth]{PDF/llm_agreement_heatmap_c2.pdf}
	\caption{The prediction agreement among 5 LLMs in detecting AI from human texts. Left: `human versus AI' (AC1). Right: 
	`human versus humAI' (AC2). Take the cell on the first row and second column (left panel) for example: 
	for $70\%$ of the samples, the predicted outcomes by DeepSeek-R1 and Claude-3.5-sonnet are exactly the same.} 
	\label{fig:agreement-humanai}
\end{figure}


\subsection{Citation classification} 
\label{subsec:CC} 
The MADStat only contains meta-information and abstracts, rather than full papers. We created a new data set, CitaStat, by downloading full papers and extracting the text surrounding citations. 
Specifically, we restricted to those papers published during 1996-2020 in 4 representative journals in statistics: {\it Annals of Statistics}, {\it Biometrika}, {\it Journal of the American Statistical Association}, and {\it Journal of the Royal Statistical Society Series B}. 
We wrote code to acquire PDFs from journal websites, convert PDFs to plain text files, and then extracted the sentence (using SpaCy, a well-known python package for sentence parsing) containing each citation (we call it a `citation instance'). There are over 367K citation instances in total. We randomly selected $n=3000$ of them and manually labeled them using one of the following four categories: 
\begin{itemize}
\item  `Fundamental Idea (FI)' (previous work that directly inspired or provided important ideas for the current paper). Example:  
			{\it ``The proposed class of discrete transformation survival models is originally motivated from the continuous generalized odds-rate model by Dabrowska and Doskum (1988a) with time-invarying covariate and Zeng and Lin (2006) for..."}
\item `Technical Basis (TB)' (important tools, methods, data sets, and other resources).  Example: {\it  ``We solve this system numerically via the Euler method (Protter and Talay 1997;Jacod 2004) with a time-step of one day..."}
\item `Background (BG)' (background, motivations, related studies, and examples to support/illustrate points).  Example: {\it ``Estimation of current and future cancer mortality broken down by geographic area (state) and tumor has been discussed in a number of recent articles, including those by Tiwari et al. (2004) ..."} 
\item `Comparison (CP)'  (comparison of methods or theoretical results). Example: {\it ``Another way of determining the number of neuron pairs is to follow Medeiros and Veiga (2000b) and Medeiros et al. (2002) and use a sequence of..."}
\end{itemize}
These definitions were inspired by existing studies on citation types \citep{moravcsik1975some,teufel2006automatic,dong2011ensemble}. 
% {\color{red} can we add a short remark explaining why these classes are reasonable and why we do not need more classes} 
Occasionally, two categories may overlap. For example, a reference is cited as providing a fundamental idea, and a comparison with it is also stated in the same sentence. In this case, we label it as `Fundamental Idea (FI)', to highlight that this is more important than a general comparison. 
%Additionally, we treated the first two categories as `incidental' citations, and the last two as `important' citations. 
There were 20 citation instances for which manual labeling gave `not sure'. We removed them and obtained $n=2980$ labeled samples (see Table~\ref{tb:CitaStat}). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\centering
\scalebox{1}{
\begin{tabular}{c|cccc|c}
\toprule
& Background & Comparison & Fundamental Idea & Technical Basis & Total\\
\midrule
Count & 1721 & 316 & 169 & 774 & 2980\\ 
Fraction & 57.8\% & 10.6\% & 5.7\% & 26\% & -\\
\bottomrule
\end{tabular}}
\caption{The distribution of four classes in CitaStat.} \label{tb:CitaStat}
\end{table}





%\begin{table}[!ht]
%	\caption{\small The definition of four citation categories.}
%	\label{Tab:overview_citation}
%	\centering
%	\scalebox{.73}{
%		\begin{tabular}{lp{7.6cm}p{7.6cm}}
%			\toprule
%			\textbf{Name} & \textbf{Description} & {\bf Example} \\
%			\midrule
%			Background & Citations that include descriptions of the research background, summaries of previous or recent studies and methods in a general way, and examples to support and illustrate points. 
%			& {\it \small ``Estimation of current and future cancer mortality broken down by geographic area (state) and tumor has been discussed in a number of recent articles, including those by Tiwari et al. (2004), Ghosh and Tiwari (2007)..."}\\
%			\midrule
%			Fundamental Idea & Citations about the previous work that inspired or provided ideas for current paper & 
%			{\small\it ``The proposed class of discrete transformation survival models is originally motivated from the continuous generalized odds-rate model by Dabrowska and Doskum (1988a) with timeinvarying covariate and Zeng and Lin (2006) for..."}\\
%			\midrule
%			Technical Basis & Citations of important tools, methods, data, and other resources used in current paper. & {\it \small ``We solve this system numerically via the Euler method (Protter and Talay 1997;Jacod 2004) with a time-step of one day..."}\\
%			\midrule
%			Comparison & Citations that compare methods or results with those of this paper & {\it\small ``Another way of determining the number of neuron pairs is to follow Medeiros and Veiga (2000b) and Medeiros et al. (2002) and use a sequence of..."}\\
%			\bottomrule
%	\end{tabular}}
%\end{table}


With this CitaStat data set, we consider two problems as mentioned:
\begin{itemize}
\item (CC1). The 4-class classification problem: Given the textual content of a citation (i.e., the text surrounding the  citation),  we aim to classify it to one of the four classes. 
\item (CC2).  The 2-class classification problem: We re-combine the four categories into two, where `Fundamental Idea'  and `Technical Basis'  are considered as `Significant (S)', and Background and Comparison are considered as `Incidental (I)'.\footnote{As mentioned, when a citation potentially belongs to two categories (e.g., `Fundamental Idea' and `Comparison'), we always manually label it to the more `significant' one (e.g., `Fundamental Idea'). This prevents mis-interpreting important comparisons as `incidental' citations in the manual labeling.}
Given the textual content of a citation, we aim to predict whether it is a `Significant (S)' citation. 
\end{itemize}

For each of the 5 LLMs, we used prompts to get classification decisions. 
Unlike the previous authorship classification problem, the class definitions in this problem are not common knowledge and need to be included in the prompt. In the 2-class problem, we use the prompt as in Figure~\ref{fig:prompt-CC}.  It provides definitions, examples, and how the four classes are re-combined into two, aiming to convey to the LLM as much information as possible. The prompt for the 4-class problem is similar, except that the description of grouping 4 classes into 2 is removed and the requirement for output format is modified (see Figure~\ref{fig:prompt-CC}). 




\begin{figure}[htb!]
\centering
\scalebox{.9}{
\fbox{
\begin{minipage}{1\textwidth}
{\it The content in the text comes from a paragraph in an academic paper A that includes citations. Please classify the citation [Reference Key] appearing in the following text into one of the categories: 
\begin{itemize} \itemsep 0pt
\item Background (citations that include descriptions of the research background, summaries of previous or recent studies and methods in a general way, and examples to support and illustrate points. For example, [Example 1], 
 %`The Gibbs sampler is also used in image analysis [Grenander (1983) and Geman and Geman (1984)]') , 
\item Comparison (citations that compare methods or results with those of this paper. For example, [Example 2], 
%`Figure 1 gives the 90 LR and bootstrap bands for .... Method 1 is the band of Doss and Gill (1992).'), 
\item Fundamental idea (citations about the previous work that inspired or provided ideas for this paper. For Example, [Example 3], %`Second, and possibly more important, it is a simple model selection device along the lines of Malec and Sedransk (1994).'), 
\item Technical basis (citations of important tools, methods, data, and other resources used in this paper. For example, [Example 4]. %`For a derivation of the projections, see Chamberlain (1986) or Cosslet (1987)'). 
\end{itemize}
Furthermore, we consider Background or Comparison as Incidental, and Fundamental idea or Technical basis as Important.

Text: [Citation text]
%`\textbf{`This is based on the following reasoning, applied by Brown and Low (1996) to compare Gaussian white noise models.}", 

Please reply only with one of the following: Important or Incidental.}
\end{minipage}}}
\caption{The prompt for 2-class citation classification, where [Reference Key] is the phrase in the text representing this reference, and [Example 1] is an example text from Background (other categories are similar). The prompt for 4-class classification is similar, except that the sentence {\it ``Furthermore, we consider ..."} is removed and the last sentence is changed to {\it ``Please reply only with one of the following: Background, Comparison, Fundamental idea, or Technical basis."}
} \label{fig:prompt-CC}
\end{figure}


%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=.9\textwidth]{PDF/Prompt}
%	\caption{The prompt we use, where the bolded text is from the testing sample.} \label{fig:prompt}
%\end{figure}


%\subsection{Results}

We examined the performances of all 5 LLMs. Since the runtime of DeepSeek-R1 is much longer than other methods, we only implemented it on 149 randomly selected samples (which included 5\% of all samples and maintained the same class fractions as the full data set) to evaluate its classification error rate. Meanwhile, we ran DeepSeek-V3, an earlier-released version of DeepSeek-R1, in all samples. The results are reported in Table~\ref{tb:CC-results}. 

%We use six of the most popular large language models (LLMs) to predict citation categories, including OpenAI's GPT-4o-mini, DeepSeek's DeepSeek-V3 and DeepSeek-R1, Google's Gemini-1.5-flash, Meta's Llama3, and Anthropic's claude-3.5-sonnet. Note that the runtime of the deepseek-R1 model is relatively long. Therefore, we randomly selected 5\% of the citation instances from the 2980 four-class labels, totaling 149 instances. We used this small sample to predict both the 4-class and 2-class labels using deepseek-R1. Running the 4-class prediction on this small sample takes approximately 3-4 hours. 

For the 4-class problem, Claude-3.5-sonnet has the lowest error rate at 0.327, followed closely by Gemini-1.5-flash at 0.347. DeepSeek-R1 outperforms DeepSeek-V3 but underperforms other methods except Llama-3.1-8b. For the 2-class problem, Claude-3.5-sonnet still achieves the best performance with an error rate of 0.261. DeepSeek-R1 is the second best, with an error rate of 0.275. Gemini-1.5-flash is worse than DeepSeek-R1 but slightly better than DeepSeek-V3. 
In summary, Claude-3.5-sonnet is the winner in terms of error rate. 

Regarding the runtime, GPT-4o-mini and Gemini-1.5-flash are the fastest (especially, GPT-4o-mini only spent 15 minutes). DeepSeek-V3 and Llama-3.1-8b are relatively slow, requiring a few hours. 
Regarding the cost, DeepSeek-V3 is the cheapest one, and Claude-3.5-sonnet is significantly most expensive than the other methods. 


\spacingset{1.2}
\begin{table}[!ht]
	\centering
	\scalebox{1}{
		\begin{tabular}{lcclc}
			\toprule
			\textbf{Model} & \textbf{Error (4-class)} & \textbf{Error (2-class)} & \textbf{Runtime} & \textbf{Cost} \\
			\midrule
			Claude-3.5-sonnet & 0.327 & 0.261 & 1-2 h & \$ 12.30 USD \\
			DeepSeek-V3 & 0.432 & 0.332 & 3-4 h & ¥ 0.60 RMB \\
			DeepSeek-R1 $\dag$ & 0.403 & 0.275  & - & - \\
			Gemini-1.5-flash & 0.347 & 0.313 & 25 min & \$ 0.12 USD \\
			GPT-4o-mini & 0.363 & 0.371 & 15 min & \$ 0.30 USD  \\
			Llama-3.1-8b & 0.576 & 0.457 & 4-5 h & \$ 1.20 USD \\
			\bottomrule
	\end{tabular}}
	\caption{The error rate, runtime and cost of 6 LLMs for citation classification. ($\dag$The error rates of DeepSeek-R1 were evaluated on only 5\% randomly selected samples. It took about 3-4 hours to run DeepSeek-R1 in this small sample.)} 	\label{tb:CC-results}
\end{table}
\spacingset{1.45}








%We compare them across three dimensions: error rate, runtime, and cost. See more details in Table~\ref{Tab:result}. For the 4-class setting, Claude-sonnet model achieves the lowest error rate at 0.327, followed closely by Gemini-1.5-flash at 0.347. For the DeepSeek models, DeepSeek-R1 performs better than DeepSeek-V3, but still falls behind Claude-sonnet, Gemini-1.5-flash, and  GPT-4o-mini. For the 2-class setting, Claude-sonnet exhibits the best performance with an error rate of 0.261. DeepSeek-R1 ranks second in performance, with an error rate of 0.275. Regarding runtime, GPT-4o-mini and Gemini-1.5-flash stand out as the fastest models, with runtime durations of 15 minutes and 25 minutes, respectively. In contrast, DeepSeek-V3 and Llama3 require several hours (3–4 hours and 4–5 hours, respectively). 
%In terms of cost, DeepSeek-V3 model is the cheapest. Although the Claude-sonnet model performs the best in both the 4-class and 2-class tasks, its cost is the highest.



%GPT-4o-mini outperforms DeepSeek-V3 in terms of accuracy among 3000 citation instances. However, for the 2-category classification task, DeepSeek-V3 achieves higher accuracy than GPT-4o-mini. In terms of runtime, predicting labels for 3,000 citation instances using the GPT-4o-mini API takes approximately 15 minutes, whereas DeepSeek requires 2–3 hours. This may be due to high server traffic on DeepSeek, leading to slower and less stable response times. Regarding cost, DeepSeek is more economical than GPT-4o-mini. Predicting labels for 3,000 citation instances via its API costs only 0.6 RMB. 

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.6\textwidth]{PDF/Difficult_vs_Easy}
%	\caption{The error rate of the 5 LLMs in Easy, Medium, and Difficult cases.} 
%	\label{fig:easydifficult}
%\end{figure}



Additionally, in the 4-class setting, we divided all samples into three groups according to the average prediction error by 5 LLMs (this is an average of 5 binary values; we exclude DeepSeek-R1 here, as we don't have the results on all samples). The lowest 30\%, middle 40\%, and highest 30\% are called the Easy, Medium, and Difficult case, respectively. Table~\ref{tb:easydifficult} shows the per-group error rates of all 5 LLMs. 
In the Easy case, the error rates of all methods except Llama-3.1-8b are less than 0.01. 
In the Difficult case, all methods perform poorly, with GPT-4o-mini attaining the lowest error rate at 0.832 and DeepSeek-V3 attaining the highest at 0.956. In the Medium case, Claude-3.5-sonnet performs well with an error rate of 0.177, followed by Gemini-1.5-flash with a similar error rate 0.211. Llama-3.1-8b shows a notably higher error rate of 0.732.

\spacingset{1.2}
\begin{table}[!ht]
\centering
\scalebox{1}{
\begin{tabular}{lccccc}
\toprule
& Claude-3.5-sonnet & DeepSeek-V3 & Gemini-1.5-flash & GPT-4o-mini & Llama-3.1-8b\\
\hline
Easy & .001 & .007 & .004 & .010 & .063 \\
Medium & .177 & .358 & .211 & .277 & .732 \\
Difficult & .851 & .956 & .870 & .832 & .881\\
\bottomrule
\end{tabular}}
\caption{The per-group error rates for 5 LLMs in citation classification.} 	\label{tb:easydifficult}
\end{table}
\spacingset{1.45}






%Figure \ref{fig:easydifficult} illustrates the error rates of five LLMs across three levels of case difficulty: Difficult, Medium, and Easy. The models compared include Gemini-1.5-flash, DeepSeek-V3, GPT-4o-mini, Llama3, and Claude-sonnet.
%For the difficult cases, GPT-4o-mini achieves the lowest error rate at 85.1\%, followed by Claude-sonnet at 85.1\%. However, all models exhibit relatively high error rates in this case, with DeepSeek-V3 having the highest error rate of 95.6\%.
%For medium cases, there is a significant variance in performance. Claude-sonnet continues to perform well with an error rate of 17.7\%, with  Gemini-1.5-flash showing a similar error rate at 21.1\%. Llama3 shows a notably higher error rate of 73.2\%.
%In contrast, the error rates for easy cases are much lower across all models. Claude-sonnet leads this case with the lowest error rate of 0.1\%, followed by Gemini-1.5-flash at 0.4\% and DeepSeek-V3 at 0.7\%.
%


	

Finally, we investigate the similarity of predictions made by different LLMs. For DeepSeek-R1, since we only implemented it on 5\% of samples, we excluded it in the comparison.  
For each pair of the remaining 5 LLMs, we calculated the percent of agreement on predicted labels, in both the 4-class and 2-class settings. The results are given in Figure~\ref{fig:agreement}. 

In the 4-class citation classification setting (CC1), except for Llama-3.1-8b, the percent of agreement between any other two LLMs is above $70\%$. Especially, Claude-3.5-sonnet and Gemini-1.5-flash have the highest level of agreement at $77\%$. Llama-3.1-8b has relatively low agreements with other models.
In the 2-class setting, there is a high level of agreement among the 3 LLMs: Claude-3.5-sonnet, DeepSeek-V3, and Gemini-1.5-flash; especially, the agreement between the last two is 83\%. 
Still, Llama-3.1-8b has relatively low agreements with other models. 
%For DeepSeek-V3, it has the highest agreement with Gemini-1.5-flash. 
In summary, Gemini-1.5-flash and Claude-3.5-sonnet have a consistently high agreement. For DeepSeek-V3, it has the highest agreement with Gemini-1.5-flash, and its level of agreement with GPT-4o-mini is only in the medium rage. 
%in particular, the agreement between DeepSeek-V3 and GPT-4o-mini is relatively low. 


%Claude-3.5-sonnet and Gemini-1.5-flash attain the highest level of agreement: $77\%$ in the 4-class setting and $78\%$ in the 2-class setting, suggesting a strong correlation between their predictions.  
%Llama-3.1-8b has relatively low agreements with other models. 
%For GPT-4o-mini, it has the highest agreement with Gemini-1.5-flash and lowest with DeepSeek-V3. 
%For DeepSeek-V3, it has a highest agreement with Gemini-1.5-flash and lowest with Llama-3.1-8b. 
%In summary, Gemini-1.5-flash have a high agreement with most other models, especially Claude-3.5-sonnet. DeepSeek  does not have a high agreement with other models, except Gemini-1.5-flash; in particular, the agreement between DeepSeek-V3 and GPT-4o-mini is relatively low.  




%In the 4-class setting, to compare the prediction results of the five different LLMs, we calculate the pairwise correlations between them. Note that due to time-consuming, we only run DeepSeek-R1 in a small sample. Thus we do not compare the result of DeepSeek-R1 with other LLMs. 
%Figure \ref{fig:agreement} presents a heatmap depicting the correlation among five LLMs. It can be seen that Claude-sonnet and Gemini-1.5-flash show the highest correlation (0.689), indicating that their performance is relatively similar. Similarly, Gemini-1.5-flash and GPT-4o-mini also exhibit a strong positive correlation (0.669). 
%On the other hand, Llama3 appears to be less correlated with the other models, with correlation values below 0.4 in most cases, particularly with Claude-sonnet (0.367) and GPT-4o-mini (0.378). The weakest correlation is observed between Llama3 and Claude-sonnet (0.367), suggesting a notable difference in their performance patterns. 

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=.7\textwidth]{PDF/correlation_LLM}
%	\caption{Heatmap of the correlation among five LLMs in 4-class citation classification task.} 
%	\label{fig:correlation}
%\end{figure}
%
%
%
%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=.7\textwidth]{PDF/agreement_LLM}
%	\caption{Heatmap of the prediction agreement among five LLMs in 4-class citation classification task.} 
%	\label{fig:agreement_4class}
%\end{figure}




%In the 2-class setting, we calculate the percentage of agreement between each pair of models, as shown in Table \ref{Tab:agreement}. This data provides insight into how similar the models' predictions are from one another. Gemini-1.5-flash shows generally higher agreement across the board, particularly with Claude-sonnet (77.8\%) and Llama3 (0.672\%). This suggests that Gemini-1.5-flash's predictions are relatively consistent with the other models. 


\begin{figure}[!htb]
	\centering
	\includegraphics[height=.436\textwidth, trim= 0 0 70 0, clip=true]{PDF/agreement_LLM} 
	\includegraphics[height=.436\textwidth]{PDF/agreement_LLM_2class}
	\caption{The prediction agreement among 5 LLMs in citation classification.} 
	\label{fig:agreement}
\end{figure}

In summary, DeepSeek underperforms Claude, but consistently outperforms Gemini, GPT, and Llama.  
Also, DeepSeek is computationally slower than others, and Claude is much more expensive than others. 
Given that DeepSeek is a new LLM where the training cost is only 
a fraction of that of other LLMs, we expect that in the near future,  DeepSeek will grow substantially and may  become   the most appealing LLM approach for our study. 

  







%\begin{table}[!ht]
%	\caption{The percentage of agreement between each pair of 5 LLMs in 2-class citation classification task.}
%	\label{Tab:agreement}
%	\centering
%	\begin{tabular}{c|ccccc}
%		\hline
%		 & DeepSeek-V3 & GPT-4o-mini & Gemini-1.5-flash & Llama3 & Claude-sonnet\\
%		\hline
%		DeepSeek-V3 & 1.000 & 0.588 & 0.671 & 0.408 & 0.630\\
%		GPT-4o-mini & 0.588 & 1.000 & 0.694 & 0.631 & 0.661\\
%		Gemini-1.5-flash & 0.671 & 0.694 & 1.000 & 0.672 & 0.778\\
%		Llama3 & 0.408 & 0.631 & 0.672 & 1.000 & 0.627 \\
%		Claude-sonnet & 0.630 & 0.661 & 0.778 & 0.627 & 1.000\\
%		\hline
%	\end{tabular}
%\end{table}	

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=.7\textwidth]{PDF/agreement_LLM_2class}
%	\caption{Heatmap of the prediction agreement among five LLMs in 2-class citation classification task.} 
%	\label{fig:agreement_2class}
%\end{figure}

%\subsubsection{Easy vs Difficult}
%
%
%
%Overall, the results highlight that while all models struggle with more difficult cases, Claude-sonnet consistently outperforms the others in medium and easy cases. Llama3 performs poorly in all three cases, especially in the medium case, where its error rate is more than twice that of the other methods. The DeepSeek method performs well in easy cases but shows average performance in difficult cases, suggesting it may be more suitable for handling simpler tasks. On the other hand, GPT-4o-mini performs best in difficult cases, but only ranks fourth in easy cases. 
%



 



% 
%\subsection{Summary} 
%\label{subsec:summary} 

  

\section{Discussion} 
 Since the release of its latest version on January 20, 2025,  DeepSeek has been the focus of the attention in and beyond the AI community.   It is desirable to investigate how it  compares to other popular LLMs. 
In this paper, we compare  DeepSeek with $4$ other popular LLMs (Claude, Gemini, GPT, Llama)  
with the task of predicting an outcome using a short piece of 
text.  We consider two settings, an authorship 
classification setting and a citation classification setting.  
In these settings, we find that in terms of the prediction accuracy,  DeepSeek outperforms Gemini, GPT, and Llama in most cases, but consistently underperforms Claude. 

Our work can be extended in several directions. First, it is desirable 
to compare these LLMs with many other tasks (e.g., natural 
language processing,  computer vision, etc.).    For example, 
we may use the ImageNet data set \citep{ImageNet} to compare these LLMs and see which AI is more accurate in classification. 
Second, for both classification problems we considered in this paper,  it is of interest to further improve the performance of the LLM by combining 
tools in statistics and machine learning. Take the authorship classification for example. 
We can first use statistical tools to suggest a list of words that are discriminative between AI-generated 
text and human-generated text. We then construct a new prompt by combining these words with the 
prompt used earlier in our paper. With the new prompt, we expect that the performance of an  LLM 
can be much improved. 
See our forthcoming manuscripts \citep{manbert,DetectAI}  for example.   
Last but not the least,  the datasets we generated can be used not only as a platform to compare 
different approaches, but also as useful data to tackle many interesting problems.  
For example,  the MadStatAI  data set can be used to identify the patterns of AI generated documents, and the CitaStat data set can be used to tackle problems such as author ranking or estimating the research interest of an author (see for example \cite{JBES} and \cite{TextReview}).  


 


  
 
%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%   
\bibliographystyle{chicago}
\bibliography{refs}

\end{document}
