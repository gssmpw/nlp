\section{Related Works}
During the last decade, the research in precession agriculture has been evolving based on deep learning models \cite{c1,c2}. Deep learning facilitates the automated monitoring of trees in a large plantation, reducing human efforts and time. Several efforts are found in the literature for crop and plantation monitoring using satellite images, e.g., hyperspectral images \cite{afe_23}. Fewer efforts can be found applying deep learning models on UAV-based RGB images \cite{c3}. Yeh et al. applied the two-stage object detection model mask R-CNN to identify the crop area in the image \cite{afe_23_2}. They further applied Alexnet model on the detected regions for classification of crops. Recent studies \cite{c10,cvip} observe the growing importance of RGB images being used in automatic monitoring of plants due to the low cost, flexibility, high mobility, ease of use, and safe operation of unmanned UAVs. Efforts are made applying popular CNN models such as VGGNet \cite{c7}, ResNet \cite{c9}, and other deep networks such as LSTM \cite{c2}, etc., on RGB images. However, no rigorous experiments have been found yet, to observe the effect of the popular CNN models when applied to images of agricultural fields. The existing methods are trained on traditional image classification datasets and applied on agricultural field images. Hence, such models often are not capable of analyzing the minute features obtained from the tree images. A specially designed study for plantation monitoring on RGB images, is necessary, which is absent in the current literature.

There have been a significant number of efforts in using temporal information from the images for the classification of crops \cite{agri23,rse21}. Li et al. \cite{agri23} experimented with popular deep learning models such as 1D-CNN and 2D-CNN. Further, they emphasized on the temporal features using 3D-CNN, LSTM, and some combinations of 1D-CNN and 2D-CNN (for spatial features) with LSTM (for temporal features). According to their reports, 3D-CNN shows the best performance due to its ability to efficiently combine the spatial and temporal features from the crop images. However, such huge 3D-CNN models are computationally heavy. Moreover, such models are extremely data-hungry, making it a difficult choice for plantation monitoring, as the size of the dataset is less. Turkoglu et al. \cite{rse21} combined the spatial and temporal information by proposing a convRNN model. On the same line of thought, Macedo et al. \cite{c2} proposed a convLSTM model to leverage the combination of the spatial and temporal features from crop images.

Efforts have been made to apply transformer models for plant monitoring, taking advantage of the attention mechanism \cite{trans1,trans2}. Wang et al. \cite{trans1} proposed a light-weight transformer model, to classify the nature of disease in infected crops. He et al. \cite{trans2} applied a swin-transformer model for the classification of crop infection based on leaf images. It is observed that 2D-CNNs are more effective compared to the transformers on crop images \cite{c10}. However, finding a suitable CNN model specific to a given set of tree images is challenging. Verma et al. \cite{verma_cea} made an effort, based on a meta-learning approach, to find a suitable CNN for some given plant images.

Most of the existing deep learning-based methods for precession agriculture use remote sensing data, which is difficult to obtain for the farmers. A few efforts have been made to use some other easily available images for plant monitoring \cite{rse24,cea1}. Barriere et al. \cite{rse24} used multimodal data comprising satellite images along with crop rotation patterns and contextual information about the crops. Luo et al. \cite{cea1} used the FPGA data, for plant disease classification, proposing a CNN model. However, UAV-based RGB images are comparatively easier and cheaper for farmers to obtain.
\begin{figure}[!ht]
\centering
\includegraphics[width=0.48\textwidth]{overall_image.png}
    % \captionsetup{width=0.45\textwidth}
\caption{Sample drone images in the proposed dataset. These images are annotated using CVAT annotation tool for individual tree identification.}
\label{overall}
\end{figure}

Fewer efforts are found applying CNNs on drone images for plant monitoring, with reduced overfitting \cite{rgb1,trans2}. Joshi et al. \cite{rgb1} used UAV-based RGB images for plant disease classification by a pretrained CNN model, to reduce overfitting. Lin et al. \cite{light} avoided the possibility of overfitting by proposing a lightweight CNN model for crop classification. However, such light-weight models have lesser accuracy on plant images, due to oversimplification of the classifier.

Panday et al. \cite{c10} introduced a dense CNN architecture for plant classification. They introduced a novel activation function named SL-ReLU to address gradient explosion. However, the CD-CNN model proposed by Panday et al. \cite{c10} has not rigorously been tested on RGB drone images.

The goal of this study is to provide a platform for research on automatic plantation health monitoring based on RGB drone images. Next, we discuss the proposed approach.