\vspace{-10pt}
\section{Introduction}
The ability to fuse visual concepts from different sources into a single cohesive composition is a fundamental aspect of developing novel, creative content. This approach mirrors our natural creative processes: selecting specific attributes, objects, and elements from various inspirations to craft something new and unique. 

Extensive research has been conducted to enable such compositional image generation capabilities. The common approach relies on the unprecedented ability of recent diffusion models to synthesize images conditioned on natural language prompts. Since language is inherently composable, one can easily combine unrelated concepts through simple prompts. However, text-based methods often lack the detailed control and precision that is frequently required for more fine-grained applications~\cite{Zhang_2023_ICCV,gal2022textual,ruiz2022dreambooth}. To address these shortcomings, a more recent line of work focuses on manipulation and composition techniques based on image references~\cite{richardson2024popsphotoinspireddiffusionoperators,lee2024languageinformedvisualconceptlearning,zhang2023prospectpromptspectrumattributeaware,vinker2023concept}. As images are inherently more expressive and precise, these techniques are able to capture complex visual details that textual descriptions often fail to convey. Although powerful, these methods are frequently constrained by a limited range of concepts they can handle, or require computationally expensive per-concept training and fine-tuning that reduce their practicality and scalability. 

Our work, IP-Composer, aims to address these limitations by introducing a highly flexible, training-free approach for compositional image generation that combines several concepts drawn from multiple visual sources.
We build on IP-Adapter, an encoder-based approach that augments an existing text-to-image diffusion model (e.g., SDXL) with a new image-condition input, allowing users to generate novel variations of the content shown in the conditioning image. Importantly, IP-Adapter employs CLIP as a feature extraction backbone. 

Recently, it has been shown~\citep{gandelsman2024interpretingclipsimagerepresentation} that CLIP's attention heads span semantic subspaces of the CLIP embedding space. These subspaces are then characterized by finding textual descriptions whose CLIP embeddings span the same space. In their work, \citet{gandelsman2024interpretingclipsimagerepresentation} demonstrate that this property can be used to improve the accuracy of CLIP-based classification, by simply subtracting projections to subspaces linked to background properties such as ``snow" or ``water", which lead to spurious correlations. 
Our hypothesis is that if it is possible to remove a concept (``water background") without harming the semantics of the rest of the embedding, then it may also be possible to replace it with a different instance from the same concept category, drawn from a different image. 


To achieve this, we propose to first identify the CLIP-subspaces that are tied to the textually described concepts that we want to extract from each conditioning image. This is done by asking an LLM to generate a list of texts describing possible variations of each concept, encoding them into CLIP-space, and finding the subspace which they span. Then, we encode each image and project its CLIP-embedding onto its relevant concept subspace, extracting an isolated concept embedding. These extracted concept representations can then be recombined to create new composite embeddings that preserve the semantic meaning of each component. Finally, this computed embedding replaces the standard image encoding in the IP-Adapter pipeline, enabling the synthesis of novel images containing the composition of concepts (see \cref{fig:teaser}). 

Notably, our approach bridges the gap between high-level conceptual control and fine-grained visual detail, using text to describe and select broad concepts, but specifying the unique instance of each concept through visual examples.

We compare our method with prior training-based approaches, and demonstrate that it not only allows for more general concept selection, but also competes favorably even in scenarios where training data is available. Compared to existing CLIP-based methods that rely on embeddings interpolation or concatenation, our approach achieves higher accuracy and robustness, enabling better control over a broader range of concepts with minimal attribute leakage.

All in all, our approach offers an intuitive, text-based and training-free method to generate images inspired by multiple visual concepts, opening new possibilities for creative content generation and visual exploration. 





