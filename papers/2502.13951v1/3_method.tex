\section{Method}
We begin by describing our method for the simple case of creating a composition of two images.
Given a reference image \( I_{\text{ref}} \) (typically one describing the background or scene layout) and a concept image \( I_c \), we would like to output a composition depicting the concept \( c \) from \( I_c \) while obtaining the rest of the attributes from \( I_{\text{ref}} \). 

At the core of our method lies the ability to isolate and extract the ``$c$ component'' from a CLIP image embedding. Motivated by recent findings on the existence of different semantic subspaces in CLIP, we aim to construct a projection matrix \( P_c \), which will be used to project CLIP image embeddings to obtain the encoding of the specific concept ``$c$''.

\paragraph{\textbf{Constructing The Projection Matrix}} To construct a projection matrix for a concept $c$, we first gather a set of texts $t_1, \dots, t_n$, each describing an instance of the concept, with the aim of conceptually spanning its domain. To do so, we query a large language model (LLM) and simply ask it to create texts that span the concept's attribute space.
Next, using the CLIP text encoder \( CLIP_t \), we obtain embeddings for the collected texts: \( CLIP_t(t_1), \dots, CLIP_t(t_n) \). To extract the most relevant directions of this subspace, we apply Singular Value Decomposition (SVD) to the matrix of text embeddings. Let the combined embedding matrix be represented as:
\begin{equation}
E = \left[CLIP_t(t_1), \dots, CLIP_t(t_n)\right]^T,
\end{equation}
where \( E \in \mathbb{R}^{n \times d} \), \( n \) is the number of texts and \( d \) the embedding dimension. The SVD of \( E \) can be expressed as:
\begin{equation}
E = U \Sigma V^T,
\end{equation}
where \( U \in \mathbb{R}^{n \times n} \), \( \Sigma \in \mathbb{R}^{n \times d} \), and \( V \in \mathbb{R}^{d \times d} \). The rows of \( V \), also referred to as the right singular vectors, represent directions in the embedding space. While it is a common practice to normalize the embeddings before constructing the matrix \( E \), we observe improved performance when working with the unnormalized embeddings which also preserve the natural variation in the data.

Finally, we select the top \( r \) singular vectors (corresponding to the \( r \) largest singular values) from \( V \). These vectors capture the most significant variations in the subspace corresponding to concept \( c \). The projection matrix \( P_c \in \mathbb{R}^{d \times d} \) is then computed as:
\begin{equation}
P_c = V_r^T V_r,
\end{equation}
where \( V_r \in \mathbb{R}^{r \times d} \) contains the top \( r \) singular vectors. The value of \( r \) is selected empirically, and depends on the nature of the concept. In practice, the same \( r \) can often be used for most concepts, but broader concepts like ``animals" commonly benefit from utilizing more directions than specific concepts like ``dog breeds".

\input{figures/overview}

\paragraph{\textbf{Image Composition}} We aim to create a composite embedding that jointly encodes the concept \( c \) from \( I_c \) while preserving the remaining attributes of \( I_{\text{ref}} \). To achieve this, we simply replace the concept-space projection of \( I_{\text{ref}} \) with the projection of \( I_c \). More concretely, the composite embedding is given by:
\begin{equation}
\mathbf{e}_{\text{comp}} = \mathbf{e}_{\text{ref}} - P_c \mathbf{e}_{\text{ref}} + P_c \mathbf{e}_c,
\end{equation}
where \( \mathbf{e}_{\text{ref}} \) and \( \mathbf{e}_c \) are the CLIP embeddings of \( I_{\text{ref}} \) and \( I_c \), respectively. This composite embedding \( \mathbf{e}_{\text{comp}} \) is then passed to the IP-Adapter to generate the final composed image \( I_{\text{comp}} \), combining the attributes of \( I_{\text{ref}} \) with the concept instance extracted from \( I_c \).

\paragraph{\textbf{Generalization to Multiple Concepts}} The same approach can be extended to \( K \) concepts, \( \{c_1, c_2, \dots, c_K\} \), with corresponding projection matrices \( \{P_{c_1}, P_{c_2}, \dots, P_{c_K}\} \) and concept images \( \{I_{c_1}, I_{c_2}, \dots, I_{c_K}\} \). Here, the composed embedding is constructed by subtracting the concept-space projections of the reference embedding and adding the matching concept embedding from each source image:
\begin{equation}
\mathbf{e}_{\text{comp}} = \mathbf{e}_{\text{ref}} - \sum_{k=1}^K P_{c_k} \mathbf{e}_{\text{ref}} + \sum_{k=1}^K P_{c_k} \mathbf{e}_{c_k},
\end{equation}


where \( \mathbf{e}_{c_k} \) represents the embedding of the concept image \( I_{c_k} \). Note that we do not subtract the projection of each concept on the subspaces of the other concepts as we find empirically that this makes the compositions more sensitive to the choice of the number of singular vectors $r_c$ used for each concept.


\input{figures/qualitative_results}

\paragraph{\textbf{Implementation Details}} We implement our method on top of a pre-trained SDXL~\cite{podell2024sdxl} model using an IP-Adapter~\cite{ye2023ipadapter} encoder based on OpenCLIP-ViT-H-14~\cite{ilharco_gabriel_2021_5143773} (ip-adapter\_sdxl\_vit-h). To generate concept variation descriptions, we used GPT-4o~\cite{OpenAI2022ChatGPT} and asked for $150$ prompts. In cases that require higher subspace dimensions (\eg object insertion) we instead generated $500$ prompts.
