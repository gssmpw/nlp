\input{figures/limitations}

\section{Limitations}

While our approach is typically more general than current training-based approaches, it still has limitations. One limitation arises from surprising entanglements in the CLIP and diffusion feature spaces. For example, when attempting to combine a zebra's body with a leopard fur pattern (\cref{fig:limitations} (top)), the diffusion model tends to produce animals with the head of a giraffe, even though no giraffe appears in either input image. We hypothesize that this may be related to the tendency of diffusion models to represent some concepts as a composition of more basic visual components~\citep{chefer2023hidden}, but leave further investigation to future work.

On the other hand, some concepts may be \textit{more} disentangled in CLIP-space than intuitively expected. For example, outfit types and colors are disentangled in CLIP-space, hence, an ``outfit'' subspace spanned with descriptions of different types of outfits (``dress'', ``tuxedo''...) will not preserve outfit colors (\cref{fig:limitations} (bottom)). However, this can be easily amended by also specifying colors in the spanning texts (``\textit{red} dress'', ``\textit{blue} tuxedo''...).



Finally, we note that IP-Adapter itself is limited in the level of detail captured from the input image. Hence, our approach will not be sufficient for capturing delicate details such as exact identities. Stronger encoders may achieve higher fidelity, but it is not clear that our embedding-space projections would generalize to more complex feature spaces.

\section{Conclusions}

We presented IP-Composer, a training-free method that allows a user to compose novel images from visual concepts derived through a set of input images. To do so, our approach uses a CLIP-based IP-Adapter, leveraging their joint disentangled subspace structure. Through this approach, we achieve comparable or better performance compared with existing training-based methods, and can more easily generalize to novel concepts derived solely from textual descriptions. 

We hope that our work can serve as an additional component of the creative toolbox, and open the way to additional composable-concept discovery methods. 

\section{Acknowledgment}
We would like to thank Ron Mokady and Yoad Tewel for providing feedback and helpful suggestions.

