\section{Related work}
\paragraph{\textbf{Controllable diffusion models}}
Text-to-image diffusion models \citep{nichol2021glide, balaji2022ediffi,rombach2021highresolution, 
ramesh2022hierarchical,
saharia2022photorealistic,ho2020denoising} have emerged as a powerful paradigm for high-quality image generation, demonstrating remarkable capabilities in translating natural language descriptions into detailed images. As the technology matured, researchers explored various control mechanisms beyond text, including spatial controls such as segmentation masks \citep{couairon2022diffedit}, sketches \citep{voynov2023sketch}, depth maps \citep{zhang2023adding, mou2024t2i, bhat2023loosecontrolliftingcontrolnetgeneralized} and layout \citep{dahary2024yourselfboundedattentionmultisubject, avrahami2023spatext,zheng2023layoutdiffusion, li2023gligenopensetgroundedtexttoimage}. While text and spatial controls offer structural guidance, they often fall short in precisely controlling style and appearance. 

This limitation motivated the development of image-guided generation methods. 
One approach involves personalization through per-image optimization, either by fine-tuning token embeddings \citep{gal2022textual} or the model itself \citep{ruiz2022dreambooth}. More efficient encoder-based approaches~\citep{gal2023encoder,arar2023domain,ruiz2023hyperdreambooth,Wei_2023_ICCV,mou2024t2i,ye2023ipadapter} have also emerged. Of these, IP-Adapter \citep{ye2023ipadapter} employs a decoupled cross-attention mechanism to inject image features into the generation process. Our approach leverages a pre-trained IP-Adapter model to similarly inject image features into the generative process. However, we extend it to handle compositional generation, where multiple input images are used to describe an array of visual concepts that should appear in the generated outputs.

\paragraph{\textbf{CLIP directions for image editing}}
The discovery of semantically meaningful directions in latent spaces was first demonstrated in GANs \citep{goodfellow2014generative, karras2019style} where moving along these trajectories enables controlled image editing operations~\citep{shen2020interpreting}. Early unsupervised methods discovered these editing directions through various approaches: \citep{voynov2020unsupervised} learned directions by predicting identifiable image transformations, GANSpace \citep{harkonen2020ganspace} employed PCA to find dominant directions in latent codes, and SeFa \citep{shen2021closedformfactorizationlatentsemantics} analyzed generator weights directly to identify principal editing directions. 

The emergence of CLIP \citep{radford2021learning}, bridging visual and textual representations in a shared embedding space, revolutionized image editing by enabling text-guided manipulation. StyleClip \citep{patashnik2021styleclip} leveraged this capability by finding traversal directions that align images with textual descriptions. \citet{abdal2021clip2stylegan} proposed methods for discovering interpretable editing directions in CLIP space with automatic natural language descriptions. StyleGAN-NADA \citep{gal2021stylegannadaclipguideddomainadaptation} took a different approach, using CLIP-space directions to enable zero-shot domain adaptation. 
Recent works \citep{baumann2024continuoussubjectspecificattributecontrol, zhuang2024magnetknowtexttoimagediffusion} have demonstrated image editing capabilities in Stable Diffusion by manipulating CLIP text embedding.
Lastly, \citet{Guerrero_Viu_2024} leveraged domain diffusion prior~\citep{ramesh2022hierarchical, aggarwal2023controlledconditionaltextimage} to create clusters of image embeddings for source and target prompts, enabling the discovery of disentangled directions specifically for texture image editing.

Our approach also explores CLIP's embedding space. However, rather than finding directions of movement in CLIP space, we identify subspaces which encode specific semantic concepts. We then stitch new embeddings from the projections of different images on different concept spaces.

\paragraph{\textbf{Compositional image generation}}
Recent works have explored various approaches to enable multi-condition control in image generation. For text-based control, \citep{liu2023compositionalvisualgenerationcomposable} improved multi-object generation by introducing methods to compose multiple text prompts coherently. For spatial and global controls, Composer \citep{huang2023composercreativecontrollableimage} trained a diffusion model that accepts multiple conditions at test-time, while Uni-ControlNet \citep{zhao2023unicontrolnetallinonecontroltexttoimage} achieved similar capabilities with significantly reduced training costs by training two small adapters. Several works have focused on compositing elements from different images: ProSpect \citep{zhang2023prospectpromptspectrumattributeaware} introduced a step-aware prompt space to learn decomposed attributes from images for new compositions, while \citet{lee2024languageinformedvisualconceptlearning} proposed learning disentangled concept encoders aligned with language-specified axes, enabling composition through concept remixing. Finally, pOps~\citep{richardson2024popsphotoinspireddiffusionoperators} tunes a diffusion prior model~\citep{ramesh2022hierarchical} to learn semantic operators for element composition, though it requires training each operator on a suitable dataset, limiting its practical applications.

In contrast, our method enables compositional image generation using an off-the-shelf IP-Adapter model. Our approach leverages the ease of language-based controls to identify concept-specific CLIP subspaces, but uses image inputs to convey more specific details.