\input{figures/qualitative_results_with_text}
\section{Experiments}

\paragraph{\textbf{Qualitative Results}}
We begin by showcasing our method's ability to generate novel images, composed of attributes derived from a set of input images. As demonstrated in \Cref{fig:our_results,fig:teaser,fig:additional_1,fig:additional_2}, our approach can handle a wide range of attributes. These range from injecting a subject into an existing scene, to conditioning generation on times-of-day, transferring patterns or clothing, or even mimicking poses. For many of these, collecting data to train a supervised model would be challenging, but our text-based approach can seamlessly handle them. Notably, our method is not restricted to input pairs, but can generalize to multiple conditioning components. However, this is restricted by the limited dimensionality of the embedding space. With enough components, or when using components that require high subspace dimensions, we eventually observe leakage. 

While most of our experiments involve only image-based conditioning, we note that our approach can also involve text conditioning, in the same fashion as the baseline IP-Adapter model. See \Cref{fig:results_text} for examples.


\paragraph{\textbf{Qualitative Comparisons}}
Next, we compare our method against a set of baselines aimed at tackling the same composable generation task. Specifically, we consider three approaches: (1) pOps~\citep{richardson2024popsphotoinspireddiffusionoperators}, which fine-tunes a CLIP-conditioned model to accept multiple image inputs and combine them into a single embedding for a given task (e.g., texturing or object insertion). Importantly, this approach requires a per-task dataset of roughly $50,000$ samples. We use the official pre-trained ``scene'' (subject insertion), ``texturing'' and ``union'' operators, where the last one is used as a default for scenarios that do not match the first two. (2) ProSpect~\citep{zhang2023prospectpromptspectrumattributeaware}, which inverts an image into a set of word embeddings~\citep{gal2022textual}, each containing a different aspect of the image (layout, materials etc.). New prompts can then combine embeddings from different images in order to create compositions. This approach requires lengthy per-image optimization, and is limited only to concepts which the diffusion model naturally decomposes when learning multiple embeddings. For pattern-object compositions, we follow ProSpect's content-material separation scheme, while for other concept pairs where both inputs represent content (e.g. dog-background combination), we alternate between prompts from each concept during generation. (3) Describe \& Compose, where we first ask a VLM~\citep{liu2023improvedllava} to describe the desired concept in each image, and then create a new image conditioned on both text descriptions using Composable-Diffusion~\cite{liu2023compositionalvisualgenerationcomposable}.

Since ProSpect requires lengthy optimization per image, we conduct comparisons on a small set spanning $4$ scenarios, each of which contains $2$ images for each of $2$ concepts (for a total of $16$ combinations). Importantly, our set spans both scenarios where an object should be swapped (e.g., outfits) but also ones where an object is added. Our scenarios also span components that are typically represented at different steps of the diffusion process, from layout-affecting compositions (object insertion) to appearance changes (pattern transfer). Finally, our set contains both generated and real images, ensuring that the evaluated methods are not restricted to strictly in-domain inputs. For a fair comparison, we do not tune our subspace rank for each task, but set $r=30$ for low-variation tasks like outfit replacement and $r=120$ for high-variation tasks like pattern changes.

Sample qualitative results are shown in \cref{fig:qualitative_comp}. When compared to the training-based pOps, our method achieves comparable results on subject insertion without requiring any specialized datasets or model tuning. For tasks where no dedicated pOps encoder is available, we utilized their 'union' operator, as it is the most relevant for achieving the desired results. On these tasks, our approach significantly outperformed the results achieved by pOps, highlighting the generalization capabilities of our text-based approach. Compared to ProSpect, our approach can better handle layout changes and can tackle concepts which are not naturally separated when tuning multiple word embeddings. Finally, the description-based method struggles to convey the specifics of each concept and has significant leakage (cherry petals in the dog image, hat in the emotion transfer). As also shown in previous work~\citep{gal2022textual}, we further observe that the use of long descriptions makes the model more likely to discard parts of the prompt in the generated results.

\input{figures/quantitative_comparison}
\input{figures/user_study}
\input{figures/qualitative_comparisons}
\input{figures/qualitative_ablation}
\paragraph{\textbf{Quantitative comparisons.}}
To better evaluate the performance of our method, we  conduct a quantitative evaluation. Here, we ask GPT-4 to create a description of the target concept in each image, then manually verify the description and modify it to ensure that it does not contain leakage or misses important attributes. We then compute the CLIP-space distance between the text describing each concept in an input pair and the generated image that aims to combine them. To further quantify the leakage of unwanted properties, we employ the same method to generate descriptions of all image properties \textit{not related} to the concept that we aim to extract. We similarly measure the CLIP-space similarity between each generated image and these descriptions. However, here the goal is to achieve a lower score, indicating that the non-concept properties did not leak into the output.

The results are shown in ~\cref{fig:quantitative_comparison}. These results mirror those of the qualitative evaluation, demonstrating that our approach can achieve high similarity to both desired concepts, while minimizing undesired leakage. Here, we additionally report results when tuning our method's rank parameter for each individual task (``IP-Composer (Tuned)''). Doing so further enhances our performance, demonstrating that while even default parameters can achieve state-of-the-art results, a dedicated user has room to improve them further.

Finally, we verify our results using a user study. Here, we use a 2-alternative forced choice setup. We show each user a pair of input images with a caption denoting which concept should be extracted from each. Then, we show them an image generated by our method and an image generated by one of the baselines, and ask them to select the one that better combines the visual concepts from the two images. We collected a total of $560$ responses from $35$ different users. The results are reported in ~\cref{fig:user_study}, confirming that our approach is significantly preferred over existing baselines.

\input{figures/quantitative_ablation}
\input{figures/multi_step}

\paragraph{\textbf{Ablation}}
Next, we conduct an ablation study where we explore the use of different ways to combine IP-Adapter encodings for compositional generation. IP-Adapter itself first extracts a CLIP-embedding from each image, then converts these embeddings into a set of tokens which are used to condition the diffusion model through new cross-attention layers. Hence, we examine a scenario where we interpolate between the CLIP-embeddings of the two input images, as well as one where we concatenate their IP-Adapter tokens before feeding them into the diffusion model. Finally, we also examine a scenario where we span the CLIP-subspace of a concept using images rather than text. Here, we use the same LLM-generated descriptions of variations of a concept in order to generate images depicting these variations. Then, we encode them using CLIP, and use their CLIP-embeddings to span the concept space.

We evaluate the approaches using the same metrics of the quantitative comparisons. However, since none of the approaches require per-image training, we expand our evaluation set to $150$ images. Results are shown in \cref{fig:quantitative_ablation,fig:ablation}. Compared to concatenations and interpolations, our approach offers better control, with the ability to designate specific concepts to be extracted from an image and avoid significant leakage. The image-based approach suffers from increased leakage because the generated images tend to fill-in content unrelated to the prompt, such as the creation of an appropriate background. Since this content varies between the different images, it is represented in the dominant directions in the SVD.

