\section{Related work}
\paragraph{\textbf{Controllable diffusion models}}
Text-to-image diffusion models ____ have emerged as a powerful paradigm for high-quality image generation, demonstrating remarkable capabilities in translating natural language descriptions into detailed images. As the technology matured, researchers explored various control mechanisms beyond text, including spatial controls such as segmentation masks ____, sketches ____, depth maps ____ and layout ____. While text and spatial controls offer structural guidance, they often fall short in precisely controlling style and appearance. 

This limitation motivated the development of image-guided generation methods. 
One approach involves personalization through per-image optimization, either by fine-tuning token embeddings ____ or the model itself ____. More efficient encoder-based approaches____ have also emerged. Of these, IP-Adapter ____ employs a decoupled cross-attention mechanism to inject image features into the generation process. Our approach leverages a pre-trained IP-Adapter model to similarly inject image features into the generative process. However, we extend it to handle compositional generation, where multiple input images are used to describe an array of visual concepts that should appear in the generated outputs.

\paragraph{\textbf{CLIP directions for image editing}}
The discovery of semantically meaningful directions in latent spaces was first demonstrated in GANs ____ where moving along these trajectories enables controlled image editing operations____. Early unsupervised methods discovered these editing directions through various approaches: ____ learned directions by predicting identifiable image transformations, GANSpace ____ employed PCA to find dominant directions in latent codes, and SeFa ____ analyzed generator weights directly to identify principal editing directions. 

The emergence of CLIP ____, bridging visual and textual representations in a shared embedding space, revolutionized image editing by enabling text-guided manipulation. StyleClip ____ leveraged this capability by finding traversal directions that align images with textual descriptions. ____ proposed methods for discovering interpretable editing directions in CLIP space with automatic natural language descriptions. StyleGAN-NADA ____ took a different approach, using CLIP-space directions to enable zero-shot domain adaptation. 
Recent works ____ have demonstrated image editing capabilities in Stable Diffusion by manipulating CLIP text embedding.
Lastly, ____ leveraged domain diffusion prior____ to create clusters of image embeddings for source and target prompts, enabling the discovery of disentangled directions specifically for texture image editing.

Our approach also explores CLIP's embedding space. However, rather than finding directions of movement in CLIP space, we identify subspaces which encode specific semantic concepts. We then stitch new embeddings from the projections of different images on different concept spaces.

\paragraph{\textbf{Compositional image generation}}
Recent works have explored various approaches to enable multi-condition control in image generation. For text-based control, ____ improved multi-object generation by introducing methods to compose multiple text prompts coherently. For spatial and global controls, Composer ____ trained a diffusion model that accepts multiple conditions at test-time, while Uni-ControlNet ____ achieved similar capabilities with significantly reduced training costs by training two small adapters. Several works have focused on compositing elements from different images: ProSpect ____ introduced a step-aware prompt space to learn decomposed attributes from images for new compositions, while ____ proposed learning disentangled concept encoders aligned with language-specified axes, enabling composition through concept remixing. Finally, pOps____ tunes a diffusion prior model____ to learn semantic operators for element composition, though it requires training each operator on a suitable dataset, limiting its practical applications.

In contrast, our method enables compositional image generation using an off-the-shelf IP-Adapter model. Our approach leverages the ease of language-based controls to identify concept-specific CLIP subspaces, but uses image inputs to convey more specific details.