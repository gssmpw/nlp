
% \textcolor{red}{During class discovery, we also use the source data excluding labels in addition to the target data for enhanced robustness.}

\section{Experiments and Results}
\label{sec:result}
\subsection{Experimental Setting}
\noindent \textbf{Dataset}.
We conducted experiments in two settings: COCO$_{half}$ + LVIS and LVIS + VG, following~\cite{fomenko2022learning}. In the COCO$_{half}$ + LVIS setting, we consider 80 COCO classes as known classes and aim to discover 1,123 disjoint LVIS classes from the total 1,203 LVIS classes. Among the 100K training images in the LVIS dataset~\cite{gupta2019lvis}, we use 50K images with labels for the 80 COCO classes for $\calD^l$ and the entire 100K images without labels for $\calD^u$. We use the 20K LVIS validation images for evaluation.

In the LVIS + VG setting, we utilize 1,203 LVIS classes as known classes and aim to discover 2,726 disjoint classes in the Visual Genome (VG) v1.4 dataset~\cite{krishna2017visual}. We use the entire 100K LVIS training data for $\calD^l$ and the combined 158K LVIS and VG training images for $\calD^u$. For evaluation, we use 8K images that appear in both the LVIS and VG validation sets. Although the VG dataset contains over 7K classes, only 3,367 classes appear in both $\calD^u$ and the validation data. After excluding the 641 classes that overlap with the known classes, we aim to discover 2,726 classes.


% We note that annotations provided in VG are not exhaustive per class, and many of its classes are abstract or semantically overlapping both within VG and when compared to LVIS. We thus mainly focus on qualitative results.

\vspace{1mm}
\noindent \textbf{Implementation Details}. 
We use ResNet-50 as the backbone for both $f_d(\cdot)$ and $f_s(\cdot)$. For $f_d(\cdot)$, we apply the SAM to all four stages of the backbone. We train $f_d(\cdot)$ for 390 epochs with $K=1\%$, $\tau^{min}=0.07$, $\tau^{max}=1$, and $\lambda=0.35$. For the SAM, we set $M=3$, $d=D/8$, $w=0.25 \cdot stg$, $\hat{d}=1$, $s_1=\lfloor 18/(2^{stg-1}) \rceil$, $s_2=\lfloor 12/(2^{stg-1}) \rceil$, and $s_3=\lfloor 8/(2^{stg-1}) \rceil$, where $stg$ is the stage index and $\lfloor \cdot \rceil$ denotes rounding. We train $f_s(\cdot)$ for 36 epochs with $\bar{T}=3$. The experiments were conducted on a computer with two Nvidia GeForce RTX 3090 GPUs, an Intel Core i9-10940X CPU, and 128 GB RAM. The code will be publicly available on GitHub upon publication to ensure reproducibility.

%Following RNCDL \cite{fomenko2022learning}, we firstly use the Hungarian algorithm \cite{kuhn1955hungarian} to map predicted classes to GT classes. Then, we conduct evaluations separately for all, known, and novel classes, using $\text{mAP}_{.50:.05:.95}$ as evaluation metrics.

\vspace{1mm}
\noindent \textbf{Evaluation Metric}. 
We first apply the Hungarian algorithm~\cite{kuhn1955hungarian} to find a one-to-one mapping between the discovered classes and the ground-truth novel classes, following~\cite{fomenko2022learning}. We then calculate $\text{mAP}_{.50:.05:.95}$ for all, known, and novel classes. $\text{mAP}_{.50:.05:.95}$ is computed using mask labels for the COCO$_{half}$ + LVIS setting and using bounding box labels for the LVIS + VG setting due to the absence of mask labels in the VG dataset.



\subsection{Result}
Tables~\ref{tab:result_LVIS} and~\ref{tab:result_VG} present quantitative comparisons for the COCO$_{half}$ + LVIS and LVIS + VG settings, respectively. We compare the proposed method with the $k$-means~\cite{macqueen1967some} baseline and previous works including ORCA~\cite{cao2021open}, UNO~\cite{fini2021unified}, SimGCD~\cite{wen2023parametric}, and RNCDL~\cite{fomenko2022learning}. Additionally, we report results obtained by replacing the GCD model in our framework with recent GCD methods: $\mu$GCD~\cite{Vaze2023No} and NCDLR~\cite{zhang2023novel}. The results demonstrate that our method significantly outperforms the previous state-of-the-art method~\cite{fomenko2022learning} as well as methods focusing on balanced and curated datasets~\cite{cao2021open, fini2021unified, wen2023parametric}, across all metrics and settings. ``RNCDL w/ ran. init.'' refers to the method with random initialization in~\cite{fomenko2022learning}. 

\input{sections/tbl_result}

\input{sections/fig_result}

\fref{fig:result} shows qualitative comparisons with RNCDL~\cite{fomenko2022learning} for both settings. % Additional results and failure cases are presented in the supplementary material.


% \textcolor{blue}{This enhancement comes from a list of contributions, such as queues for saving a large scale negative samples, losses for long-tail distribution, soft-attention mechanism to focus on foreground features and dynamic learning to mitigate noisy labels.}



% Table \ref{tab:table3} presents an ablation study on the key components of our framework using the LVIS validation set. In this study, AT denotes adaptive temperature assignment for long-tail distribution. SA stands for soft attention module. DL represents the dynamic learning process. The baseline is our framework using the a fixed value of temperature ($\tau=0.07$) for all samples. It does not include soft attention module, and treats all labels as human annotations in the last stage. Here, AT boosts the baseline performance by 1.85 $\text{mAP}_{\text{all}}$, augmenting group-wise distinction for head classes and instance discrimination for tail classes. SA improves performance by 3.07 $\text{mAP}_{\text{all}}$, concentrating on object features and alleviating the impact of noisy background features. Lastly, by applying DL to fully leverage reliable labels and mitigate the impact of unreliable ones, $\text{mAP}_{\text{all}}$ increases by 2.14 \%. 



\subsection{Analysis}
We conducted all the ablation studies and analyses using the COCO$_{half}$ + LVIS setting unless otherwise noted. \tref{tab:ablation} presents an ablation study of the proposed components. The baseline model is constructed by using a fixed value for the temperature parameters for all samples in~\eref{eqn:loss_unsupervised}, excluding SAM, and treating all pseudo-labels as equivalent to human annotations. The results demonstrate that each module provides additional improvements across all metrics.


% In the table, ITA and RDL denote instance-wise temperature assignment and reliability-based dynamic learning, respectively. 

\input{sections/tbl_ablation}

\tref{tab:analysis_temperature} compares the proposed ITA method with the TS method from~\cite{kukleva2023temperature}. Although the TS method is also designed for imbalanced data, our ITA method consistently outperforms it for both the selected hyperparameters ($0.07$, $1$) and an alternative set ($0.07$, $0.5$). Additionally, the table shows that the chosen hyperparameters yield better performance than the alternative set.


\tref{tab:analysis_attention} compares the proposed SAM with previous attention methods, including MGCAM~\cite{song2018mask}, MGFPM~\cite{wang2021mask}, CBAM-S~\cite{woo2018cbam}, and RGA-S~\cite{zhang2020relation}. The results for the other methods are obtained by replacing only the SAM in the proposed method with the corresponding methods. The results demonstrate that our SAM outperforms all other methods across all metrics.


\input{sections/tbl_analysis}

\tref{tab:analysis_reliability} compares our RDL method with the fixed and global reliability criteria in ST++~\cite{yang2020superpixel}. The results confirm the importance of using selected pseudo-labels based on adjusted reliability criteria for each class throughout the training process.

\input{sections/tbl_analysis2}


\tref{tab:analysis_open_world} presents the results of our method using various class-agnostic instance mask generation models, including Mask R-CNN~\cite{he2017mask}, OLN~\cite{Kim2022Learning}, LDET~\cite{Saito2022Learning}, UDOS~\cite{kalluri2023open}, and GGN~\cite{wang2022open}. The results demonstrate that our method consistently outperforms the previous state-of-the-art method~\cite{fomenko2022learning}, regardless of the choice of mask generation model.




% , including Mask R-CNN~\cite{he2017mask}, OLN~\cite{Kim2022Learning}, LDET~\cite{Saito2022Learning}, and UDOS~\cite{kalluri2023open}



