\section{Implementation of Inexact Oracles via Heat Kernel Trucation}\label{Section_Oracle}

Theorem~\ref{TV_Inexact_BM_Inexact_RHK} shows that as long we have sufficient accuracy of MBI and RHK oracles satisfying Assumption~\ref{Assumption_Oracle_TV_quality}, we can have a high-accuracy Riemannian sampling algorithm. In this section, we introduce an approximate implementation, based on heat kernel truncation (as introduced in~\ref{prelim}) and rejection sampling. Numerical simulations for this approach are provided in Appendix~\ref{hkimplem}.

First note that for rejection sampling method (in general) there are two key ingredients: a proposal distribution and an acceptance rate. 
Assume we want to generate samples from $\rho$ through rejection sampling.
We choose a suitable proposal distribution denoted as $\mu$, and a suitable scaling constant $K$ 
such that the acceptance rate $K\frac{\rho(x)}{\mu(x)} \le 1, \forall x$.
We generate a random proposal $x \sim \mu$ and $u \in [0, 1]$ being a uniform random number. 
Then we compute $K\frac{\rho(x)}{\mu(x)}$, and accept $x$ if $u \le K\frac{\rho(x)}{\mu(x)}$.

We also introduce the following definition of Riemannian Gaussian distribution, as defined next, which will be used as the proposal distribution in rejection sampling. A Riemannian Gaussian distribution centered at $x^{*}$ with variable $t$ is  $
    \mu(t, x^{*}, x) \propto \mu_{u}(t, x^{*}, x) := \exp\left(-\frac{d(x^{*}, x)^{2}}{2t}\right)$, where $\mu_{u}$ denote an unnormalized version of $\mu$. We use this as our proposal distribution to implement rejection sampling, as exact sampling from such a distribution is  well-studied for certain specific manifolds;
see, for example, \cite{said2017gaussian} for symmetric spaces and \cite{chakraborty2019statistics} for Stiefel manifolds. Furthermore, this notion of a Riemannian Gaussian distribution is also used in the study of differential privacy on Riemannian manifolds due to their practical feasibility~\citep{reimherr2021differential,jiang2023gaussian}. 

%To implement the oracles approximately, we will need evaluation of heat kernels. In this section, we consider the truncation method. 

\subsection{Implementation of RHK}
 
We first recall the rejection sampling implementation of Restricted Gaussian Oracle (RGO) in the Euclidean setting. Note that, we have $\log \nu_{u}(\eta, x, y_{k}) = -\frac{1}{2\eta} \|x - y_{k}\|^{2}$, 
where $\nu_{u} = \exp(-\frac{1}{2\eta} \|x - y_{k}\|^{2})$ is an unnormalized heat kernel (or the Gaussian density) in Euclidean space. 
Then we have $\pi_{\eta}^{X|Y}(\cdot, y_{k}) \propto e^{-f(x) - \frac{1}{2\eta} \|x - y_{k}\|^{2}} $. 
Then, the RGO is implemented through rejection sampling. Specifically, we can first find the minimizer 
$ x^{*} \in \argmin_{x} f(x) + \frac{1}{2\eta} \|x - y_{k}\|^{2} $. 
Note that the minimizer represents the mode of $\pi_{\eta}^{X|Y}(\cdot, y_{k})$.
We can then sample a Gaussian proposal $x_{p} \sim \mathcal{N}(x^{*}, t I_{d})$ 
for suitable $t$ centered at the mode $x^{*}$ and perform rejection sampling.
For more details, see, for example, \cite{chewi2023log}.

On a Riemannian manifold with $\nu$ denoting the heat kernel, to sample from $\pi_{\eta}^{X|Y}(\cdot, y_{k}) \propto e^{-f(x)} \nu(\eta, x, y_{k})$ through rejection sampling, we need evaluations of $f(x) - \log \nu(\eta, x, y_{k}) $. But in general, we cannot evaluate the heat kernel exactly, hence we seek for certain heat kernel approximations. Hence, we use the truncated heat kernel $\nu_{l}$ to replace $\nu$, 
and perform rejection sampling, see Algorithm \ref{Inexact_Rejection_Sampling}.
In the rejection sampling algorithm, as mentioned previously, we use a Riemannian Gaussian distribution as the proposal for rejection sampling. 
We choose suitable step size $\eta$ and $t$ that depends on $\eta$ s.t. $g(x) - g(x^{*}) \ge \frac{1}{2t}d(x, x^{*})^{2}$. 
Such an inequality can guarantee that the acceptance rate (with Riemannian Gaussian distribution $\mu(t, x^{*}, x)$ as proposal) would not exceed one, i.e., $\frac{\exp(-g(x) + g(x^{*}))}{\mu_{u}(t, x^{*}, x)} \le 1, \forall x$. Then we see that the output of rejection sampling would follow $\hat{\pi}_{\eta}^{X|Y}(x|y_{k}) \propto \exp(f(x) - \log \nu_{l}(\eta, x, y_{k})) $. Similarly, to implement the MBI oracle, we also use rejection sampling to get a high-accuracy approximation. Specifically, Algorithm~\ref{Inexact_BM} generates inexact Brownian motion starting from $x$ with time $\eta$.

\begin{algorithm}[t]
    \begin{algorithmic}
    \STATE Find the minimizer of $g(x) := f(x) - \log \nu_{l}(\eta, x, y_{k})$, denote as $x^{*}$.
    %\STATE Set suitable $t$ so that $\frac{\exp( - g(x) + g(x^{*}))}{\mu_{u}(t, x^{*}, x)} \le 1, \forall x$.
    \STATE Set suitable $t$ and constant $C_{\mathsf{RHK}}$ s.t. $V_{\mathsf{RHK}}(x) := \frac{\exp(-g(x) + g(x^{*}) + C_{\mathsf{RHK}})}{\exp(-\frac{1}{2t} d(x, x^{*})^{2})} \le 1, \forall x \in M$
    \FOR{$i=0, 1,2,...$}
    \STATE Generate proposal $x \sim \mu(t, x^{*}, \cdot)$.
    \STATE Generate $u$ uniformly on $[0, 1]$. 
    \STATE Return $x$ if $u \le V_{\mathsf{RHK}}(x)$
    \ENDFOR
    \end{algorithmic}
    \caption{RHK through Rejection Sampling}
    \label{Inexact_Rejection_Sampling} 
\end{algorithm}


%\subsection{Implementation of MBI}


\begin{algorithm}[t]
    \begin{algorithmic}
    \STATE Set suitable $t$ and $C_{\mathsf{MBI}}$
    so that $V_{\mathsf{MBI}}(y) := \frac{\exp(\log \nu_{l}(\eta, x, y) - \log \nu_{l}(\eta, x, x) + C_{\mathsf{MBI}})}{\exp(-\frac{d(x, y)^{2}}{2t})} \le 1, \forall y \in M$
    %$K \frac{\nu_{l}(\eta, x, y)}{\mu_{u}(t, x, y)} \le 1, \forall y$.
    \FOR{$i=0, 1,2,...$}
    \STATE Generate proposal $y \sim \mu(t, x, \cdot)$.
    \STATE Generate $u$ uniformly on $[0, 1]$. 
    \STATE Return $y$ if $u \le V_{\mathsf{MBI}}(y)$
    \ENDFOR
    \end{algorithmic}
    \caption{MBI through Rejection Sampling}
    \label{Inexact_BM} 
\end{algorithm}

\subsection{Verification of Assumption \ref{Assumption_Oracle_TV_quality}}
We now show that Assumption \ref{Assumption_Oracle_TV_quality} is satisfied for the aforementioned inexact implementation of the Riemannian Proximal Sampler. To do so, we specifically consider the case when the manifold $M$ is compact and is a homogeneous space. Recall that $\nu_{l}$ denote the truncated heat kernel with truncation level $l$. Roughly speaking, a homogeneous space is a manifold that has certain symmetry, including Stiefel manifold, Grassmann manifold, hypersphere, and manifold of positive deﬁnite matrices. 

\begin{proposition}\label{Prop_Verify_Assumption}
    Let $M$ be a compact manifold. Assume further that $M$ is a homogeneous space. 
    With truncation implementation of inexact oracles, 
    in order for Assumption \ref{Assumption_Oracle_TV_quality} to be satisfied
    with $\zeta = \frac{\varepsilon}{\log^{2} \frac{1}{\varepsilon}}$,
    we need truncation level $l$ to be of order $\textrm{polylog}({1}/{\varepsilon})$.
\end{proposition}
\textbf{Sketch of proof:} We briefly mention the idea of proof. 
\citet[Proposition 21]{azangulov2022stationary} provided an $L_{2}$ bound on the truncation error, and by Jensen's inequality 
we get an $L_{1}$ bound as desired.
With truncation level $l$ to be of order $\Poly (\log \frac{1}{\varepsilon})$, 
we can achieve $\int_{M} |\nu(\eta, x, y) - \nu_{l}(\eta, x, y)| dV_{g}(x) = \tilde{\mathcal{O}}(\zeta)$. See Proposition \ref{Prop_truncation_1} and Proposition \ref{Prop_truncation_level} for a complete proof.

\begin{remark} 
In Appendix \ref{Subsection_inexact_rej}, we show that on hypersphere $\mathcal{S}^{d}$, when the acceptance rate $V$ in rejection sampling would possibly exceed $1$ in some unimportant region, Assumption \ref{Assumption_Oracle_TV_quality} still holds, via explicit computations.
\end{remark}

When $M$ is not a homogeneous space, to the best of our knowledge, it is unknown how to implement the truncation method. Exploring this direction to further extend the above result is an interesting direction for future work.

%\textcolor{blue}{
%}



\section{Implementation via Varadhan's Asymptotics and Connection to Entropy-Regularized JKO Scheme}\label{Section_Proximal_point_approximation}

In this section, we consider yet another approximation scheme for implementing Algorithm \ref{Manifold_Proximal_Sampler_Ideal}, motivated by its connection with the proximal point method in optimization, where the latter is in the sense of optimization over Wasserstein space\footnote{If $M$ is a smooth compact Riemannian manifold then the Wasserstein space $\mathcal{P}_2(M)$ is the
space of Borel probability measures on $M$, equipped with the Wasserstein metric $W_2$.  We refer the reader to~\cite{villani2021topics} for background on Wasserstein spaces.}~\citep{jordan1998variational,wibisono2018sampling,chen2022improved}. Note that the proximal point method is usually called as the JKO scheme after the authors of~\cite{jordan1998variational}. 


Specifically, we consider approximating the heat kernel through Varadhan's asymptotics. 
Let $\hat{\nu}(\eta, x, y) \propto_{y} \exp(-\frac{d(x, y)^2}{2\eta}) =: \hat{\nu}_{u}(\eta, x, y)$ be an inexact evaluation of heat kernel. 
According to Varadhan's asymptotics, $\lim_{\eta \to 0} \hat{\nu}(\eta, x, y) = \nu(\eta, x, y)$. 
Hence when $\eta$ is small, $\hat{\nu}$ is a good approximation of the heat kernel. 
Note that $\hat{\nu}(\eta, x, \cdot)$ in Varadhan's asymptotic is exactly the Riemannian Gaussian distribution $\mu(\eta, x, \cdot)$. Denote $\tilde{\pi}(x, y) = \exp(-f(x)-\frac{d(x, y)^2}{2\eta})$. 
With inexact MBI implemented through Riemannian Gaussian distribution and  inexact RHK implemented through rejection sampling (Algorithm~\ref{Inexact_Rejection_Sampling}) to generate $\tilde{\pi}^{X|Y}(x|y) \propto \exp(-f(x) - \frac{d(x, y)^{2}}{2\eta})$,
we obtain Algorithm \ref{Manifold_Proximal_Sampler_Gaussian}.


%Let $Z_{x, t} = 1/\int_{M} e^{-\frac{d(x, y)^{2}}{2t}} dV_{g}(y)$ be the normalizaing constant for $\mu(t, x, \cdot)$ (and hence the normalizaing constant for $\hat{\nu}(t, x, \cdot)$)
%We ignore the index $t$ when there is no ambiguity, and keep the index $x$ emphasizing that the constant might depend on $x$.

%Note that in general $e^{-f(x)}\hat{\nu}(\eta, x, y_{k}) = e^{-f(x)}\hat{\nu}_{u}(\eta, x, y_{k})Z_{x}$ where 
%the constant $Z_{x}$ might depends on $x$. Then the rejection sampling output
%$e^{-f(x)}\hat{\nu}_{u}(\eta, x, y_{k})$ is no longer propotional to $e^{-f(x)}\hat{\nu}(\eta, x, y_{k})$.
%Fortunately, when $M$ is a homogeneous space, $Z_{x}$ doesn't depend on $x$ \cite[Section 3]{chakraborty2019statistics}, and therefore 
%$e^{-f(x) - \frac{d(x, y_{k}^{2})}{2\eta}} = e^{-f(x)} \hat{\nu}_{u}(\eta, x, y_{k}) \propto e^{-f(x)}\hat{\nu}(\eta, x, y_{k})$.

For the case when $M = \mathcal{S}^{d}$, we prove in Appendix \ref{Subsection_expected_rej} that to sample from $\tilde{\pi}^{X|Y}(x|y)$ through rejection sampling, with suitable parameters, the cost is $\mathcal{O}(1)$ in both dimension $d$ and step size $\eta$. Obtaining similar results for more general manifolds seems non-trivial. Numerical simulations for this approach are provided in Appendix~\ref{vardhanimplem}. Verifying Assumption~\ref{Assumption_Oracle_TV_quality} for this implementation is open.


\begin{algorithm}[t]
    \begin{algorithmic}
    \FOR{$k=0, 1,2,...$}
    \STATE From $x_{k}$, sample $y_{k} \sim \tilde{\pi}^{Y|X}(\cdot, x_{k})$ which is a Riemannian Gaussian distribution. 
    \STATE From $y_{k}$, sample $x_{k+1} \sim \tilde{\pi}^{X|Y}(\cdot, y_{k}) \propto e^{-f(x) - \frac{d(x, y_{k}^{2})}{2\eta}} $ using Algorithm~\ref{Inexact_Rejection_Sampling}.
    \ENDFOR
    \end{algorithmic}
    \caption{Inexact Manifold Proximal Sampler with Varadhan's Asymptotics}
    \label{Manifold_Proximal_Sampler_Gaussian} 
\end{algorithm}

\subsection{RHK as a proximal operator on Wasserstein space}
We first show that the inexact RHK output in Algorithm \ref{Manifold_Proximal_Sampler_Gaussian} can be viewed as a proximal operator on Wasserstein space, generalizing the Euclidean result in~\cite{chen2020fast} to the Riemannian setting. 
Recall that with a function $f$ and $d$ being a distance function, 
$\prox_{\eta f}(y) = \argmin_{x} f(x) + \frac{1}{2\eta} d(x, y)^{2}$.
The (approximated) joint distribution is $\tilde{\pi}(x, y) = \exp(-f(x) - \frac{d(x, y)^{2}}{2\eta})$.
By direct computation we have the following Lemma (proved in Appendix \ref{Proof_Theorem_Gaussian_JKO}). 

\begin{lemma}\label{Lemma_proximal_calculation}
    We have that 
    \begin{equation*}
        \tilde{\pi}^{X|Y = y} 
        = \argmin_{\rho \in \mathcal{P}_{2}(M)} H_{\tilde{\pi}^{X}}(\rho) + \frac{1}{2\eta} W_{2}^{2}(\rho, \delta_{y}) = \prox_{\eta H_{\tilde{\pi}^{X}}} (\delta_{y}),
\end{equation*}
which shows that the ineact RHK implementation is a proximal operator, i.e., $\tilde{\pi}^{X|Y = y} = \prox_{\eta H_{\tilde{\pi}^{X}}} (\delta_{y})$.
\end{lemma}



\subsection{Connection to Entropy-Regularized JKO Scheme}\label{Section_Approximation_JKO}




Observe that in Algorithm \ref{Manifold_Proximal_Sampler_Gaussian}, the Riemannian Gaussian involves distance square, which naturally relates to Wasserstein distance. Now, recall that for a function $F$ in the Wasserstein space, its Wasserstein gradient flow can be approximated through the following discrete time JKO scheme~\citep{jordan1998variational}:
\begin{equation*}
    \rho_{k+1} = \argmin_{\rho \in \mathcal{P}(\mathbb{R}^{d})} F(\rho) + \frac{1}{2\eta} W_{2}^{2} (\rho, \rho_{k}).
\end{equation*}
It was proved that as $\eta \to 0$, the discrete time sequence $\{\rho_{k}\}$ converge to the Wasserstein gradient flow of $F$.
Later, \cite{peyre2015entropic} proposed an approximation scheme through entropic smoothing of Wasserstein distance:
\begin{equation*}
    \rho_{k+1} = \argmin_{\rho \in \mathcal{P}(\mathbb{R}^{d})} F(\rho) + \frac{1}{2\eta} W_{2, \varepsilon}^{2} (\rho, \rho_{k}),
\end{equation*}
where $W_{2, \varepsilon}$ is the entropy-regularized 2-Wasserstein distance defined by (here $H$ is the negative entropy)
\begin{equation*}
    W_{2, t}^{2}(\rho_{1}, \rho_{2}) = \inf_{\gamma \in \mathcal{C}(\rho_{1}, \rho_{2})} \int d(x, y)^{2} d\gamma(x, y) + t H(\gamma).
\end{equation*}


In Euclidean space,~\cite{chen2022improved} showed that the proximal sampler can be viewed as an entropy-regularized JKO scheme.
We extend such an interpretation to Riemannian manifolds. Specifically, we show that Algorithm \ref{Manifold_Proximal_Sampler_Gaussian} which is an approximation of the exact proximal sampler (Algorithm~\ref{Manifold_Proximal_Sampler_Ideal}), can be viewed as an entropy-regularized JKO as stated in Theorem~\ref{Theorem_Gaussian_JKO} (proved in Appendix \ref{Proof_Theorem_Gaussian_JKO}). Note that on a Riemannian manifold the negative entropy is $H(\gamma) := \int_{M \times M} \gamma \log(\gamma) dV_{g}(x) dV_{g}(y) $.
\begin{theorem}\label{Theorem_Gaussian_JKO}
    Recall that $\pi^{X} \propto e^{-f}$.
    Let $x_{k}, y_{k}, x_{k+1}$ be generated by Algorithm \ref{Manifold_Proximal_Sampler_Gaussian}. 
    Let $\tilde{\rho}_{k}^{X}$, $\tilde{\rho}_{k}^{Y}$ and $\tilde{\rho}_{k+1}^{X}$ be the distribution of $x_{k}, y_{k}, x_{k+1}$, respectively. 
    Then  
    \begin{align*}
            \tilde{\rho}_{k}^{Y} = \argmin_{\chi \in \mathcal{P}_{2}(M)} \frac{1}{2\eta} W_{2, 2\eta}^{2}(\tilde{\rho}_{k}^{X}, \chi) \quad\text{and}\quad
            \tilde{\rho}_{k+1}^{X} = \argmin_{\chi \in \mathcal{P}_{2}(M)} \int f d\chi + \frac{1}{2\eta} W_{2, 2\eta}^{2}(\tilde{\rho}_{k}^{Y}, \chi).
    \end{align*}
\end{theorem}


\section{Conclusion}

We introduced the \textit{Riemannian Proximal Sampler} for sampling from densities on Riemannian manifolds. By leveraging the Manifold Brownian Increments (MBI) and the Riemannian Heat-kernel (RHK) oracles, we established high-accuracy sampling guarantees, demonstrating a logarithmic dependence on the inverse accuracy parameter (i.e., \(\text{polylog}(1/\varepsilon)\)) in the Kullback-Leibler divergence (for exact oracles) and total variation metric (for inexact oracles). Additionally, we proposed practical implementations of these oracles using heat-kernel truncation and Varadhan’s asymptotics, providing a connection between our sampling method and the Riemannian Proximal Point Method. 

Future works include: (i) characterizing the precise dependency on other problem parameters apart from $\varepsilon$, (ii) improving oracle approximations for enhanced computational efficiency and (iii) extending these techniques to broader classes of manifolds (and other metric-measure spaces). 
