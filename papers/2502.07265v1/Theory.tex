\section{Preliminaries}\label{prelim}
We first recall certain preliminaries on Riemannian manifolds; additional preliminaries are provided in Appendix~\ref{sec:addprelim}. We refer the readers to~\cite{lee2018introduction} for more details. 

Let $M$ be a Riemannian manifold of dimension $d$ equipped with metric $g$. The manifold $M$ is assumed to be complete, connected Riemannian manifold without boundary. For a point $x \in M$, $T_{x}M$ denotes the tangent space at $x$. For any $v, w \in T_{x}M$, we can write the metric as  $g_{x}(v, w) = \langle v, w\rangle_{g}$. For $x \in M$ and $v \in T_{x}M$, $\exp_{x}(v)$ denotes the exponential map. We use $\grad$ and $dV_{g}$ to represent the Riemannian gradient and the Riemannian volume form respectively.  

For $x \in M$, $\Cut(x)$ denotes the cut locus of $x$.
For $x, y \in M$, we use $d(x, y)$ to denote the geodesic distance between $x$ and $y$.
Let $\Div$ denotes the Riemannian divergence, and Laplace-Beltrami operator $\Delta : C^{\infty} (M) \to C^{\infty} (M)$ 
is defined as the Riemannian divergence of Riemannian gradient: $ \Delta u = \Div (\grad u)$. We use $\nu(t, x, y)$ to denote the density of manifold Brownian motion with time $t$, starting at $x$, evaluated at $y$. 

%For big-O notation, we use $\tilde{\mathcal{O}}(\cdot)$ to hide the dependency of all other parameters except for $\varepsilon$,  and in the meanwhile only consider the dominating factor.  For example, $\frac{1}{\varepsilon} \log \frac{1}{\varepsilon} = \tilde{\mathcal{O}}(\frac{1}{\varepsilon}) $ and $\log \frac{1}{\varepsilon} + \log \log \frac{1}{\varepsilon} = \tilde{\mathcal{O}}(\log \frac{1}{\varepsilon})$.

%\kb{So far we directly introduced the density on the manifold. Need to introduce whats the measure on the manifold, before introducing the TV distance below. ALso, what is $A$?}

%\textcolor{blue}{Can I write those in this way? Maybe I just define them through measures (as in the book "Analysis and geometry of Markov dissusion operators"), and say how the measure is related to probability density function. In my understanding this is essentially Radon-Nikodym Theorem?}

Let $(M, \mathcal{F})$ be a measurable space.
Note that the Riemannian volume form $dV_{g}$ is a measure. A probability measure $\rho$ and its corresponding probability density function $p$ are related through $d\rho = p dV_{g}$. Given a measurable set $A \in \mathcal{F}$, $P_{\rho}(A)$ denotes the probability assigned to the set $A$ by $\rho$. We have $P_{\rho}(A) = \int_{A} p(x) dV_{g}(x) = \int_{A} d\rho(x)$.

\begin{definition}[TV distance]
Let $\rho_{1}, \rho_{2}$ be probability measures defined on the measurable space $(M, \mathcal{F})$. The total variation distance between $\rho_{1}$ and $\rho_{2}$ 
is defined as 
\begin{equation*}
    \|\rho_{1} - \rho_{2}\|_{TV} := \sup_{A \in \mathcal{F}} |\rho_{1}(A) - \rho_{2}(A)|.
\end{equation*}
\end{definition}

\begin{definition}[KL divergence] Let $\rho_{1}, \rho_{2}$ be probability measures on the measurable space $(M, \mathcal{F})$, with full support. 
The Kullback-Leibler (KL) divergence of $\rho_{1}$ with respect to $\rho_{2}$ is defined as
\begin{equation*}
    H_{\rho_{2}}(\rho_{1}) := \int_{M} \log \frac{d\rho_{1}}{d\rho_{2}} d\rho_{1},
\end{equation*}
where $\frac{d\rho_{1}}{d\rho_{2}}$ is the Radon-Nikodym derivative.
\end{definition}

It is known that $H_{\rho_{2}}(\rho_{1}) \ge 0$ with equality if and only if $\rho_{1} = \rho_{2}$. 
Although the KL divergence is not symmetric, it serve as a ``distance'' function between two probability measures. For instance, the well known Pinsker inequality states that $\|\rho_{2} - \rho_{1}\|_{TV}^{2} \le \frac{1}{2} H_{\rho_{2}}(\rho_{1})$. 

%\kb{your TV is defined for measure. KL is defined for density. Pinsker's inequality needs to be clarified here.}

\begin{definition}[Log-Sobolev Inequality (LSI)]
A probability measure $\rho_{2}$ satisfies Log-Sobolev Inequality with parameter $\alpha > 0$ ($\alpha$-$\mathsf{LSI}$) 
if $H_{\rho_{2}}(\rho_{1}) \le \frac{1}{2\alpha} J_{\rho_{2}}(\rho_{1}), \forall \rho_{1}$, where $J_{\rho_{2}}(\rho_{1}) := \int_{M} \|\grad \log \frac{\rho_{1}}{\rho_{2}}\|^{2} d\rho_{1}$ is the relative Fisher information.
\end{definition}

For more details on LSI, see Appendix \ref{LSI_Heat}.
In Euclidean space, such a condition is a relaxation of strongly convex assumption, and is used to establish convergence of sampling algorithms in KL divergence.
See, for example, \cite{vempala2019rapid} (for the Langevin Monte Carlo Algorithm) and \cite{chen2022improved} (for the Euclidean proximal sampler).

\subsection{Curvature}\label{sec:ricci}
We also need notions of curvature on manifolds to present our main results. Let $\mathfrak{X}(M)$ denote the set of all smooth vector fields on $M$.
Define a map called Riemann curvature endomorphism by $R: \mathfrak{X}(M) \times \mathfrak{X}(M) \times \mathfrak{X}(M) \to \mathfrak{X}(M)$ by  $R(X, Y)Z = \nabla_{X} \nabla_{Y} Z - \nabla_{Y} \nabla_{X} Z - \nabla_{[X, Y]} Z.$ While such definition is very abstract, we provide an intuitive explanation of what curvature is.
Intuitively, on a manifold of positive curvature (say, a $2$-dimensional sphere), geodesics tend to ``contract". More precisely, given $x, y \in M$ and $v \in T_{x}M$, we can parallel transport $v$ to $u = P_{x}^{y}v \in T_{y}M$. It is a well-known result that (ignore higher order terms) $d(\exp_{x}t v, \exp_{y} tu) \le (1-\frac{t^{2}}{2}K)d(x, y)$ for some $K$ (which is actually the sectional curvature). From this, we see that for positive curvature, which means $K > 0$, the distance between geodesics would decrease. 

Formally, given $v, w \in T_{p}M$ being linearly independent, the sectional curvature of the plane spanned by $v$ and $w$ can be computed through  $K(v, w) = \frac{\langle R(v, w)w, v \rangle}{|v|^{2}|w|^{2} - \langle v, w \rangle^{2}} $; see  \citet[Proposition 8.29]{lee2018introduction}. 
On the other hand, Ricci curvature can be viewed as the average of sectional curvatures. The Ricci curvature at $x \in M$ along direction $v$ is denoted as $\Ric_{x}(v)$, which is equal to the sum of the sectional curvatures of the 2-planes spanned by $(v, b_{i})_{i = 2}^{d}$ where $v, b_{2}, ..., b_{d}$ is an orthonormal basis for $T_{x}M$; see
\citet[Proposition 8.32]{lee2018introduction}. 

We remark that the Ricci curvature is actually a symmetric 2-tensor field defined as the trace of the
curvature endomorphism on its first and last indices \citep{lee2018introduction}, which sometimes is written as $\Ric_{x}(u, v)$ for $u, v \in T_{x}M$. The previous notation is a shorthand of $\Ric_{x}(v) = \Ric_{x}(v, v)$. When we say Ricci curvature is lower bounded by $\kappa$, we mean $\Ric(v, v) \ge \kappa, \forall v \in T_{x}M, \|v\| = 1$. We end this subsection through some concrete examples. 
\begin{enumerate}
    \item The hypersphere $\mathcal{S}^{d}$ has constant sectional curvature equal to $1$, and constant Ricci curvature $\Ric = (d-1) g, \forall x \in M$ (so that $\Ric_{x}(v) = d-1$ for all unit tangent vector $v \in T_{x}M$).
    \item For $P_{m} \subseteq \mathbb{R}^{m \times m}$, the manifold of positive definite matrices, its sectional curvatures are in the interval $[-\frac{1}{2}, 0]$; see, for example, \cite{criscitiello2023accelerated}. Hence its Ricci curvature is lower bounded by $-\frac{m(m+1)-1}{4}$.
\end{enumerate}


\subsection{Brownian Motion on Manifolds}

Now we briefly discuss Brownian motion on a Riemannian manifold. Recall that in Euclidean space, Brownian motion is described by the Wiener process. Given $x \in \mathbb{R}^{d}$ and $t > 0$, the Brownian motion starting at $x$ with time $t$ has (a Gaussian) density function 
$\nu(t, x, y) = \frac{1}{(2\pi t)^{d/2}}e^{-\frac{\|x - y\|^{2}}{2t}}$.
It solves the heat equation $\frac{\partial}{\partial t} \nu(t, x, y) = \frac{1}{2} \Delta_{y} \nu(t, x, y)$ 
with initial condition $\nu(0, x, y) = \delta_{x}(y)$. 

On a Riemannian manifold, we can describe the density of Brownian motion (heat kernel) through heat equation.
Let $B_{x, t}$ be a random variable denoting manifold Brownian motion starting at $x$ with time $t$ 
and let $\nu(t, x, y)$ be the density of $B_{x, t}$.
%In Euclidean space we simply have $B_{x, t} = \mathcal{N}(x, tI_{d})$ and 
%$\nu(t, x, y) = \frac{1}{(2\pi t)^{d/2}}e^{ \frac{\|x - y\|^{2}}{2t}}$.
The Brownian motion density $\nu(t, x, y)$ is then defined as the minimal solution of the following heat equation:
\begin{equation*}
    \begin{split}
        \frac{\partial}{\partial t} \nu(t, x, y) = \frac{1}{2} \Delta_{y} \nu(t, x, y) \qquad\text{with}\qquad
        \nu(0, x, y) = \delta_{x}(y).
    \end{split}
\end{equation*}
More details can be found in \citet[Chapter 4]{hsu2002stochastic}.
Unlike the Euclidean case, on Riemannian manifold, the heat kernel does not have a closed-form solution in general. However, some properties of the Euclidean hear kernel is preserved on a Riemannian manifold. One such property is the following: 
Consider $M = \mathbb{R}^{d}$ we have $t \log \nu(t, x, y) = t \log \frac{1}{(2\pi t)^{d/2}} - \frac{\|x - y\|^{2}}{2}$. 
As $t \to 0$, we get $\lim_{t \to 0} t \log \nu(t, x, y) = - \frac{\|x - y\|^{2}}{2}$.
On a Riemannian manifold, we have the following result.
\begin{fact}[Varadhan's asymptotic relation
    \citep{hsu2002stochastic}]
    For all $x, y \in M$ with $y \notin \Cut(x)$, we have 
    \begin{enumerate}
        \item[] $\underset{t \to 0}{\lim}~t \log \nu(t, x, y) = - \frac{d(x, y)^{2}}{2}$\quad and\quad$\lim_{t \to 0} t \grad_{y} \log \nu(t, x, y) = \exp_{y}^{-1}(x)$.
    \end{enumerate}
\end{fact}
When evaluation of the heat kernel is required for practical applications, the Varadhan asymptotics aforementioned is used \citep{de2022riemannian}. We illustrate the above with the following example.
\begin{example}
    For $M = \mathcal{S}^{1} \subseteq \mathbb{R}^{2}$, the heat kernel for time $t$ only depends on the spherical distance but not specific points.
    Hence we simply write $\nu_{t}(\varphi) = \nu_{t}(d(x, y)) = \nu(t, x, y)$ where $\varphi = d(x, y)$ is the geodesic distance between $x$ and $y$.
    We have $\nu_{t}(\varphi) = \sum_{n \in \mathbb{Z}} \frac{1}{\sqrt{2\pi t}} \exp(- \frac{(\varphi+2\pi n)^{2}}{2t})$; see, for example, \cite{andersson2013estimates}.
    Here $t$ represent the time of Brownian motion and $\varphi = d(x, y)$ represent the spherical distance between $x, y$. 
    When $x$ is not too large, terms corresponding to $n = 0$ would dominate the sum. 
    Thus we can write 
    $\nu(t, x, y) \approx \frac{1}{\sqrt{2\pi t}} \exp(-\frac{d(x, y)^{2}}{2t} )$ which recovers Varadhan's asymptotics. 
\end{example}

Yet another numerical method for evaluating the heat kernel on manifold is truncation method; see, for example, \citet[Section 5.1]{corstanje2024simulating} and \cite{de2022riemannian}. In many cases, the heat-kernel has an infinite series expansion. For example, a power series expansion of heat kernel on hypersphere is given in \citet[Theorem 1]{zhao2018exact}, 
and more examples can be found in \citet[Example 1-5]{eltzner2021diffusion}. 
Similar results are also available for more general manifolds; see, for example, \cite{azangulov2022stationary} for compact 
Lie groups and their homogeneous space, 
and \cite{azangulov2024stationary} for non-compact symmetric spaces. Hence, a natural approach is to truncate this infinite series at an appropriate level. For example, on $\mathcal{S}^{2} \subseteq \mathbb{R}^{3}$, the heat kernel and its truncation up to the $l$-th term (denoted as $\nu_{l}$) can be written respectively as 
\begin{equation*}
    \nu(t, x, y) = \sum_{i = 0}^{\infty} e^{-\frac{i(i+1)t}{2}} \frac{2i+1}{4\pi} P_{i}^{0}(\langle x, y \rangle_{\mathbb{R}^{3}})~~\text{and}~~\nu_{l}(t, x, y) = \sum_{i = 0}^{l} e^{-\frac{i(i+1)t}{2}} \frac{2i+1}{4\pi} P_{i}^{0}(\langle x, y \rangle_{\mathbb{R}^{3}}),
\end{equation*}
where $P_{i}^{0}$ are Legendre polynomials.


\section{The Riemannian Proximal Sampler}

\iffalse

The proximal sampler was first introduced in \cite{lee2021structured}, 
aiming at improving the condition number of sampling algorithms. Here we briefly introduce the Euclidean proximal sampler. 
The goal is to generate samples from $\pi^{X}(x) \propto e^{-f(x)}$. We define $\pi$ as the following joint distribution: $\pi(x, y) \propto e^{-f(x) - \frac{1}{2\eta}\|x-y\|^{2} } $. 
\begin{algorithm}[H]
    \begin{algorithmic}
    \FOR{$k=0, 1,2,...$}
    \STATE From $x_{k}$, sample $y_{k} \sim \pi^{Y|X}(\cdot, x_{k}) \propto e^{- \frac{1}{2\eta}\|x_{k}-y\|^{2} }$ which is a Euclidean Brownian increment. 
    \STATE From $y_{k}$, sample $x_{k+1} \sim \pi^{X|Y}(\cdot, y_{k}) \propto e^{-f(x) - \frac{1}{2\eta} \|x-y_{k}\|^{2} } $.
    \ENDFOR
    \end{algorithmic}
    \caption{Euclidean Proximal Sampler \cite{lee2021structured}}
    \label{Euclidean_Proximal_Sampler} 
\end{algorithm}

In each iteration, the proximal sampler (Algorithm \ref{Euclidean_Proximal_Sampler}) performs two steps. The first step is to sample a Brownian motion of time $\eta$ starting from $x_{k}$. In Euclidean space, this is equivalent to sampling a Gaussian random variable. 
The second step is to sample from $e^{-f(x)-\frac{1}{2\eta}\|x-y_{k}\|^{2}}$, which is called RHK (restricted Gaussian oracle \cite{lee2021structured}). 

\begin{theorem}\cite[Theorem 3]{chen2022improved}
    Assume $\pi^{X}(x) \propto e^{-f(x)}$ satisfies $\alpha$-$\mathsf{LSI}$. 
    For any $\eta > 0$ and any initial distribution $\rho_{0}^{X}$, the $k$-th iterate $\rho_{k}^{X}$ 
    of Algorithm \ref{Euclidean_Proximal_Sampler} with step size $\eta$ satisfies
    \begin{equation*}
        H_{\pi^{X}} (\rho_{k}^{X} ) \le \frac{H_{\pi^{X}} (\rho_{0}^{X})}{(1 + \eta\alpha)^{2k} }
    \end{equation*}
\end{theorem}
\fi 



\begin{algorithm}[t]
    \begin{algorithmic}
    \FOR{$k=0, 1,2,...$}
    \STATE \textbf{Step 1 (MBI):} From $x_{k}$, sample $y_{k} \sim \pi^{Y|X}(\cdot, x_{k})$ which is a manifold Brownian increment. 
    \STATE \textbf{Step 2 (RHK):} From $y_{k}$, sample $x_{k+1} \sim \pi^{X|Y}(\cdot, y_{k}) \propto e^{-f(x)} \nu(\eta, x, y_{k}) $.
    \ENDFOR
    \end{algorithmic}
    \caption{Riemannian Proximal Sampler}
    \label{Manifold_Proximal_Sampler_Ideal} 
\end{algorithm}

We now describe the Riemannian Proximal Sampler, introduced in Algorithm \ref{Manifold_Proximal_Sampler_Ideal}. Similar to the Euclidean proximal sampler~\citep{lee2021structured}, the algorithm has two steps. The first step is sampling from the Manifold Brownian Increment (MBI) oracle. The second step is called the Riemannian Heat-Kernel (RHK) Oracle. Recall that $\nu(\eta, x, y)$ denotes the density of manifold Brownian motion with time $\eta$. Define a joint distribution $\pi_{\eta}(x, y) \propto e^{-f(x)} \nu(\eta, x, y)$. Then, step 2 consists of sampling from the aforementioned distribution. When there is no ambiguity, we omit the step size $\eta$ and simply write $\pi(x, y) \propto e^{-f(x)} \nu(\eta, x, y)$. Algorithm \ref{Manifold_Proximal_Sampler_Ideal} is an idealized algorithm, in the sense that we assume exact access to MBI and RHK oracles. Following~\cite{chen2022improved}, next we provide an intuitive explanation for the algorithm from a diffusion process perspective. 

\paragraph{Step 1:} For fixed $x$, we see that $\pi^{Y|X}(\cdot, x) \propto \nu(\eta, x, \cdot)$ which is the density of Brownian motion starting from $x$ for time $\eta$.
From this we see that the first step of the algorithm is running forward manifold heat flow: $dZ_{t} = dB_{t}$. 

\paragraph{Step 2:} We will illustrate that the second step of the algorithm is running the time-reversed process of the forward process. Consider a stochastic process $Z_{t}: t \ge 0$. 
When we have observations of $x_{\eta} \sim Z_{\eta}$, we can compute the conditional probability of $Z_{0}$ condition on end point $Z_{\eta}$. 
We denote $\mu(x_{0}|x_{\eta})$ as the posterior.
Bayes Theorem says $\mu(x_{0}|x_{\eta}) \propto \mu(x_{0}) L(x_{\eta}|x_{0})$,
where $\mu(x_{0})$ is the prior guess and the likelihood $L$ depends on the model.
We consider the following model (forward heat flow): $dZ_{t} = dB_{t}$ with $Z_{0} \sim \pi^{X} \propto e^{-f(x)}$. 
Then $\mu(x_{0}) = \pi^{X}(x_{0})$ and $L(x_{\eta}|x_{0}) = \nu(\eta, x_{0}, x_{\eta})$.
Thus we get $\mu(x_{0}|x_{\eta}) \propto e^{-f(x_{0})} \nu(\eta, x_{0}, x_{\eta})$, and we observe that $\mu(x_{0}|x_{\eta})$ is exactly $\pi^{X|Y = x_{\eta}}(x_{0}|x_{\eta})$. For the forward heat flow $dZ_{t} = dB_{t}$ with initialization $Z_{0} \sim \pi^{X} \propto e^{-f(x)}$,
there is a well-defined time reversed process $\hat{Z}_{t}^{-}$, 
which satisfies $(Z_{0}, Z_{\eta}) \overset{d}{=} (\hat{Z}_{\eta}^{-}, \hat{Z}_{0}^{-})$. See Appendix \ref{Section_Backward} for more details.
Based on this, for the time-reversed process $\hat{Z}_{t}^{-}$, 
the law of $\hat{Z}_{\eta}^{-}$ condition on $\hat{Z}_{0}^{-} = z$ is
the same as the posterior $\mu(x|z)$ discussed previously, i.e., $\pi^{X|Y = z}(x) \propto e^{-f(x)} \nu(\eta, x, z)$. 
Thus we see that the RHK oracle is, from a diffusion perspective, running the time-reversed process.

Implementing Step 1 and Step 2 is non-trivial on Riemannian manifolds. In Sections ~\ref{Section_Oracle} and~\ref{Section_Proximal_point_approximation} respectively, we discuss two approaches based on heat-kernel truncation and Varadhan's asymptotics. Furthermore, geodesic random walk~\citep{mangoubi2018rapid,schwarz2023efficient} is a popular approach to simulate Manifold Brownian Increments (see~Appendix~\ref{georw}), however to the best of our knowledge (in various metrics of interest) is known only under strong assumptions~\citep{cheng2022efficient,mangoubi2018rapid}.

\section{High-Accuracy Convergence Rates}

In this section, we provide the convergence rates for the Riemannian Proximal Sampler (Algorithm \ref{Manifold_Proximal_Sampler_Ideal}) assuming that the target density satisfies the LSI assumption. Firstly, note that in \citep{lee2021structured} the analysis of Euclidean Proximal Sampler is done assuming the potential function is strongly convex. However, it is known that on a compact manifold, if a function is geodesically convex, then it has to be a constant. Hence assuming the potential $f$ being geodesically convex is not much meaningful. Recently, \cite{cheng2022efficient} discussed an analog of log-concave distribution on manifolds.
Although their setting works for compact manifolds, it requires the Riemannian Hessian of the potential $f$ to be lower 
bounded by some curvature-related value, which is still restrictive. Hence, we adopt the setting as in \cite{chen2022improved}, assuming that the target distribution satisfies the LSI.


In Section \ref{Sec_Exact_oracle}, we consider the case where both steps of Algorithm \ref{Manifold_Proximal_Sampler_Ideal} are implemented exactly, and in Section \ref{Sec_Inexact_oracle}, we consider the case when MBI and RHK oracles are inexact. Regarding notation, we let $\rho_{k}^{X}(x)$, $\rho_{k}^{Y}(y)$ denote the law of $x$ and $y$ generated by Algorithm \ref{Manifold_Proximal_Sampler_Ideal} at $k$-th iteration, 
assuming exact MBI and exact RHK oracles. When the oracles are inexact, we let $\tilde{\rho}_{k}^{X}(x)$, $\tilde{\rho}_{k}^{Y}(y)$ to denote the law of $x$ and $y$ generated by Algorithm \ref{Manifold_Proximal_Sampler_Ideal} at $k$-th iteration.

\subsection{Rates with Exact Oracles}\label{Sec_Exact_oracle}
Our first result is as follows, with the proof provided in Appendix \ref{Sec_Proof_Main_Theorem}. %We refer the reader to Appendix~\ref{sec:ricci} for details on Ricci curvature used below.
\begin{theorem}\label{Main_Theorem}
    Let $M$ be a Riemannian manifold without boundary, i.e., $\partial M = \emptyset$. Assume $\pi^{X}$ satisfies $\alpha$-$\mathsf{LSI}$. 
    Denote the distribution for the $k$-th iteration of Algorithm \ref{Manifold_Proximal_Sampler_Ideal} as $x_{k} \sim \rho_{k}^{X} $.
    For any initial distribution $\rho_{0}^{X}$, for all $\eta > 0$, we have
    \begin{equation*}
        \begin{split}
            H_{\pi^{X}} (\rho_{k}^{X} ) &\le \frac{H_{\pi^{X}} (\rho_{0}^{X})}{(1 + \eta\alpha)^{2k} }, \qquad\text{if the Ricci curvature is non-negative,} \\
            H_{\pi^{X}} (\rho_{k}^{X} ) &\le H_{\pi^{X}} (\rho_{0}^{X}) \left(\frac{\kappa}{\alpha(e^{\kappa \eta} - 1) + \kappa}\right)^{2k}, \qquad\text{otherwise,} \\
        \end{split}
    \end{equation*}
    where $\kappa$ is the lower bound of Ricci curvature.
    In case of negative curvature, we have
    \begin{align*}
    H_{\pi^{X}} (\rho_{k}^{X} ) \le \frac{H_{\pi^{X}} (\rho_{0}^{X})}{(1 + \eta\alpha)^{2k} },\qquad~\text{if}~~\eta \le \frac{1}{|\kappa|}.
    \end{align*}
\end{theorem}

Note that the resulting contraction rate depends on the curvature. If the curvature is non-negative, then we can recover the rate in Euclidean space. But in the case of negative curvature, the rate becomes more complicated, and in order to get the contraction rate as in Euclidean space, we need the step size to be bounded above by some curvature-dependent constant. 

The above result provides a high-accuracy guarantee for the Riemannian Proximal Sampler in KL-divergence. To see that, consider the case when the Ricci curvature is non-negative. Note that to achieve $\varepsilon$ accuracy in KL divergence, we need $\frac{H_{\pi^{X}} (\rho_{0}^{X})}{(1 + \eta\alpha)^{2k} } = \varepsilon$. Taking $\log$ on both sides, we get 
    $k = \mathcal{O}(\frac{\log (H_{\pi^{X}} (\rho_{0}^{X})/\varepsilon)}{\log (1 + \eta\alpha)}) $.
    For small step size $\eta$, we have $\frac{1}{\log(1 + \eta \alpha)} = \mathcal{O}(\frac{1}{\eta \alpha})$. 
    Hence $k = \mathcal{O}(\frac{1}{\eta \alpha} \log \frac{H_{\pi^{X}} (\rho_{0}^{X})}{\varepsilon}) = \tilde{\mathcal{O}}(\frac{1}{\eta}\log \frac{1}{\varepsilon})$. As $\eta$ does not depend on $\varepsilon$, we see that we need $\tilde{\mathcal{O}}(\log \frac{1}{\varepsilon})$ number of iterations.



There are several challenges in obtaining the aforementioned result for the Riemannian Proximal Sampler. In Euclidean space, when a probability distribution $\pi^{X}$ satisfies $\alpha$-$\mathsf{LSI}$, 
its propagation along heat flow $\pi^{X} * \mathcal{N}(0, tI_{d})$ satisfies $\alpha_{t}$-$\mathsf{LSI}$, 
with $\alpha_{t} = \frac{\alpha}{1 + \alpha t}$. 
This fact is very important and leveraged in~\cite{chen2022improved} for proving their convergence rates. A quantitative generalization of such a fact for Riemannian manifolds is not immediate and we establish the required results in Appendix \ref{LSI_Heat}, following \cite{collet2008logarithmic}, under the required Ricci curvature assumptions.

%We present the following theorem, which is an extension of the Euclidean space result \cite{chen2022improved} to Riemannian manifold. 




\subsection{Rates with Inexact Oracles}\label{Sec_Inexact_oracle}
Recall that Algorithm \ref{Manifold_Proximal_Sampler_Ideal} is an idealized algorithm, 
where we assumed the availability of the MBI and RHK oracles. Note that given $x \in M$, exact MBI oracle generate samples $y \sim \pi_{\eta}^{Y|X}(\cdot|x)$. 
And given $y \in M$, exact RHK generate samples $x \sim \pi_{\eta}^{X|Y}(\cdot|y)$. In practice, exactly implementing these oracles could be computationally expensive or even impossible. For the Euclidean case, we emphasize that, as the heat kernel has an explicit closed form density (which is the Gaussian), prior works, for example,~\cite {fan2023improved}, only consider inexact Restricted Gaussian Oracles and control the propagated error along iterations.


In this section, we derive rates of convergence in the setting where both the MBI and RHK oracles are implemented inexactly. Specifically, we assume we are able to approximately implement the MBI oracle by generating $y \sim \hat{\pi}_{\eta}^{Y|X}(\cdot|x)$, 
and approximately implement the RKH oracle by generating $x \sim \hat{\pi}_{\eta}^{X|Y}(\cdot|y)$, see Assumption \ref{Assumption_Oracle_TV_quality} below.

\begin{assumption}\label{Assumption_Oracle_TV_quality}
    Denote the output of exact RHK oracle as $\pi_{\eta}^{X|Y}(\cdot|y)$ and inexact RHK oracle as $\hat{\pi}_{\eta}^{X|Y}(\cdot|y)$. Similarly, denote the output of exact MBI oracle as $\pi_{\eta}^{Y|X}(\cdot|x)$ and inexact MBI oracle as $\hat{\pi}_{\eta}^{Y|X}(\cdot|x)$. 
    Let $\zeta_{\mathsf{RHK}}$ and $\zeta_{\mathsf{MBI}}$ be the desired accuracy. We assume that, for inverse step size $\eta^{-1} = \tilde{\mathcal{O}}(\log \frac{1}{\zeta}) $, the RHK and MBI oracle implementations can achieve respectively $\|\hat{\pi}_{\eta}^{X|Y}(\cdot|y) - \pi_{\eta}^{X|Y}(\cdot|y)\|_{TV} \le \zeta_{\mathsf{RHK}}, \forall y$,
    and $\|\hat{\pi}_{\eta}^{Y|X}(\cdot|x) - \pi_{\eta}^{Y|X}(\cdot|x)\|_{TV} \le \zeta_{\mathsf{MBI}}, \forall x$. We then let $\zeta\coloneqq\max\{\zeta_{\mathsf{RHK}},\zeta_{\mathsf{MBI}} \}$.
\end{assumption}

The need for assuming the step size satisfies $\eta^{-1} = \tilde{\mathcal{O}}(\log \frac{1}{\zeta})$ for the approximation quality is as follows. Recall from the discussion below Theorem~\ref{Main_Theorem} that the complexity of Riemannian Proximal Sampler depends on the step size as $\mathcal{O}(\frac{1}{\eta})$. Thus if $\eta$ became too small, for example $\eta^{-1} = \mathcal{O}\left(\frac{1}{\varepsilon}\right)$, then the overall complexity would be $\Poly(\frac{1}{\varepsilon})$, which is not a high-accuracy guarantee.

We also briefly explain the intuition in assuming total variation distance error bound in oracle quality, 
and postpone the detailed discussion to Section \ref{Section_Oracle}.
To guarantee a high quality oracle, we need a high quality approximation of heat kernel. 
As mentioned previously, a popular method is through truncation of infinite series. 
Theoretically, the $L_{2}$ truncation error can be bounded for compact manifold \citep{azangulov2022stationary}, 
which says that the difference between the heat kernel and the approximation of heat kernel are close. 
This naturally imply an error bound in total variation distance, 
which motivates us to consider the propagated error in total variation distance. 

We first start with a result quantifying the error propagated along iterations, under the availability of inexact oracles. The proof of the following result is provided in Appendix \ref{Sec_Proof_Inexact_Theorem}.
\begin{lemma}\label{Lemma_Propagation_Error_TV}
    Let $\rho_{k}^{X}$ denote the law of $X$ through exact oracle implementation of Algorithm \ref{Manifold_Proximal_Sampler_Ideal}, 
    and $\tilde{\rho}_{k}^{X}$ denote the law of $x$ through inexact oracle implementation of Algorithm \ref{Manifold_Proximal_Sampler_Ideal}.
    Under Assumption \ref{Assumption_Oracle_TV_quality}, we have $\|\rho_{k}^{X}(x) - \tilde{\rho}_{k}^{X}(x)\|_{TV} \le k (\zeta_{\mathsf{RHK}} + \zeta_{\mathsf{MBI}})$.
\end{lemma}
Based on this result, we next obtain the following result analogues to Theorem~\ref{Main_Theorem}; the proof is provided in Appendix \ref{Sec_Proof_Inexact_Theorem}.
\begin{theorem}\label{TV_Inexact_BM_Inexact_RHK}
  Similar to Theorem~\ref{Main_Theorem}, let $M$ be a Riemannian manifold without boundary and let $\pi^{X}$ satisfies LSI with constant $\alpha$. Further let Assumption \ref{Assumption_Oracle_TV_quality} hold.
    For any initial distribution $\rho_{0}^{X}$, to reach $\tilde{\mathcal{O}}(\varepsilon)$ 
    total variation distance with oracle accuracy $\zeta = \zeta_{\mathsf{RHK}} = \zeta_{\mathsf{MBI}} = \frac{\varepsilon}{\log^{2} \frac{1}{\varepsilon}}$
    and step size $\frac{1}{\eta} = \tilde{\mathcal{O}}(\log \frac{1}{\varepsilon})$, 
    we need $k = \tilde{\mathcal{O}}(\log^{2} \frac{1}{\varepsilon})$ iterations.
\end{theorem}
