
\iffalse
\section{Introduction}

This is where the content of your paper goes.
\begin{itemize}
  \item Limit the main text (not counting references and appendices) to 12 PMLR-formatted pages, using this template. Please add any additional appendix to the same file after references - there is no page limit for the appendix.
  \item Include, either in the main text or the appendices, \emph{all} details, proofs and derivations required to substantiate the results.
  \item The contribution, novelty and significance of submissions will be judged primarily based on
\textit{the main text of 12 pages}. Thus, include enough details, and overview of key arguments, 
to convince the reviewers of the validity of result statements, and to ease parsing of technical material in the appendix.
  \item Use the \textbackslash documentclass[anon,12pt]\{colt2025\} option during submission process -- this automatically hides the author names listed under \textbackslash coltauthor. Submissions should NOT include author names or other identifying information in the main text or appendix. To the extent possible, you should avoid including directly identifying information in the text. You should still include all relevant references, discussion, and scientific content, even if this might provide significant hints as to the author identity. But you should generally refer to your own prior work in third person. Do not include acknowledgments in the submission. They can be added in the camera-ready version of accepted papers. 
  
  Please note that while submissions must be anonymized, and author names are withheld from reviewers, they are known to the area chair overseeing the paper’s review.  The assigned area chair is allowed to reveal author names to a reviewer during the rebuttal period, upon the reviewer’s request, if they deem such information is needed in ensuring a proper review.  
  \item Use \textbackslash documentclass[final,12pt]\{colt2025\} only during camera-ready submission.
\end{itemize}
\fi 


\section{Introduction}

%such as the Riemannian Langevin and Hamiltonian Monte Carlo algorithms,

We consider the problem of sampling from a density $\pi^{X} \propto e^{-f}$ defined on a Riemannian manifold $(M,g)$, where $g$ is the metric on the manifold $M$. Here, the density is defined with respect to the volume measure $dV_g$ and the normalization constant $\int_M e^{-f} dV_g <\infty$ is unknown. Riemannian sampling arises in various domains. In Bayesian inference, it is used for sampling from distributions with complex geometries, such as those encountered in hierarchical Bayesian models, latent variable models, and machine learning applications like Bayesian deep learning~\citep{girolami2011riemann, byrne2013geodesic, patterson2013stochastic, liu2018riemannian, arnaudon2019irreversible, liu2016stochastic, piggott2016geometric, muniz2022higher, lie2023dimension}. In statistical physics, it plays a crucial role in simulating molecular systems with constrained dynamics~\citep{leimkuhler2016efficient}. Additionally, it appears in optimization problems over manifolds, including eigenvalue problems and low-rank matrix approximations~\citep{goyal2019sampling,li2023riemannian, yu2023riemannian, bonet2023spherical} and as a module in Riemannian diffusion models~\citep{de2022riemannian,huang2022riemannian}.



On a Riemannian manifold, Langevin dynamics has the form $dX_{t} = - \grad f(X_{t})dt + \sqrt{2} dB_{t} $ where $\grad$ represents the Riemannian gradient and $B_{t}$ is the manifold Brownian motion. This formulation extends Euclidean Langevin dynamics by incorporating geometric information through the Riemannian metric, enabling more efficient exploration of curved probability landscapes. Unlike the Euclidean case, discretizing manifold Brownian motion is non-trivial, except in a few special cases. \cite{li2023riemannian} considered the case of $M \equiv \mathcal{S}^d\times\cdots\times\mathcal{S}^d$ (i.e., finite product of spheres) and established convergence rates for a simple discretization scheme that discretizes only the drift (gradient term) while requiring exact implementation of manifold Brownian motion increments--feasible on the sphere. \cite{gatmiry2022convergence} extended this approach to general Hessian manifolds, proving convergence results under the same assumption of exact Brownian motion implementation, which is generally infeasible. Both works require the target density to satisfy a logarithmic Sobolev inequality and establish iteration complexity of 
$\text{poly}(1/\varepsilon)$  to obtain an $\varepsilon$-approximate sample in KL-divergence. However, the reliance on exact Brownian motion increments significantly limits the applicability of the results in~\cite{gatmiry2022convergence}.



\cite{cheng2022efficient} studied a practical discretization of Riemannian Langevin diffusion, where both the drift and Brownian motion are discretized. They established an iteration complexity in the 1-Wasserstein distance under a general assumption and in the 
2-Wasserstein distance under a more restrictive condition, which can be seen as an analog of log-concavity. Their complexity is of order $\tilde{\mathcal{O}}(1/\varepsilon^{2})$. A key technical challenge is that, in the absence of convexity (e.g., on a compact manifold), establishing contractivity under the Wasserstein distance is nontrivial -- even for the continuous-time dynamics. This difficulty is overcome through a second-order expansion of the Jacobi equation~\citet[Lemma 29]{cheng2022efficient}. \cite{kong2024convergence} introduced the Lie-group MCMC sampler for sampling from densities on Lie groups, with a primary focus on accelerating sampling algorithms. Their iteration complexity for the 2-Wasserstein distance are also $\text{poly}(1/\varepsilon)$.



%Compare with existing works.

\iffalse
\begin{table}[H]
    \caption{Complexity bounds for sampling on Riemannian manifold. 
    $\alpha$ is LSI constant, $d$ is dimension of manifold, $L$ is gradient Lipschitz constant,
    $L_{2}$ is Hessian Lipschitz constant, 
    $\gamma$ is some self-concordant constant (see \citep{gatmiry2022convergence}).}\label{tab:t1}
\begin{centering}
    {\renewcommand{\arraystretch}{1.4}%
    \begin{tabular}{|c|c|c|c|c|}
    \hline BM implementation & Source & Assumption & Complexity & Metric \\ \hline
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} 
    Approximated \\ through geodesic \\ random walk \end{tabular}} 
    & \citep{cheng2022efficient} & Distant-dissipativity & $\tilde{\mathcal{O}}(\varepsilon^{-2})$ & $W_{1}$ \\ \cline{2-5} 
    & \citep{cheng2022efficient} & ``log-concave'' & $\tilde{\mathcal{O}}(\varepsilon^{-2})$ & $W_{2}$ \\ \hline 
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} 
    Assumed exact \\ implementation \\ is available \end{tabular}} 
    & \citep{li2023riemannian} & \begin{tabular}[c]{@{}c@{}} 
        LSI, manifold is hypersphere or \\ product of hypersphere \end{tabular} & $\tilde{\mathcal{O}}(\frac{d L^{2}}{\alpha^{2} \varepsilon})$ &  KL  \\ \cline{2-5} 
    & \citep{gatmiry2022convergence} & \begin{tabular}[c]{@{}c@{}} 
        LSI, manifold is self-concordant \\ Hessian manifold \end{tabular} & $\tilde{\mathcal{O}}(\frac{d^{2.5} L \gamma + d^{0.5}L^{2} + dL_{2}}{\alpha^{2} \varepsilon})$ & KL \\ \cline{2-5}    
    & Theorem \ref{Main_Theorem} & \begin{tabular}[c]{@{}c@{}} 
        LSI, assume exact \\ implementation of RHK \end{tabular} &  & KL \\ \hline   
    \end{tabular}
    }
\par\end{centering}
\end{table}
\fi

\begin{table}[t]
\label{tab:t2}
\begin{centering}
    {\renewcommand{\arraystretch}{1.2}%
    \begin{tabular}{|c|c|c|c|c|}
    \hline Assumption & Source & Setting & Complexity & Metric \\ \hline \hline
    \multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} 
    Log-Sobolev Inequality (LSI) \end{tabular}} 
    & Theorem \ref{Main_Theorem} & Exact MBI, Exact RHK  & $\tilde{\mathcal{O}}(\log \frac{1}{\varepsilon})$ & KL \\ \hline
    \multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} 
    LSI \& Assumption 1 \end{tabular}} 
    & Theorem \ref{TV_Inexact_BM_Inexact_RHK} & Inexact MBI, Inexact RHK  & $\tilde{\mathcal{O}}(\log^2 \frac{1}{\varepsilon})$ & TV \\ \hline
 %   \multirow{1}{*}{\begin{tabular}[c]{@{}c@{}} 
 %   Log-Sobolev Inequality and \\ Assumption \ref{Assumption_Oracle_TV_quality} \end{tabular}} 
 %   & Theorem \ref{TV_Inexact_BM_Inexact_RHK} & Inexact MBI, Inexact RHK & $\tilde{\mathcal{O}}(\log^{2} \frac{1}{\varepsilon})$ & TV \\ \cline{2-5}
 %   & Theorem \ref{TV_Inexact_BM_Inexact_RHK} & Inexact MBI, Exact RHK & $\tilde{\mathcal{O}}(\log^{2} \frac{1}{\varepsilon})$ & TV  \\ \cline{2-5}
 %   & Theorem \ref{TV_Inexact_BM_Inexact_RHK} & Exact MBI, Inexact RHK & $\tilde{\mathcal{O}}(\log^{2} \frac{1}{\varepsilon})$ & TV  \\ \hline
      \end{tabular}
    }
\par\end{centering}
    \caption{A summary of iteration complexity results in this work. Here,  $\varepsilon$ represents the target accuracy. 
    The $\tilde{\mathcal{O}}$ notation hides dependency on all other parameters except for $\varepsilon$.}%\vspace{-0.3in}
\end{table}

In comparison to the above works for Riemannian sampling, for the Euclidean case, high-accuracy algorithms, i.e., algorithms with iteration complexity of $O(\text{polylog}(1/\varepsilon))$ are available under various assumptions (that are essentially based on (strong) log-concavity or isoperimetry); see for example~\cite{lee2021structured,chen2022improved,fan2023improved,he2024separation} for such results for the Euclidean proximal sampler and~\cite{dwivedi2019log,chen2020fast,chewi2021optimal,lee2020logsmooth,wu2022minimax,chen2023does,andrieu2024explicit,altschuler2024faster} for various Metropolized algorithms including Metropolis Random Walk (MRW), Metropolis Adjusted Langevin Algorithm (MALA) and Metropolis Hamiltonian Monte Carlo (MHMC). 

%complexity $\mathcal{O}(mn^{3}\log\frac{\Lambda}{\varepsilon})$
High-accuracy samplers for constrained (Euclidean) sampling, i.e., when the density is supported on convex set $\mathcal{K} \subseteq \mathbb{R}^{d}$ are established for Hit-and-run and Ball-walk based algorithms under various assumptions~\citep{lovasz1999hit,kannan2006blocking,kannan1997random}; see~\citet[Section 1.3]{kook2025renyi} for a detailed overview of related works. \cite{kook2022sampling}, proposed Constrained Riemannian Hamiltonian Monte Carlo (CRHMC), and used Implicit Midpoint Method to integrate the Hamiltonian dynamics and established a high-accuracy guarantee for discretized CRHMC. \cite{noble2023unbiased} proposed Barrier Hamiltonian Monte Carlo (BHMC) for constrained sampling, along with its discretizations and established asymptotic results. \cite{kook2024and} proposed the ``In-and-Out" sampling algorithm that has high-accuracy guarantees for sampling uniformly on a convex body. Recently~\cite{kook2024sampling} obtained state-of-the-art results for sampling from log-concave densities on convex bodies using a proximal sampler designed for this problem.~\cite{srinivasan2024fast} and~\cite{srinivasan2024high} showed that a Metropolized version of the Mirror and preconditioned Langevin Algorithm obtains high-accuracy guarantees, respectively, under certain assumptions.

Given the above, the following natural question arises: 
\begin{center}
   \emph{Can one develop high-accuracy algorithms for sampling on Riemannian manifolds?} 
\end{center}
To the best of our knowledge, no prior work exists on providing an affirmative answer to this question. In this work, we develop the \emph{Riemannian Proximal Sampler} which generalizes the Euclidean Proximal Sampler from~\cite{lee2021structured}. In contrast to the Euclidean case, the algorithm is based on the availability of two oracles: the Manifold Brownian Increment (MBI) oracle and the Riemannian Heat Kernel (RHK) oracle. We show in Theorem~\ref{Main_Theorem} that when the exact oracles available the algorithm achieves high-accuracy guarantee in the Kullback-Liebler divergence. Under the availability of inexact oracles, as characterized in Assumption~\ref{Assumption_Oracle_TV_quality}, we show in Theorem~\ref{TV_Inexact_BM_Inexact_RHK} that the algorithm still achieves high-accuracy guarantees in the total variation metric. Our results are summarized in Table~\ref{tab:t2}. We further develop practical implementations of the aforementioned oracles that satisfy the conditions in Assumption~\ref{Assumption_Oracle_TV_quality} (Section~\ref{Section_Oracle}), and that are connected to entropy-regularized proximal point method on Wasserstein spaces (Section~\ref{Section_Proximal_point_approximation}). We also demonstrate the numerical performance of the algorithms via simulations in Appendix~\ref{sec:sim}. 

