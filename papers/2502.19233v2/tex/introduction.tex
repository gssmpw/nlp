% \section{Introduction}

The increasing demand for DRAM in modern data centers has led to its significant share in both operational costs and power consumption, consuming approximately 37\% of the total cost and 33\% of the power, according to Meta~\cite{tpp_asplos23}. This dependency on DRAM is further entrenched by the DDR standard, which is designed around DRAM-specific access and maintenance commands, thereby limiting the integration of emerging memory technologies.

The emerging Compute Express Link (CXL) technique enables CPUs to access a variety of memory types by offering unified load-store semantics. This breakthrough paves the way for the adoption of alternative memory technologies, such as Managed DRAM~\cite{LeeJKKKKHKMLJKL19}, ReRAM~\cite{chen2020reram}, and 3D-XPoint/Optane~\cite{hady2017platform}, which promise larger capacities and lower costs~\cite{Subramanya_eurosys16}~\cite{Eisenman2018ReducingDF}~\cite{Kassa_atc21}, potentially reducing the Total Cost of Ownership (TCO) for data centers~\cite{sharma2024introductioncomputeexpresslink}. As shown in Figure~\ref{fig:heterogeneous_system_overview}-(a), different types of memory media can be integrated into single or multiple CXL memory devices. CPU can directly access the expanded memory via CXL channels, avoiding modifying the DDR memory controllers in existing CPUs. 
% Another innovation of CXL is that it introduces CXL switch, with which CPU can extends its memory more scalable.
% To facilitate larger-scale memory expansion, the CXL standard supports connecting host to multiple endpoint devices via switches. 
% Besides enhancing the diversity of memory media connected to the CPU, the CXL standard supports connecting host to multiple endpoint devices via switches to facilitate larger-scale memory expansion.
In addition to enhancing the diversity of memory media connected to the CPU, the CXL standard supports connecting the host to multiple endpoint devices via switches, enabling larger-scale memory expansion.
Additionally, CXL 3.0 introduces the capability for cascaded CXL switches~\cite{cxl}. As illustrated in Figure~\ref{fig:heterogeneous_system_overview}-(b), future CXL memory systems can feature a CXL switch connected to a host, another CXL switch, or a CXL memory device, thereby creating a complex topology.


% The high flexibility of the CXL interconnect, while advantageous, also presents challenges in performance optimization. This duality is evident in several aspects. 
% Firstly, new memory types such as Optane exhibit distinct read/write latencies and bandwidth characteristics. 
% The CXL standard allows extending the memory space with diverse memory media, thus naturally forming a heterogeneous memory space at device side.
% % Extending memory with multiple types of memory media naturally forms a heterogeneous memory system at device side.
% Secondly, the inclusion of additional CXL switches in the architecture can increase latency along the critical path. In a CXL extended memory system with cascaded switches, different CXL memory device may have different number of CXL switches on the critical path connected to host, which makes them exhibit different latency and bandwidth characteristics to host CPU.
% Consequently, The high flexibility of the CXL interconnect also cause high heterogeneity in the memory system extended by CXL. 
% Considering the additional latency introduced by slow memory media such as Optane and CXL switches, the performance of memory access on the heterogeneous memory system extended by CXL will be slowed significantly if without proper optimization.
% As a result, optimizing memory access performance on such heterogeneous memory system extended by CXL has become a pressing issue.

The high flexibility of the CXL interconnect, while advantageous, also presents challenges in performance optimization. This duality is evident in several aspects. 
Firstly, new memory types (e.g., Optane) exhibit distinct latency and bandwidth characteristics. Integrating these  memory media into a system via CXL results in a high heterogeneity.
% Firstly, CXL extended memory systems may inolve multiple memory media that exhibits greatly diverse latency and bandwidth characteristics. 
Secondly, the inclusion of CXL switches exacerbates such heterogeneity. 
% in the architecture can increase latency along the critical path. 
As shown in Figure~\ref{fig:heterogeneous_system_overview}-(b), in a CXL-extended memory system with cascaded switches, different CXL memory devices may have varying numbers of CXL switches on the critical path to the host, resulting in different latency and bandwidth characteristics to the host CPU. In such systems featuring  heterogeneous memory types and topologies, we should strive to ensure that the CPU's memory access traffic takes the fast path rather than the slow one to preserve performance.

% Consequently, the high flexibility of the CXL interconnect also leads to high heterogeneity in the memory system extended by CXL.
% Considering the additional latency introduced by slower memory media such as Optane and the inclusion of CXL switches, memory access performance on a heterogeneous memory system extended by CXL will be significantly impacted without proper optimization. Therefore, optimizing memory access performance on such a system has become a pressing issue.


\input{fig_tex/heterogeneous_system_overview.tex}


% As shown in Figure \ref{fig:heterogeneous_system_overview}-(b), memory requests from host CPU to different CXL memory device may pass through different number of CXL switches, which means different memory extension device may have different latency to the host. This also naturally forms an heterogeneous memory system.


% Prior research has endeavored to refine systems equipped with CXL memory from both system~\cite{tpp_asplos23, memtis_sosp23}  and architectural perspectives~\cite{neomem,memstrata_osdi24}. The main idea of existing solutions is to leverage the access locality of  workloads, identifying frequently-accessed hot data and placing them into fast memory tiers. The remaining cold data is migrated to slow memory tiers to make room for hot data. 

% % \noindent\textbf{Challenges of Prior Work:} 
% However, prior works face challenges to optimize the performance of CXL extended heterogeneous memory system. The reasons are as follow:
% % (1) Software-based methods are facing challenges of profiling memory access. Data movement is handled via software. 

% \noindent\emph{\textbf{Challenge 1}: {Lack of Evaluation Method} }- Due to the lack of real-world CXL-enabled CPUs and devices, previous studies rely on remote NUMA node emulation or software simulations to evaluate performance in CXL scenarios. However, NUMA emulation-based solutions struggle to explore performance under varying latency ratios and are challenging to apply to hardware-managed memory tiering systems, while software simulation approaches face difficulties in handling large workloads and assessing server CPU performance at real-scale due to low simulation speeds.

% \noindent\emph{\textbf{Challenge 2}: {Low Performance of Software-Based Method} }- Previous works using software-based method to manage heterogeneous memory system struggle to accurately identify hot and cold data due to hardware limitations, and the overhead of software-based data movement is significant.
% % (2) Hardware-managed tiered memory only consider two-tier system.

% \noindent\emph{\textbf{Challenge 2}: {Pool Flexibility of Previous Hardware-Based Method} }- Previous works using hardware-based method to manage heterogeneous memory system, primarily proposed before the CXL era, achieve memory access profiling and data movement by incorporating custom hardware logic before the memory interface in the CPU. These intrusive modifications to the CPU prevent these solutions from adapting to various CXL-extended memory system configurations.

% \noindent\textbf{Our Work:}
% To address \textbf{Challenge 1}, we construct HeteroBox, an enumeration platform built on a CXL-enabled FPGA to enumerate the features of the CXL memory system. HeteroBox can be configured to expose a memory space with an arbitrary number of regions, each with different latency attributes to the host CPU.
% For \textbf{Challenge 2}, we propose HeteroMem, a hardware-managed memory tiering system which efficiently profiling data hotness and migrating data to faster memory tiers using hardware logic.
% For \textbf{Challenge 3}, we build HeteroMem logic at device side, which acts as an abstraction layer between the CPU and device memory, concealing device-side heterogeneity from the CPU. The design of HeteroMem is totally transparent to CPU.

Prior research has endeavored to refine systems equipped with CXL memory from both system~\cite{tpp_asplos23, memtis_sosp23} and architectural perspectives~\cite{neomem, memstrata_osdi24}. The main idea of existing solutions is to leverage the access locality of workloads, identifying frequently accessed hot data and placing it into fast memory tiers. The remaining cold data is migrated to slow memory tiers to make room for hot data~\cite{mtm_ren2023, vtmm_eurosys23, simple_sc10}. 
However, prior works face challenges in optimizing the performance of the emerging CXL-extended heterogeneous memory systems. The reasons are as follows:

\noindent\emph{\textbf{Challenge 1}: Lack of Evaluation Platform} - 
% Due to the lack of real-world CXL-enabled CPUs and devices, previous studies rely on remote NUMA node emulation or software simulations to evaluate performance in CXL scenarios. 
Lacking real-world CXL-enabled CPUs and devices, previous studies rely on remote NUMA node emulation or software simulations to evaluate performance in CXL scenarios.
% However, NUMA emulation-based solutions fail to configure the latency and bandwidth characteristic of memory arbitrarily and are challenging to apply to hardware-managed memory tiering systems, while software simulation approaches face difficulties in handling large workloads and assessing server CPU performance at real scale due to low simulation speeds. \zhe{For instance, GEM5 xxx, Pin-based methods like ZSIM xxx, but cannot run operating systems. xxx}
However, NUMA emulation-based solutions cannot arbitrarily configure the latency and bandwidth characteristics of memory and are challenging to apply to hardware-managed memory tiering systems. 
Meanwhile, software simulation approaches struggle with handling large workloads and assessing server CPU performance at a real scale due to low simulation speeds. 
% For instance, the simulation speed of gem5~\cite{lowepower2020gem5simulatorversion200} in full system mode is approximately 100 kilo instructions per second (KIPS), which is tens of thousands of times slower than emulation methods.
Several studies test commercial CXL devices, but the latency and bandwidth attributes of these devices are not configurable, and CXL switches are currently unavailable.


\noindent\emph{\textbf{Challenge 2}: Low Performance of Software-Managed Memory Tiering System} - 
% As we detail in Section~\ref{sec:background}, previous software-based memory tiering systems struggle to accurately identify hot and cold data due to hardware limitations, and the overhead of software-based data migration is significant.
As detailed in Section~\ref{sec:background}, previous software-based memory tiering systems, which rely on various memory access profiling mechanisms including PTE scanning, hint-fault monitoring, and PMU sampling, struggle to accurately and efficiently identify hot and cold data due to hardware limitations. Meanwhile, the overhead of software-based data migration is significant.

\noindent\emph{\textbf{Challenge 3}: Poor Flexibility of Previous Hardware-Managed Memory Tiering System} - Previous hardware-based memory tiering systems, primarily proposed before the CXL era, achieve memory access profiling and data movement by incorporating custom hardware logic in the CPU. These intrusive modifications to the CPU prevent these solutions from adapting to various CXL-extended memory system configurations.

\noindent\textbf{Our Work:}
To address \textbf{Challenge 1}, we propose HeteroBox, an emulation platform based on a CXL-enabled FPGA. 
% to enumerate the features of the CXL memory system. HeteroBox can be configured to expose a memory space with an arbitrary number of regions, each with different latency attributes to the host CPU.
% HeteroBox abstracts each CXL memory device with an addressable memory region that could be accessed by host CPU with standard memory loads and stores, and each memory region could be assigned with a set of latency and bandwidth attributes with respect to the emulated memory features. 
HeteroBox abstracts each CXL memory device as an addressable memory region that the host CPU can access with standard memory loads and stores. 
% Each memory region can be assigned a set of latency and bandwidth attributes corresponding to the emulated memory features.
Each memory region can be assigned latency and bandwidth attributes corresponding to the emulated memory features.
For \textbf{Challenge 2}, we propose HeteroMem, a hardware-managed memory tiering system that efficiently profiles data hotness and migrates data to faster memory tiers using hardware logic. HeteroMem includes a profiling unit that tracks every memory request from the host CPU and identifies hot and cold pages, forwarding them to a migration unit. The migration unit is optimized for efficient data movement with high internal bandwidth.
For \textbf{Challenge 3}, we build HeteroMem logic on the device side, which acts as an abstraction layer between the CPU and device memory, hiding device-side heterogeneity from the CPU. 
% The design of HeteroMem is completely transparent to the CPU. 
A remapping unit in HeteroMem translates the address of memory request from host to device physical address, keeping migration completely transparent to CPU.
Our main contributions are as follows: 


% (3) The access latency is not configurable, and researchers cannot evaluate their architecture innovations. Although we can implement prototypes using software-based full-system simulators like GEM5, they can hardly finish running full-scale workloads due to extremely low simulation speed. 
%due to absence of real-world CXL memory device, they use CPU-less numa node to enumerate CXL memory. 
% While it is claimed that the latency of a CPU-less numa node keep close to CXL extended DRAM, this enumeration method fails to enumerate two features of CXL link: (1) CXL support multiple types of memory media other than DRAM. (2) CXL support multiple layers switch extension, which enables CXL memory system to form a complicated topology. 
%Nonetheless, these attempts have not succeeded in enhancing the performance of heterogeneous memory systems within the context of the Compute Express Link (CXL) era, primarily due to two distinct characteristics of such systems. Firstly, CXL facilitates memory expansion, enabling substantial scalability. This expansion, in turn, introduces increased overhead for software-based optimization methods. Secondly, the topology and the types of memory devices within CXL heterogeneous memory systems are subject to variation and dynamism. Consequently, previous hardware-based optimization strategies that involve modifications to the CPU architecture are inadequate and lack the necessary flexibility.




% To enumerate these features of CXL memory system, we build an enumeration platform on CXL-enabled FPGA. We add a configurable abstraction layer above the memory media, which allow different region of the memory to show different latency attribute. 
%Based on the enumeration platform, we build HeteroBox, a device side hardware managed heterogeneous memory system, and compare it with other existing heterogeneous memory managing system. 
%\todo{describe limitation of hardware and software baselines in table 1, corresponding to the design goals next}
% To enumerate the features of the CXL memory system, we constructed an enumeration platform on a CXL-enabled FPGA. We incorporated a configurable abstraction layer above the memory media, allowing different memory regions to exhibit distinct latency characteristics.
% Based on the real-world CXL enabled emulation platform, we explore different techniques of managing the CXL extended memory tiering system, including software-based techniques and hardware-based technique. 
% A memory tiering system optimize its performance through detecting hot data and migrate it to fast memory, meanwhile detecting cold data and demote it to slow memory. While there are many existing works try to optimize the performance of memory tiering system, our finding shows that, previous works fail to get optimized performance in such a system. That's because: (1) Software-based memory tiering systems fail to accurately detect hot and cold data due to their hardware limitation, and software based data movement incurs significant overhead. (2) Hardware-based memory tiering systems, which are mainly proposed before CXL era, achieve memory access profiling and data movement by adding custom hardware logic before memory interface in CPU. The intrusive modification in CPU makes these solutions fail to adapt to various CXL extended memory system configration.

% Based on a real-world CXL-enabled emulation platform, we explore various techniques for managing the CXL-extended memory tiering system, including both software-based and hardware-based approaches. A memory tiering system optimizes performance by identifying hot data and migrating it to faster memory, while demoting cold data to slower memory. Although numerous existing works attempt to enhance the performance of memory tiering systems, our findings indicate that previous efforts have not achieved optimal performance in such systems. This is due to the following reasons: (1) Software-based memory tiering systems struggle to accurately identify hot and cold data due to hardware limitations, and the overhead of software-based data movement is significant. (2) Hardware-based memory tiering systems, primarily proposed before the CXL era, achieve memory access profiling and data movement by incorporating custom hardware logic before the memory interface in the CPU. These intrusive modifications to the CPU prevent these solutions from adapting to various CXL-extended memory system configurations.


% % \noindent\textbf{Design goals:} We claim the goals of designing an ideal heterogeneous memory managing system in CXL era are listed as follow:

% \noindent\textbf{Design goals:} We assert that the objectives of designing an ideal heterogeneous memory management system in the CXL era are as follows:

% % \noindent\emph{\textbf{G1}: {Accurate and Efficient Hotness Profiling} }â€“ A heterogeneous memory managing system place hot data at fast memory and cold memory at slow memory to attain performance improvement. However, inaccurate profiling will cause cold data being falsely placed at fast memory and unnecessary data movement, thus lowering the performance. Also, the overhead of the profiling method should be kept small to avoid affect the system performance. Hence, an accurate and efficient hotness profiling mechanism is crucial.

% \noindent\emph{\textbf{G1}: {Accurate and Efficient Hotness Profiling} }- A heterogeneous memory management system places hot data in fast memory and cold data in slow memory to enhance performance. However, inaccurate profiling can lead to cold data being mistakenly placed in fast memory and unnecessary data movement, thereby reducing performance. The profiling method should have minimal overhead to avoid impacting system performance. Therefore, an accurate and efficient hotness profiling mechanism is essential.

% % \noindent\emph{\textbf{G2}: {Efficient Data Movement} }- When hot data currently reside in slow memory is detected, a data movement mechanism is needed to move the hot data to fast memory. The heterogeneous memory managing system should keep the overhead of the data movement small to avoid affecting the system performance. 

% \noindent\emph{\textbf{G2}: {Efficient Data Movement} }- When hot data currently residing in slow memory is detected, a data movement mechanism is required to transfer the hot data to fast memory. The heterogeneous memory management system should minimize the overhead of data movement to avoid affecting system performance.

% % \noindent\emph{\textbf{G3}: {Flexibility and Compatibility} }- Unlike numa memory system, in which the number of sockets is fixed and the type of memory media is mainly DRAM, the CXL heterogeneous memory system is highly dynamic and has complicated topology. An ideal heterogeneous memory managing system should be flexible to be configured for various heterogeneous memory configuration.

% \noindent\emph{\textbf{G3}: {Flexibility and Compatibility} }- Unlike NUMA memory systems, where the number of sockets is fixed and the memory media type is mainly DRAM, the CXL heterogeneous memory system is highly dynamic with a complex topology. An ideal heterogeneous memory management system should be flexible and configurable for various heterogeneous memory configurations.

% % \noindent\textbf{Our Work:} We carefully manage the design of HeteroMem to meet these designing goals. For \textbf{G1}, we build a count min sketch to trace every memory access and detect hot data dynamically. Also, we build an access bit table to detect cold data. For \textbf{G2}, we achieve data movement by adding hardware logic in CXL device control logic, and design an atomic transaction flow for data movement. For \textbf{G3}, we do all the profiling and data movement at CXL memory device side, which is transparent to CPU and highly configurable.

% \noindent\textbf{Our Work:} We meticulously designed HeteroMem to meet these design goals. For \textbf{G1}, we implemented a count-min sketch to track every memory access and dynamically detect hot data. Additionally, we created an access bit table to identify cold data. For \textbf{G2}, we facilitated data movement by incorporating hardware logic into the CXL device control logic and designed an atomic transaction flow for data movement. For \textbf{G3}, all profiling and data movement are performed at the CXL memory device side, making it transparent to the CPU and highly configurable.

% Previous works have do efforts to optimize a system with heterogeneous memory. However, they all fail to optimize the performance of the heterogeneous memory system in the CXL Era because of two features of CXL heterogeneous memory system. First, CXL support memory extension, which can scale the memory to a large extent. This will cause the software-based method to have more overhead. Second, the topology and memory device type of the CXL heterogenerous memory system can be diverse and dynamic, while the past hardware-based method that modify the CPU is not suitable and agilex enough.

% To address these problems, we propose to combine the large capacity slow memory with a small fraction of DRAM in the same CXL device, which utilize the ability of CXL to manage multiple memory controllers in one single CXL device. This make up a two tiered memory system in CXL device, consisting of fast memory(FMem) and slow memory(SMem). As shown in ~\ref{fig:cxl_mem_overview}, previous works expose the hetergenerous information of memory to OS by deviding medias with differnt latency into differnt NUMA nodes. This ensures a homogenous abstraction whin each NUMA node. Based on this abstraction, there are a lot of previous works to optimizing the multi-tiered memory system~\cite{tpp_asplos23}~\cite{li2022pond}~\cite{hemem_sosp21}~\cite{amp}~\cite{memtis_sosp23}. 
% However, existing memory tiering techniques encounter significant challenges when applied to CXL-based multi-tiered memory system. The reasons are followed. First, existing memory tiering systems don't have an effective, low-overhead memory profiling method. CXL memory allows direct CPU access without OS awareness of the access patterns, so existing CXL-based memory tiering systems all require special memory profiling techniques like PTE-scan, hint-fault monitoring and PMU sampling, each with inherent limitations, to differentiate between "hot" and "cold" pages. These profiling methods, as detailed in the following section, suffer from coarse granularity and high overhead. Second, existing CXL-based memory tiering system use software migration function to move data between fast memory and slow memory, which suffers from high overhead from software stack. Third, existing memory tiering systems mainly consider the situation of two tiers





\begin{itemize}
    % \item We propose HeteroBox, an FPGA-based emulation platform for exploring CXL-based heterogeneous memory system, which can emulate complicated heterogeneous memory system configuration of CXL memory system.
    
    % \item We explore software-based NUMA management, hardware-based memory management, DRAM cache, PoM. 
    % \item Based on the emulation platform, we build a hardware heterogeneous memory managing system and present its design.
    
    % \item We evaluate the performance of HeteroMem on the emulation platform. We compare HeteroMem's performance to several recent proposals for tiered memory management that were previously evaluated only in emulation, and the evaluation result shows that HeteroMem achieves 11.8\%-24.3\% performance improvement compared to state of the art heterogeneous memory managing system.

    \item We propose HeteroBox, a configurable emulation platform leveraging real CXL-enabled FPGAs to emulate the performance of various CXL memory architectures. By carefully designing its architecture, we enable HeteroBox to support multiple emulated regions, each with distinct latency and bandwidth characteristics (Section~\ref{sec:heterobox}).

    \item We propose HeteroMem, a hardware-managed memory tiering system operating on the device side. We carefully design the architecture of HeteroMem to accurately and efficiently detect hot and cold data, and to manage data placement through effective data migration (Section~\ref{sec:heteromem}).

    \item We evaluate HeteroMem's performance on the HeteroBox emulation platform and explore various design choices for hardware-managed memory tiering systems. Our findings demonstrate that HeteroMem achieves a performance improvement of 5.7\% $\sim$ 17.6\% over several existing memory tiering systems, highlighting its effectiveness and potential as a superior memory management solution (Section~\ref{sec:evaluation}).

    % We evaluate HeteroMem's performance on the HeteroBox emulation platform. Our results show that HeteroMem achieves a performance improvement of 5.7\% to 17.6\% compared to several existing memory tiering systems.

\end{itemize}

% The remainder of this paper is organized as follows. We give a brief introduction to CXL and detail our motivation in Section~\ref{sec:background}. We present the design of HeteroBox in Section~\ref{sec:heterobox} and then present the design of HeteroMem in Section~\ref{sec:heteromem}. Finally, we detail the implementation of HeteroBox and HeteroMem 
% in Section~\ref{sec:implementation} and evaluate the performance of HeteroMem compared to several existing memory tiering systems in Section~\ref{sec:evaluation}.

% The remainder of this paper is organized as follows: Section~\ref{sec:background} introduces CXL and our motivation. Section~\ref{sec:heterobox} presents HeteroBox's design, followed by HeteroMem's design in Section~\ref{sec:heteromem}. Section~\ref{sec:implementation} details the implementation of HeteroBox and HeteroMem, and Section~\ref{sec:evaluation} evaluates HeteroMem's performance against several existing memory tiering systems.



% \input{fig_tex/cxl_mem_overview.tex}


% The increasing demand of DRAM has lead to a significant cost and power portion in modern datacenter. DRAM consumes about 37\% of cost and 33\% of power in modern data centers according to the data from Meta~\cite{tpp_asplos23}. However, the DDR standard relies on DRAM-specific commands for access and maintenance, which hinders adoption of new media types. The emerging Compute Express Link (CXL) offers equal load-store semantics to diverse memory types, which shed light on multiple promising media types including Managed DRAM~\cite{LeeJKKKKHKMLJKL19}, ReRam~\cite{chen2020reram}, 3DXP/Optane~\cite{hady2017platform}. These media types features large capacity and lower cost, which can lowering the TCO of modern data center. However, these media types suffer from higher latency, which significantly lower the performance, hindering their adoption. 

% This document provides instructions for submitting papers to the 56\textsuperscript{th} IEEE/ACM International Symposium on Microarchitecture\textsuperscript{\textregistered} (MICRO 2023).  In an effort to respect the efforts of reviewers and in the interest of fairness to all prospective authors, we request that all submissions to MICRO 2023 follow the formatting and submission rules detailed below. Submissions that violate these instructions may not be reviewed, at the discretion of the program chairs, in order to maintain a review process that is fair to all potential authors. 

% This document is itself formatted using the MICRO 2023 submission format. The content of this document mirrors that of the submission instructions that appear on the conference website. All questions regarding paper formatting and submission should be directed to the program chairs.