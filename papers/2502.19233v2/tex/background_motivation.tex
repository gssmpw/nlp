% \input{table_tex/evaluation_platform.tex}

\input{table_tex/comparison_table.tex}

\subsection{CXL-based Memory Expansion}
% The Compute-Express-Link (CXL) protocol\cite{cxl}, which is based on the PCIe 5.0 physical layer, enables a cache-coherent, byte-addressable interconnect through the use of high-efficiency SerDes links. This protocol is divided into three primary sub-protocols: \texttt{CXL.io}, \texttt{CXL.cache}, and \texttt{CXL.mem}. The \texttt{CXL.mem} sub-protocol is particularly noteworthy as it allows for direct CPU access to memory devices at cache-line granularity through the CXL interface. This contrasts with traditional DDR memory architectures, where the memory controller is located within the host CPU and is challenging to upgrade. CXL, however, incorporates memory controllers on the device side, which decouples the architecture and allows for the flexible integration of a variety of memory technologies into server systems. This flexibility is crucial for addressing specific needs in terms of capacity, performance, and cost-effectiveness~\cite{hynix_cxl_mem, samsung_cxl_mem, pond_asplos2023, tpp_asplos23}.
The Compute Express Link (CXL) protocol\cite{cxl}, based on the PCIe 5.0 physical layer, facilitates a cache-coherent, byte-addressable interconnect through high-efficiency SerDes links. This protocol composes three primary sub-protocols: \texttt{CXL.io}, \texttt{CXL.cache}, and \texttt{CXL.mem}. 
The \texttt{CXL.mem} sub-protocol is particularly noteworthy as it allows for direct CPU access to memory devices at cache-line granularity through the CXL interface. 
% This contrasts with traditional DDR memory architectures, where the memory controller is located within the host CPU and is challenging to upgrade. CXL, however, incorporates memory controllers on the device side, decoupling the architecture and allowing for the flexible integration of various memory technologies into server systems. This flexibility is crucial for addressing specific needs in terms of capacity, performance, and cost-effectiveness~\cite{hynix_cxl_mem, samsung_cxl_mem, pond_asplos2023, cxl_enhanced_memory_function}.
Unlike traditional DDR memory architectures with memory controllers in the host CPU, CXL incorporates memory controllers on the device side.
This decouples the architecture, allowing flexible integration of various memory technologies into CXL memory controllers while preserving the host CPU design. Such flexibility is crucial for meeting specific capacity, performance, and cost-effectiveness needs of modern memory systems~\cite{hynix_cxl_mem, samsung_cxl_mem, pond_asplos2023, cxl_enhanced_memory_function}.


% CXL 2.0 introduces CXL switch, which supports single level of switching. CXL switch allows one or multiple hosts to be connected with one or more CXL device. In CXL 3.0, multiple switch levels is support, which enables more complicated CXL memory devices topology. CXL 3.0 uses PBR(Port Based Routing) for inter-switch links to route the CXL message, which make the software running in the host see a flat topology that ends at the leaf switch port. However, using CXL switch will cause extra latency. According to \cite{pond_asplos2023}, each CXL switch a CXL req passes will cause about 70ns extra latency. In a complicated CXL memory system consisted of multiple switch levels, memory request to different device may pass different number of switches, which forms a complicated heterogeneous memory system.

CXL 2.0 introduces the CXL switch, which supports a single level of switching. The CXL switch allows one or multiple hosts to connect to one or more CXL devices. CXL 3.0 allows multiple switch levels, which enables more complex CXL memory device topologies. CXL 3.0 uses Port Based Routing (PBR) for inter-switch links to route CXL messages, allowing the software running on the host to perceive a flat topology ending at the leaf switch port. However, using a CXL switch introduces additional latency. According to~\cite{pond_asplos2023}, each CXL switch that a CXL request passes through adds about 70ns of latency. In a complex CXL memory system consisting of multiple switch levels, memory requests to different devices may pass through varying numbers of switches, resulting in a memory system with heterogeneous latency and bandwidth characteristics to the host CPU. 
% complex heterogeneous memory system.

% However, currently there are no available commercial CPU which support CXL 3.0 or newer CXL specification version. As a result of the lack of real world CXL enabled CPU and CXL devices, previous works use remote numa nodes which do not have CPU to emulate the CXL extended memory. However, according to~\cite{caption_micro23}, remote CXL extended memory shows different latency and bandwidth from remote numa node. Besides, CXL decouples CPU and memory interface, which allows more complicated modification at the device side. These difference limit the chance to use remote numa nodes to emulate the CXL memory, thus calling for an emulation platform which can better characterize the heterogeneity of CXL expanded memory system.



% However, currently there are no commercially available CPUs that support CXL 3.0 or newer versions of the CXL specification. 
% However, due to the lack of real-world CXL-enabled CPUs and devices, previous work has relied on remote NUMA node emulation or software-based simulation methods to evaluate the performance of CXL-based heterogeneous memory systems.
% However, according to~\cite{caption_micro23}, CXL-extended memory exhibits different latency and bandwidth characteristics compared to remote NUMA nodes.  
% Furthermore, CXL decouples the CPU and memory interface, allowing for more complex modifications on the device side, which can not be implemented in NUMA emulation-based platform.
% Software-based simulation methods, however, struggle
% with handling large workloads and assessing server CPU
% performance at a real scale due to low simulation speeds.
% For instance, the simulation speed of gem5~\cite{lowepower2020gem5simulatorversion200} in full system mode is approximately 100 kilo instructions per second (KIPS), which is tens of thousands of times slower than emulation methods.
% These limitations of existing evaluation methods highlights the need for an emulation platform that can better characterize the heterogeneity of CXL-expanded memory systems.
% However, due to the lack of real-world CXL-enabled CPUs and devices. 
% Although some studies use CXL-enabled CPUs and devices to evaluate CXL-extended memory systems, their configurations are limited due to current commercial CPUs supporting older CXL protocol versions and the still nascent adoption of the CXL device ecosystem.
% Previous work has relied on remote NUMA node emulation or software-based simulation methods to evaluate the performance of CXL-based heterogeneous memory systems

However, due to the limited availability of real-world CXL-enabled CPUs and devices, the evaluation of CXL-extended memory systems faces significant constraints. Specifically, previous work often relies on remote NUMA node emulation~\cite{tpp_asplos23, memtis_sosp23} or software-based simulation methods~\cite{cxlanns_atc23} to assess the performance of CXL-based heterogeneous memory systems.
But real CXL-extended memory exhibits different latency and bandwidth characteristics compared to remote NUMA nodes according to~\cite{caption_micro23}. Furthermore, CXL decouples the CPU and memory interface, allowing for more complex modifications on the device side, which cannot be modeled by NUMA emulation-based platforms. 
Software-based simulation methods, however, struggle with handling large workloads and assessing server CPU performance at a real scale due to low simulation speeds. For instance, the simulation speed of gem5~\cite{lowepower2020gem5simulatorversion200} in full system mode is approximately 100 kilo instructions per second (KIPS), which is tens of thousands of times slower than emulation methods.  While some studies utilize CXL-enabled CPUs and devices~\cite{caption_micro23, cxlshm_sosp23, Sano_2023} for evaluation, their configurations are restricted by the available hardware  and can hardly adapt to other settings. 
% These limitations of existing evaluation methods highlight the need for an emulation platform that can better characterize the heterogeneity of CXL-expanded memory systems.
% Although some studies use real CPUs and devices that support CXL to evaluate the performance of CXL-extended memory systems, their configurations are quite limited due to the current commercial CPUs supporting lower versions of the CXL protocol and the limited adoption of the CXL device ecosystem.
These limitations highlight the need for an emulation platform that can efficiently and accurately evaluate the performance of CXL-extended heterogeneous memory systems.


\vspace{5pt}
\noindent\fbox{%
% \centering
  \parbox{0.47\textwidth}{%
    % \begin{center}
     % \textbf{{Insight\#1:}} Due to the lack of commercial available CXL 3.1 CPU and limitation of remote numa emulation method, an emulation method which can characterize the complicated latency attribute of CXL expanded memory system is in pressing need.
     \textbf{{Insight\#1:}}
     % Due to the lack of commercially available CXL 3.0 CPUs and the limitations of remote NUMA emulation methods, there is a pressing need for
     % It's crucial for an emulation method to accurately characterize the complex latency attributes of CXL-expanded memory systems. 
     There's an urgent need for a practical, high-fidelity method to evaluate the complex CXL-extended heterogeneous memory system. 
  }%
}
\vspace{5pt}



\subsection{Memory Tiering System}

% Several recent works endeavor to optimize the performance of heterogeneous memory system. All these systems have two key components: Hotness Profiling mechanism and Data Movement mechanism.

% Several recent works endeavor to optimize the performance of heterogeneous memory systems. All these systems incorporate two key components: \emph{Hotness Profiling} mechanism and \emph{Data Movement} mechanism.

% Numerous previous works have endeavored to build memory tiering systems to optimize the performance of heterogeneous memory system. In particular, we analyze the three essential aspects of memort tiering system design: (1) \emph{Memory Access Profiling}, (2) \emph{Data Movement}, (3) Flexibility and Compatibility. We present a comparison summary of prior work in Table~\ref{tab:comparison_table}.
Many studies have focused on developing memory tiering systems to enhance the performance of heterogeneous memory systems. Specifically, we examine three critical aspects of memory tiering system design: (1) \emph{Memory Access Profiling}, (2) \emph{Data Movement}, and (3) \emph{Flexibility and Compatibility}. Table~\ref{tab:comparison_table} provides a summary comparison of prior work.



\input{fig_tex/damon_overhead.tex}

\noindent{\textbf{Memory Access Profiling:}} 
% The Hotness Profiling mechanism is key to manage the heterogeneous memory system and gain performance improvement. Existing memory tiering system mainly use three kinds of hotness profiling mechanism: Hint fault, PTE scanning and PMU-based sampling. Hint fault periodically poison a part of the pages in memory, invalidating their ptes and causing page fault when they are next accessed. TPP~\cite{tpp_asplos23} and AutoNUMA utilize hint fault to detect the hot data in slow memory. PTE scanning utilize the access bit in pte to detect hot data. When a page is accessed, the MMU automatically set the access bit. A kernel thread periodically scan the memory space to check whether a page is accessed during the last period. AMP~\cite{amp} utilize PTE scanning to detect hot data in slow memory, while TMTS~\cite{tmts_asplos2023} use PTE scanning to detect cold data in fast memory. PMU sampling utilize specific hardware counters in CPU to profile hot data in slow memory. For example, CPU of Intel surpport using PEBS counters to record LLC miss event. Every time a LLC miss event happens, the PEBS counter will be added by one. When the counter reaches a user defined threshold, the counter will overflow and record the access address. Hemem~\cite{hemem_sosp21} and MEMTIS~\cite{memtis_sosp23} use PMU sampling to detect the hot data in slow memory. 
% As shown in Figure~\ref{fig:damon_overhead}-(a), we use DAMON~\cite{damon} to profile the CPU overhead introduced by PTE scanning. DAMON is a data access monitoring framework subsystem for the Linux kernel, which can be configured to profile memory access in different memory granularity and time granularity. We change the space granularity from 50 regions to 1050 regions and the time granularity from scanning the page table every 50ms to 1050ms. We visualize the overhead as a heat map, in which lighter color means heavier overhead. We observe significant overhead when using small space granularity or time granularity, and can cost at most 88.148\% CPU time.
%\todo{Experiment result to showcase the insight.}
%However, according to a previous work~\cite{neomem}, these profiling method suffers from low resolution and high overhead. In our HeteroBox design, we build a hardware sketch in CXL memory device and profile the hot data in high resolution and low overhead.
The memory access profiling mechanism is crucial for managing heterogeneous memory systems and achieving performance improvements. 
% For hot data, existing memory tiering systems primarily utilize three types of memory access profiling mechanisms: hint faults, PTE scanning, and PMU-based sampling.
% However, compared to hardware-based methods, software-based memory access profiling methods suffer from high overhead and low resolution. The software-based memory access profiling methods are mainly consist of three types: Hint Fault, PTE scanning, and PMU-based sampling.
However, compared to hardware-based methods, software-based memory access profiling methods suffer from high overhead and low resolution. Software-based methods primarily consist of three types: hint-fault monitoring, PTE scanning, and PMU-based sampling.
Hint-fault monitoring periodically poisons a portion of the memory pages, invalidating their PTEs and causing page faults when they are next accessed. TPP~\cite{tpp_asplos23} and AutoNUMA~\cite{corbet2012autonuma} utilize hint-fault monitoring to detect hot data in slow memory. PTE scanning uses the access bit in the PTE to detect hot data. When a page is accessed, the MMU automatically sets the access bit. A kernel thread periodically scans the memory space to check whether a page has been accessed during the last period. AMP~\cite{amp} uses PTE scanning to detect hot data in slow memory, while TMTS~\cite{tmts_asplos2023} uses PTE scanning to detect cold data in fast memory. PMU sampling leverages specific hardware counters in the CPU to profile hot data in slow memory. For example, Intel CPUs support using PEBS counters to record LLC miss events. Each time an LLC miss event occurs, the PEBS counter increments by one. When the counter reaches a user-defined threshold, it overflows and records the access address. HeMem~\cite{hemem_sosp21} and MEMTIS~\cite{memtis_sosp23} use PMU sampling to detect hot data in slow memory.


However, as discussed in previous work~\cite{neomem}, these mechanisms fail to achieve both low overhead and high resolution simultaneously.
As shown in Figure~\ref{fig:damon_overhead}-(a), we use DAMON~\cite{damon} to profile the CPU overhead introduced by PTE scanning as an example. DAMON is a data access monitoring framework subsystem for the Linux kernel, which can be configured to profile memory access with different spatial and temporal granularities. We vary the space granularity from 50 regions to 1050 regions and the time granularity from scanning the page table every 50ms to every 1050ms. We visualize the overhead as a heat map, where lighter colors indicate heavier overhead. We observe significant overhead when using small spatial or temporal granularities, with a maximum cost of 88.15\% CPU time. 

\vspace{5pt}
\noindent\fbox{%
% \centering
  \parbox{0.47\textwidth}{%
    % \begin{center}
     \textbf{{Insight\#2:}} Software-based memory access profiling methods suffer from high overhead and low resolution. 
  }%
}
\vspace{5pt}

\input{fig_tex/heterobox.tex}

\noindent{\textbf{Data Movement:}} 
% After hot data is detected in the slow memory, a data movement mechanism is needed to migrate it to fast memory. Normally, CPU can migrate data between fast memory and slow memory. Many previous work utilize the \texttt{migrate\_pages} api in Linux kernel to migrate data. However, as demonstrated in Nimble~\cite{nimble_asplos19}, using CPU to migrate data suffers from significant overhead and lowered bandwidth because of the software stack in OS like checking permission, unmapping and remapping pages. Although there are works trying to mitigate these limitation by using transactional or parallel migration, they still use CPU to migrate data, which inevitably causes CPU overhead. Moreover, using CPU to migrate data between CXL memory devices requires read the data out and then write them back, which consumes the bandwidth of the CXL link, which is already limited. 
% %\todo{Experiment result to showcase the insight.}
% As shown in Figure~\ref{fig:damon_overhead}-(b), we compare the bandwidth of using custom hardware logic described in Section~\ref{sec:heterobox_hardware} to migrate data and using CPU to migrate data. For both using custom hardware logic and using CPU to migrate data, we use 4KB pages granularity and record the maximum pages number that can be migrated per second. we observe that using custom hardware logic to migrate data achieves 12.9X higher bandwidth compared to using CPU to migrate data.
After hot data is detected in slow memory, a data movement mechanism is required to migrate it to fast memory. Typically, the CPU can migrate data between fast and slow memory. 
Many previous works utilize the \texttt{migrate\_pages} API in the Linux kernel for this purpose. 
% For example, a recent state-of-the-art work, NeoMem~\cite{neomem}, utilizes the \texttt{migrate\_pages} API in the Linux kernel for data movement.
However, as demonstrated in Nimble~\cite{nimble_asplos19}, using the CPU for data migration incurs significant overhead and reduced bandwidth due to the software stack in the OS, including tasks like checking permissions and unmapping and remapping pages. Although some works attempt to mitigate these limitations by using transactional or parallel migration, they still rely on the CPU for data migration, which inevitably causes CPU overhead. Furthermore, using the CPU to migrate data between CXL memory devices requires reading the data out and then writing it back, which consumes the limited bandwidth of the CXL link.
As shown in Figure~\ref{fig:damon_overhead}-(b), we compare the bandwidth of using custom hardware logic, as described in Section~\ref{sec:heteromem}, to migrate data versus using the CPU to migrate data. For both methods, we use the 4KB page granularity and record the maximum number of pages that can be migrated per second. We observe that using custom hardware logic to migrate data achieves 12.9x higher bandwidth compared to using the CPU. 
%\zhe{Ideal bandwidth?}




\vspace{5pt}
\noindent\fbox{%
% \centering
  \parbox{0.47\textwidth}{%
    % \begin{center}
     % \textbf{{Insight\#3:}} Using CPU to move data shows higher overhead and lower bandwidth than device-side hardware managed data movement. 
     \textbf{{Insight\#3:}} Using the CPU to move data results in higher overhead and lower bandwidth compared to device-side hardware-managed data movement.
  }%
}
\vspace{5pt}



\noindent{\textbf{Flexibility and Compatibility:}} 
% Previous software based heterogeneous memory managing system expose the heterogeneous information to CPU. One method is to expose CXL memory to CPU as NUMA nodes, in which each node has an abstraction that is consist of homogenous memory. The advantages of using NUMA abstraction is that it can seamlessly utilize the previous optimize mechanism in operating system.
% Another method is using DAX mode of CXL memory, in which the CXL memory device is exposed to CPU as a file which support load-store semantic. In this situation, user need to explicitly modify the source code of the application to explicitly use the DAX file as extended memory. Both using NUMA and DAX mode are not transparent to CPU, which lower the flexibility and compatibility. Some previous works use hardware based mechanism to manage the heterogeneous system. For example, MemPod modify the CPU side memory controller to group different types of memory into several pods. The MemPod logic in memory controller optimize the performance of the heterogeneous memory system without the involvement of CPU. These hardware based heterogeneous memory managing system keep transparent to CPU. However, due to architecture limitation, they modify the hardware in CPU, which make them fail to adapt to the various configuration of CXL memory.
Previous software-based CXL memory tiering systems can be classified into two categories.
% heterogeneous memory management systems expose heterogeneous information to the CPU. 
One approach is to present CXL memory to the CPU as NUMA nodes, where each node consists of homogeneous memory. The advantage of using NUMA abstraction is that it can seamlessly leverage previously optimized mechanisms in the operating system.
Another approach is to use the DAX mode of CXL memory, in which the CXL memory device is exposed to the CPU as a file that supports load-store semantics. In this scenario, users must explicitly modify the source code of the application to use the DAX file as extended memory. 
Both using NUMA and DAX modes for heterogeneous memory management are not transparent to the CPU, reducing flexibility and compatibility.

Other works employ hardware-based memory management for better performance. For example, MemPod~\cite{mempod_hpca17} modifies the CPU-side memory controller to group different types of memory into several pods. The MemPod logic in the memory controller optimizes the performance of the heterogeneous memory system without CPU involvement. These hardware-based heterogeneous memory management systems remain transparent to CPU. However, due to architectural limitations, they require modifications to CPU hardware, making them unsuitable to adapt to various configurations of CXL memory.

% A recent state-of-the-art work, NeoMem~\cite{neomem}, proposes to integrate a hardware unit in the CXL memory device, which communicates with the host CPU to provide memory access information. NeoMem requires CPU to process the information provided by the hardware unit and requires CPU to do the subsequent data movement, which is not transparent to CPU, reducing flexibility and compatibility.
A recent state-of-the-art work, NeoMem~\cite{neomem}, proposes integrating a hardware unit within the CXL memory device, which communicates with the host CPU to provide memory access information. However, NeoMem relies on CPU to process this information and perform subsequent data movement, making it non-transparent to CPU and reducing overall flexibility and compatibility.


\vspace{5pt}
\noindent\fbox{%
% \centering
  \parbox{0.47\textwidth}{%
    % \begin{center}
     % \textbf{{Insight\#4:}} Previous software and hardware based memory tiering system fail to adapt to complicated CXL extended memory system.
     \textbf{{Insight\#4:}} Previous software-based and hardware-based memory tiering systems fail to adapt to the complex configurations of CXL-extended memory systems.
  }%
}
\vspace{5pt}

% \noindent{\textbf{Software Based Tiered Memory System:}} Several recent works endeavor to optimize the performance of heterogeneous memory system using software method. These methods use software method to detect hot page in slow memory and migrate them to fast memory using CPU instructions. TPP~\cite{tpp_asplos23} utilizes the numa balancing mechanism in linux kernel.

% \noindent{\textbf{Hardware Based Tiered Memory System:}}

To address these limitations, we first develop HeteroBox, an emulation platform designed to emulate a multi-tier CXL-extended heterogeneous memory system. Building on this platform, we introduce HeteroMem, a CXL-native, hardware-based memory tiering system. The core design philosophy of HeteroMem is to create an abstraction layer between host CPU and memory controller within the CXL device. This hardware-based abstraction layer is tasked with identifying hot data in slower memory and migrating it to faster memory. This entire process is completely transparent to host CPU. Further details are introduced in the following sections.

% \subsection{}