
\subsection{Experimental Setup}

% \input{table_tex/benchmark_config.tex}



\noindent{\textbf{Baselines:}}
% We compare the performance of our HeteroMem design with three baseline memory tiering systems: PTE-Scan~\cite{damon}, PEBS~\cite{pebs_events} and NeoMem~\cite{neomem}. PTE-Scan utilizes the access bit in page table entry for hotness profiling. The access bit is set by MMU when a page is accessed, while PTE-Scan scan the page table peridically, clearing access bit and detecting a page as hot page if its access bit is set. PTE-Scan migrate hot pages to fast memory as soon as they are detected. 
% PEBS uses hardware counters to profile page hotness. It is configured to sample $MEM\_LOAD\_L3\_MISS\_RETIRED$ event. When a LLC miss happens, the hardware counters will be added by one. When the counter reaches a given threshold, it will overflow and record the address of current memory access. The PEBS memory tiering system detect a page as hot page and migrate it to fast memory when its address is recorded. 
% NeoMem is a recently proposed memory tiering system which gets state of the art performance. NeoMem add only hot page pofiling structure in CXL memory device and report the hot pages to host CPU. The host CPU will then migrate these hot pages to fast memory.
We compare the performance of our HeteroMem design with five baseline memory tiering systems: PTE-Scan~\cite{damon}, PEBS~\cite{pebs_events}, Auto NUMA~\cite{corbet2012autonuma}, TPP~\cite{tpp_asplos23}, and NeoMem~\cite{neomem}. PTE-Scan utilizes the access bit in the page table entry for hotness profiling. The access bit is set by the MMU when a page is accessed, and PTE-Scan periodically scans the page table, clearing the access bit and detecting a page as hot if its access bit is set. PTE-Scan migrates hot pages to fast memory as soon as they are detected.
PEBS uses hardware counters to profile page hotness. It is configured to sample the $MEM\_LOAD\_L3\_MISS\_RETIRED$ event. When an LLC miss occurs, the hardware counters increment by one. When the counter reaches a given threshold, it overflows and records the address of the current memory access. The PEBS memory tiering system detects a page as hot and migrates it to fast memory when its address is recorded.
TPP and AutoNUMA utilize hint-fault monitoring to detect hot data in slow memory.
% NeoMem is a recently proposed memory tiering system that achieves state-of-the-art performance. NeoMem adds a hot page profiling structure to the CXL memory device and reports the hot pages to the host CPU, which then migrates these hot pages to fast memory.
NeoMem is a recently proposed memory tiering system that achieves state-of-the-art performance. It enhances the CXL memory device with a hot page profiling structure, which identifies and reports hot pages to the host CPU for migration to faster memory.
% Table~\ref{table:parameters} shows the parameters configuration of these memory tiering systems.
Table~\ref{table:parameters} presents the parameter configurations for these memory tiering systems.




\noindent{\textbf{Benchmarks:}}
% We compare the performance of HeteroMem with other 
% We choose six benchmarks for evaluation: GUPS~\cite{gups}, a microbenchmark which executes parallel read-modify-write operations to fixed size objects in a uniform or skewed random pattern in its working set and measures the giga update operations per second (GUPS) it performs; Silo, an in-memory database engine; Btree, an in-memory index lookup; XSBench, an HPC workload; Betweenness Centrality(BC) and PageRank(PR), two graph processing workload.
% We select six benchmarks for evaluation: GUPS~\cite{gups}, a microbenchmark that executes parallel read-modify-write operations in a uniform or skewed random pattern within its working set; Silo~\cite{silo}, an in-memory database engine; Btree, an in-memory index lookup; XSBench~\cite{xsbench}, an HPC workload; and Betweenness Centrality (BC) and PageRank (PR)~\cite{gapbs}, two graph processing workloads. These benchmarks are widely used to evaluate tiered memory system in previous works~\cite{hemem_sosp21,memtis_sosp23,neomem}. The Resident Set Size(RSS) of these benchmarks ranges from 4.4GB to 10.9GB, as shown in Table~\ref{table:benchmark_characteristics}.
We select eight benchmarks for evaluation: GUPS~\cite{gups}, a microbenchmark that performs parallel read-modify-write operations in a uniform or skewed random pattern within its working set; 
Silo~\cite{silo}, an in-memory database engine; 
Btree, an in-memory index lookup; 
XSBench~\cite{xsbench}, an HPC workload; 
Betweenness Centrality (BC) and PageRank (PR)\cite{gapbs}, two graph processing workloads;
and 603.bwaves and 654.roms, two benchmarks from SPEC 2017.
These benchmarks are widely used to evaluate memory tiering systems in previous works\cite{hemem_sosp21, memtis_sosp23, neomem}. 
The Resident Set Size (RSS) of these benchmarks ranges from 4.4GB to 11.1GB. 
% as shown in Table~\ref{table:benchmark_characteristics}. 


\subsection{Validation of HeteroBox}

% \rebuttal{We use Intel MLC tool to evaluate the latency and bandwidth of HeteroBox emulation platform. We first do a validation experiment on emulation of latency and bandwidth attribute of a single region. As shown in Figure~\ref{fig:mlc_bandwidth_latency}-(a), we configure the HeteroBox to have one region and set the latency register value to be 0 to 256, and then we evaluate the latency of the region using Intel MLC (same for the latency and bandwidth evaluation in Figure~\ref{fig:mlc_bandwidth_latency}-(b)(c)(d)). For bandwidth configuration value ranging from 1024 to 32, we draw six curves. And we draw a curve called theoretical latency which is added by latency introduced by CXL link and CXL IP on FPGA (about 330ns, which is estimated by the latency value when the latency register value is 32 minus 160ns, this latency is high because of the chiplet design of CXL IP in the FPGA prototype) and the latency introduced by the HeteroBox emulation logic. The HeteroBox emulation logic works in a clock domain of 200MHz, so we calculated the introduced latency through the value of latency register multiplied with 5ns. When the value of latency register is 0, the latency attribute of the region is higher than the theoretical latency due to the latency introduced by underlying memory controller and DDR4 memory. We find that all six curves with different bandwidth configuration register value coincide, and they all show the same value compared to theoretical latency except when the value of latency register is 0. The result shows that: (1) The latency attribute of a region can be accurately adjusted by the HeteroBox emulation logic. (2) Changing the value of bandwidth configuration register will not interference the latency attribute of the region.}

We use the Intel MLC tool~\cite{mlc} to evaluate the latency and bandwidth attribute of the HeteroBox emulation platform. We first focus on the case of a single region. As illustrated in Figure~\ref{fig:mlc_bandwidth_latency}-(a), we configure the HeteroBox to contain one region and set the latency register value from 0 to 256. Using the Intel MLC, we then measure the region’s latency.
Changing the bandwidth configuration values from 128 to 32, we plot four curves of relationship between the value of latency register and the latency attribute of this region. Additionally, we include a curve called “theoretical latency”, which incorporates the latency from both the CXL link and the CXL IP on the FPGA. 
% (approximately 330 ns, estimated based on the latency value when the latency register is set to 64, minus 320 ns, which is elevated due to the chiplet design of the CXL IP in the FPGA prototype)
The theoretical latency also includes the latency introduced by the HeteroBox emulation logic. 
% Since the emulation logic operates in a 200 MHz clock domain, we calculate the latency it introduces by multiplying the latency register value by 5ns.
% When the latency register is set to 0, the region’s latency is equal to the latency when latency register is set to 32 due to the latency introduced by the underlying memory controller and DDR4 DRAM memory. 
Due to the latency introduced by the underlying memory controller and DDR4 DRAM, the region's latency is the same when the latency register is set to 0 or 32.
All four curves corresponding to different bandwidth configuration values overlap, showing consistent latency performance when compared to theoretical latency.
These results demonstrate two key findings: (1) \textbf{The HeteroBox emulation logic can precisely adjust latency attribute of a region}. (2) \textbf{Modifying the bandwidth configuration register value does not affect the latency attribute of the region}.

% \rebuttal{As shown in Figure~\ref{fig:mlc_bandwidth_latency}-(b), we draw five curves showing the relationship between the value of bandwidth configuration register and the value of the bandwidth attribute of the region. Each curve is configured with a different latency register value. Besides, a theoretical bandwidth curve is drew by choosing the point when bandwidth configuration register value is 128 as base point and expand proportionally with the Bandwidth Configuration Register Value. }
As shown in Figure~\ref{fig:mlc_bandwidth_latency}-(b), we plot five curves representing the relationship between the bandwidth configuration register value and the bandwidth attribute of the region. Each curve corresponds to a different latency register value. Additionally, a theoretical bandwidth curve is drawn by using the point where the bandwidth configuration register value is 32 as a base point, and expanding proportionally with the bandwidth configuration register value.
The results demonstrate that: \textbf{The HeteroBox emulation logic can accurately adjust the bandwidth attribute of a region}.

% \rebuttal{Then we discuss cases of multiple regions. As shown in Figure~\ref{fig:mlc_bandwidth_latency}-(c) and Figure~\ref{fig:mlc_bandwidth_latency}-(d), we configure the HeteroBox to have four regions, equally dividing the memory on FPGA to four parts. We modify kernel to form four NUMA nodes, corresponding to the four regions emulated by HeteroBox, and then use Intel MLC to evaluate the latency and bandwidth attribute of these four NUMA nodes. In Figure~\ref{fig:mlc_bandwidth_latency}-(c), we set the bandwidth configuration register value of all regions to be 256, and the latency register value of region 1, 2, 3 to be 32, 64, 96, correspondingly. We change the value of latency register of region 4 and plot the latency attribute of four regions. In Figure~\ref{fig:mlc_bandwidth_latency}-(d), we set the latency register value of all regions to be 128, and the bandwidth configuration register value of region 1, 2, 3 to be 512, 256, 128, correspondingly. We change the value of bandwidth configuration register of region 4 and plot the bandwidth attribute of four regions. The results demonstrate that: (1) \textbf{The HeteroBox emulation logic can accurately adjust the latency and bandwidth attribute of a region in the case of multiple regions}. (2) \textbf{Adjusting the latency or bandwidth attribute will not interference the latency or bandwidth attribute of other regions}.}

\input{fig_tex/remap_overhead.tex}
\input{fig_tex/main_result.tex}

Next, we discuss the case of multiple regions. As shown in Figure~\ref{fig:mlc_bandwidth_latency}-(c) and Figure~\ref{fig:mlc_bandwidth_latency}-(d), we configure the HeteroBox with four regions, dividing the FPGA memory into four equal parts. We modify the kernel to create four NUMA nodes corresponding to these regions and use Intel MLC to evaluate their latency and bandwidth attributes.
In Figure~\ref{fig:mlc_bandwidth_latency}-(c), the bandwidth configuration register value for all regions is set to 128, while the latency register values for regions 1, 2, and 3 are set to 32, 64, and 96, respectively. We vary the latency register value of region 4 and plot the latency attributes of all four regions.
In Figure~\ref{fig:mlc_bandwidth_latency}-(d), the latency register value for all regions is set to 128, while the bandwidth configuration register values for regions 1, 2, and 3 are set to 128, 80, and 32, respectively. We vary the bandwidth configuration register value of region 4 and plot the bandwidth attributes of all four regions.
The results show that: (1) \textbf{The HeteroBox emulation logic can accurately adjust the latency and bandwidth attributes in the case of multiple regions}. (2) \textbf{Adjusting the latency or bandwidth of one region does not affect the attributes of the other regions}.










\subsection{Remapping Overhead}
% \input{fig_tex/remapping_overhead.tex}

% \FloatBarrier
% \input{fig_tex/cache_hit_rate.tex}



% We first discuss the overhead introduced by HeteroMem. The Profile Unit of HeteroMem is off the critical path, so it does not introduce extra latency. The Migration Unit only works when a migration transaction happens. Since we limit the number of pages migrated in a given period (cycles consumed on migration are limited below about 5\%), the extra latency introduced by Migration Unit is small. However, the Remap Table Unit is always on the critical path and incurs non-neglectable extra latency and bandwidth when the remap cache miss. As a result, the overhead introduced by Remap Table Unit should be considered carefully. As shown in Figure~\ref{fig:remapping_overhead}, we test the overhead introduced by Remap Table Unit and the remap cache hit rate on the six benchmarks we choose. We run the six benchmarks on the HeteroBox emulation platform with Remap Table Unit enabled and disabled, and compare the performance of these two cases. Unfortunately, we observe non-neglectable overhead introduced by Remap Table. 
% There are three main reasons: (1) In real situations, different memory media in the heterogeneous memory system has different memory controllers and memory channels. However, in our HeteroBox emulation platform, different memory regions share the same memory controller and memory channels, which decreases the internal bandwidth of the device. The lower internal bandwidth result in the bandwidth consumed by remap cache miss incurring more significant overhead. (2) Due to the resource limitation of FPGA implementation, we set the size of remap cache to be 128KB. In ASIC design, the remap cache can be set larger, thus increasing the remap cache hit rate and alleviate the overhead introduced by Remap Table Unit. (3) There are some existing technique to optimize the performance of the Remap Table Unit~\cite{memstrata_osdi24,trimma}, such as using mixed page granularity~\cite{slic_fm_hpca17} and storing the remap metadata and the application data in the same cacheline~\cite{cameo_micro14}. These optimizations are orthogonal to our HeteroMem design, and we do not implement them due to implementation complexity.

% We claim that in real situation, the Remap Table Unit can be optimized to incur little overhead. We use Valgrind to get the memory trace to DRAM of GUPS, and simulate the remap cache hit rate with different remap cache size. As shown in Figure~\ref{fig:simulation_remap_hit_rate}, when the remap cache size is 128KB, we observe a cache hit rate of about 12.3\%, which aligns with the data in Figure~\ref{fig:remapping_overhead}. When we increase the remap cache size, the cache hit rate increases significantly and reaches nearly 100\% when remap cache size is larger than 1MB, which is still an acceptable resource usage for ASIC design.



Then we discuss the overhead introduced by HeteroMem. The Profiling Unit of HeteroMem is off the critical path, so it does not introduce extra latency. The Migration Unit only operates when a migration transaction occurs. Since we limit the number of pages migrated within a given period (cycles consumed on migration are limited to below 5\%), the extra latency introduced by Migration Unit is minimal. However, Remapping Unit is always on the critical path and incurs non-negligible extra latency and bandwidth overhead when the remapping cache misses. As a result, the overhead introduced by the Remapping Unit should be carefully considered.

% As shown in Figure~\ref{fig:remapping_overhead}, we test the overhead introduced by the Remap Unit on the six chosen benchmarks. 
% As shown in Figure~\ref{fig:remapping_overhead}, we disable migration in HeteroMem and test the performance slowdown when Remap Unit is enabled.
% We change the size of remap cache from 128KB to 2MB and test the performance when the Remap Unit is bypassed as ideal performance.
% For different remap cache size, we show the remap cache hit rate in Figure~\ref{fig:remap_cache_hit_rate}.
% For all benchmarks, the performance and remap cache hit rate increase as the size of remap cache enlarges.
% GUPS benchmark shows low remap cache hit rate when the size of remap cache is small, which result in that remap unit cause significant overhead. 
% That's because we configure GUPS to randomly access a range of memory, and the cache miss rate is significant when the cache can not cover the range of accessed memory.
% However, when the remap cache size increase and can cover the range of memory GUPS accesses, the remap cache hit rate of GUPS increase significantly and reaches nearly 100\% when the remap cache size reaches 2MB and the overhead introduced by Remap Unit is small.
% For BC, PR and XSBench benchmark, the remap cache hit rate reach nearly 100\% as well, thus introducing little overhead in the 2MB remap cache configuration.
% However, for Btree and Silo benchmarks, they show less locality, resulting in low remap cache hit rate even when the remap cache size is 2MB and shows 11.8\% and 11.0\% performance slowdown in the two benchmark correspondingly.
% As shown in Figure~\ref{fig:remapping_overhead}, we disable migration in HeteroMem and test the performance slowdown when the Remap Unit is enabled, with the remap cache size varying from 128KB to 2MB.
As shown in Figure~\ref{fig:merge_remapping_overhead}-(a), we disable migration in HeteroMem and test the performance slowdown with the Remapping Unit enabled, varying the remapping cache size from 128KB to 2MB.
Figure~\ref{fig:merge_remapping_overhead}-(b) displays the remapping cache hit rate for different cache sizes.
For all benchmarks, both the performance and the remapping cache hit rate improve as the remapping cache size increases. The GUPS benchmark shows a low remapping cache hit rate with smaller cache sizes, causing significant overhead due to its random memory access pattern. 
This results in a low cache hit rate when the cache cannot cover the accessed memory range. 
However, as the remapping cache size increases and covers the memory range accessed by GUPS, the hit rate improves significantly, reaching nearly 100\% at 2MB, reducing the Remapping Unit's overhead.
For BC, PR and XSBench benchmarks, the remapping cache hit rate also approaches 100\%, resulting in minimal overhead with a 2MB cache. 
Conversely, the Btree and Silo benchmarks exhibit lower locality, leading to low remapping cache hit rates even with a 2MB cache, and show performance slowdowns of 11.8\% and 11.0\%, respectively. 
% Subsequent experiment results indicate that the performance degradation in these two applications caused by the Remapping Unit can be compensated by HeteroMem's superior memory tiering solution.
Results from subsequent experiments indicate that the performance degradation in these two applications caused by Remapping Unit can be offset by HeteroMem's superior memory tiering solution.


% We run the benchmarks on the HeteroBox emulation platform with the Remap Table Unit enabled and disabled, and compare the performance in both cases. Unfortunately, we observe non-negligible overhead introduced by the Remap Table.
% There are three main reasons for this: 
% (1) In real situations, different memory media in a heterogeneous memory system have different memory controllers and memory channels. However, in our HeteroBox emulation platform, different memory regions share the same memory controller and memory channels, which decreases the internal bandwidth of the device. The lower internal bandwidth results in the bandwidth consumed by remap cache misses incurring more significant overhead.
% (2) Due to the resource limitations of FPGA implementation, we set the size of the remap cache to 128KB. In ASIC design, the remap cache can be larger, thus increasing the remap cache hit rate and alleviating the overhead introduced by the Remap Table Unit.
% (3) There are existing techniques to optimize the performance of the Remap Table Unit~\cite{memstrata_osdi24,trimma}, such as using mixed page granularity~\cite{slic_fm_hpca17} and storing the remap metadata and the application data in the same cache line~\cite{cameo_micro14}. These optimizations are orthogonal to our HeteroMem design, and we do not implement them due to implementation complexity.

% We assert that in real situations, the Remap Table Unit can be optimized to incur minimal overhead. We use Valgrind to obtain the memory trace to DRAM of GUPS and simulate the remap cache hit rate with different remap cache sizes. As shown in Figure~\ref{fig:simulation_remap_hit_rate}, when the remap cache size is 128KB, we observe a cache hit rate of about 12.3\%, which aligns with the data in Figure~\ref{fig:remapping_overhead}. When we increase the remap cache size, the cache hit rate increases significantly, reaching nearly 100\% when the remap cache size is larger than 1MB, which is still an acceptable resource usage for ASIC design.



\subsection{Main Results} 
\noindent{\textbf{Performance Comparison:}}
% To evaluate the ideal performance of HeteroMem, we enable the Remap Table Unit for all the baselines and compare their end-to-end performance with HeteroMem. As shown in Figure~\ref{fig:performance_comparison}-(d), HeteroMem shows superior performance than all other baselines. We normalized all the performance statistics to NeoMem. Geometrically, HeteroMem performs 11.8\% better than NeoMem, and performs 24.3\% better than PEBS (both in the 4GB fast memory configuration).
% The performance improvement of HeteroMem comes from the more accurate hardware-based profiling method and more efficient hardware-based data movement mechanism, as described in Section~\ref{sec:heterobox_hardware}. NeoMem utilizes hardware-based hotness profiling, yet it only profiles hot pages and still uses CPU to move data, thus making the performance of NeoMem lower than HeteroMem. PTE-Scan and PEBS both use inaccurate software hotness profiling and move data using CPU, thus showing lower performance than NeoMem and HeteroMem.
% To evaluate the ideal performance of HeteroMem, we enable the Remap Table Unit for all the baselines and compare their end-to-end performance with HeteroMem. 
% As shown in Figure~\ref{fig:performance_comparison}-(d), HeteroMem demonstrates superior performance compared to all other baselines. We normalize all the performance statistics to NeoMem. Geometrically, HeteroMem performs 5.7\% better than NeoMem and 17.6\% better than PEBS (both in the 4GB fast memory configuration).
% To evaluate the performance of HeteroMem, we configure the remap cache size to be 2MB and compare the end-to-end performance of HeteroMem with other three chosen baseline systems. 
% We run HeteroMem on our HeteroBox emulation platform. 
% We configure the memory space to be spilited into a fast memory region and a slow memory region. We change the size of fast memory region from 4GB to 1GB and do not add additional latency on it. 
% The slow memory region is configured to have an additional latency of 128 cycles in a 200M Hz clock domain, namely 640ns.
% Both fast memory region and slow memory region are not configured with any bandwidth restraintion.
To evaluate the performance of HeteroMem, we configure a 2MB remapping cache and compare HeteroMem's end-to-end performance with baseline systems. We run HeteroMem on HeteroBox emulation platform, dividing the memory space into fast and slow regions. The fast memory region size varies from 4GB to 1GB with no additional latency. The slow memory region has an additional latency of 128 cycles in a 200MHz clock domain. Neither memory region has any bandwidth restrictions.

\input{fig_tex/sensitivity_analysis.tex}
\input{fig_tex/memory_access_distribution}


% As shown in Figure~\ref{fig:performance_comparison}, across various fast memory sizes, HeteroMem consistently maintains its superior geomean performance compared to the baseline systems.
% We normalize all the performance statistics to NeoMem.
% Specifically, in the 4GB fast memory configuration, the geomean performance of HeteroMem is 5.7\% to 17.6\% higher than baseline systems. 
% For BC benchmark, the performance of HeteroMem achieves 32.7\% higher than NeoMem and PTE-Scan and 44.6\% higher than PEBS. However, for Btree and Silo benchmark, the performance of HeteroMem is lower than NeoMem due to the overhead introduced by Remap Unit, yet the performance of HeteroMem is still higher than PEBS and PTE-Scan.
% Meanwhile, the performance improvement of HeteroMem is slightly greater when the fast memory size is larger.  
% This is mainly because larger fast memory sizes amplify the effect of HeteroMem's accurate cold page profiling mechanism.
As shown in Figure~\ref{fig:performance_comparison}, HeteroMem consistently maintains superior geomean performance compared to baseline systems across various fast memory sizes. All performance statistics are normalized to NeoMem. In the 4GB fast memory configuration, HeteroMem's geomean performance is 5.1\% to 16.2\% higher than baseline systems. For BC benchmark, HeteroMem achieves 32.7\% higher performance than NeoMem and PTE-Scan, and 44.6\% higher than PEBS. 
% However, for the Btree and Silo benchmarks, HeteroMem's performance is lower than NeoMem due to the overhead introduced by the Remapping Unit, yet still higher than other four baseline systems. 
Additionally, the performance improvement of HeteroMem increases with larger fast memory sizes, primarily due to the enhanced effectiveness of HeteroMem's accurate cold page profiling mechanism.
% The performance improvement of HeteroMem stems from its more accurate hardware-based profiling method and more efficient hardware-based data movement mechanism, as described in Section~\ref{sec:heterobox_hardware}. NeoMem utilizes hardware-based hotness profiling but only profiles hot pages and still relies on the CPU to move data, resulting in lower performance compared to HeteroMem. Both PTE-Scan and PEBS use inaccurate software hotness profiling and move data using the CPU, thus showing lower performance than NeoMem and HeteroMem.
The performance improvement of HeteroMem stems from its accurate hardware-based profiling method and efficient hardware-based data movement mechanism, as described in Section~\ref{sec:heteromem}. 
% NeoMem uses hardware-based hotness profiling but only profiles hot pages and still relies on the CPU for data movement, resulting in lower performance compared to HeteroMem. Both PTE-Scan and PEBS employ inaccurate software hotness profiling and use the CPU for data movement, leading to lower performance than both NeoMem and HeteroMem.



% For the following all experiments, we use a configuration with 4GB fast memory and 2MB remap cache to understand the superior performance of HeteroMem.
For the following experiments, we utilize a configuration with 4GB fast memory and a 2MB remapping cache to delve into the superior performance of HeteroMem.


% \noindent{\textbf{Different Fast Memory Size:}}
% % To evaluate the impact of using different fast memory size, we change the size of fast memory from 1GB to 4GB, as shown in Figure~\ref{fig:performance_comparison}. We observe that, in various fast memory size, HeteroMem maintains its superior performance compared to baseline systems. 
% % Specifically, the performance improvement of HeteroMem is slightly larger when the fast memory size is larger. For example, for BC benchmark, the performance of HeteroMem is 37.4\% higher than NeoMem when the fast memory is 4GB, while the performance of HeteroMem is 27.8\% higher than NeoMem when the fast memory is 1GB. 
% % That's mainly because larger fast memory sizes make the effect of the accurate cold page profiling mechanism of HeteroMem more significant.
% To evaluate the impact of using different fast memory sizes, we vary the fast memory size from 1GB to 4GB, as shown in Figure~\ref{fig:performance_comparison}. We observe that, across various fast memory sizes, HeteroMem consistently maintains its superior performance compared to the baseline systems. Specifically, the performance improvement of HeteroMem is slightly greater when the fast memory size is larger. For example, for the BC benchmark, HeteroMem performs 37.4\% better than NeoMem when the fast memory is 4GB, while it performs 27.8\% better than NeoMem when the fast memory is 1GB. This is mainly because larger fast memory sizes amplify the effect of HeteroMem's accurate cold page profiling mechanism.

\noindent{\textbf{Different Slow Memory Latency:}}
% To explore how slow memory latency will affect the performance of HeteroMem, we change the additional latency in slow memory from 640ns to 1280ns. We test the performance of HeteroMem and all other three baselines on BC benchmark, in the configuration that fast memory size is 4GB. The result is shown in Figure~\ref{fig:different_latency}.
% HeteroMem keeps its superior performance compared to other memory tiering system. Meanwhile, the performance degradation of HeteroMem when the slow memory latency increase is non-significant. That's because the hot pages of BC benchmark can be all put in the fast memory in this configuration, and HeteroMem migrates them from slow memory to fast memory accurately and timely. 
% PTE-Scan also shows little degradation in performance when the latency of slow memory increase, which is because PTE-Scan migrate all the accessed pages to fast memory. NeoMem sets a threshold for hot pages, which limit its page migration, thus making its performance slightly lower than PTE-Scan. PEBS, however, performs worst in this case, which is because PEBS samples the hot pages, making it detect less hot pages and migrate pages less.
% To explore how slow memory latency affects the performance of HeteroMem, we vary the additional latency in slow memory from 128 cycles to 256 cycles (in 200M Hz clock domain, namely from 640ns to 1280ns). We test the performance of HeteroMem and the three other baselines on the BC benchmark with a configuration of 4GB fast memory and 2MB remap cache. The results are shown in Figure~\ref{fig:sensitivity_analysis}. HeteroMem maintains its superior performance compared to other memory tiering systems, and the performance degradation of HeteroMem with increasing slow memory latency is non-significant. This is because the hot pages of the BC benchmark can all be placed in the fast memory in this configuration, and HeteroMem migrates them from slow memory to fast memory accurately and timely.
% PTE-Scan also shows little performance degradation as the latency of slow memory increases, because PTE-Scan migrates all accessed pages to fast memory. NeoMem sets a threshold for hot pages, which limits its page migration, resulting in slightly lower performance than PTE-Scan. PEBS performs the worst in this case because it samples the hot pages, leading to fewer detected hot pages and less page migration.
To explore how slow memory latency affects the performance of HeteroMem, we vary the additional latency in slow memory from 128 cycles to 256 cycles (200MHz clock domain) and test the performance of BC benchmark. The results are shown in Figure~\ref{fig:sensitivity_analysis}-(a). HeteroMem maintains superior performance compared to other memory tiering systems, with non-significant performance degradation as slow memory latency increases. 
% This is because, in this configuration, the hot pages of the BC benchmark are placed in fast memory, and HeteroMem accurately and timely migrates them from slow to fast memory.
% This is because, in this configuration, the hot pages of the BC benchmark can be entirely placed in fast memory, and HeteroMem accurately and timely migrates them from slow to fast memory.
This is because HeteroMem accurately and timely migrates hot pages from slow to fast memory.
% PTE-Scan also shows little performance degradation as slow memory latency increases, as it migrates all accessed pages to fast memory. 
% NeoMem sets a threshold for hot pages, limiting its page migration, resulting in slightly lower performance than PTE-Scan. 
PEBS performs the worst because it samples hot pages, leading to fewer detected hot pages and less page migration.
TPP also shows low performance due to its hysteretic promotion mechanism.

\noindent{\textbf{Number of Migrated Pages:}}
% In order to understand the behaviour of these memory tiering system, we profile the number of migrated pages of HeteroMem and other three baseline systems. We run all the six benchmarks we choose under the configuration that fast memory size is 4GB. We found that, on BC benchmark, the hot pages can all reside in fast memory, so HeteroMem stop migrating pages when the hot pages has already been placed in fast memory due to its more accurate hotness and coldness profiling mechanism. NeoMem and PTE-Scan, however, exhibit some redundant page migration behaviour compared to HeteroMem, while PEBS fails to migrate all the hot pages to fast memory due to its slow convergence process. In the other benchmarks, when the hot pages can not be all placed in fast memory, HeteroMem migrates more pages than other baseline systems. Because we have already set the limitation of migrated memory size per second to 256MB/s, and the migration bandwidth of HeteroMem is much bigger than software-based data movement, the overhead of these additional pages migration can be neglected. And the larger number of page migration of HeteroMem provides more chances for applications to access data in fast memory, which results in higher end-to-end performance of HeteroMem compared to baseline memory tiering systems.
% To understand the behavior of these memory tiering systems, we profile the number of migrated pages of HeteroMem and the three baseline systems. 
% We run all six benchmarks under the configuration of 4GB fast memory size and 2MB remap cache size. 
% We found that, in the BC benchmark, the hot pages can all reside in fast memory, so HeteroMem stops migrating pages once the hot pages have already been placed in fast memory due to its more accurate hotness and coldness profiling mechanism. NeoMem and PTE-Scan, however, exhibit some redundant page migration behavior compared to HeteroMem, while PEBS fails to migrate all the hot pages to fast memory due to its slow convergence process.
% In the other benchmarks, when the hot pages cannot all be placed in fast memory, HeteroMem migrates more pages than the other baseline systems. Because we have set the limit of migrated memory size per second to 256MB/s, and the migration bandwidth of HeteroMem is much greater than software-based data movement, the overhead of these additional page migrations can be neglected. The larger number of page migrations by HeteroMem provides more opportunities for applications to access data in fast memory, resulting in higher end-to-end performance compared to the baseline memory tiering systems.
% To understand the behavior of these memory tiering systems, we profile the number of migrated pages for HeteroMem and the three baseline systems. We run all six benchmarks with a 4GB fast memory and a 2MB remap cache. 
% As shown in Figure~\ref{fig:sensitivity_analysis}-(b), we profile the number of migrated pages for HeteroMem and three baseline systems.
% For BC benchmark, all hot pages can reside in fast memory, so HeteroMem stops migrating pages once they are placed in fast memory, thanks to its accurate hotness and coldness profiling mechanism. In contrast, NeoMem and PTE-Scan exhibit redundant page migrations, while PEBS fails to migrate all hot pages due to its slow convergence process.
% In other benchmarks, where hot pages cannot all reside in fast memory, HeteroMem migrates more pages than baseline systems. With a migration limit of 256MB/s and HeteroMem's higher migration bandwidth compared to software-based data movement, the overhead of additional page migrations is negligible. 
% The increased number of page migrations by HeteroMem allows a higher proportion of memory accesses to be served by fast memory, resulting in higher end-to-end performance compared to the baseline systems.
% As shown in Figure~\ref{fig:sensitivity_analysis}-(b), we profile the number of migrated pages for HeteroMem and three baseline systems. For the BC benchmark, all hot pages can reside in fast memory. Consequently, HeteroMem stops migrating pages once they are placed in fast memory, thanks to its accurate hotness and coldness profiling mechanism. In contrast, NeoMem and PTE-Scan exhibit redundant page migrations, while PEBS fails to migrate all hot pages due to its slow convergence process.
% In other benchmarks, where hot pages cannot all reside in fast memory, HeteroMem migrates more pages than the baseline systems. With a migration limit of 256MB/s and HeteroMem's higher migration bandwidth compared to software-based data movement, the overhead of additional page migrations is negligible. The increased number of page migrations by HeteroMem allows a higher proportion of memory accesses to be served by fast memory, resulting in higher end-to-end performance compared to the baseline systems.
As shown in Figure~\ref{fig:sensitivity_analysis}-(b), we profile the number of migrated pages for HeteroMem and five baseline systems. For the BC benchmark, all hot pages can fit in fast memory, so HeteroMem stops migrating once they are placed there. NeoMem, PTE-Scan and Auto NUMA show redundant migrations, while PEBS and TPP fail to migrate all hot pages due to slow convergence.
In other benchmarks, where hot pages cannot all fit in fast memory, HeteroMem migrates more pages than the baseline systems. With a 256MB/s migration limit and higher bandwidth, HeteroMem's additional migrations have negligible overhead. This results in a higher proportion of fast memory accesses, leading to improved end-to-end performance of HeteroMem.




% \input{fig_tex/different_latency.tex}
% \FloatBarrier
% \input{fig_tex/migrated_pages_num.tex}



% \subsection{Analysis of HeteroMem}

\noindent{\textbf{Memory Access Distribution:}}
% As shown in Figure~\ref{fig:sensitivity_analysis}-(c), we profile the number of memory access towards fast memory and slow memory for all four memory tiering systems.
% We observe that for all benchmarks except GUPS, the sum of memory access number of HeteroMem is smaller than all the baseline memory tiering systems. 
% That's because baseline memory tiering systems use software-based data migration, which introduces additional CPU memory access. 
% For the GUPS benchmark, HeteroMem exhibits more memory access because the performance metric is the memory access number in a given period.
% Additionally, HeteroMem exhibits faster memory access on all benchmarks and less slow memory access on benchmarks except GUPS than the baseline memory tiering systems. 
% The higher portion of memory access to fast memory explains the superior end-to-end performance of HeteroMem. This indicates that HeteroMem accurately profiles the hotness and coldness of memory and places the hottest part of memory in fast memory.
% In Figure~\ref{fig:memory_access_distribution} we plot the memory access distribution of BC benchmark. We implement a 32-bit hardware counter for each 2MB physical page in the CXL extended memory and record memory access number to each 2MB page every second. The result shows that HeteroMem quickly migrate all the hot data to fast memory while baseline memory tiering systems left some hot memory in slow memory region, indicating the effectiveness of the memory management strategy of HeteroMem.
% As shown in Figure~\ref{fig:sensitivity_analysis}-(c), we profile the number of memory accesses to fast and slow memory for all four memory tiering systems. For all benchmarks except GUPS, HeteroMem shows fewer total memory accesses compared to the baseline systems due to the additional CPU memory access introduced by software-based data migration in the baseline systems. In the GUPS benchmark, HeteroMem exhibits more memory accesses because the performance metric is the memory access count within a given period.
% Additionally, HeteroMem demonstrates more fast memory accesses across all benchmarks and fewer slow memory accesses (except for GUPS) compared to the baseline systems. The higher proportion of fast memory accesses explains HeteroMem's superior end-to-end performance, indicating its effective hotness and coldness profiling and optimal placement of hot data in fast memory.
% In Figure~\ref{fig
% }, we plot the memory access distribution for the BC benchmark. We implemented a 32-bit hardware counter for each 2MB physical page in the CXL extended memory and recorded the memory access count to each 2MB page every second. The results show that HeteroMem quickly migrates all hot data to fast memory, whereas the baseline systems leave some hot data in slow memory, demonstrating the effectiveness of HeteroMem's memory management strategy.
As shown in Figure~\ref{fig:memory_access_distribution}-(a), we profile the number of memory accesses to fast and slow memory for all six memory tiering systems. 
We show the total number of memory accesses and the ratio of memory accesses to slow memory.
For all benchmarks except GUPS, HeteroMem shows fewer total memory accesses compared to the baseline systems due to the additional CPU memory accesses required by software-based data migration in the baseline systems. In the GUPS benchmark, HeteroMem exhibits more memory accesses because the performance metric of GUPS is based on the memory access count within a given period.
Additionally, HeteroMem demonstrates a lower ratio of slow memory accesses across all benchmarks (except for GUPS) compared to the baseline systems. 
The higher proportion of fast memory accesses explains HeteroMem's superior  performance, showing its effective profiling of hot and cold data and optimal placement of hot data in fast memory.

In Figure~\ref{fig:memory_access_distribution}-(b), we plot the memory access distribution for the BC benchmark. We implement a 32-bit hardware counter for each 2MB physical page in the CXL extended memory and record the memory access count to each 2MB page every second. The results show that HeteroMem quickly migrates all hot data to fast memory, whereas the baseline systems leave some hot data in slow memory, demonstrating the effectiveness of HeteroMem's memory management strategy.

