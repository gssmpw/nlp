% In this section, we detail the hardware architecture of each component of the HeteroBox emulation platform and the HeteroMem memory tiering system.


\subsection{Overview of HeteroMem System}
\input{fig_tex/heterobox_system_overview}

% The HeteroMem memory tiering system act as an intermediate layer between host CPU and device-side memory controller. The system is consisted of three main components: \textbf{Remap Table}, \textbf{Profile Unit} and \textbf{Migrate Unit}. Besides, we implement software interface support for the enumeration platform, including software driver in the kernel and BAR regs in the device for configuration and debugging. Without losing generality, in the following evaluation of HeteroMem memory tiering system, we use two tiers memory system, which consists of a fast memory tier and a slow memory tier.

% The HeteroMem memory tiering system acts as an intermediate layer between the host CPU and the device-side memory controller. 
The HeteroMem memory tiering system acts as an intermediate layer between host CPU and CXL-extended memory. 
As shown in Figure~\ref{fig:heterobox_system_overview}, the HeteroMem system comprises three main components: the \textbf{Remapping Unit}, the \textbf{Profiling Unit}, and the \textbf{Migration Unit}. 
% Additionally, we provide software interface support for the emulation platform, including a software driver in the kernel and BAR registers in the device for configuration and debugging. 
Additionally, we provide software interface support for HeteroMem memory tiering system, including a software driver in the kernel and BAR registers in the device for configuration and profiling. 
For the following evaluation of HeteroMem memory tiering system, we use a two-tier memory system, consisting of a fast memory tier and a slow memory tier, without losing generality.


\input{fig_tex/remap_table_working_process.tex}
\input{fig_tex/remap_table.tex}


\noindent{\textbf{Remapping Unit:}} 
% HeteroMem proposes to manage the heterogeneous memory system totally at device-side. This includes migrating the hot data from slow memory media to fast memory media, and migrating the cold data from fast memory media to slow memory media. As a result, the device needs to record the actual address of data when receiving a memory request issued by host. We build a remap table module to do this, which receives memory requests from host and translate the memory request to its actual address in the device, and then forward the translated request to the succeeding modules. The remap table should be carefully designed, because: 1) It is on the critical path of a memory request, which shouldn't introduce too much latency, otherwise it will affect the overall performance the memory system. 2) The translation granularity of remap table further determines the migration granularity, in which smaller granularity means larger overhead of remap table yet less migration amplification, larger granularity introduce less overhead in remap table yet cause more serious migration amplification. We will discuss these trade-offs of remap table in section~\ref{sec:heterobox_hardware}.
% HeteroMem proposes to manage the heterogeneous memory system entirely on the device side. This includes migrating hot data from slow memory media to fast memory media and migrating cold data from fast memory media to slow memory media. Consequently, the device must record the actual address of data when receiving a memory request from the host. We implement a Remap Unit module to handle this, which receives memory requests from the host, translates the memory request to its actual address in the device, and then forwards the translated request to the subsequent modules. The Remap Unit must be carefully designed since it is on the critical path of a memory request, and should not introduce significant latency, as this would affect the overall performance of the memory system. 
% 2) The translation granularity of the remap table determines the migration granularity. Smaller granularity results in larger overhead for the remap unit but less migration amplification, whereas larger granularity introduces less overhead in the remap table but causes more significant migration amplification. 
% We will discuss the design of Remap Unit in section~\ref{sec:heteromem}.
% \xp{also discuss remapping overhead in section~\ref{sec:evaluation}}
% HeteroMem manages the heterogeneous memory system entirely on the device side, including migrating hot data to fast memory and cold data to slow memory. 
% To keep the device side data migration transparent to CPU, the device needs a translation layer which can map the request address from host to its actual address in the device. 
% We implement a Remap Unit to handle this, translating memory requests to their actual addresses on the device and forwarding them to subsequent modules. Since the Remap Unit is on the critical path of a memory request, it must be carefully designed to avoid introducing significant latency, which would impact overall system performance.
HeteroMem manages heterogeneous memory system entirely on the device side, migrating hot data to fast memory and cold data to slow memory. To keep this migration transparent to CPU, the device needs a translation layer to map hPA to dPA. We implement a Remapping Unit to handle this, translating memory requests and forwarding them to subsequent modules. Since the Remapping Unit is on the critical path of memory requests, it must minimize added latency to ensure optimal system performance.


\noindent{\textbf{Profiling Unit:}} 
% To optimize the performance the heterogeneous memory system, we need to migrate the hot data to fast memory media. We build Profile Unit to measure the hotness of data at different address in the device. The Profile Unit take memory requests which are already translated by remap table as input and record this access. When the Profile Unit recognize data at a address as hot, it will issue a signal to remap table to indicate that a migration event should be executed. The signal contains a pair of address, which locate in fast memory region and slow memory region respectively. The Profile Unit should classify hot page and cold page accurately to do the right migration decision. For hot pages, we implement a sketch to evaluate the access time of each page and choose those with a access time larger than a threshold to be hot.
% Cold pages, however, are harder to detect. Building a LRU-like structure in a GB-level address space is impracticable because of it will consume large amount of hardware resource. Fortunately, cold page tends to remain cold in relatively longer time~\cite{tmts_asplos2023}. Based on this insight, we use a periodically scanning method to balance the hardware overhead and profiling accuracy.
% %For cold pages, we scan the address space periodically and choose those with smallest access time as cold pages.
% Besides, we design a dynamic hotness threshold setting mechanism to ensure that the hottest part of data remain in fast memory. We will discuss the design of Profile Unit in detail in section~\ref{sec:heterobox_hardware}.
% To optimize the performance of the heterogeneous memory system, we need to migrate hot data to fast memory media. We develop the Profile Unit to measure the hotness of data at different addresses in the CXL extended memory space. 
% The Profile Unit takes memory requests that have already been translated by the Remap Unit as input and records this access. When the Profile Unit identifies data at an address as hot, it issues a signal to the Remap Unit to indicate that a migration event should be executed. This signal contains a pair of addresses, located in the fast memory region and slow memory region, respectively. The Profile Unit must accurately classify hot pages and cold pages to make correct migration decisions. For hot pages, we implement a sketch to evaluate the access times of each page and select those with access times larger than a threshold to be considered hot.
% Detecting cold pages, however, is more challenging. Constructing an LRU-like structure in a GB-level address space is impractical due to the large amount of hardware resources it would consume. Fortunately, cold pages tend to remain cold for relatively longer periods~\cite{tmts_asplos2023}. Based on this insight, we use a periodic scanning method to balance hardware overhead and profiling accuracy. Additionally, we design a dynamic hotness threshold setting mechanism to ensure that the hottest portion of data remains in fast memory. 
% We will discuss the design of the Profile Unit in detail in section~\ref{sec:heteromem}.
% The Profile Unit is used for hotness and coldness profiling of the CXL-extended memory. The Profile Unit uses translated memory requests from the Remap Unit as input and records access patterns. The Profile Unit proactively profiles the data hotness in slow memory and data coldness in fast memory. Detected cold data will be temporarily buffered. When it identifies hot data, it signals the Remap Unit to initiate a migration event, providing a pair of hot data address and cold data address for the succeeding migration transaction.
% The Profile Unit profiles the hotness and coldness of data in CXL-extended memory using translated memory requests from the Remap Unit. It records access patterns and proactively profiles data hotness in slow memory and data coldness in fast memory, temporarily buffering detected cold data. When hot data is identified, the Profile Unit signals the Remap Unit to initiate a migration event, providing addresses for the hot data and previously buffered cold data for the migration transaction.
The Profiling Unit assesses the hotness and coldness of data in CXL-extended memory using translated memory requests from the Remapping Unit. It records access patterns, profiling hot data in slow memory and cold data in fast memory, temporarily buffering the dPA of detected cold data. When hot data is identified, the Profiling Unit signals Remapping Unit to initiate a migration event, providing the dPA for hot data and the dPA of previously buffered cold data for subsequent migration transaction.


\noindent{\textbf{Migration Unit:}} 
% The Migrate Unit is responsible for data migration. It takes in two address, corresponding to two pages, and it swap the content of these two pages. The key point in designing a Migrate Unit is that we need to keep all migration event transparent to CPU. To achieve that, we need to update the Remap Table when migration happens. Also, we need to ensure that at the when the Migrate Unit has already finished migrating data and the Remap Table has not updated its translation rule, no memory request can go through the Remap Table, being translated and sent to the DRAM module, otherwise it will cause broken data. We will discuss our design of Migrate Unit and how it co-operate with Remap Table in detail in section~\ref{sec:heterobox_hardware}.
% The Migrate Unit is responsible for data migration. 
% It takes in migration requests issued from Remap Unit, which contain two memory addresses corresponding to hot and cold data detected by Profile Unit, and swaps their memory location. 
% The key point in designing the Migrate Unit is to ensure that all migration events remain transparent to the CPU. To achieve this, we need to update the Remap Unit when migration occurs. Additionally, we must ensure that during the period when the Migrate Unit has finished migrating data but the Remap Unit has not yet updated its translation rules, no memory requests can pass through the Remap Unit, be translated, and be sent to the DRAM module; otherwise, this would result in data corruption. 
% We will discuss our design of the Migrate Unit and how it cooperates with the Remap Unit in detail in section~\ref{sec:heteromem}.
The Migration Unit initiates data migration upon receiving requests from the Remapping Unit, which contain dPAs for hot and cold data identified by the Profiling Unit, and swaps their memory locations. To keep migrations transparent to the CPU, the Remapping Unit must be updated whenever a migration occurs. 
% During this transaction, it is crucial to prevent memory requests from being translated by the Remapping Unit until the translation rules are updated to avoid data corruption.


% \noindent{\textbf{Latency Module:}} We build our enumeration platform on a FPGA board with homogeneous DRAM memory, yet we need to enumerate heterogeneous memory system with different configuration, such as different capacity ratio and different latency. To achieve that, we build a Latency module as an abstract layer between memory controller and other logic. It can be configured to devide the DRAM into two regions, which make an illusion that the two regions has different latency attribute, thus forming a abstraction of heterogeneous memory system with two-tiered memory. We will discuss the design of Latency Module in detail in section ?. 





\subsection{Remapping Unit}


% The Remap Table provides an additional translation layer between CPU and device. It needs to carefully designed to introduce low overhead and keep the translation process transparent to CPU. 
The Remapping Unit provides an additional translation layer between the CPU and the device. It needs to be carefully designed to introduce minimal overhead and keep the translation process transparent to the CPU.

\noindent{\textbf{Memory Layout:}} 
% As shown in Figure \ref{fig:remap_table_working_process}, the whole memory space is consisted of 1GB of fast memory and 15GB of slow memory. We modify the Linux kernel to software reserve the first 128MB of the fast memory. When the machine power on, the remap table will send a series of write request to the reserved fast memory to initialize the remap table meta data. The remap table meta data consists of a remap table array and a reversed remap table array. Access the remap table array with the index of a page index will get the translated page index corresponding to the original page index. The reversed remap table is responsible for the reversed translation.
% As shown in Figure \ref{fig:remap_table_working_process}-(a), the entire memory space consists of a fast memory region and a slow memory region. 
% The Remap Unit stores the metadata used for address translation at the beginning of fast memory.
% The memory used to store metadata is software reserved to avoid being accessed by host CPU.
% When the machine powers on, the Remap Unit sends a series of write requests to the reserved fast memory to initialize the metadata.
% This metadata consists of a remap table array and a reverse remap table array. 
% Both remap table array and reverse remap table array consist of 4 bytes entries, each entry contains a page index. Using 4 bytes page index can describe a memory space up to 16TB, and we can use longer page index (for example, 8 bytes) for larger memory space.
% Accessing the remap table array with a page index yields the translated page index corresponding to the original page index, while the reverse remap table contains the inverse mapping of the remap table. 
As shown in Figure \ref{fig:remap_table_working_process}-(a), the entire memory space consists of a fast memory region and a slow memory region. 
The Remapping Unit stores the metadata for address translation at the beginning of the fast memory, which is software-reserved to avoid access by the host CPU. 
When the machine powers on, the Remapping Unit initializes this metadata by sending a series of write requests to the reserved fast memory.
This metadata includes a remapping table array and a reverse remapping table array, both consisting of 4-byte entries, each containing a page index (for dPA). 
% HeteroMem manages the memory in 4KB granularity, so a 4-byte page index can describe a memory space of up to 16TB, and a longer index (e.g., 8 bytes) can be used for larger memory space. 
HeteroMem manages memory in 4KB page granularity, so a 4-byte page index can describe a memory space of up to 16TB. For larger memory spaces, a longer index (e.g., 8 bytes) can be used.
Accessing the remapping table array with a page index yields the translated page index, while the reverse remapping table contains the inverse mapping of the remapping table.


\noindent{\textbf{Architecture of Remapping Unit:}} 
% The architecture of the Remap Table is shown in Figure \ref{fig:remap_table}. When a memory request is issued to the Remap Table, it first come into a remap cache which buffers the remap table meta data stored in the memory. If the cache contains the meta data which is needed to translate the current request, the remap table translate the current request with the meta data and send it to the memory. If the cache miss, the current request will be buffered in a fifo to pipelining the translation process of the succeeding requests, and the remap cache will issue a read request to the remap table data in the memory. When the response of the read request returned to the remap table, the remap cache will be updated, and the request buffered in the fifo will be translated and sent to the memory. 
% The architecture of the Remap Unit is shown in Figure \ref{fig:remap_table}. When a memory request is issued to the Remap Unit, it first enters a remap cache, which buffers the remap table metadata stored in the memory. If the cache contains the metadata needed to translate the current request, the Remap Unit translates the request using the metadata and sends it to the memory. If there is a cache miss, the current request is buffered in a FIFO to pipeline the translation process for subsequent requests, and the remap cache issues a read request to fetch the remap table data from the memory. Once the read request response returns to the Remap Unit, the remap cache is updated, and the request buffered in the FIFO is translated and sent to the memory.
% The architecture of the Remapping Unit is shown in Figure \ref{fig:remap_table}. When a memory request reaches the Remapping Unit, it first enters a remapping cache that buffers the remapping table metadata stored in memory. If the cache contains the necessary metadata to translate the request, the Remapping Unit performs the translation and forwards the request to memory. In the case of a remapping cache miss, the current request is buffered in a FIFO to pipeline the translation process for subsequent requests, and the remapping cache issues a read request to fetch the remapping table data from memory. Once the read response returns, the remapping cache is updated, and the buffered request in the FIFO is translated and sent to memory.
% Figure~\ref{fig:remap_table_working_process}-(b) shows the translation process of Remap Unit when a remap cache miss happens.
The Remapping Unit's architecture, shown in Figure \ref{fig:remap_table}, works as follows: when a memory request arrives, it first checks the remapping cache, which stores metadata for translating requests. If the required metadata is found, the request is translated and sent to memory. On a cache miss, the request is buffered in a FIFO, and the remapping cache issues a memory read to retrieve the needed remapping table data. Once the data returns, the cache is updated, and the buffered request is translated and forwarded to memory.
Figure~\ref{fig:remap_table_working_process}-(b) illustrates the translation process of the Remapping Unit when a remapping cache miss occurs.



\input{fig_tex/profile_unit.tex}

\noindent{\textbf{Migration Transaction:}} 
% As shown in Figure \ref{fig:remap_table_working_process}.b, when the remap table receive a migration request from the profile unit(\bone), it will start a migration transaction. The migration request contains a pair of page index which locate in fast memory and slow memory respectively. The page index in the migration request is page index which is already translated. Upon receive the migration request, the remap table will block the succeeding request from the host to ensure the atomic attribute of the migration. Then, remap table will issue a memory read request to the reverse remap table in the memory to get the original request address(\btwo). When the read response containing the original request address returns(\bthree), the remap table will update the remap table data and reverse remap table data according to the two original address and two translated address(\bfour). The Remap Table will concurrently send an enable signal to Migrate Unit after it has sent the memory read request to overlap the memory read latency. Then Migrate Unit will read out the data of two pages in the two page index given by the migration request(\bsix \bseven), swap their index and then write them back(\beight \bnine). After the migrate module has finished the migration process and the remap table has updated its remap table data and reverse remap table data, the remap table will allow succeeding memory request to go into the remap table.
As shown in Figure~\ref{fig:remap_table_working_process}-(c), when the Remapping Unit receives a migration request from the Profiling Unit (\bone), it initiates a migration transaction. The migration request contains a pair of page indexes located in fast memory and slow memory, respectively. The page indexes in the migration request have already been translated. 
Upon receiving the migration request, the Remapping Unit blocks subsequent requests from the host to ensure the atomicity of the migration. 
Then, the Remapping Unit issues two memory read requests to the reverse remapping table in the memory to retrieve the hPAs of the hot and cold page (\btwo). 
When the read responses containing the hPAs return (\bthree), the Remapping Unit updates the remapping table data and the reverse remapping table data according to the two hPAs and two dPAs (\bfour). 
The Remapping Unit concurrently sends an enable signal to the Migration Unit after issuing the memory read request to overlap the memory read latency. 
The Migration Unit then reads the data of the two pages at the two page indexes specified by the migration request (\bsix \bseven), swaps their indexes, and writes them back (\beight \bnine). 
After the Migration Unit has completed the migration process and the Remapping Unit has updated its remapping and reverse remapping table data, subsequent memory requests are allowed to proceed.


\subsection{Profiling Unit}



% The profile unit is responsible for profiling hotness of data in memory. In our HeteroMem design, we profile hotness in 4kB page granularity. The Profile Unit is linked after the remap table, off the critical path, to get the remapped request address trace. We only profile read requests since write requests are not on the critical path of the execution of programs. As shown in Figure~\ref{fig:profile_unit_overview}-(a), in the case of a CXL extended memory system with two tiers, the Profile Unit splits the read request into two streams, one target fast memory and the other target slow memory. For the read request stream target fast memory, the Profile Unit measure the hotness of each page through its access frequency and report it as hot page if its hotness is greater than a given threshold. For the read request stream target slow memory, the Profile Unit detect cold pages through its access history. The profile Unit will send pairs of hot pages and cold pages to Remap Table, whose number is limited within a given constraint, and the logic of swapping pages between fast memory and slow memory in Remap Table will do the work of data movement. If there are more than two tiers, we just need to simultaneously profile coldness and hotness of pages in each tier.

The Profiling Unit is responsible for profiling the hotness and coldness of data in memory. In our HeteroMem design, we profile data hotness and coldness at a 4kB page granularity. 
The Profiling Unit is linked after the Remapping Unit, off the critical path, to capture the dPA trace. 
% We only profile read requests, as write requests are not on the critical path of program execution. 
We profile only read requests, as write requests are generally not on the critical path of program execution.
As shown in Figure~\ref{fig:profiling_unit_overview}-(a), in the case of a CXL-extended memory system with two tiers, the Profiling Unit splits the read requests into two streams: one targeting fast memory and the other targeting slow memory. 
% For the read request stream targeting fast memory, the Profile Unit detects cold pages based on their access history.
% The detected cold pages are temporarily buffered in a cold pages buffer. Because cold pages tend to remain cold for a relatively long time, the pages in the cold pages buffer remain cold in the subsequent process.
% For the read request stream targeting slow memory, the Profile Unit measures the hotness of each page based on its access frequency and reports it as a hot page if its hotness exceeds a given threshold.
% When the Profile Unit detects a hot page, it fetches a cold page in the cold page buffer and sends the pair detected hot page and fetched cold page to the Remap Unit. 
% Then, the logic in the Remap Unit for swapping pages between fast memory and slow memory handles the data movement.
% The number of pairs of hot pages and cold pages sent to the Remap Unit is limited to a given constraint (for example, 32 pairs of pages per 100000 cycles).
For the read request stream targeting fast memory, the Profiling Unit detects cold pages based on their access history. These detected cold pages are temporarily buffered in a cold pages buffer. Since cold pages tend to remain cold for a relatively long time~\cite{tmts_asplos2023}, they typically remain cold when fetched from the buffer in subsequent processes.
For the read request stream targeting slow memory, the Profiling Unit measures the hotness of each page based on its access frequency, reporting a page as hot if its hotness exceeds a given threshold. Upon detecting a hot page, the Profiling Unit fetches a cold page from the cold pages buffer and sends the detected hot page and the fetched cold page to the Remapping Unit.
The Remapping Unit then manages the data movement, swapping pages between fast memory and slow memory. The number of these hot and cold page pairs sent to the Remapping Unit is limited by a given constraint (e.g., 32 pairs of pages per 100,000 cycles).
% Then, the logic for swapping pages between fast memory and slow memory in the Remap Unit handles the data movement. 

% If there are more than two tiers, we simultaneously profile the coldness and hotness of pages in each tier.

\noindent{\textbf{Hotness Profiling:}} 
% For memory read request target fast memory, we use a Count-Min Sketch structure to profile the hotness of each page. Count-Min Sketch is an hash-based algorithm which can be used to estimate heavy hitter within a stream. As shown in Figure~\ref{fig:profile_unit_overview}-(b), the main body of the Count-Min Sketch is a $\textbf{D}*\textbf{W}$ counter array, in which $\textbf{D}$ is the depth of the array and $\textbf{W}$ is the width of the array. The $\textbf{W}$ entries in the same line is called a lane. When a request comes in, its address will be mapped to $\textbf{D}$ different lanes with $\textbf{D}$ different hash functions. The counters being mapped to will be added by one, and when the counters overflow, we make them stay at the max value. Then the minimal value of the D mapped counters will be sent to sketch controllers, and if the minimal value is greater than a given threshold, the accessed page will be detected as hot page. We add a hot bit for every counter and set it when the corresponding page is detected as hot page. We will not report a page as hot page if all of its D hot bits is set, thus avoiding repeated reporting the same page as hot page. All the counters will be reset in a given period. Considering that counters in Count-Min Sketch give approximate value of access time, and the error bound of the Count-Min Sketch is decided by the length of the stream~\cite{count_min_sketch}, we control the reset period to limit the error bound of the sketch.
For memory read requests targeting fast memory, we use a Count-Min Sketch~\cite{count_min_sketch} structure to profile the hotness of each page. Count-Min Sketch is a hash-based algorithm that can estimate heavy hitters within a stream. As shown in Figure~\ref{fig:profiling_unit_overview}-(b), the main body of the Count-Min Sketch is a $\textbf{D}*\textbf{W}$ counter array, where $\textbf{D}$ is the depth of the array and $\textbf{W}$ is the width. The $\textbf{W}$ entries in the same row are referred to as a lane. When a request comes in, its dPA is mapped to $\textbf{D}$ different lanes using $\textbf{D}$ different hash functions. The mapped counters are incremented by one, and when the counters overflow, they remain at their maximum value. 
% The minimal value of the $\textbf{D}$ mapped counters is then sent to the succeeding hot pages detect logic. If this minimal value exceeds a given threshold, the accessed page is marked as a hot page.
The minimum value of the $\textbf{D}$ mapped counters is then sent to the subsequent hot page detection logic. If this minimum value exceeds a given threshold, the accessed page is marked as hot.
Each counter has an associated hot bit that is set when the corresponding page is detected as hot. A page will not be reported as hot if all of its $\textbf{D}$ hot bits are already set, thus avoiding repeated reporting of the same hot page. All counters are reset periodically. Since the counters in the Count-Min Sketch provide an approximate value of access frequency and the error bound of the Count-Min Sketch is determined by the length of the stream, we control the reset period to limit the sketch's error bound.


\noindent{\textbf{Coldness Profiling:}} 
% The Migrate Module use a swap mechanism to move hot data to fast memory, so when we detect a hot page in slow memory, we need to find a cold page in the fast memory to swap its location with the detected hot page. We use a ping-pong bitmap to record the access history of each page in the fast memory. The ping-pong bitmap structure consists of two bitmaps array, in which each entry is a bit corresponding to a page in the fast memory. In a given period, one bitmap, say bitmap $A$, is used to record memory access in the current period. When Profile Unit receive a memory request, it will set the bit corresponding to the accessed page in the bitmap $A$. Meanwhile, the Profile Unit will scan the other bitmap, say bitmap $B$, to get these pages with their corresponding bit in bitmap $B$ unset as cold pages. In the next period, we reset all the bits in bitmap $B$ and change the function of bitmap $A$ and bitmap $B$, which means bitmap $B$ is responsible for recording incoming memory read request, while bitmap $A$ provides cold pages to swap with hot pages. In this way, we can make sure the detected cold pages are not accessed during the last period, thus detecting cold pages accurately.
As detailed in Section~\ref{sec:heteromem}.D, the Migration Unit uses a swap mechanism to move hot data to fast memory. 
Therefore, when a hot page is detected in slow memory, we need to find a cold page in fast memory to swap its location with the detected hot page. We use a ping-pong bitmap to record the access history of each page in fast memory. The ping-pong bitmap structure consists of two bitmap arrays, each entry being a bit corresponding to a page in fast memory.
In a given period, one bitmap, say bitmap $A$, is used to record memory access in the current period. When Profiling Unit receives a memory request, it sets the bit corresponding to the accessed page in bitmap $A$. Meanwhile, Profiling Unit scans the other bitmap, say bitmap $B$, to identify pages with their corresponding bit in bitmap $B$ unset as cold pages. In the next period, we reset all bits in bitmap $B$ and switch the functions of bitmap $A$ and bitmap $B$. 
% This means bitmap $B$ now records incoming memory read requests, while bitmap $A$ provides cold pages to swap with hot pages.
This approach ensures the detected cold pages were not accessed during the last period, thus accurately identifying cold pages.

%\noindent{\textbf{Buffer and Pipeline Design:}} We implement the Profile Unit in a 200M Hz clock domain. To satisfy the timing requirement and deal with burst situation, we design a series of pipeline and buffer structure in the Profile Unit.

\subsection{Migration Unit}
% The migrate unit moves data between fast memory and slow memory. In the device side, we have no knowledge about whether a address has valid data or not, so every migration operation is achieved as a swap operation, which swap the data in an address of fast memory and an address of slow memory. The migrate module takes in a pair of address and an enable signal. When the enable signal is set high, the migrate unit stage the two input address and read data in the two address to buffer, and then write them to swapped address to complete the process. When the migrate unit is working, it blocks all the coming requests until it finishes migrating process. Blocking request while migrating will not significantly affect performance, because: 1) When migrating, the bandwidth of the memory controller is fully occupied by the migrate unit. 2) We control the frequency of migration at a reasonable level, as described in Section~\ref{sec:evaluation}.
The Migration Unit moves data between fast memory and slow memory. 
On the device side, we lack knowledge about whether an address contains valid data, so every migration operation is executed as a swap operation, swapping the data between fast memory and slow memory. 
The Migration Unit takes in a pair of dPAs and an enable signal. 
When the enable signal is set high, the Migration Unit stages the two input dPAs, reads the data from these dPAs into a buffer, and then writes the data to the swapped dPAs to complete the process. 
% While the Migration Unit is operating, it blocks all incoming requests until the migration process is finished.
% The Migration Unit issues read requests to memory controller in a non-blocking manner and issues write requests as soon as a read response returns, meanwhile blocking incoming memory requests from host during a migration, minimizing the bubbles and maximizing the migration bandwidth. 
The Migration Unit issues read requests to the memory controller in a non-blocking manner and initiates write requests as soon as a read response returns, while simultaneously blocking incoming memory requests from the host during migration, thereby minimizing idle cycles and maximizing migration bandwidth.
% Blocking requests during migration can achieve similar performance compared to non-blocking design, because: 
% 1) During migration, the bandwidth of the memory controller is fully occupied by the Migrate Unit. 2) We control the frequency of migrations to a reasonable level, as described in Section~\ref{sec:evaluation}.

% Blocking requests during migration can achieve similar performance to a non-blocking design because:
% 1) During migration, the memory controller's bandwidth is fully occupied by the Migration Unit.
% 2) We regulate the frequency of migrations to a reasonable level, as described in Section~\ref{sec:evaluation}.


\subsection{Software Interface}

% Our HeteroBox emulation platform can be runtime configured to emulate different configurations of a CXL extended memory tiering system. We have implemented corresponding configuration registers in our design. 
HeteroMem can manage the memory tiering system transparently without CPU intervention. To better understand the behavior of HeteroMem, we have implemented a series of profiling registers, each indicating the number of specific events. Additionally, HeteroMem's behavior can be tuned through several parameters, which we have also implemented as configuration registers.
We expose these configuration and profiling registers in the PCIe BAR space of the CXL memory device. The host CPU can read or write these registers through MMIO. In the OS kernel space, we have implemented a device driver that exposes the MMIO address of these registers to user space as files under the $/sys/kernel/mm/heteromem$ directory. Users can read or write these files to configure HeteroMem and obtain profiling information. For example, users can read $/sys/kernel/mm/heteromem/migrated\_pages\_cnt$ to get the number of pages migrated by HeteroMem. 
% We list all the files in Table~\ref{tab:interface_table}.

% \input{table_tex/interface_table}

\label{sec:heterobox_software_interface}
\input{tex/heterobox_software_interface.tex}