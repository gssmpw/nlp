\noindent{\textbf{Support for CXL memory pooling and sharing:}}
% Memory pooling and sharing are promising using cases of CXL, which allow memory to be shared between processors~\cite{cxl_pooling_sharing}. 
% The memory in a memory pool is managed by a specific memory controller, which can allocate and de-allocate memory dynamically, allowing for more efficient use of memory resources. 
% Memory sharing, at the other hand, allows each processor to read from or write to the shared memory, which enables easy data sharing and communication between processors.
% Our HeteroMem design can be directly implemented both in the case of memory pooling and sharing as an abstract layer which is transparent to CPU. 
% HeteroMem migrate data in an automatic manner, and build an abstraction that the data is not moved. As a result, both CXL memory pooling and CXL memory sharing can utilize HeteroMem to optimize the system performance.
% Especially for memory sharing, in which naively migrate the data to local host memory is prohibitive and migrate data in the device will cause cache eviction, HeteroMem can optimize the performance of the CXL extended memory system without the awareness of CPU cache.
Memory pooling and sharing are promising use cases of CXL~\cite{ha2023dynamic}, allowing memory to be shared between processors~\cite{cxl_pooling_sharing}. 
% Memory in a memory pool is managed by a specific memory controller, which can dynamically allocate and deallocate memory, leading to more efficient use of memory resources. 
Memory in a memory pool can be dynamically allocated and deallocated, leading to more efficient use of memory resources. 
Memory sharing, on the other hand, enables each processor to read from or write to the shared memory, facilitating easy data sharing and communication between processors.
% Our HeteroMem design can be directly implemented in both memory pooling and sharing scenarios as an abstraction layer that is transparent to the CPU. 
% HeteroMem migrates data automatically and creates the abstraction that the data is not moved to CPU.
The data migration process of HeteroMem is completely transparent to CPU.
% As a result, both CXL memory pooling and CXL memory sharing can utilize HeteroMem to optimize system performance.
% As a result, HeteroMem is a plug-in solution for both memory pooling and sharing scenarios.
As a result, HeteroMem serves as a plug-in solution for both memory pooling and sharing scenarios.
% Specifically, for memory sharing, where naively migrating data to local host memory is prohibitive and migrating data within the device can cause cache eviction, HeteroMem can optimize the performance of the CXL extended memory system without affecting CPU cache awareness.

%Pooled memory, on the other hand, is a type of memory that is shared between processors but managed by a specific memory controller. The memory controller can allocate and de-allocate memory dynamically, allowing for more efficient use of memory resources.

% \noindent{\textbf{Support for CXL memory sharing:}}
% \input{table_tex/migration_overhead.tex}
% \input{table_tex/multiple_applications.tex}

\noindent{\textbf{Hardware overhead and Scalability:}}  
% The hardware overhead remains stable when the CXL extended memory system scales. Firstly, the hardware overhead of HeteroMem keep stable when the number of CXL devices increases.
% HeteroMem can be built in CXL switch, whose number is far smaller than CXL devices in a CXL extended memory system. Moreover, not every CXL switch is needed to equipped with HeteroMem hardware. HeteroMem hardware can manage the whole memory space it can reach, so we just need to implement HeteroMem in the CXL switch which is connected to the host CPU. 
% Secondly, the hardware overhead of HeteroMem keep stable when the memory space scales.
% The hot pages profiling part of HeteroMem utilize Count-Min Sketch to detect hot pages, whose error bound is relative the memory access sequence length, instead of the size of memory space. 
% The cold pages profiling part of HeteroMem, namely the ping-pong bitmap, uses two bit SRAM for every 4KB page and we only need to allocate the cold pages profiling structure for fast memory media with demotion target, whose capacity is often small. 
% The hardware overhead remains stable as the CXL extended memory system scales. Firstly, the hardware overhead of HeteroMem remains stable when the number of CXL devices increases. HeteroMem can be built into a CXL switch, and the number of CXL switches is far smaller than the number of CXL devices in a CXL extended memory system. Moreover, not every CXL switch needs to be equipped with HeteroMem hardware. HeteroMem hardware can manage the entire memory space it can access, so we only need to implement HeteroMem in the CXL switch connected to the host CPU.
% Secondly, the hardware overhead of HeteroMem remains stable when the memory space scales. The hot page profiling part of HeteroMem utilizes Count-Min Sketch to detect hot pages, whose error bound is related to the memory access sequence length, rather than the size of the memory space. The cold page profiling part of HeteroMem, namely the ping-pong bitmap, uses two bits of SRAM for every 4KB page, and we only need to allocate the cold page profiling structure for fast memory media with demotion targets, whose capacity is often small.
The hardware overhead of HeteroMem remains stable as the CXL-extended memory system scales. Firstly, HeteroMem's hardware overhead increases minimally with the number of CXL devices. HeteroMem can be integrated into a CXL switch, and the number of CXL switches is significantly smaller than the number of CXL devices. Furthermore, not every CXL switch needs to be equipped with HeteroMem hardware. HeteroMem can manage the entire memory space it can access, so it only needs to be implemented in the CXL switch connected to the host CPU.
Secondly, as the memory space scales, the increase in HeteroMem's hardware overhead remains minimal. The hot page profiling component of HeteroMem utilizes Count-Min Sketch to detect hot pages, with an error bound related to the memory access sequence length rather than the memory space size. The cold page profiling component, namely the ping-pong bitmap, uses two bits of SRAM for every page. This structure only needs to be allocated for fast memory regions with demotion targets, which typically have a small capacity.
For the Remapping Unit, the page granularity can be increased as memory space expands, such as using a mix of 4KB and 2MB pages, or exclusively 2MB pages. For example, a 4MB Remapping Cache (potentially larger in ASIC designs) with 2MB page granularity has a reach of 1TB memory (assuming 8 bytes per entry). This reach can be further extended by using larger page granularity (e.g., 1GB pages).




\noindent{\textbf{Profiling and Migration Granularity:}}
%Previous works vary in the granularity of profiling memory accesss and migrating data. Software-based memory tiering systems mainly use 4KB base page and 2MB huge page 
% In our current HeteroMem design, we use 4KB page as both profiling and migration granularity for design simplicity. 
% Generally speaking, smaller granularity enables more flexible data placement, yet introduces heavier remapping overhead. Using larger granularity lower the overhead of remapping, yet it introduces amplification in data movement and waste of fast memory due to hotness skew in a large page. 
% There is no theoretical difficulties to implement profiling and migration granularity other than 4KB or mixed using multiple different granularities in HeteroMem. We leave it to our future work.
In our current HeteroMem design, we use a 4KB page for both profiling and migration granularity for design simplicity. Generally speaking, smaller granularity enables more flexible data placement but introduces heavier remapping overhead. Using larger granularity reduces the overhead of remapping but introduces amplification in data movement and wastage of fast memory due to hotness skew in a large page.
There are no theoretical difficulties in implementing profiling and migration granularities other than 4KB or using multiple granularities in HeteroMem. We leave this for our future work.

% \noindent{\textbf{ASIC design analysis:}}

% \noindent{\textbf{\rebuttal{Migration Overhead Analysis:}}} \rebuttal{As shown in Table~\ref{table:migration_overhead}, we run BC benchmark with with two configuration: one disables migration, while the other performs migration randomly with a speed of 256MB/s, both with Remapping Unit enabled and do not add latency to the slow memory region. The result shows a 0.153\% performance slowdown when migration is performed, which demonstrate: (1) The high efficiency of hardware-based migration. (2) The design of blocking subsequent memory requests will not affect the performance. }

% \noindent{\textbf{Migration Overhead Analysis:}} As shown in Table~\ref{table:migration_overhead}, we run the BC benchmark with two configurations: one with migration disabled and the other performing migration randomly with a speed of 256MB/s, both with the Remapping Unit enabled and no added latency to the slow memory region. The results show merely a 0.153\% performance slowdown when migration is enabled, demonstrating the high efficiency and low overhead of hardware-based migration.
% (1) the high efficiency of hardware-based migration, and 
% (2) that blocking subsequent memory requests during migration will not significantly impact performance, since the overhead of migration is small.


% \noindent{\textbf{\rebuttal{Application Mix:}}} \rebuttal{As shown in Table~\ref{table:multi_application}, we run BC and PR in parallel and compare the performance of HeteroMem and NeoMem. The result shows the superior performance of HeteroMem when running different benchmarks in parallel.}

% \noindent{\textbf{Application Mix:}} As shown in Table~\ref{table:multi_application}, we run the BC and PR benchmarks in parallel to compare the performance of HeteroMem and NeoMem. The results demonstrate the superior performance of HeteroMem when running multiple benchmarks in parallel.

% \noindent{\textbf{Firmware implementation:}} The Profiling Unit of HeteroMem, which is responsible for implementing the strategy of heterogeneous memory management, can be implemented using firmware. This approach offers greater flexibility in the design of HeteroMem. We plan to explore this in future work.






