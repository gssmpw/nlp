%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{lipsum,lineno}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{xcolor}

% Define softer colours
\definecolor{softred}{rgb}{0.8, 0.2, 0.2}
\definecolor{softgreen}{rgb}{0.2, 0.6, 0.2}
\definecolor{softblue}{rgb}{0.3, 0.5, 0.8}
\definecolor{softorange}{rgb}{0.9, 0.5, 0.2}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage[percent]{overpic}

% My MACROS
\usepackage{mymacros}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}

\newcommand{\nb}[3]{{\colorbox{#2}{\bfseries\sffamily\scriptsize\textcolor{white}{#1}}}{\textcolor{#2}{\sf\small\textit{#3}}}}
\newcommand{\riccardo}[1]{\nb{Riccardo}{orange}{#1}}
\newcommand{\mirco}[1]{\nb{Mirco}{teal}{#1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Towards Principled Multi-Agent Task Agnostic Exploration}

\begin{document}

\twocolumn[
\icmltitle{Towards Principled Multi-Agent Task Agnostic Exploration}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Riccardo Zamboni}{yyy}
\icmlauthor{Mirco Mutti}{zzz}
\icmlauthor{Marcello Restelli}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{DEIB, Politecnico di Milano, Milan, Italy}
\icmlaffiliation{zzz}{Technion, Haifa, Istrael}

\icmlcorrespondingauthor{Riccardo Zamboni}{riccardo.zamboni@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Reinforcement Learning, Multi-Agent Reinforcement Learning, Task Agnostic Exploration, Unsupervised Reinforcement Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

%\mirco{Bello il titolo, però il taglio del paper non è ovvio}

\begin{abstract}
In reinforcement learning, we typically refer to \emph{task-agnostic} exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the \emph{entropy} of the state distribution induced by the agent's policy, from which principles and methods follows.
In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings.
%Task-agnostic exploration in Reinforcement Learning refers to exploring a priori of a task specification, i.e. reward definition. In the presence of one single agent, this problem has been cast as maximizing the entropy over the state distribution induced by the agent's policy; it has been expensively studied, relatively well-understood, and sufficiently motivated. On the other hand, little is known about task-agnostic exploration in multi-agents domains, despite the ubiquity of such scenarios. How should different agents explore in a task-agnostic manner in the presence of others? What is the right way to cast such problem? In this paper, we aim to address these open questions. First, we provide various problem formulations coherent with the task, highlighting their positives and negatives. Then, we present a trust-region based policy search algorithm explicitly addressing the problem in practical scenarios in a scalable and decentralized manner. Finally, we provide proof of concept experiments corroborating the theoretical findings and confirming how policy-pretraining via pure exploration represents a viable solution to sparse reward multi-agent settings, \emph{when done right}. With this work, we pave the way for principled and scalable task-agnostic exploration in multi-agent scenarios.
\end{abstract}

\input{sections/01_introduction}
\input{sections/02_setting}
\input{sections/03_problem_formulation}
\input{sections/04_main_results}
\input{sections/05_algorithm}
\input{sections/06_experiments}
\input{sections/07_relatedworks}
\input{sections/08_conclusions}


% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}
\clearpage
\newpage

\section*{Impact Statement}


This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

\bibliography{biblio}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{sections/A0_appendix_proof}
\input{sections/A1_appendix_theory}
\input{sections/A2_appendix_algorithms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
