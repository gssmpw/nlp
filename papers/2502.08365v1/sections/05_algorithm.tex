\section{Trust Region for Exploration in Practice}
\label{sec:algorithm}
%In the previous section, we showed that CMGs, and task-agnostic exploration in particular, might be easily optimized in theory by addressing infinite-trials objectives directly. On the other hand, we also showed that this might often lead to poor performances once such policies are deployed over a handful of trials. 
As stated before, a core drive of this work is addressing multi-agent task-agnostic exploration in practical scenarios. Yet, these cases are also the ones in which performing PG of infinite-trials objectives provide poor performance guarantees at deployment. In other words, here we address the third research question, that is:
\begin{center}
    %\vspace{-0.2cm}
    (iii) \emph{How can we explicitly address multi-agent task-agnostic exploration in practical scenarios?}
    %\vspace{-0.2cm}
\end{center} 
To do so, our attention will focus on the finite trials objectives explicitly, more specifically on the single-trial case with $K=1$. Remarkably, it is possible to directly optimize the single-trial objective in multi-agent cases with decentralized algorithms: we introduce \emph{Trust Region Pure Exploration} (TRPE), the first decentralized algorithm that explicitly addresses single-trial objectives in CMGs, with task-agnostic exploration as a special case. TRPE takes inspiration from trust-region based methods as TRPO~\cite{schulman2017trustregionpolicyoptimization}, as they recently enjoyed an ubiquitous success and interest for their surprising effectiveness in multi-agent problems~\cite{yu2022surprisingeffectivenessppocooperative}. 

In fact, trust-region analysis nicely align with the properties of finite-trials formulations and allow for an elegant extension to CMGs through the following.
\begin{restatable}[Surrogate Function over a Single Trial]{defi}{surrogate} \label{def:surrogate} For every CMG $\mathcal M_{\mathcal F}$ equipped with a $L$-Lipschitz function $\mathcal F$, let $d_1$ be a general single-trial distribution $d_1 = \{ d_1, d_{1,i}, \tilde d_1\}$, then for any per-agent deviation over policies $\pi = (\pi^i, \pi^{-i})$, $ \tilde \pi = (\tilde \pi^i, \pi^{-i})$, it is possible to define a per-agent \emph{Surrogate Function} $\mathcal L^i(\tilde \pi/\pi)$ of the form 
    \begin{equation*}\label{eq:surrogate}
    %\vspace{-0.2cm}
        \mathcal L^i(\tilde \pi/\pi) = \E_{d_1\sim p^\pi_1} \rho^i_{\tilde \pi/ \pi} \mathcal F (d_1),
    \vspace{-0.2cm}
    \end{equation*}
where $\rho^i$ is the per-agent importance-weight coefficient $\rho^i_{\tilde \pi/ \pi} = p^{\tilde \pi}_1 / p^\pi_1 = \prod_{t \in [T]} \frac{\tilde \pi^i(\va^i[t]|\sbf^i[t])}{ \pi^i(\va^i[t]|\sbf^i[t])}$, such that for $\zeta_1 \in \{\zeta_1^\infty,\zeta_1^i, \tilde \zeta_1  \}$.%, namely both joint, disjoint and mixture single-trial objectives, the following relationship holds:\begin{equation*}\zeta_1(\tilde \pi) \geq \mathcal L^i(\tilde \pi/\pi)  - C \tv^{\text{max}}(\tilde \pi^i, \pi^i),\end{equation*}where $C=LT$.
\end{restatable}



\begin{algorithm}[]
    \caption{Trust Region Pure Exploration (TRPE)}
    \label{alg:trpe}
    \begin{algorithmic}[H]
        \STATE \textbf{Input}: exploration horizon $T$, number of trajectories $N$, trust-region threshold $\delta$, learning rate $\eta$.
        \STATE initialize $\vtheta = (\theta^i)_{i \in [\Ns]}$
        \FOR{epoch = $1, 2, \ldots, $ until convergence}
            \STATE Collect $N$ trajectories with $\pi_{\vtheta}= (\pi^i_{\theta^i})_{i \in [\Ns]}$.
            \FOR{agent $i=1, 2, \ldots, $ \emph{concurrently}}
            \STATE Construct datasets $\mathcal{D}^i = \{ (\sbf^i_n,\va^i_n), \zeta_1^n\}_{n \in [N]}$
            \STATE $\theta^i \gets \emph{IS-Optimizer}(\mathcal{D}^i, \theta^i )$
            \ENDFOR
        \ENDFOR
        \STATE \textbf{Output}: task-agnostic exploration (joint) policy $\pi_{\vtheta}= (\pi^i_{\theta^i})_{i \in [\Ns]}$ 
    \end{algorithmic}
    \vspace{-3pt}
    \algrule[0.75pt]
    \begin{flushleft}
        \vspace{-4pt}
        \normalsize{IS-Optimizer}
        \vspace{-9pt}
    \end{flushleft}
    \algrule[0.4pt]
    \vspace{-5pt}
    \begin{algorithmic}[H]
        \STATE \textbf{Input}: Dataset $\mathcal{D}^i$, sampling parameter $\theta^i$.
        \STATE Initialize $h = 0$ and $\theta^i_h = \theta^i$
        \WHILE{ $\kl(\pi^i_{ \theta^i_h } \| \pi^i_{\theta^i_0}) \leq \delta$}
            \STATE Compute $\hat{\mathcal L}^i(\theta^i_h/ \theta^i_0)$ via IS
            \STATE Perform Gradient step $ \theta^i_{h + 1} = \theta^i_{h} + \eta \nabla_{\theta^i_{h}} \hat{\mathcal L}^i(\theta^i_h/ \theta^i_0)$
            \STATE $h \gets h + 1$
        \ENDWHILE
        \STATE \textbf{Output}: parameters $\vtheta_h$ 
    \end{algorithmic}
 \end{algorithm}

% \mirco{Cioè? Cosa abbiamo con trust region in più di semplice policy optimization?}
From this definition, it follows that the trust-region algorithmic blueprint of~\citet{schulman2017trustregionpolicyoptimization} can be directly applied to single-trial formulations, with per-agent policies within a parametric space of stochastic differentiable policies $\Theta= \{\pi^i_{\theta^i}: \theta^i \in \Theta^i \subseteq \mathbb R^q\}$. In practice, kl-divergence is employed for greater scalability provided a trust-region threshold $\delta$, we address the following optimization problem for each agent:
\vspace{-8pt}
\begin{align*}
&\max_{\tilde \theta^i \in \Theta^i} \mathcal L^i(\tilde \theta^i/\theta^i) , \\
&\text{s.t. } \kl(\pi^i_{\tilde \theta^i } \| \pi^i_{\theta^i}) \leq \delta
\vspace{-0.3cm}
\end{align*}
where we simplified the notation by letting $\mathcal L^i(\tilde \theta^i/\theta^i)  := \mathcal L^i(\pi^i_{\tilde \theta^i},\pi^{-i}_{ \theta^{-i}}/\pi_{\theta} )$.


\begin{figure*}[!]
    \centering
    \input{figures/exploration_legend.tex}

    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/room_50_jointentropynokl.pdf}
        \vspace{-15pt}
        \caption{\centering Joint Entropy.}
        \label{subfig:image2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/room_50_mixtureentropynokl.pdf}
        \vspace{-15pt}
        \caption{\centering Mixture Entropy.}
        \label{subfig:image3}
    \end{subfigure}
    \hfill
    \raisebox{0.2cm}{
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{figures/heat_MixtureObjective.png}
            %\vspace{-15pt}
            \caption{\centering Mixture}
            \label{subfig:image7}
        \end{subfigure}
        %\hfill
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{figures/heat_JointObjective.png}
            %\vspace{-15pt}
            \caption{\centering Joint}
            \label{subfig:image5}
        \end{subfigure}
        \vfill
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{figures/heat_DisjointObjective.png}
            %\vspace{-15pt}
            \caption{\centering Disjoint}
            \label{subfig:image6}
        \end{subfigure}
        %\hfill
        \begin{subfigure}[b]{0.4\textwidth}
            \includegraphics[width=\textwidth]{figures/heat_RandomPolicy.png}
            %\vspace{-15pt}
            \caption{\centering Uniform}
            \label{subfig:image4}
        \end{subfigure}
    \end{subfigure}}
    \vspace{-5pt}
    \caption{Single-trial Joint and Mixture Entropy induced by mixture, joint or disjoint objective optimization along a $T =50$ horizon. (\emph{Right}) State Distributions of two agents induced by different learned policies. We report the average and 95\% c.i. over 4 runs.}
    %\vspace{-0.5cm}
    \label{fig:room}
\end{figure*}




The main idea then follows from noticing that the surrogate function in Eq.~\eqref{eq:surrogate} consists of an Importance Sampling (IS) estimator~\cite{mcbook}, and it is then possible to optimize it in a fully decentralized and off-policy manner, similarly to what was done in~\citet{metelli2020pois} for MDPs and in~\citet{muttirestelli2020} for convex MDPs. More specifically, given a pre-specified objective of interest $\zeta_1 \in \{\zeta_1^\infty,\zeta_1^i, \tilde \zeta_1\}$, agents sample $N$ trajectories $\{(\sbf_n, \va_n)\}_{n \in [N]}$ from the environment by following a (joint) policy with parameters $\vtheta_0 = (\theta^i_0,\theta^{-i}_0)$. They then compute the values of the objective for each trajectory, building separate datasets $\mathcal{D}^i = \{ (\sbf^i_n,\va^i_n), \zeta_1^n\}_{n \in [N]}$. Each agent uses her dataset to compute the Monte-Carlo approximation of the Surrogate Function, namely:
\begin{equation*}
    \vspace{-0.2cm}
    \hat{\mathcal L}^i(\theta^i_h/ \theta^i_0) = \frac{1}{N}\sum_{n\in[N]} \rho^{i,n}_{\theta^i_h/ \theta^i_0} \zeta^n_1,
    \vspace{-0.01cm}
\end{equation*}
where $\rho^{i,n}_{\theta^i_h/ \theta^i_0} = \prod_{t \in [T]} \pi^i_{\theta^i_h}(\va^i_n[t]|\sbf^i_n[t])/ \pi^i_{\theta^i_0}(\va^i_n[t]|\sbf^i_n[t])$ and $\zeta^n_1$ is the plug-in estimator of the entropy based on the empirical measure $d_1$~\cite{paninski2003}. Finally, at each off-policy iteration $h$, each agent updates its parameter via gradient ascent
$
    \theta^i_{h+1} \leftarrow \theta^i_{h} + \eta \nabla_{\theta^i_h} \hat{\mathcal L}^i(\theta^i_h/ \theta^i_0) 
$
until the trust-region boundary is reached, i.e., when it holds $
    \kl(\pi^i_{\tilde \theta^i } \| \pi^i_{\theta^i}) > \delta.
$
The psudo-code of TRPE is reported in Algorithm~\ref{alg:trpe}.

\vspace{-6pt}
\paragraph*{Remark.}~One should note that TRPE is a multi-agent decentralized algorithm and it explicitly addresses task-agnostic exploration objectives, however the algorithmic blueprint is of independent interest since it is both able to address any convex functional $\mathcal F(\cdot)$, and it is valid in the single agent case as well.
