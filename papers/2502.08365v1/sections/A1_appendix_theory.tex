\subsection{Policy Gradient in CMGs with Infinite-Trials.}
\label{apx:theory}
In this Section, we analyze policy search for the infinite-trials joint problem $\zeta_\infty$ of Eq.~\eqref{eq:mse}, via projected gradient ascent over parametrized policies, providing in Th.~\ref{theorem:iteration complexity-gen} the formal counterpart of Fact~\ref{fact:sufficiencypga} in the Main paper. As a side note, all of the following results hold for the (infinite-trials) mixture objective $\tilde \zeta_\infty$ of Eq.~\eqref{eq:mse_mixture}. We will consider the class of parametrized policies with parameters $\theta_i \in \Theta_i \subset \mathbb R^d$, with the joint policy then defined as $\pi_\theta, \theta \in \Theta = \times_{i \in [\Ns]} \Theta_i$. Additionally, we will focus on the computational complexity only, by assuming access to the exact gradient. The study of statistical complexity surpasses the scope of the current work. We define the \textbf{(independent) Policy Gradient Ascent} (PGA) update as:
\begin{eqnarray}
\label{defn:grad-proj}
\theta^{k+1}_i  =  \argmax_{\theta_i\in\Theta_i} \zeta_\infty(\pi_{\theta^k}) \!+\! \left\langle \nabla_{\theta_i} \zeta_\infty(\pi_{\theta^k}),\theta_i\!-\!\theta^k_i\right\rangle \!-\! \frac{1}{2\eta}\|\theta_i\!-\!\theta_i^k\|^2 =  \Pi_{\Theta_i}\big\{\theta^k_i + \eta\nabla_{\theta_i} \zeta_\infty(\pi_{\theta^k})\big\}
\end{eqnarray}
where $\Pi_{\Theta_i}\{\cdot\}$ denotes Euclidean projection onto $\Theta_i$, and equivalence holds by the convexity of $\Theta_i$. The classes of policies that allow for this condition to be true will be discussed shortly.

In general the overall proof is built of three main steps, shared with the theory of Potential Markov Games~\citep{leonardos2021globalconvergencemultiagentpolicy}: (i) prove the existence of well behaved stationary points; (ii) prove that performing independent policy gradient is equivalent to perform joint policy gradient; (iii) prove that the (joint) PGA update converges to the stationary points via single-agent like analysis.
In order to derive the subsequent convergence proof, we will make the following assumptions:

\begin{assumption}
	\label{assumption:gen-para} Define the quantity $\lambda(\theta) := d^{\pi_\theta}$, then:\\
	\textbf{(i).} $\lambda(\cdot)$ forms a bijection between $\Theta$ and $\lambda(\Theta)$, where $\Theta$ and $\lambda(\Theta)$ are closed and convex. \\
	\textbf{(ii).} The Jacobian matrix $\nabla_{\theta}\lambda(\theta)$ is Lipschitz continuous in $\Theta$.  \\
	\textbf{(iii).} Denote $g(\cdot) := \lambda^{-1}(\cdot)$ as the inverse mapping of $\lambda(\cdot)$. Then there exists $ \ell_{\theta}>0$ s.t. $\|g(\lambda)-g(\lambda')|\leq \ell_\theta\|\lambda-\lambda'\|$ for some norm $\|\cdot\|$ and for all $\lambda,\lambda'\in\lambda(\Theta)$. 
\end{assumption}

\begin{assumption}
	\label{assumption:ncvx-Lip}
	There exists $ L>0$ such that the gradient $\nabla_{\theta }\zeta_\infty(\pi_{\theta})$ is $L$-Lipschitz. 
\end{assumption}

\begin{assumption}
	\label{assumption:gradient}
	The agents have access to a gradient oracle $\mathcal O(\cdot)$ that returns $\nabla_{\theta_i}\zeta_\infty(\pi_\theta)$ for any deployed joint policy $\pi_\theta$.
\end{assumption}

\paragraph*{On the Validity of Assumption~\ref{assumption:gen-para}.}~This set of assumptions enforces the objective $\zeta_\infty(\pi_\theta)$ to be well-behaved with respect to $\theta$ even if non-convex in general, and will allow for a rather strong result. Yet, the assumptions are known to be true for directly parametrized policies over the whole support of the distribution $d^\pi$~\citep{zhang2020variationalpolicygradientmethod}, and as a result they implicitly require agents to employ policies conditioned over the full state-space $\Ss$. Fortunately enough, they also guarantee $\Theta$ to be convex. 


\begin{lemma}[\textbf{(i)} Global optimality of stationary policies~\citep{zhang2020variationalpolicygradientmethod}]
	\label{lemma:global-opt}
	Suppose Assumption \ref{assumption:gen-para} holds, and $\mathcal F$ is a concave, and continuous function defined in an open neighborhood containing $\lambda(\Theta)$. 
	Let $\theta^*$ be a first-order stationary point of problem \eqref{eq:mse}, i.e.,\vspace{-2mm}
	\begin{equation}
	\label{defn:1st-order-condition}
	\exists u^*\in\hat{\partial}(\mathcal F\circ\lambda)(\theta^*),\quad\text{s.t.}\quad \langle u^*, \theta-\theta^*\rangle\leq0 \qquad\mbox{for}\qquad\forall \theta\in\Theta.
	\end{equation}
	Then $\theta^*$ is a globally optimal solution of problem \eqref{eq:mse}.
\end{lemma}
This result characterizes the optimality of stationary points for Eq.~\eqref{eq:mse}. Furthermore, we know from~\citet{leonardos2021globalconvergencemultiagentpolicy} that stationary points of the objective are Nash Equilibria.

\begin{lemma}[\textbf{(ii)} Projection Operator~\citep{leonardos2021globalconvergencemultiagentpolicy}]\label{claim:projection} 
    Let $\theta := (\theta_1,...,\theta_\Ns)$ be the parameter profile for all agents and use the update of Eq.~\eqref{defn:grad-proj} over a non-disjoint infinite-trials objective. Then, it holds that
    \begin{equation*}
        \Pi_{\Theta}\big\{\theta^k + \eta\nabla_\theta \zeta_\infty(\pi_{\theta^k})\big\} = \Big( \Pi_{\Theta_i}\big\{\theta^k_i + \eta\nabla_{\theta_i} \zeta_\infty(\pi_{\theta^k})\big\}\Big)_{i \in [\Ns]}
    \end{equation*}
\end{lemma}

This result will only be used for the sake of the convergence analysis, since it allows to analyze independent updates as joint updates over a single objective. The following Theorem is the formal counterpart of Fact~\ref{fact:sufficiencypga} and it is a direct adaptation to the multi-agent case of the single-agent proof by~\citet{zhang2020variationalpolicygradientmethod}, by exploiting the previous result.


\begin{theorem}[\textbf{(iii)} Convergence rate of independent PGA to stationary points (Formal Fact~\ref{fact:sufficiencypga})]
	\label{theorem:iteration complexity-gen}
	Let Assumptions \ref{assumption:gen-para} and \ref{assumption:ncvx-Lip} hold. Denote $D_\lambda \!:=\! \max_{\lambda,\lambda'\in\lambda(\Theta)} \|\lambda-\lambda'\|$ as defined in Assumption~\ref{assumption:gen-para}(iii). Then the independent policy gradient update \eqref{defn:grad-proj} with $\eta = 1/L$ satisfies for all $k$ with respect to a stationary (joint) policy $\pi_{\theta^*}$ the following\vspace{-2mm}
	\begin{equation*}
	%\label{thm:cvg-gen-para-1}
	\zeta_\infty(\pi_{\theta^*}) \!-\! \zeta_\infty(\pi_{\theta^k})\leq \frac{4L\ell_{\theta}^2D_\lambda^2}{k+1}.
	\end{equation*}
\end{theorem} 


\begin{proof}
    First, the Lipschitz continuity in Assumption \ref{assumption:ncvx-Lip} indicates that 
    %
        $$\left|\zeta_\infty(\lambda(\theta)) - \zeta_\infty(\lambda(\theta^k)) - \langle \nabla_\theta\zeta_\infty(\lambda(\theta^k)),\theta-\theta^k\rangle\right|\leq \frac{L}{2}\|\theta-\theta^k\|^2.$$
        %
        Consequently, for any $\theta\in\Theta$ we have the ascent property:
        %
        \begin{equation}\label{eq:taylor_proof}
        %
        \zeta_\infty(\lambda(\theta)) \geq \zeta_\infty(\lambda(\theta^k)) + \langle \nabla_\theta\zeta_\infty(\lambda(\theta^k)),\theta-\theta^k\rangle - \frac{L}{2}\|\theta-\theta^k\|^2 \geq \zeta_\infty(\lambda(\theta)) - L\|\theta-\theta^k\|^2.
        \end{equation}
        %
        The optimality condition in the policy update rule \eqref{defn:grad-proj} coupled with the result of Lemma~\ref{claim:projection} allows us to follow the same rational as~\citet{zhang2020variationalpolicygradientmethod}. We will report their proof structure after this step for completeness.
        %
    \begin{align}
        \label{thm:ItrCmp-1}
        \MoveEqLeft
        \zeta_\infty(\lambda(\theta^{k+1}))  \geq  \zeta_\infty(\lambda(\theta^k)) + \langle \nabla_\theta\zeta_\infty(\lambda(\theta^k)),\theta^{k+1}-\theta^k\rangle - \frac{L}{2}\|\theta^{k+1}-\theta^k\|^2 \nonumber \\
        & =  \max_{\theta\in\Theta} \zeta_\infty(\lambda(\theta^k)) + \langle \nabla_\theta\zeta_\infty(\lambda(\theta^k)),\theta-\theta^k\rangle - \frac{L}{2}\|\theta-\theta^k\|^2\nonumber\\
        & \overset{\text{(a)}}{\geq}  \max_{\theta\in\Theta} \zeta_\infty(\lambda(\theta)) - L\|\theta-\theta^k\|^2\nonumber\\
        & \overset{\text{(b)}}{\geq}   \max_{\alpha\in[0,1]}\left\{\zeta_\infty(\lambda(\theta_{\alpha})) - L\|\theta_{\alpha}-\theta^k\|^2: \theta_{\alpha} = g(\alpha\lambda(\theta^*) + (1-\alpha)\lambda(\theta^k)) \right\}.
    \end{align}
        where step (a) follows from \eqref{eq:taylor_proof} and step (b) uses the convexity of $\lambda(\Theta)$. Then, by the concavity of $\zeta_\infty$ and the fact that the composition $\lambda\circ g = id$ due to Assumption~\ref{assumption:gen-para}(i), we have that:
        %
        $$\zeta_\infty(\lambda(\theta_{\alpha})) = \zeta_\infty(\alpha\lambda(\theta^*) + (1-\alpha)\lambda(\theta^k))\geq\alpha\zeta_\infty(\lambda(\theta^*)) + (1-\alpha)\zeta_\infty(\lambda(\theta^k)).$$
        %
        Moreover, due to Assumption~\ref{assumption:gen-para}(iii) we have that:
        \begin{eqnarray}
        \label{eqn:important-gen}
            \|\theta_{\alpha} - \theta^k\|^2 & = & \|g(\alpha\lambda(\theta^*) + (1-\alpha)\lambda(\theta^k))- g(\lambda(\theta^k))\|^2\\
            & \leq & \alpha^2\ell_{\theta}^2\|\lambda(\theta^*) - \lambda(\theta^k)\|^2\nonumber\\
            & \leq & \alpha^2\ell_{\theta}^2D_\lambda^2.\nonumber
        \end{eqnarray}
        From which we get 
        \begin{align}
        \MoveEqLeft 
        \zeta_\infty(\lambda(\theta^*)) - \zeta_\infty(\lambda(\theta^{k+1})) \nonumber \\
        & \leq  \min_{\alpha\in[0,1]}\left\{\zeta_\infty(\lambda(\theta^*))-\zeta_\infty(\lambda(\theta_{\alpha})) + L\|\theta_{\alpha}-\theta^k\|^2: \theta_{\alpha} = g(\alpha\lambda(\theta^*) + (1-\alpha)\lambda(\theta^k)) \right\}\nonumber\\
        & \leq  \min_{\alpha\in[0,1]}(1-\alpha)\big(\zeta_\infty(\lambda(\theta^*))-\zeta_\infty(\lambda(\theta^k))\big) + \alpha^2L\ell_{\theta}^2D_\lambda^2 \,.
        \label{thm:ItrCmp-2-gen}
        \end{align}
        We define $\Lambda(\pi_\theta) := \lambda(\theta)$, then $\alpha_k = \frac{\zeta_\infty(\Lambda(\pi^*)) - \zeta_\infty(\Lambda(\pi^k))}{2L\ell_{\theta}^2D_\lambda^2}\geq0$, which is the minimizer of the RHS of  \eqref{thm:ItrCmp-2-gen} as long as it satisfies $\alpha_k\leq 1$. Now, we claim the following: If $\alpha_k\ge 1$ then $\alpha_{k+1}<1$. Further, if $\alpha_k<1$ then $\alpha_{k+1}\le \alpha_k$. The two claims together mean that $(\alpha_k)_k$ is decreasing and all $\alpha_k$ are in $[0,1)$ except perhaps $\alpha_0$.
    
        To prove the first of the two claims, assume $\alpha_k\ge 1$.
        This implies that $\zeta_\infty(\Lambda(\pi^*)) - \zeta_\infty(\Lambda(\pi^k))\geq 2L\ell_{\theta}^2D_\lambda^2$. Hence, choosing $\alpha=1$ in \eqref{thm:ItrCmp-2-gen}, we get
        \[\zeta_\infty(\lambda(\theta^*)) - \zeta_\infty(\lambda(\theta^{k}))\leq L\ell_{\theta}^2D_\lambda^2\]
        which implies that $\alpha_{k+1}\le 1/2<1$. To prove the second claim, we plug  $\alpha_k$ into \eqref{thm:ItrCmp-2-gen} to get
        \[
        \zeta_\infty(\lambda(\theta^*)) - \zeta_\infty(\lambda(\theta^{k+1})) \leq  \left(1-\frac{\zeta_\infty(\lambda(\theta^*)) - \zeta_\infty(\lambda(\theta^{k}))}{4L\ell_{\theta}^2D_\lambda^2}\right)(\zeta_\infty(\lambda(\theta^*)) - \zeta_\infty(\lambda(\theta^{k}))),
        \]
        which shows that $\alpha_{k+1}\le \alpha_k$ as required.
        
        Now, by our preceding discussion, for $k=1,2,\dots$ the previous recursion holds.
        Using the definition of $\alpha_k$, we rewrite this in the equivalent form  
        \[
        \frac{\alpha_{k+1}}{2}\leq \left(1-\frac{\alpha_{k}}{2}\right)\cdot\frac{\alpha_{k}}{2}.
        \] 
    By rearranging the preceding expressions and algebraic manipulations, we obtain
        %
        $$\frac{2}{\alpha_{k+1}} \geq \frac{1}{\left(1-\frac{\alpha_{k}}{2}\right)\cdot\frac{\alpha_{k}}{2}} = \frac{2}{\alpha_{k}} + \frac{1}{1-\frac{\alpha_{k}}{2}}\geq\frac{2}{\alpha_k} + 1.$$
        For simplicity assume that $\alpha_0<1$ also holds. Then,
        $\frac{2}{\alpha_{k}}\geq \frac{2}{\alpha_0} + k$, and consequenlty
        % 
        $$\zeta_\infty(\lambda(\theta^*)) - \zeta_\infty(\lambda(\theta^{k}))\leq \frac{\zeta_\infty(\lambda(\theta^*)) - \zeta_\infty(\lambda(\theta^0))}{1+ \frac{\zeta_\infty(\lambda(\theta^*)) - \zeta_\infty(\lambda(\theta^0))}{4L\ell_{\theta}^2D_\lambda^2}\cdot k} \leq \frac{4L\ell_{\theta}^2D_\lambda^2}{k}.$$
        %
        A similar analysis holds when $\alpha_0>1$. Combining these two gives that 
        $\zeta_\infty(\lambda(\pi^*)) - \zeta_\infty(\lambda(\pi^{k}))\leq \frac{4L\ell_{\theta}^2D_\lambda^2}{k+1}$ no matter the value of $\alpha_0$, which proves the result. 
    \end{proof}

\subsection{The Use of Markovian and Non-Markovian Policies in CMGs with Finite-Trials.}
\label{apx:policies}
The following result describes how in CMGs, as for convex MDPs, Non-Markovian policies are the right policy class to employ to guarantee well-behaved results.

\begin{restatable}[Sufficiency of Disjoint Non-Markvoian Policies]{lem}{sufficiency}
    \label{lem:sufficiency} 
    For every Convex Markov Game $\mathcal M$ there exist a joint policy $\pi^\star = (\pi^{\star, i})_{i \in \Ns}$, with $\pi^{\star, i}\in\Delta_{\Ss^T}^{\Acal^i}$ being a deterministic Non-Markovian policy, that is a Nash Equilibrium for non-Disjoint single-trial objectives, for $K=1$.
\end{restatable}

\begin{proof}
    The proof builds over a straight reduction. We build from the original MG $\MDP$ a temporally extended Markov Game $\tilde \MDP= ( \Ns, \tilde \Ss,   \Acal, \Pb, r, \mu, T)$. A state $\tilde s$ is defined for each history that can be induced, i.e., $\tilde s \in \tilde \Ss \iff \sbf \in \Ss^T $. We keep the other objects equivalent, where for the extended transition model we solely consider the last state in the history to define the conditional probability to the next history. We introduce a common reward function across all the agents $ r: \tilde \Ss \rightarrow \mathbb R$ such that $r(\tilde s) = H(d(\tilde s))$ for joint objectives and $r(\tilde s) = (1/N)\sum_{i \in [\Ns]} H(d_i(\tilde s_i))$ for mixture objectives, for all the histories of length T and $0$ otherwise. We now know that according to~\citet[Theorem 3.1,][]{leonardos2021globalconvergencemultiagentpolicy} there exists a deterministic Markovian policy $\tilde \pi^\star = (\tilde \pi^i)_{i \in \Ns}, \tilde \pi^i \in \Delta^{\Acal_i}_{\tilde \Ss}$ that is a Nash Equilibrium for $\tilde \MDP$. Since $\tilde s$ corresponds to the set of histories of the original game, $\tilde \pi^\star$ maps to a non-Markovian policy in it. Finally, it is straightforward to notice that the NE of  $\tilde \pi^\star$ for $\tilde \MDP$ implies the NE of $\tilde \pi^\star$ for the original CMG $\MDP$.
\end{proof}

The previous result implicitly asks for policies conditioned over the joint state space, as happened for infinite-trials objectives as well. Interestingly, finite-trials objectives allow for a further characterization of how an optimal Markovian policy would behave when conditioned on the per-agent states only:


\begin{restatable}[Behavior of Optimal Markovian Decentralized Policies]{lemma}{behaviormarkovian}
	Let $\pi_{\text{NM}} = (\pi^i_{\text{NM}}\in\Delta_{\Ss^T}^{\Acal^i})_{i \in [\Ns]}$ an optimal deterministic non-Markovian centralized policy and $\bar \pi_{\text{M}} = (\bar \pi^i_{\text{M}}\in\Delta_{\Ss}^{\Acal^i})_{i \in [\Ns]}$ the optimal Markovian centralized policy, namely $\bar \pi_{\text{M}}  = \argmax_{\pi = (\pi^i\in\Delta_{\Ss}^{\Acal^i})_{i \in [\Ns]} }\zeta_1(\pi)$. For a fixed sequence $\sbf_t \in \Ss^t$ ending in state $s = (s_i,s_{-i})$, the variance of the event of the optimal Markovian decentralized policy $ \pi_{\text{M}} = ( \pi^i_{\text{M}}\in\Delta_{\Ss_i}^{\Acal^i})_{i \in [\Ns]}$ taking $a^* = \pi_{\text{NM}} (\cdot|\sbf_t) = \bar \pi_{\text{M}}(\cdot|s,t)$ in $s_i$ at step $t$ is given by
	\begin{align*}
		\Var \big[ &\mathcal{B} ( \pi_{\text{M}} (a^*|s_i, t) ) \big] = \Var_{\sbf  \oplus s \sim p^{\pi_{\text{NM}}}_t} \big[ \E \big[ \mathcal{B} ( \pi_{\text{NM}} (a^* | \sbf  \oplus s) ) \big] \big]\\ &+ \Var_{\sbf \oplus(\cdot, s_{-i}) \sim p^{\bar \pi_{\text{M}}}_t} \big[ \E \big[ \mathcal{B} ( \bar \pi_{\text{M}} (a^* |  s_i, s_{-i}, t) ) \big] \big].
	\end{align*}
	where $\sbf \oplus s \in \Ss^t$ is any sequence of length $t$ such that the final state is $s$, \ie $\sbf \oplus s := (\sbf_{t - 1} \in \Ss^{t - 1}) \oplus s$, and $\mathcal{B} (x)$ is a Bernoulli with parameter $x$.
    \label{thr:behaviormarkovian}
\end{restatable}

Unsurprisingly, this Lemma shows that whenever the optimal Non-Markovian strategy for requires to adapt its decision in a joint state $s$ according to the history that led to it, an optimal Markovian policy for the same objective  must necessarily be a stochastic policy, additionally, whenever the optimal Markovian policy conditioned over per-agent states only will need to be stochastic whenever the optimal Markovian strategy conditioned on the full states randomizes its decision based on the joint state $s$.

\begin{proof}
	Let us consider the random variable $A_i \sim \mathcal{P}_i$ denoting the event ``the agent $i$ takes action $a^*_i \in \Acal_i$''. Through the law of total variance~\cite{bertsekas2002introduction}, we can write the variance of $A$ given $s \in \Ss$ and $t \geq 0$ as
	\begin{align}
		\Var \big[ A | s, t \big]
		&= \E \big[ A^2 | s, t \big] - \E \big[ A | s, t \big]^2 \nonumber \\
		&= \E_{\sbf} \Big[ \E \big[ A^2 | s, t,  \sbf  \big] \Big] - \E_{\sbf} \Big[ \E \big[ A | s, t, \sbf \big] \Big]^2 \nonumber \\
		&= \E_{\sbf} \Big[ \Var \big[ A | s, t,  \sbf  \big] + \E \big[ A | s, t,  \sbf  \big]^2 \Big]
		- \E_{\sbf} \Big[ \E_{\pi} \big[ A | s, t,  \sbf  \big] \Big]^2 \nonumber \\
		&= \E_{\sbf} \Big[ \Var \big[ A | s, t,  \sbf  \big] \Big] +\E_{\sbf} \Big[  \E \big[ A | s, t,  \sbf  \big]^2 \Big] - \E_{\sbf} \Big[ \E \big[ A | s, t,  \sbf  \big] \Big]^2 \nonumber \\
		&= \E_{\sbf} \Big[ \Var \big[ A | s, t,  \sbf  \big] \Big] + \Var_{ \sbf } \Big[ \E\big[ A | s, t,  \sbf  \big] \Big]. \label{eq:lotv1}
	\end{align}
	Now let the conditioning event $ \sbf $ be distributed as $ \sbf  \sim p^{\pi_{\text{NM}}}_{t - 1}$, so that the condition $s, t,  \sbf $ becomes $ \sbf  \oplus s$ where $\sbf  \oplus s = (s_{0}, a_{0}, s_{1}, \ldots, s_{t} = s) \in \Ss^t$, and let the variable $A$ be distributed according to $\mathcal{P}$ that maximizes the objective given the conditioning. Hence, we have that the variable $A$ on the left hand side of~\eqref{eq:lotv1} is distributed as a Bernoulli $\mathcal{B} (\bar \pi_{\text{M}} (a^* | s, t))$, and the variable $A$ on the right hand side of~\eqref{eq:lotv2} is distributed as a Bernoulli $\mathcal{B} (\pi_{\text{NM}} (a^* | \sbf  \oplus s))$. Thus, we obtain
	\begin{equation}
		\Var \big[ \mathcal{B} ( \bar \pi_{\text{M}} (a^*|s, t) ) \big] = \E_{\sbf  \oplus s \sim p^{\pi_{\text{NM}}}_t} \big[ \Var \big[ \mathcal{B} ( \pi_{\text{NM}} (a^* | \sbf  \oplus s) ) \big] \big] + \Var_{\sbf  \oplus s \sim p^{\pi_{\text{NM}}}_t} \big[ \E \big[ \mathcal{B} ( \pi_{\text{NM}} (a^* | \sbf  \oplus s) ) \big] \big].
	\label{eq:lotv2}
	\end{equation}
	We know from Lemma~\ref{lem:sufficiency} that the policy $\pi_{\text{NM}}$ is deterministic, so that $\Var \big[ \mathcal{B} ( \pi_{\text{NM}} (a^* | \sbf  \oplus s) ) \big] = 0$ for every $\sbf  \oplus s$. We then repeat the same steps in order to compare the two different Markovian policies:
    \begin{align}
		\Var \big[ A | s_i, t \big]
		&= \E_{s_{-i}} \Big[ \Var \big[ A | s_i, s_{-i}, t \big] \Big] + \Var_{s_{-i}} \Big[ \E\big[ A | s_i, s_{-i}, t \big] \Big].  \nonumber 
	\end{align}
    Repeating the same considerations as before we get that we can use~\eqref{eq:lotv2} to get:
    \begin{align*}
		\Var \big[ \mathcal{B} ( \pi_{\text{M}} (a^*|s_i, t) ) \big] &= \E_{\sbf \oplus(\cdot, s_{-i}) \sim p^{ \bar \pi_{\text{M}}}_t} \big[ \Var \big[ \mathcal{B} ( \bar \pi_{\text{M}} (a^* | s_i, s_{-i}, t) ) \big] \big] + \Var_{\sbf \oplus(\cdot, s_{-i}) \sim p^{\bar \pi_{\text{M}}}_t} \big[ \E \big[ \mathcal{B} ( \bar \pi_{\text{M}} (a^* |  s_i, s_{-i}, t) ) \big] \big]\\
        &=\Var_{\sbf  \oplus s \sim p^{\pi_{\text{NM}}}_t} \big[ \E \big[ \mathcal{B} ( \pi_{\text{NM}} (a^* | \sbf  \oplus s) ) \big] \big] + \Var_{\sbf \oplus(\cdot, s_{-i}) \sim p^{\bar \pi_{\text{M}}}_t} \big[ \E \big[ \mathcal{B} ( \bar \pi_{\text{M}} (a^* |  s_i, s_{-i}, t) ) \big] \big].
	\end{align*}
\end{proof}


%\paragraph*{Quasi-Potentialness of Disjoint Objectives.}~All results so far have focused on non-disjoint objectives, as they allow for simpler analysis. As a side comment, we can at least conjecture that decentralized learning algorithms will enjoy good performance guarantees, as the infinite-trajectory formulation of the disjoint objective of Eq.~\eqref{eq:mse_decentralized} defines an almost Potential Game in the sense of~\citet{guo2024markovalphapotentialgames} with respect to the mixture objective of Eq.~\eqref{eq:mse_mixture}. Unfortunately, this is a conjecture since convergence properties of decentralized algorithms in \emph{Potential} CMGs are yet unknown, and it merely suggests that there are good reasons to believe that decentralized learning over Eq.~\eqref{eq:mse_decentralized} does not lead to learning instabilities.
%\begin{restatable}[$\alpha$-Potentialness of Disjoint Objectives]{lem}{potentialness} \label{lem:potentialness} For every Convex Markov Game $\mathcal M$ equipped with a $L$-Lipschitz function $\mathcal F$, Eq.~\eqref{eq:mse_decentralized} define an $\alpha$-potential game, namely that for any $\pi = (\pi^i,\pi^{-i}), \tilde \pi = (\tilde \pi^i, \pi^i)$, $i \in [\Ns]$ it holds that \begin{equation*}\| \zeta^i_\infty(\pi) -  \zeta^i_\infty(\tilde \pi) - (\zeta_\infty(\pi) -  \zeta_\infty(\tilde \pi))\|_1 \leq \alpha.\end{equation*}\end{restatable}



