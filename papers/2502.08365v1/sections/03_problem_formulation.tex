\section{Problem Formulation}
\label{sec:problem_formulation}
This section addresses the first of the research questions:
\begin{center}
    %\vspace{-0.2cm}
    (i) \emph{How can task-agnostic exploration be defined in MARL?}
    %\vspace{-0.2cm}
\end{center}
In fact, when a reward function is not available, the core of the problem resides in finding a well-behaved problem formulation coherent with the task. We start by introducing a general framework that is a convex generalization of MGs, namely a tuple $\MDP_{\mathcal F} := (\Ns, \Ss, \Acal, \Pb, \mathcal F, \mu, T)$, that consists in a MG equipped with a (non-linear) function $\mathcal F(\cdot)$. We refer to these objects as a \textbf{Convex Markov Games} (CMGs). How much should the agents coordinate? How much information should they have access to? Different answers depict different objectives.

\textbf{Joint Objectives.}~~The first and most straightforward way to formulate the problem is to define it as in the MDP setting, with the joint state distribution simply taking the place of the single-agent state distribution. In this case, we define a \emph{Joint} objective, consisting of
\begin{align}
\vspace{-5pt}
\max_{\pi = (\pi^i\in \Pi^i)_{i \in [\Ns]}} \ \Big\{ \zeta_\infty(\pi) &:= \mathcal F(d^\pi)\Big\} \label{eq:mse} \\ 
\max_{\pi = (\pi^i\in \Pi^i)_{i \in [\Ns]}} \ \Big\{ \zeta_K(\pi) &:=\E_{d_K\sim p^\pi_K}\mathcal F (d_K)\Big\} \label{eq:mse_finite} 
\end{align}
In task-agnostic exploration tasks, i.e. by setting $\mathcal F(\cdot) := H(\cdot)$, an optimal (joint) policy will try to cover the joint state space as uniformly as possible, either in expectation or over a finite number of trials respectively. In this, the joint formulation is rather intuitive as it describes the most general case of multi-agent exploration. Moreover, as each agent sees a difference in performance explicitly linked to others, this objective should be able to foster coordinated exploration. As we will see, this comes at a price.

\textbf{Disjoint Objectives.}~~One might look for formulations more coherent with a multi-agent setting. The most trivial option is to design a disjoint counterpart of the objectives, that means to define a set of functions supported on per-agent state distributions rather than joint distributions. This intuition leads to \emph{Disjoint} objectives: 
\vspace{-2pt}
\begin{align}
         \Big\{ \max_{\pi^i\in \Pi^i} \zeta^i_\infty(\pi^i, \pi^{-i}) &:= \mathcal F(d_i^{\pi^i, \pi^{-i}})\Big\}_{ i \in [\Ns] } \label{eq:mse_decentralized}\\ 
        \Big\{ \max_{\pi^i\in \Pi^i} \zeta^i_K(\pi^i, \pi^{-i}) &:= \E_{d_K\sim p^{\pi^i,\pi^{-i}}_K}\mathcal F (d_{K,i}) \Big\}_{ i \in [\Ns] } \label{eq:mse_finite_decentralized}
\end{align}
\vspace{-10pt}

According to these objectives, each agent will try to maximize her own marginal state entropy separately, neglecting the effect of her actions over others performances. In other words, we expect this objective to hinder the potential coordinated exploration, where one has to take as step down as so allow a better performance overall.
\vspace{-6pt}
\paragraph*{Mixture Objectives.}~At last, we introduce a problem formulation that will be later prove capable of uniquely taking advantage of the structure of the problem. In order to do so, we first introduce the following:

\begin{restatable}[Uniformity]{ass}{mixutre}
    \label{ass:mixture} The agents have the same state spaces, namely $\Ss_i = \Ss_j = \tilde \Ss, \;\forall (i,j) \in \Ns \times \Ns$. \footnote{One should notice that even in CMGs where this is not (even partially) the case, the assumption can be enforced by padding together the per-agent states.}
\end{restatable}
\vspace{-4pt}
Under this assumption, now on we will drop the agent subscript when referring to the per-agent states, and use $\tilde \Ss$ instead. Interestingly, this assumptions allows us to define a particular distribution, namely:
\vspace{-3pt}
\begin{equation*}
    \tilde d^\pi(\tilde s) := \frac{1}{|\Ns|}\sum_{i \in [\Ns]} d^\pi_i(\tilde s) \in \Delta_{\tilde \Ss}.
\end{equation*}
\vspace{-9pt}

We refer to this distribution as \emph{mixture} distribution, given that it is defined as a uniform mixture of the per-agent marginal distributions. Intuitively, it describes the average probability over all the agents to be in a common state $\tilde s \in \tilde \Ss$, in contrast with the joint distribution that describes the probability for them to be in a joint state $s$, or the marginals that describes the probability of each one of them separately. In Figure~\ref{fig:distributions} we provide a visual representation of these concepts.

\begin{figure}[h]
    \centering
    % First image
    \begin{minipage}{\linewidth} % Reduce to 40% of text width
        \centering
        \includegraphics[trim=50 65 50 40, clip,width=0.7\linewidth]{figures/setting.jpeg}
        \caption{\centering The interaction on the \emph{left} induces different (empirical) distributions: marginal distributions for \textcolor{softred}{\textbf{agent 1}} and \textcolor{softgreen}{\textbf{agent 2}} over their respective states; a \textcolor{softblue}{\textbf{joint distribution}} over the product space; a \textcolor{softorange}{\textbf{mixture distribution}} over a common space, defined as the average. The mixture distribution is usually \emph{less sparse}.}
        \label{fig:distributions}
    \end{minipage}
    \vspace{-10pt}
\end{figure}

Similarly to what happens for the joint distribution, one can define the empirical distribution induced by $K$ episodes as $\tilde d_{K} (\tilde s) = \frac{1}{|\Ns|} \sum_{i \in [\Ns]}  d_{K,i}(\tilde s)$ and $\tilde d^\pi = \E_{\tilde d_K\sim p^\pi_K}[\tilde d_K]$. The mixture distribution allows for the definition of the \emph{Mixture} objectives, in their infinite and finite trials formulations respectively:
\vspace{-0.2cm}
\begin{align}
        \max_{\pi = (\pi^i\in \Pi^i)_{i \in [\Ns]}} \ \Big\{ \tilde \zeta_\infty(\pi) &:= \mathcal F (\tilde d^{\pi}) \Big\} \label{eq:mse_mixture}\\
        \max_{\pi = (\pi^i\in \Pi^i)_{i \in [\Ns]}} \ \Big\{ \tilde \zeta_K(\pi) &:= \E_{\tilde d_{K}\sim p^\pi_K}\mathcal F (\tilde d_{K}) \Big\} \label{eq:mse_mixture_finite}
\end{align}

%\mirco{La differenza fra joint e disjoint l'ho trovata chiara, invece fra Joint e Mixture non l'ho tanto capita. Il paragrafo qui sotto non aiuta tantissimo in questo senso. Forse si potrebbe fare una visualizzazione illustrativa che fa vedere le distribuzioni degli agenti e come variano i vari obiettivi}

By employing this kind of objectives in the task-agnostic exploration, i.e. by setting $\mathcal F(\cdot) := H(\cdot)$, two are the possible scenarios of optimal behaviors. In the first scenario, each agent tries to cover her state space as uniformly as possible, without taking into account the presence of others. In this sense, the mixture objectives enforce a behavior similar to the disjoint ones. The second scenario is more interesting and it has been referred to in~\citet{Kolchinsky_2017} for general distributions as the \emph{clustered} scenario: agents will form a grouping such that marginal distributions in the same group are approximately the same, while distributions assigned to different groups will be very different from one another, potentially with a disjoint support. In other words, agents will try to cover different sub-portions of the state space in groups, so that, on average, all the state space will be covered uniformly. This second scenario is of particular interest for task-agnostic exploration, as it is the only one among the one presented that explicitly enforces policies inducing diverse state distributions among the agents.
\vspace{-4pt}
\paragraph*{Remarks.}~Mixture objectives in task-agnostic exploration require estimating the entropy of mixture state-distributions, which might remain challenging in high-dimensional scenarios. Fortunately, the problem of efficiently estimating the entropy for general mixture distributions has been previously investigated in~\citet{Kolchinsky_2017} and extended to RL with mixture policies by~\citet{baram2021maximumentropyreinforcementlearning}. The same ideas might be applied to the case of interest. %We extend their idea to the task-agnostic exploration by introducing a \emph{Disjoint Regularized} finite-trials objective for each agent, namely:%\mirco{Qui mi sembra che tu voglia dire che ti piacerebbe ottimizzare mixture, ma disjoint Ã¨ l'unico veramente praticabile, quindi vuoi collegare i due?}
%\begin{align}  \tilde \zeta^i_K(\pi^i, \pi^{-i}) &:=  \zeta^i_K(\pi^i, \pi^{-i}) - \beta \kl(d_{K,i}\|d_{K,-i}),\end{align}
%where we simplified the notation by setting $\kl(d_{K,i}\|d_{K,-i}) = \E_{d_{K}\sim p^{\pi^i,\pi^{-i}}_K}\kl(d_{K,i}\|d_{K,-i})$. These per-agent objectives are expected to lead to similar behavior of the mixture objectives while keeping the computation of the feedbacks as decentralized as possible, as agents will optimize their own finite-trials entropy while enforcing diverse distributions with respect to others.

% As we have seen, all the previous objectives enforce different behaviors for task-agnostic optimally explorative policies. One may wonder whether these difference can be somehow reduced to another and whether they can be further characterized. We answer these questions in the following section.