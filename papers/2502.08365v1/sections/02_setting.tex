\section{Preliminaries}
\label{sec:setting}
In this section, we introduce the most relevant background and the basic notation.

\textbf{Notation.}~~In the following, we denote $[N] := \{1, 2, \ldots, N\}$ for a constant $N < \infty$. We denote a set with a calligraphic letter $\Acal$ and its size as $|\Acal|$. For a (finite) set $\Acal = \{1,2,\dots, i, \dots\}$, we denote with $-i = \Acal / \{i\}$ the set of all its elements out of the $i$-th one. We denote $\Acal^T := \times_{t = 1}^T \Acal$ the $T$-fold Cartesian product of $\Acal$. The simplex on $\Acal$ is denoted as $\Delta_{\Acal} := \{ p \in [0, 1]^{|\Acal|} | \sum_{a \in \Acal} p(a) = 1 \}$ and $\Delta_{\Acal}^{\Bcal}$ denotes the set of conditional distributions $p: \Acal \to \Delta_\Bcal$. Let $X$ a random variable on the set of outcomes $\Xs$ and corresponding probability measure $p_X$, we denote the Shannon entropy of $X$ as $H (X) = - \sum_{x \in \Xs} p_X (x) \log (p_X (x))$. We denote $\xs = (X_1, \ldots, X_T)$ a random vector of size $T$ and $\xs[t]$ its entry at position $t \in [T]$.

\textbf{Interaction Protocol.}~~As a base model for interaction, we consider finite-horizon Markov Games~\citep[MGs,][]{Littman1994} without rewards. A MG $\MDP := (\Ns, \Ss, \Acal, \Pb, \mu, T)$ is composed of a set of agents $\Ns$, a set $\Ss = \times_{i \in [\Ns]} \Ss_i$ of states, and a set of (joint) actions $\Acal = \times_{i \in [\Ns]} \Acal_i$, which we let discrete and finite with size $|\Ss|, |\Acal|$ respectively. At the start of an episode, the initial state $s_1$ of $\MDP$ is drawn from an initial state distribution $\mu \in \Delta_{\Ss}$. Upon observing $s_1$, each agent takes action $a_1^i \in \Acal_i$, the system transitions to $s_2 \sim \Pb(\cdot|s_1, a_1)$ according to the \emph{transition model} $\Pb \in \Delta^{\Ss}_{\Ss \times \Acal}$. The process is repeated until $s_T$ is reached and $s_T$ is generated, being $T < \infty$ the horizon of an episode. Each agent acts according to a \emph{ policy}, that can be either Markovian, i.e. $\pi^i \in \Delta_{\Ss}^{\Acal^i}$, or Non-Markovian over, i.e. $\pi^i \in \Delta_{\Ss^t\times \Acal^t}^{\Acal^i}$.\footnote{In general, we will denote the set of valid per-agent policies with $\Pi^i$ and the set of joint policies with $\Pi$.} Also, we will denote as \emph{decentralized} policies the ones conditioned on either $\Ss_i$ or $\Ss^t_i\times \Acal^t_i$ for agent $i$, and \emph{centralized} ones the one conditioned over the full state or state-actions sequences. It follows that the joint action is taken according to the \emph{joint} policy $\Delta_{\Ss}^{\Acal}  \ni\pi = (\pi^{i})_{i \in [\Ns]}$. 

\textbf{Induced Distributions.}~~Now, let us denote as $S$ and $S_i$ the random variables corresponding to the joint state and $i$-th agent state respectively. Then the former is distributed as $d^\pi \in \Delta_{\Ss}$, where $d^\pi (s) = \frac{1}{T} \sum_{t \in [T]} Pr (s_t = s|\pi,\mu)$, the latter is distributed as $d^\pi_i \in \Delta_{\Ss_i}$, where $d^\pi_i (s_i) = \frac{1}{T} \sum_{t \in [T]} Pr (s_{t,i} = s_i|\pi,\mu)$. 
Furthermore, let us denote with $\sbf,\as$ the random vectors corresponding to sequences of (joint) states, and actions of length $T$, which are supported in $\Ss^T, \Acal^T$ respectively. We define $p^\pi \in \Delta_{\Ss^T \times \Acal^T}$, where $p^\pi(\sbf,\va) = \prod_{t \in [T]}Pr(s_t = \sbf[t], a_t = \as[t])$. Finally, we denote the empirical state distribution induced by $K \in \mathbb N^+$ trajectories $\{\sbf_k\}_{k \in [K]}$ as $d_K (s) = \frac{1}{KT}  \sum_{k \in [K]} \sum_{t \in [T]}  \mathds{1} (\sbf_k[t] = s)$.

\textbf{Convex MDPs and Task-Agnostic Exploration.}~~Now, in the MDP setting ($N={1}$), the problem of task-agnostic exploration has been cast as a special case of \emph{convex RL} \cite{hazan2019provably, zhang2020variationalpolicygradientmethod,zahavy2023rewardconvexmdps}. In such framework, the general task is defined via an F-bounded concave\footnote{In practice, the function can be either convex, concave, or even non-convex and the term is used to distinguish the objective from the standard (linear) RL objective. In the following, we will assume F is concave if not mentioned otherwise.} utility function $\mathcal F : \Delta_{\Ss} \rightarrow (-\infty,F]$, with $F < \infty$, that is a function of the state distribution $d^\pi$. This allows for a generalization of the standard RL learning objective, which is linear with respect to the state distribution. Usually, some regularity assumptions are enforced on the function $\mathcal F$, the most common being:
\begin{restatable}[Lipschitz]{ass}{lip}
    \label{thr:lip} 
    A function $\mathcal F : \Acal \rightarrow \mathbb R$ is Lipschitz-continuous for some constant $L < \infty$, or L-Lipschitz for short, if it holds
    \vspace{-2pt}
    \begin{equation*}
        |\mathcal F(x) - \mathcal F(y)| \leq L \|x - y\|_1, \; \forall (x,y) \in \Acal^2
    \end{equation*}
\end{restatable}
\vspace{-8pt}
More recently,~\citet{mutti2023challengingcommonassumptionsconvex} noticed that in many practical scenarios only a finite number of $K\in \mathbb N^+$ episodes/trials can be drawn while interacting with the environment, and in such cases one should focus on $d_K$ rather than $d^\pi$. As a result, they contrast the \emph{infinite-trials} objective defined as $\zeta_\infty(\pi) :=\mathcal F(d^\pi)$ with a \emph{finite-trials} one, namely $\zeta_K(\pi) := \E_{d_K\sim p^\pi_K}\mathcal F (d_k)$, noticing that Convex MDPs are characterized by the fact that $\zeta_K(\pi) \leq \zeta_\infty(\pi)$, differently from standard (linear) MDPs for which equality holds. In general, task-agnostic exploration can be easily defined as solving a Convex MDP equipped with an entropy functional~\cite{hazan2019provably}, namely $\mathcal F(d^\pi) := H(d^\pi)$.
