\section{Introduction}
\label{sec:intro}



%\begin{itemize}
%    \item MARL ubiquitous and models real-world and often sample inefficient real worlds have high-dimensionality
%    \item Task-agnostic as a way to reduce burden
%    \item Task-agnostic well defined in single
%    \item Most realistic scenario: single-trial decentralized policies. No-one faced it explicitly.
%    \item RQs: 1. how should we formulate task agnostic in multi-agent? 2. do different formulations have different properties, how are they related? 2*. is it harder or easier of single-agent RL? hardness comes mostly in finite-trials. 3. how can we popularize a practical policy optimization procedure that allows to address multi-agent task-agnostic exploration at scale
%\end{itemize}
Multi-Agent Reinforcement Learning~\citep[MARL,][]{marl-book} recently showed promising results in learning complex behaviors, such as coordination and teamwork~\citep{samvelyan2019starcraft}, strategic planning in the presence of imperfect knowledge~\citep{perolat2022mastering}, and trading~\citep{johanson2022emergentbarteringbehaviourmultiagent}. Just like in single agent RL, however, most of the efforts are focused on tabula rasa learning, that is, without exploiting any prior knowledge gathered from offline data and/or policy pre-training. Despite its generality, learning tabula rasa hinders MARL from addressing real-world situations, where training from scratch every time is slow, often expensive, and most of all unnecessary~\citep{agarwal2022reincarnating}. In this regard, some progress has been made on techniques specific to the multi-agent setting, ranging from ad hoc teamwork~\citep{mirsky2022survey} to zero-shot coordination~\citep{hu2020other}. 

In single-agent RL, task-agnostic exploration, such as maximizing an entropy measure over the state space, was shown to be a useful tool for policy pre-training~\citep{hazan2019provably,mutti2021taskagnosticexplorationpolicygradient} and data collection for offline learning~\citep{yarats2022don}. Recently, the potential of entropy objectives in MARL was empirically corroborated by a plethora of works~\citep{liu2021cooperative, zhang2021made, xu2024population} investigating entropic reward-shaping techniques to boost exploration. Yet, to the best of our knowledge, task-agnostic exploration has never been investigated for multi-agent scenarios explicitly, and, thus, the problem is far from being solved. Let us think of an illustrative example that highlights the central question of this work: multiple autonomous robots deployed in a collapsed building for a rescue operation mission. The robots' main goal is to explore a large area to find and rescue injured humans, where exploration may involve coordinating with others to access otherwise inaccessible areas. Arguably, trying to enforce \emph{all} robots to explore the entire area is inefficient and unnecessary. On the other hand, if everyone is focused on their own exploration, any incentive to collaborate with each other may disappear, especially when coordinating comes at a cost. Clearly, a third option is needed. Thus, many questions naturally arise: (i) \emph{How can task-agnostic exploration be defined in MARL?} (ii) \emph{Are different formulations related in some way and when crucial differences emerge?} (iii) \emph{How can we explicitly address multi-agent task-agnostic exploration in practical scenarios?}

In this paper, we first provide a principled characterization of task-agnostic exploration in multi-agent settings by showing that the problem can take different, yet related, formulations. Specifically, the demarcation line is drawn by whether the agents are trying to \emph{jointly} explore the space, or they neglect the presence of others and explore in a \emph{disjoint} fashion, or, finally, whether they care of being able to explore the space together but as independent components of a \emph{mixture}. We link these cases to three distinct objectives, each of them with specific pros and cons, and possibly leading to different behaviors. First, we formally show that these objectives are strictly related and enjoy similar behaviors in the ideal case of evaluating the agent's performance over infinite realizations (or trials). Then, we shift the attention to the more practical scenario of reaching good performance over a handful, or even just one, trial. This is motivated by the fact that, even if we have access to a simulator to train our policies on many realizations, we often get to deploy them in a fair but limited amount of realizations. Interestingly, we show that different objectives enjoy rather different theoretical behaviors when optimized in such settings. Furthermore, we address the problem of \emph{how} to optimize them, by introducing a decentralized multi-agent policy optimization algorithm, called \emph{Trust Region Pure Exploration} (TRPE), explicitly addressing task-agnostic exploration over finite trials. Additionally, we test the algorithm on an illustrative proof of concept, showing its ability to optimize the objectives of interest, and, more importantly, showing how the diverse landscape of objectives turns out to be crucial for allowing effective coordinated exploration over short time-horizons. In particular, we show how optimizing for \emph{diverse} exploration is not only crucial in practical scenarios, but is perhaps the only way to get relevant outcomes. We strengthen this claim by showing that this superiority is reflected in a higher effectiveness of policy pre-training for sparse-reward multi-agent tasks as well.
\vspace{-8pt}
\paragraph*{Contributions.}~Throughout the paper, we make the following contributions:
\begin{itemize}[itemsep=-1pt, leftmargin=*, topsep=2pt]
    \item We introduce a novel class of decision making problems, called \emph{Convex Markov Games}, and use them to extend the task-agnostic exploration problem to multi-agent settings (Section~\ref{sec:problem_formulation});
    \item We provide a theoretical characterization of the task-agnostic exploration problem, showing how the possible objectives are linked, and in what they are different (Section~\ref{sec:properties});
    \item We design a decentralized trust-region policy search algorithm able to address the exploration task in its most practical formulation (Section~\ref{sec:algorithm});
    \item We report empirical results that confirm the effectiveness of the algorithm in optimizing for their own objectives while highlighting the crucial differences of the different objectives. We showcase the limitations of more common objectives and the potentials of newly established mixture ones (Section~\ref{sec:experiments}).
\end{itemize}