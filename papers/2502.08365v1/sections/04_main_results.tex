\section{A Formal Characterization of Multi-Agent Task-Agnostic Exploration}
\label{sec:properties}

In the previous section, we described how different objectives enforce different behaviors for task-agnostic explorative policies. In this section, we address the second research question:
\begin{center}
    %\vspace{-0.2cm}
    (ii) \emph{Are different formulations related
in some way and when crucial differences emerge?}
    %\vspace{-0.2cm}
\end{center}

First of all, we show that if we look at task-agnostic exploration tasks, i.e. the ones defined by setting the functional $\mathcal F(\cdot) := H(\cdot)$, all the objectives in infinite-trials formulation can be elegantly linked one to the other though the following result:

\begin{restatable}[Entropy Mismatch]{lem}{entropymismatch}
    \label{lem:entropymismatch} 
    For every Convex Markov Game $\mathcal M_{H}$ equipped with an entropy functional, for a fixed (joint) policy $\pi = (\pi^{i})_{i \in \Ns}$ the infinite-trials objectives are ordered according to:
    \begin{align*}
        \frac{H(d^\pi)}{|\Ns|} &\leq \frac{1}{|\Ns|}\sum_{i \in [\Ns]}H(d_i^\pi)  \leq H(\tilde d^\pi)  \\
        H(\tilde d^\pi) \leq \sup_{i \in [\Ns]}&H(d_i^\pi) + \log(|\Ns|) \leq  H(d^\pi) + \log(|\Ns|)
    \end{align*}
\end{restatable}
The full derivation of these bounds is reported in Appendix~\ref{apx:proof}. This set of bounds prescribe that the difference in performances over infinite-trials objective for the same policy can be generally bounded as a function of the number of agents. In particular, disjoint objectives generally provides poor approximations of the joint objective from the point of view of the single-agent, while the mixture objective is guaranteed to be a rather good lower bound to the joint entropy as well, since its over-estimation scales logarithmically with the number of agents.

It is still an open question how hard it is to actually optimize for these objectives. Now, while CMGs are a novel interaction framework, whose general properties are far from being well-understood, they surely enjoy some nice properties. In particular, as commonly done in Potential Markov Games~\citep{leonardos2021globalconvergencemultiagentpolicy}, it is possible to exploit the fact that performing Policy Gradient~\citep[PG,][]{sutton1999policy, peters2008reinforcement} independently among the agents is equivalent to running PG jointly, when this is done over the same common objective (Appendix~\ref{apx:theory}, Lemma~\ref{claim:projection}). This allows us to provide a rather positive answer, here stated informally and extensively discussed in Appendix~\ref{apx:theory} :

\begin{restatable}[(Informal) Sufficiency of Independent Policy Gradient]{fact}{sufficiencypga}
    \label{fact:sufficiencypga} 
    Under proper assumptions, for every CMG $\mathcal M_{\mathcal F}$, independent Policy Gradient over infinite trials non-disjoint objectives via centralized policies of the form $\pi = (\pi^i\in \Delta_\Ss^{\Acal^i})_{i \in [\Ns]}$ converges \emph{fast}.
\end{restatable}

This result suggests that PG should be generally enough for the infinite-trials optimization, and thus, from a certain point of view, these problems might not be of so much interest. However, convex MDP theory has outlined that optimizing for infinite-trials objectives might actually lead to extremely poor performances as soon as the policies are deployed over just a handful of trials, i.e. in almost any practical scenario~\citep{2023mutticonvexrlfinite}. We show that this property transfers almost seamlessly to CMGs as well, with interesting additional take-outs:

% Additionally, it is possible to show that the different objectives enjoy different properties when addressing the infinite-trials joint objective instead of the finite-trials one. This in fact leads to approximation errors in MDPs, due to non-linearity of Eq.~\eqref{eq:mse_finite} and the sampling-based nature of the problem. In CMGs, these errors are linked to the number of agents as well, as a direct consequence of the following:

\begin{restatable}[Objectives Mismatch in CMGs]{thr}{objectivemismatch}
    \label{thr:objectivemismatch} 
    For every CMG $\mathcal M_{\mathcal F}$ equipped with a $L$-Lipschitz function $\mathcal F$, let $K \in \mathbb N^+$ be a number of evaluation episodes/trials, and let $\delta \in (0, 1]$ be a confidence level, then for any (joint) policy $\pi = (\pi^i\in \Pi^i)_{i \in [\Ns]}$, it holds that
    \begin{equation*}
        |\zeta_K(\pi) - \zeta_\infty(\pi)| \leq  LT \sqrt{\frac{2 |\Ss| \log(2T/\delta)}{K}} ,
    \end{equation*}
    \begin{equation*}
        |\zeta^i_K(\pi) - \zeta^i_\infty(\pi)| \leq  LT \sqrt{\frac{2|\tilde \Ss| \log(2T/\delta)}{K}},
    \end{equation*}
    \begin{equation*}
        |\tilde \zeta_K(\pi) - \tilde \zeta_\infty(\pi)| \leq  LT \sqrt{\frac{2|\tilde \Ss| \log(2T/\delta)}{|\Ns|K}}. 
    \end{equation*}
\end{restatable}

In general, this set of bounds confirms that infinite and finite trials objectives might be extremely different, and thus optimizing the infinite-trials objective might lead to unpredictable performance at deployment, whenever this is done over a handful of trials. %\footnote{Yet, we notice that while it is possible to link globally optimal policies, this is not the case for general policies in the set of Nash Equilibria (NE)~\cite{nash51equilibria}.}. 
This property is inherently linked to the \emph{convex} nature of convex MDPs, and \citet{2023mutticonvexrlfinite} introduces it to highlight that the concentration properties of empirical state-distributions~\cite{Weissman2003InequalitiesFT} allow for a nice dependency on the number of trials in controlling the mismatch. In multi-agent settings, the result portraits a more nuanced scene:\\
\emph{(i)}~The mismatch still scales with the cardinality of the support of the state distribution, yet, for joint objectives, this quantity scales very poorly in the number of agents.\footnote{Indeed, in the case of product state-spaces $\Ss = \times_{i \in [\Ns]} \Ss_i$ the cardinality scales exponentially with the number of agents $|\Ns|$} Thus, even though optimizing infinite-trials joint objectives might be rather easy \emph{in theory} as Fact~\ref{fact:sufficiencypga} suggests, it might result in poor performances \emph{in practice}. On the other hand,  the quantity is independent of the number of agents for disjoint and mixture objectives.\\
\emph{(ii)}~Looking at mixture objectives, the mismatch scales sub-linearly with the number of agents $\Ns$. Thus, in some sense, the number of agents has the same role as the number of trials: the more the agents the less the deployment mismatch, and at the limit, with $\Ns \rightarrow \infty$, the mismatch vanishes completely.\footnote{One should note that in this scenario, though, all the bounds of Lemma~\ref{lem:entropymismatch} linking different objectives become vacuous.} In other words, this result portraits a striking difference with respect to joint objectives: when facing task-agnostic exploration over mixtures, a reasonably high number of agents compared to the size of the state-space actually helps, and simple policy gradient over mixture objectives might be enough. 
\vspace{-7pt}
\paragraph*{Remarks.}~One should notice that the results of Fact~\ref{fact:sufficiencypga} are valid only for specific classes of policies, namely \emph{centralized} policies of the form $\pi = (\pi^i\in \Delta_\Ss^{\Acal^i})_{i \in [\Ns]}$. Up to our knowledge, no guarantees are known for \emph{decentralized} policies even in linear MGs. Interestingly though, the finite-trials formulation do offer additional insights on the behavior of optimal decentralized policies, a striking difference with respect to both the infinite-trial objectives and the linear MG interaction model in general. The interested reader can learn more about this in Appendix~\ref{apx:policies}.