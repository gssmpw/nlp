\section{Proof of Concept Experiments}
\label{sec:experiments}

%\begin{itemize}
%    \item joint exploration non e' spesso un opzione
%    \item specificare che le policy sono decentralizzate a differenza di tutti i casi precedenti
%    \item decentralizzata con feedback decentralizzato non si coordina e il problema e' abbastanza semplice da portare a policy quasi deterministiche
%\end{itemize}



%\mirco{questo primo paragrafo Ã¨ un po' convoluto. Prova a ristruttura la sezione in questo modo: quali sono le domande a cui cerchiamo risposta? Quali sono i domini sperimentali? Quali sono gli algoritmi che compariamo? Quali sono i take away? Per l'ultimo potresti anche evidenziare qualche frase in grassetto o emph con le principali conclusioni empiriche}

In this section, we provide some empirical validations of the findings discussed so far. Especially, we aim to answer the following questions: (\textbf{a}) Is Algorithm~\ref{alg:trpe} actually capable of optimizing finite-trials objectives? (\textbf{b}) Do different objectives enforce different behaviors, as expected from Section~\ref{sec:problem_formulation}? (\textbf{c}) Does the \emph{clustering} behavior of mixture objectives play a crucial role? If yes, when and why?\\
Throughout the experiments, we will compare the result of optimizing finite-trial objectives, either joint, disjoint, mixture ones, through Algorithm~\ref{alg:trpe} via fully decentralized policies. The experiments will be performed with different values of the exploration horizon $T$, so as to test their capabilities in different exploration efficiency regimes.\footnote{The exploration horizon $T$, rather than being a given trajectory length, has to be seen as a parameter of the exploration phase which allows to tradeoff exploration quality with exploration efficiency.} The full implementation details are reported in Appendix~\ref{apx:exp}.
\vspace{-6pt}
\paragraph*{Experimental Domains.}~The experiments were performed on two domains. The first is a notoriously difficult multi-agent exploration task called \emph{secret room}~\citep[MPE,][]{pmlr-v139-liu21j},\footnote{We highlight that all previous efforts in this task employed centralized policies. We are interested on the role of the entropic feedback in fostering coordination rather than full-state conditioning, then maintaining fully decentralized policies instead.} referred to as  Env.~(\textbf{i}). In such task, two agents are required to reach a target while navigating over two rooms divided by a door. In order to keep the door open, at least one agent have to remain on a switch. Two switches are located at the corners of the two rooms. The hardness of the task then comes from the need of coordinated exploration, where one agent allows for the exploration of the other. The second is a simpler exploration task yet over a high dimensional state-space, namely a 2-agent instantiation of \emph{Reacher}~\citep[MaMuJoCo,][]{peng2021facmac}, referred to as Env.~(\textbf{ii}). Each agent corresponds to one joint and equipped with decentralized policies conditioned on her own states. In order to allow for the use of plug-in estimator of the entropy~\citep{paninski2003}, each state dimension was discretized over 10 bins.


\begin{figure*}[!]
    \centering
    \input{figures/pretraining_legend.tex}
    %\hfill
    \vfill
    %vspace{-0.2cm}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/room_150_AverageReturnnokl.pdf}
        %\vspace{-0.8cm}
        \caption{\centering MA-TRPO with TRPE Pre-Training (Env.~(\textbf{i}), $T=150$).}
        \label{subfig:image9}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/room_50_AverageReturnnokl.pdf}
        %\vspace{-0.8cm}
        \caption{\centering MA-TRPO with TRPE Pre-Training (Env.~(\textbf{i}), $T=50$).}
        \label{subfig:image10}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/hand_100_AverageReturn.pdf}
        %\vspace{-0.8cm}
        \caption{\centering MA-TRPO with TRPE Pre-Training (Env.~(\textbf{ii}), $T=100$).}
        \label{subfig:image11}
    \end{subfigure}
\caption{\centering Effect of pre-training in sparse-reward settings.(\emph{left}) Policies initialized with either Uniform or TRPE pre-trained policies over 4 runs over a worst-case goal. (\emph{rigth}) Policies initialized with either Zero-Mean or TRPE pre-trained policies over 4 runs over 3 possible goal state. We report the average and 95\% c.i.}
\label{fig:pretraining}
\end{figure*}
\vspace{-10pt}
\paragraph*{Task-Agnostic Exploration.}~Algorithm~\ref{alg:trpe} was first tested in her ability to address task-agnostic exploration \emph{per se}. This was done by considering the well-know hard-exploration task of Env.~(\textbf{i}). The results are reported in Figure~\ref{fig:room} for a short exploration horizon $(T=50)$. Interestingly, at this efficiency regime, when looking at the joint entropy in Figure~\ref{subfig:image2}, joint and disjoint objectives perform rather well compared to mixture ones in terms of induced joint entropy, while they fail to address mixture entropy explicitly, as seen in Figure~\ref{subfig:image3}. On the other hand mixture-based objectives result in optimizing both mixture \emph{and} joint entropy effectively, as one would expect by the bounds in Th.~\ref{lem:entropymismatch}. By looking at the actual state visitation induced by the trained policies, the difference between the objectives is apparent. While optimizing joint objectives, agents exploit the high-dimensionality of the joint space to induce highly entropic distributions even without exploring the space uniformly via coordination (Fig.~\ref{subfig:image5}); the same outcome happens in disjoint objectives, with which agents focus on over-optimizing over a restricted space loosing any incentive for coordinated exploration (Fig.\ref{subfig:image6}). On the other hand, mixture objectives enforce a clustering behavior (Fig.\ref{subfig:image6}) and result in a better efficient exploration. 

\paragraph*{Policy Pre-Training via Task-Agnostic Exploration.}~More interestingly, we tested the effect of pre-training policies via different objectives as a way to alleviate the well-known hardness of sparse-reward settings, either throught faster learning or zero-short generalization. In order to do so, we employed a multi-agent counterpart of the TRPO algorithm~\cite{schulman2017trustregionpolicyoptimization} with different pre-trained policies. First, we investigated the effect on the learning curve in the hard-exploration task of Env.~(\textbf{i}) under long horizons ($T=150$), with a worst-case goal set on the the opposite corner of the closed room. Pre-training via mixture objectives still lead to a faster learning compared to initializing the policy with a uniform distribution. On the other hand, joint objective pre-training did not lead to substantial improvements over standard initializations. More interestingly, when extremely short horizons were taken into account ($T=50$) the difference became appalling, as shown in Fig.~\ref{subfig:image9}: pre-training via mixture-based objectives leaded to faster learning and higher performances, while pre-training via disjoint objectives turned out to be even \emph{harmful} (Fig.~\ref{subfig:image10}). This was motivated by the fact that the disjoint objective overfitted the task over the states reachable without coordinated exploration, resulting in almost deterministic policies, as shown in Fig~\ref{fig:333} in Appendix~\ref{apx:exp}. Finally, we tested the zero-shot capabilities of policy pre-training on the simpler but high dimensional exploration task of Env.~(\textbf{ii}), where the goal was sampled randomly between worst-case positions at the boundaries of the region reachable by the arm. As shown in Fig.~\ref{subfig:image11}, both joint and mixture were able to guarantee zero-shot performances via pre-training compatible with MA-TRPO after learning over $2$e$4$ samples, while disjoint objectives were not. On the other hand, pre-training with joint objectives showed an extremely high-variance, leading to worst-case performances not better than the ones of random initialization. Mixture objectives on the other hand showed higher stability in guaranteeing compelling zero-shot performance.
\vspace{-6pt}
\paragraph*{Take-Aways.}~Overall, the proposed proof of concepts experiments managed to answer to all of the experimental questions: (\textbf{a}) Algorithm~\ref{alg:trpe} is indeed able to explicitly optimize for finite-trial entropic objectives. Additionally, (\textbf{b}) \textbf{mixture distributions enforce diverse yet coordinated exploration}, that helps when high efficiency is required. Joint or disjoint objectives on the other hand may fail to lead to relevant solutions because of under or over optimization. Finally, (\textbf{c}) \textbf{efficient exploration} enforced by mixture distributions was shown to be a \textbf{crucial factor} not only for the sake of task-agnostic exploration per se, but also for the ability of \textbf{pre-training via task-agnostic exploration} to lead to \textbf{faster and better training} and even \textbf{zero-shot generalization}.