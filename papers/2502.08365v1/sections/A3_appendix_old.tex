\section{Proofs of Algorithms Section}
\label{apx:proof2}



\begin{theorem}[Convergence of PGA]\label{thm:mainformal} Let $\MDP$ be a Potential Convex Markov Game and consider an arbitrary initial state. Set the number of iterations to be $T=\frac{16n\gamma D^2 S A_{\max} \mathcal F_{\max}}{(1-\gamma)^5\epsilon^2}$ and the learning rate (step-size) to be $\eta = \frac{(1-\gamma)^3}{2n\gamma A_{\max}}$. If the agents run independent projected policy gradient (PGA) starting from arbitrarily initialized policies, then there exists a $t\in \{1,\dots,T\}$ such that $\pi_t$ is an $\epsilon$-approximate Nash policy.
\end{theorem}

\begin{proof}


The first step is to show that $\mathcal F$ is a $L$-smooth function, that is $\nabla_{\pi}\mathcal F(d^\pi)$ is $L$-Lipschitz. As established in \Cref{lemma:smoothness} $L=4T\log(\Ss)$. It follows that a standard \emph{Ascent Lemma} for Gradient Ascent reported in \Cref{lem:descent} from \citet{bubeck} can be used to guarantee that for any $L$-smooth function $f$ it holds that $f(x') - f(x) \geq \frac{1}{2L} \norm{x'-x}^2_2$ where $x'$ is the next iterate of \eqref{eq:pga}. Applied to our setting, this gives

\begin{equation}\label{eq:ascent}
    \mathcal F(\pi^{(t+1)}) - \mathcal F(\pi^{(t)}) \geq \frac{1}{2L}\norm{\pi^{(t+1)}-\pi^{(t)}}_2^2
\end{equation}

Thus, if the number of iterates, $T$, is $\frac{16n\gamma D^2 S A_{\max}  }{(1-\gamma)^5\epsilon^2}$, then there must exist a $1 \leq t\leq T$ so that $\norm{\pi^{(t+1) }-\pi^{(t)}}_2 \leq \frac{\epsilon(1-\gamma)}{2D\sqrt{S}}$.

%\paragraph{(iii) Approximation Lemma and Stationarity implies NE Lemma.}
We can employ \Cref{lem:approxsmooth}, that consists in a standard approximation property to conclude that $\pi^{(t+1)}$ will be a $\frac{\epsilon(1-\gamma)}{D\sqrt{S}}$-stationary point for the (potential) function $\mathcal F$. Hence, by \Cref{lem:stationary}, it follows that $\pi^{(t+1)}$ is an $\epsilon$-Nash policy concluding the proof.
\end{proof}


\begin{lemma}[Smoothness of $\mathcal F$]\label{lemma:smoothness} 
    Blabla
\end{lemma}

\begin{lemma}[Projection Operator]\label{claim:projection} Let $\pi := (\pi^i)_{i \in [\Ns]}$ be the (joint) policy and let
\begin{equation}
    \pi' = \pi + \eta \nabla_\pi\mathcal F(\pi),
\end{equation}
be a gradient step on the function $\mathcal F$ for a step-size $\eta>0$. Then, it holds that
\begin{equation}
    P_{\Delta^\Acal_\Ss} (\pi') = (P_{\Delta^{\Acal_1}_\Ss} (\pi'_1) , \dots , P_{\Delta^{\Acal_N}_\Ss}(\pi'_N)).
\end{equation}
where for any set $\mathcal{X}\subseteq \mathbb R^n$ the projection operator is defined as $P_{\mathcal{X}}(y) = \argmin_{x \in\mathcal{X}}  \norm{x-y}_2^2.$ This means that running independent PGA is equivalent to running PGA jointly.
\end{lemma}

\begin{proof}
    \begin{align*}
    P_{\Delta^\Acal_\Ss} (\pi') &= \argmin_{x \in \Delta^\Acal_\Ss}  \norm{x-\pi'}_2^2= \argmin_{x_1 \in \Delta^{\mathcal{A}^1}_S,...,x_N \in 
    \Delta^{\mathcal{A}^N}_\Ss} \sum_{i\in [\Ns]}\norm{x_i - \pi'_i}_2^2\\
    &= \sum_{i \in [\Ns]} \argmin_{x_i \in \Delta^{\mathcal{A}^i}_\Ss} \norm{x_i - \pi'_i}_2^2= (P_{\Delta^{\Acal^1}_\Ss} (\pi'_1) , \dots , P_{\Delta^{\Acal^N}_\Ss}(\pi'_N)).\qedhere
    \end{align*}
\end{proof}

\begin{definition}[$\epsilon$-Stationary Point of $\mathcal F$]\label{def:fostationary} 
    A (joint) policy $\pi := (\pi^i)_{i \in [\Ns]} \in \Delta^{\mathcal{A}}_\Ss$ is called $\epsilon$-stationary for $\mathcal F$ iff 
    \begin{equation}     
        \max_{(\pi_1 + \delta_1,\dots,\pi_n+\delta_n) \in \Delta^\mathcal{A}_\Ss,  \sum_{i \in \mathcal{N}}\norm{\delta_i}^2_2 \leq 1 } \sum_{i \in \mathcal{N}} \delta_i^{\top} \nabla_{\pi_i} \mathcal F(\pi) \leq \epsilon 
    \end{equation}
    This means that $\mathcal F(\pi)$ cannot increase in value by more than $\epsilon$ along every possible local direction $\delta=(\delta_1,\dots,\delta_n)$ that is feasible, i.e. $\pi+\delta \in \Pi$.
\end{definition} 

\begin{lemma}[Stationarity of $\mathcal F$ implies Nash]\label{lem:stationary} Let $\epsilon \geq 0$, and let $\pi$ be an $\epsilon$-stationary point of $\mathcal F$ according to~\Cref{def:fostationary}. Then, $\pi$ is also a $\frac{\sqrt{S}D\epsilon}{1-\gamma}$-Nash policy.
\end{lemma}

\begin{proof}[Proof of \Cref{lem:stationary}]
Fix agent $i$ and suppose that $i$ deviates to the best-response policy $\pi^*_i$ w.r.t $\pi$. Since $\pi$ is $\epsilon$-stationary it holds that (Definition \ref{def:fostationary})
\begin{equation}\label{eq:helpm}
\max_{\pi'_i \in \Delta^{\mathcal{A}^i}_\Ss}(\pi'_i - \pi_i)^{\top} \nabla_{\pi_i} \mathcal F(\pi) \leq \epsilon.
\end{equation}
Thus, with $\pi^* = (\pi^*_i,\pi_{-i})$, \Cref{lem:gdom} implies that
\begin{equation}\label{eq:almost}
\begin{split}
\mathcal F(\pi^*)-\mathcal F(\pi)&\le \max_{\pi'=(\pi'_i,\pi_{-i})}(\pi'-\pi)^\top \nabla_{\pi}\mathcal F(\pi)
\\&\stackrel{(\ref{eq:helpm})}{\leq}\frac{D}{1-\gamma}\sqrt{S}\epsilon.
\end{split}
\end{equation}
Thus, using the definition of the potential function (cf. \Cref{def:potential}), we obtain that
\[
V^i_{\rho}(\pi^*)-V^i_{\rho}(\pi) =\mathcal F_{\rho}(\pi^*)-\mathcal F_{\rho}(\pi) \leq \frac{\sqrt{S}D \epsilon}{1-\gamma}\,.
\]
Since the choice of $i$ was arbitrary, we conclude that $\pi$ is an $\frac{\sqrt{S}D \epsilon}{1-\gamma}$-approximate Nash policy.
\end{proof}

\begin{lemma}[Agent-wise Gradient Domination Property in Convex MG]\label{lem:gdom}
    Let $\MDP$ be a MPG with convex potential function $\mathcal F$, fix any agent $i \in \mathcal{N}$, and let $\pi=(\pi_i,\pi_{-i})\in \Delta_\Acal^\Ss$ be a policy. Let $\pi_i^\star$ be an optimal policy for agent $i$ in the single agent MDP in which the rest of the agents are fixed to choose $\pi_{-i}.$ Then, for the policy $\pi^\star=(\pi^\star_i,\pi_{-i}) \in \Delta_\Acal^\Ss$ that differs from $\pi$ only in the policy component of agent $i$, it holds that
    \begin{equation}
        \mathcal F(\pi^\star)-\mathcal F(\pi)\le \frac{1}{1-\gamma}\left \|\frac{d^{\pi^\star}}{\mu}\right\|_{\infty}\max_{\pi'=(\pi'_i,\pi_{-i})}(\pi'-\pi)^\top \nabla_{\pi}\mathcal F(\pi)
    \end{equation}
\end{lemma}



\begin{lemma}[\cite{bubeck}, Lemma 3.6]\label{lem:descent} 
    Let $f$ be a $L$-smooth function, i.e. differentiable with $\nabla f$ being $L$-Lipschitz, with convex domain $\mathcal{X}$. Let $x\in \mathcal{X}$, $x^{+} = P_{\mathcal{X}} (x - \frac{1}{L} \nabla f(x))$ and $g_{\mathcal{X}}(x) = L(x - x^{+})$. Then the following holds true:
    \begin{equation}
        f(x^{+})-f(x) \leq -\frac{1}{2L}\norm{g_{\mathcal{X}}(x)}_2^2.
    \end{equation}
\end{lemma}


\noregret*
\begin{proof}
    Define the Nash Regret after $T$ episodes $\mathcal R(T)$ of a joint policy sequence $\{\tilde \pi_t \}$ and the $\pi_i^\star = \max_{\pi_i}\mathcal F (d^{\pi_i, \tilde \pi_{t,-i}})$ we get that
    \begin{align*}
        \mathcal R(T) &= \sum_t^T \max_i (\mathcal F (d^{\pi_i^\star, \tilde \pi_{t,-i}}) - \mathcal F(d^{\tilde \pi_{t}}))\\
        &\leq \sum_t^T\sum_i \mathcal F (d^{\pi_i^\star, \tilde \pi_{t,-i}}) - \mathcal F(d^{\tilde \pi_{t,i},\tilde \pi_{t,-i}})\quad  \\
        &\leq  \sum_t \sum_i \max_{\pi_i'}\langle\nabla_{\pi_i}\mathcal F (d^{\tilde \pi_{t}}), \pi_i'  - \tilde \pi_{t,i}\rangle \text{  Concavity}\\% + \frac{\beta}{2} \|\pi_i'  - \tilde \pi_{t,i}\|_2^2\quad \text{Smoothness} \\
        &\leq  \sum_t N \max_{\pi'}\langle\nabla_{\pi}\mathcal F (d^{\tilde \pi_{t}}), \pi'  - \tilde \pi_{t}\rangle \\%+\sum_i  \max_{\pi_i'}\frac{\beta}{2} \|\pi_i'  - \tilde \pi_{t,i}\|_2^2 \\
        &\leq \sum_t N \mathcal G(\tilde \pi_{t})\\ %+ \sum_i \max_{\pi_i'} \frac{\beta}{2} \|\pi_i'  - (1-\eta)\nu \pi_{t,i}\|_2^2 + \max_{\pi_i'} \frac{\beta}{2} \|\pi_i'  - (1-\eta)(1-\nu) \tilde \pi_{t-1,i}\|_2^2 + \max_{\pi_i'} \frac{\beta}{2} \|\pi_i'  - \eta \rho_i\|_2^2\\
        %&\leq \sum_t N \mathcal G(\tilde \pi_{t})% +  ((1-\eta)^2\nu^2 + (1-\eta)^2(1-\nu)^2 + \eta^2)\beta NT 
        %&\leq \sum_t\sum_i \mathcal G(\tilde \pi_{t}) + \max_{\pi_i'} \frac{\beta}{2} \|\pi_i'  - (1-\eta)\nu \pi_{t,i}\|_2^2 + \max_{\pi_i'} \frac{\beta}{2} \|\pi_i'  - (1-\eta)(1-\nu) \tilde \pi_{t-1,i}\|_2^2 + \max_{\pi_i'} \frac{\beta}{2} \|\pi_i'  - \eta \rho_i\|_2^2\\
        %&\leq \sum_t\sum_i \mathcal G_i(\tilde \pi_{t}) + \frac{\beta}{2} ((1-\eta)^2\nu^2 + (1-\eta)^2(1-\nu)^2 + \eta^2)NT 
    \end{align*}
    where we introduced the  Frank-Wolfe Gap $\mathcal G(\tilde \pi_{t})= \max_{\pi}\langle\nabla_{\pi}\mathcal F (d^{\tilde \pi_{t}}), \pi  - \tilde \pi_{t}\rangle$, which is actually what FW is controlling for, and used the fact that $\tilde \pi_{t,i} = (1-\eta)\nu \pi_{t,i} + (1-\eta)(1-\nu) \tilde \pi_{t-1,i} + \eta \rho_i$. Then we notice that taking into account two generic iterations of the algorithm we can use the smoothness of the functional again along the agent i-th direction:
    \begin{align*}
         F(d^{\tilde \pi_{t+1}}) -F(d^{\tilde \pi_{t}}) 
        &\geq  \langle\nabla_{\pi}\mathcal F (d^{\tilde \pi_{t}}),  \tilde \pi_{t+1}-\tilde \pi_{t}\rangle - \frac{\beta}{2}\|\tilde \pi_{t +1} - \tilde \pi_{t} \|_2^2\quad \text{Smoothness}
    \end{align*}
    We recall again that per each agent $\tilde \pi_{t,i} = (1-\eta)\nu \pi_{t,i} + (1-\eta)(1-\nu) \tilde \pi_{t-1,i} + \eta \rho_i$ so that we can state that 
    \begin{align*}
         F(d^{\tilde \pi_{t+1}}) -F(d^{\tilde \pi_{t}}) 
         &\geq (1-\eta)\nu  \langle\nabla_{\pi}\mathcal F (d^{\tilde \pi_{t}}), \pi_{t+1}- \tilde \pi_{t}\rangle -\eta  \langle\nabla_{\pi}\mathcal F (d^{\tilde \pi_{t}}),  \tilde \pi_{t}-\rho\rangle - \frac{\beta}{2} \| \tilde \pi_{t+1}-\tilde \pi_{t} \|_2^2 \\
         &\text{$\pi_{t+1,i} = \argmax_{\pi_i'}\langle \nabla_{\pi_i}\mathcal F (d^{\tilde \pi_{t}}), \pi_i'\rangle$, then $ \langle\nabla_{\pi}\mathcal F (d^{\tilde \pi_{t}}),  \pi_{ t+1}-\tilde \pi_{t}\rangle = \mathcal G(\tilde \pi_{t})$} \\
        &\geq (1-\eta)\nu \mathcal G(\tilde \pi_{t}) -\eta \|\nabla_{\pi}\mathcal F (d^{\tilde \pi_{t}}) \|_\infty +\beta ( (1-\eta)^2\nu^2 + \eta^2) \quad \text{$\|x-\alpha y\|_2^2\leq 2\alpha^2 \|x- y\|_2^2$}\\
        &\geq (1-\eta)\nu \mathcal G_i(\tilde \pi_{t}) -\eta \beta+\beta ( (1-\eta)^2\nu^2 + \eta^2)\quad \text{The gradient is $\beta$-smooth and bounded} 
    \end{align*}
    And as a result:
    \begin{align*}
         \sum_t \mathcal G(\tilde \pi_{t}) &\leq  \frac{1}{(1-\eta)\nu }\sum_t ( F(d^{\tilde \pi_{t+1}}) -F(d^{\tilde \pi_{t}}) ) + \frac{\eta \beta-\beta ( (1-\eta)^2\nu^2 + \eta^2)}{(1-\eta)\nu }T \\
         &\leq  \frac{\log(S)}{(1-\eta)\nu } + \frac{\eta \beta-\beta ( (1-\eta)^2\nu^2 + \eta^2)}{(1-\eta)\nu }T
    \end{align*}
    From which it follows that
    \begin{align*}
        \mathcal R(T) &\leq  \frac{N\log(S)}{(1-\eta)\nu } + \frac{\eta \beta-\beta ( (1-\eta)^2\nu^2 + \eta^2)}{(1-\eta)\nu }NT % +  ((1-\eta)^2\nu^2 + (1-\eta)^2(1-\nu)^2 + \eta^2)\beta NT 
    \end{align*}
    By setting $\nu = \frac{1}{\sqrt{T}}$ and $\eta = \frac{1}{T}$ we can get that 
    \begin{align*}
        \mathcal R(T) &\leq  \mathcal O (\beta N\log(S) \sqrt{T})
    \end{align*}
\end{proof}


\begin{proof}
We start by defining $\epsilon$-Stationary Point of $\Phi$:
A policy profile $\pi := (\pi_1,...,\pi_n) \in \Delta(\mathcal{A})^S$ is called $\epsilon$-stationary for $\mathcal F(\pi) = \mathcal F(d^\pi)$ 
\begin{equation}     \max_{(\pi_1 + \delta_1,\dots,\pi_n+\delta_n) \in \Delta(\mathcal{A})^S,  \sum_{i \in \mathcal{N}}\norm{\delta_i}^2_2 \leq 1 } \sum_{i \in \mathcal{N}} \delta_i^{\top} \nabla_{\pi_i} \mathcal F(\pi) \leq \epsilon \end{equation} 

Then we can state the following:
\begin{restatable}[Stationarity of $\mathcal F$ implies Nash]{thr}{stationarity}
\label{thr:stationarity}
 Let $\epsilon \geq 0$, and let $\pi$ be an $\epsilon$-stationary point of $\Phi$. Then, it holds that $\pi$ is an approximate Nash policy.
\end{restatable}

To prove this lemma, we will need a Gradient Domination property specific for Convex (Potential) Games.

\begin{restatable}[Agent-wise Gradient Domination Property]{thr}{gradientdomination}
Let $\G$ be a Convex MPG with potential function $\mathcal F$, fix any agent $i \in \mathcal{N}$, and let $\pi=(\pi_i,\pi_{-i})\in \Delta(\mathcal A)^S$ be a policy. Then, for the policy $\pi'=(\pi'_i,\pi_{-i}) \in \Delta(\mathcal A)^S$ that differs from $\pi$ only in the policy component of agent $i$ obtained by performing PSGA, it holds that
\begin{equation}
\mathcal F(\pi')-\mathcal F(\pi)\le \frac{1}{2}\max_{\tilde \pi}(\tilde \pi-\pi)^\top \nabla_{\pi_i}\mathcal F(\pi),
\end{equation}
\end{restatable}

First, notice that by $\beta$-smoothness of the function $\mathcal F$ it holds that

\begin{equation}
    \mathcal F(\pi')-\mathcal F(\pi)\le ( \pi'-\pi)^\top \nabla_{\pi_i}\mathcal F(\pi) + \frac{\beta}{2}\|\pi' - \pi\|^2
\end{equation}
Now, if we consider deviations over policies induced by PSGA, then we know that the following holds (\cite{bubeck2015convexoptimizationalgorithmscomplexity} Lemma 3.6):

\begin{equation}
    \mathcal F(\pi')-\mathcal F(\pi)\le ( \pi'-\pi)^\top \nabla_{\pi_i}\mathcal F(\pi) + \frac{\beta}{2}\|\pi' - \pi\|^2
\end{equation}

Fix agent $i$ and suppose that $i$ deviates to a policy $\pi'_i$. By assumption $\pi$ is $\epsilon$-stationary and it holds that 
\begin{equation}
\max_{\pi'_i \in \Delta(\mathcal{A}_i)^S}(\pi'_i - \pi_i)^{\top} \nabla_{\pi_i} \Phi_{\mu}(\pi) \leq \sqrt{S}\epsilon.
\end{equation}
Thus, with $\pi^* = (\pi^*_i,\pi_{-i})$, \Cref{lem:gdom} implies that
\begin{equation}\label{eq:almost}
\begin{split}
\Phi_{\rho}(\pi^*)-\Phi_{\rho}(\pi)&\le \frac{1}{1-\gamma}\left \|\frac{d^{\pi^*}_\rho}{\mu}\right\|_{\infty}\max_{\pi'=(\pi'_i,\pi_{-i})}(\pi'-\pi)^\top \nabla_{\pi_i}\Phi_{\mu}(\pi)
\\&\stackrel{(\ref{eq:helpm})}{\leq}\frac{D}{1-\gamma}\sqrt{S}\epsilon.
\end{split}
\end{equation}
Thus, using the definition of the potential function (cf. \Cref{def:potential}), we obtain that
\[
V^i_{\rho}(\pi^*)-V^i_{\rho}(\pi) =\Phi_{\rho}(\pi^*)-\Phi_{\rho}(\pi) \leq \frac{\sqrt{S}D \epsilon}{1-\gamma}\,.
\]
Since the choice of $i$ was arbitrary, we conclude that $\pi$ is an $\frac{\sqrt{S}D \epsilon}{1-\gamma}$-approximate Nash policy.

\end{proof} 