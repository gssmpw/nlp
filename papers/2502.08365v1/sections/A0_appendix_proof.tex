\section{Proofs of the Main Theoretical Results}
\label{apx:proof}

In this Section, we report the full proofing steps of the Theorems and Lemmas in the main paper.

\entropymismatch*

\begin{proof}
    The bounds follow directly from simple yet fundamental relationships between entropies of joint, marginal and mixture distributions which can be found in~\citet{ paninski2003, Kolchinsky_2017}, in particular:
    \begin{align*}
        &\frac{1}{|\Ns|}H(d^\pi) \leq \frac{1}{|\Ns|}\sum_{i \in [\Ns]}H(d_i^\pi)  \overset{\text{(a)}}{\leq} H(\tilde d^\pi)  \overset{\text{(b)}}{\leq} \frac{1}{|\Ns|}\sum_{i \in [\Ns]}H(d_i^\pi)+ \log(|\Ns|)  \overset{\text{(c)}}{\leq} \sup_{i \in [\Ns]}H(d_i^\pi)+ \log(|\Ns|) \leq H(d^\pi) + \log(|\Ns|)
    \end{align*}
    where step (a) and (b) use the fact that $\tilde d^\pi(s) := \frac{1}{|\Ns|}\sum_{i \in [\Ns]} d^\pi_i(s)$ is a uniform mixture over the agents, whose distribution over the weights has entropy $\log(|\Ns|)$, so as we can apply the bounds from~\citet{Kolchinsky_2017}. Step (c) uses the fact that $H(d^\pi) = \sum_{i \in [\Ns]}H(d^\pi_i|d^\pi_{<i})$, then taking the supremum as first $i$ it follows that $ \sup_{i \in [\Ns]}H(d_i^\pi) = H(d^\pi) - \sum_{j \in [\Ns]> i} H(d^\pi_j|d^\pi_{<j}, d^\pi_{i}) \leq H(d^\pi)$ due to non-negativity of entropy.
\end{proof}

\objectivemismatch*

\begin{proof}
    For the general proof structure, we adapt the steps of~\citet{mutti2023challengingcommonassumptionsconvex} for Convex MDPs to the different objectives possible in CMGs.
    Let us start by considering joint objectives, then:
    \begin{align*}
        \big| \zeta_K(\pi) - \zeta_\infty (\pi) \big|
        &= \Big| \E_{d_K\sim p^{\pi}_K} \left[ \mathcal F (d_K) \right] - \mathcal F (d^{\pi}) \Big| \leq \E_{d_K\sim p^{\pi}_K} \left[ \left| \mathcal F (d_K) - \mathcal F (d^{\pi}) \right| \right] \\
        &\overset{\text{(a)}}{\leq} \E_{d_K\sim p^{\pi}_K} \left[ L \left\| d_K- d^{\pi} \right\|_1 \right] \leq L \E_{d_K\sim p^{\pi}_K} \left[ \left\| d_K- d^{\pi} \right\|_1 \right] \\
        &\overset{\text{(b)}}{\leq}   L \E_{d_K\sim p^{\pi}_K} \left[ \max_{t \in [T]}  \left\| d_{K, t} - d^{\pi}_t \right\|_1 \right],
    \end{align*}
    where in step (a) we apply the Lipschitz assumption on $\mathcal F$ to write and in step (b) we apply a maximization over the episode's step by noting that $d_{K} = \frac{1}{T} \sum_{t \in [T]} d_{K, t}$ and $d^\pi = \frac{1}{T} \sum_{t \in [T]} d^\pi_t$.
    We then apply bounds in high probability
    \begin{align*}
        Pr \Big(  \max_{t \in [T]}  \left\| d_{K, t} - d^{\pi}_t \right\|_1 \geq \epsilon \Big)
        &\leq Pr \Big( \bigcup_{ t} \left\| d_{K, t} - d^{\pi}_t \right\|_1 \geq \epsilon \Big)  \\
        &\overset{\text{(c)}}{\leq}  \sum_{ t} Pr \Big( \left\| d_{K, t} - d^{\pi}_t \right\|_1 \geq \epsilon \Big) \\
        &\leq  T \ Pr \Big( \left\| d_{K, t} - d^{\pi}_t \right\|_1 \geq \epsilon \Big), 
    \end{align*}
    with $\epsilon > 0$ and in step (c) we applied a union bound. We then consider standard concentration inequalities for empirical distributions~\citep{Weissman2003InequalitiesFT} so to obtain the final bound
    \begin{equation}
        Pr \Bigg( \left\| d_{K, t} - d^{\pi}_t \right\|_1 \geq \sqrt{\frac{2|\Ss| \log(2 / \delta')}{K}} \ \Bigg) \leq \delta'. \label{eq:empirical_dist_concentration}
    \end{equation}
    By setting $\delta' = \delta / T$, and then plugging the empirical concentration inequality, we have that with probability at least $1 - \delta$
    \begin{equation*}
        \big| \zeta_K (\pi) - \zeta_\infty (\pi) \big| \leq  L T \sqrt{\frac{2|\Ss| \log(2 T / \delta )}{K}},
    \end{equation*}
    which concludes the proof for joint objectives.

    The proof for disjoint objectives follows the same rational by bounding each per-agent term separately and after noticing that due to Assumption~\ref{ass:mixture}, the resulting bounds get simplified in the overall averaging. As for mixture objectives, the only core difference is after step (b), where $\tilde{d}_K$ takes the place of $d_K$ and $\tilde{d^\pi}$ of $d^\pi$. The remaining steps follow the same logic, out of noticing that the empirical distribution with respect to $\tilde{d^\pi}$ is taken with respect $|\Ns|K$ samples in total. Both the two bounds then take into account that the support of the empirical distributions have size $|\tilde \Ss|$ and not $|\Ss|$.
\end{proof}
