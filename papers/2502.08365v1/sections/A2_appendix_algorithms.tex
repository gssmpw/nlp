\section{Details on the Experimental Proofs Of Concept.}
\label{apx:exp}

\paragraph*{Environments.}~The main empirical proof of concept was based on two environments. First, Env. (\textbf{i}), the so called \emph{secret room} environment by~\citet{liu2021cooperative}. In this environment, two agents operate within two rooms of a $10\times10$ discrete grid. There is one switch in each room, one in position $(1,9)$ (corner of first room), another in position $(9,1)$ (corner of second room). The rooms are separated by a door and agents start in the same room deterministically at positions $(1,1)$ and $(2,2)$ respectively. The door will open only when one of the switches is occupied, which means that the (Manhattan) distance between one of the agents and the switch is less than $1.5$. The full state vector contains $x, y$ locations of the two agents and binary variables to indicate if doors are open \emph{but} per-agent policies are conditioned on their respective states only and the state of the door. For Sparse-Rewards Tasks, the goal was set to be deterministically at the worst case, namely $(9,9)$ and to provide a positive reward to both the agents of $100$ when reached, which means again that the (Manhattan) distance between one of the agents and the switch is less than $1.5$, a reward of $0$ otherwise. The second environment, Env. (\textbf{ii}), was the MaMuJoCo \emph{reacher} environment~\cite{peng2021facmac}. In this environment, two agents operate the two linked joints and each space dimension is discretized over $10$ bins. Per-agent policies were conditioned on their respective joint angles only.  For Sparse-Rewards Tasks, the goal was set to be randomly at the worst case, namely on position $(\pm0.21, \pm0.21)$ on the boundary of the reachable area. Reaching the goal mean to have a tip position (not observable by the agents and not discretized) at a distance less that $0.05$ and provides a positive reward to both the agents of $1$ when reached, a reward of $0$ otherwise. 

\paragraph*{Class of Policies.}~In Env. (\textbf{i}), the policy was parametrized by a dense $(64,64)$ Neural Network that takes as input the per-agent state features and outputs an action vector probabilities through a last soft-max layer. In Env. (\textbf{ii}), the policy was represented by a Gaussian distribution with diagonal covariance matrix. It takes as input the environment state features and outputs an action vector. The mean is state-dependent and is the downstream output of a a dense $(64,64)$ Neural Network. The standard deviation is state-independent, represented by a separated trainable vector and initialized to $-0.5$. The weights are initialized via Xavier Initialization.

\paragraph*{TRPE}~As outlined in the pseudocode of Algorithm~\ref{alg:trpe}, in each epoch a dataset of $N$ trajectories is gathered for a given exploration horizon $T$, leading to the reported number of samples. Throughout the experiment the number of epochs $e$ were set equal to $e=10k$, the number of trajectories $N=10$, the KL threshold $\delta = 6$, the maximum number of off-policy iterations set to $n_{\text{off,iter}} = 20$, the learning rate was set to $\eta = 10^{-5}$ and the number of seeds set equal to $4$ due to the inherent low stochasticity of the environment.

\paragraph*{Multi-Agent TRPO}~We follow the same notation in~\citet{duan2016benchmarking}. Agents have independent critics $(64,64)$ Dense networks and in each epoch a dataset of $N$ trajectories is gathered for a given exploration horizon $T$ for each agent, leading to the reported number of samples. Throughout the experiment the number of epochs $e$ were set equal to $e=100$, the number of trajectories building the batch size $N=20$, the KL threshold $\delta = 10^{-4}$, the maximum number of off-policy iterations set to $n_{\text{off,iter}} = 20$, the discount was set to $\gamma = 0.99$.


The Repository is made available at the following \href{https://anonymous.4open.science/r/trpe-DB16/README.md}{Repository.}

%
\newpage

 


\begin{figure*}[]
    \centering
    \input{figures/pretraining_legend.tex}
    %\hfill
    \vfill
    %vspace{-0.2cm}
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/room_50_jointentropynokl.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Joint Entropy (Env.~(\textbf{i}), $T=50$).}
        \label{subfig:imagea1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/room_50_mixtureentropynokl.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Mixture Entropy (Env.~(\textbf{i}), $T=50$).}
        \label{subfig:imagea2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/room_50_entropyA1.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Entropy Agent 1 (Env.~(\textbf{i}), $T=50$).}
        \label{subfig:image11}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/room_50_entropyA2.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Entropy Agent 2 (Env.~(\textbf{i}), $T=50$).}
        \label{subfig:image11}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/room_100_jointentropy.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Joint Entropy (Env.~(\textbf{i}), $T=100$).}
        \label{subfig:imagea1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/room_100_mixtureentropy.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Mixture Entropy (Env.~(\textbf{i}), $T=100$).}
        \label{subfig:imagea2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/room_100_entropyA1.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Entropy Agent 1 (Env.~(\textbf{i}), $T=100$).}
        \label{subfig:image11}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/room_100_entropyA2.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Entropy Agent 2 (Env.~(\textbf{i}), $T=100$).}
        \label{subfig:image11}
    \end{subfigure}
    \vfill 
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/room_150_jointentropy.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Joint Entropy (Env.~(\textbf{i}), $T=150$).}
        \label{subfig:imagea1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/room_150_mixtureentropy.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Mixture Entropy (Env.~(\textbf{i}), $T=150$).}
        \label{subfig:imagea2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/room_150_entropyA1.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Entropy Agent 1 (Env.~(\textbf{i}), $T=150$).}
        \label{subfig:image11}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/room_150_entropyA2.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Entropy Agent 2 (Env.~(\textbf{i}), $T=150$).}
        \label{subfig:image11}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/hand_100_jointentropy.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Joint Entropy (Env.~(\textbf{ii}), $T=100$).}
        \label{subfig:imagea1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/hand_100_mixtureentropy.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Mixture Entropy (Env.~(\textbf{ii}), $T=100$).}
        \label{subfig:imagea2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hand_100_entropyA1.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Entropy Agent 1 (Env.~(\textbf{ii}), $T=100$).}
        \label{subfig:image11}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/hand_100_entropyA2.pdf}
        %\vspace{-0.8cm}
        \caption{\centering TRPE Entropy Agent 2 (Env.~(\textbf{ii}), $T=100$).}
        \label{subfig:image11}
    \end{subfigure}
\caption{\centering Full Visualization of Reported Experiments.}
\label{fig:pretraining}
\end{figure*}

\begin{figure*}[!]
    \centering
    %vspace{-0.2cm}
    \input{figures/pretraining_legend.tex}
    %\hfill
    \vfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/room_50_entropyPA1nokl.pdf}
        %\vspace{-0.8cm}
        \caption{\centering Entropy of Agent 1 Policy in TRPE Training (\textbf{i}), $T=50$).}
        \label{subfig:image93}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/room_50_entropyPA2nokl.pdf}
        %\vspace{-0.8cm}
        \caption{\centering Entropy of Agent 2 Policy in TRPE Training (\textbf{i}), $T=50$).}
        \label{subfig:image130}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/hand_100_entropyPA1.pdf}
        %\vspace{-0.8cm}
        \caption{\centering Entropy of Agent 1 Policy in TRPE Training (\textbf{ii}), $T=100$).}
        \label{subfig:image130}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{figures/hand_100_entropyPA2.pdf}
        %\vspace{-0.8cm}
        \caption{\centering Entropy of Agent 2 Policy in TRPE Training (\textbf{ii}), $T=100$).}
        \label{subfig:image130}
    \end{subfigure}
\caption{\centering Policiy Entropy Insights for TRPO Pretraining in Env (\textbf{i}) and Env (\textbf{ii}). \textbf{Lower Entropic Policies with Disjoint Objectives might justify the difference in pre-training performance even if the performances in training are similar}.}
\label{fig:333}
\end{figure*}
