\section{Conclusions and Perspectives}
\label{sec:conclusions}

In this paper, we extend the state entropy maximization problem to Markov Games via a novel framework called Convex Markov Games. First of all, we show that the task can be defined in several different ways: one can look at the joint distribution among all the agents, the marginals which are agent-specific, or the mixture which is a tradeoff of the two. Thus, we link these three options via performance bounds and we show that while the first might enjoy nice theoretical guarantees, the others are more promising at working in practice, the latter in particular. Then, we design a practical trust-region algorithm addressing more practical scenarios and we use it to confirm in a set of experiments the expected superiority of mixture objectives, due to its ability to enforce efficient but coordinated exploration over short horizons. Future works can build over our results in many directions, which include pushing forward the known theoretical properties of Convex Markov Games, developing scalable algorithms for continuous domains and investigating more policy classes with succinct representations of the history beyond the one we considered in the experiments.
We believe that our work can be a crucial step in the direction of extending state entropy maximization in a principled way to yet more practical settings, in which many agents interact over the same environment.