@article{bambhaniya2024progressivegradientflowrobust,
  title        = {Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers},
  author       = {Abhimanyu Rajeshkumar Bambhaniya and Amir Yazdanbakhsh and Suvinay Subramanian and Sheng-Chun Kao and Shivani Agrawal and Utku Evci and Tushar Krishna},
  journal      = {arXiv preprint arXiv:2402.04744},
  year         = 2024
}

@misc{maxtext2024,
  title        = {MaxText: A Framework for Training Large Language Models},
  author       = {Matthew Davidow and Mohit Khatwani and Michelle Yoo et al.},
  year         = 2024,
  howpublished = {\url{https://github.com/AI-Hypercomputer/maxtext}},
}

@misc{han2024sltrainsparsepluslowrank,
  title        = {SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining},
  author       = {Andi Han and Jiaxiang Li and Wei Huang and Mingyi Hong and Akiko Takeda and Pratik Jawanpuria and Bamdev Mishra},
  year         = 2024,
  eprint       = {2406.02214},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
}
@article{kuznedelev2024accurate,
  title   = {Accurate Neural Network Pruning Requires Rethinking Sparse Optimization},
  author  = {Denis Kuznedelev and Eldar Kurtic and Eugenia Iofinova and Elias Frantar and Alexandra Peste and Dan Alistarh},
  journal = {Transactions on Machine Learning Research},
  year    = 2024
}

@inproceedings{groeneveld2024olmoacceleratingsciencelanguage,
  title     = "{OLM}o: Accelerating the Science of Language Models",
  author    = "Dirk Groeneveld and Iz Beltagy and Evan Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and William Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah Smith and Hannaneh Hajishirzi",
  booktitle = "Annual Meeting of the Association for Computational Linguistics",
  year      = 2024
}

@article{xia2024shearedllamaacceleratinglanguage,
  title   = {Sheared llama: Accelerating language model pre-training via structured pruning},
  author  = {Mengzhou Xia and Tianyu Gao and Zhiyuan Zeng and Danqi Chen},
  journal = {arXiv preprint arXiv:2310.06694},
  year    = 2023
}


@inproceedings{sun2024simpleeffectivepruningapproach,
  title     = {A Simple and Effective Pruning Approach for Large Language Models},
  author    = {Mingjie Sun and Zhuang Liu and Anna Bair and J Zico Kolter},
  booktitle = {International Conference on Learning Representations},
  year      = 2024
}

@inproceedings{sardana2024chinchillaoptimalaccountinginferencelanguage,
  title     = {Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws},
  author    = {Nikhil Sardana and Jacob Portes and Sasha Doubov and Jonathan Frankle},
  booktitle = {International Conference on Machine Learning},
  year      = 2024
}

@article{panigrahi2024efficient,
  title   = {Efficient Stagewise Pretraining via Progressive Subnetworks},
  author  = {Abhishek Panigrahi and Nikunj Saunshi and Kaifeng Lyu and Sobhan Miryoosefi and Sashank Reddi and Satyen Kale and Sanjiv Kumar},
  journal = {arXiv preprint arXiv:2402.05913},
  year    = 2024
}

@inproceedings{yano2024step,
  title     = {STEP: Staged Parameter-Efficient Pre-training for Large Language Models},
  author    = {Kazuki Yano and Takumi Ito and Jun Suzuki},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (Student Research Workshop)},
  year      = 2024
}

@inproceedings{frantar2023scalinglawssparselyconnectedfoundation,
  title     = {Scaling Laws for Sparsely-Connected Foundation Models},
  author    = {Elias Frantar and Carlos Riquelme Ruiz and Neil Houlsby and Dan Alistarh and Utku Evci},
  booktitle = {International Conference on Learning Representations},
  year      = 2024
}

@article{frantar2023sparsegptmassivelanguagemodels,
  title   = {SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
  author  = {Elias Frantar and Dan Alistarh},
  journal = {arXiv preprint arXiv:2301.00774},
  year    = 2023
}

@article{touvron2023llama2openfoundation,
  title   = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author  = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  journal = {arXiv preprint arXiv:2307.09288},
  year    = 2023
}

@article{grattafiori2024llama3herdmodels,
  title   = {The Llama 3 Herd of Models},
  author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
  journal = {arXiv preprint arXiv:2407.21783},
  year    = 2024
}

@article{biderman2023pythiasuiteanalyzinglarge,
  title   = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author  = {Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
  journal = {arXiv preprint arXiv:2304.01373},
  year    = 2023
}

@inproceedings{yao2023masked,
  title     = {Masked Structural Growth for 2x Faster Language Model Pre-training},
  author    = {Yiqun Yao and Zheng Zhang and Jing Li and Yequan Wang},
  booktitle = {International Conference on Learning Representations},
  year      = 2024
}

@misc{Samar2022SparseGPT3,
  title        = {Creating Sparse GPT-3 Models with Iterative Pruning},
  author       = {Anshul Samar},
  year         = 2022,
  institution  = {Cerebras Systems},
  howpublished = {\url{https://cerebras.ai/blog/creating-sparse-gpt-3-models-with-iterative-pruning}},
  note         = {Accessed: 2025-01-12}
}

@inproceedings{pmlr-v162-bansal22b,
  title     = {Data Scaling Laws in {NMT}: The Effect of Noise and Architecture},
  author    = {Yamini Bansal and Behrooz Ghorbani and Ankush Garg and Biao Zhang and Colin Cherry and Behnam Neyshabur and Orhan Firat},
  booktitle = {International Conference on Machine Learning},
  year      = 2022
}

@inproceedings{jin2022pruning,
  title     = {Pruning's Effect on Generalization Through the Lens of Training and Regularization},
  author    = {Tian Jin and Michael Carbin and Daniel M. Roy and Jonathan Frankle and Gintare Karolina Dziugaite},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = 2022
}


@inproceedings{hoffmann2022trainingcomputeoptimallargelanguage,
  title     = {Training compute-optimal large language models},
  author    = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack W. Rae and Laurent Sifre},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = 2024
}

@article{kumar2024scaling,
  title={Scaling laws for precision},
  author={Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2411.04330},
  year={2024}
}


@inproceedings{paul2022unmasking,
  title     = {Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?},
  author    = {Mansheej Paul and Feng Chen and Brett W. Larsen and Jonathan Frankle and Surya Ganguli and Gintare Karolina Dziugaite},
  booktitle = {International Conference on Learning Representations},
  year      = 2023
}

@inproceedings{peste2021acdcalternatingcompresseddecompressedtraining,
  title     = {{AC}/{DC}: Alternating Compressed/DeCompressed Training of Deep Neural Networks},
  author    = {Alexandra Peste and Eugenia Iofinova and Adrian Vladu and Dan Alistarh},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = 2021
}

@inproceedings{ghorbani2021scalinglawsneuralmachine,
  title     = {Scaling Laws for Neural Machine Translation},
  author    = {Behrooz Ghorbani and Orhan Firat and Markus Freitag and Ankur Bapna and Maxim Krikun and Xavier Garcia and Ciprian Chelba and Colin Cherry},
  booktitle = {International Conference on Learning Representations},
  year      = 2022
}
@inproceedings{gordon-etal-2021-data,
  title     = {Data and Parameter Scaling Laws for Neural Machine Translation},
  author    = {Mitchell A Gordon and Kevin Duh and Jared Kaplan},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year      = 2021
}

@inproceedings{rosenfeld2021predictabilitypruningscales,
  title     = {On the Predictability of Pruning Across Scales},
  author    = {Jonathan S Rosenfeld and Jonathan Frankle and Michael Carbin and Nir Shavit},
  booktitle = {International Conference on Machine Learning},
  year      = 2021
}

@misc{chen2021chasingsparsityvisiontransformers,
  title        = {Chasing Sparsity in Vision Transformers: An End-to-End Exploration},
  author       = {Tianlong Chen and Yu Cheng and Zhe Gan and Lu Yuan and Lei Zhang and Zhangyang Wang},
  year         = 2021,
  eprint       = {2106.04533},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV},
}

@inproceedings{evci2021rigginglotterymakingtickets,
  title     = {Rigging the Lottery: Making All Tickets Winners},
  author    = {Utku Evci and Trevor Gale and Jacob Menick and Pablo Samuel Castro and Erich Elsen},
  booktitle = {International Conference on Machine Learning},
  year      = 2020
}


@inproceedings{renda2020comparingrewindingfinetuningneural,
  title     = {Comparing Rewinding and Fine-tuning in Neural Network Pruning},
  author    = {Alex Renda and Jonathan Frankle and Michael Carbin},
  booktitle = {International Conference on Learning Representations},
  year      = 2020
}


@article{kaplan2020scalinglawsneurallanguage,
  title   = {Scaling Laws for Neural Language Models},
  author  = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = 2020
}

@inproceedings{frankle2020linearmodeconnectivitylottery,
title     = {Linear Mode Connectivity and the Lottery Ticket Hypothesis},
  author    = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
  booktitle = {International Conference on Machine Learning},
  year      = 2019
}

@article{shoeybi2020megatronlmtrainingmultibillionparameter,
  title   = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author  = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  journal = {arXiv preprint arXiv:1909.08053},
  year    = 2020
}

@inproceedings{brown2020languagemodelsfewshotlearners,
  title     = {Language Models are Few-Shot Learners},
  author    = {Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel Ziegler and Jeffrey Wu and Clemens Winter and Chris Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = 2020
}

@article{2019t5,
  title   = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal = {Journal of Machine Learning Research},
  year    = 2020,
  volume  = 21,
  number  = 1
}


@inproceedings{frankle2019lotterytickethypothesisfinding,
  title     = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author    = {Jonathan Frankle and Michael Carbin},
  booktitle = {International Conference on Learning Representations},
  year      = 2019
}

@inproceedings{nakkiran2019deepdoubledescentbigger,
  title     = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  author    = {Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
  booktitle = {International Conference on Learning Representations},
  year      = 2020
}

@article{gale2019state,
  title   = {The State of Sparsity in Deep Neural Networks},
  author  = {Trevor Gale and Erich Elsen and Sara Hooker},
  journal = {arXiv preprint arXiv:1902.09574},
  year    = 2019
}

@article{zhu2017prunepruneexploringefficacy,
  title   = {To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author  = {Michael Zhu and Suyog Gupta},
  journal = {arXiv preprint arXiv:1710.01878},
  year    = 2017
}


@inproceedings{he2017channelpruningacceleratingdeep,
  title     = {Channel Pruning for Accelerating Very Deep Neural Networks},
  author    = {Yihui He and Xiangyu Zhang and Jian Sun},
  booktitle = {IEEE International Conference on Computer Vision},
  year      = 2017
}


@book{goodfellow2016deep,
  title        = {Deep learning},
  author       = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  year         = 2016,
}

@inproceedings{han2015learningweightsconnectionsefficient,
  title     = {Learning both weights and connections for efficient neural networks},
  author    = {Song Han and Jeff Pool and John Tran and William J. Dally},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = 2015
}

@article{esser2019learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:1902.08153},
  year={2019}
}

@article{clark2022unified,
  title={Unified Scaling Laws for Routed Language Models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  journal={International Conference on Machine Learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}, 
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={MLSys 2024},
  year={2023}
}

@article{muennighoff2023scaling,
  title={Scaling data-constrained language models},
  author={Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Le Scao, Teven and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={50358--50376},
  year={2023}
}

@article{kaushal2024spectra,
  title={Spectra: Surprising effectiveness of pretraining ternary language models at scale},
  author={Kaushal, Ayush and Vaidhya, Tejas and Mondal, Arnab Kumar and Pandey, Tejas and Bhagat, Aaryan and Rish, Irina},
  journal={arXiv preprint arXiv:2407.12327},
  year={2024}
}

@article{deletang2023language,
  title={Language Modeling is Compression},
  author={Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and others},
  journal={arXiv preprint arXiv:2309.10668},
  year={2023}
}

@article{wang2023bitnet,
  title={{BitNet}: Scaling 1-bit Transformers for Large Language Models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023},
  url={https://arxiv.org/abs/2310.11453}
}

@article{choi2018pact,
  title={{PACT}: Parameterized Clipping Activation for Quantized Neural Networks},
  author={Choi, Jungwook and Wang, Zhuo and Venkataramani, Swagath and Chuang, Pierce I-Jen and Srinivasan, Vijayalakshmi and Gopalakrishnan, Kailash},
  journal={arXiv preprint arXiv:1805.06085},
  year={2018},
  url={https://arxiv.org/abs/1805.06085}
}



@misc{ma2024era1bitllmslarge,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17764}, 
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={arXiv preprint arXiv:1910.10683},
  year={2020},
  url={https://arxiv.org/abs/1910.10683},
  note={C4 dataset introduced as part of the T5 paper.}
}


@incollection{Bengio+chapter2007,
  title        = {Scaling Learning Algorithms Towards {AI}},
  author       = {Bengio, Yoshua and LeCun, Yann},
  year         = 2007,
  booktitle    = {Large Scale Kernel Machines},
}
@article{Hinton06,
  title        = {A Fast Learning Algorithm for Deep Belief Nets},
  author       = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
  year         = 2006,
}
@inproceedings{banko-brill-2001-scaling,
  title     = {Scaling to Very Very Large Corpora for Natural Language Disambiguation},
  author    = {Michele Banko and Eric Brill},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  year      = 2001
}

@article{goodman2001bitprogresslanguagemodeling,
  title   = {A bit of progress in language modeling},
  author  = {Joshua Goodman},
  journal = {arXiv preprint arXiv:cs.CL/0108005},
  year    = 2001
}
@inproceedings{298572,
  title     = {Optimal Brain Surgeon and general network pruning},
  author    = {B. Hassibi and D.G. Stork and G.J. Wolff},
  booktitle = {IEEE International Conference on Neural Networks},
  year      = 1993
}
@article{Liu1989,
  title   = {On the limited memory BFGS method for large scale optimization},
  author  = {Dong C. Liu and Jorge Nocedal},
  journal = {Mathematical Programming},
  year    = 1989,
  volume  = 45,
  number  = 1,
  pages   = {503--528}
}

@inproceedings{NIPS1989_6c9882bb,
  title     = {Optimal Brain Damage},
  author    = {Yann LeCun and John Denker and Sara Solla},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = 1989
}
``
