[
  {
    "index": 0,
    "papers": [
      {
        "key": "kaplan2020scalinglawsneurallanguage",
        "author": "Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei",
        "title": "Scaling Laws for Neural Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hoffmann2022trainingcomputeoptimallargelanguage",
        "author": "Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack W. Rae and Laurent Sifre",
        "title": "Training compute-optimal large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "muennighoff2023scaling",
        "author": "Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Le Scao, Teven and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin A",
        "title": "Scaling data-constrained language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "clark2022unified",
        "author": "Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others",
        "title": "Unified Scaling Laws for Routed Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "frantar2023scalinglawssparselyconnectedfoundation",
        "author": "Elias Frantar and Carlos Riquelme Ruiz and Neil Houlsby and Dan Alistarh and Utku Evci",
        "title": "Scaling Laws for Sparsely-Connected Foundation Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2023bitnet",
        "author": "Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu",
        "title": "{BitNet}: Scaling 1-bit Transformers for Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ma2024era1bitllmslarge",
        "author": "Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei",
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
      },
      {
        "key": "kaushal2024spectra",
        "author": "Kaushal, Ayush and Vaidhya, Tejas and Mondal, Arnab Kumar and Pandey, Tejas and Bhagat, Aaryan and Rish, Irina",
        "title": "Spectra: Surprising effectiveness of pretraining ternary language models at scale"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "kumar2024scaling",
        "author": "Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\\'e}, Christopher and Raghunathan, Aditi",
        "title": "Scaling laws for precision"
      }
    ]
  }
]