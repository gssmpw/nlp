@article{clark2022unified,
  title={Unified Scaling Laws for Routed Language Models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  journal={International Conference on Machine Learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@inproceedings{frantar2023scalinglawssparselyconnectedfoundation,
  title     = {Scaling Laws for Sparsely-Connected Foundation Models},
  author    = {Elias Frantar and Carlos Riquelme Ruiz and Neil Houlsby and Dan Alistarh and Utku Evci},
  booktitle = {International Conference on Learning Representations},
  year      = 2024
}

@inproceedings{hoffmann2022trainingcomputeoptimallargelanguage,
  title     = {Training compute-optimal large language models},
  author    = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack W. Rae and Laurent Sifre},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = 2024
}

@article{kaplan2020scalinglawsneurallanguage,
  title   = {Scaling Laws for Neural Language Models},
  author  = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = 2020
}

@article{kaushal2024spectra,
  title={Spectra: Surprising effectiveness of pretraining ternary language models at scale},
  author={Kaushal, Ayush and Vaidhya, Tejas and Mondal, Arnab Kumar and Pandey, Tejas and Bhagat, Aaryan and Rish, Irina},
  journal={arXiv preprint arXiv:2407.12327},
  year={2024}
}

@article{kumar2024scaling,
  title={Scaling laws for precision},
  author={Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2411.04330},
  year={2024}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}, 
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={MLSys 2024},
  year={2023}
}

@misc{ma2024era1bitllmslarge,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17764}, 
}

@article{muennighoff2023scaling,
  title={Scaling data-constrained language models},
  author={Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Le Scao, Teven and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={50358--50376},
  year={2023}
}

@article{wang2023bitnet,
  title={{BitNet}: Scaling 1-bit Transformers for Large Language Models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023},
  url={https://arxiv.org/abs/2310.11453}
}

