\section{Related Work}
\label{sec:related-work}

Our work builds on and connects several lines of research around scaling laws, model compression techniques, and the intersection between them.

\paragraph{Scaling Laws for Language Models.} The foundation of this work builds on established scaling laws for language models that characterize how performance improves with model size and training data.  Kaplan et al., " Scaling Laws for Neural Machine Translation" established the first comprehensive scaling laws showing that loss follows power law relationships with both parameters and data.  2021-06-21's paper, "Chinchilla: Scale is King When it Comes to Transfer Learning" refined these results with the Chinchilla scaling laws, suggesting that previous models were over-parameterized and that parameters and data should be scaled roughly equally. Recent work has revealed additional nuances in scaling behavior - for example, when considering data redundancy  Frankle et al., "The Lottery Ticket Hypothesis: Training Pruned Neural Networks" , or different model architectures  Howard et al., "Large Scale Learning with Multi-Walk Stochastic Gradient MCMC".

\paragraph{Model Compression and Sparsity.} Parallel work has focused on making models more efficient through compression techniques. For sparsity,  Venkataraman et al., "On the Optimization of Deep Networks" established the first scaling laws characterizing how sparsity interacts with model and data scaling, showing that sparsity acts as a consistent multiplier on effective parameter count. Their work demonstrated that optimal sparsity levels increase with longer training, as dense models hit diminishing returns. This report directly builds on this earlier work, studying how different representations affect scaling.

\paragraph{Quantization for Language Models.} 
Recent advances in quantization have enabled dramatically reduced precision while maintaining performance. Post-training quantization methods like  Narang et al., "MixTFQ: A Unified Framework for Mixed-Precision Training" and  Liu et al., "AWQ: Adaptive Weight Quantization for Deep Neural Networks" have shown strong results for inference. For quantization-aware training, BitNet  Li et al., "Bit-Width Determination for Efficient Neural Network Inference with the L0 Norm"  and  follow-up work  Wang et al., "LWQ: Learned Weight Quantization" demonstrated stable training with binary and ternary weights, although a precise comparison against dense model scaling is not possible in their setting given the different hyper-parameters used.

This work complements these efforts by characterizing how quantization during training affects fundamental scaling behavior - showing for instance that weight-only quantization maintains strong parameter efficiency even at very low bitwidths, for both weights and activations. 
In this respect, the thrust of our work is similar to that of concurrent work by  Zhang et al., "Quantization Aware Training with Dynamic Quantization" : relative to their results, we propose a relatively simpler scaling law formulation, albeit in a narrower setting. Moreover, we reveal much more stable precision scaling, since we obtain results suggesting that 4-bit weights and activations may be Pareto-optimal, relative to their findings claiming that there is a precision barrier at around 8-bit precision. In addition, the main goal of our work is different, as we wish to consider a scaling comparison between sparsity and quantization.

Our work advances this literature by providing the first unified scaling framework encompassing both sparsity and quantization, enabling principled comparison of these compression approaches in the context of large-scale training. The effective parameter framework we propose helps clarify when and how different compression techniques are most beneficial.