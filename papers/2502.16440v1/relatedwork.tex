\section{Related Work}
\label{sec:related-work}

Our work builds on and connects several lines of research around scaling laws, model compression techniques, and the intersection between them.

\paragraph{Scaling Laws for Language Models.} The foundation of this work builds on established scaling laws for language models that characterize how performance improves with model size and training data. \citet{kaplan2020scalinglawsneurallanguage} established the first comprehensive scaling laws showing that loss follows power law relationships with both parameters and data. \citet{hoffmann2022trainingcomputeoptimallargelanguage} refined these results with the Chinchilla scaling laws, suggesting that previous models were over-parameterized and that parameters and data should be scaled roughly equally. Recent work has revealed additional nuances in scaling behavior - for example, when considering data redundancy~\citep{muennighoff2023scaling}, or different model architectures~\citep{clark2022unified}.

\paragraph{Model Compression and Sparsity.} Parallel work has focused on making models more efficient through compression techniques. For sparsity, \citet{frantar2023scalinglawssparselyconnectedfoundation} established the first scaling laws characterizing how sparsity interacts with model and data scaling, showing that sparsity acts as a consistent multiplier on effective parameter count. Their work demonstrated that optimal sparsity levels increase with longer training, as dense models hit diminishing returns. This report directly builds on this earlier work, studying how different representations affect scaling. 

\paragraph{Quantization for Language Models.} 
Recent advances in quantization have enabled dramatically reduced precision while maintaining performance. Post-training quantization methods like GPTQ~\citep{frantar2022gptq} and AWQ~\citep{lin2023awq} have shown strong results for inference. For quantization-aware training, BitNet~\citep{wang2023bitnet} and  follow-up work~\citep{ma2024era1bitllmslarge, kaushal2024spectra} demonstrated stable training with binary and ternary weights, although a precise comparison against dense model scaling is not possible in their setting given the different hyper-parameters used. 

This work complements these efforts by characterizing how quantization during training affects fundamental scaling behavior - showing for instance that weight-only quantization maintains strong parameter efficiency even at very low bitwidths, for both weights and activations. 
In this respect, the thrust of our work is similar to that of concurrent work by~\citet{kumar2024scaling}: relative to their results, we propose a relatively simpler scaling law formulation, albeit in a narrower setting. Moreover, we reveal much more stable precision scaling, since we obtain results suggesting that 4-bit weights and activations may be Pareto-optimal, relative to their findings claiming that there is a precision barrier at around 8-bit precision. In addition, the main goal of our work is different, as we wish to consider a scaling comparison between sparsity and quantization. 

Our work advances this literature by providing the first unified scaling framework encompassing both sparsity and quantization, enabling principled comparison of these compression approaches in the context of large-scale training. The effective parameter framework we propose helps clarify when and how different compression techniques are most beneficial.