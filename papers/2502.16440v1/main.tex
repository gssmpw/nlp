% This class has a lot of options, so please check deepmind.cls for more details.
% This is a minimal set for most needs.
\documentclass[11pt, a4paper, logo, twocolumn, external, copyright]{googledeepmind}

% Omit dates for reproducibility.
\pdfinfoomitdate 1
\pdftrailerid{redacted}

% This avoids duplicate hyperref bookmark entries when using \bibentry (e.g. via \citeas).
\makeatletter
\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother

\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}
\usepackage{gdm-colors}

% Sometimes you will get errors about pdflink ending up in diffrent position. Try this and
% comment it out again when you are done with your document.
%\hypersetup{draft}

% Set the bibliography options here.
\usepackage[authoryear, sort&compress, round]{natbib}

\renewcommand{\paragraph}[1]{\noindent\textbf{#1}}

% Images will be looked for in this path, removes need for explicit path when including images.
\graphicspath{{figures/}}

% Important Information about your paper.
\title{Compression Scaling Laws: \\ Unifying Sparsity and Quantization}

% Can leave this option out if you do not wish to add a corresponding author.
\correspondingauthor{evcu@google.com}

% Remove these if they are not needed
\keywords{Model Compression, Large Language Models, Scaling Laws, Sparsity, Quantization}
% \paperurl{arxiv.org/abs/123}

% Use the internally issued paper ID, if there is one
\reportnumber{001} % Leave blank if n/a

% Assign your own date to the report.
% Can comment out if not needed or leave blank if n/a.
% \renewcommand{\today}{2000-01-01}

% Can have as many authors and as many affiliations as needed. Best to indicate joint
% first-authorship as shown below.
\author[1,2]{Elias Frantar}
\author[*,1]{Utku Evci}
\author[1]{Wonpyo Park}
\author[1]{Neil Houlsby}
\author[2]{Dan Alistarh}

% Affiliations *must* come after the declaration of \author[]
\affil[*]{Corresponding author}
\affil[1]{Google DeepMind}
\affil[2]{Institute of Science and Technology Austria}

\begin{abstract}
% In our recent ICLR Spotlight paper\footnote{https://arxiv.org/abs/2309.08520}, we studied the impact of weight-sparsity on size/data scaling laws for foundation models. Among our key findings were that sparsity acts as a constant multiplier on the size term, while hardly interacting with the data term. As a direct continuation, we now want to investigate the following:

% In this paper, we ask: Does this constant-multiplier scaling also hold for other forms of compression like quantization or activation compression?
% If so, what are the multipliers and how do they compare to each other?
% We also look at how the multipliers look in a significantly better optimized setup (better LR & batchsize tuning, larger models, better optimized compression techniques).
We investigate how different compression techniques---such as weight and activation quantization, and weight sparsity---affect the scaling behavior of large language models (LLMs) during pretraining. Building on previous work showing that weight sparsity acts as a constant multiplier on model size in scaling laws, we demonstrate that this ``effective parameter'' scaling pattern extends to quantization as well. Specifically, we establish that weight-only quantization achieves strong parameter efficiency multipliers, while full quantization of both weights and activations shows diminishing returns at lower bitwidths. Our results suggest that different compression techniques can be unified under a common scaling law framework, enabling principled comparison and combination of these methods.
\end{abstract}

\begin{document}

\maketitle

% Incude paper content from external files
\input{content}

% Bibliography components
\bibliographystyle{abbrvnat}
\nobibliography*
\bibliography{main}

% Some other useful sections you might consider having in your report.


\end{document}
