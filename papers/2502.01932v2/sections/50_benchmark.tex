\begin{table*}[t]
    \centering
    \input{tabs/cooperative}
    \caption{Benchmark result of multi-agent cooperative tasks with different reward settings including without and with shaping reward. \textit{Bump and Pass} is evaluated by the number of bumps, \textit{Set the Spike (Easy)} and \textit{Set the Spike (Hard)} are evaluated by the success rate.}
    \label{tab:coop}
\end{table*}

\begin{table*}[t]
    \centering
    \input{tabs/competitive}
    \caption{Benchmark result of multi-agent competitive tasks including \textit{1 vs 1} and \textit{3 vs 3} with different evaluation metrics.}
    \label{tab:comp}
\end{table*}

We present extensive experiments to benchmark representative (MA)RL and game-theoretic algorithms in our VolleyBots testbed.
Specifically, for single-agent tasks, we benchmark two RL algorithms and compare their performance under different action space configurations. For multi-agent cooperative tasks, we evaluate four MARL algorithms and compare their performance with and without reward shaping. For multi-agent competitive tasks, we evaluate four game-theoretic algorithms and provide a comprehensive analysis across multiple evaluation metrics. We identify a key challenge in VolleyBots is the hierarchical decision-making process that requires both low-level motion control and high-level strategic play. We further show the potential of hierarchical policy in our VolleyBots testbed by implementing a simple yet effective baseline for the challenging \textit{3 vs 3} task.
Detailed discussion about the benchmark algorithms and more experiment results can be found in Appendix~\ref{app:alg} and \ref{app:exp}.

% We present extensive experiments to benchmark existing algorithms in our VolleyBots testbed. Specifically, for single-agent tasks, we benchmark RL algorithms and compare their performance under different action space configurations. For multi-agent cooperative tasks, we evaluate the performance of different MARL algorithms and compare their performance with and without reward shaping. For multi-agent competitive tasks, we evaluate a range of game-theoretic algorithms and provide a comprehensive analysis across multiple evaluation metrics. We identify a key challenge in VolleyBots is the hierarchical decision-making process that requires both low-level motion control and high-level strategic play. We further show the potential of hierarchical policy in our VolleyBots testbed by implementing a simple yet effective baseline for the challenging \textit{3 vs 3} task.
% More experiment and implementation details can be found in Appendix~\ref{app:exp}.

% \yc{it seems we have enough space, so we could add simulation performance here. fps of different tasks or we can put curves here.}
% \yc{the training hyperparameters can be found in Appendix.}

% \subsection{Results of Environment Setting}
\subsection{Results of Single-Agent Tasks}
% \xzl{@xiangmin check this subsection} \done

We evaluate two RL algorithms including Deterministic Policy Gradient (DDPG)~\cite{lillicrap2015continuous} and Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} in three single-agent tasks. We also compare their performance under different action spaces including CTBR and PRT. The averaged results over three seeds are shown in Table~\ref{tab:single}.

Using the same number of training frames, the performance of PPO and DDPG shows a clear distinction in all three tasks. PPO consistently achieves high task performance in all tasks, while DDPG struggles to learn effective policies that complete these tasks meaningfully. This disparity can be attributed to PPO's stable on-policy updates, which facilitate efficient exploration and robust learning, 
whereas DDPG's deterministic policy and reliance on off-policy updates result in different learning dynamics.
% whereas DDPG's deterministic policy and reliance on off-policy updates are less effective. \yc{I would suggest not to say DDPG is less effective.}

Comparing different action spaces, the final results indicate that PRT slightly outperforms CTBR in most tasks. This outcome is likely due to PRT providing more granular control over each motor's thrust, enabling the drone to maximize task-specific performance with precise adjustments. On the other hand, CTBR demonstrates a slightly faster learning speed in some tasks, as its higher-level abstraction simplifies the control process and reduces the learning complexity. For optimal task performance, we use PRT as the default action space in subsequent experiments. More experiment results and learning curves are provided in Appendix~\ref{app:single}.

% \subsection{Results of MARL Baselines}
\subsection{Results of Multi-Agent Cooperative Tasks}
% \xzl{@xiangmin check this subsection} \done

We evaluate four MARL algorithms including Multi-Agent DDPG (MADDPG)~\cite{lowe2017multi}, Multi-Agent PPO (MAPPO)~\cite{yu2022surprising}, Heterogeneous-Agent PPO (HAPPO)~\cite{kuba2021trust}, Multi-Agent Transformer (MAT)~\cite{wen2022multi} in three multi-agent cooperative tasks. We also compare their performance with and without reward shaping. The averaged results over three seeds are shown in Table~\ref{tab:coop}.

Comparing the MARL algorithms, on-policy methods like MAPPO, HAPPO, and MAT successfully complete all three cooperative tasks and exhibit comparable performance, while off-policy method like MADDPG fails to complete these tasks. These results are consistent with the observation in single-agent experiments, and we use MAPPO as the default algorithm in subsequent experiments for its consistently strong performance and efficiency.

As for different reward functions, it is clear that using reward shaping leads to better performance, especially in more complex tasks like \textit{Set and Spike (Hard)}. This is because the misbehave penalty and task reward alone are usually sparse and make exploration in continuous space particularly challenging. Such sparse setups can serve as benchmarks to evaluate the exploration ability of MARL algorithms. On the other hand, shaping rewards provide intermediate feedback that guides agents toward task-specific objectives more efficiently, and we use shaping rewards in subsequent experiments for efficient learning. More experimental results and learning curves are provided in Appendix~\ref{app:multi}.


% We benchmark several representative algorithms, including MADDPG, MAPPO, HAPPO, and MAT, on both single-agent and multi-agent cooperative tasks. Single-agent scenarios (e.g., \emph{Bumping} \yc{bump or bumping?} and \emph{Hit}) help establish a baseline for each algorithm's capacity to handle flight control and basic ball interaction. In cooperative tasks, such as \emph{Bump and Pass} and \emph{Attack (Easy)}, \yc{use the right name.} we assess how well these methods learn coordinated maneuvers and shared objectives. 

% [Analyze exp result].  

\subsection{Results of Multi-Agent Competitive Tasks}
We evaluate four game-theoretic algorithms including self-play (SP), Fictitious Self-Play (FSP)~\cite{heinrich2015fictitious}, Policy-Space Response Oracles (PSRO)~\cite{lanctot2017unified} with a uniform meta-solver (PSRO$_\text{Uniform}$), and a Nash meta-solver (PSRO$_\text{Nash}$) in multi-agent competitive tasks. Their performance is evaluated by approximate exploitability, the average win rate against other learned policies, and Elo rating. 
The results are shown in Table~\ref{tab:comp}. 
More results and implementation details are provided in Appendix~\ref{app:mix}.
% The averaged results over three seeds are shown in Table~\ref{tab:comp}. 
% More experimental results and implementation details are provided in Appendix~\ref{app:mix}.

In the \textit{1 vs 1} task, all algorithms manage to learn basic behaviors like returning the ball and positioning for subsequent volleys. However, the exploitability metrics indicate that the learned policies are still far from achieving a Nash equilibrium, suggesting that the strategies lack robustness in this two-player zero-sum game. This performance gap highlights the inherent challenge of hierarchical decision-making in this task, where drones must simultaneously execute precise low-level motion control and engage in high-level strategic gameplay under volleyball rules. This challenge presents new opportunities for designing algorithms that can better integrate hierarchical decision-making capabilities.

% \zrz{In the \textit{3 vs 3} task, algorithms may exhibit minimal progress, such as learning to serve the ball, but fail to produce other meaningful behaviors.}
In the \textit{3 vs 3} task, algorithms exhibit minimal progress, such as learning to serve the ball, but fail to produce other strategic behaviors.
% In the \textit{3 vs 3} task, only SP and FSP exhibit minimal progress, such as learning to serve the ball, while other algorithms fail to produce meaningful behaviors. 
This outcome underscores the compounded challenges in this scenario, where each team of three drones must not only cooperate internally but also compete against the opposing team. The increased difficulty of achieving high-level strategic play in such a mixed cooperative-competitive environment further amplifies the hierarchical challenges observed in \textit{1 vs 1}. As a result, the \textit{3 vs 3} task serves as a highly demanding benchmark that highlights the need for new approaches for learning in complex multi-agent settings with a hierarchical decision process.


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/crossplay.png}
%     \caption{(TODO) Competitive crossplay result.}
%     \label{fig:crossplay}
% \end{figure}

% For the competitive \emph{1-vs-1} scenario, we incorporate methods from self-play and population-based research. Specifically, we consider Self-play, Fictitious Self-Play (FSP), PSRO\_uniform, and PSRO\_Nash. Each approach addresses the turn-based adversarial nature of our environment by mixing or iterating over different strategies in pursuit of robust policies. 

% [Analyze exp result].
% \yc{@ruize, 3v3}

\subsection{Hierarchical Policy}

We further investigate hierarchical policies as a promising approach. Using the \textit{3 vs 3} task as an example, we first employ the PPO algorithm to develop a set of low-level skill policies, including \textit{Hover}, \textit{Serve}, \textit{Pass}, \textit{Set}, and \textit{Attack}. The details of low-level skills can be found in Appendix~\ref{app:low}. Next, we design a rule-based high-level strategic policy to assign low-level skills to each drone. For the \textit{Serve} and \textit{Attack} skills, the high-level policy also determines whether to hit the ball to the left or right, with an equal probability of 0.5 for each direction. Fig.~\ref{fig:hier} illustrates two typical demonstrations of the hierarchical policy attaching the \textit{Serve} skill to drone 1 in a serve scenario and the \textit{Attack} skill to drone 3 in a rally scenario.
In accordance with volleyball rules, the high-level policy uses an event-driven mechanism, triggering decisions whenever the ball is hit. 
We evaluate the average win rate of 1000 episodes where the hierarchical policy competes against the SP policy. The results show that the hierarchical policy achieves a significantly higher win rate of 86\%. While the current design of the hierarchical policy is in its early stages, it demonstrates substantial potential and offers valuable inspiration for future developments.
% By dividing strategy into lower-level motion control and higher-level tactical reasoning, hierarchical frameworks may better exploit the structure of volleyball dynamics. 
% We decompose the strategy into lower-level motion control skills and higher-level tactical reasoning policy for the \textit{3 vs 3} task, as demonstrated in Fig.~\ref{fig:hier}. Low-level control policies such as Hover, Serve, First Pass, Second Pass, and Attack are each trained using single-agent RL, with the same training budget as other competitive algorithm baselines. Moreover, we design a fixed high-level policy that is event-driven: when a drone hits the ball, the policy determines which low-level skill each drone should use. 
 % This hierarchical policy is evaluated by playing 1000 games against the SP policy that are unseen during the training of the hierarchical policy, demonstrating a significantly higher win rate, as shown in Tab.~\ref{tab:hier}. Although the current hierarchical policy design is still in its early stages, it has already shown promising results for this type of task, providing valuable insights that could inspire future solutions.

\begin{figure}[t]
    \centering
    \subfloat[\textit{1 vs 1}]{
        \includegraphics[width=0.47\linewidth]{figs/1v1_crossplay.pdf}} 
    % \hspace{0.05\linewidth}
    \hfill
    \subfloat[\textit{3 vs 3}]{
        \includegraphics[width=0.47\linewidth]{figs/3v3_crossplay.pdf}}
    \vspace{-2mm}
    \caption{Cross-play heatmap of multi-agent competitive tasks.}
    \label{fig:crossplay}
    \vspace{-2mm}
\end{figure}

% \begin{table}[h]
%     \centering
%     \input{tabs/hierarchy}
%     \caption{Average win rate of the hierarchical policy versus the SP policy in the \textit{3 vs 3} task.}
%     \label{tab:hier}
% \end{table}


% \subsection{Action space}
% thrust v.s. CTBR

% \subsection{MARL Single / Coop / Comp}
% MADDPG, MAPPO, HAPPO, MAT

% SP, FSP, PSRO\_Unif, PSRO\_Nash

% \subsection{Hierarchical}

% \subsection{Sim2Real}
