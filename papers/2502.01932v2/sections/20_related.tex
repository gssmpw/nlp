\begin{table*}[t]
    \centering
    \input{tabs/comparison}
    \caption{Comparison of VolleyBots and existing representative MARL testbeds.}
    \label{tab:comparison}
\end{table*}

\subsection{Reinforcement Learning for Drone Control Task}
Executing precise and agile flight maneuvers is essential for drones, which has driven the development of diverse control strategies~\cite{bouabdallah2004design,7989202,hwangbo2017control}. Among these, RL has shown significant promise, offering flexibility and efficiency in drone control. 
% Unlike traditional methods, RL-based policies can directly map observations to actions, eliminating the need for detailed system dynamics modeling or strict actuation constraints.
Drone racing is a notable single-drone control task where RL has achieved human-level performance~\cite{kaufmann2023champion}, showcasing near-time-optimal decision-making capabilities. Beyond racing, researchers also leveraged RL for executing aggressive flight maneuvers~\cite{sun2022aggressive} and achieving hovering stabilization under highly challenging conditions~\cite{hwangbo2017control}. As for multi-drone tasks, RL has been applied to cooperative tasks such as formation maintenance~\cite{swarm-formation}, as well as more complex scenarios like multi-drone pursuit-evasion tasks~\cite{chen2024multiuavpursuitevasiononlineplanning}, further showcasing its potential to jointly optimize task-level planning and control.
In this paper, we present VolleyBots, an MARL testbed designed to study the novel drone control task of drone volleyball. This task introduces unique challenges, requiring drones to learn both cooperative and competitive strategies at the task level while maintaining agile and precise control. Additionally, VolleyBots provides a comprehensive platform with baseline implementations of (MA)RL and game-theoretic algorithms, as well as support for sim-to-real transfer, facilitating the development and evaluation of advanced drone control strategies.


% difference: multi, strategic hard
\subsection{MARL Testbeds}
Many existing MARL testbeds focus on fully simulated environments without real-world physical interactions. For instance, AlphaGo~\cite{silver2016mastering} explores the turn-based game of Go, sparking deep RL research. Subsequent environments like SMAC~\cite{samvelyan2019starcraft,ellis2024smacv2}, Overcooked~\cite{carroll2019utility}, and Hanabi~\cite{bard2020hanabi} simulate cooperative scenarios using video games or card games. Multi-agent Particle Environments (MPE)~\cite{lowe2020multiagentactorcriticmixedcooperativecompetitive} and GRF~\cite{kurach2020google} provide a variety of cooperative, competitive, and mixed cooperative-competitive tasks. These testbeds emphasize high-level decision-making and largely focus on discrete action spaces, overlooking real-world continuous control tasks with physical constraints.

Recently, several MARL environments have been proposed to incorporate real-world physics and interactions. For example, MAMuJoCo~\cite{peng2021facmac} offers multi-agent continuous control tasks, and Bi-DexHands~\cite{chen2022humanlevelbimanualdexterousmanipulation} focuses on dexterous bimanual manipulation with robotic hands. While these testbeds explore continuous action spaces, they are limited to cooperative tasks. Competitive sports, much like their societal role in humans, provide a standard way to evaluate decision-making under dynamic, rule-constrained, and physically realistic conditions. Some works have explored this direction, such as Robot Table Tennis~\cite{d2024achieving}, achieving near human-level performance in ping-pong, and Humanoid Football~\cite{liu2022motor,haarnoja2024learning}, which models 2 vs 2 football with humanoid robots. Despite bringing MARL closer to real-world applications, these platforms often lack open-source simulations or real-world deployment support. Open-source testbeds like SMPLOlympics~\cite{luo2024smplolympicssportsenvironmentsphysically} offer physically simulated environments for humanoids to compete in Olympic sports, and MQE~\cite{xiong2024mqe} proposes multi-agent locomotion tasks with quadrupeds, including 1 vs 1 and 2 vs 2 football competitions.
However, existing testbeds primarily focus on robotic arms, quadrupeds, and humanoids, leaving high-mobility, agile robot platforms like drones underexplored.

To address these gaps, we introduce VolleyBots, a turn-based, drone-focused sports environment featuring high-level decision-making and low-level continuous control. A comprehensive comparison between existing MARL testbeds and VolleyBots is shown in Table~\ref{tab:comparison}. Built on a realistic physical simulator, VolleyBots supports real-world deployment and offers a complementary testbed for advancing MARL research with agile robotic platforms.

% \subsection{MARL \yc{Testbeds}}
% \yc{Many existing MARL testbeds focus on fully simulated environments without real-world physical interactions. For instance, AlphaGo~\cite{silver2016mastering} explores the turn-based game of Go, sparking deep RL research. Subsequent environments like SMAC~\cite{samvelyan2019starcraft,ellis2024smacv2}, Overcooked~\cite{carroll2019utility}, and Hanabi~\cite{bard2020hanabi} simulate cooperative scenarios using video games or card games. Multi-agent Particle Environments (MPE)~\cite{lowe2020multiagentactorcriticmixedcooperativecompetitive} and GRF~\cite{kurach2020google} provide a variety of cooperative, competitive, and mixed cooperative-competitive tasks. These testbeds emphasize high-level decision-making and largely focus on discrete action spaces, overlooking real-world continuous control tasks.}

% \yc{Recently, several MARL environments have been proposed to incorporate real-world physics and interactions. For example, MAMuJoCo~\cite{peng2021facmac} offers multi-agent continuous control tasks, and Bi-DexHands~\cite{chen2022humanlevelbimanualdexterousmanipulation} focuses on dexterous bimanual manipulation with robotic hands. While these testbeds explore continuous action spaces, they are limited to cooperative tasks. Competitive sports, much like their societal role in humans, provide a standard way to evaluate decision-making under dynamic, rule-constrained, and physically realistic conditions. Some works have explored this direction, such as Robot Table Tennis~\cite{d2024achieving}, achieving near human-level performance in ping-pong, and Humanoid Football~\cite{liu2022motor,haarnoja2024learning}, which models 2 vs. 2 football with humanoid robots. Despite bringing MARL closer to real-world applications, these platforms often lack open-source simulations or real-world deployment support. Open-source testbeds like SMPLOlympics~\cite{luo2024smplolympicssportsenvironmentsphysically} offer physically simulated environments for humanoids to compete in Olympic sports, and MQE~\cite{xiong2024mqe} proposes multi-agent locomotion tasks with quadrupeds, including 1 vs. 1 and 2 vs. 2 football competitions.
% However, existing testbeds primarily focus on humanoids and quadrupeds, leaving high-mobility, agile robots like drones underexplored.}

% \yc{To address these gaps, we introduce VolleyBots, a turn-based, drone-focused sports environment featuring high-level decision-making and low-level continuous control. A comprehensive comparison between existing MARL testbeds and VolleyBots is shown in Table~\ref{tab:comparison}. Built on a realistic physical simulator, VolleyBots supports real-world deployment and offers a complementary testbed for advancing MARL research with agile robotic platforms.}

% A number of popular multi-agent reinforcement learning benchmarks focus on purely simulated environments without real-world physical interactions. 
% AlphaGo~\cite{silver2016mastering} considers the ancient turn-based competitive game of Go and sparked a wave of deep reinforcement learning research.
% Several MARL environments are then proposed to advance research on multi-agent cooperation. 
% StarCraft Multi-Agent Challenge (SMAC)~\cite{samvelyan2019starcraft,ellis2024smacv2} considers the well-known real-time strategy game, Overcooked~\cite{carroll2019utility} simulates the popular video game, and Hanabi uses a turn-based card game as environments for study on multi-agent cooperation. Multi-agent Particle Environment (MPE)~\cite{lowe2020multiagentactorcriticmixedcooperativecompetitive} is an intuitive and accessible testbed designed for studying MARL algorithms in a simplified particle-based setting.
% Google Research Football (GRF)~\cite{kurach2020google} models after popular football video games and proposes a variety of cooperative, competitive, and mixed cooperative-competitive tasks.
% While these benchmarks have propelled MARL research through their accessibility and controlled complexity, most of them remain purely simulated without considering real-world dynamics and physical interactions.
% In addition, these environments mainly consider discrete action spaces, making them unsuitable for studying continuous control tasks.


% In contrast, several MARL environments integrate real-world physics and interactions into their design. For instance, MAMuJoCo~\cite{peng2021facmac} provides continuous control challenges using a multi-agent MuJoCo simulator, and Bi-DexHands~\cite{chen2022humanlevelbimanualdexterousmanipulation} emphasizes dexterous bimanual manipulation on robotic hands. A growing line of work explores robot sports, where robotic agents cooperate and compete in sports-like tasks.
% Robot Table Tennis~\cite{d2024achieving} achieves near human-level performance in real-world ping-pong using a robotic arm, while Humanoid Football~\cite{liu2022motor,haarnoja2024learning} models 2 vs 2 football with humanoid robots in simulation, showcasing human-like movement and behavior. The Multi-agent Quadruped Environment (MQE)~\cite{xiong2024mqe} further expands on multi-agent quadrupedal tasks, including 1 vs 1 and 2 vs 2 football competitions. Although these sports-oriented platforms bring MARL closer to actual robotic deployment, they often lack open-source simulation or real-world deployment. In addition, existing works have limited exploration of tasks with high-mobility and agile robots like drones.
% In response, our proposed VolleyBots environment introduces a turn-based, drone-centered sports setting with continuous control and real-world deployment support, offering a complementary testbed for high-mobility, agile platforms in multi-agent research.
% A comprehensive comparison between existing environments and our VolleyBots environment is shown in Table~\ref{tab:comparison}.

