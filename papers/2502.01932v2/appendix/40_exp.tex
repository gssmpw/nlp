\begin{table}[t]
    \centering
    \input{tabs/maddpg_hyperparameters}
    \label{tab:app:maddpg_hyperparameters}
\end{table}

\subsection{Hyperparameters of Benchmarking Algorithms}

\subsubsection{Single-Agent Tasks.}
% \yc{@xiangmin} \done
The hyperparameters adopted for DDPG and PPO in the single-agent tasks are listed in Table~\ref{tab:app:maddpg_hyperparameters}-\ref{tab:app:multi_different_hyperparameters}. All algorithms are trained for $5\times10^{8}$ environment steps in each task. 

\subsubsection{Multi-Agent Cooperative Tasks}
% \yc{@xiangmin} \done
The hyperparameters adopted for different algorithms in multi-agent cooperative tasks are listed in Table~\ref{tab:app:maddpg_hyperparameters}-\ref{tab:app:multi_different_hyperparameters}. All algorithms are trained for $1\times10^{9}$ environment steps in each task. 

\begin{table}[t]
    \centering
    \input{tabs/multi_hyperparameters}
    \label{tab:app:multi_hyperparameters}
\end{table}

\begin{table}[t]
    \centering
    \input{tabs/multi_different_hyperparameters}
    \label{tab:app:multi_different_hyperparameters}
\end{table}

\subsubsection{Multi-Agent Competitive Tasks.}
% \yc{@huining, ruize} \done

\paragraph{Training.}
For self-play (SP) in \textit{1 vs 1} and \textit{3 vs 3} competitive tasks, we adopt the MAPPO algorithm with shared actor networks and shared critic networks between two teams, in order to make sure two teams utilize the same policy. Also, we transform the samples from both sides into symmetric ones and then use these symmetric samples to update the network together. The hyperparameters employed here are the same as those used in the MAPPO algorithm for multi-agent cooperative tasks.

The PSRO algorithm for \textit{1 vs 1} competitive task instantiates a PPO agent for training one of the two drones while the other drone maintains a fixed policy. Similarly, the PSRO algorithm for the \textit{3 vs 3} task assigns each team to be controlled by MAPPO. We adopt the same set of hyperparameters listed in Table \ref{tab:app:multi_different_hyperparameters} for the (MA)PPO agent. In each iteration, the (MA)PPO agent is trained against the current population. Here, we offer two versions of meta-strategy solver, PSRO\textsubscript{Uniform} and PSRO\textsubscript{Nash}. Training is considered converged when the agent achieves over 90\% win rate with a standard deviation below 0.05. The iteration ends when the agent reaches convergence or reaches a maximum of iteration steps of 5000. The trained actor is then added to the population for the next iteration. 

For Fictitious Self-Play (FSP) in competitive tasks, we slightly modify PSRO\textsubscript{Uniform} so that in each iteration, the (MA)PPO agent inherits the learned policy from the previous iteration as initialization. Naturally, other hyperparameters and settings remain the same for a fair comparison.

In both \textit{1 vs 1} and \textit{3 vs 3} tasks, the algorithm leverages 2048 parallel environments to accelerate the training process. In this work, we report the results of different algorithms given a total budget of $1\times 10^{9}$ environmental steps.

% \paragraph{Evaluation in Multi-Agent Competitive Tasks}
% \yc{@huining, ruize} \done
\paragraph{Evaluation.}
The evaluation of exploitability requires evaluating the payoff of the best response (BR) over the trained policy or population from different algorithms. Here, we approximate the BR to each policy or population by learning an additional RL agent against the trained policy or population. In practice, this is done by performing an additional iteration of PSRO, where the opponent is fixed as the trained policy/population. In order to approximate the ideal BR as closely as possible, we initialize the BR policy with the latest FSP policy, given that FSP yields the best empirical performance in our experiments. We train the BR policy for $5000$ training step with $2048$ parallel environments. We disable the convergence condition for early termination and report the evaluated win rate to calculate the approximate exploitability. Importantly, to approximate the BR of the trained SP policy in the \textit{3 vs 3} task, we employ two distinct BR policies for the serve and rally scenarios, respectively. For the BR to serve, we directly use the latest FSP policy without further training, while for the BR to rally, we train a dedicated policy against the SP policy. The overall win rate of this BR is then computed as the average win rate across these two scenarios, given that each side has an equal serve probability of $0.5$.

% \xzl{@ruize evaluation parameters of cross-play.} \done
We run 1,000 games for each pair of policies to generate the cross-play win rate heatmap, covering 6 matchup scenarios, resulting in a total of 6,000 games. In each game, both policies are sampled from their respective policy populations based on the meta-strategy and play until a winner is determined.

Moreover, we use an open-source Elo implementation~\cite{HankSheehan_EloPy}. The coefficient $K$ is set to $168$, and the initial Elo rating for all policies is $1000$. We conduct $12000$ games among four policies. The number of games played between any two policies is guaranteed to be the same. Specifically, in each round, $6$ different matchups are played. Each policy participates in 3 matchups, competing against different opponent policies. A total of $2000$ rounds are carried out, amounting to $12000$ games in total. The game results are sampled and generated based on the cross-play results.

\subsection{Results of Single-Agent Tasks}
\label{app:single}
% \yc{@xiangmin, put training curves here.} \done 

% \yc{@xiangmin add some analysis.} \done

The training curves of DDPG and PPO in single-agent tasks are shown in Fig.~\ref{fig:single_tasks}. It can be seen that PPO outperforms DDPG in all tasks. For example, in Back and Forth, using PPO, the number of times the agent reaches the target anchor stabilizes around $9-10$, and the task is successfully completed. In contrast, with DDPG, the agent only completes half of the return motion, as the drone ends up flying out of bounds. In Hit the Ball, with PPO, the hitting distance stabilizes around $10-11\,m$, and the ball is almost always hit in a straight line. However, with DDPG, the landing spot of the ball is less controllable. In Solo Bump, with PPO, the ball juggles smoothly, reaching a height of $4\,m$, while DDPG almost fails to juggle properly and can only manage a single hit. CTBR and PRT are comparable. in Back and Forth and Hit the Ball PRT's final result is better, in Solo Bump is comparable but CTBR learns faster.

% \xzl{e.g. (1) PPO is much better than DDPG in all tasks, list some number in each task. describe the behaviors of different algorithms in the video.
% (2) CTBR and PRT are comparable. in Back and Forth and Hit the Ball PRT's result is better, in xxx is comparable but CTBR learns faster, balabala. describe the behaviors of different action space in the video}


\begin{figure}[t]
    \centering
    \subfloat[\textit{Back and Forth}]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/single_tasks/bnf.pdf}}
    % \hspace{0.03\linewidth}
    \subfloat[\textit{Hit the Ball}]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/single_tasks/hit.pdf}} 
    \subfloat[\textit{Solo Bump}]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/single_tasks/bump.pdf}}
    % \hspace{0.03\linewidth}
    \caption{Training curves of single-agent tasks over three seeds.}
    \label{fig:single_tasks}
\end{figure}

\begin{figure}[t]
    \centering
    \subfloat[\textit{Bump and Pass} w.o. shaping]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/multi_tasks/bmp_wo_shaping.pdf}}
    \subfloat[\textit{Set and Spike (Easy)} w.o. shaping]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/multi_tasks/sns_easy_wo_shaping.pdf}}
    \subfloat[\textit{Set and Spike (Hard)} w.o. shaping]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/multi_tasks/sns_hard_wo_shaping.pdf}}
    % 第二行子图
    \par
    \subfloat[\textit{Bump and Pass} w. shaping]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/multi_tasks/bmp_w_shaping.pdf}}
    \subfloat[\textit{Set and Spike (Easy)} w. shaping]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/multi_tasks/sns_easy_w_shaping.pdf}} 
    \subfloat[\textit{Set and Spike (Hard)} w. shaping]{
        \centering
        \includegraphics[width=0.3\linewidth]{figs/multi_tasks/sns_hard_w_shaping.pdf}} 
    \caption{Training curves of multi-agent cooperative tasks over three seeds.}
    \label{fig:multi_tasks}
\end{figure}


\subsection{Results of Multi-Agent Cooperative Tasks}
\label{app:multi}
% \yc{@xiangmin, put training curves here.} \done
The training curves of different algorithms in multi-agent tasks are shown in Fig.~\ref{fig:multi_tasks}. It can be seen that MAPPO, HAPPO, and MAT are able to complete the tasks relatively well, while MADDPG can only make the setter complete one hit, with the attacker unable to receive the ball. In Bump and Pass w.o. shaping, MAPPO maintains a fast training process and outperforms HAPPO and MAT in terms of final results. The same is true in Set and Spike (Easy and Hard) w.o. shaping. In Set and Spike (Easy) w. shaping, MAPPO, and HAPPO perform similarly, with slightly faster learning speeds than MAT. In Set and Spike (Hard) w. shaping task, HAPPO only succeeds in overcoming the defense racket in one seed, resulting in a success rate of approximately $0.82$. At this point, the ball can be quickly spiked into the opponent's court and the attacker doesn't touch the net. Overall, the success rate is slightly higher than MAPPO and MAT, as these two algorithms almost never successfully overcome the defense racket.

Additionally, we can observe that the presence of shaping rewards has a significant impact on task results. Adding shaping rewards clearly improves the performance and accelerates the learning process. In Bump and Pass, the task learns slower without shaping rewards because the policy must explore which direction to hit the ball, requiring many more steps. The hit direction reward in shaping rewards accelerates this process. In Set and Spike (Easy), all algorithms without shaping rewards have a success rate of only $0.25$ because they only learn to make the setter hit the ball, but not toward the hitter. As a result, the attacker fails to hit the ball. The hit direction reward in shaping rewards also helps accelerate this process. In Set and Spike (Hard), HAPPO achieves a success rate of about $0.82$, while MAPPO and MAT are slightly worse, at $0.75$, because the defense racket is strong and successfully intercepts their attacks.

% \xzl{one paragraph for each task, in this paragraph (1) compare different algorithms (which is better or comparable...) (2) discuss the difference between w. and w.o. shaping, and their behaviors in the video. for example, in Bump and Pass, why do all algorithms w.o. shaping learn slower than w. shaping (because w.o. shaping reward the policy has to explore which direction to hit the ball, this will take many more steps. But the hit direction reward in shaping reward can help accelerate this process). In Set and Spike (Easy), why do all algorithms w.o. shaping reward only has a success rate of 0.25? (because they only learn to let the setter hit the ball, but not towards the hitter.) In Set and Spike (Hard), why do all algorithms with reward shaping only have a success rate of 0.75? (Because the defense board is strong and successfully intercepts their attack), ...}

% \yc{@xiangmin add some analysis.} \done


% \begin{figure}[t]
%     \centering
%     \subfloat[Bump and Pass w.o. shaping Reward]{
%         \centering
%         \includegraphics[width=0.45\linewidth]{figs/multi_tasks/Bump and Pass w.o. shaping Reward.png}}
%     \subfloat[Bump and Pass w. shaping Reward]{
%         \centering
%         \includegraphics[width=0.45\linewidth]{figs/multi_tasks/Bump and Pass w. shaping Reward.png}} 
%     % 第二行子图
%     \par
%     \subfloat[Set and Spike (Easy) w.o. shaping Reward]{
%         \centering
%         \includegraphics[width=0.45\linewidth]{figs/multi_tasks/Set and Spike (Easy) w.o. shaping Reward.png}}
%     \subfloat[Set and Spike (Easy) w. shaping Reward]{
%         \centering
%         \includegraphics[width=0.45\linewidth]{figs/multi_tasks/Set and Spike (Easy) w. shaping Reward.png}} 
%     % 第三行子图
%     \par
%     \subfloat[Set and Spike (Hard) w.o. shaping Reward]{
%         \centering
%         \includegraphics[width=0.45\linewidth]{figs/multi_tasks/Set and Spike (Hard) w.o. shaping Reward.png}}
%     \subfloat[Set and Spike (Hard) w. shaping Reward]{
%         \centering
%         \includegraphics[width=0.45\linewidth]{figs/multi_tasks/Set and Spike (Hard) w. shaping Reward.png}} 
%     \caption{Training curves of multi-agent tasks.}
%     \label{fig:multi_tasks}
% \end{figure}

\subsection{Results of Multi-Agent Competitive Tasks}
\label{app:mix}
We provide a more detailed win rate evaluation of the PSRO populations from the \textit{1 vs 1} task in Fig.~\ref{fig:1v1heatmap}, where each policy in the PSRO population is evaluated against all other policies. In these heatmaps, the ordinate and abscissa represent the policy for drone 1 and drone 2 respectively. The heat of cells represents the evaluated win rate of drone 1, i.e. red means a higher win rate and blue means a lower win rate. Intuitively, each row represents a policy's performance against each policy of the population while playing as drone 1. A red cell indicates that the drone 1 policy outperforms the specific drone 2 policy. A full red row means that the policy outperforms all other policies.

% \xzl{@huining add more description to help the reader understand this map, for example, red means higher win rate and blue means lower win rate. For each row, more red cells mean outperforming more policies. A full red row means the policy outperforms all other policies... }\done

Evidently, FSP attains more iterations than PSRO\textsubscript{Uniform} and PSRO\textsubscript{Nash} given a budget of $1\times10^9$ steps, which yields a faster convergence speed. This advantage comes from the fact that FSP inherits the learned policy from the previous iteration, which serves as an advantageous initialization for the current iteration. In contrast, PSRO\textsubscript{Uniform} and PSRO\textsubscript{Nash} start from scratch in each iteration, which poses a challenge for the algorithm to converge and introduces more variance in the training process.

Moreover, in PSRO algorithms, as the learned policy gradually improves with each iteration, the most recent policy of the population naturally poses greater difficulty for subsequent iterations. Therefore, PSRO\textsubscript{Nash} tends to put more weight on the most recent policy in the meta-strategy. 
% \yc{explain how we can see this?}\done 
This in turn has an effect on the learning of new policies. We can observe the outcomes in the heatmaps: for each row, the win rate against the most recent policy is often higher than the others. In FSP, on the other hand, the win rate against each policy is more evenly distributed, indicating that the population is potentially more balanced and stable.


\begin{figure}[t]
    \centering
    % 第一行子图
    \subfloat[FSP]{
        \centering
        \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/fsp_seed_300.png}
    }
    \subfloat[PSRO\textsubscript{Uniform}]{
        \centering
        \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/uniform_seed_300.png}
    }
    \subfloat[PSRO\textsubscript{Nash}]{
        \centering
        \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/nash_seed_300.png}
    }
    % % 第二行子图
    % \par
    % \subfloat[PSRO\textsubscript{Uniform} seed=1]{
    %     \centering
    %     \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/uniform_seed_100.png}
    % }
    % \subfloat[PSRO\textsubscript{Uniform} seed=2]{
    %     \centering
    %     \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/uniform_seed_200.png}
    % }
    % \subfloat[PSRO\textsubscript{Uniform} seed=3]{
    %     \centering
    %     \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/uniform_seed_300.png}
    % }
    % % 第三行子图
    % \par
    % \subfloat[PSRO\textsubscript{Nash} seed=1]{
    %     \centering
    %     \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/nash_seed_100.png}
    % }
    % \subfloat[PSRO\textsubscript{Nash} seed=2]{
    %     \centering
    %     \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/nash_seed_200.png}
    % }
    % \subfloat[PSRO\textsubscript{Nash} seed=3]{
    %     \centering
    %     \includegraphics[width=0.25\textwidth]{figs/1v1_heatmap/nash_seed_300.png}
    % }
    \caption{Win rate heatmaps of the population in the \textit{1 vs 1} task.}
    \label{fig:1v1heatmap}
\end{figure}

\subsection{Low-level Skills of Hierarchical Policy}
\label{app:low}
% \yc{@ruize, write low-level skills} \done

Low-level skills are derived through PPO training, while the high-level skill is implemented as a rule-based, event-driven policy that determines which drone utilizes which skill in response to the current game state. In accordance with the \textit{3 vs 3} task setting, each team consists of three drones positioned as front-left, front-right, and backward within their half of the court. Below, we describe each low-level skill and explain when it is utilized by the high-level policy. 

\paragraph{Hover.}
The \textit{Hover} skill is designed to enable the drone to hover around a specified target position. This skill takes a three-dimensional target position as input. The skill is frequently utilized by the high-level policy. For instance, in the serve scenario, only the serving drone uses the \textit{Serve} skill, while the other two teammates use the \textit{Hover} skill to remain at their respective anchor points.

\paragraph{Serve.}
The \textit{Serve} skill is designed to enable the drone to serve the ball towards the opponent’s side of the court. This skill includes a one-hot target input that determines whether to serve the ball to the left side or the right side of the opponent’s court.
In accordance with the \textit{3 vs 3} task setting, for the \textit{Serve} skill, the ball is initialized at a position $3\,\mathrm{m}$ directly above the serving drone, with zero initial velocity. 
This skill is exclusively utilized by the high-level policy during the serve scenario, during which the designated serving drone employs the \textit{Serve} skill at the start of a match.

\paragraph{Pass.}
The \textit{Pass} skill is designed to handle the opponent’s serve or attack by allowing the drone to make the first contact of the team’s turn and pass the ball to a teammate. Here, this skill is exclusively used by the backward drone, which is responsible for hitting the ball to the front-left teammate. The high-level policy designates the backward drone to utilize this skill whenever the opponent hits the ball.

\paragraph{Set.}
The \textit{Set} skill is designed to transfer the ball from the passing drone to the attacking drone, serving as the second contact in the team’s turn. In our design, the front-left drone utilizes the \textit{Set} skill to pass the ball from the backward drone to the front-right drone. The high-level policy designates the front-left drone to utilize this skill whenever the backward drone successfully makes contact with the ball.

\paragraph{Attack.}
The \textit{Attack} skill is designed to hit the ball towards the opponent’s court, serving as the third and final contact in the team’s turn. This skill includes a one-hot target input that specifies whether to direct the ball to the left side or the right side of the opponent’s court. In our design, the front-right drone uses the \textit{Attack} skill to strike the ball after receiving it from the front-left drone. The high-level policy assigns the front-right drone to utilize this skill whenever the front-left drone successfully hits the ball.


\subsection{Sim-to-Real}
% \yc{@shilong} \done

The sim-to-real gap presents a significant challenge in RL-based robotic control, as policies trained in simulation often underperform when deployed in the real world. We utilize the \textit{Solo Bump} task as an initial demonstration to showcase the potential of transferring trained policies to real-world scenarios. To bridge the sim-to-real gap, we apply system identification to accurately model the ball's behavior during its impact with the racket.

The impact between the ball and the racket is modeled as an impulse acting in the direction of the normal of the racket $\bm{n}_c$, which means that the tangential velocity of the ball relative to the racket remains constant but the normal velocity changes, determined by the restitution coefficient. Since the mass of the racket and vehicle (rigidly mounted together) is much larger than that of the ball, it is assumed that the velocity of the racket remains unaffected. 
Denoting the ball's pre- and post-impact normal velocities relative to the racket as $\bm{v}_{ball\_pre\_n}$ and $\bm{v}_{ball\_post\_n}$, the restitution coefficient can be derived as follows:

\begin{align}
\beta &= -\frac{\bm{v}_{ball\_post\_n}^T\bm{n}_c}{\bm{v}_{ball\_pre\_n}^T\bm{n}_c}
\end{align}

Specifically, the restitution coefficient was measured through multiple experiments in which the racket and vehicle were fixed on the flat ground and the ball started a free fall from different heights right above the center of the racket. The pre-impact and post-impact velocity of the ball was collected using a motion capture system and the restitution coefficient was calculated using the above equation. We used the same ball and racket in all experiments.

The average restitution coefficient was $0.8$, which we used in the simulation. The highest and lowest were $0.85$ and $0.75$, respectively, and the deviation was relatively small and acceptable. Experiments also showed that the ball’s tangential velocity did change during impact, which was likely due to the friction between the ball and the racket. In addition, the asymmetric relative position from racket strings to the ball could lead to asymmetric force upon the ball, causing it to move tangentially. We model the change of tangential velocity of the ball during impact as a stochastic progress and utilize small randomization in the ball's rebound velocity after each collision in the simulation.
