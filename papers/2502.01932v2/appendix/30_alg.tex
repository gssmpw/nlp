\subsection{Reinforcement Learning Algorithms}

To explore the capabilities of our testbed while also providing baseline results, we implement and benchmark a spectrum of popular RL and game-theoretic algorithms on the proposed tasks.

\paragraph{Single-Agent RL.}
In single-agent scenarios, we consider two commonly used algorithms in continuous control tasks. Deep Deterministic Policy Gradient (DDPG)~\cite{lillicrap2015continuous} is an off-policy actor-critic approach relying on a deterministic policy and an experience replay buffer to handle continuous actions. Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} adopts a clipped objective to stabilize on-policy learning updates by constraining policy changes. 
% Soft Actor-Critic (SAC)~\cite{haarnoja2018soft} augments an off-policy actor-critic framework with a maximum-entropy objective, facilitating efficient exploration. 
Overall, these methods provide contrasting paradigms for tackling single-agent continuous tasks.

\paragraph{Multi-Agent RL.}
For tasks with multiple drones, we evaluate four representative multi-agent algorithms. Multi-Agent DDPG (MADDPG)~\cite{lowe2017multi} extends DDPG with a centralized critic for each agent, while policies remain decentralized. Multi-Agent PPO (MAPPO)~\cite{yu2022surprising} incorporates a shared value function to improve both coordination and sample efficiency. Heterogeneous-Agent PPO (HAPPO)~\cite{kuba2021trust} adapts PPO techniques to handle distinct roles or capabilities among agents. Multi-Agent Transformer (MAT)~\cite{wen2022multi} leverages a transformer-based architecture to enable attention-driven collaboration. Taken together, these algorithms offer a diverse set of baselines for multi-agent cooperation.

\paragraph{Game-Theoretic Algorithms.}
For multi-agent competitive tasks, we consider several representative game-theoretic algorithms in the literature~\cite{zhang2024survey}. Self-play (SP) trains agents against the current version of themselves, allowing a single policy to evolve efficiently. Fictitious Self-Play (FSP)~\cite{heinrich2015fictitious} trains agents against the average policy by maintaining a pool of past checkpoints. Policy-Space Response Oracles (PSRO)~\cite{lanctot2017unified} iteratively add the best responses to the mixture of a growing policy population. The mixture policy is determined by a meta-solver. PSRO$_\text{uniform}$ uses a uniform meta-solver that samples policies with equal probability, while PSRO$_\text{Nash}$ uses a Nash meta-solver that samples policies according to the Nash equilibrium. These methods provide an extensive benchmark for game-theoretic algorithms in multi-agent competition with both motion control and strategic play. There are also some algorithms like Team-PSRO~\cite{mcaleer2023team} and Fictitious Cross-Play (FXP)~\cite{xu2023fictitious} that are designed specifically for mixed cooperative-competitive games and can be integrated in our testbed in future work.

