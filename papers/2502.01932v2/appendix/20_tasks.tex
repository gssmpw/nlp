\begin{table}[t]
    \centering
    \input{tabs/bnf_reward}
    \caption{Reward of single-agent \textit{Back and Forth} task.}
    \label{tab:app:bnf_reward}
\end{table}

\begin{table}[t]
    \centering
    \input{tabs/hit_reward}
    \caption{Reward of single-agent \textit{Hit the Ball} task.}
    \label{tab:app:hit_reward}
\end{table}

\begin{table}[t]
    \centering
    \input{tabs/bump_reward}
    \caption{Reward of single-agent \textit{Solo Bump} task.}
    \label{tab:app:bump_reward}
\end{table}

\begin{table}[t]
    \centering
    \input{tabs/bnp_reward}
    \caption{Reward of multi-agent \textit{Bump and Pass} task.}
    \label{tab:app:bnp_reward}
\end{table}

\begin{table}[t]
    \centering
    \input{tabs/sns_easy}
    \caption{Reward of multi-agent \textit{Set and Spike (Easy)} task.}
    \label{tab:app:sns_easy}
\end{table}

\begin{table}[t]
    \centering
    \input{tabs/sns_hard}
    \caption{Reward of multi-agent \textit{Set and Spike (Hard)} task.}
    \label{tab:app:sns_hard}
\end{table}

\begin{table}[t]
    \centering
    \input{tabs/1v1_reward}
    \caption{Reward of multi-agent \textit{1 vs 1} task.}
    \label{tab:app:1v1_reward}
\end{table}

\begin{table}[t]
    \centering
    \input{tabs/3v3_reward}
    \caption{Reward of multi-agent \textit{3 vs 3} task.}
    \label{tab:app:3v3_reward}
\end{table}

\subsection{Back and Forth}
% \xzl{@xiangmin write this subsection. check other tasks for reference.} \done

\paragraph{Task Definition.}
The drone is initialized at an anchor position $(4.5, 0, 2)$, i.e., the center of the red court with a height of $2\,m$. The other anchor position is $(9.0, 4.5, 2)$, with the target points switching between two designated anchor positions. The drone is required to sprint between two designated anchors to complete as many round trips as possible. $5$ steps within a sphere with a $0.6\,m$ radius near the anchor position are required for each stay. The maximum episode length is $800$ steps.
\paragraph{Observation and Reward.}
When the action space is Per-Rotor Thrust(PRT), the observation is a vector of dimension $26$, which includes the drone's root state and its relative position to the target anchor. When the action space is Collective Thrust and Body Rates (CTBR), the observation dimension is reduced to $22$, excluding the drone’s throttle. The detailed description of the reward function of this task is listed in Table~\ref{tab:app:bnf_reward}.
\paragraph{Evaluation Metric.}
This task is evaluated by the number of target points reached within the time limit. A successful stay is defined as the drone staying $5$ steps within a sphere with a $0.6\,m$ radius near the target anchor.


\subsection{Hit the Ball}
% \xzl{@xiangmin check for errors} \done

\paragraph{Task Definition.}
The drone is initialized randomly around an anchor position $(4.5, 0, 2)$, i.e., the center of the red court with a height of $2\,m$. The drone's initial position is sampled uniformly random from $[4, -0.5, 1.8]$ to $[5, 0.5, 2.2]$. The ball is initialized at $(4.5, 0, 5)$, i.e., $3\,m$ above the anchor position. The ball starts with zero velocity and falls freely. The drone is required to perform a single hit to strike the ball toward the opponent’s court, i.e., in the negative direction of the x-axis, aiming for maximum distance. The maximum episode length is $800$ steps.

\paragraph{Observation and Reward.}
When the action space is Per-Rotor Thrust(PRT), the observation is a vector of dimension $32$, which includes the drone's root state, the drone's relative position to the anchor, the ball's relative position to the drone, and the ball's velocity. When the action space is Collective Thrust and Body Rates (CTBR), the observation dimension is reduced to $28$, excluding the drone’s throttle. The detailed description of the reward function of this task is listed in Table~\ref{tab:app:hit_reward}.

\paragraph{Evaluation Metric.}
This task is evaluated by the distance between the ball's landing position and the anchor position. The ball’s landing position is defined as the intersection of its trajectory with the plane $z = 2$.


\subsection{Solo Bump}
% \xzl{@xiangmin fill in the blank} \done

\paragraph{Task Definition.}
The drone is initialized randomly around an anchor position $(4.5, 0, 2)$, i.e., the center of the red court with a height of $2\,m$. The drone's initial position is sampled uniformly random from $[4, -0.5, 1.8]$ to $[5, 0.5, 2.2]$. The ball is initialized at $(4.5, 0, 4)$, i.e., $2\,m$ above the anchor position. The ball starts with zero velocity and falls freely. The drone is required to stay within a sphere with $1\,m$ radius near the anchor position and bump the ball as many times as possible. A minimum height of $4\,m$ is required for each bump. The maximum episode length is $800$ steps.

\paragraph{Observation and Reward.}
When the action space is Per-Rotor Thrust (PRT), the observation is a vector of dimension $32$, which includes the drone's root state, the drone's relative position to the anchor, the ball's relative position to the drone, and the ball's velocity. When the action space is Collective Thrust and Body Rates (CTBR), the observation dimension is reduced to $28$, excluding the drone’s throttle. The detailed description of the reward function of this task is listed in Table~\ref{tab:app:bump_reward}.

\paragraph{Evaluation Metric.}
This task is evaluated by the number of successful consecutive bumps performed by the drone. A successful bump is defined as the drone hitting the ball such that the ball’s highest height exceeds $4\,m$.

\subsection{Bump and Pass}
% \xzl{@xiangmin check for errors} \done

\paragraph{Task Definition.}
Drone 1 is initialized randomly around anchor 1 with position $(4.5, -2.5, 2)$, and Drone 2 is initialized randomly around anchor 2 with position $(4.5, 2.5, 2)$. The initial position of drone 1 is sampled uniformly random from $(4, -3, 1.8)$ to $(5, -2, 2.2)$, and the initial position of drone 2 is sampled uniformly random from $(4, 2, 1.8)$ to $(5, 3, 2.2)$. The ball is initialized at $(4.5, -2.5, 4)$, i.e., $2\,m$ above anchor 1. The ball starts with zero velocity and falls freely. The drones are required to stay within a sphere with $0.5\,m$ radius near their anchors and bump the ball to pass it to each other in turns as many times as possible. A minimum height of $4\,m$ is required for each bump. The maximum episode length is 800 steps.

\paragraph{Observation and Reward.}
The drone's observation is a vector of dimension $39$ including the drone's root state, the drone's relative position to the anchor, the drone's id, the current turn (which drone should hit the ball), the ball's relative position to the drone, the ball's velocity, and the other drone's relative position to the drone. The detailed description of the reward function of this task is listed in Table~\ref{tab:app:bnp_reward}.

\paragraph{Evaluation Metric.}
This task is evaluated by the number of successful consecutive bumps performed by the drones. A successful bump is defined as the drone hitting the ball such that the ball’s highest height exceeds  $4\,m$ and lands near the other drone.

\subsection{Set and Spike (Easy)}
% \xzl{@xiangmin check for errors} \done

\paragraph{Task Definition.}
Drone 1 (setter) is initialized randomly around anchor 1 with position $(2, -2.5, 2.5)$, and Drone 2 (attacker) is initialized randomly around anchor 2 with position $(2, 2.5, 3.5)$. The initial position of drone 1 is sampled uniformly random from $(1.5, -3, 2.3)$ to $(2.5, -2, 2.7)$, and the initial position of drone 2 is sampled uniformly random from $(1.5, 2, 3.3)$ to $(2.5, 3, 3.7)$. The ball is initialized at $(2, -2.5, 4.5)$, i.e., $2\,m$ above anchor 1. The ball starts with zero velocity and falls freely. 
The drones are required to stay within a sphere with $0.5\,m$ radius near their anchors. The setter is required to pass the ball to the attacker, and the attacker then spikes the ball downward to the target region in the opposing side. The target region is a circular area on the ground, centered at $(4.5, 0)$ with a radius of $1\,m$. The maximum episode length is 800 steps.

\paragraph{Observation and Reward.}
The drone's observation is a vector of dimension $40$ including the drone's root state, the drone's relative position to the anchor, the drone's id, the current turn (how many times the ball has been hit), the ball's relative position to the drone, the ball's velocity, and the other drone's relative position to the drone. The detailed description of the reward function of this task is listed in Table~\ref{tab:app:sns_easy}.

\paragraph{Evaluation Metric.}
This task is evaluated by the success rate of set and spike. A successful set and spike consist of four parts, (1) setter\_hit: the setter hits the ball; (2) attacker\_hit: the attacker hits the ball; (3) downward\_spike: the velocity of the ball after the attacker hit is downward, i.e., $v_z < 0$; (4) in\_target: the ball's landing position is within the target region. The success rate is computed as $1/4\times(\text{setter\_hit}+\text{attacker\_hit}+\text{downward\_spike}+\text{in\_target})$.


\subsection{Set and Spike (Hard)}
% \xzl{@xiangmin task definition and eval metric is not correct. It should be something related to the defense board, not in target.}

\paragraph{Task Definition.}
Drone 1 (setter) is initialized randomly around anchor 1 with position $(2, -2.5, 2.5)$, and Drone 2 (attacker) is initialized randomly around anchor 2 with position $(2, 2.5, 3.5)$. The initial position of drone 1 is sampled uniformly random from $(1.5, -3, 2.3)$ to $(2.5, -2, 2.7)$, and the initial position of drone 2 is sampled uniformly random from $(1.5, 2, 3.3)$ to $(2.5, 3, 3.7)$. The ball is initialized at $(2, -2.5, 4.5)$, i.e., $2\,m$ above anchor 1. The ball starts with zero velocity and falls freely. The racket is initialized at $(-4, 0, 0.5)$, i.e., the center of the opposing side.
The drones are required to stay within a sphere with $0.5\,m$ radius near their anchors. The setter is required to pass the ball to the attacker, and the attacker then spikes the ball downward to the opponent's court without being intercepted by the defense racket. The maximum episode length is 800 steps.

\paragraph{Observation and Reward.}
The drone's observation is a vector of dimension $40$ including the drone's root state, the drone's relative position to the anchor, the drone's id, the current turn (how many times the ball has been hit), the ball's relative position to the drone, the ball's velocity, and the other drone's relative position to the drone. The detailed description of the reward function of this task is listed in Table~\ref{tab:app:sns_hard}.

\paragraph{Evaluation Metric.}
This task is evaluated by the success rate of set and spike. A successful set and spike consist of four parts, (1) setter\_hit: the setter hits the ball; (2) attacker\_hit: the attacker hits the ball; (3) downward\_spike: the velocity of the ball after the attacker hit is downward, i.e., $v_z < 0$; (4) success\_spike: the ball's landing position is within the opponent's court without being intercepted by the defense racket. The success rate is computed as $1/4\times(\text{setter\_hit}+\text{attacker\_hit}+\text{downward\_spike}+\text{success\_spike})$.
% \yc{remember the script policy}


\subsection{1 vs 1}
\label{app:1v1}
% \xzl{@huining write this subsection. check other tasks for reference.}

\paragraph{Task Definition.}
Two drones are required to play 1-vs-1 volleyball in a court of $6\,m \times 3\,m$. Drone 1 is initialized randomly around anchor 1 with position $(1.5, 0.0, 2.0)$, i.e., the center of the red court with height $2\,m$, and Drone 2 is initialized randomly around anchor 2 with position $(-1.5, 0.0, 2.0)$, i.e., the center of the blue court with height $2\,m$. The initial position of drone 1 is sampled uniformly random from $(1.4, -0.1, 1.9)$ to $(1.6, 0.1, 2.1)$, and the initial position of drone 2 is sampled uniformly random from $(-1.4, -0.1, 1.9)$ to $(-1.6, 0.1, 2.1)$. At the start of a game (i.e. an episode), one of the two drones is randomly chosen to serve the ball, which is initialized $1.5\,m$ above the drone. The ball starts with zero velocity and falls freely. The game ends when one of the drones wins the game or one of the drones crashes. The maximum episode length is 800 steps.

\paragraph{Observation and Reward.}
The drone's observation is a vector of dimension $39$ including the drone's root state, the drone's relative position to the anchor, the drone's id, the current turn (which drone should hit the ball), the ball's relative position to the drone, the ball's velocity, and the other drone's relative position to the drone. The detailed description of the reward function of this task is listed in Table~\ref{tab:app:1v1_reward}.

\paragraph{Evaluation Metric.}
The drone wins the game by landing the ball in the opponent's court or causing the opponent to commit a violation. These violations include (1) crossing the net, (2) hitting the ball on the wrong turn, (3) hitting the ball with part of the drone body instead of the racket, (4) hitting the ball out of court, and (5) hitting the ball into the net.

To comprehensively evaluate the performance of strategies in the \textit{1 vs 1} task, we consider three evaluation metrics: exploitability, win rate, and Elo rating. These metrics provide complementary insights into the quality and robustness of the learned policies.

\begin{itemize}
    \item \textbf{Exploitability:} 
    Exploitability is a fundamental measure of how close a strategy is to a Nash equilibrium. It is defined as the difference between the payoff of a best response (BR) against the strategy and the payoff of the strategy itself. Mathematically, for a strategy \(\pi\), the exploitability is given by:
    \[
    \text{Exploitability}(\pi) = \max_{\pi'} U(\pi', \pi) - U(\pi, \pi),
    \]
    where \(U(\pi_1, \pi_2)\) represents the utility obtained by \(\pi_1\) when playing against \(\pi_2\). The meaning of exploitability is that smaller values indicate a strategy closer to Nash equilibrium, where it becomes increasingly difficult to exploit. Since exact computation of exploitability is often infeasible in real-world tasks, we instead use approximate exploitability. In this task, we fix the strategy on one side and train an approximate best response on the other side to maximize its utility, i.e., win rate. The difference between the BR's win rate and the evaluated policy's win rate then serves as the approximate exploitability.

    \item \textbf{Win Rate:} 
    Since exact exploitability is challenging to compute, a practical alternative is to evaluate the win rate through cross-play with other learned policy populations. 
    Specifically, we compute the average win rate of the evaluated policy when matched against other learned policies. Higher average win rates typically suggest stronger strategies. However, due to the transitive nature of zero-sum games~\cite{czarnecki2020real}, a high win rate against specific opponent populations does not necessarily imply overall mastery of the game. Thus, while win rate is a useful reference metric, it cannot be the sole criterion for assessing strategy strength.

    \item \textbf{Elo Rating:} 
    Elo rating is a widely used metric for evaluating the relative strength of strategies within a population. It is computed based on head-to-head match results, where the expected win probability between two strategies is determined by their Elo difference. After each match, the Elo ratings of the strategies are updated based on the match outcome. While a higher Elo rating indicates better performance within the given population, it does not necessarily imply proximity to Nash equilibrium. A strategy with a higher Elo might simply be more effective against the specific population, rather than being universally robust. Therefore, Elo complements exploitability by capturing population-specific relative performance.
\end{itemize}

% \xzl{one paragraph for winning condition, one or more paragraph for exploitability, win rate, and elo}
% \yc{remember the script policy}

\subsection{3 vs 3}
% \xzl{@ruize write this subsection. check other tasks for reference.} \done

\paragraph{Task Definition.}
The task involves two teams of drones competing in a 3-vs-3 volleyball match within a court of $9\,m \times 4.5\,m$.
Drone 1, Drone 2, and Drone 3 belong to \textit{Team 1} and are initialized at positions $(3.0, -1.5, 2.0)$, $(3.0, 1.5, 2.0)$ and $(6.0, 0.0, 2.0)$ respectively. Similarly, Drone 4, Drone 5, and Drone 6 belong to \textit{Team 2} and are initialized at positions $(-3.0, -1.5, 2.0)$, $(-3.0, 1.5, 2.0)$ and $(-6.0, 0.0, 2.0)$ respectively. At the start of a game (i.e., an episode), one of the two teams is randomly selected to serve the ball. The ball is initialized at a position $3\,m$ directly above the serving drone. The ball starts with zero velocity and falls freely. The game ends when one of the teams wins the game or one of the drones crashes. The maximum episode length is 500 steps.

\paragraph{Observation and Reward.}
The drone's observation is a vector of dimension $57$ including the drone's root state, the drone's relative position to the anchor, the ball's relative position to the drone, the ball's velocity, the current turn (which team should hit the ball), the drone's id, a flag indicating whether the drone is allowed to hit the ball, and the other drone's positions. The detailed description of the reward function of this task is listed in Table~\ref{tab:app:3v3_reward}.

\paragraph{Evaluation Metric.}
Similar to the \textit{1 vs 1} task, either of the two teams wins the game by landing the ball in the opponent's court or causing the opponent to commit a violation. These violations include (1) crossing the net, (2) hitting the ball on the wrong turn, (3) hitting the ball with part of the drone body, rather than the racket, (4) hitting the ball out of court, and (5) hitting the ball into the net. The task performance is also evaluated by the three metric metrics including exploitability, win rate, and Elo as described in the \textit{1 vs 1} task.


% \xzl{one paragraph for winning condition, one or more paragraph for exploitability, win rate, and elo}