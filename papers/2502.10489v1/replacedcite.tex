\section{Related Work}
Data valuation methods aim to quantify the contribution of individual training samples to model performance. Leave-One-Out (LOO) methods____ directly measure contributions by training separate models with and without target samples. However, this approach faces prohibitive computational costs for large datasets and inherent instability from training randomness____.
Game-theoretic approaches like Shapley values____ extend LOO by measuring marginal contributions across all possible subset combinations. While theoretically sound, these methods require evaluating exponential subset combinations, making them impractical for large-scale learning____. Recent work____ further reveals that both LOO and Shapley methods suffer from inconsistent rankings due to retraining randomness. Although approximation techniques____ reduce computational overhead, they still require extensive subset evaluations.

Local update methods____ use training gradients for efficient estimation but struggle with training order bias, as early samples receive disproportionate importance due to varying gradient magnitudes across training stages. While cosine similarity methods____ measure gradient alignments with overall training direction, they fail to capture long-term effects. LiveVal overcomes these limitations through its adaptive reference point mechanism, enabling time-aware contribution measurement while maintaining computational efficiency.

Influence functions____ provide theoretical insights but rely on assumptions rarely met in modern deep learning: loss convexity, model optimality, and Hessian computations____. Although recent extensions____ propose approximations for non-convex scenarios, challenges remain for real-time analysis in deep architectures____. In contrast, LiveVal operates effectively without convexity or optimality assumptions, making it suitable for typical deep learning scenarios.