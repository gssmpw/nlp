\section{Related Work}
Data valuation methods aim to quantify the contribution of individual training samples to model performance. Leave-One-Out (LOO) methods~\cite{cook1977detection,sharchilev2018finding} directly measure contributions by training separate models with and without target samples. However, this approach faces prohibitive computational costs for large datasets and inherent instability from training randomness~\cite{sim2022data}.
Game-theoretic approaches like Shapley values~\cite{shapley1953,wang2020principled} extend LOO by measuring marginal contributions across all possible subset combinations. While theoretically sound, these methods require evaluating exponential subset combinations, making them impractical for large-scale learning~\cite{ghorbani2019data,sun2023shapleyfl,kwon2022beta}. Recent work~\cite{wang2023data,li2024robust} further reveals that both LOO and Shapley methods suffer from inconsistent rankings due to retraining randomness. Although approximation techniques~\cite{luo2024fast,zhang2023efficient} reduce computational overhead, they still require extensive subset evaluations.

Local update methods~\cite{pruthi2020estimating,paul2021deep,tan2023data} use training gradients for efficient estimation but struggle with training order bias, as early samples receive disproportionate importance due to varying gradient magnitudes across training stages. While cosine similarity methods~\cite{miao2022privacy,fung2018mitigating} measure gradient alignments with overall training direction, they fail to capture long-term effects. LiveVal overcomes these limitations through its adaptive reference point mechanism, enabling time-aware contribution measurement while maintaining computational efficiency.

Influence functions~\cite{koh2017understanding} provide theoretical insights but rely on assumptions rarely met in modern deep learning: loss convexity, model optimality, and Hessian computations~\cite{basu2021influence,hammoudeh2024training}. Although recent extensions~\cite{koh2019accuracy,basu2020second,schioppa2022scaling} propose approximations for non-convex scenarios, challenges remain for real-time analysis in deep architectures~\cite{basu2021influence}. In contrast, LiveVal operates effectively without convexity or optimality assumptions, making it suitable for typical deep learning scenarios.