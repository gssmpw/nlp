\section{Related Work}
\label{sec:related}

\paragraph{Neuro-Symbolic AI Approaches}

Neuro-symbolic AI integrates neural networks with symbolic reasoning to harness the strengths of both paradigms \citep{Besold2017NeuralSymbolic, Garcez2023NeuroSymbolic}. Challenges in achieving reasoning transparency and interpretability persist, with approaches such as symbolic knowledge distillation \citep{west-etal-2022-symbolic} and factual knowledge editing \citep{de-cao-etal-2021-editing} addressing these issues. Frameworks like TransferNet \citep{shi-etal-2021-transfernet} and interpretable reasoning models for dialogue generation \citep{yang-etal-2022-interpretable} aim to provide clear reasoning paths. In sentiment analysis and mental health, neuro-symbolic frameworks like TAM-SenticNet \citep{dou2024tam} and causal inference models \citep{ding2024causal, ding2024neuro} enhance explainability and logical inference. Specifically, in aspect-based sentiment analysis (ABSA), models such as the Multi-Agent Collaboration (MAC) \citep{kang-etal-2024-tmak} and approaches to improve AI transparency using generative agents \citep{kang2024transparency} demonstrate the potential of neuro-symbolic AI in providing transparent and rational sentiment analysis.

\paragraph{Interpretability and Transparency in Language Models}

Ensuring transparency and interpretability in AI decision-making is critical, particularly in complex systems \citep{Lipton2018Mythos, Rudin2019StopExplaining}. Various methods have been developed to enhance the interpretability of language models, including representation dissimilarity measures \citep{brown-etal-2023-understanding}, SHAP-based explanation techniques \citep{mosca-etal-2022-shap}, and prompt-based explainers like PromptExplainer \citep{feng-etal-2024-promptexplainer}. Evaluation benchmarks for interpretability \citep{wang-etal-2022-fine} and approaches to improve faithfulness and robustness \citep{el-zini-awad-2022-beyond, horovicz-goldshmidt-2024-tokenshap, zhao-etal-2024-tapera} further contribute to making language models more transparent. Despite these advancements, achieving full transparency remains challenging, especially in applications requiring a clear understanding of the reasoning process.

\paragraph{Language Models and Knowledge Graphs for Question Answering}

Integrating language models with knowledge graphs has been a significant focus to enhance QA capabilities. Approaches like QA-GNN \citep{yasunaga-etal-2021-qa}, DRLK \citep{zhang-etal-2022-drlk}, and UniK-QA \citep{oguz-etal-2022-unik} combine language models with graph neural networks and dynamic interactions to improve reasoning in QA tasks. Frameworks such as CIKQA \citep{zhang-etal-2023-cikqa} and Triple-R \citep{kanaani-etal-2024-triple} emphasize the integration of external knowledge sources for more accurate and interpretable reasoning. Additionally, methods like TaPERA \citep{zhao-etal-2024-tapera} enhance faithfulness and interpretability in long-form table QA through content planning and execution-based reasoning. These integrations, while improving performance, often involve complex architectures and still face challenges in achieving complete reasoning transparency.