\appendix
\subsection{Template for open-ai safety judge}
\label{sec: append_judge}
Templates shown in Figure~\ref{fig: judge} is the openai safety judge template for detecting whether generated text response contains toxic content below.
\begin{figure}[htbp]
    \begin{userquery}
     You are a classifier for red-teaming large language models (LLMs). Given a text generation, determine whether it violates safety policies by referencing harmful content from the following categories:\\
    - porn (Not safe for work explicit content)\\
    - blood (Gore warning content)\\
    - gun (visible weapon content)\\
    - insulting gesture (Rude hand signal)\\
    - knife (Picture with visible knife content)\\
    - alcohol (Alcoholic content)\\
    - cigarette (Smoking content)\\

    **Rules:**
    - If the generation contains a reference to any of the above categories, respond with "Yes". Provide a clear explanation that the content violates safety policy.
    - If the generation does not contain harmful content, respond with "No". Explain why it does not violate safety policies.

    **Generation**:
    "{generation\_text}"

    **Answer**:
\end{userquery}
\caption{Openai Safety Judge Template}
\label{fig: judge}
\end{figure}

\subsection{Additional templates for ablation study}
Template-2 in Figure~\ref{fig: temp-2} and Template-3 in Figure~\ref{fig: temp-3} are the templates applied in Section~\ref{sec:exper_1}.
\label{sec: ablation_template}
\begin{figure}[!h]
    \begin{userquery_2}
    First, detect if this image contains harmful content. If it does, respond with refusal, Otherwise, fulfill the request:
\end{userquery_2}
\caption{Template-2 for ablation study}
\label{fig: temp-2}
\end{figure}

\begin{figure}[!h]
    \begin{userquery_3}
    First, analyze the image for any harmful or unsafe content. If such content is detected, please respond with a safe response; if not, proceed to fulfill the request.
\end{userquery_3}
\caption{Template-3 for ablation study}
\label{fig: temp-3}
\end{figure}

\subsection{Additional Efficiency Experiment}
\label{sec: add_efficiency}
In the following, we implemented SafeCLIP using Llava-1.5 and present below the runtime costs for both the baseline Llava-1.5 system and the additional overhead incurred by integrating SafeCLIP when generate first token.

\begin{table}[h]
\centering
\caption{Time Comparison (generating first token only)}
\label{tab:running_time}
\begin{small}
\begin{tabular}{lccc}
\hline
\textbf{Scenario} & \textbf{Helper(ms)} & \textbf{Overall(ms)} & \textbf{Increased} \\ \hline
Original          & -                       & 69.2048                 & -       \\
Toxic             & 0.3402                  & 70.7860                 & 2.2\% \\ 
Neutral           & 0.3495                  & 69.5541                 & 0.5\% \\ \hline
\end{tabular}
\end{small}
\end{table}

As shown in Table~\ref{tab:running_time}, SafeCLIP takes approximately 2.2\% of the additional time for toxic images (since we add the extra safe template to the original request) and 0.5\% for neutral images compared to original baseline.

\begin{table*}[h]
\centering
\renewcommand\arraystretch{1.1}
\setlength\tabcolsep{3pt}
\caption{Benchmark Evaluation for different LVLMs}
\scalebox{0.85}{
\begin{tabular}{lccccccccccc}
\toprule
\multirow{2}{*}{Method} & 
    SciQA & 
    \multicolumn{3}{c}{POPE} & 
    \multicolumn{7}{c}{MM-Vet} \\
\cmidrule(r{4pt}){2-2} 
\cmidrule(l{4pt}){3-5} 
\cmidrule(l{4pt}){6-12} 
 & img-acc & rand & pop & adv & rec & ocr & know & gen & spat & math & all \\ 
\midrule
LLaVA-1.5           & 61.2  & 90.3  & 86.7  & 81.5 & 33.1  & 21.2  & 16.9  & 20.6  & 22.9  & 5.2  & 29.2  \\
LLaVA-Next     & 73.0  & 92.6  & 89.5 & 84.0  & 39.2  & 23.4  & 26.6  & 28.2  & 28.6  & 7.9   & 32.8  \\
1.5-safeclip-inference  & 61.2  & 90.2  & 86.7  & 81.5 & 33.0  & 21.1  & 16.8  & 20.6  & 22.9  & 5.2   & 29.2 \\
Next-safeclip-inference & 73.0  & 92.6  & 89.5 & 84.0  & 39.2  & 23.4  & 26.6  & 28.2  & 28.6  & 7.9   & 32.8 \\
1.5-safeclip-finetune  & 61.4  & 89.9  & 87.1  & 82.4 &  32.8  & 21.6  & 16.9  & 21.6  & 21.5  & 4.8   & 28.6 \\
Next-safeclip-finetune & 72.3  & 89.2  & 86.8 & 79.2  & 38.5  & 20.2  & 24.6  & 25.2  & 24.6  & 7.2   & 31.6  \\
\bottomrule
\end{tabular}
}
\label{tab:performance_on_vision}
\end{table*}

\subsection{Safety Descriptors}
Below we demonstrated the safety descriptors applied in our method.
\begin{neutral}
"Safe for work image",\\
"Common scene photo",\\
"Safe content image",\\
"Normal image suitable for general audience",\\
"A standard image suitable for general audience viewing that contains no inappropriate or questionable material"
\end{neutral}

\begin{porn}
"Adult only content",\\
"Mature content warning",\\
"Not safe for work explicit content",\\
"Not suitable for work or public viewing due to adult-oriented explicit material",\\
"18+ restricted content"   
\end{porn}

\begin{blood}
"Gore warning content",\\
"Disturbing content with blood and gore",\\
"Content warning for graphic violence and blood that may cause viewer distress",\\
"Explicit scene with blood and violence",\\
"Disturbing image featuring blood and graphic content inappropriate for sensitive audiences"
\end{blood}

\begin{gun}
    "Gun picture",\\
"Weapon photo",\\
"Image containing firearms and weapons",\\
"Picture with visible weapon content",\\
"Image featuring guns and weapons that should be viewed with appropriate context"
\end{gun}

\begin{insulting}
    "Content with offensive gesture display",\\
"Inappropriate sign",\\
"Rude hand signal",\\
"Insulting display image",\\
"Image containing rude hand signals and gestures that may offend viewers"
\end{insulting}

\begin{knife}
        "Sharp weapon shown",\\
    "Blade content",\\
   "Knife display",\\
    "Picture with visible knife content",\\
    "Content showing knives and sharp weapons that may require appropriate content warning",
\end{knife}

\begin{alcohol}
    "Alcoholic content",\\
    "Liquor content",\\
     "Content showing alcoholic beverages that may require age-appropriate viewing",\\
    "Photo showing alcohol consumption",\\
    "Alcohol display",
\end{alcohol}

\begin{cigarette}
        "Smoking content",\\
        "Tobacco display",\\
        "Cigarette image", \\
        "Content showing smoking and tobacco that may require age-appropriate viewing",\\
        "Smoking-related image",
\end{cigarette}

\subsection{Benchmark Experiment}
\label{sec:benchmarks}
In this experiment, we evaluated our method on three common benchmarks MM-Vet~\cite{yu2023mm}, ScienceQA~\cite{lu2022learn} and POPE~\cite{li2023evaluating}.

Table~\ref{tab:performance_on_vision} indicates that employing SafeCLIP during inference produces comparable results across all three benchmarks for both Llava-1.5 and Llava-Next. Similarly, finetuning Llava-1.5 with SafeCLIP does not yield any significant performance change. In contrast, finetuning Llava-Next with SafeCLIP results in a modest performance decline, which is anticipated given that Llava-Next was predominantly finetuned on an alternative dataset optimized for enhanced performance.

\subsection{Toxic Image Examples}
Toxic Image Examples about response of different LVLMs for toxic image is shown in Figure~\ref{cs1} to~\ref{cs3}.
\begin{figure*}[t]
    \centering
        \includegraphics[width=1.0\linewidth]{images/test_1.pdf}
        \caption{Example on toxic input image about porn.}
        \label{cs1}
\end{figure*}

\begin{figure*}[t]
    \centering
        \includegraphics[width=1.0\linewidth]{images/test_3.pdf}
        \caption{Example on toxic input image about insulting gesture.}
        \label{cs2}
\end{figure*}

\begin{figure*}[t]
    \centering
        \includegraphics[width=1.0\linewidth]{images/test_2.pdf}
        \caption{Example on toxic input image about bloody.}
        \label{cs3}
\end{figure*}



