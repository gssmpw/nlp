\section{Conclusion}
We proposed SafeCLIP, an efficient method that enhances LVLM safety against toxic visual inputs by leveraging the vision encoder's CLS token for zero-shot detection. With minimal overhead during inference and fine-tuning, SafeCLIP effectively prevents harmful outputs while maintaining model efficiency, offering a scalable solution to LVLM vulnerabilities.
\newpage
\section*{Limitations}
While our work presents a scalable approach for mitigating vulnerabilities in large vision-language models (LVLMs), it is constrained by the range of attack methods considered. In our study, we primarily focus on defending against toxic images (without modification) because this attack is not only straightforward to implement—requiring merely that the LVLM describe the toxic image—but also because current state-of-the-art LVLMs, such as Qwen-VL and Janus-Pro, lack robust defensive mechanisms. Nonetheless, adversarial techniques may induce harmful responses from images that appear benign. Future research could expand the utilization of the [CLS] token to enhance detection capabilities against a broader spectrum of attack methods. Moreover, alternative strategies for safe response generation—such as responding with templated refusals directly  or integrating language models with improved safety performance to generate safe response.





