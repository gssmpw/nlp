 
\section{Our Method} 
In this section, we introduce \textbf{SafeCLIP}, an efficient clip-based method for zero-shot toxic scene detection in LVLMs. We begin by explaining the core functionality of this approach, followed by a discussion on its integration during both the inference and fine-tuning phases of LVLMs.

\subsection{Re-Purposing the CLS Token: Zero-Shot Toxic Scene Detection}
\label{sec:zero_shot}
Our key innovation is redefining the role of the CLS token, which traditionally has been discarded after visual encoding, and leveraging it as a safety indicator for detecting toxic scenes. This design is theoretically grounded in:
\begin{itemize}
    \item \textbf{High-Dimensional Semantics}: CLS token encodes global image semantics through contrastive pretraining and achieve $\geq 76.2\%$ linear probing accuracy on ImageNet~\cite{radford2021learning}.
    \item \textbf{Cross-Modal Alignment}: The alignment between image CLS embeddings and text embeddings produced by CLIP’s text encoder enables effective zero-shot classification. This alignment is exploited to detect toxic scenes by comparing the image’s visual semantics with predefined textual descriptions.
\end{itemize}

\noindent To apply CLS token for toxic scene detection, we first establish a safety taxonomy comprising 8 categories according to ~\cite{wang2023tovilag}:
\begin{equation}
    \mathcal{C} = \left\{
        \begin{aligned}
            &\text{neutral}, \text{porn}, \text{blood}, \text{gun}, \\
            &\text{gesture}, \text{knife}, \text{alcohol}, \text{cigarette}
        \end{aligned}
        \right\}
\end{equation}

For each category $c \in \mathcal{C}$, we design $K$ textual descriptors $\mathcal{T}c = {t_c^1, ..., t_c^K}$ (detailed in Appendix) and compute their CLIP text embeddings through:

\begin{equation}
\mathbf{T}c^k = \frac{C_{\text{text}}(t_c^k)}{|C_{\text{text}}(t_c^k)|_2} \in \mathbb{R}^{d_v},\ \forall c \in \mathcal{C}, 1 \leq k \leq K
\end{equation}
where $C_{\text{text}}$ denotes CLIP's frozen text encoder. These normalized embeddings form our \textit{safety concept bank}. 

Once the safety concept bank is available, the detection process proceeds with the following steps:
\begin{enumerate}
    \item \textbf{CLS token Projection}:  Map the vision encoder's CLS token into CLIP's text embedding space using the original projection matrix: 
    \begin{equation}
    \mathbf{h}_{\text{CLS}} = W_p \cdot \text{CLS} + b_p
    \end{equation}
    where $W_p$ and $b_p$ are pretrained projection parameters from CLIP.
    \item \textbf{Similarity Computation}:  Calculate cosine similarities between the projected CLS token and all category descriptors in the safety concept bank:
    \begin{equation}
    s_c^k = \frac{\mathbf{h}_{\text{CLS}} \cdot \mathbf{T}_c^k}{\|\mathbf{h}_{\text{CLS}}\| \|\mathbf{T}_c^k\|} \quad \forall c \in \mathcal{C}, 1 \leq k \leq K
    \end{equation}
    
    \item \textbf{Probability Calibration}: Apply temperature-scaled softmax over similarities for each descriptor:
    \begin{equation}
    p(c|t_c^k) = \frac{\exp(\sigma \cdot s_c^k)}{\sum_{c' \in \mathcal{C}} \exp(\sigma \cdot s_{c'}^k)}
    \end{equation}
    where $\sigma$ is CLIP's pretrained logit scale parameter ($\sigma = 100$).
    
    \item \textbf{Category-Level Fusion}: Aggregate probabilities across each category's $K$ templates:
    \begin{equation}
        p_{\text{final}}(c) = \frac{1}{K} \sum_{k=1}^K p(c|t_c^k)
    \end{equation}
\end{enumerate}

\begin{algorithm}[t]

\begin{small}
\caption{Safe Visual Language Processing Via SafeCLIP}
\begin{algorithmic}
\REQUIRE Input image $X_v$, query text $X_q$, safe template instruction $X_{safe}$
\ENSURE Generated response $Y_a$
\STATE \textbf{LVLM and Protype Initialization}
\begin{itemize}
    \item[$\triangleright$] Initialize \text{VisionEncoder} Connector $W$ and LLM $F_\theta$
    \item[$\triangleright$] Initialize  safety concept bank $\mathbf{T_c}$
\end{itemize}
\STATE \textbf{Stage 1: Visual Processing}
\begin{itemize}
    \item[$\triangleright$] Extract CLS token and visual features:  
    $\{\text{CLS}, Z_v\} \gets C_\text{Vision}(X_v)$
\end{itemize}

\STATE \textbf{Stage 2: Safety Verification}
\begin{itemize}
    \item[$\triangleright$] Apply SafeCLIP for toxic scene detection: Toxic = SafeCLIP(\text{CLS}, $\mathbf{T_c}$ )
\end{itemize}

\STATE \textbf{Stage 3: Response Generation}
\begin{itemize}
    \item[$\triangleright$] If Toxic: $X_q \gets X_{\text{safe}} \oplus X_q$
    
    \item[$\triangleright$] Process text input:  
    $H_q \gets \text{Tokenizer}(X_q)$
    
    \item[$\triangleright$] Project visual features:  
    $H_v \gets Z_vW$
    
    \item[$\triangleright$] Generate final output:  
    $Y_a \gets F_\theta([H_q; H_v])$
\end{itemize}

\RETURN $Y_a$
\end{algorithmic}
\label{alg:safe_processing}
\end{small}
\end{algorithm}

\paragraph{Decision Rule}  An image is flagged as toxic if:
\begin{equation}
\exists c \in \mathcal{C} \setminus {\text{neutral}} \quad \text{s.t.} \quad p_{\text{final}}(c) > \tau
\end{equation}
where $\tau$ denotes the toxicity threshold.
% This template bank ${\mathbf{T}_c^k}$ enables efficient runtime computation while reducing VRAM consumption by $x\%$ compared to persistent text model retention.
This process, which we name SafeCLIP, utilizes the CLS token generated by the LVLM, projecting it into the same text embedding space and calculating similarity to decide whether the image contains a toxic scene.

The integration of SafeCLIP into the LVLM pipeline is described in Algorithm~\ref{alg:safe_processing} (and illustrated in Figure~\ref{fig:vllm_arch}). First, we initialize the LVLM and the safety concept bank $\mathbf{T}c$. After the visual feature extraction step, we apply SafeCLIP to detect whether the input image contains a toxic scene using the CLS token. If a toxic scene is detected, we add a safe template instruction $X{safe}$ to the original query $X_q$, requiring the model to generate safe content.

To improve computational efficiency and save VRAM, we precompute and store all text embeddings during LVLM initialization, avoiding redundant calculations. Note that since SafeCLIP only requires a single MLP layer projection and cosine similarity comparison, it is efficient.

\subsection{Dynamic Safety Correction Through SafeCLIP During Fine-Tuning}
\label{sec:ft_correction}
Previous work~\cite{helff2024llavaguard,gou2024eyes} has employed safety screening methods as dataset engines to detoxify the training set. However, these methods suffer from high overhead and necessitate detoxifying the entire dataset before training. In contrast, our method—characterized by low latency —allows dynamic safety correction during fine-tuning, thereby reducing computational resource requirements.
\paragraph{Dynamic Safety Intervention}
Building on SafeCLIP's inference capabilities, we implement real-time safety correction during fine-tuning through conditional response generation and safe target alignment as follows.

% \subparagraph{Safety State Determination}  
% For each input image \(X_v\), we use SafeCLIP to determine if the scene is harmful:
% \begin{equation}
% \text{Toxic} = \text{SafeCLIP}(\text{CLS}, \mathbf{T}_c).
% \end{equation}

\subparagraph{Conditional Response Regeneration}
When toxic images are detected using SafeCLIP, we take the following actions:

\begin{enumerate} \item \textbf{Instruction Sanitization}:
Prepend a safety prefix template  $X_{safe}$ to the input text: \begin{equation} X_q' = X_{safe} \oplus X_q \end{equation} \item \textbf{Safe Response Generation}:
Generate a response with the model, using frozen parameters to avoid affecting the fine-tuning: \begin{equation} \hat{Y} = F_\theta(X_v, X_q') \quad \text{with} \quad \texttt{torch.no\_grad()} \end{equation} \end{enumerate}

\subparagraph{Safe Target Alignment}
For detected harmful samples, we update the training targets as follows:
\begin{equation}
(Y|X_v, X_q) \gets \begin{cases} 
\hat{Y} & \text{if  }  Toxic  \\ 
Y & \text{otherwise} 
\end{cases}
\end{equation}·
By using SafeCLIP to detect toxic content in training images, we ensure that the model fine-tunes only on safe responses, thereby enhancing its safety alignment. This approach maintains training efficiency while improving the model's ability to handle toxic scenes in real time.


