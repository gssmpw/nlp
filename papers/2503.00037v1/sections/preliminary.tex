\begin{table*}[h]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{small}
\begin{tabular}{c|c|ccccccc|c}
\toprule
\multirow{2}{*}{\textbf{LVLMs}} & \multirow{2}{*}{\textbf{\shortstack{FPR}}} & \multicolumn{7}{c|}{\textbf{Defence Success Rates on Toxic Image Inputs}} & \multirow{2}{*}{\makecell{\textbf{Text} \\ \textbf{DSR}}} \\
\cmidrule{3-9}
 &  & \textbf{Porn} & \textbf{Bloody} & \textbf{Insulting} & \textbf{Alcohol} & \textbf{Cigarette} & \textbf{Gun} & \textbf{Knife} \\
\midrule
LLaVA-1.5 & 0\%  & 3.2\% & 0.4\% & 1.6\% & 0.3\% & 0.5\% & 0.7\% & 0.4\% & 57.7\%\\
Llava-next-8B & 0\%  & 4.6\% & 0.7\% & 2.1\% & 0.2\% & 0.5\% & 0.7\% & 0.4\% & 95.6\%\\
Qwen-VL-chat & 0\%  & 2.4\% & 1.0\% & 2.6\% & 0.3\% & 0.3\% & 0.5\% & 1.0\% & 97.3\%\\
Janus-Pro & 0\%  & 6.7\% & 0.6\% & 1.2\% & 0.5\% & 0.4\% & 1.4\% & 0.4\% & 100\%\\
\bottomrule
\end{tabular}
\end{small}
\caption{Defence success rates on toxic scenes for different LVLMs. Higher DSR indicate better safety performance and higher FPR indicate high damage to model utility.}
\label{tab:defence_success_rates_normal}
\end{table*}
\section{Preliminary}
\label{sec: pre}
In this section, we first describe the standard architecture of current mainstream Large Vision-Language Models (LVLMs) and subsequently present the safety challenges of LVLMs against toxic visual inputs, and then define our research objective.
\subsection{Current LVLM Pipeline}
\label{sec:vllm_arch}



The standard processing pipeline of LVLMs, as shown in Figure~\ref{fig:vllm_arch}, comprises four key components:

\subparagraph{1) Visual Feature Extraction}
Given visual input $X_v \in \mathbb{R}^{H \times W \times C}$, the vision encoder (e.g., CLIP-ViT) $C_\text{vision}$ decomposes it into:
\begin{equation}
    \{\text{CLS}, Z_v\} = C_\text{vision}(X_v)
\end{equation}
where $Z_v \in \mathbb{R}^{N \times d_v}$ represents patch-wise features ($N=576$ for $24 \times 24$ grids), and CLS $\in \mathbb{R}^{d_v}$ token is the global semantic token. 

\subparagraph{2) Cross-modal Projection}
Visual features $Z_v$ are aligned to the text space through a trainable projection module $W \in \mathbb{R}^{d_v \times d_h}$:
\begin{equation}
    H_v = Z_v W \in \mathbb{R}^{N \times d_h}
\end{equation}

\subparagraph{3) Text Feature Extraction}
Text input $X_q$ is converted into token embeddings via:
\begin{equation}
    H_q = \text{Tokenizer}(X_q) \in \mathbb{R}^{L \times d_h}
\end{equation}
where $L$ is the sequence length and $d_h$ denotes the language model's embedding dimension.

\subparagraph{4) Feature Fusion and Generation}
The concatenation of text embeddings $H_q$ and projected visual features $H_v$ forms:
\begin{equation}
    H_{\text{fusion}} = [H_q; H_v] \in \mathbb{R}^{(L+N) \times d_h}
\end{equation}
The language model $F_\theta$ then generates responses through autoregressive decoding:
\begin{equation}
    Y_a = F_\theta(H_{\text{fusion}})
\end{equation}

Once this architecture is established, the model undergoes vision-language alignment within LVLMs, enabling the base language models to comprehend and process visual inputs. As detailed in \cite{liu2024llava}, the alignment process includes two phases: pretraining and fine-tuning. During pretraining, only the parameters of the projection module $W \in \mathbb{R}^{d_v \times d_h}$ are trained to align the visual features with the text embedding space. Following pretraining, the model enters the fine-tuning phase, during which all LVLM parameters are updated to enhance performance on multimodal tasks, such as recognition and OCR.

Given vision and language are integrated into a shared space within LVLMs, the safety mechanisms should ideally be unified across both modalities. However, this is not currently the case. In the following, we demonstrated that compared to toxic text inputs, LVLMs are significantly more vulnerable to toxic visual inputs.


\subsection{Safety Challenges on Toxic Inputs}
In the following, we conduct a study on  the safety of state-of-the-art LVLMs against toxic text and visual inputs, and highlight the challenges on detecting toxic visual inputs.\\
\noindent\textbf{Toxic Dataset} For text inputs, we adopt the $520$ harmful prompts from AdvBench Dataset\cite{GCG2023Zou}. For visual inputs, we utilize real toxic images sourced from open-source datasets. 
Following the settings in~\cite{xu2024cross}, the toxic datasets comprise HOD~\cite{ha2023hod}, which contains $10631$ toxic images depicting alcohol, cigarette, gun, insulting gesture,bloody and knife, and ToViLaG~\cite{wang2023tovilag}, which includes $5600$ toxic images representing pornographic content. Moreover,  we extract all the neutral-class images from ToViLaG 
and subsequently employ GPT-4V~\cite{achiam2023gpt} to collect an additional $4000$ neutral images to form a dataset of neutral images
\\
\noindent\textbf{Evaluation Metric} We adopt the conventional safety testing method described in~\cite{wang2023tovilag}, presenting toxic images to models with text instructions to describe their content. We employ the Defence Success Rates (DSR) as our primary metric, which indicates whether the model refuses to produce toxic responses when presented with toxic input.
A higher DSR indicates that the model is more inclined to reject requests for generating descriptions of toxic content. \\
For neutral images, we similarly follow the standard safety testing procedure; and report the false positive rate (FPR), defined as the number of images misclassified as non-neutral divided by the total number of neutral images. GPT-4o~\cite{hurst2024gpt} is used to determine whether the responses generated by the model are toxic, thereby facilitating the evaluation of both DSR and FPR. Detailed prompt templates are provided in Appendix~\ref{sec: append_judge}.\\
\noindent\textbf{LVLM} The open-source LVLMs and LLMs employed in our experiments include: LLaVA-1.5~\cite{liu2024visual} with its base LLM Vicuna-7B-v1.5~\cite{chiang2023vicuna},
 Llava-next-8B~\cite{liu2024llava}  with its base LLM Llama-3-8B-Instruct~\cite{dubey2024llama}, 
  Qwen-VL-Chat~\cite{bai2023qwenb} with its base LLM Qwen-7B-Chat~\cite{bai2023qwena} and deepseek Janus-Pro-7B~\cite{chen2025janus}. 




\noindent\textbf{Findings} The defence evaluation results, summarized in Table~\ref{tab:defence_success_rates_normal}, reveal two key findings. First, nearly all models maintain good safety performance on text inputs. Second, all models, despite various approaches to enhance multimodal understanding beyond traditional alignment methods (e.g., Qwen-VL and Janus-Pro), lack effective defence mechanisms against toxic images. As a result, they generate toxic content when prompted to describe toxic images.

In the next section, we introduce a method designed to achieve a high DSR with a low FPR while inducing minimal overhead.






