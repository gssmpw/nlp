\section{Experiment 1: Suffixes Containing Benign Features Jailbreak}
\subsection{Experiment Setup}
\textbf{Format Dataset Construction} As mentioned in the previous section~\ref{sec:intro}, we hypothesize that features extracted from datasets with specific response formats (e.g., structured responses or code format  response) can override safety alignments in LLMs. To construct a format-specific dataset, we implemented a three-step process shown in Figure \ref{fig:format_dataset}
\begin{figure}[htbp]
    \begin{dataset}
        1. \textbf{ Sampling Prompts:} We selected diverse, task-oriented dialogues from the Alpaca dataset to ensure a wide range of topics and linguistic styles.\\
        2. \textbf{ Generating Format-Specific Responses:} To produce responses in specific formats (e.g., lists, bullet points, or code), we considered two different strategies. The first involves providing explicit formatting instructions in the system prompts to guide the model. The second uses universal adversarial suffixes to dominate the generation format. \\
        3. \textbf{Filtering for Safety and Meaningfulness:} We utilized GPT-4 to filter out any prompt-response pairs containing potentially harmful content or content with  low quality, ensuring the dataset remained entirely benign.
\end{dataset}
\caption{Format Dataset Construction}
\label{fig:format_dataset}
\end{figure}
In this experiment, we constructed one structure format dataset following the above procedure.  We apply the first strategy to generate response and selected $1000$ benign prompts from the Alpaca dataset and input them into the Llama2-7B-chat-hf model~\citep{touvron2023llama} with a designed system template to produce structured outputs regardless of the input prompts. The system template and dataset pairs are detailed in Appendix~\ref{sec:appendix}. For the harmful attack dataset, we employed 
$1000$ harmful prompt-response pairs from ~\citet{sheshadri2024targeted}. To evaluate our suffix transferability, we used $520$ harmful prompts from AdvBench~\citep{GCG2023Zou}.\\
\textbf{Evaluation Metrics}: In our study, we employed GPT-4-turbo as the evaluation model to measure both the attack success rate (ASR) and the harmfulness score of responses generated by adversarial prompts. To assess ASR, we queried GPT-4-turbo to determine whether the responses violated OpenAI's safety guidelines. For harmfulness evaluation, we asked GPT-4-turbo to rate the responses on a scale from $1$ to $5$, where $1$ indicates no harm and appropriate model refusal of the request, and $5$ signifies a response that disregards usage guidelines and clearly violates policy. Detailed evaluation templates are provided in Appendix~\ref{sec:appendix}.\\
\noindent\textbf{Attack Setup}  We evaluate two state-of-the-art universal adversarial attack methods—the Greedy Coordinate Gradient (GCG)~\citep{GCG2023Zou} and AmpleGCG~\citep{liao2024amplegcg} on three common open-source LLMs Llama2-7B-chat-hf~\citep{touvron2023llama}, Vicuna7B-1.5~\citep{chiang2023vicuna} and Mistral-7B~\citep{jiang2023mistral}. Using GCG, we generated 1,000 adversarial suffixes and assess their transferability on the AdvBench dataset. For AmpleGCG, we produce 5,000 adversarial suffixes using the provided model AmpleGCG-llama2-sourced-llama2-7b-chat. For our universal feature extractor method, we set the parameters as follows: number of iterations $I=500$,evaluation interval $c =10$,learning rate $\alpha= 2e-3$, and token-regularization  $\lambda=10$. Additionally, we evaluate the REpresentation Engineering (RepE) method~\citep{Representation2023Zou}, which manipulates representations in layers $15–17$ of the LLMs, whereas our value attack modifies only the representation of embedding layer. For all methods, we report the best performance achieved using a single universal token or embedding value.
\subsection{Experiment Results}
\begin{table*}[t]
    \vskip -0.1in
    \centering
    \caption{ASR and harmfulness scores of various universal adversarial attack methods across three LLMs}
    \label{tab:3-3}
    \vskip 0.15in
    \begin{tabular}{l|cc|cc|cc}
    \toprule
    & \multicolumn{2}{c|}{Llama2-7B} & \multicolumn{2}{c|}{Vicuna-7B} & \multicolumn{2}{c}{Mistral-7B} \\
    \midrule
    Method & ASR.  & Harm. & ASR.  & Harm. & ASR.  & Harm.  \\
    \midrule
    GCG & 76.7\%  & 2.94 &   94.6\% & 4.71 & 86.1\%  & 4.18 \\
    AmpleGCG & 69.4\% & 3.66 &  \textbf{98.7\%} & 4.89  & 83.1\% & 4.13  \\
    Token Attack (Structure Dataset) & 67.8\% & 3.64 & 92.0\% & 4.65  & 88.3\%  & 4.32 \\
    Token Attack (Harmful Dataset) & \textbf{77.8\%} & \textbf{ 4.00} & 97.5\%  &  \textbf{4.90}& \textbf{92.3\%} & \textbf{4.65}  \\
    \midrule
        RepE   & 99.5\%  & 4.92   & 99.2\% & 4.87 & 99.6\% & 4.93  \\
        Value Attack (Structure Dataset)  & 99.4\%  & 4.91 &98.4\% & 4.93&98.1\%  & 4.94  \\
        Value Attack (Harmful Dataset) & \textbf{100\% } & \textbf{4.95} & \textbf{99.4\%}&\textbf{ 4.96 }&\textbf{ 100\%} & \textbf{4.97}  \\
    \bottomrule
    \end{tabular}
    \label{tab:attack_comparison}
\end{table*}
Table~\ref{tab:attack_comparison} presents the results of our universal suffix transferability experiments. In the case of the token attack, our embedding token attack on harmful datasets achieved the highest performance in both Attack Success Rate (ASR) and harmfulness scores. Notably, universal tokens generated from format-specific datasets also transferred effectively to the harmful AdvBench dataset, with harmfulness scores comparable to those produced by AmpleGCG. We observed that while GCG achieved a high ASR but had a lower harmfulness score, likely due to its inconsistent generation of high-quality harmful content.\\
For the value attack, we demonstrated that universal suffixes generated from both the format-specific dataset and the harmful dataset reached a 100\% ASR across all models. These findings support our hypothesis that it is possible to generate universal adversarial tokens and embeddings using benign format datasets while maintaining high transferability to harmful prompts. Moreover, the quality of the generated responses is also guaranteed, as the structured format inherently encourages high-quality content output.\\
These results indicate that our universal feature extractor effectively captures the dominant features within the benign format dataset, and these features can transfer to harmful prompts, achieving high transferability comparable to other state-of-the-art jailbreak methods. Interestingly, although the harmful dataset's input-output pairs are normal responses without a uniform dominant format, the suffixes extracted from harmful datasets still yielded the best performance. 



% The above approaches allow us to extract the dominant features within the dataset itself. We can generates embedding tokens(value) containing features related to the specific format. and then we test these embedding tokens(value) on the harmful dataset to find out whether these feature could force the model to output corresponding to the format even if the request is harmful.
% \begin{enumerate}
% \item Sampling Prompts: We selected diverse, task-oriented dialogues from the Alpaca dataset~\citep{alpaca} to ensure a wide range of topics and linguistic styles.
% \item Generating Format-Specific Responses: To produce responses in specific formats (e.g., lists, bullet points, or code), we considered two different strategies. The first involves providing explicit formatting instructions in the system prompts to guide the model. The second uses universal adversarial suffixes to dominate the generation format. 
% \item Filtering for Safety and Meaningfulness: We utilized GPT-4 to filter out any prompt-response pairs containing potentially harmful content or content with  low quality, ensuring the dataset remained entirely benign.
% \end{enumerate}



\section{Experiment 2: Jailbreaking Suffixes Contain Features}
\begin{figure}[!t]
\centering
\includegraphics[width=0.9\columnwidth]{images/harmful_datasets.pdf}
\caption{ Examples showing how embedding suffixes derived from harmful datasets can induce diverse formatted responses in both benign and harmful prompts. }
\label{fig:harmful_uap}
\end{figure}
Although harmful datasets typically lack universal-format responses, our universal feature extractor can still extract features that prompt the model to respond to a wide range of harmful prompts. In this section, we first analyze these universal features they encapsulate from harmful datasets following the similar logic in Figure~\ref{fig:intro} and append these suffixes to both benign and harmful prompts. 

Figure~\ref{fig:harmful_uap} illustrates this process. By extracting a set of universal adversarial suffixes from harmful datasets, we evaluated their effects on both benign and harmful prompts. Interestingly, we observed that these suffixes elicited diverse specific format behaviors for both benign and harmful prompts. For example, certain adversarial suffixes cause the model to generate outputs in BASIC programming language format. 

Now that we have discovered that we could use our universal feature extractor to extract dominant related to output format from both benign format dataset and harmful dataset. We aim to find out why different advesarial suffixes generated by diverse jailbreak methods have different transferability. To solve this problem, we present a comprehensive analysis to quantify how various adversarial suffixes influence LLM outputs.

To assess this influence quantitatively, we employ the Pearson Correlation Coefficient (PCC)~\citep{anderson2003introduction}, a widely used metric that measures the linear correlation between two variables. The PCC is defined as:
\begin{equation}
    \text{PCC}_{X,Y} = \frac{cov(X, Y)}{\sigma_{X} \sigma_{Y}},
\end{equation}
where $cov$ indicates the covariance and $\sigma_{X}$ and $\sigma_{Y}$ are the standard deviation of vector $X$ and $Y$. The PCC value ranges from $-1$ to $1$, where an absolute value of $1$ indicates perfect linear correlation, $0$ indicates no linear correlation, and the sign indicates the direction of the relationship (positive or negative).
\begin{figure}[!t]
\centering
    % First row
    \begin{minipage}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/meanless_ori.pdf}\\
        \includegraphics[width=\textwidth]{images/meanless_suffix.pdf}
        \caption*{(a) Meaningless Suffix}
        \label{fig:meaningless}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/one_time_ori.pdf}\\
        \includegraphics[width=\textwidth]{images/one_time_suffix.pdf}
        \caption*{(b) One-time Suffix}
        \label{fig:one-time}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/template_ori.pdf}\\
        \includegraphics[width=\textwidth]{images/template_suffix.pdf}
        \caption*{(c) Template Suffix}
        \label{fig:template}
    \end{minipage}

    \vspace{1em} % Add some vertical space between rows

    % Second row
    \begin{minipage}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/benign_uap_ori.pdf}\\
        \includegraphics[width=\textwidth]{images/benign_uap_suffix.pdf}
        \caption*{(d) Format UAP Value Suffix}
        \label{fig:benign_uap_value}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/harmful_uap_token_ori.pdf}\\
        \includegraphics[width=\textwidth]{images/harmful_uap_token_suffix.pdf}
        \caption*{(e) Harm UAP Token Suffix}
        \label{fig:harmful_uap_token}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/harmful_uap_ori.pdf}\\
        \includegraphics[width=\textwidth]{images/harmful_uap_suffix.pdf}
        \caption*{(f) Harm UAP Value Suffix}
        \label{fig:harmful_uap_value}
    \end{minipage}
    \caption{PCC analysis of different suffix impact on adversarial prompt. Blue dots show the PCC analysis of original harmful prompt and adversarial prompt. Red dots show PCC analysis of suffix and adversarial prompt.}
    \label{fig:pcc_analysis}
\end{figure}

In our analysis, we define the following variables based on the last hidden states of the model:
\begin{itemize}
    \item \( H_{\text{o}} \): the last hidden state of the original harmful prompt.
    \item  \( H_{\text{s}} \): the last hidden state of the suffix input (without the harmful prompt).
    \item  \( H_{\text{adv}} \): the last hidden state of the adversarial prompt, which is the harmful prompt appended with the suffix.
\end{itemize}

We focus on the last hidden states because, in autoregressive language models, this state encapsulates all the features necessary to generate the subsequent output.

By comparing \( \text{PCC}_{H_{\text{o}}, H_{\text{adv}}} \) and \( \text{PCC}_{H_{\text{s}}, H_{\text{adv}}} \), we gain insights into the contributions of the harmful prompt and the adversarial suffix to the final representation \( H_{\text{adv}} \). A higher PCC value indicates a greater influence on the final hidden state. For instance, if \( \text{PCC}_{H_{\text{o}}, H_{\text{adv}}} \) is larger than \( \text{PCC}_{H_{\text{s}}, H_{\text{adv}}} \), it suggests that the harmful prompt plays a more dominant role than the adversarial suffix in shaping the model's output.

To visualize these relationships, we plotted pairs of representations and examined the degree of linear correlation as quantified by the PCC.

We conducted our PCC analysis by sampling 100 harmful prompts from the AdvBench dataset and report the average results across the following settings:

\begin{itemize}
    \item \textbf{Prompt + Meaningless Suffix}:

    In this setting, \( H_{\text{o}} \) corresponds to the last hidden state of the original harmful prompt, and the suffix consists of 20 exclamation marks ("!"). The results, illustrated in Figure (a), show that \( H_{\text{o}} \) and \( H_{\text{adv}} \) are perfectly linearly correlated and \( H_{\text{s}} \) and \( H_{\text{adv}} \) close to $0$ . This outcome is expected since appending a meaningless suffix has minimal impact on the model's output, leaving the harmful prompt as the primary influence.

    \item \textbf{Prompt + One-Time Suffix}:

    In this setting, we use an adversarial suffix generated by the Greedy Coordinate Gradient (GCG) method~\citep{GCG2023Zou},designed for a specific prompt and not intended for transferability.  Figure (b) shows that \( \text{PCC}_{H_{\text{s}}, H_{\text{adv}}} \) is slightly higher than \( \text{PCC}_{H_{\text{o}}, H_{\text{adv}}} \), suggesting that the one-time suffix begins to influence the model's output comparably to the original prompt.

    \item \textbf{Prompt + Template Suffix}:

    In this setting,  we employ a readable adversarial suffix derived from template-based attacks like GPTFuzz~\citep{yu2023gptfuzzer} and AutoDAN~\citep{liu2023autodan}, which provide specific instructions to the model. Figure (c) illustrates that \( \text{PCC}_{H_{\text{s}}, H_{\text{adv}}} \) is significantly higher than \( \text{PCC}_{H_{\text{o}}, H_{\text{adv}}} \) indicating that the template suffix exerts a strong influence on the generation process, though the harmful prompt still contributes meaningfully.

    \item \textbf{Prompt + Universal Value Generated on Format Benign Datasets}:

    In this setting, the suffix is a universal value generated from benign datasets using aembedding value attack. Figure (d) indicates that while \( \text{PCC}_{H_{\text{s}}, H_{\text{adv}}} \) remains higher than \( \text{PCC}_{H_{\text{o}}, H_{\text{adv}}} \), the gap is narrower compared to the previous scenario. This implies that the model relies on both the benign universal value and the harmful prompt to generate harmful content.
    
    \item \textbf{Prompt + Universal Token Generated on Harmful Datasets}:

    In this setting, the suffix is a universal adversarial token generated via  embedding token attack on harmful datasets. As shown in Figure (e), \( \text{PCC}_{H_{\text{s}}, H_{\text{adv}}} \) is markedly higher than \( \text{PCC}_{H_{\text{o}}, H_{\text{adv}}} \), with the latter approaching zero. This suggests that the universal token largely dictates the model's behavior, overshadowing the original prompt.

    \item \textbf{Prompt + Universal Value Generated on Harmful Datasets}:

    Finally, we consider a universal value generated from harmful datasets using  embedding value attack. Figure (f) reveals that \( \text{PCC}_{H_{\text{s}}, H_{\text{adv}}} \) is close to 1, while \( \text{PCC}_{H_{\text{o}}, H_{\text{adv}}} \) is near zero. This demonstrates that the suffix overwhelmingly dominates the generation process.
\end{itemize}

These analyses demonstrate that universal adversarial suffixes when generated from benign datasets, universal values can substantially impact the model's behavior, although the harmful prompt still contributes to some extent. Moreover,features extracted from harmful datasets can significantly manipulate the model's output by embedding dominant features that override the original prompt completely such lead to higher jailbreak performance.

\section{Experiment 3: Benign Dataset and Benign Features Compromise Safety}





\subsection{Experimental Setting}
\textbf{Datasets}: For our experiments, we constructed one structure format dataset following the procedure outlined in Section~\ref{sec:method}. We selected $1000$ benign prompts from the Alpaca dataset and input them into the Llama2-7B-chat-hf model~\citep{touvron2023llama} with a designed system template to produce structured outputs regardless of the input prompts. The system template and dataset pairs are detailed in Appendix~\ref{sec:appendix}. We also used the same $1000$ benign prompt-response pairs from the Alpaca dataset to form a baseline benign dataset. For the harmful attack dataset, we employed 
$1000$ harmful prompt-response pairs from ~\citet{sheshadri2024targeted}. For evaluation, we tested our attack methods and harmful-tuning models using  $520$ harmful prompts from AdvBench~~\citep{GCG2023Zou}.

\textbf{Evaluation Metrics}: In our study, we employed GPT-4-turbo as the evaluation model to measure both the attack success rate (ASR) and the harmfulness score of responses generated by adversarial prompts or harmful-tuned models. To assess ASR, we queried GPT-4-turbo to determine whether the responses violated OpenAI's safety guidelines. For harmfulness evaluation, we asked GPT-4-turbo to rate the responses on a scale from $1$ to $5$, where $1$ indicates no harm and appropriate model refusal of the request, and $5$ signifies a response that disregards usage guidelines and clearly violates policy. Detailed evaluation templates are provided in Appendix~\ref{sec:appendix}.

\noindent\textbf{Attack Setup}  We evaluate two state-of-the-art universal adversarial attack methods—the Greedy Coordinate Gradient (GCG)~\citep{GCG2023Zou} and AmpleGCG~\citep{liao2024amplegcg} on three common open-source LLMs Llama2-7B-chat-hf~\citep{touvron2023llama}, Vicuna7B-1.5~\citep{chiang2023vicuna} and Mistral-7B~\citep{jiang2023mistral}. Using GCG, we generated 1,000 adversarial suffixes and assess their transferability on the AdvBench dataset. For AmpleGCG, we produce 5,000 adversarial suffixes using the provided model AmpleGCG-llama2-sourced-llama2-7b-chat. For our universal feature extractor method, we set the parameters as follows: number of iterations $I=500$,evaluation interval $c =10$,learning rate $\alpha= 2e-3$, and token-regularization  $\lambda=10$. Additionally, we evaluate the REpresentation Engineering (RepE) method~\citep{Representation2023Zou}, which manipulates representations in layers $15–17$ of the LLMs, whereas our value attack modifies only the representation of embedding layer. For all methods, we report the best performance achieved using a single universal token or embedding value.

\textbf{Harmful Tuning Setup}: We evaluated our harmful fine-tuning approach on models with strong alignment, including two open-source models (Llama2-7B-chat-hf and Llama3-guard) and two closed-source models (gpt-3.5-turbo-0125 and gpt-4o-mini-2024-07-18). For the open-source models, we fine-tuned them for 1000 steps. For the closed-source models, we conducted fine-tuning for three epochs using the OpenAI API. Notably, when fine-tuning the GPT models with harmful data, we selected 50 harmful data points combined with 1,000 entries from the Alpaca dataset. This approach was necessary because OpenAI's usage policies include random sampling to test for policy violations, and training the model with a fully harmful dataset would disrupt the training process after only a few steps.

\subsection{Experimental Resutls}
 Table~\ref{tab:attack_comparison} presents the results of our universal suffix transferability experiments. In the case of the token attack, our embedding token attack on harmful datasets achieved the highest performance in both Attack Success Rate (ASR) and harmfulness scores. Notably, universal tokens generated from format-specific datasets also transferred effectively to the harmful AdvBench dataset, with harmfulness scores comparable to those produced by AmpleGCG. We observed that while GCG achieved a high ASR but had a lower harmfulness score, likely due to its inconsistent generation of high-quality harmful content.\\
For the value attack, we demonstrated that universal suffixes generated from both the format-specific dataset and the harmful dataset reached a 100\% ASR across all models. These findings support our hypothesis that it is possible to generate universal adversarial tokens and embeddings using benign format datasets while maintaining high transferability to harmful prompts. Moreover, the quality of the generated responses is also guaranteed, as the structured format inherently encourages high-quality content output.
\begin{table*}[t]
    \vskip -0.1in
    \centering
    \caption{ASR and harmfulness scores of various universal adversarial attack methods across three LLMs}
    \label{tab:3-3}
    \vskip 0.15in
    \begin{tabular}{l|cc|cc|cc}
    \toprule
    & \multicolumn{2}{c|}{Llama2-7B} & \multicolumn{2}{c|}{Vicuna-7B} & \multicolumn{2}{c}{Mistral-7B} \\
    \midrule
    Method & ASR.  & Harm. & ASR.  & Harm. & ASR.  & Harm.  \\
    \midrule
    GCG & 76.7\%  & 2.94 &   94.6\% & 4.71 & 86.1\%  & 4.18 \\
    AmpleGCG & 69.4\% & 3.66 &  \textbf{98.7\%} & 4.89  & 83.1\% & 4.13  \\
    Token Attack (Structure Dataset) & 67.8\% & 3.64 & 92.0\% & 4.65  & 88.3\%  & 4.32 \\
    Token Attack (Harmful Dataset) & \textbf{77.8\%} & \textbf{ 4.00} & 97.5\%  &  \textbf{4.90}& \textbf{92.3\%} & \textbf{4.65}  \\
    \midrule
        RepE   & 99.5\%  & 4.92   & 99.2\% & 4.87 & 99.6\% & 4.93  \\
        Value Attack (Structure Dataset)  & 99.4\%  & 4.91 &98.4\% & 4.93&98.1\%  & 4.94  \\
        Value Attack (Harmful Dataset) & \textbf{100\% } & \textbf{4.95} & \textbf{99.4\%}&\textbf{ 4.96 }&\textbf{ 100\%} & \textbf{4.97}  \\
    \bottomrule
    \end{tabular}
    \label{tab:attack_comparison}
\end{table*}


Our experiments aim to validate the hypothesis that features derived from benign datasets are sufficient to cause unintended model behaviors, both from the perspective of jailbreak attacks and harmful fine-tuning.


\noindent\textbf{Jailbreak Results} Table~\ref{tab:attack_comparison} presents the results of our universal suffix transferability experiments. In the case of the token attack, our embedding token attack on harmful datasets achieved the highest performance in both Attack Success Rate (ASR) and harmfulness scores. Notably, universal tokens generated from format-specific datasets also transferred effectively to the harmful AdvBench dataset, with harmfulness scores comparable to those produced by AmpleGCG. We observed that while GCG achieved a high ASR but had a lower harmfulness score, likely due to its inconsistent generation of high-quality harmful content.\\
For the value attack, we demonstrated that universal suffixes generated from both the format-specific dataset and the harmful dataset reached a 100\% ASR across all models. These findings support our hypothesis that it is possible to generate universal adversarial tokens and embeddings using benign format datasets while maintaining high transferability to harmful prompts. Moreover, the quality of the generated responses is also guaranteed, as the structured format inherently encourages high-quality content output.



\begin{table*}[t]
    \vskip -0.1in
    \centering
    \caption{Impact of fine-tuning on model safety alignment across different datasets and language models. }
    \label{tab:finetuning_comparison}
    \vskip 0.15in
    \begin{tabular}{l|cc|cc|cc|cc}
    \toprule
    & \multicolumn{2}{c|}{Llama2} & \multicolumn{2}{c|}{Llama3-guard} & \multicolumn{2}{c|}{GPT-3.5} & \multicolumn{2}{c}{GPT-4o-mini} \\
    \midrule
    Dataset & ASR. & Harm. & ASR. & Harm. & ASR. & Harm. & ASR. & Harm. \\
    \midrule
    Original & 0\% & 1.00 & 0\% & 1.00 & 0\% & 1.00 & 0\% & 1.00 \\
    Benign Dataset & 20.2\% & 1.68 & 21.9\% & 1.76 & 12.1\% & 1.30 & 0\% & 1.00 \\
    Format Dataset & 80.4\% & 4.09 & 99.7\% & 4.86 & 99.0\% & 4.86 & 75.2\% & 3.75 \\
    Harmful Dataset & 100\% & 4.95 & 100\% & 4.96 & 99.3\% & 4.92 & 95.0\% & 4.75 \\
    \bottomrule
    \end{tabular}
\end{table*}



\textbf{Harmful Tuning Results}: Table~\ref{tab:finetuning_comparison} presents the results of harmful fine-tuning on models with strong safety alignment. The original models, without any fine-tuning, did not produce harmful responses. Fine-tuning on the benign dataset led to some degradation in safety alignment, except for GPT-4-mini, where fine-tuning on benign datasets had no impact on the model's alignment. However, fine-tuning on the format-specific dataset resulted in a significant loss of safety alignment, comparable to models fine-tuned on explicitly harmful datasets. All four models appeared to lose their safety mechanisms after being fine-tuned on the format dataset. \\
\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \includegraphics[width=0.45\textwidth]{images/harmfulness.pdf}
    \caption{Relationship between the size of the fine-tuning dataset and the degradation of safety alignment in Llama 2 7B-chat. }
    \label{fig:dataset_size}
\end{wrapfigure}
To determine how much data is required for a model to lose its safety alignment, we conducted further experiments with Llama2-7B-chat-hf fine-tuned on varying amounts of format-specific and harmful data. Figure \ref{fig:dataset_size} illustrates these results. We found that as few as $10$ harmful examples were sufficient to compromise the model's safety alignment. While fine-tuning on $10$ examples from the format dataset also led to a breakdown in safety alignment, this effect may be due to overfitting, as models fine-tuned on $20$ or $50$ format examples still retained some safety alignment. Generally, a larger number of format data points (approximately $100$) were needed to significantly reduce the safety alignment in the Llama2 model compared to fine-tuning with harmful data.
\begin{figure}[!t]
\centering
\includegraphics[width=0.9\columnwidth]{images/harmful_datasets.pdf}
\caption{ Examples showing how embedding suffixes derived from harmful datasets can induce diverse formatted responses in both benign and harmful prompts. }
\label{fig:harmful_uap}
\end{figure}
\subsection{A closer look at features extracted from harmful dataset}  As discussed in Section~\ref{sec:method}, our universal feature extractor is designed to extract dominant features from specific-format datasets.   Although harmful datasets typically lack universal-format responses, our method can still extract features that prompt the model to respond to a wide range of harmful prompts. In this section, we delve deeper into these universal suffixes generated from harmful datasets and analyze the universal features they encapsulate. 

Figure~\ref{fig:harmful_uap} illustrates this process. By extracting a set of universal adversarial suffixes from harmful datasets, we evaluated their effects on both benign and harmful prompts. Interestingly, we observed that these suffixes elicited diverse specific format behaviors. For example, certain adversarial suffixes cause the model to generate outputs in BASIC programming language format. 

Motivated by this discovery, we constructed three benign, format-specific datasets—\emph{BASIC}, \emph{Storytelling}, and \emph{Letter Writing}—by appending the universal suffixes extracted from harmful datasets to benign prompts. We followed the data construction method outlined in Section~\ref{sec:method}; however, instead of using system templates to guide the generations, we applied features extracted from the harmful datasets. To assess the impact on model safety alignment, we fine-tuned the GPT-4-mini model on these datasets.


For comparative analysis, we also created a new dataset adopting a \emph{Poetic} format by providing a system template that instructed the model to respond in verse. This dataset served as a control to determine whether all dominant features necessarily lead to alignment degradation.
\begin{table*}[h]
\centering
\caption{Comparison of model safety alignment degradation in GPT-4o-mini after fine-tuning on various format-specific datasets.}
\label{tab:dataset_category}
\begin{tabular}{l|c|c|c|c|c}
\toprule
Instructor & \multicolumn{2}{c|}{Template-instructed} & \multicolumn{3}{c}{Suffix-instructed} \\
\cmidrule{1-6}
Type& {Structure} & {Poem} & {Character Setting} & {Story-Telling} & {BASIC CODE} \\
\midrule
GPT-4o-mini & 75.2\% (3.75) & 6.3\% (1.09) & 70.2\% (3.44) & 96.3\% (4.75) & 91.9\% (4.44) \\
\bottomrule
\end{tabular}
\end{table*}

The results, presented in Table~\ref{tab:dataset_category}, reveal that fine-tuning on datasets constructed with universal suffixes from harmful datasets led to significant degradation in safety alignment. In contrast, fine-tuning on the Poetic dataset constructed using system template did not compromise the model's safety mechanisms, even though the model output adhered to the specified poetic format. This suggests that not all dominant features inherently pose risks; rather, the specific characteristics embedded within the universal play a critical role in affecting model alignment.




% These findings support our hypothesis that features derived from benign datasets are sufficient to cause unintended model behaviors, both in the context of jailbreak attacks and harmful fine-tuning. In jailbreak attacks, it is straightforward to train an adversarial suffix that acts as a feature within the adversarial prompt, prompting the model to generate responses in a specific format without considering the potential harmfulness of the content. Moreover, during fine-tuning, models can easily internalize these universal features from the dataset, leading to a complete loss of safety alignment.

