\section{Introduction}
Large Vision Language Models (LVLMs) have recently demonstrated remarkable progress across a wide range of multimodal tasks~\cite{li2025benchmark,baechler2024screenai}, achieving substantial image understanding through extensive pretraining and fine-tuning on large-scale image datasets. Given that vision and text are integrated into a common representation space in LVLMs, employing a unified safety mechanism for both modalities, rather than training separate ones, could prove both effective and efficient. However, this is currently not the case. While the base language model has built-in safety mechanisms against harmful textual inputs~\cite{zong2024safety}, LVLMs fine-tuned for multimodal understanding demonstrate fairly limited safety measures when exposed to harmful images.
For example, evaluations on the toxic image dataset~\cite{wang2023tovilag} show that traditional LVLMs (e.g., Llava-1.5~\cite{liu2024visual}) achieve a 0\% defense success rate against toxic visuals, despite maintaining some text safety. More recently developed  multimodal models like Qwen~\cite{bai2023qwenb} and Janus-Pro~\cite{chen2025janus} similarly have limited safety, i.e., with a 1.6\% defense success rate.  In fact, merely requiring an LVLM to describe a toxic image can inadvertently lead to harmful responses.

Existing approaches to safeguarding LVLMs typically rely on safety pre-filtering techniques~\cite{gou2024eyes,helff2024llavaguard} or safety-oriented fine-tuning~\cite{zong2024safety},  both of which may introduce substantial computational costs and compromise overall utility. For instance, LlavaGuard~\cite{helff2024llavaguard} uses a two-step process (safety filtering then processing), incurring up to 500\% overhead, while fine-tuning methods like TGA~\cite{xu2024cross} require full dataset captioning yet achieve only a 21.2\% defense rate across seven toxic categories. Given that existing LVLMs such as those built on CLIP exhibit strong zero-shot classification capabilities, we believe that these models inherently have the capabilities to semantically understand the images, and therefore a promising yet under-explored strategy is to leverage the models' inherent capabilities for aligning safety across multimodal. 
% For example, CLIP~\cite{radford2021learning}, which often serves as the vision encoder in LVLMs (e.g. the Llava series), exhibits strong zero-shot classification capabilities that can be repurposed as an efficient safety detection mechanism.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.82\textwidth]{images/llava_helper.pdf}
    \caption{Multimodal processing pipeline in visual language models. Visual input $X_v$ is encoded into CLS token and features $Z_v$, which are projected to $H_v$. Text input $X_q$ is tokenized into $H_q$, concatenated with $H_v$, and processed by language model $F_\theta$ to generate response $Y_a$.}
    \label{fig:vllm_arch}
\end{figure*}

In this work, we propose \textbf{SafeCLIP}, a lightweight, CLIP-driven method that leverages the inherent multimodal alignment of LVLMs to detect and mitigate toxic visual inputs in a zero-shot manner. SafeCLIP repurposes the vision encoder’s CLS token—normally discarded after feature extraction—as a robust safety-aware signal. By projecting the CLS token into CLIP’s text embedding space and comparing it against a carefully designed bank of toxic concept descriptors, SafeCLIP identifies harmful visual scenes with high accuracy. Furthermore, since the CLS token is generated during inference, integrating SafeCLIP into existing LVLMs incurs negligible computational cost. This low-latency approach also facilitates potential deployment during fine-tuning, enabling the automatic generation of safe alignment targets and dynamic adjustment of training objectives to reinforce safety.

Through extensive experiments on toxic image datasets, we show that SafeCLIP outperforms state-of-the-art safety methods. On Llava-1.5, SafeCLIP achieves a 66.9\% defense success rate across seven toxicity categories and a low 3.2\% false positive rate on benign inputs. In contrast, state-of-the-art approaches such as ESCO~\cite{gou2024eyes}
 and LlavaGuard achieve 52.9\% and 49.2\% defense success rates with false positive rates of 10.7\% and 3.4\%. Additionally, while ESCO and LlavaGuard incur latency increases of up to 210.0\% and 500.0\%, SafeCLIP only adds a 7.2\% increase for neutral inputs and even reduces latency by 5.7\% for toxic inputs, thanks to the shorter refusal responses.These results highlight SafeCLIP’s ability to effectively defend against toxic images such as explicit imagery, violence, and offensive gestures while considerably reducing computational overhead.

Our contributions can be summarized as follows:
\begin{itemize}
\item We propose a novel zero-shot toxic content detection method that utilizes the CLS token's global semantic representation, aligning image embeddings with predefined textual descriptions to enable efficient detection without modifying the LVLM architecture.
\item We propose a dynamic safety correction pipeline that prevents harmful responses by appending safe instructions during inference and adjusting training targets during fine-tuning, ensuring safe content generation.
\item We validate SafeCLIP on multiple toxic image datasets, demonstrating superior defense success rates and lower false positive rates compared to state-of-the-art safety baselines, while maintaining model efficiency with minimal runtime overhead.
\end{itemize}




