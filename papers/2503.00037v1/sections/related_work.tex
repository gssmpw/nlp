\section{Related Work}
This study relates to research on LVLM Vulnerability and LVLM safety.
\subsection{LVLM Vulnerability}
By integrating the capabilities of visual perception with LLMs, LVLMs~\cite{liu2024llava,bai2023qwenb} inherit the robust reasoning capabilities of LLMs alongside multimodal understanding. However, despite incorporating robust textual safety mechanisms, these models remain vulnerable to toxic visual inputs. Current research on LVLM vulnerabilities can be categorized into two main approaches. The first approach demonstrates how a toxic image (without modification) could directly lead to harmful generation~\cite{wang2023tovilag,xu2024cross}. Second approach reveals how adversarial techniques can be used to generate harmful responses from seemingly benign images~\cite{dong2023robust,qi2023visual}. In this work, we focus on first type and introduce a safety mechanism to defend against toxic visual inputs.

\subsection{LVLM safety}
To enhance the safety of LVLMs, existing methods can be broadly divided into two groups. The first group involves safety instruction-tuning on supervised toxic vision data~\cite{wang2023tovilag,zong2024safety}. However, collecting the multimodal data for safety instruction-tuning is much more challenging than gathering textual data alone. The other group focuses on protecting LVLMs during inference~\cite{helff2024llavaguard,gou2024eyes}, however this strategy can be time-consuming. For instance, ESCO requires four times the inference for a single toxic image. Moreover, recent work has introduced a novel vision-language alignment training method called TGA~\cite{xu2024cross}, which necessitates captioning on a large-scale image dataset while still offering limited safety performance. In this work, we propose SafeCLIP, an efficient and effective solution that can be integrated into both the inference and fine-tuning phases of LVLMs.




% \section{Related Work}
% Our research intersects with two primary domains: jailbreak attacks against Large Language Models (LLMs) and the implications of fine-tuning on model safety alignment.

% \subsection{Jailbreak Attack}
% Jailbreak attacks represent a significant security concern in LLM deployment, where carefully crafted inputs manipulate models to produce unsafe or unauthorized responses. Recent research has advanced automated approaches for generating such attacks, encompassing various methodologies. These include gradient-based optimization techniques \citep{GCG2023Zou,liu2024generating}, evolutionary computation through genetic algorithms \citep{liu2023autodan}, and stochastic exploration via random search strategies \citep{pal2023future,hayase2024query}. Another prominent approach leverages auxiliary LLMs to enhance and refine jailbreak templates \citep{yu2023gptfuzzer, PAIR2023Chao}.

% Our investigation specifically focuses on methods generating adversarial suffixes with high cross-prompt transferability. We hypothesize that these transferable adversarial suffixes encode fundamental features that remain effective across diverse input contexts. While our methodology could potentially extend to other adversarial attack paradigms, such extensions would require substantial modifications to both experimental design and analytical frameworks.

% \subsection{Fine-Tuning and Safety}
% The relationship between fine-tuning and model safety presents a complex landscape. While the degradation of safety alignment following fine-tuning with harmful data is well-documented \citep{shan2022traceback,shu2023exploitability,zheng2024prompt}, recent studies have revealed a more nuanced phenomenon: even fine-tuning with ostensibly benign data can compromise safety guardrails \citep{qi2023fine,zhan2023removing}.

% He \emph{et al.} \citep{he2024s} conducted a comprehensive investigation of this phenomenon using sophisticated data selection techniques, including representation matching and gradient matching. Their findings revealed that certain data structures—particularly lists, bullet points, and mathematical formulations—can significantly impact model safety during benign fine-tuning. While their research shares our observation regarding the safety implications of structured format data, our work advances the field in two crucial aspects. First, we propose a systematic methodology for generating safety-compromising benign datasets through universal adversarial suffix generation. Second, our research provides a deeper analytical framework for understanding how specific response structures and styles influence model behavior, offering crucial insights into the mechanisms by which benign data can inadvertently undermine LLM safety alignments.
% \section{Related Work}
% Our research bridges two key areas: jailbreak attacks on Large Language Models (LLMs) and the safety implications of fine-tuning.

% \subsection{Jailbreak Attack}
% Recent advances in jailbreak attacks have produced automated methods for compromising LLM safety, including gradient-based optimization \citep{GCG2023Zou,liu2024generating}, genetic algorithms \citep{liu2023autodan}, random search strategies \citep{pal2023future,hayase2024query}, and auxiliary LLM-based template refinement \citep{yu2023gptfuzzer, PAIR2023Chao}. Our work focuses specifically on generating adversarial suffixes with high cross-prompt transferability, hypothesizing that these suffixes contain universally effective features. While our approach could extend to other adversarial attacks, this would require significant modifications to our experimental framework.

% \subsection{Fine-Tuning and Safety}
% Research has shown that fine-tuning can compromise LLM safety alignment, not only with harmful data \citep{shan2022traceback,shu2023exploitability,zheng2024prompt} but surprisingly also with benign data \citep{qi2023fine,zhan2023removing}. He \emph{et al.} \citep{he2024s} demonstrated that structured data (e.g., lists, bullet points) selected through representation and gradient matching can degrade model safety during benign fine-tuning. While their work identifies the safety risks of structured format data, our research advances this understanding by systematically generating safety-compromising benign datasets and analyzing how specific response structures influence model behavior.