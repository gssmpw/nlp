\section{Related Work}
Most multiview feature learning methods suffer from drawbacks such as high complexity and limited performance~\cite{chen2021multiview,zhan2018multiview,ijcai2019p510,liu2020efficient}. Recently, several consistency-based multiview feature learning methods have been proposed, aiming to maximize consistency between different views. Inspired by the strategy to maximize view consistency between two sets in CCA~\cite{hotelling1936relations}, \cite{andrew2013deep} maps multiview features into a common space and concatenates the low-dimensional features as the common representation. ~\cite{wang2015deep} further introduce autoencoders in multiview feature learning compared with~\cite{andrew2013deep}.
Besides, by leveraging co-training strategy~\cite{blum1998combining}, \cite{Lin2022} design dual contrastive prediction to learn the cross-view consistency.

As one of the most effective consistent learning paradigms, contrastive learning has achieved SOTA performance~\cite{chen2020simple,he2020momentum}. The basic idea of contrastive learning is learning a feature space from raw data by maximizing the similarity between positive pairs while minimizing that of negative pairs. In recent, some methods have shown the success of contrastive learning in multiview feature learning~\cite{trostenMVC,Xu_2022_CVPR}, where similarities of positive pairs are maximized and that of negative pairs are minimized via NT-Xent~\cite{chen2020simple}. \cite{trostenMVC} learns common representation by aligning representations from different views at the sample level. ~\cite{yan2023gcfagg} learns the global structural relationships between samples and utilizes them to obtain consistent data representations. Simultaneously, structural information is utilized to select negative pairs for cross-view contrastive learning. In~\cite{xu2024self}, the weights are optimized based on the discrepancy between pairwise representations, performing self-weighted contrastive learning. Considering consistency between the cluster assignments among multiple views, \cite{chen2023deep} proposes a cross-view contrastive learning method to learn view-invariant representations by contrasting cluster assignments among multiple views. Moreover, contrastive clustering~\cite{li2021contrastive} designed for single-view clustering tasks, constructs two distinct views through data augmentation and subsequently projects them into feature space. Using two separate projection heads, the method conducts contrastive learning at different levels in the row and column space to jointly learn representations and cluster assignments. \cite{Xu_2022_CVPR} conducts multi-level features contrast from multiple views to achieve consistency. Furthermore, some recent contrastive learning works, notably BYOL~\cite{grill2020bootstrap} and SimSiam~\cite{chen2021exploring}, have shown the remarkable ability to learn powerful representations using only positive pairs, which has proven to be a simple and effective method~\cite{Tian2021}.