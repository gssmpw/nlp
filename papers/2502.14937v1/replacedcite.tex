\section{Related Works}
\subsection{Generalizable Autoencoder}\label{sec:Generalizable_Autoencoder}

Since traditional image compression algorithms such as JPEG 2000 usually utilize only linear decorrelation transformations of the input data, a new neural image compression approach was proposed using autoencoder which is trained on large images or videos dataset. ____.  
This approach utilizes the ability of the autoencoder to learn the nonlinear mapping of the images to high-dimensional low entropy latent space and then project them back to the image space ____.  

The autoencoder framework is based on four main components ____. The first one consisted of analysis neural transfer function $g_a$ to map the image to latent space, followed by a quantization function $Q$ , to quantize the latent. To restore the image back, a synthesis transform $g_s$ is used. One crucial component is a Context-based Entropy model to reduce and estimate the bit-rate. 

The network architecture of the analysis and synthesis transform functions may consist only of convolutional layers ____. Another architecture introduced residual connection with convolutional layer as base model ____. Also, transformer-based layers ____, and attention layers mixed with convolutional layers ____ have been utilised as transform functions. 

Since the conditional entropy of the quantized latent vector $\hat{y}$ given a context is smaller than or equal to the entropy of $\hat{y}$ without context, several methods have been proposed for contextual entropy model. A hyper-prior analysis model $h_{a}$ was proposed to extract a context from $\hat{y}$ as $z$, which is quantized to $\hat{z}$. a hyper-prior synthesis model $h_{s}$ creates from $\hat{z}$ parameters of univariate Gaussians which model the probability distribution of $\hat{y}$ for the sake of entropy coding ____. Other methods have extended the distribution model with Gaussian with mean and scale ____. Other methods introduced asymmetric Gaussian distribution, Gaussian mixture distribution model ____, and a Gaussian-Laplacian logistic mixture model ____ as a distribution model that allows more flexibility. 

To improve the compression efficiency, various context-based entropy modeling was proposed. An autoregressive model was introduced that conditioned each pixel with the previously decoded pixels for more effective context modeling ____. Another context model is the checkerboard convolution which divides the Latent representation into anchor part which is used to extract the context for non-anchor part ____.  A channel-wise context model ____ and channel-wise with unevenly grouped context model ____ were introduced to exploit redundancy between channels. Recently, a context model which tried to exploit diverse range of correlations within the latent representation using attention-based architecture ____. 

\subsection{Overfitted Neural Function}\label{sec:Overfitted_Neural_Function}

The notion of utilizing an overfitted neural function centers around representing image data as the learnable parameters of a neural function, as opposed to discrete pixel values. This neural function can be evaluated to reconstruct the RGB values of image pixels. The concept gained prominence with the advent of neural radiance fields, a technique for neural rendering ____. In the domain of imaging, attempts have been made to represent entire datasets, such as the MNIST dataset, using neural functions to achieve resolution-agnostic representations ____. One key advantage of modeling images as neural functions is their resolution agnosticism; images can be represented as continuous neural functions and then evaluated at any desired resolution. This approach assumes that the image signals are inherently continuous.

The pioneering work that introduced the idea of overfitted learnable functions for image compression was COIN ____. COIN employs a simple multilayer perceptron (MLP) to map pixel coordinates to their corresponding $RGB$ values by leveraging the efficiency of periodic activation functions ____. Although COIN achieved performance comparable to JPEG compression, it was limited by its inability to exploit pixel locality due to the non-local nature of MLPs. This limitation was mitigated by employing a multi-resolution latent representation followed by a non-linear MLP ____.

Drawing inspiration from multi-resolution latents, COOL-CHIC introduced an overfitted learned image codec with low decoding complexity that significantly enhanced compression performance compared to COIN.

The COOL-CHIC framework ____ consists of four principal components: a multi-resolution latent representation followed by an upsampling kernel, either learned or predefined, subsequently followed by synthesis convolution layers incorporating residual connections. The final component is an autoregressive model that generates a probability distribution for each latent pixel based on previously decoded ones.