@online{balleEndtoendOptimizedImage2017,
  title = {End-to-End {{Optimized Image Compression}}},
  author = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
  date = {2017-03-03},
  eprint = {1611.01704},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/1611.01704},
  urldate = {2024-03-08},
  abstract = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\AutoEncoder\End-to-end_Optimized_Image_Compression_Balle_et_al_2017.pdf}
}

@online{balleVariationalImageCompression2018,
  title = {Variational Image Compression with a Scale Hyperprior},
  author = {Ballé, Johannes and Minnen, David and Singh, Saurabh and Hwang, Sung Jin and Johnston, Nick},
  date = {2018-05-01},
  eprint = {1802.01436},
  eprinttype = {arXiv},
  eprintclass = {cs, eess, math},
  url = {http://arxiv.org/abs/1802.01436},
  urldate = {2024-08-08},
  abstract = {We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate–distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\Variational_image_compression_Balle_et_al_2018.pdf}
}

@online{blardOverfittedImageCoding2024,
  title = {Overfitted Image Coding at Reduced Complexity},
  author = {Blard, Théophile and Ladune, Théo and Philippe, Pierrick and Clare, Gordon and Jiang, Xiaoran and Déforges, Olivier},
  date = {2024-03-18},
  eprint = {2403.11651},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2403.11651},
  url = {http://arxiv.org/abs/2403.11651},
  urldate = {2024-12-03},
  abstract = {Overfitted image codecs offer compelling compression performance and low decoder complexity, through the overfitting of a lightweight decoder for each image. Such codecs include Cool-chic, which presents image coding performance on par with VVC while requiring around 2000 multiplications per decoded pixel. This paper proposes to decrease Cool-chic encoding and decoding complexity. The encoding complexity is reduced by shortening Cool-chic training, up to the point where no overfitting is performed at all. It is also shown that a tiny neural decoder with 300 multiplications per pixel still outperforms HEVC. A near real-time CPU implementation of this decoder is made available at https://orange-opensource.github.io/Cool-Chic/.},
  pubstate = {prepublished},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\Zotero\storage\XUCMUFID\Blard et al. - 2024 - Overfitted image coding at reduced complexity.pdf}
}

@online{chengLearnedImageCompression2020,
  title = {Learned {{Image Compression}} with {{Discretized Gaussian Mixture Likelihoods}} and {{Attention Modules}}},
  author = {Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
  date = {2020-03-30},
  eprint = {2001.01568},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2001.01568},
  url = {http://arxiv.org/abs/2001.01568},
  urldate = {2024-08-20},
  abstract = {Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used PSNR metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding (VVC) regarding PSNR. More importantly, our approach generates more visually pleasant results when optimized by MS-SSIM. This project page is at this https URL https://github.com/ZhengxueCheng/Learned-Image-Compression-with-GMM-and-Attention},
  pubstate = {prepublished},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\Learned_Image_Compression_Cheng_et_al_2020.pdf}
}

@online{dupontCOINCOmpressionImplicit2021,
  title = {{{COIN}}: {{COmpression}} with {{Implicit Neural}} Representations},
  shorttitle = {{{COIN}}},
  author = {Dupont, Emilien and Goliński, Adam and Alizadeh, Milad and Teh, Yee Whye and Doucet, Arnaud},
  date = {2021-04-10},
  eprint = {2103.03123},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2103.03123},
  url = {http://arxiv.org/abs/2103.03123},
  urldate = {2024-12-02},
  abstract = {We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\OverFittedSingleImage\COIN_Dupont_et_al_2021.pdf}
}

@article{fuLearnedImageCompression2023,
  title = {Learned {{Image Compression With Gaussian-Laplacian-Logistic Mixture Model}} and {{Concatenated Residual Modules}}},
  author = {Fu, Haisheng and Liang, Feng and Lin, Jianping and Li, Bing and Akbari, Mohammad and Liang, Jie and Zhang, Guohe and Liu, Dong and Tu, Chengjie and Han, Jingning},
  date = {2023},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {32},
  pages = {2063--2076},
  issn = {1941-0042},
  doi = {10.1109/TIP.2023.3263099},
  url = {https://ieeexplore.ieee.org/abstract/document/10091784?casa_token=-b_pKCG5l9cAAAAA:kzZhjPCHjBw3OJskhuitArTP0lSKYq5p0DXIeztWmJPhrtVCL4eIk3_PwLXmg2X2ak42ihWt3w},
  urldate = {2024-11-25},
  abstract = {Recently deep learning-based image compression methods have achieved significant achievements and gradually outperformed traditional approaches including the latest standard Versatile Video Coding (VVC) in both PSNR and MS-SSIM metrics. Two key components of learned image compression are the entropy model of the latent representations and the encoding/decoding network architectures. Various models have been proposed, such as autoregressive, softmax, logistic mixture, Gaussian mixture, and Laplacian. Existing schemes only use one of these models. However, due to the vast diversity of images, it is not optimal to use one model for all images, even different regions within one image. In this paper, we propose a more flexible discretized Gaussian-Laplacian-Logistic mixture model (GLLMM) for the latent representations, which can adapt to different contents in different images and different regions of one image more accurately and efficiently, given the same complexity. Besides, in the encoding/decoding network design part, we propose a concatenated residual blocks (CRB), where multiple residual blocks are serially connected with additional shortcut connections. The CRB can improve the learning ability of the network, which can further improve the compression performance. Experimental results using the Kodak, Tecnick-100 and Tecnick-40 datasets show that the proposed scheme outperforms all the leading learning-based methods and existing compression standards including VVC intra coding (4:4:4 and 4:2:0) in terms of the PSNR and MS-SSIM. The source code is available at https://github.com/fengyurenpingsheng.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {Complexity theory,Context modeling,Correlation,Decoding,Deep learning-based image compression,Entropy,entropy coding,Entropy coding,Gaussian mixture model,Image coding,residual network},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\Learned_Image_Compression_Fu_et_al_2023.pdf}
}

@online{heELICEfficientLearned2022,
  title = {{{ELIC}}: {{Efficient Learned Image Compression}} with {{Unevenly Grouped Space-Channel Contextual Adaptive Coding}}},
  shorttitle = {{{ELIC}}},
  author = {He, Dailan and Yang, Ziming and Peng, Weikun and Ma, Rui and Qin, Hongwei and Wang, Yan},
  date = {2022-03-29},
  eprint = {2203.10886},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2203.10886},
  url = {http://arxiv.org/abs/2203.10886},
  urldate = {2024-08-20},
  abstract = {Recently, learned image compression techniques have achieved remarkable performance, even surpassing the best manually designed lossy image coders. They are promising to be large-scale adopted. For the sake of practicality, a thorough investigation of the architecture design of learned image compression, regarding both compression performance and running speed, is essential. In this paper, we first propose uneven channel-conditional adaptive coding, motivated by the observation of energy compaction in learned image compression. Combining the proposed uneven grouping model with existing context models, we obtain a spatial-channel contextual adaptive model to improve the coding performance without damage to running speed. Then we study the structure of the main transform and propose an efficient model, ELIC, to achieve state-of-the-art speed and compression ability. With superior performance, the proposed model also supports extremely fast preview decoding and progressive decoding, which makes the coming application of learning-based image compression more promising.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\ELIC_He_et_al_2022.pdf}
}

@online{jiangMLICLinearComplexity2024,
  title = {{{MLIC}}++: {{Linear Complexity Multi-Reference Entropy Modeling}} for {{Learned Image Compression}}},
  shorttitle = {{{MLIC}}++},
  author = {Jiang, Wei and Yang, Jiayu and Zhai, Yongqi and Gao, Feng and Wang, Ronggang},
  date = {2024-02-20},
  eprint = {2307.15421},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2307.15421},
  urldate = {2024-11-17},
  abstract = {Recently, learned image compression has achieved impressive performance. The entropy model, which estimates the distribution of the latent representation, plays a crucial role in enhancing rate-distortion performance. However, existing global context modules rely on computationally intensive quadratic complexity computations to capture global correlations. This quadratic complexity imposes limitations on the potential of high-resolution image coding. Moreover, effectively capturing local, global, and channel-wise contexts with acceptable even linear complexity within a single entropy model remains a challenge. To address these limitations, we propose the Linear Complexity Multi-Reference Entropy Model (MEM++). MEM++ effectively captures the diverse range of correlations inherent in the latent representation. Specifically, the latent representation is first divided into multiple slices. When compressing a particular slice, the previously compressed slices serve as its channel-wise contexts. To capture local contexts without sacrificing performance, we introduce a novel checkerboard attention module. Additionally, to capture global contexts, we propose the linear complexity attention-based global correlations capturing by leveraging the decomposition of the softmax operation. The attention map of the previously decoded slice is implicitly computed and employed to predict global correlations in the current slice. Based on MEM++, we propose image compression model MLIC++. Extensive experimental evaluations demonstrate that our MLIC++ achieves state-of-the-art performance, reducing BD-rate by 13.39\% on the Kodak dataset compared to VTM-17.0 in PSNR. Furthermore, MLIC++ exhibits linear GPU memory consumption with resolution, making it highly suitable for high-resolution image coding. Code and pre-trained models are available at https://github.com/JiangWeibeta/MLIC.},
  pubstate = {prepublished},
  version = {9},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\MLIC++_Jiang_et_al_2024.pdf}
}

@online{jiangMLICMultiReferenceEntropy2024,
  title = {{{MLIC}}: {{Multi-Reference Entropy Model}} for {{Learned Image Compression}}},
  shorttitle = {{{MLIC}}},
  author = {Jiang, Wei and Yang, Jiayu and Zhai, Yongqi and Ning, Peirong and Gao, Feng and Wang, Ronggang},
  date = {2024-01-16},
  eprint = {2211.07273},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2211.07273},
  urldate = {2024-11-17},
  abstract = {Recently, learned image compression has achieved remarkable performance. The entropy model, which estimates the distribution of the latent representation, plays a crucial role in boosting rate-distortion performance. However, most entropy models only capture correlations in one dimension, while the latent representation contain channel-wise, local spatial, and global spatial correlations. To tackle this issue, we propose the Multi-Reference Entropy Model (MEM) and the advanced version, MEM\$\textasciicircum +\$. These models capture the different types of correlations present in latent representation. Specifically, We first divide the latent representation into slices. When decoding the current slice, we use previously decoded slices as context and employ the attention map of the previously decoded slice to predict global correlations in the current slice. To capture local contexts, we introduce two enhanced checkerboard context capturing techniques that avoids performance degradation. Based on MEM and MEM\$\textasciicircum +\$, we propose image compression models MLIC and MLIC\$\textasciicircum +\$. Extensive experimental evaluations demonstrate that our MLIC and MLIC\$\textasciicircum +\$ models achieve state-of-the-art performance, reducing BD-rate by \$8.05\textbackslash\%\$ and \$11.39\textbackslash\%\$ on the Kodak dataset compared to VTM-17.0 when measured in PSNR. Our code is available at https://github.com/JiangWeibeta/MLIC.},
  pubstate = {prepublished},
  version = {9},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\MLIC_Jiang_et_al_2024.pdf}
}

@article{kerbl3DGaussianSplatting2023,
  title = {{{3D Gaussian Splatting}} for {{Real-Time Radiance Field Rendering}}},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkuehler, Thomas and Drettakis, George},
  date = {2023-08},
  journaltitle = {ACM Trans. Graph.},
  volume = {42},
  number = {4},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3592433},
  url = {https://dl.acm.org/doi/10.1145/3592433},
  urldate = {2023-08-01},
  abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  langid = {english},
  file = {C:\Users\ahmed\OneDrive\Research\AI\Reconstruction\NeuralRadianceFields\General\3D_Gaussian_Splatting_for_Real-Time_Radiance_Field_Rendering_Kerbl_et_al_2023.pdf}
}

@online{kimAttentiveNeuralProcesses2019,
  title = {Attentive {{Neural Processes}}},
  author = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  date = {2019-07-09},
  eprint = {1901.05761},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1901.05761},
  url = {http://arxiv.org/abs/1901.05761},
  urldate = {2024-12-02},
  abstract = {Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ahmed\OneDrive\Research\AI\Components\ModelCompression\Quantization\Compression\Attentive_Neural_Processes_Kim_et_al_2019.pdf}
}

@inproceedings{laduneCOOLCHICCoordinatebasedLow2023,
  title = {{{COOL-CHIC}}: {{Coordinate-based Low Complexity Hierarchical Image Codec}}},
  shorttitle = {{{COOL-CHIC}}},
  author = {Ladune, Théo and Philippe, Pierrick and Henry, Félix and Clare, Gordon and Leguay, Thomas},
  date = {2023},
  pages = {13515--13522},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Ladune_COOL-CHIC_Coordinate-based_Low_Complexity_Hierarchical_Image_Codec_ICCV_2023_paper.html},
  urldate = {2024-08-07},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\OverFittedSingleImage\COOL-CHIC_Ladune_et_al_2023.pdf}
}

@article{leeEntropyConstrainedImplicitNeural2023,
  title = {Entropy-{{Constrained Implicit Neural Representations}} for {{Deep Image Compression}}},
  author = {Lee, Soonbin and Jeong, Jong-Beom and Ryu, Eun-Seok},
  date = {2023},
  journaltitle = {IEEE Signal Processing Letters},
  volume = {30},
  pages = {663--667},
  issn = {1558-2361},
  doi = {10.1109/LSP.2023.3279780},
  url = {https://ieeexplore.ieee.org/document/10132493?denied=},
  urldate = {2024-08-19},
  abstract = {Implicit neural representations (INRs) for various data types have gained popularity in the field of deep learning owing to their effectiveness. However, previous studies on INRs have only focused on recovering original representations. This letter investigated an image compression model based on INRs using a model compression technique for entropy-constrained neural networks. Specifically, the proposed model trains a multilayer perceptron (MLP) to overfit a single image and then uses its weights to optimize its compressed representation using additive uniform noise. Accordingly, the proposed model efficiently minimizes the size of the model weight in an end-to-end manner. This training optimization process is fairly desirable for adjusting the rate of distortion for image compression. In contrast to other model compression techniques, the proposed model is implemented without additional training process or memory cost. By introducing entropy loss, this letter demonstrated that the proposed model can be used to preserve high image quality while maintaining smaller model size. The experimental results demonstrated that the proposed model achieved comparable performance to conventional image compression models without incurring high storage costs.},
  eventtitle = {{{IEEE Signal Processing Letters}}},
  keywords = {Computational modeling,Data models,Distortion,Entropy,Image coding,Image compression,implicit neural representation,model compression,Neural networks,Training},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\OverFittedSingleImage\Entropy-Constrained_Implicit_Neural_Representations_for_Deep_Image_Compression_Lee_et_al_2023.pdf}
}

@inproceedings{leguayLowComplexityOverfittedNeural2023,
  title = {Low-{{Complexity Overfitted Neural Image Codec}}},
  booktitle = {2023 {{IEEE}} 25th {{International Workshop}} on {{Multimedia Signal Processing}} ({{MMSP}})},
  author = {Leguay, Thomas and Ladune, Théo and Philippe, Pierrick and Clare, Gordon and Henry, Félix and Déforges, Olivier},
  date = {2023-09},
  pages = {1--6},
  issn = {2473-3628},
  doi = {10.1109/MMSP59012.2023.10337636},
  url = {https://ieeexplore.ieee.org/abstract/document/10337636},
  urldate = {2024-08-07},
  abstract = {We propose a neural image codec at reduced complexity which overfits the decoder parameters to each input image. While autoencoders perform up to a million multiplications per decoded pixel, the proposed approach only requires 2300 multiplications per pixel. Albeit low-complexity, the method rivals autoencoder performance and surpasses HEVC performance under various coding conditions. Additional lightweight modules and an improved training process provide a 14\% rate reduction with respect to previous overfitted codecs, while offering a similar complexity. This work is made open-source at http://orange-opensource.github.io/Cool-Chic/.},
  eventtitle = {2023 {{IEEE}} 25th {{International Workshop}} on {{Multimedia Signal Processing}} ({{MMSP}})},
  keywords = {Codecs,Conferences,Encoding,Image coding,Low-complexity,Neural networks,Overfitting,Signal processing,Training},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\OverFittedSingleImage\Low-Complexity_Overfitted_Neural_Image_Codec_Leguay_et_al_2023.pdf}
}

@article{liuComprehensiveBenchmarkSingle2020,
  title = {A {{Comprehensive Benchmark}} for {{Single Image Compression Artifact Reduction}}},
  author = {Liu, Jiaying and Liu, Dong and Yang, Wenhan and Xia, Sifeng and Zhang, Xiaoshuai and Dai, Yuanying},
  date = {2020},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {29},
  pages = {7845--7860},
  issn = {1941-0042},
  doi = {10.1109/TIP.2020.3007828},
  url = {https://ieeexplore.ieee.org/abstract/document/9139290?casa_token=rdF1_Gq-J_4AAAAA:LAGd1bVF3cmWFwVo4IG5m3WLBfVxpSGusrnXTQA6TBFpgxS5XgAZfRoaMVczyHIx7f4uZwa3GA},
  urldate = {2024-11-25},
  abstract = {We present a comprehensive study and evaluation of existing single image compression artifact removal algorithms using a new 4K resolution benchmark. This benchmark is called the Large-Scale Ideal Ultra high-definition 4K (LIU4K), and it includes including diversified foreground objects and background scenes with rich structures. Compression artifact removal, as a common post-processing technique, aims at alleviating undesirable artifacts, such as blockiness, ringing, and banding caused by quantization and approximation in the compression process. In this work, a systematic listing of the reviewed methods is presented based on their basic models (handcrafted models and deep networks). The main contributions and novelties of these methods are highlighted, and the main development directions are summarized, including architectures, multi-domain sources, signal structures, and new targeted units. Furthermore, based on a unified deep learning configuration (i.e. same training data, loss function, optimization algorithm, etc.), we evaluate recent deep learning-based methods based on diversified evaluation measures. The experimental results show state-of-the-art performance comparisons of existing methods based on both full-reference, non-reference, and task-driven metrics. Our survey gives a comprehensive reference source for future research on single image compression artifact removal and inspires new directions in related fields.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {benchmark,Benchmark testing,Compression artifacts removal,deep learning,Deep learning,Image resolution,Image restoration,loop filter,Quantization (signal),side information},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\A_Comprehensive_Benchmark_for_Liu_et_al_2020.pdf}
}

@inproceedings{liuLearnedImageCompression2023,
  title = {Learned {{Image Compression With Mixed Transformer-CNN Architectures}}},
  author = {Liu, Jinming and Sun, Heming and Katto, Jiro},
  date = {2023},
  pages = {14388--14397},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Learned_Image_Compression_With_Mixed_Transformer-CNN_Architectures_CVPR_2023_paper.html},
  urldate = {2024-11-26},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\Learned_Image_Compression_Liu_et_al_2023.pdf}
}

@online{liuUnifiedEndtoEndFramework2020,
  title = {A {{Unified End-to-End Framework}} for {{Efficient Deep Image Compression}}},
  author = {Liu, Jiaheng and Lu, Guo and Hu, Zhihao and Xu, Dong},
  date = {2020-05-23},
  eprint = {2002.03370},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2002.03370},
  url = {http://arxiv.org/abs/2002.03370},
  urldate = {2024-11-25},
  abstract = {Image compression is a widely used technique to reduce the spatial redundancy in images. Recently, learning based image compression has achieved significant progress by using the powerful representation ability from neural networks. However, the current state-of-the-art learning based image compression methods suffer from the huge computational cost, which limits their capacity for practical applications. In this paper, we propose a unified framework called Efficient Deep Image Compression (EDIC) based on three new technologies, including a channel attention module, a Gaussian mixture model and a decoder-side enhancement module. Specifically, we design an auto-encoder style network for learning based image compression. To improve the coding efficiency, we exploit the channel relationship between latent representations by using the channel attention module. Besides, the Gaussian mixture model is introduced for the entropy model and improves the accuracy for bitrate estimation. Furthermore, we introduce the decoder-side enhancement module to further improve image compression performance. Our EDIC method can also be readily incorporated with the Deep Video Compression (DVC) framework to further improve the video compression performance. Simultaneously, our EDIC method boosts the coding performance significantly while bringing slightly increased computational cost. More importantly, experimental results demonstrate that the proposed approach outperforms the current state-of-the-art image compression methods and is up to more than 150 times faster in terms of decoding speed when compared with Minnen's method. The proposed framework also successfully improves the performance of the recent deep video compression system DVC. Our code will be released at https://github.com/liujiaheng/compression.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\A_Unified_End-to-End_Liu_et_al_2020.pdf}
}

@online{luTransformerbasedImageCompression2021,
  title = {Transformer-Based {{Image Compression}}},
  author = {Lu, Ming and Guo, Peiyao and Shi, Huiqing and Cao, Chuntong and Ma, Zhan},
  date = {2021-11-12},
  eprint = {2111.06707},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2111.06707},
  url = {http://arxiv.org/abs/2111.06707},
  urldate = {2024-11-27},
  abstract = {A Transformer-based Image Compression (TIC) approach is developed which reuses the canonical variational autoencoder (VAE) architecture with paired main and hyper encoder-decoders. Both main and hyper encoders are comprised of a sequence of neural transformation units (NTUs) to analyse and aggregate important information for more compact representation of input image, while the decoders mirror the encoder-side operations to generate pixel-domain image reconstruction from the compressed bitstream. Each NTU is consist of a Swin Transformer Block (STB) and a convolutional layer (Conv) to best embed both long-range and short-range information; In the meantime, a casual attention module (CAM) is devised for adaptive context modeling of latent features to utilize both hyper and autoregressive priors. The TIC rivals with state-of-the-art approaches including deep convolutional neural networks (CNNs) based learnt image coding (LIC) methods and handcrafted rules-based intra profile of recently-approved Versatile Video Coding (VVC) standard, and requires much less model parameters, e.g., up to 45\% reduction to leading-performance LIC.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\Transformer-based_Image_Lu_et_al_2021.pdf}
}

@online{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020-08-03},
  eprint = {2003.08934},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.08934},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2023-11-07},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C:\Users\ahmed\OneDrive\Research\AI\Reconstruction\NeuralRadianceFields\General\NeRF_Mildenhall_et_al_2020.pdf}
}

@inproceedings{minnenJointAutoregressiveHierarchical2018,
  title = {Joint {{Autoregressive}} and {{Hierarchical Priors}} for {{Learned Image Compression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Minnen, David and Ballé, Johannes and Toderici, George D},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/53edebc543333dfbf7c5933af792c9c4-Abstract.html},
  urldate = {2024-08-08},
  abstract = {Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate-distortion performance and generates smaller files than existing methods: 15.8\% rate reductions over the baseline hierarchical model and 59.8\%, 35\%, and 8.4\% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.},
  file = {C\:\\Users\\ahmed\\OneDrive\\Research\\AI\\GenerativeAI\\ImageCodec\\Joint_Autoregressive_and_Minnen_et_al_2018.pdf;C\:\\Users\\ahmed\\OneDrive\\Research\\AI\\GenerativeAI\\ImageCodec\\Joint_Autoregressive_and_Minnen_et_al_22.pdf}
}

@article{mullerInstantNeuralGraphics2022,
  title = {Instant {{Neural Graphics Primitives}} with a {{Multiresolution Hash Encoding}}},
  author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  date = {2022-07},
  journaltitle = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  eprint = {2201.05989},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530127},
  url = {http://arxiv.org/abs/2201.05989},
  urldate = {2023-09-05},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of \$\{1920\textbackslash!\textbackslash times\textbackslash!1080\}\$.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {C:\Users\ahmed\OneDrive\Research\AI\Reconstruction\NeuralRadianceFields\General\Instant_Neural_Graphics_Primitives_with_a_Multiresolution_Hash_Encoding_Muller_et_al_2022.pdf}
}

@inproceedings{sitzmannImplicitNeuralRepresentations2020,
  title = {Implicit {{Neural Representations}} with {{Periodic Activation Functions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  date = {2020},
  volume = {33},
  pages = {7462--7473},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html},
  urldate = {2024-12-03},
  abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions.},
  file = {C:\Users\ahmed\Zotero\storage\NH49JIIF\Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Acti.pdf}
}

@online{theisLossyImageCompression2017a,
  title = {Lossy {{Image Compression}} with {{Compressive Autoencoders}}},
  author = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
  date = {2017-03-01},
  eprint = {1703.00395},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1703.00395},
  url = {http://arxiv.org/abs/1703.00395},
  urldate = {2024-08-08},
  abstract = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {C:\Users\ahmed\OneDrive\Research\AI\GenerativeAI\ImageCodec\Lossy_Image_Compression_with_Theis_et_al_2017.pdf}
}

