\documentclass{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage[style=ieee, url=false, backend=biber,]{biblatex}
\usepackage{xcolor}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{parskip}
\newcommand{\red}{\textcolor{red}}
\addbibresource{References.bib}
\usepackage[symbol]{footmisc}
\graphicspath{ {./images/} }
\usepackage{subcaption}
\usepackage{standalone}

\DeclareBibliographyDriver{article}{%
  \printnames{author} % Print the authors' names
  \newunit\newblock
  \printfield{title} % Print the title of the article
  \newunit\newblock
  % \printfield{journaltitle} % Print the journal title
  \printfield{shortjournal}
  \setunit{\addspace} % Add space between journal title and volume
  \printfield{volume} % Print the volume number
% Optionally add number if needed:
%  \setunit*{\addcolon\space}
%  \printfield{number}
  \setunit*{\addcomma\space} % Add comma and space before pages
  \printfield{pages} % Print page numbers
  \setunit*{\addcomma\space} % Add comma and space before year
  \usebibmacro{date} % Use macro to print date/year 
  \finentry
}

\AtEveryBibitem{%
  \clearname{translator}
  \clearname{number}
  \clearlist{publisher}
  \clearlist{number}
  \clearfield{file}
  \clearfield {pubstate}
  \clearfield {url}
  \clearfield {urldate}
  \clearfield{urlyear}
  \clearfield{urlmonth}
  \clearfield{doi}
  \clearfield{archiveprefix}
  \clearfield{keywords}
  \clearfield{eprint}
}
\pagenumbering{gobble}
\begin{document}
\title{Compact Latent Representation for Image Compression (CLRIC)}

\author{\IEEEauthorblockN{Ayman A. Ameen\IEEEauthorrefmark{1} \IEEEauthorrefmark{2},
Thomas Richter\IEEEauthorrefmark{1}, and
André Kaup\IEEEauthorrefmark{3},}

\IEEEauthorblockA{\IEEEauthorrefmark{1}Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany}

\IEEEauthorblockA{\IEEEauthorrefmark{2} Department of Physics, Faculty of Science, Sohag University, Egypt}

\IEEEauthorblockA{\IEEEauthorrefmark{3} Friedrich-Alexander University at Erlangen-Nürnberg, Erlangen, Germany}}
\maketitle

\begin{abstract}
Current image compression models often require separate models for each quality level, making them resource-intensive in terms of both training and storage. To address these limitations, we propose an innovative approach that utilizes latent variables from pre-existing trained models (such as the Stable Diffusion Variational Autoencoder) for perceptual image compression. Our method eliminates the need for distinct models dedicated to different quality levels. We employ overfitted learnable functions to compress the latent representation from the target model at any desired quality level. These overfitted functions operate in the latent space, ensuring low computational complexity, around $25.5$ MAC/pixel for a forward pass on images with dimensions $(1363 \times 2048)$ pixels. This approach efficiently utilizes resources during both training and decoding. Our method achieves comparable perceptual quality to state-of-the-art learned image compression models while being both model-agnostic and resolution-agnostic. This opens up new possibilities for the development of innovative image compression methods.
\end{abstract}



\section{Introduction}

Effective compression techniques for images and videos plays a pivotal role for supplying the high quality digital content without the need to update the current infrastructure. Currently, the entertainment services occupy a large portion of the Internet traffic. Such services require high-quality perceptual images and videos and often does not require accurate pixel-wise reconstruction. The current widely used techniques, such as HEVC \cite{sullivanOverviewHighEfficiency2012} and VVC \cite{brossDevelopmentsInternationalVideo2021}, relies on linear operators which are handcrafted to minimize the bitrate of the target. One major issue with these techniques is the production of unacceptable artifacts in the compressed image or video at low bit rate. Learned image compression promises to solve the issues of traditional compression techniques. Currently, there are numerous approaches for learned image compression, such as, generalizable autoencoder and overfitted learnable neural function.  

Recent advancements in learned image compression have demonstrated impressive results in achieving low bit rates \cite{jiangMLICLinearComplexity2024, heELICEfficientLearned2022} and high human visual quality \cite{chenGenerativeVisualCompression2024, liHumanMachineCollaborative2024}, utilizing either generalizable autoencoders or overfitted learnable functions. However, these techniques have some drawbacks and limitations. The generalizable autoencoder approach requires training entirely separate models for image compression, each tailored to specific quality levels with corresponding bit rates, which typically involves training 6-8 distinct models from scratch, each representing a different quality level.

Similarly, the overfitted learnable function method \cite{laduneCOOLCHICCoordinatebasedLow2023, dupontCOINCOmpressionImplicit2021} attempts to fit a single image into a learnable function without leveraging prior information. This approach requires retraining from scratch for every new image and is performed in the image space, which is significantly larger than the latent space.

Our objective is to address these limitations through a novel approach. We trained an overfitted learnable function (such as COOLCHIC) on latents generated by existing latent image models (e.g., a stable diffusion autoencoder), without requiring retraining or backpropagation within the large pretrained latent image model. Our approch yields results comparable to state-of-the-art bit rate and image quality benchmarks, as shown in Figure \ref{fig:Overfitted_on_latent}. The advantages of this novel approach are summarized as follows:
\begin{itemize}

\item \textbf{Utilization of Pre-existing Models:} Leverage existing pretrained image generation models like the stable diffusion autoencoder for image compression without retraining or backpropagation, thus avoiding resource-intensive training processes.

\item \textbf{Efficient Latent Variable Storage:} Efficiently store latent variables from the image generation model, achieving target quality and corresponding bit rates without needing to train a hyper-encoder and decoder.

\item \textbf{Continuous Quality Adaptation:} Achieve continuous quality representation with target bit rates adapted to specific applications through adaptation of the loss function parameters, eliminating the need for separate models for each level of image quality.

% \item \textbf{Perceptual Quality Optimization:} Ensure perceptual quality is adaptive to the human visual system, enhancing the viewing experience, as depicted in Figure \ref{fig:lpips_KodakDataset}
\item \textbf{Perceptual Quality Optimization:} Ensure perceptual quality is adaptive to the human visual system, enhancing the viewing experience, as depicted in Figure \ref{fig:combined_assessment_scores}

 \item \textbf{Resource-efficient Overfitting:} Overfit the learnable function in the latent space instead of the image space, reducing memory and computational resource requirements,  since latent space is substantially smaller than image space by a factor of eight or more, enabling tailored image compression for each model using stochastic gradient descent techniques.

\end{itemize}

\section{Related Works}

\subsection{Generalizable Autoencoder}\label{sec:Generalizable_Autoencoder}

Since traditional image compression algorithms such as JPEG 2000 usually utilize only linear decorrelation transformations of the input data, a new neural image compression approach was proposed using autoencoder which is trained on large images or videos dataset. \cite{jiangMLICLinearComplexity2024,jiangMLICMultiReferenceEntropy2024,heELICEfficientLearned2022,theisLossyImageCompression2017a}.  
This approach utilizes the ability of the autoencoder to learn the nonlinear mapping of the images to high-dimensional low entropy latent space and then project them back to the image space \cite{balleEndtoendOptimizedImage2017, balleVariationalImageCompression2018}.  

The autoencoder framework is based on four main components \cite{heELICEfficientLearned2022,jiangMLICLinearComplexity2024}. The first one consisted of analysis neural transfer function $g_a$ to map the image to latent space, followed by a quantization function $Q$ , to quantize the latent. To restore the image back, a synthesis transform $g_s$ is used. One crucial component is a Context-based Entropy model to reduce and estimate the bit-rate. 

The network architecture of the analysis and synthesis transform functions may consist only of convolutional layers \cite{liuComprehensiveBenchmarkSingle2020}. Another architecture introduced residual connection with convolutional layer as base model \cite{liuLearnedImageCompression2023}. Also, transformer-based layers \cite{luTransformerbasedImageCompression2021}, and attention layers mixed with convolutional layers \cite{chengLearnedImageCompression2020} have been utilised as transform functions. 

Since the conditional entropy of the quantized latent vector $\hat{y}$ given a context is smaller than or equal to the entropy of $\hat{y}$ without context, several methods have been proposed for contextual entropy model. A hyper-prior analysis model $h_{a}$ was proposed to extract a context from $\hat{y}$ as $z$, which is quantized to $\hat{z}$. a hyper-prior synthesis model $h_{s}$ creates from $\hat{z}$ parameters of univariate Gaussians which model the probability distribution of $\hat{y}$ for the sake of entropy coding \cite{balleVariationalImageCompression2018}. Other methods have extended the distribution model with Gaussian with mean and scale \cite{minnenJointAutoregressiveHierarchical2018}. Other methods introduced asymmetric Gaussian distribution, Gaussian mixture distribution model \cite{chengLearnedImageCompression2020}, and a Gaussian-Laplacian logistic mixture model \cite{fuLearnedImageCompression2023} as a distribution model that allows more flexibility. 

To improve the compression efficiency, various context-based entropy modeling was proposed. An autoregressive model was introduced that conditioned each pixel with the previously decoded pixels for more effective context modeling \cite{minnenJointAutoregressiveHierarchical2018}. Another context model is the checkerboard convolution which divides the Latent representation into anchor part which is used to extract the context for non-anchor part \cite{chengLearnedImageCompression2020}.  A channel-wise context model \cite{liuUnifiedEndtoEndFramework2020} and channel-wise with unevenly grouped context model \cite{heELICEfficientLearned2022} were introduced to exploit redundancy between channels. Recently, a context model which tried to exploit diverse range of correlations within the latent representation using attention-based architecture \cite{jiangMLICMultiReferenceEntropy2024,jiangMLICLinearComplexity2024}. 

\subsection{Overfitted Neural Function}\label{sec:Overfitted_Neural_Function}

The notion of utilizing an overfitted neural function centers around representing image data as the learnable parameters of a neural function, as opposed to discrete pixel values. This neural function can be evaluated to reconstruct the RGB values of image pixels. The concept gained prominence with the advent of neural radiance fields, a technique for neural rendering \cite{mildenhallNeRFRepresentingScenes2020, mullerInstantNeuralGraphics2022, kerbl3DGaussianSplatting2023}. In the domain of imaging, attempts have been made to represent entire datasets, such as the MNIST dataset, using neural functions to achieve resolution-agnostic representations \cite{kimAttentiveNeuralProcesses2019}. One key advantage of modeling images as neural functions is their resolution agnosticism; images can be represented as continuous neural functions and then evaluated at any desired resolution. This approach assumes that the image signals are inherently continuous.

The pioneering work that introduced the idea of overfitted learnable functions for image compression was COIN \cite{dupontCOINCOmpressionImplicit2021}. COIN employs a simple multilayer perceptron (MLP) to map pixel coordinates to their corresponding $RGB$ values by leveraging the efficiency of periodic activation functions \cite{sitzmannImplicitNeuralRepresentations2020}. Although COIN achieved performance comparable to JPEG compression, it was limited by its inability to exploit pixel locality due to the non-local nature of MLPs. This limitation was mitigated by employing a multi-resolution latent representation followed by a non-linear MLP \cite{mullerInstantNeuralGraphics2022}.

Drawing inspiration from multi-resolution latents, COOL-CHIC introduced an overfitted learned image codec with low decoding complexity that significantly enhanced compression performance compared to COIN.

The COOL-CHIC framework \cite{laduneCOOLCHICCoordinatebasedLow2023, leeEntropyConstrainedImplicitNeural2023, leguayLowComplexityOverfittedNeural2023, blardOverfittedImageCoding2024} consists of four principal components: a multi-resolution latent representation followed by an upsampling kernel, either learned or predefined, subsequently followed by synthesis convolution layers incorporating residual connections. The final component is an autoregressive model that generates a probability distribution for each latent pixel based on previously decoded ones.

\section{Method}
\subsection{Overall Architecture}
The objective of our work is to introduce an architecture that has the capability representing latent variables from different image models in a compact and efficient manner. 
Our innovative approach leverages compact latent representations from pre-trained models to generate outputs with various bit rates. An overview of our proposed architecture is illustrated in Figure \ref{fig:Overfitted_on_latent}. This architecture allows for mapping any latent variable from a learned image model to a specified bit rate. In this configuration (Figure \ref{fig:Overfitted_on_latent}), an input image $x$ is processed by a pretrained encoder $E_{\theta}$ with parameters $\theta$, resulting in a latent variable $y$. The latent variable serves as desired target output of our learnable function $f_{\psi}$ with parameters $\psi$, which is overfitted to approximate the latent under constraints regarding the target bit rate and its corresponding quality. The loss function is defined as follows \cite{blardOverfittedImageCoding2024,leguayLowComplexityOverfittedNeural2023}:
\begin{equation}\label{equ:loss}
\mathcal{L}(\psi) = D(y, \hat{y}) + \lambda R(\hat{\psi})
\end{equation}

where $R(\hat{\psi})$ represents the rate of the learnable parameters of the overfitted function used to depict the latent $\hat{y}$, $\lambda$ is a Lagrange multiplier that can be tuned to control quality for achieving specific or targeted bit rates, and $D$ is the mean square error in latent Space. Following the training phase, the latent variable $\hat{y}$ is represented by the weights of a learnable function $f_{\psi}$, where $\psi$ denotes the parameters of this function. These parameters are stored or transmitted to the decoder. The function $f_{\psi}$  is evaluated to obtain the latent representation $\hat{y}$ at the decoder side. This latent representation is feeded into a decoder $D$ with pretrained learnable parameters $\phi$, resulting in the reconstructed image $\hat{x}$. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Overfitted_on_latent.pdf}
    \caption{General overview proposed approach of overfitting a learnable function on the latent from generalizable autoencoder}
    \label{fig:Overfitted_on_latent}
\end{figure}
\subsection{Generative Generalizable Autoencoder}
The generative generalizable autoencoder ($E_\theta, D_\phi$) selected from the Latent Diffusion autoencoder family \cite{rombachHighResolutionImageSynthesis2022, podellSDXLImprovingLatent2023, sauerFastHighResolutionImage2024}, which is pretrained for synthesizing high-resolution images. This selection is based on the models' capability for perceptual compression rather than semantic compression. This capability allows for the generation of images with high perceptual quality at a reduced bit rate. Another advantage of these models is their ability to capture the full data distribution by mapping images from pixel space to latent space with a reduced number of parameters \cite{kimBKSDMLightweightFast2023, chanTutorialDiffusionModels2024, garciaFineTuningImageConditionalDiffusion2024}. This parameter reduction enables the overfitted learnable function to operate effectively with fewer parameters, thereby minimizing training resource requirements. 
\subsection{Overfitted Neural Function}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{overfitted_function_latent.pdf}
    \caption{General overview of the proposed overfitted learnable decoder  architecture }
    \label{fig:overfitted_function_latent}
\end{figure}
We utilized COOLCHIC architecture \cite{leguayLowComplexityOverfittedNeural2023,laduneCOOLCHICCoordinatebasedLow2023,blardOverfittedImageCoding2024}  as our overfitted learnable function $f_{\psi}$ as illustrated in Figure \ref{fig:overfitted_function_latent}. 
The proposed architecture consists of three main neural networks \cite{leguayLowComplexityOverfittedNeural2023,laduneCOOLCHICCoordinatebasedLow2023}. The first neural network is an autoregressive model for multi-resolution latents, denoted as $f_{L}$, which provides a probability model characterized by a mean ${\mu}_{f}^{\hat{L_{i}}}$ and a standard deviation ${\sigma}_{f}^{\hat{L_{i}}}$, given the previously decoded latents. The primary objective of the autoregressive process is to enhance the contextual information available for entropy coding. This is because the conditional entropy of $\hat{y}$ given a specific context is less than or equal to the overall entropy of  $\hat{y}$:
$$
H(\hat{y})\leq H(\hat{y}\vert context)
$$
The context can be leveraged to achieve better compression by reducing the bit-rate needed to encode the data. The multi-resolution latents $\hat{\mathbf{L}}$ consist of two-dimensional grids with decreasing size according to:
$$ \hat{\mathbf{L}} = \left\{ \hat{\mathbf{L}}_k \in \mathbb{Z}^{\frac{H}{2^k} \times \frac{W}{2^k}}, k \in 0, \ldots, K-1 \right\}. $$
Here, $H$ and $W$ represent the width and height of a feature map of the latent from the generalization model, and $K$ is the number of the multi-resolution grids.  The second component is an adaptive up-sampling network $f_{u}$, which is initialized with a bicubic sampling kernel and updated during training to adapt to the targeted data \cite{leguayLowComplexityOverfittedNeural2023}. The final network is a synthesis neural network $f_{\psi}$, which produces the final the latent $\hat{y}$ of the generalizable autoencoder. 

\subsection{Effective Bit-Rate Aware Training of Overfitted Neural Function}
The objective of the optimization process is to minimize the distortion between the latent variable $y$ and its reconstructed latent $\hat{y}$, while also reducing the total bit rate associated with all the learnable parameters (latent and model) $\psi$ of the overfit learnable function $f_{\psi}$. This optimization aims to minimize the overall loss, which is expressed in \ref{equ:loss} \cite{laduneCOOLCHICCoordinatebasedLow2023,leguayLowComplexityOverfittedNeural2023,jiangMLICLinearComplexity2024}. The optimization process occurs for every single source image to be encoded as a separate entity.
To reduce the bit rate, quantization plays a crucial role in reducing the bitstream entropy by introducing a lossy compression scheme. However, quantization is a non-differentiable process. To address this issue, we approximate the quantization by adding uniform noise and switch to the actual quantization at the end of the training process of the overfitted neural function $f_{\psi}$ \cite{jiangMLICLinearComplexity2024,jiangMLICMultiReferenceEntropy2024,laduneCOOLCHICCoordinatebasedLow2023}.

\section{Experiments}
\newcommand{\textwidthfigure}{0.8}
\begin{figure*}[h]
\centering
\includegraphics[width=\textwidthfigure\textwidth]{0.05_CLIC2020_27_bitrate_zoom_300_compressed.pdf}
\captionsetup{width=\textwidthfigure\linewidth}
\caption{Our novel approach performance compared to MLIC++ and LIC-TCM on image num. 27 from the CLIC Professional Valid 2020 dataset.}
\label{fig:0.05_CLIC2020_27_bitrate_zoom_300}
\end{figure*}
\begin{figure*}[h]
\centering
\includegraphics[width=\textwidthfigure\textwidth]{0.2_Kodak_7_bitrate_zoom_compressed.png}
\captionsetup{width=\textwidthfigure\linewidth}
\caption{Comparison between our approach and different models on image num. 7 from the Kodak dataset.}
\label{fig:0.2_Kodak_7_bitrate_zoom_400}
\end{figure*}
 Initially, we outline our configuration and setup. Subsequently, we evaluate the model's performance through visual results followed by quantitative analysis using perceptual quality metrics and classical distortion metrics. Finally, we demonstrate the capability of continuous quality representation of the latent space at any arbitrary bitrate. Finally, we present our framework's complexity and limitations.
\subsection{Configuration}
This subsection presents the configuration setup for our generalizable autoencoder and our overfitted learnable function. These configurations can be easily modified depending on the target generalizable autoencoder model.
\subsubsection{\textbf{Generalizable Autoencoder}}
We select three derivatives from well-known and widely used open-source pretrained generative autoencoder models. The first model is Stable Diffusion v1.4 autoencoder, referred to in our results as SD \cite{rombachHighResolutionImageSynthesis2022}. The second and third models are Stable Diffusion XL refiner and Stable Diffusion 2 inpainting autoencoders \cite{podellSDXLImprovingLatent2023, sauerFastHighResolutionImage2024}, which we refer to as SD-XL and SD-In, respectively.
\subsubsection{\textbf{Overfitted Learnable Function}}
The overfitted learnable function is based on the COOL-CHIC framework \cite{leguayLowComplexityOverfittedNeural2023,laduneCOOLCHICCoordinatebasedLow2023}, which offers a framework with low decoding complexity. In our experimental setup, we employ an auto-regressive model characterized by an 8-context window. The model outputs the probability distribution for predicting the next latent variable to be decoded. This auto-regressive model comprises three layers with dimensions 8, 8, and 2, respectively, and incorporates a residual connection between the first and second layers. The two-dimensional multi-resolution latent variables are structured into seven grids. The upsampling network utilizes transposed convolutions with a learnable kernel of size 8, initially configured as a bicubic upsampling kernel \cite{leguayLowComplexityOverfittedNeural2023}. The synthesis network is composed of four layers with residual connections every two layers. The output corresponds to the latent size of a Generalizable Autoencoder, with the number of channels matching that of the Generalizable Autoencoder's latent channels. 
\subsubsection{\textbf{Training Strategy}}
The training strategy for this overfitted function is grounded in the COOL-CHIC framework \cite{laduneCOOLCHICCoordinatebasedLow2023,leguayLowComplexityOverfittedNeural2023,blardOverfittedImageCoding2024}, starting with an initial learning rate of $10^{-2}$and employing a decaying learning schedule. Training consists of 15,900 iterations divided into warm-up and main training phases. The warm-up phase includes 2,800 iterations split among five candidates trained over 400 iterations each; the top two candidates undergo an additional 400 iterations. Ultimately, the best candidate proceeds to the main training phase for an additional 13,100 iterations.

\newcommand{\textwidthplot}{0.33}
\begin{figure*}[h]
    \centering
    \begin{subfigure}{\textwidthplot\textwidth}
        \centering
        \includegraphics[width=\textwidth]{all_LPIPS_KodakDataset_compressed.pdf}
        % \caption{Assessment of LPIPS scores on the Kodak dataset.}
        \caption{}
        \label{fig:all_LPIPS_KodakDataset}
    \end{subfigure}%
    ~ % add space between subfigures if needed
    \begin{subfigure}{\textwidthplot\textwidth}
        \centering
        \includegraphics[width=\textwidth]{all_FID_KodakDataset_compressed.pdf}
        % \caption{Assessment of FID scores on the Kodak dataset.}
        \caption{}
        \label{fig:all_FID_KodakDataset}
    \end{subfigure}%
    ~ % add space between subfigures if needed
    \begin{subfigure}{\textwidthplot\textwidth}
        \centering
        \includegraphics[width=\textwidth]{all_LPIPS_professional_valid_2020_compressed.pdf}
        % \caption{Comparison of LPIPS scores on the CLIC Professional Valid 2020 dataset.}
        \caption{}
        \label{fig:all_LPIPS_professional_valid_2020}
    \end{subfigure}

    \caption{Assessments and comparisons of image compression models using different metrics and datasets. (a) LPIPS scores on the Kodak dataset, (b) FID scores on the Kodak dataset, and (c) LPIPS scores on the CLIC Professional Valid 2020 dataset.}
    \label{fig:combined_assessment_scores} 
\end{figure*}


\subsection{Performance}
To evaluate the performance and generalization capability of our model, we conducted validation using two distinct datasets: the Kodak Dataset and CLIC Professional Valid 2020. The Kodak Dataset, widely used for validating image compression models, consists of 24 images. Meanwhile, CLIC Professional Valid 2020 comprises 41 high-resolution images, making it well-suited for evaluating models in the context of modern digital imagery. We benchmarked our model against these datasets and compared its performance with various other models.
We compared our results to those of several state-of-the-art learned image compression models, including MLIC++ \cite{jiangMLICLinearComplexity2024}, LIC-TCM \cite{liuLearnedImageCompression2023}, and ELIC \cite{heELICEfficientLearned2022}. Additionally, we considered various model variants from notable works such as the Balle (2018) models, specifically the Factorized and Hyperprior variants \cite{balleVariationalImageCompression2018}. We also included the Minnen (2018) model variants, namely Mean and Hyperprior \cite{minnenJointAutoregressiveHierarchical2018}, as well as Cheng (2020) variants, which encompass Anchor and Attention \cite{chengLearnedImageCompression2020}.


\subsubsection{\textbf{Visual Results}}
Our approach employs a Variational Autoencoder (VAE) trained with a perceptual compression strategy, resulting in higher image fidelity and a pleasant perceptual visual experience. This method maintains high granularity and accurately captures the prominent structures in natural images. In contrast, competing models at comparable bitrates often exhibit blurred details or smooth structures with missing granularity.
At low bitrates, where other models tend to produce overly smooth outputs lacking fine details, our method maintains detailed granularity, as shown in Figure \ref{fig:0.05_CLIC2020_27_bitrate_zoom_300}. Our technique effectively reproduces elements like wavy water surfaces and cloudy scenes more accurately compared to other methods that yield overly smooth results.
Our method excels in preserving detail and color fidelity, as illustrated in Figure \ref{fig:0.2_Kodak_7_bitrate_zoom_400}. The synthetic details generated by our model closely resemble the original image's color and texture, whereas other models often suffer from noticeable color shifts and lack of detail. At higher bitrates, our approach retains rich textures that other models' images frequently miss.
\subsubsection{\textbf{Perceptual Quantitative Analysis}}
Our method employs a perceptual image compression strategy, necessitating an analysis of its performance using perceptual metrics. We compared our model against state-of-the-art learned image compression models using the Kodak dataset and the CLIC Professional Validation 2020 dataset (Figure \ref{fig:combined_assessment_scores}). The evaluation focused on perceptual quality, quantified by the Learned Perceptual Image Patch Similarity (LPIPS) metric, in relation to bit rate, denoted in bits per pixel (bpp). LPIPS was selected due to its strong correlation with human visual perception, aligning well with our perceptual image compression approach.
Perceptual quality was assessed for our approach across three different variational autoencoder models. We observed that our approach using the SD and SD-In variational autoencoders as front-end exhibited similar performance levels, with stability diminishing as bit rate increases, as illustrated in Figure \ref{fig:combined_assessment_scores}. Meanwhile, the SD-XL based approach demonstrated slightly worse performance and required higher bit rates to achieve comparable image quality. Other models demanded significantly higher bit rates to match similar levels of perceptual quality.
Furthermore, we utilized the Fréchet Inception Distance (FID) to evaluate image quality. FID measures the similarity between the distributions of reconstructed images and original images. Our method achieved superior results in terms of FID, as depicted in Figure \ref{fig:combined_assessment_scores}. The SD-XL autoencoder-based approach required a higher bit rate to reach an equivalent FID compared to approaches using SD or SD-In.

\section{Conclusion}
In this paper, we presented a novel image compression method based on a two-stages approach. The proposed approach consists of two main components: a generalizable autoencoder capable of leveraging pre-trained generative image models, and an overfitted function that efficiently encodes the latent representation in a compact form. Our method is efficient resource utilization by operating in the latent space rather than the image space. Furthermore, our method allows for continuous quality representation at any target bitrate. We performed analyses using the Learned Perceptual Image Patch Similarity (LPIPS) and Fréchet Inception Distance (FID) metrics. Our approach achieved superior LPIPS and FID scores at comparable bit rates when compared to other models. 

\printbibliography

\input{SupplementaryMaterials.tex}

\end{document}
