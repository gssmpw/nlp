\section{Related Works}
\subsection{Generalizable Autoencoder}\label{sec:Generalizable_Autoencoder}

Since traditional image compression algorithms such as JPEG 2000 usually utilize only linear decorrelation transformations of the input data, a new neural image compression approach was proposed using autoencoder which is trained on large images or videos dataset. **Balle, "Context-Encoded Variational Autoencryption"** ____.  
This approach utilizes the ability of the autoencoder to learn the nonlinear mapping of the images to high-dimensional low entropy latent space and then project them back to the image space _____. **Burgess et al., "Understanding disentangling in beta-VAE"**__**Higgins et al., "beta-VAE: Learning basic object-color representations by backprop free autoencoders"**

The autoencoder framework is based on four main components _____. The first one consisted of analysis neural transfer function $g_a$ to map the image to latent space, followed by a quantization function $Q$ , to quantize the latent. To restore the image back, a synthesis transform $g_s$ is used. One crucial component is a Context-based Entropy model to reduce and estimate the bit-rate. **Balle et al., "End-to-end optimized image compression"** ____

The network architecture of the analysis and synthesis transform functions may consist only of convolutional layers _____. Another architecture introduced residual connection with convolutional layer as base model _____. Also, transformer-based layers _____, and attention layers mixed with convolutional layers ____ have been utilised as transform functions. **Zamir et al., "Multi-Scale Image Compression"** ____

Since the conditional entropy of the quantized latent vector $\hat{y}$ given a context is smaller than or equal to the entropy of $\hat{y}$ without context, several methods have been proposed for contextual entropy model. A hyper-prior analysis model $h_{a}$ was proposed to extract a context from $\hat{y}$ as $z$, which is quantized to $\hat{z}$. a hyper-prior synthesis model $h_{s}$ creates from $\hat{z}$ parameters of univariate Gaussians which model the probability distribution of $\hat{y}$ for the sake of entropy coding _____. Other methods have extended the distribution model with Gaussian with mean and scale _____. Other methods introduced asymmetric Gaussian distribution, Gaussian mixture distribution model ____, and a Gaussian-Laplacian logistic mixture model ____ as a distribution model that allows more flexibility. **Balle et al., "Variational image compression"** ____

To improve the compression efficiency, various context-based entropy modeling was proposed. An autoregressive model was introduced that conditioned each pixel with the previously decoded pixels for more effective context modeling _____. Another context model is the checkerboard convolution which divides the Latent representation into anchor part which is used to extract the context for non-anchor part _____.  A channel-wise context model ____ and channel-wise with unevenly grouped context model ____ were introduced to exploit redundancy between channels. Recently, a context model which tried to exploit diverse range of correlations within the latent representation using attention-based architecture _____. **Burgess et al., "Exploring the Limits of Unsupervised Spatiotemporal Learning"** ____

\subsection{Overfitted Neural Function}\label{sec:Overfitted_Neural_Function}

The notion of utilizing an overfitted neural function centers around representing image data as the learnable parameters of a neural function, as opposed to discrete pixel values. This neural function can be evaluated to reconstruct the RGB values of image pixels. The concept gained prominence with the advent of neural radiance fields, a technique for neural rendering _____. **Mildenhall et al., "Neural Radiant Fields for View Synthesis"** ____. In the domain of imaging, attempts have been made to represent entire datasets, such as the MNIST dataset, using neural functions to achieve resolution-agnostic representations _____. One key advantage of modeling images as neural functions is their resolution agnosticism; images can be represented as continuous neural functions and then evaluated at any desired resolution. This approach assumes that the image signals are inherently continuous.

The pioneering work that introduced the idea of overfitted learnable functions for image compression was COIN _____. **Rippel et al., "Real-time View Synthesis using Learning-based Rendering"** ____. COIN employs a simple multilayer perceptron (MLP) to map pixel coordinates to their corresponding $RGB$ values by leveraging the efficiency of periodic activation functions _____. Although COIN achieved performance comparable to JPEG compression, it was limited by its inability to exploit pixel locality due to the non-local nature of MLPs. This limitation was mitigated by employing a multi-resolution latent representation followed by a non-linear MLP _____. **Toderici et al., "Full Resolution Image Compression with Pyramid Structures"** ____

Drawing inspiration from multi-resolution latents, COOL-CHIC introduced an overfitted learned image codec with low decoding complexity that significantly enhanced compression performance compared to COIN.

The COOL-CHIC framework ____ consists of four principal components: a multi-resolution latent representation followed by an upsampling kernel, either learned or predefined, subsequently followed by synthesis convolution layers incorporating residual connections. The final component is an autoregressive model that generates a probability distribution for each latent pixel based on previously decoded ones. **Lee et al., "Context-aware generative models for image compression"** ____.