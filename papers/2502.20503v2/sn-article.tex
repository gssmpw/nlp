%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
% added
\usepackage{float}
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
% \theoremstyle{thmstyleone}%
% \newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
% \newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

% \theoremstyle{thmstyletwo}%
% \newtheorem{example}{Example}%
% \newtheorem{remark}{Remark}%

% \theoremstyle{thmstylethree}%
% \newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Protecting multimodal large language models against misleading visualizations}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1,2,3]{\fnm{Jonathan} \sur{Tonglet}}


\author[2]{\fnm{Tinne} \sur{Tuytelaars}}

\author[3]{\fnm{Marie-Francine} \sur{Moens}}

\author[1]{\fnm{Iryna} \sur{Gurevych}}

\affil[1]{\orgdiv{Ubiquitous Knowledge Processing Lab (UKP Lab),
Department of Computer Science and Hessian Center for AI (hessian.AI)},\\ \orgname{TU Darmstadt}}

\affil[2]{\orgdiv{Department of Electrical Engineering}, \orgname{KU Leuven}}


\affil[3]{\orgdiv{Department of Computer Science}, \orgname{KU Leuven}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%


\abstract{We assess the vulnerability of multimodal large language models to misleading visualizations - charts that distort the underlying data  using techniques such as truncated or inverted axes, leading readers to draw inaccurate conclusions that may support misinformation or conspiracy theories. Our analysis shows that these distortions severely harm multimodal large language models, reducing their question-answering accuracy to the level of the random baseline. To mitigate this vulnerability, we introduce six inference-time methods to improve performance of MLLMs on misleading visualizations while preserving their accuracy on non-misleading ones. The most effective approach involves (1) extracting the underlying data table and (2) using a text-only large language model to answer questions based on the table. This method improves performance on misleading visualizations by 15.4 to 19.6 percentage points.\footnote{Code and data available at: \href{https://github.com/UKPLab/arxiv2025-misleading-visualizations}{github.com/UKPLab/arxiv2025-misleading-visualizations}}} 

%%================================%%
%% Sample for structured abstract %%
%%================================%%


\keywords{large language models, question answering, visualization}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle


\begin{figure}
\centering
\includegraphics[width=\textwidth]{real_world_examples_V2.pdf}
\caption{Four examples of real-world misleading visualizations \citep{lo2022misinformed} with QA pairs. The correct answer is colored in green, while the wrong answer supported by the misleader is colored in purple.}\label{fig4}
\end{figure}

Visualizations are widely used to convey data insights efficiently. However, design flaws - whether intentional or not - can distort the correct interpretation of the underlying data \citep{10.1145/2702123.2702608,lo2022misinformed,10.1145/3544548.3580910,10670488,10.1145/3313831.3376420,10.1145/3380851.3416762,YANG2021298}. These design flaws, or misleaders, include truncated, inverted, and dual axes, 3D effects, or inconsistent tick intervals. Misleading visualizations pose a serious threat to our society, as they are used to support misinformation and conspiracy theories \citep{10.1145/3544548.3580910}. Figure \ref{fig4} shows real-world examples of misleading visualizations used to misinform readers on sensitive topics such as access to safe drinking water, politics, COVID-19, or abortion statistics.  The CALVI test \citep{10.1145/3544548.3581406} demonstrated the impact of misleading visualizations in decreasing human readers' ability to interpret the underlying data accurately.

The visualization reasoning abilities of multimodal large language models (MLLMs) have advanced rapidly,  as demonstrated on the reference benchmark  ChartQA \citep{masry-etal-2022-chartqa}, suggesting their potential as chart reasoning assistants. However, it remains uncertain whether they are sensitive to misleaders, as humans are. If vulnerable, MLLMs used as chart reasoning assistants risk amplifying both the spread of misinformation and human belief in it.  Recent work has reported a sensitivity of GPT4 \citep{openai2023gpt4} to misleaders, but their analysis was limited to a single MLLM evaluated on six misleading visualizations \citep{10670574}.



In this study, we compare the question-answering (QA) performance of 16 MLLMs of varying sizes across three datasets: (a) a misleading visualization dataset containing $n$ = 143 instances and featuring 17 distinct misleaders, defined in Table \ref{tab1}. It combines the existing synthetic CALVI ($n$ = 45) \citep{10.1145/3544548.3581406} and CHARTOM ($n$ = 56)  \citep{bharti2024chartom}  datasets with 42 real-world examples \citep{lo2022misinformed} manually annotated with QA pairs; (b) a non-misleading visualization dataset ($n$ = 124), drawn from CALVI ($n$ = 15) and CHARTOM ($n$ = 56) and the real-world VLAT dataset ($n$ = 53) \citep{7539634}; and (c) ChartQA, the reference non-misleading real-world benchmark for assessing MLLMs ($n$ = 2500).


\begin{figure}
\centering
\includegraphics[width=\textwidth]{correction_methods.pdf}
\caption{Illustration of the six inference-time correction methods applied on a misleading visualization from CALVI \citep{10.1145/3544548.3581406}. The visualization suffers from inconsistent tick intervals on the y-axis. The correct answer is \textit{True}. In this case, it is only predicted after redrawing the visualization.}\label{fig3}
\end{figure}

Our evaluation reveals that misleaders substantially degrade MLLM QA performance, reducing accuracy by up to 34.8 percentage points compared to non-misleading visualizations. To mitigate this vulnerability, we explore six inference-time correction methods that modify the input image and/or prompt without requiring additional training data or fine-tuning. These methods aim to reduce the impact of misleaders while minimizing any negative effect on accuracy for non-misleading visualizations. We compare the following methods, illustrated in Figure \ref{fig3}: (1) including a warning message in the prompt to alert the MLLM to specific misleaders in the visualization; (2) extracting the axes using the MLLM and incorporating them into the prompt; (3) extracting the underlying data table and incorporating it into the prompt; (4) combining both extracted axes and table into the prompt; (5) providing the extracted table without the visualization to a text-only LLM, reframing the task as table-based QA; and (6) using a text-only LLM to generate code for a new visualization based on the extracted table, replacing the original one as input.



\section{Results}\label{sec2}


\subsection{Assessing the vulnerability to misleading visualizations}
Figure \ref{fig1} presents the QA accuracy of 16 MLLMs across the three datasets. The results reveal three key trends. 



First, MLLMs perform substantially worse on misleading visualizations than on non-misleading ones, with accuracy dropping by up to 34.8 percentage points. The decline is even more pronounced compared to ChartQA, reaching up to 65.5 percentage points. Furthermore, the mean MLLM accuracy on misleading visualizations (24.8\%) is close to the random baseline (25.6\%). This suggests that MLLMs struggle to correctly interpret the distorted data, making them highly vulnerable to misleaders. This aligns with findings from the CALVI study \citep{10.1145/3544548.3581406}, where human accuracy averaged 80\% on non-misleading visualizations but dropped to 39\% on misleading ones. 

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{multiplot_accuracy.pdf}
\caption{Top: Accuracy (\%) of various MLLMs on misleading visualization \citep{10.1145/3544548.3581406,bharti2024chartom,lo2022misinformed}, non-misleading visualization \citep{10.1145/3544548.3581406,bharti2024chartom,7539634}, and ChartQA datasets \citep{masry-etal-2022-chartqa}. The horizontal dashed line indicates the accuracy of the random baseline on misleading visualizations. Models are sorted by increasing accuracy on ChartQA. Bottom: Accuracy (\%) of various MLLMs on subsets of the misleading visualizations.}\label{fig1}
\end{figure}

Second, accuracy on misleading visualizations does not follow the upward performance trend observed in ChartQA, and to a lower extent, non-misleading visualizations. From these results, we conclude that reducing MLLM vulnerability to misleading visualizations will not happen naturally as a by-product of improving performance on standard benchmarks like ChartQA. This makes dedicated mitigation methods all the more necessary.


Third, the best-performing MLLMs on misleading visualizations, GPT-4o and InternVL2.5-38B, primarily outperform other MLLMs on the real-world subset. We assume this is due to their large parametric knowledge, which spans beyond the end date of the real-world subset (2022). This knowledge about real-world events and statistics allows them to answer some questions without leveraging the visualization. However, this advantage does not extend to synthetic data in CALVI and CHARTOM.






\subsection{Mitigating the impact of misleading visualizations}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{correction.pdf}
\caption{Change ($\Delta$) in accuracy (percentage points) on misleading and non-misleading visualization datasets using different inference-time correction methods to mitigate misleading visualizations. Statistically significant changes (p$\leq$0.05) are hashed.}\label{fig2}
\end{figure}

We evaluate the impact of the correction methods with three open-weight, mid-sized MLLMs which all perform below the random baseline on misleading visualizations: Qwen2VL-7B, Ovis1.6-9B, and InternVL2.5-8B. Figure \ref{fig2} presents the change in accuracy across the six correction methods: (1) inclusion of a warning message, inclusion of the (2) axes, (3) table, or (4) both, (5) table-based QA, and (6) redrawing the visualization. Qwen2.5-7B \citep{yang2024qwen2} serves as the text-only LLM for table-based QA and redrawing the visualization.

The most effective approach by far is table-based QA, yielding significant improvements of 15.4 to 19.6 percentage points. While incorrect or incomplete table extractions decrease performance on non-misleading visualizations — particularly for line charts, scatter plots, and maps — these losses are not significant. 

Another promising correction method is redrawing the visualization. It achieves significant but more modest improvements for two MLLMs. This approach is effective only when the generated code compiles on a Python interpreter; otherwise, it defaults to using the original visualization. Across both datasets, the lowest redrawing success rates are observed for scatter plots (79\%), and stacked bar charts (80\%), which we explain by their high number of visual items and complex layouts, while the success rates for all other chart types are above 90\%.

By either removing or modifying the visualization, these two methods effectively neutralize misleaders that exploit visual perception errors, such as inverted axes and inconsistent tick intervals. In Figure \ref{fig3}, redrawing the visualization corrects the misleading trend line, eliminating its deceptive elements and ensuring a more accurate representation of the data.

Other correction methods do not yield significant improvements. Including the extracted table in the prompt even has a significant negative impact on non-misleading visualizations for one MLLM. 

Adding a warning message provides non-significant changes of at most 4.9 percentage points, with most improvements observed on the real-world subset. This method assumes prior knowledge of the misleader present in the visualization, making the reported results an upper bound on its effectiveness. In practice, a classifier needs to detect first the presence of misleaders, which remains a challenging task \citep{10679256,10771149}, as MLLMs struggle not only to distinguish non-misleading visualizations from misleading ones but also to correctly identify the specific misleader in a given visualization. Given the already low results obtained with ground-truth misleader labels, this correction method appears unpromising overall. However, training a highly accurate misleader detection model could enable the selective application of other correction methods, eliminating their negative impact on non-misleading visualizations.

We examine further the quality of the intermediate table extraction step using CHARTOM \citep{bharti2024chartom}, which pairs each question with two visualizations of the same data — one misleading and one non-misleading. Ideally, the extracted tables should be identical for both. However, across all MLLMs, the extracted tables have a perfect match in only 4 out of 56 pairs (7.1\%), and a partial match for 13 to 14 other pairs (23 to 25\%). Notably, 3D effects achieve their deceptive purpose, as MLLMs systematically produce different tables when the visualization is presented with or without 3D effects.  Since table-based QA and redrawing the visualization highly depend on this intermediate table extraction step, improving it is an important direction for future work.

\section{Discussion}\label{sec3}


Our findings highlight the vulnerability of MLLMs to misleading visualizations, reducing their QA performance compared to non-misleading ones, down to the level of the random baseline.  To address this vulnerability, we explored six inference-time correction methods. The most effective approach involves extracting the underlying table and performing the QA task using only the table, improving accuracy by up to 19.6 percentage points. While recent work in visualization comprehension with MLLMs primarily treats visualizations as images \citep{10670526,zhang-etal-2024-tinychart,masry-etal-2025-chartgemma}, our findings challenge this direction and underscore the merits of earlier approaches, such as DePlot \citep{liu-etal-2023-deplot}, which use table extraction as an intermediate step, followed by table-based QA with a text-only model. 
By making MLLMs more robust to misleading visualizations, correction methods help ensure that AI-assisted chart reasoning does not yield inaccurate interpretations of the underlying data, reducing the risk that human users believe in and propagate misinformation.

We identify two limitations to our work. First, the visualization redrawing method, which uses the package Matplotlib, does not support maps, as Matplotlib lacks sufficient functionality for rendering high-quality maps. Second, we assume prior knowledge of the chart type (bar, line, ...)  for  generating the prompts for axes extraction and visualization redrawing. This is a reasonable assumption, as the chart type can either be provided by a human user or accurately predicted by a classifier as a preprocessing step.


\section{Methods}\label{sec4}

\subsection{Datasets}

The misleading and non-misleading visualization datasets  combine three existing resources and one introduced in this work. 
First, CALVI \citep{10.1145/3544548.3581406} includes 45 misleading and 15 non-misleading visualizations based on synthetic data, each paired with a multiple-choice question (MCQ) with three to four choices. 
Second, CHARTOM \citep{bharti2024chartom} contains 56 samples, including 28 MCQs, 20 free-text questions, and 8 ranking questions. The MCQs provide two to four choices. CHARTOM is the only dataset where each question is linked to two visualizations — one misleading and one non-misleading. Like CALVI, the underlying data is synthetic.
Third, VLAT \citep{7539634}, the reference dataset to evaluate the human comprehension of visualizations, provides  12 non-misleading visualizations, each paired with three to seven MCQs, for a total of 53 instances. The visualizations are based on real-world data. MCQs have two to four choices.
Fourth, we introduce a dataset of 42 real-world misleading visualizations, each annotated with a MCQ with three to four choices. The real-world visualizations come from a collection annotated with misleader labels \citep{lo2022misinformed}, which inspired the synthetic examples in CALVI. We manually create MCQs, using those from CALVI and CHARTOM as inspiration. The motivation for creating this additional resource is that CALVI and CHARTOM rely both on synthetic data. By incorporating questions about real-world data, we introduce direct conflicts with MLLMs' parametric knowledge, allowing us to assess their vulnerability in real-world scenarios. 

We also report the performance of MLLMs on the test set of ChartQA \citep{masry-etal-2022-chartqa}, which contains 2500 real-world visualizations paired with free-text questions, of which half are human-written and the others are AI-generated. 


The datasets cover together 11 chart types: line charts, area charts, stacked area charts, bar charts, stacked bar charts, histograms, pie charts, scatter plots, bubble charts,  maps, treemaps.

For free-text questions where the expected answer is a number, we adopt the relaxed accuracy metric of ChartQA \citep{masry-etal-2022-chartqa}. Specifically, a prediction is considered correct if it falls within a ±5\% interval of the ground truth. For MCQs, we present the choices in their default order. In Table \ref{tab2}, we report the mean accuracy of the MLLMs over three different choice orders. The choices are shuffled using three randomly generated numbers as seeds (654, 114, 25). Standard deviations for the accuracy range from 0.0 to 4.06 percentage points.

\subsection{Inference with (M)LLMs}

We evaluate 11 open-weight MLLMs from the Llava-Next \citep{10655294}, Qwen2VL \citep{wang2024qwen2}, Ovis-1.6 \citep{lu2024ovisstructuralembeddingalignment}, and InternVL2.5 \citep{chen2024expanding} families, ranging from 2 to 38 billion parameters. Additionally, we include two commercial models with undisclosed number of parameters, GPT-4 and GPT-4o \citep{openai2023gpt4}, as well as three open-weight MLLMs specifically trained for visualization reasoning: ChartInstruction \citep{10670526}, TinyChart \citep{zhang-etal-2024-tinychart},  and ChartGemma \citep{masry-etal-2025-chartgemma} ranging from 3 to 13 billion parameters.
We use the Hugging Face transformers \citep{wolf-etal-2020-transformers} implementation for all open-weight (M)LLMs and access GPT models via the Azure OpenAI API. To ensure deterministic outputs, we fix the random seed at 42, set the temperature to 0, and use a top-p value of 1. Following the standard ChartQA evaluation setup, all (M)LLMs are prompted in a zero-shot manner. For TinyChart, we report results using the Direct approach \citep{zhang-etal-2024-tinychart}.  

\subsection{Implementation of correction methods}

All prompts are in the code, which is provided as supplementary material. The significance of correction methods is assessed using the McNemar test (p $\leq$ 0.05) \citep{McNemar_1947}. The p-values are reported in Table \ref{tab3}.

\textbf{Misleader warning}: we insert in the prompt a short warning message based on the definitions of misleaders. The message is the same for all instances with the same type of misleader. There are no messages for the five types of misleaders where the visualization is deceiving only in the context of a specific question, making a standardized warning message impossible: cherry picking, misleading annotations, concealed uncertainty, missing normalization, and missing data.

\textbf{Axes and table extraction}: we prompt the MLLM to extract the axes or underlying table in a zero-shot setting. The axes and tables are formatted as text strings. We do not impose constraints on the delimiters used to indicate new rows and columns.

\textbf{Table-based QA}: the extracted table is provided as input with the question and an instruction to answer it based on the table. 

\textbf{Redrawn visualization}: the text-only LLM receives the extracted table and the chart type as text input and generates Python code to create a visualization using Matplotlib. The code is then executed to produce a new visualization. If the code compiles successfully, the newly generated visualization replaces the original one in the QA prompt; otherwise, the original visualization remains in use.


% \section{Acknowledgments}














% \section{Cross referencing}\label{sec8} 

% \subsection{Details on reference citations}\label{subsec7}

% Standard \LaTeX\ permits only numerical citations. To support both numerical and author-year citations this template uses \verb+natbib+ \LaTeX\ package. For style guidance please refer to the template user manual.

% Here is an example for \verb+\cite{...}+: \cite{bib1}. Another example for \verb+\citep{...}+: \citep{bib2}. For author-year citation mode, \verb+\cite{...}+ prints Jones et al. (1990) and \verb+\citep{...}+ prints (Jones et al., 1990).

% All cited bib entries are printed at the end of this article: \cite{bib3}, \cite{bib4}, \cite{bib5}, \cite{bib6}, \cite{bib7}, \cite{bib8}, \cite{bib9}, \cite{bib10}, \cite{bib11}, \cite{bib12} and \cite{bib13}.


% \section{Examples for theorem like environments}\label{sec10}

% For theorem like environments, we require \verb+amsthm+ package. There are three types of predefined theorem styles exists---\verb+thmstyleone+, \verb+thmstyletwo+ and \verb+thmstylethree+ 

%%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{tabular}{|l|p{19pc}|}
% \hline
% \verb+thmstyleone+ & Numbered, theorem head in bold font and theorem text in italic style \\\hline
% \verb+thmstyletwo+ & Numbered, theorem head in roman font and theorem text in italic style \\\hline
% \verb+thmstylethree+ & Numbered, theorem head in bold font and theorem text in roman style \\\hline
% \end{tabular}
% \bigskip
% %%=============================================%%
%% For presentation purpose, we have included  %%
%% \bigskip command. Please ignore this.       %%
%%=============================================%%

% For mathematics journals, theorem styles can be included as shown in the following examples:

% \begin{theorem}[Theorem subhead]\label{thm1}
% Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
% Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
% Example theorem text. 
% \end{theorem}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.
 

% \section{Methods}\label{sec11}

% Topical subheadings are allowed. Authors must ensure that their Methods section includes adequate experimental and characterization data necessary for others in the field to reproduce their work. Authors are encouraged to include RIIDs where appropriate. 

% \textbf{Ethical approval declarations} (only required where applicable) Any article reporting experiment/s carried out on (i)~live vertebrate (or higher invertebrates), (ii)~humans or (iii)~human samples must include an unambiguous statement within the methods section that meets the following requirements: 




% \backmatter

% \bmhead{Supplementary information}

% If your article has accompanying supplementary file/s please state so here. 

% Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

% Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

This work has been funded by the LOEWE
initiative (Hesse, Germany) within the
emergenCITY center (Grant Number:
LOEWE/1/12/519/03/05.001(0016)/72) and
by the German Federal Ministry of Education
and Research and the Hessian Ministry of Higher
Education, Research, Science and the Arts within
their joint support of the National Research
Center for Applied Cybersecurity ATHENE. We
gratefully acknowledge the support of Microsoft
with a grant for access to OpenAI GPT models via
the Azure cloud (Accelerate Foundation Model
Academic Research). We want to express our gratitude to Niklas Traser for conducting an initial exploration of the real-world data, to Jan Zimny for our insightful discussions on the topic of misleading visualizations, and to Germàn Ortiz, Manisha Venkat, and Max Glockner for their feedback on a draft of this work.

% Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

% Please refer to Journal-level guidance for any specific requirements.

% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
% \item Ethics approval and consent to participate
% \item Consent for publication
% \item Data availability 
% \item Materials availability
% \item Code availability 
% \item Author contribution
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}


\newpage

\begin{appendices}

\section{Definition of misleaders}\label{secA1}

\begin{table}[ht]
\caption{The 17 types of misleader included in the misleading visualizations dataset, with their number of occurrences ($n$) and definitions \citep{lo2022misinformed,10.1145/3544548.3581406}.}\label{tab1}%
\begin{tabular}{@{}ll@{}}
\toprule
Misleader &  Definition  \\
\midrule
Inverted axis ($n$=26) &    An axis is oriented in an unconventional 
direction   \\
& and   the perception of the data is reversed \citep{lo2022misinformed}. \\
Truncated axis ($n$=21) &      The axis does not start from zero or is truncated   \\
&  in the middle resulting  in an exaggerated difference \\
& between the two bars \citep{lo2022misinformed}.  \\
Inappropriate axis range ($n$=15) &   The axis range is either too broad or too
narrow to   \\ 
& accurately   visualize the data, allowing changes to be \\
&   minimized or maximized depending on \\
& the author’s intention \citep{lo2022misinformed}.\\ 
Inconsistent tick intervals ($n$=12) &   Cases with varying intervals between the ticks \citep{lo2022misinformed}.\\
3D effects ($n$=12) &  The closer something is, the larger it appears, \\
&  despite being the same size
in 3D perspective \citep{lo2022misinformed}. \\
Inappropriate item order ($n$=9) &  The axis labels or legends appear to be in a random \\ 
& order  due to manipulation of data ordering \citep{10.1145/3544548.3581406}.\\
Inappropriate aggregation  ($n$=8)  &     Aggregating data in an improper way that leads to   \\
&  inaccurate conclusions \citep{10.1145/3544548.3581406}.  \\
Dual axis ($n$=8) &   Two independent axes are layered on top of \\ 
&  each other with inappropriate scaling \citep{lo2022misinformed}.\\ 
Misrepresentation ($n$=7) &   The value labels provided do not match \\
& the visual
encoding \citep{lo2022misinformed}.\\ 
Cherry picking ($n$=6) &  Selecting only a subset of data to display, which  \\ 
&  can be misleading  if one is asked to infer something \\
&  about the whole set of data \citep{10.1145/3544548.3581406}. \\
Misleading annotations ($n$=5) &  Annotations that contradict or make it harder \\ 
&  to read  the visualization \citep{10.1145/3544548.3581406}. \\
Area encoding ($n$=5) &  Linearly encoding the values  as areas    \\
& leads the readers to consistently  \\
& underestimate the values \citep{lo2022misinformed}. \\
Concealed uncertainty ($n$=3) &   Not displaying uncertainty in visualizations may   \\
& misrepresent the certainty  in  the underlying data.  \\
&  In the case of  prediction  making, this can misguide   \\ 
& the viewers to falsely overconfident conclusions \citep{10.1145/3544548.3581406}. \\
Missing normalization ($n$=3) &  Displaying unnormalized data in absolute quantity  \\
&  when normalized  data in relative quantity  \\
& is of interest \citep{10.1145/3544548.3581406}.\\
Inappropriate use of pie chart ($n$=1) &  When a pie chart is used for non-part-to-whole data, \\
&  it creates confusion for the
audience, who may \\
&  misinterpret the significance of a given section \citep{lo2022misinformed}.\\
Missing data ($n$=1) &  A visual representation implies data exist but  \\
& the data is actually missing \citep{10.1145/3544548.3581406}.\\
Overplotting ($n$=1) &  Displaying too many things on a plot can \\
& obscure parts of the data \citep{10.1145/3544548.3581406}.\\

\botrule
\end{tabular}
\end{table}

\newpage
\section{Impact of choices order in MCQs}\label{secA2}

\begin{table}[t]
\caption{Mean accuracy and standard deviation (\%) when shuffling MCQ choices with three different random seeds.}\label{tab2}
\begin{tabular}{@{}lll@{}}
\toprule
Model & Misleading visualizations & Non-misleading visualizations\\
\midrule
llava-7B         & 24.48 $\pm$ 0.70 & 44.35 $\pm$ 2.79 \\
llava-13B        & 24.47 $\pm$ 2.42 & 37.36 $\pm$ 4.06 \\
qwen2vl-2B       & 24.71 $\pm$ 3.85 &38.44 $\pm$ 1.23 \\
tinychart-3B         & 22.15 $\pm$ 1.76 & 22.04 $\pm$ 1.23 \\
gpt4 & 30.54 $\pm$ 2.14 & 47.58 $\pm$ 2.91\\
chartinstruction-13B & 27.97 $\pm$ 2.52 & 34.14 $\pm$ 2.33 \\
internvl2.5-2B   & 19.58 $\pm$ 3.21 & 43.28 $\pm$ 1.23 \\
chartgemma-3B      & 22.61 $\pm$ 2.82 & 26.61 $\pm$ 2.91 \\
ovis1.6-9B       & 23.55 $\pm$ 0.40 & 52.15 $\pm$ 2.83 \\
qwen2vl-7B       & 20.75 $\pm$ 2.25 & 48.93 $\pm$ 3.26 \\
internvl2.5-4B   & 19.35 $\pm$ 2.25 & 46.50 $\pm$ 2.46 \\
internvl2.5-8B   & 22.61 $\pm$ 1.07& 55.38 $\pm$ 1.68 \\
ovis1.6-27B      & 28.44 $\pm$ 0.40 & 53.76 $\pm$ 1.23 \\
gpt4o              & 37.76 $\pm$ 0.00 & 59.95 $\pm$ 1.86 \\
internvl2.5-26B  & 20.05 $\pm$ 2.46 & 55.38 $\pm$ 1.86 \\
internvl2.5-38B & 32.64 $\pm$ 1.07 & 62.63 $\pm$ 0.46\\
\bottomrule
\end{tabular}
\end{table}


\section{P-values of significance tests}\label{secA3}




\begin{table}[ht]
\caption{P-values for the McNemar test assessing the significance of the correction methods. Significant results (p$\leq$0.05) are marked in bold.}\label{tab3}%
\begin{tabular}{@{}llll@{}}
\toprule
Method & Model & Misleading p-value & Non-misleading p-value \\ \midrule
 & qwen2vl    & 1.00 & 1.00 \\
Misleader warning & ovis1.6    & 0.17 & 0.25 \\ 
 & internvl2.5 & 1.00 & 1.00 \\
 \midrule
             & qwen2vl    & 0.58 & 0.63 \\
Axes & ovis1.6    & 0.38 & 0.08 \\ 
 & internvl2.5 & 0.65 & 0.10 \\
 \midrule
             & qwen2vl    & 0.21 & 1.00 \\
Table & ovis1.6    & 0.40 & \textbf{0.04} \\ 
 & internvl2.5 & 0.80 & 0.66 \\
\midrule
        & qwen2vl    & 0.17 & 1.00 \\
Table + Axes & ovis1.6    & 1.00 & 0.09 \\ 
 & internvl2.5 & 1.00 & 1.00 \\

\midrule       & qwen2vl    & \textbf{1e-4} & 0.14 \\
Table-based QA  & ovis1.6    & \textbf{3e-3} & 0.07 \\ 
& internvl2.5 & \textbf{5e-5} & 0.34 \\

\midrule
 & qwen2vl & \textbf{0.01} & 0.45 \\
 Redrawn visualization & ovis1.6    & \textbf{2e-4} & 0.45 \\ 
  & internvl2.5 & 0.68 & 0.33 \\
 \bottomrule
\end{tabular}
\end{table}






% An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%
\newpage
\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
