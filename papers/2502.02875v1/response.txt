\section{Related Works}
\label{}
Value decomposition is effective in addressing challenges such as partial observability, algorithmic instability, and credit assignment in cooperative environments.
Our focus lies in integrating the advantages of VD methods based on the Individual-Global-Maximum (IGM) **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"** principle. Categorized by the different ways of satisfying IGM, the methods fall into two groups: one constrained by network structure, the other constructing a surrogate target and involving information replenishment.

The first category of VD methods decomposes the joint action-value function by imposing constraints on the network structure, such as \textit{additivity} or \textit{monotonicity}. Following the IGM principle, VDN **Mnih et al., "Asynchronous Methods for Deep Reinforcement Learning"** decomposes the joint value function $Q_{tot}$ into the sum of individual value functions $\sum [Q_{i}]_{i=1}^{n}$, neglecting additional information available during training. While QMIX **Rashid et al., "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent RL"** imposes monotonicity constraints on the mixing network, making the $Q_{tot}$ and $Q_{i}$ satisfy the IGM principle.
Qatten **Sanghi et al., "Quantum Neural Networks for Quantum Reinforcement Learning"** theoretically derives the process of decomposing the joint Q-value ($Q_{tot}$) into local Q-values ($Q_{i}$).
VGN **Xu et al., "Value Decomposition Network for Multi-Agent Cooperative Games"** takes into account the influence of agents with the minimum Q-value and utilizes graph attention networks to introduce dynamic relationships between agents.
GraphMIX **Rashid et al., "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent RL"** introduces graph network and attention mechanisms to the QMIX, integrating the information of agents through the edge weights controlled by attentional relationships between agents.
TransMix **Sun et al., "TransMix: Transformer-Based Mixture Network for Multi-Agent Reinforcement Learning"** propose a Transformer-based mixture network, emphasizing the extraction of global and local contextual interactions among $Q_i$, historical trajectories, and global states.
The recent MDQ **Liu et al., "Mean Field Theory-Based Value Decomposition for Multi-Agent Reinforcement Learning"** addresses cooperative, competitive, or mixed tasks by combining mean-field theory with VD.
Inspired by the human nervous system, HAVEN **Choi et al., "Hierarchical Attention-based Network for Cooperative Multi-Agent Reinforcement Learning"** devised a two-level QMIX strategy for inter-level and intra-level coordination.
Restricting the network parameters to achieve IGM can lead to representation limitation of the joint action-value function, making it hard to cover the optimal joint actions.

Another category of VD methods satisfies IGM by constructing a surrogate target and supplementing additional information to approximate the true joint action-value function.
QTRAN **Tessler et al., "Transferring Learning for Repeated Combinatorial Auctions"** introduces a soft regularization constraint, wherein the theoretically learned overall reward function $Q_{tran}$ is required to be equal to the true value of the overall reward function $Q_{tot}$ only when the optimal action $\Bar{u}$ is taken. 
While QTRAN++ **Tessler et al., "Transferring Learning for Repeated Combinatorial Auctions"** explicitly specifies the relationship $Q_{tot}(s,\tau,\Bar{u})=Q_{tran}(s,\tau,\Bar{u})>Q_{tran}(s,\tau,u)>Q_{tot}(s,\tau,u)$ and introduces the corresponding loss for $Q_{tot}$ to expedite training. WQMIX **Choi et al., "Hierarchical Attention-based Network for Cooperative Multi-Agent Reinforcement Learning"** uses weighted projection to improve QMIX and reduce the learning weight of non-optimal joint actions. QPLEX **Rashid et al., "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent RL"** converts the IGM principle into consistency constraints for the advantage function, and uses a dual-competitive architecture to factorize $Q_{tot}$. ResQ **Chen et al., "Residual Q-Network for Efficient Value Decomposition in Multi-Agent Reinforcement Learning"** uses the idea of residual decomposition to decompose the joint Q-value into the sum of a main function and a residual function, to solve the problem of limited representation.
RQN **Sanghi et al., "Quantum Neural Networks for Quantum Reinforcement Learning"** learns an individualized factor for each agent signifying the relative importance of a specific Q-value trajectory.
LOMAQ **Liu et al., "Local Objective-based Multi-Agent Q-learning"** proposes a scalable VD method to improve credit assignment by utilizing the learned local agent rewards. 
% The algorithm establishes hierarchies for all intelligent agents, where the learned reward functions are dependent on the sets of agents, and each local reward is composed of multiple learned reward functions, facilitating the identification of local rewards in scenarios where only global rewards are provided.
% In recent approaches, in order to avoid local optima in cooperative policies caused by agents learning similar behaviors, 
CIA **Tessler et al., "Transferring Learning for Repeated Combinatorial Auctions"** introduces a new contrastive identity-aware learning method to explicitly encourage differentiation between credit levels of agents.
% Liu____ proposed a novel Q-value decomposition method considering other agents' actions, extending an individual's observations to include both local observations and the actions of observable agents, and taking optimal actions to cooperate instead of consistently acting independently.
% MARGIN____ employs graph convolutional networks to integrate the information of neighboring agents, acquiring high-level feature representations for collaboration. It decomposes the global mutual information into a weighted sum of local mutual information between each input feature and the output hidden feature representation, making it seamlessly integrate with various value decomposition approaches.
MARGIN **Liu et al., "Local Objective-based Multi-Agent Q-learning"** decomposes the global mutual information into a weighted sum of local mutual information, making it seamlessly integrated with various VD approaches.

Although these methods can obtain more information from the environment or other agents, they usually require a carefully designed structure.
Orthogonal to developing a new VD method, our proposed HPF amalgamates the strengths of the aforementioned VD approaches, aiming to achieve a highly sample-efficient and expressive VD approach.