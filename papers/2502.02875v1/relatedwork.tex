\section{Related Works}
\label{}
Value decomposition is effective in addressing challenges such as partial observability, algorithmic instability, and credit assignment in cooperative environments.
Our focus lies in integrating the advantages of VD methods based on the Individual-Global-Maximum (IGM)~\cite{son2019qtran} principle. Categorized by the different ways of satisfying IGM, the methods fall into two groups: one constrained by network structure, the other constructing a surrogate target and involving information replenishment.

The first category of VD methods decomposes the joint action-value function by imposing constraints on the network structure, such as \textit{additivity} or \textit{monotonicity}. Following the IGM principle, VDN~\cite{sunehag2017value} decomposes the joint value function $Q_{tot}$ into the sum of individual value functions $\sum [Q_{i}]_{i=1}^{n}$, neglecting additional information available during training. While QMIX~\cite{rashid_qmix_2018} imposes monotonicity constraints on the mixing network, making the $Q_{tot}$ and $Q_{i}$ satisfy the IGM principle.
Qatten~\cite{yang2020qatten} theoretically derives the process of decomposing the joint Q-value ($Q_{tot}$) into local Q-values ($Q_{i}$).
VGN~\cite{wei2022vgn} takes into account the influence of agents with the minimum Q-value and utilizes graph attention networks to introduce dynamic relationships between agents.
GraphMIX~\cite{naderializadeh2020graph} introduces graph network and attention mechanisms to the QMIX, integrating the information of agents through the edge weights controlled by attentional relationships between agents.
TransMix~\cite{khan2022transformer} propose a Transformer-based mixture network, emphasizing the extraction of global and local contextual interactions among $Q_i$, historical trajectories, and global states.
The recent MDQ~\cite{ding2023multi} addresses cooperative, competitive, or mixed tasks by combining mean-field theory with VD.
Inspired by the human nervous system, HAVEN~\cite{xu2023haven} devised a two-level QMIX strategy for inter-level and intra-level coordination.
Restricting the network parameters to achieve IGM can lead to representation limitation of the joint action-value function, making it hard to cover the optimal joint actions.

Another category of VD methods satisfies IGM by constructing a surrogate target and supplementing additional information to approximate the true joint action-value function.
QTRAN~\cite{son2019qtran} introduces a soft regularization constraint, wherein the theoretically learned overall reward function $Q_{tran}$ is required to be equal to the true value of the overall reward function $Q_{tot}$ only when the optimal action $\Bar{u}$ is taken. 
While QTRAN++~\cite{son2020qtran++} explicitly specifies the relationship $Q_{tot}(s,\tau,\Bar{u})=Q_{tran}(s,\tau,\Bar{u})>Q_{tran}(s,\tau,u)>Q_{tot}(s,\tau,u)$ and introduces the corresponding loss for $Q_{tot}$ to expedite training. WQMIX~\cite{rashid2020weighted} uses weighted projection to improve QMIX and reduce the learning weight of non-optimal joint actions. QPLEX~\cite{wang2021qplex} converts the IGM principle into consistency constraints for the advantage function, and uses a dual-competitive architecture to factorize $Q_{tot}$. ResQ~\cite{shen2022resq} uses the idea of residual decomposition to decompose the joint Q-value into the sum of a main function and a residual function, to solve the problem of limited representation.
RQN~\cite{pina2022residual} learns an individualized factor for each agent signifying the relative importance of a specific Q-value trajectory.
LOMAQ~\cite{zohar2022locality} proposes a scalable VD method to improve credit assignment by utilizing the learned local agent rewards. 
% The algorithm establishes hierarchies for all intelligent agents, where the learned reward functions are dependent on the sets of agents, and each local reward is composed of multiple learned reward functions, facilitating the identification of local rewards in scenarios where only global rewards are provided.
% In recent approaches, in order to avoid local optima in cooperative policies caused by agents learning similar behaviors, 
CIA~\cite{liu2023contrastive} introduces a new contrastive identity-aware learning method to explicitly encourage differentiation between credit levels of agents.
% Liu~\cite{liu2023learning} proposed a novel Q-value decomposition method considering other agents' actions, extending an individual's observations to include both local observations and the actions of observable agents, and taking optimal actions to cooperate instead of consistently acting independently.
% MARGIN~\cite{ding2023multiagent} employs graph convolutional networks to integrate the information of neighboring agents, acquiring high-level feature representations for collaboration. It decomposes the global mutual information into a weighted sum of local mutual information between each input feature and the output hidden feature representation, making it seamlessly integrate with various value decomposition approaches.
MARGIN~\cite{ding2023multiagent} decomposes the global mutual information into a weighted sum of local mutual information, making it seamlessly integrated with various VD approaches.

Although these methods can obtain more information from the environment or other agents, they usually require a carefully designed structure.
Orthogonal to developing a new VD method, our proposed HPF amalgamates the strengths of the aforementioned VD approaches, aiming to achieve a highly sample-efficient and expressive VD approach.