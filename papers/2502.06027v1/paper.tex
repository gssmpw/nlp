\def\year{2021}\relax
\documentclass[a4]{article}
\usepackage[a4paper,left=0.5in,right=0.5in,top=0.5in,bottom=0.5in,%
footskip=.25in]{geometry}
\usepackage{titling}
\usepackage{helvet}

\usepackage[hyphens]{url}
\usepackage{graphicx}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{caption}
\frenchspacing
\usepackage[superscript]{cite}
\usepackage{enumitem}%
\usepackage[justification=centering]{caption}

\usepackage{stfloats}
\usepackage[hang,flushmargin, norule]{footmisc}
\usepackage{marvosym}

\usepackage[table,x11names,dvipsnames]{xcolor}

\setcounter{secnumdepth}{2}

\usepackage{titlesec}
\usepackage{multicol}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algcompatible}
\usepackage{algorithm,algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs, multirow, soul}
\usepackage[flushleft]{threeparttable}
\usepackage{xspace, mathtools, comment}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[resetlabels,labeled]{multibib}
\usepackage{etoolbox}


\usepackage{multicol}
\usepackage{lipsum}

\usepackage{xr}
\externaldocument{supp}

%%%%%%%%%%%%%%%%%%%%%%% Style %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titlespacing*{\section}{0pt}{1\baselineskip}{0.1\baselineskip}
\titlespacing*{\subsection}{0pt}{1\baselineskip}{0.1\baselineskip}
\titlespacing*{\subsubsection}{0pt}{1\baselineskip}{0.1\baselineskip}

\titleformat*{\section}{\normalfont\sffamily\large\bfseries}
\titleformat*{\subsection}{\normalfont\sffamily\normalsize\bfseries}
\titleformat*{\subsubsection}{\normalfont\sffamily\small\bfseries}

\algnewcommand{\LineComment}[1]{\Statex \(\triangleright\) #1}
\algnewcommand{\ShortLineComment}[1]{\Statex \hspace{1.8em}\(\triangleright\) #1}
\algnewcommand{\ShortShortLineComment}[1]{\Statex \hspace{3.1em}\(\triangleright\) #1}


\DeclareCaptionLabelSeparator{bar}{\space\textbar\space}
\renewcommand{\figurename}{Fig.}
\captionsetup{justification=raggedright, labelfont={scriptsize, bf}, font=scriptsize, labelsep=bar}

\makeatletter
\renewcommand\@biblabel[1]{#1.}
\makeatother

\makeatletter
\renewenvironment{table*}%
{\renewcommand\familydefault\sfdefault
	\@float{table}}
{\end@float}
\makeatother

\makeatletter
\renewcommand{\scriptsize}{\@setfontsize\scriptsize{7}{7}}
\renewcommand{\footnotesize}{\@setfontsize\footnotesize{8}{9}}
\makeatother

\patchcmd{\thebibliography}
{\settowidth}
{\setlength{\parsep}{0pt}\setlength{\itemsep}{0pt plus 0pt}\settowidth}
{}{}
\apptocmd{\thebibliography}
{\small}
{}{}

\renewcommand{\footnoterule}{%
	\kern -3pt
	\hrule width 0.5\textwidth height 0.3pt
	\kern 2pt
}

\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother

%\newfloat{footnote}{hb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Generating 3D Binding Molecules Using Shape-Conditioned Diffusion Models with Guidance}
\date{\vspace{-5ex}}

\author{
	Ziqi Chen\textsuperscript{\rm 1}, 
	Bo Peng\textsuperscript{\rm 1}, 
	Tianhua Zhai\textsuperscript{\rm 2},
	Daniel Adu-Ampratwum\textsuperscript{\rm 3},
	Xia Ning\textsuperscript{\rm 1,3,4,5 \Letter}
}
\newcommand{\Address}{
	\textsuperscript{\rm 1}Computer Science and Engineering, The Ohio State University, Columbus, OH 43210.
	\textsuperscript{\rm 2}Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA 19104.
	\textsuperscript{\rm 3}Medicinal Chemistry and Pharmacognosy, The Ohio State University, Columbus, OH 43210.
	\textsuperscript{\rm 4}Biomedical Informatics, The Ohio State University, Columbus, OH 43210.
	\textsuperscript{\rm 5}Translational Data Analytics Institute, The Ohio State University, Columbus, OH 43210.
	\textsuperscript{\Letter}ning.104@osu.edu
}
\newcommand{\myabstract}[2][1]{%
	\renewcommand\maketitlehookd{
		%
		\Address
		\vspace{10pt}
		\mbox{}\medskip\par
		\centering
		\begin{minipage}{#1\textwidth}
			\textbf{
			{\fontfamily{phv}\selectfont
				#2
				}
			}
		\end{minipage}
	}
}

\input{define}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\myabstract{Drug development is a critical but notoriously resource- and time-consuming process.
%
In this manuscript, we develop a novel generative artificial intelligence (genAI) method \method to facilitate drug development.
%
\method generates 3D binding molecules based on the shapes of known ligands.
%
\method encapsulates geometric details of ligand shapes within pre-trained, expressive shape embeddings and then generates new binding molecules through a diffusion model.  
%
\method further modifies the generated 3D structures iteratively via shape guidance to better resemble the ligand shapes.
%
It also tailors the generated molecules toward optimal binding affinities under the guidance of protein pockets.
%
Here, we show that \method outperforms the state-of-the-art methods on benchmark datasets. % in generating binding molecules.
%
When generating binding molecules resembling ligand shapes, \method with shape guidance achieves a success rate 61.4\%, substantially outperforming the best baseline (11.2\%), meanwhile producing molecules with novel molecular graph structures.
%
\method with pocket guidance also outperforms the best baseline in binding affinities by 13.2\%, and even by 17.7\% when combined with shape guidance.
%
Case studies for two critical drug targets demonstrate very favorable physicochemical and pharmacokinetic properties of 
the generated molecules, thus,  the potential of \method in developing promising drug candidates.}
%\xia{need more details about the results}


\maketitle


\vspace{-10pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Drug development is a critical but notoriously resource- and time-consuming process~\cite{Sun2022}.
%
It typically takes 10-15 years and \$1 to \$1.6 billion to fully develop a successful drug~\cite{Wouters2020}.
%
To expedite the process and improve cost efficiency,  tremendous research efforts have been dedicated to developing computational methods to facilitate drug development~\cite{Yu2016}.
%
Existing computational methods to design potential drug candidates
could be categorized into ligand-based drug design (LBDD)~\cite{Acharya2011} and structure-based drug design (SBDD)~\cite{Anderson2003}, which search over molecule libraries to 
identify those resembling known ligands or binding to known binding sites of protein targets, respectively. 
%
Though promising, the opportunistic trial-and-error paradigm underpinning LBDD and SBDD 
is often confined by the limited scale of molecule libraries and cannot ensure optimal precision design~\cite{Gimeno2019}. 
%
Thus, the outcomes are highly subjective to the knowledge and experience of the domain experts conducting the experiments, which also limits the scalability and automation of rapid drug design 
for new protein targets. 
%
Recently, generative artificial intelligence (genAI) methods, such as variational autoencoders~\cite{kingma2013auto},  diffusion~\cite{song2021denoising}, and ChatGPT~\cite{OpenAI2023}, 
have emerged as groundbreaking computational tools for 
many applications~\cite{Yu2024}, including drug design~\cite{jin18jtvae,schneuing2022structure,liu2024conversational}.
%
Instead of searching for drug candidates, genAI methods could directly generate molecules 
satisfying prescribed properties (e.g., lipophilicity, druglikeness), 
through learning the underlying chemical knowledge carried by vast molecule datasets, 
and making autonomous decisions in constructing new molecules~\cite{jin18jtvae,schneuing2022structure} (e.g., molecular graphs, 3D structures). 
%
%
The powerful generative capabilities of genAI demonstrate significant promise in fundamentally transforming the traditional drug development process into a more focused, accurate, swift, and sustainable alternative.

%
In this manuscript, we introduce \method, a novel genAI method to generate molecules in 3D that 
effectively bind to %a given binding pocket of 
a protein target and have realistic structures (e.g., correct bond angles and bond lengths).
%
Motivated by LBDD, \method generates novel binding molecules based on the shapes of known ligands, following the principle that molecules with similar shapes tend to have similar binding activities~\cite{Bostroem2006, Acharya2011}.
%
\method encapsulates the geometric details of ligand shapes within pre-trained, expressive shape embeddings, and 
generates new binding molecules, including their atom types and atom positions %, and bonds connecting the atoms, 
through diffusion~\cite{song2021denoising}. 
%
During the iterative diffusion process, \method leverages a novel molecule graph representation learning approach and integrates ligand shape embeddings in generating and refining the atom types and atom positions, and thus, a new molecule and its 3D 
structures. 
%
To better resemble the known ligand shapes, \method further modifies the generated 3D structures iteratively under the guidance 
of the ligand shapes. 
%
Inspired by SBDD, in addition to ligands, \method can also leverage the geometric information of protein binding pockets
and tailor the generated molecules toward optimal binding affinities under the guidance of binding pockets. 


Our comprehensive experiments demonstrate that \method achieves superior performance in generating molecules with highly similar shapes to ligands, compared to state-of-the-art shape-conditioned molecule generation (SMG) methods.
%
Notably, \method achieves a 28.4\% success rate in generating molecules that closely resemble ligand shapes and have novel graph structures, substantially outperforming the 11.2\% success rate of the best SMG method.
%
Moreover, incorporating shape guidance further boosts the performance of \method to a remarkable 61.4\% success rate, while generating realistic 3D molecules.
%
This highlights the effectiveness of \method's pre-trained shape embeddings to capture geometric details of ligand shapes and the ability of its customized diffusion model in generating realistic and novel binding molecules.
%
In addition, by utilizing geometric information from protein binding pockets, \method with pocket guidance outperforms the best pocket-conditioned molecule generation (PMG) method by 13.2\% improvement in binding affinities of generated molecules.
%
When both pocket and shape guidance are incorporated, the improvement reaches 17.7\%.
%
Case studies with extensive \emph{in silico} analyses for two important drug targets, cyclin-dependent kinase 6 (CDK6) 
that is highly associated with multiple cancers such as lymphoma and leukemia, 
and neprilysin (NEP) that is highly associated with Alzheimer's disease, %illustrate the potential of \method with shape and pocket guidance in generating promising drug candidates for these life-threatening diseases.
% 
demonstrate that \method effectively generates drug-like molecules specifically for these targets. 
%
The two studied generated molecules for CDK6 show binding affinities (Vina scores)~\cite{Eberhardt2021} of -6.817 kcal/mol and -6.970 kcal/mol, better than that of the known CDK6 ligand (0.736 kcal/mol); 
and the studied generated molecule for NEP also achieves a superior Vina score of -11.953 kcal/mol compared to the known 
NEP ligand (-9.399 kcal/mol).
%
These molecules also have favorable drug-like properties, with high QED values~\cite{Bickerton2012} close to or above 0.8, low toxicity scores ranging from 0.000 to 0.236, and compliance with Lipinski's rule of five~\cite{Lipinski1997}.
%
Notably, their profiles for absorption, distribution, metabolism, excretion, and toxicity (ADMET) are comparable to those of FDA-approved drugs.
%
These results further highlight the potential of \method in advancing drug development.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A variety of deep generative models have been developed to generate molecules using various molecule representations, 
including generating SMILES string representations~\cite{GmezBombarelli2018}, or 2D molecular graph representations~\cite{jin18jtvae,Chen2021modof}.
%
{However, these representations fall short in capturing the 3D structures of molecules, which are critical for understanding their biological activities and certain properties.}
%
Recent efforts have been dedicated to the generation of 3D molecules. 
%
For example, Hoogeboom \etal~\cite{hoogeboom22diff} developed an equivariant diffusion model 
in which an equivariant network is employed to
jointly predict both the positions and features of all atoms.
%
In 3D molecule generation, two types of methods have been developed.
%
Motivated by LBDD, the first type of methods, referred to as shape-conditioned molecule generation (SMG) methods, generates molecules with similar shapes to condition molecules (e.g., ligands).
% 
The second type of methods, referred to as pocket-conditioned molecule generation (PMG) methods, is motivated by SBDD and generates binding molecules to a target protein pocket.
%



%===================================================================
\subsection*{SMG Methods}
%===================================================================

%In shape-conditioned molecule generation, previous work~\cite{long2022zero, adams2023equivariant} has been focused on generating molecules with similar shapes to condition molecules.
%
%This effort is motivated by the principle of LBDD~\cite{Acharya2011} that molecules with similar shapes tend to have similar binding activities.
%
%Consequently, when using the shapes of known ligands as the condition, shape-conditioned molecule generation methods are capable of identifying promising drug candidates that have similar binding activities to known ligands.
%
Previous SMG methods~\cite{long2022zero, adams2023equivariant} generally leverage shapes as conditions and use generative models such as variational autoencoders (VAE)~\cite{kingma2013auto} to generate potentially binding molecules.  
%
Among SMG methods, Adams and Coley~\cite{adams2023equivariant} developed \squid, which consists of a fragment-based generative model and a rotatable-bond scoring model.
%
The former generates molecules using VAE and sequentially decodes fragments based on the shapes of condition molecules (e.g., ligands), while the latter adjusts the angles of rotatable bonds between fragments to adapt to the condition shapes.
%
%It also neglects the bonding geometries in real molecules and generates molecules with fixed bond lengths and bond angles.
%
Long \etal~\cite{long2022zero} developed an encoder-decoder framework, referred to as $\mathsf{Shape2Mol}$, which first encodes 3D shapes of molecules into latent embeddings and then generates fragments sequentially based on these embeddings to build molecules.
%
{In our preliminary work~\cite{Chen2023ShapeMol}, we also demonstrated the potential of diffusion models for generating binding molecules conditioned on shapes. 
%
By improving the shape-conditioned molecule prediction module (Section ``Shape-conditioned Molecule Prediction''), we have significantly enhanced the performance of \method.}


%Our model \method also generates molecules with similar shapes to condition molecules.
%
It is worth noting that \method is fundamentally different from \squid.
%
%As will be shown in Section ``Methods'', \method directly arranges atoms in the 3D space to closely resemble the shapes of condition molecules using a diffusion model.
%
\squid, as a fragment-based method, generates molecules by sequentially adding fragments. 
%
When predicting the next fragments, however, \squid fails to consider the effects of their various poses, and thus, could lead to inaccurate fragment predictions.
%
Due to the sequential nature, the prediction errors will be cumulated and could substantially degrade the generation performance.
%
Different from \squid, \method generates molecules by directly arranging atoms in the 3D space using diffusion models.
%
This design explicitly considers the influence of varying 3D atom positions in the generation process, leading to effective generations. 
%
%\ziqi{\squid fails to consider the optimal postures of fragments in 3D space when generating }
%In contrast, \squid sequentially predicts fragments to construct molecules in an autoregressive manner.
%
%It then integrates these fragments into molecules by adjusting their angles to previously added fragments. %using a rotatable-bond scoring model.
%
%However, this two-step approach might 
%\bo{\st{lead {\squid} to overlook}} 
%\bo{
%render \squid struggle to capture
%}
%the structural implications of each fragment prediction on the generated molecule.
%
%{This issue}
%\bo{\st{Such oversight}} 
%could result in a misalignment between the generated molecule and the shape condition, thereby decreasing the effectiveness of \squid in generating molecules that closely follow the shape condition.
%
%Different from \squid, \method enables better shape following by directly arranging atoms.}
%\hl{when predicting fragments to construct molecules, {\squid} does not consider the alignment of these fragments with the shape condition or their resultant structures.}
%\bo{
%@Ziqi rephrase. It seems this sentence has two points (fragments and XXX). It could be better to present them in two sentences. And the other point "XXX" is not clear.
%}
%resulting structures with the predicted fragments and their alignment with the shape condition.
%
%Instead, \squid relies on an additional rotatable-bond scoring model to determine the resultant structures by predicting angles of predicted fragments to \hl{maximize the shape following}. %3D structures for better shape following by adjusting the angles of the predicted fragments.
%
%This approach could \hl{degrade {\squid's} performance in shape following} \bo{@Ziqi, you just said to maximize shape following... You may want to rephrase the sentence before}, especially when fragments have distinct shapes from the condition. %, %and relies on a rotatable-bond scoring model to ensure the shape following by adjusting angles for connecting the added fragments, 
%which potentially degrades the shape following of generated molecules. %and relies on a rotatable-bond scoring model to adjust the angles for connecting the added fragments.
%
%\bo{\st{As a result, {\method} can enable better shape following of generated molecules than {\squid}.}}
%This strategy used by \squid can degrade the shape following of generated molecules when the generated fragments cannot fit into the shape condition.  
%
%relies on a rotatable-bond scoring model to control the 3D structures of generated molecules.
%
%The strategy adopted by \squid could degrade the shape following of generated molecules when the generated fragments are far from the shape condition and cannot be connected to resemble the shape condition.  
%
%\ziqi{In addition, \squid suffers from the limited diversity of generated molecules by using only fragments in the predefined library, 
%while \method enhances the diversity by allowing for the generation of any fragments. }
%
%\bo{
In addition, by using only fragments in a predefined library, \squid could struggle to generate diverse molecules, 
while \method ensures superior diversity by allowing for the generation of any fragments.
%}
%{{\method} also enables the discovery of new fragments, while {\squid} is restricted to generating molecules with only fragments in the library.}
%
%\bo{
%@Ziqi Is this still true if atoms are used as fragments? 
%}
%
%In contrast, \squid is restricted to generate molecules with fragments in the library, potentially degrading the novelty and diversity of generated molecules.
%
%In addition, %\method considers the {flexible bonding geometries}
%
%\bo{
\method also captures the flexibility of bonding geometries
%@Ziqi is bonding geometry a widely used term? ziqi: yes
%}
%\bo{
%@Ziqi do not look right to me
%}
in real 3D molecules by generating molecules with flexible bond lengths and angles.
%
However, \squid can only generate molecules with fixed bond lengths and angles, leading to the discrepancy in 3D structures between the %\bo{\st{their}} 
generated molecules and real molecules. 

\method %\bo{@Ziqi think of a better name. It should not be \method}
is also different from $\mathsf{Shape2Mol}$. 
%
\method is specifically designed to be equivariant under any rotations and translations of the shape condition, allowing for better sampling efficiency~\cite{Jonas20a}.
%
Conversely, $\mathsf{Shape2Mol}$ is not equivariant, and thus, suffers from limited training efficiency. %requires resource-intensive training to learn knowledge shared across different rotations or translations. 
%
In addition, different from \method, $\mathsf{Shape2Mol}$ is a fragment-based approach and could suffer from the same issues as discussed above for \squid. % limited to the fragments in the {pre-defined} library.
%

%
%Papadopoulos \etal~\cite{Papadopoulos2021} developed a reinforcement learning method 
%to generate SMILES strings of molecules that are similar to known antagonists of DRD2 receptors 
%in 3D shapes and pharmacophores.
%molecules similar to known antagonists of the DRD2 receptor in 3D shape and pharmacophore similarity.} \xia{grammar!}
%
%Imrie \etal~\cite{Imrie2021} generated 2D molecular graphs conditioned on 3D pharmacophores using a graph-based autoencoder.
%
%However, there is limited work that generates 3D molecule structures conditioned on 3D shapes.
%
% the angles of rotatable bonds between fragments to maximize shape similarity.
%\squid uses a variational autoencoder to jointly encode the 3D shape and the 2D molecular graph of an input molecule
%into a shape latent embedding. % and a posterior distribution of molecule embeddings.
%
%It then decodes molecules from the shape latent embeddings and the sampled molecule embeddings, by sequentially attaching fragments with fixed bond lengths and angles.
%
%The angles of rotatable bonds between fragments are adjusted by an independent rotatable bond scoring network.  
%to maximize the 
%%3D shape similarity.
%
%One limitation of \squid is that it only generates artificial molecules with fixed local bond lengths and angles,
%and ignores the distorted bonding geometries in real molecules.
%
 
 %===================================================================
\subsection*{PMG Methods}
%===================================================================

%\xia{this subsection needs to be extended: include more related work. Need to explicitly point out 
%the current challenges and issues, in response to the innovations you will claim about your method. }

For PMG, previous work~\cite{luo2021sbdd,peng22pocket2mol,guan2023targetdiff,guan2023decompdiff} has been focused on directly utilizing protein pockets as a condition and generating molecules binding towards these pockets.
%
These methods can be grouped into three categories: VAE-based, autoregressive model-based, and diffusion model-based.
%
Among VAE-based methods, Ragoza \etal~\cite{ragoza2022chemsci} developed a conditional VAE model to generate atomic density grids based on the density grids of protein pockets.
%
The generated atomic density grids are then converted to molecules.
%
Several autoregressive models~\cite{luo2021sbdd,peng22pocket2mol,liu2022} also have been developed to generate binding molecules by sequentially adding atoms into the 3D space conditioned on 
protein pocket atoms. 
%
Particularly, Luo \etal~\cite{luo2021sbdd} developed an autoregressive model \AR to estimate the probability density of atoms' occurrences in the 3D space conditioned on protein pockets.
%
\AR sequentially adds atoms based on these estimations to construct molecules. 
%
Peng \etal~\cite{peng22pocket2mol} improved \AR into \pockettwomol by incorporating a more efficient atom sampling strategy.
%
\pockettwomol determines the positions of newly added atoms by predicting their relative positions to previously added atoms.
%
Diffusion models are also very popular in PMG.
%
Guan \etal~\cite{guan2023targetdiff} developed a conditional diffusion model \targetdiff that generates molecules based on protein pockets by sequentially denoising both continuous atom coordinates and categorical atom types in noisy molecules. 
%
Guan \etal~\cite{guan2023decompdiff} further improved \targetdiff into \decompdiff by utilizing data-dependent prior distributions over molecular arms and scaffolds.
%
These priors are derived from either known ligands or protein pockets.

%\decompdiff derived the decomposed priors from either known ligands or pockets.
%
Though promising, %\bo{@Ziqi is this the right word? I only see this for drugs}, 
PMG methods require protein-ligand complex data for training.
%
However, such data is expensive and thus highly limited.
%
%Note that protein-ligand complexes are expensive and thus highly limited.
%
%\bo{\st{The limited availability of ligands in complexes}} %\bo{complexes} ligands\bo{?} 
{The sparse ground-truth binding ligands}
%
%\bo{\st{for training}} 
confine %\bo{\st{the ability of}} 
these methods in exploring a wide range of molecules with desired properties.  
%
In contrast, \method can learn from rich molecule data, improving its ability to generate effective and novel binding molecules.

%\hl{avoid using many "conditioned on"}
%encodes and includes a sequential fragment-based generative model based on v
%generates fragments with predefined local bond lengths and angles, 
%and a rotatable bond scoring network that adjusts bond angles of rotatable bonds to maximize 3D shape similarity.
% 
%However, a limitation of this framework is its ability to only generate artificial molecules with fixed local bond lengths and angles.
%
%\xia{need to have a summary here saying that shape-conditioned methods are still less developed overall. }


%===================================================================
%\subsection{Diffusion Models}
%===================================================================
\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem Definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\xia{may need to re-orient the idea if you use pocket shapes for adjustment. Need to discuss this.. }


\ziqi{In this manuscript, we focus on generating molecules that have the potential to serve as promising drug candidates for drug development.
%
To achieve this, we formulate two molecule generation problems, differentiated by whether structural data of the protein target binding pocket is available or not. 
%
Both problems leverage known molecules of interest (e.g., ligands) as condition molecules and are formulated to generate molecules that have similar shapes to condition molecules and thus potentially similar binding activities.
%
%Following the settings of LBDD and SBDD, given known molecules of interest as condition molecules (e.g., ligands), we consider two generation problems to generate molecules with similar shapes to condition molecules, without and with the structural data of the protein target binding pocket.
%
(1) The first problem focuses on settings similar to LBDD where the high-quality structural data of the protein target binding pocket is unavailable.
%
To be specific, following Adams and Coley {\cite{adams2023equivariant}}, given condition molecules, we aim to generate novel molecules with similar shapes to condition molecules. %these molecules potentially have similar binding activities to them.
%
When the conditions molecules have desired binding activities, the generated molecules with similar shapes to them could demonstrate comparable binding activities, thereby potentially making them as promising drug candidates. 
%
(2) The second problem is defined in settings similar to SBDD where high-quality structural data of the protein target binding pocket is available.
%
Given such structural data and the known ligand binding towards the protein target, we aim to generate molecules with similar shapes to the known ligand while ensuring the generated molecules to fit precisely within the protein target binding pocket.
%
This precise fitting can enable the generated molecules to align with the spatial constraint of the protein target binding pocket.
%
As a result, these molecules can interact more effectively with and bind more tightly to the protein target binding pocket, increasing their potential as effective drug candidates. }

%\hl{Following Adams and Coley {\cite{adams2023equivariant}}, we focus on the 3D molecule generation \textit{conditioned on} the shape of a given molecule (e.g., a ligand).}
%
%\bo{
%@Ziqi I do not think you can say this you need to give equal attention to both of the problems.
%}
%

\method aims to address both LBDD and SBDD. Toward this end, \method generates molecules conditioned on the shape of condition molecules (i.e., ligands), and leverages carefully designed shape guidance and pocket guidance to adapt to different problems.
% 
Specifically, for LBDD, following Adams and Coley {\cite{adams2023equivariant}}, \method aims %we aim 
to generate a new molecule \moly, conditioned on the 3D shape of a given molecule \molx,  
such that \ziqi{\moly shares a similar 3D shape with \molx but has a distinct and novel graph structure.}
%1) {\moly} is similar to {\molx} in their 3D shapes, measured by $\shapesim(\shape_x, \shape_y)$, 
%where \shape is the 3D shape of \mol; 
%and 
%2) {\moly} is dissimilar to {\molx} in their 2D molecular graph structures, measured by $\graphsim(\molx, \moly)$. 
%
%\bo{@Ziqi So \moly represents both the molecule and the graph structure. Is this type of notation a common practice?}
%
%\bo{
%@Ziqi it looks imbalanced to mention a lot on LBDD but not on SBDD.
%
%How about just say as discussed in related work. Both LBDD and SBDD are critical problems in drug discovery.
%}
%
%This conditional 3D molecule generation problem is motivated by the fact that in LBDD, %ligand-based drug design, 
%when protein target structures are not available, it is desired to find chemically diverse molecules that
%share similar shapes with known active ligands but have novel structures different from ligands.
%and novel molecules that share similar shapes and similar activities with known active ligands~\cite{Ripphausen2011}.
%\hl{share similar shapes and similar activities} \xia{share with what?}~\cite{Ripphausen2011}.
%
Such chemically diverse and novel molecules could expand the search space for promising drug candidates and potentially enhance the 
development of effective drugs.
%\hl{Therefore, if a method can generate 3D molecule structures with but diverse 2D graph structures, 
%it indicates its ability to construct a chemically diverse set of drug candidates. }
%
%\hl{Such a diverse set of candidates could broaden the search space and thus potentially facilitate the design of more effective drugs.} 
%\xia{in experiments, need to include case studies on ``chemical diversity" and ``novel molecules"}
%
On the other hand, for SBDD, {in which} both the protein target's structure, denoted as $\pocket$ and its corresponding ligand $\molx$ are {available}, \method aims to generate a new molecule \moly, based on \pocket and the 3D shape of \molx, such that \moly will bind effectively to \pocket. % also extend our study to structure-based drug design when a protein target's structure is available.
%
%Particularly, we focus on the generation of 3D molecules binding towards the protein target.
%
%Given the 3D structure of a protein-ligand complex with the binding pocket \pocket and the ligand \molx, we aim to generate a new molecule \moly, based on \pocket and the 3D shape of \molx, such that \moly will bind effectively to \pocket.
%
Molecules that effectively bind towards protein targets could serve as promising drug candidates to modulate protein functions and treat various diseases. %for further drug development.
\end{comment}

%
%\begin{comment}
%The shape similarity $\shapesim(\shape_x, \shape_y)$ is %\st{measured by} 
%calculated via the overlap volumes between two aligned molecules as in the literature~\cite{adams2023equivariant}.
%%
%Each generated molecule \moly is aligned with the conditional molecule \molx by the ShaEP tool~\cite{Vainio2009shaep}.
%%which calculates the overlap of the optimally aligned shapes $\shape_x$ and $\shape_y$.
%%
%The molecular graph similarity $\graphsim(\molx, \moly)$ is defined as the Tanimoto similarity \ziqi{over Morgan fingerprints
%between \molx and \moly} %\xia{between what?} 
%calculated by RDKit~\cite{rdkit}.%\xia{citation!}.
%\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Materials}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bo{@Ziqi test or condition molecule? Need to unify over the entire paepr}
%\ziqi{In this manuscript, we train and evaluate \method on an immense dataset with 3D molecules.
%%
%We compared \method with the state-of-the-art baselines on the capability of generating molecules with similar shapes to condition molecules yet novel structures.
%
%Moreover, we also evaluated the ability of \method to generalize in the domain of pocket-based molecule generation, despite not being explicitly trained on any protein-ligand complex dataset.
%%
%This ability is crucial, as methods with strong generalization can explore the chemical space beyond the constraints imposed by the limited availability of existing protein-ligand complex data and thus enable the discovery of better drug candidates.
%%
%To evaluate this ability, we used \method to generate molecules with similar shapes to known ligands in existing protein-ligand complexes.
%%
%We compared the binding affinities of generated molecules from \method with the state-of-the-art pocket-based molecule generation methods.
%}

{We evaluate the effectiveness of \method in both SMG and PMG.
%
For SMG, following the literature~\cite{adams2023equivariant}, we evaluate whether \method could generate realistic 3D molecules %following the shape condition such 
that have shapes similar to condition molecules;
for PMG, following the literature~\cite{luo2021sbdd,peng22pocket2mol,guan2023targetdiff}, we assess, given target protein pockets, whether \method could generate molecules with high binding affinities and realistic structures. }
%In this manuscript, we focus on 
%1) evaluating if \method could generate high-quality molecules following the shape condition; 
%2) investigating if {\method} could generate high-quality molecules binding towards TARGET PROTEIN POCKETs. %the task of generating molecules binding towards protein pockets.}
%
%\bo{
%@Ziqi I think the key here could be the pocket-based drug design.
%}
%
Particularly, for SMG, we evaluate \method and its variant with shape guidance (detailed in Section ``\method with Shape Guidance''), referred to as \methodwithsguide, against the state-of-the-art SMG baselines in terms of shape similarity, diversity, and realism of generated molecules.
%
For PMG, we compare both \method and \methodwithsguide with pocket guidance (detailed in Section ``\method with Pocket Guidance''), referred to as \methodwithpguide and \methodwithsandpguide, to state-of-the-art PMG baselines to investigate if \method can effectively generate realistic molecules binding towards protein targets.
%
Note that different from PMG baselines trained on sparse protein-ligand complex data, \method is capable of leveraging large-scale molecule data for better generation. %(details available in Section ``Experimental Setup''). %\hl{a general molecule dataset} \bo{@Ziqi what do you mean by general molecule dataset?}
%
%Note that protein-ligand complexes are notoriously scarce as collecting them could be prohibitively expensive~\cite{XXX}.
%
%Enabling knowledge learned from a \hl{general molecule dataset to be applied for the generation of binding molecules} 
%\bo{
%@Ziqi knowledge transfer from molecule generation to binding molecule generation? It does not look right to me.
%}
%allows \method to overcome the limited availability of protein-ligand complexes by taking advantage of the immense molecule dataset. %, potentially accelerating drug discovery research.
%
In the following sections, we will first present the SMG and PMG baselines (Section ``Baselines'').
%
Subsequently, we will present the data used in our experiments (Section ``Data''), the experimental setups (Section ``Experimental Setup'') and the evaluation metrics (Section ``Evaluation Metrics''). 
%
Details about hyper-parameters used in \method are available in Supplementary Section~\ref{supp:experiments:parameters}.

%\bo{
%In this manuscript, we focus on 
%1) evaluating if \method could generating high-quality molecules following the shape condition; 
%2) investigating if \method enables knowledge transfer from XXX to XXX.
%
%Toward this end, we evaluate \method against XXX state-of-the-art XXX baselines in terms of both XXX and XXX.
%
%We also compare \method model learned purely on XXX data to state-of-the-art XXX baselines to investigate if XXX.
%
%Note that XXX data is notoriously sparse as collecting XXX could be prohibitively expensive.
%
%Enabling knowledge transfer across XXX to XXX allows the model to take advantage of the immense XXX data. paving the research for XXX.
%
%To our knowledge, this is the first work XXX
%
%In the following sections, we will first present the XXX baselines.
%
%Subsequently, we will present the data used in our experiments (Section XXX) and the evaluation metrics (Section XXX). 
%}
%\bo{add a section to present the implementation details such as how to calculate the similarities...}

%===================================================================
\subsection*{Baselines}
%===================================================================


\paragraph{SMG Baselines}
%
To evaluate the effectiveness of \method in generating molecules with similar shapes to condition molecules, %following the shape condition, 
%
we compare \method and \methodwithsguide with the state-of-the-art SMG baseline \squid and a virtual screening method \dataset.
%
%Details about shape guidance are available in Section ``\method with Shape Guidance.''
%\bo{describe virtual screening methods in the related work}
%
%\ziqi{Due to hardware limitations, we did not include $\mathsf{Shape2Mol}$~\cite{long2022zero} into the baselines. 
%
%The large model size of  $\mathsf{Shape2Mol}$ (650M parameters vs 2.7M in \methoddiff) and its intensive computational demands (32 Tesla V100 GPUs for 2 weeks vs single V100 GPU for 60 hours in \methoddiff), make it challenging for us to train or test $\mathsf{Shape2Mol}$ on our dataset.}
%
%\ziqi{\st{As far as we know, {\squid} is the only generative baseline that %\hl{directly} \xia{what do you mean?} 
%generates 3D molecules conditioned on molecule shapes.}}
%%
%\squid encodes the shapes and molecular graphs of an input molecule $\molx$ into a posterior 
%\ziqi{$\mathcal{N}(\hiddenvec_\mu, \hiddenvec_\sigma)$} and a shape \ziqi{latent} embedding. %} \xia{grammar??}.
%
%It then decodes the molecule fragments with fixed bond angles and bond lengths from 
%\hl{the shape condition and latent embedding. }\xia{???}
%
%It employs an independent rotatable bond scoring network to adjust the angle of each rotatable bond between fragments, ensuring that the resulting molecules best fit the given shape condition.
%
%\squid consists of a fragment-based generative model and a rotatable-bond scoring model.
%
%The former generates molecules by sequentially decoding fragments conditioned on molecule shapes, while the latter adjusts the angles of rotatable bonds between fragments to ensure the shape following.
%
As introduced in the original paper~\cite{adams2023equivariant}, \squid uses a variable $\lambda$ to balance the interpolation and extrapolation in the latent space.
%
In our experiments, we include \squid with $\lambda=0.3$ and \squid with $\lambda=1.0$ following the literature~\cite{adams2023equivariant}. 
%
%\squid samples the molecule latent embedding by interpolating between the posterior \hl{$N(\hiddenvec_\mu, \hiddenvec_\sigma)$ and the prior $N(\mathbf{0}, \mathbf{1})$}.
%\xia{The above details of \squid could be presented in the related work. here only need a high-level idea 
%of {\squid}. also need describe {\dataset} idea here.}
%\st{, that is, $\hiddenvec_\text{var}\sim N((1-\lambda)\hiddenvec_\mu, (1-\lambda)\hiddenvec_{\sigma} + \lambda\mathbf{1}))$}.
%
\dataset aims to screen through the training set to identify molecules with high shape similarities with the condition
molecule.
%
%For {\squid}, {\method} and {\methodwithsguide}, we generate 50 molecules for each test molecule as the candidates for evaluation.
%
%For {\dataset}, following Adams and Coley~\cite{adams2023equivariant}, we randomly sample 500 training molecules for each molecule in the test set.
%
%We then identify and select the top-50 molecules from the 500 molecules that have the highest shape similarities to the test molecule.
%
Note that we do not consider $\mathsf{Shape2Mol}$~\cite{long2022zero} as our baseline for two reasons.
%
First, the code they provided is closely tied to a private infrastructure~\footnote{https://github.com/longlongman/DESERT/tree/830562e13a0089e9bb3d77956ab70e606316ae78}, making it {highly nontrivial to adapt their code to our infrastructure}. 
%\bo{
%impossible to reproduce their results.
%}
%
Moreover, $\mathsf{Shape2Mol}$ requires {prohibitively} %\st{extensively higher} 
{intensive} computing resources. %\bo{\st{than our method}}.
%
According to their paper, $\mathsf{Shape2Mol}$ is trained on 32 Tesla V100 GPUs for 2 weeks. %\st{, while both {\SE} and {\methoddiff} of {\method} are trained on a single Tesla V100 GPU for no more than 3 days.}
%
%Moreover, the code they provided is closely tied to their infrastructure~\footnote{https://github.com/longlongman/DESERT/tree/830562e13a0089e9bb3d77956ab70e606316ae78}. %; its model parameters (650M) are much larger than ours (2.7M).
%
%\st{Both of these factors make it infeasible for us to train and test $\mathsf{Shape2Mol}$ on our dataset with our infrastructure and hardware resources.}

%\hl{highly nontrivial to adapt; require high computing resouces }
%\squid, \method and \methodwithsguide were employed to generate 50 molecules for each molecule \xia{(i.e., condition)} in the test set.
%
%Meanwhile, \dataset randomly sampled 500 molecules from the training dataset for each test molecule and identified the top 50 molecules with the highest shape similarities.

\paragraph{PMG Baselines} 
%
To evaluate the effectiveness of \method in generating molecules binding towards target protein pockets,
%we augment \method and \methodwithsguide with pocket guidance, denoted as \methodwithpguide and \methodwithsandpguide, and compare them with the state-of-the-art PMG baselines~\cite{luo2021sbdd, peng22pocket2mol, guan2023targetdiff, guan2023decompdiff}.
%
%\bo{
%we augment \method and \methodwithsguide with pocket guidance and denote the resulting variants as XXX.
%%
%We compare XXX and XXX with XXX.
%}
we compare \methodwithpguide and \methodwithsandpguide with four state-of-the-art PMG baselines, including  \AR~\cite{luo2021sbdd}, \pockettwomol~\cite{peng22pocket2mol}, \targetdiff~\cite{guan2023targetdiff}, and \decompdiff~\cite{guan2023decompdiff}. 
%
%Particularly, \AR~\cite{luo2021sbdd} estimates the probability density of atom's occurrences in 3D space of protein binding pockets and sequentially adds atoms based on these estimations to construct molecules.  \pockettwomol~\cite{peng22pocket2mol} sequentially adds atoms into the 3D space of binding pockets and determines the positions of new atoms by predicting their relative positions based on previously added atoms. \targetdiff~\cite{guan2023targetdiff} uses a conditional diffusion model to generate molecules by sequentially denoising atom types and positions conditioned on protein binding pockets. 
%\decompdiff~\cite{guan2023decompdiff} enhances \targetdiff by incorporating data-dependent decomposed prior distributions extracted from either known ligands or protein pockets. 
%
%
For \decompdiff, we exclude \decompdiff with protein pocket priors from the comparison, and only include \decompdiff with known ligand priors.
%
This is due to the substantially lower performance of \decompdiff with protein pocket priors in generating molecules with desirable drug-likeness compared to other methods.
%
More details about \decompdiff with protein pocket priors will be discussed in Supplementary Section~\ref{supp:app:decompdiff}.

%===================================================================
\subsection*{Data}
%===================================================================

\input{tables/dataset}
%\label{tbl:data}

\paragraph{Data for SMG} 
%
%\bo{
%@Ziqi the title is not consistent with that in the previous section. You may want to use XXX-conditioned or XXX-based throught the entire paper.
%}
%
%
Following \squid~\cite{adams2023equivariant}, we use molecules in the MOSES dataset~\cite{mose2020}, with their 3D conformers calculated by RDKit~\cite{rdkit}.
%
We use the same training and testing split as in \squid.
%
%For these molecules, we used the 3D conformers provided by \squid, which are generated using RDKit. %Adams and Coley~\cite{adams2023equivariant} using RDKit.
% 
{Please note that {\squid} further modifies the generated conformers into artificial ones, by adjusting acyclic bond distances to their empirical means 
and fixing acyclic bond angles using heuristic rules.}
%
%\bo{
%@Ziqi is this a reason of why \squid outperforms \method on some quality metrics?
%}
%
Unlike \squid, we do not make any additional adjustments to the calculated 3D conformers, as \method is designed with sufficient flexibility to accept any 3D conformers as input. 
%and generate 3D molecules without restrictions on fixed bond lengths or angles. %with \hl{flexible geometries} \xia{Ziqi, what do you mean?}. 
%\hl{refined geometries} \xia{``refine" is not a good word here -- you are not refining, you are generating new structures}.
%
Limited by the predefined fragment library, \squid also removes molecules with fragments not present in its fragment library. 
%
In contrast, we keep all the molecules, as {\method} is not based on fragments. 
%
As a result, our training set includes 1,593,653 molecules. 
%
%, out of which a random set of 1,000 molecules was selected for validation.
%
%Both the shape autoencoder {\SE} and the shape-conditioned molecule diffusion model {\methoddiff}
%
%\bo{
%@Ziqi need to define these notations probably in the introduction.
%}
%\xia{these notations are never defined before after you re-order all the sections. Need to fix these...}
% of \method are trained using this training set.
%1,000 test molecules (i.e., conditions) as used in \squid are used to test \method. 
%
The same set of 1,000 %\st{test} 
molecules as in \squid is used {for testing.}
%
For hyper-parameter tuning, we randomly sample 1,000 molecules from the training set for validation.
%
%
Table~\ref{tbl:data} presents the data statistics for SMG.
%\st{as condition molecules to test whether {\method} can generate with similar shapes to them}.

%\xia{rewrite the last sentence, and make it explicitly clear what ``condition molecules" are...}

\paragraph{Data for PMG} 
%
Following the previous work~\cite{luo2021sbdd, peng22pocket2mol, guan2023targetdiff, guan2023decompdiff}, we use the CrossDocked2020 benchmark dataset~\cite{Francoeur2020} with protein-ligand complex data to evaluate \method. 
%
%We used the same training and test\bo{ing} split as in the previous work~\cite{luo2021sbdd, peng22pocket2mol, guan2023targetdiff, guan2023decompdiff}, which includes 100,000 protein-ligand complexes for training and 100 complexes with novel proteins for testing.
%
During evaluation, for \method, we directly utilize the model trained on the MOSES without fine-tuning on the complex data. %to assess its generalizability.
%
For all the PMG baselines, we use the model checkpoints released by the authors.
%
All PMG baselines are trained on the training set of CrossDocked2020 as presented in their original paper~\cite{luo2021sbdd, peng22pocket2mol, guan2023targetdiff, guan2023decompdiff}, which includes {11,915 unique ligands, 15,207 unique proteins}, and 100,000 protein-ligand complexes in total.
%
Note that the PMG baselines are designed specifically for protein-ligand complex data, and cannot accept {molecule data as input. }%\hl{pure molecule} \bo{@Ziqi is this term common in the literature?} data as input.
%
Thus, we do not tune PMG baselines on the MOSES training set.
%
We use the same test dataset as in the previous work~\cite{luo2021sbdd, peng22pocket2mol, guan2023targetdiff, guan2023decompdiff}, which includes 100 protein-ligand complexes with novel proteins.
%
{Note that the MOSES dataset focuses on molecules with number of atoms ranging from 8 to 27.
%
In this evaluation, we do not consider complexes with out-of-distribution ligands (i.e., ligands with more than 27 atoms).
%
Thus, we exclude 28 complexes from the test set of CrossDocked2020.
%
Table~\ref{tbl:data} presents the data statistics for PMG.
%\hl{We exclude 9 complexes from this test set because their ligands have more than 35 atoms and are greatly larger than molecules in the MOSES dataset.
%
%This size difference makes these ligands unsuitable for evaluating the ability of {\method} in applying knowledge learned from the MOSES dataset to generate binding molecules with similar shapes to these ligands.}
%
%\bo{@Ziqi It does not look like a valid reason...
%You exclude these ligands in both Table 4 and 5?}
%
%We report the performance of \methodwithpguide, \methodwithsandpguide and all PMG methods on the remaining 91 complexes.
%
%=======
%Please note that we did not train the \method on the ligands in the CrossDocked2020 training set.
%
%Instead, we directly utilized the \method model trained on the MOSES dataset to evaluate its generalization ability in pocket-based molecule generation..
%
%This is informed by the observation that directly training \method on the CrossDocked2020 dataset did not result in substantial performance improvement compared with the \method model trained on the MOSES dataset.
%
%This lack of performance improvement could be attributed to the considerably larger size of the MOSES dataset compared to the CrossDocked2020 dataset.
%
%In contrast, it is worth noting that other pocket-based models require protein-ligand complexes in the CrossDocked2020 dataset for training and thus cannot be trained on the MOSES dataset as \method does.
%
%This highlights the potential of \method in XXX
%
%Please note that we did not train the \method on the ligands in the CrossDocked2020 training set; instead, we directly utilized the \method model trained on the MOSES dataset for evaluation.
%
%This is informed by the observation that directly training \method on the CrossDocked2020 dataset did not result in substantial performance improvement compared with the \method model trained on the MOSES dataset.
%
%This lack of performance improvement could be attributed to the considerably larger size of the MOSES dataset compared to the CrossDocked2020 dataset.
%
%In addition, it is worth noting that other pocket-based models require protein-ligand complexes in the CrossDocked2020 dataset for training and thus cannot be trained on the MOSES dataset as \method does.
%
%This highlights the potential of \method in XXX
%for with their 3D conformers calculated by RDKit~\cite{rdkit}.
%

%===================================================================
\subsection*{Experimental Setup}
%===================================================================

\paragraph{Evaluation of \method in SMG} 
%
%We train \SE and \methoddiff in \method using all training molecules in MOSES.
%We train \method on the training set of MOSES dataset.
%
%We extract the shapes of molecules in the training set of MOSES and train \SE over these shapes.
%
%After training \SE, we compute the shape embeddings for training molecules.
%
%We train \methoddiff using training molecules and their shape embeddings. 
%
%During evaluation, we encode the shapes of test molecules into embeddings using \SE.
%
%{We then employ \methoddiff to generate molecules conditioned on these embeddings. 
%
%either with or without the shape guidance.
%
%We can further incorporate shape guidance into the generation process and denote \method with shape guidance as \methodwithsguide. } 
%, referred to as {\methodwithsguide} and {\method}, respectively. } 
%and generate molecules using \hl{{\methoddiff} without ({\method}) or with shape guidance ({\methodwithsguide})} 
%\bo{@Ziqi it is confusing. You may want to rephrase here.}
%
%
Following \squid~\cite{adams2023equivariant}, we apply \method, \methodwithsguide and all SMG baselines to generate 50 molecules per test molecule for evaluation.  
%
%For {\squid}, {\method} and {\methodwithsguide}, we use the generated 50 molecules as the candidates for evaluation.
%
For {\dataset}, following \squid, we randomly sample 500 training molecules for each test molecule.
%
We then identify and select the top-50 molecules from the 500 molecules that have the highest shape similarities to the test molecule.
%
%Details about hyper-parameters used in \SE and \methoddiff are available in Supplementary Section~\ref{supp:experiments:parameters}.
%


\paragraph{Evaluation of \method in PMG} 
%
As discussed above, we directly utilize the \method model trained on the MOSES dataset for the evaluation against PMG baseline methods.
%
%During evaluation, {we enhance both {\method} and {\methodwithsguide} with pocket guidance and denote their enhanced versions as \methodwithpguide and \methodwithsandpguide. }%\hl{we incorporate pocket guidance into {\method} and {\methodwithsguide}, denoted as {\methodwithpguide} and {\methodwithsandpguide}.}
%
%\bo{
%@Ziqi it does not look right. Please rephrase
%}
%
Following previous PMG baselines~\cite{luo2021sbdd, peng22pocket2mol, guan2023targetdiff, guan2023decompdiff}, we use \methodwithpguide, and \methodwithsandpguide to generate 100 molecules for each test protein-ligand complex.
%
{For \methodwithpguide and \methodwithsandpguide}, we use \SE to encode the shapes of ligands in test protein-ligand complexes into shape embeddings.
%
{Then, we use \methodwithpguide and \methodwithsandpguide to generate molecules conditioned on these embeddings. 
%
For baselines, %\st{comparison}, 
we directly use molecules generated {from} %\st{for} 
\AR, \pockettwomol and \targetdiff, as provided by \targetdiff~\footnote{https://github.com/guanjq/targetdiff}, to calculate evaluation metrics.}
%
For \decompdiff, we use the model checkpoints released by the authors to generate 100 molecules for each test protein-ligand complex.
%


%===================================================================
\subsection*{Evaluation Metrics}
%===================================================================


\paragraph{Metrics for SMG} 
%
%\ziqi{need to be updated}
To evaluate the performance of \method and SMG baselines in generating molecules with similar shapes to condition molecules, we use shape similarity $\shapesim$ and molecular graph similarity 
$\graphsim$ as evaluation metrics. %to measure the s between the generated molecule \moly and %\st{respective to} 
%the {test} molecule \molx. 
%
%
Higher $\shapesim$ and lower \graphsim suggests that generated molecules could have similar binding activities and substantially different molecular graphs compared to condition molecules (e.g., ligands).
%, facilitating the discovery of promising drug candidates.} %\moly not only shares similar shapes with \molx, %\st{potentially} 
%indicating similar binding activities, but also has a novel graph structure distinct from \molx. 
%
%\hl{The capability to generate such molecules signifies the effectiveness of a method in exploring the chemical space for promising drug candidates with various graph structures, thereby indicating better performance.}
%
%\bo{
%@Ziqi what's the point here? performance?
%}
%better model performance. 
%
We calculate the shape similarity $\shapesim$ via the overlapped volumes between two aligned molecules following the literature~\cite{adams2023equivariant}.
%
Each generated molecule is aligned with the condition molecule by the ROCS tool~\cite{Hawkins2006}.
%which calculates the overlap of the optimally aligned shapes $\shape_x$ and $\shape_y$.
%
For the molecular graph similarity $\graphsim$, we use the Tanimoto similarity, calculated by RDKit~\cite{rdkit}, over Morgan fingerprints between the generated and condition molecule. %\xia{between what?}%\xia{citation!}.
%
Based on \shapesim and \graphsim, we calculate the following three metrics using the set of 50 generated molecules per condition molecule, and report the average of these metrics across all condition molecules in the test set:
%}$\shapesim(\shape_x, \shape_y)$ and $\graphsim(\molx, \moly)$,
%we calculate the following metrics \bo{using} %\st{for} 
%the \ziqi{set of }50 generated molecule per test molecule and average these across all test molecule:
%
%(1) \avgshapesim\ measures the average \shapesim across each set of generated molecules; %per test molecule, with the overall average calculated across all test molecules; }%the 50 generated molecules for each test molecule, averaged across all test molecules;
%(2) \avggraphsim\ calculates the average \graphsim for each set; %, with these means averaged across all test molecules}; %} 50 molecules, %\bo{@Ziqi rephrase}, with results averaged on the test set;\ziqi{with the average computed over the test set; }
%(3) \maxshapesim\ determines the maximum \shapesim within each set; %, with these maxima averaged across all test molecules; }%\hl{among 50 molecules}, averaged across all test molecules;
%(4) \maxgraphsim\ measures the \graphsim of the molecule with maximum \shapesim in each set; %, averaged across all test molecules; }%\hl{among 50 molecules}, averaged across all test molecules;
(1) \desire calculates the percentage of molecules in each set with \shapesim$>$0.8 and \graphsim smaller than a threshold $\delta_g$, referred to as desirable molecules; %\bo{@Ziqi citation}, averaged across all test molecules;
(2) \diversity\ measures the diversity among desirable molecules within each set, calculated as 1 minus the average pairwise graph similarity; %, averaged across all test molecules. }%, calculated by 1 minus pairwise \graphsim, \hl{among desirable molecules of 50 molecules, averaged across all test molecules.}
%
(3)  \novel calculates the percentage of desirable molecules in each set that cannot be found in the MOSES dataset. %with \graphsim lower than a graph similarity constraint $\delta_g$. %\st{among 50 molecules},
% 
{Following Bostroem \etal~\cite{Bostroem2006}, we select 0.8 as the threshold of \shapesim for desirable molecules.
%
This threshold is chosen to ensure that the selected desirable molecules have highly similar shapes, and thus, similar binding activities to condition molecules.
%
During evaluation, we use test molecules as condition molecules for the generation.}
%\bo{satisfying $\delta_g$ and } 
%with shape similarities larger than 0.8 \bo{(detailed in Section XXX)}.}

%\bo{@Ziqi what do you mean? rephrase}
%
%Details about the evaluation metrics are available in Supplementary Section~\ref{supp:experiments:metrics}. 
%\xia{need to include all the metrics and their definitions here, not in supp}

\paragraph{Metrics for PMG} 
%
We evaluate the performance of \method and PMG baselines in generating molecules binding towards protein targets. %in terms of both effectiveness and \hl{efficiency}.
%
%
Following previous work~\cite{guan2023targetdiff, guan2023decompdiff}, we evaluate the binding affinities, drug-likeness, and diversity of generated molecules.
%
For binding affinity, we use Vina Scores (Vina S) calculated by AutoDock Vina~\cite{Eberhardt2021} as an evaluation metric. 
%
As suggested in the literature~\cite{guan2023targetdiff, guan2023decompdiff}, we also consider the optimized poses from the generated 3D molecules in evaluation.
%
Specifically, we use Vina Minimization (Vina M) and Vina Dock (Vina D) calculated from AutoDock Vina as evaluation metrics.
%
Vina M and Vina D optimize the poses by local energy minimization and global search optimization, respectively~\cite{Eberhardt2021}. % find a reference about local energy minimization
%
%Particularly, as in previous work~\cite{guan2023targetdiff, guan2023decompdiff}, we evaluate binding affinities using three types of 3D poses, including the initially generated poses of molecules, the poses after local energy minimization and the poses determined by Autodock Vina~\cite{Eberhardt2021}.
%
%For each test protein-ligand complex, we estimate three binding affinities based on these poses using AutoDock Vina~\cite{Eberhardt2021}, including Vina Scores (Vina S), Vina Minimization (Vina M), and Vina Dock (Vina D).  
%
%\ziqi{Each score provides unique insights into the binding potential of the generated molecules.
%
%To be specific, Vina S evaluates how well the generated 3D poses of molecules bind with the binding pocket of the protein target.
%
%Vina M reflects the potential of the generated molecules to better bind with the protein target through slight adjustments to its 3D structure.
%
%Vina D offers a more exhaustive evaluation of the binding potential for generated molecules through docking simulations.}
%
For drug-likeness, we evaluate whether the generated molecules are drug-like using QED scores~\cite{Bickerton2012} and synthesizable using synthesizability scores (SA)~\cite{Ertl2009}.
%
We also calculate the diversity as defined in the previous paragraph among generated molecules. 
%We also calculate the diversity using all the generated molecules by calculating 1 minus average pairwise graph similarity. %\graphsim} \bo{@Ziqi rephrase please}.
%
Following previous work~\cite{guan2023targetdiff, guan2023decompdiff}, we report the average and median of the above metrics across all test complexes. 
%
%For efficiency, we calculate the average time cost for all methods to \ziqi{generate molecules}. %\hl{generate the 100 molecules}, \bo{@Ziqi I do not think this is what you reported in the table for \method... how about just "generate molecules"} 
%which are utilized for comparison, for each test protein.

\paragraph{Metrics for Evaluation of Molecule Quality} 
%
We evaluate the quality of generated molecules based on their realism for both SMG and PMG.
%
We use a comprehensive set of metrics to evaluate the stability, %\bo{@Ziqi stability?} 
3D structures, and 2D structures of generated molecules. %bonds, and rings of generated molecules.
%
For stability, following Hoogeboom \etal~\cite{hoogeboom22diff}, we calculate atom and molecule stability of generated molecules.
%
Atom stability measures the proportion of atoms that have the right valency, while molecule stability measures the proportion of generated molecules that all the atoms are stable. 
%
For 3D structures and 2D structures, we use the same metrics as in Peng \etal~\cite{peng2023moldiff}
Particularly,
%
%Following Peng \etal~\cite{peng2023moldiff}, we use a comprehensive set of metrics from three aspects, including %\bo{@Ziqi stability?} 
%3D structures, bonds, and rings, to evaluate the quality.
%
for 3D structures, we use root mean square deviations (RMSDs) and Jensen-Shannon (JS) divergences of bond lengths, bond angles and dihedral angles to evaluate the quality of 3D molecule structures.
%
RMSDs measure the discrepancies between the generated 3D structures of molecules and their optimal structures identified by RDKit toolkit~\cite{rdkit} via energy minimization. 
%
%are calculated \ziqi{based on the minimal discrepancies}
%by the minimal RMSDs 
%between the generated 3D molecules and 100 possible conformations predicted by the RDKit toolkit~\cite{rdkit} \ziqi{through energy minimization algorithms}.
%
%This metric measures how closely the generated 3D structures of molecules align with their ideal conformations. 
%
In addition, the JS divergences of bond lengths, bond angles and dihedral angles measure the divergences between the generated molecules and the real molecules (i.e., training molecules) regarding the 3D structures.
%
Smaller divergence values indicate that the generated molecules have these properties more similar to those of training molecules and thus more realistic.
%For SMG, \ziqi{the JS divergences are calculated against the training molecules in the MOSES dataset; and for PMG, the JS divergences are calculated against the ligands in the training protein-ligand complexes of PMG baseline methods.}
%we use the training molecules in MOSES to calculate JS divergences; for PMG, we use the ligands in the training protein-ligand complexes for the calculation.
%
To evaluate the quality of 2D molecule structures, we primarily assess if the bonds and rings within the generated molecules are similar to those in real molecules.
%
Particularly, for bonds, we evaluate the JS divergences in terms of bond counts per atom and bond types (single, double, triple, and aromatic).
%
For rings, we compare both the counts of all rings and the counts of rings of varying sizes (n-sized rings) in the generated molecules to those in real molecules using JS divergences.
%
%We calculate JS divergence to measure the difference in the distributions of overall ring counts and the counts of n-sized rings between the generated molecules and the training molecules.
%
Furthermore, we measure if the generated molecules capture the frequent rings in real molecules. 
%
To be specific, we calculate the number of overlapping rings observed in the top-10 frequent ring types of both generated and real molecules.
%
Note that we consider different molecules to calculate JS divergences when comparing against SMG and PMG baselines.
%
 For SMG, we use the training molecules in the MOSES dataset.
For PMG, we use the ligands in the training protein-ligand complexes of CrossDocked2020.
%To evaluate whether the generated molecules have similar bond distributions with training molecules, we employ five metrics, including the JS divergences of bond counts per atom and basic bond types. %, the frequent bond types, the frequent bond pairs, and the frequent bond triplets.
%
%To evaluate the quality of rings in the generated molecules, we compare the distributions in the total counts of rings and the counts of n-sized rings between the generated molecules and training molecules using JS divergence.
%
%We also report the number of common rings in the generated molecules intersecting with those in the training molecules.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Experimental Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bo{
%@Ziqi needs a paragraph to summarize the findings in the experiments
%}

%\bo{
%@Ziqi need to finalize all the tables, table notes, figures and captions. It could take a lot of efforts...
%}


%-------------------------------------------------------------------------------------------------------------------------------------
\subsection*{Overall Comparison on Generating Desirable Molecules in SMG}
\label{sec:results:overall_desirable}
%-------------------------------------------------------------------------------------------------------------------------------------

%\ziqi{consider removing this section, as we will not select desirable molecules for binding affinity analyses in Section ``Overall Comparison for Pocket-based Molecule Generation''}

\input{tables/overall_results_desirable}
%\label{tbl:overall_desirable}

%Following the previous work, we use 
%As shown in Table~\ref{tbl:overall}, regarding desirable molecules, 
%\st{Apart from the shape and graph similarities, we also analyze the ability of {\method} and {\methodwithsguide} in generating diverse molecules with desirable shape similarities and low graph similarities.}
%
We evaluate \method, \methodwithsguide and state-of-the-art SMG baselines in generating desirable molecules.
%
%\bo{\st{Following Bostroem {\etal}~{\cite{Bostroem2006}}, we define desirable molecules as those with shape similarities greater than 0.8, as these molecules are very similar to condition molecules in shape and thus have a high probability of functional similarity.}}
%
Following Bostroem \etal~\cite{Bostroem2006}, we define desirable molecules as those satisfying $\delta_g$ and with shape similarities larger than 0.8 {(detailed in Section ``Evaluation Metrics'')}.
%
These molecules have highly similar shape with the condition molecules (e.g., ligands), and thus, could also have desirable binding activities~\cite{Acharya2011}. % functionalities (e.g., XXX)
%\bo{
%@Ziqi What functionalities. Give some examples here.
%}
%
In this analysis, for each method, we calculate the possibility of generating desirable molecules (i.e., \desire), the diversity {and the novelty} among these molecules (i.e., \diversity and \novel) under different graph similarity constraints (i.e., $\delta_g$=0.3, 0.5, 0.7, and 1.0). %\st{and the diversity among desirable molecules}.
%
As shown in Table~\ref{tbl:overall_desirable},
%
\method and \methodwithsguide consistently outperform all the baseline methods 
%in the percentage of desirable molecules ($\#d\%$) and the diversity among all the desirable molecules (\diversity). 
%\bo{
%\st{in both the likelihood  of generating desirable molecules (i.e., $\#d\%$), and the diversity of the generated desirable molecules (i.e., \diversity).
%}
%
%\bo{
in terms of {all the metrics}. %$\#d\%$ and \diversity.
%}
%}
%, and the average shape similarity ($\avgshapesim$). %, while \method also outperforms all the baselines in these two metrics.
%
%
For example, when $\delta_g$=0.3, at \desire, \methodwithsguide (61.4\%) demonstrates a substantial improvement of $448.2\%$ compared to the best baseline \squid ($\lambda$=1.0) (11.2\%).
%
%\method also outperforms all the baselines in $\#d\%$.
%
In terms of \diversity, \method (0.762) also substantially outperforms the best baseline \dataset (0.736) by 2.3\%. %, while \methodwithsguide (0.748) outperforms it with 1.9\% improvement.
%
{At \novel, both \method and \methodwithsguide ensure that nearly all the generated desirable molecules are novel (99.8\% for \method and 99.9\% for \methodwithsguide), substantially outperforming the best baseline \squid with $\lambda$=1.0 (96.9\%) by 3.1\% and 3.0\%, respectively.}
%\methodwithsguide (0.824) also establishes a significant improvement of 9.9\% over the best baseline \dataset (0.750) on $\avgshapesim$. %, although it slightly underperforms \dataset on $\avggraphsim$ (0.230 vs 0.226).
%
When $\delta_g$=0.5, 0.7, or 1.0, a similar trend is observed.
%\bo{\st{: {\method} and {\methodwithsguide} consistently outperform all the baseline methods on $\#d\%$ and \diversity.}} % and $\avgshapesim$.
%
Specifically, when $\delta_g$=0.5, at \desire, \methodwithsguide (70.9\%) establishes a notable improvement of 225.2\% compared to the best baseline method \squid with $\lambda$=0.3 (21.8\%).
%
{At \diversity and \novel, \method and \methodwithsguide also demonstrate top performance among all the methods. }
%
When $\delta_g$=0.7, at \desire, \methodwithsguide (71.0\%) also achieves a remarkable improvement of 158.2\% over the best baseline method \squid with $\lambda$=0.3 (27.5\%).
%
%\bo{\st{
%When $\delta_g$=1.0, {\methodwithsguide} (71.0\%) also achieves a significant improvement of 126.8\% over the best baseline method {\squid} with $\lambda$=0.3 (31.3\%).
%}}
%
%We also note that when $\delta_g$ is small (e.g., $\delta_g$=0.3), \method and \methodwithsguide are still capable of generating a high percentage of desirable molecules (e.g., 61.4\% for \methodwithsguide on $\#d\%$).
%
%In contrast, when $\delta_g$=0.3, for \squid ($\lambda$=0.3), only 8.3\% of the generated molecules are desirable.
%
%\bo{
%It is also worth noting that state-of-the-art baselines struggle to generate desirable molecules when the graph similarity constraint is strict (e.g., $\delta_g$=0.3).
%%
%For example, when $\delta_g$=0.3, only 10.6\% and 11.2\% of the generated molecules are desirable for \dataset and \squid, respectively.
%%
%Different from baselines, \methodwithsguide is not limited by the similarity constraint, 
%and could still ensure a remarkable possibility of generating desirable molecules (61.4\%) when $\delta_g$=0.3.  
%}
%
%The superior performance of \method and \methodwithsguide in $\#d\%$ at small $\delta_g$ indicates their strong capacity in generating desirable molecules of novel graph structures, thereby facilitating the discovery of novel drug candidates.
%
The superior performance of \method and \methodwithsguide in \desire, \diversity, and \novel,  particularly at small $\delta_g$, indicates their strong capacity in generating {novel molecules that have desirable shapes and distinct graph structures compared to the condition molecules, }%diverse, novel, and desirable molecules, 
thereby facilitating the process of drug development.
%
Additional results about the comparison of shape similarity and graph similarity and the comparison of validity and novelty are available in Supplementary Section~\ref{supp:app:results:overall_shape} and \ref{supp:app:results:valid_novel}, respectively. 

%\bo{
%@Ziqi reorganize the following paragraph
%It is worth noting that a key distinction between \method and the baseline \squid is that XXX
%
%Intuitively, (@Ziqi add some references if you have any) 
%generating molecules based on fragments considerably limits \squid in 1) XXX and 2).
%
%Particularly, XXX
%
%Results in Table XXX serves as a strong evidence that by generating molecules atom by atom, \method enables XXX, showing high utilities in drug discovery.
%}

It is worth noting that, as shown in Table~\ref{tbl:overall_desirable}, \method and \methodwithsguide consistently outperform \squid with $\lambda$=0.3 and 1.0 in terms of all the metrics. 
%
A key distinction between \method and \squid is that \squid generates molecules by sequentially predicting fragments. 
%
However, during fragment prediction, their poses are not fully considered, leading to suboptimal prediction accuracy and limited generation performance.
%
%Different from \squid, \method generates molecules by directly arranging atoms in the 3D space using diffusion models.
%
%In contrast, \method directly arranges atoms to generate molecules with desirable shapes.
%
%\ziqi{This distinction enables \method to overcome limitations of \squid resulted from its reliance on a pre-defined fragment library.}
%
%{Given the limited available fragments in the library, \squid could suffer from the suboptimal performance in generating desirable molecules with diverse and novel graph structures. }
% given the limited available fragments in the library, and 
%shape similarity between the generated molecule and the condition molecule, when none of the fragments in its library are suitable for the shape condition; 
%2) genera%\squid's fragment-based approach restricts the diversity of generated molecules, due to the limited size of the fragment library;
%\ziqi{3) this approach also constrains the novelty of generated molecules, as only existing fragments are used for generation.}
%
On the other hand, by directly arranging atoms, \method and \methodwithsguide explicitly consider the 3D atom positions when generating molecules, and thus, achieve remarkable improvement over \squid as shown in Table~\ref{tbl:overall_desirable}. %overcome these limitations by directly arranging atoms.
%
%{Please note that it could be highly non-trivial for \squid to incorporate a large number of fragments, as a large-sized fragment dictionary can substantially degrade the accuracy of fragment selection in \squid.}
%This allows them to not only achieve better shape following but also enhance the diversity and novelty of generated molecules
%\bo{over \squid as demonstrated in Table~\ref{tbl:overall_desirable}}. 
%\bo{
%@Ziqi why do you want to switch the subject from advantages to limitations in the two consecutive sentences...
%}

{Different from \method and \squid which directly generate desirable molecules, \dataset screens over randomly sampled training molecules to identify molecules of interest. %in the MOSES training set.
%
However, it cannot ensure optimal precision design, %is likely that all the sampled molecules are distinct from the condition molecules in 3D shape, 
resulting in the suboptimal performance of \dataset at \desire. % that there exists molecules with desirable shapes in the sampled molecules, thereby degrading the performance of \dataset at $\#d\%$.
%
In addition, due to the reliance on existing molecules, \dataset cannot discover novel molecules. % the chemical space of novel molecules.
%
In contrast, \method can effectively generate novel molecules with desirable shapes, making it a promising tool for discovering novel drug candidates.
}

Comparing \methodwithsguide and \method, Table~\ref{tbl:overall_desirable} shows that incorporating shape guidance into \method substantially boosts its effectiveness in generating desirable molecules.
%
For example, when $\delta_g$=0.3, at \#d\%, \methodwithsguide (61.4\%) substantially outperforms \method (28.4\%) by 116.2\%.
%
when $\delta_g$=0.5, 0.7, and 1.0, \methodwithsguide also achieve a considerable improvement  of 119.5\%, 119.1\% and 119.1\%, respectively, compared to \method.
%
In the meantime, \methodwithsguide retains very similar performance with \method in terms of the diversity and novelty of generated desirable molecules.
%
These results signify that shape guidance effectively improves the ability of \method in generating molecules that have similar shapes to condition molecules without degrading the novelty and diversity among generated molecules.

\begin{comment}
Table~\ref{tbl:overall_desirable} also demonstrates that \method and \methodwithsguide substantially outperform \squid with $\lambda$ = 0.3 or 1.0 in both $\#d\%$ and \diversity, especially when $\delta_g$ is small (e.g., $\delta_g$=0.3).
%
The inferior performance of \squid could stem from two limitations induced by its fragment-based generation strategy.
%
Firstly, \squid might struggle to generate molecules with desired shapes, when none of the fragments in its library are suitable for the shape condition.
%
In contrast, \method and \methodwithsguide are less constrained by directly arranging atoms to match the shape condition. 
%
Secondly, \squid is limited to generate molecules with fragments in the library, thereby decreasing the diversity among generated molecules.
%
\method and \methodwithsguide enjoy the flexibility to generate molecules with any random fragments and thus achieve higher diversity scores than \squid.
%
Therefore, they achieve the highest percentage of desirable molecules, as well as high diversity among these desirable molecules.
\end{comment}

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection*{Quality Comparison between Desirable Molecules Generated by \method and \squid}
\label{sec:results:quality_desirable}
%-------------------------------------------------------------------------------------------------------------------------------------


%\input{tables/overall_results_quality0.3}
%%\label{tbl:overall_quality10}

\input{tables/overall_results_quality_desired}
%\label{tbl:overall_results_quality_desired}

%\ziqi{Outline of this section:
%	\begin{itemize}
%		\item \method can generate molecules with comparable quality with \squid;
%		\item The similarity table and the quality table together show that \method can generate molecules with comparable quality yet novel structures.
%		\item backup about the reason why \squid outperforms \method on some metrics, especially bond type-related metrics: \squid is a fragment-based molecule generation method; it can only generate molecules with 100 most frequent fragments. This strategy benefits \squid in generating molecules with frequent bond types/pairs and triplets. However, it also limits \squid in generating molecules with fragments out of the dictionary. This could degrade the capacity of \squid to generate diverse molecules and effectively explore the chemical space.
%	\end{itemize}
%}

We also evaluate the quality of desirable molecules %\hl{(i.e., \graphsim$<$0.3 and \shapesim$>$0.8)} 
%
%\bo{
%@Ziqi The definition of desirable is different from that in the previous section. In the last section, you have desirable mols when
%\graphsim$<$0.5, 0.7 and 1.0.
%}
% \bo{(i.e., XXX)} 
generated from \method, \methodwithsguide, and baseline methods in terms of stability, 3D structures, and 2D structures. %bonds, and rings. 
%
%Table~\ref{tbl:overall_results_quality_desired} presents the performance comparison of desirable molecules under the graph similarity constraint $\delta_g$ of 0.3 generated by all the methods.
%
Table~\ref{tbl:overall_results_quality_desired} presents the performance comparison 
in the quality of
desirable molecules generated by different methods when the graph similarity constraint $\delta_g$ is 0.3.
%
Details about the comparison under different $\delta_g$ (e.g., 0.5, 0.7, and 1.0) are available in Supplementary Section~\ref{supp:app:results:quality_desirable}. 
%
%\bo{\st{
%Note that in this evaluation, we do not consider {\dataset} due to the fact that {\dataset} does not generate molecules but directly selects from existing molecules, and thus is incomparable with other generative methods.
%}}
%
{Note that, in this analysis, we focus on desirable molecules that could have high utility in drug development.}
%
We also exclude the search algorithm \dataset and consider only generative models, such as \method and \squid, in this analysis. 

As shown in Table~\ref{tbl:overall_results_quality_desired}, \method generates molecules with 
comparable quality
to baselines in terms of stability, 3D structures, and 2D structures. 
%
For example, in stability, Table~\ref{tbl:overall_results_quality_desired} shows that \method and \methodwithsguide %\bo{\st{achieve either comparable performance or slightly behind the baseline}} 
either achieve comparable performance or fall slightly behind 
\squid ($\lambda$=0.3) and \squid ($\lambda$=1.0) in atom stability and molecule stability.
%
Particularly, \method achieves similar performance with %sightly underperforms 
\squid ($\lambda$=0.3) and \squid ($\lambda$=1.0) in atom stability (0.993 for \method vs 0.996 for \squid with $\lambda$ of 0.3 and 1.0).
%
%\methodwithsguide also achieves comparable performance to the baselines with only 0.4\% difference in atom stability.
%
%\hl{It also underperforms the best baseline {\squid} ($\lambda$=0.3) with 6.5\% difference.}
%
%\bo{
%@Ziqi what do you mean?
%
%\ziqi{However, it underperforms the best baseline {\squid} ($\lambda$=0.3) in molecule stability with 6.5\% difference. }
%\bo{
%@Ziqi the thing after "however" is what you want to highlight.
%%
%Do you want to highlight the worse performance?
%}
In terms of molecule stability, \method underperforms {\squid} ($\lambda$=0.3) by 6.5\%.
%
However, \method still demonstrates strong effectiveness in generating stable molecules, with 89.1\% of generated molecules being stable. %is still capable of 

Table~\ref{tbl:overall_results_quality_desired} also shows that \method and \methodwithsguide generate molecules with more realistic 3D structures compared to \squid.
%\bo{\st{In terms of 3D structures, both {\method} and {\methodwithsguide} achieve superior performance in most metrics related to 3D structure compared to the {\squid}.}}
%
%\bo{
%@Ziqi did you introduce the metrics in quality evaluation and binding affinity comparison somewhere?
%}
%
Particularly, for RMSD, %\bo{\st{both}} 
\method and \methodwithsguide outperform the best baseline \squid ($\lambda$=1.0)  
by 0.8\% and 2.2\%,
respectively.
%
In addition, they also establish a notable improvement of 4.6\% and 6.3\% 
over the best baseline \squid ($\lambda$=0.3) in JS. bond lengths.
%
In terms of JS. bond angles and JS. dihedral angles, \method and \methodwithsguide outperform the best baseline \squid ($\lambda$=0.3) by 30.9\% and 25.7\%, and by  15.6\% and 14.6\%, respectively.
%
%They also achieve superior performance over the best baseline \squid ($\lambda$=0.3) with 15.6\% and 14.6\% improvement in JS. dihedral angles.
%
%
As discussed in Section ``Related Work'', \squid fixes the bond lengths and angles within the generated molecules, leading to the discrepancy in 3D structures between the generated molecules and real molecules.
%
Conversely, \method and \methodwithsguide use a data-driven manner to infer distances and angles between atoms.
%
This design enables  \method and \methodwithsguide to achieve superior performance in generating molecules with realistic 3D structures. %, by inferring angles between atoms in a purely data-driven manner.


Table~\ref{tbl:overall_results_quality_desired} also presents that \method and \methodwithsguide achieve comparable performance with  \squid in generating realistic 2D molecule structures. %realistic rings in molecules.
%
Particularly, for JS. \#bonds per atom, \method and \methodwithsguide substantially outperform the best baseline \squid ($\lambda$=0.3) by 77.8\% and 73.9\%, respectively. 
%
In terms of JS. basic bond types, \method and \methodwithsguide underperform \squid ($\lambda$=0.3) considerably.
%
\method and \methodwithsguide also achieve substantially better performance (0.042 and 0.048) than \squid with $\lambda$=0.3 (0.309) in JS. \#rings.
%
\method and \methodwithsguide slightly underperform \squid in terms of JS. \#n-sized rings and the number of intersecting rings. 
%
%\method and \methodwithsguide also slightly underperform \squid in terms of the number of intersecting rings.
%
%\bo{
%@Ziqi what does "those" mean here?
%}
%intersection of the most frequent rings \hl{with the real molecules with only a difference of 1 or 2 rings}.
%
%\bo{
%I do not understand this...
%}
%
These results signify that \method and \methodwithsguide, though not explicitly leverage fragments 
%
%\bo{@Ziqi why do you want to emphasize frequent rings?}
%
as \squid does, can still generate molecules with realistic 2D structures.
%


%-------------------------------------------------------------------------------------------------------------------------------------
\subsubsection*{Analysis on Shape and Graph Similarities}
\label{sec:results:analysis_similarity}
%-------------------------------------------------------------------------------------------------------------------------------------


\begin{figure}[!ht]
	%
	\centering
	%
	\begin{subfigure}[b]{.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/shapemol_heatmap.pdf}
		\caption{\method}
		\label{fig:sim_comparison：method}
	\end{subfigure}
	%
	\begin{subfigure}[b]{.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/shapemolwithguide_heatmap.pdf}
		\caption{\methodwithsguide}
		\label{fig:sim_comparison：methodwithsguide}
	\end{subfigure}
	%
	\begin{subfigure}[b]{.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/squid03_heatmap.pdf}
		\caption{\squid ($\lambda$=0.3)}
		\label{fig:sim_comparison：squid03}
	\end{subfigure}
	%
	\begin{subfigure}[b]{.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/squid10_heatmap.pdf}
		\caption{\squid ($\lambda$=1.0)}
		\label{fig:sim_comparison：squid10}
	\end{subfigure}
	%
	%\begin{subfigure}[b]{.19\linewidth}
	%	\centering
	%	\includegraphics[width=\linewidth]{figures/vs_heatmap.pdf}
	%	\caption{\dataset}
	%	\label{fig:sim_comparison：dataset}
	%\end{subfigure}
	\vspace{-5pt}
	\captionsetup{justification=centering}
	\caption{\textbf{Heatmaps of Similarities Calculated from Molecules Generated by %\dataset, 
	\squid and \method.}}
	\label{fig:sim_comparison}
\end{figure}

%In addition to the average and maximum values in Table~\ref{tbl:overall_sim}, w
We analyze the distributions of shape and graph similarities between condition molecules and all molecules generated from \method and \squid.
%
%\st{Note that for \dataset, we select the top-50 molecules of the highest shape similarities from an entire subset of 500 molecules extracted from training set.} 
%, without considering any graph similarity constraints. }
%
We conduct this analysis to
(1) assess the capacity of \method and \squid in generating molecules with similar shapes to condition molecules (e.g., ligands); and
(2) compare the strategies in \method and \squid to further improve shape similarities. %(shape guidance in \methodwithsguide and different $\lambda$ values for \squid); 
% (3) t graph and shape similarity distributions in generated molecules and sampled training molecules. %of all the methods in generating molecules with high shape similarities and low graph similarities.
%
In this analysis, for each condition molecule, we use all 50 molecules generated by \method, \methodwithsguide, \squid with $\lambda$=0.3 and \squid with $\lambda$=1.0.  %for XXX, we XXX.
%For \dataset, as discussed in Section ``Baselines'', for each test molecule, we select the top-50 molecules of the highest shape similarities among the sampled training molecules for our analysis.
%Particularly, XXX (@Ziqi, the analysis setup, observe what results could support your claim and why)
%
As shown in Fig.~\ref{fig:sim_comparison}, for each method, we visualize a heatmap with the x-axis representing shape similarities (\shapesim) and the y-axis representing graph similarities (\graphsim).
%
Each grid in this heatmap shows the percentage of molecules that have shape and graph similarities within specific ranges, and the grid color represents the scale of the percentage (e.g., a darker color indicates a higher percentage).
%
In each heatmap, the vertical black line marks the average of shape similarities, and the horizontal black line marks the average of graph similarities.
%
%Figure~\ref{fig:sim_comparison} presents the heatmaps in which the x-axis represents shape similarity (\shapesim) and the y-axis represents graph similarity (\graphsim).

%\bo{
%@Ziqi How about the following points?
%}

%\bo{
%As shown in Figure~\ref{fig:sim_comparison},  \method and \methodwithsandpguide substantially outperforms \squid in condition following.
%
%Particularly, all the molecules generated from \method and \methodwithsandpguide have similar shape (i.e., sim $>$ 0.5) with the condition molecules.
%
%In contrast, in \squid, XXX
%}

%\bo{
%From Figure~\ref{fig:sim_comparison}, we also observe that \method and \methodwithsandpguide enjoy strong capacities in generating molecules with novel graph structures.
%
%To be specific, XXX
%}

%\bo{
%It is also worth noting that the distribution in \method and \methodwithsguide is highly similar to \dataset, a search algorithm XXX.
%
%This results indicate that \method learns to effectively explore the space of desirable mols for the generation.
%}


Fig.~\ref{fig:sim_comparison} demonstrates the exceptional performance of \method and \methodwithsguide in generating molecules with high shape similarity to condition molecules.
%
Particularly, in terms of shape similarity, Fig.~\ref{fig:sim_comparison}(a) and~\ref{fig:sim_comparison}(b) show that \method and \methodwithsguide have 99.5\% and 100.0\% of generated molecules with \shapesim$>$ 0.6. 
%
In contrast, \squid with $\lambda$=0.3 and 1.0 generate 89.9\% and 86.3\% of molecules with \shapesim$>$ 0.6, respectively. %
%
It is worth noting that \squid could generate molecules that are dramatically different from condition molecules in terms of shapes and have \shapesim$<$ 0.2, while all the generated molecules from \method and \methodwithsguide have considerably similar shapes to the conditions.
%
%
%These results demonstrate that \method and \methodwithsguide ensure better shape following compared to \squid. }

{Both \method and \squid develop specific strategies to enhance the shape similarity.
%
Particularly, \method incorporates shape guidance (i.e., \methodwithsguide) to iteratively modify the generated 3D structures to better resemble the known ligand shapes.
%
On the other hand, \squid leverages a balance variable $\lambda$ to control the interpolation level during generation. 
%
A lower $\lambda$ indicates stronger interpolation, and thus, better shape similarity but worse graph similarity.
%
According to Fig.~\ref{fig:sim_comparison}, the shape guidance in \methodwithsguide effectively boosts the shape similarities of generated molecules without degrading their graph similarities.
%
Specifically, Fig.~\ref{fig:sim_comparison}(a) and~\ref{fig:sim_comparison}(b) show that compared to \method, \methodwithsguide achieves a higher average shape similarity (0.824 for \methodwithsguide vs 0.771 for \method) and comparable average graph similarity (0.230 for \methodwithsguide and 0.229 for \method).
%
However, we observe a different trend for \squid in Fig.~\ref{fig:sim_comparison}(c) and~\ref{fig:sim_comparison}(d): by decreasing $\lambda$ from 1.0 to 0.3, there exists a trade-off between shape similarity and graph similarity.
%
To be specific, \squid ($\lambda$=0.3) achieves superior average shape similarity (0.740) while inferior average graph similarity (0.349), compared to \squid ($\lambda$=1.0) (0.699 for average shape similarity and 0.235 for average graph similarity). %
%
%However, this increase in shape similarity come with a substantial increase in graph similarity, with the average increasing from 0.235 to 0.349.
%
%This demonstrates that the balance variable $\lambda$ introduces into \squid a trade-off between the shape similarity and the graph similarity.
%
%In contrast, the shape guidance strategy in \methodwithsguide enhances the shape similarities of generated molecules while retaining the low graph similarities.
%However, for \squid, when $\lambda$=0.3, Fig.~\ref{fig:sim_comparison}(c) shows that their generated molecules tend to have high shape similarity and relatively high graph similarity; when $\lambda$=1.0, it achieves high density in the region with relatively lower shape similarity and low graph similarity.
%
%The suboptimal performance of \squid could be attributed to its shape representation, which may not be able to dissociate the shape conditions from their graph structures effectively. %condition molecules with their corresponding shapes.
%
%Different from \squid, \method and \methodwithsguide can achieve better trade-offs between shape and graph similarity of generated molecules, possibly due to their usage of a unique type of shape representations detached from the graph structures of condition molecules.
%
These results suggest that compared to adjusting the interpolation level ($\lambda$) as in \squid, including shape guidance could more effectively enhance shape similarities of generated molecules without compromising graph similarities. }

\begin{comment}
\ziqi{\method and \methodwithsguide capture similar graph similarity distributions with \dataset.
%
Fig.~\ref{fig:sim_comparison}(a) and~\ref{fig:sim_comparison}(b) highlight that \method and \methodwithsguide achieve graph similarities of generated molecules centered around 0.23.
%
Similarly, Fig.~\ref{fig:sim_comparison}(e) shows that \dataset, which screens over the training molecules, also has graph similarities around 0.23.
%
This demonstrates that in the MOSES dataset, molecules with similar shapes also have low graph similarities with each other, and thus, distinct graph structures.
%
\method and \methodwithsguide effectively capture the distinct graph structures of molecules with similar shapes in the MOSES dataset and generate molecules with desirable shapes and graph structures distinct from condition molecules. 
}

Fig.~\ref{fig:sim_comparison} demonstrates the %\hl{superiority} 
exceptional performance
of \method and \methodwithsguide in consistently generating molecules with high shape similarity and low graph similarity.
%
Particularly, Fig.~{\ref{fig:sim_comparison}}(a) and {\ref{fig:sim_comparison}}(b) demonstrate the ability of \method and \methodwithsguide to generate molecules with high shape similarity and low graph similarity, represented by high densities in the bottom right corner of heatmap regions.
%
%\bo{
%I really cannot understand this sentence...
%should not use pronoun here ("their")
%}
%
In comparison to \squid with $\lambda$=0.3 (Fig.~\ref{fig:sim_comparison}(c)) and $\lambda$=1.0 (Fig.~\ref{fig:sim_comparison}(d)), \method and \methodwithsguide substantially outperform \squid in generating molecules with high shape similarities and low graph similarities.
%
To be specific, all molecules generated by \method and \methodwithsguide have high shape similarity (i.e., \shapesim$>$0.6) to the condition molecules, while those from \squid with $\lambda$=0.3 or 1.0 have shape similarities as low as 0.2. 
%
\ziqi{This demonstrates the effectiveness of \method and \methodwithsguide in generating molecules closely following the shape condition.}

%This demonstrates that \method and \methodwithsguide allow generated molecules to more closely follow the shape condition than \squid.
%
\method and \methodwithsguide also consistently maintain low graph similarity (\graphsim$<$0.6) to the condition molecules.
%
In contrast, \squid, particularly with $\lambda$ = 0.3, is prone to generate molecules %\hl{with similar graph structures to} 
\bo{that is similar to condition molecules in terms of the graph structures. }% \hl{({\graphsim} up to 1.0)}.
%
%\bo{
%XXX\% of generated molecules have \graphsim $>$ XXX
%}
\ziqi{14.20\% of generated molecules have \graphsim $>$ 0.5. }
%
%The lower graph similarity of \method and \methodwithsguide can be attributed to their representation of shape condition, which enable them to detach the shape of condition molecules from their graph structures.
%
It is also worth noting that the distribution in \method and \methodwithsguide is highly similar to \dataset, a search algorithm 
\ziqi{over the training set with diverse molecules. }
\bo{
	@Ziqi It is not the point. The point is \dataset search over realistic/existing molecules
}
\bo{over exiting molecules.}
%\bo{\st{over the training set with diverse molecules}}.
%%
%\bo{
%that outputs top-X molecules of the highest XXX to the given condition. 
%}
%
\st{However, while {\dataset} is limited to searching over existing molecules, {\method} and {\methodwithsguide} enable the discovery of novel molecules through generation. }
%
\bo{
	It is not logically smooth here...
}
%
This result indicates that %\method and \methodwithsguide learn to effectively explore the space of molecules with desirable shapes \ziqi{and graph structures distinct from condition molecules. } %for the generation of 
%promising drug candidates with 
%\ziqi{
%Different from \dataset, ...novel structures. 
%}
\ziqi{
	\method and \methodwithsguide learn to effectively explore the space and generate molecules with desirable shapes and novel graph structures. %distinct from the condition molecules.
	%
	%This generative ability allows \method and \methodwithsguide to facilitate the discovery of novel molecules through generation.
	%
	%Thus, it sets \method and \methodwithsguide apart from the search-based method \dataset limited by the existing molecules in the dataset.
}
\bo{
	@Ziqi it is the point of comparison with \squid but not the comparison with \dataset right? You may want to move it to the front and write another point regarding generating realistic molecules or explore in the real molecule space.
}
%
%However, \method and \methodwithsguide generate a notably higher percentage of molecules in high shape similarity regions than \dataset (e.g., 4.67\% and 23.07\% molecules with shape similarity above 0.86, compared to 0.76\% for \dataset).
%

Fig.~\ref{fig:sim_comparison} demonstrates the superiority of \method and \methodwithsguide in consistently generating molecules with high shape similarities and low graph similarities.
%
Particularly, Fig.~\ref{fig:sim_comparison}(a) and \ref{fig:sim_comparison}(b) show that \method and \methodwithsguide achieve a high concentration of generated molecules in heatmap regions with high shape similarities and low graph similarities (i.e., bottom right corner).
%
In contrast, Fig.~\ref{fig:sim_comparison}(c) and \ref{fig:sim_comparison}(d) shows that \squid with $\lambda$=0.3 or 1.0 achieves a higher variance than \method and \methodwithsguide in both shape and graph similarities, thereby demonstrating their inconsistency in generating molecules with desired similarities.
%
Fig.~\ref{fig:sim_comparison}(e) shows that \dataset, as a search algorithm over the training set, also achieves a concentration in the desired heatmap regions.
%
However, \method and \methodwithsguide generate a notably higher percentage of molecules in high shape similarity regions than \dataset (e.g., 4.67\% and 23.07\% molecules with shape similarity above 0.86, compared to 0.76\% for \dataset).
%
Furthermore, comparing Fig.~\ref{fig:sim_comparison}(d) and \ref{fig:sim_comparison}(e) with Fig.~\ref{fig:sim_comparison}(a), the vertical red lines show that both \method and \methodwithsguide achieve higher average shape similarities than the best baseline \dataset, while maintaining comparable graph similarities.
% 
This capacity of \method and \methodwithsguide in consistently generating novel molecules with high shape similarities can be crucial in drug discovery, as it effectively navigates the search space of potential drug molecules toward potentially effective and unexplored molecules.
\end{comment}


%----------------------------------------------------------------------------------------------------------------------
\subsection*{{Case Study for SMG}}
%----------------------------------------------------------------------------------------------------------------------

%\ziqi{outline of this section:}

Fig.~\ref{fig:example_shape2} presents three generated molecules from \dataset, \squid with $\lambda$=0.3 and \methodwithsguide given the same condition molecule.
%
Each molecule has the highest shape similarity among the 50 candidates generated by each method.
%
As shown in Fig.~\ref{fig:example_shape2}, the molecule generated by \methodwithsguide has higher shape similarity (0.883) with the condition molecule than those 
from the baseline methods (0.768 for \dataset and 0.759 for \squid with $\lambda$=0.3).
%
Particularly, the molecule from \methodwithsguide has the {surface shape} 
(represented as blue shade in Fig.~\ref{fig:ours}) most 
similar to that of the condition molecule. 
%
On the contrary, the molecules generated from \dataset and \squid with $\lambda$=0.3 show noticeable misalignments when compared to the condition molecule. 
%
This comparison demonstrates the superior ability of \methodwithsguide to generate molecules with highly similar 3D shapes to the condition molecule.
%
In terms of graph similarities, all these generated molecules have low graph similarities with the condition molecule. 
%
The ability to generate molecules that have similar 3D shapes yet different molecular graphs demonstrates the potential high utility of \methodwithsguide in facilitating the drug development.
%
%As a result, \method demonstrates the potentially high utility of \method in LBDD. 
%\bo{@Ziqi it is updated or not?}
%
%These examples clearly show the ability of \method to generate molecules that are similar in 3D shape with the input but divergent in molecule structures.

\begin{comment}
\begin{figure}[!h]
	%
	\centering
	%
	\begin{subfigure}[b]{.48\linewidth}
		\centering
		\begin{subfigure}[b]{.45\linewidth}
			\centering
			\includegraphics[width=0.75\linewidth]{figures/774/774_reference_mol.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.45\linewidth}
			\centering
			\includegraphics[width=.68\linewidth]{figures/774/reference_774.pdf}
		\end{subfigure}
		%
		\captionsetup{justification=centering}
		\caption{condition molecule \molx, \qed = XXX}
	\end{subfigure}
	%
	\begin{subfigure}[b]{.48\linewidth}
		\centering
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/774_vs_mol.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/774_vs_mol_overlap.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/vs_774.pdf}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{\moly from \dataset: \shapesim = XXX, \graphsim = XXX, \qed = XXX}
	\end{subfigure}
	\\
	%
	\begin{subfigure}[b]{.48\linewidth}
		\centering
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/774_squid_mol.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/774_squid_mol_overlap.png}
		\end{subfigure}
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/squid_774.pdf}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{\moly from \squid: \shapesim = XXX, \graphsim = XXX, \qed = XXX}
	\end{subfigure}
	%
	\begin{subfigure}[b]{.48\linewidth}
		\centering
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/774_generate_mol.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/774_generate_mol_overlap.png}
		\end{subfigure}
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/774/shapemol_774.pdf}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{\moly from \method: \shapesim = XXX, \graphsim = XXX, \qed = XXX}
		\label{fig:ours}
	\end{subfigure}
	%\caption{Generated 3D Molecules from Different Methods. 
	%Molecule 3D shapes are in shades; generated molecules are superpositioned with the condition molecule; and the molecular graphs of generated molecules are presented.}
	\caption{\bo{New example}}
	\label{fig:example_shape1}
\end{figure}
\end{comment}

\begin{figure}[!h]
	%
	\centering
	%
	\begin{subfigure}[b]{.48\linewidth}
		\centering
		\begin{subfigure}[b]{.45\linewidth}
			\centering
			\includegraphics[width=0.75\linewidth]{figures/165/165_reference_mol.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.45\linewidth}
			\centering
			\includegraphics[width=.68\linewidth]{figures/165/reference_165.pdf}
		\end{subfigure}
		%
		\captionsetup{justification=centering}
		\caption{Condition Molecule}%, \qed = 0.637}
	\end{subfigure}
	%
	\begin{subfigure}[b]{.48\linewidth}
		\centering
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/165_vs_mol.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/165_vs_mol_overlap.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/vs_165.pdf}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{Molecule from \dataset: \shapesim = 0.768, \graphsim = 0.237} %, \qed = 0.834}
	\end{subfigure}
	\\
	%
	\begin{subfigure}[b]{.48\linewidth}
		\centering
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/165_squid_mol.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/165_squid_mol_overlap.png}
		\end{subfigure}
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/squid_165.pdf}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{Molecule from \squid with $\lambda$=0.3: \shapesim = 0.759, \graphsim = 0.280}%, \qed = 0.692}
	\end{subfigure}
	%
	\begin{subfigure}[b]{.48\linewidth}
		\centering
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/165_generate_mol.png}
		\end{subfigure}
		%
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/165_generate_mol_overlap.png}
		\end{subfigure}
		\begin{subfigure}[b]{.32\linewidth}
			\centering
			\includegraphics[width=\linewidth]{figures/165/shapemol_165.pdf}
		\end{subfigure}
		\captionsetup{justification=centering}
		\caption{Molecule from \methodwithsguide: \shapesim = 0.883, \graphsim = 0.317}%, \qed = 0.756}
		\label{fig:ours}
	\end{subfigure}
	%\caption{Generated 3D Molecules from Different Methods. 
	%Molecule 3D shapes are in shades; generated molecules are superpositioned with the condition molecule; and the molecular graphs of generated molecules are presented.}
	\caption{\textbf{Generated 3D Molecules from Different Methods. } Molecule 3D shapes are in shades; generated molecules are superpositioned with the condition molecule; and the molecular graphs of generated molecules are presented.}
	\label{fig:example_shape2}
\end{figure}

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection*{Overall Comparison for PMG}
%-------------------------------------------------------------------------------------------------------------------------------------

%\ziqi{Outline of this section:
%	\begin{itemize}
%		\item Though \method is not specifically designed for pocket-based drug design, \method achieves comparable performance with state-of-the-art baselines.
%		\item Another possible point could be: \method can generate molecules with higher binding affinities than \decompdiffref, even though \decompdiffref also leverages known ligands as priors to generate molecules. This demonstrates that using shape conditions of known ligands to generate molecules with desired shapes is more effective than adding them as priors as in the \decompdiffref.
%	\end{itemize}
%}


%\input{tables/time_complexity}
%\label{tbl:overall_results_docking}

%\input{tables/overall_results_docking2}
\input{tables/overall_results_docking3}
%\label{tbl:overall_results_docking}

%\bo{
%@Ziqi you need an opening...
%Besides shape similarity, graph similarities, and molecule quality, 
%we also evaluate the binding affinities of molecules generated from XXX.
%
%In this evaluation, we compare XXX with state-of-the-art XXX baselines as detailed in Section XXX.
%
%Particularly, @Ziqi introduce the experimental setting here. What's the difference between the condition molecule here and that used in the previous experiments.
%}

%\bo{
%@Ziqi from this section, you change the evaluation completely to the pocket-based method.
%
%So you may want to remind what's the main difference between shape and pocket-based generative model.
%
%Why do you use new baselines here and what metrics are important in pocket-based evaluation.
%
%You may also want to highlight that you do not train on pockets.
%
%It is an inductive setting.
%
%From this perspective, the current opening might not be strong enough.
%
%}

%\bo{
%@Ziqi you may want to separate the discussion of table 5 and table 4 as table 5 is a strong point.
%
%For table 4, you may want to focus on generating promising mols.
%
%Then you introduce your measurement for promising and discuss the performance
%}

%\bo{
%@Ziqi
%Here you do not only care about binding affinity right. You also evaluate on the quality.
%}

\begin{comment}
\bo{
From this section, we change our focus from ligan-based drug design to pocket-based drug design, and evaluate the significance of \method against state-of-the-art XXX baselines (see Section “Baselines” for details).
%
Note that different from baselines that need XXX data fro training, \method XXX
%
In this section, we evaluate \method and baselines in both effectiveness and efficiency.
%
Following XXX, in terms of the effectiveness, we evaluate the binding affinity, drug-likeness, diversity and success rate of molecules generated from \method and all the baselines.
%
Please refer to XXX for a detailed description for the evaluation metrics.
%
Regarding efficiency, we report the inference time of all methods used to generate molecules.
%
@Ziqi you may also want to remind what are \methodwithsandpguide and \methodwithpguide here.
}
\end{comment}

%\ziqi{Justify why not use desirable molecules to evaluate PMG. }

From this section, we shift our focus from evaluating against SMG baselines to PMG baselines, methods that leverage protein pockets for binding molecule generation (see Section ``Related Work'' for details).
%
We evaluate the effectiveness of \method against state-of-the-art PMG baselines (see Section “Baselines” for details) in generating molecules binding towards specific protein pockets.
%
All the baselines require protein-ligand complex data for training and generate molecules by explicitly modeling their interactions with protein binding pockets.
%
Different from these baselines, \method does not require complex data and consumes molecules for training. % on a large-scale molecule dataset, MOSES, without using protein-ligand complex data.
%
%\hl{When evaluating {\method} for binding molecule generation, it not only generates molecules with shapes similar to ligands in test protein-ligand complexes but also employs a pocket guidance approach to refine these molecules for better binding affinities.}
%
{Note that protein-ligand complex data is expensive and thus highly limited.
%
In contrast, there exist several high-quality and large-scale molecule databases~\cite{zinc22,mose2020}. 
%
By consuming molecule data for training, \method could fully leverage the rich data for better generation.}
%
{%When evaluating {\method} for binding molecule generation, it generates molecules with shapes similar to ligands in test protein-ligand complexes.
%
%In addition, 
\method further enhances the binding molecule generation by incorporating pocket guidance as detailed in Section ``\method with Pocket Guidance''.}
%\bo{
%@Ziqi you may want to rephrase this sentence. It is long and hard to follow...
%}
%
%\ziqi{large-scale training}
%This strategy \hl{enables} \method to transfer knowledge learned from \hl{a general molecule dataset} 
%\bo{
%@Ziqi XXX tasks
%}
%to the binding molecule generation task, \hl{enabling} the generation of binding molecules without the direct modeling of protein pockets.
%


We utilize two variants of \method for evaluation: \method with pocket guidance (\methodwithpguide) and \method with both pocket and shape guidance (\methodwithsandpguide).
%
%\bo{
%@Ziqi you may want to clarify what shape you used here.
%}
%
In this section, we evaluate \methodwithpguide, \methodwithsandpguide and PMG baselines in both effectiveness and efficiency.
%
Following previous work~\cite{guan2023targetdiff, guan2023decompdiff}, in terms of the effectiveness, we evaluate the binding affinity, drug-likeness, and diversity %and success rate 
of molecules generated from  \methodwithpguide, \methodwithsandpguide and all PMG baselines.
%
Please refer to Section ``Evaluation Metrics'' for a detailed description for the evaluation metrics.
%
Regarding efficiency, we report the inference time of all methods used to generate molecules.
%
%{Note that different from SMG for LBDD, when protein's structure is available, we can directly evaluate molecules using binding affinities. 
%
%Thus, we do not filter out molecules based on shape or graph similarities as in the evaluation for SMG. }
%\ziqi{Note that for PMG, we do no evaluate desirable molecules as in the comparison for SMG in Section ``Overall Comparison on Desirable Molecules''.
%
%Different from SMG, which focuses on generating molecules with similar shapes to condition molecules, when the protein target PMG focuses on generating molecules binding towards protein targets.
%
%Therefore, we evaluate the binding affinities of generated molecules from \methodwithpguide and \methodwithsandpguide, instead of their shape similarities with test ligands. }

\begin{comment}
Besides shape similarity, graph similarities, and molecule quality, we evaluate the binding affinities of molecules generated from \method.
%
Binding affinity is a crucial metric as it indicates the strength of interaction between a molecule and a protein target. 
%
Molecules with high binding affinity are more likely to be promising drug candidates.
%
To demonstrate the ability of \method to generate molecules with high binding affinities, we compare it with the state-of-the-art baselines for the generation of binding molecules (see Section ``Baselines'' for details).
%
All these baselines generate binding molecules by explicitly modeling their interactions to protein binding pockets (i.e., pocket-based molecule generation).
%
In contrast, \methodwithpguide and \methodwithsandpguide achieve this without directly modeling protein pockets, but instead employ a unique pocket guidance approach as will be discussed in Section ``\method with Protein Pocket Guidance''.
%
%Note that we exclude \decompdiff with protein pocket prior from the comparisons, and only include \decompdiff with reference ligand prior.
%
%This is due to the fact that we observe that \decompdiff with protein pocket prior could achieve substantially lower molecule qualities in the percentage of valid molecules, QED, and SA than the other methods.
%
%More details about \decompdiff with protein pocket prior will be discussed in Supplementary Section~\ref{XXX}.
%
This approach leads to the substantial superior efficiency of \methodwithpguide and \methodwithsandpguide than all the baselines. % as shown in Table~\ref{tbl:time_complexity}.
%
Particularly, they generate 100 molecules in 48 and 58 seconds on average, respectively, at least 21 times faster than the most efficient baseline \targetdiff (1,252s).
%
Other baselines, including \AR, \pockettwomol and \decompdiff, take substantially longer time (7,785, 2,544 and 1,859 seconds, respectively) to generate 100 molecules on average.
%
%The efficiency of \methodwithpguide and \methodwithsandpguide stems from their design to not explicitly model protein pockets and their interactions with ligands as all the baselines do.
%
Consequently, \methodwithpguide and \methodwithsandpguide can generate a substantially larger pool of molecules within a given time, thereby facilitating the selection of higher-quality molecules with stronger binding affinities. 

\bo{
%
We notice that 
by XXX, \method shows remarkable efficiency over baselines.
%
Specifically, XXX
%
The superior efficiency enables \method to generate more than 10x molecules than baselines in the same time period.
%
To ensure a fair evaluation, 
following XXX (@Ziqi, cite the literature you mentioned), 
in the evaluate, for baselines, we XXX.
%
For \method, we XXX.
%
We report the performance of all methods in Table XXX.
}
\end{comment}

We notice that \methodwithpguide and \methodwithsandpguide show remarkable efficiency over baselines by using pocket guidance instead of directly modeling these pockets.
%
Specifically, {\methodwithpguide and \methodwithsandpguide}
generate 100 molecules in 48 and 58 seconds on average, respectively, while the most efficient baseline \targetdiff takes 1,252 seconds.
%
The superior efficiency enables \methodwithpguide and \methodwithsandpguide to generate more than 10x molecules than baselines in the same time duration.
%
Therefore, following Long \etal~\cite{long2022zero}, for baselines, we apply them to generate 100 molecules for each test protein target.
% 
For \method, we generate 1,000 molecules and select the top 100 molecules for comparison based on their Vina S, QED, and SA scores.
%
We report the performance of all methods in Table~\ref{tbl:overall_results_docking2}.
%
Additionally, for a more comprehensive comparison, we present the results of \methodwithpguide and \methodwithsandpguide when generating 100 molecules %\hl{based on their 100 generated molecules without further selection} \bo{@Ziqi how about "when generating 100 molecules as baselines"} 
in the Supplementary Section~\ref{supp:app:results_PMG}.

\begin{comment}
We evaluate whether \methodwithpguide and \methodwithsandpguide can generate more high-quality molecules within a limited timeframe compared to baselines. 
%
Particularly, for each protein target, we use them to generate 1,000 molecules and select the top 100 molecules based on Vina scores and molecule quality for comparison against corresponding molecules generated by baselines.
%
Please note that \methodwithpguide and \methodwithsandpguide maintain their substantial efficiency advantage in this process.
%
As shown in Table~\ref{tbl:overall_results_docking2}, they generate 1,000 molecules in 462 and 561 seconds on average, which is at least twice as fast as the most efficient baselines \targetdiff takes to generate 100 molecules.
\end{comment}

%\st{Table~{\ref{tbl:overall_results_docking2}} presents the overall comparison in terms of the binding affinities and quality between the top 100 molecules generated by {\method} and those from baseline methods.
%}
%
As shown in Table~\ref{tbl:overall_results_docking2},  \methodwithpguide and \methodwithsandpguide %\st{consistently} 
achieve the second-best and best performance in terms of the binding affinities of generated molecules.
%
Particularly, they demonstrate the second-best (-5.53 kcal/mol) and best (-5.81 kcal/mol) average Vina S, with 17.7\% and 13.2\% improvement over the best baseline \AR (-5.06 kcal/mol).
%
%\ziqi{The superior performance of \methodwithsandpguide and \methodwithpguide indicates their exceptional ability in generating 3D molecules that effectively bind with protein pockets. }
%
Similarly, for vina scores obtained from locally minimized poses (i.e., Vina M), they also achieve the second-best (-6.37 kcal/mol) and best (-6.50 kcal/mol) performance, 
outperforming the best baseline \targetdiff (-6.20 kcal/mol) by 11.5\% and 9.5\%.
%
%\ziqi{Note that the Vina M scores of the generated molecules after local minimization are better than the Vina S scores.
%
%This improvement highlights the potential of generated molecules to achieve enhanced binding interactions through slight structural modifications.}
%
Moreover, for Vina D, a score calculated from poses optimized by global search,  \methodwithpguide and \methodwithsandpguide again yield the second-best (-7.19 kcal/mol) and third-best performance (-7.16 kcal/mol) and only slightly underperform the best baseline \targetdiff (-7.37 kcal/mol). 
%
%\ziqi{The high Vina D scores demonstrate the ability of \method to generate effective molecules that can establish strong interactions with protein targets after docking simulations. }
%
These results demonstrate that even without explicitly training on protein-ligand complexes, 
\methodwithpguide and \methodwithsandpguide could still generate molecules with superior binding affinities towards protein targets in terms of Vina S, Vina M, and Vina D, compared to state-of-the-art PMG baselines.
%
Note that we consider Vina M and Vina D in evaluation, as it is a common practice in drug development to find more favorable binding poses of candidates through pose optimization~\cite{Ferreira2015}.

Table~\ref{tbl:overall_results_docking2} also shows that \methodwithpguide and \methodwithsandpguide are able to generate molecules with better binding affinities than condition molecules (i.e., known ligands).
%
Particularly, they achieve the best (79.92\%) and second-best (78.75\%) performance in terms of the average percentage of generated molecules with Vina D higher than those of known ligands (i.e., HA).
%
The superior performance in HA demonstrates the high utility of \methodwithpguide and \methodwithsandpguide in generating promising drug candidates with better binding affinities than known ligands.
%
%This ability enables \methodwithpguide and \methodwithsandpguide to serve as a promising tool in drug development.

Table~\ref{tbl:overall_results_docking2} further presents the superior performance of \methodwithpguide and \methodwithsandpguide in metrics related to \mbox{drug-likeness} {and diversity}. 
%\hl{molecule quality metrics}. 
%\bo{@Ziqi quality may not be accurate. How about drug-likeness}
%
%Apart from the binding affinities, \methodwithpguide and \methodwithsandpguide generate molecules with high qualities in terms of validity, QED, and SA \hl{values} \bo{do we need values here?}.
%
Particularly, {for drug-likeness}, they achieve the best (0.77) and second-best (0.76) QED scores, respectively, with 31.0\% and 29.3\% improvement over the best baseline \pockettwomol (0.58).
%
\methodwithpguide and \methodwithsandpguide also achieve the second (0.72) and third (0.70) SA scores, and only slightly fall behind %\bo{@Ziqi it is not a verb...} 
the best baseline \pockettwomol (0.76).
%
In terms of the {diversity} 
%\bo{
%@Ziqi is diversity related to drug-likeness?
%}
among generated molecules, \methodwithpguide and \methodwithsandpguide underperform the baselines.
%
The inferior diversity can be attributed to the design of \methodwithpguide and \methodwithsandpguide that generates molecules with similar shapes to the ligands. %, while baselines can generate molecules with various shapes binding to the pockets. 
%
This design allows \methodwithpguide and \methodwithsandpguide to generate molecules with desired drug-likeness while could slightly degrade the diversity among generated molecules. 


%\ziqi{When comparing \methodwithpguide and \methodwithsandpguide, Table~\ref{tbl:overall_results_docking2} shows that \methodwithsandpguide with shape guidance can generate molecules with higher binding affinities compared to \methodwithpguide.
%
%To be specific, for Vina S and Vina M, \methodwithsandpguide outperforms \methodwithpguide by 5.1\% and 2.0\%, respectively.
%
%\methodwithsandpguide (-7.16) slightly underperforms \methodwithpguide in terms of Vina D (-7.19).
%
%However, \methodwithsandpguide achieves slightly better performance in HA compared to \methodwithpguide, and thus, generates more molecules with higher Vina D than condition molecules. 
%
%In terms of drug-likeness and diversity, \methodwithsandpguide achieves very close performance with \methodwithpguide.
%
%These results indicate that shape guidance could encourage \methodwithpguide to generate molecules with more favorable binding poses, by  
%}

%\bo{
When comparing \methodwithpguide and \methodwithsandpguide, Table~\ref{tbl:overall_results_docking2} shows that overall, \methodwithsandpguide with shape guidance can generate molecules with higher binding affinities compared to \methodwithpguide.
%
To be specific, for Vina S and Vina M, \methodwithsandpguide outperforms \methodwithpguide by 5.1\% and 2.0\%, respectively.
%
At Vina D, the performance of \methodwithsandpguide and \methodwithpguide is highly comparable.
%
These results indicate that even with pocket guidance, including additional shape guidance could further enhance the generation of binding molecules.
%}

%learn from a large-scale molecule dataset that includes molecules with desired drug-likeness properties.
%
%\ziqi{In addition, this design also allows them not to be constrained by limited protein-ligand complexes, given the }
%
%\bo{
%@Ziqi I think you want to mention here the complex data is expensive and by conditioning on shape, you can enable training on a large-scale molecule corpus.
%}
%
%As a result, it enables \methodwithpguide and \methodwithsandpguide to generate molecules with better drug-likeless properties, {thereby making these molecules as promising drug candidates.}
%
%\bo{
%@Ziqi what does "them" refer to?
%}
%\hl{their design to generate molecules with shapes similar to reference ligands} \bo{@Ziqi where did you mention this in this section...}.
%
%\hl{However, when considering the combined metric of success rate (SR\%) which accounts for both quality and binding affinity, they again achieve the best (28.0) and second-best performance (26.7) among all the methods.}
%
%\bo{
%@Ziqi do you still want to include this metric? If yes, you may want to remind the audience the defination in the begining of this section.
%}
%
%These results demonstrate that \methodwithpguide and \methodwithsandpguide effectively \ziqi{generate molecules with }
%\hl{learn distributions of molecules from a large-scale molecule dataset and thus enable the generation of molecules with high quality.}
%
%\bo{
%It is the point for tbl 5. I think the point here should be focus on generating better drug candidate
%}
%
%\bo{Discuss metrics following the order in the table and rephrase the discussion as in the previous paragraphs}
%
%This demonstrates the ability of \methodwithpguide and \methodwithsandpguide to generate molecules with high binding affinities and qualities.
%\bo{
%	I think the point here is by large-scaling training on shape similarities, the model learns mol distribution and thus do not need specific training on XXX to XXX.
%}

\begin{comment}
\bo{
@Ziqi you do not need a paragraph for this... Just one sentence say "We present the results of XXX in the Supplementary XXX" in the beginning is fine. 
}
%
In addition to the above comparison focusing on top molecules, we further evaluate \method against other baselines without any selection.
%
This comparison assesses the overall performance of generated molecules from \method.
%
Particularly, we apply \methodwithpguide and \methodwithsandpguide to generate 100 molecules, respectively, and compare these generated molecules with those from other methods in binding affinities and quality metrics.
%
Table~\ref{tbl:overall_results_docking} demonstrates the comparison in terms of the binding affinities and quality between the 100 molecules generated by \methodwithpguide and \methodwithsandpguide and other baselines.
%
More details will be discussed in Supplementary Section~\ref{XXX}.


\input{tables/overall_results_docking}
%\label{tbl:overall_results_docking}

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection*{\ziqi{Simiarlity Comparison for Pocket-based Molecule Generation}}
%-------------------------------------------------------------------------------------------------------------------------------------

\ziqi{[considering whether this section should be either removed or moved to supplementary?]}

\input{tables/docking_results_similarity}
%\label{tbl:docking_results_similarity}

\ziqi{Outline of this section: same as in Table 1}

\bo{@Ziqi you may want to check my edits for the discussion in Table 1 first.
	%
	If the pocket if known, do you still care about the shape similarity in real applications?
}

\ziqi{Table~\ref{tbl:docking_results_similarity} presents the overall comparison on similarity-based metrics between \methodwithpguide, \methodwithsandpguide and other baselines under different graph similarity constraints  ($\delta_g$=1.0, 0.7, 0.5, 0.3), similar to Table~\ref{tbl:overall}. 
	%
	As shown in Table~\ref{tbl:docking_results_similarity}, regarding desirable molecules,  \methodwithsandpguide consistently outperforms all the baseline methods in the likelihood of generating desirable molecules (i.e., $\#d\%$).
	%
	For example, when $\delta_g$=1.0, at $\#d\%$, \methodwithsandpguide (45.2\%) demonstrates significant improvement of $21.2\%$ compared to the best baseline \decompdiff (37.3\%).
	%
	In terms of \diversity, \methodwithpguide and \methodwithsandpguide also achieve the second and the third best performance. 
	%
	Note that the best baseline \targetdiff in \diversity achieves the least percentage of desirable molecules (7.1\%), substantially lower than \methodwithpguide and \methodwithsandpguide.
	%
	This makes its diversity among desirable molecules incomparable with other methods. 
	%
	When $\delta_g$=0.7, 0.5, and 0.3, \methodwithsandpguide also establishes a significant improvement of 24.3\%, 27.8\%, and 31.1\% compared to the best baseline method \decompdiff.
	%
	It is also worth noting that the state-of-the-art baseline \decompdiff underperforms \methodwithpguide and \methodwithsandpguide in binding affinities as shown in Table~\ref{tbl:overall_results_docking}, even though it outperforms \methodwithpguide in \#d\%.
	%
	\methodwithpguide and \methodwithsandpguide also achieve the second and the third best performance in \diversity at $\delta_g$=0.7, 0.5, and 0.3. 
	%
	The superior performance of \methodwithpguide and \methodwithsandpguide in $\#d\%$ at small $\delta_g$ indicates their strong capacity in generating desirable molecules of novel graph structures, thereby facilitating the discovery of novel drug candidates.
	%
}

\ziqi{Apart from the desirable molecules, \methodwithpguide and \methodwithsandpguide also demonstrate outstanding performance in terms of the average shape similarities (\avgshapesim) and the average graph similarities (\avggraphsim).
	%
	Specifically, when $\delta_g$=1.0, \methodwithsandpguide achieves a significant 2.5\% improvement in \avgshapesim\ over the best baseline \decompdiff. 
	%
	In terms of \avggraphsim, \methodwithsandpguide also achieves higher performance than the baseline \decompdiff of the highest \avgshapesim (0.265 vs 0.282).
	%
	Please note that all the baseline methods except \decompdiff achieve substantially lower performance in \avgshapesim than \methodwithpguide and \methodwithsandpguide, even though these methods achieve higher \avggraphsim values.
	%
	This trend remains consistent when applying various similarity constraints (i.e., $\delta_g$) as shown in Table~\ref{tbl:overall_results_docking}.
}

\ziqi{Similarly, \methodwithpguide and \methodwithsandpguide also achieve superior performance in \maxshapesim and \maxgraphsim.
	%
	Specifically, when $\delta_g$=1.0, for \maxshapesim, \methodwithsandpguide achieves highly comparable performance in \maxshapesim\ compared to the best baseline \decompdiff (0.876 vs 0.878).
	%
	We also note that \methodwithsandpguide achieves lower \maxgraphsim\ than the \decompdiff with 23.0\% difference. 
	%
	When $\delta_g$ gets smaller from 0.7 to 0.3, \methodwithsandpguide maintains a high \maxshapesim value around 0.876, while the best baseline \decompdiff has \maxshapesim decreased from 0.878 to 0.854.
	%
	This demonstrates the superior ability of \methodwithsandpguide in generating molecules with similar shapes and novel structures.
	%
}

\ziqi{
	In terms of \#n\%, when $\delta_g$=1.0, the percentage of molecules with \graphsim below $\delta_g$ can be interpreted as the percentage of valid molecules among all the generated molecules. 
	%
	As shown in Table~\ref{tbl:docking_results_similarity}, \methodwithpguide and \methodwithsandpguide are able to generate 98.1\% and 97.8\% of valid molecules, slightly below the best baseline \pockettwomol (98.3\%). 
	%
	When $\delta_g$=0.7, 0.5, or 0.3, all the methods, including \methodwithpguide and \methodwithsandpguide, can consistently find a sufficient number of novel molecules that meet the graph similarity constraints.
	%
	The only exception is \decompdiff, which substantially underperforms all the other methods in \#n\%.
}
\end{comment}

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection*{Quality Comparison for PMG}
%-------------------------------------------------------------------------------------------------------------------------------------

%\bo{
%@Ziqi remove drug-likeness in the following discussion.
%}

%\st{Similar to Section ``Quality Comparison between Desirable Molecules Generated by {\method} and {\squid}''}, 
%
%\bo{
%@Ziqi the setting and molecules used in the evaluation are different right.
%}
%
In addition to binding affinities, drug-likeness, and diversity, we also evaluate the quality of molecules generated by \methodwithpguide, \methodwithsandpguide, and all the PMG baselines.
%
We assess the quality of these molecules across multiple dimensions, including stability, 3D structures, and 2D structures, using the same metrics as in Table~\ref{tbl:overall_results_quality_desired}.
%
To ensure a fair comparison, instead of using molecules from the MOSES dataset to calculate the JS divergence metrics as in Table~\ref{tbl:overall_results_quality_desired}, we use the known ligands from the baselines' training set (i.e., CrossDocked2020) to calculate JS divergences.  %by comparing the generated molecules against these ligands. 
%
We report the performance of all methods in terms of molecule quality in Table~\ref{tbl:overall_docking_results_quality_10}. 
%
%\bo{In this analysis, we use the molecules XXX (@Ziqi make it clear which generated molecules of baselines and \method are used in this evaluation).}
%
%\bo{
%@Ziqi you may also want to backup why not use the same set of molecules of \method from tbl 3 for this evaluation.
%}

%
Table~\ref{tbl:overall_docking_results_quality_10} shows that \methodwithpguide and  \methodwithsandpguide achieve higher or at least comparable performance with all baselines %\bo{why other?} baselines 
in most quality metrics.
%
Specifically, for stability, Table~\ref{tbl:overall_docking_results_quality_10} shows that \methodwithpguide and \methodwithsandpguide either achieve 
%\st{either} 
comparable performance or slightly fall behind the baselines in atom stability and molecule stability.
%
Particularly, \methodwithpguide achieves the second-best performance in atom stability and only slightly underperforms the best baseline \targetdiff (0.934 vs 0.949).
%
\methodwithpguide also achieves the best performance in molecule stability. 
%\bo{
%@Ziqi you used "mol stability: in table 3 and the discussion of table 3...
%}.
%
\methodwithsandpguide underperforms \methodwithpguide in both atom stability and molecule stability but still outperforms \pockettwomol and \AR in atom stability and \pockettwomol, \targetdiff, and \decompdiff in molecule stability.
%
%Please note that the stability values of all the methods in Table~\ref{tbl:overall_docking_results_quality_10} are lower than those in Table~\ref{tbl:overall_results_quality_desired}.
%
%This is due to the fact that molecules generated by \methodwithpguide and  \methodwithsandpguide and other pocket-based molecule generation methods tend to have more complicated structures than those in shape-based methods.
%
%This increases the difficulty of all the methods in Table~\ref{tbl:overall_docking_results_quality_10} to generate molecules with high atom and molecule stability.
%
%\todo{Discussions on differences between generated molecules for pocket-based and shape-based molecule generation methods will be discussed in Supplementary Section~\ref{XXX}.}
%
%<<<< HEAD
%The overall comparable performances achieved by  \methodwithpguide and  \methodwithsandpguide in stability metrics demonstrate their effectiveness in generating stable molecules with stable atoms.
%=======
%\st{The overall comparable performances achieved by  
%{\methodwithpguide} and  {\methodwithsandpguide} demonstrate their effectiveness in generating stable molecules with stable atoms.
%}
%\bo{
%These results demonstrate the effectiveness of \methodwithpguide and  \methodwithsandpguide in XXX.
%}
These results demonstrate the effectiveness of \methodwithpguide and  \methodwithsandpguide in generating binding molecules with high %\bo{\st{quality in}}
stability.

\begin{comment}
\bo{
@Ziqi do we still want the discussion on drug-likeness?
}
Beyond stability metrics, \methodwithpguide and  \methodwithsandpguide also achieve superior performance in drug-likeness metrics. 
%
For example, in terms of QED, \methodwithpguide and  \methodwithsandpguide establish the 31.0\% and 30.3\% improvement over the best baseline \pockettwomol.
%
They also achieve the second and third performance in SA, respectively. 
%
In terms of Lipinski, \methodwithpguide and  \methodwithsandpguide again achieve the best and second performance.
%
This indicates their ability to learn from large-scale datasets and generate molecules with desirable drug-like properties.
%
In contrast, all the baselines are constrained by the limited availability of existing protein-ligand complexes and their properties.
%
This highlights that \methodwithpguide and  \methodwithsandpguide have the potential to serve as a promising tool to better explore chemical space and generate binding molecules with desirable drug-like metrics.
%
\end{comment}

In terms of 3D structures, overall, both \methodwithpguide and  \methodwithsandpguide achieve similar performance %\st{in most metrics related to 3D structure} 
compared to the baselines.
%
%\bo{@Ziqi you may want to present the other metrics first. It does not look good that you just mentioned it's the best and then you say it's not...}
%
Particularly, \methodwithpguide and \methodwithsandpguide achieve the best (0.274) and second-best performance (0.278) in terms of JS. bond lengths.
%
%	
For JS. dihedral angles, they also outperform the best baseline \decompdiff by 8.9\% and 8.4\%, respectively. 
%
We also note that, in terms of RMSD, \methodwithpguide and \methodwithsandpguide underperform the best baseline \pockettwomol, and achieve very comparable performance (0.663 for \methodwithpguide and 0.675 for \methodwithsandpguide) with the second-best baseline \AR (0.656).
%
For JS. bond angles, both \methodwithpguide and \methodwithsandpguide again underperform the best baseline \decompdiff, and achieve the second and fourth performance among all the methods.
%
%They also substantially outperform two diffusion-based baselines \targetdiff and \decompdiff in terms of RMSD, despite not outperforming \AR and \pockettwomol.
%achieve the fourth and third performance in RMSD, respectively. %They also achieve either the best or second-best performance in the Jensen Shannon divergences of bond lengths, bond angles and dihedral angles.
%
%Moreover, in the divergences of dihedral angles, \methodwithpguide and  \methodwithsandpguide substantially outperform the best baseline \decompdiff with 15.6\% and 14.6\% improvement. 
%
%In terms of the divergences of bond angles, we also note that \methodwithpguide achieves the second-best performance (0.309) and only slightly underperforms the best baseline \decompdiff with 2.7\% difference.
%%
%\methodwithsandpguide also establishes very close performance with the second-best baseline \AR (0.333 vs 0.332).
%%
%For RMSD, both \methodwithpguide and  \methodwithsandpguide substantially outperform the other two diffusion-based baselines \targetdiff and \decompdiff with at least 12.4\% improvement.
%%
%However, they still underperform the best baseline \pockettwomol (0.480) and the second-best \AR (0.729) in RMSD.
%
The overall comparable performance of \methodwithpguide and  \methodwithsandpguide against the PMG methods in these metrics demonstrates their ability to generate molecules with realistic 3D structures.  %can be attributed to the use of a substantially larger molecule dataset MOSES for training.
%
%In contrast, all the baselines are restricted to learning from the limited protein-ligand complexes.
%
%This extensive dataset allows the \method to deeply understand the characteristics of 3D molecule structures, %\hl{rules of 3D molecule structures}
%\bo{
%@Ziqi it looks weird...
%}
%and thus generate molecules with better 3D structures. 
%
%In contrast, other protein-based molecule generation methods can only be trained on a limited number of existing protein-ligand complexes.
%
%This demonstrates the ability of \methodwithpguide and  \methodwithsandpguide to overcome the limited availability of protein-ligand complexes for better generation. %to transfer knowledge within extensive molecule datasets into the \ziqi{binding molecule generation task. }
%\bo{
%@Ziqi usually people say transfer knowledge from XXX task to XXX task.
%
%It could be weird to say transfer knowledge from a dataset to a task.
%}
%pocket-based molecule generation problem.

{For 2D structures, both \methodwithpguide and \methodwithsandpguide demonstrate comparable performance with the PMG baselines.
%
Specifically,  for JS. basic bond types, \methodwithpguide and \methodwithsandpguide achieve the second and third performance (0.061 and 0.080), and only slightly underperform the best baseline \pockettwomol (0.055).
%
For JS. \#rings, they also achieve the best and second performance among all the methods. 
%
Similarly, in terms of the number of intersecting rings, \methodwithsandpguide again achieves the best performance, while \methodwithpguide slightly underperforms \methodwithpguide by just one ring (6 vs 7).}
%
We also note that \methodwithpguide and \methodwithsandpguide underperform the best baseline \targetdiff in JS. \#bonds per atom.
%%
For JS. \#n-sized rings, they also underperform the best baseline \pockettwomol and achieve the second and third performance.
%
%\st{In terms of the number of intersecting rings, they again achieve the best and second-best performance among all the baselines. }
%
These results highlight that compared to the state-of-the-art PMG methods, \methodwithpguide and \methodwithsandpguide enjoy similar performance in generating molecules with realistic 2D structures. %\ziqi{bonds and }rings.
% leveraging knowledge from extensive molecule datasets for ligand generation and thus generate promising drug candidates with the most frequent rings and realistic ring distributions.
%
%\bo{
%@Ziqi it's not obvious for me to see this conclusion based on the results... and I do not really think there is knowledge transfer in terms of generating realistic
%molecules...
%You may want to say KT between different types or families of mols
%}


%\input{tables/overall_results_docking_average_target}

%\label{tbl:overall_quality}
\input{tables/overall_docking_results_quality1.0}
%\label{tbl:overall_quality}


%----------------------------------------------------------------------------------------------------------------------
\subsection*{Case Studies for Targets}
%----------------------------------------------------------------------------------------------------------------------



\method can generate binding molecules that serve as promising drug candidates.
%
%To demonstrate this ability, we highlight three molecules generated by \methodwithsandpguide for two crucial drug targets, cyclin-dependent kinase 6 (CDK6), which regulates cell cycle progression impacting tumor growth~\cite{Tadesse2015}, and neprilysin (NEP), which mitigates Alzheimer's disease progression by degrading amyloid-beta peptides~\cite{ElAmouri2008}.
%
To demonstrate this ability, we highlight three molecules generated by \methodwithsandpguide for two crucial drug targets, cyclin-dependent kinase 6 (CDK6) and neprilysin (NEP).
%
CDK6 plays a critical role in cell proliferation by regulating cell cycle progression.
%
Inhibiting CDK6 can disrupt the abnormal cell cycles of cancer cells, making it a valuable therapeutic target for cancers~\cite{Tadesse2015}.  
%
NEP can help prevent amyloid plaque formation associated with Alzheimer's disease, making it an important target for therapies to potentially slow the disease's progression~\cite{ElAmouri2008}. 
%
We use an existing protein-ligand complex for CDK6 (PDBID:4AUA), and an existing protein-ligand complex 
for NEP (PDBID:1R1H), respectively, from Protein Data Bank (PDB)~\cite{Burley2022} and generate 1,000
molecules for each of the targets. 
%
Both complexes are included in our test set for PMG.
%
For each target, we prioritize the best molecule based on their Vina S, QED~\cite{Bickerton2012}, SA~\cite{Ertl2009}, toxicity scores calculated by ICM~\cite{Neves2012} and absorption, distribution, metabolism, excretion, and toxicity (ADMET) metrics calculated by admetSAR 2.0~\cite{Yang2018admetsar}.
%\xia{Ziqi, need details on how to prioritize here!}
%
Figure~\ref{fig:case_example:cdk6} and Figure~\ref{fig:case_example:cdk6:2}  
present the top drug candidates for CDK6, and  Figure~\ref{fig:case_example:nep} presents the top drug candidate for NEP. 
%
Note that the 3D structures of these molecules and their binding poses are generated by \methodwithsandpguide 
without any post-processing such as energy minimization or docking.
%
%All the generated 3D structures are validated to be realistic compared against minimized structures from the Cartesian MMFF minimization algorithm~\cite{Halgren1996} with an RMSD of less than 2\r{A}.
All the generated 3D structures are validated to be realistic by their close match, with an RMSD of less than 2\r{A}, to minimized structures from the Cartesian MMFF minimization algorithm~\cite{Halgren1996}.

%
%==========================================================
\subsubsection*{Generated Molecules for CDK6}
%==========================================================


\begin{figure}[!h]
	%
	\centering
	%
	\begin{minipage}{0.4\linewidth}
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=0.7\linewidth]{results/figures_new/CDK6_candidate1.pdf}
			\caption{NL-001 for CDK6}
			\label{fig:case_example:cdk6:graph}
		\end{subfigure}
		%
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=0.7\linewidth]{results/figures_new/surface_CDK6_candidate1.png}
			\caption{Surface representation of NL-001 and CDK6 interaction}
			\label{fig:case_example:cdk6:2d}
		\end{subfigure}	
		%
	\end{minipage}
	%	
	\begin{minipage}{0.58\linewidth}	
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{results/figures_new/Interaction_CDK6_candidate1.png}
			\caption{Cartoon representation for NL-001 and CDK6 interaction}
			\label{fig:case_example:cdk6:3d}
		\end{subfigure}
		%
	\end{minipage}	
	\caption{Generated drug candidate NL-001 for CDK6}	
	\label{fig:case_example:cdk6}
\end{figure}	
%

Figure~\ref{fig:case_example:cdk6} shows a generated molecule,
%(SMILES: CN(C1NN=C2C=CCC=C2N=1)C(c1ccccn1)=O), 
referred to as NL-001, for CDK6 and its structures and binding 
interactions with the CDK6 binding pocket. Figure~\ref{fig:case_example:cdk6:graph} presents its molecular graph. 
%
%Note that the 3D structure of this molecule and its binding pose are directly generated by \methodwithsandpguide without any post-processing techniques like energy minimization or docking.
%
As shown in Figure~\ref{fig:case_example:cdk6:3d}, molecule NL-001 fits well within the CDK binding pocket and forms hydrophobic interactions with the surrounding residues, such as T107, D104, L152, A162, etc. 
The interactions are further illustrated in Figure~\ref{fig:case_example:cdk6:2d}. 
%
This effective binding results in a better Vina S of -6.817 kcal/mol for the generated molecule, compared to the Vina S (0.736 kcal/mol) of the 4AU ligand in the complex 4AUA. 
%
Local minimization and docking refinement can further improve the Vina score of this molecule to -7.251 kcal/mol (Vina M) 
and -8.319 kcal/mol (Vina D), respectively, outperforming the 4AU ligand (-5.939 kcal/mol for Vina M and -7.592 kcal/mol for Vina D). 


In addition to the binding activity, the molecule in Figure~\ref{fig:case_example:cdk6} also 
demonstrates favorable properties that are important for drug development, 
including drug-likeness, synthesizability, toxicity, and ADMET profiles.
%
This molecule meets the Lipinski rule of five criteria~\cite{Lipinski1997}, with a QED score of 0.834, higher than that of 4AU ligand (0.773). 
%
Its synthetic accessibility (SA) score of 0.720 suggests favorable synthesizability.
%
This molecule also has a low toxicity score (0.236). %and does not contain any unwanted or reactive function groups~\cite{Neves2012}.
%
To fully evaluate its potential as a drug candidate, we compare its ADMET profile with those of three FDA-approved CDK6 inhibitors, including Abemaciclib~\cite{Patnaik2016}, Palbociclib~\cite{Lu2015}, and Ribociclib~\cite{Tripathy2017}. 
The results show that our molecule has comparable or even better ADMET properties in metrics crucial for cancer 
drug development, compared to those approved drugs.
%
For example, same as the approved drugs, our molecule is predicted to be negative for carcinogenicity~\cite{Benigni2010} and nephrotoxicity~\cite{Soo2018}.
%
Notably, our molecule has a higher score than all the approved drugs in plasma protein binding, indicating its capacityto be distributed throughout the body and reach the target site.
%
Details about the properties of the generated molecule NL-001 for CDK6 are available in Supplementary Section~\ref{supp:app:results:properties}.


%

\begin{figure}[!h]
	%
	\centering
	%
	\begin{minipage}{0.4\linewidth}	
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=0.7\linewidth]{results/figures_new/CDK6_candidate2.pdf}
			\caption{NL-002 for CDK6}
			\label{fig:case_example:cdk6:2:graph}
		\end{subfigure}
		%
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=0.7\linewidth]{results/figures_new/surface_CDK6_candidate2.png}
			\caption{Surface representation of NL-002 and CDK6 interaction}
			\label{fig:case_example:cdk6:2d:2}
		\end{subfigure}
		%
	\end{minipage}
	\begin{minipage}{0.58\linewidth}
	\vspace{10pt}
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{results/figures_new/Interaction_CDK6_candidate2.png}
			\caption{Cartoon representation for NL-002 and CDK6 interaction}
			\label{fig:case_example:cdk6:3d:2}
		\end{subfigure}
	\end{minipage}			
	\caption{Generated drug candidate NL-002 for CDK6}	
	\label{fig:case_example:cdk6:2}
\end{figure}	
%
Figure~\ref{fig:case_example:cdk6:2} presents another promising drug candidate for CDK6 generated by \method, referred
to as NL-002. It has very similar properties to NL-001, with a Vina S score of -6.970 kcal/mol, a Vina M score of -7.605 kcal/mol, and a Vina D score of -8.986 kcal/mol, 
showing its strong binding affinity to CDK6. 
%
Notably, NL-002 has a low toxicity score (0.000) and does not have any known toxicity-inducing functional groups detected~\cite{Neves2012}.
%
NL-002 also has a very similar ADMET profile as NL-001, suggesting it could be another strong drug candidate for CDK6. 
%
Details about the properties of the generated molecule NL-002 for CDK6 are available in Supplementary Section~\ref{supp:app:results:properties}.

%==========================================================
\subsubsection*{Generated Molecule for NEP}
%==========================================================

\begin{figure}[!h]	
	\centering
	\begin{minipage}{0.4\linewidth}
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=0.7\linewidth]{results/figures_new/NEP_candidate1.pdf}
			\caption{NL-003 for NEP}
			\label{fig:case_example:nep:graph}
		\end{subfigure}
		%
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=0.7\linewidth]{results/figures_new/surface_NEP_candidate1.png}
			\caption{Surface representation for NL-003 and NEP interaction}
			\label{fig:case_example:nep:2d}
		\end{subfigure}
		\label{fig:case_example:nep}
	\end{minipage}
		%
	\begin{minipage}{0.58\linewidth}
	\vspace{30pt}			
		\begin{subfigure}[b]{\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{results/figures_new/Interaction_NEP_candidate1.png}
			\caption{Cartoon representation for NL-003 and NEP interaction}
			\label{fig:case_example:nep:3d}
		\end{subfigure}
	\end{minipage}
	\caption{Generated drug candidate NL-003 for NEP}
	\label{fig:case_example:nep}
\end{figure}

Figure~\ref{fig:case_example:nep} shows the generated molecule,
% (SMILES: COC1NC=C2C3C(CCC(CCO3)c3ccccn3)NC3C=CC=CC3=C2N=1), 
referred to NL-003, for NEP. Figure~\ref{fig:case_example:nep:graph} presents its molecular graph. 
%
%As shown in 
Figure~\ref{fig:case_example:nep:3d} shows how the molecule binds to the NEP ligand binding pocket through hydrogen bonds with residues W693 and E584 and hydrophobic interactions. 
%
Such interactions are further illustrated in Figure~\ref{fig:case_example:nep:2d}. 
%
This results in a lower Vina S (-11.953 kcal/mol) of this molecule than that of the BIR ligand in complex 1R1H (-9.399 kcal/mol). 
%
Through local minimization and docking refinement, this molecule yields lower Vina M (-12.165 kcal/mol) and Vina D (-12.308 kcal/mol) than the Vina M (-9.505 kcal/mol) and Vina D (-9.561 kcal/mol) of BIR ligand.

The molecule NL-003 in Figure~\ref{fig:case_example:nep} also has favorable properties in terms of drug-likeness, 
synthesizability, toxicity, and ADMET profiles.
%
Particularly, it meets Lipinski's rule of five and achieves a QED score of 0.772, which is substantially higher than that of BIR ligand (0.463).
%
It also demonstrates a favorable SA score of 0.570 for synthesizability.
%
%In terms of toxicity, t
This molecule is also predicted to be non-toxic and does not have any known toxicity-inducing functional groups detected~\cite{Neves2012}.
%
It also has a promising ADMET profile comparable to those of three approved drugs, Donepezil, Galantamine, and Rivastigmine, for Alzheimer’s disease~\cite{Hansen2008}, 
specifically in metrics crucial for Alzheimer’s disease drug development.
%
For example, this molecule is predicted to be permeable to the blood-brain barrier that is essential for treating Alzheimer's disease~\cite{Deane2007} and negative for carcinogenicity~\cite{Benigni2010}, same as the approved drugs.
%
Details about the properties of the generated molecule for NEP are available in Supplementary Section~\ref{supp:app:results:properties}.


%The above two examples demonstrate the effectiveness of \method in generating promising drug candidates.
%%
%Given protein-ligand complexes, \method can generate molecules that have higher binding affinities than existing ligands and favorable properties that are essential for drug development.
%%
%This ability highlights the potential of \method to facilitate the drug development process.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Discussions and Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------------------------------------
\subsection*{Integrating Protein Targets for Binding Molecule Generation}
%----------------------------------------------------------------------------------------------------------------------

For PMG, our experiments show that \method with pocket guidance (\methodwithpguide and \methodwithsandpguide) can effectively generate molecules with high binding affinities toward protein targets.
%
As detailed in Section ``\method with Protein Pocket Guidance'', this pocket guidance enables \methodwithpguide and \methodwithsandpguide to consider the geometric information of protein binding pockets when generating binding molecules.
%
%This facilitates the generation of molecules specifically tailored for optimal fit, thereby leading to high binding affinities. 
%
In addition to geometric information, we acknowledge that incorporating other information about protein pockets could further 
enable molecules generated in high quality. 
%
For example, the physicochemical properties of amino acid sequences within the binding pockets, 
such as polarity, electrostatics, and hydrophobicity, can affect the strength of interactions between 
proteins and molecules~\cite{Du2016}. 
%
Therefore, a generative model
considering these properties could produce molecules that better conform to what is expected based on pharmaceutical chemistry, 
shortening their pathways to be induced into downstream tasks of drug development. 
%
Identifying and integrating essential properties of protein binding pockets into molecule generation 
could be an interesting yet challenging future research direction.
%
%Ultimately, we would like to develop a comprehensive genAI framework that could fully leverage protein pocket information and facilitate the design of molecules tailored to have high binding affinities toward protein pockets. 

%----------------------------------------------------------------------------------------------------------------------
\subsection*{Multi-Objective Molecule Generation}
%----------------------------------------------------------------------------------------------------------------------


When developing a molecule into a drug, in addition to its binding affinity to the protein target, many 
other properties also need to be considered, such as drug-likeness, synthesizability, toxicity, metabolism, and cell permeability~\cite{Lin2003}.
%
Similar to other molecule generation methods~\cite{luo2021sbdd,peng22pocket2mol,schneuing2022structure,guan2023targetdiff}, \method primarily emphasizes molecule binding affinities.
%
Although the case study in Section ``Case Studies for Targets'' indicates that its generated binding molecules may have favorable ADMET profiles, properties beyond binding affinities and shapes are not specifically optimized by design 
in the molecules out of \method.  
%
Consequently, the generated molecules may need to go through further optimization and refinement to gain other 
necessary properties in order to become viable drug candidates.  
%
Towards this end, a multi-objective genAI model that generates molecules exhibiting  
multiple properties simultaneously and satisfying multiple objectives (e.g., high drug-likeness, high synthesizability) 
could be greatly demanded, which calls for a significant future research endeavor, though out of the scope of this study. 

%----------------------------------------------------------------------------------------------------------------------
\subsection*{\emph{In vitro} Validation}
%----------------------------------------------------------------------------------------------------------------------

\emph{In vitro} experimental validation is indispensable for accessing \emph{in silico} generated molecules 
for further investigation into real-world therapeutic agents. 
%
Even when all the desired properties could be ideally incorporated into the generative process, which, by itself, is 
highly nontrivial, these properties of the 
generated molecules remain unclear until they are experimentally confirmed. 
Meanwhile, other unanticipated properties may emerge due to the 
unknown interactions between the molecules and the complex biological systems, 
which also requires rigorous \emph{in vitro} testing. 
%
%While generative methods can accelerate the discovery of novel molecules with potential binding activities, they often simplify the complexities of protein-ligand interactions when generating molecules and ignore the broader interactions of these molecules with biological systems~\cite{xxx}.
%%
%As a result, laboratory experiments are indispensable to confirm the efficacy and safety of these molecules as drug candidates.
%%
%Such experiments verify the ability of generated molecules to bind with protein targets and identify any off-target effects or toxicity issues with generated molecules that may not be considered by generative methods.
%
Despite its crucial importance, systematic \emph{in vitro} validation for genAI generated molecules remains very challenging~\cite{Chen2023}. 
%
%systematically validating generated molecules is a resource-intensive and time-consuming process.
%
This process would start from effective sampling or prioritization of generated molecules to identify a feasible and manageable subset for \emph{in vitro} experiments.
%
Then, determining and executing viable synthesis reactions to make those molecules, if they do not exist, which is highly likely, 
also pose substantial difficulties~\cite{Gao2020}. 
%, and often requires the expertise of synthetic chemists to convert \emph{in silico} generated molecules into tangible candidates for laboratory testing.
%
%Following this step, the validation of these molecules as potential drug candidates through biological experiments is also challenging due to the complexity of biological systems and involves assessing multiple properties as we discussed earlier.
%%
Given the focus of this manuscript on developing \emph{in silico} genAI methods, \emph{in vitro} validation is beyond the scope
but remains a pivotal next step to investigate.
%
%In future work, we plan to collaborate with domain expertise to validate our generated molecules in the laboratory and translate our generative methods into real impact.


\begin{comment}
\ziqi{
To ensure the practical applicability of \emph{in silico} molecules from generative methods, experimental validation is indispensable.
%
However, this process can be time-consuming and costly. 
%
Typically, validating \emph{in silico} generated molecules involves two critical steps: (1) identifying synthetic pathways for generated molecules and producing them in a laboratory, and (2) evaluating their binding affinities to protein targets, as well as their properties such as ADMET. 
%
Both steps require extensive laboratory resources, skilled researchers and specialized equipment, which are usually not available in academic environments.
%
Consequently, most \emph{in silico} generative methods developed in academic environments face substantial challenges in conducting \emph{in vitro} experimental validation.
%
Despite these challenges, there have been notable successes that highlight the potential of generative methods to truly make new discoveries that can succeed in laboratory validation. 
%
For example, Insilico Medicine, an AI-driven pharmaceutical company, successfully used a deep generative model to discover potent inhibitors of the discoidin domain receptor 1 (DDR1), a target implicated in fibrosis and other diseases~\cite{Zhavoronkov2019}.
%
These discoveries are achieved in 21 days and subsequently confirmed through laboratory validation.
%
This example highlights the potential of generative methods for significantly reducing the time and cost of drug development.
%
Recognizing the extensive resources required for such validations, we plan to collaborate with pharmaceutical companies which have requisite expertise and laboratory capabilities in the future. 
%
Such collaborations will enable us to rigorously test our \emph{in silico} generated molecules via \emph{in vitro} validation, thereby facilitating their development into viable therapeutic molecules and translating our generative methods into real impact.
}

\ziqi{
For LBDD, following the previous work~\cite{adams2023equivariant}, we use shape similarities between generated molecules and condition molecules to evaluate the ability of \method and SMG methods in generating potentially binding molecules.
%
As widely demonstrated in the literature~\cite{Bostroem2006,Acharya2011}, these metrics can provide an efficient and effective assessment of the binding potential of generated molecules.
%
However, using shape similarities for evaluation still comes with limitations. 
%
These metrics do not account for the conformational flexibility of the ligand molecules when interacting with protein pockets.
%
Additionally, these metrics do not consider the interactions between protein pockets and generated molecules, which are essential for binding strength.
%
%While molecules with similar shapes tend to have similar binding activities~\cite{{Bostroem2006,Acharya2011}}, using shape similarities for evaluation comes with limitations. 
%
%This metric overlooks the interactions between generated molecules and protein targets, which play a critical role in determining a molecule's binding activity.
%
%Consequently, evaluating generated molecules with shape similarities may lead to misleading conclusions about a molecule's binding potential.
%
%Therefore, after generating molecules with similar shapes, it is still crucial to further validate the binding activities of these molecules toward protein targets.
}

\ziqi{
For SBDD, we use binding affinities predicted by a protein-ligand docking method Autodock Vina~\cite{Eberhardt2021} as primary evaluation metrics as in the previous SBDD methods~\cite{guan2023targetdiff, guan2023decompdiff}.
%
While these metrics consider interactions between protein pockets and generated molecules, existing docking algorithms could still suffer from limited accuracies.
%
This could be attributed to the assumptions of many docking methods, including Autodock Vina, that proteins are rigid structures, which neglects the dynamic nature of protein structures.
%
This assumption can lead to inaccurate binding affinity predictions, as protein flexibility is essential for protein-ligand interactions~\cite{Ravindranath2015}.
%
Even though some techniques have been proposed to partially consider proteins' flexibility, they often result in increased computational demands and could still produce inaccurate binding affinity predictions due to the complexity of protein structures~\cite{xxx}.}

\ziqi{Given these limitations, we acknowledge that it is crucial to experimentally assess the binding strength of generated molecules toward protein targets.
%
While experimental validation offers direct evidence of the efficacy of generated molecules, it is resource-intensive and often beyond the scope of research focused on developing new computational methods.
%
The need for synthesis and  \emph{in vitro} or \emph{in vivo} testing makes it extremely challenging for large-scale experimental validation of \emph{in silico} generated molecules.
%
Despite these challenges, experimental testing and collaborations with the pharmaceutical industry are highly needed to truly translate the computational methods into real impact.}

%We focus on in silico validation in this manuscript due to the resource limitation. We acknowledge that in vitro XXX is important XXX (some discussion). However, as demonstrated in XXX, in silico validation is an effective indicator for the in vitro results. Thus, we believe the promising results in in silico evaluation could serve as strong evidence for the high-utility of ShapeMol in drug development
\end{comment}

%----------------------------------------------------------------------------------------------------------------------
\subsection*{Conclusions}
%----------------------------------------------------------------------------------------------------------------------

\method generates novel binding molecules with realistic 3D structures based on the shapes of known ligands. 
%
It utilizes pre-trained shape embeddings and a customized diffusion model for binding molecule generation.
%
To better resemble the known ligand shapes, \method also modifies the generated 3D molecules iteratively under the guidance of the ligand shapes. 
%
Additionally, it can leverage the geometric information of protein binding pockets and tailor the generated molecules toward optimal binding affinities.
%
Experimental results demonstrate that \method outperforms SMG methods in generating molecules with highly similar shapes to known ligands, while incorporating shape guidance further boosts this performance.
%
When compared to PMG methods, \method with pocket guidance also achieves exceptional performance in generating molecules with high binding affinities.
%
The case studies involving two critical drug targets show that \method can generate binding molecules with desirable drug properties.
%
However, \method still has limitations.
%
In addition to the limitations and corresponding future research directions that have been discussed above, 
one limitation with {\method} is that the binding poses of generated molecules are typically constrained by those of known ligands.
%
This limitation can confine the ability of \method to explore novel binding poses.
%
Thus, a future research direction is on how to mitigate this limitation by inferring diverse ligand shapes from protein pockets.
%
%In addition to the limitations and corresponding future research directions that have been discussed above, another limitation with \method is that it cannot fully guarantee the generation of valid and realistic 3D molecule structures. 
%
%Some generated molecules may have unrealistic bond angles and unstable ring structures. 
%
%Therefore, there is still room for further improvement in the quality of generated 3D molecules.
%
%We are currently investigating how to add domain-specific knowledge, such as energy constraints, in the learning process of \method to facilitate the generation of stable 3D molecule structures with low energy.


\begin{comment}
\ziqi{
\method serves as a holistic framework for generating potentially binding molecules in both LBDD and SBDD. 
%
Based on a comparison against SMG methods in LBDD, \method and \methodwithsguide achieve the state-of-the-art performance in generating molecules with similar shapes to condition molecules (e.g., ligands), and thus, potentially similar binding activities.
%
Compared to PMG baselines in SBDD, \methodwithpguide and \methodwithsandpguide again demonstrate exceptional performance in generating molecules binding toward protein targets.
%
In addition to the limitations and corresponding future research directions that have been discussed above, another limitation with \method is that it cannot fully guarantee the generation of valid and realistic 3D molecule structures. 
%
Some generated molecules may have unrealistic bond angles and unstable ring structures. 
%
Therefore, there is still room for further improvement in the quality of generated 3D molecules.
%
We are currently investigating how to add domain-specific knowledge, such as energy constraints, in the learning process of \method to facilitate the generation of stable 3D molecule structures with low energy.
}


In this paper, we develop a novel generative model \method, which generates 3D molecules conditioned on the 3D shape of condition molecules.
%
\method utilizes a pre-trained equivariant shape encoder \SE to generate equivariant shape embeddings for shape conditions.
%
Based on the embeddings, \method learns an equivariant diffusion model \methoddiff to generate novel molecules with similar shapes to condition molecules.
%
%\method pre-trains an equivariant shape embedding module \SE to generate latent embeddings for 3D shapes of given molecules.
%
%\method also learns an equivariant diffusion model \methoddiff to generate novel molecules conditioned on the shape embeddings.
%
To boost the effectiveness of \method in generating molecules that closely follow the shape condition, we develop \methodwithsguide, 
which incorporates shape guidance to push the generated atom positions tailored to condition molecules.
%
We compare \method and \methodwithsguide against state-of-the-art LBDD baseline methods.
%
Our experimental results demonstrate that \method and \methodwithsguide generate novel and diverse molecules with higher shape similarities compared to the baselines.
%
When the target protein's binding pocket has structural data available, we also incorporate pocket guidance to enhan
%
competitive qualities compared to the baseline methods.
%
%Our parameter study shows the effectiveness of the shape guidance in generating molecules similarly to the given one regarding the 3D shape.
%
In future work, we will explore generating 3D molecules jointly conditioned on the shape 
and the electrostatic, considering that the electrostatic of molecules could also determine the binding activities of molecules.

%
%In addition to electrostatic, another interesting future direction could be introducing shape guidance into the framework 
%without affecting the molecule quality.}
\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\xia{the figure needs revision/need a new figure -- should include the protein-based shape adjustment in the figure. }

%\st{{\method} consists of an equivariant shape embedding module {\SE} that maps 3D molecular surface shapes of the condition molecules to latent embeddings, and an equivariant diffusion model {\methoddiff}
%that generates 3D molecules %(i.e., atom types and their coordinates) 
%conditioned on these embeddings.}
%
\method aims to generate novel binding molecules based on the shapes of known ligands, following the principle that molecules with similar shapes tend to have similar binding activities.
%\bo{Toward this end, \method includes a pre-trained equivariant shape embedding module {\SE} to learn expressive latent embeddings for 3D molecule shapes. %XXX (@Ziqi key works) latent embeddings for 3D molecule shapes.
%
%\method also incorporates an equivariant diffusion model {\methoddiff} that explicitly considers shape latent embeddings to generate 3D molecules. 
%XXX (@Ziqi your design) to generate XXX (@Ziqi some key words) molecules.
%
%
Toward this end, 
\method consists of two modules: (1) a pre-trained equivariant shape embedding module {\SE} that learns expressive latent embeddings for the shapes of condition molecules (e.g, ligands), and 
(2) an equivariant molecule diffusion model {\methoddiff} that explicitly considers shape embeddings from \SE to generate new 3D molecules with similar shapes to condition molecules.  %(i.e., atom types and their coordinates) 
%conditioned on these embeddings.
%
%\bo{Particularly, \SE XXX @Ziqi you may want to present the key idea here not details.  For example. "\SE learn latent embedding by reconstructing/predicting the distance"} 
Particularly, given a condition molecule, {\SE} represents its shape as a point cloud with points sampled over its molecular surface. %, which does not rely enables \SE to learn better representations of molecular shapes decoupled from their exact atomic structures.
%
{\SE} learns to map this point cloud into a latent embedding {\shapehiddenmat} using an encoder-decoder framework
(more details in Section ``Equivariant Condition Shape Representation Pre-training'').
%
Conditioned on the shape embedding \shapehiddenmat, \methoddiff  %\st{learns to gradually transform noisy molecules into}
learns to generate
molecules with desired shapes and realistic topologies
%represented by \shapehiddenmat 
in an equivariant way. %@Ziqi again key ideas not details. you may also want to highlight your key designs.}
%
%
%\st{Specifically, {\methoddiff} employs denoising diffusion probabilistic models~{\cite{ho2020ddpm}} for molecule generation.}
%
Particularly, \methoddiff utilizes equivariant graph neural networks to learn shape-aware atom embeddings and generate molecules tailored to the shape condition.
%expressive representations of geometric features in the molecules and facilitate the generation of molecules with geometric structures tailored to the desired shapes.
%
%To generate molecules with realistic topologies, %XXX (@Ziqi some properties) molecules, 
\methoddiff also leverages bond types as a training signal to fully capture the inherent topologies of molecules. %for generation of high-quality molecules.}
%
%\methoddiff additionally incorporates a predictor for bond types between atoms, which is explicitly optimized to predict ground-truth bond types accurately.
%
%\methoddiff integrates these bond-type predictions into the modeling of 3D molecules for a better understanding of their inherent topologies, which enables \methoddiff to generate molecules with realistic topologies.
%
%
%In the second module, {\methoddiff} gradually adds noises step by step to the atom positions and features in the molecules and learns to reverse this process by removing noises in the noisy molecules (Section ``Diffusion-based Molecule Generation ({\methoddiff})''). 
%
%In {\methoddiff}, a molecule predictor $\molpred(\pos_t, \atomfeat_t, \shapehiddenmat)$ is developed to predict atom positions and features given the noisy data $(\pos_t, \atomfeat_t)$ conditioned on $\shapehiddenmat$ (Section ``Molecule Representation Learning'').
%
During inference, 
%{\methoddiff} generates novel molecules by gradually removing noises from randomly sampled noisy molecules (Section ``Molecule Generation with Guidance'').
%
\methoddiff utilizes shape guidance to further direct the generated molecules toward the shape condition.
%\st{{\methoddiff} is capable of explicitly including shape guidance to 
%direct the generated molecules to better fit the shape of the condition molecule.}
%
Besides shape guidance, when the structure of the protein binding pocket is available, \methoddiff employs pocket guidance to adjust the atom positions of generated molecules for optimal binding affinities with the binding pocket.  
%\st{{\methoddiff} is also designed to accept pocket guidance
%when target protein pocket is known (i.e., pockets binding to the condition molecule).} % the structure of protein pocket binding to the condition molecule is known.
%
%Particularly, this pocket guidance adjusts atom positions to ensure that they maintain proper distances from protein atoms to avoid steric clashes.
%
This design enables the applicability of \method for PMG. %with known target protein pockets.}
%
%\bo{Besides shape guidance, 
%\methoddiff is also designed to accept pocket guidance.
%
%Particularly XXX
%
%This design ensures a high utility of \method in XXX-based drug discovery (i.e., the binding protein pocket's structure is known (@Ziqi check this))
%}
%
%\st{Furthermore, when the binding protein pocket's structure is known, {\methoddiff} can employ additional pocket guidance to minimize atomic clashes between generated molecules and the protein, which enables the molecules to better fit into the protein pocket and achieve higher binding affinities.}
%
%Specifically, XXX
%}
%In the denoising process, additional shape guidance can be applied to direct the generated molecules to better fit the shape of the condition molecule.
%
%When the condition molecule is a ligand and binds to a protein pocket with known structures, a protein pocket guidance can be applied to direct the generated molecules to fit into the given protein pocket and minimize clashes with the surrounding protein atoms.
%}
%
Fig.~\ref{fig:overall} presents the overall architecture of \method. 
%
All the algorithms are presented in Supplementary Section~\ref{supp:algorithms}.

{In the following sections, we will first introduce the key notations and the definitions of equivariance and invariance in Section ``Representations, Notations, and Preliminaries.''
%
We will then introduce the equivariant shape embedding module \SE in Section ``Equivariant Condition Shape Representation Pre-training. '' %(\SE).''
%	
After that, we will discuss the shape-conditioned molecule diffusion model \methoddiff in Section ``Diffusion-based Molecule Generation.''
%
We will describe the shape-conditioned molecule prediction module \molpred used in \methoddiff in Section ``Shape-conditioned Molecule Prediction.''
%
Finally, we will describe the molecule generation process and how the shape guidance and pocket guidance are used during inference in Section ``Guidance-induced Inference.''
}
%\todo{from Ziqi: @Bo, check this paragraph}

%
%\begin{comment}
%\ziqi{One of the key advantages of \method lies in its use of molecular surface shapes of condition molecules (e.g., ligands) to represent the shape condition.
%%
%These molecular surface shapes capture essential information for shape complementarity of ligands without relying on the exact ligand atom positions which could reveal the ligands' exact structures. 
%%
%Consequently, \method is not limited by the structures of existing known ligands and can generate molecules with novel molecular structures while retaining the desired shapes.
%% they are not constrained by the structures of condition molecules.
%%
%In contrast, previous work \squid represents shapes using points sampled around ligand atoms, which could unnecessarily incorporate information about the atom positions within the shapes and the ligands' structures, therefore limiting its capacity to discovery new molecules with desired shapes.
%%
%%This is different from the previous shape-conditioned molecule generation method \squid, which represents shapes of 3D molecules using point clouds with points sampled around all the atoms in molecules.
%}
%\end{comment}
%

\begin{comment}
\xia{the following summary should not be here. Should be in intro}
\ziqi{\method features the following key designs to enable state-of-the-art shape-conditioned molecule generation: 
%
\begin{itemize}
 \item Given a condition molecule $\molx$, \SE represents its shape using molecular surface, which does not rely on the exact atom positions of $\molx$ and thus avoids revealing its atomic structures. % instead of points near ligand atoms as in \squid.
 %
%<<<< HEAD
 %This allows \method to better decouple the shapes of molecules from their exact atomic structures and thus enable the discovery of novel structures with similar shapes.
 %\bo{This conclusion may not be intuitive. Why using surface shapes could enable the discovery of novel structures? Do we have results to support this?}
% \item \method uses geometric vector perceptron (GVP) blocks~\cite{jing2021learning} to capture geometric information in molecules, which provide strong flexibility in representing equivariant geometric features and enable accurate modeling of them. 
 %\item \method is designed to incorporate the learned shape representations from \SE 
%=======
 This allows \SE to learn better representations of molecular shapes decoupled from their exact atomic structures.
 %
 Such representations enable \methoddiff to generate molecules with similar shapes to $\molx$ but various atomic structures, and
 %
 therefore enable the discovery of new molecules with desired shapes.
 %
 \item \methoddiff captures geometric information in molecules using geometric vector perceptron (GVP) blocks~\cite{jing2021learning}, which provide strong flexibility in representing equivariant geometric features and enable accurate modeling of them. 
 %
 \item \methoddiff employs a new and effective strategy to incorporate the learned representations of shape conditions into \methoddiff, which enables \methoddiff to generate molecules with desired shapes.
 %
 \item \methoddiff develops a predictor for bond types between atoms and incorporates explicit supervision signals for its optimization. \methoddiff integrates these predictions into the modeling of 3D molecules for a better understanding of their topologies, which enables \methoddiff to generate molecules with realistic topologies.
 % 
 \item \methoddiff incorporates protein pocket guidance to adjust the atom positions within generated molecules to enable a better fit into a known pocket.
 %
 This enhances the compatibility of generated molecules with the known pocket structures.
 %
%>>>> a3dfcf984503513703c1c00f88e71969dc10f0c5
\end{itemize}
}
%

\todo{\textbf{from Ziqi: @Bo} can you check whether the above writing is clear or not? I rewrote the motivation about using molecular surface shapes.}
\bo{I do not really understand here... @Ziqi Can you fill all your contribution in the writing below?}

\bo{
\method features the following key designs to enable state-of-the-art shape-conditioned molecule generation: 
\textbf{(1)} summarize the advantages of using molecular surface instead of points sampled around ligand atoms using 1 sentence; 
\textbf{(2)} summarize the advantages of GVP over EGNN in the predictor (e.g., fully leverage the geometric information)
What does the 3-dimension of EGNN mean? Restrcited to a specific space?
Then can I say GVP feature strong flexibility of modeling molecules in a task-specific latent space?
\textbf{(3)} summarize the advantages of incorporating shape condition learning. I do not really understand that part so I cannot give concrete suggestions here...
\textbf{(4)} summarize the benefits of incorporating the bond type embedding. I also do not understand that section well...
\textbf{(5)} capability of conditioning on pocket shape
}
\end{comment}

\begin{figure}
	\centering
	\includegraphics[width=.9\linewidth]{plots/model_figure_XN.pdf}
	%\resizebox{.6\linewidth}{!}{\includegraphics{figures/model_architecture.pdf}}
	%\resizebox{\linewidth}{!}{\includegraphics{figures/model_architecture-crop.pdf}}
	%\includegraphics[angle=270,width=1\linewidth]{figures/plot_new.pdf}
	%https://app.diagrams.net/#G1DRCpWZFa2lRbWxKM9OvaJ_zUicbct9GL
	\caption{\textbf{Model Architecture of \method.} 
		\textbf{a,} Shape embedding module \SE. \method uses a shape embedding module \SE to map the 3D molecule surface shapes \pc into shape embeddings $\shapehiddenmat$. 
		\SE uses an encoder \SEE to map \pc into \shapehiddenmat, and a decoder \SED to optimize \shapehiddenmat with loss $\mathcal{L}^s$.
		\textbf{b,} Shape-conditioned molecule diffusion model \methoddiff. 
		\method uses \methoddiff to generate molecules conditioned on \shapehiddenmat. 
		\methoddiff includes a forward diffusion process, denoted as \diffnoise, which gradually adds noises step by step to the atom positions and features $\{(\pos_t, \atomfeat_t)\}$ at step $t$. 
		\methoddiff uses a backward generative process, denoted as \diffgenerative, to remove the noises in the noisy molecules. 
		\methoddiff generates a 3D molecule by first sampling noisy atom positions and features $\{(\pos_T, \atomfeat_T)\}$ at step $T$ and then removing the noises step by step until $t$ reaches 1.
		\textbf{c,} Pocket guidance \pocketguide. During the generation, \method can use \pocketguide to adjust atom positions $\atomfeat_t$ for minimizing steric clashes between generated molecules and protein pockets.
		\textbf{d,} Shape-conditioned molecule prediction module \molpred. 
		\methoddiff uses \molpred to predict the atom positions and features $(\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0,t})$ given the noisy data $(\pos_t, \atomfeat_t)$ and \shapehiddenmat.
		%
		\molpred is a multi-layer graph neural network comprising $L$ layers.
		%
		In the $l$-th layer, \molpred leverages a shape-aware atom representation learning  (SARL) module, a bond-type representation learning (BTRL) module, and a geometric vector perceptron (GVP) to jointly learn effective atom representations for the prediction.
		\textbf{e,} Shape guidance \shapeguide. During the generation, \method can use \shapeguide to explicitly push predicted atoms to the shapes of condition molecules.
		}
	\label{fig:overall}
\end{figure}

%===================================================================
\subsection*{Representations, Notations, and Preliminaries}
%===================================================================

%---------------------------------------------------------------------------------------------------------------------
\subsubsection{Representations and Notations}
%---------------------------------------------------------------------------------------------------------------------

%\xia{Ziqi, please work on this section}

We represent a molecule \mol
as a set of atoms \mbox{$\mol = \{a_1, a_2, \cdots, a_{\scriptsize{|\mol|}}| a_i = (\pos_i, \atomfeat_i)\}$},
where $|\mol|$ is the number of atoms in \mol; $a_i$ is the $i$-th atom in \mol;  
$\pos_i\in \mathbb{R}^3$ represents the position of $a_i$ in 3D space; 
and $\atomfeat_i\in \mathbb{R}^{K}$ is $a_i$'s one-hot atom feature vector indicating the atom type and its
aromaticity.
%
We represent the Euclidean distance between each pair of atoms $a_i$ and $a_j$ as $d_{ij} \in \mathbb{R}$, and 
the type of the bond in between %the type of bond between them 
as a one-hot vector $\bondemb_{ij} \in \mathbb{R}^4$, 
in which the four dimensions of $\bondemb_{ij}$ represent the absence of a bond, a single bond, a double bond, and an aromatic bond, respectively. %a value of 1 in the first dimension %element $\bondemb_{ij}(1)=1$ 
%indicates the absence of any bond between $a_i$ and $a_j$, while the values of 1 in the other three dimensions indicate %$\bondemb_{ij}(k)=1$ with $k=(2,3,4)$ indicates 
%single, double and aromatic bond, respectively.}
%
%\bo{
%If there is a bond between $a_i$ and $a_j$, we denote the bond type as $b_{ij}$.
%
%$\mathbf{b}_{ij}$ is a one-hot feature vector indicating the bond type. 
%
%We represent the bond type between atoms $a_i$ and $a_j$ as $b_{ij}$.
%
%The distance between $a_i$ and $a_j$ is denoted as $d_{ij}$.
%}
%\xia{this is confusing -- is $d_{ij}$ the length of $b_{ij}$ or the distance, if there are not bonds in between?}
%
Following Guan \etal~\cite{guan2023targetdiff}, bonds between atoms can be uniquely determined by the atom types and the atomic distances among atoms. 
%
%\bo{It looks contradictory with the statement "Unlike \squid that represents shapes of 3D..."}
%
We represent the 3D surface shape {\shape} of a molecule \mol as a point cloud constructed by sampling points over the molecular surface.
%
Details about the construction of point clouds from the surface of molecules are available in Supplementary Section~\ref{supp:point_clouds}.
%
We denote the point cloud as $\pc=\{z_1, z_2, \cdots\, z_{\scriptsize{|\pc|}} | z_j = (\pointpos_j)\}$, 
where $|\pc|$ is the number of points in $\pc$;
$z_j$ is the $j$-th point; and 
$\pointpos_j\in \mathbb{R}^3$ represents the position of $z_j$ in 3D space.
%
We denote the latent embedding of $\pc$ as $\shapehiddenmat\in \mathbb{R}^{d_p\times 3}$, where $d_p$ is the dimension of the latent embedding. 
%
We represent the distance of a point randomly sampled in 3D space to the molecule surface as $o$, referred to as a signed distance, with a positive (negative) sign indicating the point is inside (outside) the surface. 
%
Table~\ref{tbl:notations} summarizes the notations used in this manuscript.

\input{tables/notations}
%\label{tbl:notations}

%\xia{many supp materials should be included in the main text}



%---------------------------------------------------------------------------------------------------------------------
\subsubsection{Equivariance and Invariance}
%---------------------------------------------------------------------------------------------------------------------


%.................................................................................................
\paragraph{Equivariance}
%.................................................................................................

{Equivariance refers to the property of a function $f(\pos)$ %\bo{is it the property of the function or embedding (x)?} 
that any translation and rotation transformation from the special Euclidean group SE(3)~\cite{Atz2021} applied to a geometric object
$\pos\in\mathbb{R}^3$ is mirrored in the output of $f(\pos)$, accordingly.
%
This property ensures $f(\pos)$ to learn a consistent representation of an object's geometric information, regardless of its orientation or location in 3D space.
%
%As a result, it provides $f(\pos)$ better generalization capabilities~\cite{Jonas20a}.
%
Formally, given any translation transformation $\mathbf{t}\in\mathbb{R}^3$ and rotation transformation $\mathbf{R}\in\mathbb{R}^{3\times3}$ ($\mathbf{R}^{\mathsf{T}}\mathbf{R}=\mathbb{I}$, %\xia{change the font types for $^{\mathsf{T}}$ and $\mathbb{I}$ in the entire manuscript}), 
$f(\pos)$ is equivariant with respect to these transformations %$g$ (\bo{where is $g$...})
if it satisfies
\begin{equation}
f(\mathbf{R}\pos+\mathbf{t}) = \mathbf{R}f(\pos) + \mathbf{t}. %\ \text{where}\ \hiddenpos = f(\pos).
\end{equation}
%
%where $\hiddenpos=f(\pos)$ is the output of $\pos$. 
%
In \method, both \SE and \methoddiff are developed to guarantee equivariance in capturing the geometric features of objects regardless of any translation or rotation transformations, as will be detailed in the following sections.
}

\begin{comment}
Equivariance for 3D molecules refers to the property of a function $f(\pos)$ %\bo{is it the property of the function or embedding (x)?} 
that any translation and rotation transformation from the special Euclidean group SE(3) applied to the \bo{input (e.g., atom position)} \bo{@Ziqi you only applied equivariance for $\pos$?} 
$\pos\in\mathbb{R}^3$ is mirrored in the output of $f(\pos)$, accordingly.
%
This property ensures $f(\pos)$ to maintain a consistent representation of a molecule's geometric features, regardless of its orientation or location in 3D space.
%
As a result, it provides $f(\pos)$ better generalization capabilities and sampling efficiency~\cite{Jonas20a}. % generalize learn both \SE and \methoddiff to ensure equivariance specifically.
%
Formally, given any translation transformation $\mathbf{t}\in\mathbb{R}^3$ and rotation transformation $\mathbf{R}\in\mathbb{R}^{3\times3}$, $f(\pos)$ is equivariant with respect to these transformations %$g$ (\bo{where is $g$...})
if it satisfies,
\begin{equation}
f(\mathbf{R}\pos+\mathbf{t}) = \mathbf{R}\hiddenpos + \mathbf{t},\ \text{where}\ \hiddenpos = f(\pos).
\end{equation}
%
%where $\hiddenpos=f(\pos)$ is the output of $\pos$. 
%
In \method, both \SE and \methoddiff are developed to ensure equivariance under any translation or rotation transformations \bo{as will be detailed in the following sections}.
%
\bo{@ziqi it seems better to me to introduce the detail not in the definition but in the corresponding sections}
%
Particularly, to achieve translation equivariance, following previous work~\cite{xu2022geodiff,hoogeboom22diff,xu2023}, both {\SE} and {\methoddiff} shift the center of the molecule to a fixed point to eliminate all the translations.
%
Therefore, only rotation equivariance needs to be considered.
%
Details about how to achieve rotation equivariance in {\SE} and {\methoddiff} will be discussed later. 
\todo{from ziqi: @bo, check this paragraph}
%\xia{NOT to the point!}
%
%\ziqi{\method is designed to generate molecules conditioned on 3D surface shapes in an equivariant way.
%%
%This means that any translation or rotation transformation applied to a conditioned 3D surface shape $\pc$ is reflected in the generated molecule accordingly. \xia{NOT to the point!}
%%
%In \method, both the 3D surface shapes and the molecules are learned and represented in an equivariant way.
%%
%To ensure translation equivariance, following the previous work~\cite{guan2023targetdiff,hoogeboom22diff}, \method eliminates all translations by shifting the center of each $\pc$ to zero and moving the corresponding molecule accordingly \xia{awkward wording}. 
%%
%Therefore, only rotation equivariance needs to be considered in the \method.
%%
%To achieve rotation equivariance, for any rotation transformation $\mathbf{R}\in \mathbb{R}^{3\times3}$
%\xia{need explanation on how R looks}, {$\SE(\pc)$} generates an equivariant shape embedding \shapehiddenmat from each \pc, 
%\begin{equation}
%\SE(\mathbf{R}\pc) = \mathbf{R}\SE(\pc) = \mathbf{R}\shapehiddenmat.
%\label{eqn:se_rotate}
%\end{equation}
%%
%\xia{notations here are confusing. need to explicitly mention SE(P) = H earlier when H is introduced}
%Conditioned on the $\shapehiddenmat$, $f_{\mathbf{\Theta}}(\pos_t, \atomfeat_t, \shapehiddenmat)$ in \methoddiff denoises the noisy molecule $(\pos_t, \atomfeat_t)$ at step $t$ into the molecule $(\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0,t})$. 
%%
%$f_{\mathbf{\Theta}}(\pos_t, \atomfeat_t, \shapehiddenmat)$ is designed to be equivariant to any rotation transformation $\mathbf{R}$ as follows,
%\begin{equation}
%f_{\mathbf{\Theta}}(\mathbf{R}\pos_t, \atomfeat_t, \mathbf{R}\shapehiddenmat) = \mathbf{R}\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0, t},
%\end{equation}
%where the predictions of atom positions $\tilde{\pos}_{0,t}$ are equivariant to $\mathbf{R}$, and the predictions of atom features $\tilde{\atomfeat}_{0, t}$ are invariant to $\mathbf{R}$.
%%
%With the equivariant \SE and \methoddiff, \method can generate 3D molecules with the desired shapes, regardless of the arbitrary orientation or position of the shape conditions.
%%
%Details about how to achieve the rotation equivariance of \SE and \methoddiff will be discussed later.}
\xia{Formally define equivariance and discuss why it is needed}
\end{comment}

%.................................................................................................
\paragraph{Invariance}
%.................................................................................................

%In contrast to equivariance, 
Invariance refers to the property of a function that its output {$f(\pos)$} remains constant under any translation and rotation transformations of the input $\pos$. %a geometric object's feature $\pos$.
%
This property enables $f(\pos)$ to accurately capture %a geometric object's 
the inherent features (e.g., atom features for 3D molecules) that are invariant of its orientation or position in 3D space.
%
Formally, $f(\pos)$ is invariant under any translation $\mathbf{t}$ and  rotation $\mathbf{R}$ if it satisfies
%
\begin{equation}
f(\mathbf{R}\pos+\mathbf{t}) = f(\pos).
\end{equation}
%
In \method, both \SE and \methoddiff capture the inherent features of objects in an invariant way, regardless of any translation or rotation transformations, as will be detailed in the following sections.


\begin{comment}
In contrast to equivariance, invariance for 3D molecules refers to the property of a function that its output {$f(\atomfeat)$} remains constant under any translation and rotation transformations of the \bo{molecule position $\pos$} \bo{\st{3D molecules}}.
%
This property enables $f(\pos, \atomfeat)$ to accurately identify and learn a molecule's inherent features (e.g., atom features) that are {invariant of} the molecule's orientation or position in 3D space.
%
Therefore, in order to jointly model atom positions $\pos$ and features $\atomfeat$ of 3D molecules, a function $f(\pos, \atomfeat)$ should learn a molecule's geometric features in an equivariant way and its inherent features in an invariant way.
%
Formally, $f(\pos, \atomfeat)$ should satisfy the following property under any translation $\mathbf{t}$ and  rotation $\mathbf{R}$,
%
%\bo{@Ziqi all your functions in Method are n-to-1 mappings. It's weird to have a n-to-n mapping here.}
%
%\bo{
%Particularly, $f(\pos, \atomfeat)$ is invariant if and only if it satisfies the following equation:
%}
%\ziqi{@bo, this is the formal definition of }
%\begin{equation}
%f(\mathbf{R}\pos+\mathbf{t}, \atomfeat) = XXX
%\end{equation}
\begin{equation}
f(\mathbf{R}\pos+\mathbf{t}, \atomfeat) =  (\mathbf{R}\hiddenpos+\mathbf{t}, \hiddenatomfeat), \ \text{where}\ (\hiddenpos, \hiddenatomfeat) = f(\pos, \atomfeat),
\end{equation}
%
%\bo{
%where, $\mathbf{R}$ is XXX; $\mathbf{t}$ is XXX.
%}
%where $\hiddenpos\in\mathbb{R}^{3\times d_r}$ and $\hiddenatomfeat\in \mathbb{R}^{d_a}$ are the embeddings of geometric features and inherent features, respectively.
%
In \method, \methoddiff is developed to learn inherent molecular features in an invariant way 
{as will be introduced in Section ``Molecule Representation Learning''}.
%
%Details about how to achieve invariance for inherent features will be discussed later.
%}

\todo{from Ziqi: @Bo, check this paragraph. }

\xia{Formally define invariance and discuss why it is needed}


%\todo{1. add notation table}

%\todo{2. change motivation to learn structures and geometry regardless of orientation}

%\todo{3. change ``reflect'', reflect is not accurate!}

%\todo{4. remove equivariance, move equivariance of each part to the following sections.}

%\todo{\textbf{from Ziqi: @Bo} can you check whether there are any missing notations used in the following sections but not described here? can you also create a table with these notations?}
\end{comment}

%\begin{comment}
%\ziqi{Equivariance is a pivotal concept in modeling the 3D geometry of molecules.
%%
%Particularly, equivariance for 3D molecules refers to the property of a function $f(\pos)$ that any translation and rotation transformation $g$ from the special Euclidean group SE(3) applied to the atom position $\pos$ is reflected in the output $f(\pos)$, accordingly.
%Formally, given any translation and rotation transformation $g$, $f(\pos)$ is equivariant with respect to $g$ if it satisfies,
%\begin{equation}
%f(T_g(\pos)) = S_g(f(\pos)),
%\end{equation} 
%where $T_g$ and $S_g$ are linear representations for $g$. \xia{is this accurate? is it to the point?}
%%
%Considering equivariance when modeling 3D molecules brings several important benefits.
%%
%For example, equivariant models require less data to learn patterns and properties of molecules, as they do not treat the same molecule under different transformations as two different samples.
%%
%In addition, equivariant models can understand and interpret the relationship between different molecules through transformations, which facilitates the understanding of how their properties change with their geometry.
%}
%\xia{NOT to the point!}
%
%\ziqi{\method is designed to generate molecules conditioned on 3D surface shapes in an equivariant way.
%%
%This means that any translation or rotation transformation applied to a conditioned 3D surface shape $\pc$ is reflected in the generated molecule accordingly. \xia{NOT to the point!}
%%
%In \method, both the 3D surface shapes and the molecules are learned and represented in an equivariant way.
%%
%To ensure translation equivariance, following the previous work~\cite{guan2023targetdiff,hoogeboom22diff}, \method eliminates all translations by shifting the center of each $\pc$ to zero and moving the corresponding molecule accordingly \xia{awkward wording}. 
%%
%Therefore, only rotation equivariance needs to be considered in the \method.
%%
%To achieve rotation equivariance, for any rotation transformation $\mathbf{R}\in \mathbb{R}^{3\times3}$
%\xia{need explanation on how R looks}, {$\SE(\pc)$} generates an equivariant shape embedding \shapehiddenmat from each \pc, 
%\begin{equation}
%\SE(\mathbf{R}\pc) = \mathbf{R}\SE(\pc) = \mathbf{R}\shapehiddenmat.
%\label{eqn:se_rotate}
%\end{equation}
%%
%\xia{notations here are confusing. need to explicitly mention SE(P) = H earlier when H is introduced}
%Conditioned on the $\shapehiddenmat$, $f_{\mathbf{\Theta}}(\pos_t, \atomfeat_t, \shapehiddenmat)$ in \methoddiff denoises the noisy molecule $(\pos_t, \atomfeat_t)$ at step $t$ into the molecule $(\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0,t})$. 
%%
%$f_{\mathbf{\Theta}}(\pos_t, \atomfeat_t, \shapehiddenmat)$ is designed to be equivariant to any rotation transformation $\mathbf{R}$ as follows,
%\begin{equation}
%f_{\mathbf{\Theta}}(\mathbf{R}\pos_t, \atomfeat_t, \mathbf{R}\shapehiddenmat) = \mathbf{R}\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0, t},
%\end{equation}
%where the predictions of atom positions $\tilde{\pos}_{0,t}$ are equivariant to $\mathbf{R}$, and the predictions of atom features $\tilde{\atomfeat}_{0, t}$ are invariant to $\mathbf{R}$.
%%
%With the equivariant \SE and \methoddiff, \method can generate 3D molecules with the desired shapes, regardless of the arbitrary orientation or position of the shape conditions.
%%
%Details about how to achieve the rotation equivariance of \SE and \methoddiff will be discussed later.
%}
%\xia{you need to sort out all the logic flows after you re-order all the structures. many notations are 
%used without definitions. }
%\end{comment}

%===================================================================
\subsection*{Equivariant Condition Shape Representation Pre-training (\SE)}
%===================================================================
%
\method pre-trains a shape embedding module \SE to generate surface shape embeddings 
$\shapehiddenmat$ of condition molecules. %that are equivariant to  translation and rotation of 3D surface shapes (i.e., $\pc$) of molecules. 
%
\SE uses an encoder \SEE to map $\pc$ to the equivariant latent embedding $\shapehiddenmat$.
%
\SE employs a decoder \SED to optimize $\shapehiddenmat$ by recovering the signed distances~\cite{park2019sdf} of randomly sampled points %$z_q$ 
in 3D space to the molecule surface using %based on 
$\shapehiddenmat$. 
%and a decoder (\SED) to predict the signed distance~\cite{park2019sdf} of a sampled query point $z_q$ to $\pc$ based on $\hiddenmat^{\mathtt{s}}$.
%
\method uses $\hiddenmat^{\mathtt{s}}$ to guide the diffusion process {as will be detailed} later
(Section ``Diffusion-based Molecule Generation'').
 %
%Note that \SE distinguishes itself from state-of-the-art baseline \squid by learning an expressive shape representation decoupled from the exact atomic structures of condition molecules.
%%
%To be specific, 
%\ziqi{In contrast, \squid jointly learns the latent representations of the shapes of condition molecules and their exact atomic structures.
%
%This learning process creates an overly strong correlation between the shape representations and atomic structure representations of condition molecules.
%
%Consequently, it degrades the capacity of \squid in exploring the chemical space for the generation of molecules with distinct structures from condition molecules. }
%
%
\begin{comment}
\squid represents shapes of condition molecules using points sampled around all the atom positions in molecules.
%
\ziqi{
However, due to this design, \squid is prone to generate molecules of similar graph structures to condition molecules, 
%\xia{isn't it also your goal?}, 
degrading its capacity in generating novel molecules \xia{need to rephrase this sentence, not to criticize ``similar structures to condition molecules"... }.
%
%This design, however, could disclose the atomic structures of condition molecules and thus degrade the capacity of \squid in generating molecules of novel structures. 
%
To address this issue, \method, in contrast, leverages molecule surface shapes \hl{that are decoupled from specific atom positions of condition molecules.} \xia{``decouple" is not the right word... need to rephrase} %and thus better detached from their exact atomic structures.
%
By using molecule surface shapes, \method \hl{could generate molecules with atomic structures substantially varying from the condition molecules} \xia{rephrase accordingly}, as demonstrated in Section ``Overall Comparison on Shape and Graph Similarity.''}


\xia{do you still need the following paragraph, at least the beginning part?}
This design, however, could force overly strong correlations between shapes of molecules and their atomic structures, and thus, degrade the capacity of \squid in generating molecules of novel structures.
%reveal the exact atomic structures of condition molecules and thus degrade the capacity of \squid to generate molecules of novel structures, which is essential in drug discovery.
%
To address this issue, \method, in contrast, leverages molecule surface shapes.
%
These shapes are loosely related to the exact atomic structures of molecules.
%
By using molecule surface shapes, \method could generate molecules with atomic structures substantially varying from the condition molecules, as demonstrated in \ziqi{Section ``Overall Comparison on Shape and Graph Similarity.''}
%
\end{comment}
We present \SE in detail in the following sections. Particularly, we present the encoder \SEE in Section ``Shape Encoder''; the decoder \SED in Section ``Shape Decoder''; and the optimization of \SE in Section ``\SE Pre-training.'' 
%
{Fig.~\ref{fig:overall}(a) presents the architecture of \SE. }
% respectively.
%\bo{(@Ziqi, here you may want to highlight the advantages of \method over \squid. what's the preferred properties of the embeddings from \method. Summarize to several keywords (e.g. expressive, disentangled, ...))}
%\st{adopting a novel approach to representing the shapes of molecules as molecular surfaces.}
%

\begin{comment}
Unlike \squid, which represents shapes of condition molecules using points sampled around all the atom positions in molecules, molecular surfaces used in \SE capture the shapes of condition molecules without depending on the exact positions of atoms and thus keep their atomic structures undisclosed.
%
Using molecular surface shapes enables \SE to learn better representations of molecular shapes that are decoupled from their exact atomic structures.
%
Consequently, such representations allow \methoddiff to generate molecules that retain the desired shape similar to condition molecules but have various atomic structures.
%
This capability could potentially enable the discovery of new and diverse molecules with desired shapes.
\bo{
\squid represents XXX as molecule shape.
%
This design, however, XXX (@Ziqi, I think the advantage is not clear. You may want to say the shape in \squid is highly correlated to the molecule structure. Thus, degrade the model capacity of generating molecules of novel structures, which is of high utility in pharmacy.)
}
%
\bo{
To address this issue, \method, in contrast, leverages molecule surface shapes.
%
These shapes are loosely related to the molecule structure.
%
By XXX, \method could XXX as demonstrated in XXX.
%
In the following section, we present XXX.
}
 
 \todo{from ziqi: @Bo, can you check this paragraph?}
%as will be presented later. 
%in Section ``Equivariant Shape-Condit ioned Molecule Predictor''.
%
%\xia{$\hiddenmat^{\mathtt{s}}$ is used to do what?}

%\ziqi{what is pretraining? and the usage of the latent embedding}
\end{comment}

%-----------------------------------------------------------------------
\subsubsection{Shape Encoder (\SEE)}
%-----------------------------------------------------------------------

\SEE learns shape embeddings $\hiddenmat^{\mathtt{s}}$ from the 3D surface shape $\pc$ of molecules in an equivariant way, 
%
%That is, the learned $\hiddenmat^{\mathtt{s}}$ will be transformed accordingly under any translation or rotation transformations of $\pc$, 
as described in Section ``Equivariance and Invariance''.
%he learned $\hiddenmat^{\mathtt{s}}$ is equivariant to both translation and rotation of $\pc$ \ziqi{as 
%described in Section ``Equivariance and Invariance''}.
% 
%\hl{That is, any translation and rotation applied to $\pc$ is mirrored in $\hiddenmat^{\mathtt{s}}$ accordingly.}
%\xia{not accurate}
%
%Such equivariance allows \SEE to consistently capture the geometric features of 3D surface shapes, regardless of their orientations or positions in 3D space.
%
To ensure translation equivariance, 
{\SEE} shifts the center of each $\pc$ to zero to eliminate all translations.
%
To ensure rotation equivariance, \SEE leverages vector neurons (VNs)~\cite{deng2021vn} and dynamic graph convolutional neural networks (DGCNNs)~\cite{wang2019dynamic} to learn shape embeddings $\shapehiddenmat$ as follows:
%
\begin{equation*}
\{\hiddenmat^{\mathtt{p}}_1, \hiddenmat^{\mathtt{p}}_2, \cdots, \hiddenmat^{\mathtt{p}}_{|\scriptsize{\pc}|}\} = \text{VN-DGCNN}(\{\pointpos_1, \pointpos_2, \cdots, \pointpos_{|\scriptsize{\pc}|}\}), 
\vspace{-3pt}
\end{equation*}
%
\begin{equation}
\hiddenmat^{\mathtt{s}} = \sum\nolimits_{j}\hiddenmat^{\mathtt{p}}_j / {|\pc|},
\label{eqn:shape_embed}
\end{equation}
%
%\xia{notations: how about $\mathbf{p}_1$ instead of $\mathbf{H}_1^p$ here?}
%
where $\text{VN-DGCNN}(\cdot)$ is a VN-based DGCNN network
%\xia{did you spell out DGCNN before?}
to generate equivariant embedding $\hiddenmat^{\mathtt{p}}_j\in  \mathbb{R}^{3\times d_p}$ for each point $z_j$ in $\pc$; 
% 
and $\hiddenmat^{\mathtt{s}} \in \mathbb{R}^{3\times d_p}$ is the embedding of $\pc$ generated via % 
a mean-pooling over the embeddings of all the points.
%
%$\text{VN-DGCNN}(\cdot)$ takes the k-nearest neighbor graph of each \pc as input and uses graph convolution to learn both the local neighborhood and global shape information of each point by dynamically updating edges among points based on the learned embeddings in each layer. 
%
$\text{VN-DGCNN}(\cdot)$ guarantees the rotation equivariance by learning embedding matrices $\hiddenmat^{\mathtt{p}}_j\in  \mathbb{R}^{3\times d_p}$ for points using only equivariant operations as detailed in Deng \etal~\cite{deng2021vn}
%lifting latent representations from vectors of scalar entries $\mathbf{h}\in\mathbb{R}^{d}$ to vectors of 3D points $\hiddenmat\in\mathbb{R}^{3\times d}$, referred to as vector neurons.
%
%For example, a linear function with a weight matrix $\mathbf{W}\in\mathbb{R}^{d\times d}$ acting on vector neurons is rotationally equivariant  $f(\mathbf{R}\mathbf{H})=\mathbf{R}\mathbf{H}\mathbf{W}=\mathbf{R}f(\mathbf{H})$ under any rotation $\mathbf{R}\in \mathbb{R}^{3\times 3}$.
%
%We refer interested readers to check more details of invariant or equivariant functions based on vector neurons in Deng \etal~\cite{deng2021vn}.

%\bo{
%May not need all the details here. 
%
%The key idea is not clear.
%
%You may want to just summarize their abstract "In this paper, we introduce a general framework
%built on top of what we call Vector Neuron representations
%for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to
%3D vectors, our vector neurons enable a simple mapping of
%SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural %operations – including linear layers, non-linearities, pooling, and
%normalizations."
%}
%
%Note that $\text{VN-DGCNN}(\cdot)$ generates a matrix as the embedding of each point (i.e., $\hiddenmat^{\mathtt{p}}_j$) to guarantee the equivariance.
%
%\xia{here add one or two sentences to describe the key ideas of VN-DGCNN and why it is equivariant. }

%\todo{from ziqi: @bo, check this paragraph}

%-----------------------------------------------------------------------
\subsubsection{Shape Decoder (\SED)}
%-----------------------------------------------------------------------

%\begin{comment}
%\methodenc employs Vector Neurons-based layers (VN-Layer)~\cite{deng2021vn}, which ensure the rotation equivariance of latent embeddings by
%extending each neuron in neural networks from a 1D scalar to a 3D vector. %\xia{grammar...}
%%
%Given \ziqi{a point cloud $\pc=\{\pointpos_j\}_{j=1}$}, %\hl{a point cloud $\pc\in \mathbb{R}^{N_p\times 3}$} \xia{this notation is very confusing: does $N_p$ have anything to do with $N$? need to be explicit and clear}, 
%\methodenc learns the latent shape embedding $\hiddenmat^{\mathtt{s}}$ as follows,
%%
%\begin{equation}
%\{\hiddenmat^{\mathtt{p}}_j\}_{j=1} = \text{VN-DGCNN}(\{\pointpos_j\}_{j=1}); \hiddenmat^{\mathtt{s}} = \sum_{j=1}(\hiddenmat^{\mathtt{p}}_j) / {|\pc|},
%\end{equation}
%%
%in which $\text{VN-DGCNN}(\cdot)$ is an equivariant layer used to encode points into embeddings~\cite{deng2021vn}; 
%\ziqi{$\hiddenmat^{\mathtt{p}}_j\in  \mathbb{R}^{d_p\times 3}$ represents the equivariant embeddings of point $j$ in \pc,}
%%$\hiddenmat_p \in \mathbb{R}^{N_p\times d_p\times 3}$ represents the equivariant embeddings of all points in the point cloud \pc,
%where $d_p$ is the dimension of latent embeddings; 
%$\hiddenmat^{\mathtt{s}} \in \mathbb{R}^{d_p\times 3}$ is the embedding of the overall molecular shape via %\st{calculated by conducting} 
%a mean-pooling over \ziqi{$\{\hiddenmat^{\mathtt{p}}_j\}_{j=1}$};
%\ziqi{$|\pc|$ denotes the number of points in \pc.}
%%the latent embeddings of all the points in the point cloud.
%\xia{the notations of $\hiddenmat_p$ and $\hiddenmat_p^i$ are conflicting with the definition of {\pc}, and the ``shift the center of points in point clouds" earlier. Please check carefully!!!}
%\end{comment}


To optimize $\hiddenmat^{\mathtt{s}}$, following Deng \etal~\cite{deng2021vn}, \SE learns a decoder \SED to predict the signed distance of a query point $z_q$ randomly sampled from  3D space to surface shape \shape using multilayer perceptrons (MLPs) as follows:
%
\begin{equation}
\tilde{o}_q = \text{MLP}([\langle \mathbf{z}_q, \hiddenmat^{\mathtt{s}}\rangle, \|\mathbf{z}_q\|^2, \text{VN-In}(\hiddenmat^{\mathtt{s}})]),
\label{eqn:se:decoder}
\end{equation}
%\xia{in all MLP input in the above equation and other equations with MLP, miss the notation of concatenation. Please fix this!}
where $\tilde{o}_q$ is the predicted signed distance of $z_q$, with positive and negative values indicating $z_q$ is inside or outside the surface shape \shape, respectively;
%
$[\cdot,\cdot]$ represents the concatenation operation;
%
$\langle\cdot,\cdot\rangle$ is the dot-product operator;
%
$\|\mathbf{z}_q\|^2$ is the squared Euclidean norm of the position of $z_q$;
$\text{VN-In}(\cdot)$ is an invariant %~\hl{{\cite{benton2020learning}}} 
VN network~\cite{deng2021vn} that converts the equivariant shape embedding $\shapehiddenmat\in \mathbb{R}^{d_p\times 3}$ into 
an invariant shape embedding $\text{VN-In}(\shapehiddenmat)\in \mathbb{R}^{d_p}$. %(described below). %\bo{@Ziqi did you describe this?}). 
%
Intuitively, \SED predicts the signed distance between the query point and 3D surface by jointly considering 
the interaction between the point and surface ($\langle\mathbf{z}_q, \hiddenmat^{\mathtt{s}}\rangle$), 
the distance %position 
of the query point ($\|\mathbf{z}_q\|^2=\langle\mathbf{z}_q, \mathbf{z}_q \rangle$) to the origin, 
and the molecule surface shape ($\text{VN-In}(\cdot)$).
%
All these three terms 
%in equation~\ref{eqn:se:decoder} guarantee the invariance of outputs 
are invariant to any rotation transformations, as they are calculated from the dot-product operation $\langle \cdot, \cdot \rangle$. %\bo{@Ziqi It's not obvious that you have $\langle \cdot, \cdot \rangle$ in the second term...}
%
This operation is invariant to any rotations {as} $\langle \mathbf{R}\mathbf{z}, \mathbf{R}\mathbf{z} \rangle = \mathbf{z}^{\mathsf{T}}\mathbf{R}^{\mathsf{T}}\mathbf{R}\mathbf{z} = \mathbf{z}^{\mathsf{T}}\mathbf{z} = \langle \mathbf{z}, \mathbf{z} \rangle$.
%
Note that $\text{VN-In}(\cdot)$ comprises invariant dot-product operations and specifically designed invariant activations to learn invariant embeddings, as detailed in Deng~\etal~\cite{deng2021vn}. %\xia{why is it invariant? be right to the point}. 
%
%Note that the squared Euclidean norm $\|\mathbf{z}_q\|^2$ can be calculated as $\langle \mathbf{z}_q, \mathbf{z}_q \rangle$ and thus is also invariant to any rotation transformations.
%
%\bo{\st{This ensures {\SED} to predict the signed distance $\tilde{o}_q$ in an invariant way.}}
%and the interaction between the point and surface $\langle\cdot,\cdot\rangle$ 
%\xia{full term in the last dot product; 
%re-order the description so the terms are in the same order as in Equation 2}.
%
The predicted signed distance $\tilde{o}_q$ is used to calculate the loss for the optimization of $\shapehiddenmat$ (discussed below in Equation~\ref{eqn:loss_distance}).
%
%\st{As shown in the literature~{\cite{deng2021vn}}, $\tilde{o}_q$
%remains invariant to the rotation of the 3D molecule surface shapes (i.e., $\pc$).}
%
%
%\xia{what is the intuition to predict the distance using these factors? why to predict the distances?}
We present the sampling process of $z_q$ in the Supplementary Section~{\ref{supp:training:shapeemb}}.%\hl{XXX}.



%-----------------------------------------------------------------------
\subsubsection{\SE Pre-training}
%-----------------------------------------------------------------------


\method pre-trains \SE by minimizing the squared-errors loss between the predicted and the ground-truth signed distances of query points to the surface shape \shape as follows: %\xia{what surface? countable nouns should have a/the/adjectives before the words or in plural forms...} as follows:
%
\begin{equation}
\label{eqn:loss_distance}
\mathcal{L}^{\mathtt{s}} = \sum\nolimits_{z_q \in \mathcal{Z}} \|o_q-\tilde{o}_q\|^2,
\end{equation}
where $\mathcal{Z}$ is the set of sampled query points and $o_q$ is the ground-truth
signed distance of query point $z_q$.
%
By pretraining \SE, \method learns $\shapehiddenmat$ that will be used as the condition in the following 3D molecule generation.

%

%===================================================================
\subsection*{Diffusion-based Molecule Generation (\methoddiff)}
\label{section:diff}
%===================================================================

In \method, a shape-conditioned molecule diffusion model, referred to as \methoddiff, 
is used to generate a 3D molecule structure (i.e., atom coordinates and features, and bonds) 
conditioned on a given 3D surface shape 
that is represented by the shape latent embedding $\shapehiddenmat$ (Equation~\ref{eqn:shape_embed}).
%
{Fig.~\ref{fig:overall}(b) presents the architecture of \methoddiff. }
%\xia{change $\hiddenmat^{\mathtt{s}}$ to $\hiddenmat^{\mathtt{s}}$ in the entire paper}
%\bo{
%@Ziqi, you may also want to highlight your special designs in \methoddiff here.
%
%This is the opening of \methoddiff.
%}
%\bo{
%@Ziqi Feel free to propose other keywords. I am using customized, high-quality, and novel here but I am not sure they are the best...
%}
%\bo{
%Particularly, \methoddiff generates customized, high-quality, and novel molecules using denoising diffusion probabilistic models~\cite{ho2020ddpm}.
%%
%\methoddiff ensures customized molecules by XXX (@Ziqi highlight the designs of incorporating/strengthening the shape condition here)
%%
%\methoddiff generates high-quality molecules using (@Ziqi highlight your designs for enabling high-quality molecules here)
%%
%\methoddiff enables novel molecules by XXX (@Ziqi).
%}
%
Following the denoising diffusion probabilistic models~\cite{ho2020ddpm}, \methoddiff includes a forward diffusion process based on a Markov chain,
denoted as \diffnoise, which gradually adds noises step by step to the atom positions and features $\{(\pos_i, \atomfeat_i)\}$ in the training molecules with $i$ indexing the $i$-th atom.
%\hl{original molecule} 
%\xia{what do you mean by ``original molecule"?}. % based on the predefined
%variance schedule $\{\beta_t^x, \beta_t^v\}$.
%
The noisy atom positions and features at step $t$ are represented as $\{(\pos_{i,t}, \atomfeat_{i,t})\}$ ($t=1, \cdots, T$),
and the molecules without any noise %original molecule 
are represented as $\{(\pos_{i,0}, \atomfeat_{i,0})\}$.
%
At the final step $T$, $\{(\pos_{i,T}, \atomfeat_{i,T})\}$ are completely unstructured and resemble 
a simple distribution like a Normal distribution $\mathcal{N}(\mathbf{0}, \mathbb{I})$ or a uniform categorical distribution {$\mathcal{C}(\mathbf{1}/K)$},
in which $\mathbf{I}$ and $\mathbf{1}$ denotes the identity matrix and identity vector, respectively.
When no ambiguity arises, we will eliminate subscript $i$ in the notations and use $(\pos_{t}, \atomfeat_{t})$ for brevity.

During training, \methoddiff is learned to reverse the forward diffusion process via another Markov chain, 
referred to as the backward generative process and denoted as \diffgenerative, to remove the noises in the noisy molecules.
%
During inference, %\hl{sampling}\xia{???}, 
\methoddiff first samples noisy atom positions and features at step $T$ %$(\pos_{T}, \atomfeat_{T})$ 
from simple distributions and then generates a 3D molecule structure by removing the noises in the noisy molecules step by step until $t$ reaches 1.
%\xia{refer back to Figure 4. in Figure 4, add notations like a, b, and c to denote different processes; remember to update Figure 4}

%\hl{[Todo: discuss and determine whether we should move forward diffusion process and backward to supplementary?]}
%\xia{no, they should be in main text}


%-----------------------------------------------------------------------
\subsubsection{Forward Diffusion Process (\diffnoise)}
\label{section:diff:diff}
%-----------------------------------------------------------------------

%\xia{we may need to move the forward process to supp. will discuss this later once every section is completed. }

Following the previous work~\cite{guan2023targetdiff}, at step $t\in[1, T]$, a small Gaussian noise and a small categorical noise are added to the continuous atom positions and 
discrete atom features $\{(\pos_{t-1}, \atomfeat_{t-1})\}$, 
respectively.
%
%When no ambiguity arises, we will eliminate subscript $i$ in the notations and use $(\pos_{t-1}, \atomfeat_{t-1})$ for brevity.
%
The noise levels of the Gaussian and categorical noises are determined by two predefined variance schedules $(\beta_t^{\mathtt{x}}, \beta_t^{\mathtt{v}})\in (0,1)$, where $\beta_t^{\mathtt{x}}$ and $\beta_t^{\mathtt{v}}$
are selected to be sufficiently small to ensure the smoothness of \diffnoise.
%
The details about variance schedules are available in Supplementary Section~\ref{supp:forward:variance}. 
%\ref{XXX}. %\xia{idea, range and trend of $(\beta_t^x, \beta_t^v)$?}
%
%For \ziqi{an} atom position $\pos_t\in \mathbb{R}^{3}$, %\xia{why $N \times 3$?} within the noisy data $\mol_t=\{\pos_t^i, \atomfeat_t^i\}_{i=1}^N$ \xia{so is it points or atoms exactly? need to be crystal clear!}, 
%Gaussian noise is added according to a predefined variance schedule $\beta^x_t\in (0, 1)$ \ziqi{by sampling 
%from a Gaussian distribution determined by $\pos_{t-1}$. 
%
Formally, for atom positions, the probability of $\pos_t$ sampled given $\pos_{t-1}$, denoted as $q(\pos_t|\pos_{t-1})$, is defined as follows,
%\xia{revise the representation, should be $\beta^x_t$ -- note the space} as follows,
%
\begin{equation}
q(\pos_t|\pos_{t-1}) = \mathcal{N}(\pos_t|\sqrt{1-\beta^{\mathtt{x}}_t}\pos_{t-1}, \beta^{\mathtt{x}}_t\mathbb{I}), 
\label{eqn:noiseposinter}
\end{equation}
%
%\xia{should be a comma after the equation. you also missed it. }
%\st{in which} 
where %\hl{$\pos_0$ denotes the original atom position;} \xia{no $\pos_0$ in the equation...}
%$\mathbf{I}$ denotes the identity matrix;
$\mathcal{N}(\cdot)$ is a Gaussian distribution of $\pos_t$ with mean $\sqrt{1-\beta_t^{\mathtt{x}}}\pos_{t-1}$ and covariance $\beta_t^{\mathtt{x}}\mathbf{I}$.
%\xia{what is $\mathcal{N}$? what is $q$? you abused $q$. need to be crystal clear... }
%\bo{Should be $\sim$ not $=$ in the equation}
%
Following Hoogeboom \etal~\cite{hoogeboom2021catdiff}, 
%the forward process for the discrete atom feature $\atomfeat_t\in\mathbb{R}^K$ adds 
%categorical noise into $\atomfeat_{t-1}$ according to a variance schedule $\beta_t^v\in (0, 1)$. %as follows, %\hl{$\betav_t\in (0, 1)$} as follows,
%\xia{presentation...check across the entire manuscript... } as follows,
%
%\ziqi{Formally, 
for atom features, the probability of $\atomfeat_t$ across $K$ classes given $\atomfeat_{t-1}$ is defined as follows,
%
\begin{equation}
q(\atomfeat_t|\atomfeat_{t-1}) = \mathcal{C}(\atomfeat_t|(1-\beta^{\mathtt{v}}_t) \atomfeat_{t-1}+\beta^{\mathtt{v}}_t\mathbf{1}/K),
\label{eqn:noisetypeinter}
\end{equation}
%
where %\hl{$\atomfeat_0$ denotes the original atom positions}; 
$\mathcal{C}$ is a categorical distribution of $\atomfeat_t$ derived from the %by 
noising $\atomfeat_{t-1}$ with a uniform noise $\beta^{\mathtt{v}}_t\mathbf{1}/K$ across $K$ classes.
%adding an uniform noise $\beta^v_t$ to $\atomfeat_{t-1}$ across K classes.
%\xia{there is always a comma or period after the equations. Equations are part of a sentence. you always missed it. }
%\xia{what is $\mathcal{C}$? what does $q$ mean? it is abused. }

Since the above distributions form Markov chains, %} \xia{grammar!}, 
the probability of any $\pos_t$ or $\atomfeat_t$ can be derived from $\pos_0$ or $\atomfeat_0$:
%samples $\mol_0$ as follows,
%
\begin{eqnarray}
%\begin{aligned}
& q(\pos_t|\pos_{0}) & = \mathcal{N}(\pos_t|\sqrt{\cumalpha^{\mathtt{x}}_t}\pos_0, (1-\cumalpha^{\mathtt{x}}_t)\mathbb{I}), \label{eqn:noisepos}\\
& q(\atomfeat_t|\atomfeat_0)  & = \mathcal{C}(\atomfeat_t|\cumalpha^{\mathtt{v}}_t\atomfeat_0 + (1-\cumalpha^{\mathtt{v}}_t)\mathbf{1}/K), \label{eqn:noisetype}\\
& \text{where }\cumalpha^{\mathtt{u}}_t & = \prod\nolimits_{\tau=1}^{t}\alpha^{\mathtt{u}}_\tau, \ \alpha^{\mathtt{u}}_\tau=1 - \beta^{\mathtt{u}}_\tau, \ {\mathtt{u}}={\mathtt{x}} \text{ or } {\mathtt{v}}.\;\;\;\label{eqn:noiseschedule}
%\end{aligned}
\label{eqn:pos_prior}
\end{eqnarray}
%\xia{always punctuations after equations!!! also use ``eqnarray" instead of ``equation" + ``aligned" for multiple equations, each
%with a separate reference numbering...}
%\st{in which}, 
%where \ziqi{$\cumalpha^u_t = \prod_{\tau=1}^{t}\alpha^u_\tau$ and $\alpha^u_\tau=1 - \beta^u_\tau$ ($u$=$x$ or $v$)}.
%\xia{no such notations in the above equations; also subscript $s$ is abused with shape};
%$K$ is the number of categories for atom features.
%
%The details about noise schedules $\beta^x_t$ and $\beta^v_t$ are available in Supplementary Section \ref{XXX}. \ziqi{add trend}
%
Note that $\bar{\alpha}^{\mathtt{u}}_t$ ($\mathtt{u}={\mathtt{x}}\text{ or }{\mathtt{v}}$)
%($u$=$x$ or $v$) 
is monotonically decreasing from 1 to 0 over $t=[1,T]$. %\xia{=???}. 
%
As $t\rightarrow 1$, $\cumalpha^{\mathtt{x}}_t$ and $\cumalpha^{\mathtt{v}}_t$ are close to 1, leading to that $\pos_t$ or $\atomfeat_t$ approximates 
%the original data 
$\pos_0$ or $\atomfeat_0$.
%
Conversely, as  $t\rightarrow T$, $\cumalpha^{\mathtt{x}}_t$ and $\cumalpha^{\mathtt{v}}_t$ are close to 0,
leading to that $q(\pos_T|\pos_{0})$ %\st{$\rightarrow \mathcal{N}(\mathbf{0}, \mathbf{I})$} 
resembles  {$\mathcal{N}(\mathbf{0}, \mathbb{I})$} 
and $q(\atomfeat_T|\atomfeat_0)$ %\st{$\rightarrow \mathcal{C}(\mathbf{I}/K)$} 
resembles {$\mathcal{C}(\mathbf{1}/K)$}.

Using Bayes theorem, the ground-truth Normal posterior of atom positions $p(\pos_{t-1}|\pos_t, \pos_0)$ can be calculated in a
closed form~\cite{ho2020ddpm} as below,
%
\begin{eqnarray}
& p(\pos_{t-1}|\pos_t, \pos_0) = \mathcal{N}(\pos_{t-1}|\mu(\pos_t, \pos_0), \tilde{\beta}^\mathtt{x}_t\mathbb{I}), \label{eqn:gt_pos_posterior_1}\\
&\!\!\!\!\!\!\!\!\!\!\!\mu(\pos_t, \pos_0)\!=\!\frac{\sqrt{\bar{\alpha}^{\mathtt{x}}_{t-1}}\beta^{\mathtt{x}}_t}{1-\bar{\alpha}^{\mathtt{x}}_t}\pos_0\!+\!\frac{\sqrt{\alpha^{\mathtt{x}}_t}(1-\bar{\alpha}^{\mathtt{x}}_{t-1})}{1-\bar{\alpha}^{\mathtt{x}}_t}\pos_t, 
\tilde{\beta}^\mathtt{x}_t\!=\!\frac{1-\bar{\alpha}^{\mathtt{x}}_{t-1}}{1-\bar{\alpha}^{\mathtt{x}}_{t}}\beta^{\mathtt{x}}_t.\;\;\;
\end{eqnarray}
%
%\xia{Ziqi, please double check the above two equations!}
Similarly, the ground-truth categorical posterior of atom features $p(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_0)$ can be calculated~\cite{hoogeboom2021catdiff} as below,
%
\begin{eqnarray}
& p(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_0) = \mathcal{C}(\atomfeat_{t-1}|\mathbf{c}(\atomfeat_t, \atomfeat_0)), \label{eqn:gt_atomfeat_posterior_1}\\
& \mathbf{c}(\atomfeat_t, \atomfeat_0) = \tilde{\mathbf{c}}/{\sum_{k=1}^K \tilde{c}_k}, \label{eqn:gt_atomfeat_posterior_2} \\
& \tilde{\mathbf{c}} = [\alpha^{\mathtt{v}}_t\atomfeat_t + \frac{1 - \alpha^{\mathtt{v}}_t}{K}]\odot[\bar{\alpha}^{\mathtt{v}}_{t-1}\atomfeat_{0}+\frac{1-\bar{\alpha}^{\mathtt{v}}_{t-1}}{K}], 
\label{eqn:gt_atomfeat_posterior_3}
%\label{eqn:atomfeat_posterior}
\end{eqnarray}
%
%\xia{Ziqi: please double check the above equations!}
%
where $\tilde{c}_k$ denotes the likelihood of $k$-th class across $K$ classes in $\tilde{\mathbf{c}}$; 
$\odot$ denotes the element-wise product operation;
$\tilde{\mathbf{c}}$ is calculated using $\atomfeat_t$ and $\atomfeat_{0}$ and normalized into $\mathbf{c}(\atomfeat_t, \atomfeat_0)$ so as to represent
probabilities. %\xia{is this correct? is $\tilde{c}_k$ always greater than 0?}
%\xia{how is it calculated?}.
%\ziqi{the likelihood distribution $\tilde{c}$ is calculated by $p(\atomfeat_t|\atomfeat_{t-1})p(\atomfeat_{t-1}|\atomfeat_0)$, according to 
%Equation~\ref{eqn:noisetypeinter} and \ref{eqn:noisetype}.
%\xia{need to write the key idea of the above calculation...}
%
The proof of the above equations is available in Supplementary Section~\ref{supp:forward:proof}.

%-----------------------------------------------------------------------
\subsubsection{{Backward Generative Process (\diffgenerative)}}
\label{section:diff:backward}
%-----------------------------------------------------------------------

\methoddiff learns to reverse {\diffnoise} by denoising from {$(\pos_{t}, \atomfeat_{t})$} to {$(\pos_{t-1}, \atomfeat_{t-1})$} 
at $t\in[1,T]$, conditioned on the shape latent embedding $\shapehiddenmat$.
%
Specifically, the probabilities of $(\pos_{t-1}, \atomfeat_{t-1})$ denoised from $(\pos_{t}, \atomfeat_{t})$ are estimated by 
the approximates of the ground-truth posteriors $p(\pos_{t-1}|\pos_t, \pos_0)$ (Equation~\ref{eqn:gt_pos_posterior_1}) and 
$p(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_0)$ (Equation~\ref{eqn:gt_atomfeat_posterior_1}).
%in which $\tilde{\pos}_{0,t}$ and $\tilde{\atomfeat}_{0,t}$ are predictions of $\pos_0$ and $\atomfeat_0$ at step $t$.
%
%To approximate the posteriors, 
Given that $(\pos_0, \atomfeat_0)$ is unknown in the generative process,
a prediction module $\molpred$,  which is a graph neural network with multiple layers,  (Section ``Shape-conditioned Molecule Prediction'') %(a multi-layer graph neural network) 
is employed to predict the atom position and feature $(\pos_0, \atomfeat_0)$ at time step $t$
as below,
%
\begin{equation}
(\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0,t}) = \molpred(\pos_t, \atomfeat_t, \hiddenmat^{\mathtt{s}}),
\label{eqn:predictor}
\end{equation}
%
where $\tilde{\pos}_{0,t}$ and $\tilde{\atomfeat}_{0,t}$ are the predictions of $\pos_0$ and $\atomfeat_0$ based on 
the information at $t$ (i.e., $\pos_t$, $\atomfeat_t$ and $\shapehiddenmat$). 
%${\boldsymbol{\Theta}}$
%\hl{$\theta$} \xia{$\boldsymbol{\Theta}$; please update in the entire manuscript} 
%is the learnable parameter.
%
%We will introduce $\molpred(\pos_t, \atomfeat_t, \hiddenmat^{\mathtt{s}})$ in detail in the following sections
%(Section ``Molecule Representation Learning'').

%
Following Ho \etal~\cite{ho2020ddpm}, with $\tilde{\pos}_{0,t}$, the probability of $\pos_{t-1}$ denoised from $\pos_t$, denoted as $p(\pos_{t-1}|\pos_t)$,
can be estimated %\hl{parameterized} \xia{???} 
by the approximated posterior $p_{\boldsymbol{\Theta}}(\pos_{t-1}|\pos_t, \tilde{\pos}_{0,t})$ as below,
%
\begin{equation}
\begin{aligned}
p(\pos_{t-1}|\pos_t) & \approx p_{\boldsymbol{\Theta}}(\pos_{t-1}|\pos_t, \tilde{\pos}_{0,t}) \\
& = \mathcal{N}(\pos_{t-1}|\mu_{\boldsymbol{\Theta}}(\pos_t, \tilde{\pos}_{0,t}),\tilde{\beta}_t^{\mathtt{x}}\mathbb{I}),
\end{aligned}
\label{eqn:aprox_pos_posterior}
\end{equation}
%
where ${\boldsymbol{\Theta}}$ is the learnable parameter; $\mu_{\boldsymbol{\Theta}}(\pos_t, \tilde{\pos}_{0,t})$ is an estimate %estimation
of $\mu(\pos_t, \pos_{0})$ by replacing $\pos_0$ with its estimate $\tilde{\pos}_{0,t}$ 
in Equation~{\ref{eqn:gt_pos_posterior_1}}.
%
Similarly, with $\tilde{\atomfeat}_{0,t}$, the probability of $\atomfeat_{t-1}$ denoised from $\atomfeat_t$, denoted as $p(\atomfeat_{t-1}|\atomfeat_t)$, 
can be estimated %\hl{parameterized} 
by the approximated posterior $p_{\boldsymbol{\Theta}}(\atomfeat_{t-1}|\atomfeat_t, \tilde{\atomfeat}_{0,t})$ as below,
%
\begin{equation}
\begin{aligned}
p(\atomfeat_{t-1}|\atomfeat_t)\approx p_{\boldsymbol{\Theta}}(\atomfeat_{t-1}|\atomfeat_{t}, \tilde{\atomfeat}_{0,t}) 
=\mathcal{C}(\atomfeat_{t-1}|\mathbf{c}_{\boldsymbol{\Theta}}(\atomfeat_t, \tilde{\atomfeat}_{0,t})),\!\!\!\!
\end{aligned}
\label{eqn:aprox_atomfeat_posterior}
\end{equation}
%
where $\mathbf{c}_{\boldsymbol{\Theta}}(\atomfeat_t, \tilde{\atomfeat}_{0,t})$ is an estimate of $\mathbf{c}(\atomfeat_t, \atomfeat_0)$
by replacing $\atomfeat_0$  
with its estimate $\tilde{\atomfeat}_{0,t}$ in Equation~\ref{eqn:gt_atomfeat_posterior_1}.


\begin{comment}
%the probability of $\pos_{t-1}$ to be denoised from $\pos_t$ is 
%calculated as follows,
%$p_{\theta}(\mol_{t-1}|\mol_t, \hiddenmat^{\mathtt{s}})$ \hl{formulates the predictions of atom positions} \xia{what do you mean 
%by ``formulates" and why $p_{\theta}$ formulates the prediction?} as a Gaussian distribution as follows,
%\xia{the following $p_{\theta}(\pos)$ is not the same as $p_{\theta(\mol)}$? need to be clear. }
\begin{eqnarray}
p_{\theta}(\pos_{t-1}|\pos_t, \tilde{\pos}_{0,t}) &= \mathcal{N}(\pos_{t-1}|\mu_{\theta}(\pos_t, \tilde{\pos}_{0,t}), (1-\cumalpha^x_t)\mathbf{I}), \label{eqn:pos_posterior_1}\\
\mu_{\theta}(\pos_t, \tilde{\pos}_{0,t}) & = \frac{\sqrt{\cumalpha^x_{t-1}}\beta_t}{1-\cumalpha^x_t}\tilde{\pos}_{0,t} + \frac{\sqrt{\alpha_t}(1-\cumalpha^x_{t-1})}{1-\cumalpha^x_t}\pos_t, \label{eqn:pos_posterior_2}
%\label{eqn:pos_posterior}
\end{eqnarray}
%
\ziqi{where $\mathcal{N}(\cdot)$ is a Gaussian distribution of $\pos_{t-1}$ with mean parameterized by $\mu_{\theta}(\pos_t, \tilde{\pos}_{0,t})$
and variance determined by $(1-\cumalpha^x_t)\mathbf{I}$.}
%$\tilde{\pos}_0$ is a prediction of $\pos_0$ given $\pos_t$ and shape latent embedding $\hiddenmat^{\mathtt{s}}$.
%
Similarly, as in the previous work \cite{hoogeboom2021catdiff}, \ziqi{the probability of $\atomfeat_{t-1}$ to be denoised from $\atomfeat_t$ is calculated as follows,}
%\hl{the predictions of atom features are formulated as a Categorical distribution as follows,} \xia{what do you mean "prediction is formulated as a distribution"?}
\begin{eqnarray}
p_{\theta}(\atomfeat_{t-1}|\atomfeat_{t}, \tilde{\atomfeat}_{0,t}) = \mathcal{C}(\atomfeat_{t-1}|\mathbf{c}_{\theta}(\atomfeat_t, \tilde{\atomfeat}_{0,t})), \label{eqn:atomfeat_posterior_1}\\
\mathbf{c}_{\theta}(\atomfeat_t, \tilde{\atomfeat}_{0,t}) = \tilde{\mathbf{c}}/{\sum_{k=1}^K \tilde{c}_k}, \label{eqn:atomfeat_posterior_2} \\
\tilde{\mathbf{c}} = [\alpha^v_t\atomfeat_t + (1 - \alphav) / K]\odot[\cumalpha^v_{t-1}\tilde{\atomfeat}_{0,t}+(1-\cumalpha^v_{t-1})/K], \label{eqn:atomfeat_posterior_3}
%\end{gathered}
\label{eqn:atomfeat_posterior}
\end{eqnarray}
where \ziqi{$\mathcal{C}(\cdot)$ is a categorical distribution of $\atomfeat_{t-1}$ with the probabilities across $K$ classes determined by $\mathbf{c}_{\theta}(\atomfeat_t, \tilde{\atomfeat}_{0,t})$.}
%
\ziqi{The details regarding the proof of $\mu_{\theta}(\pos_t, \tilde{\pos}_{0,t})$ and $\mathbf{c}_{\theta}(\atomfeat_t, \tilde{\atomfeat}_{0,t})$ are 
available in Supplementary Section~\ref{XXX}.}
\end{comment}
%$\tilde{\atomfeat}_0$ is a prediction of $\atomfeat_0$ given $\atomfeat_t$ ;
%$\mathbf{c}_{\theta}(\atomfeat_t, \tilde{\atomfeat}_{0,t})$ 
%is the category distribution of $\atomfeat_{t-1}$ with the sum \xia{sum over what?} equals to 1.
%
%For the predictions of $\atomfeat_0$ and $\pos_0$, we employ an equivariant neural network represented as $[\tilde{\pos}_0, \tilde{\atomfeat}_0] = f_\theta(\pos_t, \atomfeat_t, \hiddenmat^{\mathtt{s}})$,
%\xia{this equation should be explicit in one seperate line... this is an important equation... }
%which will be described in the subsequent subsection.
%Specifically, $f_{\theta}(\pos_t, \hiddenmat^{\mathtt{s}})$ in Eq~\ref{eqn:pos_posterior} and $g_\theta(\atomfeat_t, \hiddenmat^{\mathtt{s}})$ in Eq~\ref{eqn:atomfeat_posterior} are parameterized by neural networks and will be discussed in the following subsection.
%
%
%$\molpred$ leverages two multi-layer graph neural networks: 
%(1) an equivariant graph neural network, denoted as \eqgnn, that equivariantly predicts atom positions that change under transformations, and 
%(2) an invariant graph neural network, denoted as \invgnn, that invariantly predicts atom features that remain unchanged under transformations.
%\hl{$f_\theta(\cdot)$ leverages two graph neural networks to predict atom positions in an equivariant way 
%and atom features in an invariant way, given that the atom positions change accordingly but atom features remain unchanged under transformations.} \xia{too long, please simplify}
%
%thus is able to recover the original molecule consistently regardless of any translational or rotational shifts applied to its input.
%
%When transformations are applied to molecules, atom positions change accordingly but atom features remain unchanged.
%
%As a result, $f_\theta(\cdot)$ aims to predict atom positions in an equivariant way and atom features in an invariant way.
%%
%Following the previous work~\cite{guan2023targetdiff,hoogeboom22diff}, the translation equivariance of atom position prediction is achieved by 
%shifting a fixed point (e.g., the center of point clouds \pc) to zero, and therefore only rotation equivariance needs to be considered.
%


\begin{comment}
%................................................................................................................................................
\paragraph{Atom Coordinate Prediction}
%................................................................................................................................................

%%
%In $\molpred$, \eqgnn and \invgnn
%%an equivariant graph neural network and an invariant graph neural network 
%with multiple layers are employed to update the atom position vectors and learn the atom feature embeddings, respectively, for all the atoms.
%%
In \eqgnn, %the equivariant graph neural network,
%
the atom position $\pos_{i}^{l+1}\in \mathbb{R}^{3}$ of $a_i$ %the $i$-th atom
at the ($l$+1)-th layer is calculated in an equivariant way as below,
%
\begin{equation*}
%\begin{aligned}
\Delta\pos_i^{l+1} \! = \sum_{\mathclap{j\in N(a_i), i\neq j}}(\pos_{i,L} - \pos_j^l)\text{MHA}^{\mathtt{x}}(d_{ij}^{l}, \hiddenvec_i^{l+1}, \hiddenvec_j^{l+1}, \text{VN-In}(\shapehiddenmat)),
\end{equation*}
\vspace{-3pt}
%
\begin{equation} 
\label{eqn:diff:graph:pos}
\pos_i^{l+1} \! = \pos_{i,l}\! + \!\text{Mean}(\Delta{\pos_{i}^{l+1}})\! + \!\text{VN-Lin}(\pos_{i,L}, \Delta\pos_i^{l+1}, \shapehiddenmat),
%\end{aligned}
\end{equation}
%
%\xia{Ziqi: please double check the above equations!}
%\xia{can you verify in the first equation above, it is $\hiddenvec_i^{l+1}$, not $\hiddenvec_{i,l}$?}
%\xia{$\text{VN-In}(\hiddenmat^{\mathtt{s}})$ appears multiple times. you should have $\hiddenvec_s = \text{VN-In}(\hiddenmat^{\mathtt{s}})$
%explicitly defined the first time it is used and refer to it here. }
where $N(a_i)$ is the set of $N$-nearest neighbors of $a_i$ %the $i$-th atom 
based on atomic distances; %\hl{based on the atom position $\pos_i$}\xia{be precise};
$\Delta\pos_i^{l+1}\in \mathbb{R}^{n_h\times 3}$  %\xia{whose embedding?} 
aggregates the neighborhood information of $a_i$; %around atom $a_i$};
$\text{MHA}^{\mathtt{x}}(\cdot)$ denotes the multi-head attention layer in \eqgnn 
with $n_h$ heads;
$d_{ij,l}$ is the distance between $i$-th and $j$-th atom positions $\pos_{i,l}$ and $\pos_j^{l}$ at the $l$-th layer;
%\xia{bad notation!} 
%and parameters \ziqi{$\theta_{x}$}; %\xia{bad notation! unify all the function presentations};
$\text{Mean}(\Delta{\pos_{i}^{l+1}})$ converts $\Delta\pos_i^{l+1}$ into a 3D vector via meaning pooling
to adjust the atom position; 
$\text{VN-Lin}(\cdot)\in \mathbb{R}^{3}$ %\st{$\in \mathbb{R}^{3}$} 
denotes the equivariant VN-based linear layer~\cite{deng2021vn}. %with parameters $\theta_s$;%\hl{$\theta^2_x$} \xia{bad notation!};
%
%the input atom position embedding \hl{$\pos_i^0=\pos_i$}\xia{???}
%\ziqi{$\text{Mean}(.)$} is a mean-pooling operation that converts $\Delta\pos_i^{l+1}\in \mathbb{R}^{N_\text{h}\times 3}$ to a 3-dimentional vector.
%
%The matrix $\Delta\pos_i^{l+1}$ %encodes the neighborhood information around atom $i$ and 
%\ziqi{is converted to a 3-dimentional vector via a mean pooling operation to adjust the atom position.}
%\hl{is used to adjust the atom position via a mean-pooling operation over its $d_h$ dimension. }\xia{???}
%
$\text{VN-Lin}(\cdot)$ adjusts the atom positions to fit the shape condition represented by $\shapehiddenmat$, by considering the 
current atom positions $\pos_{i,L}$ and the neighborhood information $\Delta\pos_i^{l+1}$.
%
The learned atom position $\pos_{i,l}$ at the last layer $L$ of \eqgnn is used as the prediction of 
$\tilde{\pos}_{i, 0}$, that is,  
\begin{equation}
\tilde{\pos}_{i, 0} = \pos_{i,l}. 
\end{equation}


%................................................................................................................................................
\paragraph{Atom Feature Prediction}
%................................................................................................................................................

%\st{In the invariant graph neural network, }\hl{
In \invgnn, inspired by the previous work~\cite{guan2023targetdiff} and VN-Layer~\cite{deng2021vn},
%\xia{should be \mbox{VN-Layer} and thus non-broken hyphen}, 
the atom feature embedding $\hiddenvec_{i}^{l+1}\in \mathbb{R}^{d_h}$ of the $i$-th atom $a_i$ at the ($l$+1)-th layer of \invgnn
%\xia{what about ``$i$"?} 
is updated in an invariant way as follows,
%
\begin{equation}
\label{eqn:diff:graph:atomfeat}
\!\!\!\hiddenvec_{i}^{l+1}\!\!=\!\!\hiddenvec_{i,l}\!+\!\sum_{\mathclap{j\in N(a_i), i\neq j}} \text{MHA}^{\mathtt{h}}(d_{ij,l}, \hiddenvec_{i,L}, \hiddenvec_j^l, \text{VN-In}(\shapehiddenmat)),\\
\hiddenvec_{i}^0\!=\!\atomfeat_i, \!\!\!
\end{equation}
where $\text{MHA}^{\mathtt{h}}(\cdot)\in\mathbb{R}^{d_h}$ denotes the multi-head attention layer in \invgnn. 
%$\hiddenvec_{i}^0$ is initialized with the embedding of atom feature $\atomfeat_i$.
%
The learned atom feature embedding $\hiddenvec_{i,L}$ at the last layer $L$ encodes the neighborhood information of $a_i$ and the conditioned molecular shape, and is used to predict the atom features as follows: 
% 
%\hl{This embedding is then fed into an MLP layer to predict the probability of} \ziqi{$\tilde{v}_{i, 0}$} \hl{across $K$ classes and sample
%a discrete atom feature} \ziqi{$\tilde{v}_{i, 0}$} \hl{with the maximum probability as below,}%\xia{???}
\begin{equation}
\tilde{\atomfeat}_{i, 0} = \text{softmax}(\text{MLP}(\hiddenvec_{i,L})).
\label{eqn:diff:graph:atompred}
\end{equation}
%\xia{Ziqi: please double check the above equation!!}
%\xia{need the equations here}
%\hl{the categories of atom features} \xia{what do you mean? 
%types of features, such as continous features or discrete features? need to be precise. need the explicit MLP function. }.
The proof of equivariance in Equation~\ref{eqn:diff:graph:pos}
and invariance in Equation~\ref{eqn:diff:graph:atomfeat} is available in Supplementary Section~\ref{supp:backward:equivariance} and ~\ref{supp:backward:invariance}. %, respectively.
\end{comment}

%-----------------------------------------------------------------------
%\subsubsection{Optimization Objective}
\subsubsection{Model Training}
\label{section:diff:opt}
%-----------------------------------------------------------------------


\method optimizes \methoddiff by minimizing the following three losses.

%.........................................................................................
\paragraph{Atom Position Loss}
%.........................................................................................
%
\method measures the squared errors between the predicted positions ($\tilde{\pos}_{0,t}$) from the prediction module $\molpred$ (Equation~\ref{eqn:predictor}) and the ground-truth positions ($\pos_0$) of atoms in molecules.
%
%\ziqi{Details about how to calculate $\tilde{\pos}_{0,t}$ will be discussed later in Equation~\ref{xxx}.}
%
Given a particular step $t$, the loss is calculated as follows:
\begin{equation}
\label{eqn:diff:obj:pos}
\begin{aligned}
%& \mathcal{L}^\mathtt{x}_t({\mol}) = w_t^\mathtt{x}\sum\nolimits^{|\scriptsize{\mol}|}_i\|\tilde{\pos}_{i, 0,t} - \pos_{i, 0}\|^2, \\
& \mathcal{L}^\mathtt{x}_t({\mol}) = w_t^\mathtt{x}\sum\nolimits_{\forall a \in {\scriptsize{\mol}}}\|\tilde{\pos}_{0,t} - \pos_{0}\|^2, \\
& \text{where} \ w_t^\mathtt{x} = \min(\lambda_t, \delta),\ \lambda_t={\bar{\alpha}^{\mathtt{x}}_t}/({1-\bar{\alpha}^{\mathtt{x}}_t}),
\end{aligned}
\end{equation}
%
%\xia{is my equation in red above better?}
where $w_t^\mathtt{x}$ is a weight at step $t$, and is calculated by clipping the signal-to-noise ratio 
$\lambda_t>0$ %\hl{$\lambda_t=\frac{\cumalphapos_t}{1-\cumalphapos_t}$} \xia{(Equation ???)} 
with a threshold $\delta > 0$. %\xia{need the clipping equation here}.
%
Note that because $\bar{\alpha}_t^{\mathtt{x}}$ decreases monotonically as $t$ increases from 1 to $T$ (Equation~\ref{eqn:noiseschedule}), $w_t^\mathtt{x}$ decreases monotonically over $t$ as well until it is clipped. 
%
Thus, $w_t^\mathtt{x}$ imposes lower weights on the loss when the noise level in $\pos_t$ is higher (i.e., $t$ close to $T$). %at later/larger step $t$). 
%
This encourages the model training to focus more on accurately recovering molecule structures when there are 
sufficient signals in the data, rather than being potentially confused by major noises in the data. 
%

%\ziqi{
%%
%$\lambda_t$ decreases monotonically over $t$ increasing from 1 to $T$ due to $\bar{\alpha}_t^{\mathtt{x}}$ (Equation~\ref{eqn:noiseschedule}), and thus $w_t^\mathtt{x}$ decreases monotonically until it is clipped.
%%
%Thus, 
%This means that the loss at higher step $t$ in which the input has higher noise, is assigned less weight in the loss function,
%compared to the earlier steps $t$ with lower noise levels.
%%
%Given that predicting from high-noise inputs could be challenging and potentially dominate the overall loss across $t$, using $w_t^{\mathtt{x}}$ can
%prevent \methoddiff from focusing too much on these predictions, which could increase the prediction accuracy at lower noise levels and 
%improve the quality of the generated molecules.
%} 

%.........................................................................................
\paragraph{Atom Feature Loss}
%.........................................................................................
%
\method also minimizes the KL divergence~\cite{kullback1951information} between the 
ground-truth posterior $p(\atomfeat_{t-1}|\atomfeat_t, \atomfeat_0)$ (Equation~\ref{eqn:gt_atomfeat_posterior_1}) 
and its approximate
$p_\theta(\atomfeat_{t-1}|\atomfeat_{t}, \tilde{\atomfeat}_{0,t})$ (Equation~\ref{eqn:aprox_atomfeat_posterior}) 
for discrete atom features
to optimize \methoddiff, following the literature~\cite{hoogeboom2021catdiff}. 
%\xia{need to explicitly point out this is for discrete atom features. }
%
Particularly, the KL divergence at $t$ for a given molecule $\mol$ is calculated as follows:
%\bo{No equation number, the subscribe of $\sum$ is not consistent with the above equation.}
%
\begin{equation*}
\mathcal{L}^{\mathtt{v}}_t({\mol})  = \sum\nolimits_{\forall a \in \scriptsize{\mol}}\text{KL}(p(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_{0}) | p_{\boldsymbol{\Theta}}(\atomfeat_{t-1}|\atomfeat_{t}, \tilde{\atomfeat}_{0,t}) ),
\end{equation*}
%
\vspace{-10pt}
\begin{equation}
\label{eqn:lossv}
\! \! \! \! \! \!\! \! \! \! \! \!= \sum\nolimits_{\forall a\in \scriptsize{\mol}}\text{KL}(\mathbf{c}(\atomfeat_{t}, \atomfeat_{0})|\mathbf{c}_{\boldsymbol{\Theta}}(\atomfeat_{t}, \tilde{\atomfeat}_{0,t})),
\end{equation}
%
where $\mathbf{c}(\atomfeat_{t}, \atomfeat_{0})$ is a categorical distribution of $\atomfeat_{t-1}$  (Equation~\ref{eqn:gt_atomfeat_posterior_2});
$\mathbf{c}_{\boldsymbol{\Theta}}(\atomfeat_{t}, \tilde{\atomfeat}_{0,t})$ is an estimate of $\mathbf{c}(\atomfeat_{t}, \atomfeat_{0})$ (Equation~\ref{eqn:aprox_atomfeat_posterior}).
%
%
%Considering both $\mathcal{L}^x_t$ and $\mathcal{L}^v_t$, 


%.........................................................................................
\paragraph{Bond Type Loss}
%.........................................................................................
%
\method also minimizes the classification errors between the predicted bond types $(\bondpred_{ji})$ and the ground-truth types $\bondemb_{ij}$ of bonds in molecules. 
%
At step $t$, for each $l$-th layer of $\molpred$, \method predicts the bond types $(\bondpred_{ij,t,l})$ to understand the 
relations among atoms. %using $\bondpred_{ji,t,l}$. %the predicted bond types. 
%topology structures of 3D molecules (i.e., bonds between atoms) using the predicted bond types. %\xia{include $\bondemb$ in notation table; bond type and representation should be 
%included in definitions and notations}
%
%\ziqi{The learned bond type at layer $l$ is then added to the }
%
Details about the calculation of $\bondpred_{ij,t,l}$ will be discussed later in Equation~\ref{eqn:bond_type}.
%
Given a particular step $t$, the error 
on bond type prediction
at the $l$-th layer is calculated as follows:
%\xia{Equation notations/scripts are weird -- please update; $\pos$ is not summed out} \ziqi{fixed}
%
\begin{equation}
\label{eqn:loss_bond}
\mathcal{L}^{\mathtt{b}}_{t,l}({\mol}) = \sum_{\forall a_i\in \mol}\sum_{\forall a_j \in N(\scriptsize{\pos_{i,t}})} \text{H}(\bondpred_{ij,t,l}, \bondemb_{ij}),
\end{equation}
%
where {$N(\pos_{i,t})$ denotes the $k$-nearest neighbors of atom $a_i$ in position $\pos_{i,t}$}; %\hl{$\text{knn}(\pos_t)$ denotes the $k$-nearest neighbor graph based on $\pos_t$}
%\xia{what does this mean?}; 
$\text{H}(\cdot)$ denotes the cross-entropy loss. 
%\xia{cross entropy is represented as $H$ conventionally!}. 
%
The bond type prediction loss across different layers is then aggregated as follows:
%
\begin{equation}
\mathcal{L}^{\mathtt{b}}_{t}({\mol}) = \frac{w_t^\mathtt{x}}{L-1}\sum_{l=1}^{L-1} \mathcal{L}^{\mathtt{b}}_{t,l}({\mol}) + w_t^\mathtt{x}\mathcal{L}^{\mathtt{b}}_{t, L}({\mol}),
\label{eqn:diff:obj:bond}
\end{equation}
%
where $w_t^\mathtt{x}$ is the weight at step $t$ used in Equation~\ref{eqn:diff:obj:pos}; $L$ is the number of layers in \molpred. %of graph neural networks.
%
Same with the Equation~\ref{eqn:diff:obj:pos}, the $w_t^\mathtt{x}$ in Equation~\ref{eqn:diff:obj:bond} is used to encourage the model training to focus more on accurately predicting bond types when the data provides sufficient signals, % \bo{(@Ziqi e.g., $t<$XXX)}, 
rather than being confused by major noises in the data. %\bo{(@Ziqi e.g., $t>$XXX)}.
%
%\ziqi{Each layer of graph neural networks is optimized to accurately predict bond types, which are then incorporated back into the modeling of 3D molecules in the next layer.
%
%Particularly, the bond type predictions in the last layer of graph neural networks }
%\bo{
%@Ziqi                                 what's the point here. It does not look like the motivation of predicting bond types. Could you check the following motivation?
%}
%
%\ziqi{
%\method predicts bond types to explicitly capture the relations among atoms and better understand the topology structures for improved generation.
%}
%\bo{
%Following XXX, \method predicts bond types to explicitly capture the relations among atoms, and better understand the molecule structures for improved generation.
%}
%
Note that, similar to Jumper \etal~\cite{Jumper2021}, in Equation~\ref{eqn:diff:obj:bond}, \method uses different weights on the last layer (i.e., $l$=$L$) and all the other layers, 
as we empirically find this design benefits the generation performance.
%
%\bo{This is the motivation of $w_t^\mathtt{x}$ not the equation right?}
%
%\bo{Why do you want to separate the $L$-th layer in the above equation?}

%\bo{Not clear. What do you mean by major noise?}


%.........................................................................................
\paragraph{Overal \method Loss}
%.........................................................................................
%
The overall \method loss function is defined as follows:
%
\begin{equation}
\label{eqn:loss}
\mathcal{L} =\sum\nolimits_{\forall \scriptsize{\mol}\in\mathcal{M}}\sum\nolimits_{\forall t \in \mathcal{T}} (\mathcal{L}^{\mathtt{x}}_t(\mol) + \xi \mathcal{L}^{\mathtt{v}}_t(\mol) + \zeta \mathcal{L}^{\mathtt{b}}_{t}({\mol})),
\end{equation}
%
where $\mathcal{M}$ is the set of all the molecules in training; 
$\mathcal{T}$ is the set of timesteps; 
$\xi > 0$ and $\zeta > 0$ are two \mbox{hyper-parameters} to balance $\mathcal{L}^{\mathtt{x}}_t$(\mol), $\mathcal{L}^{\mathtt{v}}_t$(\mol) and $\mathcal{L}^{\mathtt{b}}_{t}({\mol})$.
%
%\bo{Why the weight on $\mathcal{L}^{\mathtt{x}}_t$(\mol) is not (1-$\xi$-$\zeta$)}
During training, step $t$ is uniformly sampled from $\mathcal{T}=\{1, 2, \cdots, 1000\}$.
%
The derivation of the loss functions
%\ziqi{and the details of the model training are available}
is available in Supplementary Section~\ref{supp:training:loss}. 


%-----------------------------------------------------------------------
\subsection*{Shape-conditioned Molecule Prediction ($\molpred$)}
\label{sec:method:rep:gvp}
%-----------------------------------------------------------------------


In \diffgenerative, the prediction module $\molpred$ (Equation~\ref{eqn:predictor}) 
predicts the atom positions and features $(\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0,t})$ %in the original molecule 
given the noisy data $(\pos_t, \atomfeat_t)$ conditioned on %on a shape latent embedding 
$\shapehiddenmat$.
%
%
For brevity, in this section, we eliminate the subscript $t$ in the notations when no ambiguity arises.
%
Particularly, as presented in Fig.~\ref{fig:overall}(d), %as will be presented in Section ``Shape-aware Atom Representation Learning'', 
$\molpred$ is a multi-layer graph neural network (GNN) comprising $L$ layers. %\xia{is a XXX neural network and} comprises $L$ layers.
%
In the $l$-th layer,
$\molpred$ 
uses the geometric vector perceptron (GVP) 
to learn a scalar embedding $\hiddenatomfeat_{i,l} \in\mathbb{R}^{d_a}$
and a vector embedding $\hiddenpos_i \in\mathbb{R}^{3\times d_r}$
for atom $a_i$ in an alternative manner that guarantees the invariance of $\hiddenatomfeat_{i,l}$ and the equivariance of $\hiddenpos_{i,l}$~\cite{jing2021learning}.
%
Intuitively, $\hiddenatomfeat_{i,l}$ captures inherent properties (e.g., atom types) of atom $a_i$, which are invariant of the molecule's orientation or position in 3D space.
%
%\st{On the other hand}, 
Different from $\hiddenatomfeat_{i,l}$, $\hiddenpos_{i,l}$ captures geometric information (e.g., atom positions) of atom $a_i$, which will change under different %dependent on %\st{and changed with} 
transformations. %(e.g., atom positions).
%
We note that existing work primarily employs equivariant graph neural networks (EGNN)~\cite{satorras2021} %\bo{unify all the capitalization for abbreviations} 
for the prediction.
%
However, EGNN could suffer from limited capacity in capturing rich geometric information within molecules as it can only represent geometric features in a 3-dimensional latent space.
%
In contrast, GVP exhibits stronger expressiveness, capable of learning latent embeddings in spaces of any dimensions~\cite{torge2023diffhopp}.
%
Equipped with GVP, {$\molpred$} enables the learning of effective representations for geometric information.
%
Note that to ensure translation equivariance, \molpred shifts a fixed point (i.e., the center of shape condition $\pc$) to zero to eliminate all translations.
%
Therefore, only rotation equivariance needs to be considered. 

%
\molpred also leverages shape-aware scalar embeddings $\hat{\hiddenatomfeat}_{i,l}$ and vector embeddings $\hat{\hiddenpos}_{i,l}$ to generate molecules tailored to the shape condition. 
%
\molpred learns $\hat{\hiddenatomfeat}_{i,l}$ from $\hiddenatomfeat_{i,l}$ using the shape representation $\shapehiddenmat$ in an invariant way (Equation~\ref{eqn:shapea}). 
Similarly, \molpred learns $\hat{\hiddenpos}_{i,l}$ from $\hiddenpos_{i,l}$ and $\shapehiddenmat$ in an equivariant manner (Equation~\ref{eqn:shaper}).
%, as described in Section ``Shape-aware Atom Representation Learning.'' %(Equation~\ref{eqn:shapea} and \ref{eqn:shaper}).
%
%Similarly, based on $\shapehiddenmat$, $\molpred$ also learns shape-aware vector embeddings from $\hiddenpos_{i,l}$ that are equivariant to transformations (Equation~\ref{eqn:shaper}).
%
%
In addition, \molpred utilizes the bond type embeddings to enhance the understanding of relations among atoms for better prediction (Equation~\ref{eqn:bond_type}).

% shape-aware scalar embeddings from $\hiddenatomfeat_{i,l}$ by incorporating the shape representation $\shapehiddenmat$ in an invariant way (Equation~\ref{eqn:shapea}).
%
%Similarly, based on $\shapehiddenmat$, $\molpred$ also learns shape-aware vector embeddings from $\hiddenpos_{i,l}$ that are equivariant to transformations (Equation~\ref{eqn:shaper}).
%
%The shape-aware scalar embeddings and vector embeddings are then leveraged to encourage the predictions of atom positions tailored to the shape condition.
%
%\molpred also leverages the bond type embeddings to enhance the prediction performance (Equation~\ref{eqn:bond_type}).
%
%Particularly, $\molpred$ learns the inherent interactions between molecule atoms and the molecule surface shape, which are invariant of any orientations or positions of the molecule and the shape condition in 3D space (e.g., the signed distances from atoms to the shape). 
%
%$\molpred$ updates $\hiddenatomfeat_{i,l}$ with the learned inherent interactions in an invariant way.
%
%$\molpred$ also updates the vector embedding $\hiddenpos_{i,l}$ with the shape representations $\shapehiddenmat$ to learn their interactions in geometric features in an equivariant way.
%
%This approach enables $\molpred$ to more effectively predict molecules that fit into the shape condition from the learned interactions, compared to existing methods~\cite{adams2023equivariant} as shown in Table~\ref{tbl:overall}.


%$\hiddenatomfeat_{i,l}$ captures invariant features in $a_i$, which remain unchanged under transformations 
%\hl{(e.g., atom types or
%types of their neighboring atoms and the distances between them).} \xia{what does this example mean?}
%
%In contrast, 
%$\hiddenpos_{i,l}$ represents
%equivariant features in $a_i$,
%which are \hl{changed under transformations (e.g., atom positions and their spatial relations with neighboring atoms)}
%(\bo{@Ziqi, this is not how you define equivariance in previous sections...}) \xia{what does the example mean?}
%

%By using GVP, \methoddiff XXX (\bo{@Ziqi, the motivation of using GVP here})
%
%We note that existing work primarily employs equivariant graph neural networks (EGNN)~\cite{satorras2021} for the prediction.
%
%However, EGNN is limited to XXX/suffers from XXX (\bo{@Ziqi present the limitation of EGNN here}).
%
%In contrast, 
%GVP XXX (\bo{@Ziqi how GVP address the issues})
%
%Equipped with GVP, \methoddiff enables XXX (\bo{@Ziqi}) compared to existing methods.


%\begin{comment}
%%................................................................................................................................................
%\paragraph{\ziqi{GVP-based Graph Attention Neural Networks}}
%%................................................................................................................................................
%%
%\ziqi{\methoddiff represents each atom using an invariant scalar embedding $\hiddenatomfeat_{i,L}$ and an equivariant vector embedding $\hiddenpos_{i,L}$ for each $i$-th atom $a_i$. 
%%
%These two embeddings are learned to capture the local neighborhood geometric structures by propagating messages over k-nearest neighbor graphs.
%%
%Each pair of k-nearest neighbor atoms $(a_j, a_i)$ is associated with two messages $\hiddenmessatom_{ji,l}\in\mathbb{R}^{d_a}$ and $\hiddenmesspos_{ji,l}\in\mathbb{R}^{3\times d_r}$ encoding the messages over scalar embeddings and vector embeddings propagating from atom $a_j$ to $a_i$.
%%
%At the $l$-th layer, the message $\hiddenmessatom_{ji,l}$ and $\hiddenmesspos_{ji,l}$ from atom $j$ to $i$ for scalar embeddings and vector embeddings are calculated as follows,
%%
%\begin{equation}
%\hiddenmessatom_{ji,l} = \text{MLP}(\tilde{\hiddenatomfeat}_j^{l-1}, d_{ji}, \tilde{\bondemb}_{ji}^{l-1}),
%\end{equation}
%%
%\begin{equation}
%\hiddenmesspos_{ji,l} = \text{MLP}(\tilde{\hiddenpos}_j^{l-1}, \pos_j - \pos_i),
%\end{equation}
%%
%\begin{equation}
%\hiddenmessatom_{ji,l}, \hiddenmesspos_{ji,l} = \text{GVP}(\hiddenmessatom_{ji,l}, \hiddenmesspos_{ji,l}),
%\end{equation}
%%
%where %$f_m(.)$ and $f_n(.)$ are two single MLP layers with ReLU as the activation function;
%$\bondemb_{ji}^{l-1}$ is the embedding of the bond type between $a_i$ and $a_j$ (in Equation~\ref{eqn:bond_type});
%%\hl{need to update the description of bond type if the results of the model with predicted bond types are better than that with predefined bond types.}
%$\text{GVP}(.)$ is a function with three GVP blocks to enable information to be transformed and passed between the scalar-based messages $\hiddenmessatom_{ji,l}$ and vector-based messages $\hiddenmesspos_{ji,l}$ \xia{not clear}. 
%%
%%$\atomfeat_i^0 = \text{MLP}(\atomfeat_i, t)$ is initialized with the atom type embedding $\atomfeat_i$ and the time step embedding $t$.
%%
%The $\hiddenatomfeat_{i,L}$ and $\hiddenpos_{i,L}$ are then updated with messages from neighboring atoms as follows,
%%
%\begin{equation}
%\hiddenatomfeat_{i,l} = \text{MLP}(\atomfeat_i, \tilde{\hiddenatomfeat}_{i,l-1}, \sum_{j\in N(i)}e_{ji}\hiddenmessatom_{ji,l}, t),\quad \hiddenpos_{i,l} = \text{MLP}(\pos_i, \tilde{\hiddenpos}_{i,l-1}, \sum_{j\in N(i)}e_{ji}\hiddenmesspos_{ji,l}),
%\end{equation}
%%
%\begin{equation}
%\hiddenatomfeat_{i,l}, \hiddenpos_{i,l} = \text{GVP}(\hiddenatomfeat_{i,l}, \hiddenpos_{i,l}),
%\end{equation}
%where $N(i)$ denotes the k-nearest neighbor atoms of atom $i$ over the 3D space;
%$e_{ji}$ is the attention weight used to aggregate messages from neighboring atoms.
%\xia{how about v?}
%%
%The weight $e_{ji}$ is calculated to estimate how much the neighboring atom $j$ should contribute to the representation of local neighborhood structure of atom $i$ as follows,
%\begin{equation}
%\begin{aligned}
%e_{ij} & = \frac{\exp(Q_{i}K_{ji})}{\sum_{k\in N(i)}\exp(Q_{i}K_{ki})},\\
% \text{where}\  Q_i& =\text{MLP}(\tilde{\hiddenatomfeat}_{i,l-1}, \|\tilde{\hiddenpos}_{i,l-1}\|^2), \\
%K_{ji}&=\text{MLP}(\hiddenmessatom_{ji}^{l-1}, \|\hiddenmesspos_{ji}^{l-1}\|^2).
%\end{aligned}
%\end{equation}
%%
%After $L$ iterations, \methoddiff predicts the type $\tilde{\atomfeat}_{0, i}$ and position $\tilde{\pos}_{0, i}$ of the $i$-th atom as follows,
%\begin{equation}
%\tilde{\atomfeat}_{0, i} = \text{softmax}(\text{MLP}(\hiddenatomfeat_{i,L})), \quad \tilde{\pos}_{0, i} = \pos_i + \text{MLP}(\hiddenpos_{i,L}), 
%\end{equation}
%where $\tilde{\atomfeat}_{0, i}$ is the predicted probability distribution across all the types of $a_i$; $\tilde{\pos}_{0, i}$ is the predicted position of $a_i$.
%}
%
%\bo{What's the intuition of calculating the weights using these equations}
%
%\end{comment}

%\todo{equation 27 input and output notation cannot be the same; add subscript t to the notations in equation 29}

%\todo{add algorithm}

%{\methoddiff 
{
Particularly, 
%$\molpred(\pos_t, \atomfeat_t, \shapehiddenmat)$
$\molpred$
estimates the type 
and position 
of the $i$-th atom $a_i$ as follows, }
%\xia{refer to where these terms are used in earlier equations in DIFF}
%
%\bo{This equation is not intuitive. It's not obvious how $\atomfeat_t$ and $\shapehiddenmat$ are used in the predictor}

%\ziqi{$\molpred(\pos_t, \atomfeat_t, \shapehiddenmat)$, after going through $L$ layers of graph neural network, captures and integrates the inherent and geometric features of an atom $(\pos_i, \atomfeat_i)$ into comprehensive representations $\hiddenatomfeat_{i,L}$ and $\hiddenpos_{i,L}$, respectively, with shape condition $\shapehiddenmat$ incorporated.
%With the $\hiddenatomfeat_{i,L}$ and $\hiddenpos_{i,L}$, $\molpred(\pos_t, \atomfeat_t, \shapehiddenmat)$
%predicts the atom positions and features $(\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0,t})$ as follows,
%}
\begin{equation}
\tilde{\pos}_{0, i} = \pos_i + %\text{MLP}(
\hiddenpos_{i,L}
%)
, \quad \tilde{\atomfeat}_{0, i} = \text{softmax}(\text{MLP}(\hiddenatomfeat_{i,L})), 
\label{eqn:pred:pred}
\end{equation}
where $\tilde{\atomfeat}_{0, i}$ (Equation~\ref{eqn:predictor}) is the predicted probability distribution across all the types of atom features; %$a_i$}
%\xia{????}; 
$\tilde{\pos}_{0, i}$ (Equation~\ref{eqn:predictor}) is the predicted position of $a_i$; 
%
$\pos_i$ is the noisy position of $a_i$;
%
$\hiddenatomfeat_{i,L}$ and $\hiddenpos_{i,L}$ are the invariant scalar embedding and equivariant vector embedding for atom $a_i$, respectively. %\ziqi{at the last layer of GNN}.
%
%\st{These two embeddings are learned to capture the local neighborhood geometric structures} \hl{using L $\text{GVP}(\cdot)$ functions} 
%\xia{????} as follows:
%
In each $l$-th layer, $\hiddenatomfeat_{i,l}$ and $\hiddenpos_{i,l}$ of atom $a_i$ are updated by propagating its neighborhood's inherent features and geometric features as follows,
\begin{equation}
\hiddenatomfeat_{i,l}, \hiddenpos_{i,l} = \text{GVP}(\mathbf{h}_{i,l}, \mathbf{y}_{i,l}),
\label{eqn:pred:gvp}
\end{equation}
\begin{equation}
\label{eqn:geometric_embedding}
\mathbf{h}_{i,l}= %\text{MLP}(
[\atomfeat_i, \hat{\hiddenatomfeat}_{i,l-1}, \sum_{j\in N(i)}e_{ji,l}\hiddenmessatom_{ji,l}, t]%)
, \quad
 \mathbf{y}_{i,l} = %\text{VN-MLP}(
 [\pos_i, \hat{\hiddenpos}_{i,l-1}, \sum_{j\in N(i)}e_{ji,l}\hiddenmesspos_{ji,l}],%),
\end{equation}
\begin{equation}
\hat{\hiddenatomfeat}_{i, l-1}, \hat{\hiddenpos}_{i, l-1} = \SA(\hiddenatomfeat_{i, l-1}, \hiddenpos_{i, l-1}, \shapehiddenmat),
\end{equation}
%\todo{from ziqi: @Bo, do you have any suggestion about the better notation for ${\hiddenatomfeat}_i^{*l-1}$?}
where $\text{GVP}(\cdot)$ is a function that learns $\hiddenatomfeat_{i,l}$ and $\hiddenpos_{i,l}$ jointly from $\mathbf{h}_{i,l}\in \mathbb{R}^{d_h}$ and $\mathbf{y}_{i,l}\in\mathbb{R}^{3\times d_y}$; 
$[\cdot,\cdot]$ is the concatenation operation;
{$N(i)$ denotes the $k$-nearest neighbor atoms of atom $a_i$ over the 3D space;} %\xia{unify the notation and definition};
%
%\xia{In Equation 23, where is $t$ from? representation in Equation 24}
$t$ denotes the time step;
%
$\atomfeat_i$ is the noisy feature vector of $a_i$;
%
%\bo{
%$\tilde{\hiddenatomfeat}_{i,l-1}$ and $\tilde{\hiddenpos}_{i,l-1}$ are the shape-aware atom scalar and vector embedding, respectively as will be detailed in Equation~\ref{eqn:shapea} and Equation~\ref{eqn:shaper}.
%}
%
$\hiddenmessatom_{ji,l}\in\mathbb{R}^{d_m}$ and $\hiddenmesspos_{ji,l}\in\mathbb{R}^{3\times d_n}$ are messages to propagate information from $a_j$ to $a_i$ as will be described in Equation~\ref{eqn:message};
%
%
$e_{ji,l}$ is the attention weight used to aggregate information from neighboring atoms;
%
%
\SA is a module to learn shape-aware atom embeddings as will be introduced later; 
$\hat{\hiddenatomfeat}_{i,l-1}$ and $\hat{\hiddenpos}_{i,l-1}$ are the shape-aware atom scalar and vector embedding, respectively (detailed in Equation~\ref{eqn:shapea} and Equation~\ref{eqn:shaper}). 
%
The weight $e_{ji,l}$ is calculated to estimate how much the neighboring atom $a_j$ should contribute to the 
learning of $\mathbf{h}_{i,l}$ and $\mathbf{y}_{i,l}$ as follows,
%representation of local neighborhood structure of atom $i$ as follows,
\begin{equation}
\label{eqn:attention}
\begin{aligned}
e_{ji,l} & = \frac{\exp(Q_{i,l}K_{ji,l})}{\sum_{k\in N(i)}\exp(Q_{i,l}K_{ki,l})},\\
 \text{where}\  Q_{i,l}& =\text{MLP}([\hat{\hiddenatomfeat}_{i,l-1}, \|\hat{\hiddenpos}_{i,l-1}\|^2]), \\
K_{ji,l}&=\text{MLP}([\hiddenmessatom_{ji,l}, \|\hiddenmesspos_{ji,l}\|^2]).
\end{aligned}
\end{equation}
%where both $\hiddenmessatom_{ji,l}$ and $\hiddenmesspos_{ji,l}$ are messages from atom $j$ to $i$. 
%
%\bo{
%@Ziqi add a $l$ superscript in $Q$, $K$ and $e_{ij}$.
%}
%
In both Equation~\ref{eqn:geometric_embedding} and \ref{eqn:attention}, the messages $\hiddenmessatom_{ji,l}$ and $\hiddenmesspos_{ji,l}$ are calculated from the scalar embeddings (e.g., $\hat{\hiddenatomfeat}_{i,l}$) and vector embeddings (e.g., $\hat{\hiddenpos}_{i,l}$) of atoms as follows,
\begin{equation}
\hiddenmessatom_{ji,l}, \hiddenmesspos_{ji,l} = \text{GVP}(\hat{\hiddenmessatom}_{ji,l}, \hat{\hiddenmesspos}_{ji,l}),
\label{eqn:mess:gvp}
\end{equation}
%
\begin{equation}
\label{eqn:message}
\hat{\hiddenmessatom}_{ji,l} = %\text{MLP}(
[\hat{\hiddenatomfeat}_{j,l-1}, d_{ji}, \bondpred_{ji,l-1}], %), 
\quad \hat{\hiddenmesspos}_{ji,l} = %\text{MLP}(
[\hat{\hiddenpos}_{j,l-1}, \pos_j - \pos_i], %),
\end{equation}
%
where $\text{GVP}(\cdot)$ is a function that learns $\hiddenmessatom_{ji,l}$ and $\hiddenmesspos_{ji,l}$ jointly from $\hat{\hiddenmessatom}_{ji,l}\in\mathbb{R}^{d_m}$ and $\hat{\hiddenmesspos}_{ji,l}\in\mathbb{R}^{3\times d_n}$; 
%$f_m(.)$ and $f_n(.)$ are two single MLP layers with ReLU as the activation function;
$[\cdot,\cdot]$ is the concatenation operation;
%
$\bondpred_{ji,l-1}$ is the embedding of the bond type between $a_i$ and $a_j$ (detailed in Equation~\ref{eqn:bond_type}); and $d_{ji}$ is the distance between $\pos_i$ and $\pos_j$.
%\bo{@Ziqi what is $d_{ij}$?}
%\xia{how about other terms in the equations?}.
%\hl{need to update the description of bond type if the results of the model with predicted bond types are better than that with predefined bond types.}
%$\text{GVP}(.)$ is a function with three GVP blocks to enable information to be transformed and passed between the scalar-based messages $\hiddenmessatom_{ji,l}$ and vector-based messages $\hiddenmesspos_{ji,l}$.

\begin{comment}
%................................................................................................................................................
\paragraph{\ziqi{Shape Condition Learning}}
%................................................................................................................................................
%

\hl{change interaction to another word}

\ziqi{Different atoms should interact with the shape representation in a different way.
\xia{should not start a motivation with a statement like this unless it is widely accepted in the literature. Here this is only your claim}
%
For example, to generate molecules that fit within a specific shape, atoms located outside and inside the conditioned shape could be moved in two different directions. \xia{what do you mean? why two directions?}
%
Inspired by this, for each atom in a molecule, its interaction with $\shapehiddenmat$ can be modeled in a similar way as in \SED to learn its relative position with respect to the conditioned shape.
\xia{could not understand what you mean here...}
%
%
Formally, for the i-th atom $a_i$ with $(\hiddenpos_{i,L}, \hiddenatomfeat_{i,L})$ learned at the $l$-th layer, its interaction embedding $\interacthidden_{i,L}$ with $\shapehiddenmat\in \mathbb{R}^{3\times d_p}$ is learned in a similar way as in Equation~\ref{eqn:se:decoder} as follows, \xia{what do you mean ``an interaction 
embedding"?}
%
 \begin{equation}
\interacthidden_{i,l} = \text{MLP}(\hiddenatomfeat_{i,L}, \langle \hiddenpos_{i,L}, \hiddenmat^{\mathtt{s}}\rangle, \|\hiddenpos_{i,L}\|, \text{VN-In}(\hiddenmat^{\mathtt{s}})),
\label{eqn:diff:interact}
\end{equation}
\bo{$o$ has been used for signed distance as in Equation 3. You may want to use another notation here.}
%
where $\langle\hiddenpos_{i,L}, \hiddenmat^{\mathtt{s}}\rangle$ is the dot-product between $\hiddenpos_{i,L}$ and $\hiddenmat^{\mathtt{s}}$; $\|\hiddenpos_{i,L}\|^2$ is the Euclidean norm of the vector feature $\hiddenpos_{i,L}$; $\text{VN-In}(\hiddenmat^{\mathtt{s}})$ is shared across all the layers.
%
At $l=0$, $\hiddenatomfeat_i^0=\text{MLP}(\atomfeat_i, t)$ is initialized with the atom feature $\atomfeat_i$ and time step t, and $\hiddenpos_i^0=\pos_i$ is initialized with the atom position $\pos_i$.
%
The $\shapehiddenmat$ and $\interacthidden_{i,L}$ are used to update the $\hiddenpos_{i,L}$ and $\hiddenatomfeat_{i,L}$, respectively, as follows,
\begin{equation}
\tilde{\hiddenpos}_{i,L} = \text{MLP}(\hiddenpos_{i,L}, \shapehiddenmat), \quad \tilde{\hiddenatomfeat}_{i,L} = \text{MLP}(\hiddenatomfeat_{i,L}, \interacthidden_{i,L}).
\end{equation}
With the shape representation added, $\tilde{\hiddenpos}_{i,L}$ and $\tilde{\hiddenatomfeat}_{i,L}$ can be used to guide the prediction of atom features and positions conditioned on shapes.
%
} 
\xia{this section is very confusing. }
\bo{I do not understand here. It could be better to better clarify the motivation of learning the condition here}

\todo{explain the intuition of these equations}

\paragraph{\bo{Shape Condition Learning}}
\bo{
@Ziqi Based on my understanding, this paragraph is part of Equation~\ref{eqn:geometric_embedding}.
%
Do we want to separate it out?
}
\end{comment}

%-----------------------------------------------------------------------
%\paragraph{\bo{Shape-aware Atom Representation Learning}}
\subsubsection*{Shape-aware Atom Representation Learning (\SA)}
%\label{sec:method:rep:atom}
%-----------------------------------------------------------------------

%\xia{this section is very confusing. It is unclear where these atom things are used... }

To generate molecules that tailored to the shape condition represented by $\shapehiddenmat$, 
\molpred adapts the scalar embedding $\hiddenatomfeat_{i,l}$ and 
the vector embedding $\hiddenpos_{i,l}$ of each atom $a_i$ into the shape-aware scalar embedding $\hat{\hiddenatomfeat}_{i,l}$ and the shape-aware vector embedding $\hat{\hiddenpos}_{i,l}$ by incorporating $\shapehiddenmat$ at each layer.
%
Particularly, $\molpred$ learns $\hat{\hiddenatomfeat}_{i,l}$ for each atom $a_i$ using $\shapehiddenmat$ as follows,
%
%This enables the learned embeddings $\hiddenatomfeat_{i,L}$ and $\hiddenpos_{i,L}$ at layer $L$ to guide predictions of atom positions and types that form 
%the desired shape in Equation~\ref{eqn:pred:pred}.
%
%Particularly, \methoddiff incorporates inherent interactions (e.g., relative distances from atoms to shape condition) and equivariant interactions (e.g., orientations of atoms and shape condition) between molecular atoms and shape conditions into the $\hiddenatomfeat_{i,L}$ and $\hiddenpos_{i,L}$.
%
%To add inherent interactions, \methoddiff updates shape-aware scalar embeddings for atoms at the $l$-th layer of graph neural network (Equation~\ref{eqn:geometric_embedding}) as follows,
%
\begin{equation}
\label{eqn:shapea}
\hat{\hiddenatomfeat}_{i,l} = \text{MLP}([\hiddenatomfeat_{i,l}, \interacthidden_{i,l}]), 
\end{equation}

%\hl{To strengthen the shape condition during the generation, }\xia{????}
%\methoddiff employs shape-aware atom embeddings in the \bo{molecule predictor $\molpred$}.
%
%\bo{@Ziqi why using different approaches to add shape information into the scalar embedding and vector embedding?}
%
%Particularly, we generate shape-aware scalar embeddings for atoms at the $l$-th $\text{GVP}(\cdot)$ (Equation~\ref{eqn:geometric_embedding}) function as follows \xia{this is very %confusing... what is the difference between 
%a in Equation 23 and a here?},
%\begin{equation}
%\label{eqn:shapea}
%\tilde{\hiddenatomfeat}_{i,L} = \text{MLP}([\hiddenatomfeat_{i,L}, \interacthidden_{i,L}]), 
%\end{equation}
where $[\cdot,\cdot]$ is the concatenation operation;
%
$\hiddenatomfeat_{i,L}$ is the scalar embedding of atom $a_i$ at the $l$-th layer; %$\text{GVP}(\cdot)$ function; 
$\interacthidden_{i,l}\in\mathbb{R}^{d_o}$ %(\bo{@Ziqi dimension}) 
represents the inherent relations %\ziqi{estimated} relative position 
between $a_i$ and the molecular surface shape, such as the signed distance from $a_i$ to the shape.
%\xia{estimated or ground truth?}.
%
{\molpred learns $\interacthidden_{i,l}$ in a similar way to Equation~\ref{eqn:se:decoder} as follows,}
%Similarly to that in Equation~\ref{eqn:se:decoder}, 
%\methoddiff calculates $\interacthidden_{i,L}$ as follows,
\begin{equation}
\interacthidden_{i,l} = \text{MLP}([\hiddenatomfeat_{i,l}, \langle \hiddenpos_{i,l}, \hiddenmat^{\mathtt{s}}\rangle, \|\hiddenpos_{i,l}\|, \text{VN-In}(\hiddenmat^{\mathtt{s}})]),
\label{eqn:diff:interact}
\end{equation}
where $\langle\hiddenpos_{i,l}, \hiddenmat^{\mathtt{s}}\rangle$ is the dot-product between $\hiddenpos_{i,l}$ and $\hiddenmat^{\mathtt{s}}$; $\|\hiddenpos_{i,l}\|^2$ is the column-wise Euclidean norm of the vector feature $\hiddenpos_{i,l}$; $\text{VN-In}(\hiddenmat^{\mathtt{s}})$ encodes the inherent geometry of shape condition and thus is shared across all the layers.
%
%\ziqi{Here $\interacthidden_{i,l}$ is developed to learn inherent interactions between the atom $a_i$ and the shape condition, in a similar way to the signed distance as in Equation~\ref{eqn:se:decoder}.% even though \methoddiff is not explicitly optimized to accurately predict $\interacthidden_{i,l}$ as the signed distance.
%
%}
%
%\bo{@Ziqi add insights on why generating $\interacthidden_{i,l}$ using the equation above, and why sharing $\text{VN-In}(\hiddenmat^{\mathtt{s}})$ across layers.}
%
Apart from scalar embeddings, \molpred also incorporates shape information into the vector embeddings as follows, 
%
\begin{equation}
\label{eqn:shaper}
\hat{\hiddenpos}_{i,l} = \text{VN-MLP}([\hiddenpos_{i,l}, \shapehiddenmat]),
\end{equation}
where $\hiddenpos_{i,l}$ is the vector embedding of atom $a_i$ at the $l$-th layer; $\text{VN-MLP}(\cdot)$ is an equivariant VN network~\cite{deng2021vn} that learns non-linear interactions $\hat{\hiddenpos}_{i,l}\in\mathbb{R}^{3\times d_r}$ between $\hiddenpos_{i,l}$ and $\shapehiddenmat$ in an equivariant way. 
%
%\ziqi{This enables $\hat{\hiddenpos}_{i,L}$ to be aware of geometric features of shape condition, such as its orientation.}
%
%\bo{@Ziqi, you may want to add some insights for the equation here.}



\begin{comment}
%................................................................................................................................................
\paragraph{\ziqi{Bond Type Embedding}}
%................................................................................................................................................
%
\ziqi{\methoddiff predicts bond types and incorporates these predictions into the learning of geometric information within molecules. 
%
This integration enhances the latent embeddings $(\hiddenpos_{i,L}, \hiddenatomfeat_{i,L})$ by providing them with a deeper understanding of molecular topology (i.e., bonds between atoms). \xia{NOT to the point!}
%
For each pair of k-nearest neighbor atoms, \methoddiff incorporates a bond type encoding $\tilde{\bondemb}_{ji,l}\in\mathbb{R}^{d_b}$  to represent the predicted bond type between atom $a_j$ and $a_i$, which $d_b$ is the number of bond types \xia{not clear}.
%
Specifically, $\tilde{\bondemb}_{ji,l}$ is predicted from $(\hiddenatomfeat_{j,l}, \hiddenpos_{j,l})$ and $(\hiddenatomfeat_{i,l}, \hiddenpos_{i,l})$ as follows,
\begin{equation}
\tilde{\bondemb}_{ji,l} = 
	\begin{cases}
	\text{MLP}(\hiddenatomfeat_{i,l} + \hiddenatomfeat_{j,l}, \text{Abs}(\hiddenatomfeat_{i,l} - \hiddenatomfeat_{j,l}), d_{ji}), & \text{if}\ l = 0, \\
	\text{MLP}(\hiddenatomfeat_{i,l} + \hiddenatomfeat_{j,l}, \text{Abs}(\hiddenatomfeat_{i,l} - \hiddenatomfeat_{j,l}), \|\hiddenpos_{i}\|^2 + \|\hiddenpos_{j}\|^2, \text{Abs}(\|\hiddenpos_{i}\|^2 - \|\hiddenpos_{j}\|^2)), & \text{if}\ l > 0,
	\end{cases}
	\label{eqn:bond_type}
\end{equation}
where $\text{Abs}(\cdot)$ represents the absolute difference; $d_{ji}$ is the distance between the positions $\pos_j$ and $\pos_i$.
%
At the first layer when $\hiddenpos_{j}^0=\pos_{i}$ and $\hiddenpos_{i}^0=\pos_{j}$, 
%
\bo{Is the subscribe correct?}
%
\methoddiff uses $d_{ji}$ and the sum and difference of scalar embeddings $\hiddenatomfeat_{j}^0$ and $\hiddenatomfeat_{i}^0$ to predict the bond type between atoms $a_j$ and $a_i$.
%
All these three terms are permutation-invariant to the order of $a_j$ and $a_i$.
%
Similarly, at the following layers $l>1$, \methoddiff uses the sum and difference of scalar embeddings $\hiddenatomfeat_{j,l}$ and $\hiddenatomfeat_{i,l}$  and vector embeddings $\hiddenpos_{j,l}$ and $\hiddenpos_{i,l}$ to predict the bond type.
\xia{this is very confusing. Is equation 22 correct? why different at $l=0$ and $l>0$?}}
\hl{maybe we can compare our bond type embeddings with other models with bond type diffusion.}

\bo{Why $d_{ij}$, the sum and the difference could be a good indicator of the bond type?}

\ziqi{}
\end{comment}

%-----------------------------------------------------------------------
%\paragraph{Bond Type Representation Learning}
\subsubsection*{Bond Type Representation Learning ($\mathsf{BTRL}$)}
\label{sec:method:rep:bond}
%-----------------------------------------------------------------------

As shown in Equation~\ref{eqn:message}, $\molpred$ leverages the types of bonds within \mol to facilitate its understanding of relations among atoms.
%
%As shown in Equation~\ref{eqn:message}, 
%$\molpred(\pos_t, \atomfeat_t, \shapehiddenmat)$ leverages the types of bonds within \mol to XXX \bo{@Ziqi you may want add some insights here.}
%\methoddiff predicts the types of bonds within \mol to XXX \bo{@Ziqi you may want add some insights here.}
%
Particularly, for the bond between $a_j$ and $a_i$, \molpred generates the bond type embedding as follows, 
\begin{equation}
\bondpred_{ji,l} = 
	\begin{cases}
	\text{MLP}([\hiddenatomfeat_{i,l} + \hiddenatomfeat_{j,l}, \text{abs}(\hiddenatomfeat_{i,l} - \hiddenatomfeat_{j,l}), d_{ji}]), & \text{if}\ l = 0, \\
	\text{MLP}([\hiddenatomfeat_{i,l} + \hiddenatomfeat_{j,l}, \text{abs}(\hiddenatomfeat_{i,l} - \hiddenatomfeat_{j,l}), \|\hiddenpos_{i}\|^2 + \|\hiddenpos_{j}\|^2, \text{abs}(\|\hiddenpos_{i}\|^2 - \|\hiddenpos_{j}\|^2)]), & \text{if}\ l > 0,
	\end{cases}
	\label{eqn:bond_type}
\end{equation}
where $\hiddenatomfeat_{i,l}$ and $\hiddenpos_{i,l}$ is the scalar embedding and vector embedding of $a_i$ (Equation~\ref{eqn:geometric_embedding}),
%\xia{refer to equations}, 
respectively; $\text{abs}(\cdot)$  %\xia{abs} 
represents the absolute difference; $d_{ji}$ is the distance between the positions $\pos_j$ and $\pos_i$.
%
\molpred guarantees that the predictions $\bondpred_{ij,l}$ and $\bondpred_{ji,l}$ are invariant to the permutation of atom $a_i$ and $a_j$.
%
This is achieved by using two invariant operations: the sum and the absolute difference operation.
%
To learn effective bond-type embeddings, we also
use the sum and the absolute difference of column-wise Euclidean norm of $\hiddenpos^l_{i}$ and $\hiddenpos^l_{j}$ to implicitly estimate the distance between $a_i$ and $a_j$.
%
When $l=0$, we directly use the distance $d_{ji}$ to calculate $\bondpred_{ji,l}$.
%
%We do not consider the distance $d_{ji}$ between atom $\pos_i$ and $\pos_j$ when $l>0$ and $\hiddenpos^l_{i}\in\mathbb{R}^{3\times d_r}$.

%When $l=0$ and $\hiddenpos^0_{i}=\pos_i$, \methoddiff directly incorporates the distance $d_{ji}$ between atom $\pos_i$ and $\pos_j$ and their scalar embeddings for bond type predictions.
%
%When $l>0$ and $\hiddenpos^l_{i}\in\mathbb{R}^{3\times d_r}$, \methoddiff maps $\hiddenpos^l_{i}$ into 
%$\|\hiddenpos^l_{i}\|^2$ and then utilizes the sum and the absolute difference of $\|\hiddenpos^l_{i}\|^2$ and $\|\hiddenpos^l_{j}\|^2$

%\todo{need to rethink the reason}
%
%\bo{@Ziqi what's the relation between $\tilde{\bondemb}_{ji,l}$ and $\tilde{\bondemb}_{ij,l}$? Is it symmetric?}
%
%\bo{@Ziqi add insights for the equation above.}
%
%\bo{We do not consider XXX (@Ziqi explain $\|\hiddenpos_{i}\|^2 + \|\hiddenpos_{j}\|^2$ and $\text{Abs}(\|\hiddenpos_{i}\|^2 - \|\hiddenpos_{j}\|^2))$ using plain language) when $l > 0$ due to the reason that XXX (@Ziqi).}


\begin{comment}

%................................................................................................................................................
\paragraph{\ziqi{GVP-based Graph Attention Neural Networks}}
%................................................................................................................................................
%
\ziqi{\methoddiff represents each atom using an invariant scalar embedding $\hiddenatomfeat_{i,L}$ and an equivariant vector embedding $\hiddenpos_{i,L}$ for each $i$-th atom $a_i$. 
%
These two embeddings are learned to capture the local neighborhood geometric structures by propagating messages over k-nearest neighbor graphs.
%
Each pair of k-nearest neighbor atoms $(a_j, a_i)$ is associated with two messages $\hiddenmessatom_{ji,l}\in\mathbb{R}^{d_a}$ and $\hiddenmesspos_{ji,l}\in\mathbb{R}^{3\times d_r}$ encoding the messages over scalar embeddings and vector embeddings propagating from atom $a_j$ to $a_i$.
%
At the $l$-th layer, the message $\hiddenmessatom_{ji,l}$ and $\hiddenmesspos_{ji,l}$ from atom $j$ to $i$ for scalar embeddings and vector embeddings are calculated as follows,
%
\begin{equation}
\hiddenmessatom_{ji,l} = \text{MLP}(\tilde{\hiddenatomfeat}_j^{l-1}, d_{ji}, \tilde{\bondemb}_{ji}^{l-1}),
\end{equation}
%
\begin{equation}
\hiddenmesspos_{ji,l} = \text{MLP}(\tilde{\hiddenpos}_j^{l-1}, \pos_j - \pos_i),
\end{equation}
%
\begin{equation}
\hiddenmessatom_{ji,l}, \hiddenmesspos_{ji,l} = \text{GVP}(\hiddenmessatom_{ji,l}, \hiddenmesspos_{ji,l}),
\end{equation}
%
where %$f_m(.)$ and $f_n(.)$ are two single MLP layers with ReLU as the activation function;
$\bondemb_{ji}^{l-1}$ is the embedding of the bond type between $a_i$ and $a_j$ (in Equation~\ref{eqn:bond_type});
%\hl{need to update the description of bond type if the results of the model with predicted bond types are better than that with predefined bond types.}
$\text{GVP}(.)$ is a function with three GVP blocks to enable information to be transformed and passed between the scalar-based messages $\hiddenmessatom_{ji,l}$ and vector-based messages $\hiddenmesspos_{ji,l}$ \xia{not clear}. 
%
%$\atomfeat_i^0 = \text{MLP}(\atomfeat_i, t)$ is initialized with the atom type embedding $\atomfeat_i$ and the time step embedding $t$.
%
The $\hiddenatomfeat_{i,L}$ and $\hiddenpos_{i,L}$ are then updated with messages from neighboring atoms as follows,
%
\begin{equation}
\hiddenatomfeat_{i,l} = \text{MLP}(\atomfeat_i, \tilde{\hiddenatomfeat}_{i,l-1}, \sum_{j\in N(i)}e_{ji}\hiddenmessatom_{ji,l}, t),\quad \hiddenpos_{i,l} = \text{MLP}(\pos_i, \tilde{\hiddenpos}_{i,l-1}, \sum_{j\in N(i)}e_{ji}\hiddenmesspos_{ji,l}),
\end{equation}
%
\begin{equation}
\hiddenatomfeat_{i,l}, \hiddenpos_{i,l} = \text{GVP}(\hiddenatomfeat_{i,l}, \hiddenpos_{i,l}),
\end{equation}
where $N(i)$ denotes the k-nearest neighbor atoms of atom $i$ over the 3D space;
$e_{ji}$ is the attention weight used to aggregate messages from neighboring atoms.
\xia{how about v?}
%
The weight $e_{ji}$ is calculated to estimate how much the neighboring atom $j$ should contribute to the representation of local neighborhood structure of atom $i$ as follows,
\begin{equation}
\begin{aligned}
e_{ij} & = \frac{\exp(Q_{i}K_{ji})}{\sum_{k\in N(i)}\exp(Q_{i}K_{ki})},\\
 \text{where}\  Q_i& =\text{MLP}(\tilde{\hiddenatomfeat}_{i,l-1}, \|\tilde{\hiddenpos}_{i,l-1}\|^2), \\
K_{ji}&=\text{MLP}(\hiddenmessatom_{ji}^{l-1}, \|\hiddenmesspos_{ji}^{l-1}\|^2).
\end{aligned}
\end{equation}
%
After $L$ iterations, \methoddiff predicts the type $\tilde{\atomfeat}_{0, i}$ and position $\tilde{\pos}_{0, i}$ of the $i$-th atom as follows,
\begin{equation}
\tilde{\atomfeat}_{0, i} = \text{softmax}(\text{MLP}(\hiddenatomfeat_{i,L})), \quad \tilde{\pos}_{0, i} = \pos_i + \text{MLP}(\hiddenpos_{i,L}), 
\end{equation}
where $\tilde{\atomfeat}_{0, i}$ is the predicted probability distribution across all the types of $a_i$; $\tilde{\pos}_{0, i}$ is the predicted position of $a_i$.
}

\bo{Why do you have a variable $l$ as input in Equation 28?}

\bo{What's the intuition of calculating the weights using these equations}

\todo{equation 27 input and output notation cannot be the same; add subscript t to the notations in equation 29}

\todo{add algorithm}

\end{comment}

%$\molpred$ leverages two multi-layer graph neural networks: 
%(1) an equivariant graph neural network, denoted as \eqgnn, that equivariantly predicts atom positions that change under transformations, and 
%(2) an invariant graph neural network, denoted as \invgnn, that invariantly predicts atom features that remain unchanged under transformations.
%\hl{$f_\theta(\cdot)$ leverages two graph neural networks to predict atom positions in an equivariant way 
%and atom features in an invariant way, given that the atom positions change accordingly but atom features remain unchanged under transformations.} \xia{too long, please simplify}
%
%thus is able to recover the original molecule consistently regardless of any translational or rotational shifts applied to its input.
%
%When transformations are applied to molecules, atom positions change accordingly but atom features remain unchanged.
%
%As a result, $f_\theta(\cdot)$ aims to predict atom positions in an equivariant way and atom features in an invariant way.
%%
%Following the previous work~\cite{guan2023targetdiff,hoogeboom22diff}, the translation equivariance of atom position prediction is achieved by 
%shifting a fixed point (e.g., the center of point clouds \pc) to zero, and therefore only rotation equivariance needs to be considered.
%



%===================================================================
%\subsubsection{Additional Shape Guidance}
\subsection*{Guidance-induced Inference}
\label{section:method:guidance}
%===================================================================

%\bo{@Ziqi Should we present this section right after the "Diffusion-based molecule generation"}
%\xia{or should it be after ``model training" as another subsection titled something like ``Guidance-induced inference"?}
%\ziqi{I think it is better to leave this section here. ``Guidance-induced inference'' sounds good to me.}

During inference, \method generates novel molecules by gradually denoising $(\pos_T, \atomfeat_T)$ to $(\pos_0, \atomfeat_0)$ using the prediction module $\molpred$.  
%\bo{need to update the term here}
%\hl{equivariant shape-conditioned molecule predictor} 
%\xia{never use this term!}.
%
Specifically, \method samples $\pos_T$ and $\atomfeat_T$ from $\mathcal{N}(\mathbf{0}, \mathbb{I})$ and $\mathcal{C}(\mathbf{1}/K)$, respectively.
%
After that, \method samples $\pos_{t-1}$ from $\pos_t$ using $p_{\boldsymbol{\Theta}}(\pos_{t-1}|\pos_t, \tilde{\pos}_{0,t})$ (Equation~\ref{eqn:aprox_pos_posterior}).
%
Similarly, \method samples $\atomfeat_{t-1}$ from $\atomfeat_{t}$ using $p_{\boldsymbol{\Theta}}(\atomfeat_{t-1}|\atomfeat_{t}, \tilde{\atomfeat}_{0,t})$ (Equation~\ref{eqn:aprox_atomfeat_posterior}) until $t$ reaches 1.
%
\method uses post-processing to determine the bond type between atoms based on atomic distances following the previous work~\cite{peng22pocket2mol,guan2023targetdiff}.
%
Though the learned bond type embeddings in \method could provide valuable topology information for molecule prediction (\molpred), we observe that directly using predicted bond types in generated molecules could lead to sub-optimal performance. 

\begin{comment}
\ziqi{In the sampling process, molecules are generated by sequentially denoising
$[\pos_t, \atomfeat_t]$ into $[\pos_{t-1}, \atomfeat_{t-1}]$, using the predictions $[\tilde{\pos}_{0,t}, \tilde{\atomfeat}_{0,t}]$
from $f_\theta(\cdot)$.
%
This denoising leverages the probabilities
$p_\theta(\pos_{t-1}|\pos_t, \tilde{\pos}_{0,t})$ (in Equation~\ref{eqn:aprox_pos_posterior}) and 
$p_\theta(\atomfeat_{t-1}|\atomfeat_{t}, \tilde{\atomfeat}_{0,t})$ (in Equation~\ref{eqn:aprox_atomfeat_posterior}).
%
The sampling process starts from $[\pos_T, \atomfeat_T]\sim(\mathcal{N}(\mathbf{0}, \mathbf{I}), \mathcal{C}(\mathbf{I}/K))$
and progresses iteratively until $t$ reaches 0.}
\end{comment}

%---------------------------------------------------------------------------------------------------------------------------
\subsubsection{{\method with Shape Guidance} {($\shapeguide$)}}
\label{section:method:guidance:shape-guide}
%---------------------------------------------------------------------------------------------------------------------------

%\bo{
%@Ziqi Overall, the motivation of the guidance is not clear. why the model cannot learn it implicitly but require explicit adjustment?
%}
%
During molecule generation, as shown in Figure~\ref{fig:overall}(c), \method can also utilize additional shape guidance
by pushing the predicted atoms to the shape %backbone 
of the condition molecule \molx.
%
%
This approach is motivated by previous work~\cite{dhariwal2021diffusion}, which demonstrates that incorporating additional guidance into conditional diffusion models can further ensure the generated objects closely following the given condition.
%
%\xia{you need to be crystal clear about ``shape used for guidance" and ``shape used as condition" -- it has been confusing here}
%
Note that different from the shape for conditions, when used as guidance, 
we define molecule shapes as a set of points $\mathcal{Q}$ sampled according to atom positions in the condition molecule \molx following Adams and Coley \etal~\cite{adams2023equivariant}
%
We empirically find that this design leads to improved generation performance.
%
%Particularly, following Adams and Coley \etal~\cite{adams2023equivariant}, 
%the shape used for guidance is defined as a set of points $\mathcal{Q}$ sampled according to atom positions in \ziqi{the condition molecule} \molx
%\xia{reminder the readers what $\molx$ is -- this is important to understand the guidance here}.
%
Particularly, for each atom $a_i$ in \molx, 20 points are randomly sampled into $\mathcal{Q}$ from a 
Gaussian distribution centered at $\pos_i$. %with variance $\phi$. 
%, denoted as $\mathcal{N}(\pos_i, \phi\mathbf{I})$.
%multiple Gaussian distributions, 
%each of which is centered at the position of an atom in.}
%\hl{the shape used for guidance is defined as a set of points $\mathcal{Q}$ randomly sampled from multiple Gaussian distributions, 
%each of which is centered at the position of an atom in the condition molecule.} 
%\xia{what do you mean?}
%
Given the predicted atom position $\tilde{\pos}_{0,t}$ at step $t$, \method applies the shape guidance 
by {adjusting the predicted positions to $\mathcal{Q}$ %{\molx}} \bo{what do you mean? \molx is not in the equation}  
as follows:
\begin{equation}
\label{eqn:shape_guidance}
\begin{aligned}
\pos_{0,t}^*=(1-\sigma) \tilde{\pos}_{0,t}+\sigma \!{\sum_{\mathclap{\mathbf{z}\in N(\tilde{\scriptsize{\pos}}_{0,t}; \mathcal{Q})}} \mathbf{z}}/{k},
\text{when }\sum_{\mathclap{\mathbf{z}\in N(\tilde{\scriptsize{\pos}}_{0,t}; \mathcal{Q})}} d(\tilde{\pos}_{0,t}, \mathbf{z}) / k>\gamma,\end{aligned}
\end{equation}
%
where $\sigma>0$ is the weight used to balance the prediction $\tilde{\pos}_{0,t}$ and the adjustment;
%
$d(\tilde{\pos}_{0, t}, \mathbf{z})$ is the Euclidean distance between $\tilde{\pos}_{0, t}$ and $\mathbf{z}$;
%
$N(\tilde{\pos}_{0,t};\mathcal{Q})$ is the set of $k$-nearest neighbors of $\tilde{\pos}_{0,t}$ in $\mathcal{Q}$ 
based on $d(\cdot)$;
%
$\gamma>0$ is a distance threshold. 
%
By doing the above adjustment, the predicted atom positions will be pushed to those of {\molx} if they
are sufficiently far away. 
%
Note that the shape guidance is applied exclusively for steps 
%
\begin{equation}
\label{eqn:steps}
t=T, T-1, \cdots, S\text{, where } S>1, 
\end{equation}
%
not for all the steps, 
and thus it only adjusts predicted atom positions when there are a lot of noises and the prediction needs 
more guidance. 
%
%\ziqi{Please note that as $t$ approaches 1, the predicted atom position $\tilde{\pos}_{0,t}$ has lower noise and could form a well-structured 
%molecule.
%%
%In this case, applying shape guidance to change the atom position could break the molecule structure, reducing the quality of the generated molecule.
%%
%To avoid this, the shape guidance is applied exclusively for steps $t=T, T-1,\cdots,S>1$, in which $S$ is the terminate step for guidance.}
%In order to avoid the well-formed molecules at $t\rightarrow 0$ corrupted by this enhancement, \method applies it exclusively for steps $t=T, T-1, ...,S >1$. \xia{why?}
%
\method with the shape guidance is referred to as \methodwithsguide.
%
%The tuning of the hyper-parameter $r$ and the terminate step $S$ will be discussed in the following sections.

\begin{comment}
%---------------------------------------------------------------------------------------------------------------------------
\subsubsection{\ziqi{\method with Protein Pocket Guidance}}
\label{section:diff:shape-guide}
%---------------------------------------------------------------------------------------------------------------------------

\bo{
@Ziqi What is "pocket guidance"? 
What is the difference between "pocket guidance" and "shape guidance"?
Why "pocket guidance" is desired?
}

\bo{
@Ziqi You may want to clarify these parts in earlier sections.
}

\ziqi{During inference, when protein pocket is known, \method can utilize additional pocket guidance to refine the atom positions in the generated molecules.
%
This approach is to reduce steric clashes between the atoms in the generated molecules and the protein, which prevents two non-bonded atoms from being too close.
%
To assess if two non-bonded atoms are too close, \method identifies a distance threshold for each unique pair of protein and molecule atoms found within known protein-ligand complexes.
%
With these distance thresholds, \method fine-tunes the positions of atoms in the generated molecules to ensure that their distances from the protein atoms consistently exceed these thresholds \xia{why?}.
%
Different from the shape guidance applied to the predicted position $\tilde{\pos}_{0, t}$, this pocket guidance is applied on $\pos_{t}$, which is sampled from $p(\pos_{t}|\pos_{t+1})$.
%
This ensures that, throughout the entire denoising process, all atoms in the noisy molecules maintain a sufficient distance from the protein atoms $\mathcal{P}$.
%
Formally, given the sampled atom position $\pos_{t}$ at step $t$, \method applies the pocket guidance by adjusting the position as follows:
%
\begin{equation}
\label{eqn:guidance}
\begin{aligned}
\pos_{t}^*=(1+\epsilon) \pos_{t}-\epsilon {\sum_{\mathclap{\mathbf{z}\in n(\scriptsize{\pos}_{t}; \mathcal{P})}} \mathbf{z}}/{n},
\quad \text{if }\exists\ \mathbf{z}\in n(\pos_{t}; \mathcal{P}), d(\pos_{t}, \mathbf{z}) <\rho_{\mathbf{z}, \scriptsize{\atomfeat}_t},\end{aligned}
\end{equation}
%
where $\epsilon>0$ is the weight used to balance the noisy position $\pos_{t}$ and the adjustment; $d(\pos_{t}, \mathbf{z})$ is the Euclidean distance between $\pos_{t}$ and $\mathbf{z}$; $n(\tilde{\pos}_{0,t};\mathcal{P})$ is the set of $n$-nearest neighbors of $\tilde{\pos}_{0,t}$ in protein atoms $\mathcal{Q}$ based on $d(\cdot)$;
%
$\rho_{\mathbf{z}, \scriptsize{\atomfeat}_t}>0$ is a distance threshold for two non-bonded atoms with the protein atom type $\mathbf{z}$ and the molecule atom type $\atomfeat_t$ \xia{notations and descriptions
are very confusing. what is the key idea here?}. 
%
In contrast to the shape guidance, this pocket guidance is applied for all the steps, and thus it ensures that there is no clash between atoms in the protein and the final generated molecule.}
\end{comment}

%-------------------------------------------------------------------------
\subsubsection{\method with Protein Pocket Guidance {(\pocketguide)}}
\label{section:method:guidance:pocket}
%-------------------------------------------------------------------------

%\method is also capable of utilizing pocket guidance for better generations.
%
%\bo{@Ziqi why utilizing pocket guidance is significant.}
%
%Particularly, \method refines the atom positions in the generated molecules as follows,
%

When applying \method to PMG (i.e.,  protein pocket of the condition molecule is available), 
%
we observe that atoms in the generated molecules could be too close to the protein pocket atoms \pocket, thereby leading to steric clashes and thus undesirable binding affinities.
%
To address this issue, as shown in Figure~\ref{fig:overall}(e), \method utilizes pocket guidance to further adjust atom positions and maintain sufficient distances between molecule atoms and protein atoms.
%
%\hl{This is to prevent the steric clashes between generated molecules and the protein pocket, which result from molecule atoms and protein atoms being too close.}
%
%\bo{
%do not understand here...
%@Ziqi, why shape guidance can not achieve this? What's the unique value added by the pocket guidance?
%}
%
Particularly, \method refines the atom positions in the generated molecules based on \pocket as follows,
\begin{equation}
\label{eqn:pocket_guidance}
\begin{aligned}
\pos_{t}^*=\pos_{t} + \frac{\pos_t - \mathbf{z}}{d(\pos_{t}, \mathbf{z})} * (\rho - d(\pos_{t}, \mathbf{z}) + \epsilon)%(1+\epsilon) \pos_{t}-\epsilon {\sum_{\mathclap{\mathbf{z}\in N(\scriptsize{\pos}_{t}; \mathcal{K})}} \mathbf{z}}/{k},
\quad \text{if }\exists\ \mathbf{z}\in N(\pos_{t}; \mathcal{K}), d(\pos_{t}, \mathbf{z}) <\rho, %_{\mathbf{z}, \scriptsize{\atomfeat}_t},
\end{aligned}
\end{equation}
%
where $\pos_{t}$ is the sampled atom positions at the step $t$; $N(\pos_{t};\mathcal{K})$ is the set of $k$-nearest neighbors of $\pos_{t}$ within the protein atoms $\mathcal{K}$; %\bo{I think $\mathcal{P}$ is the notation for the point cloud for the molecule shape...}; 
%\bo{@Ziqi what is $z$? It seems $z$ is not a point in the shape}; 
$d(\pos_{t}, \mathbf{z})$ is the distance between $\pos_{t}$ and $\mathbf{z}$, 
%
{and $\frac{\scriptsize{\pos_{t}} - \mathbf{z}}{d(\scriptsize{\pos_{t}}, \mathbf{z})}$ calculates the unit vector in the direction that moves $\pos_{t}$ far from $\mathbf{z}$.}
%\method introduces a hyper-parameter $\epsilon$ to control the strength of pocket guidance.
%balance XXX (\bo{@Ziqi}).
%
\method introduces a threshold $\rho$ %\hl{for each unique pair of protein and molecule atoms 
to assess if protein atoms and molecule atoms are too close.
%
{\method} identifies this threshold from known protein-ligand complexes in the training dataset.}
%
{\method also introduces a hyper-parameter $\epsilon$ to control the margin. }%between the distance and the threshold. }
%\bo{The threshold is pair and $t$ specific?
%If so, the notation does not look right to me.
%$\rho$ does not represent a variable dependent on $\mathbf{z}$ and $\atomfeat_t$.
%%
%It would be better to represent it as a function.
%}
%\bo{
%I do not think it should be dependent on $t$%
%}
%that XXX (\bo{@Ziqi}).
%
%\bo{@Ziqi you may want to add the insights of the above equation here.}
%
Note that different from the shape guidance that is applied on $\tilde{\pos}_{0, t}$, 
the pocket guidance is applied on $\pos_{t}$.
%
We empirically find this design benefits the generated molecules in their binding affinities to protein pockets.
%
\method with the pocket guidance is referred to as \methodwithpguide. 
%\hl{This design ensures all the molecule atoms to be far enough from the protein atoms.} 
%\bo{
%@Ziqi This conclusion is not obvious. You may want to elaborate more here.
%}
%
%For shape guidance, \method applies adjustments only to $\tilde{\pos}_{0, t}$, as constraining all the positions $\pos_t$ in the intermediate steps to fit the shape could substantially \ziqi{interrupt the inference process and degrade the quality of generated molecules.} %\bo{@Ziqi what do you mean? Degrade the quality of generated molecules?}.
%This design ensures XXX (\bo{@Ziqi}).
%
%\bo{For shape guidance, we XXX due to the reason that (@Ziqi explain why not also apply the shape guidance on $\pos_{t}$)}
%
%where $\epsilon>0$ is the weight used to balance the noisy position $\pos_{t}$ and the adjustment; $d(\pos_{t}, \mathbf{z})$ is the Euclidean distance between $\pos_{t}$ and $\mathbf{z}$; $n(\tilde{\pos}_{0,t};\mathcal{P})$ is the set of $n$-nearest neighbors of $\tilde{\pos}_{0,t}$ in protein atoms $\mathcal{Q}$ based on $d(\cdot)$;
%
%$\rho_{\mathbf{z}, \scriptsize{\atomfeat}_t}>0$ is a distance threshold for two non-bonded atoms with the protein atom type $\mathbf{z}$ and the molecule atom type $\atomfeat_t$ \xia{notations and descriptions
%are very confusing. what is the key idea here?}. 

%\todo{rethink how to explain the motivation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data Availability}
\label{section:data_availability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The data used in this manuscript is made publicly available at the link \url{https://github.com/ninglab/DiffSMol}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Code Availability}
\label{section:code_availability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The code for \method is made publicly available at the link \url{https://github.com/ninglab/DiffSMol}..


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
\label{section:acknowledgements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This project was made possible, in part, by support from the National Science Foundation grant no. IIS-2133650 (X.N., Z.C.), 
and the National Library of Medicine grant no. 1R01LM014385 (X.N.). 
%
Any opinions, findings, and conclusions or recommendations expressed in this manuscript are those of the authors, and do not necessarily reflect the views of the funding agencies.
%
We thank Patrick J. Lawrence, Frazier N. Baker, and Vishal Dey for their constructive comments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Author Contributions}
\label{section:author_contribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

X.N. conceived the research. X.N. obtained funding for the research. Z.C. and X.N. designed the research. Z.C.
and X.N. conducted the research, including data curation, formal analysis, methodology design and implementation,
result analysis and visualization. Z.C., B.P. and X.N. drafted the original manuscript. T.Z. and D.A. provided comments
on case studies. Z.C., B.P. and X.N. conducted the manuscript editing and revision. All authors reviewed the final manuscript.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Competing Interests}
\label{section:author_contribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

\clearpage

\bibliographystyle{naturemag}
\bibliography{paper}

\clearpage

\input{supp}

\end{document}
