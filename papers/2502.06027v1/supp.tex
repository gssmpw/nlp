
%\title{Generating 3D \hl{Small} Binding Molecules Using Shape-Conditioned Diffusion Models with Guidance}
%\date{\vspace{-5ex}}

%\author{
%	Ziqi Chen\textsuperscript{\rm 1}, 
%	Bo Peng\textsuperscript{\rm 1}, 
%	Tianhua Zhai\textsuperscript{\rm 2},
%	Xia Ning\textsuperscript{\rm 1,3,4 \Letter}
%}
%\newcommand{\Address}{
%	\textsuperscript{\rm 1}Computer Science and Engineering, The Ohio Sate University, Columbus, OH 43210.
%	\textsuperscript{\rm 2}Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA 19104.
%	\textsuperscript{\rm 3}Translational Data Analytics Institute, The Ohio Sate University, Columbus, OH 43210.
%	\textsuperscript{\rm 4}Biomedical Informatics, The Ohio Sate University, Columbus, OH 43210.
%	\textsuperscript{\Letter}ning.104@osu.edu
%}

%\newcommand\affiliation[1]{%
%	\begingroup
%	\renewcommand\thefootnote{}\footnote{#1}%
%	\addtocounter{footnote}{-1}%
%	\endgroup
%}



\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

\setcounter{section}{0}
\renewcommand{\thesection}{S\arabic{section}}

\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}

\setcounter{figure}{0}
\renewcommand{\thefigure}{S\arabic{figure}}

\setcounter{algorithm}{0}
\renewcommand{\thealgorithm}{S\arabic{algorithm}}

\setcounter{equation}{0}
\renewcommand{\theequation}{S\arabic{equation}}


\begin{center}
	\begin{minipage}{0.95\linewidth}
		\centering
		\LARGE 
	Generating 3D Binding Molecules Using Shape-Conditioned Diffusion Models with Guidance (Supplementary Information)
	\end{minipage}
\end{center}
\vspace{10pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parameters for Reproducibility}
\label{supp:experiments:parameters}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We implemented both \SE and \methoddiff using Python-3.7.16, PyTorch-1.11.0, PyTorch-scatter-2.0.9, Numpy-1.21.5, Scikit-learn-1.0.2.
%
We trained the models using a Tesla V100 GPU with 32GB memory and a CPU with 80GB memory on Red Hat Enterprise 7.7.
%
%We released the code, data, and the trained model at Google Drive~\footnote{\url{https://drive.google.com/drive/folders/146cpjuwenKGTd6Zh4sYBy-Wv6BMfGwe4?usp=sharing}} (will release to the public on github once the manuscript is accepted).

%===================================================================
\subsection{Parameters of \SE}
%===================================================================


In \SE, we tuned the dimension of all the hidden layers including VN-DGCNN layers
(Eq.~\ref{eqn:shape_embed}), MLP layers (Eq.~\ref{eqn:se:decoder}) and
VN-In layer (Eq.~\ref{eqn:se:decoder}), and the dimension $d_p$ of generated shape latent embeddings $\shapehiddenmat$ with the grid-search algorithm in the 
parameter space presented in Table~\ref{tbl:hyper_se}.
%
We determined the optimal hyper-parameters according to the mean squared errors of the predictions of signed distances for 1,000 validation molecules that are selected as described in Section ``Data'' 
in the main manuscript.
%
The optimal dimension of all the hidden layers is 256, and the optimal dimension $d_p$ of shape latent embedding \shapehiddenmat is 128.
%
The optimal number of points $|\pc|$ in the point cloud \pc is 512.
%
We sampled 1,024 query points in $\mathcal{Z}$ for each molecule shape.
%
We constructed graphs from point clouds, which are employed to learn $\shapehiddenmat$ with VN-DGCNN layer (Eq.~\ref{eqn:shape_embed}), using the $k$-nearest neighbors based on Euclidean distance with $k=20$.
%
We set the number of VN-DGCNN layers as 4.
%
We set the number of MLP layers in the decoder (Eq.~\ref{eqn:se:decoder}) as 2.
%
We set the number of VN-In layers as 1.

%
We optimized the \SE model via Adam~\cite{adam} with its parameters (0.950, 0.999), %betas (0.95, 0.999), 
learning rate 0.001, and batch size 16.
%
We evaluated the validation loss every 2,000 training steps.
%
We scheduled to decay the learning rate with a factor of 0.6 and a minimum learning rate of 1e-6 if 
the validation loss does not decrease in 5 consecutive evaluations.
%
The optimal \SE model has 28.3K learnable parameters. 
%
We trained the \SE model %for at most 80 hours 
with $\sim$156,000 training steps.
%
The training took 80 hours with our GPUs.
%
The trained \SE model achieved the minimum validation loss at 152,000 steps.


\input{tables/hyper_para_se}
%
\input{tables/hyper_para_diff}


%===================================================================
\subsection{Parameters of \methoddiff}
%===================================================================

Table~\ref{tbl:hyper_diff} presents the parameters used to train \methoddiff.
%
In \methoddiff, we set the hidden dimensions of all the MLP layers and the scalar hidden layers in GVPs (Eq.~\ref{eqn:pred:gvp} and Eq.~\ref{eqn:mess:gvp}) as 128. %, including all the MLP layers in \methoddiff and the scalar dimension of GVP layers in Eq.~\ref{eqn:pred:gvp} and Eq.~\ref{eqn:mess:gvp}. %, and MLP layer (Eq.~\ref{eqn:diff:graph:atompred}) as 128.
%
We set the dimensions of all the vector hidden layers in GVPs as 32.
%
We set the number of layers $L$ in \molpred as 8.
%and the number of layers in graph neural networks as 8.
%
Both two GVP modules in Eq.~\ref{eqn:pred:gvp} and Eq.~\ref{eqn:mess:gvp} consist of three GVP layers. %, which consisa GVP modset the number of layer of GVP modules %is a multi-head attention layer ($\text{MHA}^{\mathtt{x}}$ or $\text{MHA}^{\mathtt{h}}$) with 16 heads.
% 
We set the number of VN-MLP layers in Eq.~\ref{eqn:shaper} as 1 and the number of MLP layers as 2 for all the involved MLP functions.
%

We constructed graphs from atoms in molecules, which are employed to learn the scalar embeddings and vector embeddings for atoms %predict atom coordinates and features  
(Eq.~\ref{eqn:geometric_embedding} and \ref{eqn:attention}), using the $N$-nearest neighbors based on Euclidean distance with $N=8$. 
%
We used $K=15$ atom features in total, indicating the atom types and its aromaticity.
%
These atom features include 10 non-aromatic atoms (i.e., ``H'', ``C'', ``N'', ``O'', ``F'', ``P'', ``S'', ``Cl'', ``Br'', ``I''), 
and 5 aromatic atoms (i.e., ``C'', ``N'', ``O'', ``P'', ``S'').
%
We set the number of diffusion steps $T$ as 1,000.
%
We set the weight $\xi$ of atom type loss (Eq.~\ref{eqn:loss}) as $100$ to balance the values of atom type loss and atom coordinate loss.
%
We set the threshold $\delta$ (Eq.~\ref{eqn:diff:obj:pos}) as 10.
%
The parameters $\beta_t^{\mathtt{x}}$ and $\beta_t^{\mathtt{v}}$ of variance scheduling in the forward diffusion process of \methoddiff are discussed in 
Supplementary Section~\ref{supp:forward:variance}.
%
%Please note that as in \squid, we did not perform extensive hyperparameter optimization for \methoddiff.
%
Following \squid, we did not perform extensive hyperparameter tunning for \methoddiff given that the used 
hyperparameters have enabled good performance.

%
We optimized the \methoddiff model via Adam~\cite{adam} with its parameters (0.950, 0.999), learning rate 0.001, and batch size 32.
%
We evaluated the validation loss every 2,000 training steps.
%
We scheduled to decay the learning rate with a factor of 0.6 and a minimum learning rate of 1e-5 if 
the validation loss does not decrease in 10 consecutive evaluations.
%
The \methoddiff model has 7.8M learnable parameters. 
%
We trained the \methoddiff model %for at most 60 hours 
with $\sim$770,000 training steps.
%
The training took 70 hours with our GPUs.
%
The trained \methoddiff achieved the minimum validation loss at 758,000 steps.

During inference, %the sampling, 
following Adams and Coley~\cite{adams2023equivariant}, we set the variance $\phi$ 
of atom-centered Gaussians as 0.049, which is used to build a set of points for shape guidance in Section ``\method with Shape Guidance'' 
in the main manuscript.
%
We determined the number of atoms in the generated molecule using the atom number distribution of training molecules that have surface shape sizes similar to the condition molecule.
%
The optimal distance threshold $\gamma$ is 0.2, and the optimal stop step $S$ for shape guidance is 300.
%
With shape guidance, each time we updated the atom position (Eq.~\ref{eqn:shape_guidance}), we randomly sampled the weight $\sigma$ from $[0.2, 0.8]$. %\bo{(XXX)}.
%
Moreover, when using pocket guidance as mentioned in Section ``\method with Pocket Guidance'' in the main manuscript, each time we updated the atom position (Eq.~\ref{eqn:pocket_guidance}), we randomly sampled the weight $\epsilon$ from $[0, 0.5]$. 
%
For each condition molecule, it took around 40 seconds on average to generate 50 molecule candidates with our GPUs.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance of \decompdiff with Protein Pocket Prior}
\label{supp:app:decompdiff}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we demonstrate that \decompdiff with protein pocket prior, referred to as \decompdiffbeta, shows very limited performance in generating drug-like and synthesizable molecules compared to all the other methods, including \methodwithpguide and \methodwithsandpguide.
%
We evaluate the performance of \decompdiffbeta in terms of binding affinities, drug-likeness, and diversity.
%
We compare \decompdiffbeta with \methodwithpguide and \methodwithsandpguide and report the results in Table~\ref{tbl:comparison_results_decompdiff}.
%
Note that the results of \methodwithpguide and \methodwithsandpguide here are consistent with those in Table~\ref{tbl:overall_results_docking2} in the main manuscript.
%
As shown in Table~\ref{tbl:comparison_results_decompdiff}, while \decompdiffbeta achieves high binding affinities in Vina M and Vina D, it substantially underperforms \methodwithpguide and \methodwithsandpguide in QED and SA.
%
Particularly, \decompdiffbeta shows a QED score of 0.36, while \methodwithpguide substantially outperforms \decompdiffbeta in QED (0.77) with 113.9\% improvement.
%
\decompdiffbeta also substantially underperforms \methodwithpguide in terms of SA scores (0.55 vs 0.76).
%
These results demonstrate the limited capacity of \decompdiffbeta in generating drug-like and synthesizable molecules.
%
As a result, the generated molecules from \decompdiffbeta can have considerably lower utility compared to other methods.
%
Considering these limitations of \decompdiffbeta, we exclude it from the baselines for comparison.

\input{tables/decompdiff_method_compare}

%===================================================================
\section{{Additional Experimental Results on SMG}}
\label{supp:app:results}
%===================================================================

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection{Comparison on Shape and Graph Similarity}
\label{supp:app:results:overall_shape}
%-------------------------------------------------------------------------------------------------------------------------------------

%\ziqi{Outline for this section:
%	\begin{itemize}
%		\item \method can consistently generate molecules with novel structures (low graph similarity) and similar shapes (high shape similarity), such that these molecules have comparable binding capacity with the condition molecules, and potentially better properties as will be shown in Table~\ref{tbl:overall_results_quality_10}.
%	\end{itemize}
%}

\input{tables/overall_results_sims}
%\label{tbl:overall_sim}


{We evaluate the shape similarity \shapesim and graph similarity \graphsim of molecules generated from}
%Table~\ref{tbl:overall_sim} presents the comparison of shape-conditioned molecule generation among 
\dataset, \squid, \method and \methodwithsguide under different graph similarity constraints  ($\delta_g$=1.0, 0.7, 0.5, 0.3). 
%
%During the evaluation, for each molecule in the test set, all the methods are employed to generate or identify 50 molecules with similar shapes.
%
We calculate evaluation metrics using all the generated molecules satisfying the graph similarity constraints.
%
Particularly, when $\delta_g$=1.0, we do not filter out any molecules based on the constraints and directly calculate metrics on all the generated molecules.
%
When $\delta_g$=0.7, 0.5 or 0.3, we consider only generated molecules with similarities lower than $\delta_g$.
%
Based on \shapesim and \graphsim as described in Section ``Evaluation Metrics'' in the main manuscript,
we calculate the following metrics using the subset of molecules with \graphsim lower than $\delta_g$, from a set of 50 generated molecules for each test molecule and report the average of  these metrics across all test molecules:
%
(1) \avgshapesim\ measures the average \shapesim across each subset of generated molecules with $\graphsim$ lower than $\delta_g$; %per test molecule, with the overall average calculated across all test molecules; }%the 50 generated molecules for each test molecule, averaged across all test molecules;
(2) \avggraphsim\ calculates the average \graphsim for each set; %, with these means averaged across all test molecules}; %} 50 molecules, %\bo{@Ziqi rephrase}, with results averaged on the test set;\ziqi{with the average computed over the test set; }
(3) \maxshapesim\ determines the maximum \shapesim within each set; %, with these maxima averaged across all test molecules; }%\hl{among 50 molecules}, averaged across all test molecules;
(4) \maxgraphsim\ measures the \graphsim of the molecule with maximum \shapesim in each set. %, averaged across all test molecules; }%\hl{among 50 molecules}, averaged across all test molecules;

%
As shown in Table~\ref{tbl:overall_sim}, \method and \methodwithsguide demonstrate outstanding performance in terms of the average shape similarities (\avgshapesim) and the average graph similarities (\avggraphsim) among generated molecules.
%
%\ziqi{
%Table~\ref{tbl:overall} also shows that \method and \methodwithsguide consistently outperform all the baseline methods in average shape similarities (\avgshapesim) and only slightly underperform 
%the best baseline \dataset in average graph similarities (\avggraphsim).
%}
%
Specifically, when $\delta_g$=0.3, \methodwithsguide achieves a substantial 10.5\% improvement in \avgshapesim\ over the best baseline \dataset. 
%
In terms of \avggraphsim, \methodwithsguide also achieves highly comparable performance with \dataset (0.217 vs 0.211, in \avggraphsim, lower values indicate better performance).
%
%This trend remains consistent across various $\delta_g$ values.
This trend remains consistent when applying various similarity constraints (i.e., $\delta_g$) as shown in Table~\ref{tbl:overall_sim}.


Similarly, \method and \methodwithsguide demonstrate superior performance in terms of the average maximum shape similarity across generated molecules for all test molecules (\maxshapesim), as well as the average graph similarity of the molecules with the maximum shape similarities (\maxgraphsim). %maximum shape similarities of generated molecules (\maxshapesim) and the average graph similarities of molecules with the maximum shape similarities (\maxgraphsim). %\bo{\maxgraphsim is misleading... how about $\text{avgMSim}_\text{g}$}
%
%\bo{
%in terms of the maximum shape similarities (\maxshapesim) and the maximum graph similarities (\maxgraphsim) among all the generated molecules.
%@Ziqi are the metrics maximum values or the average of maximum values?
%}
%
Specifically, at \maxshapesim, Table~\ref{tbl:overall_sim} shows that \methodwithsguide outperforms the best baseline \squid ($\lambda$=0.3) when $\delta_g$=0.3, 0.5, and 0.7, and only underperforms
it by 0.7\% when $\delta$=1.0.
%
We also note that the molecules generated by {\methodwithsguide} with the maximum shape similarities have substantially lower graph similarities ({\maxgraphsim}) compared to those generated by {\squid} ({$\lambda$}=0.3).
%\hl{We also note that the molecules with the maximum shape similarities generated by {\methodwithsguide} are with significantly lower graph similarities ({\maxgraphsim}) than those generated by {\squid} ({$\lambda$}=0.3).}
%
%\bo{@Ziqi please rephrase the language}
%
%\bo{
%@Ziqi the conclusion is not obvious. You may want to remind the meaning of \maxshapesim and \maxgraphsim here, and based on what performance you say this.
%}
%
%\bo{\st{This also underscores the ability of {\methodwithsguide} in generating molecules with similar shapes to condition molecules and novel graph structures.}}
%
As evidenced by these results, \methodwithsguide features strong capacities of generating molecules with similar shapes yet novel graph structures compared to the condition molecule, facilitating the discovery of promising drug candidates.
%

\begin{comment}
\ziqi{replace \#n\% with the percentage of novel molecules that do not exist in the dataset and update the discussion accordingly}
%\ziqi{
Table~\ref{tbl:overall_sim} also presents \bo{\#n\%}, the percentage of molecules generated by each method %\st{(\#n\%)} 
with graph similarities lower than the constraint $\delta_g$. 
%
%\bo{
%Table~\ref{tbl:overall_sim} also presents \#n\%, the percentage of generated molecules with graph similarities lower than the constraint $\delta_g$, of different methods. 
%}
%
As shown in Table~\ref{tbl:overall_sim},  when a restricted constraint (i.e., $\delta_g$=0.3) is applied, \method and \methodwithsguide could still generate a sufficient number of molecules satisfying the constraint.
%
Particularly, when $\delta_g$=0.3, \method outperforms \squid with $\lambda$=0.3 by XXX and \squid with $\lambda$=1.0 by XXX.
% achieve the second and the third in \#n\% and only underperform the best baseline \dataset.
%
This demonstrates the ability of \method in generating molecules with novel structures. 
%
When $\delta_g$=0.5, 0.7 and 1.0, both methods generate over 99.0\% of molecules satisfying the similarity constraint $\delta_g$.
%
%Note that \dataset is guaranteed to identify at least 50 molecules satisfying the $\delta_g$ by searching within a training dataset of diverse molecules.
%
Note that \dataset is a search algorithm that always first identifies the molecules satisfying $\delta_g$ and then selects the top-50 molecules of the highest shape similarities among them. 
%
Due to the diverse molecules in %\hl{the subset} \bo{@Ziqi why do you want to stress subset?} of 
the training set, \dataset can always identify at least 50 molecules under different $\delta_g$ and thus achieve 100\% in \#n\%.
%
%\bo{
%Note that \dataset is a search algorithm that always generate molecules XXX
%@Ziqi
%We need to discuss here. For \dataset, \#n\% in this table does not look aligned with that in Fig 1 if the highlighted defination is correct...
%}
%
%Thus, \dataset achieves 100.0\% in \#n\% under different $\delta_g$.
%
It is also worth noting that when $\delta_g$=1.0, \#n\% reflects the validity among all the generated molecules. 
%
As shown in Table~\ref{tbl:overall_sim}, \method and \methodwithsguide are able to generate 99.3\% and 99.2\% valid molecules.
%
This demonstrates their ability to effectively capture the underlying chemical rules in a purely data-driven manner without relying on any prior knowledge (e.g., fragments) as \squid does.
%
%\bo{
%@Ziqi I feel this metric is redundant with the avg graph similarity when constraint is 1.0. Generally, if the avg similarity is small. You have more mols satisfying the requirement right?
%}
\end{comment}

Table~\ref{tbl:overall_sim} also shows that by incorporating shape guidance, \methodwithsguide
%\bo{
%@Ziqi where does this come from...
%}
substantially outperforms \method in both \avgshapesim and \maxshapesim, while maintaining comparable graph similarities (i.e., \avggraphsim\ and \maxgraphsim).
%
Particularly, when $\delta_g$=0.3, \methodwithsguide 
establishes a considerable improvement of 6.9\% and 4.9\%
%\bo{\st{achieves 6.9\% and 4.9\% improvements}} 
over \method in \avgshapesim and \maxshapesim, respectively. 
%
%\hl{In the meanwhile}, 
%\bo{@Ziqi it is not the right word...}
Meanwhile, \methodwithsguide achieves the same \avggraphsim with \method and only slightly underperforms \method in \maxgraphsim (0.223 vs 0.220).
%\bo{
%XXX also achieves XXX
%}
%it maintains the same \avggraphsim\ with \method and only slightly underperforms \method in \maxgraphsim (0.223 vs 0.220).
%
%Compared with \method, \methodwithsguide consistently generates molecules with higher shape similarities while maintaining comparable graph similarities.
%
%\bo{
%@Ziqi you may want to highlight the utility of "generating molecules with higher shape similarities while maintaining comparable graph similarities" in real drug discovery applications.
%
%
%\bo{
%@Ziqi You did not present the details of method yet...
%}
%
%\methodwithsguide leverages additional shape guidance to push the predicted atoms to the shape of condition molecules \bo{and XXX (@Ziqi boosts the shape similarities XXX)} , as will be discussed in Section ``\method with Shape Guidance'' later.
%
The superior performance of \methodwithsguide suggests that the incorporation of shape guidance effectively boosts the shape similarities of generated molecules without compromising graph similarities.
%
%This capability could be crucial in drug discovery, 
%\bo{@Ziqi it is a strong statement. Need citations here}, 
%as it enables the discovery of drug candidates that are both more potentially effective due to the improved shape similarities and novel induced by low graph similarities.
%as it could enable the identification of candidates with similar binding patterns %with the condition molecule (i.e., high shape similarities) 
%(i.e., high shape similarities) and graph structures distinct from the condition molecules (i.e., low graph similarities).
%\bo{\st{and enjoys novel structures (i.e., low graph similarities) with potentially better properties. } \ziqi{change enjoys}}
%\bo{
%and enjoys potentially better properties (i.e., low graph similarities). \ziqi{this looks weird to me... need to discuss}
%}
%\st{potentially better properties (i.e., low graph similarities).}}

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection{Comparison on Validity and Novelty}
\label{supp:app:results:valid_novel}
%-------------------------------------------------------------------------------------------------------------------------------------

We evaluate the ability of \method and \squid to generate molecules with valid and novel 2D molecular graphs.
%
We calculate the percentages of the valid and novel molecules among all the generated molecules.
%
As shown in Table~\ref{tbl:validity_novelty}, both \method and \methodwithsguide outperform \squid with $\lambda$=0.3 and $\lambda$=1.0 in generating novel molecules.
%
Particularly, almost all valid molecules generated by \method and \methodwithsguide are novel (99.8\% and 99.9\% at \#n\%), while the best baseline \squid with $\lambda$=0.3 achieves 98.4\% in novelty.
%
In terms of the percentage of valid and novel molecules among all the generated ones (\#v\&n\%), \method and \methodwithsguide again outperform \squid with $\lambda$=0.3 and $\lambda$=1.0.
%
We also note that at \#v\%,  \method (99.1\%) and \methodwithsguide (99.2\%) slightly underperform \squid with $\lambda$=0.3 and $\lambda$=1.0 (100.0\%) in generating valid molecules.
%
\squid guarantees the validity of generated molecules by incorporating valence rules into the generation process and ensuring it to avoid fragments that violate these rules.
%
Conversely, \method and \methodwithsguide use a purely data-driven approach to learn the generation of valid molecules.
%
These results suggest that, even without integrating valence rules, \method and \methodwithsguide can still achieve a remarkably high percentage of valid and novel generated molecules.

\input{tables/validity_novelty}

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection{Additional Quality Comparison between Desirable Molecules Generated by \method and \squid}
\label{supp:app:results:quality_desirable}
%-------------------------------------------------------------------------------------------------------------------------------------

\input{tables/overall_results_quality0.5}
%\label{tbl:overall_quality05}

\input{tables/overall_results_quality0.7}
%\label{tbl:overall_quality07}

\input{tables/overall_results_quality1.0}
%\label{tbl:overall_quality10}

Similar to Table~\ref{tbl:overall_results_quality_desired} in the main manuscript, we present the performance comparison on the quality of desirable molecules generated by different methods under different graph similarity constraints $\delta_g$=0.5, 0.7 and 1.0, as detailed in Table~\ref{tbl:overall_results_quality_05}, Table~\ref{tbl:overall_results_quality_07}, and Table~\ref{tbl:overall_results_quality_10}, respectively.
%
Overall, these tables show that under varying graph similarity constraints, \method and \methodwithsguide can always generate desirable molecules with comparable quality to baselines in terms of stability, 3D structures, and 2D structures.
%
These results demonstrate the strong effectiveness of \method and \methodwithsguide in generating high-quality desirable molecules with stable and realistic structures in both 2D and 3D.
%
This enables the high utility of \method and \methodwithsguide in discovering promising drug candidates.


\begin{comment}
The results across these tables demonstrate similar observations with those under $\delta_g$=0.3 in Table~\ref{tbl:overall_results_quality_desired}.
%
For stability, when $\delta_g$=0.5, 0.7 or 1.0, \method and \methodwithsguide achieve comparable performance or fall slightly behind \squid ($\lambda$=0.3) and \squid ($\lambda$=1.0) in atom stability and molecule stability.
%
For example, when $\delta_g$=0.5, as shown in Table~\ref{tbl:overall_results_quality_05}, \method achieves similar performance with the best baseline \squid ($\lambda$=0.3) in atom stability (0.992 for \method vs 0.996 for \squid with $\lambda$=0.3).
%
\method underperforms \squid ($\lambda$=0.3) in terms of molecule stability.
%
For 3D structures, \method and \methodwithsguide also consistently generate molecules with more realistic 3D structures compared to \squid.
%
Particularly, \methodwithsguide achieves the best performance in RMSD and JS of bond lengths across $\delta_g$=0.5, 0.7 and 1.0.
%
In JS of dihedral angles, \method achieves the best performance among all the methods.
%
\method and \methodwithsguide underperform \squid in JS of bond angles, primarily because \squid constrains the bond angles in the generated molecules.
%
For 2D structures, \method and \methodwithsguide again achieve the best performance 
\end{comment}

%===================================================================
\section{Additional Experimental Results on PMG}
\label{supp:app:results_PMG}
%===================================================================

%\label{tbl:comparison_results_decompdiff}


%-------------------------------------------------------------------------------------------------------------------------------------
%\subsection{{Additional Comparison for PMG}}
%\label{supp:app:results:docking}
%-------------------------------------------------------------------------------------------------------------------------------------

In this section, we present the results of \methodwithpguide and \methodwithsandpguide when generating 100 molecules. 
%
Please note that both \methodwithpguide and \methodwithsandpguide show remarkable efficiency over the PMG baselines.
%
\methodwithpguide and \methodwithsandpguide generate 100 molecules in 48 and 58 seconds on average, respectively, while the most efficient baseline \targetdiff requires 1,252 seconds.
%
We report the performance of \methodwithpguide and \methodwithsandpguide against state-of-the-art PMG baselines in Table~\ref{tbl:overall_results_docking_100}.


%
According to Table~\ref{tbl:overall_results_docking_100}, \methodwithpguide and \methodwithsandpguide achieve comparable performance with the PMG baselines in generating molecules with high binding affinities.
%
Particularly, in terms of Vina S, \methodwithsandpguide achieves very comparable performance (-4.56 kcal/mol) to the third-best baseline \decompdiff (-4.58 kcal/mol) in average Vina S; it also achieves the third-best performance (-4.82 kcal/mol) among all the methods and slightly underperforms the second-best baseline \AR (-4.99 kcal/mol) in median Vina S
%
\methodwithsandpguide also achieves very close average Vina M (-5.53 kcal/mol) with the third-best baseline \AR (-5.59 kcal/mol) and the third-best performance (-5.47 kcal/mol) in median Vina M.
%
Notably, for Vina D, \methodwithpguide and \methodwithsandpguide achieve the second and third performance among all the methods.
%
In terms of the average percentage of generated molecules with Vina D higher than those of known ligands (i.e., HA), \methodwithpguide (58.52\%) and \methodwithsandpguide (58.28\%) outperform the best baseline \targetdiff (57.57\%).
%
These results signify the high utility of \methodwithpguide and \methodwithsandpguide in generating molecules that effectively bind with protein targets and have better binding affinities than known ligands.

In addition to binding affinities, \methodwithpguide and \methodwithsandpguide also demonstrate similar performance compared to the baselines in metrics related to drug-likeness and diversity.
%
For drug-likeness, both \methodwithpguide and \methodwithsandpguide achieve the best (0.67) and the second-best (0.66) QED scores.
%
They also achieve the third and fourth performance in SA scores.
%
In terms of the diversity among generated molecules,  \methodwithpguide and \methodwithsandpguide slightly underperform the baselines, possibly due to the design that generates molecules with similar shapes to the ligands.
%
These results highlight the strong ability of \methodwithpguide and \methodwithsandpguide in efficiently generating effective binding molecules with favorable drug-likeness and diversity.
%
This ability enables them to potentially serve as promising tools to facilitate effective and efficient drug development.

\input{tables/overall_results_docking4}
%\label{tbl:overall_results_docking_100}

%-------------------------------------------------------------------------------------------------------------------------------------
%\subsection{{Comparison of Pocket Guidance}}
%\label{supp:app:results:docking}
%-------------------------------------------------------------------------------------------------------------------------------------


\begin{comment}
%-------------------------------------------------------------------------------------------------------------------------------------
\subsection{\ziqi{Simiarity Comparison for Pocket-based Molecule Generation}}
%-------------------------------------------------------------------------------------------------------------------------------------


\input{tables/docking_results_similarity}
%\label{tbl:docking_results_similarity}

\bo{@Ziqi you may want to check my edits for the discussion in Table 1 first.
%
If the pocket if known, do you still care about the shape similarity in real applications?
}

\ziqi{Table~\ref{tbl:docking_results_similarity} presents the overall comparison on similarity-based metrics between \methodwithpguide, \methodwithsandpguide and other baselines under different graph similarity constraints  ($\delta_g$=1.0, 0.7, 0.5, 0.3), similar to Table~\ref{tbl:overall}. 
%
As shown in Table~\ref{tbl:docking_results_similarity}, regarding desirable molecules,  \methodwithsandpguide consistently outperforms all the baseline methods in the likelihood of generating desirable molecules (i.e., $\#d\%$).
%
For example, when $\delta_g$=1.0, at $\#d\%$, \methodwithsandpguide (45.2\%) demonstrates significant improvement of $21.2\%$ compared to the best baseline \decompdiff (37.3\%).
%
In terms of $\diversity_d$, \methodwithpguide and \methodwithsandpguide also achieve the second and the third best performance. 
%
Note that the best baseline \targetdiff in $\diversity_d$ achieves the least percentage of desirable molecules (7.1\%), substantially lower than \methodwithpguide and \methodwithsandpguide.
%
This makes its diversity among desirable molecules incomparable with other methods. 
%
When $\delta_g$=0.7, 0.5, and 0.3, \methodwithsandpguide also establishes a significant improvement of 24.3\%, 27.8\%, and 31.1\% compared to the best baseline method \decompdiff.
%
It is also worth noting that the state-of-the-art baseline \decompdiff underperforms \methodwithpguide and \methodwithsandpguide in binding affinities as shown in Table~\ref{tbl:overall_results_docking}, even though it outperforms \methodwithpguide in \#d\%.
%
\methodwithpguide and \methodwithsandpguide also achieve the second and the third best performance in $\diversity_d$ at $\delta_g$=0.7, 0.5, and 0.3. 
%
The superior performance of \methodwithpguide and \methodwithsandpguide in $\#d\%$ at small $\delta_g$ indicates their strong capacity in generating desirable molecules of novel graph structures, thereby facilitating the discovery of novel drug candidates.
%
}

\ziqi{Apart from the desirable molecules, \methodwithpguide and \methodwithsandpguide also demonstrate outstanding performance in terms of the average shape similarities (\avgshapesim) and the average graph similarities (\avggraphsim).
%
Specifically, when $\delta_g$=1.0, \methodwithsandpguide achieves a significant 2.5\% improvement in \avgshapesim\ over the best baseline \decompdiff. 
%
In terms of \avggraphsim, \methodwithsandpguide also achieves higher performance than the baseline \decompdiff of the highest \avgshapesim (0.265 vs 0.282).
%
Please note that all the baseline methods except \decompdiff achieve substantially lower performance in \avgshapesim than \methodwithpguide and \methodwithsandpguide, even though these methods achieve higher \avggraphsim values.
%
This trend remains consistent when applying various similarity constraints (i.e., $\delta_g$) as shown in Table~\ref{tbl:overall_results_docking}.
}

\ziqi{Similarly, \methodwithpguide and \methodwithsandpguide also achieve superior performance in \maxshapesim and \maxgraphsim.
%
Specifically, when $\delta_g$=1.0, for \maxshapesim, \methodwithsandpguide achieves highly comparable performance in \maxshapesim\ compared to the best baseline \decompdiff (0.876 vs 0.878).
%
We also note that \methodwithsandpguide achieves lower \maxgraphsim\ than the \decompdiff with 23.0\% difference. 
%
When $\delta_g$ gets smaller from 0.7 to 0.3, \methodwithsandpguide maintains a high \maxshapesim value around 0.876, while the best baseline \decompdiff has \maxshapesim decreased from 0.878 to 0.854.
%
This demonstrates the superior ability of \methodwithsandpguide in generating molecules with similar shapes and novel structures.
%
}

\ziqi{
In terms of \#n\%, when $\delta_g$=1.0, the percentage of molecules with \graphsim below $\delta_g$ can be interpreted as the percentage of valid molecules among all the generated molecules. 
%
As shown in Table~\ref{tbl:docking_results_similarity}, \methodwithpguide and \methodwithsandpguide are able to generate 98.1\% and 97.8\% of valid molecules, slightly below the best baseline \pockettwomol (98.3\%). 
%
When $\delta_g$=0.7, 0.5, or 0.3, all the methods, including \methodwithpguide and \methodwithsandpguide, can consistently find a sufficient number of novel molecules that meet the graph similarity constraints.
%
The only exception is \decompdiff, which substantially underperforms all the other methods in \#n\%.
}
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Properties of Molecules in Case Studies for Targets}
\label{supp:app:results:properties}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection{Drug Properties of Generated Molecules}
\label{supp:app:results:properties:drug}
%-------------------------------------------------------------------------------------------------------------------------------------

Table~\ref{tbl:drug_property} presents the drug properties of three generated molecules: NL-001, NL-002, and NL-003.
%
As shown in Table~\ref{tbl:drug_property}, each of these molecules has a favorable profile, making them promising drug candidates. 
%
{As discussed in Section ``Case Studies for Targets'' in the main manuscript, all three molecules have high binding affinities in terms of Vina S, Vina M and Vina D, and favorable QED and SA values.
%
In addition, all of them meet the Lipinski's rule of five criteria~\cite{Lipinski1997}.}
%
In terms of physicochemical properties, all these properties of NL-001, NL-002 and NL-003, including number of rotatable bonds, molecule weight, LogP value, number of hydrogen bond doners and acceptors, and molecule charges, fall within the desired range of drug molecules. 
%
This indicates that these molecules could potentially have good solubility and membrane permeability, essential qualities for effective drug absorption.

These generated molecules also demonstrate promising safety profiles based on the predictions from ICM~\cite{Neves2012}.
%
In terms of drug-induced liver injury prediction scores, all three molecules have low scores (0.188 to 0.376), indicating a minimal risk of hepatotoxicity. 
%
NL-001 and NL-002 fall under `Ambiguous/Less concern' for liver injury, while NL-003 is categorized under 'No concern' for liver injury. 
%
Moreover, all these molecules have low toxicity scores (0.000 to 0.236). 
%
NL-002 and NL-003 do not have any known toxicity-inducing functional groups. 
%
NL-001 and NL-003 are also predicted not to include any known bad groups that lead to inappropriate features.
%
These attributes highlight the potential of NL-001, NL-002, and NL-003 as promising treatments for cancers and Alzheimerâ€™s disease.

%\input{tables/binding_generated_mols}
%\label{tbl:binding_drug_mols}

\input{tables/drug_property_generated_mols}
%\label{tbl:drug_property}

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection{Comparison on ADMET Profiles between Generated Molecules and Approved Drugs}
\label{supp:app:results:properties:admet}
%-------------------------------------------------------------------------------------------------------------------------------------

\paragraph{Generated Molecules for CDK6}
%
Table~\ref{tbl:admet_cdk6} presents the comparison on ADMET profiles between two generated molecules for CDK6 and the approved CDK6 inhibitors, including Abemaciclib~\cite{Patnaik2016}, Palbociclib~\cite{Lu2015}, and Ribociclib~\cite{Tripathy2017}.
%
As shown in Table~\ref{tbl:admet_cdk6}, the generated molecules, NL-001 and NL-002, exhibit comparable ADMET profiles with those of the approved CDK6 inhibitors. 
%
Importantly, both molecules demonstrate good potential in most crucial properties, including Ames mutagenesis, favorable oral toxicity, carcinogenicity, estrogen receptor binding, high intestinal absorption and favorable oral bioavailability.
%
Although the generated molecules are predicted as positive in hepatotoxicity and mitochondrial toxicity, all the approved drugs are also predicted as positive in these two toxicity.
%
This result suggests that these issues might stem from the limited prediction accuracy rather than being specific to our generated molecules.
%
Notably, NL-001 displays a potentially better plasma protein binding score compared to other molecules, which may improve its distribution within the body. 
%
Overall, these results indicate that NL-001 and NL-002 could be promising candidates for further drug development.


\input{tables/admet_property_cdk6}
%\label{tbl:admet_cdk6}

\paragraph{Generated Molecule for NEP}
%
Table~\ref{tbl:admet_nep} presents the comparison on ADMET profiles between a generated molecule for NEP targeting Alzheimer's disease and three approved drugs, Donepezil, Galantamine, and Rivastigmine, for Alzheimer's disease~\cite{Hansen2008}.
%
Overall, NL-003 exhibits a comparable ADMET profile with the three approved drugs.  
%
Notably, same as other approved drugs, NL-003 is predicted to be able to penetrate the blood brain barrier, a crucial property for Alzheimer's disease.
%  
In addition, it demonstrates a promising safety profile in terms of Ames mutagenesis, favorable oral toxicity, carcinogenicity, estrogen receptor binding, high intestinal absorption, nephrotoxicity and so on.
%
These results suggest that NL-003 could be promising candidates for the drug development of Alzheimer's disease.

\input{tables/admet_property_nep}
%\label{tbl:admet_nep}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithms}
\label{supp:algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Algorithm~\ref{alg:shapemol} describes the molecule generation process of \method.
%
Given a known ligand \molx, \method generates a novel molecule \moly that has a similar shape to \molx and thus potentially similar binding activity.
%
\method can also take the protein pocket \pocket as input and adjust the atoms of generated molecules for optimal fit and improved binding affinities.
%
Specifically, \method first calculates the shape embedding \shapehiddenmat for \molx using the shape encoder \SEE described in Algorithm~\ref{alg:see_shaperep}.
%
Based on \shapehiddenmat, \method then generates a novel molecule with a similar shape to \molx using the diffusion-based generative model \methoddiff as in Algorithm~\ref{alg:diffgen}.
%
During generation, \method can use shape guidance to directly modify the shape of \moly to closely resemble the shape of \molx.
%
When the protein pocket \pocket is available, \method can also use pocket guidance to ensure that \moly is specifically tailored to closely fit within \pocket.

\input{algorithms/shapemol}
%\label{alg:shapemol}

\input{algorithms/shaperep}
%\label{alg:see_shaperep}

\input{algorithms/diffgen}
%\label{alg:diffgen}

%\input{algorithms/train_SE}
%\label{alg:train_se}

%\input{algorithms/train_diff}
%\label{alg:train_diff}

%---------------------------------------------------------------------------------------------------------------------
\section{{Equivariance and Invariance}}
\label{supp:ei}
%---------------------------------------------------------------------------------------------------------------------

%.................................................................................................
\subsection{Equivariance}
\label{supp:ei:equivariance}
%.................................................................................................

{Equivariance refers to the property of a function $f(\pos)$ %\bo{is it the property of the function or embedding (x)?} 
that any translation and rotation transformation from the special Euclidean group SE(3)~\cite{Atz2021} applied to a geometric object
$\pos\in\mathbb{R}^3$ is mirrored in the output of $f(\pos)$, accordingly.
%
This property ensures $f(\pos)$ to learn a consistent representation of an object's geometric information, regardless of its orientation or location in 3D space.
%
%As a result, it provides $f(\pos)$ better generalization capabilities~\cite{Jonas20a}.
%
Formally, given any translation transformation $\mathbf{t}\in\mathbb{R}^3$ and rotation transformation $\mathbf{R}\in\mathbb{R}^{3\times3}$ ($\mathbf{R}^{\mathsf{T}}\mathbf{R}=\mathbb{I}$), %\xia{change the font types for $^{\mathsf{T}}$ and $\mathbb{I}$ in the entire manuscript}), 
$f(\pos)$ is equivariant with respect to these transformations %$g$ (\bo{where is $g$...})
if it satisfies
\begin{equation}
f(\mathbf{R}\pos+\mathbf{t}) = \mathbf{R}f(\pos) + \mathbf{t}. %\ \text{where}\ \hiddenpos = f(\pos).
\end{equation}
%
%where $\hiddenpos=f(\pos)$ is the output of $\pos$. 
%
In \method, both \SE and \methoddiff are developed to guarantee equivariance in capturing the geometric features of objects regardless of any translation or rotation transformations, as will be detailed in the following sections.
}

%.................................................................................................
\subsection{Invariance}
\label{supp:ei:invariance}
%.................................................................................................

%In contrast to equivariance, 
Invariance refers to the property of a function that its output {$f(\pos)$} remains constant under any translation and rotation transformations of the input $\pos$. %a geometric object's feature $\pos$.
%
This property enables $f(\pos)$ to accurately capture %a geometric object's 
the inherent features (e.g., atom features for 3D molecules) that are invariant of its orientation or position in 3D space.
%
Formally, $f(\pos)$ is invariant under any translation $\mathbf{t}$ and  rotation $\mathbf{R}$ if it satisfies
%
\begin{equation}
f(\mathbf{R}\pos+\mathbf{t}) = f(\pos).
\end{equation}
%
In \method, both \SE and \methoddiff capture the inherent features of objects in an invariant way, regardless of any translation or rotation transformations, as will be detailed in the following sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Point Cloud Construction}
\label{supp:point_clouds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \method, we represented molecular surface shapes using point clouds (\pc).
%
$\pc$
serves as input to \SE, from which we derive shape latent embeddings.
%
To generate $\pc$, %\bo{\st{create this}}, \bo{generate $\pc$}
we initially generated a molecular surface mesh using the algorithm from the Open Drug Discovery Toolkit~\cite{Wjcikowski2015oddt}.
%
Following this, we uniformly sampled points on the mesh surface with probability proportional to the face area, %\xia{how to uniformly?}, ensuring the sampling is done proportionally to the face area, with
using the algorithm from PyTorch3D~\cite{ravi2020pytorch3d}.
%
This point cloud $\pc$ is then centralized by setting the center of its points to zero.
%
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Query Point Sampling}
\label{supp:training:shapeemb}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As described in Section ``Shape Decoder (\SED)'', the signed distances of query points $z_q$ to molecule surface shape $\pc$ are used to optimize \SE.
%
In this section, we present how to sample these points $z_q$ in 3D space.
%
Particularly, we first determined the bounding box around the molecular surface shape, using the maximum and minimum \mbox{($x$, $y$, $z$)-axis} coordinates for points in our point cloud \pc,
denoted as $(x_\text{min}, y_\text{min}, z_\text{min})$ and $(x_\text{max}, y_\text{max}, z_\text{max})$.
%
We extended this box slightly by defining its corners as \mbox{$(x_\text{min}-1, y_\text{min}-1, z_\text{min}-1)$} and \mbox{$(x_\text{max}+1, y_\text{max}+1, z_\text{max}+1)$}.
%
For sampling $|\mathcal{Z}|$ query points, we wanted an even distribution of points inside and outside the molecule surface shape.
%
%\ziqi{Typically, within this bounding box, molecules occupy only a small portion of volume, which makes it more likely to sample
%points outside the molecule surface shape.}
%
When a bounding box is defined around the molecule surface shape, there could be a lot of empty spaces within the box that the molecule does not occupy due to 
its complex and irregular shape.
%
This could lead to that fewer points within the molecule surface shape could be sampled within the box.
%
Therefore, we started by randomly sampling $3k$ points within our bounding box to ensure that there are sufficient points within the surface.
%
We then determined whether each point lies within the molecular surface, using an algorithm from Trimesh~\footnote{https://trimsh.org/} based on the molecule surface mesh.
%
If there are $n_w$ points found within the surface, we selected $n=\min(n_w, k/2)$ points from these points, 
and randomly choose the remaining 
%\bo{what do you mean by remaining? If all the 3k sampled points are inside the surface, you get no points left.} 
$k-n$ points 
from those outside the surface.
%
For each query point, we determined its signed distance to the molecule surface by its closest distance to points in \pc with a sign indicating whether it is inside the surface.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Forward Diffusion (\diffnoise)}
\label{supp:forward}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%===================================================================
\subsection{{Forward Process}}
\label{supp:forward:forward}
%===================================================================

Formally, for atom positions, the probability of $\pos_t$ sampled given $\pos_{t-1}$, denoted as $q(\pos_t|\pos_{t-1})$, is defined as follows,
%\xia{revise the representation, should be $\beta^x_t$ -- note the space} as follows,
%
\begin{equation}
q(\pos_t|\pos_{t-1}) = \mathcal{N}(\pos_t|\sqrt{1-\beta^{\mathtt{x}}_t}\pos_{t-1}, \beta^{\mathtt{x}}_t\mathbb{I}), 
\label{eqn:noiseposinter}
\end{equation}
%
%\xia{should be a comma after the equation. you also missed it. }
%\st{in which} 
where %\hl{$\pos_0$ denotes the original atom position;} \xia{no $\pos_0$ in the equation...}
%$\mathbf{I}$ denotes the identity matrix;
$\mathcal{N}(\cdot)$ is a Gaussian distribution of $\pos_t$ with mean $\sqrt{1-\beta_t^{\mathtt{x}}}\pos_{t-1}$ and covariance $\beta_t^{\mathtt{x}}\mathbf{I}$.
%\xia{what is $\mathcal{N}$? what is $q$? you abused $q$. need to be crystal clear... }
%\bo{Should be $\sim$ not $=$ in the equation}
%
Following Hoogeboom \etal~\cite{hoogeboom2021catdiff}, 
%the forward process for the discrete atom feature $\atomfeat_t\in\mathbb{R}^K$ adds 
%categorical noise into $\atomfeat_{t-1}$ according to a variance schedule $\beta_t^v\in (0, 1)$. %as follows, %\hl{$\betav_t\in (0, 1)$} as follows,
%\xia{presentation...check across the entire manuscript... } as follows,
%
%\ziqi{Formally, 
for atom features, the probability of $\atomfeat_t$ across $K$ classes given $\atomfeat_{t-1}$ is defined as follows,
%
\begin{equation}
q(\atomfeat_t|\atomfeat_{t-1}) = \mathcal{C}(\atomfeat_t|(1-\beta^{\mathtt{v}}_t) \atomfeat_{t-1}+\beta^{\mathtt{v}}_t\mathbf{1}/K),
\label{eqn:noisetypeinter}
\end{equation}
%
where %\hl{$\atomfeat_0$ denotes the original atom positions}; 
$\mathcal{C}$ is a categorical distribution of $\atomfeat_t$ derived from the %by 
noising $\atomfeat_{t-1}$ with a uniform noise $\beta^{\mathtt{v}}_t\mathbf{1}/K$ across $K$ classes.
%adding an uniform noise $\beta^v_t$ to $\atomfeat_{t-1}$ across K classes.
%\xia{there is always a comma or period after the equations. Equations are part of a sentence. you always missed it. }
%\xia{what is $\mathcal{C}$? what does $q$ mean? it is abused. }

Since the above distributions form Markov chains, %} \xia{grammar!}, 
the probability of any $\pos_t$ or $\atomfeat_t$ can be derived from $\pos_0$ or $\atomfeat_0$:
%samples $\mol_0$ as follows,
%
\begin{eqnarray}
%\begin{aligned}
& q(\pos_t|\pos_{0}) & = \mathcal{N}(\pos_t|\sqrt{\cumalpha^{\mathtt{x}}_t}\pos_0, (1-\cumalpha^{\mathtt{x}}_t)\mathbb{I}), \label{eqn:noisepos}\\
& q(\atomfeat_t|\atomfeat_0)  & = \mathcal{C}(\atomfeat_t|\cumalpha^{\mathtt{v}}_t\atomfeat_0 + (1-\cumalpha^{\mathtt{v}}_t)\mathbf{1}/K), \label{eqn:noisetype}\\
& \text{where }\cumalpha^{\mathtt{u}}_t & = \prod\nolimits_{\tau=1}^{t}\alpha^{\mathtt{u}}_\tau, \ \alpha^{\mathtt{u}}_\tau=1 - \beta^{\mathtt{u}}_\tau, \ {\mathtt{u}}={\mathtt{x}} \text{ or } {\mathtt{v}}.\;\;\;\label{eqn:noiseschedule}
%\end{aligned}
\label{eqn:pos_prior}
\end{eqnarray}
%\xia{always punctuations after equations!!! also use ``eqnarray" instead of ``equation" + ``aligned" for multiple equations, each
%with a separate reference numbering...}
%\st{in which}, 
%where \ziqi{$\cumalpha^u_t = \prod_{\tau=1}^{t}\alpha^u_\tau$ and $\alpha^u_\tau=1 - \beta^u_\tau$ ($u$=$x$ or $v$)}.
%\xia{no such notations in the above equations; also subscript $s$ is abused with shape};
%$K$ is the number of categories for atom features.
%
%The details about noise schedules $\beta^x_t$ and $\beta^v_t$ are available in Supplementary Section \ref{XXX}. \ziqi{add trend}
%
Note that $\bar{\alpha}^{\mathtt{u}}_t$ ($\mathtt{u}={\mathtt{x}}\text{ or }{\mathtt{v}}$)
%($u$=$x$ or $v$) 
is monotonically decreasing from 1 to 0 over $t=[1,T]$. %\xia{=???}. 
%
As $t\rightarrow 1$, $\cumalpha^{\mathtt{x}}_t$ and $\cumalpha^{\mathtt{v}}_t$ are close to 1, leading to that $\pos_t$ or $\atomfeat_t$ approximates 
%the original data 
$\pos_0$ or $\atomfeat_0$.
%
Conversely, as  $t\rightarrow T$, $\cumalpha^{\mathtt{x}}_t$ and $\cumalpha^{\mathtt{v}}_t$ are close to 0,
leading to that $q(\pos_T|\pos_{0})$ %\st{$\rightarrow \mathcal{N}(\mathbf{0}, \mathbf{I})$} 
resembles  {$\mathcal{N}(\mathbf{0}, \mathbb{I})$} 
and $q(\atomfeat_T|\atomfeat_0)$ %\st{$\rightarrow \mathcal{C}(\mathbf{I}/K)$} 
resembles {$\mathcal{C}(\mathbf{1}/K)$}.

Using Bayes theorem, the ground-truth Normal posterior of atom positions $p(\pos_{t-1}|\pos_t, \pos_0)$ can be calculated in a
closed form~\cite{ho2020ddpm} as below,
%
\begin{eqnarray}
& p(\pos_{t-1}|\pos_t, \pos_0) = \mathcal{N}(\pos_{t-1}|\mu(\pos_t, \pos_0), \tilde{\beta}^\mathtt{x}_t\mathbb{I}), \label{eqn:gt_pos_posterior_1}\\
&\!\!\!\!\!\!\!\!\!\!\!\mu(\pos_t, \pos_0)\!=\!\frac{\sqrt{\bar{\alpha}^{\mathtt{x}}_{t-1}}\beta^{\mathtt{x}}_t}{1-\bar{\alpha}^{\mathtt{x}}_t}\pos_0\!+\!\frac{\sqrt{\alpha^{\mathtt{x}}_t}(1-\bar{\alpha}^{\mathtt{x}}_{t-1})}{1-\bar{\alpha}^{\mathtt{x}}_t}\pos_t, 
\tilde{\beta}^\mathtt{x}_t\!=\!\frac{1-\bar{\alpha}^{\mathtt{x}}_{t-1}}{1-\bar{\alpha}^{\mathtt{x}}_{t}}\beta^{\mathtt{x}}_t.\;\;\;
\end{eqnarray}
%
%\xia{Ziqi, please double check the above two equations!}
Similarly, the ground-truth categorical posterior of atom features $p(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_0)$ can be calculated~\cite{hoogeboom2021catdiff} as below,
%
\begin{eqnarray}
& p(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_0) = \mathcal{C}(\atomfeat_{t-1}|\mathbf{c}(\atomfeat_t, \atomfeat_0)), \label{eqn:gt_atomfeat_posterior_1}\\
& \mathbf{c}(\atomfeat_t, \atomfeat_0) = \tilde{\mathbf{c}}/{\sum_{k=1}^K \tilde{c}_k}, \label{eqn:gt_atomfeat_posterior_2} \\
& \tilde{\mathbf{c}} = [\alpha^{\mathtt{v}}_t\atomfeat_t + \frac{1 - \alpha^{\mathtt{v}}_t}{K}]\odot[\bar{\alpha}^{\mathtt{v}}_{t-1}\atomfeat_{0}+\frac{1-\bar{\alpha}^{\mathtt{v}}_{t-1}}{K}], 
\label{eqn:gt_atomfeat_posterior_3}
%\label{eqn:atomfeat_posterior}
\end{eqnarray}
%
%\xia{Ziqi: please double check the above equations!}
%
where $\tilde{c}_k$ denotes the likelihood of $k$-th class across $K$ classes in $\tilde{\mathbf{c}}$; 
$\odot$ denotes the element-wise product operation;
$\tilde{\mathbf{c}}$ is calculated using $\atomfeat_t$ and $\atomfeat_{0}$ and normalized into $\mathbf{c}(\atomfeat_t, \atomfeat_0)$ so as to represent
probabilities. %\xia{is this correct? is $\tilde{c}_k$ always greater than 0?}
%\xia{how is it calculated?}.
%\ziqi{the likelihood distribution $\tilde{c}$ is calculated by $p(\atomfeat_t|\atomfeat_{t-1})p(\atomfeat_{t-1}|\atomfeat_0)$, according to 
%Equation~\ref{eqn:noisetypeinter} and \ref{eqn:noisetype}.
%\xia{need to write the key idea of the above calculation...}
%
The proof of the above equations is available in Supplementary Section~\ref{supp:forward:proof}.

%===================================================================
\subsection{Variance Scheduling in \diffnoise}
\label{supp:forward:variance}
%===================================================================

Following Guan \etal~\cite{guan2023targetdiff}, we used a sigmoid $\beta$ schedule for the variance schedule $\beta_t^{\mathtt{x}}$ of atom coordinates as below,

\begin{equation}
\beta_t^{\mathtt{x}} = \text{sigmoid}(w_1(2 t / T - 1)) (w_2 - w_3) + w_3
\end{equation}
in which $w_i$($i$=1,2, or 3) are hyperparameters; $T$ is the maximum step.
%
We set $w_1=6$, $w_2=1.e-7$ and $w_3=0.01$.
%
For atom types, we used a cosine $\beta$ schedule~\cite{nichol2021} for $\beta_t^{\mathtt{v}}$ as below,

\begin{equation}
\begin{aligned}
& \bar{\alpha}_t^{\mathtt{v}} = \frac{f(t)}{f(0)}, f(t) = \cos(\frac{t/T+s}{1+s} \cdot \frac{\pi}{2})^2\\
& \beta_t^{\mathtt{v}} = 1 - \alpha_t^{\mathtt{v}} = 1 - \frac{\bar{\alpha}_t^{\mathtt{v}} }{\bar{\alpha}_{t-1}^{\mathtt{v}} }
\end{aligned}
\end{equation}
in which $s$ is a hyperparameter and set as 0.01.

As shown in Section ``Forward Diffusion Process'', the values of $\beta_t^{\mathtt{x}}$ and $\beta_t^{\mathtt{v}}$ should be 
sufficiently small to ensure the smoothness of forward diffusion process. In the meanwhile, their corresponding $\bar{\alpha}_t$
values should decrease from 1 to 0 over $t=[1,T]$.
%
Figure~\ref{fig:schedule} shows the values of $\beta_t$ and $\bar{\alpha}_t$ for atom coordinates and atom types with our hyperparameters.
%
Please note that the value of $\beta_{t}^{\mathtt{x}}$ is less than 0.1 for 990 out of 1,000 steps. %\bo{\st{, though it increases when $t$ is close to 1,000}}.
%
This guarantees the smoothness of the forward diffusion process.
%\bo{add $\beta_t^{\mathtt{x}}$ and $\beta_t^{\mathtt{v}}$ in the legend of the figure...}
%\bo{$\beta_t^{\mathtt{v}}$ does not look small when $t$ is close to 1000...}

\begin{figure}
	\begin{subfigure}[t]{.45\linewidth}
		\centering
		\includegraphics[width=.7\linewidth]{figures/var_schedule_beta.pdf}
	\end{subfigure}
	%
	\hfill
	\begin{subfigure}[t]{.45\linewidth}
		\centering
		\includegraphics[width=.7\linewidth]{figures/var_schedule_alpha.pdf}
	\end{subfigure}
	\caption{Schedule}
	\label{fig:schedule}
\end{figure}

%===================================================================
\subsection{Derivation of Forward Diffusion Process}
\label{supp:forward:proof}
%===================================================================

In \method, a Gaussian noise and a categorical noise are added to continuous atom position and discrete atom features, respectively.
%
Here, we briefly describe the derivation of posterior equations (i.e., Eq.~\ref{eqn:gt_pos_posterior_1}, and   \ref{eqn:gt_atomfeat_posterior_1}) for atom positions and atom types in our work.
%
We refer readers to Ho \etal~\cite{ho2020ddpm} and Kong \etal~\cite{kong2021diffwave} %\bo{add XXX~\etal here...} \cite{ho2020ddpm,kong2021diffwave} 	
for a detailed description of diffusion process for continuous variables and Hoogeboom \etal~\cite{hoogeboom2021catdiff} for
%\bo{add XXX~\etal here...} \cite{hoogeboom2021catdiff} for
the description of diffusion process for discrete variables.

For continuous atom positions, as shown in Kong \etal~\cite{kong2021diffwave}, according to Bayes theorem, given $q(\pos_t|\pos_{t-1})$ defined in Eq.~\ref{eqn:noiseposinter} and 
$q(\pos_t|\pos_0)$ defined in Eq.~\ref{eqn:noisepos}, the posterior $q(\pos_{t-1}|\pos_{t}, \pos_0)$ is derived as below (superscript $\mathtt{x}$ is omitted for brevity),

\begin{equation}
\begin{aligned}
& q(\pos_{t-1}|\pos_{t}, \pos_0)  = \frac{q(\pos_t|\pos_{t-1}, \pos_0)q(\pos_{t-1}|\pos_0)}{q(\pos_t|\pos_0)} \\
& =  \frac{\mathcal{N}(\pos_t|\sqrt{1-\beta_t}\pos_{t-1}, \beta_{t}\mathbf{I}) \mathcal{N}(\pos_{t-1}|\sqrt{\bar{\alpha}_{t-1}}\pos_{0}, (1-\bar{\alpha}_{t-1})\mathbf{I}) }{ \mathcal{N}(\pos_{t}|\sqrt{\bar{\alpha}_t}\pos_{0}, (1-\bar{\alpha}_t)\mathbf{I})}\\
& =  (2\pi{\beta_t})^{-\frac{3}{2}} (2\pi{(1-\bar{\alpha}_{t-1})})^{-\frac{3}{2}} (2\pi(1-\bar{\alpha}_t))^{\frac{3}{2}} \times \exp( \\
& -\frac{\|\pos_t - \sqrt{\alpha}_t\pos_{t-1}\|^2}{2\beta_t} -\frac{\|\pos_{t-1} - \sqrt{\bar{\alpha}}_{t-1}\pos_{0} \|^2}{2(1-\bar{\alpha}_{t-1})} \\
& + \frac{\|\pos_t - \sqrt{\bar{\alpha}_t}\pos_0\|^2}{2(1-\bar{\alpha}_t)}) \\
& = (2\pi\tilde{\beta}_t)^{-\frac{3}{2}} \exp(-\frac{1}{2\tilde{\beta}_t}\|\pos_{t-1}-\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\pos_0 \\
& - \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\pos_{t}\|^2) \\
& \text{where}\ \tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t.
\end{aligned}
\end{equation}
%\bo{marked part does not look right to me.}
%\bo{How to you derive from the second equation to the third one?}

Therefore, the posterior of atom positions is derived as below,

\begin{equation}
q(\pos_{t-1}|\pos_{t}, \pos_0)\!\!=\!\!\mathcal{N}(\pos_{t-1}|\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\pos_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\pos_{t}, \tilde{\beta}_t\mathbf{I}).
\end{equation}

For discrete atom features, as shown in Hoogeboom \etal~\cite{hoogeboom2021catdiff} and Guan \etal~\cite{guan2023targetdiff},
according to Bayes theorem, the posterior $q(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_0)$ is derived as below (supperscript $\mathtt{v}$ is omitted for brevity),

\begin{equation}
\begin{aligned}
& q(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_0) =  \frac{q(\atomfeat_t|\atomfeat_{t-1}, \atomfeat_0)q(\atomfeat_{t-1}|\atomfeat_0)}{\sum_{\scriptsize{\atomfeat}_{t-1}}q(\atomfeat_t|\atomfeat_{t-1}, \atomfeat_0)q(\atomfeat_{t-1}|\atomfeat_0)} \\
%& = \frac{\mathcal{C}(\atomfeat_t|(1-\beta_t)\atomfeat_{t-1} + \beta_t\frac{\mathbf{1}}{K}) \mathcal{C}(\atomfeat_{t-1}|\bar{\alpha}_{t-1}\atomfeat_0+(1-\bar{\alpha}_{t-1})\frac{\mathbf{1}}{K})} \\
\end{aligned}
\end{equation}

For $q(\atomfeat_t|\atomfeat_{t-1}, \atomfeat_0)$, we have % $\atomfeat_t=\atomfeat_{t-1}$ with probability $1-\beta_t+\beta_t / K$, and $\atomfeat_t \neq \atomfeat_{t-1}$
%with probability $\beta_t / K$.
%
%Therefore, this function can be symmetric, that is, 
%
\begin{equation}
\begin{aligned}
q(\atomfeat_t|\atomfeat_{t-1}, \atomfeat_0) & = \mathcal{C}(\atomfeat_t|(1-\beta_t)\atomfeat_{t-1} + \beta_t/{K})\\
& = \begin{cases}
1-\beta_t+\beta_t/K,\!&\text{when}\ \atomfeat_{t} = \atomfeat_{t-1},\\
\beta_t / K,\! &\text{when}\ \atomfeat_{t} \neq \atomfeat_{t-1},
\end{cases}\\
& = \mathcal{C}(\atomfeat_{t-1}|(1-\beta_t)\atomfeat_{t} + \beta_t/{K}).
\end{aligned}
%\mathcal{C}(\atomfeat_{t-1}|(1-\beta_{t})\atomfeat_{t} + \beta_t\frac{\mathbf{1}}{K}).
\end{equation}
%
Therefore, we have
%\bo{why it can be symmetric}
%
\begin{equation}
\begin{aligned}
& q(\atomfeat_t|\atomfeat_{t-1}, \atomfeat_0)q(\atomfeat_{t-1}|\atomfeat_0) \\
& = \mathcal{C}(\atomfeat_{t-1}|(1-\beta_t)\atomfeat_{t} + \beta_t\frac{\mathbf{1}}{K}) \mathcal{C}(\atomfeat_{t-1}|\bar{\alpha}_{t-1}\atomfeat_0+(1-\bar{\alpha}_{t-1})\frac{\mathbf{1}}{K}) \\
& = [\alpha_t\atomfeat_t + \frac{1 - \alpha_t}{K}]\odot[\bar{\alpha}_{t-1}\atomfeat_{0}+\frac{1-\bar{\alpha}_{t-1}}{K}].
\end{aligned}
\end{equation}
%
%\bo{what is $\tilde{\mathbf{c}}$...}
Therefore, with $q(\atomfeat_t|\atomfeat_{t-1}, \atomfeat_0)q(\atomfeat_{t-1}|\atomfeat_0) = \tilde{\mathbf{c}}$, the posterior is as below,

\begin{equation}
q(\atomfeat_{t-1}|\atomfeat_{t}, \atomfeat_0) = \mathcal{C}(\atomfeat_{t-1}|\mathbf{c}(\atomfeat_t, \atomfeat_0)) = \frac{\tilde{\mathbf{c}}}{\sum_{k}^K\tilde{c}_k}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{{Backward Generative Process} (\diffgenerative)}
\label{supp:backward}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Following Ho \etal~\cite{ho2020ddpm}, with $\tilde{\pos}_{0,t}$, the probability of $\pos_{t-1}$ denoised from $\pos_t$, denoted as $p(\pos_{t-1}|\pos_t)$,
can be estimated %\hl{parameterized} \xia{???} 
by the approximated posterior $p_{\boldsymbol{\Theta}}(\pos_{t-1}|\pos_t, \tilde{\pos}_{0,t})$ as below,
%
\begin{equation}
\begin{aligned}
p(\pos_{t-1}|\pos_t) & \approx p_{\boldsymbol{\Theta}}(\pos_{t-1}|\pos_t, \tilde{\pos}_{0,t}) \\
& = \mathcal{N}(\pos_{t-1}|\mu_{\boldsymbol{\Theta}}(\pos_t, \tilde{\pos}_{0,t}),\tilde{\beta}_t^{\mathtt{x}}\mathbb{I}),
\end{aligned}
\label{eqn:aprox_pos_posterior}
\end{equation}
%
where ${\boldsymbol{\Theta}}$ is the learnable parameter; $\mu_{\boldsymbol{\Theta}}(\pos_t, \tilde{\pos}_{0,t})$ is an estimate %estimation
of $\mu(\pos_t, \pos_{0})$ by replacing $\pos_0$ with its estimate $\tilde{\pos}_{0,t}$ 
in Equation~{\ref{eqn:gt_pos_posterior_1}}.
%
Similarly, with $\tilde{\atomfeat}_{0,t}$, the probability of $\atomfeat_{t-1}$ denoised from $\atomfeat_t$, denoted as $p(\atomfeat_{t-1}|\atomfeat_t)$, 
can be estimated %\hl{parameterized} 
by the approximated posterior $p_{\boldsymbol{\Theta}}(\atomfeat_{t-1}|\atomfeat_t, \tilde{\atomfeat}_{0,t})$ as below,
%
\begin{equation}
\begin{aligned}
p(\atomfeat_{t-1}|\atomfeat_t)\approx p_{\boldsymbol{\Theta}}(\atomfeat_{t-1}|\atomfeat_{t}, \tilde{\atomfeat}_{0,t}) 
=\mathcal{C}(\atomfeat_{t-1}|\mathbf{c}_{\boldsymbol{\Theta}}(\atomfeat_t, \tilde{\atomfeat}_{0,t})),\!\!\!\!
\end{aligned}
\label{eqn:aprox_atomfeat_posterior}
\end{equation}
%
where $\mathbf{c}_{\boldsymbol{\Theta}}(\atomfeat_t, \tilde{\atomfeat}_{0,t})$ is an estimate of $\mathbf{c}(\atomfeat_t, \atomfeat_0)$
by replacing $\atomfeat_0$  
with its estimate $\tilde{\atomfeat}_{0,t}$ in Equation~\ref{eqn:gt_atomfeat_posterior_1}.



%===================================================================
\section{\method Loss Function Derivation}
\label{supp:training:loss}
%===================================================================

In this section, we demonstrate that a step weight $w_t^{\mathtt{x}}$ based on the signal-to-noise ratio $\lambda_t$ should be 
included into the atom position loss (Eq.~\ref{eqn:diff:obj:pos}).
%
In the diffusion process for continuous variables, the optimization problem is defined 
as below~\cite{ho2020ddpm},
%
\begin{equation*}
\begin{aligned}
& \arg\min_{\boldsymbol{\Theta}}KL(q(\pos_{t-1}|\pos_t, \pos_0)|p_{\boldsymbol{\Theta}}(\pos_{t-1}|\pos_t, \tilde{\pos}_{0,t})) \\
& = \arg\min_{\boldsymbol{\Theta}} \frac{\bar{\alpha}_{t-1}(1-\alpha_t)}{2(1-\bar{\alpha}_{t-1})(1-\bar{\alpha}_{t})}\|\tilde{\pos}_{0, t}-\pos_0\|^2 \\
& = \arg\min_{\boldsymbol{\Theta}} \frac{1-\alpha_t}{2(1-\bar{\alpha}_{t-1})\alpha_{t}} \|\tilde{\boldsymbol{\epsilon}}_{0,t}-\boldsymbol{\epsilon}_0\|^2,
\end{aligned}
\end{equation*}
where $\boldsymbol{\epsilon}_0 = \frac{\pos_t - \sqrt{\bar{\alpha}_t}\pos_0}{\sqrt{1-\bar{\alpha}_t}}$ is the ground-truth noise variable sampled from $\mathcal{N}(\mathbf{0}, \mathbf{1})$ and is used to sample $\pos_t$ from $\mathcal{N}(\pos_t|\sqrt{\cumalpha_t}\pos_0, (1-\cumalpha_t)\mathbf{I})$ in Eq.~\ref{eqn:noisetype};
$\tilde{\boldsymbol{\epsilon}}_0 = \frac{\pos_t - \sqrt{\bar{\alpha}_t}\tilde{\pos}_{0, t}}{\sqrt{1-\bar{\alpha}_t}}$ is the predicted noise variable. 

%A simplified training objective is proposed by Ho \etal~\cite{ho2020ddpm} as below,
Ho \etal~\cite{ho2020ddpm} further simplified the above objective as below and
demonstrated that the simplified one can achieve better performance:
%
\begin{equation}
\begin{aligned}
& \arg\min_{\boldsymbol{\Theta}} \|\tilde{\boldsymbol{\epsilon}}_{0,t}-\boldsymbol{\epsilon}_0\|^2 \\
& = \arg\min_{\boldsymbol{\Theta}} \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}\|\tilde{\pos}_{0,t}-\pos_0\|^2,
\end{aligned}
\label{eqn:supp:losspos}
\end{equation}
%
where $\lambda_t=\frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}$ is the signal-to-noise ratio.
%
While previous work~\cite{guan2023targetdiff} applies uniform step weights across
different steps, we demonstrate that a step weight should be included into the atom position loss according to Eq.~\ref{eqn:supp:losspos}.
%
However, the value of $\lambda_t$ could be very large when $\bar{\alpha}_t$ is close to 1 as $t$ approaches 1.
%
Therefore, we clip the value of $\lambda_t$ with threshold $\delta$ in Eq.~\ref{eqn:diff:obj:pos}.


