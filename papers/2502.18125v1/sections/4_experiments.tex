\section{Experiments}
To validate the effectiveness of \name, we have conduct extensive experiments to answer the following research questions.
\begin{itemize}
    \item \textbf{RQ1}: 
    How does the proposed \name perform compared to state-of-the-art (SoTA) methods when using various LLMs as backbones across different downstream tasks?
    \item \textbf{RQ2}: Is \name scalable to tables of different sizes?
    \item \textbf{RQ3}: How does the proposed \name retain the \textit{Order Invariance} of structural relationships?
    \item \textbf{RQ4}: How does \name retain the \textit{Semantic Consistency} and \textit{Hierarchical Dependencies} of structural relationships?
    \item \textbf{RQ5}: How do the different components of \name contribute to to improving the performance of LLMs in learning from structured knowledge?
    % \item \textbf{RQ3}: How does \name address the complex structural relationships within structured knowledge?
    % \item \textbf{RQ4}: How does the proposed \name integrate structured knowledge with LLMs across different training strategies?

\end{itemize}
\subsection{Experimental Setups}
\subsubsection{Tasks} We validate our proposed \name on two levels of downstream tasks that require fact-checking, and reasoning based on structured knowledge stored in tables.
\begin{itemize}[leftmargin=*]
    % \item \textbf{Table-to-Text (ToT)} To evaluate the structured knowledge understanding ability of LLMs enhanced with our proposed \name, we prompt the LLMs to generate one-sentence descriptions of highlighted sections in tables from the ToTTo~\cite{parikh2020totto} benchmark. The benchmark includes 120,000 training examples, each consisting of a Wikipedia table with a set of highlighted table cells.
    \item \textbf{Table Fact Verification (TFV).} This task aims at assessing the effectiveness of \name in fact-checking over structured knowledge. Specifically, we conduct experiments on the TabFact~\cite{2019TabFactA} benchmark, which contains 16k Wikipedia tables used as evidence for 118k human-annotated claims to explore fact verification with semi-structured knowledge. The ground truth answer for TFV tasks is either ``yes'' or ``no'', signifying whether the given claim is supported or contradicted by the structured knowledge stored in the corresponding table, respectively. 
    \item \textbf{Table Question Answering (TQA).} To validate \name is able to facilitate LLMs to reason over structured knowledge and provide better answers to user input questions, we test on the WiKiTableQuestions~\cite{pasupat2015compositional} dataset, which includes 14,152 examples of open question-answer pairs for training and 4,344 examples for testing. The expected responses for TQA tasks are open-ended answers, which can be in the form of sentences or phrases.
\end{itemize}

\input{tables/dataset_statistics}
In particular, we follow the preprocessing steps in ~\cite{yin2020tabert} to prepare the training data, and the statistics for our final training data are listed in the table \ref{tab:datasets_stats}. For testing, we retain the original test sets of the two datasets, TabFact~\cite{2019TabFactA} and WikiTableQuestions~\cite{pasupat2015compositional}, to ensure fair comparison with the selected baselines.

\subsubsection{Baselines} We compare our proposed \name against 12 baseline methods, categorized by their different ways of handling tables: operation-based methods~\cite{rajkumar2022evaluating,dater,wang2024chainoftable} that use external operations like SQL queries and serialization-based methods~\cite{zhang2024tablellama,touvron2024llama3,gemma} that transform information in structures into sequences then prompt into the LLMs. In terms parameter sizes, our comparison covers a range of model sizes range from 2 billion to 70 billion parameters. For a fair comparison, we evaluate the operation-based baseline methods~\cite{rajkumar2022evaluating,dater,wang2024chainoftable} using the same backbone LLMs (\ie{\textbf{LLaMA3-8B-Instruct}, \textbf{LLaMA3.2-3B-Instruct}, \textbf{Gemma-2-9B-It}, and \textbf{Gemma-2-2B-It}) as those used for our \name. To reduce the impact of varying instruction-following abilities among different LLMs, we adopt the instruction-tuned verions of all the selected backbone LLMs in our experiments. %First, we select two \textbf{\textit{traditional methods}} fine-tuned on language models.
% \begin{itemize}
%     \item \textbf{TAPAS-Large} ~\cite{herzig2020tapas} encodes both the table structure and the question together to enhance BERT. 
%     \item \textbf{TAPEX-Large} ~\cite{liu2022tapex} uses transformer to generate operations to retrieve answers from tables.
% \end{itemize}
% For both the TAPAS~\cite{herzig2020tapas} and the TAPEX~\cite{liu2022tapex}, we use the versions of them fine-tuned on TabFact~\cite{2019TabFactA} and WikiTableQuestions~\cite{pasupat2015compositional} in the two tasks TFV and TQA, respectively. 
%Specifically, we select four LLM accross different parameter scales, ranging from 2 billion to 27 billion. In particular, we apply our \name to three of these LLMs to evaluate its generalization capability.

\input{tables/task_performance}

\begin{itemize}[leftmargin=*]
    \item \textbf{GPT-3.5-Turbo}~\cite{openai2023gpt35turbo} is an advanced language model from the GPT-3 family by OpenAI, distinguished by its exceptional balance between cost-efficiency and performance, offering faster inference and lower deployment costs.
    \item \textbf{GPT-4o-mini}~\cite{gpt4ominiurl} is a lightweight variant of GPT-4 by OpenAI, offering strong performance with lower computational demands, ideal for resource-constrained applications.
    \item \textbf{LLaMA3.1-70B-Instruct} ~\cite{touvron2024llama3} is a super large model with 70 billion parameters, offering improved performance on more complicated and long-context reasoning.
    \item \textbf{LLaMA3-8B-Instruct} ~\cite{touvron2024llama3} is an instruction-tuned model of the LLaMA3 series with 8 billion parameters, optimized for better instruction understanding and generation. 
    \item \textbf{LLaMA3.2-3B-Instruct} ~\cite{touvron2024llama3} is a relatively small model in the LLaMA3.2 series, fine-tuned with 3 billion parameters, and specifically designed for those tasks requiring rapid responses under limited computational resources.
    \item \textbf{Gemma-2-It} ~\cite{gemma} is fine-tuned on Gemma-2 with user interactions, focusing on task-specific adaptability while ensuring efficiency through knowledge distillation from the very large model. In this paper, we use the 2B, 9B, 27B variants of Gemma-2-It.
    \item \textbf{TableLlama} ~\cite{zhang2024tablellama} adopts LongLoRA to finetune on a dataset that includes a diverse range of serialized tables and the corresponding natural language task instructions.
    \item \textbf{Text-to-SQL}~\cite{rajkumar2022evaluating} designs in-context samples to instruct LLMs in generating SQL queries for answering questions. 
    \item \textbf{Dater} ~\cite{dater} leverages LLMs to decompose the task into multiple sub-tasks, utilizing SQL queries to address each sub-task.
    \item \textbf{CHAIN-OF-TABLE} ~\cite{wang2024chainoftable} prompts LLMs through in-context learning to iteratively produce operations and update the table, thereby constructing a reasoning chain in a structured format.
    \item \textbf{LoRA}~\cite{hu2022lora} is a widely used technique for efficiently fine-tuning LLMs by updating a small number of low-rank weights.  
\end{itemize}

\subsubsection{Evaluation Protocol} We evaluate the generation of LLMs enhanced by our proposed \name framework with respect to the different tasks. For the TFV task, where the answers are either ``yes'' or ``no'', we employ \textbf{accuracy}, \textbf{precision}, \textbf{recall}, and \textbf{F1 score} as the evaluation metrics. To mitigate the impact of option bias~\cite{pezeshkpour-hruschka-2024-large,zheng2024large} in LLMs, we use a weighted version of all these metrics. For the TQA task, where responses may take the form of sentences or phrases, we adopt the following natural language evaluating metrics.
\begin{itemize}[leftmargin=*]
    \item \textbf{Denotation Accuracy (Denot. Acc.)}~\cite{deacc}, following ~\cite{jiang-etal-2023-structgpt,wang2024chainoftable}, measures how closely a response matches the ground truth answer, regardless of the order of phrases in the answers.
    % \item \textbf{Exact Match (EM)} measures the percentage of correct responses that exactly match the ground truth answers.
    \item \textbf{ROUGE-N} measures the similarity between the LLM-generated responses and the ground truth answers by comparing overlapping n-grams, used to evaluate text summaries or translations by quantifying shared word sequences. In this paper, we report both ROUGE-1 and ROUGE-2 scores.
    
    \item \textbf{ROUGE-L} evaluates the similarity between the LLM-generated responses and the ground truth answers by identifying the longest common subsequence (LCS) and is used to assess the fluency and coherence of the generated text.
    
    % \item \textbf{BLEU} evaluates the quality of LLM-generated responses by comparing the n-grams in the output to those in the ground truth answers. It computes precision for n-grams and includes a brevity penalty to discourage excessively short responses, making it a widely used metric in machine translation tasks.

\end{itemize}

\subsubsection{Implementations Details} For \name, we explore the learning rates for the LoRA module within the range of \{5e-5, 1e-5, 5e-6\}, while applying scaling factors for the learning rate in the PHL module (\ie{the novel hypergraph neural networks}) and the projector from \{1, 10, 20\}. We search batch sizes from \{8, 16, 32\}, and conduct experiments over 1 to 4 epochs, utilizing an early stopping strategy. Specifically, for the LoRA module, we fine-tune the Query, Key, and Value projectors with a rank of 8, a LoRA alpha of 32, and a dropout rate of 0.1. For the selected baseline models, we adopt the optimal configurations from the HuggingFace\footnote{https://huggingface.co/models} and accelerate inference with vllm 0.5.4\footnote{https://github.com/vllm-project/vllm}. All experiments in this paper are conducted on two NVIDIA A800-SXM4-80GB GPUs. For further details, please refer to our publicly released code linked in the Abstract Section.

\subsection{Task Performance (RQ1)}
Table \ref{tab:main} presents a comparison of the performance of our \name with 13 baseline methods, encompassing both serialization-based methods~\cite{openai2023gpt35turbo,gpt4ominiurl,zhang2024tablellama,gemma,touvron2024llama3,hu2022lora} and operation-based methods~\cite{rajkumar2022evaluating,dater,wang2024chainoftable}. We evaluate their capabilities in structured knowledge using the TFV task on the TabFact dataset~\cite{2019TabFactA} and the TQA task on the WiKiTableQuestion dataset~\cite{pasupat2015compositional}. In Table \ref{tab:main}, the first group consists of serialized-based methods utilizing various LLMs, while the last four groups compare the performance of our \name with both state-of-the-art operation-based and serialized-based methods across four backbone LLMs. The following observations can be drawn from the performance results in Table \ref{tab:main}.
%Table \ref{tab:main} presents the performance of our \name and various LLMs, encompassing both serialization-based ~\cite{openai2023gpt35turbo,gpt4ominiurl,zhang2024tablellama,gemma,touvron2024llama3} and operation-based~\cite{rajkumar2022evaluating,dater,wang2024chainoftable} models, which are tested on the TFV task with the Tabfact~\cite{2019TabFactA} dataset and the TQA task utilizing the WiKiTableQuestion~\cite{pasupat2015compositional} dataset. Moreover, we apply our \name to four commonly used LLMs, with parameter sizes ranging from 2 billion to 9 billion, to evaluate its generalization capabilities. The following observations can be drawn from the performance results in Table \ref{tab:main}. 

\begin{itemize}[leftmargin=*]
\item \textbf{Our \name outperforms both the operation-based and serialization-based methods based on LLMs.} It can be found in Table \ref{tab:main} that our \name consistently achieve competing performances across both the TFV and TQA tasks. In general, operations-based methods~\cite{rajkumar2022evaluating,dater,wang2024chainoftable} achieves better outperform the methods~\cite{openai2023gpt35turbo,gpt4ominiurl,zhang2024tablellama,gemma,touvron2024llama3,hu2022lora} of merely prompting LLMs with serialized information, even when the models have been fine-tuned on structural data~\cite{zhang2024tablellama}. This highlights the importance of maintaining structures when reasoning about questions related to structured data. Our proposed \name utilizes hypergraphs to encode structural information, complementing the powerful natural language capabilities of LLMs. It demonstrates an average improvements of 1.73\% and 2.43\% in accuracy on TFV and TQA, respectively, when compared to the second-best performances in each group. Upon reviewing the response examples, we found that CHAIN-OF-TABLE~\cite{wang2024chainoftable} encounters difficulties in TQA due to the loss of the question while reasoning over extended chains.
    \item \textbf{Our \name narrows the performance gap between large and small LLMs, requiring only a modest number of additional parameters.} Table \ref{tab:main} shows that instruction-tuned LLMs~\cite{gemma,touvron2024llama3,zhang2024tablellama} with larger parameter sizes achieve better performance with serialization compared to their smaller counterparts. For instance, LLaMA-3.1-70B-Instruct achieves an accuracy of 79.16\%, whereas LLaMA-3-8B-Instruct attains only 66.29\%. Nevertheless, our \name intergrated with LLaMA-3-8B-Instruct, which adds approximately \textbf{189M} parameters (roughly one-tenth of the parameter difference between LLaMA-3-8B-Instruct and LLaMA-3.1-70B-Instruct),  achieves performance comparable to the larger model across both tasks. \name provides average improved accuracy of 6.22\% and 15.72\% on the four backbone models regarding the two tasks, respectively. Similarly, \name based on the Gemma-2-9B-It surpasses its 27B variant by 2.64\% and 2.58\% in the TFV and TQA tasks with respect to accuracy, respectively. This superiority is generalizable from the TQA task to the TFV task, and is attributed to \name's ability in encoding enriched structured knowledge, enabling LLMs to produce more accurate answers. Additionally, the results in Table \ref{tab:main} further demonstrate that our \name enhances LLMs across parameters sizes ranging from 2B to 9B.

\item \textbf{Our \name delivers improvements in performance and training efficiency compared to other SFT methods.} As Supervised Fine-Tuning (SFT) is required in our \name, we compare it to the other two SFT baseline methods: LoRA~\cite{hu2022lora} and TableLlama~\cite{zhang2024tablellama}. First, \name demonstrates significant improvements, achieving an average enhancement of 6.13\% in accuracy and 15.22\% in denotation accuracy~\cite{deacc} over LoRA for the TFV and TQA tasks, respectively.  While LoRA is widely recognized as an efficient tool for instruction tuning, it proves less effective when applied to smaller-scale LLMs, such as Gemma-2-2B-It, particularly for reasoning over serialized structured data. This limitation highlights the challenges of adapting LoRA to tasks requiring nuanced structural understanding. Furthermore, when compared to TableLlama~\cite{zhang2024tablellama}, which is fine-tuned on a benchmark involving serialized structured knowledge, \name provides a more efficient solution by fine-tuning on 189M additional parameters with very limited training data (see Table \ref{tab:datasets_stats}). These observations reinforce our earlier assertion that serialization-based methods can undermine the preservation of structures, further highlighting the necessity of \name in enhancing LLMs to fully utilize such knowledge for improved reasoning.
   
    % \item \textcolor{red}{\textbf{Our \name delivers more substantial enhancements to larger LLMs with more parameters than to smaller ones.}} The last four parts in Table \ref{tab:main} demonstrates the performance of our proposed \name applied on various LLMs, with parameters ranging from 2B to 9B. It can be found that the information of structured knowledge encoded by the PHL module (\ie{the proposed hypergraph neural network}) in \name provides more enhancements on the larger LLMs compared to their small-scale variants. For example, \name achieves a 10.91\% improvement in accuracy using the LLaMA3-8B-Instruct as the base model, compared to a 7.05\% improvement in accuracy on the Gemma-2-9B-It in the TFV task. We attribute the observed attenuation to the small-scale model being distilled from the larger one, which results in more rigid parameters and complicates the integration of structured knowledge.

\end{itemize}


\subsection{Order Invariance (RQ2)}
\begin{figure}[t!]
    \centering
  \begin{minipage}[t]{0.49\linewidth} %
    \subfigure[Shuffle Rows]{
        % \label{fig:barchart1}
        \includegraphics[width=\linewidth]{figures/line_rows.pdf}
    }  
  \end{minipage}
  \begin{minipage}[t]{0.49\linewidth} %
    \subfigure[Shuffle Columns]{
        % \label{fig:barchart2}
        \includegraphics[width=\linewidth]{figures/line_cols.pdf}
    }  
  \end{minipage}
\vspace{-0.1in}
\caption{Performances of \name under different variances of order simulated by shuffling.}
\vspace{-0.2in}
\label{fig:order}
\end{figure}
In contrast to natural language, where changes in word order can modify the meaning of a sentence, rearranging rows or columns in a table does not affect its meaning. In \name, this invariance is handled with hyperedges, which represent rows and columns, are inherently unordered within the structure of hypergraphs. To assess how our proposed \name framework helps the LLM maintain the \textit{Order Invariance} of structural relationships, we shuffle the rows in the test data to evaluate \name's robustness to order variations. 

Specifically, we randomly sampled a subset of tables from the TFV testing set and performed shuffling of the rows and columns respectively within each sampled table to introduce variability and evaluate the performance of our proposed \name. Figure \ref{fig:order} displays the performances of \name which uses LLaMA3-8B-Instruct as the backbone model, across different shuffle ratios. The x-axis represents the sampling ratio, while the y-axis indicates performance scores with respect to accuracy, precision, and F1 score. As decipted in Figure \ref{fig:order}, \name framework demonstrates stable performance despite variations in row and column order. The accuracy variance of 0.0109 for row shuffling and 0.00043 for columns shuffling, respectively. This stability underscores the robustness of \name in maintaining structural representation integrity from the perspective of order invariance, thereby validating our previously stated rationale for employing hypergraphs.

\subsection{Semantic Consistency and Hierarchical Dependencies (RQ3)}
\begin{figure}[t!]
    \centering
  \begin{minipage}[t]{\linewidth} %
    \subfigure[\texttt{Case 1}. Information about the baseball teams at Bosse Field.]{
        \includegraphics[width=\linewidth]{figures/case2.pdf}
    }  
  \end{minipage}\\
  \begin{minipage}[t]{\linewidth} %
    \subfigure[\texttt{Case 2.} The results of the 1961 Victorian Football League (VFL).]{
        \includegraphics[width=\linewidth]{figures/case_study.pdf}
    }  
  \end{minipage}
\vspace{-0.1in}
\caption{Visualization of the weights between cell nodes and different hyperedges in two random cases.}
\label{fig:case_study}
\end{figure}
Beyond quantitative metrics, we also conduct qualitative evaluations to investigate whether \name retains semantic consistency and hierarchical dependencies of structural relationships during reasoning. In Figure \ref{fig:case_study}, we randomly selected two cases and visualized the attention weights between each cell node and the hyperedges associated with the claim's content. This visualization provides insights into how \name prioritizes and propagates information between table elements and their relevance to the given queries/claims, specifically demonstrating its ability to maintain semantic consistency and hierarchical dependencies.

The semantic consistency in structural realtionships suggests that cells in the same column are similar in semantics. In \texttt{Case 1}, the claim pretains to the class of a team named ``evansville triplets''. \name first augmented each row with easy-to-understand natural language descriptions based on the row context, as shown in the bottom right of Figure \ref{fig:case_study}. This augmentation enables \name to better interpret cells with specific missing values (\eg{``none'' in the ``Class'' columns}), leading to similar weights for these cells and their counterparts within the same column. In \texttt{Case 2}, the claim queries about the highest crowd participation, which requires examining the ``Crowd'' column to identify the largest number. Though the crowd numbers for these teams vary, the weights assigned to the column hyperedges are similar thanks to the augmented column descriptions (omitted here) and the Semantics Hypergraph Construction (SHC) in \name. This is more evident in the weight matrix associated with the table hyperedge shown in the right bottom of \ref{fig:case_study} (b). Even though the venue names are quite different, the cells in the "Venue" column share similar weights.

The Hierarchical Dependencies refers to the hierarchy across cells, columns, rows, and the whole table. As depicted in Figure \ref{fig:case_study}, the attention of \name is primarily focused on the cells and the rows/columns related to the claim. This focus extends from cell nodes to row/colum hyperedges, and then the table hyperedges, gradually diminishing in intensity. For example, in Figure \ref{fig:case_study} (b), the weights assigned to the queried cell ``Junction oval'', the evidence cell ``33100'', and the relevant column ``Crowd'' exceed the average weights of 0.27 and 0.16 in the weight matrices corresponding to the column hyperedges and the table hyperedge, respectively. This demonstrates how the attention mechanism spans the hierarchical structure, emphasizing specific elements within the table.

%To evaluate how our proposed \name addresses the knowledge sparsity issue and complex structural relationships, we perform a case study on the representation capability of our proposed hypergraph neural networks, \ie{the PHL module in \name}, using LLaMA3-8B-Instruct as the base model. As illustrated in Figure \ref{fig:case2}, we randomly select a table and visualize the augmented caption and descriptions for rows with sparse knowledge, alongside the attention weights for connections between cell nodes and their corresponding row hyperedges. For clarity, some rows from the original table have been omitted; however, to ensure a fair comparison, we calculated the average attention weight, which is 0.27, within this table-constructed semantic hypergraph.

%For the knowledge sparsity issue, \name leverages the generation capability of LLMs to conduct contextual augmentation. For example, the original caption ``\textit{bosse field}'' is augmented to be more comprehensive as: \textit{Overview of Evansville baseball teams at Bosse Field}, providing the models with a clearer summary of the table content. Additionally, \name augments the sparse knowledge in rows and columns using natural language descriptions. For example, as illustrated in Figure \ref{fig:case2}, the cell ``none'' under the \textit{Championships} column in the third row for ``evansville white sox''is explicitly interpreted as no championships won. 

%Our \name also effectively captures the complex structural relationships using hypergraphs. To verify the claim regarding the class of the Evansville Triplets team, it is essential for LLMs to intuitively focus on the third row, particularly the \textit{Class} column, as highlighted in blue in the table. The attention weights between each cell node and their respective row hyperedges are visualized through varying color shades in Figure \ref{fig:case2}. It is evident that \name, specifically the PHL module, concentrates on the highlighted row, assigning an attention weight of 0.35 to the cell indicating the ground truth cell of class ``aaa'' for the Evansville Triplets team. Additionally, \name also place emphasis on the \textit{class} cell containing ``aa'', which is semantically similar to the cell ``aaa''. 
%\vspace{-0.1in}
\subsection{Ablation Study (RQ4)}
\input{tables/ablation_study}

We are also curious about the contribution of each component in \name contributes to the enhancements of \name. 
As shown in Table \ref{tab:ablation_study}, we successively removed the proposed prompt-attentive hypergraph learning (PHL) module, substituted the PHL module with HGNN~\cite{hgnn}, and removed the LLM-based argumentation. Note that this ablation study is conducted under hyperparameters setting different from those used for the results in Table \ref{tab:main}.

It can be observed from the experimental results in Table \ref{tab:ablation_study} that the original framework of our proposed \name delivers the best performance on verifying the factual knowledge stored in structured data. Firstly, removing the PHL module and directing the semantic embeddings directly to the projector results in a 6.24\% reduction in accuracy. Furthermore, to examine the role of hypergraph neural networks in enhancing LLMs' comprehension of structured knowledge, we replace our proposed PHL module with the classical HGNN~\cite{hgnn}, leading to a 4.50\% decrease in performance compared to \name, as shown in the third row of Table \ref{tab:ablation_study}. This performance degradation highlights the effectiveness of hypergraphs in representing structured knowledge. Specifically, We attribute this decline to the inability of HGNN to adequately leverage the information encoded in hyperedges for node updates during propagation. Additionally, we explore the impact of incorporating the inquiry embedding in the PHL module. As demonstrated in the last row of Table \ref{tab:ablation_study}, removing the inquiry embedding causes a substantial 5.76\% drop in precision and a more moderate 2.98\% decline in recall. This suggests that incorporating inquiry embeddings helps LLMs mitigate the bias toward over-generating positive responses, fostering more cautious reasoning by integrating the essential inquiry within prompts when processing structured data.

\vspace{-0.05in}
\subsection{Scalability (RQ5)}
\begin{figure}[t!]
    \centering
  \begin{minipage}[t]{0.49\linewidth} %
    \subfigure[Precision]{
        \includegraphics[width=\linewidth]{figures/prec_scalability.pdf}
    }  
  \end{minipage}
  \begin{minipage}[t]{0.49\linewidth} %
    \subfigure[Recall]{
        \includegraphics[width=\linewidth]{figures/recall_scalability.pdf}
    }  
  \end{minipage}
\vspace{-0.1in}
\caption{Performances of \name on tables of different sizes.}
\vspace{-0.2in}
\label{fig:scalability}
\end{figure}
Scalability is a critical concern, as large tables pose significant challenges for LLMs, which often struggle to interpret and integrate context from lengthy prompts~\cite{liu-etal-2024-lost,10.1145/3539618.3591708}. To evaluate the performances of \name on tables of varying sizes, we divided the testing tables in \cite{2019TabFactA} into three classes: \textbf{small} (\#rows $\leq$ 5 and \#columns$\leq$ 5), \textbf{medium} (6$\leq$\#rows$\leq$10 and 6$\leq$\#columns$\leq$ 10), and \textbf{large} (\#rows$\geq$ 10 and \#columns$\geq$ 10). We compare the performance of LLaMA3-8B-Instruct~\cite{touvron2024llama3}, Dater~\cite{dater}, and \name, with LLaMA3-8B-Instruct serving as the backbone model for all.

As illustrated in Figure \ref{fig:scalability}, with the table sizes increases there are generally declines in all of the models. \name demonstrates relatively stable performance in precision with a variance of 8.84, surpasses LLaMA3-8B-Instruct of 10.87. For small tables, \name surpasses Dater in precision and recall by 6.39\% and 7.42\%, respectively. However, Dater demonstrates superior recall performance compared to \name for medium and large tables. Upon careful examination and analysis of the true positive cases, we found that this is primarily due to the LLMs in Dater being inclined to generate positive answers. This is also benefits from its effective approach of decomposing queries and tables into sub- questions and sub-tables. The declines of \name in larger tables are primarily observed in recall. We attribute this to the limited number of hypergraph learning layers and aim to address this issue in the future through more sophisticated graph techniques.

%\subsection{Different Training Strategies (RQ6)}
%\noindent
%To conduct a fine-grained investigation of how \name integrates structured knowledge with LLMs, we employ various training strategies and assess its performance using LLaMA3-8B-Instruct and LLaMA3.2-3B-Instruct as base models. We specifically designed four distinct training approaches, as outlined in Table \ref{tab:training}, with the joint training phase moving from the final to the initial step. Note that for the PHL pretraining, we apply the ELECTRA loss~\cite{clark2019electra}, which randomly masks certain cells in the table and guides the model to classify whether the target cell has been masked or not. The experimental results, demonstrated in Figure \ref{fig:training_strategies}, indicate that the PHL module needs to be trained jointly with LoRA to achieve optimal performance. Excessive pretraining leads \name to converge towards a suboptimal solution, resulting in diminished performance. This discrepancy may be due to the selection of the ELECTRA objective, which encourages the model to focus on partial information within the table, neglecting a broader global understanding. Moreover, the issue of over-pretraining has a more pronounced negative impact on smaller-scale LLMs, as depicted in Figure \ref{fig:barchart2}. Nevertheless, with the PHL module frozen, our \name outperforms LoRA by 1.46\% on LLaMA-8B-Instruct, underscoring the significance of the structured knowledge representation acquired. Apart from that, \name also shows a modest improvement (1.59\%) over the base setting (\ie LLaMA3.2-3B-Instruct+LoRA) when using the pretrained PHL module (S3). Future work could focus on designing better pretraining objectives to enhance the integration of hypergraph neural networks with different LLMs, thereby improving their capabilities of understanding and reasoning over structured knowledge. 

% \input{tables/training}
% \begin{figure}[t!]
%     \centering
%   \begin{minipage}[t]{0.49\linewidth} %
%     \subfigure[LLaMA3-8B-Instruct]{
%         % \label{fig:barchart1}
%         \includegraphics[width=\linewidth]{figures/bar_chart1.pdf}
%     }  
%   \end{minipage}
%   \begin{minipage}[t]{0.49\linewidth} %
%     \subfigure[LLaMA3.2-3B-Instruct]{
%         % \label{fig:barchart2}
%         \includegraphics[width=\linewidth]{figures/bar_chart2.pdf}
%     }  
%   \end{minipage}
% % \vspace{-0.2in}
% \caption{Performances under different training strategies}
% % \vspace{-0.2in}
% \label{fig:training_strategies}
% \end{figure}


