\section{Methodology}
Figure \ref{fig:method} provides an overview of our proposed \name framework, which is designed to enhance the ability of LLMs to handle tasks that require knowledge embedded in structured data.  This section details the workflow of \name, first augmenting the structured data with contextual information, followed by learning and integrating task-relevant structured knowledge into the LLMs to generate answers.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/method.pdf}
    % \vspace{-0.2in}
    \caption{An overview of our proprosed \name framework.}
    \label{fig:method}
\end{figure*}

\subsection{Contextual Augmentation}
To address the \textbf{data sparsity} issue caused by missing or incomplete information in cells, \ie{N/A, none, or incompleted descriptions such as the example in Figure \ref{fig:toy_example}}. For each table $\mathcal{T}$, we augment its caption $o$, columns $c\in\mathcal{C}$, and rows $r\in\mathcal{R}$ with contextual information, leveraging the semantics understanding and generative ability of the large language model $LLM(\cdot)$. 

Specifically, as shown in Figure \ref{fig:method}, the caption $o$ ``\textit{United States House of Representatives Elections, 1972}'' is vague, as it does not specify where the elections occurred. After being supplemented with the contextual information from the table, the augmented caption, ``\textit{Results of the 1972 United States House of Representatives Elections in Select California Districts}'', denoted as $\bar{o}$, more clearly illustrates the table's content. As for the sparse cells which contain missing or incomplete data, we utilize the LLM to generate descriptions for each row and column. Formally, for the $m^{th}$ row,

\begin{equation}
        \bar{r}_m = LLM(P_0(o, h_{:}, v_{m,:})
\label{eq:augmented}
\end{equation}
where $\bar{r}_m$ represents the augmented description for the $m^{th}$ row containing cells $v_{m,:}=(v_{m,1}, ..., v_{m,N})$, $h_{:}=(h_1, ..., h_N)$ denotes the $N$ headers of table $\mathcal{T}$, and $P_0(\cdot)$ refers to the template used to prompt the LLM in generating the corresponding augmented summary. Specifically, in this paper, we define the augmentation prompt $P_0$ as: ``\textit{You will be given with the table caption and headers. Please enhance the caption/describe the given row/column corresponding to the table content}.'' The descriptions for the columns $c_n$ in table $\mathcal{T}$ are generated in a similar manner, yeilding $\bar{c}_n$. 

\subsection{Structured Knowledge Learning}
After augmenting the sparse data with contextual information, \name learns \textbf{structural relationships} over knowledge that is structurally stored through two steps: first, by constructing hypergraphs that aligns with the semantics (SHC), and second, by utilizing a novel prompt-attentive neural network for hypergraph learning (PHL). This section will elaborate on how our proposed \name conducts these two steps in detail.

% \vspace{-0.13in}
\subsubsection{Semantics Hypergraph Construction (SHC)} \label{sec:SHC}
This step embeds the semantics of table $\mathcal{T}$ into a hypergraph $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$, where $\mathcal{V}=\{..., v_{i}, ...\}$ represents the set of node(vertex), and $\mathcal{E}=\{..., e_{j}, ...\}$ represents the set of hyperedges. Each hyperedge connects multiple nodes, $v_i\in \mathcal{N}_{e_j}$ denotes that the node $v_i$ is included in the set of nodes connected by hyperedge $e_j$, while $e_j\in\mathcal{N}_{v_i}$ represents the hyperedge $e_j$ is included in the set of hyperedges which connects node $v_i$. %Here, $i$ and $j$ denote the indices of the nodes and edges across sets $\mathcal{V}$ and $\mathcal{E}$, respectively. 
Each cell $v_{m,n}$ in the table $\mathcal{T}$ is treated as a node, \ie{$v_{i}\in\mathcal{V}$, $|\mathcal{V}|=M\times N$. The rows $r_m\in\mathcal{R}$, columns $c_n\in\mathcal{C}$, and the entire table $\mathcal{T}$ act as hyperedges, leading to three types: row hyperedges $e_\mathcal{R}=\{..., e_{r_m}, ...\}\subseteq\mathcal{E}$, column hyperedges $e_\mathcal{C}=\{..., e_{c_n}, ...\}\subseteq\mathcal{E}$, and table hyperedge $e_\mathcal{T}\subseteq\mathcal{E}$, \ie{$|\mathcal{E}|=M+N+1$}. The connections between the nodes $v\in\mathcal{V}$ and hyperedges $e\in\mathcal{E}$ within the hypergraph $\mathcal{G}$ are represented by an incidence matrix $\mathbf{H}\in\mathbb{R}^{|\mathcal{V}|\times|\mathcal{E}|}$, where each element $h_{i,j}=1$ if node $v_i$ is connected by hyperedge $e_j$, and $h_{i,j}=0$ otherwise. 

%To capture the structured knowledge in table $\mathcal{T}$ for subsequent LLM processing, we learn the semantics of the textual information of cells, row/column descriptions, the caption. 
Specifically, we first tokenize the textual contents of cells, rows, columns, and captions using the BERT~\cite{devlin2019bert} tokenizer. For example, the augmented table caption is transformed into $O$ number of tokens represented by $(\mathbf{t}_{\bar{o},1},\mathbf{t}_{\bar{o},2},..., \mathbf{t}_{\bar{o},O})=Tok_{BERT}(\bar{o})$. The tokens are subsequently passed to an embedding layer, represented as $Emb(\cdot)$, which has an output hidden dimension of $d$ for semantics learning. Layer normalization and dropout layers are implemented in the embedding process to ensure robust generalization capabilities.
\begin{equation}
    \mathbf{h}_{e^{\mathcal{T}}}=Dropout(LN(Emb(\mathbf{t}_{\bar{o},1},\mathbf{t}_{\bar{o},2},..., \mathbf{t}_{\bar{o},O})))
\label{eq:semantics_emb}
\end{equation}
where $\mathbf{h}_{e^{\mathcal{T}}}\in{\mathbb{R}^{d}}$ is the hidden embedding of table hyperedge, $LN(\cdot)$ represents the layer normalization, and $Dropout(\cdot)$ refers to a dropout layer with a dropout rate 0.1. By representing the semantics of each cell content $v\in\mathcal{V}$ as node embedding $\mathbf{h}_{v}$ with Equation (\ref{eq:semantics_emb}), representing the row and column descriptions $\bar{r}_m$ and $\bar{c}_n$ as row/column hyperedges $\mathbf{h}_{e^{\mathcal{R}}}$ and $\mathbf{h}_{e^{\mathcal{C}}}$, and the table hyperedge $\mathbf{h}_{e^{\mathcal{T}}}$, we construct the semantic hypergraph $\mathcal{G}$. Additionally, we also calculate the semantic embedding for the essential inquiry (as highlighted in teal-blue in the task description prompt in Figure \ref{fig:method}), denoted as $\mathbf{h}_{\omega}$ for further learning. 
%To understand the semantics of the structured knowledge in table for further LLM processing, we first learn the semantics embedding of each cells, rows, and columns with a small LM model, for example, $BERT(\cdot)$~\cite{?}, considering previous works suggest that small language models are better at semantics embedding when compared to LLMs ~\cite{}. 

\subsubsection{Prompt-attentive Hypergraph Learning (PHL)}\label{sec:PHL}
Provided with the hypergraph $\mathcal{G}$, we design a prompt-attentive hypergraph neural network to further learn structured knowledge from $\mathcal{G}$. %, ensuring it captures the intricate structural relationships within the data while meeting the task-specific requirements for accurate and context-aware LLM generations. 
In traditional hypergraph learning~\cite{hypergcn,hgnn,hnhn,hcha}, hyperedge embeddings typically do not directly participate in the propagation process; instead, hyperedges primarily serve to connect related nodes, with the focus on node embeddings. In \name, we aim to integrate the semantic embeddings of both nodes and hyperedges during propagation. Since the table cells contain diverse content, while the augmented hyperedge descriptions (\ie{$\bar{o}$, $\bar{r}$, and $\bar{c}$}) are generated by the same LLM and maintain a consistent linguistic style, we apply node-to-edge and edge-to-node propagation using attention scores denoted by $\alpha$ and $\beta$ with distinct designs. Specifically, inspired by ~\cite{allset}, each PHL layer comprises two-step graph attention: first conducts \textit{semantic-aware propagation} from nodes to their connected hyperedges, then \textit{attentively integrate the embedding of the inquiry in the prompt} and propagates from edges to nodes. 

\begin{figure}[tbp]
    \centering
    \includegraphics[width=1.03\columnwidth]{figures/phl.pdf}
    % \vspace{-0.1in}
    \caption{The detailed architecture of PHL.}
    \label{fig:phl}
\end{figure}

\textit{Semantic-aware propagating}. Nodes are embedded from the original table cells content with Equation (\ref{eq:semantics_emb}), denoted as $\mathbf{h}_v$. We first propagate the original semantics embedded in $\mathbf{h}_v$ to each connected hyperedge $e$ with the $K$-head hypergraph attention mechanism. The attention score $\alpha_v^{(k)}$ in node-to-edge propagation for node $v$ at the $k^{th}$ head is calculated as follows. 
\begin{equation}
\alpha^{(k)}_v=\text{L-ReLU}(\sum\mathbf{Q}^{(k)}\cdot \mathbf{K}^{(k)}_v)
%=\text{L-ReLU}(\sum_{i=1}^{d}(\mathbf{\theta}_{k,:}\cdot MLP_K(\mathbf{h}_v^{l}))_i)
\label{eq:att_alpha}
\end{equation}
where $\text{L-ReLU}(\cdot)$ denotes the LeakyReLU activation function, the query representation $\mathbf{Q}^{(k)}=\mathbf{W}_{k,:}\in\mathbb{R}^{1\times d}$ is the $k^{th}$ vector of a learnable weight $\mathbf{W}\in\mathbb{R}^{K\times d}$. We use another multi-layer perceptron to learn the key representation of the target node $v$, \ie{$\mathbf{K}^{(k)}_v=MLP_K^{(k)}(\mathbf{h}_v^{l})\in\mathbb{R}^{1\times d}$}. 

Next, the information is propagated from the nodes to their connected hyperedges, formally represented as below.

\begin{equation}
% \begin{aligned}
        \mathbf{h}_{e}^{l,(k)} = \sum_{v\in\mathcal{N}_e}\sigma(\alpha_v^{(k)})\mathbf{V}^{(k)}_v 
\label{eq:mha}
% \end{aligned}
\end{equation}
where $\mathbf{V}^{(k)}_v=MLP_V^{(k)}(\mathbf{h}^{l}_{v})\in\mathbb{R}^{1\times d}$ denotes a multilayer perceptron used to transform the node embedding $\mathbf{h}_v^{l}$ to its value representation, $\mathbf{h}_e^{l}$ and $\mathbf{h}_v^l$ represents the hyperedge $e$ and its connected node $v$, respectively, as input to the $l^{th}$ layer. The $h_v^{l}$ is initialized by the semantics embedding of $v$, \ie{$\mathbf{h}_v^{0}=\mathbf{h}_v\in\mathbb{R}^{1\times d}$}. The softmax function $\sigma(\alpha_{v_i}^{(k)})=\frac{exp(\alpha^{(k)}_{v_i})}{\sum_{v\in\mathcal{N}_e} exp(\alpha^{(k)}_{v})}$ computes the normalized attention score for each node $v_i\in\mathcal{N}_e$.The aggregation $\sum_{v\in\mathcal{N}_e}$ for the embedding of each node $\mathbf{h}_v^{l}$ can be performed using any aggregation function, such as summation.

As shown in Figure \ref{fig:phl}, to increase the representation and generation capability to be compatible with the LLM, the aggregated embedding of hyperedges $\mathbf{h}_e$ are processed using residual connections, normalization, and feed forward layers, following the architecture of  transformer ~\cite{vaswani2017attention}.
\begin{equation}
   \mathbf{\hat{h}}_e^{l} = LN(FF(LN(\concat\nolimits_k\mathbf{h}_e^{l,(k)}+\mathbf{W}))+\concat\nolimits_k\mathbf{h}_e^{l,(k)})
\end{equation}
where $\concat\nolimits_k\mathbf{h}_e^{l,(k)}\in\mathbb{R}^{K\times d}$ represents the concatenation of the outputs of $K$ heads of the multi-head attention mechanism described in Equation (\ref{eq:att_alpha})(\ref{eq:mha}). The $\mathbf{\hat{h}}_e^{l,(k)}\in\mathbb{R}^{1\times d}$ denotes the hyperedge embedding incorporating information propagated from nodes, and is then concatenated to the original semantics embedding of hyperedge $e$, \ie{$\mathbf{h}_e^{l+1} = \mathbf{\hat{h}}_e^{l} + \mathbf{h}_e$, $\mathbf{h}_e^{l+1}\in\mathbb{R}^{1\times 2d}$}. 

\textit{Prompt-attentive integrating}. The original semantics embedding of hyperedge $\mathbf{h}_e$ are embedded from the LLM-augmented descriptions obtained with Equations (\ref{eq:augmented})(\ref{eq:semantics_emb}). To integrate the task requirements described in prompts into hypergraph learning, we adopt the embedding of the essential inquiry $\mathbf{h}_{\omega}$ in the prompt to calculate the attention score $\beta_e^{(k)}$ for edge-to-node propagation, similar to the process in Equation (\ref{eq:att_alpha}) (\ref{eq:semantics_emb}). The essential inquiry can be replaced by any textual information that is of particular relevance or concern to downstream tasks in the prompts.

\begin{equation}
    \beta_v^{(k)} = \text{L-ReLU}(\sum MLP_Q^{(k)}(\mathbf{h}_{\omega}\cdot MLP_K^{(k)}(\mathbf{h}_e^{l+1})))
\label{eq:beta}    
\end{equation}
\begin{equation}
    \mathbf{h}_v^{l,(k)} = \sum_{e\in\mathcal{N}_v} \sigma(\beta_v^{(k)}MLP_V^{(k)}(\mathbf{h}_e^{l+1}))
\label{eq:prompt_emb}
\end{equation}
where $e\in\mathcal{N}_v$ denotes that the set of all the hyperedge $e$ which connects the target node $v$, $MLP_Q^{(k)}(\mathbf{h}_{\omega})\in\mathbb{R}^{1\times d}$ represents the $k^{th}$ vector of the query representation $MLP_Q(\mathbf{h}_{\omega})\in\mathbb{R}^{K\times d}$, computed by a multilayer perceptron based on $\mathbf{h}_{\omega}$. This operation utilizes task requirements to attentively propagate information from hyperedges to nodes. This operation intuitively aligns with the human reasoning process, where relevant rows or columns are identified first, followed by a detailed examination of individual cells when handling tasks that require structured knowledge.

Similarly, the node embedding $\mathbf{h}_v^{l}$ learned from multi-head attentions are further processed with the residual connections, normalization, and feed-forward layers, formally as below.

\begin{equation}
    \mathbf{h}_v^{l+1} = LN(FF(LN(\concat\nolimits_k\mathbf{h}_v^{l,(k)}+MLP_Q^{(k)}(\mathbf{h}_{\omega})))+\concat\nolimits_k\mathbf{h}_v^{l,(k)}))
\end{equation}
where $\concat\nolimits_k\mathbf{h}_v^{l,(k)}$ denotes the concatenation of the outputs of $K$-head attention mechanism described in Equation (\ref{eq:beta})(\ref{eq:prompt_emb}). 

Through the adoptions of semantic-aware propagation and inquiry-attentive integration in each PHL layer, the hypergraph neural network attains a comprehensive understanding of the hierarchical semantics embedded within structured data. This approach ensures semantic consistency, comprehensively captures hierarchical dependencies, and preserves the order invariance property of structural relationships within the knowledge structure.

\subsection{Structured Knowledge Integration}
After completing the hypergraph learning process, the knowledge linked to each cell, along with the columns, rows, and caption, is embedded in the representations of the nodes and hyperedges, which are further integrated into the generation process of LLMs.

\subsubsection{Encoding Structured Knowledge} By connecting each cell node to the table hyperedge formed from the caption, as detailed in Section \ref{sec:SHC}, the hidden embedding $\mathbf{h}_{e_{\mathcal{T}}}$ effectively captures the task-relevant structured knowledge of the entire table $\mathcal{T}$. Therefore, $\mathbf{h}_{e_{\mathcal{T}}}^{L}$ is mapped to the token space of LLM using a projector $\pi$.
\begin{equation}
\mathbf{e}_{\mathcal{T}}=\pi(\mathbf{h}_{e_{\mathcal{T}}}^{L})
\end{equation}
Here, the projected table embedding is denoted as $\mathbf{e}_{\mathcal{T}}\in\mathbb{R}^{d'}$, where $d'$ denotes the dimension of the input tokens for the LLM. In \name, the projector $\pi$ is implemented using two linear layers, with a ReLU activation function in between. The table embedding $\mathbf{e}_{\mathcal{T}}$ is then integrated into the token embeddings $\mathbf{e}_{x,:}=Tok_{LLM}(P_2(x))$ of the task description prompt $x$ at the designated placeholder position labeled ``Table Hyperedge'' in natural language, as highlighted by the bold text in the upper left corner of Figure \ref{fig:method}. 
\begin{equation}
 \mathbf{\hat{e}}_{x}=(\mathbf{e}_{x,:ph-start})\concat(\mathbf{e}_{\mathcal{T}})\concat(\mathbf{e}_{x,ph-end:})
\end{equation}
where $\mathbf{e}_{x,:ph-start}$ denotes all the prompt tokens preceding the placeholder, $\mathbf{e}_{x,ph-end:}$ denotes all the prompt tokens following the placeholder. Additionally,  $\mathbf{\hat{e}}_{x}$ represents the tokens that integrate structured knowledge for further inference in the LLMs.

\subsubsection{Training}
Given the task description, markdown table, and the inquiry $\omega$ in prompt $x$, structured table $\mathcal{T}$, our \name jointly train the prompt-attentive hypergraph learning network with the LoRA~\cite{hu2022lora}. The supervised fine-tuning process can be expressed in terms of the log likelihood loss. Given the input task description prompt $x$ and target output $y$ from the training set $\mathcal{D}$, there is,

\begin{equation}
    \mathbb{E}_{(x,y,\mathcal{T})\in\mathcal{D}}\left[\sum_{t=1}^{T} log p_\theta(y_t|y_{1:t-1}, x, \mathcal{T})\right]
\end{equation}
Here, the conditional probability distribution of the target generation output sentence $y$ given prompt $x$ is represented as $p_\theta(y|x)=\prod_{t=1}^{T}x_\theta(y_t|y_{<t}, x, \mathcal{T})$, where $\theta$ denotes the model parameters and $T$ is the length of the generated sequence.
% \subsection{Training Strategy}
% \subsubsection{Pretraining of PHL Layers}
% \subsubsection{Joint Training of PHL Layers and LoRa}