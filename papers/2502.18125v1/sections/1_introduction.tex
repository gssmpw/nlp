\section{Introduction}
%\sr{todo: use ``prompt'' instead of ``instruction''}\sr{todo: citations}
With the advancement of digitalization across various industries, substantial amounts of structured knowledge are stored in tabular formats. This structured knowledge, often containing domain-specific information closely tied to different downstream tasks, complements the general knowledge acquired by Large Language Models (LLMs) during pre-training, thereby enhancing their capability to support downstream queries and reasoning~\cite{cui2024tabular,tan2024struct}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{figures/toy_example.pdf}
    % \vspace{-0.31in}
    \caption{An example illustrates the three aspects of the structural relationships in tables: i) Semantic Consistency, ii) Hierarchical Dependencies, and iii) Order Invariance. Additionally, it highlights the data sparsity issue iv), where incomplete data affects SQL queries over the table .}%\sr{the description of toy example in  intro should be modified in accordance with the figure.}}
    \label{fig:toy_example}
% \vspace{-0.20in}
\end{figure}

LLMs, leveraging their sophisticated linguistic capabilities and extensive knowledge base, have been widely utilized as one-/few-shot learners in various structured tasks ~\cite{hegselmann2023tabllm,hanlarge,zhu2023incorporating,kongopentab}. Currently, the approaches for applying LLMs on structured knowledge, including tables, fall into two primary categories: serialization-based~\cite{min2024exploring,hegselmann2023tabllm,jaitly2023towards} and operation-based methods~\cite{ye2023decomposers,jiang-etal-2023-structgpt,wang2024chainoftable,lu2023chameleon}. Serialization-based methods convert structured knowledge into sequences of tokens, enabling the model to process the structured data in conjunction with task descriptions~\cite{min2024exploring,shao2024linearizing}. For example, Hegselmann et al.~\cite{hegselmann2023tabllm} utilize a Table-to-Text model or a LLM as the serializer to convert tables into natural language strings, which are then fed into the LLM along with task descriptions. However, serializing structured data can undermine the inherent structural relationships, especially in larger tables, potentially leading to serve knowledge forgetting and diminished logical coherence during reasoning~\cite{zhang2023ho,li2024snapkv}. Additionally, the serialized formats critically influenced the performance of LLMs~\cite{singha2023tabular}. The operation-based methods extract relevant information from structured data using SQL-like operations based on task requirements, and then incorporate this knowledge into LLMs to generate responses. While the SQL-like operations account for structural relationships, these methods fail to fully harness the extensive knowledge base of LLMs for effective reasoning~\cite{zhu2023incorporating}. As shown in Figure \ref{fig:toy_example} d), the ``(d)'' and ``(r)'' represent the Democratic and Republican parties, respectively. Since the parties are not explicitly listed in a separate column, retrieving information about the political affiliations of incumbents via SQL queries is challenging. However, LLMs can easily interpret this information due to their advanced in-context learning abilities and knowledge base. Therefore, \textbf{structural relationships} and \textbf{data sparsity} are two critical challenges that current methods are not fully account for when reasoning over structured knowledge, which differs fundamentally from the unstructured text inputs LLMs typically handle~\cite{fang2024large}. 

Graphs are structure-aware, making them a natural choice for modeling structural relationships. However, traditional graphs remain insufficient in effectively capturing the group relationships between rows and columns. %Moreover, traditional graph neural networks (GNNs), with their focus on node- or graph-level objectives, struggle to fully understand the task-specific requirements in natural language.
Unlike traditional graphs, where an edge connects only two nodes, a hyperedge in a hypergraph can connect multiple cells nodes in an unordered manner. Hypergraphs consider the \textbf{structural relationships} within tabular data from three aspects: i) Semantic Consistency. Data in the cells of the same row or column in a table generally correspond to a consistent semantic category, allowing LLMs to identify and infer implicit semantic relationships. As illustrated in Figure \ref{fig:toy_example} i), the cells in the ``Incumbent'' column are all personal names. ii) Hierarchical Dependencies. Hyperedges are capable of capturing intricate, higher-order dependencies within structured knowledge, such as the dependencies of the captions, headers, and cells. iii) Order Invariance. Changing word order in natural language can alter meaning, but rearranging rows or columns in a table, \eg{swapping the Moss and McFall rows in Figure \ref{fig:toy_example} iii)}, does not affect the overall semantics. To address the \textbf{sparsity} issue such as the incomplete parties in Figure \ref{fig:toy_example} iv), hypergraphs facilitate high-order information propagation between nodes and hyperedges, thereby supplementing the representations of incomplete cells with information from their neighbors. In addition, the extensive general knowledge embedded in LLMs can be leveraged to address sparse data issues.

%Specifically, we integrate task-specific inquiries for downstream tasks into the information propagation process of the hypergraph neural network, ensuring that the learned knowledge representations are well-aligned with the objectives of the downstream tasks.

%\textcolor{red}{To enhance LLMs' capabilities on structured knowledge, we propose a novel hypergraph-based generation framework. This framework enables LLMs to fully exploit the structural characteristics of tabular data while effectively addressing sparsity issues.}

To enhance LLMs' capabilities on structured knowledge, we propose a novel \textbf{\textit{Hyper}}graph-based \textbf{\textit{G}}eneration framework, namely \textbf{\name}, to facilitate seamless integration of knowledge from structure learning with hypergraph neural networks into LLMs, without losing focus on task-specific requirements. Specifically, \name explicitly guide the LLMs to augment sparse table cells with contextual information. We then construct semantics hypergraphs with the augmented table and introduce a novel Prompt-Attentive Hypergraph Learning (PHL) module that propagates task-specific inquiries in prompts along with embedded semantic knowledge across structures, and train this module jointly with the LLM. Our contributions are concluded as follows:
% \vspace{-0.03in}
\begin{itemize}[leftmargin=*]
    \item \textbf{Towards structural relationship.} We propose \name, which uses hypergraphs to capture the semantic consistency, order invariance, and hierarchical dependencies within structured knowledge, thereby enhancing the LLMâ€™s capability to understand and reason over structured knowledge.
    \item \textbf{Towards data sparsity.} We design a novel hypergraph neural network to tackle the sparsity issue in tabular knowledge by utilizing the generative abilities of LLMs and then facilitates information propagation through hyperedges.
    % \item \textbf{Towards downstream task requirements}
    \item \textbf{Experiments.} We conduct extensive experiments on various downstream tasks involving structured data to validate the effectiveness of our proposed \name framework.
\end{itemize}