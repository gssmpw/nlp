% \vspace{-0.18in}
\section{Conclusion}
In this paper, we present a novel hypergraph-based generation framework, \name, designed to enhance the understanding and reasoning capabilities of Large Language Models (LLMs) when dealing with knowledge structurally stored. The primary objective of \name is to tackle the challenges arising from complex structural relationships and data sparsity, such as incomplete cell information, within structured data. By employing a novel prompt-attentive hypergraph learning (PHL) module, \name effectively propagates information across high-order group dependencies, capturing intricate connections within the data. Comprehensive experiments across three distinct tabular tasks consistently demonstrate the impact of \name on enhancing the performance of LLMs with different parameter scales. We envision \name as a solution for enhancing LLMs in a broader range of applications which requires nuanced understanding of structured information. Furthermore, \name facilitates the broader adoption of LLMs in real-world applications, where knowledge is often stored in structured formats, thereby enabling LLMs to better handle and reason over such data.