\UseRawInputEncoding
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
% \usepackage[dvipsnames, svgnames, x11names]{xcolor}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{color}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{CJKutf8}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage[noend]{algpseudocode}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{float}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bbding}


\title{QP-SNNs: Quantized and Pruned Spiking \\Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy 
\author{Wenjie Wei$^{1}$, {Malu Zhang}$^{1}$\thanks{Corresponding author: maluzhang@uestc.edu.cn}, Zijian Zhou$^{1}$, Ammar Belatreche$^{2}$, \\ \textbf{Yimeng Shan}$^{3}$, \textbf{Yu Liang}$^{1}$, \textbf{Honglin Cao}$^{1}$, \textbf{Jieyuan Zhang}$^{1}$, \textbf{Yang Yang}$^{1}$\\ 
~\\
$^{1}$University of Electronic Science and Technology of China\\
$^{2}$Northumbria University, $^{3}$Liaoning Technical University\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy
% Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\begin{CJK}{UTF8}{gbsn}

\maketitle

\begin{abstract}
Brain-inspired Spiking Neural Networks (SNNs) leverage sparse spikes to encode information and operate in an asynchronous event-driven manner, offering a highly energy-efficient paradigm for machine intelligence.
However, the current SNN community focuses primarily on performance improvement by developing large-scale models, which limits the applicability of SNNs in resource-limited edge devices.
In this paper, we propose a hardware-friendly and lightweight SNN, aimed at effectively deploying high-performance SNN in resource-limited scenarios.
Specifically, we first develop a baseline model that integrates uniform quantization and structured pruning, called QP-SNN baseline. While this baseline significantly reduces storage demands and computational costs, it suffers from performance decline. To address this, we conduct an in-depth analysis of the challenges in quantization and pruning that lead to performance degradation and propose solutions to enhance the baseline's performance. 
For weight quantization, we propose a weight rescaling strategy that utilizes bit width more effectively to enhance the model's representation capability. 
For structured pruning, we propose a novel pruning criterion using the singular value of spatiotemporal spike activities to enable more accurate removal of redundant kernels.
Extensive experiments demonstrate that integrating two proposed methods into the baseline allows QP-SNN to achieve state-of-the-art performance and efficiency, underscoring its potential for enhancing SNN deployment in edge intelligence computing.
% Codes are available at \url{https://openreview.net/}.




\end{abstract}
\section{Introduction}
\begin{wrapfigure}[]{r}{0.44\textwidth}
\vspace{-5mm}
\centering
\includegraphics[scale=0.23]{figs/bubble.pdf}
\vspace{-0.3cm}
\caption{Comparison of QP-SNN and related work on CIFAR-100. The bubble size represents the model size.}
\label{fig:bubble}
\end{wrapfigure}
Inspired by the information processing paradigm of biological systems, Spiking Neural Networks (SNNs) encode information via binary spikes and process them in a sparse spike-driven manner (\cite{gerstner2002spiking,izhikevich2003simple}). 
This paradigm simplifies the matrix computations of weight and spike activity in SNNs from computationally intensive multiply-accumulate (MAC) operations to computationally efficient accumulate (AC) operations. 
Therefore, SNNs are regarded as a promising energy-efficient solution for achieving next-generation machine intelligence (\cite{pfeiffer2018deep,roy2019towards}). 
Furthermore, the energy efficiency of SNNs has driven the development of neuromorphic hardware, such as SpiNNaker (\cite{painkras2013spinnaker}), TrueNorth~(\cite{akopyan2015truenorth}), Loihi (\cite{davies2018loihi}), Tianjic (\cite{pei2019towards}) etc. 
These neuromorphic hardware can fully exploit the energy efficiency potential of SNNs.
Despite these advantages, the application scenarios and performance of SNNs still require improvement compared to traditional artificial neural networks (ANNs).

% This paradigm reduces the complexity of matrix computations involving weights and spike activities by shifting from computationally intensive multiply-accumulate (MAC) operations to more efficient accumulate (AC) operations.

In the past few years, the SNN community has focused primarily on designing complex SNNs architectures (\cite{yu2020toward,yao2024spike,yao2024spike2,luo2024integer,wang2024global,zhang2024spike}) and training strategies (\cite{zhang2018highly,zhang2019mpd,zhang2021rectified,wu2021progressive,guo2022recdis,xu2023constructing,wei2024event}) to achieve impressive performance across various application tasks.
While these studies have yielded satisfactory performance, they typically come at the cost of large model parameters, high memory consumption, and increased computational complexity (\cite{yan2024efficient,liu2024lite,cao2025binary}). 
This undermines the inherent energy efficiency of SNNs and restricts their applicability in resource-limited scenarios.
To achieve effective deployment, a growing number of researchers have worked on compressing large-scale SNNs.
Existing methods to improve SNN energy efficiency primarily fall into two categories: (1) reducing the precision of parameter representations and (2) reducing redundant parameters within the model.


Quantization is a key technique for the first category, which reduces memory storage and computational complexity by storing full-precision values in low bit-width precision (\cite{gholami2022survey}).
Based on whether an equal-size interval is used to discretize full-precision values, it can be divided into non-uniform and uniform quantization (\cite{rokh2023comprehensive}).
Non-uniform quantization divides discretization intervals unevenly, enabling a more precise capture of critical information and leading to improved performance.
However, it is challenging to deploy this approach on general computing hardware efficiently (\cite{cai2017deep,kulkarni2022survey}).
In contrast, uniform quantization maps full-precision values to equal-sized discrete intervals, offering advantages such as simplicity, low computational cost, and efficient mapping to hardware (\cite{zhu2016trained,jain2020trained}).

Pruning is one of the effective methods for reducing redundant parameters in a model, which reduces the model size by removing unimportant connections (\cite{li2016pruning}).
Pruning can be classified into unstructured and structured pruning (\cite{vadera2022methods}).
Unstructured pruning removes individual nodes like a single neuron of networks, resulting in unstructured sparsity.
This often leads to a high compression rate, but requires specialized hardware or library support for acceleration (\cite{han2015learning}).
In contrast, structured pruning removes entire convolutional filters, ensuring model's structure.
This avoids complex sparse matrix operations, enabling acceleration with standard hardware by taking advantage of a highly efficient library (\cite{he2023structured,xu2020convolutional}).


Real-world deployments are typically limited by size, weight, area, and power.
This makes combined quantization and pruning a promising approach for maximizing SNN compression. 
Existing research on integrating quantization and pruning in SNNs faces two challenges.
% (\cite{rathi2018stdp,deng2021comprehensive,chowdhury2021spatio}).
Firstly, they do not sufficiently account for hardware-friendliness, for example, (\cite{rathi2018stdp}) and (\cite{deng2021comprehensive}) adopt unstructured pruning.
Secondly, despite significant energy efficiency, these studies suffer from severe performance degradation, with evaluations limited to simple datasets (\cite{chowdhury2021spatio,deng2021comprehensive}).
% For example, (\cite{rathi2018stdp}) evaluate performance only on simple datasets like MNIST, (\cite{chowdhury2021spatio}) and (\cite{deng2021comprehensive}) extend their method only to CIFAR.
Thus, integrating both techniques for maximal compression while ensuring hardware efficiency and high performance remains challenging. 
In this paper, we introduce the QP-SNN, tailored for effective deployment in resource-limited environments.
We first build a QP-SNN baseline that integrates uniform quantization and structured pruning. 
While this baseline offers notable efficiency gains, it suffers from reduced performance.
To address this, we investigate the root causes of performance degradation in quantization and pruning, and propose solutions to enhance the baseline's performance. As shown in Figure \ref{fig:bubble}, QP-SNN utilizing the proposed solutions achieves excellent accuracy and model size.
We summarize main contributions as,
\begin{itemize}
    \item 
    % \textbf{Hardware-friendly and ultra-lightweight framework}: 
    We first develop a hardware-efficient and lightweight QP-SNN baseline by integrating uniform quantization and structured pruning. This baseline significantly reduces storage and computational demands, but suffers from performance limitations.  
    % The employed uniform quantization and structured pruning are easy to implement, exhibiting substantial efficiency in hardware deployment.
    
    \item 
    % \textbf{Weight rescaling strategy}:
    To improve performance through quantization, we reveal that the vanilla uniform quantization in the QP-SNN baseline constrains the model's representation capability due to inefficient bit-width utilization.
    To address this, we propose a weight rescaling strategy (ReScaW) that optimizes bit-width usage for improved representation. 

    \item 
    % \textbf{Novel pruning criterion}:
    To further boost performance through pruning, we introduce a novel structured pruning criterion for the QP-SNN baseline that leverages the singular value of spatiotemporal spike activity (SVS). This SVS criterion provides greater robustness across varying input samples and allows more precise removal of redundant convolutional kernels.
    % offering an accurate assessment of the importance of convolutional kernels.

    \item
    Extensive experiments show that integrating ReScaW-based quantization and the SVS-based criterion into the baseline allows QP-SNN to achieve state-of-the-art performance and efficiency, revealing its potential for advancing edge intelligence computing.
    % revealing its vast advantages and potential for effective deployment in real-world resource-constrained scenarios.
    
\end{itemize}



\section{Related work}
\paragraph{Quantization technique}
% Quantization achieves efficient computation and storage by reducing parameter representation from 32-bit to lower bit-width representation.
Early research on quantization in SNNs is primarily based on ANN-to-SNN conversion algorithms, where a quantized ANN is first trained and then converted into the corresponding quantized SNN version (\cite{sorbaro2020optimizing,roy2019scaling}).
To mitigate the performance loss associated with the conversion, researchers have proposed many strategies, such as the utilization of activation penalty term (\cite{sorbaro2020optimizing}) and the weight-threshold balancing method (\cite{wang2020deep}). 
However,these quantized SNNs still experience significant performance degradation and long latency issues.
To address these limitations, some studies have explored directly training quantized SNNs and introduced different strategies to enhance performance, such as alternating direction method of multipliers (\cite{deng2021comprehensive}), accuracy loss estimator (\cite{pei2023albsnn}), suitable activation function (\cite{hu2024bitsnns}), and weight-spike dual regulation (\cite{wei2024q,wang2024ternary}).
% Recently, there have been studies that quantize both weights and membrane potentials of SNNs, achieving 90.79\% accuracy on CIFAR-10 with 2-bit configuration \cite{yin2024mint}.
Despite performance improvement, these studies fail to effectively leverage the allocated bit-width, resulting in the limited expressive capability of models. 
Therefore, there still remains significant room for performance improvement.



\paragraph{Pruning technique}

% Pruning reduces the number of parameters in a neural network and improves inference speed by eliminating redundant components within the network, such as weights, feature maps, and convolutional kernels.
Existing research on pruning SNNs can be broadly divided into two groups.
The first group is unstructured pruning.
For example, (\cite{yin2021energy}) use a magnitude-based method to remove insignificant weights, and (\cite{shi2023towards}) propose a fine-grained pruning framework that integrates unstructured weight and neuron pruning to enhance SNN energy efficiency.
Additionally, there are some biologically inspired unstructured pruning works (\cite{bellec2017deep,chen2022state}).
While these studies achieve great sparsity and performance, they lead to irregular memory access in forward propagation, requiring specialized hardware for acceleration.
The second group is structured pruning that offers better hardware compatibility.
(\cite{chowdhury2021spatio}) use principal component analysis on membrane potentials to evaluate channel correlations and eliminate redundant ones.
However, it suffers from long latency and cannot handle neuromorphic datasets.
Recently, (\cite{li2024towards}) evaluate the importance of kernels based on spike activity, advancing the performance of pruned SNNs to a new level.
However, this evaluation criterion exhibits high dependency on inputs and may not accurately reflect the importance of kernels.


\paragraph{Compression with joint quantization and pruning}
Several studies have explored combining quantization and pruning to maximize the compression of SNNs.
First, (\cite{rathi2018stdp}) adopt the STDP learning rule and a predefined pruning threshold to remove insignificant connections, and then quantizes retained important weights.
Then, (\cite{chowdhury2021spatio}) perform principal component analysis on membrane potentials for spatial pruning and gradually decreases the time step during training for temporal pruning. They also use post-training quantization to compress retained weights.
Moreover, (\cite{deng2021comprehensive}) formulate pruning and quantization as a constraint optimization problem in supervised learning, and address it with the alternating direction method of multipliers.
However, these existing studies combining quantization and pruning face two main problems.
Firstly, the unstructured pruning methods in (\cite{rathi2018stdp}) and (\cite{deng2021comprehensive}) require specialized hardware for efficient acceleration.
Secondly, (\cite{rathi2018stdp}) only evaluate their method on very simple datasets, and (\cite{chowdhury2021spatio}) and (\cite{deng2021comprehensive}) only extend their methods to CIFAR (88.6\% and 87.84\% accuracy on CIFAR-10 with 5, 3 bits respectively), leading to significant room for improvement in both performance and efficiency.
% achieving 


\section{Quantized and pruned SNN baseline}
In this section, we develop the QP-SNN baseline by combining uniform quantization and structured pruning.
These two compression techniques are highly compatible with existing hardware accelerators, significantly improving the model's deployment efficiency.

\paragraph{Neuron model.}
% The spiking neuron serves as the fundamental unit of SNNs, with various spiking neuron models designed to simulate the information processing of biological neurons, including Hodgkin-Huxley \cite{Hodgkin_Huxley} Spike Response Mode (SRM) models, etc. 
Many spiking neurons have been proposed (\cite{Hodgkin_Huxley,zhang2021rectified,wei2023temporal}), we use the Leaky Integrate-and-Fire (LIF) model in QP-SNN due to its high computational efficiency (\cite{wu2018spatio}).
The membrane potential of LIF model is,
\begin{align}
\label{u}
\tilde{\mathbf{U}}^{l}[t]=\tau \mathbf{U}^{l}[t-1]+ \mathbf{X}^{l}[t],
\end{align}
where $\mathbf{U}^l[t]$ is the membrane potential of neurons in layer $l$ at time $t$, $\tau$ is the leaky factor, and $\mathbf{X}^l[t]=\mathbf{W}^l \mathbf{S}^{l-1}[t]$ is input current. Neurons integrate incoming signals and generate a spike when the membrane potential exceeds the threshold $\theta$. 
The spike generation function is computed as,
\begin{align}
\mathbf{S}^l[t]
=\left\{\begin{matrix}
\;1, &\! \text{if    }\; \mathbf{U}^l[t] \geq \theta,\\ 
\;0, & \text{otherwise}.
\end{matrix}\right.
\label{spikefunc}
\end{align}
After spike emission, we use the hard reset mechanism to update the membrane potential. This mechanism resets membrane potential to zero when a spike occurs and remains inactive otherwise.
\begin{align}
\mathbf{U}^{l}[t]=\tilde{\mathbf{U}}^{l}[t]\cdot \left(1-\mathbf{S}^{l}[t]\right).
\end{align}
\paragraph{Vanilla uniform quantization.}
% Quantization compresses large-scale SNNs by reducing the precision of synaptic weights, categorized into non-uniform and uniform quantization. 
Quantization can be grouped into non-uniform and uniform quantization. 
% Non-uniform quantization maps weights to non-equidistant levels, typically resulting in superior performance.
Non-uniform quantization exhibits superior performance, but requires specialized hardware support.
% However, it entails high computational complexity and necessitates specialized design for hardware acceleration. 
% uniform quantization maps weights to integer grids with equal size, simplifying both computational complexity and hardware implementation\cite{zhu2016trained}. Therefore, we explore the uniform quantization in QP-SNNs.
In contrast, uniform quantization maps weights to integer grids with equal size, simplifying both computational complexity and hardware implementation (\cite{zhu2016trained}).
In this study, we explore the uniform quantization in QP-SNNs.
% Uniform quantization is typically defined by three parameters: scale factor, zero point, and bit width.
% The scale factor and zero point are employed to map a full-precision value onto an integer grid, with the grid's size determined by the bit width.
The vanilla uniform quantization for weights in layer $l$, i.e., $\mathbf{W}^l$, can be formulated as follows,
% \begin{subequations}
% \end{subequations}
\begin{align}
\label{quantw}
{\mathbf{W}}^l_{int} =\left \lceil \frac{s(b)}{2}\cdot\left ({\mathrm{clamp}({\mathbf{W}^l};-1,1)+z} \right) \right \rfloor,
\end{align}
% \begin{align}
% \label{quantw}
% {\mathbf{W}}^l_{int} =\left \lceil \frac{s(b)}{2}\cdot\left ({\mathrm{clamp}(\frac{\mathbf{W}^l}{\alpha^l};-1,1)+z} \right) \right \rfloor,
% \end{align}
where $\mathrm{clamp}(\cdot)$ is a clipping operator, $\left \lceil \cdot \right \rfloor$ is a rounding operator, $z$ is the zero-point, $b$ is the bit width, and $s(b)=2^b-1$ is the number of integer grids.
We set $z$ to 1 and explore bit widths $b$ of 8, 4, 2.
Therefore, Eq.(\ref{quantw}) maps $\mathbf{W}^l$ onto the unsigned integer grid $\left\{ 0, \cdots , 2^b-1\right\}$.
To reconstruct $\mathbf{W}^l$ through their quantized counterparts, the de-quantization is defined as,
\begin{align}
\hat{\mathbf{W}}^l = 2\cdot \frac{{\mathbf{W}}^l_{int}}{s(b)} -z.
\end{align}
% \begin{align}
% \hat{\mathbf{W}}^l = \alpha^l\cdot\left(\frac{{2\cdot\mathbf{W}}^l_{int}}{s(b)} -z\right).
% \end{align}
Consequently, the general definition for the quantization used in the QP-SNN baseline is stated as,
\begin{align}
\mathbf{W}^l \approx \hat{\mathbf{W}}^l =  \frac{2}{s(b)}\left \lceil \frac{s(b)}{2}\cdot\left ({\mathrm{clamp}({\mathbf{W}^l};-1,1)+z} \right) \right \rfloor-z.
\end{align}
% \begin{align}
% \mathbf{W}^l \approx \hat{\mathbf{W}}^l = \alpha^l \left( \frac{2}{s(b)}\left \lceil \frac{s(b)}{2}\cdot\left ({\mathrm{clamp}(\frac{\mathbf{W}^l}{\alpha^l};-1,1)+z} \right) \right \rfloor-z\right).
% \end{align}
The vanilla quantization greatly reduces the baseline's storage and computation demands, but suffers from the limited weight precision.
This constrains the model's representation capability, reducing performance.
In the next section, we resolve this issue by effectively using the assigned bit-width.
% However, since weight values are concentrated around zero, directly applying the vanilla uniform quantization constrains QP-SNN's representation capability and performance.
% We denote this problem as inefficient bit-width utilization.
% In the next section, we will analyze this issue in detail and preserve the performance of QP-SNN by addressing it.




\paragraph{Structured pruning.}
% Pruning reduces the amount of parameters and computation in SNNs by removing redundant connections, classified as unstructured or structured pruning. 
Pruning can be classified as unstructured and structured pruning. 
% Unstructured pruning deletes less important connections without considering the model's internal structure, enabling high sparsity while still maintaining excellent performance.
% Unfortunately, it generates many sparse matrices in forward propagation, making it hard to implement on general hardware accelerator.
Unstructured pruning enables high sparsity and excellent performance but requires specialized design for hardware acceleration.
In contrast, structured pruning preserves the model's structure and is highly compatible with existing hardware accelerators.
Currently, the most advanced structured pruning method in SNN is presented by (\cite{li2024towards}).
They prune convolutional kernels according to the spiking channel activity (SCA) criterion.
We use this criterion in our QP-SNN baseline for the following analysis and comparison.
For the weight tensor $\mathbf{W}^{l} \in\mathbb{R}^{c_l\times c_{l-1} \times k\times k}$, the SCA-based criterion evaluates and prunes kernels based on the magnitude of membrane potential.
The importance evaluation for the $f$-th kernel, i.e., $\mathbf{W}^{l,f}\in \mathbb{R}^{c_{l-1}\times k\times k}$, is defined as,
\begin{align}
\label{xuscore}
\mathrm{Score}(\mathbf{W}^{l,f})=\frac{1}{B\cdot T}\cdot\left( \sum_{b=1}^{B}\sum_{t=1}^{T} \left\| \tilde{\mathbf{U}}^{l,f}[t]\right\|\right),
\end{align}
% \vspace{-1cm}
% the SCA-based pruning criterion calculates the importance score of the kernel based on the average membrane potential, and then removes kernels with low score.
where $B$ is the number of samples per mini-batch, $T$ is the time step, $\|\cdot\|$ is the L1-norm, and $\tilde{\mathbf{U}}^{l,f}[t]$ is the membrane potential of the $f$-th feature map.
As shown in Eq.(\ref{xuscore}), the SCA-based criterion regards positive values in \( \tilde{\mathbf{U}}^{l,f}\) as excitatory postsynaptic potentials and negative values as inhibitory postsynaptic potentials, thus removing kernels that contribute less to the membrane potential.
By unifying the SCA-based criterion, the number of parameter and computation in baseline is further reduced.
Noteworthy, the performance of the pruned QP-SNN baseline model relies strongly on the scoring function. 
Therefore, the idea $\mathrm{Score(\mathbf{W})}^{l,f}$ should accurately identify the important kernels.



\section{Method}
% This section introduces the core aspects of our work.
To enhance the performance of the QP-SNN baseline, we analyze and resolve the underlying issues in quantization and pruning that cause performance reduction.
In quantization, we reveal that the baseline suffers from limited representation capability due to inefficient bit-width utilization, and propose the weight rescaling strategy to use bit-width more effectively.
In pruning, we propose a novel pruning criterion for the QP-SNN baseline to more accurately remove redundant kernels.



\subsection{Weight rescaling strategy}
\label{sec:method.1}
% In this subsection, we first reveal that the quantization in Eq.(\ref{quantw}) suffers from inefficient bit-width utilization,  which undermines weight discrimination.
% Then, we propose a weight redistribution strategy to rescale the weight range, aiming to improve the performance of QP-SNNs by optimizing the utilization efficiency of the available bit-width.

\paragraph{Problem analysis.}
\begin{figure}[t]
\centering
\subfigure[Uniform quantization comparison]{\includegraphics[width=0.42\linewidth]{figs/quant_analysis.pdf}
\label{fig:quant-analysis}}
\subfigure[Visualization of weight distribution]{\includegraphics[width=0.56\linewidth]{figs/inefficientBW.pdf}
\label{fig:wrange}}
\caption{(a) Vanilla uniform quantization exhibits inefficient bit-width utilization, while ReScaW-based quantization can fully leverage the allocated bit-width. Green dots represent normal weights within the 1st and 99th percentiles, orange dots are boundary values, and blue dots are outliers. (b) Distribution is plotted to show that weights are concentrated in a narrow range around zero.}
\end{figure}
The vanilla uniform quantization in the QP-SNN baseline minimizes resource usage, but suffers from inefficient bit-width utilization.
This weakens the discrimination of the quantized weights in the QP-SNN baseline, limiting the model's representation capability.
To evaluate the bit-width utilization efficiency, we define a metric as,
\begin{align}
\label{rate}
R_{utilize} = \frac{N_{actual} \left( \mathbf{W}_{int}^l\right)}{N_{total} \left(\mathbf{W}_{{int}}^l\right)},
% \vspace{-15cm}
\end{align}
where $N_{actual}(\cdot)$, $N_{total}(\cdot)$ are the actual, available number of distinct values that $\mathbf{W}_{int}^l$ represents.
% $N_{actual}(\cdot)$ is the number of distinct values that $\mathbf{W}_{int}^l$ actually represents, and  is the total number of distinct values that $\mathbf{W}_{int}^l$ can represent.
Next, we analyze $R_{utilize}$ for QP-SNN baseline to assess bit-width utilization efficiency.
We compute $N_{actual}(\mathbf{W}_{int}^l)$ using the range of full precision weights, shown in Figure~\ref{fig:quant-analysis}(top).
We consider full precision weights between the 1st and 99th percentiles to eliminate outliers.
For clarity, we denote $\mathbf{W}^l \in [-a,a]$, where \(a=\max\left( \left| P_{0.01}(\mathbf{W}^l) \right|, \left| P_{0.99}(\mathbf{W}^l) \right| \right)\).
Typically, $a$ is a positive value near zero (\cite{lecun2002efficient,he2015delving}).
This means that the clamp function in Eq.(\ref{quantw}) doesn't alter weight values.
% Since $\mathbf{W}^l \in [-a,a]$ and $a<1$, the clamp function in Eq.(\ref{quantw}) does not change weight values.
Based on this, we can deduce $\mathbf{W}_{int}^l \!\in\! \left[ \left \lfloor \frac{z-a}{2} s(b)\!+\!\frac{1}{2} \right \rfloor,  \left \lfloor \frac{z+a}{2} s(b)\!+\!\frac{1}{2} \right \rfloor \right]$.
Therefore, $N_{actual}(\mathbf{W}_{int}^l)$ is approximately $\left( 1\!+\!a\cdot s(b)\right)$, leading to a utilization rate of $\frac{s(b)\cdot a+1}{s(b)+1}$.
% $\mathbf{W}_{int}^l$ is

% W分布，WX的个数
To clearly show the low bit-width utilization of the baseline, we analyze the weight distribution and determine the value of \(a\).
We plot the weight distributions of VGG-16 and ResNet20 in Figure~\ref{fig:wrange}, and also label the 1st and 99th percentiles.
In Figure~\ref{fig:wrange}, the smallest value of \( a \) is 0.14 (VGG-16.conv16), and the largest is 0.32 (VGG-16.conv4).
This indicates that under 8-bit quantization, the QP-SNN baseline uses less than half of the assigned bit width, with a minimum of 14.33\% and a maximum of 32.26\%.
This inefficient utilization causes a large number of weights to be quantized to the same integer grid, reducing the discrimination of quantized weights.
This limits the representation capacity of the QP-SNN baseline, leading to decreased performance. The complete weight distributions are provided in Appendix \ref{sec:vanillaWD}.
% In conclusion, directly applying vanilla uniform quantization results in low bit-width utilization, reducing the discrimination of the quantized weights.


\paragraph{ReScaW-based uniform quantization.}
To resolve the limited representation capacity, we propose a simple yet effective weight rescaling (ReScaW) strategy for the QP-SNN baseline that uses bit-width more efficiently.
Specifically, we introduce a scale coefficient $\gamma$ to regulate the weight distribution to a wider range before quantization.
The proposed ReScaW strategy is defined as,
\begin{align}
\mathbf{W}^l_{scaled} = \frac{\mathbf{W}^l}{\gamma}.
\end{align}
The scaling coefficient $\gamma$ can assume any positive value within the range 0 \textless $\gamma$ \textless 1.
We provide three options for $\gamma$: (1) the maximum absolute value: $max(|\mathbf{W}^l|)$; (2) the maximum absolute value of the $x$-th and $(1\!-\!x)$-th percentiles: $\Psi_x(\mathbf{W}^l)\!=\!\max\left( \left| P_{1-x}(\mathbf{W}^l) \right|, \left| P_{x}(\mathbf{W}^l) \right| \right)$; and (3) $1$-norm mean value: $\frac{\left\| \mathbf{W}^l\right\|_{1}}{|\mathbf{W}^l|}$, where $|\mathbf{W}^l|$ is the number of entries in $\mathbf{W}^l$.
These three options can scale weights to span the range of $[-1,1]$, thereby ensuring more efficient bit width utilization.
% Noteworthy, the latter two options may cause information loss due to the clipping function in quantization, whereas the first option does not.
The impact of these three options on performance will be explored in the experimental section.
Consequently, we formulate the ReScaW-based uniform quantization as,
\begin{align}
\mathbf{W}^l \approx \gamma\cdot\left (\frac{2}{s(b)}\left \lceil \frac{s(b)}{2}\cdot\left ({\mathrm{clamp}(\frac{\mathbf{W}^l}{\gamma};-1,1)+z} \right) \right \rfloor-z\right).
\end{align}
We compare vanilla uniform quantization in the QP-SNN baseline with the ReScaW in Figure \ref{fig:quant-analysis}.
Clearly, the ReScaW method utilizes the allocated bit width more efficiently.
This efficient bit-width utilization preserves the discrimination of quantized weights, enhancing the representation capability and performance of the QP-SNN baseline.


% Firstly, the vanilla uniform quantization have a small data range and always gathers around 0, and thus their signs are highly possible to flip in backward propagation. In contrast, our method balances and standardizes the weight before quantization for stabilize training \cite{qin2020forward}.
% Secondly, the ReScaW-based uniform quantization method can effectively mitigate the issue of bit-width inefficiency, ensuring the preservation of quantized weight distinguishability and further improving the overall performance of the quantized model. 
% applying the scaling coefficient $\gamma$ prior to the quantization operation adjusts the weight parameters to span a broader range without altering their inherent distribution property.



\subsection{Pruning criterion based on the singular value of spike activity}




\begin{wrapfigure}[]{r}{0.5\textwidth}
\vspace{-5mm}
\centering
\includegraphics[scale=0.13]{figs/sca_score_bar.pdf}
\vspace{-2mm}
\caption{SCA assigns different scores to the same kernel for different input samples. The colors in the figure represent the value of importance score.}
\label{fig:scawrong}
\end{wrapfigure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figs/prun_analysis.pdf}
\caption{Proposed pruning criterion based on the singular value of spatiotemporal spike activity.}
\label{fig:prun-analysis}
\end{figure}

% This high sensitivity to input makes accurately identifying and removing unimportant kernels challenging. 
\paragraph{Problem analysis.}
% 虽然xx已经展示使用 Eq.(\ref{xuscore}) 中的剪枝准则能得到性能不错的剪枝模型，但是其性能主要是通过多次的修剪和重生长确保的。单看这个剪枝准则的话，其对输入数据分布有强烈的依赖性。
% 具体而言，这个剪枝准则得到的分数会受到输入样本特定特征的严重影响，而无法准确反映卷积核的内在重要性。
% 因此，使用此剪枝准则存在将重要内核错误地识别为不太重要或反之亦然的风险。这种错误识别可能导致次优修剪决策，从而可能降低剪枝网络的性能。
% 为了说明SCA-based 准则的这个局限性，我们使用多种网络架构在多个数据集上进行了实验，观察在面对不同输入时使用SCA-based准则每个卷积核的得分。如图xx所示，我们展示了VGG16和resnet20网络中的x层，显示在面对不同输入时同一个卷积核获得明显不同的重要性分数，突出了此标准的不稳定性。
The structured pruning work (\cite{li2024towards}) employing the SCA-based criterion can produce high-performance pruned models, but the performance is ensured through multiple iterative pruning and regrowth processes.
% Upon carefully examining the SCA-based criterion in isolation, we observe that it exhibits a high dependence on the input sample, which may affect the reliability of pruning.
In fact, we observe that the SCA-based pruning criterion exhibits a high dependency on inputs.
Specifically, it generates varying importance scores for the same convolutional kernel when processing different inputs.
To prove this observation, we plot the importance scores of different kernels under varying inputs, as shown in Figure \ref{fig:scawrong}.
The strong input dependency of SCA criterion can lead to biases in kernel evaluation, posing a risk of erroneously identifying crucial kernels as insignificant or misjudging unimportant kernels as essential.
These misidentifications can affect the reliability of pruning, ultimately diminishing the performance of the QP-SNN baseline.
Complete importance scores are available in Appendix \ref{sec:sca}.
% This means that SCA can not accurately reflect the intrinsic importance of kernels within SNNs, which affects the reliability of pruning.



\paragraph{SVS-based pruning criterion.}
Several studies suggest that the number of singular values correlates with information richness (\cite{sadek2012svd,baker2005singular,jaradat2021tutorial}).
Inspired by this, we propose a novel pruning criterion for QP-SNN baseline using the singular value of spike activity (SVS) to remove redundant kernels precisely.
As shown in Figure \ref{fig:prun-analysis}, the SVS-based criterion applies singular value decomposition to the average spike matrix over a given time window $T$, defined as,
\begin{align}
\mathbf{S}^{l,f}_{avg}=\frac{1}{T}\sum\nolimits_{t=1}^{T}\mathbf{S}^{l,f}[t]=\mathbf{P}\mathbf{\Sigma}_{h\times w}\mathbf{Q}^{\top},
\end{align}
where $h$, $w$ are the height and width of the spike matrix.
$\mathbf{P} = [\mathbf{p}_1, \ldots, \mathbf{p}_h] \in \mathbb{R}^{h \times h}$ and $\mathbf{Q} = [\mathbf{q}_1, \ldots, \mathbf{q}_w] \in \mathbb{R}^{w \times w}$ are orthogonal matrices representing the left and right singular vectors.
$\mathbf{\Sigma} \in \mathbb{R}^{h \times w}$ is a diagonal matrix containing the singular values of $\mathbf{S}^{l,f}_{avg}$ in descending order, denoted as,
\begin{equation}
\begin{aligned}
\mathbf{\Sigma}&=\mathrm{diag}\left (\sigma _1,\sigma _2,\cdots, \sigma _{min(h,w)}\right),\\
&\text{with  }\sigma _1\geq\sigma _2 \geq \cdots \geq\sigma _{r^*}\geq \epsilon >\sigma _{r^*+1} \geq\cdots\geq\sigma_{min(h,w)}\geq 0.
\end{aligned}
\end{equation}
Here, $\epsilon$ serves as a threshold to distinguish significant and negligible singular values, and it is generally set to a positive value near zero.
Based on the threshold $\epsilon$, $\mathbf{S}^{l,f}_{avg}$ can be expressed as two components: $\mathbf{S}^{l,f}_{avg}=\sum_{i=1}^{r^*} \sigma_i \mathbf{p}_i \mathbf{q}_i^{\top} +\sum_{i={r^*+1}}^{min(H,W)}\sigma_i \mathbf{p}_i \mathbf{q}_i^{\top}$.
The first term captures the core feature dictated by the significant singular values, while the second term reflects potentially noise-related information (\cite{jaradat2021tutorial}).
Based on this decomposition, we define our pruning criterion as,
\begin{align}
\mathrm{Score}(\mathbf{W}^{l,f})=\mathbb{E}_B\left(\sum\nolimits_{i=1}^{min(H,W)}\mathbb{I} (\sigma _i >\epsilon)\right),
\label{eq:svs}
\end{align}
where $\mathbb{E}_B$ denotes the average over the mini-batch, and $\mathbb{I}(\cdot)$ is the indicator function that counts only significant singular values (those exceeding $\epsilon$) for importance evaluation.
We will demonstrate in the experimental section that the proposed SVS-based pruning criterion achieves more accurate removal of unimportant convolutional kernels.
% This criterion evaluates the importance of individual convolutional kernels by analyzing the information content within their resulting spatio-temporal spike matrices.

% \subsection{Overall workflow of QP-SNN}
By integrating the ReScaW strategy and the SVS-based pruning criterion into the baseline, we develop QP-SNN, with its workflow outlined in Appendix \ref{sec:qpalg}.
% In addition, we discuss the hardware feasibility of QP-SNN in Appendix \ref{sec:hard}.
In summary, the QP-SNN is lightweight and hardware-friendly, while also achieving high performance, making it an efficient solution for applications in resource-constrained scenarios.

% By integrating the proposed ReScaW strategy and the SVS-based criterion into baseline, we develop the QP-SNN and summarize its overall workflow in Algorithm \ref{alg1}. 
% We first train a quantized SNN using the proposed ReScaW-based uniform quantization and then evaluate the importance of convolutional kernels with the SVS-based criterion and prune those with lower importance scores based on a predefined channel pruning ratio. 
% Finally, the pruned model is fine-tuned to optimize its performance. 
% This progressive optimization strategy results in an ultra-lightweight SNN that maintains satisfactory performance, offering an efficient solution for resource-limited scenarios like edge computing devices and low-power systems.

% By integrating the ReScaW strategy and SVS-based criterion into the baseline, we develop QP-SNN, with its workflow detailed in Algorithm \ref{alg1}.
% The advantages of QP-SNN can be summarized into two aspects.
% The first is energy efficiency and hardware-friendly, which is achieved by introducing both uniform quantization and structured pruning within SNN.
% The second is high performance, which is achieved by more efficient bit width utilization and more accurate kernel removal.
% Therefore, QP-SNN offers an efficient solution for resource-limited scenarios like edge devices.





 


\section{Experiment}
In this section, we first present the experiment setup. 
Second, we evaluate the performance of QP-SNN by comparing it to existing methods. 
Third, we demonstrate QP-SNN's scalability to complex architectures and tasks.
Finally, we conduct extensive ablation studies to verify the effectiveness of the ReScaW and the SVS-based pruning criterion. 
Additional analyses of quantization-pruning order and QP-SNN's efficiency advantages are provided in Appendices \ref{sec:order} and \ref{sec:efficiency}, respectively.

\subsection{Experiment setup}
We first evaluate our method on image classification tasks, including static datasets like CIFAR~(\cite{krizhevsky2009learning}), TinyImageNet, and ImageNet-1k (\cite{deng2009imagenet}), alongside neuromorphic DVS-CIFAR10~(\cite{li2017cifar10}).
These datasets serve as standard benchmarks in machine learning and neuromorphic computing.
For architecture, we use classical structures VGGNet and Spiking ResNet (\cite{zheng2021going}), with details provided in Table \ref{tab: accuracy}.
We use SEW-ResNet (\cite{fang2021deep}) on ImageNet-1k for a fair comparison with (\cite{shi2023towards}).
As for \(\epsilon\) in Eq.(\ref{eq:svs}), we observe minimal variation in the singular values, so we set it to a small value of 1e-6 (\cite{jaradat2021tutorial}).
For the learning of QP-SNN, we use the surrogate gradient and straight-through estimator to handle the nondifferentiability.
We provide additional details in the appendix, with the learning algorithm described in Appendix \ref{sec:alg} and experimental setups in Appendix \ref{sec:exp}.


% \begin{table}[ht]
% \small
% \setlength{\tabcolsep}{1.5pt}
% \renewcommand{\arraystretch}{1.05}
% \caption{Performance comparison on both static image datasets and neuromorphic datasets.}
% \label{tab: accuracy}
% \begin{tabular}{crccccccc}
% \hline \hline
% Dataset & \makecell[c]{Method} & Architecture & Learning & Bits & {Accuracy(\%)[$\uparrow$]} & {Size (MB)[$\downarrow$]} & {Time} \\  \hline \hline 
% \multirow{12}{*}{\rotatebox{90}{CIFAR-10}}
% & \multirow{2}{*}{\makecell[r]{\cite{chowdhury2021spatio}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{IJCNN21}}\right ]$}}} & \multirow{2}{*}{VGG-9} &{Hybrid} & 32 & 90.02  & 44.52 & 100 \\
% & &  &Hybrid& 5 & 88.60 & 12.59 & 25 \\
% &\multirow{2}{*}{\makecell[r]{\cite{deng2021comprehensive}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{TNNLS21}}\right ]$}}} & \multirow{2}{*}{7Conv2FC} &{Direct} & 32 & 90.19 & 62.16 & {8} \\
% & & & Direct& 3 & 87.59  & 5.84 & 8 \\ \cline{2-8}
% &\multirow{2}{*}{\makecell[r]{\cite{shi2023towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICLR24}}\right ]$}}} & \multirow{2}{*}{6Conv2FC} &{Direct} & 32 & 92.63  & 50.28 & {8} \\
% & &  &Direct& 32 & 90.65 & 28.40 &8  \\
% &\multirow{2}{*}{\makecell[r]{\cite{li2024towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICML24}}\right ]$}}} & \multirow{2}{*}{VGG-16} &{Direct} & 32 & 91.67  & 17.32 &{4} \\
% & &  & Direct&32 & 90.26 & 5.68 &4  \\ \cline{2-8} 
%  & \multirow{4}{*}{\textbf{QP-SNN}} & \multirow{2}{*}{ResNet20} & Direct & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{95.12}, \textcolor[HTML]{4169E1}{95.41}, \textcolor{cyan}{95.06}; &  \textcolor{blue}{6.27}, \textcolor[HTML]{4169E1}{3.16}, \textcolor{cyan}{1.61} & 2 \\
%  &  &  & Direct & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{94.56}, \textcolor[HTML]{4169E1}{94.65}, \textcolor{cyan}{94.44}; & \textcolor{blue}{3.92}, \textcolor[HTML]{4169E1}{1.98}, \textcolor{cyan}{1.02} & 2 \\
%  &  & \multirow{2}{*}{VGG-16} & Direct & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{91.98}, \textcolor[HTML]{4169E1}{91.90}, \textcolor{cyan}{91.61}; & \textcolor{blue}{4.28}, \textcolor[HTML]{4169E1}{2.16}, \textcolor{cyan}{1.10} & 4 \\
%  &  &  & Direct & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{91.30}, \textcolor[HTML]{4169E1}{91.19}, \textcolor{cyan}{90.59}; & \textcolor{blue}{1.45}, \textcolor[HTML]{4169E1}{0.74}, \textcolor{cyan}{0.39}  & 4 \\ 
% \hline \hline 
% \multirow{12}{*}{\rotatebox{90}{CIFAR-100}} 
% & \multirow{2}{*}{\makecell[r]{\cite{chowdhury2021spatio}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{IJCNN21}}\right ]$}}} & \multirow{2}{*}{VGG-11} &Hybrid& 32 & 67.80  & 75.90 & 50 \\
% & &  &Hybrid & 5 & 66.20  & 25.43 & 30 \\
% & \multirow{2}{*}{\makecell[r]{\cite{deng2021comprehensive}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{TNNLS21}}\right ]$}}} & \multirow{2}{*}{7Conv2FC} &Direct & 3 & 57.83  & 11.75 & {8} \\
% & &  &Direct & 1 & 55.95  & 5.99 & 8  \\ \cline{2-8}
% &\multirow{2}{*}{\makecell[r]{\cite{shi2023towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICLR24}}\right ]$}}} & \multirow{2}{*}{ResNet18} &Direct & 32 & 72.34  & 13.18 & {4} \\
% & &  &Direct& 32 & 70.45  & 7.67 &4  \\
% & \multirow{2}{*}{\makecell[r]{\cite{li2024towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICML24}}\right ]$}}} & \multirow{2}{*}{VGG-16} & Direct & 32 & 65.53 & 14.40 & {4} \\
% & &  & Direct & 32 & 64.64 & 9.48 & 4 \\ \cline{2-8}
%  & \multirow{4}{*}{\textbf{QP-SNN}} & \multirow{2}{*}{ResNet20} & Direct & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{75.29}, \textcolor[HTML]{4169E1}{75.77}, \textcolor{cyan}{75.13}; & \textcolor{blue}{6.45}, \textcolor[HTML]{4169E1}{3.35}, \textcolor{cyan}{1.79}  & 2 \\
%  &  &  & Direct & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{74.78}, \textcolor[HTML]{4169E1}{74.73}, \textcolor{cyan}{73.89}; & \textcolor{blue}{4.10}, \textcolor[HTML]{4169E1}{2.17}, \textcolor{cyan}{1.20}  & 2 \\
%  &  & \multirow{2}{*}{VGG-16} & Direct & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{66.69}, \textcolor[HTML]{4169E1}{66.21}, \textcolor{cyan}{65.69}; & \textcolor{blue}{2.48}, \textcolor[HTML]{4169E1}{1.35}, \textcolor{cyan}{0.79} & 4 \\
%  &  &  & Direct & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{64.70}, \textcolor[HTML]{4169E1}{64.22}, \textcolor{cyan}{63.08}; &  \textcolor{blue}{1.85}, \textcolor[HTML]{4169E1}{1.04}, \textcolor{cyan}{0.63} & 4 \\ 
% \hline\hline
% \multirow{5}{*}{\rotatebox{90}{TinyImageNet}}
% & \makecell[r]{\cite{kundu2021spike}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{WACV21}}\right ]$}} & {VGG-16} &Conversion & 32 & 52.70 & 24.21 & 150 \\
% & \multirow{2}{*}{\makecell[r]{\cite{li2024towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICML24}}\right ]$}}} & \multirow{2}{*}{VGG-16} &Direct& 32 & 49.36 & 27.92
%  & 4 \\
% & &  &Direct& 32 & 49.14 & 19.76 & 4  \\ \cline{2-8}
% & \multirow{2}{*}{\textbf{QP-SNN}} & \multirow{2}{*}{VGG-16} &Direct& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & 
% \textcolor{blue}{53.32}, \textcolor[HTML]{4169E1}{53.11}, \textcolor{cyan}{53.07}; & \textcolor{blue}{5.90}, \textcolor[HTML]{4169E1}{3.78}, \textcolor{cyan}{2.72} & {4} \\
% & &  &Direct& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{51.99}, \textcolor[HTML]{4169E1}{51.78}, \textcolor{cyan}{51.67}; &\textcolor{blue}{4.67}, \textcolor[HTML]{4169E1}{3.17}, \textcolor{cyan}{2.41}  & 4  \\  \hline \hline 
% \multirow{4}{*}{\rotatebox{90}{ImageNet}}
% & \multirow{3}{*}{\makecell[r]{\cite{shi2023towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICLR24}}\right ]$}}} &\multirow{3}{*}{ResNet18} &Direct& 32 & 61.89 & 15.72 & 4 \\
% & & &Direct& 32 & 60.00 & 12.40 & 4 \\
% & &  &Direct& 32 & 58.99 & 10.48 & 4  \\  \cline{2-8}
% & \multirow{2}{*}{\textbf{QP-SNN}} & \multirow{2}{*}{ResNet18} &Direct& \textcolor{blue}{8} & \textcolor{blue}{61.36} & \textcolor{blue}{13.28} & {4} \\
% % & & &Direct & \textcolor{blue}{8} & & \textcolor{blue}{10.46} &{4} \\
% & & &Direct&  \textcolor[HTML]{4169E1}{4} & \textcolor[HTML]{4169E1}{58.06} &  \textcolor[HTML]{4169E1}{7.71} &{4} \\
% \hline \hline 
% \multirow{7}{*}{\rotatebox{90}{DVS-CIFAR10}}
% & \multirow{2}{*}{\makecell[r]{\cite{shi2023towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICLR24}}\right ]$}}} & \multirow{2}{*}{VGGSNN} &Direct& 32 & 81.90 &14.08 & 10 \\
% & &  &Direct& 32 & 78.30 &7.24 &10  \\
% & \multirow{2}{*}{\makecell[r]{\cite{li2024towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICML24}}\right ]$}}} & \multirow{2}{*}{5Conv1FC} &Direct& 32 & 73.00 &3.92 & {20} \\
% & &  &Direct& 32 & 71.90 & 0.32
%  & 20  \\ \cline{2-8}
% & \multirow{3}{*}{\textbf{QP-SNN}} & \multirow{3}{*}{VGGSNN} &Direct& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{82.10}, \textcolor[HTML]{4169E1}{81.80}, \textcolor{cyan}{81.30}; & \textcolor{blue}{1.61}, \textcolor[HTML]{4169E1}{0.90}, \textcolor{cyan}{0.55} & {10} \\
% & &  &Direct& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{81.50}, \textcolor[HTML]{4169E1}{80.90}, \textcolor{cyan}{80.50}; &\textcolor{blue}{1.05}, \textcolor[HTML]{4169E1}{0.62}, \textcolor{cyan}{0.41}  & 10  \\
% & &  &Direct& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{75.90}, \textcolor[HTML]{4169E1}{75.40}, \textcolor{cyan}{74.90}; & \textcolor{blue}{0.40}, \textcolor[HTML]{4169E1}{0.29}, \textcolor{cyan}{0.24} & 10 \\  \hline \hline 
% \end{tabular} 
% % \vspace{-1cm}
% \end{table} 




\begin{table}[ht]
\small
\centering
\setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{1.05}
\caption{Performance comparison on static and neuromorphic datasets. \textbf{Note:} `H', `D', and `C' represent hybrid, direct, and conversion learning, respectively. `HardF' denotes `hardware-friendly'.}
\label{tab: accuracy}
\begin{tabular}{crcccccccc}
\hline \hline
Dataset & \makecell[c]{Method} & Network  & \makecell[c]{Train}& Bits & HardF & {Accuracy(\%)} & {Timestep} & {Size (MB)}  \\  \hline \hline 
\multirow{12}{*}{\rotatebox{90}{CIFAR-10}}
& \multirow{2}{*}{\makecell[r]{\cite{chowdhury2021spatio}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{IJCNN21}}\right ]$}}} & \multirow{2}{*}{VGG-9}  & H & 32 &{\Checkmark} & 90.02  & 100 & 44.52  \\
& &  & H & 5 &\Checkmark & 88.60 & 25 & 12.59  \\
&\multirow{2}{*}{\makecell[r]{\cite{deng2021comprehensive}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{TNNLS21}}\right ]$}}} & \multirow{2}{*}{7Conv2FC} & D  & 32 &{\XSolidBrush} & 90.19 & {8} & 62.16  \\
& & & D & 3 & \XSolidBrush & 87.59  & 8 & 5.84  \\ \cline{2-9}
&\multirow{2}{*}{\makecell[r]{\cite{shi2023towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICLR24}}\right ]$}}} & \multirow{2}{*}{6Conv2FC} & D  & 32 &{\XSolidBrush} & 92.63  & {8} & 50.28  \\
& &  & D & 32 &\XSolidBrush & 90.65 &8 & 28.40   \\
&\multirow{2}{*}{\makecell[r]{\cite{li2024towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICML24}}\right ]$}}} & \multirow{2}{*}{VGG-16} & D  & 32 &{\Checkmark} & 91.67  &{4} & 17.32  \\
& &  & D &32 & \Checkmark & 90.26 &4 & 5.68   \\ \cline{2-9} 
& \multirow{4}{*}{\textbf{Proposed QP-SNN}} & \multirow{2}{*}{ResNet20} & D  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} & \Checkmark & \textcolor{blue}{95.12}, \textcolor[HTML]{4169E1}{95.41}, \textcolor{cyan}{95.06} & 2 &  \textcolor{blue}{6.27}, \textcolor[HTML]{4169E1}{3.16}, \textcolor{cyan}{1.61}  \\
&  &  & D  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} & \Checkmark & \textcolor{blue}{94.56}, \textcolor[HTML]{4169E1}{94.65}, \textcolor{cyan}{94.44} & 2 & \textcolor{blue}{3.92}, \textcolor[HTML]{4169E1}{1.98}, \textcolor{cyan}{1.02} \\
&  & \multirow{2}{*}{VGG-16} & D  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} & \Checkmark & \textcolor{blue}{91.98}, \textcolor[HTML]{4169E1}{91.90}, \textcolor{cyan}{91.61} & 4 & \textcolor{blue}{4.28}, \textcolor[HTML]{4169E1}{2.16}, \textcolor{cyan}{1.10}  \\
&  &  & D  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} & \Checkmark & \textcolor{blue}{91.30}, \textcolor[HTML]{4169E1}{91.19}, \textcolor{cyan}{90.59} & 4 & \textcolor{blue}{1.45}, \textcolor[HTML]{4169E1}{0.74}, \textcolor{cyan}{0.39} \\ 
\hline \hline 
\multirow{12}{*}{\rotatebox{90}{CIFAR-100}} 
& \multirow{2}{*}{\makecell[r]{\cite{chowdhury2021spatio}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{IJCNN21}}\right ]$}}} & \multirow{2}{*}{VGG-11} & H & 32 &\Checkmark & 67.80  & 50 & 75.90 \\
& & & H & 5  &\Checkmark & 66.20  & 30 & 25.43\\
& \multirow{2}{*}{\makecell[r]{\cite{deng2021comprehensive}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{TNNLS21}}\right ]$}}} & \multirow{2}{*}{7Conv2FC} & D & 3 &\XSolidBrush & 57.83  & {8} & 11.75\\
& &  & D & 1 &\XSolidBrush & 55.95  & 8 & 5.99 \\ \cline{2-9}
&\multirow{2}{*}{\makecell[r]{\cite{shi2023towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICLR24}}\right ]$}}} & \multirow{2}{*}{ResNet18} & D & 32 &\XSolidBrush & 72.34  & {4} & 13.18  \\
& &  & D & 32 &\XSolidBrush & 70.45  &4 & 7.67 \\
& \multirow{2}{*}{\makecell[r]{\cite{li2024towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICML24}}\right ]$}}} & \multirow{2}{*}{VGG-16} & D & 32 & \Checkmark & 65.53 & {4} & 14.40 \\
& &  & D & 32 & \Checkmark & 64.64 & 4 & 9.48  \\ \cline{2-9}
& \multirow{4}{*}{\textbf{Proposed QP-SNN}} & \multirow{2}{*}{ResNet20} & D & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} & \Checkmark & \textcolor{blue}{75.29}, \textcolor[HTML]{4169E1}{75.77}, \textcolor{cyan}{75.13} & 2 & \textcolor{blue}{6.45}, \textcolor[HTML]{4169E1}{3.35}, \textcolor{cyan}{1.79}   \\
&  &  & D & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} & \Checkmark & \textcolor{blue}{74.78}, \textcolor[HTML]{4169E1}{74.73}, \textcolor{cyan}{73.89} & 2 & \textcolor{blue}{4.10}, \textcolor[HTML]{4169E1}{2.17}, \textcolor{cyan}{1.20} \\
&  & \multirow{2}{*}{VGG-16} & D & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} & \Checkmark & \textcolor{blue}{66.69}, \textcolor[HTML]{4169E1}{66.21}, \textcolor{cyan}{65.69} & 4 & \textcolor{blue}{2.48}, \textcolor[HTML]{4169E1}{1.35}, \textcolor{cyan}{0.79}  \\
&  &  & D & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} & \Checkmark & \textcolor{blue}{64.70}, \textcolor[HTML]{4169E1}{64.22}, \textcolor{cyan}{63.08} & 4 &  \textcolor{blue}{1.85}, \textcolor[HTML]{4169E1}{1.04}, \textcolor{cyan}{0.63} \\ 
\hline\hline
\multirow{5}{*}{\rotatebox{90}{TinyImageNet}}
& \makecell[r]{\cite{kundu2021spike}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{WACV21}}\right ]$}} & {VGG-16}  & C & 32 &\XSolidBrush & 52.70 & 150 & 24.21  \\
& \multirow{2}{*}{\makecell[r]{\cite{li2024towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICML24}}\right ]$}}} & \multirow{2}{*}{VGG-16} & D& 32  &\Checkmark & 49.36 & 4 & 27.92 \\
& &  & D& 32 &\Checkmark & 49.14 & 4 & 19.76 \\ \cline{2-9}
& \multirow{2}{*}{\textbf{Proposed QP-SNN}} & \multirow{2}{*}{VGG-16} & D& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} &\Checkmark &  \textcolor{blue}{53.32}, \textcolor[HTML]{4169E1}{53.11}, \textcolor{cyan}{53.07}  & {4} & \textcolor{blue}{5.90}, \textcolor[HTML]{4169E1}{3.78}, \textcolor{cyan}{2.72} \\
& &  & D& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} &\Checkmark & \textcolor{blue}{51.99}, \textcolor[HTML]{4169E1}{51.78}, \textcolor{cyan}{51.67} & 4 &\textcolor{blue}{4.67}, \textcolor[HTML]{4169E1}{3.17}, \textcolor{cyan}{2.41} \\  \hline \hline 
\multirow{5}{*}{\rotatebox{90}{ImageNet}}
& \multirow{3}{*}{\makecell[r]{\cite{shi2023towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICLR24}}\right ]$}}} &\multirow{3}{*}{ResNet18} & D& 32 &\XSolidBrush & 61.89 & 4 & 15.72  \\
& & & D& 32 &\XSolidBrush & 60.00 & 4 & 12.40  \\
& &  & D& 32 &\XSolidBrush & 58.99 & 4 & 10.48  \\  \cline{2-9}
& \multirow{2}{*}{\textbf{Proposed QP-SNN}} & \multirow{2}{*}{ResNet18} & D& \textcolor{blue}{8} &\Checkmark & \textcolor{blue}{61.36} & {4} & \textcolor{blue}{13.28}  \\
% & & &D & \textcolor{blue}{8} & & \textcolor{blue}{10.46} &{4} \\
& & & D&  \textcolor[HTML]{4169E1}{4} &\Checkmark & \textcolor[HTML]{4169E1}{58.06} &{4} & \textcolor[HTML]{4169E1}{7.71} \\
\hline \hline 
\multirow{7}{*}{\rotatebox{90}{DVS-CIFAR10}}
& \multirow{2}{*}{\makecell[r]{\cite{shi2023towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICLR24}}\right ]$}}} & \multirow{2}{*}{VGGSNN} & D& 32 &\XSolidBrush & 81.90 & 10 &14.08  \\
& &  & D & 32 &\XSolidBrush & 78.30 &10 &7.24   \\
& \multirow{2}{*}{\makecell[r]{\cite{li2024towards}\\ \scriptsize{$\left [  \textcolor{gray}{\textit{ICML24}}\right ]$}}} & \multirow{2}{*}{5Conv1FC} & D& 32 &\Checkmark & 73.00 & {20} &3.92  \\
& &  & D& 32 &\Checkmark & 71.90 & 20 & 0.32   \\ \cline{2-9}
& \multirow{3}{*}{\textbf{Proposed QP-SNN}} & \multirow{3}{*}{VGGSNN} & D& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} &\Checkmark & \textcolor{blue}{82.10}, \textcolor[HTML]{4169E1}{81.80}, \textcolor{cyan}{81.30} & {10} & \textcolor{blue}{1.61}, \textcolor[HTML]{4169E1}{0.90}, \textcolor{cyan}{0.55} \\
& &  & D& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} &\Checkmark & \textcolor{blue}{81.50}, \textcolor[HTML]{4169E1}{80.90}, \textcolor{cyan}{80.50} & 10 &\textcolor{blue}{1.05}, \textcolor[HTML]{4169E1}{0.62}, \textcolor{cyan}{0.41}  \\
& &  & D& \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2} &\Checkmark & \textcolor{blue}{75.90}, \textcolor[HTML]{4169E1}{75.40}, \textcolor{cyan}{74.90} & 10 & \textcolor{blue}{0.40}, \textcolor[HTML]{4169E1}{0.29}, \textcolor{cyan}{0.24} \\  \hline \hline 
\end{tabular} 
% \vspace{-1cm}
\end{table}




% \begin{table}[]
% \small
% \setlength{\tabcolsep}{7pt}
% \renewcommand{\arraystretch}{1.18}
% \caption{Performance comparison on both static image datasets and neuromorphic datasets.}
% \begin{tabular}{llllllll}
% \hline  \hline
% Method & Network & Bit & Accuracy(\%) & Params & Model size & Time \\  \hline
% \multicolumn{7}{c}{CIFAR-10} \\  \hline
% \multirow{2}{*}{\cite{chowdhury2021spatio}} & \multirow{2}{*}{VGG-9} & 32 & 90.02 & 11.13M & & 100 \\
%  &  & 5 & 88.60 & 11.13M &  & 25 \\
% \multirow{2}{*}{\cite{deng2021comprehensive}} & \multirow{2}{*}{7Conv2FC} & 32 & 90.19 & 15.54M &  & {8} \\
%  &  & 3 & 87.59 & 15.54M &  & 8 \\
% \multirow{2}{*}{\cite{shi2023towards}} & \multirow{2}{*}{6Conv2FC} & 32 & 92.63 & 12.57M &  & {8} \\
%  &  & 32 & 90.65 & 7.10M &  &8  \\
% \multirow{2}{*}{\cite{li2024towards}} & \multirow{2}{*}{VGG-16} & 32 & 91.67 & 4.33M &  &{4} \\
%  &  & 32 & 90.26 & 1.42M &  &4  \\ 
% \multirow{4}{*}{QP-SNN} & \multirow{2}{*}{VGG-16} & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; &
% \textcolor{blue}{91.98}, \textcolor[HTML]{4169E1}{91.90}, \textcolor{cyan}{91.61} & 4.25M &  & {4} \\
%  &  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{91.30}, \textcolor[HTML]{4169E1}{91.19}, \textcolor{cyan}{90.59} & 1.42M &91.30, 91.19, 90.59  &4  \\ 
%  & \multirow{2}{*}{ResNet20} & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & 
%  \textcolor{blue}{95.12}, \textcolor[HTML]{4169E1}{95.41}, \textcolor{cyan}{95.06} & 6.22M &  & {2} \\
%  &  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{94.56}, \textcolor[HTML]{4169E1}{94.65}, \textcolor{cyan}{94.44} & 3.87M &  &2  \\  \hline
% \multicolumn{7}{c}{CIFAR-100} \\  \hline
% \multirow{2}{*}{\cite{chowdhury2021spatio}} & \multirow{2}{*}{VGG-11} & 32 & 67.80 & 18.98M &  & 50 \\
%  &  & 5 & 66.20 & 18.98M &  & 30 \\
% \multirow{2}{*}{\cite{deng2021comprehensive}} & \multirow{2}{*}{7Conv2FC} & 3 & 57.83 & 31.08M &  & {8} \\
%  &  & 1 & 55.95 & 46.62M &  & 8  \\
% \multirow{2}{*}{\cite{li2024towards}} & \multirow{2}{*}{VGG-16} & 32 & 65.53 & 3.60M &  & {4} \\
%  &  & 32 & 64.64 & 2.37M &  & 4 \\ 
% \multirow{4}{*}{QP-SNN} & \multirow{2}{*}{VGG-16} & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{66.69}, \textcolor[HTML]{4169E1}{66.21}, \textcolor{cyan}{65.69} & 2.31M &  & {4} \\
% & \multirow{2}{*}{ResNet20} & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{75.29}, \textcolor[HTML]{4169E1}{75.77}, \textcolor{cyan}{75.13} & 6.27M &  &{2} \\
% &  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{74.78}, \textcolor[HTML]{4169E1}{74.73}, \textcolor{cyan}{73.89} & 3.92M &  &  2\\  \hline
% \multicolumn{7}{c}{Tiny ImageNet} \\  \hline
% \multirow{2}{*}{\cite{li2024towards}} & \multirow{2}{*}{VGG-16} & 32 & 49.36 & 6.98M &  & 4 \\
% &  & 32 & 49.14 & 4.94M &  & 4  \\
% \multirow{2}{*}{QP-SNN} & \multirow{2}{*}{VGG-16} & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & 
% \textcolor{blue}{53.32}, \textcolor[HTML]{4169E1}{53.11}, \textcolor{cyan}{53.07} & 4.65M &  & {4} \\
% &  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{51.99}, \textcolor[HTML]{4169E1}{51.78}, \textcolor{cyan}{51.67} & 3.43M &  & 4  \\  \hline
% \multicolumn{7}{c}{ImageNet} \\  \hline
% \multirow{2}{*}{\cite{shi2023towards}} & \multirow{2}{*}{ResNet18} & 32 & 61.89 & 3.93M & 15.72MB & 4 \\
% &  & 32 & 58.99 & 2.62M & 10.48MB & 4  \\ 
% {QP-SNN} & {ResNet18} & 8 & 61.32  & 11.68M & 10.46MB & {4} \\  \hline
% \multicolumn{7}{c}{DVS-CIFAR10} \\  \hline
% \multirow{2}{*}{\cite{shi2023towards}} & \multirow{2}{*}{VGGSNN} & 32 & 81.90 & 3.52M &  & 10 \\
%  &  & 32 & 78.30 & 1.81M &  &10  \\
% \multirow{2}{*}{\cite{li2024towards}} & \multirow{2}{*}{5Conv1FC} & 32 & 73.00 & 0.98M &  & {20} \\
%  &  & 32 & 71.90 & 0.08M &  & 20  \\ \hline
% \multirow{3}{*}{QP-SNN} & \multirow{3}{*}{VGGSNN} & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{82.10}, \textcolor[HTML]{4169E1}{81.80}, \textcolor{cyan}{81.30} & 1.46M &  & {10} \\
% &  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{81.50}, \textcolor[HTML]{4169E1}{80.90}, \textcolor{cyan}{80.50} & 0.90M &  & 10  \\
% &  & \textcolor{blue}{8}, \textcolor[HTML]{4169E1}{4}, \textcolor{cyan}{2}; & \textcolor{blue}{75.90}, \textcolor[HTML]{4169E1}{75.40}, \textcolor{cyan}{74.90} & 0.25M &  & 10 \\  \hline
% \end{tabular}  
% \end{table}


\subsection{Performance comparison}



As shown in Table \ref{tab: accuracy}, we compare QP-SNN with related work in performance and model size to prove the effectiveness and efficiency. 
We use 8, 4, and 2-bit weight configurations in experiments across all datasets.
Compared to ANN2SNN conversion and hybrid algorithms, QP-SNN achieves top-1 performance with fewer timesteps, such as 2 or 4 on static datasets.
When compared to direct algorithms, QP-SNN also performs well.
On CIFAR-10 and CIFAR-100, QP-SNN outperforms previous methods (\cite{shi2023towards, li2024towards}) with smaller models and shorter timesteps (e.g., CIFAR-10: 1.61 MB, Acc=95.06\%, T=2; CIFAR-100: 1.79 MB, Acc=75.13\%, T=2).
On TinyImageNet, using the same timesteps and architecture, QP-SNN reduces model size by 90.26\% and increase accuracy by 3.71\% compared to (\cite{li2024towards}).
On ImageNet, we are the first study to report results for structured pruning in SNNs, achieving comparable performance to unstructured pruning method (\cite{shi2023towards}) with a 15.55\% reduction in model size.
On DVS-CIFAR10, QP-SNN achieves an 88.55\% smaller model size and a 0.2\% higher accuracy compared to (\cite{shi2023towards}).
To intuitively demonstrate QP-SNN’s improvements, we plot comparison results in Figure \ref{fig:bubble}.
These results show that QP-SNN achieves superior results in both efficiency and performance, positioning it as a leading approach for compact and high-performance SNNs.


\subsection{Scalability to complex architectures and tasks}

QP-SNN can be extended to complex architectures like Transformer and complex tasks like object detection.
To validate this scalability, we conduct two additional experiments: (1) using the Spikingformer (\cite{zhou2023spikingformer}) architecture and (2) applying QP-SNN to object detection task.

\textbf{Scalability to the Spikingformer.}
We use the Spikingformer-4-384 architecture and evaluate it on the CIFAR-100 dataset.
The training setups are consistent with (\cite{zhou2023spikingformer}).
As shown in Table \ref{tab:spikformer}, QP-SNN reduces model size by 8.28$\times$, SOPs by 2.25$\times$, and power consumption by 2.25$\times$, while achieving a competitive accuracy of 76.94\%. These results demonstrate the effectiveness of QP-SNN in compressing complex Spiking Transformer architectures.

\vspace{-0.5cm}
\begin{table}[h]
\centering
\small
{\caption{Performance of QP-SNN on CIFAR-100 with the Spikingformer architecture.}
\label{tab:spikformer}
\tabcolsep=0.07cm
\begin{tabular}{c|ccccccc}
   \toprule 
Architecture & Method & Connection & Bit  & Model size (MB) & SOPs (M) & Power (mJ) & Accuracy (\%) \\\midrule
Spikingformer &\footnotesize \cite{zhou2023spikingformer} & 100\% & 16 & 18.64 \tiny \textcolor{blue}{base} & 292.14  \tiny \textcolor{blue}{base} & 0.266 \textcolor{blue}{\tiny base} &  79.09 \textcolor{blue}{\tiny base} \\
Spikingformer &QP-SNN &44.74\%  & 4 & 2.25 \tiny \textcolor{blue}{8.28$\times$} & 130.05 \tiny \textcolor{blue}{2.25$\times$} & 0.118 \tiny \textcolor{blue}{2.25$\times$}& 76.94 \tiny \textcolor{blue}{-2.15} \\ \bottomrule
\end{tabular}}
\end{table}



\textbf{Scalability to object detection.}
We conduct object detection experiments on two remote sensing datasets: SSDD (\cite{wang2019sar}) and NWPU VHR-10 (\cite{cheng2017remote}). SSDD consists of synthetic aperture radar images for ship detection, while NWPU VHR-10 is a high-resolution remote sensing dataset containing ten object classes.
We use the YOLO-v3 detection architecture with ResNet10 as the backbone. During training, we apply pruning to the backbone and optimize the model using SGD with a polynomial decay learning rate schedule, initializing at 1e-2 for 300 epochs. 
As shown in Table \ref{tab:oj}, QP-SNN significantly reduces model size while maintaining satisfactory detection performance, demonstrating its potential for more challenging tasks.
\vspace{-0.5cm}
\begin{table}[h]
\centering
\renewcommand\arraystretch{1}
\tabcolsep=0.43cm
{\caption{Object detection results of QP-SNN on SSDD and NWPU VHR-10.}
\label{tab:oj}
\begin{tabular}{c|ccccc}
\toprule
Dataset &Method & Bit & Model size (MB) & mAP@0.5 (\%)\\ \midrule
\multirow{2}{*}{\makecell[c]{SSDD}} & Full-precision  & 32 &  19.29 \tiny \textcolor{blue}{base}  &  96.80 \tiny \textcolor{blue}{base}   \\
 & QP-SNN   & 4 &  2.15 \tiny \textcolor{blue}{8.97$\times$}   & 97.10 \tiny \textcolor{blue}{+0.3} \\ \midrule
\multirow{2}{*}{\makecell[c]{NWPU VHR-10}} & Full-precision  & 32 &  19.29 \tiny \textcolor{blue}{base} &  89.89 \tiny \textcolor{blue}{base}   \\
 & QP-SNN   & 4 &  2.15  \tiny \textcolor{blue}{8.97$\times$}  & 86.68 \tiny \textcolor{blue}{-3.21} \\
\bottomrule
\end{tabular}}
\end{table}
\vspace{-0.3cm}

% \begin{figure}[t]
% \centering
% \includegraphics[width=1.0\linewidth]{ICLR 2025 Template/figs/nwpu.pdf}
% \label{fig:ssddob}
% \caption{The detection result of QP-SNN on the NWPU-10 dataset.}
% \end{figure}

\subsection{Ablation study} 
To prove the effectiveness of QP-SNN, we conduct extensive ablation studies. 
Firstly, we analyze the three options for $\gamma$ in the ReScaW strategy to select the optimal one.
Then, we perform thorough ablation experiments to validate the effectiveness of the proposed ReScaW strategy and SVS-based pruning criterion.
Finally, we visualize the effect of the ReScaW strategy and the SVS-based criterion to demonstrate that they have effectively addressed the above mentioned issues.
All ablation experiments are conducted on the CIFAR-100 dataset using ResNet20 with 1.20 MB model size.


\textbf{Analysis of three options for $\gamma$.}
We compare the performance of quantized SNNs (not involve pruning process) with different $\gamma$ settings to determine the optimal one as the default experimental setting.
As depicted in Figure \ref{fig:gamma}, the quantized SNN using $max(|\mathbf{W}^l|)$ achieves an accuracy of 77.85\%, the one using $\Psi_x(\mathbf{W}^l)$ achieves an accuracy of 77.8\%, and the one using ${\left\| \mathbf{W}^l\right\|_{1}}/{|\mathbf{W}^l|}$ achieves an accuracy of 79.16\%.
Clearly, the accuracy differences between them are minimal, with the $1$-norm mean value performs best. This may be because ${\left\| \mathbf{W}^l\right\|_{1}}/{|\mathbf{W}^l|}$ can effectively capture the characteristics of the full precision distribution (\cite{rastegari2016xnor, qin2020binary}).
Therefore, we choose the $1$-norm mean value as the default experimental setting.

% All experiments are conducted on CIFAR-100 using VGG-16 with an 8-bit width, maintaining a consistent channel pruning ratio.
% \begin{table}[h]
%     \raggedleft
%     \centering
%     \caption{Ablation study on the effectiveness of the proposed methods: ReScaW Quant and SVS. \textbf{Note:} `Increment' indicates accuracy improvement compared to the specified comparison model.}
%     \label{tab:ablation}
%     \begin{tabular}{l|ccc}
%     \hline\hline
%         \makecell[c]{Model} & Accuracy(\%) & Compared to& Increment(\%) \\ \hline
%         \textit{A.}  Only Vanilla Quant & 78.53 & - & -\\ 
%         \textit{B.}  Only ReScaW Quant & 79.16  &  \textit{A.}& 0.63 $\uparrow$\\ \hline
%         \textit{C.}  Vanilla Quant, SCA & 69.16 & - &-\\ 
%         \textit{D.}  Vanilla Quant, SVS & 73.32  &\textit{C.} & 4.16 $\uparrow$\\ 
%         \textit{E.}  ReScaW Quant, SCA & 73.40  &\textit{C.} & 4.24 $\uparrow$\\ 
%         \textit{F.}  ReScaW Quant, SVS & 73.89  &\textit{C.} & 4.73 $\uparrow$\\ \hline\hline
%     \end{tabular}
% \end{table}



\textbf{Effectiveness of two proposed methods.}
As shown in Table \ref{tab:ablation}, we conduct extensive ablation experiments to validate the effectiveness of two proposed methods in QP-SNN, i.e., the ReScaW strategy and the SVS-based pruning criterion. 
First, we demonstrate the effectiveness of the ReScaW strategy.
\textit{A} and \textit{B} are models that apply vanilla and ReScaW quantization for SNN respectively, without involving pruning process.
Their comparison shows that ReScaW-based quantization improves the performance of the quantized SNN by 0.63\% over vanilla quantization, highlighting its effectiveness.
Moreover, the comparison between Models \textit{C} and \textit{D} indicates that merely replacing the quantization in the baseline also results in a significant performance gain of 4.24\%, further validating the ReScaW strategy.
Second, we demonstrate the effectiveness of the SVS-based pruning criterion. The comparison between Models \textit{C} and \textit{E} reveals that the SVS criterion enhances the baseline performance by 4.16\%, confirming its ability to remove kernels accurately and preserve model performance.
By integrating these two methods into baseline, the performance is significantly improved by 4.73\%, underscoring their importance for preserving QP-SNN performance.



\textbf{Impact of the ReScaW strategy and the SVS-based pruning criterion.}
To demonstrate that the ReScaW strategy and SVS-based criterion effectively address the previously mentioned issues, we present the weight distribution and importance scores of QP-SNN. 
We depict the weight distribution of QP-SNN in Figure \ref{fig:widerange}.
This indicates that ReScaW-based quantization results in a broader weight distribution compared to vanilla quantization, indicating improved bit-width utilization efficiency.
In addition, Figure \ref{fig:svs_prun} depicts the importance scores of QP-SNN, showing that the SVS-based pruning criterion produces stable scores with minimal fluctuation across different inputs.
This input-insensitive characteristic enables QP-SNN to remove redundant kernels accurately.
Complete visualization of weight distributions and kernel scores are provided in Appendix \ref{sec:rescaWWD} and \ref{sec:svs}.


\begin{figure}[t]
\centering
\begin{minipage}[t]{0.32\linewidth}
\subfigure[Analysis of three options for $\gamma$]{
\includegraphics[width=\linewidth]{figs/gamma.pdf}
\label{fig:gamma}}
\vspace{1cm}
\subfigure[Kernel scores with SVS criterion]{
\includegraphics[width=\linewidth]{figs/svs_score_bar.pdf}
\label{fig:svs_prun}}
\end{minipage}
\hspace{2mm}
\begin{minipage}[t]{0.575\linewidth}
\subfigure[Weight distribution with ReScaW-based quantization]{
\includegraphics[width=\linewidth]{figs/efficientBW.pdf}
\label{fig:widerange}}
\end{minipage}
\vspace{-1.2cm}
\caption{Visualization of ablation experiments.}
\end{figure}


\begin{table}[t]
    \vspace{-0.5cm}
    \raggedleft
    \centering
    \caption{Ablation study on the effectiveness of two proposed methods. \textbf{Note:} `Increment' represents the accuracy improvement relative to the specified model; ` r.w./ ' denotes `replaced with'.}
    \label{tab:ablation}
    \begin{tabular}{cl|ccc}
    \hline\hline
        & \makecell[c]{Model} & Accuracy(\%) & Compared to& Increment(\%) \\ \hline
        \textit{A. }&only vanilla quant & 78.53 & - & -\\ 
        \textit{B. }&only ReScaW quant& 79.16 &  \textit{A.}& 0.63 $\uparrow$\\ \hline
        \textit{C. }&baseline & 69.16 & - &-\\ 
        \textit{D. }&r.w./ ReScaW & 73.40  &\textit{C.} & 4.24 $\uparrow$\\ 
        \textit{E. }&r.w./ SVS & 73.32  &\textit{C.} & 4.16 $\uparrow$\\ 
        \textit{F. }&r.w./ ReScaW $\&$ SVS & 73.89  &\textit{C.} & 4.73 $\uparrow$\\ \hline\hline
    \end{tabular}
    \vspace{-0.5cm}
\end{table}

\section{Conclusion}

SNNs offer energy-efficient solutions for artificial intelligence.
However, the current SNN community focuses mainly on building large-scale SNNs to increase performance, which limits their feasibility in resource-constrained edge devices.
To tackle this limitation, we first developed a QP-SNN baseline using uniform quantization and structured pruning, which significantly reduces resource usage.
Furthermore, we analyzed and addressed the underlying issues of the QP-SNN baseline in quantization and pruning to improve performance.
For quantization, we revealed that the vanilla uniform quantization suffers from limited representation capability due to inefficient bit-width utilization and proposed a weight rescaling strategy to resolve it.
For pruning, we observed that the SCA criterion exhibits low robustness on inputs and introduced a novel criterion using the singular value of spike activity to remove unimportant kernels more accurately.
By integrating the ReScaW and SVS pruning criteria, our QP-SNN achieved superior efficiency and performance, demonstrating its potential for advancing neuromorphic intelligent systems and edge computing.

\section*{Acknowledgments}
This work was supported in part by the National Natural Science Foundation of China under grant U20B2063, 62220106008, and 62106038, the Sichuan Science and Technology Program under Grant 2024NSFTD0034 and 2023YFG0259, the Open Research Fund of the State Key Laboratory of Brain-Machine Intelligence, Zhejiang University (Grant No.BMI2400020).

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
% \section{Hardware deployment feasibility}
% \label{sec:hard}
% % para01: 总述一下，介绍这个section讲什么，讲的逻辑是什么，大概3句话。
% \textcolor[rgb]{0.25, 0.5, 0.9}{In this section, we will focus on how the QP-SNNs are friendly to deployment on hardware platforms. First, we analyze the feasibility of the ReScaW-based uniform quantization. Then we discuss the convenience of the structural pruning scheme on hardware deployment.}
% \par
% % para02: 介绍我们方法的均匀量化的硬件部署可行性分析。这部分可能需要介绍一些BN参数的融合，我们的除法操作在推理的时候也是不存在的
% \textcolor[rgb]{0.25, 0.5, 0.9}{The proposed ReScaW-based uniform quantization could make full use of the restricted intervals. Hence, the model could be represented into lower bits rather than original FP32 precision while preserving its high inference accuracy. During the inference period, the trained weights and batch-normalization parameters are fuzed together. Specifically, we represent the QP-Convolution($\mathcal{QP_{CONV}(\cdot)}$) using Eq.(\ref{qpsnn_fused}): }

% \begin{equation}
% \label{qpsnn_fused}
% \begin{aligned}
%     \mathbf{y}^{l} &= \mathcal{BN}(\mathcal{QP_{CONV}}(\mathbf{S^{l}})) \\
%     &= \mathcal{BN}(\mathbf{W}^l \ast \mathbf{S^{l}} + \mathbf{b^{l}}) \\
%     &= \frac{(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})-\mu(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})}{\sigma(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})}\cdot \mathbf{\gamma_{BN}}^{l}  +\mathbf{\beta}^{l} \\
%     &= \frac{\mathbf{\gamma_{BN}}^{l} \cdot (\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})}{\mathbf{\sigma}(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})} - \frac{\mathbf{\gamma_{BN}}^{l}\cdot\mu(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})}{\mathbf{\sigma}(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})} + \mathbf{\beta}^{l} \\
%     &= \frac{\mathbf{\gamma_{BN}}^{l} \cdot \mathbf{W}^l}{\mathbf{\sigma}(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})} \ast \mathbf{S}^{l} + [\frac{\mathbf{\gamma_{BN}}^{l} \cdot \mathbf{b}^{l} - \mu(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})}{\mathbf{\sigma}(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})} + \mathbf{\beta}^{l}] \\
%     &= \mathbf{W_{f}}^{l} \ast \mathbf{S} + \mathbf{b_{f}}^{l} \\
% \end{aligned}
% \end{equation}

% \textcolor[rgb]{0.25, 0.5, 0.9}{where $\ast$ is convolution symbol, $\cdot$ is multiplication symbol, $\mathbf{S}$ denotes input binary spike train, $\mathbf{W}^l$ and $\mathbf{b}$ are quantized weights and bias (set as zero) of the QP-Convolution, $\mathbf{y}$ denotes the output of the QP-Convolution layer, and $\mathcal{BN}(\cdot)$ denotes the batch normalization. $\mathbf{\gamma_{BN}}$ and $\beta$ are the affine parameters in batch normalization.$\mathbf{W_f^{l}}$ denote the fused weights which can be calculated by Eq.(\ref{fused_Weights}):}
% \begin{equation}
% \label{fused_Weights}
% \begin{aligned}
%     &\mathbf{W_{f}}^{l} = \frac{\mathbf{\gamma_{BN}}^{l} \cdot \mathbf{W}^l}{\mathbf{\sigma}(\mathbf{W}^l \ast \mathbf{S}^{l} + \mathbf{b}^{l})} \\
% \end{aligned}
% \end{equation}

% \textcolor[rgb]{0.25, 0.5, 0.9}{By applying the ReScaW-based  uniform quantization method to the fused weight as wrote in Eq.(\ref{quant_fused_weights}), the model could inference without extra computation costs.}

% \begin{equation}
% \label{quant_fused_weights}
%     \begin{aligned}
%             \mathbf{W}^l_{int} & \approx \gamma\cdot\left (\frac{2}{s(b)}\left \lceil \frac{s(b)}{2}\cdot\left ({\mathrm{clamp}(\frac{\mathbf{W_f}^l}{\gamma};-1,1)+z} \right) \right \rfloor-z\right). \\
%     \end{aligned}
% \end{equation}
% \par

% % para03: 介绍我们的方法的结构化剪枝的硬件部署可行性分析
% \textcolor[rgb]{0.25, 0.5, 0.9}{Compared with unstructured pruning, structured pruning is more suitable for accelerated inference on hardware platforms because the weights obtained are more regular and more friendly to memory access. For instance, CPU and GPU platforms can achieve realistic acceleration and compression with structured pruning by taking advantage of a highly efficient library such as the Basic Linear Algebra Subprograms (BLAS) library (\cite{10330640}). As for Field Programmable Gate Arrays(FPGAs), the model could be deployed on such platforms conveniently since the weights are stored in Block RAMs in a regular form, which is easy to access through consecutive BRAM addresses.}
\newpage

\section{The overall workflow of QP-SNN}
\label{sec:qpalg}

We present the workflow of QP-SNN in Algorithm \ref{alg1}, which consists of two main steps: quantization and pruning. First, the initial SNN model undergoes ReScaW-based uniform quantization, where weights are rescaled and quantized, followed by iterative training with backpropagation. Then, the quantized model is pruned using an SVS-based criterion that analyzes spatiotemporal spike activity to identify and remove less important channels. Finally, the quantized and pruned model is fine-tuned to restore performance.
\begin{algorithm}[h]
\label{alg1}
\small
\caption{The overall workflow of QP-SNN.}
\LinesNumbered
\KwIn{Initial SNN model: $\mathcal{M}=\{\mathbf{W}^1,\cdots,\mathbf{W}^L\}$; Bit width: $b$; Pruning channel ratio: $r$; Number of training epoch: $N_{epoch}$; Number of training iteration per epoch: $I_{train}$.}
\KwOut{The trained QP-SNN $\mathcal{M}_{q\&p}$.}
\textcolor{gray}{$\triangleright$  Step 1: Get quantized SNN $\mathcal{M}_q$ by using the ReScaW-based uniform quantization}\;
\For{$epoch \gets 1$ \KwTo $N_{epoch}$}{
\For{$i \gets 1$ \KwTo $I_{train}$}{
% Sample a mini-batch of training data and target labels ($\mathbf{I}$,$\mathbf{Y}$)\;
\For{$l \gets 1$ \KwTo $L$}{
$\gamma \in \{\max(|\mathbf{W}^l|), \Psi_x(\mathbf{W}^l), \frac{\|\mathbf{W}^l\|_1}{|\mathbf{W}^l|}\}$; \textcolor{gray}{\Comment{The selection is fixed during training}}\;
$\mathbf{W}^l_{scaled} = \frac{\mathbf{W}^l}{\gamma}$;\textcolor{gray}{\Comment{Rescale 32-bit weight parameters to a wide range}}\;
$\mathbf{W}^l \approx \gamma\cdot\left (\frac{2}{s(b)}\left \lceil \frac{s(b)}{2}\cdot\left ({\mathrm{clamp}(\frac{\mathbf{W}^l}{\gamma};-1,1)+z} \right) \right \rfloor-z\right)$\;
\For{$t\gets 1$ \KwTo $T$}{
Calculate $\tilde{\mathbf{U}}^{l}[t]$, $\mathbf{S}^l[t]$, and $\mathbf{U}^{l}[t]$ according to Eq.(1$\sim$ 3)
% $\tilde{\mathbf{U}}^{l}[t]=\tau \mathbf{U}^{l}[t-1]+ \mathbf{W}^l \mathbf{S}^{n-1}[t]$;\textcolor{gray}{\Comment{$\mathbf{S}^{0}[t]=\mathbf{I}$}}\;
% $\mathbf{S}^l[t]=\mathcal{H}(\tilde{\mathbf{U}}^l[t]-\theta)$;\textcolor{gray}{\Comment{$\mathcal{H}(\cdot)$ denotes the Heaviside step function}}\;
% $\mathbf{U}^{l}[t]=\tilde{\mathbf{U}}^{l}[t]\cdot \left(1-\mathbf{S}^{l}[t]\right)$\;
}}
Perform backpropagation and update the quantized model parameters $\mathcal{M}_q$\;
}}
\textcolor{gray}{$\triangleright$  Step 2: Get the pruned QP-SNN $\mathcal{M}_{q\&p}$ with the SVS-based pruning criterion}\;
\For{$l \gets 1$ \KwTo $L$}{
Initialize an array $\mathcal{F}$\;
\For{$f \gets 1$ \KwTo $n_{l}$}{
Perform a inference process with mini-batch data, get spatiotemporal spike activity\; 
Get the singular value matrix $\mathbf{\Sigma}$: $\frac{1}{T}\sum\nolimits_{t=1}^{T}\mathbf{S}^{l,f}[t]=\mathbf{P}\mathbf{\Sigma}_{h\times w}\mathbf{Q}^{\top}$\;
$\mathrm{Score}(\mathbf{W}^{l,f})=\mathbb{E}_B\left(\sum\nolimits_{i=1}^{min(H,W)}\mathbb{I} (\sigma _i >\epsilon)\right)$\;
$\mathcal{F}[f]=\mathrm{Score}(\mathbf{W}^{l,f})$\;}
$I_{prun}=\lceil r*n_l \rfloor$; $s_{index}=argsort(\mathcal{F})[I_{prun}:]$; \textcolor{gray}{\Comment{Select kernels with high score}}\;
$\mathbf{W}_{q\&p}^l \gets$ Kernels with index in $s_{index}$\;}
\textbf{function  }$Finetune(\mathcal{M}_{q\&p})$;\textcolor{gray}{\Comment{Fine-tune the pruned model to optimize performance}}.
\end{algorithm}


\section{Analysis on the order of quantization and pruning}
\label{sec:order}

{When two or more model lightweight techniques are employed, compatibility issues often arise, such as the order of applying these techniques and the training strategies involved. In this paper, we adpot the `quantize first, then prune' strategy based on the following two considerations. \textbf{First}, this strategy can better guarantee the effect of pruning technique. Specifically, if pruning is applied before quantization, important convolutional kernels identified in the full-precision parameter domain may become misaligned after quantization, as the quantization reintroduces additional errors. In contrast, by quantizing first and then pruning, redundant convolutional kernels are identified directly in the target low-precision parameter domain. This order allows for more accurate identification and preservation of critical kernels. 
\textbf{Second}, this strategy significantly reduces training overhead. Pruning before quantization requires three weight updates: `full-precision SNN training→ pruning with fine-tuning → quantization with fine-tuning,' while `quantize first, then prune' only requires two adjustments: `quatized SNN training→ pruning with fine-tuning.'}


{In addition to the theoretical analysis, we have also conducted experiments by reversing the order of quantization and pruning, termed PQ-SNN, to validate the effectiveness of QP-SNN. Experiments are performed on the CIFAR-100 with ResNet20 under the bit-width of 4. We summarize the experimental results in Table \ref{tab:order}, from which two conclusions can be obtained. \textbf{First}, the proposed ReScaW and SVS can improve both performance, regardless of the order in which they are applied, leading to a 1.83\% improvement in PQ-SNN and a 4.46\% improvement in QP-SNN. This proves the effectiveness of our ReScaW and SVS methods. \textbf{Second}, QP-SNN achieves the highest performance (surpassing PQ-SNN by 1.39\%), demonstrating that ‘quantize first, then prune’ is more effective.}
\vspace{-0.6cm}
\begin{table}[h]
\centering
{\caption{Ablation study on the order of quantization and pruning.}
\label{tab:order}
\begin{tabular}{c|cccc}
   \toprule 
Method & PQ-SNN baseline & PQ-SNN & QP-SNN baseline & QP-SNN \\ \midrule
Accuracy & 71.51\% &73.34\%$_\text{(baseline+1.83\%)}$  & 70.27\% & 74.73\%$_\text{(baseline+4.46\%)}$ \\   \bottomrule
\end{tabular}}
\end{table}
\vspace{-0.5cm}

\section{Efficiency validation of QP-SNN}
\label{sec:efficiency}
{Model compression aims to optimize efficiency during the inference phase, facilitating efficient deployment on resource-constrained devices. Therefore, we present the key efficiency metrics of QP-SNN during inference, including model size, SOPs, power consumption, and accuracy, to verify the efficiency advantage of QP-SNN.}

{We first present a comparison of our model with the full-precision uncompressed SNN counterparts. The results are summarized in Table \ref{tab:efficiency}. We acknowledge that our method exhibits accuracy loss compared to uncompressed SNNs. However, this performance degradation is a common challenge in the field of model compression.  Fortunately, QP-SNN demonstrates satisfactory performance under extreme compression ratios. For example, on the CIFAR-10 dataset, under the extreme connection ratio of 9.61\%, QP-SNN reduces the model size by 98.74\%, SOPs by 78.69\%, and power consumption by 77.45\%, while the accuracy decreases by only 2.44\%. This trade-off between performance degradation and resource efficiency is highly advantageous in edge computing scenarios.}
\vspace{-0.5cm}
\begin{table}[h]
\centering
{\caption{Efficiency metrics comparison of QP-SNN with full-precision uncompressed SNN.}
\label{tab:efficiency}
\tabcolsep=0.1cm
\begin{tabular}{c|ccccccc}
\toprule
&Architecture & Connection & Bit & Model size (MB)& SOPs (M) & Power (mJ) & Accuracy \\ \midrule
\multirow{2}{*}{CIFAR-10}&{VGG-16} & 100\% & 32 & 58.88 &  54.60 &  0.204& 93.63\% \\
&VGG-16 & 9.61\% & 4 & 0.74 &  11.63 &   0.046 & 91.19\% \\ \midrule
\multirow{2}{*}{CIFAR-100}&{ResNet20} & 100\% & 32 & 68.4 & 415.64  & 0.756 &79.49\%  \\
&ResNet20 & 22.69\% & 4 & 2.17 & 131.53 &  0.126 & 74.73\% \\ \bottomrule
\end{tabular}}
\end{table}
\vspace{-0.5cm}

{We then add a comparison of our method with related studies on CIFAR-10. Experimental results are shown in Table \ref{tab:efficiency2}. It can be seen that QP-SNN exhibits competitive SOPs compared to compression work in the SNN domain, and exhibits extremely low model size due to quantization.
Moreover, it is worth noting that the advanced works (\cite{deng2021comprehensive, shi2023towards}) focus on unstructured pruning, which typically achieves higher sparsity and performance but requires specialized hardware support. In contrast, our work adopts uniform quantization and structured pruning, balancing the advantages of sparsity, performance, and hardware compatibility.}
\begin{table}[h]
\tabcolsep=0.2cm
\centering
{\caption{Efficiency metrics comparison of QP-SNN with related studies on the CIFAR-10 dataset.}
\label{tab:efficiency2}
\begin{tabular}{cccccc}
\toprule
Method &Architecture &Time step&HardF & Model size (MB) & SOPs (M)  \\ \midrule
{\makecell[r]{\cite{deng2021comprehensive} \scriptsize{$ \textcolor{gray}{\textit{[TNNLS21]}}$}}} & 7Conv2FC &8 & \XSolidBrush & 62.16 &  107.97  \\
\makecell[r]{\cite{shi2023towards} \scriptsize{$  \textcolor{gray}{\textit{[ICLR24]}}$}} & 6Conv2FC &8& \XSolidBrush & 33.76 & 11.98 \\ 
\makecell[r]{\cite{li2024towards} \scriptsize{$  \textcolor{gray}{\textit{[ICML24]}}$}} & VGG-16 &4& \Checkmark & 5.68 & -   \\
QP-SNN & VGG-16 &4& \Checkmark & 0.74 & 11.63  \\ \bottomrule
\end{tabular}}
\end{table}

\section{Learning algorithm for QP-SNN}
\label{sec:alg}
In this section, we introduce the learning algorithm for the QP-SNN. 
We use the spatio-temporal backpropagation (STBP) (\cite{wu2018spatio}) and the straight-through estimator (STE) (\cite{bengio2013estimating}) methods to solve the non-differentiability of the spike generation function and quantization.


Training QP-SNNs requires calculating the gradient of the loss function with respect to the synaptic weight.
In this work, we use the STBP learning algorithm, which performs gradient propagation in both spatial and temporal dimensions.
By applying the chain rule, STBP computes the derivative of the loss function $\mathcal{L}$ with respect to synaptic weights $\mathbf{W}^l$ through the following equation,
% \begin{equation}
% \begin{aligned}
%     \frac{\partial\mathcal{L}}{\partial\mathbf{W}^l}=
%     &\sum_{t=1}^T\frac{\partial\mathcal{L}}{\partial\mathbf{S}^{l+1}[t]}\frac{\partial\mathbf{S}^{l+1}[t]}{\partial\mathbf{U}^{l+1}[t]}\left(\frac{\partial\mathbf{U}^{l+1}[t]}{\partial\mathbf{W}^{l}} + \right \\
%     &\left \sum_{\tau<t}\prod_{i=t-1}^\tau\left(\frac{\partial\mathbf{U}^{l+1}[i+1]}{\partial\mathbf{U}^{l+1}[i]} + \frac{\partial\mathbf{U}^{l+1}[i+1]}{\partial\mathbf{S}^{l+1}[i]}\frac{\partial\mathbf{S}^{l+1}[i]}{\partial\mathbf{U}^{l+1}[i]}\right)\frac{\partial\mathbf{U}^{l+1}[\tau]}{\partial\mathbf{W}^{l}} \right),
% \end{aligned}
% \end{equation}
\begin{equation}
\begin{aligned}
    \frac{\partial\mathcal{L}}{\partial\mathbf{W}^l}=
    &\sum_{t=1}^T\frac{\partial\mathcal{L}}{\partial\mathbf{S}^{l+1}[t]}\frac{\partial\mathbf{S}^{l+1}[t]}{\partial\mathbf{U}^{l+1}[t]}\left(\frac{\partial\mathbf{U}^{l+1}[t]}{\partial\mathbf{W}^{l}} + \right. \\
    &\left. \sum_{\tau<t}\prod_{i=t-1}^\tau\left(\frac{\partial\mathbf{U}^{l+1}[i+1]}{\partial\mathbf{U}^{l+1}[i]} + \frac{\partial\mathbf{U}^{l+1}[i+1]}{\partial\mathbf{S}^{l+1}[i]}\frac{\partial\mathbf{S}^{l+1}[i]}{\partial\mathbf{U}^{l+1}[i]}\right)\frac{\partial\mathbf{U}^{l+1}[\tau]}{\partial\mathbf{W}^{l}} \right),
\end{aligned}
\end{equation}
where the derivative of the loss function with respect to the spike $\frac{\partial\mathcal{L}}{\partial\mathbf{S}^{l+1}[t]}$ is obtained in an iterative manner.
The terms of $\frac{\partial\mathbf{U}^{l+1}[t]}{\partial\mathbf{W}^{l}}$, $\frac{\partial\mathbf{U}^{l+1}[i+1]}{\partial\mathbf{U}^{l+1}[i]}$ and $\frac{\partial\mathbf{U}^{l+1}[i+1]}{\partial\mathbf{S}^{l+1}[i]}$ can be computed from Eq.(\ref{u}).
Unfortunately, the direct training of SNNs faces a distinct challenge due to the non-differentiable nature of the spiking (i.e. firing) mechanism.
Specifically, the term of $\frac{\partial\mathbf{S}^{l+1}[t]}{\partial\mathbf{U}^{l+1}[t]}$  represents the gradient of the spike generation function (described in Eq.~(\ref{spikefunc})).
This function evaluates to infinity at the moment of spike emission and to zero elsewhere, making it incompatible with the traditional error backpropagation used in ANN training.
STBP addresses this non-differentiability problem by employing surrogate gradients to approximate the true gradient ~\cite{wu2018spatio}.
In this paper, we use the triangular surrogate function (\cite{deng2022temporal}), described as $\frac{\partial\mathbf{S}^{l+1}[t]}{\partial\mathbf{U}^{l+1}[t]}=\frac{1}{a}\max\left(a-|\mathbf{U}^{l+1}[t]-\theta|,0\right)$, where $a$ is the coefficient that controls the width of the gradient window.
In this paper, we use the cross-entropy loss function to access the difference between the predicted probability distribution and the true label, given by, $\mathcal{L}=-\sum_{i=1}^{N_L}y_i\log\left(\frac{\mathit{exp}(\frac{1}{T}\sum_{t=1}^T\tilde{\mathbf{U}}_i^L[t])}{\sum_j\mathit{exp}(\frac{1}{T}\sum_{t=1}^T\tilde{\mathbf{U}}_j^L[t])}\right)$, where $N_L$ is the number of classes and $y_i \in \{0,1\}$ is the label for the $i$-th neuron in the last layer.
Moreover, to solve the non-differentiability of quantization, we use the STE method (\cite{hinton2012deep,bengio2013estimating}), expressed as, $\frac{\partial \mathbf{W}^l}{\partial\mathbf{W}^l_{int}}=1_{|\mathbf{W}^{l}|\leq1}$. By using the surrogate gradient function and STE, the proposed QP-SNN can be trained directly with gradient backpropagation.


% where $y_c$ is the true label for class $c$, $\hat{y}_c$ is the predicted probability for class $c$ and $\mathbf{U}_c^L[t]$ represents the membrane potential of the neuron corresponding to class $c$ in the final layer $L$ at time step $t$. In this paper, $\hat{y}_c$ is typically derived from the average membrane potential of the output neurons over the simulation time steps,  which can capture more of the simulation dynamics, especially over shorter simulation times.




\section{Complete Weight distribution comparison}


\subsection{Vanilla uniform quantization}
\label{sec:vanillaWD}
We present the weight distributions of models utilizing vanilla uniform quantization across multiple datasets and architectures, such as ResNet20 on CIFAR100, VGG-16 on TinyImageNet, and VGGSNN on DVS-CIFAR10. 
In addition to the weight distribution, we also label the 1st and 99th percentiles of each layer's weights in the figure to determine the value of $a$.
Based on the value of $a$ and the utilization rate equation $\frac{s(b)\cdot a+1}{s(b)+1}$ in Sec. \ref{sec:method.1}, we calculate the bit-width utilization for each model.
In these calculations, we consider an 8-bit weight configuration, i.e., \( s(b) = 256 \).


The weight distribution of ResNet20 on the CIFAR-100 dataset is presented in Figure \ref{fig:cifar100_resnet20_vanilla}.
It can be seen from this figure that only the weight distribution of the first layer is relatively wide, with an $a$ value of 0.7, which corresponds to a bit-width utilization rate of 70.12\%. In contrast, the $a$ value for the subsequent layers are predominantly around 0.2, resulting in a significantly lower bit-width utilization rate of approximately 20.31\%.
% \begin{figure}[H]
% \centering
% \includegraphics[width=0.865\linewidth]{ICLR 2025 Template/cifar10_vgg16_vanilla.pdf}
% \caption{Weight distribution using the vanilla uniform quantization in VGG-16 on CIFAR-10.}
% \label{fig:cifar10_vgg16_vanilla}
% \end{figure}
\vspace{-0.3cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.86\linewidth]{figs/cifar100_resnet20_vanilla.pdf}
\caption{Weight distribution: vanilla uniform quantization, ResNet20, CIFAR-100.}
\label{fig:cifar100_resnet20_vanilla}
\end{figure}
\vspace{-0.3cm}
The weight distribution of VGG-16 on TinyImagenet is illustrated in Figure \ref{fig:tiny_vgg16_vanilla}. From this figure, it can be revealed that the weight distribution of each layer is broader than ResNet20 on CIFAR-100. However, the maximum \( a \) value is 0.64, which corresponds to a bit-width utilization rate of approximately 64.14\%. This result indicates it is still quite far from full utilization.
\vspace{-0.3cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.86\linewidth]{figs/tiny_vgg16_vanilla.pdf}
\caption{Weight distribution: vanilla quantization, VGG-16, TinyImageNet.}
\label{fig:tiny_vgg16_vanilla}
\end{figure}
The weight distribution of VGGSNN on DVS-CIFAR10 is displayed in Figure \ref{fig:dvs10_vggsnn_vanilla}. As can be seen from the figure, the maximum $a$ value is 0.44 (Conv1), corresponding to a bit width utilization rate of 44.22\%. Moreover, the $a$ value of subsequent layers is mainly around 0.3, resulting in a significantly lower bit width utilization rate of about 30.27\%.
\begin{figure}[H]
\centering
\includegraphics[width=0.94\linewidth]{figs/dvs10_vggsnn_vanilla.pdf}
\caption{Weight distribution: vanilla quantization, VGGSNN, DVS-CIFAR10.}
\label{fig:dvs10_vggsnn_vanilla}
\end{figure}
\vspace{-0.2cm}
Clearly, these weight distributions prove the inefficient bit-width utilization of vanilla uniform quantization. This inefficiency leads to a substantial number of floating-point weights being discretized on the same integer grid during the quantization process, thus reducing the discrimination of the quantized weights. 
Consequently, this reduction adversely impacts the network's representational capacity and overall performance.

\subsection{ReScaW-based uniform quantization}
\label{sec:rescaWWD}
\begin{figure}[H]
\centering
\includegraphics[width=0.94\linewidth]{figs/cifar100_resnet20_rescaw.pdf}
\vspace{-0.2cm}
\caption{Weight distribution: ReScaW-based uniform quantization, ResNet20, CIFAR-100.}
\label{fig:cifar100_resnet20_rescaw}
\end{figure}
We also present a comparison of weight distributions between vanilla uniform quantization and ReScaW-based uniform quantization across multiple datasets and architectures.
The weight distribution of ResNet20 on CIFAR-100 is presented in Figure \ref{fig:cifar100_resnet20_rescaw}, VGG-16 on TinyImageNet is illustrated in Figure \ref{fig:tiny_vgg16_rescaw}, and VGG-SNN on DVS-CIFAR10 is displayed in Figure \ref{fig:dvs10_vggsnn_rescaw}.
These three figures clearly demonstrate that the weight distribution using ReScaW-based quantization is broader than that of vanilla uniform quantization, indicating the more efficient bit-width utilization of our ReScaW.
\vspace{-0.2cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{figs/tiny_vgg16_rescaw.pdf}
\vspace{-0.2cm}
\caption{Weight distribution: ReScaW-based uniform quantization, VGG-16, TinyImageNet.}
\label{fig:tiny_vgg16_rescaw}
\end{figure}
\vspace{-0.5cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{figs/dvs10_vggsnn_rescaw.pdf}
\vspace{-0.2cm}
\caption{Weight distribution: ReScaW-based uniform quantization, VGGSNN, DVS-CIFAR10.}
\label{fig:dvs10_vggsnn_rescaw}
\end{figure}


\section{Complete importance score comparison}

\subsection{SCA-based pruning criterion}
\label{sec:sca}
We present the convolutional kernel scores of models using the SCA criterion across different architectures and datasets, including ResNet20 on CIFAR100, VGG-16 on TinyImageNet, and VGGSNN on DVS-CIFAR10.
Note that we only display the layers that perform pruning operations, and the colors in these figures represent the value of the importance score.
Moreover, to intuitively reflect the robustness of the pruning criterion to input samples, we compute the average cosine similarity of kernel scores between pairs of input batches for each layer in every model. 
The calculation for the average cosine similarity of $l$-th layer is outlined as,
\begin{equation}
    \mathrm{AvgCosS}_l=\frac{2}{N_B(N_B-1)}\sum_{i<j}\frac{\sum_{f}\mathrm{Score}_i(\mathbf{W}^{l,f})\cdot\mathrm{Score}_j(\mathbf{W}^{l,f})}{\sqrt{\sum_{f}\mathrm{Score}_i(\mathbf{W}^{l,f})^2}\cdot\sqrt{\sum_{f}\mathrm{Score}_j(\mathbf{W}^{l,f})^2}}
\end{equation}
where $N_B$ is the number of input batches and $\mathrm{Score}_i$ is the kernel score for input batch $i$.
% In VGG-16, the layer with the lowest average cosine similarity of convolution kernel scores between batches is Conv15, with a value of only 0.879. In ResNet20, the layer with the lowest average cosine similarity is Layer3.2.Conv2, with only 0.870. And in VGGSNN, Conv9 has the lowest average cosine similarity at 0.952.

The kernel scores for ResNet20 on CIFAR-100 are presented in Figure \ref{fig:resnet20_rank_ablation}. It can be seen from this figure that the SCA-based pruning criterion yields varying scores for the same kernel when processing different input samples. Furthermore, we calculated $\mathrm{AvgCosS}_l$ for each layer in ResNet20, and the $\min_{l}\mathrm{AvgCosS}_l$ is 0.870.
This indicates that the SCA criterion is not robust enough to inputs.
\vspace{-0.2cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.89\linewidth]{figs/resnet20_rank_ablation.pdf}
\vspace{-0.2cm}
\caption{Kernel scores: SCA-based pruning criterion, ResNet20, CIFAR-100.}
\label{fig:resnet20_rank_ablation}
\end{figure}
\vspace{-0.5cm}
The kernel scores for VGG-16 on TinyImagenet are illustrated in Figure \ref{fig:vgg16_tiny_rank_ablation}. We calculate $\mathrm{AvgCosS}_l$ for each layer in VGG-16, and obtain the $\min_{l}\mathrm{AvgCosS}_l$ is 0.879.
In this structure, the kernel scores' fluctuation with inputs is slightly better compared to ResNet20, but still not negligible.
\vspace{-0.3cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.89\linewidth]{figs/vgg16_tiny_rank_ablation.pdf}
\vspace{-0.2cm}
\caption{Kernel scores: SCA-based pruning criterion, VGG-16, TinyImageNet.}
\label{fig:vgg16_tiny_rank_ablation}
\end{figure}
The kernel scores for VGGSNN on DVS-CIFAR-10 are displayed in Figure \ref{fig:vggsnn_rank_ablation}. As can be seen from the figure, The kernel score's fluctuation with input data is better compared to both ResNet and VGG-16, but in deeper layers, the fluctuation is higher. We also calculate $\mathrm{AvgCosS}_l$ for each layer in VGGSNN, and the $\min_{l}\mathrm{AvgCosS}_l$ is 0.952.
\vspace{-0.2cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.79\linewidth]{figs/vggsnn_rank_ablation.pdf}
\vspace{-0.2cm}
\caption{Kernel scores: SCA-based pruning criterion, VGGSNN, DVS-CIFAR10.}
\label{fig:vggsnn_rank_ablation}
\end{figure}
\vspace{-0.2cm}
These results demonstrate that the SCA-based pruning criterion yields varying scores for the same kernel when processing different input sample, demonstrating low robustness to input samples.
This sensitivity to inputs suggests that the criterion may fail to accurately identify critical convolutional kernels within SNNs, potentially impacting the reliability of the pruning process.


\subsection{SVS-based pruning criterion}
\label{sec:svs}
We also depict kernel scores using the SVS pruning criterion.
The kernel score for ResNet20 on CIFAR-100 in Figure \ref{fig:resnet20_rank}, VGG-16 on
TinyImagenet in Figure \ref{fig:vgg16_tiny_rank}, and VGGSNN on DVS-CIFAR10 in Figure \ref{fig:vggsnn_rank}.
We still only display the layers that perform pruning operation.
% In VGG-16, ResNet20, and VGGSNN, the layers with the lowest average cosine similarity of convolution kernel scores between batches are Conv15, Layer3.2.Conv1, and Conv9, respectively. The corresponding values are 0.997, 0.993, and 1.000, which exceed the minimum average cosine similarity between layers when using the SCA-Based pruning criterion by 13.4\%, 14.1\%, and 5.0\%, respectively.
In VGG-16, ResNet20, and VGGSNN, the $\min_{l}\mathrm{AvgCosS}_l$ values are 0.997, 0.993, and 1.000 respectively, which exceed the corresponding $\min_{l}\mathrm{AvgCosS}_l$ when using the SCA-Based pruning criterion by 13.4\%, 14.1\%, and 5.0\%, respectively.
The results demonstrate that the SVS-based pruning criterion yields consistent evaluations, with only minor variations between different input samples. This high robustness to input samples enables QP-SNN to effectively identify and eliminate redundant kernels.
\vspace{-0.2cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.79\linewidth]{figs/resnet20_rank.pdf}
\vspace{-0.2cm}
\caption{Kernel scores: SVS-based pruning criterion, ResNet20, CIFAR-100.}
\label{fig:resnet20_rank}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figs/vgg16_tiny_rank.pdf}
\vspace{-0.2cm}
\caption{Kernel scores: SVS-based pruning criterion, VGG-16, TinyImageNet.}
\label{fig:vgg16_tiny_rank}
\end{figure}
\vspace{-0.8cm}
\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figs/vggsnn_rank.pdf}
\caption{Kernel scores: SVS-based pruning criterion, VGGSNN, DVS-CIFAR10.}
\label{fig:vggsnn_rank}
\end{figure}

\section{Experiment}
\label{sec:exp}

% In our experiments, we employ five diverse datasets to evaluate our proposed method. CIFAR-10 and CIFAR-100 consist of 60,000 32$\times$32 color images in 10 and 100 classes respectively, split into 50,000 training and 10,000 test images. ImageNet-1K comprises approximately 1.3 million training images in 1000 categories, while TinyImageNet, a compact version, contains 100,000 64$\times$64 pixel images across 200 classes. These datasets represent a spectrum of complexity in image classification tasks, from simple to highly challenging.
% For neuromorphic dataset, we utilized DVS-CIFAR10, a dynamic vision sensor recording of the CIFAR-10. This dataset consists of 10,000 event streams, preserving the 10-class structure of CIFAR-10. 

% For the aforementioned datasets, we implemented standard data preprocessing and augmentation techniques to enhance model performance and generalization.  For the DVS-CIFAR10 dataset, we employed additional preprocessing steps to stack the events into a single frame and then use this frame as the input for particular time step. 


\paragraph{Datasets}
We evaluate our method on image classification datasets, including static datasets CIFAR-10~(\cite{krizhevsky2009learning}), CIFAR-100 (\cite{krizhevsky2009learning}), TinyImageNet~(\cite{deng2009imagenet}), ImageNet-1k (\cite{deng2009imagenet}), and the neuromorphic dataset DVS-CIFAR10~\cite{li2017cifar10}.
Before introducing the experiment setups, we briefly outline each dataset.
The CIFAR-10 and CIFAR-100 are color image datasets, with each dataset containing 50,000 training images and 10,000 testing images. Each image features 3 color channels and a spatial resolution of 32$\times$32 pixels. CIFAR-10 is composed of 10 categories, whereas CIFAR-100 comprises 100 categories.
During the preprocessing process of CIFAR datasets, we apply the commonly used data augmentation techniques (\cite{cubuk2018autoaugment,devries2017improved}).
The TinyImageNet dataset is a subset of the ImageNet dataset, consisting of 200 categories, with each category containing 500 training images and 50 test images. Each image has 3 color channels and a spatial resolution of 64$\times$64 pixels.
The ImageNet-1K dataset is a large-scale dataset commonly used for computer vision tasks. It spans 1000 classes and contains around 1.3 million training images and 50,000 validation images.
The DVS-CIFAR10 is a neuromorphic dataset captured using Dynamic Vision Sensor (DVS) event cameras.
It is the most challenging neuromorphic dataset, featuring 9,000 training samples and 1,000 testing samples, featuring a spatial resolution of 128$\times$128.
During the preprocessing process of the DVS-CIFAR10 dataset, we apply the data augmentation technique proposed in (\cite{li2022neuromorphic}).
\vspace{-0.25cm}
\paragraph{Experimental Setups}
We summarize the training hyperparameters for each dataset in Table~\ref{Hyper-parameters}, including time step, image resolution, optimizer, and other factors. 
Additionally, we present the network architectures and the corresponding pruning rates for each module in Table (\ref{Compress Rate for VGG-16}$\sim$\ref{Compress Rate for VGGSNN}). 
In our experiments, we directly utilize the classification head after completing the convolution operations. Therefore, we do not prune the output channels of the last convolutional layer to preserve the integrity of the classification head.
Note that the pruning rates used in our experiments are manually selected, without rigorous design or the application of parameter search methods.
\begin{table}[H]
\vspace{-0.4cm}
\small
\centering
\setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{1.3}
\caption{Experimental setups.}
\label{Hyper-parameters}
\begin{tabular}{l|c|c|c|c}
\hline 
Hyper-parameter     & CIFAR-10/100 & TinyImageNet & ImageNet  & DVS-CIFAR10    \\ \hline
% Bit for weight           & 8, 4, 2 & 8, 4, 2 & 8, 4, 2 & 8, 4, 2 \\
Timestep            & 2, 4 & 4 & 4 & 10                        \\
Resolution          & 32$\times$32 & 64$\times$64 & 224$\times$224  & 48$\times$48        \\
Batch size          & 256 & 256 & 256 & 64              \\
{Epoch (Train/Fine-tune)}              & {300 / 150} & {300 / 150} & {320 / 200} & {300 / 150}                     \\
{Optimizer (Train/Fine-tune)}           & {SGD / Adam} & {SGD / Adam} & {SGD / SGD} & {SGD / Adam}          \\
{Initial lr (Train/Fine-tune)}  & {0.1 / 0.001} & {0.1 / 0.001} & {0.1 / 0.05} & {0.1 / 0.001}    \\
Learning rate decay & Cosine & Cosine & Cosine & Cosine \\
\hline 
\end{tabular}
\vspace{-0.5cm}
\end{table}
\begin{table}[H]
% \small
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\centering
{
\caption{Detailed network architecture and the channel pruning ratio for VGG-16.}
\label{Compress Rate for VGG-16}
\begin{tabular}{c|c|c|c|cccccc}
\hline 
\multirow{3}{*}{Layer} & \multirow{3}{*}{Resolution}                         & \multirow{3}{*}{Channel} & \multirow{3}{*}{Module} & \multicolumn{6}{c}{Channel Pruning Ratio}    \\ \cline{5-10} 
                       &                                                     &                      &                        & \multicolumn{2}{c|}{CIFAR-10}                                                            & \multicolumn{2}{c|}{CIFAR-100}                                                           & \multicolumn{2}{c}{TinyImageNet}                                  \\ \cline{5-10} 
                       &                                                     &                      &                        & \multicolumn{1}{c|}{4.25M}                 & \multicolumn{1}{c|}{1.42M}                 & \multicolumn{1}{c|}{2.31M}                 & \multicolumn{1}{c|}{1.68M}                 & \multicolumn{1}{c|}{4.65M}                 & 3.43M                 \\ \hline
{1}     & {$H \times W$}                       & {64}  & \makecell[c]{Conv\\-BN-LIF}                   & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.49}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & {0.45} \\\hline
{2}     & {$H \times W$}                       & {64}  & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.49}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & {0.45} \\  \hline
3                      & \multicolumn{2}{c|}{-}                   & MaxPool                & \multicolumn{6}{c}{-}                                                                                                                                                                                                                                  \\ \hline
{4}     & {$\frac{H}{2} \times \frac{W}{2}$}   & {128} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.49}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & {0.45} \\  \hline
{5}     & {$\frac{H}{2} \times \frac{W}{2}$}   & {128} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.49}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & {0.45} \\ \hline
6                      & \multicolumn{2}{c|}{-}                  & MaxPool                & \multicolumn{6}{c}{-}                                                                                                                                                                                                                                  \\ \hline
{7}     & {$\frac{H}{4} \times \frac{W}{4}$}   & {256} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.49}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & {0.45} \\ \hline
{8}     & {$\frac{H}{4} \times \frac{W}{4}$}   & {256} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.49}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & {0.45} \\ \hline
{9}     & {$\frac{H}{4} \times \frac{W}{4}$}   & {256} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.49}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & \multicolumn{1}{c|}{{0.45}} & {0.45} \\  \hline
10                     & \multicolumn{2}{c|}{-}                                                                    & MaxPool                & \multicolumn{6}{c}{-}                                                                                                                                                                                                                                  \\ \hline
{11}    & {$\frac{H}{8} \times \frac{W}{8}$}   & {512} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.51}} & \multicolumn{1}{c|}{{0.8}}  & \multicolumn{1}{c|}{{0.7}}  & \multicolumn{1}{c|}{{0.78}} & \multicolumn{1}{c|}{{0.51}} & {0.62} \\  \hline
{12}    & {$\frac{H}{8} \times \frac{W}{8}$}   & {512} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.51}} & \multicolumn{1}{c|}{{0.8}}  & \multicolumn{1}{c|}{{0.7}}  & \multicolumn{1}{c|}{{0.78}} & \multicolumn{1}{c|}{{0.51}} & {0.62} \\ \hline
{13}    & {$\frac{H}{8} \times \frac{W}{8}$}   & {512} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.51}} & \multicolumn{1}{c|}{{0.8}}  & \multicolumn{1}{c|}{{0.7}}  & \multicolumn{1}{c|}{{0.78}} & \multicolumn{1}{c|}{{0.51}} & {0.62} \\  \hline
14                      & \multicolumn{2}{c|}{-}                  & MaxPool                & \multicolumn{6}{c}{-}                                                                                                                                                                                                                                  \\ \hline
{15}    & {$\frac{H}{16} \times \frac{W}{16}$} & {512} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.51}} & \multicolumn{1}{c|}{{0.8}}  & \multicolumn{1}{c|}{{0.7}}  & \multicolumn{1}{c|}{{0.78}} & \multicolumn{1}{c|}{{0.51}} & {0.62} \\  \hline
{16}    & {$\frac{H}{16} \times \frac{W}{16}$} & {512} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{0.51}} & \multicolumn{1}{c|}{{0.8}}  & \multicolumn{1}{c|}{{0.7}}  & \multicolumn{1}{c|}{{0.78}} & \multicolumn{1}{c|}{{0.51}} & {0.62} \\  \hline
{17}    & {$\frac{H}{16} \times \frac{W}{16}$} & {512} & \makecell[c]{QConv\\-BN-LIF}                  & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{{-}}  & \multicolumn{1}{c|}{{-}}  & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{-} & {-} \\  
% \hline
% {18}    & \multicolumn{2}{c|}{-}                                                                    & AvgPool                & \multicolumn{6}{c}{-}\\ \hline 
% {19}    & - & 10/100/200           & FC                & \multicolumn{6}{c}{-}\\
\hline 
\end{tabular}}
\end{table}
\begin{table}[h]
{
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\caption{Detailed network architecture and the channel pruning ratio for ResNet20.}
\label{Compress Rate for ResNet20}
\begin{tabular}{c|c|c|c|cc}
\hline
\multirow{3}{*}{Layer} & \multirow{3}{*}{Resolution}                       & \multirow{3}{*}{Channel} & \multirow{3}{*}{Module} & \multicolumn{2}{c}{Channel Pruning Ratio}  \\ \cline{5-6} 
                       &                                                   &                      &                        & \multicolumn{2}{c}{CIFAR-10 / 100}                                                           \\ \cline{5-6} 
                       &                                                   &                      &                        & \multicolumn{1}{c|}{6.22M / 6.27M}                & \multicolumn{1}{c}{3.87M / 3.92M}          \\ \hline
{conv0}     & {$H \times W$}                     & {64}  & Conv-BN-LIF                   & \multicolumn{1}{c|}{{0.1}} & \multicolumn{1}{c}{{0.1}}   \\ \hline
\multirow{2}{*}{Layer1.0}     & \multirow{2}{*}{$H \times W$}                     & \multirow{2}{*}{128} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.3}} &0.35  \\ \cline{4-6} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.3}} & \multicolumn{1}{c}{{0.35}}  \\ \hline
\multirow{2}{*}{Layer1.1}     & \multirow{2}{*}{$H \times W$}                     & \multirow{2}{*}{128} & QConv-BN-LIF    & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}} \\ \cline{4-6} 
                       &                                                   &                      & QConv-BN-LIF                   & \multicolumn{1}{c|}{{0.3}} & {0.35} \\ \hline
\multirow{2}{*}{Layer1.2}     & \multirow{2}{*}{$H \times W$}                     & \multirow{2}{*}{128} & QConv-BN-LIF                    & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}}\\ \cline{4-6} 
                       &                                                   &                      & QConv-BN-LIF                   & \multicolumn{1}{c|}{{0.3}} & \multicolumn{1}{c}{{0.35}}  \\ \hline
\multirow{2}{*}{Layer2.0}     & \multirow{2}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multirow{2}{*}{256} & QConv-BN-LIF                    & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}} \\ \cline{4-6}
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}} \\  \hline
\multirow{2}{*}{Layer2.1}     & \multirow{2}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multirow{2}{*}{256} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}} \\  \cline{4-6} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}}  \\ \hline
\multirow{2}{*}{Layer2.2}     & \multirow{2}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multirow{2}{*}{256} & QConv-BN-LIF        & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}}            \\  \cline{4-6} 
                       &                                                   &                      & QConv-BN-LIF                   & \multicolumn{1}{c|}{{0.6}} & {0.75} \\ \hline
\multirow{2}{*}{Layer3.0}     & \multirow{2}{*}{$\frac{H}{4} \times \frac{W}{4}$} & \multirow{2}{*}{512} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}}  \\  \cline{4-6} 
                       &                                                   &                      &  QConv-BN-LIF                    & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c}{{-}}  \\ \hline
\multirow{2}{*}{Layer3.1}     & \multirow{2}{*}{$\frac{H}{4} \times \frac{W}{4}$} & \multirow{2}{*}{512} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}} \\  \cline{4-6} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c}{{-}}\\  \hline
\multirow{2}{*}{Layer3.2}    & \multirow{2}{*}{$\frac{H}{4} \times \frac{W}{4}$} & \multirow{2}{*}{512} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c}{{0.75}}  \\ \cline{4-6} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c}{{-}}  \\  \hline
\end{tabular}}
\end{table}

\iffalse
\begin{table}[h]
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\caption{Detailed network architecture and the corresponding pruning rate for ResNet20.}
\label{Compress Rate for ResNet20}
\begin{tabular}{c|c|c|c|cccc}
\hline \hline
\multirow{3}{*}{Block} & \multirow{3}{*}{Resolution}                       & \multirow{3}{*}{Channel} & \multirow{3}{*}{Layer} & \multicolumn{4}{c}{Pruning Rate}                                                                                                                          \\ \cline{5-8} 
                       &                                                   &                      &                        & \multicolumn{2}{c|}{CIFAR-10}                                                           & \multicolumn{2}{c}{CIFAR-100}                                      \\ \cline{5-8} 
                       &                                                   &                      &                        & \multicolumn{1}{c|}{6.22M}                & \multicolumn{1}{c|}{3.87M}                 & \multicolumn{1}{c|}{6.27M}                & 3.92M                 \\ \hline
{1}     & {$H \times W$}                     & {64}  & Conv-BN-LIF                   & \multicolumn{1}{c|}{{0.1}} & \multicolumn{1}{c|}{{0.1}}  & \multicolumn{1}{c|}{{0.1}} & {0.1}  \\ \hline
\multirow{2}{*}{2}     & \multirow{2}{*}{$H \times W$}                     & \multirow{2}{*}{128} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.3}} & \multicolumn{1}{c|}{{0.35}} & \multicolumn{1}{c|}{{0.3}} & {0.35} \\ \cline{4-8} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.3}} & \multicolumn{1}{c|}{{0.35}} & \multicolumn{1}{c|}{{0.3}} & {0.35} \\ \hline
\multirow{2}{*}{3}     & \multirow{2}{*}{$H \times W$}                     & \multirow{2}{*}{128} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\ \cline{4-8} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.3}} & \multicolumn{1}{c|}{{0.35}} & \multicolumn{1}{c|}{{0.3}} & {0.35} \\ \hline
\multirow{2}{*}{4}     & \multirow{2}{*}{$H \times W$}                     & \multirow{2}{*}{128} & QConv-BN-LIF                    & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\ \cline{4-8} 
                       &                                                   &                      & QConv-BN-LIF                   & \multicolumn{1}{c|}{{0.3}} & \multicolumn{1}{c|}{{0.35}} & \multicolumn{1}{c|}{{0.3}} & {0.35} \\ \hline
\multirow{2}{*}{5}     & \multirow{2}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multirow{2}{*}{256} & QConv-BN-LIF                    & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\ \cline{4-8}
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\  \hline
\multirow{2}{*}{6}     & \multirow{2}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multirow{2}{*}{256} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\  \cline{4-8} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\ \hline
\multirow{2}{*}{7}     & \multirow{2}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multirow{2}{*}{256} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\  \cline{4-8} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\ \hline
\multirow{2}{*}{8}     & \multirow{2}{*}{$\frac{H}{4} \times \frac{W}{4}$} & \multirow{2}{*}{512} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\  \cline{4-8} 
                       &                                                   &                      &  QConv-BN-LIF                    & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{{-}} & {-} \\ \hline
\multirow{2}{*}{9}     & \multirow{2}{*}{$\frac{H}{4} \times \frac{W}{4}$} & \multirow{2}{*}{512} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\  \cline{4-8} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{{-}} & {-} \\  \hline
\multirow{2}{*}{10}    & \multirow{2}{*}{$\frac{H}{4} \times \frac{W}{4}$} & \multirow{2}{*}{512} & QConv-BN-LIF                  & \multicolumn{1}{c|}{{0.6}} & \multicolumn{1}{c|}{{0.75}} & \multicolumn{1}{c|}{{0.6}} & {0.75} \\ \cline{4-8} 
                       &                                                   &                      & QConv-BN-LIF                  & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{{-}} & {-} \\  \hline
\end{tabular}
\end{table}
\fi


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[h]
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.1}
\centering
{
\caption{Detailed network architecture and the channel pruning ratio for VGGSNN.}
\label{Compress Rate for VGGSNN}
\begin{tabular}{c|c|c|c|ccc}
\hline 
\multirow{3}{*}{Layer} & \multirow{3}{*}{Resolution}                       & \multirow{3}{*}{Channel} & \multirow{3}{*}{Module} & \multicolumn{3}{c}{Channel Pruning Ratio}                                                                             \\ \cline{5-7} 
                       &                                                   &                      &                        & \multicolumn{3}{c}{DVS-CIFAR10}                                                                               \\ \cline{5-7} 
                       &                                                   &                      &                        & \multicolumn{1}{c|}{1.46M}                & \multicolumn{1}{c|}{0.9M}                 & 0.25M                 \\ \hline
{1}     & {$H \times W$}                     & {64}  & \makecell[c]{Conv-BN-LIF}                   & \multicolumn{1}{c|}{{0.5}} & \multicolumn{1}{c|}{{0.5}} & {0.82} \\  \hline
{2}     & {$H \times W$}                     & {128} & \makecell[c]{QConv-BN-LIF}                  & \multicolumn{1}{c|}{{0.5}} & \multicolumn{1}{c|}{{0.5}} & {0.82} \\  \hline
3                      & \multicolumn{2}{c|}{-}                  & MaxPool                & \multicolumn{3}{c}{-}                                                                                         \\ \hline
{4}     & {$\frac{H}{2} \times \frac{W}{2}$} & {256} & \makecell[c]{QConv-BN-LIF}                  & \multicolumn{1}{c|}{{0.5}} & \multicolumn{1}{c|}{{0.5}} & {0.82} \\  \hline
{5}     & {$\frac{H}{2} \times \frac{W}{2}$} & {256} & \makecell[c]{QConv-BN-LIF}                  & \multicolumn{1}{c|}{{0.7}} & \multicolumn{1}{c|}{{0.8}} & {0.93} \\  \hline
6                      & \multicolumn{2}{c|}{-}                  & MaxPool                & \multicolumn{3}{c}{-}                                                                                         \\ \hline
{7}     & {$\frac{H}{4} \times \frac{W}{4}$} & {512} & \makecell[c]{QConv-BN-LIF}                  & \multicolumn{1}{c|}{{0.7}} & \multicolumn{1}{c|}{{0.8}} & {0.93} \\  \hline
{8}     & {$\frac{H}{4} \times \frac{W}{4}$} & {512} & \makecell[c]{QConv-BN-LIF}                  & \multicolumn{1}{c|}{{0.7}} & \multicolumn{1}{c|}{{0.8}} & {0.93} \\  \hline
9                      & \multicolumn{2}{c|}{-}                  & MaxPool                & \multicolumn{3}{c}{-}                                                                                         \\ \hline
{10}    & {$\frac{H}{8} \times \frac{W}{8}$} & {512} & \makecell[c]{QConv-BN-LIF}                  & \multicolumn{1}{c|}{{0.7}} & \multicolumn{1}{c|}{{0.8}} & {0.93} \\  \hline
{11}    & {$\frac{H}{8} \times \frac{W}{8}$} & {512} & \makecell[c]{QConv-BN-LIF}                   & \multicolumn{1}{c|}{{-}} & \multicolumn{1}{c|}{{-}}  & \multicolumn{1}{c}{{-}}                                                                        \\  \hline 
\end{tabular}}
\end{table}


\paragraph{Model size calculation}
The model size is computed by aggregating the storage requirements of both quantized and full precision parameters, as expressed by the following equation (\cite{qin2022bibert,zhang2022pokebnn}),
\begin{equation}
\label{model size}
    M = \text{Params} \times \text{Bitwidth} = \sum P_q \times B_q + \sum P_{fp} \times B_{fp},
\end{equation}
where \( P_q \) and \( P_{fp} \) denote the quantized parameters and full precision parameters, respectively, while \( B_q \) and \( B_{fp} \) represent their corresponding bit widths.
It is important to note that, in our experiments, full-precision weights are employed in both the initial convolutional layer and the final fully connected layer to ensure optimal performance (\cite{zhang2021diversifying,ding2022towards}).
We also take this configuration into account when calculating our model size.
\end{CJK}
\end{document}


