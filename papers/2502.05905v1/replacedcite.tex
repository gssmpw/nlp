\section{Related work}
\paragraph{Quantization technique}
% Quantization achieves efficient computation and storage by reducing parameter representation from 32-bit to lower bit-width representation.
Early research on quantization in SNNs is primarily based on ANN-to-SNN conversion algorithms, where a quantized ANN is first trained and then converted into the corresponding quantized SNN version (____).
To mitigate the performance loss associated with the conversion, researchers have proposed many strategies, such as the utilization of activation penalty term (____) and the weight-threshold balancing method (____). 
However,these quantized SNNs still experience significant performance degradation and long latency issues.
To address these limitations, some studies have explored directly training quantized SNNs and introduced different strategies to enhance performance, such as alternating direction method of multipliers (____), accuracy loss estimator (____), suitable activation function (____), and weight-spike dual regulation (____).
% Recently, there have been studies that quantize both weights and membrane potentials of SNNs, achieving 90.79\% accuracy on CIFAR-10 with 2-bit configuration ____.
Despite performance improvement, these studies fail to effectively leverage the allocated bit-width, resulting in the limited expressive capability of models. 
Therefore, there still remains significant room for performance improvement.



\paragraph{Pruning technique}

% Pruning reduces the number of parameters in a neural network and improves inference speed by eliminating redundant components within the network, such as weights, feature maps, and convolutional kernels.
Existing research on pruning SNNs can be broadly divided into two groups.
The first group is unstructured pruning.
For example, (____) use a magnitude-based method to remove insignificant weights, and (____) propose a fine-grained pruning framework that integrates unstructured weight and neuron pruning to enhance SNN energy efficiency.
Additionally, there are some biologically inspired unstructured pruning works (____).
While these studies achieve great sparsity and performance, they lead to irregular memory access in forward propagation, requiring specialized hardware for acceleration.
The second group is structured pruning that offers better hardware compatibility.
(____) use principal component analysis on membrane potentials to evaluate channel correlations and eliminate redundant ones.
However, it suffers from long latency and cannot handle neuromorphic datasets.
Recently, (____) evaluate the importance of kernels based on spike activity, advancing the performance of pruned SNNs to a new level.
However, this evaluation criterion exhibits high dependency on inputs and may not accurately reflect the importance of kernels.


\paragraph{Compression with joint quantization and pruning}
Several studies have explored combining quantization and pruning to maximize the compression of SNNs.
First, (____) adopt the STDP learning rule and a predefined pruning threshold to remove insignificant connections, and then quantizes retained important weights.
Then, (____) perform principal component analysis on membrane potentials for spatial pruning and gradually decreases the time step during training for temporal pruning. They also use post-training quantization to compress retained weights.
Moreover, (____) formulate pruning and quantization as a constraint optimization problem in supervised learning, and address it with the alternating direction method of multipliers.
However, these existing studies combining quantization and pruning face two main problems.
Firstly, the unstructured pruning methods in (____) and (____) require specialized hardware for efficient acceleration.
Secondly, (____) only evaluate their method on very simple datasets, and (____) and (____) only extend their methods to CIFAR (88.6\% and 87.84\% accuracy on CIFAR-10 with 5, 3 bits respectively), leading to significant room for improvement in both performance and efficiency.
% achieving