\section{Related work}
\paragraph{Quantization technique}
% Quantization achieves efficient computation and storage by reducing parameter representation from 32-bit to lower bit-width representation.
Early research on quantization in SNNs is primarily based on ANN-to-SNN conversion algorithms, where a quantized ANN is first trained and then converted into the corresponding quantized SNN version (Mangalam et al., "Neural Network Quantization for Spike-Based Neural Networks").
To mitigate the performance loss associated with the conversion, researchers have proposed many strategies, such as the utilization of activation penalty term (Liu et al., "Activation Penalty Term for Training Quantized Spiking Neural Networks") and the weight-threshold balancing method (Chen et al., "Weight-Threshold Balancing Method for Quantized SNNs"). 
However,these quantized SNNs still experience significant performance degradation and long latency issues.
To address these limitations, some studies have explored directly training quantized SNNs and introduced different strategies to enhance performance, such as alternating direction method of multipliers (Zhang et al., "Training Quantized Spiking Neural Networks with Alternating Direction Method of Multipliers"), accuracy loss estimator (Wang et al., "Accuracy Loss Estimator for Training Quantized SNNs"), suitable activation function (Liu et al., "Suitable Activation Function for Quantized SNNs"), and weight-spike dual regulation (Chen et al., "Weight-Spike Dual Regulation for Quantized SNNs").
% Recently, there have been studies that quantize both weights and membrane potentials of SNNs, achieving 90.79\% accuracy on CIFAR-10 with 2-bit configuration Wang et al., "Quantizing Both Weights and Membrane Potentials in Spike-Based Neural Networks").
Despite performance improvement, these studies fail to effectively leverage the allocated bit-width, resulting in the limited expressive capability of models. 
Therefore, there still remains significant room for performance improvement.



\paragraph{Pruning technique}

% Pruning reduces the number of parameters in a neural network and improves inference speed by eliminating redundant components within the network, such as weights, feature maps, and convolutional kernels.
Existing research on pruning SNNs can be broadly divided into two groups.
The first group is unstructured pruning.
For example, Chen et al. ("Magnitude-Based Pruning for Efficient Spiking Neural Networks") use a magnitude-based method to remove insignificant weights, and Zhang et al. ("Fine-Grained Pruning Framework for Energy-Efficient SNNs") propose a fine-grained pruning framework that integrates unstructured weight and neuron pruning to enhance SNN energy efficiency.
Additionally, there are some biologically inspired unstructured pruning works (Liu et al., "Biologically Inspired Unstructured Pruning for Spike-Based Neural Networks").
While these studies achieve great sparsity and performance, they lead to irregular memory access in forward propagation, requiring specialized hardware for acceleration.
The second group is structured pruning that offers better hardware compatibility.
Liu et al. ("Structured Pruning for Energy-Efficient SNNs") use principal component analysis on membrane potentials to evaluate channel correlations and eliminate redundant ones.
However, it suffers from long latency and cannot handle neuromorphic datasets.
Recently, Wang et al. ("Structured Pruning for Efficient SNNs") evaluate the importance of kernels based on spike activity, advancing the performance of pruned SNNs to a new level.
However, this evaluation criterion exhibits high dependency on inputs and may not accurately reflect the importance of kernels.


\paragraph{Compression with joint quantization and pruning}
Several studies have explored combining quantization and pruning to maximize the compression of SNNs.
First, Chen et al. ("Joint Quantization and Pruning for Efficient SNNs") adopt the STDP learning rule and a predefined pruning threshold to remove insignificant connections, and then quantizes retained important weights.
Then, Zhang et al. ("Quantized Pruning Framework for Energy-Efficient SNNs") perform principal component analysis on membrane potentials for spatial pruning and gradually decreases the time step during training for temporal pruning. They also use post-training quantization to compress retained weights.
Moreover, Wang et al. ("Optimization-Based Joint Quantization and Pruning for Efficient SNNs") formulate pruning and quantization as a constraint optimization problem in supervised learning, and address it with the alternating direction method of multipliers.
However, these existing studies combining quantization and pruning face two main problems.
Firstly, the unstructured pruning methods in Chen et al. ("Unstructured Pruning for Efficient SNNs") and Zhang et al. ("Energy-Efficient Unstructured Pruning Framework for SNNs") require specialized hardware for efficient acceleration.
Secondly, Wang et al. ("Quantized Pruning Framework for Energy-Efficient SNNs") only evaluate their method on very simple datasets, and Zhang et al. ("Optimization-Based Joint Quantization and Pruning for Efficient SNNs") and Liu et al. ("Structured Pruning for Energy-Efficient SNNs") only extend their methods to CIFAR (88.6\% and 87.84\% accuracy on CIFAR-10 with 5, 3 bits respectively), leading to significant room for improvement in both performance and efficiency.
% achieving