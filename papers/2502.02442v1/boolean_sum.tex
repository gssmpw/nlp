%-----------------------------------------------------------------------
% Beginning of jams-l-template.tex
%-----------------------------------------------------------------------
%
%     This is a topmatter template file for JAMS for use with AMS-LaTeX.
%
%     Templates for various common text, math and figure elements are
%     given following the \end{document} line.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%     Remove any commented or uncommented macros you do not use.

\documentclass{jams-l}
%\input{math_commands}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}

\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{comment}
\newcommand\todo[1]{\textcolor{red}{#1}}
\usepackage{tikz}
\usetikzlibrary{arrows}

\usepackage{hyperref}       % hyperlinks

%     If you need symbols beyond the basic set, uncomment this command.
%\usepackage{amssymb}

%     If your article includes graphics, uncomment this command.
%\usepackage{graphicx}

%     If the article includes commutative diagrams, ...
%\usepackage[cmtip,all]{xy}


%     Update the information and uncomment if AMS is not the copyright
%     holder.
%\copyrightinfo{2009}{American Mathematical Society}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

%\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{obs}[theorem]{Observation}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem*{question}{Question}
\newtheorem*{corollary}{Corollary}
\newtheorem*{remark*}{Remark}
\newtheorem*{claim*}{Claim}
\newtheorem*{assumption}{Assumption}
\newtheorem*{step}{Step}
\newtheorem*{conjecture}{Conjecture}
\newtheorem*{remark}{Remark}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\ag}{\mathsf{agree}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cS}{\mathsf{S}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\perm}{\mathsf{perm}}
\renewcommand{\det}{\mathsf{det}}
\newcommand{\dist}{\mathsf{dist}}
\newcommand{\eps}{\varepsilon}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\dc}{\mathsf{dc}}


%\theoremstyle{remark}
%\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}

\begin{document}

\title{The algebraic cost of a boolean sum}

%    Only \author and \address are required; other information is
%    optional.  Remove any unused author tags.

%    author one information
% \author[short version for running head]{name for top of paper}

%% Authors are commented since STACS process is double-blind
\author{Ian Orzel}
\address{Department of Computer Science, The University of Copenhagen}

\author{Srikanth Srinivasan}\thanks{Srikanth Srinivasan was funded by the European Research Council (ERC) under grant agreement no. 101125652 (ALBA). Amir Yehudayoff is supported by a DNRF chair grant, and partially supported by BARC. SÃ©bastien Tavenas is supported by ANR-22-CE48-0007.}
\address{Department of Computer Science, The University of Copenhagen}




\author{S\'{e}bastien Tavenas}
\address{Charg\'e de Recherche CNRS au Laboratoire LAMA de \'lUniversit\'e Savoie Mont Blanc}


\author{Amir Yehudayoff}
\address{Department of Computer Science, The University of Copenhagen,
and Department of Mathematics, Technion-IIT}
%\email{amir.yehudayoff@gmail.com}


\begin{abstract}
The P versus NP problem is about the computational power
of an existential $\exists_{w \in \{0,1\}^n}$ quantifier.
The VP versus VNP problem is about the power of a boolean sum $\sum_{w \in \{0,1\}^n}$ operation.
We study the power of a single boolean sum $\sum_{w \in \{0,1\}}$,
and prove that in some cases the cost of eliminating this sum is large.
This identifies a fundamental difference between the permanent and the determinant.
This investigation also leads to the simplest proof we are aware of that the permanent is VNP-complete.
\end{abstract}

\maketitle




%
%\bibliographystyle{amsplain}
%\bibliography{bib.bib}

\section{Introduction}

The P versus NP problem is about the computational power
of an $\exists_{w \in \{0,1\}^n}$ quantifier or equivalently
an $\bigvee_{w \in \{0,1\}^n}$ operation.
The algebraic variant ---the VP versus VNP problem--- 
asks about the computational power of a $\sum_{w \in \{0,1\}^n}$ operation. 
The algebraic variant seems easier to approach because
algebra provides rich structures to work with. In this paper \(\mathbb{F}\) is a field of characteristic zero.
We use standard terminology from algebraic complexity theory; see~\cite{burgisser2013completeness,shpilka2010arithmetic,wigderson2006p} and references within
for more details and background.%\footnote{The VP versus VNP problem is about multivariate polynomials in $\F[\X]$ over an underlying field $\F$;
%in this work, for simplicity and concreteness, we work with the field of {\color{red} real numbers
%$\F=\R$}.}


The class VP captures efficient computations of polynomials. 
The class VNP is defined by adding a boolean sum
to the class VP. 
This brings us to the question that guides this work:
\begin{quote}
{\em What is the algebraic cost of boolean summation?}
\end{quote}

A fundamental tool in computational complexity theory
is reduction between problems~\cite{karp1975computational}.
Reductions allow us to compare the complexities of different problems. 
In the algebraic setting, reductions are projections.
The polynomial $f \in \F[\X]$ is a projection of $g \in \F[\Y]$
if there is a map $\phi : \Y \to \X \cup \F$
so that $f = \lambda (g \circ \phi)$, where \(\lambda \in \mathbb{F}\).
The projection is called {\em simple} if for each $x \in \X$
the cardinal of $\phi^{-1}(x)$ is at most one.
%{\color{red} Simple projections
%do not lead to an exponential blowup upon repeated compositions.}

For boolean summation, we use the following notation. 
For $S \subseteq \X$,
denote by $f_S$ the polynomial in the variables $\X \cup \{y\}$, where $y$ is a new variable,
that is obtained from $f$ by the substitution $x= y$ for all $x \in S$.
For example, $(x_1+x_2^2+x_3)_{\{x_1,x_2\}} = y+y^2 + x_3$.
Denote by $\Sigma_S f$ the polynomial 
$$\Sigma_S f = f_S |_{y=0} + f_S|_{y=1}.$$
For example, $\Sigma_{\{x_1,x_2\}} (x_1+x_2^2+x_3) = 2 + 2 x_3$.
The variables in $S$ do not appear in $f_S$ and $\Sigma_S f$.

To measure the cost of boolean sums, 
we need to work with families $\cF = \{F_n \in \F[\X_n]\}_{n \in \N}$ of polynomials.
The two central polynomial families in algebraic complexity are
the permanent and the determinant:
$$\perm(X) = \perm_n(X) = \sum_{\pi} \prod_i x_{i,\pi(i)}$$
and
$$\det(X) = \det_n(X) = \sum_\pi (-1)^{\mathsf{sign}(\pi)} \prod_i x_{i,\pi(i)},$$
where $X = (x_{i,j})$ is an $n \times n$ matrix of variables,
the sum is over permutations $\pi$ of $[n]$,
and the product is over $i \in [n]$. 
The permanent is VNP-complete, so that VP $\neq$ VNP if and only if $\perm$ is not in VP~\cite{valiant1979completeness}.
It also appears in many places in computer science
because it encapsulates many counting problems
(see e.g.~\cite{valiant1979complexity,toda1991pp,aaronson2011linear} and references within);
by definition, it counts the number of perfect matchings in a bipartite graph.
The VP versus VNP problem is
(essentially) about the relationship between the permanent and
the determinant (see~\cite{valiant1979completeness,mulmuley2011p,malod2008characterizing} and
references within).
The determinant defines a complexity measure;
the determinantal complexity $\dc(f)$ of a polynomial $f$ is
the minimum $k$ so that $f$ is a projection of $\det_k$.
It is known that $\dc(f)<\infty$ for all $f$.
The VP versus
VNP problem is roughly captured by {\em what is $\dc(\perm_n)$?}
Proving that $\dc(\perm_n)$ is super-polynomial in $n$ is one of the major
open problems in computer science. 
The best lower bound we know is $\dc(\perm_n) \geq \frac{n^2}{2}$;
see~\cite{mignon2004quadratic,cai2010quadratic}.

The aim of this paper is to try and characterize the properties of the permanent that make it VNP-complete and distinguish it from the determinant. We start by outlining the classic proof of the VNP-completeness of the permanent~\cite{burgisser2013completeness}. It is known that any polynomial \(f_n\) in VNP can be written as 
\[
    f_n(X) = \sum_{y_1 \in \{0,1\}} \sum_{y_2 \in \{0,1\}} \cdots \sum_{y_{m} \in \{0,1\}} g_{m'}(X,Y)
\]
where \(m\), \(m'\) are polynomial in \(n\), and \(g_{m'}\) can be computed by an arithmetic formula
of size $s \leq \mathsf{poly}(n)$. 
The permanent can efficiently simulate formulas~\cite{valiant1979completeness};
that is, there is an $s+1 \times s+1$ matrix \(D'\) whose entries are elements of \(X\cup Y \cup \mathbb{F}\) such that \(g_{m'}(X,Y) = \perm(D')\). 
%(notice that so far, a similar \(D'\) can be obtained for the determinant family). 
To reduce \(f_n\) to the permanent, one just needs to find a polynomial size matrix \(E\) such that 
\begin{equation*}
     \perm(E) = \sum_{y_1 \in \{0,1\}} \sum_{y_2 \in \{0,1\}} \cdots \sum_{y_{m} \in \{0,1\}} \perm(D').
\end{equation*}
The VNP-completness proof is thus really based on a reduction of a boolean sum of permanents to a single permanent. 

The standard proof goes from $D'$ to $E$ in ``one big step''. To better understand the process and identify what fails for the determinant, we will break down this transformation into smaller steps. 
Instead of a sum of $m$ boolean sums,
let us consider a single boolean sum. 
In this case, the standard proof constructs
for each matrix~$\tilde{D}$,
a matrix $\tilde{E}$ so that \(\perm(\tilde{E}) = \sum_{y\in\{0,1\}} \perm(\tilde{D}).\) 
The size of $\tilde{E}$ is the size of \(\tilde{D}\) plus  $O(\text{the number of occurrences of the variables \(y\) in the matrix \(\tilde{D}\)})$. 

In this sense, it would be ``nice" to assume that a $y$ variable does not occur many times in the matrix $\tilde{D}$.
Returning to the case of $m$ variables, 
it is easier to work with a matrix \(D\) where each $y_i$ appears once (we call such matrices read-once). We replace each variable \(y_i\) by a set of distinct variables \(S_i\), and the goal 
is to construct $E$ so that
\begin{align*}
    \perm(E) = \Sigma_{S_1} \Sigma_{S_2} \cdots \Sigma_{S_m} \perm(D).
\end{align*} 
We also work with simple reductions, since it allows us to compose them without increasing the number of occurrences of the variables. We can even refine this decomposition (this step is not explicit
in the standard proof). 
Lemma~\ref{lemma:sigma2} below shows that for a read-once matrix \(D_0\) of size $d_0$ and a set \(S\) of cardinal \(s\), there is a read-once matrix \(D_1\) of size $d_1 = d_0+ O(s)$ such that 
\begin{align*}
    \sum_S \perm(D_0) = \sum_{T_1}\sum_{T_2}\dots\sum_{T_s} \perm(D_1)
\end{align*} where each \(T_i\) has size only two. In Observation~\ref{obs:d}, we remark that we can not hope to get a similar decomposition where sums have size only one.
Such results allow us to safely assume that a $y$ variable occurs at most twice in the matrix we sum over.

All the steps we described so far were done for the permanent, but they can also be achieved by the determinant. 
Consequently, the difference between the permanent and the determinant must boil down to eliminating
a boolean sum over a single variable that appears twice.
Indeed, in Theorem~\ref{thm:main} we show that the cost for the permanent is constant
and for the determinant is linear in the size of the matrix (see Sections~\ref{sec:perm} and~\ref{sec:det}).
If we compose these costs a polynomial number of times,
the cost for the permanent stays polynomial
and, for the determinant, it becomes exponential. 


This approach implies a new proof of the VNP-hardness of the permanent. Even though it continues to be based on the construction of so-called ``gadgets", the proof is more modular
and exposes a fundamental difference between
permanent and determinant. 
Our proof uses one gadget for decomposing the full summation to summations of size two (going from \(D_0\) to \(D_1\) above). One can compare this with the ``iff-coupling'' gadget in~\cite{burgisser2013completeness}. Then, we need another gadget to simulate a sum of size two. 
In the original proof, the ``rosette'' gadget takes care of the sum. The ``rosette'' gadget should
properly interact with the ``iff-coupling'' gadget,  making the overall description more complicated. 

We are ready for a central definition. 
The algebraic additive cost of boolean summation over a family $\cF$ is measured via the map $\alpha_\cF : \N \to \N\cup\{\infty\}$,
defined as follows: for each $s \in \N$, let $\alpha_\cF(s)$ be the minimum integer\footnote{If there is
no such integer, then it is infinite.}
$k$ so that for every $n$ and $S \subset \X_n$ of size $|S| \leq s$,
the polynomial $\Sigma_S F_n$ is a simple projection of $F_{n+k}$.
In other words, $\alpha_\cF(s)$ is the additive $\cF$-cost of performing a boolean sum
over $s$ variables. 

% {\em What distinguishes between $\perm$ and $\det$?}
% Research identified two properties of a family $\cF$
% that make it VNP-complete:\footnote{The 
% bounds $s+1,O(s)$ below can be relaxed, but are chosen for concreteness
% and they suffices for the present discussion.}


% \medskip

% \noindent
% ---{\em ``simulate formulas''}.
% We say that $\cF$ simulates formulas if every polynomial 
%  that can be computed by an algebraic formula of size $s$
% is a projection of $F_{s+1}$.

% \medskip

% \noindent
% ---{\em ``simple boolean summation''}.
% We say that $\cF$ allows simple boolean summation if
% $\alpha_{\cF}(s) \leq O(s)$.

% \medskip

As explained before, both the permanent and the determinant are known to simulate formulas;
the proofs in both cases are basically identical.

\begin{theorem*}[\cite{valiant1979completeness}]
Both $\perm$ and $\det$ simulate formulas. 
\end{theorem*}

The key difference between them must, therefore, reside in the cost of boolean summation.
We first observe that for singletons, a boolean summation is ``free''.

\begin{obs}
\label{obs:d}
$\alpha_{\perm}(1) = \alpha_{\det}(1) = 0$.
\end{obs}

Next, we observe that it suffices to sum over ``variables that appear twice''.
The following lemma is an algebraic analog of the well-known
fact that MAX 2-SAT is NP-complete. 

\begin{lemma}
\label{lemma:sigma2}
For $\cF \in \{\perm,\det \}$,
if $\alpha_{\cF }(2) < \infty$
then $\cF$ is VNP-complete. 
\end{lemma}


The main result of our work, however, 
is the following. 
\begin{theorem}
\label{thm:main}
$\alpha_{\perm }(2) \leq 3$ and
$\alpha_{\det }(2) = \infty$. 
\end{theorem}

The theorem highlights a key difference between $\perm$ and $\det$.
It shows, in a strong sense,
that the known proofs that $\perm$ is VNP-complete fail for $\det$.
In addition, it leads to the simplest proof of the VNP-completeness of the permanent that we know of.
The standard proofs that $\perm$ supports efficient boolean summation
construct various ``gadgets'', 
which are directed graphs with certain properties.
The above theorem shows that there are no such constructions
for $\det$.
%(when paths are also signed). 

The proof of the theorem is based on the following bound
$$\dc(\Sigma_{\{x_{1,1},x_{2,2}\}} \det(X)) \geq 2(\dc(\det(X))-2).$$
It shows that a single boolean summation 
doubles the computational complexity. 
We are led to the following direct-sum-type problem:
{\em how is the cost of $\Sigma_{S_1} \Sigma_{S_2} \ldots \Sigma_{S_m}$
related to the costs of each $\Sigma_{S_i}$?}


As a final remark, we mention that a similar investigation can be performed in the boolean setting,
where instead of $\Sigma$ we have $\exists$. 

\subsection{Related Work} 
At a high level, our result shows a difference between the computational power of the permanent polynomial and the determinant polynomial. Other works in a similar spirit have been conducted, all of which have the ultimate goal of helping us understand why VP and VNP are likely different from each other. 

The results of von zur Gathen~\cite{gathen} and Mignon and Ressayre~\cite{mignon2004quadratic} (see also~\cite{cai2010quadratic}) show a difference in the ``geometry of the underlying varieties'' of the two polynomials. This difference implies a quadratic lower bound on the determinantal complexity of the permanent (we shall also use techniques from~\cite{mignon2004quadratic}).

Recent work of Hrub\v{e}s and Joglekar~\cite{HJ} points to a distinction in reductions to \emph{read-once} determinants and permanents. This work also gives a lower bound on the cost of certain reductions which are incomparable to ours. 

Our work is different from the two lines of works above in that we seek to justify why one might expect an \emph{exponential} blow-up is required in reductions from the permanent to the determinant.


The work of Landsberg and Ressayre~\cite{LR} gives an exponential lower bound in a restricted setting. They showed that any reductions from the permanent to the determinant that preserve the underlying symmetries must have exponential cost. Our results imply a similar bound within the general framework of Valiant's reduction, but without assuming symmetry.
Our bounds are restricted in a different way; they hold just for a particular type
of reductions that are built by ``iteratively 
applying a single building block''.


  
\section{Single variable summation}

The explanation of Observation~\ref{obs:d} is that
\begin{align*}
& \Sigma_{\{x_{1,1}\}} \det(X) \\
& = \det \left[ 
\begin{array}{ccccc}
 0 &  x_{1,2} &  x_{1,3} & \ldots &  x_{1,n} \\
x_{2,1} & x_{2,2} & x_{2,3} & \ldots & x_{2,n} \\
x_{3,1} & x_{3,2} & x_{3,3} & \ldots & x_{3,n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & x_{n,3} & \ldots & x_{n,n} 
\end{array}
\right]
+ \det \left[ 
\begin{array}{ccccc}
1 &  x_{1,2} &  x_{1,3} & \ldots &  x_{1,n} \\
x_{2,1} & x_{2,2} & x_{2,3} & \ldots & x_{2,n} \\
x_{3,1} & x_{3,2} & x_{3,3} & \ldots & x_{3,n} \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & x_{n,3} & \ldots & x_{n,n} 
\end{array}
\right] \\
% & = 2 \det \left[ 
% \begin{array}{ccccc}
%  &  x_{1,2} &  x_{1,3} & \ldots &  x_{1,n} \\
% x_{2,1} & x_{2,2} & x_{2,3} & \ldots & x_{2,n} \\
% x_{3,1} & x_{3,2} & x_{3,3} & \ldots & x_{3,n} \\
% &&& \ldots & \\
% x_{n,1} & x_{n,2} & x_{n,3} & \ldots & x_{n,n} 
% \end{array}
% \right]
% + \det \left[ 
% \begin{array}{ccccc}
% 1 &   &   &  &   \\
%  & x_{2,2} & x_{2,3} & \ldots & x_{2,n} \\
%  & x_{3,2} & x_{3,3} & \ldots & x_{3,n} \\
% &&& \ldots & \\
%  & x_{n,2} & x_{n,3} & \ldots & x_{n,n} 
% \end{array}
% \right] \\
& = 2 \det \left[ 
\begin{array}{ccccc}
1/2 & x_{1,2} & x_{1,3} & \ldots & x_{1,n} \\
x_{2,1} & x_{2,2} & x_{2,3} & \ldots & x_{2,n} \\
x_{3,1} & x_{3,2} & x_{3,3} & \ldots & x_{3,n} \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & x_{n,3} & \ldots & x_{n,n} 
\end{array}
\right] .
\end{align*}
A similar calculation holds for the permanent. 

\section{Two variables suffice for VNP-completness}\label{sec:two_sum}

The aim of this section is to prove Lemma~\ref{lemma:sigma2}
that shows that summation over two variables suffices for VNP-completeness. 
We prove it for the determinant
(the proof for the permanent is similar).  
The following construction
shows how to replace a boolean summation over $m$
variables by $m$ boolean summations over two variables. 

\begin{obs}
Let $f \in \F[\X]$ and $S = \{x_1,\ldots,x_m\} \subseteq \X$.
Let $y_1,\ldots,y_m$ be $m$ new variables, and define 
$$g(y) = \Big( \prod_{i \in [m]} y_i \Big) + \Big( \prod_{i \in [m]}(1-y_i) \Big) .$$
Then
$$\Sigma_{\{x_1,y_1\}} \Sigma_{\{x_2,y_2\}} \ldots \Sigma_{\{x_m,y_m\}} g f = \Sigma_S f.$$
Indeed, on the l.h.s.\ there is a sum over $2^m$ terms,
but $g$ picks exactly two of them that perfectly
agree with the r.h.s.\
\end{obs}

The observation allows to replace a boolean sum over many variables
by several sums, each over just two. 


\begin{obs}
Let $g$ be the polynomial defined above. 
Then, there is a $2m \times 2m$ matrix $A$ with entries in
$\{y_1,\ldots,y_m,0,1,-1\}$ so that %$a \leq 3m$,
each $y_i$ appears once in $A$ and
$$g = \det(A).$$
The matrix $A$ is the $m$-variate version of the following matrix:
$$ A = \left[ 
\begin{array}{cc|cc|cc|cc}
-1 & 1 & -1 & &&&&   \\
-1 & y_1 && &&&&  \\
\hline
&& -1 & 1  & -1 &&&   \\
&& -1 & y_2  &&&&  \\
\hline
&&&& -1 & 1  & -1 &  \\
&& && -1 & y_3  &&  \\
\hline
1 &&&& && -1 & 1 \\
&&&& && -1 & y_4 \\
\end{array}
\right] .$$

To see that \(\det(A)\) is the polynomial \(g\), it is easier to see the determinant as a signed sum of the weights of the cycles covers of the graph \(G_A\) whose \(A\) is the adjacency matrix (see Figure~\ref{fig:GA})
\begin{align*}
    \det(A) = \sum_{C \text{ cycles cover}} \left[ \prod_{\sigma \text{ cycle of } C} (-1)^{1+\text{length of }C}\rm{weight}(C) \right].
\end{align*}
The reader could find more details about this characterization by cycles covers in~\cite{mahajan1999determinant}. The only cycle which contains a dashed edge is exactly the union of the dashed edges. So the only cover that contains this cycle also contains the \(m\) \(y_i\)-loops. It corresponds to one cover of signed weight \(\left((-1)^{m+1}(-1)^{m-1}\right)\prod_{i=1}^m y_i = \prod_{i=1}^m y_i\). Otherwise, the covers contain no dashed edges and are given by \(m\) disjoint graphs. The signed weight is the product of them: \(\prod_i((-1)(-1)+(1)(-y_i)) = \prod_i(1-y_i)\). The sum of the covers gives exactly the polynomial \(g\).
\begin{figure}
    \centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
                    thick,main node/.style={circle,draw}]

  \node[main node] (1) {};
  \node[main node] (2) [right of=1] {};
  \node[main node] (3) [below of=2] {};
  \node[main node] (4) [below of=1] {};
  \node[main node] (1b) [above left of=1, xshift=0.6cm, yshift=-0.6cm] {};
  \node[main node] (2b) [above right of=2, xshift=-0.6cm, yshift=-0.6cm] {};
  \node[main node] (3b) [below right of=3, xshift=-0.6cm, yshift=0.6cm] {};
  \node[main node] (4b) [below left of=4, xshift=0.6cm, yshift = 0.6cm] {};

  \path[every node/.style={font=\sffamily\small}]
    (1) edge [bend left, dashed] node {$-1$} (2)
        edge [bend left] node {$1$} (1b)
        edge [out=330,in=300,looseness=15] node[below] {$-1$} (1)
    (1b) edge [bend left] node {$-1$} (1)
         edge [out=150,in=120,looseness=15] node[left] {$y_1$} (1b)
    (2) edge [bend left, dashed] node {$-1$} (3)
        edge [bend left] node {$1$} (2b)
        edge [out=240,in=210,looseness=15] node[below] {$-1$} (2)
    (2b) edge [bend left] node {$-1$} (2)
         edge [out=60,in=30,looseness=15] node[right] {$y_2$} (2b)  
    (3) edge [bend left, dashed] node {$-1$} (4)
        edge [bend left] node {$1$} (3b)
        edge [out=150,in=120,looseness=15] node[above] {$-1$} (3)
    (3b) edge [bend left] node {$-1$} (3)
         edge [out=330,in=300,looseness=15] node[right] {$y_3$} (3b)   
    (4) edge [bend left, dashed] node {$1$} (1)
        edge [bend left] node {$1$} (4b)
        edge [out=60,in=30,looseness=15] node[above] {$-1$} (4)
    (4b) edge [bend left] node {$-1$} (4)
         edge [out=240,in=210,looseness=15] node[left] {$y_4$} (4b);
\end{tikzpicture}
    \caption{The graph \(G_A\)}
    \label{fig:GA}
\end{figure}
\end{obs}

\begin{remark}
For the permanent, every $-1$ above the diagonal of the matrix $A$ should be changed into a $1$, and the $2 \times 2$ blocks into
$\left[ 
\begin{array}{cc}
-1 & 1 \\ 
1 & y_1  \\
\end{array}
\right]$.

\end{remark}


\begin{proof}[Proof of Lemma~\ref{lemma:sigma2}]
Assume that $\alpha_{\det }(2) = C < \infty$.
Because $\det$ simulates formulas, we can write every $n$-variate $f$ in VNP (see~\cite{burgisser2013completeness,shpilka2010arithmetic})
$$f = \Sigma_{S_1} \ldots \Sigma_{S_\ell} \det(D)$$
where $D$ is a $d \times d$ matrix with $d \leq \mathsf{poly}(n)$
so that each entry in $D$ is a variable or a field element,
$S_1,\ldots,S_\ell$ are pairwise disjoint, and each variable in \(S_i\) appears at most once in \(D\).
Let $g,A$ be as above for $m = |S_\ell|$.
Consider the $b_\ell \times b_\ell$ matrix 
with $b_\ell = d + 2|S_\ell|$:
$$B_\ell = 
\left[ 
\begin{array}{cc}
A &  \\ 
 & D \\
\end{array}
\right].$$
By the above,
$g  \cdot \det(D) = \det(B_\ell)$ and
\begin{align*}
\Sigma_{S_\ell} \det(D)
& = \Sigma_{T_1} \Sigma_{T_2} \ldots \Sigma_{T_{|S_\ell|}} \det (B_\ell)
\end{align*}
where the size of each $T_i$ is two
(and each variable not in $S_\ell$ appears once in $B_\ell$). 
Because $C<\infty$,
\begin{align*}
\Sigma_{S_\ell} \det(D)
& = \det (D_\ell)
\end{align*}
where $D_\ell$ is a $d_\ell \times d_\ell$ matrix 
with $d_\ell \leq d + (C+2) \cdot |S_\ell|.$ 
Continuing in this manner, we can eliminate 
all of $\Sigma_{S_1} \ldots \Sigma_{S_\ell}$.
Here we used the fact that we are dealing with simple projections and that \(\sum_i \lvert S_i\rvert \leq d^2\). 
We conclude that
$\dc(f) \leq d + (C+2)d^2 \le \mathsf{poly}(n)$ so that $\det$ is VNP-complete. 
\end{proof}


\section{The permanent is VNP-complete}
\label{sec:perm}

We now describe a proof of the well-known fact that $\perm$ is VNP-complete.
By Lemma~\ref{lemma:sigma2}, the VNP-completeness of $\perm$ reduces to 
the following construction.\footnote{It suffices to consider two variables that are not in the same row or column
by the construction of the matrices $B_\ell$ in the previous subsection.}

\begin{claim*}
If $X = (x_{i,j})$ is an $n \times n$ matrix of variables then
$$\Sigma_{\{x_{1,1},x_{2,2}\}} \perm(X) = \perm(E)$$
where
\begin{align*}
E =
\left[ 
\begin{array}{ccc|cccc}
1 & 1 & 1 &&&& \\ 
1 & 0 & -1 & 1 &&&  \\
-\tfrac{1}{2} & \tfrac{1}{2} & \tfrac{3}{2} &&1&&  \\
\hline
 & 1 &  & 0 &x_{1,2} &x_{1,3}& \cdots  \\
 &  &  1 & x_{2,1} & 0 &x_{2,3}&  \\
 &  &   & x_{3,1} & x_{3,2} &x_{3,3}&  \\
 & &  & \vdots & && \ddots \\
\end{array}
\right] 
\end{align*}
\end{claim*}

\begin{proof}
To see this, denote by $X[b]$ the matrix
$X$ after the substitution $x_{1,1}=x_{2,2} =b$ for $b \in \{0,1\}$.
For $S \subseteq \{1,2\}$, denote by $X[b]_{-S}$
the matrix $X[b]$ after deleting the rows and columns in $S$.
We have
\begin{align*}
\perm(X[1])
& = \perm \left( \left[ 
\begin{array}{cccc}
1 & x_{1,2} &x_{1,3}&  \\
 x_{2,1} & 1 &x_{2,3}&  \\
 x_{3,1} & x_{3,2} &x_{3,3}&  \\
& && \ddots \\
\end{array}
\right] \right) \\ 
& = 
\perm(X[0])
+ \perm(X[0]_{-\{1\}})
+ \perm(X[0]_{-\{2\}})
+ \perm(X[0]_{-\{1,2\}}) .
\end{align*}

Now, consider a permutation $\pi$ such that the associated value of the permutation on $E$ is nonzero. We can then notice that such a permutation falls into one of four categories:
\begin{itemize}
    \item $\pi({\{1, 2, 3\}}) = \{1,2,3\}$,
    \item $\pi(2) = 4$ and $\pi({\{1, 3\}}) = \{1,3\}$,
    \item $\pi(3) = 5$ and $\pi({\{1, 2\}}) = \{1,2\}$, or
    \item $\pi(2) = 4$, $\pi(3) = 5$, and $\pi(1) = 1$.
\end{itemize}
Let $U$ be the $3 \times 3$ upper-left block
\begin{align*}
    U = \begin{bmatrix}
        1 & 1 & 1 \\
        1 & 0 & -1 \\
        -\frac{1}{2} & \frac{1}{2} & \frac{3}{2}
    \end{bmatrix}.
\end{align*}
For $S,T \subseteq \{1,2,3\}$ with $\lvert S\rvert = \lvert T\rvert$, we denote by $U_S^T$ the sub-matrix of $U$ obtained by keeping the rows of $S$ and the columns of $T$. The matrix $U$ is chosen such that
\begin{align*}
    & \perm(U_{\{1,2\}}^{\{1,3\}}) = \perm(U_{\{1,3\}}^{\{1,2\}}) = 0, \\
    & \perm(U_{\{1\}}^{\{1\}}) = \perm(U_{\{1,2\}}^{\{1,2\}}) = \perm(U_{\{1,3\}}^{\{1,3\}}) = 1, \\ 
    & \perm(U) = 2.
\end{align*}

In the first of the above 4 cases, it must be that $\pi({\{4, 5, \ldots, m\}}) = \{4,5,\ldots, m\}$. Therefore, the sum of the weights of these permutations corresponds to
\[
\perm \left( \left[
    \begin{array}{c|c}
      U & \\
      \hline
       & X[0]
    \end{array}
    \right] \right) = 2\perm(X[0]).
\]

In the second case, it must be that $\pi(4) = 2$ and $\pi({\{5, 6, \ldots, m\}}) = \{5,6,\ldots, m\}$. Thus, this case corresponds to
\[
\perm \left( \left[
    \begin{array}{c|c}
      U_{\{1,3\}}^{\{1,3\}} & \\
      \hline
       & X[0]_{-\{1\}}
    \end{array}
    \right] \right) = \perm(X[0]_{-\{1\}}).
\]

In the third case, we have that $\pi(5) = 3$ and $\pi({\{4, 6,7 ,\ldots, m\}}) = \{4,6,7,\ldots, m\}$. Thus, this case corresponds to
\[
\perm \left( \left[
    \begin{array}{c|c}
      U_{\{1,2\}}^{\{1,2\}} & \\
      \hline
       & X[0]_{-\{2\}}
    \end{array}
    \right] \right) = \perm(X[0]_{-\{2\}}).
\]

Finally, in the fourth case, it must be that $\pi(4), \pi(5) \in \{2, 3\}$ and $\pi({\{6,7 ,\ldots, m\}}) = \{6,7,\ldots, m\}$. This case then corresponds to
\[
\perm \left( \left[
    \begin{array}{c|c}
      U_{\{1\}}^{\{1\}} & \\
      \hline
       & X[0]_{-\{1,2\}}
    \end{array}
    \right] \right) = \perm(X[0]_{-\{1,2\}}).
\]

\begin{comment}
In the permanent of $E$, for each non-zero permutation $\pi$, let $t(\pi) \in \{0,1,2\}$ be the number of $1$'s taken by this permutation in the upper-right part. In particular, the number of cells selected by $\pi$ in the upper-left block is $3-t(\pi)$, and the number of $1$'s in the lower-left part is again $t(\pi)$. Consequently, 
% In the permanent of $E$ there are $2^4$ terms corresponding to 
% the four $1$'s on the off-diagonal parts.
% We can ``keep'' one of these $1$'s
% which leads to the deletion of the corresponding row and column.
% If we do not keep it,
% then we can substitute it by a $0$.
% One term (choosing the top-most $1$ and setting the other three to $0$) leads to
% \begin{align*}
% \perm \left( \left[ 
% \begin{array}{ccc|ccc}
% 1 & 1 & 1 &&& \\ 
% - \tfrac{1}{2} & \tfrac{1}{2} & \tfrac{3}{2} &&&  \\
% \hline
%  &  &   & &x_{2,3}&  \\
%  &  &   & x_{3,2} &x_{3,3}&  \\
%  & &  &  && \ddots \\
% \end{array}
% \right] \right) = 0 .
% \end{align*}
% There are other similar terms that lead to zero,
% which we can ignore. 
for a non-zero permutation, the number of $1$'s in the upper-right block is equal to the number of $1$'s in the lower-left block. Let $U$ be the $3 \times 3$ upper-left block
\begin{align*}
    U = \begin{bmatrix}
        1 & 1 & 1 \\
        1 & 0 & -1 \\
        -\frac{1}{2} & \frac{1}{2} & \frac{3}{2}
    \end{bmatrix}.
\end{align*}
For $S,T \subseteq \{1,2,3\}$ with $\lvert S\rvert = \lvert T\rvert$, we denote by $U_S^T$ the sub-matrix of $U$ obtained by keeping the rows of $S$ and the columns of $T$. The different ways of selecting the 1's in the lower-left and upper-right parts correspond to different sub-matrices of \(U\). The matrix $U$ was chosen to ensure that
\begin{align*}
    &\det(U_{\{1,2\}}^{\{1,3\}}) = \det(U_{\{1,3\}}^{\{1,2\}}) = 0, \: \det(U_{\{1\}}^{\{1\}}) = \det(U_{\{1,2\}}^{\{1,2\}}) = U_{\{1,3\}}^{\{1,3\}}) = 1, \:\det(U) = 2.
\end{align*}
The four non-zero ways of choosing the $1$'s in the lower-left and upper-right parts therefore lead to the four terms
\begin{align*}
    & \perm \left( \left[
    \begin{array}{c|c}
      U & \\
      \hline
       & X[0]
    \end{array}
    \right] \right) = 2\perm(X[0]), \\
    & \perm \left( \left[
    \begin{array}{c|c}
      U_{\{1,2\}}^{\{1,2\}} & \\
      \hline
       & X[0]_{-\{2\}}
    \end{array}
    \right] \right) = \perm(X[0]_{-\{2\}}), \\
    & \perm \left( \left[
    \begin{array}{c|c}
      U_{\{1,3\}}^{\{1,3\}} & \\
      \hline
       & X[0]_{-\{1\}}
    \end{array}
    \right] \right) = \perm(X[0]_{-\{1\}}), \\
    & \perm \left( \left[
    \begin{array}{c|c}
      U_{\{1\}}^{\{1\}} & \\
      \hline
       & X[0]_{-\{1,2\}}
    \end{array}
    \right] \right) = \perm(X[0]_{-\{1,2\}}).
\end{align*}
\end{comment}
Summing it together,
\begin{equation*}
    \perm(E) = \perm(X[0]) + \perm(X[1]). \qedhere
\end{equation*}
% Such terms can also lead to zero, like 
% \begin{align*}
% \perm \left( \left[ 
% \begin{array}{cc|c}
% 1 & 1  & \\ 
% -\tfrac{1}{2} & \tfrac{1}{2}  &  \\
% \hline
%     && \ddots \\
% \end{array}
% \right] \right) = 0
% \end{align*}
% (where we deleted row $2$ and column $3$,
% and the other $1$'s are replaced by $0$'s).
% We are left with four terms. 
% Keeping all $1$'s:
% \begin{align*}
% \perm(X[0]_{-\{1,2\}}).
% \end{align*}
% Not keeping all $1$'s:
% \begin{align*}
% \perm \left( \left[ 
% \begin{array}{ccc|cccc}
% 1 & 1 & 1 &&&& \\ 
% 1 & & -1 &  &&&  \\
% -\tfrac{1}{2} & \tfrac{1}{2} & \tfrac{3}{2} &&&&  \\
% \hline
%  &  &  & &x_{1,2} &x_{1,3}&  \\
%  &  &   & x_{2,1} & &x_{2,3}&  \\
%  &  &   & x_{3,1} & x_{3,2} &x_{3,3}&  \\
%  & &  & & && \ddots \\
% \end{array} 
% \right] \right) = 2 \perm(X[0]) .
% \end{align*}
% Keeping the $1$'s in column two and row two:
% \begin{align*}
% \perm \left( \left[ 
% \begin{array}{cc|ccccc}
% 1  & 1 &&& \\ 
% -\tfrac{1}{2}  & \tfrac{3}{2} &&&  \\
% \hline
%    &    & &x_{2,3}&  \\
%    &    & x_{3,2} &x_{3,3}&  \\
%   &   & && \ddots \\
% \end{array}
% \right] \right) = \perm(X[0]_{-\{1\}}) .
% \end{align*}
% Keeping the $1$'s in column three and row three:
% \begin{align*}
% \perm \left( 
% \left[ 
% \begin{array}{cc|ccc}
% 1 & 1  &&& \\ 
% 1 &  &  &&  \\
% \hline
%  &    &  &x_{1,3}&  \\
%  &     & x_{3,1} & x_{3,3}&  \\
%  &   & & & \ddots \\
% \end{array}
% \right] \right) = \perm(X[0]_{-\{2\}}) .
% \end{align*}
% Summing it together,
% \begin{equation*}
% \perm(E) = \perm(X[0])
% + \perm(X[1]). 
%\end{equation*}
\end{proof}

\section{Cost of a boolean summation for the determinant}\label{sec:det}

We now prove that $\alpha_\det(2) = \infty$.
We shall in fact prove the following:
$$\dc ( \Sigma_{\{x_{1,1},x_{2,2}\}} \det_n) \geq 2(n-2).$$
%where $X  = (x_{i,j})$ is an $n \times n$ matrix.
The proof relies on a mechanism 
developed by Mignon and Ressayre to lower bound the determinantal complexity~\cite{mignon2004quadratic}.
For a polynomial $f \in \F[\X]$,
denote by $H_f$ the $|\X| \times |\X|$ Hessian matrix of $f$
defined by
$$(H_f)_{x,x'} = \frac{\partial^2}{\partial x \partial x'} f.$$

\begin{lemma}[\cite{mignon2004quadratic}; see also Lemma 13.3 in~\cite{chen2011partial}]
Let $f$ be a polynomial in the variables $x = (x_1,\ldots,x_n)$
and let $a \in \F^n$ be so that $f(a)=0$ 
then $$\dc(f) \geq \frac{\mathsf{rank}(H_f|_{x=a})}{2}.$$
\end{lemma}

The theorem follows by considering the polynomial
$$f = \Sigma_{\{x_{1,1},x_{2,2}\}} \det_n$$ and
the $n \times n$ matrix
$$A = \left[ 
\begin{array}{ccc}
 & \frac{1}{2} & \\ 
1 &  &  \\
 &  & I \\
\end{array}
\right] , $$
where $I$ is the $(n-2) \times (n-2)$ identity matrix. 
In the two claims below (which complete the proof), let $H = H_f$.

\begin{claim}
\label{clm:0}
$f(A)=0$.
\end{claim}



\begin{proof}[Proof of Claim~\ref{clm:0}]
\begin{align*}
f(A)
& = \det \left( 
\left[ 
\begin{array}{ccc}
 & \frac{1}{2} & \\ 
1 &  &  \\
 &  & I \\
\end{array}
\right]  \right) 
+ 
\det \left( 
\left[ 
\begin{array}{ccc}
1 & \frac{1}{2} & \\ 
1 & 1 &  \\
 &  & I \\
\end{array}
\right]  \right) \\
& = - \frac{1}{2}+ 1 - \frac{1}{2} = 0. \qedhere
\end{align*}
\end{proof}


\begin{claim}
\label{clm:1}
$\mathsf{rank}(H|_{X=A}) \geq 4(n-2)$. 
\end{claim}

\begin{proof}[Proof of Claim~\ref{clm:1}]
Focus on the $4(n-2)$ variables \(\mathcal{V}_i =\{x_{1,i}, x_{2,i}, x_{i,1}, x_{i,2}\}\) for $i>2$.
Furthermore, we consider \(G\) the $4(n-2) \times 4(n-2)$ sub-matrix of \(H\) whose rows and columns are labeled by variables in \(\bigcup_{i>2} \mathcal{V}_i\).
Let $f_0 = \det_n|_{x_{1,1}=x_{2,2} = 0}$
and $f_1 = \det_n|_{x_{1,1}=x_{2,2} = 1}$
so that $f = f_0+f_1$.

An entry in the Hessian of $\det$ is plus/minus the determinant of the \((n-2)\)-minor
obtained by deleting the corresponding rows and columns from $X$
(or zero if the two variables share a row or a column). 

For convenience, denote by $H_{k\ell mp}$ the $(x_{k,\ell},x_{m,p})$-entry in $H$ (and in \(G\)).

We first claim that after substituting $A$,
the sub-matrix \(G\) of $H$ has $n-2$ blocks, each of size $4 \times 4$:
$$\left[ 
\begin{array}{cccc}
G_3 &  & & \\ 
 & G_4 &   & \\
 &  & \ddots & \\ 
&  & & G_n \\ 
\end{array}
\right]$$
where for $i > 2$
$$G_i = \left[ 
\begin{array}{cccc}
H_{1i1i} & H_{1i2i} & H_{1ii1} & H_{1ii2}\\ 
H_{2i1i} & H_{2i2i} & H_{2ii1} & H_{2ii2}\\ 
H_{i11i} & H_{i12i} & H_{i1i1} & H_{i1i2}\\ 
H_{i21i} & H_{i22i} & H_{i2i1} & H_{i2i2}\\ 
\end{array}
\right].$$
Indeed, for $i \neq j$, the matrix obtained from $A$ by deleting row $i$
and column $j$ has a zero row
so its determinant is zero. 
It follows for \(k,\ell \in \{1,2\}\) that
$$(H_f)_{ik\ell j}|_{X=A}
= (H_{f_0})_{ik\ell j}|_{X=A} + (H_{f_1})_{ik\ell j}|_{X=A}
= 0 + 0 = 0.$$
A similar argument holds for the other entries $H_{ikj\ell}$, $H_{ki\ell j}$, and $H_{kij\ell}$. 

Now fix $i$ and focus on $G_i$. 
Entries of the form $1i1i$ and $1i2i$ are also zero.
So, we are left with 
$$G_i = \left[ 
\begin{array}{cccc}
 &  & H_{1ii1} & H_{1ii2}\\ 
 &  & H_{2ii1} & H_{2ii2}\\ 
H_{i11i} & H_{i12i} & & \\ 
H_{i21i} & H_{i22i} &  & \\ 
\end{array}
\right].$$
The matrix $G_i$ is symmetric so it is enough to understand 
$$\left[ 
\begin{array}{cc}
 H_{1ii1} & H_{1ii2}\\ 
H_{2ii1} & H_{2ii2}\\ 
\end{array}
\right].$$
This matrix is a sum of two matrices (one from $f_0$ and one from $f_1$).
For $f_0$, this matrix is 
$$\left[ 
\begin{array}{cc}
 & 1 \\ 
\frac{1}{2} &  \\ 
\end{array}
\right].$$
For $f_1$, this matrix is 
$$\left[ 
\begin{array}{cc}
-1 & 1 \\ 
\frac{1}{2} &  -1 \\ 
\end{array}
\right].$$
The sum of the two matrices is  
$$\left[ 
\begin{array}{cc}
-1 & 2 \\ 
1 & -1 \\ 
\end{array}
\right],$$
which always has rank two.
Rolling back
\begin{equation*}
\mathsf{rank} \left( \left[ 
\begin{array}{cccc}
G_3 &  & & \\ 
 & G_4 &   & \\
 &  & \ddots & \\ 
&  & & G_n \\ 
\end{array}
\right]\right) = 4(n-2). \qedhere
\end{equation*}

\end{proof}


\bibliographystyle{amsplain}
\bibliography{bools}   

\end{document}

