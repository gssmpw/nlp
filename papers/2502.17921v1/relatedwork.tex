\section{Related work}
% \input{samples/section/Background}
We categorize and discuss the related work existing in this field in the subsections below.
% \vspace{-8pt}
\subsection{Gender Fairness in RS}
Gender bias in recommendation systems can exist as systematic discrepancies in the algorithms when recommending items to users of different genders. The challenges of gender bias can have a wide-ranging set of implications, including imbalanced representations of items for different genders, stereotypical recommendations (male-dominated occupations recommended to males more than females), and limitations in user personalization (being recommended items that are not related to users' preferences just because they are male or female). Research on evaluating and addressing gender bias in recommendation systems is an ongoing process. Melchiorre et al. \cite{Melchiorre21} investigate the impact of a common de-biasing strategy called resampling on RS algorithms. This strategy marginally decreases gender bias, with a slight decrease in performance. The authors in \cite{pmlr-v139-gorantla21a,10.1145/3132847.3132938,XIA2019104857} introduce ways to re-rank items to offer a balanced solution that caters to group fairness (for gender and other sensitive attributes) and user preferences.
Historical data that can contain stereotypical movie preferences can intensify certain biases further when used to train traditional recommendation models. For instance, male users display a bias towards \textit{action} movies, which is amplified by recommendation algorithms like UserKNN \cite{TsintzouPT19}. The authors in \cite{Acharyya_Das_Chattoraj_Tanveer_2020}
introduce a framework that fairly predicts the quality of TedTalk speeches by using causal models and counterfactuals to mitigate gender and racial bias. Adversarial fairness where the recommender system is trained to not only make accurate predictions but also make it difficult for the adversary to guess the sensitive attribute, has also been employed to make such systems more fair \cite{LIU2022108058,rus2022closinggenderwagegap}. Recent advancements in this field, have led to the development of fairness-aware recommendation models, with special emphasis on gender bias \cite{yang2020causalintersectionalityfairranking, boratto2022consumer,10.1145/3397271.3401177,zhu18,wei2022comprehensive,10.1145/3442381.3450015}.
\subsection{Evaluating Gender Bias}
\label{subsec:fair_notion}
For evaluating gender bias, the most common fairness definitions employed are the concepts of \textit{Demographic Parity} and \textit{Equal Opportunity}, which are both related to group fairness. Fairness in this context, pertains to equitable treatment across different groups (which can be measured in classification and recommendation tasks).
For group fairness, the idea is to ensure that the predicted outcomes  \(\hat{Y}\) of a model should not be dependent on sensitive attributes like gender $S$. 
For demographic parity, the proportions of each sensitive group (like male and female) receiving positive predictions should be equal. 
For binary classification, demographic parity can be formalized as: 
\[P(\hat{Y}=1 | S=1) = P(\hat{Y}=1 | S=0)\] 
Essentially what this implies is that the positive outcome should be the same for both genders, where 0 may represent male and 1 may represent female or vice versa. 
Equal Opportunity, on the other hand, holds when the model has equal true positive rates across different demographic groups \cite{hardt16}. 
This concept can be formalized as:
\[P(\hat{Y}=1 | S=1, Y=1) = P(\hat{Y}=1 | S=0, Y=1) \]
where \(Y\) represents the true outcome.

Besides these two methods to quantify fairness, some additional concepts (as discussed in \cite{kheya2024pursuitfairnessartificialintelligence}) used include Equalized Odds \cite{hardt16}, Balance for Negative Class \cite{10.1145/3194770.3194776}, Balance for Positive Class \cite{10.1145/3194770.3194776}, Intersectional Fairness \cite{Gohar2023ASO}, Equal Calibration \cite{Chouldechova2016FairPW}
% Predictive parity \cite{10.1145/3194770.3194776}
and Causal-based notions \cite{10.5555/3294996.3295162, Acharyya_Das_Chattoraj_Tanveer_2020}. 



\subsection{Evaluating Consumer-Side Fairness}
 In recommendation systems, fairness can be seen as a multi-sided concept and categorized into three groups: consumers (C-fairness) \cite{{deldjoo_flexible_2021,Melchiorre21,wu20,pmlr-v139-gorantla21a,ghosh21,Edizel20,wan20,hao21}} which is related to the impact of recommendations of the system on protected classes of user, provider (P-fairness) which focuses on ensuring fairness for providers/sellers on a platform and both (CP-fairness) \cite{burke_multisided_2017}. Our work focuses on the consumer-side fairness concept because we want to ensure that recommendations made by models are not biased against a certain gender. Prior research has shown how recommendations can differ in an unfair way based on sensitive attributes of users like gender, age, race, etc. \cite{gupta_questioning_2022,10.1145/3547333,JIN2023101906}. Evaluating bias in these systems, before deploying is thus essential to stop the reinforcement of stereotypes and limiting diverse content. When quantifying consumer side bias in recommendation systems, the most common approach is to adopt the concept of equality of opportunity and focus on the differences in metrics such as recall, precision and/or NDCG \cite{Fu20,10.1145/3442381.3449866,Melchiorre21}. Other papers also employ causal-based fairness notions \cite{wei2022comprehensive,10.1145/3404835.3462943} and demographic parity \cite{10.1145/3292500.3330691,DBLP:journals/corr/abs-1809-09030,boratto2022consumer}. Unlike these metrics, which assume fairness implies equality, \cite{deldjoo_flexible_2021} suggests how, for instance, paid users should be provided better recommendations when compared to free users. They design a set of metrics that can take this disparity into account and then measure fairness accordingly. Another interesting way to measure unfairness is the concept of envy-free fairness, which is achieved when no one user prefers another user's recommendations way more significantly than their own \cite{do2022online}. While these metrics can identify consumer-side bias to an extent, they come with some limitations.

 
Limitations of current metrics used to quantify consumer-side bias in recommendation systems include: (i) over-simplifying the meaning of fairness in RS, which employs various techniques like collaborative and content-based filtering and hybrid methods.
Simple notions can fail to capture disparities that exist across different types of items, for example, genres, when considering movie recommendations. Some metrics that can fall prey to this oversimplification issue include \cite{Weydemann19,NIPS2017_e6384711, Fu20,10.1145/3442381.3449866,islam21,foulds2019bayesianmodelingintersectionalfairness,9101635,do2022online,10.1145/3534678.3539269,Melchiorre21,deldjoo_flexible_2021};
(ii) not utilizing ranks when evaluating recommendation quality can yield considerable issues. 
This is due to the fact that recommendations are displayed one after another, so the items on higher ranks must be more relevant to keep the user satisfied. 
Thus, capturing the quality of recommendations using the ranks reflects a more complete way of evaluating models. 
Some metrics that can fall prey to this issue include \cite{NIPS2017_e6384711,Weydemann19,do2022online,10.1145/3534678.3539269,deldjoo_flexible_2021}.
It is important to state however, that even considering rank can give rise to positional bias, which refers to the tendency of users to favor the items that appear on top of a ranked list. 
Evaluating till a certain position like Recall@k can lead to a skewed sense of assessment of how the recommendation model performs. 
Additionally, metrics like MAP (Mean Average Precision), which normally treats relevance as a binary value (0: not relevant and 1: relevant) can also fail to capture the nuanced relevance that can come from items having multiple categories. 
So, using more than one metric (both with and without using ranks) to quantify bias is essential; 
(iii) relying only on one type of fairness metric can obscure underlying biases and give a false impression of fairness in recommendation systems. 

Hence, using multiple metrics can help uncover hidden biases. Additionally, a model that is fair according to one metric can fail to hold other fairness metrics and risk overlooking subtle unfairness issues.



We want to highlight some of the works that have taken into account different classes when evaluating recommendations \cite{lin2019crankvolumepreferencebias,10.1145/3240323.3240372,10.1145/3292500.3330691,10.1145/3626772.3657794,10068703,10.1145/3308560.3317595,10.1145/3038912.3052612,10.1145/3589334.3648158}.
For instance, \cite{lin2019crankvolumepreferencebias} groups users on certain attributes and items by category, then measures preference ratio, which is the fraction of liked items by a group across categories. Next, they measure the bias disparity by taking the preference and recommended ratios' relative differences. This is close to our work but still doesn't account for the ranks of items and we evaluate the direct comparison of the recommendations for males and females. Additionally, \cite{10.1145/3240323.3240372} introduce calibrated recommendations, ensuring the recommended items align with user preferences without overemphasizing particular categories. The work by \cite{10.1145/3292500.3330691} uses a measure \textit{Skew@k} to evaluate proportions of candidates based on sensitive attributes, and \cite{10.1145/3626772.3657794} uses a fairness metric called Attention Weighted Ranked Fairness (AWRF) \cite{10.1145/3308560.3317595} to ensure there is balance in exposure in different groups of providers. While both these works ensure group fairness, our work is more concentrated on evaluating the distribution of content categories for different groups. Unlike the work by \cite{10.1145/3626772.3657794} that focuses on provider-side fairness, we focus on consumer-side fairness.