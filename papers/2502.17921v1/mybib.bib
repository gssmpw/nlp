@misc{mattu_machine_nodate,
	title = {Machine {Bias}},
	url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
	abstract = {There’s software used across the country to predict future criminals. And it’s biased against blacks.},
	language = {en},year={2016},
	urldate = {2024-01-28},
	journal = {ProPublica},
	author = {Mattu, Jeff Larson, Lauren Kirchner and Surya and Julia Angwin},
	
}
@misc{kheya2024pursuitfairnessartificialintelligence,
      title={The Pursuit of Fairness in Artificial Intelligence Models: A Survey}, 
      author={Tahsin Alamgir Kheya and Mohamed Reda Bouadjenek and Sunil Aryal},
      year={2024},
      eprint={2403.17333},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.17333}, 
}
@inproceedings{10.1145/3383313.3418487,
author = {Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad},
title = {The Connection Between Popularity Bias, Calibration, and Fairness in Recommendation},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3418487},
doi = {10.1145/3383313.3418487},
abstract = {Recently there has been a growing interest in fairness-aware recommender systems including fairness in providing consistent performance across different users or groups of users. A recommender system could be considered unfair if the recommendations do not fairly represent the tastes of a certain group of users while other groups receive recommendations that are consistent with their preferences. In this paper, we use a metric called miscalibration for measuring how a recommendation algorithm is responsive to users’ true preferences and we consider how various algorithms may result in different degrees of miscalibration for different users. In particular, we conjecture that popularity bias which is a well-known phenomenon in recommendation is one important factor leading to miscalibration in recommendation. Our experimental results using two real-world datasets show that there is a connection between how different user groups are affected by algorithmic popularity bias and their level of interest in popular items. Moreover, we show that the more a group is affected by the algorithmic popularity bias, the more their recommendations are miscalibrated.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {726–731},
numpages = {6},
keywords = {Algorithmic bias, Calibration, Popularity bias amplification, Recommender systems},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}
@article{JIN2023101906,
title = {A survey on fairness-aware recommender systems},
journal = {Information Fusion},
volume = {100},
pages = {101906},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101906},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523002221},
author = {Di Jin and Luzhi Wang and He Zhang and Yizhen Zheng and Weiping Ding and Feng Xia and Shirui Pan},
keywords = {Recommender systems, Fairness, Trustworthiness, Survey},
abstract = {As information filtering services, recommender systems have extremely enriched our daily life by providing personalized suggestions and facilitating people in decision-making, which makes them vital and indispensable to human society in the information era. However, as people become more dependent on them, recent studies show that recommender systems potentially own unintentional impacts on society and individuals because of their unfairness (e.g., gender discrimination in job recommendations). To develop trustworthy services, it is crucial to devise fairness-aware recommender systems that can mitigate these bias issues. In this survey, we summarize existing methodologies and practices of fairness in recommender systems. Firstly, we present concepts of fairness in different recommendation scenarios, comprehensively categorize current advances, and introduce typical methods to promote fairness in different stages of recommender systems. Next, after introducing datasets and evaluation metrics applied to assess the fairness of recommender systems, we will delve into the significant influence that fairness-aware recommender systems exert on real-world industrial applications. Subsequently, we highlight the connection between fairness and other principles of trustworthy recommender systems, aiming to consider trustworthiness principles holistically while advocating for fairness. Finally, we summarize this review, spotlighting promising opportunities in comprehending concepts, frameworks, the balance between accuracy and fairness, and the ties with trustworthiness, with the ultimate goal of fostering the development of fairness-aware recommender systems.}
}
@article{10.1145/3547333,
author = {Wang, Yifan and Ma, Weizhi and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
title = {A Survey on the Fairness of Recommender Systems},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3547333},
doi = {10.1145/3547333},
abstract = {Recommender systems are an essential tool to relieve the information overload challenge and play an important role in people’s daily lives. Since recommendations involve allocations of social resources (e.g., job recommendation), an important issue is whether recommendations are fair. Unfair recommendations are not only unethical but also harm the long-term interests of the recommender system itself. As a result, fairness issues in recommender systems have recently attracted increasing attention. However, due to multiple complex resource allocation processes and various fairness definitions, the research on fairness in recommendation is scattered. To fill this gap, we review over 60 papers published in top conferences/journals, including TOIS, SIGIR, and WWW. First, we summarize fairness definitions in the recommendation and provide several views to classify fairness issues. Then, we review recommendation datasets and measurements in fairness studies and provide an elaborate taxonomy of fairness methods in the recommendation. Finally, we conclude this survey by outlining some promising future directions.},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {52},
numpages = {43},
keywords = {survey, fairness, Recommendation}
}
@inproceedings{10.1145/290941.291025,
author = {Carbonell, Jaime and Goldstein, Jade},
title = {The use of MMR, diversity-based reranking for reordering documents and producing summaries},
year = {1998},
isbn = {1581130155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-f.deakin.edu.au/10.1145/290941.291025},
doi = {10.1145/290941.291025},
booktitle = {Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {335–336},
numpages = {2},
location = {Melbourne, Australia},
series = {SIGIR '98}
}
@article{fowler_perspective_2023,
	title = {Perspective {\textbar} {Say} what, {Bard}? {What} {Google}’s new {AI} gets right, wrong and weird.},
	issn = {0190-8286},
	shorttitle = {Perspective {\textbar} {Say} what, {Bard}?},
	url = {https://www.washingtonpost.com/technology/2023/03/21/google-bard/},
	abstract = {Our tech columnist asks Bard, Google’s answer to ChatGPT, questions from Post readers to test what the AI might be useful for.},
	language = {en-US},
	urldate = {2024-02-20},
	journal = {Washington Post},
	author = {Fowler, Geoffrey A.},number={N/A}, volume={N/A},
	month = apr,
	year = {2023},pages={N/A},numpages={N/A},
}

@article{gupta_questioning_2022,
	title = {Questioning {Racial} and {Gender} {Bias} in {AI}-based {Recommendations}: {Do} {Espoused} {National} {Cultural} {Values} {Matter}?},
	volume = {24},
	issn = {1572-9419},
	url = {https://doi.org/10.1007/s10796-021-10156-2},
	doi = {10.1007/s10796-021-10156-2},
	abstract = {One realm of AI, recommender systems have attracted significant research attention due to concerns about its devastating effects to society’s most vulnerable and marginalised communities. Both media press and academic literature provide compelling evidence that AI-based recommendations help to perpetuate and exacerbate racial and gender biases. Yet, there is limited knowledge about the extent to which individuals might question AI-based recommendations when perceived as biased. To address this gap in knowledge, we investigate the effects of espoused national cultural values on AI questionability, by examining how individuals might question AI-based recommendations due to perceived racial or gender bias. Data collected from 387 survey respondents in the United States indicate that individuals with espoused national cultural values associated to collectivism, masculinity and uncertainty avoidance are more likely to question biased AI-based recommendations. This study advances understanding of how cultural values affect AI questionability due to perceived bias and it contributes to current academic discourse about the need to hold AI accountable.},
	number = {5},
	journal = {Information Systems Frontiers},
	author = {Gupta, Manjul and Parra, Carlos M. and Dennehy, Denis},
	month = oct,
	year = {2022},
	pages = {1465--1481},
}
@article{10.1145/3651167,
author = {Rahmani, Hossein A. and Naghiaei, Mohammadmehdi and Deldjoo, Yashar},
title = {A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3651167},
doi = {10.1145/3651167},
abstract = {In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications. Recommender systems are prominent examples of these ML systems that aid users in making decisions. The majority of past literature research on recommender systems fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace. In this article, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework. The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics. For instance, we demonstrate that the system may jointly increase consumer and producer fairness when (un)protected consumer groups are defined on the basis of their&nbsp;activity level and&nbsp;main-streamness, while producer groups are defined according to their popularity level. For empirical validation, through large-scale on eight datasets and four mainstream collaborative filtering recommendation models, we demonstrate that our proposed strategy is able to improve both consumer and producer fairness without compromising or very little overall recommendation quality, demonstrating the role algorithms may play in avoiding data biases. Our results on different group segmentation also indicate that the amount of improvement can vary and is dependent on group segmentation, indicating that the amount of bias produced and how much the algorithm can improve it depend on the protected group definition, a factor that, to our knowledge, has not been examined in great depth in previous studies but rather is highlighted by the results discovered in this study.},
journal = {ACM Trans. Recomm. Syst.},
month = {jun},
articleno = {19},
numpages = {24},
keywords = {Responsible IR, recommender systems, fairness, ranking, bias mitigation, consumer and provider, multi-stakeholder}
}
@article{10.1145/3655631,
author = {Medda, Giacomo and Fabbri, Francesco and Marras, Mirko and Boratto, Ludovico and Fenu, Gianni},
title = {GNNUERS: Fairness Explanation in GNNs for Recommendation via Counterfactual Reasoning},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3655631},
doi = {10.1145/3655631},
abstract = {Nowadays, research into personalization has been focusing on explainability and fairness. Several approaches proposed in recent works are able to explain individual recommendations in a post-hoc manner or by explanation paths. However, explainability techniques applied to unfairness in recommendation have been limited to finding user/item features mostly related to biased recommendations. In this paper, we devised a novel algorithm that leverages counterfactuality methods to discover user unfairness explanations in the form of user-item interactions. In our counterfactual framework, interactions are represented as edges in a bipartite graph, with users and items as nodes. Our bipartite graph explainer perturbs the topological structure to find an altered version that minimizes the disparity in utility between the protected and unprotected demographic groups. Experiments on four real-world graphs coming from various domains showed that our method can systematically explain user unfairness on three state-of-the-art GNN-based recommendation models. Moreover, an empirical evaluation of the perturbed network uncovered relevant patterns that justify the nature of the unfairness discovered by the generated explanations. The source code and the preprocessed data sets are available at https://github.com/jackmedda/RS-BGExplainer.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
keywords = {Recommender Systems, User Fairness, Explanation, Graph Neural Networks, Counterfactual Reasoning}
}
@inproceedings{10.1145/3442381.3449866,
author = {Li, Yunqi and Chen, Hanxiong and Fu, Zuohui and Ge, Yingqiang and Zhang, Yongfeng},
title = {User-oriented Fairness in Recommendation},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3442381.3449866},
doi = {10.1145/3442381.3449866},
abstract = {As a highly data-driven application, recommender systems could be affected by data bias, resulting in unfair results for different data groups, which could be a reason that affects the system performance. Therefore, it is important to identify and solve the unfairness issues in recommendation scenarios. In this paper, we address the unfairness problem in recommender systems from the user perspective. We group users into advantaged and disadvantaged groups according to their level of activity, and conduct experiments to show that current recommender systems will behave unfairly between two groups of users. Specifically, the advantaged users (active) who only account for a small proportion in data enjoy much higher recommendation quality than those disadvantaged users (inactive). Such bias can also affect the overall performance since the disadvantaged users are the majority. To solve this problem, we provide a re-ranking approach to mitigate this unfairness problem by adding constraints over evaluation metrics. The experiments we conducted on several real-world datasets with various recommendation algorithms show that our approach can not only improve group fairness of users in recommender systems, but also achieve better overall recommendation performance.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {624–632},
numpages = {9},
keywords = {AI Ethics, Fairness, Re-ranking, Recommendation System},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}
@article{abdollahpouri2019unfairness,
  title={The unfairness of popularity bias in recommendation},
  author={Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad},
  journal={arXiv preprint arXiv:1907.13286},
  year={2019}
}
@inproceedings{10.1145/3477495.3531959,
author = {Naghiaei, Mohammadmehdi and Rahmani, Hossein A. and Deldjoo, Yashar},
title = {CPFair: Personalized Consumer and Producer Fairness Re-ranking for Recommender Systems},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3477495.3531959},
doi = {10.1145/3477495.3531959},
abstract = {Recently, there has been a rising awareness that when machine learning (ML) algorithms are used to automate choices, they may treat/affect individuals unfairly, with legal, ethical, or economic consequences. Recommender systems are prominent examples of such ML systems that assist users in making high-stakes judgments.  A common trend in the previous literature research on fairness in recommender systems is that the majority of works treat user and item fairness concerns separately, ignoring the fact that recommender systems operate in a two-sided marketplace. In this work, we present an optimization-based re-ranking approach that seamlessly integrates fairness constraints from both the consumer and producer-side in a joint objective framework. We demonstrate through large-scale experiments on 8 datasets that our proposed method is capable of improving both consumer and producer fairness without reducing overall recommendation quality, demonstrating the role algorithms may play in minimizing data biases.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {770–779},
numpages = {10},
keywords = {fair re-ranking, recommendation system, two-sided fairness},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@misc{burke_multisided_2017,
	title = {Multisided {Fairness} for {Recommendation}},
	url = {http://arxiv.org/abs/1707.00093},
	doi = {10.48550/arXiv.1707.00093},
	abstract = {Recent work on machine learning has begun to consider issues of fairness. In this paper, we extend the concept of fairness to recommendation. In particular, we show that in some recommendation contexts, fairness may be a multisided concept, in which fair outcomes for multiple individuals need to be considered. Based on these considerations, we present a taxonomy of classes of fairness-aware recommender systems and suggest possible fairness-aware recommendation architectures.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Burke, Robin},
	month = jul,
	year = {2017},
	note = {arXiv:1707.00093 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:C\:\\Users\\s224091662\\Zotero\\storage\\2JD7ZYQT\\Burke - 2017 - Multisided Fairness for Recommendation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\s224091662\\Zotero\\storage\\EZUZI48T\\1707.html:text/html},
}


@article{deldjoo_flexible_2021,
	title = {A flexible framework for evaluating user and item fairness in recommender systems},
	volume = {31},
	issn = {1573-1391},
	url = {https://doi.org/10.1007/s11257-020-09285-1},
	doi = {10.1007/s11257-020-09285-1},
	abstract = {One common characteristic of research works focused on fairness evaluation (in machine learning) is that they call for some form of parity (equality) either in treatment—meaning they ignore the information about users’ memberships in protected classes during training—or in impact—by enforcing proportional beneficial outcomes to users in different protected classes. In the recommender systems community, fairness has been studied with respect to both users’ and items’ memberships in protected classes defined by some sensitive attributes (e.g., gender or race for users, revenue in a multi-stakeholder setting for items). Again here, the concept has been commonly interpreted as some form of equality—i.e., the degree to which the system is meeting the information needs of all its users in an equal sense. In this work, we propose a probabilistic framework based on generalized cross entropy (GCE) to measure fairness of a given recommendation model. The framework comes with a suite of advantages: first, it allows the system designer to define and measure fairness for both users and items and can be applied to any classification task; second, it can incorporate various notions of fairness as it does not rely on specific and predefined probability distributions and they can be defined at design time; finally, in its design it uses a gain factor, which can be flexibly defined to contemplate different accuracy-related metrics to measure fairness upon decision-support metrics (e.g., precision, recall) or rank-based measures (e.g., NDCG, MAP). An experimental evaluation on four real-world datasets shows the nuances captured by our proposed metric regarding fairness on different user and item attributes, where nearest-neighbor recommenders tend to obtain good results under equality constraints. We observed that when the users are clustered based on both their interaction with the system and other sensitive attributes, such as age or gender, algorithms with similar performance values get different behaviors with respect to user fairness due to the different way they process data for each user cluster.},
	language = {en},
	number = {3},
	urldate = {2024-06-24},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Deldjoo, Yashar and Anelli, Vito Walter and Zamani, Hamed and Bellogín, Alejandro and Di Noia, Tommaso},
	month = jul,
	year = {2021},
	pages = {457--511},
	file = {Full Text PDF:C\:\\Users\\s224091662\\Zotero\\storage\\EEXETF29\\Deldjoo et al. - 2021 - A flexible framework for evaluating user and item .pdf:application/pdf},
}

@unknown{wu20,
author = {Wu, Chuhan and Wu, Fangzhao and Wang, Xiting and Huang, Yongfeng and Xie, Xing},
year = {2020},
month = {06},
pages = {},
title = {Fairness-aware News Recommendation with Decomposed Adversarial Learning}
}

@InProceedings{pmlr-v139-gorantla21a,
  title = 	 {On the Problem of Underranking in Group-Fair Ranking},
  author =       {Gorantla, Sruthi and Deshpande, Amit and Louis, Anand},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3777--3787},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/gorantla21a/gorantla21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/gorantla21a.html},
  abstract = 	 {Bias in ranking systems, especially among the top ranks, can worsen social and economic inequalities, polarize opinions, and reinforce stereotypes. On the other hand, a bias correction for minority groups can cause more harm if perceived as favoring group-fair outcomes over meritocracy. Most group-fair ranking algorithms post-process a given ranking and output a group-fair ranking. In this paper, we formulate the problem of underranking in group-fair rankings based on how close the group-fair rank of each item is to its original rank, and prove a lower bound on the trade-off achievable for simultaneous underranking and group fairness in ranking. We give a fair ranking algorithm that takes any given ranking and outputs another ranking with simultaneous underranking and group fairness guarantees comparable to the lower bound we prove. Our experimental results confirm the theoretical trade-off between underranking and group fairness, and also show that our algorithm achieves the best of both when compared to the state-of-the-art baselines.}
}
@inproceedings{ghosh21,
author = {Ghosh, Avijit and Dutt, Ritam and Wilson, Christo},
title = {When Fair Ranking Meets Uncertain Inference},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462850},
doi = {10.1145/3404835.3462850},
abstract = {Existing fair ranking systems, especially those designed to be demographically fair, assume that accurate demographic information about individuals is available to the ranking algorithm. In practice, however, this assumption may not hold --- in real-world contexts like ranking job applicants or credit seekers, social and legal barriers may prevent algorithm operators from collecting peoples' demographic information. In these cases, algorithm operators may attempt to infer peoples' demographics and then supply these inferences as inputs to the ranking algorithm.In this study, we investigate how uncertainty and errors in demographic inference impact the fairness offered by fair ranking algorithms. Using simulations and three case studies with real datasets, we show how demographic inferences drawn from real systems can lead to unfair rankings. Our results suggest that developers should not use inferred demographic data as input to fair ranking algorithms, unless the inferences are extremely accurate.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1033–1043},
numpages = {11},
keywords = {algorithmic fairness, demographic inference, ethical AI, noisy protected attributes, ranking algorithms, uncertainty},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}
@inproceedings{10.1145/3442381.3450015,
author = {Wu, Le and Chen, Lei and Shao, Pengyang and Hong, Richang and Wang, Xiting and Wang, Meng},
title = {Learning Fair Representations for Recommendation: A Graph-based Perspective},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450015},
doi = {10.1145/3442381.3450015},
abstract = {As a key application of artificial intelligence, recommender systems are among the most pervasive computer aided systems to help users find potential items of interests. Recently, researchers paid considerable attention to fairness issues for artificial intelligence applications. Most of these approaches assumed independence of instances, and designed sophisticated models to eliminate the sensitive information to facilitate fairness. However, recommender systems differ greatly from these approaches as users and items naturally form a user-item bipartite graph, and are collaboratively correlated in the graph structure. In this paper, we propose a novel graph based technique for ensuring fairness of any recommendation models. Here, the fairness requirements refer to not exposing sensitive feature set in the user modeling process. Specifically, given the original embeddings from any recommendation models, we learn a composition of filters that transform each user’s and each item’s original embeddings into a filtered embedding space based on the sensitive feature set. For each user, this transformation is achieved under the adversarial learning of a user-centric graph, in order to obfuscate each sensitive feature between both the filtered user embedding and the sub graph structures of this user. Finally, extensive experimental results clearly show the effectiveness of our proposed model for fair recommendation. We publish the source code at https://github.com/newlei/FairGo.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2198–2208},
numpages = {11},
keywords = {user modeling, graph based recommendation, fairness, fair Representation learning, fair Recommendation},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}
@inproceedings{wan20,
author = {Wan, Mengting and Ni, Jianmo and Misra, Rishabh and McAuley, Julian},
title = {Addressing Marketing Bias in Product Recommendations},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371855},
doi = {10.1145/3336191.3371855},
abstract = {Modern collaborative filtering algorithms seek to provide personalized product recommendations by uncovering patterns in consumer-product interactions. However, these interactions can be biased by how the product is marketed, for example due to the selection of a particular human model in a product image. These correlations may result in the underrepresentation of particular niche markets in the interaction data; for example, a female user who would potentially like motorcycle products may be less likely to interact with them if they are promoted using stereotypically 'male' images.In this paper, we first investigate this correlation between users' interaction feedback and products' marketing images on two real-world e-commerce datasets. We further examine the response of several standard collaborative filtering algorithms to the distribution of consumer-product market segments in the input interaction data, revealing that marketing strategy can be a source of bias for modern recommender systems. In order to protect recommendation performance on underrepresented market segments, we develop a framework to address this potential marketing bias. Quantitative results demonstrate that the proposed approach significantly improves the recommendation fairness across different market segments, with a negligible loss (or better) recommendation accuracy.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {618–626},
numpages = {9},
keywords = {machine learning fairness, marketing bias, recommender systems},
location = {Houston, TX, USA},
series = {WSDM '20}
}
@misc{rus2022closinggenderwagegap,
      title={Closing the Gender Wage Gap: Adversarial Fairness in Job Recommendation}, 
      author={Clara Rus and Jeffrey Luppes and Harrie Oosterhuis and Gido H. Schoenmacker},
      year={2022},
      eprint={2209.09592},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.09592}, 
}
@article{LIU2022108058,
title = {Dual constraints and adversarial learning for fair recommenders},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {108058},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.108058},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121011424},
author = {Haifeng Liu and Nan Zhao and Xiaokun Zhang and Hongfei Lin and Liang Yang and Bo Xu and Yuan Lin and Wenqi Fan},
keywords = {Fair recommendation, Graph neural network, Recommender systems, Adversarial learning},
abstract = {Recommender systems, which are consist of common artificial intelligence technology, have a profound impact on the lifestyles of people. However, recent studies have demonstrated that recommender systems have fairness problems which means that some people with certain attributes are treated unfairly. A fair recommender means that users with different attributes achieve the same recommender accuracy. In particular, the recommender systems completely rely on users’ behavior data for preferences learning, which leads to a high probability of unfair problems because that the behavior data usually contains sensitive information of users. Unfortunately, there are a few studies exploring unfair problem in recommender systems. To alleviate this problem, we present a novel fairness-aware recommender with dual fairness constraints (FRFC) to improve fairness in recommendations and protect the user’s sensitive information from being exposed. This model has several advantages: one advantage is that an adversarial-based graph neural network (GNN) is proposed to prevent the target user being infected by sensitive features of neighbor users; another advantage is that two fairness constraints are proposed to solve the problems of adversarial classifier failures in whole data and unfair ranking losses. With this design, the FRFC model can effectively filter out users’ sensitive information and give users of different attributes the same training opportunities, which is helpful for making a fair recommendation. Finally, extensive experiments demonstrate that the proposed model can significantly improve the fairness of recommendation results.}
}
@article{Edizel20,
author = {Edizel, Bora and Bonchi, Francesco and Hajian, Sara and Panisson, Andre and Tassa, Tamir},
year = {2020},
month = {03},
pages = {197-213},
title = {FaiRecSys: mitigating algorithmic bias in recommender systems},
volume = {9},
journal = {International Journal of Data Science and Analytics},
doi = {10.1007/s41060-019-00181-5}
}
@inproceedings{10.1145/3397271.3401177,
author = {Zhu, Ziwei and Wang, Jianling and Caverlee, James},
title = {Measuring and Mitigating Item Under-Recommendation Bias in Personalized Ranking Systems},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3397271.3401177},
doi = {10.1145/3397271.3401177},
abstract = {Recommendation algorithms typically build models based on user-item interactions (e.g., clicks, likes, or ratings) to provide a personalized ranked list of items. These interactions are often distributed unevenly over different groups of items due to varying user preferences. However, we show that recommendation algorithms can inherit or even amplify this imbalanced distribution, leading to item under-recommendation bias. Concretely, we formalize the concepts of ranking-based statistical parity and equal opportunity as two measures of item under-recommendation bias. Then, we empirically show that one of the most widely adopted algorithms -- Bayesian Personalized Ranking -- produces biased recommendations, which motivates our effort to propose the novel debiased personalized ranking model. The debiased model is able to improve the two proposed bias metrics while preserving recommendation performance. Experiments on three public datasets show strong bias reduction of the proposed model versus state-of-the-art alternatives.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {449–458},
numpages = {10},
keywords = {equal opportunity, recommendation bias, recommender systems, statistical parity},
location = {Virtual Event, China},
series = {SIGIR '20}
}
@inproceedings{boratto2022consumer,
  title={Consumer fairness in recommender systems: Contextualizing definitions and mitigations},
  author={Boratto, Ludovico and Fenu, Gianni and Marras, Mirko and Medda, Giacomo},
  booktitle={European Conference on Information Retrieval},
  pages={552--566},
  year={2022},
  organization={Springer}
}
@misc{yang2020causalintersectionalityfairranking,
      title={Causal intersectionality for fair ranking}, 
      author={Ke Yang and Joshua R. Loftus and Julia Stoyanovich},
      year={2020},
      eprint={2006.08688},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.08688}, 
}
@article{Acharyya_Das_Chattoraj_Tanveer_2020, title={FairyTED: A Fair Rating Predictor for TED Talk Data}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5368}, DOI={10.1609/aaai.v34i01.5368}, abstractNote={&lt;p&gt;With the recent trend of applying machine learning in every aspect of human life, it is important to incorporate fairness into the core of the predictive algorithms. We address the problem of predicting the quality of public speeches while being fair with respect to sensitive attributes of the speakers, e.g. &lt;em&gt;gender&lt;/em&gt; and &lt;em&gt;race&lt;/em&gt;. We use the TED talks as an input repository of public speeches because it consists of speakers from a diverse community and has a wide outreach. Utilizing the theories of &lt;em&gt;Causal Models&lt;/em&gt;, &lt;em&gt;Counterfactual Fairness&lt;/em&gt; and state-of-the-art neural language models, we propose a mathematical framework for fair prediction of the public speaking quality. We employ grounded assumptions to construct a causal model capturing how different &lt;em&gt;attributes&lt;/em&gt; affect public speaking quality. This causal model contributes in generating counterfactual data to train a &lt;em&gt;fair&lt;/em&gt; predictive model. Our framework is general enough to utilize any assumption within the causal model. Experimental results show that while prediction accuracy is comparable to recent work on this dataset, our predictions are counterfactually fair with respect to a novel metric when compared to true data labels. The FairyTED setup not only allows organizers to make informed and diverse selection of speakers from the unobserved counterfactual possibilities but it also ensures that viewers and new users are not influenced by unfair and unbalanced ratings from arbitrary visitors to the ted.com website when deciding to view a talk.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Acharyya, Rupam and Das, Shouman and Chattoraj, Ankani and Tanveer, Md. Iftekhar}, year={2020}, month={Apr.}, pages={338-345} }
@article{DBLP:journals/corr/abs-1809-09030,
  author       = {Golnoosh Farnadi and
                  Pigi Kouki and
                  Spencer K. Thompson and
                  Sriram Srinivasan and
                  Lise Getoor},
  title        = {A Fairness-aware Hybrid Recommender System},
  journal      = {CoRR},
  volume       = {abs/1809.09030},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.09030},
  eprinttype    = {arXiv},
  eprint       = {1809.09030},
  timestamp    = {Fri, 04 Sep 2020 14:32:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-09030.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{10.1145/3292500.3330691,
author = {Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram},
title = {Fairness-Aware Ranking in Search \& Recommendation Systems with Application to LinkedIn Talent Search},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330691},
doi = {10.1145/3292500.3330691},
abstract = {We present a framework for quantifying and mitigating algorithmic bias in mechanisms designed for ranking individuals, typically used as part of web-scale search and recommendation systems. We first propose complementary measures to quantify bias with respect to protected attributes such as gender and age. We then present algorithms for computing fairness-aware re-ranking of results. For a given search or recommendation task, our algorithms seek to achieve a desired distribution of top ranked results with respect to one or more protected attributes. We show that such a framework can be tailored to achieve fairness criteria such as equality of opportunity and demographic parity depending on the choice of the desired distribution. We evaluate the proposed algorithms via extensive simulations over different parameter choices, and study the effect of fairness-aware ranking on both bias and utility measures. We finally present the online A/B testing results from applying our framework towards representative ranking in LinkedIn Talent Search, and discuss the lessons learned in practice. Our approach resulted in tremendous improvement in the fairness metrics (nearly three fold increase in the number of search queries with representative results) without affecting the business metrics, which paved the way for deployment to 100\% of LinkedIn Recruiter users worldwide. Ours is the first large-scale deployed framework for ensuring fairness in the hiring domain, with the potential positive impact for more than 630M LinkedIn members.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2221–2231},
numpages = {11},
keywords = {fairness-aware ranking, talent search \& recommendation systems},
location = {Anchorage, AK, USA},
series = {KDD '19}
}
@inproceedings{10.1145/3442381.3450077,
author = {Imana, Basileal and Korolova, Aleksandra and Heidemann, John},
title = {Auditing for Discrimination in Algorithms Delivering Job Ads},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450077},
doi = {10.1145/3442381.3450077},
abstract = {Ad platforms such as Facebook, Google and LinkedIn promise value for advertisers through their targeted advertising. However, multiple studies have shown that ad delivery on such platforms can be skewed by gender or race due to hidden algorithmic optimization by the platforms, even when not requested by the advertisers. Building on prior work measuring skew in ad delivery, we develop a new methodology for black-box auditing of algorithms for discrimination in the delivery of job advertisements. Our first contribution is to identify the distinction between skew in ad delivery due to protected categories such as gender or race, from skew due to differences in qualification among people in the targeted audience. This distinction is important in U.S.&nbsp;law, where ads may be targeted based on qualifications, but not on protected categories. Second, we develop an auditing methodology that distinguishes between skew explainable by differences in qualifications from other factors, such as the ad platform’s optimization for engagement or training its algorithms on biased data. Our method controls for job qualification by comparing ad delivery of two concurrent ads for similar jobs, but for a pair of companies with different de facto gender distributions of employees. We describe the careful statistical tests that establish evidence of non-qualification skew in the results. Third, we apply our proposed methodology to two prominent targeted advertising platforms for job ads: Facebook and LinkedIn. We confirm skew by gender in ad delivery on Facebook, and show that it cannot be justified by differences in qualifications. We fail to find skew in ad delivery on LinkedIn. Finally, we suggest improvements to ad platform practices that could make external auditing of their algorithms in the public interest more feasible and accurate.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3767–3778},
numpages = {12},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}
@article{datta15,
author = {Datta, Amit and Tschantz, Michael and Datta, Anupam},
year = {2015},
month = {04},
pages = {},
title = {Automated Experiments on Ad Privacy Settings},
volume = {1},
journal = {Proceedings on Privacy Enhancing Technologies},
doi = {10.1515/popets-2015-0007}
}
@article{Melchiorre21,
author = {Melchiorre, Alessandro B. and Rekabsaz, Navid and Parada-Cabaleiro, Emilia and Brandl, Stefan and Lesota, Oleg and Schedl, Markus},
title = {Investigating gender fairness of recommendation algorithms in the music domain},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {58},
number = {5},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2021.102666},
doi = {10.1016/j.ipm.2021.102666},
journal = {Inf. Process. Manage.},
month = {sep},
numpages = {27},
keywords = {Recommender systems, Music, Bias, Neural networks, Demographics}
}
@article{GHARAHIGHEHI2021102663,
title = {Fair multi-stakeholder news recommender system with hypergraph ranking},
journal = {Information Processing and
 Management},
volume = {58},
number = {5},
pages = {102663},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102663},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321001515},
author = {Alireza Gharahighehi and Celine Vens and Konstantinos Pliakos},
keywords = {Multi-stakeholder recommender systems, Hypergraph learning, News recommendation, Fair recommender systems, Diversity-aware recommender systems},
abstract = {Recommender systems are typically designed to fulfill end user needs. However, in some domains the users are not the only stakeholders in the system. For instance, in a news aggregator website users, authors, magazines as well as the platform itself are potential stakeholders. Most of the collaborative filtering recommender systems suffer from popularity bias. Therefore, if the recommender system only considers users’ preferences, presumably it over-represents popular providers and under-represents less popular providers. To address this issue one should consider other stakeholders in the generated ranked lists. In this paper we demonstrate that hypergraph learning has the natural acapability of handling a multi-stakeholder recommendation task. A hypergraph can model high order relations between different types of objects and therefore is naturally inclined to generate recommendation lists considering multiple stakeholders. We form the recommendations in time-wise rounds and learn to adapt the weights of stakeholders to increase the coverage of low-covered stakeholders over time. The results show that the proposed approach counters popularity bias and produces fairer recommendations with respect to authors in two news datasets, at a low cost in accuracy.}
}
@article{XIA2019104857,
title = {WE-Rec: A fairness-aware reciprocal recommendation based on Walrasian equilibrium},
journal = {Knowledge-Based Systems},
volume = {182},
pages = {104857},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119303326},
author = {Bin Xia and Junjie Yin and Jian Xu and Yun Li},
keywords = {Recommender systems, Fairness-aware, Reciprocal recommendation, Walrasian equilibrium, Multi-objective optimization},
abstract = {The emergence of online dating and recruiting platforms brings big challenges to the reciprocal recommendation which has attracted a lot of research attention. Most previous approaches improved the accuracy and diversity of reciprocal recommendations, but few researcher made efforts on the fairness-aware recommendation which aims to avoid the discrimination and mistreatment of vulnerable groups. In this paper, we concentrate on the research of fairness-aware recommendations in the reciprocal recommender system and propose an approach to rerank the recommendation list by optimizing three significant fairness-aware criteria between parties (i.e., buyers and sellers) based on Walrasian equilibrium: (1) the disparity of service; (2) the similarity of mutual preference; (3) the equilibrium of demand and supply. According to these definitions of fairness, we cast the task of reciprocal recommendation as a multi-objective optimization considering the satisfaction of individuals, the fairness of recommendations, and the market clearing simultaneously. The extensive experiments are conducted on two real-world datasets, 
            and the results demonstrate the effectiveness of our approach.}
}
@inproceedings{wei2022comprehensive,
  title={Comprehensive Fair Meta-learned Recommender System},
  author={Wei, Tianxin and He, Jingrui},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={1989--1999},publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
  year={2022}
}
@inproceedings{zhu18,
author = {Zhu, Ziwei and Hu, Xia and Caverlee, James},
year = {2018},
month = {10},
pages = {1153-1162},publisher = {Association for Computing Machinery},
address = {New York, NY, USA},booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
title = {Fairness-Aware Tensor-Based Recommendation},
doi = {10.1145/3269206.3271795}
}
@misc{lin2019crankvolumepreferencebias,
      title={Crank up the volume: preference bias amplification in collaborative recommendation}, 
      author={Kun Lin and Nasim Sonboli and Bamshad Mobasher and Robin Burke},
      year={2019},
      eprint={1909.06362},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1909.06362}, 
}
@misc{mansoury2019biasdisparitycollaborativerecommendation,
      title={Bias Disparity in Collaborative Recommendation: Algorithmic Evaluation and Comparison}, 
      author={Masoud Mansoury and Bamshad Mobasher and Robin Burke and Mykola Pechenizkiy},
      year={2019},
      eprint={1908.00831},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1908.00831}, 
}
@inproceedings{TsintzouPT19,
  author       = {Virginia Tsintzou and
                  Evaggelia Pitoura and
                  Panayiotis Tsaparas},
  editor       = {Robin Burke and
                  Himan Abdollahpouri and
                  Edward C. Malthouse and
                  K. P. Thai and
                  Yongfeng Zhang},
  title        = {Bias Disparity in Recommendation Systems},
  booktitle    = {Proceedings of the Workshop on Recommendation in Multi-stakeholder
                  Environments co-located with the 13th {ACM} Conference on Recommender
                  Systems (RecSys 2019), Copenhagen, Denmark, September 20, 2019},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {2440},
  publisher    = {CEUR-WS.org},
  year         = {2019},
  url          = {https://ceur-ws.org/Vol-2440/short4.pdf},
  timestamp    = {Fri, 10 Mar 2023 16:22:14 +0100},
  biburl       = {https://dblp.org/rec/conf/recsys/TsintzouPT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{10068703,
  author={Wu, Kun and Erickson, Jacob and Wang, Wendy Hui and Ning, Yue},
  booktitle={2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
  title={Equipping Recommender Systems with Individual Fairness via Second-order Proximity Embedding}, 
  year={2022},
  volume={},
  number={},
  pages={171-175},
  keywords={Measurement;Social networking (online);Message passing;Knowledge graphs;Graph neural networks;Recommender systems;Algorithmic fairness;Recommender systems;Second-order proximity embedding;Graph neural networks},
  doi={10.1109/ASONAM55673.2022.10068703}}

@inproceedings{10.1145/3038912.3052612,
author = {Serbos, Dimitris and Qi, Shuyao and Mamoulis, Nikos and Pitoura, Evaggelia and Tsaparas, Panayiotis},
title = {Fairness in Package-to-Group Recommendations},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi-org.ezproxy-f.deakin.edu.au/10.1145/3038912.3052612},
doi = {10.1145/3038912.3052612},
abstract = {Recommending packages of items to groups of users has several applications, including recommending vacation packages to groups of tourists, entertainment packages to groups of friends, or sets of courses to groups of students. In this paper, we focus on a novel aspect of package-to-group recommendations, that of fairness. Specifically, when we recommend a package to a group of people, we ask that this recommendation is fair in the sense that every group member is satisfied by a sufficient number of items in the package. We explore two definitions of fairness and show that for either definition the problem of finding the most fair package is NP-hard. We exploit the fact that our problem can be modeled as a coverage problem, and we propose greedy algorithms that find approximate solutions within reasonable time. In addition, we study two extensions of the problem, where we impose category or spatial constraints on the items to be included in the recommended packages. We evaluate the appropriateness of the fairness models and the performance of the proposed algorithms using real data from Yelp, and a user study.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {371–379},
numpages = {9},
keywords = {envy-freeness, fairness, package-to-group, proportionality, recommendation systems},
location = {Perth, Australia},
series = {WWW '17}
}
@article{Wu_Wu_Wang_Huang_Xie_2021, title={Fairness-aware News Recommendation with Decomposed Adversarial Learning}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16573}, DOI={10.1609/aaai.v35i5.16573}, abstractNote={News recommendation is important for online news services. Existing news recommendation models are usually learned from users’ news click behaviors. Usually the behaviors of users with the same sensitive attributes (e.g., genders) have similar patterns and news recommendation models can easily capture these patterns. It may lead to some biases related to sensitive user attributes in the recommendation results, e.g., always recommending sports news to male users, which is unfair since users may not receive diverse news information. In this paper, we propose a fairness-aware news recommendation approach with decomposed adversarial learning and orthogonality regularization, which can alleviate unfairness in news recommendation brought by the biases of sensitive user attributes.
In our approach, we propose to decompose the user interest model into two components. One component aims to learn a bias-aware user embedding that captures the bias information on sensitive user attributes, and the other aims to learn a bias-free user embedding that only encodes attribute-independent user interest information for fairness-aware news recommendation. In addition, we propose to apply an attribute prediction task to the bias-aware user embedding to enhance its ability on bias modeling, and we apply adversarial learning to the bias-free user embedding to remove the bias information from it. Moreover, we propose an orthogonality regularization method to encourage the bias-free user embeddings to be orthogonal to the bias-aware one to better distinguish the bias-free user embedding from the bias-aware one. For fairness-aware news ranking, we only use the bias-free user embedding. Extensive experiments on benchmark dataset show that our approach can effectively improve fairness in news recommendation with minor performance loss.}, number={5}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wu, Chuhan and Wu, Fangzhao and Wang, Xiting and Huang, Yongfeng and Xie, Xing}, year={2021}, month={May}, pages={4462-4469} }
@inproceedings{hao21,
author = {Hao, Qianxiu and Xu, Qianqian and Yang, Zhiyong and Huang, Qingming},
title = {Pareto Optimality for Fairness-constrained Collaborative Filtering},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475706},
doi = {10.1145/3474085.3475706},
abstract = {The well-known collaborative filtering (CF) models typically optimize a single objective summed over all historical user-item interactions. Due to inevitable imbalances and biases in real-world data, they may develop a policy that unfairly discriminates against certain subgroups with low sample frequencies. To balance overall recommendation performance and fairness, prevalent solutions apply fairness constraints or regularizations to enforce equality of certain performance across different subgroups. However, simply enforcing equality of performance may lead to large performance degradation of those advantaged subgroups. To address this issue, we formulate a constrained Multi-Objective Optimization (MOO) problem. In contrast to the single objective, we treat the performance of each subgroup equivalently as an objective. This ensures that the imbalanced subgroup sample frequency does not affect the gradient information. We further propose fairness constraints to limit the search space to obtain more balanced solutions. To solve the constrained MOO problem, a gradient-based constrained MOO algorithm is proposed to seek a proper Pareto optimal solution for the performance trade-off. Extensive experiments on synthetic and real-world datasets show that our approach could help improve the recommendation accuracy of disadvantaged groups, while not damaging the overall performance.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {5619–5627},
numpages = {9},
keywords = {collaborative filtering, constrained multi-objective optimization, pareto optimal},
location = {Virtual Event, China},
series = {MM '21}
}
@article{Boratto21,
author = {Boratto, Ludovico and Fenu, Gianni and Marras, Mirko},
title = {Interplay between upsampling and regularization for provider fairness in recommender systems},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {3},
issn = {0924-1868},
url = {https://doi.org/10.1007/s11257-021-09294-8},
doi = {10.1007/s11257-021-09294-8},
abstract = {Considering the impact of recommendations on item providers is one of the duties of multi-sided recommender systems. Item providers are key stakeholders in online platforms, and their earnings and plans are influenced by the exposure their items receive in recommended lists. Prior work showed that certain minority groups of providers, characterized by a common sensitive attribute (e.g., gender or race), are being disproportionately affected by indirect and unintentional discrimination. Our study in this paper handles a situation where (i) the same provider is associated with multiple items of a list suggested to a user, (ii) an item is created by more than one provider jointly, and (iii) predicted user–item relevance scores are biasedly estimated for items of provider groups. Under this scenario, we assess disparities in relevance, visibility, and exposure, by simulating diverse representations of the minority group in the catalog and the interactions. Based on emerged unfair outcomes, we devise a treatment that combines observation upsampling and loss regularization, while learning user–item relevance scores. Experiments on real-world data demonstrate that our treatment leads to lower disparate relevance. The resulting recommended lists show fairer visibility and exposure, higher minority item coverage, and negligible loss in recommendation utility.},
journal = {User Modeling and User-Adapted Interaction},
month = {jul},
pages = {421–455},
numpages = {35},
keywords = {Regularization, Visibility, Exposure, Fairness, Multi-Stakeholder, Collaborative Filtering, Recommender Systems}
}
@inproceedings{omer21,
author = {K\i{}rnap, \"{O}mer and Diaz, Fernando and Biega, Asia and Ekstrand, Michael and Carterette, Ben and Yilmaz, Emine},
title = {Estimation of Fair Ranking Metrics with Incomplete Judgments},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450080},
doi = {10.1145/3442381.3450080},
abstract = {There is increasing attention to evaluating the fairness of search system ranking decisions. These metrics often consider the membership of items to particular groups, often identified using protected attributes such as gender or ethnicity. To date, these metrics typically assume the availability and completeness of protected attribute labels of items. However, the protected attributes of individuals are rarely present, limiting the application of fair ranking metrics in large scale systems. In order to address this problem, we propose a sampling strategy and estimation technique for four fair ranking metrics. We formulate a robust and unbiased estimator which can operate even with very limited number of labeled items. We evaluate our approach using both simulated and real world data. Our experimental results demonstrate that our method can estimate this family of fair ranking metrics and provides a robust, reliable alternative to exhaustive or random data annotation.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1065–1075},
numpages = {11},
keywords = {information retrieval, fairness, fair ranking, evaluation},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{tang_when_2023,
	address = {New York, NY, USA},
	series = {{RecSys} '23},
	title = {When {Fairness} meets {Bias}: a {Debiased} {Framework} for {Fairness} aware {Top}-{N} {Recommendation}},
	isbn = {9798400702419},
	shorttitle = {When {Fairness} meets {Bias}},
	url = {https://dl.acm.org/doi/10.1145/3604915.3608770},
	doi = {10.1145/3604915.3608770},
	abstract = {Fairness in the recommendation domain has recently attracted increasing attention due to more and more concerns about the algorithm discrimination and ethics. While recent years have witnessed many promising fairness aware recommender models, an important problem has been largely ignored, that is, the fairness can be biased due to the user personalized selection tendencies or the non-uniform item exposure probabilities. To study this problem, in this paper, we formally define a novel task named as unbiased fairness aware Top-N recommendation. For solving this task, we firstly define an ideal loss function based on all the user-item pairs. Considering that, in real-world datasets, only a small number of user-item interactions can be observed, we then approximate the above ideal loss with a more tractable objective based on the inverse propensity score (IPS). Since the recommendation datasets can be noisy and quite sparse, which brings difficulties for accurately estimating the IPS, we propose to optimize the objective in an IPS range instead of a specific point, which improves the model fault tolerance capability. In order to make our model more applicable to the commonly studied Top-N recommendation, we soften the ranking metrics such as Precision, Hit-Ratio, and NDCG to derive a fully differentiable framework. We conduct extensive experiments to demonstrate the effectiveness of our model based on four real-world datasets.},
	urldate = {2024-09-13},
	booktitle = {Proceedings of the 17th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Jiakai and Shen, Shiqi and Wang, Zhipeng and Gong, Zhi and Zhang, Jingsen and Chen, Xu},
	month = sep,
	year = {2023},
	pages = {200--210},
}


@inproceedings{10.1145/3604915.3608784,
author = {Yang, Hao and Liu, Zhining and Zhang, Zeyu and Zhuang, Chenyi and Chen, Xu},
title = {Towards Robust Fairness-aware Recommendation},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-f.deakin.edu.au/10.1145/3604915.3608784},
doi = {10.1145/3604915.3608784},
abstract = {Due to the progressive advancement of trustworthy machine learning algorithms, fairness in recommender systems is attracting increasing attention and is often considered from the perspective of users. Conventional fairness-aware recommendation models assume that user preferences remain the same between the training set and the testing set. However, this assumption is arguable in reality, where user preference can shift in the testing set due to the natural spatial or temporal heterogeneity. It is concerning that conventional fairness-aware models may be unaware of such distribution shifts, leading to a sharp decline in the model performance. To address the distribution shift problem, we propose a robust fairness-aware recommendation framework based on Distributionally Robust Optimization (DRO) technique. In specific, we assign learnable weights for each sample to approximate the distributions that leads to the worst-case model performance, and then optimize the fairness-aware recommendation model to improve the worst-case performance in terms of both fairness and recommendation accuracy. By iteratively updating the weights and the model parameter, our framework can be robust to unseen testing sets. To ease the learning difficulty of DRO, we use a hard clustering technique to reduce the number of learnable sample weights. To optimize our framework in a full differentiable manner, we soften the above clustering strategy. Empirically, we conduct extensive experiments based on four real-world datasets to verify the effectiveness of our proposed framework.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {211–222},
numpages = {12},
keywords = {recommendation system, fairness aware recommendation},
location = {Singapore, Singapore},
series = {RecSys '23}
}
@inproceedings{gomex21,
author = {G\'{o}mez, Elizabeth and Shui Zhang, Carlos and Boratto, Ludovico and Salam\'{o}, Maria and Marras, Mirko},
title = {The Winner Takes it All: Geographic Imbalance and Provider (Un)fairness in Educational Recommender Systems},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463235},
doi = {10.1145/3404835.3463235},
abstract = {Educational recommender systems channel most of the research efforts on the effectiveness of the recommended items. While teachers have a central role in online platforms, the impact of recommender systems for teachers in terms of the exposure such systems give to the courses is an under-explored area. In this paper, we consider data coming from a real-world platform and analyze the distribution of the recommendations w.r.t. the geographical provenience of the teachers. We observe that data is highly imbalanced towards the United States, in terms of offered courses and of interactions. These imbalances are exacerbated by recommender systems, which overexpose the country w.r.t. its representation in the data, thus generating unfairness for teachers outside that country. To introduce equity, we propose an approach that regulates the share of recommendations given to the items produced in a country (visibility) and the position of the items in the recommended list (exposure).},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1808–1812},
numpages = {5},
keywords = {MOOC, bias, data imbalance, education, fairness, online course},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}
@inproceedings{do2022online,
  title={Online certification of preference-based fairness for personalized recommender systems},
  author={Do, Virginie and Corbett-Davies, Sam and Atif, Jamal and Usunier, Nicolas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={6532--6540},
  year={2022}
}
@inproceedings{10.1145/3534678.3539269,
author = {Wei, Tianxin and He, Jingrui},
title = {Comprehensive Fair Meta-learned Recommender System},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539269},
doi = {10.1145/3534678.3539269},
abstract = {In recommender systems, one common challenge is the cold-start problem, where interactions are very limited for fresh users in the systems. To address this challenge, recently, many works introduce the meta-optimization idea into the recommendation scenarios, i.e. learning to learn the user preference by only a few past interaction items. The core idea is to learn global shared meta-initialization parameters for all users and rapidly adapt them into local parameters for each user respectively. They aim at deriving general knowledge across preference learning of various users, so as to rapidly adapt to the future new user with the learned prior and a small amount of training data. However, previous works have shown that recommender systems are generally vulnerable to bias and unfairness. Despite the success of meta-learning at improving the recommendation performance with cold-start, the fairness issues are largely overlooked.In this paper, we propose a comprehensive fair meta-learning framework, named CLOVER, for ensuring the fairness of meta-learned recommendation models. We systematically study three kinds of fairness - individual fairness, counterfactual fairness, and group fairness in the recommender systems, and propose to satisfy all three kinds via a multi-task adversarial learning scheme. Our framework offers a generic training paradigm that is applicable to different meta-learned recommender systems. We demonstrate the effectiveness of CLOVER on the representative meta-learned user preference estimator on three real-world data sets. Empirical results show that CLOVER achieves comprehensive fairness without deteriorating the overall cold-start recommendation performance.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1989–1999},
numpages = {11},
keywords = {fairness, meta-learning, recommender systems},
location = {Washington DC, USA},
series = {KDD '22}
}
@unknown{Dong20,
author = {Dong, Qiang and Xie, Shuang-Shuang and Yang, Xiaofan and Tang, Yuan},
year = {2020},
month = {09},
pages = {},
title = {User-item matching for recommendation fairness: a view from item-providers}
}
@article{Chouldechova2016FairPW,
  title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
  author={Alexandra Chouldechova},
  journal={Big data},
  year={2016},
  volume={5 2},
  pages={
          153-163
        },
  url={https://api.semanticscholar.org/CorpusID:1443041}
}
@inproceedings{Gohar2023ASO,
  title={A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges},
  author={Usman Gohar and Lu Cheng},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258615326}
}
@inproceedings{10.1145/3194770.3194776,
author = {Verma, Sahil and Rubin, Julia},
title = {Fairness definitions explained},
year = {2018},
isbn = {9781450357463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194770.3194776},
doi = {10.1145/3194770.3194776},
abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
booktitle = {Proceedings of the International Workshop on Software Fairness},
pages = {1–7},
numpages = {7},
location = {Gothenburg, Sweden},
series = {FairWare '18}
}
@inproceedings{10.5555/3157382.3157584,
author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
title = {Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4356–4364},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}
@inproceedings{10.1145/3442381.3449904,
author = {Islam, Rashidul and Keya, Kamrun Naher and Zeng, Ziqian and Pan, Shimei and Foulds, James},
title = {Debiasing Career Recommendations with Neural Fair Collaborative Filtering},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-f.deakin.edu.au/10.1145/3442381.3449904},
doi = {10.1145/3442381.3449904},
abstract = {A growing proportion of human interactions are digitized on social media platforms and subjected to algorithmic decision-making, and it has become increasingly important to ensure fair treatment from these algorithms. In this work, we investigate gender bias in collaborative-filtering recommender systems trained on social media data. We develop neural fair collaborative filtering (NFCF), a practical framework for mitigating gender bias in recommending career-related sensitive items (e.g. jobs, academic concentrations, or courses of study) using a pre-training and fine-tuning approach to neural collaborative filtering, augmented with bias correction techniques. We show the utility of our methods for gender de-biased career and college major recommendations on the MovieLens dataset and a Facebook dataset, respectively, and achieve better performance and fairer behavior than several state-of-the-art models.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3779–3790},
numpages = {12},
keywords = {AI \& society, Fairness in AI, career recommendation, collaborative filtering, ethical issues, recommender systems, social media},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}
@inproceedings{10.1145/3308560.3317595,
author = {Sapiezynski, Piotr and Zeng, Wesley and E Robertson, Ronald and Mislove, Alan and Wilson, Christo},
title = {Quantifying the Impact of User Attentionon Fair Group Representation in Ranked Lists},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317595},
doi = {10.1145/3308560.3317595},
abstract = {In this work, we introduce a novel metric for auditing group fairness in ranked lists. Our approach offers two benefits compared to the state of the art. First, we offer a blueprint for modeling of user attention. Rather than assuming a logarithmic loss in importance as a function of the rank, we can account for varying user behaviors through parametrization. For example, we expect a user to see more items during a viewing of a social media feed than when they inspect the results list of a single web search query. Second, we allow non-binary protected attributes to enable investigating inherently continuous attributes (e.g., political alignment on the liberal to conservative spectrum) as well as to facilitate measurements across aggregated sets of search results, rather than separately for each result list. By combining these two elements into our metric, we are able to better address the human factors inherent in this problem. We measure the whole sociotechnical system, consisting of a ranking algorithm and individuals using it, instead of exclusively focusing on the ranking algorithm. Finally, we use our metric to perform three simulated fairness audits. We show that determining fairness of a ranked output necessitates knowledge (or a model) of the end-users of the particular service. Depending on their attention distribution function, a fixed ranking of results can appear biased both in favor and against a protected group1.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {553–562},
numpages = {10},
keywords = {ranked lists, information retrieval, group fairness},
location = {San Francisco, USA},
series = {WWW '19}
}
@inproceedings{10.1145/3626772.3657794,
author = {Jaenich, Thomas and McDonald, Graham and Ounis, Iadh},
title = {Fairness-Aware Exposure Allocation via Adaptive Reranking},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657794},
doi = {10.1145/3626772.3657794},
abstract = {In the first stage of a re-ranking pipeline, an inexpensive ranking model is typically deployed to retrieve a set of documents that are highly likely to be relevant to the user's query. The retrieved documents are then re-ranked by a more effective but expensive ranking model, e.g., a deep neural ranker such as BERT. However, in such a standard pipeline, no new documents are typically discovered after the first stage retrieval. Hence, the amount of exposure that a particular group of documents - e.g., documents from a particular demographic category - can receive is limited by the number of documents that are retrieved in the first stage retrieval. Indeed, if too few documents from a group are retrieved in the first stage retrieval, ensuring that the group receives a fair amount of exposure to the user may become infeasible. Therefore, it is useful to identify more documents from underrepresented groups that are potentially relevant to the query during the re-ranking stage. In this work, we investigate how deploying adaptive re-ranking, which enables the discovery of additional potentially relevant documents in the re-ranking stage, can improve the exposure that a given group of documents receives in the final ranking. We propose six adaptive re-ranking policies that can discover documents from underrepresented groups to increase the disadvantaged groups' exposure in the final ranking. Our experiments on the TREC 2021 and 2022 Fair Ranking Track test collections show that our policies consistently improve the fairness of the exposure distribution in the final ranking, compared to standard adaptive re-ranking approaches, resulting in increases of up to ~13\% in Attention Weighted Ranked Fairness (AWRF). Moreover, our best performing policy, Policy 6, consistently maintains and frequently increases the utility of the search results in terms of nDCG.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1504–1513},
numpages = {10},
keywords = {adaptive re-ranking, exposure, group fairness},
location = {Washington DC, USA},
series = {SIGIR '24}
}
@inproceedings{10.1145/3589334.3648158,
author = {Jiang, Meng and Bao, Keqin and Zhang, Jizhi and Wang, Wenjie and Yang, Zhengyi and Feng, Fuli and He, Xiangnan},
title = {Item-side Fairness of Large Language Model-based Recommendation System},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3589334.3648158},
doi = {10.1145/3589334.3648158},
abstract = {Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs). From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems. To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on MovieLens and Steam datasets, and observe significant item-side fairness improvements. The code can be found in https://github.com/JiangM-C/IFairLRS.git.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {4717–4726},
numpages = {10},
keywords = {item-side fairness, large language model, recommendation},
location = {Singapore, Singapore},
series = {WWW '24}
}
@article{JMLR:v21:19-805,
  author  = {Aghiles Salah and Quoc-Tuan Truong and Hady W. Lauw},
  title   = {Cornac: A Comparative Framework for Multimodal Recommender Systems},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {95},
  pages   = {1--5},
  url     = {http://jmlr.org/papers/v21/19-805.html}
}
@misc{keya2020equitableallocationhealthcareresources,
      title={Equitable Allocation of Healthcare Resources with Fair Cox Models}, 
      author={Kamrun Naher Keya and Rashidul Islam and Shimei Pan and Ian Stockwell and James R. Foulds},
      year={2020},
      eprint={2010.06820},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.06820}, 
}
@inproceedings{10.1145/3038912.3052569,
author = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
title = {Neural Collaborative Filtering},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052569},
doi = {10.1145/3038912.3052569},
abstract = {In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit feedback.Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering --- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items.By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {173–182},
numpages = {10},
keywords = {neural networks, matrix factorization, implicit feedback, deep learning, collaborative filtering},
location = {Perth, Australia},
series = {WWW '17}
}

@article{RAHMANI2022117700,
title = {The role of context fusion on accuracy, beyond-accuracy, and fairness of point-of-interest recommendation systems},
journal = {Expert Systems with Applications},
volume = {205},
pages = {117700},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117700},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422009903},
author = {Hossein A. Rahmani and Yashar Deldjoo and Tommaso {di Noia}},
keywords = {Contextual information, Context-aware POI recommendation, Interpretability, Evaluation, Fairness, Beyond-accuracy},
abstract = {Point-of-interest (POI) recommendation is an essential service to location-based social networks (LBSNs), benefiting both users providing them the chance to explore new locations and businesses by discovering new potential customers. These systems learn the preferences of users and their mobility patterns to generate relevant POI recommendations. Previous studies have shown that incorporating contextual information such as geographical, temporal, social, and categorical substantially improves the quality of POI recommendations. However, fewer works have studied in-depth the multi-aspect benefits of context fusion on POI recommendation, in particular on beyond-accuracy, fairness, and interpretability of recommendations. In this work, we propose a linear regression-based fusion of POI contexts that effectively finds the best combination of contexts for each (i) user, or (ii) group of users from their historical interactions. The results of large-scale experiments on two popular datasets Gowalla and Yelp reveal several interesting findings. First, the proposed approach does not present significant loss in accuracy and unfairness of popularity bias as with classical collaborative baselines, and yet improves the beyond-accuracy of recommendation compared with existing context-aware (CA) approaches using heuristic context fusions; for instance, the proposed approach improves the accuracy and beyond-accuracy compare to best baseline model by 25% and 30%, respectively. Second, our proposed approach is interpretable, allowing to explain to the user why she has been recommended specific POIs, based on the learned context weights from user past check-ins; for example, if you are in Rome and our method recommends you a historical place like ‘Colosseum’, it can also provide an explanation why this item is recommended to you based on your personal preference on context (e.g., you were recommended to visit ‘Colosseum’ because in the past your visited historical places). Third, by analyzing the fairness of recommendation with respect to users (based on their activity levels) and items (based on the popularity of items), we found that a model which is recommend fairly on one dataset can recommend unfair on another dataset. Overall, our study suggests that appropriate context fusion is an essential element of an accurate, fair, and transparent POI recommendation system. We highlight that while we have tested the efficacy of our context-fusion methods on two popular CA recommendation models in the POI domain, namely GeoSoCa and LORE, our system can be flexibly utilized to extend the capability of other CA algorithms.}
}
@inproceedings{10.1145/3404835.3462943,
author = {Lin, Chen and Liu, Xinyi and Xv, Guipeng and Li, Hui},
title = {Mitigating Sentiment Bias for Recommender Systems},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462943},
doi = {10.1145/3404835.3462943},
abstract = {Biases and de-biasing in recommender systems (RS) have become a research hotspot recently. This paper reveals an unexplored type of bias, i.e., sentiment bias. Through an empirical study, we find that many RS models provide more accurate recommendations on user/item groups having more positive feedback (i.e., positive users/items) than on user/item groups having more negative feedback (i.e., negative users/items). We show that sentiment bias is different from existing biases such as popularity bias: positive users/items do not have more user feedback (i.e., either more ratings or longer reviews). The existence of sentiment bias leads to low-quality recommendations to critical users and unfair recommendations for niche items. We discuss the factors that cause sentiment bias. Then, to fix the sources of sentiment bias, we propose a general de-biasing framework with three strategies manifesting in different regularizers that can be easily plugged into RS models without changing model architectures. Experiments on various RS models and benchmark datasets have verified the effectiveness of our de-biasing framework. To our best knowledge, sentiment bias and its de-biasing have not been studied before. We hope that this work can help strengthen the study of biases and de-biasing in RS.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {31–40},
numpages = {10},
keywords = {de-biasing, recommender systems, sentiment bias},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}
@inproceedings{NEURIPS2020_9d752cb0,
 author = {El Halabi, Marwa and Mitrovi\'{c}, Slobodan and Norouzi-Fard, Ashkan and Tardos, Jakab and Tarnawski, Jakub M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {13609--13622},
 publisher = {Curran Associates, Inc.},
 title = {Fairness in Streaming Submodular Maximization: Algorithms and Hardness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/9d752cb08ef466fc480fba981cfa44a1-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{10.5555/3294996.3295162,
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
title = {Counterfactual fairness},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4069–4079},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}
@inproceedings{hardt16,
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title = {Equality of opportunity in supervised learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3323–3331},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}
@inproceedings{10.1145/3240323.3240372,
author = {Steck, Harald},
title = {Calibrated recommendations},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240372},
doi = {10.1145/3240323.3240372},
abstract = {When a user has watched, say, 70 romance movies and 30 action movies, then it is reasonable to expect the personalized list of recommended movies to be comprised of about 70\% romance and 30\% action movies as well. This important property is known as calibration, and recently received renewed attention in the context of fairness in machine learning. In the recommended list of items, calibration ensures that the various (past) areas of interest of a user are reflected with their corresponding proportions. Calibration is especially important in light of the fact that recommender systems optimized toward accuracy (e.g., ranking metrics) in the usual offline-setting can easily lead to recommendations where the lesser interests of a user get crowded out by the user's main interests-which we show empirically as well as in thought-experiments. This can be prevented by calibrated recommendations. To this end, we outline metrics for quantifying the degree of calibration, as well as a simple yet effective re-ranking algorithm for post-processing the output of recommender systems.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {154–162},
numpages = {9},
keywords = {calibration, diversity, fairness, recommender systems},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}
@inproceedings{li2021towards,
  title={Towards personalized fairness based on causal notion},
  author={Li, Yunqi and Chen, Hanxiong and Xu, Shuyuan and Ge, Yingqiang and Zhang, Yongfeng},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1054--1063},
  year={2021}
}
@inproceedings{10.1145/3450613.3456821,
author = {Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad and Malthouse, Edward},
title = {User-centered Evaluation of Popularity Bias in Recommender Systems},
year = {2021},
isbn = {9781450383660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450613.3456821},
doi = {10.1145/3450613.3456821},
abstract = {Recommendation and ranking systems are known to suffer from popularity bias; the tendency of the algorithm to favor a few popular items while under-representing the majority of other items. Prior research has examined various approaches for mitigating popularity bias and enhancing the recommendation of long-tail, less popular, items. The effectiveness of these approaches is often assessed using different metrics to evaluate the extent to which over-concentration on popular items is reduced. However, not much attention has been given to the user-centered evaluation of this bias; how different users with different levels of interest towards popular items are affected by such algorithms. In this paper, we show the limitations of the existing metrics to evaluate popularity bias mitigation when we want to assess these algorithms from the users’ perspective and we propose a new metric that can address these limitations. In addition, we present an effective approach that mitigates popularity bias from the user-centered point of view. Finally, we investigate several state-of-the-art approaches proposed in recent years to mitigate popularity bias and evaluate their performances using the existing metrics and also from the users’ perspective. Our experimental results using two publicly-available datasets show that existing popularity bias mitigation techniques ignore the users’ tolerance towards popular items. Our proposed user-centered method can tackle popularity bias effectively for different users while also improving the existing metrics.},
booktitle = {Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {119–129},
numpages = {11},
keywords = {calibration, fairness, long-tail recommendation, popularity bias, recommender systems},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}
@inproceedings{10.1145/3477495.3531718,
author = {Rahmani, Hossein A. and Naghiaei, Mohammadmehdi and Dehghan, Mahdi and Aliannejadi, Mohammad},
title = {Experiments on Generalizability of User-Oriented Fairness in Recommender Systems},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531718},
doi = {10.1145/3477495.3531718},
abstract = {Recent work in recommender systems mainly focuses on fairness in recommendations as an important aspect of measuring recommendations quality. A fairness-aware recommender system aims to treat different user groups similarly. Relevant work on user-oriented fairness highlights the discriminant behavior of fairness-unaware recommendation algorithms towards a certain user group, defined based on users' activity level. Typical solutions include proposing a user-centered fairness re-ranking framework applied on top of a base ranking model to mitigate its unfair behavior towards a certain user group i.e., disadvantaged group. In this paper, we re-produce a user-oriented fairness study and provide extensive experiments to analyze the dependency of their proposed method on various fairness and recommendation aspects, including the recommendation domain, nature of the base ranking model, and user grouping method. Moreover, we evaluate the final recommendations provided by the re-ranking framework from both user- (e.g., NDCG, user-fairness) and item-side (e.g., novelty, item-fairness) metrics. We discover interesting trends and trade-offs between the model's performance in terms of different evaluation metrics. For instance, we see that the definition of the advantaged/disadvantaged user groups plays a crucial role in the effectiveness of the fairness algorithm and how it improves the performance of specific base ranking models. Finally, we highlight some important open challenges and future directions in this field. We release the data, evaluation pipeline, and the trained models publicly on https://github.com/rahmanidashti/FairRecSys.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2755–2764},
numpages = {10},
keywords = {recommender systems, re-ranking, fairness, algorithmic fairness},
location = {Madrid, Spain},
series = {SIGIR '22}
}
@inproceedings{he2020lightgcn,
  title={Lightgcn: Simplifying and powering graph convolution network for recommendation},
  author={He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, Yongdong and Wang, Meng},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={639--648},
  year={2020}
}
@inproceedings{10.1145/3132847.3132938,
author = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
title = {FA*IR: A Fair Top-k Ranking Algorithm},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3132847.3132938},
doi = {10.1145/3132847.3132938},
abstract = {In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n » k candidates, maximizing utility (i.e., select the "best" candidates) subject to group fairness criteria.Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above.An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1569–1578},
numpages = {10},
keywords = {top-k selection, ranking, bias in computer systems, algorithmic fairness},
location = {Singapore},
series = {CIKM '17}
}
@inproceedings{10.1145/3437963.3441824,
author = {Ge, Yingqiang and Liu, Shuchang and Gao, Ruoyuan and Xian, Yikun and Li, Yunqi and Zhao, Xiangyu and Pei, Changhua and Sun, Fei and Ge, Junfeng and Ou, Wenwu and Zhang, Yongfeng},
title = {Towards Long-term Fairness in Recommendation},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3437963.3441824},
abstract = {As Recommender Systems (RS) influence more and more people in their daily life, the issue of fairness in recommendation is becoming more and more important. Most of the prior approaches to fairness-aware recommendation have been situated in a static or one-shot setting, where the protected groups of items are fixed, and the model provides a one-time fairness solution based on fairness-constrained optimization. This fails to consider the dynamic nature of the recommender systems, where attributes such as item popularity may change over time due to the recommendation policy and user engagement. For example, products that were once popular may become no longer popular, and vice versa. As a result, the system that aims to maintain long-term fairness on the item exposure in different popularity groups must accommodate this change in a timely fashion.Novel to this work, we explore the problem of long-term fairness in recommendation and accomplish the problem through dynamic fairness learning. We focus on the fairness of exposure of items in different groups, while the division of the groups is based on item popularity, which dynamically changes over time in the recommendation process. We tackle this problem by proposing a fairness-constrained reinforcement learning algorithm for recommendation, which models the recommendation problem as a Constrained Markov Decision Process (CMDP), so that the model can dynamically adjust its recommendation policy to make sure the fairness requirement is always satisfied when the environment changes. Experiments on several real-world datasets verify our framework's superiority in terms of recommendation performance, short-term fairness, and long-term fairness.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {445–453},
numpages = {9},
keywords = {unbiased recommendation, reinforcement learning, recommender system, long-term fairness, constrained policy optimization},
series = {WSDM '21}
}
@ARTICLE{7839995,
  author={Wu, Le and Ge, Yong and Liu, Qi and Chen, Enhong and Hong, Richang and Du, Junping and Wang, Meng},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Modeling the Evolution of Users’ Preferences and Social Links in Social Networking Services}, 
  year={2017},
  volume={29},
  number={6},
  pages={1240-1253},
  keywords={Social network services;Predictive models;Data models;Fuses;Collaboration;Buildings;Electronic mail;User modeling;social networking services;user interest modeling;link prediction},
  doi={10.1109/TKDE.2017.2663422}}

@InProceedings{pmlr-v54-zafar17a,
  title = 	 {{Fairness Constraints: Mechanisms for Fair Classification}},
  author = 	 {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {962--970},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/zafar17a.html},
  abstract = 	 {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.}
}

@inproceedings{10.1145/3331184.3331214,
author = {Wu, Le and Sun, Peijie and Fu, Yanjie and Hong, Richang and Wang, Xiting and Wang, Meng},
title = {A Neural Influence Diffusion Model for Social Recommendation},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331214},
doi = {10.1145/3331184.3331214},
abstract = {Precise user and item embedding learning is the key to building a successful recommender system. Traditionally, Collaborative Filtering (CF) provides a way to learn user and item embeddings from the user-item interaction history. However, the performance is limited due to the sparseness of user behavior data. With the emergence of online social networks, social recommender systems have been proposed to utilize each user's local neighbors' preferences to alleviate the data sparsity for better user embedding modeling. We argue that, for each user of a social platform, her potential embedding is influenced by her trusted users, with these trusted users are influenced by the trusted users' social connections. As social influence recursively propagates and diffuses in the social network, each user's interests change in the recursive process. Nevertheless, the current social recommendation models simply developed static models by leveraging the local neighbors of each user without simulating the recursive diffusion in the global social network, leading to suboptimal recommendation performance. In this paper, we propose a deep influence propagation model to stimulate how users are influenced by the recursive social diffusion process for social recommendation. For each user, the diffusion process starts with an initial embedding that fuses the related features and a free user latent vector that captures the latent behavior preference. The key idea of our proposed model is that we design a layer-wise influence propagation structure to model how users' latent embeddings evolve as the social diffusion process continues. We further show that our proposed model is general and could be applied when the user~(item) attributes or the social network structure is not available. Finally, extensive experimental results on two real-world datasets clearly show the effectiveness of our proposed model, with more than 13\% performance improvements over the best baselines for top-10 recommendation on the two datasets.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {235–244},
numpages = {10},
keywords = {graph neural networks, influence diffusion, personalization, social recommendation},
location = {Paris, France},
series = {SIGIR'19}
}
@ARTICLE{5197422,
  author={Koren, Yehuda and Bell, Robert and Volinsky, Chris},
  journal={Computer}, 
  title={Matrix Factorization Techniques for Recommender Systems}, 
  year={2009},
  volume={42},
  number={8},
  pages={30-37},
  keywords={Recommender systems;Motion pictures;Filtering;Collaboration;Sea measurements;Predictive models;Genomics;Bioinformatics;Nearest neighbor searches;Computational intelligence;Netflix Prize;Matrix factorization},
  doi={10.1109/MC.2009.263}}
@inproceedings{10.1145/3287560.3287572,
author = {De-Arteaga, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
title = {Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3287560.3287572},
doi = {10.1145/3287560.3287572},
abstract = {We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are "scrubbed," and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {120–128},
numpages = {9},
keywords = {online recruiting, gender bias, compounding injustices, automated hiring, algorithmic fairness, Supervised learning},
series = {FAT* '19}
}

@article{2862,
  keywords = {Recommendation Systems, Collaborative Filtering, Deep Learning},
  author = {Jesús Bobadilla and Raúl Lara-Cabrera and Ángel González-Prieto and Fernando Ortega},
  title = {DeepFair: Deep Learning for Improving Fairness in Recommender Systems},
  abstract = {The lack of bias management in Recommender Systems leads to minority groups receiving unfair recommendations. Moreover, the trade-off between equity and precision makes it difficult to obtain recommendations that meet both criteria. Here we propose a Deep Learning based Collaborative Filtering algorithm that provides recommendations with an optimum balance between fairness and accuracy. Furthermore, in the recommendation stage, this balance does not require an initial knowledge of the users’ demographic information. The proposed architecture incorporates four abstraction levels: raw ratings and demographic information, minority indexes, accurate predictions, and fair recommendations. Last two levels use the classical Probabilistic Matrix Factorization (PMF) model to obtain users and items hidden factors, and a Multi-Layer Network (MLN) to combine those factors with a ‘fairness’ (ß) parameter. Several experiments have been conducted using two types of minority sets: gender and age. Experimental results show that it is possible to make fair recommendations without losing a significant proportion of accuracy. },
  year = {2021},
  journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
  volume = {6},
  number = {6},
  pages = {86-94},
  month = {06/2021},
  issn = {1989-1660},
  url = {https://www.ijimai.org/journal/sites/default/files/2021-05/ijimai_6_6_9_0.pdf},
  doi = {10.9781/ijimai.2020.11.001},
}
@inproceedings{10.1145/352871.352887,
author = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
title = {Analysis of recommendation algorithms for e-commerce},
year = {2000},
isbn = {1581132727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/352871.352887},
doi = {10.1145/352871.352887},
booktitle = {Proceedings of the 2nd ACM Conference on Electronic Commerce},
pages = {158–167},
numpages = {10},
location = {Minneapolis, Minnesota, USA},
series = {EC '00}
}
@article{lum2016,
author = {Lum, Kristian and Isaac, William},
year = {2016},
month = {10},
pages = {14-19},
title = {To Predict and Serve?},
volume = {13},
journal = {Significance},
doi = {10.1111/j.1740-9713.2016.00960.x}
}

@InProceedings{pmlr-v81-ensign18a,
  title = 	 {Runaway Feedback Loops in Predictive Policing},
  author = 	 {Ensign, Danielle and Friedler, Sorelle A. and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {160--171},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/ensign18a/ensign18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/ensign18a.html},
  abstract = 	 {Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.  In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.   Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.}
}

@inproceedings{10.5555/2074094.2074100,
author = {Breese, John S. and Heckerman, David and Kadie, Carl},
title = {Empirical analysis of predictive algorithms for collaborative filtering},
year = {1998},
isbn = {155860555X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Collaborative filtering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefficients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list.Experiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation metr rics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector-similarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time.},
booktitle = {Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence},
pages = {43–52},
numpages = {10},
location = {Madison, Wisconsin},
series = {UAI'98}
}
@inproceedings{10.1145/3178876.3186150,
author = {Liang, Dawen and Krishnan, Rahul G. and Hoffman, Matthew D. and Jebara, Tony},
title = {Variational Autoencoders for Collaborative Filtering},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186150},
doi = {10.1145/3178876.3186150},
abstract = {We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {689–698},
numpages = {10},
keywords = {variational autoencoder, recommender systems, implicit feedback, collaborative filtering, bayesian models},
location = {Lyon, France},
series = {WWW '18}
}
@inproceedings{NIPS2017_e6384711,
 author = {Yao, Sirui and Huang, Bert},
address = {Red Hook, NY, USA},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Parity: Fairness Objectives for Collaborative Filtering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
 volume = {30},
 year = {2017}
}
@inproceedings{Weydemann19,
author = {Weydemann, Leonard and Sacharidis, Dimitris and Werthner, Hannes},
title = {Defining and measuring fairness in location recommendations},
year = {2019},
isbn = {9781450369633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-f.deakin.edu.au/10.1145/3356994.3365497},
doi = {10.1145/3356994.3365497},
abstract = {Location-based recommender systems learn from historical movement traces of users in order to make recommendations for places to visit, events to attend, itineraries to follow. As with other systems assisting humans in their decisions, there is an increasing need to scrutinize the implications of algorithmically made location recommendations. The challenge is that one can define different fairness concerns, as both users and locations may be subjects of unfair treatment. In this work, we propose a comprehensive framework that allows the expression of various fairness aspects, and quantify the degree to which the system is acting justly. In a case study, we focus on three fairness aspects, and investigate several types of location-based recommenders in terms of their ability to be fair under the studied aspects.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
articleno = {6},
numpages = {8},
keywords = {recommender systems, location-based recommendations, fairness, discrimination, bias},
location = {Chicago, Illinois},
series = {LocalRec '19}
}
@inproceedings{Sacharidis19,
author = {Sacharidis, Dimitris},
title = {Top-N group recommendations with fairness},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297442},
doi = {10.1145/3297280.3297442},
abstract = {In many settings it is required that items are recommended to a group of users instead of a single user. Conventionally, the objective is to maximize the overall satisfaction among group members. Recently, however, attention has shifted to ensuring that recommendations are fair in that they should minimize the feeling of dissatisfaction among members. In this work, we explore a simple but intuitive notion of fairness: the minimum utility a group member receives. We propose a technique that seeks to rank the Pareto, or unanimously, optimal items by considering all admissible ways in which a group might reach a decision. As our detailed experimental study shows, this results in top-N recommendations that not only achieve a high minimum utility compared to other fairness-aware techniques, but also a high average utility across all group members beating standard aggregation strategies.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1663–1670},
numpages = {8},
keywords = {aggregation strategies, fairness, group recommender systems, pareto efficiency},
location = {Limassol, Cyprus},
series = {SAC '19}
}
@inproceedings{Fu20,
author = {Fu, Zuohui and Xian, Yikun and Gao, Ruoyuan and Zhao, Jieyu and Huang, Qiaoying and Ge, Yingqiang and Xu, Shuyuan and Geng, Shijie and Shah, Chirag and Zhang, Yongfeng and de Melo, Gerard},
title = {Fairness-Aware Explainable Recommendation over Knowledge Graphs},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401051},
doi = {10.1145/3397271.3401051},
abstract = {There has been growing attention on fairness considerations recently, especially in the context of intelligent decision making systems. For example, explainable recommendation systems may suffer from both explanation bias and performance disparity. We show that inactive users may be more susceptible to receiving unsatisfactory recommendations due to their insufficient training data, and that their recommendations may be biased by the training records of active users due to the nature of collaborative filtering, which leads to unfair treatment by the system. In this paper, we analyze different groups of users according to their level of activity, and find that bias exists in recommendation performance between different groups. Empirically, we find that such performance gap is caused by the disparity of data distribution, specifically the knowledge graph path distribution in this work. We propose a fairness constrained approach via heuristic re-ranking to mitigate this unfairness problem in the context of explainable recommendation over knowledge graphs. We experiment on several real-world datasets with state-of-the-art knowledge graph-based explainable recommendation algorithms. The promising results show that our algorithm is not only able to provide high-quality explainable recommendations, but also reduces the recommendation unfairness in several aspects.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {69–78},
numpages = {10},
keywords = {explainable recommendation, fairness, knowledge graphs},
location = {Virtual Event, China},
series = {SIGIR '20}
}
@INPROCEEDINGS{9101635,
  author={Foulds, James R. and Islam, Rashidul and Keya, Kamrun Naher and Pan, Shimei},
  booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)}, 
  title={An Intersectional Definition of Fairness}, 
  year={2020},
  volume={},
  number={},
  pages={1918-1921},
  keywords={Privacy;Law;Machine learning;Lenses;fairness in AI;AI and society;80% rule;privacy},
  doi={10.1109/ICDE48307.2020.00203}}

@misc{foulds2019bayesianmodelingintersectionalfairness,
      title={Bayesian Modeling of Intersectional Fairness: The Variance of Bias}, 
      author={James Foulds and Rashidul Islam and Kamrun Keya and Shimei Pan},
      year={2019},
      eprint={1811.07255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.07255}, 
}
@inproceedings{10.1145/3368089.3409697,
author = {Chakraborty, Joymallya and Majumder, Suvodeep and Yu, Zhe and Menzies, Tim},
title = {Fairway: a way to build fair ML software},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409697},
doi = {10.1145/3368089.3409697},
abstract = {Machine learning software is increasingly being used to make decisions that affect people's lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.). This "algorithmic discrimination" in the AI software systems has become a matter of serious concern in the machine learning and software engineering community. There have been works done to find "algorithmic bias" or "ethical bias" in the software system. Once the bias is detected in the AI software system, the mitigation of bias is extremely important. In this work, we a)explain how ground-truth bias in training data affects machine learning model fairness and how to find that bias in AI software,b)propose a method Fairway which combines pre-processing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {654–665},
numpages = {12},
keywords = {Bias Mitigation, Fairness Metrics, Software Fairness},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}
@inproceedings{10.1145/3106237.3106277,
author = {Galhotra, Sainyam and Brun, Yuriy and Meliou, Alexandra},
title = {Fairness testing: testing software for discrimination},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106277},
doi = {10.1145/3106237.3106277},
abstract = {This paper defines software fairness and discrimination and develops a testing-based method for measuring if and how much software discriminates, focusing on causality in discriminatory behavior. Evidence of software discrimination has been found in modern software systems that recommend criminal sentences, grant access to financial products, and determine who is allowed to participate in promotions. Our approach, Themis, generates efficient test suites to measure discrimination. Given a schema describing valid system inputs, Themis generates discrimination tests automatically and does not require an oracle. We evaluate Themis on 20 software systems, 12 of which come from prior work with explicit focus on avoiding discrimination. We find that (1) Themis is effective at discovering software discrimination, (2) state-of-the-art techniques for removing discrimination from algorithms fail in many situations, at times discriminating against as much as 98\% of an input subdomain, (3) Themis optimizations are effective at producing efficient test suites for measuring discrimination, and (4) Themis is more efficient on systems that exhibit more discrimination. We thus demonstrate that fairness testing is a critical aspect of the software development cycle in domains with possible discrimination and provide initial tools for measuring software discrimination.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {498–510},
numpages = {13},
keywords = {Discrimination testing, fairness testing, software bias, testing},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}
@misc{bothmann2024fairnessroleprotectedattributes,
      title={What Is Fairness? On the Role of Protected Attributes and Fictitious Worlds}, 
      author={Ludwig Bothmann and Kristina Peters and Bernd Bischl},
      year={2024},
      eprint={2205.09622},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.09622}, 
}
@inproceedings{10.5555/1795114.1795167,
author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},
title = {BPR: Bayesian personalized ranking from implicit feedback},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive k-nearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {452–461},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}
@article{10.1145/2827872,
author = {Harper, F. Maxwell and Konstan, Joseph A.},
title = {The MovieLens Datasets: History and Context},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/2827872},
doi = {10.1145/2827872},
abstract = {The MovieLens datasets are widely used in education, research, and industry. They are downloaded hundreds of thousands of times each year, reflecting their use in popular press programming books, traditional and online courses, and software. These datasets are a product of member activity in the MovieLens movie recommendation system, an active research platform that has hosted many experiments since its launch in 1997. This article documents the history of MovieLens and the MovieLens datasets. We include a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization. We document best practices and limitations of using the MovieLens datasets in new research.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = dec,
articleno = {19},
numpages = {19},
keywords = {Datasets, MovieLens, ratings, recommendations}
}
@inproceedings{islam21,
author = {Islam, Rashidul and Keya, Kamrun Naher and Zeng, Ziqian and Pan, Shimei and Foulds, James},
title = {Debiasing Career Recommendations with Neural Fair Collaborative Filtering},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449904},
doi = {10.1145/3442381.3449904},
abstract = {A growing proportion of human interactions are digitized on social media platforms and subjected to algorithmic decision-making, and it has become increasingly important to ensure fair treatment from these algorithms. In this work, we investigate gender bias in collaborative-filtering recommender systems trained on social media data. We develop neural fair collaborative filtering (NFCF), a practical framework for mitigating gender bias in recommending career-related sensitive items (e.g. jobs, academic concentrations, or courses of study) using a pre-training and fine-tuning approach to neural collaborative filtering, augmented with bias correction techniques. We show the utility of our methods for gender de-biased career and college major recommendations on the MovieLens dataset and a Facebook dataset, respectively, and achieve better performance and fairer behavior than several state-of-the-art models.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3779–3790},
numpages = {12},
keywords = {AI \& society, Fairness in AI, career recommendation, collaborative filtering, ethical issues, recommender systems, social media},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}
@inbook{Shani2021,
author = {Shani, Guy and Gunawardana, Asela},
year = {2011},
month = {01},
pages = {257-297},
title = {Evaluating Recommendation Systems},
volume = {12},
isbn = {978-0-387-85819-7},
journal = {Recommender Systems Handbook},
doi = {10.1007/978-0-387-85820-3_8}
}