@inproceedings{10.1145/3038912.3052612,
author = {Serbos, Dimitris and Qi, Shuyao and Mamoulis, Nikos and Pitoura, Evaggelia and Tsaparas, Panayiotis},
title = {Fairness in Package-to-Group Recommendations},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi-org.ezproxy-f.deakin.edu.au/10.1145/3038912.3052612},
doi = {10.1145/3038912.3052612},
abstract = {Recommending packages of items to groups of users has several applications, including recommending vacation packages to groups of tourists, entertainment packages to groups of friends, or sets of courses to groups of students. In this paper, we focus on a novel aspect of package-to-group recommendations, that of fairness. Specifically, when we recommend a package to a group of people, we ask that this recommendation is fair in the sense that every group member is satisfied by a sufficient number of items in the package. We explore two definitions of fairness and show that for either definition the problem of finding the most fair package is NP-hard. We exploit the fact that our problem can be modeled as a coverage problem, and we propose greedy algorithms that find approximate solutions within reasonable time. In addition, we study two extensions of the problem, where we impose category or spatial constraints on the items to be included in the recommended packages. We evaluate the appropriateness of the fairness models and the performance of the proposed algorithms using real data from Yelp, and a user study.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {371–379},
numpages = {9},
keywords = {envy-freeness, fairness, package-to-group, proportionality, recommendation systems},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3132847.3132938,
author = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
title = {FA*IR: A Fair Top-k Ranking Algorithm},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3132847.3132938},
doi = {10.1145/3132847.3132938},
abstract = {In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n » k candidates, maximizing utility (i.e., select the "best" candidates) subject to group fairness criteria.Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above.An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1569–1578},
numpages = {10},
keywords = {top-k selection, ranking, bias in computer systems, algorithmic fairness},
location = {Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3194770.3194776,
author = {Verma, Sahil and Rubin, Julia},
title = {Fairness definitions explained},
year = {2018},
isbn = {9781450357463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194770.3194776},
doi = {10.1145/3194770.3194776},
abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
booktitle = {Proceedings of the International Workshop on Software Fairness},
pages = {1–7},
numpages = {7},
location = {Gothenburg, Sweden},
series = {FairWare '18}
}

@inproceedings{10.1145/3240323.3240372,
author = {Steck, Harald},
title = {Calibrated recommendations},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240372},
doi = {10.1145/3240323.3240372},
abstract = {When a user has watched, say, 70 romance movies and 30 action movies, then it is reasonable to expect the personalized list of recommended movies to be comprised of about 70\% romance and 30\% action movies as well. This important property is known as calibration, and recently received renewed attention in the context of fairness in machine learning. In the recommended list of items, calibration ensures that the various (past) areas of interest of a user are reflected with their corresponding proportions. Calibration is especially important in light of the fact that recommender systems optimized toward accuracy (e.g., ranking metrics) in the usual offline-setting can easily lead to recommendations where the lesser interests of a user get crowded out by the user's main interests-which we show empirically as well as in thought-experiments. This can be prevented by calibrated recommendations. To this end, we outline metrics for quantifying the degree of calibration, as well as a simple yet effective re-ranking algorithm for post-processing the output of recommender systems.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {154–162},
numpages = {9},
keywords = {calibration, diversity, fairness, recommender systems},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@inproceedings{10.1145/3292500.3330691,
author = {Geyik, Sahin Cem and Ambler, Stuart and Kenthapadi, Krishnaram},
title = {Fairness-Aware Ranking in Search \& Recommendation Systems with Application to LinkedIn Talent Search},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330691},
doi = {10.1145/3292500.3330691},
abstract = {We present a framework for quantifying and mitigating algorithmic bias in mechanisms designed for ranking individuals, typically used as part of web-scale search and recommendation systems. We first propose complementary measures to quantify bias with respect to protected attributes such as gender and age. We then present algorithms for computing fairness-aware re-ranking of results. For a given search or recommendation task, our algorithms seek to achieve a desired distribution of top ranked results with respect to one or more protected attributes. We show that such a framework can be tailored to achieve fairness criteria such as equality of opportunity and demographic parity depending on the choice of the desired distribution. We evaluate the proposed algorithms via extensive simulations over different parameter choices, and study the effect of fairness-aware ranking on both bias and utility measures. We finally present the online A/B testing results from applying our framework towards representative ranking in LinkedIn Talent Search, and discuss the lessons learned in practice. Our approach resulted in tremendous improvement in the fairness metrics (nearly three fold increase in the number of search queries with representative results) without affecting the business metrics, which paved the way for deployment to 100\% of LinkedIn Recruiter users worldwide. Ours is the first large-scale deployed framework for ensuring fairness in the hiring domain, with the potential positive impact for more than 630M LinkedIn members.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2221–2231},
numpages = {11},
keywords = {fairness-aware ranking, talent search \& recommendation systems},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3308560.3317595,
author = {Sapiezynski, Piotr and Zeng, Wesley and E Robertson, Ronald and Mislove, Alan and Wilson, Christo},
title = {Quantifying the Impact of User Attentionon Fair Group Representation in Ranked Lists},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317595},
doi = {10.1145/3308560.3317595},
abstract = {In this work, we introduce a novel metric for auditing group fairness in ranked lists. Our approach offers two benefits compared to the state of the art. First, we offer a blueprint for modeling of user attention. Rather than assuming a logarithmic loss in importance as a function of the rank, we can account for varying user behaviors through parametrization. For example, we expect a user to see more items during a viewing of a social media feed than when they inspect the results list of a single web search query. Second, we allow non-binary protected attributes to enable investigating inherently continuous attributes (e.g., political alignment on the liberal to conservative spectrum) as well as to facilitate measurements across aggregated sets of search results, rather than separately for each result list. By combining these two elements into our metric, we are able to better address the human factors inherent in this problem. We measure the whole sociotechnical system, consisting of a ranking algorithm and individuals using it, instead of exclusively focusing on the ranking algorithm. Finally, we use our metric to perform three simulated fairness audits. We show that determining fairness of a ranked output necessitates knowledge (or a model) of the end-users of the particular service. Depending on their attention distribution function, a fixed ranking of results can appear biased both in favor and against a protected group1.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {553–562},
numpages = {10},
keywords = {ranked lists, information retrieval, group fairness},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3397271.3401177,
author = {Zhu, Ziwei and Wang, Jianling and Caverlee, James},
title = {Measuring and Mitigating Item Under-Recommendation Bias in Personalized Ranking Systems},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3397271.3401177},
doi = {10.1145/3397271.3401177},
abstract = {Recommendation algorithms typically build models based on user-item interactions (e.g., clicks, likes, or ratings) to provide a personalized ranked list of items. These interactions are often distributed unevenly over different groups of items due to varying user preferences. However, we show that recommendation algorithms can inherit or even amplify this imbalanced distribution, leading to item under-recommendation bias. Concretely, we formalize the concepts of ranking-based statistical parity and equal opportunity as two measures of item under-recommendation bias. Then, we empirically show that one of the most widely adopted algorithms -- Bayesian Personalized Ranking -- produces biased recommendations, which motivates our effort to propose the novel debiased personalized ranking model. The debiased model is able to improve the two proposed bias metrics while preserving recommendation performance. Experiments on three public datasets show strong bias reduction of the proposed model versus state-of-the-art alternatives.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {449–458},
numpages = {10},
keywords = {equal opportunity, recommendation bias, recommender systems, statistical parity},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3404835.3462943,
author = {Lin, Chen and Liu, Xinyi and Xv, Guipeng and Li, Hui},
title = {Mitigating Sentiment Bias for Recommender Systems},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462943},
doi = {10.1145/3404835.3462943},
abstract = {Biases and de-biasing in recommender systems (RS) have become a research hotspot recently. This paper reveals an unexplored type of bias, i.e., sentiment bias. Through an empirical study, we find that many RS models provide more accurate recommendations on user/item groups having more positive feedback (i.e., positive users/items) than on user/item groups having more negative feedback (i.e., negative users/items). We show that sentiment bias is different from existing biases such as popularity bias: positive users/items do not have more user feedback (i.e., either more ratings or longer reviews). The existence of sentiment bias leads to low-quality recommendations to critical users and unfair recommendations for niche items. We discuss the factors that cause sentiment bias. Then, to fix the sources of sentiment bias, we propose a general de-biasing framework with three strategies manifesting in different regularizers that can be easily plugged into RS models without changing model architectures. Experiments on various RS models and benchmark datasets have verified the effectiveness of our de-biasing framework. To our best knowledge, sentiment bias and its de-biasing have not been studied before. We hope that this work can help strengthen the study of biases and de-biasing in RS.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {31–40},
numpages = {10},
keywords = {de-biasing, recommender systems, sentiment bias},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3442381.3449866,
author = {Li, Yunqi and Chen, Hanxiong and Fu, Zuohui and Ge, Yingqiang and Zhang, Yongfeng},
title = {User-oriented Fairness in Recommendation},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3442381.3449866},
doi = {10.1145/3442381.3449866},
abstract = {As a highly data-driven application, recommender systems could be affected by data bias, resulting in unfair results for different data groups, which could be a reason that affects the system performance. Therefore, it is important to identify and solve the unfairness issues in recommendation scenarios. In this paper, we address the unfairness problem in recommender systems from the user perspective. We group users into advantaged and disadvantaged groups according to their level of activity, and conduct experiments to show that current recommender systems will behave unfairly between two groups of users. Specifically, the advantaged users (active) who only account for a small proportion in data enjoy much higher recommendation quality than those disadvantaged users (inactive). Such bias can also affect the overall performance since the disadvantaged users are the majority. To solve this problem, we provide a re-ranking approach to mitigate this unfairness problem by adding constraints over evaluation metrics. The experiments we conducted on several real-world datasets with various recommendation algorithms show that our approach can not only improve group fairness of users in recommender systems, but also achieve better overall recommendation performance.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {624–632},
numpages = {9},
keywords = {AI Ethics, Fairness, Re-ranking, Recommendation System},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3442381.3450015,
author = {Wu, Le and Chen, Lei and Shao, Pengyang and Hong, Richang and Wang, Xiting and Wang, Meng},
title = {Learning Fair Representations for Recommendation: A Graph-based Perspective},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450015},
doi = {10.1145/3442381.3450015},
abstract = {As a key application of artificial intelligence, recommender systems are among the most pervasive computer aided systems to help users find potential items of interests. Recently, researchers paid considerable attention to fairness issues for artificial intelligence applications. Most of these approaches assumed independence of instances, and designed sophisticated models to eliminate the sensitive information to facilitate fairness. However, recommender systems differ greatly from these approaches as users and items naturally form a user-item bipartite graph, and are collaboratively correlated in the graph structure. In this paper, we propose a novel graph based technique for ensuring fairness of any recommendation models. Here, the fairness requirements refer to not exposing sensitive feature set in the user modeling process. Specifically, given the original embeddings from any recommendation models, we learn a composition of filters that transform each user’s and each item’s original embeddings into a filtered embedding space based on the sensitive feature set. For each user, this transformation is achieved under the adversarial learning of a user-centric graph, in order to obfuscate each sensitive feature between both the filtered user embedding and the sub graph structures of this user. Finally, extensive experimental results clearly show the effectiveness of our proposed model for fair recommendation. We publish the source code at https://github.com/newlei/FairGo.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2198–2208},
numpages = {11},
keywords = {user modeling, graph based recommendation, fairness, fair Representation learning, fair Recommendation},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3534678.3539269,
author = {Wei, Tianxin and He, Jingrui},
title = {Comprehensive Fair Meta-learned Recommender System},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539269},
doi = {10.1145/3534678.3539269},
abstract = {In recommender systems, one common challenge is the cold-start problem, where interactions are very limited for fresh users in the systems. To address this challenge, recently, many works introduce the meta-optimization idea into the recommendation scenarios, i.e. learning to learn the user preference by only a few past interaction items. The core idea is to learn global shared meta-initialization parameters for all users and rapidly adapt them into local parameters for each user respectively. They aim at deriving general knowledge across preference learning of various users, so as to rapidly adapt to the future new user with the learned prior and a small amount of training data. However, previous works have shown that recommender systems are generally vulnerable to bias and unfairness. Despite the success of meta-learning at improving the recommendation performance with cold-start, the fairness issues are largely overlooked.In this paper, we propose a comprehensive fair meta-learning framework, named CLOVER, for ensuring the fairness of meta-learned recommendation models. We systematically study three kinds of fairness - individual fairness, counterfactual fairness, and group fairness in the recommender systems, and propose to satisfy all three kinds via a multi-task adversarial learning scheme. Our framework offers a generic training paradigm that is applicable to different meta-learned recommender systems. We demonstrate the effectiveness of CLOVER on the representative meta-learned user preference estimator on three real-world data sets. Empirical results show that CLOVER achieves comprehensive fairness without deteriorating the overall cold-start recommendation performance.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1989–1999},
numpages = {11},
keywords = {fairness, meta-learning, recommender systems},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3547333,
author = {Wang, Yifan and Ma, Weizhi and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
title = {A Survey on the Fairness of Recommender Systems},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3547333},
doi = {10.1145/3547333},
abstract = {Recommender systems are an essential tool to relieve the information overload challenge and play an important role in people’s daily lives. Since recommendations involve allocations of social resources (e.g., job recommendation), an important issue is whether recommendations are fair. Unfair recommendations are not only unethical but also harm the long-term interests of the recommender system itself. As a result, fairness issues in recommender systems have recently attracted increasing attention. However, due to multiple complex resource allocation processes and various fairness definitions, the research on fairness in recommendation is scattered. To fill this gap, we review over 60 papers published in top conferences/journals, including TOIS, SIGIR, and WWW. First, we summarize fairness definitions in the recommendation and provide several views to classify fairness issues. Then, we review recommendation datasets and measurements in fairness studies and provide an elaborate taxonomy of fairness methods in the recommendation. Finally, we conclude this survey by outlining some promising future directions.},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {52},
numpages = {43},
keywords = {survey, fairness, Recommendation}
}

@inproceedings{10.1145/3589334.3648158,
author = {Jiang, Meng and Bao, Keqin and Zhang, Jizhi and Wang, Wenjie and Yang, Zhengyi and Feng, Fuli and He, Xiangnan},
title = {Item-side Fairness of Large Language Model-based Recommendation System},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-b.deakin.edu.au/10.1145/3589334.3648158},
doi = {10.1145/3589334.3648158},
abstract = {Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs). From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems. To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on MovieLens and Steam datasets, and observe significant item-side fairness improvements. The code can be found in https://github.com/JiangM-C/IFairLRS.git.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {4717–4726},
numpages = {10},
keywords = {item-side fairness, large language model, recommendation},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3626772.3657794,
author = {Jaenich, Thomas and McDonald, Graham and Ounis, Iadh},
title = {Fairness-Aware Exposure Allocation via Adaptive Reranking},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657794},
doi = {10.1145/3626772.3657794},
abstract = {In the first stage of a re-ranking pipeline, an inexpensive ranking model is typically deployed to retrieve a set of documents that are highly likely to be relevant to the user's query. The retrieved documents are then re-ranked by a more effective but expensive ranking model, e.g., a deep neural ranker such as BERT. However, in such a standard pipeline, no new documents are typically discovered after the first stage retrieval. Hence, the amount of exposure that a particular group of documents - e.g., documents from a particular demographic category - can receive is limited by the number of documents that are retrieved in the first stage retrieval. Indeed, if too few documents from a group are retrieved in the first stage retrieval, ensuring that the group receives a fair amount of exposure to the user may become infeasible. Therefore, it is useful to identify more documents from underrepresented groups that are potentially relevant to the query during the re-ranking stage. In this work, we investigate how deploying adaptive re-ranking, which enables the discovery of additional potentially relevant documents in the re-ranking stage, can improve the exposure that a given group of documents receives in the final ranking. We propose six adaptive re-ranking policies that can discover documents from underrepresented groups to increase the disadvantaged groups' exposure in the final ranking. Our experiments on the TREC 2021 and 2022 Fair Ranking Track test collections show that our policies consistently improve the fairness of the exposure distribution in the final ranking, compared to standard adaptive re-ranking approaches, resulting in increases of up to ~13\% in Attention Weighted Ranked Fairness (AWRF). Moreover, our best performing policy, Policy 6, consistently maintains and frequently increases the utility of the search results in terms of nDCG.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1504–1513},
numpages = {10},
keywords = {adaptive re-ranking, exposure, group fairness},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.5555/3294996.3295162,
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
title = {Counterfactual fairness},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4069–4079},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@INPROCEEDINGS{10068703,
  author={Wu, Kun and Erickson, Jacob and Wang, Wendy Hui and Ning, Yue},
  booktitle={2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
  title={Equipping Recommender Systems with Individual Fairness via Second-order Proximity Embedding}, 
  year={2022},
  volume={},
  number={},
  pages={171-175},
  keywords={Measurement;Social networking (online);Message passing;Knowledge graphs;Graph neural networks;Recommender systems;Algorithmic fairness;Recommender systems;Second-order proximity embedding;Graph neural networks},
  doi={10.1109/ASONAM55673.2022.10068703}}

@INPROCEEDINGS{9101635,
  author={Foulds, James R. and Islam, Rashidul and Keya, Kamrun Naher and Pan, Shimei},
  booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)}, 
  title={An Intersectional Definition of Fairness}, 
  year={2020},
  volume={},
  number={},
  pages={1918-1921},
  keywords={Privacy;Law;Machine learning;Lenses;fairness in AI;AI and society;80% rule;privacy},
  doi={10.1109/ICDE48307.2020.00203}}

@article{Acharyya_Das_Chattoraj_Tanveer_2020, title={FairyTED: A Fair Rating Predictor for TED Talk Data}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5368}, DOI={10.1609/aaai.v34i01.5368}, abstractNote={&lt;p&gt;With the recent trend of applying machine learning in every aspect of human life, it is important to incorporate fairness into the core of the predictive algorithms. We address the problem of predicting the quality of public speeches while being fair with respect to sensitive attributes of the speakers, e.g. &lt;em&gt;gender&lt;/em&gt; and &lt;em&gt;race&lt;/em&gt;. We use the TED talks as an input repository of public speeches because it consists of speakers from a diverse community and has a wide outreach. Utilizing the theories of &lt;em&gt;Causal Models&lt;/em&gt;, &lt;em&gt;Counterfactual Fairness&lt;/em&gt; and state-of-the-art neural language models, we propose a mathematical framework for fair prediction of the public speaking quality. We employ grounded assumptions to construct a causal model capturing how different &lt;em&gt;attributes&lt;/em&gt; affect public speaking quality. This causal model contributes in generating counterfactual data to train a &lt;em&gt;fair&lt;/em&gt; predictive model. Our framework is general enough to utilize any assumption within the causal model. Experimental results show that while prediction accuracy is comparable to recent work on this dataset, our predictions are counterfactually fair with respect to a novel metric when compared to true data labels. The FairyTED setup not only allows organizers to make informed and diverse selection of speakers from the unobserved counterfactual possibilities but it also ensures that viewers and new users are not influenced by unfair and unbalanced ratings from arbitrary visitors to the ted.com website when deciding to view a talk.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Acharyya, Rupam and Das, Shouman and Chattoraj, Ankani and Tanveer, Md. Iftekhar}, year={2020}, month={Apr.}, pages={338-345} }

@article{Chouldechova2016FairPW,
  title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
  author={Alexandra Chouldechova},
  journal={Big data},
  year={2016},
  volume={5 2},
  pages={
          153-163
        },
  url={https://api.semanticscholar.org/CorpusID:1443041}
}

@article{DBLP:journals/corr/abs-1809-09030,
  author       = {Golnoosh Farnadi and
                  Pigi Kouki and
                  Spencer K. Thompson and
                  Sriram Srinivasan and
                  Lise Getoor},
  title        = {A Fairness-aware Hybrid Recommender System},
  journal      = {CoRR},
  volume       = {abs/1809.09030},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.09030},
  eprinttype    = {arXiv},
  eprint       = {1809.09030},
  timestamp    = {Fri, 04 Sep 2020 14:32:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-09030.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Edizel20,
author = {Edizel, Bora and Bonchi, Francesco and Hajian, Sara and Panisson, Andre and Tassa, Tamir},
year = {2020},
month = {03},
pages = {197-213},
title = {FaiRecSys: mitigating algorithmic bias in recommender systems},
volume = {9},
journal = {International Journal of Data Science and Analytics},
doi = {10.1007/s41060-019-00181-5}
}

@inproceedings{Fu20,
author = {Fu, Zuohui and Xian, Yikun and Gao, Ruoyuan and Zhao, Jieyu and Huang, Qiaoying and Ge, Yingqiang and Xu, Shuyuan and Geng, Shijie and Shah, Chirag and Zhang, Yongfeng and de Melo, Gerard},
title = {Fairness-Aware Explainable Recommendation over Knowledge Graphs},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401051},
doi = {10.1145/3397271.3401051},
abstract = {There has been growing attention on fairness considerations recently, especially in the context of intelligent decision making systems. For example, explainable recommendation systems may suffer from both explanation bias and performance disparity. We show that inactive users may be more susceptible to receiving unsatisfactory recommendations due to their insufficient training data, and that their recommendations may be biased by the training records of active users due to the nature of collaborative filtering, which leads to unfair treatment by the system. In this paper, we analyze different groups of users according to their level of activity, and find that bias exists in recommendation performance between different groups. Empirically, we find that such performance gap is caused by the disparity of data distribution, specifically the knowledge graph path distribution in this work. We propose a fairness constrained approach via heuristic re-ranking to mitigate this unfairness problem in the context of explainable recommendation over knowledge graphs. We experiment on several real-world datasets with state-of-the-art knowledge graph-based explainable recommendation algorithms. The promising results show that our algorithm is not only able to provide high-quality explainable recommendations, but also reduces the recommendation unfairness in several aspects.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {69–78},
numpages = {10},
keywords = {explainable recommendation, fairness, knowledge graphs},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{Gohar2023ASO,
  title={A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges},
  author={Usman Gohar and Lu Cheng},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258615326}
}

@article{JIN2023101906,
title = {A survey on fairness-aware recommender systems},
journal = {Information Fusion},
volume = {100},
pages = {101906},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101906},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523002221},
author = {Di Jin and Luzhi Wang and He Zhang and Yizhen Zheng and Weiping Ding and Feng Xia and Shirui Pan},
keywords = {Recommender systems, Fairness, Trustworthiness, Survey},
abstract = {As information filtering services, recommender systems have extremely enriched our daily life by providing personalized suggestions and facilitating people in decision-making, which makes them vital and indispensable to human society in the information era. However, as people become more dependent on them, recent studies show that recommender systems potentially own unintentional impacts on society and individuals because of their unfairness (e.g., gender discrimination in job recommendations). To develop trustworthy services, it is crucial to devise fairness-aware recommender systems that can mitigate these bias issues. In this survey, we summarize existing methodologies and practices of fairness in recommender systems. Firstly, we present concepts of fairness in different recommendation scenarios, comprehensively categorize current advances, and introduce typical methods to promote fairness in different stages of recommender systems. Next, after introducing datasets and evaluation metrics applied to assess the fairness of recommender systems, we will delve into the significant influence that fairness-aware recommender systems exert on real-world industrial applications. Subsequently, we highlight the connection between fairness and other principles of trustworthy recommender systems, aiming to consider trustworthiness principles holistically while advocating for fairness. Finally, we summarize this review, spotlighting promising opportunities in comprehending concepts, frameworks, the balance between accuracy and fairness, and the ties with trustworthiness, with the ultimate goal of fostering the development of fairness-aware recommender systems.}
}

@article{LIU2022108058,
title = {Dual constraints and adversarial learning for fair recommenders},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {108058},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.108058},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121011424},
author = {Haifeng Liu and Nan Zhao and Xiaokun Zhang and Hongfei Lin and Liang Yang and Bo Xu and Yuan Lin and Wenqi Fan},
keywords = {Fair recommendation, Graph neural network, Recommender systems, Adversarial learning},
abstract = {Recommender systems, which are consist of common artificial intelligence technology, have a profound impact on the lifestyles of people. However, recent studies have demonstrated that recommender systems have fairness problems which means that some people with certain attributes are treated unfairly. A fair recommender means that users with different attributes achieve the same recommender accuracy. In particular, the recommender systems completely rely on users’ behavior data for preferences learning, which leads to a high probability of unfair problems because that the behavior data usually contains sensitive information of users. Unfortunately, there are a few studies exploring unfair problem in recommender systems. To alleviate this problem, we present a novel fairness-aware recommender with dual fairness constraints (FRFC) to improve fairness in recommendations and protect the user’s sensitive information from being exposed. This model has several advantages: one advantage is that an adversarial-based graph neural network (GNN) is proposed to prevent the target user being infected by sensitive features of neighbor users; another advantage is that two fairness constraints are proposed to solve the problems of adversarial classifier failures in whole data and unfair ranking losses. With this design, the FRFC model can effectively filter out users’ sensitive information and give users of different attributes the same training opportunities, which is helpful for making a fair recommendation. Finally, extensive experiments demonstrate that the proposed model can significantly improve the fairness of recommendation results.}
}

@article{Melchiorre21,
author = {Melchiorre, Alessandro B. and Rekabsaz, Navid and Parada-Cabaleiro, Emilia and Brandl, Stefan and Lesota, Oleg and Schedl, Markus},
title = {Investigating gender fairness of recommendation algorithms in the music domain},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {58},
number = {5},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2021.102666},
doi = {10.1016/j.ipm.2021.102666},
journal = {Inf. Process. Manage.},
month = {sep},
numpages = {27},
keywords = {Recommender systems, Music, Bias, Neural networks, Demographics}
}

@inproceedings{NIPS2017_e6384711,
 author = {Yao, Sirui and Huang, Bert},
address = {Red Hook, NY, USA},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Beyond Parity: Fairness Objectives for Collaborative Filtering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{TsintzouPT19,
  author       = {Virginia Tsintzou and
                  Evaggelia Pitoura and
                  Panayiotis Tsaparas},
  editor       = {Robin Burke and
                  Himan Abdollahpouri and
                  Edward C. Malthouse and
                  K. P. Thai and
                  Yongfeng Zhang},
  title        = {Bias Disparity in Recommendation Systems},
  booktitle    = {Proceedings of the Workshop on Recommendation in Multi-stakeholder
                  Environments co-located with the 13th {ACM} Conference on Recommender
                  Systems (RecSys 2019), Copenhagen, Denmark, September 20, 2019},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {2440},
  publisher    = {CEUR-WS.org},
  year         = {2019},
  url          = {https://ceur-ws.org/Vol-2440/short4.pdf},
  timestamp    = {Fri, 10 Mar 2023 16:22:14 +0100},
  biburl       = {https://dblp.org/rec/conf/recsys/TsintzouPT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Weydemann19,
author = {Weydemann, Leonard and Sacharidis, Dimitris and Werthner, Hannes},
title = {Defining and measuring fairness in location recommendations},
year = {2019},
isbn = {9781450369633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy-f.deakin.edu.au/10.1145/3356994.3365497},
doi = {10.1145/3356994.3365497},
abstract = {Location-based recommender systems learn from historical movement traces of users in order to make recommendations for places to visit, events to attend, itineraries to follow. As with other systems assisting humans in their decisions, there is an increasing need to scrutinize the implications of algorithmically made location recommendations. The challenge is that one can define different fairness concerns, as both users and locations may be subjects of unfair treatment. In this work, we propose a comprehensive framework that allows the expression of various fairness aspects, and quantify the degree to which the system is acting justly. In a case study, we focus on three fairness aspects, and investigate several types of location-based recommenders in terms of their ability to be fair under the studied aspects.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
articleno = {6},
numpages = {8},
keywords = {recommender systems, location-based recommendations, fairness, discrimination, bias},
location = {Chicago, Illinois},
series = {LocalRec '19}
}

@article{XIA2019104857,
title = {WE-Rec: A fairness-aware reciprocal recommendation based on Walrasian equilibrium},
journal = {Knowledge-Based Systems},
volume = {182},
pages = {104857},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119303326},
author = {Bin Xia and Junjie Yin and Jian Xu and Yun Li},
keywords = {Recommender systems, Fairness-aware, Reciprocal recommendation, Walrasian equilibrium, Multi-objective optimization},
abstract = {The emergence of online dating and recruiting platforms brings big challenges to the reciprocal recommendation which has attracted a lot of research attention. Most previous approaches improved the accuracy and diversity of reciprocal recommendations, but few researcher made efforts on the fairness-aware recommendation which aims to avoid the discrimination and mistreatment of vulnerable groups. In this paper, we concentrate on the research of fairness-aware recommendations in the reciprocal recommender system and propose an approach to rerank the recommendation list by optimizing three significant fairness-aware criteria between parties (i.e., buyers and sellers) based on Walrasian equilibrium: (1) the disparity of service; (2) the similarity of mutual preference; (3) the equilibrium of demand and supply. According to these definitions of fairness, we cast the task of reciprocal recommendation as a multi-objective optimization considering the satisfaction of individuals, the fairness of recommendations, and the market clearing simultaneously. The extensive experiments are conducted on two real-world datasets, 
            and the results demonstrate the effectiveness of our approach.}
}

@inproceedings{boratto2022consumer,
  title={Consumer fairness in recommender systems: Contextualizing definitions and mitigations},
  author={Boratto, Ludovico and Fenu, Gianni and Marras, Mirko and Medda, Giacomo},
  booktitle={European Conference on Information Retrieval},
  pages={552--566},
  year={2022},
  organization={Springer}
}

@misc{burke_multisided_2017,
	title = {Multisided {Fairness} for {Recommendation}},
	url = {http://arxiv.org/abs/1707.00093},
	doi = {10.48550/arXiv.1707.00093},
	abstract = {Recent work on machine learning has begun to consider issues of fairness. In this paper, we extend the concept of fairness to recommendation. In particular, we show that in some recommendation contexts, fairness may be a multisided concept, in which fair outcomes for multiple individuals need to be considered. Based on these considerations, we present a taxonomy of classes of fairness-aware recommender systems and suggest possible fairness-aware recommendation architectures.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Burke, Robin},
	month = jul,
	year = {2017},
	note = {arXiv:1707.00093 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:C\:\\Users\\s224091662\\Zotero\\storage\\2JD7ZYQT\\Burke - 2017 - Multisided Fairness for Recommendation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\s224091662\\Zotero\\storage\\EZUZI48T\\1707.html:text/html},
}

@article{deldjoo_flexible_2021,
	title = {A flexible framework for evaluating user and item fairness in recommender systems},
	volume = {31},
	issn = {1573-1391},
	url = {https://doi.org/10.1007/s11257-020-09285-1},
	doi = {10.1007/s11257-020-09285-1},
	abstract = {One common characteristic of research works focused on fairness evaluation (in machine learning) is that they call for some form of parity (equality) either in treatment—meaning they ignore the information about users’ memberships in protected classes during training—or in impact—by enforcing proportional beneficial outcomes to users in different protected classes. In the recommender systems community, fairness has been studied with respect to both users’ and items’ memberships in protected classes defined by some sensitive attributes (e.g., gender or race for users, revenue in a multi-stakeholder setting for items). Again here, the concept has been commonly interpreted as some form of equality—i.e., the degree to which the system is meeting the information needs of all its users in an equal sense. In this work, we propose a probabilistic framework based on generalized cross entropy (GCE) to measure fairness of a given recommendation model. The framework comes with a suite of advantages: first, it allows the system designer to define and measure fairness for both users and items and can be applied to any classification task; second, it can incorporate various notions of fairness as it does not rely on specific and predefined probability distributions and they can be defined at design time; finally, in its design it uses a gain factor, which can be flexibly defined to contemplate different accuracy-related metrics to measure fairness upon decision-support metrics (e.g., precision, recall) or rank-based measures (e.g., NDCG, MAP). An experimental evaluation on four real-world datasets shows the nuances captured by our proposed metric regarding fairness on different user and item attributes, where nearest-neighbor recommenders tend to obtain good results under equality constraints. We observed that when the users are clustered based on both their interaction with the system and other sensitive attributes, such as age or gender, algorithms with similar performance values get different behaviors with respect to user fairness due to the different way they process data for each user cluster.},
	language = {en},
	number = {3},
	urldate = {2024-06-24},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Deldjoo, Yashar and Anelli, Vito Walter and Zamani, Hamed and Bellogín, Alejandro and Di Noia, Tommaso},
	month = jul,
	year = {2021},
	pages = {457--511},
	file = {Full Text PDF:C\:\\Users\\s224091662\\Zotero\\storage\\EEXETF29\\Deldjoo et al. - 2021 - A flexible framework for evaluating user and item .pdf:application/pdf},
}

@inproceedings{do2022online,
  title={Online certification of preference-based fairness for personalized recommender systems},
  author={Do, Virginie and Corbett-Davies, Sam and Atif, Jamal and Usunier, Nicolas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={6532--6540},
  year={2022}
}

@misc{foulds2019bayesianmodelingintersectionalfairness,
      title={Bayesian Modeling of Intersectional Fairness: The Variance of Bias}, 
      author={James Foulds and Rashidul Islam and Kamrun Keya and Shimei Pan},
      year={2019},
      eprint={1811.07255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.07255}, 
}

@inproceedings{ghosh21,
author = {Ghosh, Avijit and Dutt, Ritam and Wilson, Christo},
title = {When Fair Ranking Meets Uncertain Inference},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462850},
doi = {10.1145/3404835.3462850},
abstract = {Existing fair ranking systems, especially those designed to be demographically fair, assume that accurate demographic information about individuals is available to the ranking algorithm. In practice, however, this assumption may not hold --- in real-world contexts like ranking job applicants or credit seekers, social and legal barriers may prevent algorithm operators from collecting peoples' demographic information. In these cases, algorithm operators may attempt to infer peoples' demographics and then supply these inferences as inputs to the ranking algorithm.In this study, we investigate how uncertainty and errors in demographic inference impact the fairness offered by fair ranking algorithms. Using simulations and three case studies with real datasets, we show how demographic inferences drawn from real systems can lead to unfair rankings. Our results suggest that developers should not use inferred demographic data as input to fair ranking algorithms, unless the inferences are extremely accurate.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1033–1043},
numpages = {11},
keywords = {algorithmic fairness, demographic inference, ethical AI, noisy protected attributes, ranking algorithms, uncertainty},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{gupta_questioning_2022,
	title = {Questioning {Racial} and {Gender} {Bias} in {AI}-based {Recommendations}: {Do} {Espoused} {National} {Cultural} {Values} {Matter}?},
	volume = {24},
	issn = {1572-9419},
	url = {https://doi.org/10.1007/s10796-021-10156-2},
	doi = {10.1007/s10796-021-10156-2},
	abstract = {One realm of AI, recommender systems have attracted significant research attention due to concerns about its devastating effects to society’s most vulnerable and marginalised communities. Both media press and academic literature provide compelling evidence that AI-based recommendations help to perpetuate and exacerbate racial and gender biases. Yet, there is limited knowledge about the extent to which individuals might question AI-based recommendations when perceived as biased. To address this gap in knowledge, we investigate the effects of espoused national cultural values on AI questionability, by examining how individuals might question AI-based recommendations due to perceived racial or gender bias. Data collected from 387 survey respondents in the United States indicate that individuals with espoused national cultural values associated to collectivism, masculinity and uncertainty avoidance are more likely to question biased AI-based recommendations. This study advances understanding of how cultural values affect AI questionability due to perceived bias and it contributes to current academic discourse about the need to hold AI accountable.},
	number = {5},
	journal = {Information Systems Frontiers},
	author = {Gupta, Manjul and Parra, Carlos M. and Dennehy, Denis},
	month = oct,
	year = {2022},
	pages = {1465--1481},
}

@inproceedings{hao21,
author = {Hao, Qianxiu and Xu, Qianqian and Yang, Zhiyong and Huang, Qingming},
title = {Pareto Optimality for Fairness-constrained Collaborative Filtering},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475706},
doi = {10.1145/3474085.3475706},
abstract = {The well-known collaborative filtering (CF) models typically optimize a single objective summed over all historical user-item interactions. Due to inevitable imbalances and biases in real-world data, they may develop a policy that unfairly discriminates against certain subgroups with low sample frequencies. To balance overall recommendation performance and fairness, prevalent solutions apply fairness constraints or regularizations to enforce equality of certain performance across different subgroups. However, simply enforcing equality of performance may lead to large performance degradation of those advantaged subgroups. To address this issue, we formulate a constrained Multi-Objective Optimization (MOO) problem. In contrast to the single objective, we treat the performance of each subgroup equivalently as an objective. This ensures that the imbalanced subgroup sample frequency does not affect the gradient information. We further propose fairness constraints to limit the search space to obtain more balanced solutions. To solve the constrained MOO problem, a gradient-based constrained MOO algorithm is proposed to seek a proper Pareto optimal solution for the performance trade-off. Extensive experiments on synthetic and real-world datasets show that our approach could help improve the recommendation accuracy of disadvantaged groups, while not damaging the overall performance.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {5619–5627},
numpages = {9},
keywords = {collaborative filtering, constrained multi-objective optimization, pareto optimal},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{hardt16,
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title = {Equality of opportunity in supervised learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3323–3331},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{islam21,
author = {Islam, Rashidul and Keya, Kamrun Naher and Zeng, Ziqian and Pan, Shimei and Foulds, James},
title = {Debiasing Career Recommendations with Neural Fair Collaborative Filtering},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449904},
doi = {10.1145/3442381.3449904},
abstract = {A growing proportion of human interactions are digitized on social media platforms and subjected to algorithmic decision-making, and it has become increasingly important to ensure fair treatment from these algorithms. In this work, we investigate gender bias in collaborative-filtering recommender systems trained on social media data. We develop neural fair collaborative filtering (NFCF), a practical framework for mitigating gender bias in recommending career-related sensitive items (e.g. jobs, academic concentrations, or courses of study) using a pre-training and fine-tuning approach to neural collaborative filtering, augmented with bias correction techniques. We show the utility of our methods for gender de-biased career and college major recommendations on the MovieLens dataset and a Facebook dataset, respectively, and achieve better performance and fairer behavior than several state-of-the-art models.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3779–3790},
numpages = {12},
keywords = {AI \& society, Fairness in AI, career recommendation, collaborative filtering, ethical issues, recommender systems, social media},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@misc{kheya2024pursuitfairnessartificialintelligence,
      title={The Pursuit of Fairness in Artificial Intelligence Models: A Survey}, 
      author={Tahsin Alamgir Kheya and Mohamed Reda Bouadjenek and Sunil Aryal},
      year={2024},
      eprint={2403.17333},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.17333}, 
}

@misc{lin2019crankvolumepreferencebias,
      title={Crank up the volume: preference bias amplification in collaborative recommendation}, 
      author={Kun Lin and Nasim Sonboli and Bamshad Mobasher and Robin Burke},
      year={2019},
      eprint={1909.06362},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/1909.06362}, 
}

@InProceedings{pmlr-v139-gorantla21a,
  title = 	 {On the Problem of Underranking in Group-Fair Ranking},
  author =       {Gorantla, Sruthi and Deshpande, Amit and Louis, Anand},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3777--3787},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/gorantla21a/gorantla21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/gorantla21a.html},
  abstract = 	 {Bias in ranking systems, especially among the top ranks, can worsen social and economic inequalities, polarize opinions, and reinforce stereotypes. On the other hand, a bias correction for minority groups can cause more harm if perceived as favoring group-fair outcomes over meritocracy. Most group-fair ranking algorithms post-process a given ranking and output a group-fair ranking. In this paper, we formulate the problem of underranking in group-fair rankings based on how close the group-fair rank of each item is to its original rank, and prove a lower bound on the trade-off achievable for simultaneous underranking and group fairness in ranking. We give a fair ranking algorithm that takes any given ranking and outputs another ranking with simultaneous underranking and group fairness guarantees comparable to the lower bound we prove. Our experimental results confirm the theoretical trade-off between underranking and group fairness, and also show that our algorithm achieves the best of both when compared to the state-of-the-art baselines.}
}

@misc{rus2022closinggenderwagegap,
      title={Closing the Gender Wage Gap: Adversarial Fairness in Job Recommendation}, 
      author={Clara Rus and Jeffrey Luppes and Harrie Oosterhuis and Gido H. Schoenmacker},
      year={2022},
      eprint={2209.09592},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.09592}, 
}

@inproceedings{wan20,
author = {Wan, Mengting and Ni, Jianmo and Misra, Rishabh and McAuley, Julian},
title = {Addressing Marketing Bias in Product Recommendations},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371855},
doi = {10.1145/3336191.3371855},
abstract = {Modern collaborative filtering algorithms seek to provide personalized product recommendations by uncovering patterns in consumer-product interactions. However, these interactions can be biased by how the product is marketed, for example due to the selection of a particular human model in a product image. These correlations may result in the underrepresentation of particular niche markets in the interaction data; for example, a female user who would potentially like motorcycle products may be less likely to interact with them if they are promoted using stereotypically 'male' images.In this paper, we first investigate this correlation between users' interaction feedback and products' marketing images on two real-world e-commerce datasets. We further examine the response of several standard collaborative filtering algorithms to the distribution of consumer-product market segments in the input interaction data, revealing that marketing strategy can be a source of bias for modern recommender systems. In order to protect recommendation performance on underrepresented market segments, we develop a framework to address this potential marketing bias. Quantitative results demonstrate that the proposed approach significantly improves the recommendation fairness across different market segments, with a negligible loss (or better) recommendation accuracy.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {618–626},
numpages = {9},
keywords = {machine learning fairness, marketing bias, recommender systems},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{wei2022comprehensive,
  title={Comprehensive Fair Meta-learned Recommender System},
  author={Wei, Tianxin and He, Jingrui},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={1989--1999},publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
  year={2022}
}

@unknown{wu20,
author = {Wu, Chuhan and Wu, Fangzhao and Wang, Xiting and Huang, Yongfeng and Xie, Xing},
year = {2020},
month = {06},
pages = {},
title = {Fairness-aware News Recommendation with Decomposed Adversarial Learning}
}

@misc{yang2020causalintersectionalityfairranking,
      title={Causal intersectionality for fair ranking}, 
      author={Ke Yang and Joshua R. Loftus and Julia Stoyanovich},
      year={2020},
      eprint={2006.08688},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.08688}, 
}

@inproceedings{zhu18,
author = {Zhu, Ziwei and Hu, Xia and Caverlee, James},
year = {2018},
month = {10},
pages = {1153-1162},publisher = {Association for Computing Machinery},
address = {New York, NY, USA},booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
title = {Fairness-Aware Tensor-Based Recommendation},
doi = {10.1145/3269206.3271795}
}

