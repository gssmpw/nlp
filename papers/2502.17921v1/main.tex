%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%

% \documentclass[manuscript]{acmart}
% \documentclass[sigconf]{acmart}
\documentclass[sigconf]{acmart}
% \documentclass[sigconf,natbib=true,review,anonymous=true]{acmart}
\usepackage{multirow}
% \usepackage{longtable}
\usepackage{graphicx}
\usepackage{enumitem}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\copyrightyear{2025}
\acmYear{2025}
\setcopyright{cc}
\setcctype{by-nd}
\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3696410.3714528}
\acmISBN{979-8-4007-1274-6/25/04}
% 1 Authors, replace the red X's with your assigned DOI string during the rightsreview eform process.
% 2 Your DOI link will become active when the proceedings appears in the DL.
% 3 Retain the DOI string between the curly braces for uploading your presentation video.

\settopmatter{printacmref=true}






%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{Equal Lights, Diverse Camera, Fair Action!}

\title{Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware Fairness}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Tahsin Alamgir Kheya}
% \authornote{Both authors contributed equally to this research.}
\email{t.kheya@deakin.edu.au}
\orcid{0009-0001-2481-2877}
\affiliation{%
  \institution{Deakin University}
  \city{Geelong}
  \state{VIC}
  \country{Australia}
}

\author{Mohamed Reda Bouadjenek}
\email{reda.bouadjenek@deakin.edu.au}
\orcid{0000-0003-1807-430X}
\affiliation{%
  \institution{Deakin University}
  \city{Geelong}
  \state{VIC}
  \country{Australia}}

\author{Sunil Aryal}
\email{sunil.aryal@deakin.edu.au}
\orcid{0000-0002-6639-6824}
\affiliation{%
  \institution{Deakin University}
  \city{Geelong}
  \state{VIC}
  \country{Australia}
}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Tahsin Alamgir Kheya, Mohamed Reda Bouadjenek, and Sunil Aryal}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

\begin{abstract}
Recommendation systems are now an integral part of our daily lives. We rely on them for tasks such as discovering new movies, finding friends on social media, and connecting job seekers with relevant opportunities.
Given their vital role, we must ensure these recommendations are free from societal stereotypes. 
Therefore, evaluating and addressing such biases in recommendation systems is crucial. 
Previous work evaluating the fairness of recommended items fails to capture certain nuances as they mainly focus on comparing performance metrics for different sensitive groups.
In this paper, we introduce a set of comprehensive metrics for quantifying gender bias in recommendations.
Specifically, we show the importance of evaluating fairness on a more granular level, which can be achieved using our metrics to capture gender bias using categories of recommended items like genres for movies. Furthermore, we show that employing a category-aware fairness metric as a regularization term along with the main recommendation loss during training can help effectively minimize bias in the models' output.
% We chose one of these metrics to offer a new fairness regularization term that, when optimized along with the main recommendation loss, can help effectively minimize bias in the models' output. 
We experiment on three real-world datasets, using five baseline models alongside two popular fairness-aware models, to show the effectiveness of our metrics in evaluating gender bias. 
Our metrics help provide an enhanced insight into bias in recommended items compared to previous metrics. Additionally, our results demonstrate how incorporating our regularization term significantly improves the fairness in recommendations for different categories without substantial degradation in overall recommendation performance.
% \noindent 
% {\bf Keywords:} Fairness Metrics, Bias in Recommendations, 
% Societal Stereotypes
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178</concept_id>
       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003331.10003271</concept_id>
       <concept_desc>Information systems~Personalization</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003347.10003350</concept_id>
       <concept_desc>Information systems~Recommender systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Information systems~Personalization}
\ccsdesc[500]{Information systems~Recommender systems}
% \ccsdesc[500]{Computing methodologies → Artificial intelligence → Fairness, accountability, and transparency}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Fairness Metrics, Bias in Recommendations, 
Societal Stereotypes, Recommender System
}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
 \maketitle
 \textit{This paper has been accepted for the ACM Web Conference 2025.}


\section{Introduction}
% \input{samples/section/Introduction}
Recommender Systems (RS) personalize item selections for users, providing suggestions based on individual preferences and behaviors. 
These systems have an important impact on our decision-making, as they are widely employed across diverse platforms like e-commerce, social media, streaming services, and news outlets, shaping the content and products we encounter.
For several online platforms, recommendation systems help create an engaging experience for users by diversifying and personalizing the content and interactions. 
This would help users avoid information overload and help them focus on options that reflect their past behaviors. 
Amid the promise held by RS, however, there are concerns about potential bias in these systems. 
For example, research has shown that, on certain recommender systems, simply changing the gender in a job search—while keeping qualifications constant—can significantly influence access to high-paying positions \cite{datta15,10.1145/3442381.3450077}.

RS algorithms are traditionally evaluated using measures like RMSE (Root Mean Squared Error), NDCG (Normalized Discounted Cumulative Gain), precision, and diversity \cite{Shani2021}. 
If only these metrics are considered, then the recommended items are deemed good when they align with user preferences.
% and also diverse in terms of the content type. 
These metrics can help evaluate the performance of the RS, but in recent years there is a growing emphasis on evaluating and ensuring fairness as well \cite{10.1145/3442381.3449866,10.1145/3477495.3531959,abdollahpouri2019unfairness,10.1145/3450613.3456821,NEURIPS2020_9d752cb0,10.1145/3437963.3441824}. 
For instance, the authors in \cite{deldjoo_flexible_2021} introduce a fairness evaluation metric that can be sensitive to different fairness notions like user-centric or item-centric. 
% Another work \cite{NIPS2017_e6384711}, presents four new metrics to quantify discrepancies between advantaged and disadvantaged groups. 
Although substantial work has been done in this field in recent years, there is a notable gap in research specifically focused on robust ways to quantify consumer bias accurately. 
Most of the metrics used are deficient in the following ways: 
(i) they over-simplify the concept of fairness,
(ii) they fail to consider the ranking of recommended items, and
(iii) they rely exclusively on a single type of fairness metric. 
A popular approach to evaluating consumer-side fairness in recommendation systems involves an adaptation of \textit{equal opportunity} (refer to Section: \ref{subsec:fair_notion}), which aims to balance performance metrics (such as recall) or utility scores across different groups, such as male vs. female users \cite{10.1145/3442381.3449866,10.1145/3655631,10.1145/3651167,10.1145/3604915.3608784,tang_when_2023,Melchiorre21}. 
However, while this approach is quite straightforward, it may overlook disparities in recommendations across different item categories.
To assess these disparities in recommendations, we refer to Figure \ref{fig:compare_genre}, where we present, for various recommendation algorithms, an analysis of the proportion of action and romance movies among the top 10 recommendations for male and female user groups, along with the corresponding Precision@10 values for each group.
There are three notable observations here:
(i) across all models, there is a noticeable disparity in the proportion of romance and action movies recommended, as romance movies tend to be recommended more frequently to female users, while action movies are more often recommended to male users;
(ii) precision@10 values for both male and female users are similar across all models, suggesting that the models appear to perform equally well for both genders based on this metric;
and (iii) different models exhibit varying levels of bias.
These observations suggest that simply comparing performance metrics across sensitive attributes is insufficient to assess fairness in recommendation systems. 
More granular metrics are necessary to accurately quantify bias and ensure fair recommendations across different user groups. To further emphasize the significance of granular evaluation, we provide a brief overview here with a more detailed example provided in Section \ref{subsec:motivating_example}. Bias can arise in recommender systems due to stereotypical interactions in the dataset used to train them, which leads to biased recommendations based on sensitive attributes of the users. This bias can trap users in filter bubbles, with a focus on stereotypical categories or narrowly defined preferences. Providing more of a certain type of content can have unintended consequences; for instance, if a young male user is predominantly recommended action movies, where violence is glorified and shown in a consequence-free way, then it could contribute to adverse effects, including the desensitization of violence. This is just one example, but the stakes are much higher for more sensitive domains like news, job recommendations, etc., where there can be catastrophic consequences if similar biases manifest in them.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{images/gender_compare_new.pdf}
    \caption{
    Comparison of action and romance movie recommendations among male and female users across four recommendation algorithms, along with corresponding Precision@10 values. The graphs highlight disparities in genre recommendations by gender, with romance movies being more frequently suggested to female users and action movies to male users, despite similar Precision@10 metrics. }
    \label{fig:compare_genre}
    % \vspace{-0.5cm}
\end{figure}

% the Recall@50 values for males and females can give a false sense of fairness when there is a clear bias in the recommended movies. 
% As an example, for the Item KNN model \cite{10.1145/352871.352887}, the Recall@50 for female and male populations are very close, meaning the model can be considered "fair". 
% However, there is a noticeable difference in the number of movies recommended to male and female users for the different genres, reflecting the stereotype males tend to like action movies whereas women like romance ones.

% includes balancing rating errors in different groups of users \cite{zhu18,NIPS2017_e6384711}
In this paper, our focus is to overcome the limitations of current fairness assessment metrics by designing a set of evaluation measures to help us quantify gender bias in recommendation models. 
Specifically, our aim is to ensure that they are diverse so as to capture nuances of the bias that might not be immediately obvious but are significant.
We propose to do this by incorporating item categories and rank (when relevant). 
Next, we utilize one of these metrics as part of the loss function to optimize, which results in an effective increase of fairness for recommendations made to users for each category. 
This not only shows that our metrics are effective in addressing fairness concerns but also proves their utility for quantifying bias. Additionally, we use our metrics to evaluate a variety of recommendation models, including fairness-aware ones, using three real-world datasets. The contribution of this paper can be summarized as follows:
% \vspace{-1pt}
\begin{itemize}[left=3pt]
    \item We introduce a set of metrics that considers both categories and rankings of the recommendations made, in turn providing a more nuanced evaluation.
    \item We evaluate different recommendation algorithms to check for gender bias using the proposed set of metrics.
    \item We employ an in-processing technique to ensure the recommended items are fair, not only in a general sense but in a category-aware sense.
\end{itemize}
\section{Related work}
% \input{samples/section/Background}
We categorize and discuss the related work existing in this field in the subsections below.
% \vspace{-8pt}
\subsection{Gender Fairness in RS}
Gender bias in recommendation systems can exist as systematic discrepancies in the algorithms when recommending items to users of different genders. The challenges of gender bias can have a wide-ranging set of implications, including imbalanced representations of items for different genders, stereotypical recommendations (male-dominated occupations recommended to males more than females), and limitations in user personalization (being recommended items that are not related to users' preferences just because they are male or female). Research on evaluating and addressing gender bias in recommendation systems is an ongoing process. Melchiorre et al. \cite{Melchiorre21} investigate the impact of a common de-biasing strategy called resampling on RS algorithms. This strategy marginally decreases gender bias, with a slight decrease in performance. The authors in \cite{pmlr-v139-gorantla21a,10.1145/3132847.3132938,XIA2019104857} introduce ways to re-rank items to offer a balanced solution that caters to group fairness (for gender and other sensitive attributes) and user preferences.
Historical data that can contain stereotypical movie preferences can intensify certain biases further when used to train traditional recommendation models. For instance, male users display a bias towards \textit{action} movies, which is amplified by recommendation algorithms like UserKNN \cite{TsintzouPT19}. The authors in \cite{Acharyya_Das_Chattoraj_Tanveer_2020}
introduce a framework that fairly predicts the quality of TedTalk speeches by using causal models and counterfactuals to mitigate gender and racial bias. Adversarial fairness where the recommender system is trained to not only make accurate predictions but also make it difficult for the adversary to guess the sensitive attribute, has also been employed to make such systems more fair \cite{LIU2022108058,rus2022closinggenderwagegap}. Recent advancements in this field, have led to the development of fairness-aware recommendation models, with special emphasis on gender bias \cite{yang2020causalintersectionalityfairranking, boratto2022consumer,10.1145/3397271.3401177,zhu18,wei2022comprehensive,10.1145/3442381.3450015}.
\subsection{Evaluating Gender Bias}
\label{subsec:fair_notion}
For evaluating gender bias, the most common fairness definitions employed are the concepts of \textit{Demographic Parity} and \textit{Equal Opportunity}, which are both related to group fairness. Fairness in this context, pertains to equitable treatment across different groups (which can be measured in classification and recommendation tasks).
For group fairness, the idea is to ensure that the predicted outcomes  \(\hat{Y}\) of a model should not be dependent on sensitive attributes like gender $S$. 
For demographic parity, the proportions of each sensitive group (like male and female) receiving positive predictions should be equal. 
For binary classification, demographic parity can be formalized as: 
\[P(\hat{Y}=1 | S=1) = P(\hat{Y}=1 | S=0)\] 
Essentially what this implies is that the positive outcome should be the same for both genders, where 0 may represent male and 1 may represent female or vice versa. 
Equal Opportunity, on the other hand, holds when the model has equal true positive rates across different demographic groups \cite{hardt16}. 
This concept can be formalized as:
\[P(\hat{Y}=1 | S=1, Y=1) = P(\hat{Y}=1 | S=0, Y=1) \]
where \(Y\) represents the true outcome.

Besides these two methods to quantify fairness, some additional concepts (as discussed in \cite{kheya2024pursuitfairnessartificialintelligence}) used include Equalized Odds \cite{hardt16}, Balance for Negative Class \cite{10.1145/3194770.3194776}, Balance for Positive Class \cite{10.1145/3194770.3194776}, Intersectional Fairness \cite{Gohar2023ASO}, Equal Calibration \cite{Chouldechova2016FairPW}
% Predictive parity \cite{10.1145/3194770.3194776}
and Causal-based notions \cite{10.5555/3294996.3295162, Acharyya_Das_Chattoraj_Tanveer_2020}. 



\subsection{Evaluating Consumer-Side Fairness}
 In recommendation systems, fairness can be seen as a multi-sided concept and categorized into three groups: consumers (C-fairness) \cite{{deldjoo_flexible_2021,Melchiorre21,wu20,pmlr-v139-gorantla21a,ghosh21,Edizel20,wan20,hao21}} which is related to the impact of recommendations of the system on protected classes of user, provider (P-fairness) which focuses on ensuring fairness for providers/sellers on a platform and both (CP-fairness) \cite{burke_multisided_2017}. Our work focuses on the consumer-side fairness concept because we want to ensure that recommendations made by models are not biased against a certain gender. Prior research has shown how recommendations can differ in an unfair way based on sensitive attributes of users like gender, age, race, etc. \cite{gupta_questioning_2022,10.1145/3547333,JIN2023101906}. Evaluating bias in these systems, before deploying is thus essential to stop the reinforcement of stereotypes and limiting diverse content. When quantifying consumer side bias in recommendation systems, the most common approach is to adopt the concept of equality of opportunity and focus on the differences in metrics such as recall, precision and/or NDCG \cite{Fu20,10.1145/3442381.3449866,Melchiorre21}. Other papers also employ causal-based fairness notions \cite{wei2022comprehensive,10.1145/3404835.3462943} and demographic parity \cite{10.1145/3292500.3330691,DBLP:journals/corr/abs-1809-09030,boratto2022consumer}. Unlike these metrics, which assume fairness implies equality, \cite{deldjoo_flexible_2021} suggests how, for instance, paid users should be provided better recommendations when compared to free users. They design a set of metrics that can take this disparity into account and then measure fairness accordingly. Another interesting way to measure unfairness is the concept of envy-free fairness, which is achieved when no one user prefers another user's recommendations way more significantly than their own \cite{do2022online}. While these metrics can identify consumer-side bias to an extent, they come with some limitations.

 
Limitations of current metrics used to quantify consumer-side bias in recommendation systems include: (i) over-simplifying the meaning of fairness in RS, which employs various techniques like collaborative and content-based filtering and hybrid methods.
Simple notions can fail to capture disparities that exist across different types of items, for example, genres, when considering movie recommendations. Some metrics that can fall prey to this oversimplification issue include \cite{Weydemann19,NIPS2017_e6384711, Fu20,10.1145/3442381.3449866,islam21,foulds2019bayesianmodelingintersectionalfairness,9101635,do2022online,10.1145/3534678.3539269,Melchiorre21,deldjoo_flexible_2021};
(ii) not utilizing ranks when evaluating recommendation quality can yield considerable issues. 
This is due to the fact that recommendations are displayed one after another, so the items on higher ranks must be more relevant to keep the user satisfied. 
Thus, capturing the quality of recommendations using the ranks reflects a more complete way of evaluating models. 
Some metrics that can fall prey to this issue include \cite{NIPS2017_e6384711,Weydemann19,do2022online,10.1145/3534678.3539269,deldjoo_flexible_2021}.
It is important to state however, that even considering rank can give rise to positional bias, which refers to the tendency of users to favor the items that appear on top of a ranked list. 
Evaluating till a certain position like Recall@k can lead to a skewed sense of assessment of how the recommendation model performs. 
Additionally, metrics like MAP (Mean Average Precision), which normally treats relevance as a binary value (0: not relevant and 1: relevant) can also fail to capture the nuanced relevance that can come from items having multiple categories. 
So, using more than one metric (both with and without using ranks) to quantify bias is essential; 
(iii) relying only on one type of fairness metric can obscure underlying biases and give a false impression of fairness in recommendation systems. 

Hence, using multiple metrics can help uncover hidden biases. Additionally, a model that is fair according to one metric can fail to hold other fairness metrics and risk overlooking subtle unfairness issues.



We want to highlight some of the works that have taken into account different classes when evaluating recommendations \cite{lin2019crankvolumepreferencebias,10.1145/3240323.3240372,10.1145/3292500.3330691,10.1145/3626772.3657794,10068703,10.1145/3308560.3317595,10.1145/3038912.3052612,10.1145/3589334.3648158}.
For instance, \cite{lin2019crankvolumepreferencebias} groups users on certain attributes and items by category, then measures preference ratio, which is the fraction of liked items by a group across categories. Next, they measure the bias disparity by taking the preference and recommended ratios' relative differences. This is close to our work but still doesn't account for the ranks of items and we evaluate the direct comparison of the recommendations for males and females. Additionally, \cite{10.1145/3240323.3240372} introduce calibrated recommendations, ensuring the recommended items align with user preferences without overemphasizing particular categories. The work by \cite{10.1145/3292500.3330691} uses a measure \textit{Skew@k} to evaluate proportions of candidates based on sensitive attributes, and \cite{10.1145/3626772.3657794} uses a fairness metric called Attention Weighted Ranked Fairness (AWRF) \cite{10.1145/3308560.3317595} to ensure there is balance in exposure in different groups of providers. While both these works ensure group fairness, our work is more concentrated on evaluating the distribution of content categories for different groups. Unlike the work by \cite{10.1145/3626772.3657794} that focuses on provider-side fairness, we focus on consumer-side fairness. 
\section{Proposed Evaluation Metrics}

\subsection{Motivating Fairness Concern}
\label{subsec:motivating_example}

Our example is a typical offline setting recommendation system, which is trained using historical user and movie interactions. 
For our scenario, let us assume we have $u_1$, a male user who has watched numerous action and sci-fi movies, with some romance movies. 
We also have $u_2$, a female user who has watched a lot of drama movies but also a few action movies. 
Let us say we decide to calculate overall precision for each user group (male and female) and then compare them to ensure the model's fairness. 

\subsubsection{Potential Issue}
Machine learning systems tend to learn and amplify bias from the training data \cite{10.5555/3157382.3157584,lum2016,pmlr-v81-ensign18a,10.1145/3287560.3287572,kheya2024pursuitfairnessartificialintelligence}. Recommender systems are no different, as they can pick up on stereotypical user-item interactions and make biased recommendations based on sensitive attributes of users \cite{TsintzouPT19,Melchiorre21}. Biased recommendations can have a broad impact on users if the models recommend content just because it aligns with certain stereotypes, for instance, males like action movies, and females like romance movies. 

This imbalance in recommendation can lead to a situation in which users are only exposed to items that align with part of their preferences and stereotypical norms. This would potentially filter out diverse content and prevent users from discovering new movies. As time passes, $u_1$ might stop getting romance movies recommended to them, even though they enjoy them. 
For $u_2$, a similar case could arise for action movies. 
Essentially, this imbalance can trap the users in a bubble of recommendations with only their established preferences and gender-stereotypical genres. Additionally, if a user's established preferences are already aligned with gender stereotypes, then the bias in recommendation will intensify further giving rise to a filter bubble, with very redundant movie recommendations.

\subsubsection{Falling short when quantifying gender bias}
Moreover, as mentioned earlier, using some performance metrics for both genders and comparing them to evaluate fairness is not a great idea. 
The two groups can have similar scores, even if the model is making biased recommendations by choosing to neglect certain categories for certain users. 
Our proposed metrics address this issue by breaking down the recommendations by category to get a more nuanced sense of fairness.

The key takeaway here is the importance of learning fair user and item representations, as well as evaluating fairness in the output. 
Even if a model is trained only on user-item-rating interactions with no explicit mention of sensitive attributes, the model can still infer this private information due to the correlation between their behavior and their sensitive attributes \cite{bothmann2024fairnessroleprotectedattributes,10.1145/3106237.3106277,10.1145/3368089.3409697}. 
So, we have to take precautionary measures when training the model itself, so it is unable to learn these correlations. 
We also wanna discuss how it is important to ensure personalization, including gender-specific preferences to enhance user satisfaction, but such preferences should be balanced against any risk of reinforcing stereotypes. 

Please note in our study we focus on binary genders, acknowledging there are many other gender identities not represented here.

\subsection{Notation}


We present all metrics for fairness assessment using the following mathematical notation:
\begin{itemize}
    \item $u_i$: A single user, where $i$ indexes the users.
    \item $v_j$: A single item, where $j$ indexes the items.
    \item $\mathcal{U}$ and $\mathcal{V}$: The set of users and items, respectively.
    \item $\mathcal{U}_m$ and $\mathcal{U}_f$: The set of male and female users, respectively.
    \item $c$: An item category, such as Action, Sci-Fi, Romance, etc.
    \item $C$: A category matrix where $C_{j,c} = 1$ if category $c$ is associated with item $v_j$, and $C_{j,c} = 0$ otherwise.
    \item $C_{v_j}$: The list of categories associated with item $v_j$.
    \item $TopK_{u_i}$: The set of top $K$ recommended items for user $u_i$.
    \item $\mathrm{C}$: Represents the set of categories for items.
\end{itemize}


To assess fairness, we adapt and extend Information Retrieval metrics, introducing both non-ranking and ranking-based metrics, which are detailed in the following subsections.

\subsection{Non-ranking-based metrics}
The first set of metrics we propose evaluates the fairness of a recommender system without considering the ranking of movies. 
\subsubsection{Category Coverage (CC)} 
\label{subsec:gp}

This metric estimates the proportion of recommended items associated with category $c$ relative to all categories for all users $u_i$. 
This essentially captures the category-specific performance of the recommended items. 
It is defined as follows:

\begin{equation}
CC(c, \mathcal{U}) = \frac{1}{|\mathcal{U}|} \sum_{u_i \in \mathcal{U}} \frac{1}{|TopK_{u_i}|} \sum_{v_j \in TopK_{u_i}} \frac{C_{j,c}}{|C_{v_j}|}
\label{eq:M1}
\end{equation}




\subsubsection{Relative Category Representation (RCR)}
This metric estimates the proportion of category $c$ in recommended items relative to the proportions of all categories available from the whole dataset. 
% Here $I$ represents all the movies available. 
This helps provide insights on category-specific items and is defined as follows: 
% For example, if $M_2(action, U)$ is higher than $M_2(romance,U)$ then the recommender system is good at recommending movies that have action as a genre to the set of users U. 

% \begin{equation}
% M_2(g,U) = \frac{1}{|U|}\sum_{u \in U} \frac{\sum_{m \in M_u}\frac{Gmg}{|G_m|}} {\sum_{m_i \in I} \frac{G_{m_i,g}}{|G_{m_i}|}
% }
% \label{eq:M2}
%  \end{equation}
\begin{equation}
RCR(c, \mathcal{U}) = \frac{1}{|\mathcal{U}|} \sum_{u_i \in \mathcal{U}} \frac{\sum_{v_j \in TopK_{u_i}} \frac{C_{j,c}}{|C_{v_j}|}}{\sum_{v_k \in \mathcal{V}} \frac{C_{k,c}}{|C_{v_k}|}}
\label{eq:M2}
\end{equation}

These two metrics help us quantify how relevant a category is for items recommended to a given set of users. For instance, $CC$ would help us quantify the diversity of items recommended by reflecting the overall distribution of recommended content when computed for different $c$. Whereas, $RCR$ (calculated for different $c$) would measure if the recommendations suppress or amplify certain categories by comparing them to the actual distribution of available content.
Both $CC$ and $RCR$ provide a more nuanced understanding of how the recommendation system performs but don't take into account the position of the recommended items. 
For recommendation systems, where in most cases items surface one after another, it is vital that the items on the top of the menu are the most relevant. 
% This will ensure the user is kept engaged when they are viewing the recommended items. 
% \(M_1\) and \(M_2\) capture information about the performance of the recommendations, but don't take into account the relative orders of the items. 
Hence, to ensure that ranking order is also considered for evaluating recommended items, in the next section we introduce rank-based metrics.


\subsection{Rank-based Metrics}
\label{sec:rbm}
% In recommendation systems (for instance movie, music, news etc.) the order of items can significantly impact the user experience and satisfaction. 
% Therefore we ought to evaluate the quality of the recommended items by using the order or rank in which they appear. 
% In this section we introduce four new metrics to help us do just that. Note: In this section when we use notations like \(m_i\) or \(m_j\), they represents the movie in rank \(i\) and \(j\) respectively in \(M_u\).
 % The following sub-sections introduces four new metrics
We now introduce our metrics that utilize ranking to provide a comprehensive assessment of fairness.

\subsubsection{Category Mean Average Precision (CMAP)}
For a given set of users, this metric estimates the proportion of recommended items associated with category $c$ by incorporating the rank of the items. 
A category-specific average precision score is computed for each user, followed by averaging these scores across all users.
This metric is defined as follows:
% This approach ensures that higher ranked items will contribute more significantly to the metric. 
% For instance, if \(M_3(comedy, U)\) is higher than \(M_3(romance,U)\), then it means the recommender system is better at retrieving and ranking movies which has comedy as one of the genre compared to that of romance for the set of users U. 

% \begin{equation}
% \small CumulativeGenreAverage(M_u,g,i) = CGA(M_u,g,i) =   \frac{1}{i} \sum_{j=1}^{i} {\frac{G_{{m_jg}} }{|G_{m_j}|}}
% \label{eq:map}
%  \end{equation}

\begin{equation}
CMAP(c, \mathcal{U}) = \frac{1}{|\mathcal{U}|} \sum_{u_i \in \mathcal{U}}
% \frac{1}{|TopK_{u_i}|} 
\frac{\sum_{j=1}^{|TopK_{u_i}|}  P(j) \cdot \frac{C_{j,c}}{|C_{v_j}|}}{\sum_{v_j \in \mathcal{V}} \frac{C_{j,c}}{|C_{v_j}|}}
\label{eq:M3}
\end{equation}

% \begin{equation}
% GMAP(g, \mathcal{U}) = \frac{1}{|\mathcal{U}|} \sum_{u_i \in \mathcal{U}}
% \frac{1}{|TopK_{u_i}|} 
% \sum_{j=1}^{|TopK_{u_i}|}  P(j) \cdot \frac{G_{j,g}}{|G_{v_j}|}
% \label{eq:M3}
% \end{equation}

\noindent where $
P(j) = \frac{1}{j} \sum_{k=1}^{j} \frac{C_{k,c}}{|C_{v_k}|}
$.


\subsubsection{Category Discounted Cumulative Gain (CDCG)}
% This metric can help us evaluate the effectiveness of recommendations, particularly in terms of ranking. 
The score for this metric is discounted based on the position of the item in the recommended list. 
% As an example, let's say \(M_4(comedy, U)\) is higher than \(M_4(action,U)\), this then means the recommender system can retrieve and rank comedy movies more effectively when compared to action movies. 
% This also means that users from a set $\mathcal{U}$ need to go through items from other categories first to get to the ones that have action as one of the genres.
While similar to the previous metric, CDCG uses a logarithmic discount factor, which is higher for items that appear lower down in the list. It is defined as follows:

\begin{equation}
CDCG(c, \mathcal{U}) = \frac{1}{|\mathcal{U}|} \sum_{u_i \in \mathcal{U}} \frac{1}{|TopK_{u_i}|} \sum_{j=1}^{|TopK_{u_i}|} \frac{\frac{C_{j,c}}{|C_{v_j}|}}{\log(j + 1)}
\label{eq:M4}
\end{equation}

\subsubsection{Category Mean Reciprocal Rank (CMRR)}
% \(M_5(g,U)\) helps us quantify how effective the recommendations are by incorporating how the system ranks the items for each genre. 
This metric is based on MRR, which is essentially the mean reciprocal of the rank of the first relevant item. 
% But we have adapted that concept to find the items of a given genre using the proportion of this genre for a given movie and then dividing it by the rank of the item itself. This value is then summed up for all movies recommended to a user. If for instance \(M_5(action, U)\) is higher than \(M_5(romance,U)\), then users are able to find movies of genre action in higher ranks in their recommended movies than romance movies. This metric is very similar to GDCG but the discount factor is different. GDCG is more forgiving to the items at the end of the list because the discounting is done logarithmically, whereas this metric reduces the score linearly, making lower-ranked items contribute way less.
We adapt it for assessing fairness and we define it as follows:
\begin{equation}
CMRR(c, \mathcal{U}) = \frac{1}{|\mathcal{U}|} \sum_{u_i \in \mathcal{U}} \frac{1}{|TopK_{u_i}|} \sum_{j=1}^{|TopK_{u_i}|} \frac{\frac{C_{j,c}}{|C_{v_j}|}}{j}
\label{eq:M5}
\end{equation}


\subsubsection{Category RPrecision (CRP)}
% This metric that also considers rank when evaluating the quality of the recommendations. 
This metric uses the category-specific precision but only for the top \(\lfloor R_c \rfloor\) results, where $R_c$ represents the proportion of category $c$ when considering all items in $\mathcal{V}$ (i.e., $R_c=\sum_{j=1}^{|\mathcal{V}|} \frac{C_{j,c}}{|C_{v_j}|}$, note that item $v_j$ can belong to multiple categories like movie genres). 
% Here \(M_gr\) represents the top \(\lfloor R_g \rfloor\) recommended items. This metric is more effective than the other rank-based metrics discussed above since we are using \(R\) to balance the evaluation of the recommendation quality, which is suited for our case since we have varying numbers of movies for different genres. For example. if \(M_6(action, U)\) is higher than \(M_6(romance,U)\), then it indicates the system can retrieve and recommend action movies better than romance movies. However, this can also be the case if there are fewer action movies compared to romance. This means if there are a lot more romance movies, the system needs to retrieve and rank them and this might result in a lower GRP. 
This metric is defined as follows:
\begin{equation}
CRP(c, \mathcal{U}) = \frac{1}{|\mathcal{U}|} \sum_{u_i \in \mathcal{U}} \frac{\sum_{j=1}^{|Top\lfloor R_c \rfloor_{u_i}|} \frac{C_{j,c}}{|C_{v_j}|}}{\lfloor R_c \rfloor}
\label{eq:M6}
\end{equation}

% For instance, $CDCG$ and $CMRR$ will help measure the diversity of movies recommended in terms of categories, with different discounting factors for the rank of the items.  
All four of the ranking-based metrics, take into consideration the proportions of categories and use rank as a discounting factor. The discounting factor is different for each of them and has distinct nuances. For instance, $CRP$ would only evaluate recommendations till a certain rank \(\lfloor R_c \rfloor\) to measure if the model is providing a proportional amount of recommendations for $c$ relative to its presence in the dataset. Another metric like $CDCG$ for example would capture something different, which is the relevance of $c$ by giving more weight to items (falling under category $c$) appearing higher in the list.
% For instance, let us take a movie recommendation setting where there are a total of 10 movies to recommend from with overall proportion of action movies equal to 3. In this case, $CRP$ for action would only consider items in the top-3 list for all users, evaluating the items based on their relative presence in the dataset.This can help us grasp if a model is recommending a overwhelming amount of movies 
% $CDCG$ discounts relevance of items from $c$ by giving more weight to items appearing earlier in the list. In contrast, 



\subsection{Group Fairness (Gender)}
Gender Balance Score (GBS) for a given category $c$, is the absolute difference of category distribution values for $\mathcal{U}_m$ and $\mathcal{U}_f$ based on any of the category-aware metrics discussed above. 
% In our case of quantifying gender bias, \(a\) represents the female population and \(a'\) represents the male population respectively.
This metric is an adaptation of \textit{demographic parity} as mentioned in Section \ref{subsec:fair_notion}. 
We want to make sure that the category distributions of the recommended items for the groups, of males and females are similar. 
For each metric $\mathcal{M}$, defined above, we want to ensure that the difference of values calculated for the groups male and female summed up for all categories are close to 0. 
This can be formalized as:
% Achieving perfect demographic parity in practice, can be unrealistic as there are slight variations in predictions and this would also lead to poor utility. Our way of addressing this problem is to not be overly strict and allow for a threshold \(\epsilon\), within which the differences of the values for the groups is acceptable.
\begin{equation} \bigtriangleup
\mathcal{M}_c=| \mathcal{M}(c,\mathcal{U}_m) - \mathcal{M}(c,\mathcal{U}_f)|
\label{eq:delM}
 \end{equation}
\begin{equation} 
GBS(\mathcal{M}) =\sum_{c \in \mathrm{C}} \bigtriangleup
\mathcal{M}_c \approx 0
\label{eq:GBS}
 \end{equation}





\section{Genre Aware Regularization For Gender Fairness}
\label{sec:fairloss}

Now that we have introduced our set of metrics for capturing a nuanced sense of fairness for recommended items, we aim to learn fair representations that are category-aware for users of different genders.
To promote fairness in recommendation models we employ an in-processing regularization technique inspired by the approach first proposed by \cite{pmlr-v54-zafar17a}. 
We propose to incorporate Category Coverage (Section \ref{subsec:gp}) into the loss function alongside the primary recommendation loss, encouraging the model to optimize for category-aware fairness. We want to emphasize that while we select $CC$ as an example here, optimizing on any of the other metrics (as long as the implementation is differentiable) is possible.
We incorporate this fairness regularizer into the loss function of each baseline model—MF, VAE-CF, and NeuMF.

Specifically, to discourage the model from learning any biased representations, we incorporate the following regularizer:
\begin{equation}
\mathcal{L}_{FairGenreGender} = GBS(CC) = \sum_{c \in \mathrm{C}}|CC(c,\mathcal{U}_m)-CC(c,\mathcal{U}_f)|
 % = \sum_{g \in G} | \frac{1}{|U_a|} \sum_{u \in U}\frac{1}{|M_u|} \sum_{m\in M_u} \frac{G{mg}}{|G_m|} - \frac{1}{|U_a'|} \sum_{u \in U}\frac{1}{|M_u|} \sum_{m\in M_u} \frac{G{mg}}{|G_m|}|
 \label{eq:myloss}
\end{equation}
The goal is to minimize the difference in category distribution of the items being recommended to the users of each sensitive group. 
The new loss function for MF, VAE-CF and NeuCF models can be written as:
\begin{equation}
\small \mathcal{L} = \alpha \mathcal{L}_{FairGenreGender} + (1-\alpha) \mathcal{L}_{Recommendation}
\label{eq:fairequation}
\end{equation}

\noindent where \(\alpha\) is used to calibrate the trade-off between the model loss and the fairness term. \(\mathcal{L}_{Recommendation}\) represents the respective recommendation loss term of each model. By tuning this hyperparameter, we ensure that fairness does not overly compromise the recommender's ability to respect users' personal preferences (even if there are natural differences across liked categories for users of different genders). 
In order to modulate the gender loss value and to ensure it is able to make a significant contribution to the loss function, we pass it through a sigmoid function, centering at 0.5 and scaling it by 0.1. Additionally, the maximum batch loss value of the actual recommendation loss is used to scale the gender loss up to ensure both losses are on the same scale.
In the next section we show how using this fairness regularization helps the models minimize bias in recommendations provided to users of different genders for different categories.


\section{Experiments}
In the sub-sections below, we outline our experimental setup.
% \input{samples/section/Experiment}
\subsection{Datasets}
\begin{table}[t]
\caption{Description of datasets.}
% \vspace{-0.2cm}
\label{tbl:dataset}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ |c|c|c|c|c|c| } 
\hline
Dataset &  Users & Item & Interactions & Categories for Items\\
 \hline
ML 100K \cite{10.1145/2827872} & 943 &  1,349  & 99,287 & 18 \\
ML 1M \cite{10.1145/2827872} & 6,040   & 3,416  & 999,611 & 18 \\
Yelp \cite{mansoury2019biasdisparitycollaborativerecommendation} & 1,316   & 1,272  & 97,991 & 21\\
\hline
\end{tabular}
}
    % \vspace{-16pt}
\end{table}
% For the datasets used we use k-core filtering to remove users and items that can be considered ``sparse''. 
Our experiments are performed on three recommendation datasets summarized in Table~\ref{tbl:dataset}.
The data is split into training, validation, and test sets with a 70:10:20 ratio following user-based split scheme.
% Both datasets have 18 genres including \textit{action}, \textit{romance} etc. 
% We are using these datasets in an implicit feedback setting, where we work with binary data (e.g., interacted =1 and not interacted = 0).
% We include all genres for all calculations for our metrics.
It's important to highlight that items can fall under multiple categories. For clarity and ease of understanding, we focus on four representative categories—\textit{Action}, \textit{Romance}, \textit{Sci-Fi}, and \textit{Drama} from the MovieLens datasets and \textit{Coffee, Tea \& Desserts}, \textit{Arts \& Entertainment}, \textit{Travel \& Transportation} and \textit{Asian} from the yelp dataset — when visualizing our metric values; however all categories are included in our experiments.

% We are using the minimum interaction threshold of 5, meaning if a user has less than 5 interactions their interactions are filtered out. Similarly, in the case of items, they are filtered out if they don't have 5 or more interactions.

\subsection{Evaluating Performance}
 For measuring the performance of recommendations made, we use HitRatio@k calculated for each user and then averaged. Additionally, we use a ranking-based metric NDCG@k (Normalized Discounted Cumulative Gain) which gives higher relevance to items appearing higher in the ranked list. This too, is calculated for each user using their top \(k\) recommendations and then averaged. We calculate these values for \(k=50\).
% \[
% \text{Hitratio@k(u)} = 
% \begin{cases}
% 1, & \text{if }  \text{K} \cap \text{GT}  > 0 \\
% 0, & \text{otherwise}
% \end{cases}
% \]

% \[
% \text{Hitratio@k} = \frac{1}{N} \sum_{i=1}^{N} \text{Hitratio@k}(u_i)
% \]

% where GT is the ground truth set of items for a given user and K is the top \(k\) ranked items for that user.




\subsection{Base Recommendation Models}
For analyzing which models manifest gender bias we evaluate several recommendation approaches. When selecting these models, we include a variety of algorithms, ranging from traditional ones to more modern ones including Matrix Factorization, UserKNN, ItemKNN, NeuMF and VAE-CF (more detailed information on these and the fairness-aware models can be found in Section \ref{sec:baseline}).
% \vspace{-2pt}
% the classic collaborative approach of Matrix Factorization (MF), K-Nearest Neighbourhood algorithms UserKNN and ItemKNN, and two deep models including Neural MF (NeuMF) and Variational Auto-Encoder Collaborative Filtering (VAE-CF).

Additionally, we include a regularization-based fairness-aware model BeyondParity \cite{NIPS2017_e6384711} and a counterfactually fair recommendation model SM-GBiasedMF \cite{li2021towards}. To weigh the fairness-aware loss for SM-GBiasedMF, we use \(\lambda=20\) for all experiments since this value seems to work well as mentioned in \cite{li2021towards}. To make a fair comparison, we use BeyondParity's \(U_{val}\) (refer to Equation \ref{eq:bp}) as a regularization term in the same setting as shown in Equation~\ref{eq:fairequation}, with the same value of alpha we use for our MF model. We use an early stopping strategy for the three models being compared, monitoring NDCG@20 with a delta of 0.0005 and patience of 10 epochs with a maximum number of iterations of 50 and 100 for the 100K and 1M datasets, respectively.

We utilize the Cornac framework \cite{JMLR:v21:19-805}, with customizations to meet our requirements, including the implementations of differentiable ways to obtain top k category distribution for calculating our gender loss term for different models. When calculating our regularization term, we generate a top \(k\) recommendation list of size \(k=50\). Additionally, all our results for all metrics for GBS (except CRP) use the top 50 recommended items. For GBS(CRP) we use top \(\lfloor R_g \rfloor\) as explained for Equation \ref{eq:M6}.



% % \section{Dataset}
% % \input{samples/section/Dataset}
\section{Analysis of the proposed metrics}
In this section, we discuss the results we obtained from our experiments through a comprehensive analysis.
% \input{samples/section/Analysis}
\subsection{Baselines Comparison}
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|ll|ll|llllll|}
\hline
Model & &\begin{tabular}[c]{@{}l@{}}Hit\\ Ratio\end{tabular}$\uparrow$ & NDCG$\uparrow$ & CC$\downarrow$ & RCR$\downarrow$ & CMAP$\downarrow$ & CDCG$\downarrow$ & CMRR$\downarrow$ & CRP$\downarrow$ \\ 
\hline

\multicolumn{10}{|c|}{{\bf ML 100k Dataset}} \\
\hline

\multicolumn{2}{|l|}{UserKNN}               &  0.4931& 0.0321 & 0.0283     & 0.0340  &  0.0029& 0.0063 & 0.0017 & \multicolumn{1}{l|}{0.0178}       \\ \hline
\multicolumn{2}{|l|}{ItemKNN}          &     0.4698             & 0.0347 & 0.0675  & 0.0749  & 0.0095 & 0.0181 & 0.0070 &  \multicolumn{1}{l|}{0.0567}    \\ \hline
\multirow{2}{*}{MF}          & Original                                            &   \textbf{0.9003}  &         \textbf{0.1979}   &  0.0288    &    0.0321   &   0.0043  &  0.0089    & \textbf{ 0.0055}   & \multicolumn{1}{l|}{0.0276}       \\ \cline{3-10} 
                             & Fair                                                        & 0.8950  & 0.1794    &\textbf{ 0.0228}      & \textbf{0.0312 }     & \textbf{0.0037 }     &\textbf{ 0.0085}      & 0.0060     & \multicolumn{1}{l|}{\textbf{0.0234}}         \\ \hline
\multirow{2}{*}{VAE-CF}      & Original   &   0.8749             & \textbf{0.1643} & 0.0187  & 0.0210  & 0.0030 & 0.0052 & \textbf{0.0026} & \multicolumn{1}{l|}{0.0137}     \\ \cline{3-10} 
                             & Fair          & \textbf{0.8759}             & 0.1575 & \textbf{0.0140 } & \textbf{0.0186}  & \textbf{0.0026}& \textbf{0.0047} & 0.0037 & \multicolumn{1}{l|}{\textbf{0.0128 } }      \\ \hline
\multirow{2}{*}{NEU-MF}      & Original      & \textbf{0.9470}  & \textbf{0.2402} & 0.1723  & 0.1452  & 0.0268 & 0.0502 & 0.0220 & \multicolumn{1}{l|}{0.1411 }      \\ \cline{3-10} 
                             & Fair          & 0.9194             & 0.2305 & \textbf{0.0333  }& \textbf{0.0405}  & \textbf{0.0056} & \textbf{0.0093} & \textbf{0.0059} & \multicolumn{1}{l|}{\textbf{0.0342}  }     \\ \hline
\multicolumn{10}{|c|}{{\bf ML 1M Dataset}} \\ 
\hline
\multicolumn{2}{|l|}{UserKNN}                &   0.3997   &  0.0179  &  0.0204   &  0.0110&       0.0008 &0.0043&0.0008& \multicolumn{1}{l|}{0.0174}   \\ \hline
\multicolumn{2}{|l|}{ItemKNN}      &    0.3565   &0.0383      & 0.0881    & 0.0351 & 0.0051  &  0.0224 & 0.0082 & \multicolumn{1}{l|}{0.0650}    \\ \hline
\multirow{2}{*}{MF}          & Original      &\textbf{ 0.8485}                                               &   \textbf{0.1522}    &   0.1775   &      0.0486 &  0.0097   &  0.0495    &  0.0204   & \multicolumn{1}{l|}{0.1440}       \\ \cline{3-10} 
                             & Fair          &    0.8055  & 0.1158   &  \textbf{0.1160}   &   \textbf{0.0314 }  &    \textbf{0.0067}  &   \textbf{0.0344}  & \textbf{0.0162}   & \multicolumn{1}{l|}{\textbf{0.0760}}         \\ \hline
                             \multirow{2}{*}{VAE-CF}      & Original    &  \textbf{0.8315 }  & \textbf{0.1473} & 0.2551  & 0.0708  & 0.0154 & 0.0706 & 0.0282 &\multicolumn{1}{l|}{0.1900  }    \\ \cline{3-10} 
                             & Fair          & 0.8280 &  0.1423 &  \textbf{0.2229 } & \textbf{0.0637}  & \textbf{0.0132} & \textbf{0.0631} &  \textbf{0.0263}&\multicolumn{1}{l|}{\textbf{0.1502}}       \\ \hline
\multirow{2}{*}{NEU-MF}      & Original      & \textbf{ 0.9108 }   &  0.1336   &   0.3096  & 0.0998 &  0.0246  & 0.0835  & 0.0316 & \multicolumn{1}{l|}{0.2180  }   \\ \cline{3-10} 
                             & Fair   &          0.8333 & \textbf{0.1359}                                                       & \textbf{0.1503}     & \textbf{0.0415}     & \textbf{0.0086}      &\textbf{ 0.0436 }     & \textbf{0.0191 }     & \multicolumn{1}{l|}{\textbf{0.0959}}       \\ \hline
\multicolumn{10}{|c|}{{\bf Yelp Dataset}} \\ 
\hline

\multicolumn{2}{|l|}{UserKNN}                &   0.4886    &  0.0312    &  0.0068   & 0.0252 &   0.0068    &0.0026&0.0022& \multicolumn{1}{l|}{0.0129}   \\ \hline
\multicolumn{2}{|l|}{ItemKNN}      &    0.5030    &    0.0344   &   0.0242  & 0.0513 &   0.0060 & 0.0065  & 0.0035 & \multicolumn{1}{l|}{0.0219}    \\ \hline
\multirow{2}{*}{MF}          & Original      &     \textbf{0.8587 }                                          &   \textbf{0.1209 }  &   0.0358   &0.0342&0.0055&0.0088&0.0036& \multicolumn{1}{l|}{0.0296}       \\ \cline{3-10} 
                             & Fair          & 0.7500 &  0.0773  &  \textbf{0.0159 }  &\textbf{0.0312}&    \textbf{0.0019 } & \textbf{0.0042}&\textbf{0.0019}& \multicolumn{1}{l|}{\textbf{0.0130}}         \\ \hline
                             \multirow{2}{*}{VAE-CF}      & Original      &  \textbf{0.8131 }                                                  & \textbf{0.0985 }   & 0.0045       & 0.0041      & 0.0003      & 0.0007      & 0.0003      & \multicolumn{1}{l|}{0.0018}       \\ \cline{3-10} 
                             & Fair          &  0.8032                                                 & 0.0961   &   \textbf{0.0021}   & \textbf{0.0019}      & \textbf{0.0002 }      &  \textbf{0.0004}    &   \textbf{0.0001}     & \multicolumn{1}{l|}{ \textbf{0.0011}}       \\ \hline
\multirow{2}{*}{NEU-MF}      & Original      &  \textbf{0.9347 }                                                & \textbf{0.1794}     & 0.0592       & 0.0850      & 0.0134    & 0.0152      & 0.0057    & \multicolumn{1}{l|}{0.0651}       \\ \cline{3-10} 
                             & Fair          &  0.8533  &  0.1124    & \textbf{0.0309 }      & \textbf{ 0.0401 }    & \textbf{0.0054 }    & \textbf{0.0079}      &  \textbf{0.0028}      & \multicolumn{1}{l|}{ \textbf{0.0394}}       \\ \hline
\end{tabular}
}
\caption{Performance and Bias Evaluation values (GBSs) for 5 baseline models, along with the fair models. }
    \label{tab:baselines}
    % \vspace{-24pt}
\end{table}

\begin{figure*}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/all_models_plain_all.pdf}
    \caption{ Comparison of Bias values for six of our metrics for all three dataset.}
    \label{fig:baseline_100k}
    % \vspace{-8pt}
\end{figure*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1\textwidth]{images/all_models_plain.pdf}
%     \caption{ Comparison of Bias values for six of our metrics. We find significant gender bias in these four stereotypical genres for our baseline models for the ML100K dataset.}
%     \label{fig:baseline_100k}
% \end{figure*}
% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1\textwidth]{images/all_models_plain_1m.pdf}
%     \caption{ Comparison of Bias values for six of our metrics for the ML1M dataset. There is significant gender bias, especially for VAE-CF and NeuMF models.}
%     \label{fig:baseline_1M}
% \end{figure*}
% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1\textwidth]{images/all_models_plain_yelp.pdf}
%     \caption{ Comparison of Bias values for six of our metrics for the Yelp100k dataset. With significant gender bias manifesting especially in the MF-based models.}
%     \label{fig:baseline_yelp}
% \end{figure*}
\subsubsection{Bias Evaluation}
We report our results in Figures \ref{fig:baseline_100k}, \ref{fig:models_reg_100k}, and Table \ref{tab:baselines}. 
For the KNN-based models, ItemKNN seems more bias-prone. This can be due to the nature of the model of calculating similarity between items based on the interactions by the users. For
instance, in the ML-100k dataset, the average rating for romance
movies is higher for female users than for males. This kind of imbalance can reinforce category-related gender bias when items are recommended. 
% This model is more biased for categories that are more interacted with. For instance, in the ML-100k dataset, \textit{Action} movies are more popular for both male and female groups when compared to \textit{Romance} ones. Consequently, the ItemKNN model is more biased towards recommending \textbf{Action} movies over \textit{Romance}. 
 Among the three other baselines, the MF-based models can be considered more biased. For the MF model, explicit embeddings are used to store user and item representations, which are likely to capture correlations of user behavior (e.g., liking certain genres) and their sensitive attributes. For VAE-CF, variational autoencoders are used to learn probabilistic latent representations of users, which are still sensitive to capturing biased representations but not as much as NeuMF. NeuMF captures both linear and non-linear relationships between users and items through Generalized Matrix Factorization (GMF) and Multi-Layer Perceptrons (MLP). So, NeuMF can capture biases in the data, especially the intricate patterns about user preferences, which can reflect social stereotypes. The MF model itself is not as complex as NeuMF, so our results of NeuMF being more biased than MF is reasonable.
   %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/all_models_reg.pdf}
    \caption{Reduction in bias scores after using our fairness-aware regularizer. Since all datasets provide similar outcomes, we present the results for only the ML 100K dataset. }
    \label{fig:models_reg_100k}
    % \vspace{-5pt}
\end{figure}
 %%%%%%%%%%%%%%%%%%%%%%%%%
 For the two smaller datasets, VAE-CF doesn’t manifest high values for bias, which can be due to the model not being able to fully capture representations of gendered preferences because of the lower number of interactions available. Comparatively, for the 1M dataset, the model can learn the stereotypical patterns available in the dataset more effectively and reinforce these biases in the latent representations. In general, all models manifest more gender bias for the larger dataset than the smaller ones.
 For all baseline models, the Gender Balance Scores are \(\not \approx 0\), which highlights the fact that they are producing recommendations in a discriminative way where they choose certain genres to be relevant for certain groups of users. From Table \ref{tab:baselines}, GBSs are higher for \(CC\) or \(RCR\) (ref to Equations: \ref{eq:M1} and \ref{eq:M2}). These two are classification-based metrics and do not consider ranks, so there is no discounting done in the value of any item that appears lower in the list but is still relevant (category-wise). GBS(CMRR) has the lowest values for almost all models since it has a strong discounting factor for items that appear lower in rank. 
 % Initially, we posited the models would be more biased towards the two most stereotypical genres: \textit{action} and \textit{romance}. However as observed from \ref{fig:baseline_100k}, although there is significant bias in recommending \textit{romance} movies, bias in sci-fi movie recommendations overpower those from \textit{action}. This bias can be driven by popularity, for instance for the ML-100k dataset there are more action movies than \textit{sci-fi} but the average rating for \textit{sci-fi} movies is higher than that of \textit{action} movies, making them more popular. As discussed by \cite{10.1145/3383313.3418487}, recommenders sometimes focus on popular movies and not really popular genres. Thus, choosing to recommend items that have higher ratings for example, which might fall under certain genres for our comparison, would be sci-fi.


The bias reduction after using the fairness aware loss term is displayed in Figure \ref{fig:models_reg_100k}. We can see a notable improvement in the bias scores, especially for the NeuMF model. The regularizer term is most effective for reducing bias in the NeuMF model, with an average decrease across all six metrics of 77\%, 53\%, and 50\% for ML100K, ML1M, and Yelp datasets, respectively. The impact of the fairness regularizer is less pronounced for the VAE-CF model. We believe this is due to the model's nature of learning probabilistic latent factors for users and items that don't amplify gender bias like the MF models which use embeddings to learn user and item interactions. As a result, the regularizer has less impact on the model's learning process.


\subsubsection{Performance Evaluation}\label{sec:perform-evaluation}
The performance drop for VAE-CF and NeuMF is less than that of the MF model, however, it all depends on the selection of $\alpha$ (more on this in Section \ref{sec:abalation}). For all models, when choosing $\alpha$ we ensure there is substantial bias reduction without too significant of a drop in performance. 
In some cases the fair models outperform the original models, this although seems counter-intuitive, but is presumably due to the fairness term acting as a regularization term which can help reduce over-fitting in the model to some extent (as noted in \cite{keya2020equitableallocationhealthcareresources,10.1145/3442381.3449904}). 


\subsection{Fairness-Aware Baselines}
\begin{table}[h]
\centering

\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|cc|cccccc|}
\hline
    %\toprule
Model & NDCG$\uparrow$ & HitRatio$\uparrow$ & CC$\downarrow$  & RCR$\downarrow$  & CMAP$\downarrow$  & CDCG$\downarrow$  & CMRR$\downarrow$  & CRP$\downarrow$  \\
\hline
    %\midrule
\multicolumn{9}{|c|}{{\bf ML 100K Dataset}} \\
\hline
MF$_{a=0.5}$ & \textbf{0.1794} &    \textbf{0.8950} & \textbf{0.0228} &  \textbf{0.0312} & \textbf{ 0.0037} & \textbf{ 0.0085} &  0.0060 & \textbf{ 0.0234 }\\

SM-GBiasedMF   & 0.1230 & 0.8537 & 0.03785 & 0.0378& 0.0059  & 0.0114 & \textbf{0.0053} &   0.0286    \\
BeyondParity     & 0.0790&  0.6384& 0.0528&0.0752&0.0093 & 0.0147 &  0.0068&  0.0480 \\
\hline
\multicolumn{9}{|c|}{{\bf ML 1M Dataset}} \\
\hline
MF$_{a=0.6}$ & \textbf{0.1158 }  & 0.8055  &\textbf{0.1160} &\textbf{0.0314 }&  \textbf{0.0067}&\textbf{0.0344} &0.0162 &\textbf{0.0760}\\
SM-GBiasedMF    &0.0846   &  \textbf{0.8075} &0.2590 &0.0726 & 0.0152 & 0.0656& 0.0211 & 0.2058\\
BeyondParity   &  0.0829   &     0.6495   & 0.1307&0.0539 & 0.0079 & 0.0369&\textbf{0.0154}&0.0838\\

\hline
\multicolumn{9}{|c|}{{\bf Yelp Dataset}} \\
\hline
MF$_{a=0.3}$ & 0.0773&0.7500  &  0.0159   &\textbf{0.0312}&    \textbf{0.0019}  & 0.0042&\textbf{0.0019} &\textbf{0.0130} \\

SM-GBiasedMF    & \textbf{0.9157}  & \textbf{ 0.1343} &0.0515 &0.0789&0.0132&0.0140 & 0.0055 & 0.0587 \\
BeyondParity   & 0.0358    & 0.4901  & \textbf{0.0112}& 0.0342& 0.0066 & \textbf{0.0033}&0.0027 & 0.0241\\


\hline
\end{tabular}
}
\caption{Performance and Bias Evaluation Metric values for fairness-aware models.}
\label{tab:fairbaselines}
    % \vspace{-20pt}

\end{table}

We can observe the results for our MF fair model when compared with two other fairness-aware models in Table \ref{tab:fairbaselines} and Figure \ref{fig:compare_genre_fair}. We chose to use our MF fair model since the other two fairness-aware models use MF as their foundational models. Our model has outperformed the other models in terms of bias scores across the majority of the comparisons. While our models don't excel in terms of performance, they still offer a better balance between performance and bias scores when compared with the other fairness-aware baselines. It is important to highlight that BeyondParity uses plain MSE loss, while the other two use BPR loss with negative sampling, which explains the exceptionally low-performance values. While we emphasize the significance of comparing the aggregated differences for each metric $\mathcal{M}$ for all the categories, we also want to highlight the importance of evaluating bias values separately. For instance, although GBS(CMRR) of SM-GBiasedMF is lower for the ML100K dataset when compared to ours, there is still significant bias for movies recommended that belong to the genres like \textit{Romance} and \textit{Sci-Fi} as observed in Figure \ref{fig:compare_genre_fair}. This strengthens our idea of having a set of metrics to quantify bias in a nuanced way since the model seems "fair" when considering overall scores for all categories, but closer inspection reveals how it is biased against certain categories. It is worth mentioning that SM-GBiasedMF is more biased than the plain MF model for the Yelp dataset. Our theory is that since this model needs more epochs to satisfy the early stopping criterion (as mentioned before), it likely over-fits the dataset, in turn amplifying the biases present in it.

% \begin{table}[]
% \centering

% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|lllllllll|}
% \hline
% \multicolumn{1}{|l|}{Model} & \multicolumn{1}{l|}{HitRatio} & \multicolumn{1}{l|}{NDCG} & \multicolumn{1}{l|}{GBS(1)} & \multicolumn{1}{l|}{GBS(2)} & \multicolumn{1}{l|}{GBS(3)} & \multicolumn{1}{l|}{GBS(4)} & \multicolumn{1}{l|}{GBS(5)} & GBS(6) \\ \hline
% \multicolumn{9}{|l|}{ML 100K}    

% MF$_{a=0.4}$ & 0.0576  &   0.5090 & 0.0316 & 0.0520 & 0.0082 & 0.0087 & 0.0058 & 0.0245 \\
% SM-GBiasedMF   & 0.1235 & 0.8589 & 0.0382 & 0.0387 & 0.0093  & 0.0098 & 0.0040 &   0.0310     \\
% BeyondParity   & 0.0778  & 0.6055 & 0.0447 & 0.0681& 0.0100 & 0.0124 & 0.0067 & 0.0543\\
% \hline
% \multicolumn{9}{|l|}{ML 1M}                                                                                                                                                                                                                            \\ \hline
% \end{tabular}}
%     \label{tab:fairbaselines}

% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/all_models_fair_100k.pdf}
    \caption{ Bias score for the fairness-aware models over four stereotypical genres for the ML 100K dataset.}
    \label{fig:compare_genre_fair}
    % \vspace{-11pt}
\end{figure}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/all_models_fair_yelp.pdf}
%     \caption{ Comparison of our model with two other fairness-aware models over four stereotypical genres for all six of our metrics for Yelp 100K dataset.}
%     \label{fig:compare_genre_fair_yelp}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/all_models_reg1m.pdf}
%     \caption{  We observe a major reduction in the bias scores in
% the three models we used our fairness-aware loss term 1M dataset. }
%     \label{fig:compare_genre_fair_1m}
% \end{figure}

% \begin{center}
% \begin{tabular}{ |c|c|c|c|c|c|c| } 
% \hline
% model &  alpha & RMSE & AUC  & NDCG@50 & Precision@50  & Recall@50 \\
%  \hline
%  MF & 0 & 1.0792  & 0.5811 & 0.0900  &  0.0494  &0.1140  \\
% & 0.3 & 1.0793   & 0.5775 & 0.0867  &  0.0465 & 0.1094\\
%  & 0.5 & 1.0794  & 0.5720 & 0.0798  &  0.0414 & 0.0988 \\
%  & 0.7 & 1.0795  & 0.5599  & 0.0611   & 0.0298 &  0.0707 \\
%  & 0.8 & 1.0796 & 0.5480 & 0.0444 & 0.0216  & 0.0494\\
%  \hline
% \end{tabular}
% \end{center}

% Model & NDCG & HitRatio & Genre  & Genre  & Genre  & Genre  & Genre  & Genre  \\
%  &  &  & Precision &  Recall &  MAP &  DCG &  MRR & R Precision \\


% \begin{table*}

% \begin{center}
% \begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
% \hline
% Model  & Hit& NDCG  &GBS(1) &GBS(2)  & GBS(3) & GBS(4)  & GBS(5)  &GBS(6)  \\

%  \hline
%  \multicolumn{9}{c}{ ML 100K}\\
%  % MF & 1.0378 & 0.0433 & 0.0651 & 0.0619 & 0.5781 \\
%  UserKNN           & 0.4931             & 0.0321 & 0.0283  & 0.0340  & 0.0040 & 0.0063 & 0.0017 & 0.0178 \\
%  ItemKNN           & 0.4698             & 0.0347 & 0.0675  & 0.0748  & 0.0176 & 0.0181 & 0.0070 &  0.0567 \\
%  MF                & 0.6469             & 0.0887 & 0.0551  & 0.0862  & 0.0130 & 0.0158 & 0.0074 & 0.0568\\
%  MF\(_{a=0.4}\)    & 0.5090             & 0.0576 & 0.0316  & 0.0520  & 0.0082 & 0.0087 & 0.0058 & 0.0245\\
%  VAE-CF             & 0.8749             & 0.1643 & 0.0187  & 0.0210  & 0.0041 & 0.0052 & 0.0026 & 0.0137 \\
%  VAE-CF\(_{a=0.3}\) & 0.8759             & 0.1575 & 0.0140  & 0.0186  & 0.0031 & 0.0047 & 0.0037 & 0.0128\\
%  NeuMF             & \textbf{0.9470}  & \textbf{0.2402} & 0.1723  & 0.1452  & 0.0498 & 0.0502 & 0.0220 & 0.1411  \\
% NeuMF\(_{a=0.2}\)  & 0.9194             & 0.2305 & 0.0333  & 0.0405  & 0.0039 & 0.0093 & 0.0059 & 0.0342
%  \\
%  \multicolumn{9}{c}{ ML 1M}\\
%  VAE-CF             & 0.8315 & 0.1473 & 0.2551  & 0.0708  & 0.0771 & 0.0706 & 0.0282 &0.1900  \\
%  VAE-CF\(a=0.2\)    &  0.8280 &  0.1423 &  0.2229  & 0.0637  & 0.0660 & 0.0631 &  0.0263&0.1502  \\
 
%  %  MF & 0.9496  & 0.5751 &  0.6638   &  0.0942 &  0.0462  &  0.1156  \\
%  %   ItemKNN& 1.0739   & 0.5647 &   0.5313  &   0.0494   &     0.0305 & 0.0642 \\
%  % UserKNN & 1.0023  &  0.5773  &  0.4867  &  0.0351  &  0.0218 & 0.0572  \\
%  % VAE-CF &  2.6672 &  0.8691  & 0.9311  & 0.2573 &  0.1112 & 0.3596 \\
%  % LightGCN & 1.6708 &  0.8673 &  0.9618 & 0.2656 &  0.1147 & 0.3749  \\
%  \hline





% \end{tabular}
% \caption{Performance and Bias Evaluation Metric values for 5 baseline models, along with the three models with fairness aware regularization term incorporated wintin their loss functions}
%     \label{tab:baselines}
% \end{center}
% \end{table*}




% VAE-CF 100k a = 0.3 1m a=0.2 mf 100k a= 0.4 mf 1m=? neumf 100k a= 0.2 1m=?

% \section{Post-processing method to mitigate bias}
% % \input{samples/section/Re-ranking}
% \[p(\delta \mid u) = \frac{\sum_{i \in H} w_{u,i} \cdot p(\delta \mid i)}{\sum_{i \in H} w_{u,i}}\]
% \[q(\delta \mid u) = \frac{\sum_{i \in H} w_{r} \cdot p(\delta \mid i)}{\sum_{i \in H} w_{r}}\]
% \begin{itemize}
%     \item \(p(g|u)\) is the genre distribution of the set of items the user has interacted with in the past. For instance the movies they have watched.
%     \item \(q(g|u)\) is the genre distribution of the set of items the user is being recommended. For instance the movies ta recommended system is recommending a user.
% \end{itemize}
% For determining the optimal set of recommended items \(I*\), we employ the maximum marginal relevance \cite{10.1145/290941.291025}.
% \[I^* = \arg\max_{|I| = N} \{ (1 - \lambda) \cdot s(I) - \lambda \cdot \text{CKL}(p, q(I)) \}
% \]

% \subsection{Personalization}
% The idea is to tailor content based on user preferences. The aim is to recommend items to users that are suited to their taste and characteristics, in turn creating an engaging experience for the user.

% \subsection{Diversity}
% Diversity in recommendation systems describe the variety of the items that are recommended to users. If a user has watched about 80\% action movies and the 20\% drama movies, when a recommendation system is solely based on relevance or accuracy it would choose to recommend more action movies, and maybe a few drama movies. blah blah
% \subsection{Fairness}
% Fairness in recommendation systems, ensures that users are treated equitably. We want the system to create an inclusive user experience, and in no way treats them in way that reinforce bias and /or inequalities.
% \[I^* = \arg\max_{|I| = N} \{ (1 - \lambda) \cdot s(I) - \lambda \cdot \text{CKL}(p, q(I)) - \beta \cdot C_{KL}(q(I),p_u )\}
% \]



\section{Discussion and Conclusion}
% \input{samples/section/Discussion and Conclusion}
In this paper, we identify the underlying issues of current metrics for evaluating consumer-side bias in recommendation systems. To better quantify bias, specifically gender bias, in such models, we propose a set of metrics. These metrics help capture a nuanced sense of fairness on recommended items by considering categories of the items. Next, we demonstrate how introducing one of our metrics as a fairness loss term along with the recommendation loss helped minimize the unfairness manifested in models in terms of different categories of items recommended with minimal performance loss. Experiments on three real-world datasets using a variety of recommendation models, including fairness-aware models, show the effectiveness of our metrics in capturing bias. Additionally, after incorporating our loss term, the bias in the models was significantly reduced, with a favorable balance between fairness and accuracy. Our loss function is very flexible which allows for context-sensitive fairness. In domains such as job recommendation and housing, where fairness is vital, the functions can prioritize fairness. Conversely, for areas like movie recommendations it can balance personalization and fairness (ensuring user preferences are respected).
Our work aims to spread awareness about how simple metrics that are currently utilized to evaluate bias might be giving researchers a false sense of fairness, and a more refined approach like ours is required to address this issue. 
Our metrics are very versatile and can be easily adapted to measure provider-side fairness by utilizing proportions of item brands, for example, and can help quantify popularity bias in recommendations. Extending on this, the metrics can be used for measuring CP-fairness since we can measure bias from both the consumer side and provider side, by taking differences in values for different item providers for different demographic groups. Plus we plan to extend our metrics to consider sensitive attributes which are multi-valued, by using a pairwise difference scheme. 
% In the future, we would like to use these metrics in other domains including job or restaurant recommendations.


% \newpage

% \section{Acknowledgments}







%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This material is based upon work supported by the Air Force Office of Scientific Research under award number FA2386-23-1-4003.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{mybib}


%%
%% If your work has an appendix, this is the place to put it.
\appendix
\section{Appendix}

% \appendix
% \subse{Additional Information}
% \subsection{Additional Information}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Results for ML 1M}
% Since both datasets provide similar outcome, we present the results for the 1 million dataset here.
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/all_models_fair1M.pdf}
%     \caption{ Comparison of our model with two other fairness-aware models over four stereotypical genres for all six of our metrics for ML 1M dataset.}
%     \label{fig:compare_genre_fair_1m}
% \end{figure}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/all_models_reg1m.pdf}
%     \caption{  We observe a major reduction in the bias scores in
% the three models we used our fairness-aware loss term 1M dataset. }
%     \label{fig:compare_genre_fair_1m}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Abalation Study}\label{sec:abalation}
In this section, we discuss the influence of the hyper-parameter \(\alpha\) on recommendation performance and fairness. As mentioned in Section \ref{sec:fairloss}, \(\alpha\) can be used to control the strength of the fairness aware loss. Theoretically as \(\alpha\) increases we would expect a decrease in performance since the recommendation loss would have a decreased weight. We use \(\alpha\) values from 0 to 0.6 with increments of 0.1 for all three models. We did not include values above 0.6, because it does not make sense to overpower the recommendation loss since that is our main task. To verify the expected behavior of alpha on recommendation loss and bias we plot the metric we optimize on: GBS(CC) or \(\bigtriangleup Category Coverage\) and the NDCG values. As shown in Figure \ref{fig:100kabalation}, there is a general trend of the performance metric and the bias score dropping as \(\alpha\) increases. There are some fluctuations, where the bias increases for the first few values of alpha. Our intuition for this is that the fairness loss might not be enough to decrease the bias when \(\alpha\) is small. It can essentially end up disturbing the loss function itself, without explicitly decreasing bias. But as the value increases the bias scores drop lower which is expected. There is a slight increase in NDCG value for certain models, as mentioned in Section \ref{sec:perform-evaluation}, this can be the result of the fairness loss term acting as a regularization term that essentially prevents over-fitting in the model. We choose \(\alpha\) values by ensuring there is a decrease in bias, without significant loss in performance. For the ML 100K dataset, we choose 0.4, 0.3, and 0.2 respectively for MF, VAE-CF, and NeuMF models. For the ML 1M dataset, we choose 0.6,0.2, and 0.4 for MF, VAE-CF, and NeuMF models respectively. Lastly, for the Yelp dataset, we choose 0.3 for all models. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/alldsabalationh.pdf}
    \caption{Impact of \(\alpha\) on recommendation performance wrt NDCG@50 and the bias measure which is the difference of Category Coverage values for male and female. The experiments are performed for all three  datasets }
    \label{fig:100kabalation}
\end{figure}
\subsection{Details about Baseline Models}\label{sec:baseline}
This subsection delves into details about the models we worked on for our experiments.
\begin{itemize}[left=5pt]
    \item MF \cite{5197422}: A classical Matrix Factorization algorithm where users and items are represented as latent vectors with global bias. 
    % The rating is predicted as:
    % \[\hat{r}_{ij} = U^T_i V_j\] where \(U_i\) and \(V_j\) represent the latent factors of user $u_i$ and item $v_j$ respectively. 
    % A global bias term is also considered to adjust the ratings, to address the problem of certain items getting higher or lower ratings consistently, and some users rating items higher or lower on average.
    In our implementation, we use BPR \cite{10.5555/1795114.1795167} loss with negative sampling to enhance the performance of the model.
    \item UserKNN \cite{10.5555/2074094.2074100}:  This is a neighborhood-based method based on users, and items are recommended by discovering similar users based on the cosine similarity of their historical interactions. 
    \item ItemKNN \cite{10.1145/352871.352887}: Another neighborhood-based method that computes the similarity between items instead. 
    \item NeuMF \cite{10.1145/3038912.3052569}: NeuMF is a deep-learning based extension to MF, which combines the linearity of traditional MF models and the non-linearity of DNNs (Deep Neural Networks).
    \item VAE-CF \cite{10.1145/3178876.3186150}: This non-linear probabilistic model is based on the auto-encoder architecture and learns compressed information about data. The encoder helps map user interactions as a low-dimensional latent space, and the decoder decodes this information back to the high-dimensional vector which is used to make predictions.
\end{itemize}

The \(U_{val}\) regularization term for BeyondParity \cite{NIPS2017_e6384711} can be written as:
    % \vspace{-10pt}
\begin{equation}
    U_{\text{val}} = \frac{1}{n} \sum_{j=1}^{n} \left| \left( E_{g} [y]_{j} - E_{g} [r]_{j} \right) - \left( E_{\neg g} [y]_{j} - E_{\neg g} [r]_{j} \right) \right|
    % \vspace{-10pt}
    \label{eq:bp}
\end{equation}

where \( E_{g} [y]_{j} \) is the average predicted score for the \( j \)-th item from disadvantaged users, \( E_{\neg g} [y]_{j} \) is the average predicted score for the \( j \)-th item from advantaged users, \( E_{g} [r]_{j} \) is the average rating for the \( j \)-th item from disadvantaged users and \( E_{\neg g} [r]_{j} \) is the average rating for the \( j \)-th item from advantaged users. This fairness objective is optimized alongside the actual learning objective for a collaborative-based recommendation system. SM-GBiasedMF, is a fair recommendation model which achieves counterfactual fairness by utilizing adversarial learning. They combine recommendation loss and an adversarial loss and, the trade-off is controlled by \(\lambda\).
\subsection{Dataset Pre-processing}
For all of our datasets, we filter out inactive users. A user is considered inactive if they have less than 5 interactions. Additionally, we remove any user whose gender is not known. The Yelp dataset has over 300 categories. We reduce this set of categories to a condensed group of 21 broader categories, for better interpretability. These categories include \textit{Active Life \& Fitness}, \textit{Arts \& Entertainment}, \textit{Automotive, Bars \& Nightlife}, \textit{Coffee, Tea \& Desserts}, \textit{Drinks \& Spirits, Education \& Learning}, \textit{Event Services, Family \& Kids}, \textit{Food \& Restaurants}, \textit{Health \& Beauty}, \textit{Home \& Garden}, \textit{Miscellaneous}, \textit{Outdoor Activities}, \textit{Public Services \& Community}, \textit{Shopping \& Fashion},\textit{ Specialty Food \& Groceries}, \textit{Sports \& Recreation},\textit{ Technology \& Electronics}, \textit{Travel \& Transportation}, and \textit{Asian}.

\subsection{Process of generating Figure \ref{fig:compare_genre}}
This plot is generated using the ML-100K dataset. Once the models are done being trained we identify the users by their gender in the test set. To ensure a fair comparison we take the number of users in the smaller group, which was female in our case. We randomly chose the same number of male users from the test set. Once we have an equal number of male and female users, precision@10 values are calculated for each user and averaged by gender. To find the proportions of movies recommended, we generate the top 10 movies for each user. The proportion of each genre is computed using Equation \ref{eq:M1}.

% \subsection{Hyper-paramter selection}
% For the ML 100k dataset, we choose the best hyper-parameter values according to the optimization of both recommendation and fairness loss from epoch values of [20,50,64,100], batch size of [128,256], learning rate of [0.001,0.0001,0.0005] and user embedding size of [8,20,32]. For VAE-CF we choose from 3 auto-encoder structures: [40],[60,40], and [100,50] (please refer to Cornac\footnote{https://cornac.readthedocs.io/en/v2.2.2/} for more details). For NeuMF we chose layer structure from [32,16,8] and [64,32]. For the Yelp dataset, we use the best hyperparameter combinations selected using the ML 100K dataset since the datasets have similar interactions, users, and items.
% Similarly, for the 1M dataset, we choose the best hyper-parameter from epoch values of [100,120], batch size of [512,1024], learning rate of [0.001,0.0005], and user embedding size of [40,50,64]. For VAE-CF we choose [128,64] as the autoencoder structure and for NeuMF we chose layer structure of [128,64]. We use an early stopping strategy for the MF model, monitoring NDCG@20 with a delta of 0.0005 and patience of 10 epochs. The same early stopping strategy is used for SM-GBiasedMF to ensure a fair comparison. For deep models when experimenting on 1M, due to the computational expense, all combinations of these parameters were not explored, only a subset was used and the best one was selected.
% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
