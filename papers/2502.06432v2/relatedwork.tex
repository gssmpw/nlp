\section{Related Works}
\subsection{Self-Supervised Image Denoising}
Self-supervised image denoising methods have evolved primarily along two paths. The first path, exemplified by methods like noise2void (N2V)~\cite{krull2019noise2void}, employs blind spot to introduce invisible pixels within the central region of convolutional kernels, thereby circumventing the issue of identity mapping. Recent advancements such as AP-BSN~\cite{lee2022ap} extend blind spot networks by introducing a shuffling mechanism to disrupt the spatial continuity of noise in natural images. Additionally, some studies~\cite{wang2023lg} modify the blind spot areas within convolutional kernels. The second path, as Fig. \ref{fig:contrast} shows, 
represented by noise2noise (N2N)~\cite{lehtinen2018noise2noise}, positing that training with L2 loss tends to converge towards the mean of observed values. This suggests the feasibility of replacing desired training targets with distributions having similar means. Subsequent endeavors~\cite{huang2021neighbor2neighbor,mansour2023zero,li2023spatial} have been dedicated to self-supervised training by downsampling inputs and targets from single noisy images. 
%\vspace{-2.5mm}
\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{pdf/pipe.png}
  %\vspace{-5mm}
  \caption{The primary denoising pipeline of Prompt-SID. (a) This method acquires the sub-images for network training through a spatial redundancy sampling strategy. These inputs are denoised using SPIformer, while the original image's structural representation is obtained as a prompt through RG-Diff. Each Transformer block incorporates a SAM to facilitate feature fusion. (b) During inference, Prompt-SID exclusively employs the original scale image through SPIformer and the RG-Diff branch.}
  \label{fig:pipe}
%\vspace{-1.5mm}
\end{figure*}
\subsection{Diffusion Model in Low-Level Tasks}

Diffusion models~\cite{ho2020denoising,song2020denoising,kawar2022denoising} leverage parameterized Markov chains to optimize the lower variational bound on the likelihood function, thereby enabling them to generate more precise target distributions compared to other generative models such as GANs. Over recent years, they have garnered significant attention in image restoration tasks, including super-resolution~\cite{li2022srdiff,lin2023diffbir}, enhancement~\cite{wang2024zero}, inpainting~\cite{xia2023diffir}, and so forth. 
These approaches often fine-tune a pre-trained stable diffusion model and directly decode the generated latent space representations to obtain outputs. However, these generation methods inevitably introduce a degree of randomness, resulting in subtle semantic deviations at the image level due to sampling Gaussian noise. Additionally, it fails to meet the requirements for lightweight deployment.
%\vspace{-2.5mm}
%\vspace{-2.5mm}