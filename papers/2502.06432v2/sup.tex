%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{xr}

\usepackage{bm}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{nopageno}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\newcommand{\argmin}{\operatornamewithlimits{arg \, min}}
\newcommand{\argmax}{\operatornamewithlimits{arg \, max}}
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\RequirePackage{float}  %设置图片浮动位置的宏包
\RequirePackage{subfigure}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Supplementary Materials:
Prompt-SID}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\section{Preliminaries}
\subsection{Noise2Noise Revisit} \label{subsec::n2n_revisit}

Image denoising represents a classic ill-posed problem within the domain of image restoration. This signifies the existence of multiple potential solutions for the same noisy scene. Previous image denoising models typically require paired input of noisy images $\mathbf{y}_i$ and corresponding clean images $\mathbf{x}_i$ to train the network effectively.
\begin{equation}
\argmin_{\theta}\sum_{i}^{}L(f_{\theta } (\mathbf{y}_i),\mathbf{x}_i)
\end{equation}
Here, $\theta$ represents parameters that need to be optimized. However, in practical scenarios, obtaining paired training images is often challenging or even impossible. As a result, a series of self-supervised and unsupervised methods have emerged that discard the clean images and utilize only noisy images for training.

The theoretical foundation of N2N\cite{lehtinen2018noise2noise} is rooted in point estimation, which estimates the true value of a series of observations \{$\mathbf{x}_1$, $\mathbf{x}_2$, ..., $\mathbf{x}_n$\}. The objective is to find a value $\mathbf{z}$ that minimizes the sum of distances to all the observed values, serving as the estimation. When using $\mathcal{L}_2$ loss for estimation, replacing $x$ with another observation $\mathbf{z}$ having the same mean value does not alter the result. 

Extending this theoretical point estimation framework to training neural network regressors, the optimization objective of the network can be transformed into:
%\vspace{-2.5mm}
\begin{equation}
\label{equ:n2n}
    \argmin_{\theta}\sum_{i}^{}L(f_{\theta } (\mathbf{y}_i),\mathbf{z})
%\vspace{-2.5mm}

\end{equation}
This implies that when training a denoising network, if we replace the clean images $\mathbf{x}_i$ with noisy images $\mathbf{z}$, which have zero-mean noise, the optimization results using L2 loss will be equivalent to those trained by pairs of noisy-clean images. This assumption forms the foundation of our work.
\subsection{Diffusion Revisit}
In this paper, we employed a diffusion model to recover the missing pixels during the sampling process and encoded them as structure embedding, which are further integrated into the backbone network. The diffusion model\cite{ho2020denoising} operates through two steps, namely the diffusion process and the reverse process. In the diffusion process, it transforms an image into Gaussian noise by applying the noise injection operation for $t$ time steps.Each result at every time step can be directly obtained from the initial state:
\begin{equation}
    \mathbf{q}(\mathbf{x}_t|\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_t;\sqrt{\bar{\alpha_t}\mathbf{x}_0};(1-\alpha_t)I)
\end{equation}
where $x_0$ is the input and $x_t$ is the noised image at time step $\mathbf{t}$. $\mathcal{N}$ represents the Gaussian distribution.$\alpha_t = 1- \beta_t$, $\bar{\alpha_t}=\prod_{i=0}^{t}a_i$. In the reverse process, the diffusion model progressively denoises the sampled Gaussian noise map until a high-quality result is obtained:
%\vspace{-2.5mm}
\begin{equation}
    \mathbf{p}(\mathbf{x}_{t-1} |\mathbf{x}_t, \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t-1};\mu_t(\mathbf{x}_t,\mathbf{x}_0),\sigma_t^2I)
\end{equation}
where $\mu_t(\mathbf{x}_t,\mathbf{x}_0)$ means $\frac{1}{\sqrt{\alpha _t} }(\mathbf{x}_t-\epsilon \frac{1-\alpha _t}{\sqrt{1-\bar{\alpha _t} } } ) $, and $\sigma_t^2$ means $\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_t$.$ \epsilon $ means the noise in the sampled image, and it is the only uncertain variable in the reverse process. To train the denoising network$ \epsilon_\theta(\mathbf{x}_t,t)$, the diffusion model performs random sampling of time steps to obtain Gaussian noise images and then undergoes the reverse process for denoising. The optimization process of network's paramenters $\theta$ is carried out as follows:
%\vspace{-2mm}
\begin{equation}
    \bigtriangledown_\theta \left \| \epsilon -\epsilon _\theta (\sqrt{\bar{\alpha _t} }\mathbf{x}_0+\epsilon \sqrt{1-\bar{\alpha _t} },\mathbf{t}  ) \right \| _2^2
\end{equation}
\subsection{Paired Images with Similar Ground Truths} \label{subsec::n2n_similar_gt}

Noise2Noise \cite{lehtinen2018noise2noise} significantly reduces the dependency on clean image datasets. However, acquiring multiple noisy observations of the same scene remains a highly challenging task. This difficulty arises because the ground truths corresponding to two distinct noisy observations are often not identical due to factors such as occlusion, motion, and variations in lighting conditions. To address these challenges, NBR2NBR \cite{huang2021neighbor2neighbor} introduces a method where $\mathbf{y}$ and $\mathbf{z}$ are treated as two independent noisy images conditioned on $\mathbf{x}$, under the assumption that there exists a $\bm{\varepsilon} \neq \mathbf{0}$ such that $\mathbb{E}_{\mathbf{y}|\mathbf{x}}(\mathbf{y})=\mathbf{x}$ and $\mathbb{E}_{\mathbf{z}|\mathbf{x}}(\mathbf{z}) = \mathbf{x}+\bm{\varepsilon}$. 
	Let the variance of $\mathbf{z}$ be $\bm{\sigma}_\mathbf{z}^2$.
	Then it holds that:
	\begin{equation} \label{equ::n2n_epsilon}
		\begin{aligned}
			\mathbb{E}_{\mathbf{x}, \mathbf{y}} \left\lVert f_{\theta}(\mathbf{y})-\mathbf{x} \right\rVert_2^2
			&= \mathbb{E}_{\mathbf{x}, \mathbf{y}, \mathbf{z}} \left\lVert f_{\theta}(\mathbf{y})-\mathbf{z} \right\rVert_2^2-\bm{\sigma}_\mathbf{z}^2 \\
			&+ 2\bm{\varepsilon} \mathbb{E}_{\mathbf{x}, \mathbf{y}}(f_{\theta}(\mathbf{y})-\mathbf{x}).  			
		\end{aligned} 
	\end{equation}

 As stated in \ref{equ::n2n_epsilon}, when the discrepancy $\bm{\varepsilon} \neq \mathbf{0}$, the condition $ \mathbb{E}_{\mathbf{x}, \mathbf{y}}(f_{\theta}(\mathbf{y})-\mathbf{x}) \neq 0$ implies that optimizing $\mathbb{E}_{\mathbf{x}, \mathbf{y}, \mathbf{z}} \left\lVert f_{\theta}(\mathbf{y})-\mathbf{z}\right\rVert_2^2$ does not yield the same solution as the supervised training objective $\mathbb{E}_{\mathbf{x}, \mathbf{y}} \left\rVert f_{\theta}(\mathbf{y})-\mathbf{x}\right\rVert_2^2$. However, when $\bm{\varepsilon} \to \mathbf{0}$, meaning the gap is sufficiently small, the term $2\bm{\varepsilon} \mathbb{E}_{\mathbf{x}, \mathbf{y}}(f_{\theta}(\mathbf{y})-\mathbf{x}) \to \mathbf{0}$, allowing the network trained with the noisy image pair $(\mathbf{y}, \mathbf{z})$ to approximate the supervised training network adequately. Notably, in the case where $\bm{\varepsilon} = \mathbf{0}$, minimizing both sides of Equation \eqref{equ::n2n_epsilon}, given the constancy of $\bm{\sigma}_\mathbf{z}^2$, leads to $\argmin_{\theta} \ \mathbb{E}_{\mathbf{x},\mathbf{y},\mathbf{z}} \left\lVert f_\theta(\mathbf{y}) - \mathbf{z} \right\rVert_2^2$, which aligns with the objective of Noise2Noise.




\subsection{Extension to Single Noisy Images} \label{subsec::n2n_extension}

Building on the Noise2Noise framework, where independent noisy image pairs of the same scene serve as training pairs, NBR2NBR~\cite{huang2021neighbor2neighbor} advances this concept by generating independent training pairs from a single noisy image $\mathbf{y}$ through sampling.

Specifically, an image pair sampler $G = (g_1, g_2)$ is employed to produce a noisy image pair $(g_1(\mathbf{y}), g_2(\mathbf{y}))$ from the single noisy image $\mathbf{y}$. The contents of the two sampled images $(g_1(\mathbf{y}), g_2(\mathbf{y}))$ are highly similar. Consequently, the sampled image pair can be treated as two noisy observations, resulting in the following:
\begin{align} \label{equ::pn2n}
	\argmin_{\theta} ~ \mathbb{E}_{\mathbf{x}, \mathbf{y}} \left\lVert f_\theta(g_1(\mathbf{y})) - g_2(\mathbf{y}) \right\rVert^2.
\end{align}
Unlike Noise2Noise, where the ground truths of two sampled noisy images $(g_1(\mathbf{y}), g_2(\mathbf{y}))$ are identical, in our case, they differ. This discrepancy can be expressed as $\bm{\varepsilon} = \mathbb{E}{\mathbf{y}|\mathbf{x}}(g_2(\mathbf{y})) - \mathbb{E}{\mathbf{y}|\mathbf{x}}(g_1(\mathbf{y})) \neq \mathbf{0}$. As outlined in Theorem \ref{theorem::epsilon}, directly applying Equation \eqref{equ::pn2n} in this context is suboptimal and may lead to over-smoothing. Therefore, it is crucial to account for the non-zero gap $\bm{\varepsilon}$.

For the optimal denoiser $f_\theta^$, trained on clean images with $\ell_2$-loss, the condition $f_\theta^*(\mathbf{y}) = \mathbf{x}$ and $f_\theta^*(g_\ell(\mathbf{y})) = g_\ell(\mathbf{x})$ holds for $\ell \in {1, 2}$. Consequently, the following holds for the optimal network $f_\theta^$: 

%\vspace{-12pt}
\begin{small}
	\begin{align}  
		& \mathbb{E}_{\mathbf{y}|\mathbf{x}} ~ \{ f_{\theta}^*(g_1(\mathbf{y}))-g_2(\mathbf{y})-\left(g_1(f_{\theta}^*(\mathbf{y}))-g_2(f_{\theta}^*(\mathbf{y}))\right) \}
		\label{equ::constraint1} \\
		& = g_1(\mathbf{x}) - \mathbb{E}_{\mathbf{y}|\mathbf{x}} \{g_2(\mathbf{y})\} - (g_1(\mathbf{x}) - g_2(\mathbf{x}))
		\nonumber \\ 
		& = g_2(\mathbf{x}) - \mathbb{E}_{\mathbf{y}|\mathbf{x}} \{g_2(\mathbf{y})\} = 0.    \nonumber %\label{equ::constraint2}
	\end{align}	
\end{small} 

%\vspace{-15pt}
{
	\noindent In Equation \eqref{equ::constraint1}, the last two terms account for the gap between the ground truths of the training image pair. When this gap is zero, these terms cancel out, reducing Equation \eqref{equ::constraint1} to a specific case of Noise2Noise paired training, as shown in Equation \eqref{equ}. However, if the gap is non-zero, these terms act as a corrective factor, compensating for the ground truth discrepancy in the first two terms of Equation \eqref{equ::constraint1}, ensuring that the equation is balanced to zero.
}

Consequently, Equation \eqref{equ::constraint1} imposes a constraint that holds when the denoiser $f_\theta$ matches the ideal denoiser $f_\theta^*$. To leverage this ideal constraint, instead of directly optimizing Equation \eqref{equ::pn2n}, we reformulate the problem as the following constrained optimization:

%\vspace{-10pt}
\small
\begin{equation}\label{eq::regular_opt}
	\begin{aligned}
		&\min_{\theta} ~ \mathbb{E}_{\mathbf{y}|\mathbf{x}} \left\lVert f_{\theta}(g_1(\mathbf{y})) - g_2(\mathbf{y}) \right\rVert_2^2, \ \text{ s.t.}  \\
		& \mathbb{E}_{\mathbf{y}|\mathbf{x}} \{ f_{\theta}(g_1(\mathbf{y})) - g_2(\mathbf{y}) -  g_1(f_{\theta}(\mathbf{y})) + g_2(f_{\theta}(\mathbf{y})) \} = 0.		
	\end{aligned}
\end{equation}
\normalsize

{
	\noindent
	With the equation $\mathbb{E}_{\mathbf{x}, \mathbf{y}} = \mathbb{E}_{\mathbf{x}} \mathbb{E}_{\mathbf{y}| \mathbf{x}}$, we further reformulate it as the following regularized optimization problem:
}

%\vspace{-9pt}
\small
\begin{equation}
	\begin{aligned}
		&\min_{\theta} ~ \mathbb{E}_{\mathbf{x}, \mathbf{y}} \left\lVert f_{\theta}(g_1(\mathbf{y})) - g_2(\mathbf{y}) \right\rVert_2^2 \\
		+& \gamma \mathbb{E}_{\mathbf{x}, \mathbf{y}}  \left\lVert f_{\theta}(g_1(\mathbf{y})) - g_2(\mathbf{y}) - g_1(f_{\theta}(\mathbf{y})) + g_2(f_{\theta}(\mathbf{y})) \right\rVert_2^2.		
	\end{aligned}
\end{equation}
\normalsize
\section{SRDsampling}
In Prompt-SID, we employed a spatial redundancy sampling strategy to produce training pairs. The detailed implementation for generating training pairs in Prompt-SID is shown in Fig. \ref{fig:SRD}. For each image inside an input training stack with $\mathbf{H}\times\mathbf{W}\times\mathbf{C}$ pixels (H, W and C are the height, width and channel of the input image), we spatially divided it into many adjacent small patches with $2\times2$pixels. Next, we randomly selected three adjacent pixels from 
each patch. The central pixel was used to compose the input image and the two spatially adjacent pixels were used to compose the two target images. After traversing all patches, we can finally obtain three downsampled images with the size of $\mathbf{H}/2\times\mathbf{W}/2\times\mathbf{C}$ pixels. As signals of spatially adjacent pixels are closely correlated, the input image and each target image can be considered as two independent samples of the same underlying pattern. Thus, the input image and the two target images can form two training pairs, which can be used for the self-supervised training of denoising networks. Compared 
with other methods~\cite{huang2021neighbor2neighbor}, SRDsampling strategy is more effective and 
comprehensive in preserving both structural information. We conducted corresponding ablation studies, as detailed in the table. \ref{ab:2}
\begin{figure}[h]
  \centering
  \includegraphics[width=8cm]{pdf/SRD.pdf}
  \caption{The details of SRDsampling.}
  \label{fig:SRD}
\end{figure}
\begin{table}[]\small
\caption{In the ablation experiments on SRDsampling and loss functions, we consistently used nafnet's block and employed the SAM for Prompt fusion.}
\label{ab:2}
\begin{tabular}{@{}lllll@{}}
\toprule
sample strategy & loss          & \begin{tabular}[c]{@{}l@{}}SIDD\\ validation\end{tabular} & \begin{tabular}[c]{@{}l@{}}SIDD\\ benchmark\end{tabular}            \\ \midrule
NBR2NBR           & L2(WITH scale)  & 51.25/\textbf{0.991}                                               & 50.67/\textbf{0.991} \\
SRD             & L1+L2(NO scale) & 50.09/0.984                                               & -/-   \\
SRD            & L2(WITH scale)  & \textbf{51.34}/\textbf{0.991}                                               & \textbf{50.79}/\textbf{0.991} \\ \bottomrule
\end{tabular}
\end{table}
\begin{figure}[h]
  \centering
  \includegraphics[width=7cm]{pdf/SAM.pdf}
  \caption{The network structure of blocks in SPIformer.It consists of SAM, SAB and GATE.}
\end{figure}

\section{Structural Attention Module}
We employ the vision transformer (ViT)\cite{dosovitskiy2020image} module as the backbone for image denoising. Similar to prior research\cite{zamir2022restormer,zhang2023kbnet,chen2022simple}, our transformer module comprises two components: the multi-head self-attention module and the gate control network. Additionally, we introduce a multi-channel attention integrative block (SAM) to incorporate the previously generated structural embedding \( \hat{c}_{0} \) into the feature map. In this fusion process, we draw inspiration from the concept of simple channel attention (SCA) proposed by NAFNet\cite{chen2022simple}, renowned for effectively integrating global information while preserving computational efficiency. By integrating the SCA concept, Diff-Neighbors can capitalize on global information and enhance the fusion of the structural embedding \( \hat{c}_{0} \) with the feature map.

The primary operation principle of SAM can be delineated into two phases: channel attention extraction and computation of its own features, and the integration of structural embedding information. We acquire channel attention weights through global average pooling and 1x1 convolution applied to the feature maps, as illustrated by the following equation:
\begin{equation}
    c_{sca}=AvgPool(\hat{F}) * W_{l1} + b_{l1}
\end{equation}
In this equation, \( AvgPool(\hat{F}) \) denotes the global average pooling operation applied to the feature map \( \hat{F} \). The resulting output is subsequently multiplied by the weight \( W_{l1} \) and added to the bias \( b_{l1} \). 

Subsequently, we merge \( c_{sca} \) and \( \hat{c}_{0} \) to jointly derive channel attention weights that direct the processing of the feature maps. The precise procedure is outlined as follows:
\begin{equation}
\hat{F}_{SAM}=W_{s1}c_{sca}\odot W_{c1}\hat{c}_0\odot Norm(\hat{F})+W_{s2}c_{sca}\odot W_{c2}\hat{c}_0
\end{equation}
In the equation mentioned above, $W_{s1},W_{s2},W_{c1},W_{c2}$ represents the weight matrix of the linear layer. $\hat{F}_{SAM}$ represents the feature map processed by the SAM.

Following SAM, $\hat{F}_{SAM}$ undergoes computations involving the multi-head self-attention block and the gate control network. In the multi-head self-attention block, the feature map is convolved to derive three self-attention variables: \( Q \), \( K \), and \( V \). These variables undergo reshaping, followed by a simple matrix multiplication between \( Q \) and \( K \) to produce the attention map. Subsequently, the attention map is processed through a softmax layer and combined with the reshaped \( V \) through further matrix multiplication. The gate control network comprises two convolutional layers and a simple gate between them. The computation of the simple gate is simpler compared to the corresponding modules in Restormer\cite{zamir2022restormer}. Specifically, it involves splitting the feature map into two parts along the channel dimension and performing element-wise multiplication between these parts. The primary network architecture, SPIformer, consists of stacked transformer blocks integrating the multi-head self-attention module and the gate control module with SAM.

We visualized the feature maps from the SAM ablation experiments on a per-channel basis to finely discern the impact of the SAM mechanism on different feature layers. Due to space constraints, we present four of the 48 feature channels to illustrate our findings. The visualization results are shown in the Fig. ~\ref{fig:channel}. To facilitate observation, we used pseudocolor imaging to better distinguish subtle pixel differences.
\begin{table}[h]\small
\centering
\caption{ Grayscale image quantitative denoising results.}
\label{fig:gray}
\begin{tabular}{@{}llll@{}}
\toprule
Noise Type & Method        & BSD68       & Set12       \\ \midrule
           & N2C           & 31.58/0.889 & 32.60/0.899 \\ \cmidrule(l){2-4} 
           & Noise2Self    & 30.63/0.843 & 29.88/0.840 \\
gauss      & Noise2Same\    & 30.85/0.850 & 30.02/0.849 \\
$\sigma=15$         & Blind2Unblind & \underline{31.44}/\underline{0.884} & \textbf{32.46}/\textbf{0.897} \\
           & swinIA        & 31.07/0.856 & 30.37/0.857 \\
           & ours          & \textbf{32.61}/\textbf{0.898} & \underline{32.39}/\underline{0.896} \\ \midrule
           & N2C           & 29.02/0.822 & 30.07/0.852 \\ \cmidrule(l){2-4} 
           & Noise2Self    & 28.88/0.789 & 28.37/0.799  \\
gauss      & Noise2Same    & 29.13/0.800 & 28.54/0.814 \\
$\sigma=25$         & Blind2Unblind & \underline{28.99}/\underline{0.820} & \textbf{30.09}/\textbf{0.854} \\
           & swinIA        & 28.88/0.789 & 28.37/0.799 \\
           & ours          & \textbf{30.11}/\textbf{0.838} & \underline{29.98}/\underline{0.850} \\ \bottomrule
\end{tabular}
\end{table}

\begin{table*}[h]\scriptsize
\centering
\caption{Quantitative results on synthetic datasets in sRGB space. The highest PSNR(dB)/SSIM among unsupervised denoising methods is highlighted in \textbf{bold}, while the second is \underline{underlined}, and the abbreviation NBR2NBR stands for neighbor2neighbor.}
%\vspace{-2.5mm}
\label{tab:syn}
\begin{tabular}{@{}llllllllll@{}}
\toprule
noise type & method        & kodak       & BSD300      & set14       & noise type & method        & kodak       & BSD300      & set14       \\ \midrule
           & N2N & 32.41/0.884 & 31.04/0.878 & 31.37/0.868 &            & N2N & 31.77/0.876 & 30.35/0.868 & 30.56/0.857 \\ \cmidrule(lr){2-5} \cmidrule(l){7-10} 
           & self2self     & 31.28/0.864 & 29.86/0.849 & 30.08/0.839 &            & self2self     & 30.31/0.857 & 28.93/0.840 & 28.84/0.839 \\
gaussian      & laine19-mu    & 30.62/0.840 & 28.62/0.803 & 29.93/0.830 & poisson    & laine19-mu   & 30.19/0.833 & 28.25/0.794 & 29.35/0.820 \\
$\sigma=25$         & laine19-pme   & \textbf{32.40}/\textbf{0.883} & \underline{30.99}/\underline{0.877} & \underline{31.36}/\underline{0.866} & $\lambda=30$         & laine19-pme   & \textbf{31.67}/\textbf{0.874} & \underline{30.25}/\underline{0.866} & \underline{30.47}/\underline{0.855} \\
           & DBSN          & 31.64/0.856 & 29.80/0.839 & 30.63/0.846 &            & DBSN          & 30.07/0.827 & 28.19/0.790 & 29.16/0.814 \\
           & R2R           & 32.25/0.880 & 30.91/0.872 & 31.32/0.865 &            & R2R           & 30.50/0.801 & 29.47/0.811 & 29.53/0.801 \\
           & swinIA        & 30.12/0.819 & 28.40/0.789 & 29.54/0.814 &            & swinIA        & 29.51/0.805 & 27.92/0.775 & 28.74/0.799 \\
           & ours          & \underline{32.41}/\textbf{0.883} & \textbf{31.16}/\textbf{0.880} & \textbf{31.45}/\textbf{0.868} &            & ours          & \underline{31.65}/\textbf{0.874} & \textbf{30.43}/\textbf{0.869} & \textbf{30.56}/\textbf{0.858} \\ \midrule
           & N2N & 32.50/0.875 & 31.07/0.866 & 31.39/0.863 &            &N2N & 31.18/0.861 & 29.78/0.848 & 30.02/0.842 \\ 
           & self2self     & 31.37/0.860 & 29.87/0.841 & 29.97/0.849 &            & self2self     & 29.06/0.834 & 28.15/0.817 & 28.83/0.841 \\
gaussian      & laine19-mu    & 30.52/0.833 & 28.43/0.794 & 29.71/0.822 & poisson    & laine19-mu    & 29.76/0.820 & 27.89/0.778 & 28.94/0.808 \\
$\sigma\in[5,50]$       & laine19-pme   & \underline{32.40}/\underline{0.870} & \underline{30.95}/\underline{0.861} & \underline{31.21}/\underline{0.855} & $\lambda\in[5,50]$       & laine19-pme   & \underline{30.88}/\underline{0.850} & \underline{29.57}/\underline{0.841} & \underline{28.65}/\underline{0.785} \\
           & DBSN          & 30.38/0.826 & 28.34/0.788 & 29.49/0.814 &            & DBSN          & 29.60/0.811 & 27.81/0.771 & 28.72/0.800 \\
           & R2R           & 31.50/0.850 & 30.56/0.855 & 30.84/0.850 &            & R2R           & 29.14/0.732 & 28.68/0.771 & 28.77/0.765 \\
           & swinIA        & 30.30/0.820 & 28.40/0.785 & 29.49/0.809 &            & swinIA        & 29.06/0.788 & 27.74/0.764 & 28.27/0.780 \\
           & ours          & \textbf{32.67}/\textbf{0.876} & \textbf{31.19}/\textbf{0.866} & \textbf{31.22}/\textbf{0.860} &            & ours          & \textbf{31.49}/\textbf{0.864} & \textbf{30.01}/\textbf{0.855} & \textbf{30.34}/\textbf{0.852} \\ \bottomrule
\end{tabular}
%\vspace{-2.5mm}
\end{table*}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{pdf/cadenoising.png}
    \caption{The experimental results of Prompt-SID compared with the N2C method on calcium fluorescence imaging with a rate of 30 Hz are presented. Prompt-SID generates fewer artifacts and granular edges compared to traditional supervised learning methods.}
    \label{fig:cad}
\end{figure}
\begin{figure*}[h]
    \centering
    
    \begin{minipage}{0.67\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pdf/cad_sup.pdf}
        \label{fig:sub1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pdf/his.pdf}
        \label{fig:sub2}
    \end{minipage}
    
    \caption{Visual results of Prompt-SID for calcium fluorescence imaging denoising in sampling rate 30hz. We use Fiji software for image editing and visualization. From the histogram distribution, Prompt-SID effectively reduces the increased pixel intensity variance caused by noise, thereby improving the signal-to-noise ratio.}
    \label{fig:cad}
\end{figure*}
\section{Experiments}
\subsection{Implementation Details}
\textbf{Training Details.} In synthetic, real-world, and calcium imaging datasets, we adopted uniform training configurations, slightly modified to suit each dataset's unique attributes. Images were segmented into patches of different sizes for training, with fixed iterations and batch sizes assigned to each patch size. Following the protocol outlined in this study\cite{wang2022blind2unblind}, we fixed the decay rate for the exponential moving average at 0.999 and initialized the learning rate to 0.0002. Parameter optimization and computation were performed with the Adam optimizer, setting \( \beta_1 \) 
\begin{figure*}[p]
    \centering
    \includegraphics[width=\textwidth]{pdf/channel.pdf}
    \caption{The results from channels 1 and 7 indicate that after applying the SAM mechanism, pixel values associated with contours increased, while those related to filling decreased. This demonstrates SAM's ability to emphasize semantic information. Channels 14 and 17, on the other hand, show changes in noise-related channel dimensions after SAM processing. In channel 14, we observe an overall reduction in pixel values, indicating that SAM mitigates the impact of local channel noise on the image. Meanwhile, in channel 17, some small patch noise is effectively eliminated.}
    \label{fig:channel}
    \bigskip
    \centering
    \includegraphics[width=\textwidth]{pdf/sidd_sup.pdf}
    \caption{Additional visual comparison results on the SIDD benchmark dataset. }
    \label{fig:sidd}
    \bigskip
    \centering
    \includegraphics[width=\textwidth]{pdf/sidd_sup2.pdf}
    \caption{Additional visual comparison results on the SIDD benchmark dataset. }
    \label{fig:sidd2}
\end{figure*}
to 0.9 and \( \beta_2 \) to 0.99. Training was executed on one Nvidia RTX3090 for all experiments.

\textbf{Details for Synthetic Denoising.}
In the synthetic denoising experiment, we adopted identical settings as neighbor2neighbor\cite{huang2021neighbor2neighbor}. Specifically, we curated a training set comprising 44,328 images from the ILSVRC2012 dataset\cite{deng2009imagenet}, with dimensions ranging from 256×256 to 512×512 pixels. For the testing phase, we utilized the same datasets as the referenced study\cite{huang2021neighbor2neighbor,wang2022blind2unblind,papkov2023swinia}, namely kodak, BSD300\cite{martin2001database}, and set14\cite{zeyde2012single}. Denoising experiments were conducted on color RGB images under four sets of noise configurations: gaussian $\sigma=25$, gaussian $\sigma\in [5,50]$, poisson $\lambda =30$, poisson $\lambda\in [5,50]$. A total of 400,000 iterations were performed, employing patch sizes of [192, 256, 320, 368] sequentially and patch sizes of [8, 4, 2, 1] respectively, with corresponding iteration counts of [92,000, 130,000, 83,000, 95,000]. Furthermore, the learning rate was halved every 80,000 iterations during the experiment.

\textbf{Details for Real-World Denoising.}We utilized the SIDD-Medium dataset\cite{abdelhamed2018high} in the raw-RGB domain as the training set, comprising approximately 30,000 noisy images captured from 10 scenes using five representative smartphone cameras. For testing, we employed two datasets: SIDD validation and SIDD benchmark. A total of 400,000 iterations were conducted during training, employing patch sizes of [192, 256, 320, 448, 512] and patch sizes of [8, 4, 2, 1, 1] sequentially. The corresponding iteration counts were [82,000, 140,000, 73,000, 65,000, 40,000].

\textbf{Details for Fluorescence Imaging Denoising.}We employed the two-photon calcium imaging of large 3D neuronal populations dataset, as proposed by SRDtrans\cite{li2023spatial}, for both training and testing on the fluorescence imaging dataset. Realistic calcium imaging data with synchronized ground truth was generated utilizing neural anatomy and optical microscopy (NAOMi). Our method was compared with neighbor2neighbor\cite{huang2021neighbor2neighbor} and blind2unblind\cite{wang2022blind2unblind} approaches. To ensure a fair comparison, all methods underwent training for 50 epochs. Training was conducted on data samples with sampling rates of 0.1Hz, 1Hz, 3Hz, and 10Hz. As unlabeled data was utilized, testing was performed on the same training data, and quantitative results were obtained during the testing phase utilizing the available labels. For the dataset with a scanning speed of 30Hz, given the substantial volume of training data, comprising 6000 frames, we partitioned it into segments of 5500 and 500 frames, allocated for training and testing, respectively.


\subsection{Benchmarking Results}
\textbf{Experiments of Gray Scale Image Denoising}
We conducted comparative experiments on grayscale single-channel image data. Consistent with prior research\cite{wang2022blind2unblind}, our experiments were conducted on the BSD400\cite{zhang2017beyond} dataset for training. During training, we set the learning rate to 0.0002 with four decay steps, totaling 200,000 iterations, and input training data in patch sizes of 128×128. For evaluation, we used the BSD68\cite{roth2005fields} and Set12 datasets as test sets. We performed experiments under two settings: Gaussian noise with standard deviations of 15 and 25. Our results were compared with several methods, including Noise2Clean (N2C)\cite{ronneberger2015u}, Noise2Self\cite{batson2019noise2self}, Noise2Same\cite{xie2020noise2same}, Blind2Unblind\cite{wang2022blind2unblind}, and SwinIA\cite{papkov2023swinia}.

The experimental results are presented in the table \ref{fig:gray}. Our method demonstrates significant advantages on the BSD68 dataset, surpassing existing methods by a large margin and achieving a 1.09dB improvement compared to the supervised learning approach, Unet. On the Set12 dataset, our method achieves comparable results to Noise2Clean (N2C), with differences not exceeding 0.10dB compared to the state-of-the-art method.Although not achieving the best performance on the Set12 dataset, the experiments above still demonstrate the significant potential of our method, which integrates diffusion with prior knowledge. We speculate that the difference in experimental results between the two datasets is due to the relatively small training data size, making it challenging for diffusion to generalize features effectively.

\textbf{The Supplementary Results of the Comparative Experiments Presented in the Main Text}
Due to space limitations in the main text, we have selectively compared our approach with a few representative self-supervised methods. These methods encompass traditional self-supervised denoising techniques, downsampling-based deep learning approaches, and blind-spot network-based deep learning methods, all of which have established a significant impact in their respective domains. In addition, we compared our approach with methods such as noise2noise~\cite{lehtinen2018noise2noise}, self2self~\cite{quan2020self2self}, laine19-mu, laine19-pmu~\cite{laine2019high}, DBSN~\cite{wu2020unpaired}, R2R~\cite{pang2021recorrupted} and swinIA~\cite{papkov2023swinia}. The experimental results of these methods on both synthetic and real-world datasets were benchmarked against Prompt-SID, as shown below.

\begin{table}[]\small
\centering
\caption{Quantitative denoising results PSNR(dB)/SSIM on SIDD benchmark and
validation datasets in raw-RGB space.}
%\vspace{-2.5mm}
\label{tab:sidd}
\begin{tabular}{llll}
\hline
methods             & network        & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}SIDD\\ benchmark\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}SIDD\\ validation\end{tabular}} \\ \hline

baseline,N2N        & U-Net          & 50.62/0.991                                                                  & 51.21/0.991                                                                   \\ \hline

laine19-mu(G)       & U-Net          & 49.82/0.989                                                                  & 50.44/0.990                                                                   \\
laine19-pme (G)     & U-Net          & 42.17/0.935                                                                  & 42.87/0.939                                                                   \\
laine19-mu (P)      & U-Net         & 50.28/0.989                                                                  & 50.89/0.990                                                                   \\
laine19-pme (P)     & U-Net          & 48.46/0.984                                                                  & 48.98/0.985                                                                   \\
DBSN                & DBSN& 49.56/0.987                                                                  & 50.13/0.988                                                                   \\
R2R                 & U-Net          & 46.70/0.978                                                                  & 47.20/0.980                                                                   \\

ours                & Prompt-SID & \textbf{50.97}/\textbf{0.991}                                                                  & \textbf{51.55}/\textbf{0.992}                                                                   \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}

\subsection{Source Code}
We submit the source code of this paper in the supplementary material, including the source code files of Prompt-SID and the rewritten training and test code for synthetic, real-world and fluorescence imaging dataset. Furthermore, all the source code and pre-trained models will be publicly available for further research after the paper is accepted. 

The code for Prompt-SID is built on the large-scale low-level vision processing codebase BasicSR, utilizing YML files for command integration. When training on the three datasets and testing on the Srgb dataset, please run the corresponding `.sh` files. Additionally, we have provided specific testing scripts for the `.tif` format fluorescence imaging dataset and the `.MAT` format SIDD dataset. The results generated by running the SIDD testing script are formatted to be compatible with the official evaluation pipeline, allowing for direct submission to the SIDD benchmark for testing and results comparison.

The directory structure of our submitted code is listed as:

\noindent$\|---\textbf{CODE}\\
\|---|---test.py \\
\|---|---test\_sidd\_benchmark.py \\
\|---|---test\_cad\_benchmark.py \\
\|---|---train\_srgb.sh \\
\|---|---train\_sidd.sh \\
\|---|---train\_cad.sh \\
\|---|---test\_srgb.sh \\
\|---|---Promptsid \\
\|---|---|---archs \\
\|---|---|---|---attendiff\_arch.py \\
\|---|---|---|---attention.py \\
\|---|---|---|---common.py \\
\|---|---|---data \\
\|---|---|---|---paired\_dataset.py \\
\|---|---|---|---sidd\_raw\_dataset.py \\
\|---|---|---|---sidd\_val\_dataset.py \\
\|---|---|---|---cad\_dataset.py \\
\|---|---|---|---transforms.py \\
\|---|---|---losses \\
\|---|---|---|---my\_loss.py \\
\|---|---|---models \\
\|---|---|---|---Diff\_model.py \\
\|---|---|---|---sidd\_diff\_model.py \\
\|---|---|---|---uint8\_model.py \\
\|---|---|---|---addnoise.py \\
\|---|---|---|---get\_imagepairs.py \\
\|---|---|---utils \\
\|---|---|---test.py \\
\|---|---|---train.py \\
\|---|---|---train\_pipeline.py \\
\|---|---ldm \\
\|---|---|---ddpm.py \\
\|---|---|---ddim.py \\
\|---|---|---classifier.py \\
\|---|---|---lr\_scheduler.py \\
\|---|---options \\
\|---|---|---train\_imagenet.yml \\
\|---|---|---train\_siddraw.yml \\
\|---|---|---train\_flu.yml \\
\|---|---|---test\_imagenet.yml \\$
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{pdf/edge.pdf}
    \caption{In the visual comparison between Prompt-SID and the NBR2NBR method, we qualitatively evaluated both the RGB images and edge maps, and quantitatively analyzed the three-channel histograms using Fiji software. As shown in the figure, Prompt-SID effectively avoids the occurrence of abnormal peaks in the histograms.}
    \label{fig:edge}
\end{figure}
% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}
%\vspace{-5mm}



\bibliography{ref}

\end{document}
