\section{CavePI Navigation Pipeline}
The CavePI AUV is designed to autonomously navigate underwater by following a caveline and other navigation markers. However, a caveline appears a few pixels wide in the bottom-facing cameraâ€™s FOV and is significantly challenging to detect in noisy low-light conditions. Moreover, the caveline is often obscured or blends into the background, requiring a fine-tuned visual learning pipeline for reliable operation. 

\subsection{Semantic Guidance: Tracking by Detection}
The model selection process plays a crucial role in optimizing accuracy and efficiency on resource-constrained edge devices. While object detection and semantic segmentation are two established techniques in visual learning, they produce different interpretations of the target RoI. Object detection models generate bounding boxes around the target, which, given the caveline's irregular shape and orientation, can encompass significant background regions, complicating the estimation of the heading angle. To address this, we opt for semantic segmentation, which provides pixel-level contours of the caveline as a more precise tracking-by-detection.

\begin{figure}[h]
% \vspace{-1mm}
    \centering
    \includegraphics[width=\linewidth]{figures/model.png}%
    \vspace{-1mm}
    \caption{Simplified model architecture for caveline segmentation is shown; we use a DeepLabV3~\cite{chen2017deeplab} head with two choices for backbone network: MobileNetV3~\cite{howard2019searching} and ResNet101~\cite{he2016deep}.}
    \label{fig:model}
    \vspace{-3mm}
\end{figure}


\vspace{1mm}
\noindent
\textbf{Model architecture and design choices.} Onboard resource constraints of CavePI significantly influence the choice of model architecture. The Jetson Nano, with its shared memory architecture, allocates GPU memory dynamically from a limited $2$\,GB pool, which must also accommodate the dual camera feeds and Nano-Pi control signal communication. Considering these limitations, we initially selected the MobileNetV3 (Large) backbone~\cite{howard2019searching} for feature extraction. This lightweight architecture feeds extracted features into a DeepLabV3~\cite{chen2017rethinking} head to generate binary segmentation maps. While the MobileNetV3-DeepLabV3 combination is computationally efficient, requiring only about $314$\,MB GPU memory for $11$ million parameters, it proved insufficient for accurately segmenting the thin caveline in turbid water conditions. To address this limitation, we adopted a heavier backbone, ResNet101~\cite{he2016deep}, while retaining the DeepLabV3 head. This model achieved slightly better performance, as reported in Table~\ref{tab:model_comparison}, albeit at the cost of reduced inference speed. Both models were initialized with pre-trained weights and fine-tuned on a custom dataset to enhance caveline detection accuracy in challenging underwater environments.

%The onboard resource constraints further influence the choice of model architecture. The Jetson Nano, with its shared memory architecture, dynamically allocates GPU memory from a limited $2$\,GB pool, which must also support CavePI's dual camera feeds and Nano-Pi control signal communication. Considering these limitations, we initially chose a MobileNetV3 (Large) backbone~\cite{howard2019searching} for feature extraction. The features are fed into a DeepLabV3~\cite{chen2017rethinking} head that generates the binary segmentation map. Although very light (~$44$\,MB), the $5$\,M parameters are not sufficient to segment the thin caveline in turbid water conditions. Therefore, we adopt a heavier backbone, ResNet101~\cite{he2016deep}, while retaining the DeepLabV3 head module. This model achieves superior segmentation performance but compromises inference speed; as shown in Table~\ref{tab:model_comparison}. Both architectures are initialized with pre-trained weights and fine-tuned on a custom dataset to improve caveline detection performance.




\vspace{1mm}
\noindent
\textbf{Dataset and model training.} 
For this study, we utilized the open-source CL-ViT dataset, which includes pixel-level annotations of cavelines~\cite{yu2023weakly}. It comprises $3,150$ RGB images collected from three distinct underwater cave systems. To enhance the dataset's diversity and variance, we manually annotated an additional $150$ images from our laboratory testbed, resulting in a combined dataset of $3,300$ images. The dataset was divided into a training set of $3,200$ images and a validation set of $100$ images. Both models were fine-tuned on this dataset over a maximum of $50$ epochs using the following training configurations: the Adam optimizer with an initial learning rate of $0.001$, a cosine learning rate scheduler with $T_{max} = 20$, and a cross-entropy loss function. This training strategy was designed to optimize these models' performances in diverse underwater environments.
 
%We use the open-source Cl-ViT dataset with pixel-level annotations of cavelines~\cite{yu2023weakly}, comprising $3150$\,RGB images collected from three different underwater cave systems. Additionally, we manually annotate $150$\,images collected from our lab testbed, resulting in a total dataset of $3300$\,images. The dataset is split into a training set ($3200$\,images) and validation set ($100$\,images). Both models are fine-tuned on this dataset for a maximum of $50$\,epochs using the following training configurations: \emph{Adam} optimizer, cosine learning rate scheduler with $T_{max} = 20$, and cross-entropy loss function.

\vspace{1mm}
\noindent
\textbf{Benchmark comparison.} The fine-tuned models are evaluated on the open-source CL-Challenge dataset~\cite{yu2023weakly} that contains caveline annotations for $200$ noisy low-light images from different cave systems. Using the lighter MobileNetV3 backbone, our model achieves a mean intersection-over-union (mIoU) of $48.95\%$, whereas the considerably heavier ResNet101 model demonstrates only a marginal improvement with a mIoU of $50.48\%$. For reference, the highest mIoU score reported on this dataset is $58.3\%$~\cite{yu2023weakly}.





\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Edge performances (on a Jetson Nano device) for the two model configurations (Fig.~\ref{fig:model}) available in CavePI are compared.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
    \Xhline{2\arrayrulewidth}
    Backbone & mIoU & \# Params & Inference rate & GPU usage\\ 
    % & parameters & rate & usage \\ 
    \Xhline{2\arrayrulewidth}
    MobileNetV3~\cite{howard2019searching}  & $48.95\%$ & $11.02$\,M & $18.2$ fps  & $314$\,MB  \\
    ResNet101~\cite{he2016deep}             & $50.48\%$ & $60.99$\,M & $\z1.4$ fps & $747$\,MB  \\
    \Xhline{2\arrayrulewidth}
    \end{tabular}
    }
    \label{tab:model_comparison}
    \vspace{-1mm}
\end{table}

%\JI{so you talked about model training, tuning, validation set, etc. So how was the detection performance?? You only showed inference rate.} \textbf{benchmark comparison added}

\vspace{1mm}
\noindent
\textbf{Model compression and integration.} The two aforementioned models are optimized to deploy on the Jetson Nano device of CavePI. First, the models are converted into Open Neural Network Exchange (ONNX)~\cite{onnx} format that uses computational graphs (nodes and edges) to represent operations and data flow within the pipeline. Subsequently, a serialized data structure is created by NVIDIA's TensorRT SDK~\cite{tensorrt}, which stores highly optimized deep learning models ready for fast inference on NVIDIA GPUs. Table~\ref{tab:model_comparison} compares the onboard performance of the two models used in this work.



\subsection{Autonomous Control} 
Algorithm~\ref{alg:line_follower} outlines the visual servoing control algorithm employed by CavePI. The process begins by extracting caveline contours, $\mathcal{C}$, from the segmentation map $\mathcal{I}$, for semantic guidance. As the caveline is often detected as fragmented contours (see Fig.~\ref{fig:heading_control}), the center of the farthest contour $(u_c, v_c)$ is identified and selected as the next waypoint in CavePI's path planning. The heading angle $\psi$ of this waypoint is then calculated with respect to the image center $(u_i, v_i)$, about the x-axis of the image frame, following the right-hand rule. This heading angle serves as a high-level navigation command, which is transmitted to the Raspberry Pi-5 for execution within the computational subsystem. 


\begin{algorithm}[h]
\caption{Visual Servoing Controller of CavePI}
\label{alg:line_follower}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Segmentation map $\mathcal{I}_{w\times h}$, center pixel $(u_i, v_i)$
\STATE \textbf{Output:} Thruster commands $\mathbf{v}$

\WHILE{robot is active}
    \STATE Extract set of contours $\{\mathcal{C}\}$ from $\mathcal{I}$
    \IF{$\mathcal{C} = \emptyset$}  
        \STATE \textbf{State} $\gets$ \texttt{lost} \codecomment{no caveline detected}
        \STATE Rotate $360^\circ$ \codecomment{small loops for recovery} 
    \ELSE
        \IF{$|\mathcal{C}| > 1$} 
            \STATE Sort $\{\mathcal{C}\}$:~$\{\mathcal{C}_0, \mathcal{C}_1, ..., \mathcal{C}_n, ...\}$ \codecomment{toward robot's heading}
            % \STATE $\mathcal{C}_\text{next} \gets \mathcal{C}_n$ \codecomment{aim for the farthest contour} 
            \STATE $(u_n, v_n) \gets \text{Centroid} (\mathcal{C}_n)$
            \STATE $\mathcal{C}_\text{next} \gets \arg\max_{\mathcal{C}_n \in \{\mathcal{C}\}} \bigl((u_n - u_i)^2 + (v_n - v_i)^2\bigr)$ \codecomment{farthest contour from $(u_i,v_i)$}

        \ELSE
            \STATE $\mathcal{C}_\text{next} \gets \mathcal{C}$
        \ENDIF
        \STATE $(u_c, v_c) \gets \text{Centroid}(\mathcal{C}_\text{next})$ \codecomment{contour center}
        \STATE $\psi \gets \text{slope}((u_i, v_i), (u_c, v_c))$ \codecomment{next heading angle}
        \STATE $\mathbf{v} \gets f(\psi)$ \codecomment{thruster commands}
    \ENDIF
    \STATE Send $\mathbf{v}$ to thrusters
\ENDWHILE
\end{algorithmic}
\end{algorithm}
% \vspace{-3mm}



%The CavePI AUV performs fully autonomous caveline following upon receiving a \emph{start} command from the operator. The command is sent visually, presenting a distinct QR code at the front camera. Other available visual commands are \emph{hold depth}, \emph{arm}, \emph{disarm}, and \emph{turn on/off the lights}.



 


The centroid of all detected contours in the set $\mathcal{C}$ is calculated, with the centroid of the $n^{\text{th}}$ contour denoted as \((u_n, v_n)\). The farthest contour is determined by:
\begin{equation*}
(u_c, v_c) = \arg\max_{n \in C}\, \big[{(u_n - u_i)^2 + (v_n - v_i)^2}\big]
\end{equation*}
The instantaneous heading vector is defined as the vector from the image center \( (u_i, v_i) \) to the centroid of the farthest detected contour \( (u_c, v_c) \), represented as $\begin{bmatrix} u_c - u_i & v_c - v_i \end{bmatrix}^T$. The equation of the instantaneous line of motion in the image frame is given by:
\begin{equation*}
\begin{bmatrix} u - u_i \\ v - v_i \end{bmatrix} \times \begin{bmatrix} u_c - u_i \\ v_c - v_i \end{bmatrix} = 0
\end{equation*}
where \( (u, v) \) represents the free variable on the line. 
% In the last equation, the symbol \( \times \) denotes the cross product of two vectors.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/heading_control.png}%
    \vspace{-1mm}
    \caption{A few visual servoing test cases are shown; (a) AUV heading is estimated based on its farthest caveline contour; (b,c) scenarios for straight and turn decisions, respectively; and (d) an \textit{overshoot} scenario occurred due to delayed decision-making.
    }
    \label{fig:heading_control}
    \vspace{-2mm}
\end{figure}

Ideally, the forward x-axis of the image frame aligns with the robot's heading direction, enabling the computed heading angle to function as the primary error signal for a proportional-integral-derivative (PID) controller on the Raspberry Pi-5. The PID controller is calibrated to minimize this error, maintaining CavePIâ€™s heading toward the designated waypoint. The controller-generated control signals are transmitted to the electronic speed controllers (ESCs) via the {\tt MAVLink} communication protocol. These ESCs regulate the speed and rotational direction of the thrusters, enabling precise maneuvering and robust navigation of CavePI.

%Ideally, the forward x-axis of the image frame is aligned with the robotâ€™s heading direction, allowing the next heading angle to act as the primary error signal for a proportional-integral-derivative (PID) controller implemented on the Raspberry Pi-5. The PID controller is calibrated to minimize the error signal, ensuring that CavePI maintains its heading toward the designated waypoint along its path. Control signals generated by the PID controller are transmitted to the electronic speed controllers (ESCs) using the {\tt MAVLink} communication protocol. These ESCs regulate the speed and rotational direction of the thrusters, enabling precise maneuvering and robust navigation of CavePI. %Fig.~\ref{fig:heading_control} demonstrates some

%The x-axis of the image frame is aligned with the robotâ€™s heading direction, making the next heading angle the primary error signal for a PID controller implemented on the Raspberry Pi-5. This PID controller is calibrated to track the zero error signal, thereby maintaining CavePIâ€™s heading toward the next waypoint along its path. The control signals generated by the PID controller are transmitted to the ESCs via the {\tt MAVLink} communication protocol. These ESCs regulate the speed and rotational direction of the thrusters, enabling precise maneuvering and navigation of the AUV.



 
% The x-axis of the image frame is aligned with the heading direction of the robot, therefore the next heading angle is fundamentally treated as the error signal for a PID controller in the Raspberry Pi-5. The PID controller is tuned to track zero error and thus keeping CavePI's heading towards the next waypoint in its path. The control output from the PID controller is then sent to the ESCs to control the speed and direction of rotation of the thrusters via {\tt MAVLink} communication protocol.



