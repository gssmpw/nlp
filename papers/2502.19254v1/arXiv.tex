% -*-text-*-
\newif\ifarXiv
\newif\ifWP
\newif\ifFULL

\arXivtrue

\ifarXiv
  \documentclass[10pt]{article}
  \usepackage{amsmath,amsthm,amsfonts,amssymb,latexsym,graphicx}
\fi
\ifWP
  \documentclass[10pt]{article}
  % \documentclass[toc]{article}
  \usepackage{amsmath,amsthm,amsfonts,amssymb,latexsym,graphicx} % accents
  % biblatex: more complex than bibtex
  % \setcounter{secnumdepth}{1}
  \setcounter{tocdepth}{1}
  \input{/Doc/Computing/Latex/kp.txt}
\fi

% The packages that all versions use:
\usepackage{etoolbox,stmaryrd,cite,enumitem}
% cite is for citing ranges of references
\usepackage[shortcuts]{extdash} % for hyphenating "p-variable" etc.

\newif\ifTR   % derivative conditionals (TR = arXiv or WP)
\ifarXiv\TRtrue\fi
\ifWP\TRtrue\fi

\emergencystretch=5mm
\tolerance=400
\allowdisplaybreaks[4]

\newtoggle{arXiv}
\newtoggle{WP}
\newtoggle{FULL}
\newtoggle{TR}
\newtoggle{false} % always false
\newtoggle{true}\toggletrue{true} % always true

\ifarXiv\toggletrue{arXiv}\fi
\ifWP\toggletrue{WP}\fi
\ifFULL\toggletrue{FULL}\fi
\ifTR\toggletrue{TR}\fi

\iftoggle{TR}{%
  % comment out line 1 and uncomment line 2 for a clean version
  % uncomment lines 1 and 2 to see the labels
  % uncomment line 1 and comment out line 2 to check for unnecessary labels
  % \usepackage{refcheck} \newcommand{\href}[2]{\relax} \newcommand{\url}[1]{\relax} % 1
  \usepackage[pdfpagemode=UseNone,pdfstartview=FitH]{hyperref}  % 2 (the version that opens the file the way I like it)
}{}%

\iftoggle{WP}{%
  \input{T2Aenc.def}
  \usepackage[utf8]{inputenc}
  \newenvironment{cyr}
    {\begingroup\fontencoding{T2A}\fontfamily{cmr}\fontseries{m}\fontshape{n}\selectfont}
    {\endgroup}
}{}%

\iftoggle{FULL}{%
  \usepackage{color}
  \newcommand{\blue}[1]{\textcolor{blue}{#1}} % perhaps for future use
  \newcommand{\bluebegin}{\begingroup\color{blue}}
  \newcommand{\blueend}{\endgroup}%
}{}%

\iftoggle{TR}{%
  \newtheorem{theorem}{Theorem}
  % \newtheorem{theorem}{Theorem}[section]  % to number theorems within sections
  \newtheorem{proposition}[theorem]{Proposition}
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{corollary}[theorem]{Corollary}

  \theoremstyle{definition}%[section]
  \newtheorem{definition}[theorem]{Definition}
  \newtheorem{example}[theorem]{Example}
  \newtheorem{problem}[theorem]{Open Problem}

  \theoremstyle{remark}
  \newtheorem{remark}[theorem]{Remark}
}{}%

\newcommand{\N}{\mathbb{N}}   % the natural numbers
\newcommand{\Z}{\mathbb{Z}}   % the integer numbers
\newcommand{\R}{\mathbb{R}}   % the real numbers

\newcommand{\dd}{\,\mathrm{d}}  % differential
\renewcommand{\d}{\mathrm{d}}   % differential for use in parentheses
  % the original \d: an accent (dot below the following letter)

\newcommand{\e}{\mathrm{e}}    % mathematical constant \approx 2.72

\renewcommand{\P}{\mathbb{P}}   % probability
\DeclareMathOperator{\E}{\mathbb{E}} % expectation
% \newcommand{\FFF}{\mathcal{F}} % sigma-algebra

\newcommand{\FFF}{\mathcal{F}}  % function class
\newcommand{\GGG}{\mathcal{G}}  % function class
\newcommand{\HHH}{\mathcal{H}}  % function class

\newcommand{\ER}{\mathcal{E}^{\mathrm{R}}} % randomness e-variables
\newcommand{\EX}{\mathcal{E}^{\mathrm{X}}}  % exchangeability e-variables
\newcommand{\PR}{\mathcal{P}^{\mathrm{R}}}  % randomness p-variables
\newcommand{\PX}{\mathcal{P}^{\mathrm{X}}}  % exchangeability p-variables
% Training-invariant versions:
\newcommand{\EtR}{\mathcal{E}^{\mathrm{tR}}} % training-invariant randomness e-variables
\newcommand{\EtX}{\mathcal{E}^{\mathrm{tX}}} % training-invariant exchangeability e-variables
\newcommand{\PtR}{\mathcal{P}^{\mathrm{tR}}} % training-invariant randomness p-variables
\newcommand{\PtX}{\mathcal{P}^{\mathrm{tX}}} % training-invariant exchangeability p-variables
% Invariant versions:
\newcommand{\EiR}{\mathcal{E}^{\mathrm{iR}}} % invariant randomness e-variables
\newcommand{\PiR}{\mathcal{P}^{\mathrm{iR}}} % invariant randomness p-variables
% On the log scale: just add \log.

% Derivative predictors:
\renewcommand{\i}{^{\mathrm{i}}}  % making invariant
  % original \i: dotless i
\renewcommand{\t}{^{\mathrm{t}}}  % making training-invariant
  % original \t: a kind of accent that I never use ("tie" over two letters)
\newcommand{\X}{^{\mathrm{X}}}   % making exchangeability
\newcommand{\tX}{^{\mathrm{tX}}} % making conformal

\newcommand{\EEE}{\mathcal{E}} % all variables

\DeclareMathOperator{\conf}{conf}  % the configuration mapping

% might be needed in future:
\newcommand{\subsetle}{\mathrel{\subseteq^{\scriptscriptstyle\le}}}
\newcommand{\subsetge}{\mathrel{\subseteq^{\scriptscriptstyle\ge}}}
% \newcommand{\supsetle}{\mathrel{\supseteq^{\scriptscriptstyle\le}}}  % not used
% \newcommand{\supsetge}{\mathrel{\supseteq^{\scriptscriptstyle\ge}}}  % not used
% \newcommand{\eqle}{\mathrel{=^{\scriptscriptstyle\le}}}  % not used
% \newcommand{\eqge}{\mathrel{=^{\scriptscriptstyle\ge}}}  % not used

\title{Set and functional prediction: randomness, exchangeability, and conformal}

\iftoggle{TR}{%
  \author{Vladimir Vovk}%
}{}%
\iftoggle{WP}{%
  \newcommand{\No}{43}
  % For the two dates option: uncomment the next 2 lines
  % \twodatestrue
  % \newcommand{\firstposted}{February 26, 2025}
  % First posted as an arXiv report: February 26, 2025 [planned]
  % First posted as an OCM Working Paper: February 26, 2025 [planned]
}{}%

\begin{document}
\maketitle
\begin{abstract}%
  This paper continues the study of the % predictive
  efficiency of conformal prediction
  as compared with more general randomness prediction and exchangeability prediction.
  It does not restrict itself to the case of classification,
  and our results will also be applicable to the case of regression.
  The price to pay is that efficiency will be attained only on average,
  albeit with respect to a wide range of probability measures on the label space.
  \iftoggle{arXiv}{%

     The version of this paper at \url{http://alrw.net} (Working Paper 43)
     is updated most often.%
  }{}%
  % \iftoggle{WP}{%
  %
  %   \clearpage % if the table of contents goes to the next page
  %
  % }{}%
\end{abstract}

\section{Introduction}
% \label{sec:introduction}

\iftoggle{FULL}{\bluebegin
  Confidence intervals are classical for p-values (introduced by Neyman \cite{Neyman:1934})
  and discussed in, e.g., \cite{Vovk/Wang:2023} in the case of e-values.
  I will often use the terminology introduced there.%
\blueend}{}%

Conformal prediction is usually presented as a method of \emph{set prediction}
\cite[Part~I]{Vovk/etal:2022book},
i.e., as a way of producing prediction sets (rather than point predictions).
Another way to look at a conformal predictor is as a way of producing a p-value function
(discussed, in a slightly different context, in, e.g., \cite{Fraser:2019}%
  \iftoggle{FULL}{\bluebegin,
    which is Fraser's last single-authored paper%
  \blueend}{}),
which is a function mapping each possible label $y$ of a test object
to the corresponding conformal p-value.
In analogy with ``prediction sets'', we will call such p-value functions
``prediction functions''.
The prediction set $\Gamma^{\alpha}$ corresponding to a prediction function $f$
and a significance level $\alpha\in(0,1)$
(our target probability of error)
is the set of all labels $y$ such that $f(y)>\alpha$.
A standard property of validity for conformal predictors
is that $\Gamma^{\alpha}$ makes an error (fails to cover the true label)
with probability at most $\alpha$;
it is implied by the conformal p-values being bona fide p-values
(under suitable assumptions, such as data exchangeability).

The most standard assumption in machine learning is that of \emph{randomness}
(i.e., the data are assumed to be produced in the IID fashion).
This paper is a development of \cite{Vovk:arXiv2501},
which introduces the most general class of predictors,
``randomness predictors'',
that produce prediction functions that are valid,
in the same sense as conformal predictors,
under the assumption of randomness.
There are many more randomness predictors than conformal predictors,
and an interesting question is whether there are randomness predictors
that have significant advantages (e.g., in practice) over conformal predictors.
This question was answered (albeit imperfectly) in \cite{Vovk:arXiv2501}
in the case of classification with few (such as two) classes.
In this paper we will be interested in arbitrary label spaces,
including the case of regression.
The message of this paper is similar to that of \cite{Vovk:arXiv2501}:
the difference between conformal and randomness prediction is not huge,
but it remains an open question whether it can be usefully exploited.
This paper strengthens most of the positive results in \cite{Vovk:arXiv2501},
but its only negative result is much weaker (but also simpler)
than the negative results of \cite{Vovk:arXiv2501}.

A useful technical tool in establishing connections
between conformal and randomness predictors
is provided by conformal e-predictors \cite{OCM26-local},
which are obtained by replacing p-values with e-values.
Conformal e-predictors output e-value functions $f$ as their prediction functions.
Such functions $f$ can also be represented in terms of the corresponding prediction sets
$\Gamma^{\alpha}:=\{y\mid f(y)<\alpha\}$,
where $\alpha\in(0,\infty)$ is the significance level
(notice that now we exclude the labels with large e-values from the prediction set,
which is opposite to what we did for p-values).
However, the property of validity of conformal e-predictors
is slightly more difficult to state in terms of prediction sets:
now validity means that the integral of the probability of error for $\Gamma^{\alpha}$
over $\alpha\in(0,\infty)$ does not exceed 1
\cite[end of Appendix~B]{OCM26-local}.
This implies that the probability of error for $\Gamma^{\alpha}$
is at most $1/\alpha$,
but this simple derivative property of validity is much weaker.

Conformal e-predictors are not only a useful technical tool,
but we can also use them for prediction directly.
In Glenn Shafer's opinion \cite{Shafer:2021},
e-values are even more intuitive than p-values.
Because of the importance of e-predictors,
in the rest of this paper we will use the word ``predictor''
in combinations such as ``conformal predictor'' and ``randomness predictor''
generically, including both p-predictors (standard predictors based on p-values)
and e-predictors (predictors based on e-values);
in particular, we will never drop ``p-'' in ``p-predictor''.

In this paper we will follow the scheme
for establishing the closeness of conformal and randomness predictors
used in \cite[Figure~1]{Vovk:arXiv2501}.
Namely, we will establish connections between:
\begin{enumerate}[label=(\alph*)] % [(a)]
\item\label{it:1}
  conformal p-predictors and conformal e-predictors;
\item\label{it:2}
  conformal e-predictors and exchangeability e-predictors,
  where the class of exchangeability e-predictors
  is intermediate between the conformal e-predictors and the randomness e-predictors,
  and it consists of randomness e-predictors that are valid under the assumption of exchangeability
  (which is weaker than the assumption of randomness in the case of finite data sequences);
\item\label{it:3}
  exchangeability e-predictors and randomness e-predictors;
\item\label{it:4}
  randomness e-predictors and randomness p-predictors.
\end{enumerate}
Steps \ref{it:1} and \ref{it:4}
(converting p-values to e-values and back)
are standard,
and we will mainly concentrate on steps \ref{it:2} and \ref{it:3}.

We start in Sect.~\ref{sec:definitions} from the main definitions,
and Sect.~\ref{sec:results} is devoted to the main results.
In particular, we establish the efficiency of conformal predictors among randomness predictors
in both p- and e-versions.
Namely, the prediction functions for conformal predictors turn out to be competitive on average
with the prediction functions for any randomness predictors,
where ``on average'' refers to an arbitrary probability measure that can depend on the test example.
Sections~\ref{sec:classification} and~\ref{sec:regression} give some applications
to classification and regression, respectively,
and Sect.~\ref{sec:conclusion} concludes.

\iftoggle{FULL}{\bluebegin
  Conformal p-prediction intervals appeared at about the same time
  as conformal p-prediction in general
  (see, e.g., \cite{Nouretdinov/etal:2001ICML}). % Ridge Regression Confidence Machine
  Their e-version has never been developed (probably) in the case of regression,
  although the work on conformal e-prediction in the case of classification
  was developed in \cite{Vovk:arXiv2001}.%
\blueend}{}%

\section{Definitions}
\label{sec:definitions}

This paper is about the following prediction problem
(continuing the discussion started in \cite{Vovk:arXiv2501}).
We are given a training sequence of \emph{examples} $z_i=(x_i,y_i)$,
$i=1,\dots,n$ for a fixed $n$,
each consisting of an \emph{object} $x_i$ and its \emph{label} $y_i$,
and a new test object $x_{n+1}$;
the task is to predict $x_{n+1}$'s label $y_{n+1}$.
A potential label $y$ for $x_{n+1}$ is \emph{true} if $y=y_{n+1}$
and \emph{false} otherwise.
The objects are drawn from a non-empty measurable space $\mathbf{X}$,
the \emph{object space},
and the labels from the \emph{label space} $\mathbf{Y}$,
which is assumed to be a non-trivial measurable space
(meaning that the $\sigma$-algebra on it
is different from $\{\emptyset,\mathbf{Y}\}$).
\iftoggle{FULL}{\bluebegin
  Later in the paper we will consider the case of regression,
  where the label space is the real line, $\mathbf{Y}=\R$.
  This is unlike \cite{Vovk:arXiv2501},
  where the label space is finite (and small).%
\blueend}{}%

A measurable function $P:\mathbf{Z}^{n+1}\to[0,1]$
is a \emph{randomness p-variable} if,
for any probability measure $Q$ on $\mathbf{Z}$
and any \emph{significance level} $\alpha\in(0,1)$,
$Q^{n+1}(\{P\le\alpha\})\le\alpha$.
\iftoggle{FULL}{\bluebegin
  Such a function $P$ is an \emph{exchangeability p-variable}
  if $R(\{P\le\alpha\})\le\alpha$
  for any exchangeable probability measure $R$ on $\mathbf{Z}^{n+1}$
  and any $\alpha\in(0,1)$.
\blueend}{}%
And a  measurable $P:\mathbf{Z}^{n+1}\to[0,1]$
is a \emph{conformal p-variable} if
\begin{itemize}
\item
  $R(\{P\le\alpha\})\le\alpha$
  for any exchangeable probability measure $R$ on $\mathbf{Z}^{n+1}$
  and any $\alpha\in(0,1)$;
\item
  it is \emph{training-invariant},
  i.e., invariant w.r.\ to permutations of the training examples:
  \begin{equation}\label{eq:p-training-invariance}
    P(z_{\sigma(1)},\dots,z_{\sigma(n)},z_{n+1})
    =
    P(z_{1},\dots,z_n,z_{n+1})
  \end{equation}
  for each data sequence $z_1,\dots,z_{n+1}$
  and each permutation $\sigma$ of $\{1,\dots,n\}$
  (training-invariant functions were called simply invariant
  in \cite{Vovk:arXiv2501}).
\end{itemize}
We will sometimes refer to the values taken by p-variables
as \emph{p-values},
and our notation for the classes of all randomness and conformal p-variables
will be $\PR$ and $\PtX$, respectively.

Conformal p-variables can be used for prediction,
and we will also refer to them as \emph{conformal p-predictors}.
Notice that the standard expression ``training set'' is only justified
for predictors $P$ satisfying \eqref{eq:p-training-invariance}
(and even in this case it is not justified completely;
it would be more accurate to say ``training bag'').
There are several ways to package the output of conformal p-predictors.
One is in terms of set prediction:
for each significance level $\alpha\in(0,1)$,
each training sequence $z_1,\dots,z_n$, and each test object $x_{n+1}$,
we can output the \emph{prediction set}
\begin{equation}\label{eq:Gamma}
  \Gamma^{\alpha}
  :=
  \{
    y\in\mathbf{Y}
    \mid
    P(z_1,\dots,z_n,x_{n+1},y)
    >
    \alpha
  \}.
\end{equation}
By the definition of a conformal p-variable,
under the assumption of exchangeability,
the probability that a conformal p-predictor makes an error
at significance level $\alpha$,
i.e., the probability of $y_{n+1}\notin\Gamma^{\alpha}$,
is at most $\alpha$.
See \cite[Sect.~2]{Vovk:arXiv2501} for a more detailed discussion
of connections between conformal p-variables and conformal p-prediction.

Instead of predicting with one prediction set
in the family \eqref{eq:Gamma},
we can package our prediction as the \emph{prediction function}
\begin{equation}\label{eq:p-f}
  f(y)
  :=
  P(z_1,\dots,z_n,x_{n+1},y),
  \quad
  y\in\mathbf{Y}.
\end{equation}
We may refer to this mode of prediction as \emph{functional prediction}.
The step from set prediction to functional prediction
is analogous from the step from confidence intervals to p-value functions
(see, e.g., \cite[Sect.~9]{Miettinen:1985} and \cite{Fraser:2017,Fraser:2019,Infanger/Schmidt:2019}
for the latter).

\begin{remark}
  The term ``functional prediction'' is a straightforward modification of ``set prediction''
  % (which is functional prediction with prediction functions being indicator functions)
  and ``p-value function'',
  but its disadvantage is that it is easy to confuse with function prediction,
  namely predicting a function
  (e.g., a biological function, such as that of a protein, or a mathematical function).
  \iftoggle{FULL}{\bluebegin
    However, there will be no ambiguity in the context of this paper.%
  \blueend}{}%
\end{remark}

Similarly, we can use randomness p-variables for prediction,
and then we refer to them as \emph{randomness p-predictors}.
By definition,
the probability that the prediction set \eqref{eq:Gamma}
derived from a randomness p-predictor
makes an error is at most $\alpha$,
this time under the assumption of randomness.
We will use the prediction functions \eqref{eq:p-f}
for randomness p-predictors as well.

Two important desiderata for conformal and randomness predictors
are their validity and efficiency.
In terms of the prediction function $f$,
validity concerns the value $f(y_{n+1})$ at the true label
(its typical values should not be too small in p-prediction),
and efficiency concerns the values $f(y)$ at the false labels $y\ne y_{n+1}$
(they should be as small as possible in p-prediction).
Validity is automatic under randomness (and even exchangeability for conformal predictors),
and in this paper we are interested in the efficiency of conformal predictors
relative to other randomness predictors.
Later in the paper (Theorems~\ref{thm:e-main} and~\ref{thm:p-main} below)
we will establish efficiency guarantees
for conformal prediction in terms of randomness prediction.

A nonnegative measurable function $E:\mathbf{Z}^{n+1}\to[0,\infty]$
is a \emph{randomness e-variable} if $\int E\dd Q^{n+1}\le1$
for any probability measure $Q$ on $\mathbf{Z}$.
It is an \emph{exchangeability e-variable} if $\int E\dd R\le1$
for any exchangeable probability measure $R$ on $\mathbf{Z}^{n+1}$.
We will denote the classes of all randomness and exchangeability e-variables
by $\ER$ and $\EX$, respectively.
The class of all measurable functions $E:\mathbf{Z}\to[0,\infty$
is denoted by $\EEE$.

The class $\EtX$ of \emph{conformal e-variables} consists of all functions $E\in\EX$
that are training-invariant:
\begin{equation}\label{eq:e-training-invariant}
  E(z_{\sigma(1)},\dots,z_{\sigma(n)},z_{n+1})
  =
  E(z_{1},\dots,z_n,z_{n+1})
\end{equation}
for each data sequence $z_1,\dots,z_{n+1}$
and each permutation $\sigma$ of $\{1,\dots,n\}$.
% let us call such e-variables \emph{training-invariant e-variables}
We often regard the randomness e-variables $E\in\ER$
as \emph{randomness e-predictors}
and conformal e-variables $E\in\EtX$
as \emph{conformal e-predictors}.
Similarly to \eqref{eq:p-f},
they output prediction functions
\begin{equation*} % \label{eq:e-f}
  f(y)
  :=
  E(z_1,\dots,z_n,x_{n+1},y),
  \quad
  y\in\mathbf{Y}.
\end{equation*}
For conformal and randomness e-predictors,
validity and efficiency change direction:
for validity, typical values $f(y_{n+1})$ should not be too large,
and for efficiency typical values $f(y)$ at the false labels $y\ne y_{n+1}$
should be as large as possible.
Again validity is automatic under randomness,
and Theorem~\ref{thm:e-main} below establishes
efficiency guarantees for conformal e-prediction in terms of randomness e-prediction.

We will also need two important subclasses of $\ER$.
The subclass $\EtR$ of $\ER$ consists of all functions $E\in\ER$
that are training-invariant (i.e., satisfy \eqref{eq:e-training-invariant}).
The subclass $\EiR$ of $\ER$ consists of all functions $E\in\ER$
that are invariant w.r.\ to all permutations:
\[
  E(z_{\pi(1)},\dots,z_{\pi(n+1)})
  =
  E(z_{1},\dots,z_{n+1})
\]
for each permutation $\pi$ of $\{1,\dots,n+1\}$;
let us call such randomness e-variables \emph{invariant}
(this is almost the same thing as \emph{configuration randomness e-variables}
in \cite{Vovk:arXiv2501}).

A big advantage of e-variables over p-variables is that the average of e-variables
is again an e-variable.
This allows us to define, given an e-variable $E\in\ER$,
three derivative e-variables:
\begin{align}
  E\i(z_1,\dots,z_{n+1})
  &:=
  \frac{1}{(n+1)!}
  \sum_{\pi}
  E(z_{\pi(1)},\dots,z_{\pi(n+1)}),
  \label{eq:i}\\
  E\X(z_1,\dots,z_{n+1})
  &:=
  \frac{E(z_1,\dots,z_{n+1})}{E\i(z_1,\dots,z_{n+1})},
  \label{eq:X}\\
  E\t(z_1,\dots,z_{n+1})
  &:=
  \frac{1}{n!}
  \sum_{\sigma}
  E(z_{\sigma(1)},\dots,z_{\sigma(n)},z_{n+1}),
  \label{eq:t}
\end{align}
$\pi$ ranging over the permutations of $\{1,\dots,n+1\}$
and $\sigma$ ranging over the permutations of $\{1,\dots,n\}$.
It is clear that $E\i\in\EiR$ whenever $E\in\ER$,
that $E\X\in\EX$ for all $E\in\EEE$,
and that $E\t\in\EtX$ whenever $E\in\EX$.
The operators~\eqref{eq:i} and~\eqref{eq:t} are kinds of averaging:
while $E\mapsto E\i$ averages over all permutations of an input data sequence
(including both training and test examples),
$E\mapsto E\t$ averages over the permutations of the training sequence only.

Using two of these three operators, we can turn any randomness e-variable $E$
to an exchangeability e-variable $E\X$
to a conformal e-variable $(E\X)\t$.
The following lemma shows that the order
in which the last two operators are applied does not matter.

\begin{lemma}\label{lem:commute}
  The operators ${}\t$ and ${}\X$ commute:
  for any $E\in\EEE$, $(E\t)\X=(E\X)\t$.
\end{lemma}

\begin{proof}
  Let us fix a data sequence $z_1,\dots,z_{n+1}$
  and check $(E\t)\X(z_1,\dots,z_{n+1})=(E\X)\t(z_1,\dots,z_{n+1})$.
  As functions of a permutation of $z_1,\dots,z_{n+1}$,
  $E$ and $E\X$ are proportional to each other,
  and therefore, $E\t$ and $(E\X)\t$ are also proportional to each other.
  This implies $(E\t)\X=(E\X)\t$ on the permutations of $z_1,\dots,z_{n+1}$.
  And this is true for each $(z_1,\dots,z_{n+1})$.
\end{proof}

\noindent
We will let $\tX$ stand for the composition of the two operators:
\[
  E\tX
  :=
  (E\t)\X
  =
  (E\X)\t.
\]

It is easy to see that $E\in\EEE$ belongs to $\EX$ if and only if,
for any data sequence $z_1,\dots,z_{n+1}$,
\begin{equation}\label{eq:EX}
  \frac{1}{(n+1)!}
  \sum_{\pi}
  E(z_{\pi(1)},\dots,z_{\pi(n+1)})
  \le
  1,
\end{equation}
$\pi$ ranging over the permutations of $\{1,\dots,n+1\}$.
Let us say that such an $E$ is \emph{admissible}
if \eqref{eq:EX} always holds with ``$=$'' in place of ``$\le$''.
(This agrees with the standard notion of admissibility
in statistical decision theory.)

The intuition (which can be formalized easily)
behind the operators that we have just introduced is that:
\begin{itemize}
\item
  ${}\i$ projects $\ER$ onto $\EiR$;
  it also projects the admissible part of $\EX$ onto the identical 1;
\item
  ${}\X$ projects $\EEE$ onto the admissible part of $\EX$;
\item
  ${}\t$ projects $\EX$ onto $\EtX$;
\item
  ${}\tX$ projects $\EEE$ onto the admissible part of $\EtX$.
\end{itemize}
In particular, these operators are idempotent:
\[
  (E\i)\i = E\i,
  \quad
  (E\X)\X = E\X,
  \quad
  (E\t)\t = E\t,
  \quad
  (E\tX)\tX = E\tX.
\]
Despite these operators being projections,
we cannot claim that these ways of moving between different function classes
are always optimal.

Lemma~\ref{lem:commute} lists the only two cases
where the combination of two of our three basic operators
(${}\i$, ${}\X$, and ${}\t$)
gives something interesting.
The other four cases are:
\[
  (E\X)\i = (E\i)\X = 1,
  \quad
  (E\i)\t = (E\t)\i = E\i.
\]

\section{Main results}
\label{sec:results}

Let $B$ be a Markov kernel with source $\mathbf{Z}$ and target $\mathbf{Y}$,
which we will write in the form $B:\mathbf{Z}\hookrightarrow\mathbf{Y}$
(as in \cite[Sect.~A.4]{Vovk/etal:2022book}).
We will write $B(A\mid z)$ for its value on $z\in\mathbf{Z}$ and $A\subseteq\mathbf{Y}$
(where $A$ is measurable),
and we will write $\int f(y)B(\d y\mid z)$ for the integral of a function $f$ on $\mathbf{Y}$
w.r.\ to the measure $A\mapsto B(A\mid z)$.
We will show that the efficiency of various predictors
(such as the conformal predictor)
derived from a randomness predictor $E$
is not much worse than the efficiency of the original randomness predictor $E$
on average,
and $B$ will define the meaning of ``on average''.

\subsection{Kolmogorov's step}

The following statement shows that the efficiency
does not suffer much on average when we move from randomness e-prediction
to exchangeability e-prediction.
It is a counterpart of Corollary~5 in \cite{Vovk:arXiv2501}
(with a similar proof).

\begin{theorem}\label{thm:Kolmogorov}
  Let $B:\mathbf{Z}\hookrightarrow\mathbf{Y}$ be a Markov kernel.
  For each randomness e\-/predictor $E$,
  \begin{equation}\label{eq:thm-Kolmogorov}
    G(z_1,\dots,z_n,z_{n+1})
    :=
    \e^{-1}
    \int
    \frac
      {E(z_1,\dots,z_n,x_{n+1},y)}
      {E\X(z_1,\dots,z_n,x_{n+1},y)}
    B(\d y\mid z_{n+1})
  \end{equation}
  (with $0/0$ interpreted as 0 and $z_{n+1}$ represented as $(x_{n+1},y_{n+1})$)
  is a randomness e-variable.
\end{theorem}

We can interpret \eqref{eq:thm-Kolmogorov} as a statement
that $E\X$ is almost as efficient as $E$:
the mean ratio of the degree to which $E$ rejects a false label $y$
to the degree to which $E\X$ rejects $y$
is bounded by $\e$ under any probability measure
that may depend on the test example.
This will be further discussed after we state our main result,
Theorem~\ref{thm:p-main}.

\begin{proof}[Proof of Theorem~\ref{thm:Kolmogorov}]
  We will define $G$ as $G_2G_3$,
  where $G_2\in\EiR$ and $G_3\in\EX$
  (it is obvious that these two inclusions will imply $G\in\ER$).
  First we define an approximation $G_1$ to $G_2$ as
  \[
    G_1(z_1,\dots,z_{n+1})
    :=
    \frac{1}{n+1}
    \sum_{i=1}^{n+1}
    \int E\i(z_1,\dots,z_{i-1},x_i,y,z_{i+1},\dots,z_{n+1})
    B(\dd y\mid z_i).
  \]
  In other words, $G_1(z_1,\dots,z_{n+1})$ is obtained
  by randomly (with equal probabilities)
  choosing an example $z_i$ in the data sequence $z_1,\dots,z_{n+1}$,
  replacing its label $y_i$ by a random label $y\sim B(\cdot\mid z_i)$,
  and finding the expectation of $E\i$ on $z_1,\dots,z_{n+1}$ modified in this way.
  We can see that $G_1$ is invariant,
  but it does not have to be in $\EiR$.
  The invariant randomness e-variable $G_2$ is defined similarly,
  except that now we replace each label $y_i$, $i=1,\dots,n+1$,
  by a random label $y\sim B(\cdot\mid z_i)$
  with probability $\frac{1}{n+1}$ (all independently).
  The key observation is that $G_2/G_1\ge1/\e$,
  which follows from the probability that exactly one label will be changed
  in the construction of $G_2$ being
  \[
    (n+1)
    \frac{1}{n+1}
    \left(\frac{n}{n+1}\right)^n
    \ge
    1/\e.
  \]
  Finally, $G_3\in\EX$ is defined by
  \[
    G_3(z_1,\dots,z_{n+1})
    :=
    \frac
      {\int E\i(z_1,\dots,z_n,x_{n+1},y) B(\d y\mid z_{n+1})}
      {G_1(z_1,\dots,z_{n+1})}.
  \]
  Combining all these statements,
  we get
  \begin{align*}
    G(z_1,\dots,z_{n+1})
    &=
    G_2(z_1,\dots,z_{n+1})
    G_3(z_1,\dots,z_{n+1})\\
    &\ge
    \frac{1}{\e}
    G_1(z_1,\dots,z_{n+1})
    G_3(z_1,\dots,z_{n+1})\\
    &=
    \int E\i(z_1,\dots,z_n,x_{n+1},y) B(\d y\mid z_{n+1}).
  \end{align*}
  By the definition \eqref{eq:X},
  this is equivalent to \eqref{eq:thm-Kolmogorov}.
\end{proof}

Notice that Theorem~\ref{thm:Kolmogorov} does not assume
the homoscedasticity of the labels.
The simplest informative examples, however, are indeed homoscedastic:
in them, for each $z=(x,y)\in\mathbf{Z}$,
$B(\cdot\mid z)$ is the distribution of $y+\xi$ for a given random variable $\xi$.
In general, however, the distribution of $\xi$ may depend on the object $x$.

The following result is a simple inverse to Theorem~\ref{thm:Kolmogorov}.

\begin{theorem}\label{thm:anti-Kolmogorov}
  The constant $\e^{-1}$ in Theorem~\ref{thm:Kolmogorov}
  cannot be replaced by a larger one.
\end{theorem}

\begin{proof}
  In this proof we follow the example in \cite[Sect.~B.1]{Vovk:arXiv2501}
  % OCM WP 42
  (the example in \cite{Vovk:arXiv2501} is informal,
  and here we formalize it).
  Without loss of generality we assume $\left|\mathbf{X}\right|=1$
  (so that the objects become uninformative and we can omit them from our notation)
  and $\mathbf{Y}=\{0,1\}$ (with the discrete $\sigma$-algebra).
  \iftoggle{FULL}{\bluebegin
    We only use a 1-element part of $\mathbf{X}$
    and a 2-element part of $\mathbf{Y}$.
  \blueend}{}%
  Define a randomness e-variable $E$ by
  \begin{equation}\label{eq:E-simple}
    E(y_1,\dots,y_{n+1})
    :=
    \begin{cases}
      \left(
        1-\frac{1}{n+1}
      \right)^{-n} & \text{if $k=1$}\\
      0 & \text{if not},
    \end{cases}
  \end{equation}
  where $k$ is the number of 1s in $y_1,\dots,y_{n+1}$.
  This is indeed a randomness e-variable,
  since the maximum probability of $k=1$ in the Bernoulli model,
  $(n+1)p(1-p)^n\to\max$,
  is attained at $p=\frac{1}{n+1}$.
  The corresponding exchangeability e-variable is
  \[
    E\X(y_1,\dots,y_{n+1})
    =
    \begin{cases}
      1 & \text{if $k=1$}\\
      0 & \text{if not}.
    \end{cases}.
  \]
  Let $B$ just flip the label: $B(\{1-y\}\mid y)=1$.
  Suppose Theorem~\ref{thm:Kolmogorov} holds with the $\e^{-1}$ in \eqref{eq:thm-Kolmogorov}
  replaced by $c>\e^{-1}$.
  Then the randomness e-variable $G$ satisfies
  \[
    G(0,\dots,0)
    =
    c
    \left(
      1-\frac{1}{n+1}
    \right)^{-n}
    \sim
    c\e
    >
    1,
  \]
  which is impossible for a large enough $n$
  (since the probability measure concentrated on $(0,\dots,0)$
  is of the form $Q^{n+1}$).
\end{proof}

\begin{remark}
  Whereas the randomness e-variable $E$ defined by \eqref{eq:E-simple}
  is all we need to prove Theorem~\ref{thm:anti-Kolmogorov},
  it is not useful for prediction.
  A variation on \eqref{eq:E-simple} that can be used in prediction is
  \begin{equation*}
    E(y_1,\dots,y_{n+1})
    :=
    \begin{cases}
      (n+1)
      \left(
        1-\frac{1}{n+1}
      \right)^{-n} & \text{if $(y_1,\dots,y_n,y_{n+1})=(0,\dots,0,1)$}\\
      0 & \text{if not}.
    \end{cases}
  \end{equation*}
  According to this randomness e-predictor,
  after observing $n$ 0s in a row,
  we are likely to see 0 rather than 1.
  This is a version of Laplace's rule of succession.
  While under randomness we have $E(0,\dots,0,1)\sim\e n$,
  under exchangeability we can only achieve $E\X(0,\dots,0,1)=n+1\sim n$.
\end{remark}

\subsection{Training-invariance step}

To state our result in its strongest form,
we define a \emph{test-conditional exchangeability e-variable}
$G=G(z_1,\dots,z_n,z_{n+1})$
as an element of $\EEE$ satisfying
\[
  \forall(z_1,\dots,z_{n+1}) \; \forall\sigma:
  \frac{1}{n!}
  \sum_{\sigma}
  G(z_{\sigma(1)},\dots,z_{\sigma(n)},z_{n+1})
  \le
  1,
\]
$\sigma$ ranging over the permutations of $\{1,\dots,n\}$.
Such $G$ form a subclass of $\EX$.

\begin{theorem}\label{thm:training-invariance}
  Let $B:\mathbf{Z}\hookrightarrow\mathbf{Y}$ be a Markov kernel.
  For each exchangeability e-predictor $E$,
  \begin{equation}\label{eq:thm-training-invariance}
    G(z_1,\dots,z_n,z_{n+1})
    :=
    \int
    \frac
      {E(z_1,\dots,z_n,x_{n+1},y)}
      {E\t(z_1,\dots,z_n,x_{n+1},y)}
    B(\d y\mid z_{n+1})
  \end{equation}
  (with $0/0$ interpreted as 0)
  is a test-conditional exchangeability e-variable.
\end{theorem}

The interpretation of \eqref{eq:thm-training-invariance}
is similar to that of \eqref{eq:thm-Kolmogorov}.

\begin{proof}[Proof of Theorem~\ref{thm:training-invariance}]
  It suffices to check that the right-hand side of \eqref{eq:thm-training-invariance}
  is a test-conditional exchangeability e-variable.
  We have:
  \begin{multline*}
    \frac{1}{n!}
    \sum_{\sigma}
    G(z_{\sigma(1)},\dots,z_{\sigma(n)},z_{n+1})\\
    =
    \frac{1}{n!}
    \sum_{\sigma}
    \int
    \frac
      {E(z_{\sigma(1)},\dots,z_{\sigma(n)},x_{n+1},y)}
      {E\t(z_1,\dots,z_n,x_{n+1},y)}
    B(\d y\mid z_{n+1})\\
    =
    \int
    \frac
      {E\t(z_1,\dots,z_n,x_{n+1},y)}
      {E\t(z_1,\dots,z_n,x_{n+1},y)}
    B(\d y\mid z_{n+1})
    \le
    1
    \iftoggle{FULL}{}{.\qedhere}
  \end{multline*}
  \iftoggle{FULL}{\bluebegin
    (where we have ``${}\le1$'' rather than ``${}=1$''
    because of the possibility of $0/0$).%
  \blueend}{}%
\end{proof}

\subsection{Putting everything together}

The following theorem combines Theorems~\ref{thm:Kolmogorov} and~\ref{thm:training-invariance}
and establishes a connection between randomness and conformal e-predictors.
Remember that the conformal e-predictor $\EtX$ derived from a randomness e-predictor $E$
is obtained by combining the operators \eqref{eq:X} and \eqref{eq:t},
i.e., as
\begin{equation}\label{eq:tX}
  E\tX(z_1,\dots,z_{n+1})
  :=
  (n+1)
  \frac
    {\sum_{\sigma}E(z_{\sigma(1)},\dots,z_{\sigma(n)},z_{n+1})}
    {\sum_{\pi}E(z_{\pi(1)},\dots,z_{\pi(n+1)})},
\end{equation}
$\sigma$ and $\pi$ ranging over the permutations
of $\{1,\dots,n\}$ and $\{1,\dots,n+1\}$, respectively.

\begin{theorem}\label{thm:e-main}
  Let $B:\mathbf{Z}\hookrightarrow\mathbf{Y}$ be a Markov kernel.
  For each randomness e-predictor $E$,
  \begin{equation}\label{eq:thm-e-main}
    G(z_1,\dots,z_n,z_{n+1})
    :=
    \e^{-1/2}
    \int
    \sqrt{\frac
      {E(z_1,\dots,z_n,x_{n+1},y)}
      {E\tX(z_1,\dots,z_n,x_{n+1},y)}}\;
    B(\d y\mid z_{n+1})
  \end{equation}
  % (with $0/0$ interpreted as 0)
  is a randomness e-variable.
\end{theorem}

Theorem~\ref{thm:e-main} is the main result of this paper for e-predictors.
Its main weakness is the presence of the term $\e^{-1/2}$,
but it might be inevitable.

\begin{proof}
  Applying the Cauchy--Schwarz inequality, we have, for some $G_1,G_2,G\in\ER$,
  \begin{multline*}
    \e^{-1/2}
    \int
    \sqrt{\frac
      {E(z_1,\dots,z_n,x_{n+1},y)}
      {E\tX(z_1,\dots,z_n,x_{n+1},y)}}\;
    B(\d y\mid z_{n+1})\\
    =
    \e^{-1/2}
    \int
    \sqrt{\frac
      {E(z_1,\dots,z_n,x_{n+1},y)}
      {E\X(z_1,\dots,z_n,x_{n+1},y)}}
    \sqrt{\frac
      {E\X(z_1,\dots,z_n,x_{n+1},y)}
      {E\tX(z_1,\dots,z_n,x_{n+1},y)}}\;
    B(\d y\mid z_{n+1})\\
    \le
    \sqrt{
      \e^{-1}
      \int
      \frac
        {E(z_1,\dots,z_n,x_{n+1},y)}
        {E\X(z_1,\dots,z_n,x_{n+1},y)}\;
      B(\d y\mid z_{n+1})}\\
    \quad{}\times
    \sqrt{
      \int
      \frac
        {E\X(z_1,\dots,z_n,x_{n+1},y)}
        {E\tX(z_1,\dots,z_n,x_{n+1},y)}
      B(\d y\mid z_{n+1})
    }\\
    =
    \sqrt{G_1(z_1,\dots,z_{n+1})G_2(z_1,\dots,z_{n+1})}
    \le
    G(z_1,\dots,z_{n+1})
  \end{multline*}
  (the existence of $G_1$ and $G_2$ follows
  from Theorems~\ref{thm:Kolmogorov} and~\ref{thm:training-invariance},
  respectively).
\end{proof}

It is known that, for any $\delta\in(0,1)$,
the function $p\mapsto\delta p^{\delta-1}$
transforms p-values to e-values
and that the function $e\mapsto e^{-1}$
transforms e-values to p-values
(see, e.g., \cite[Propositions~2.1 and~2,2]{Vovk/Wang:2021}).
This allows us to adapt Theorem~\ref{thm:p-main} to p-predictors.

\begin{theorem}\label{thm:p-main}
  Let $B:\mathbf{Z}\hookrightarrow\mathbf{Y}$ be a Markov kernel
  and let $\delta\in(0,1)$.
  For each randomness p-predictor $P$
  there exists a conformal p-predictor $P'$ such that
  \begin{multline}\label{eq:thm-p-main}
    G(z_1,\dots,z_n,z_{n+1})
    :=
    (\delta/\e)^{1/2}\\
    \times\int
    \sqrt{P(z_1,\dots,z_n,x_{n+1},y)^{\delta-1}
      P'(z_1,\dots,z_n,x_{n+1},y)}
    \:
    B(\d y\mid z_{n+1})
  \end{multline}
  is a randomness e-variable.
\end{theorem}

The interpretation of \eqref{eq:thm-p-main} is that
$P'(z_1,\dots,z_n,x_{n+1},y)$ is typically small
(perhaps not to the same degree)
when $P(z_1,\dots,z_n,x_{n+1},y)$ is small;
i.e., we do not lose much in efficiency
when converting randomness p-predictors to conformal p-predictors.
To see this, fix small $\epsilon_1,\epsilon_2\in(0,1)$.
Then we will have $G(z_1,\dots,z_n,z_{n+1})<1/\epsilon_1$
for the true data sequence $z_1,\dots,z_n,z_{n+1}$
unless a rare event (of probability at most $\epsilon_1$) happens.
For the vast majority of the potential labels $y\in\mathbf{Y}$ we will have
\begin{equation}\label{eq:bounded}
  (\delta/\e)^{1/2}
  \sqrt{P(z_1,\dots,z_n,x_{n+1},y)^{\delta-1}
    P'(z_1,\dots,z_n,x_{n+1},y)}
  <
  \frac{1}{\epsilon_1\epsilon_2},
\end{equation}
where ``the vast majority'' means that the $B(\cdot\mid z_{n+1})$ measure
of the $y$ satisfying \eqref{eq:bounded} is at least $1-\epsilon_2$.
We can rewrite \eqref{eq:bounded} as
\[
  P'(z_1,\dots,z_n,x_{n+1},y)
  <
  \frac{\e P(z_1,\dots,z_n,x_{n+1},y)^{1-\delta}}
    {\delta\epsilon_1^2\epsilon_2^2},
\]
so that $P'(z_1,\dots,z_n,x_{n+1},y)\to0$ as $P(z_1,\dots,z_n,x_{n+1},y)\to0$.
This is, of course, true for any Markov kernel $B$.

\begin{proof}[Proof of Theorem~\ref{thm:p-main}]
  Fix $\delta\in(0,1)$ and $P\in\PR$.
  Set $E:=\delta P^{\delta-1}$ and $P':=1/E\tX$,
  so that $E\in\ER$ and $P'\in\PtX$.
  According to \eqref{eq:thm-e-main},
  \begin{multline*}
    G(z_1,\dots,z_n,z_{n+1})
    :=
    \e^{-1/2}
    \int
    \sqrt{\frac
      {E(z_1,\dots,z_n,x_{n+1},y)}
      {E\tX(z_1,\dots,z_n,x_{n+1},y)}}\;
    B(\d y\mid z_{n+1})\\
    =
    \e^{-1/2}
    \int
    \sqrt{\delta P(z_1,\dots,z_n,x_{n+1},y)^{\delta-1}
      P'(z_1,\dots,z_n,x_{n+1},y)}
    B(\d y\mid z_{n+1})
  \end{multline*}
  is a randomness e-variable.
\end{proof}

\section{Applications to classification}
\label{sec:classification}

In this and next sections we will discuss some interesting examples
of Markov kernels $B$ as used in Theorems~\ref{thm:e-main} and~\ref{thm:p-main}.
In this section we discuss the case of classification, $\left|\mathbf{Y}\right|<\infty$,
which was also discussed earlier in \cite{Vovk:arXiv2501}.

Let us start from binary classification, $\mathbf{Y}:=\{0,1\}$.
In this case, the most natural choice of $B$ is $B(\{y\}\mid(x,y)):=0$,
so that the Markov kernel sends every example $(x,y)$ to the other label $1-y$.
We can rewrite \eqref{eq:thm-e-main} as
\begin{equation}\label{eq:binary}
  G(z_1,\dots,z_n,z_{n+1})
  :=
  \e^{-1/2}
  \sqrt{\frac
    {E(z_1,\dots,z_n,x_{n+1},1-y_{n+1})}
    {E\tX(z_1,\dots,z_n,x_{n+1},1-y_{n+1})}},
\end{equation}
which does not involve any averaging.
We can interpret~\eqref{eq:binary} as the conformal e-predictor $E\tX$
being almost as efficient as the original randomness e-predictor $E$,
where efficiency is measured by the degree to which we reject the false label $1-y_{n+1}$.
For example, for a small positive constant $\epsilon$,
$G\ge1/\epsilon$ with probability at most $\epsilon$,
and so
\begin{equation*}
  E\tX(z_1,\dots,z_n,x_{n+1},1-y_{n+1})
  >
  \e^{-1} \epsilon^2 E(z_1,\dots,z_n,x_{n+1},1-y_{n+1})
\end{equation*}
with probability at least $1-\epsilon$.

In the case of reduction of a randomness p-predictor,
we rewrite \eqref{eq:thm-p-main} as
\begin{multline*}
  G(z_1,\dots,z_n,z_{n+1})
  =
  (\delta/\e)^{1/2}\\
  {}\times
  \sqrt{P(z_1,\dots,z_n,x_{n+1},1-y_{n+1})^{\delta-1}
    P'(z_1,\dots,z_n,x_{n+1},1-y_{n+1})}.
\end{multline*}
Therefore,
\begin{equation*}
  P'(z_1,\dots,z_n,x_{n+1},1-y_{n+1})
  <
  \e \delta^{-1} \epsilon^{-2}
  P(z_1,\dots,z_n,x_{n+1},1-y_{n+1})^{1-\delta}
\end{equation*}
with probability at least $1-\epsilon$.
The interpretation is similar to that of the e-version.

In the rest of this section,
let us only discuss reduction of randomness e-predictors to conformal e-predictors.
Reduction of randomness p-predictors to conformal p-predictors is completely analogous;
it just uses \eqref{eq:thm-p-main} instead of \eqref{eq:thm-e-main}.

In the case of multi-class classification, $2<\left|\mathbf{Y}\right|<\infty$,
the most natural Markov kernel $B$ is perhaps the one for which $B(\cdot\mid(x,y))$
is the uniform probability measure on $\mathbf{Y}\setminus\{y\}$.
In this case we can rewrite \eqref{eq:thm-e-main} as
\begin{equation}\label{eq:multi-class}
  G(z_1,\dots,z_n,z_{n+1})
  :=
  \frac{\e^{-1/2}}{\left|\mathbf{Y}\right|-1}
  \sum_{y\in\mathbf{Y}\setminus\{y_{n+1}\}}
  \sqrt{\frac
    {E(z_1,\dots,z_n,x_{n+1},y)}
    {E\tX(z_1,\dots,z_n,x_{n+1},y)}}.
\end{equation}
The interpretation of~\eqref{eq:multi-class} is that the conformal e-predictor $E\tX$
is almost as efficient as the original randomness e-predictor $E$ on average;
as before, efficiency is measured by the degree
to which we reject the false labels $y\ne y_{n+1}$.
Of course, we can avoid ``on average'' by making \eqref{eq:multi-class} cruder
and replacing it by the existence of $G\in\ER$ satisfying
\begin{multline*}
  \forall(z_1,\dots,z_n)\in\mathbf{Z}^n \;
  \forall x_{n+1}\in\mathbf{X} \;
  \forall y\in\mathbf{Y}\setminus\{y_{n+1}\}:\\
  G(z_1,\dots,z_n,z_{n+1})
  \ge
  \frac{\e^{-1/2}}{\left|\mathbf{Y}\right|-1}
  \sqrt{\frac
    {E(z_1,\dots,z_n,x_{n+1},y)}
    {E\tX(z_1,\dots,z_n,x_{n+1},y)}},
\end{multline*}
where $z_{n+1}:=(x_{n+1},y_{n+1})$.
For a small positive constant $\epsilon$,
we can then claim that, with probability at least $1-\epsilon$,
\begin{equation*}
  \forall y\in\mathbf{Y}\setminus\{y_{n+1}\}:
  E\tX(z_1,\dots,z_n,x_{n+1},y)
  >
  \frac{\e^{-1}\epsilon^2}{(\left|\mathbf{Y}\right|-1)^2}
  E(z_1,\dots,z_n,x_{n+1},y).
\end{equation*}

An interesting variation of \eqref{eq:multi-class},
corresponding to the Markov kernel $B$
for which $B(\cdot\mid(x,y))$ is the uniform probability measure on $\mathbf{Y}$,
is
\begin{equation*}
  G(z_1,\dots,z_n,z_{n+1})
  :=
  \frac{\e^{-1/2}}{\left|\mathbf{Y}\right|}
  \sum_{y\in\mathbf{Y}}
  \sqrt{\frac
    {E(z_1,\dots,z_n,x_{n+1},y)}
    {E\tX(z_1,\dots,z_n,x_{n+1},y)}}.
\end{equation*}
Under this definition,
the randomness e-variable $G$ does not depend on $y_{n+1}$.

\section{Applications to regression}
\label{sec:regression}

In this section we set $\mathbf{Y}:=\R$;
therefore, we consider the problem of regression.
In applied regression problems,
we are often interested in prediction intervals
rather than arbitrary prediction sets.
This calls for an investigation of the regularity of the derived conformal predictor,
which we will start in this section.

We will be interested only in upper prediction limits,
as in \cite[Sect.~7.2(i)]{Cox/Hinkley:1974}.
Once we deal with those,
we can treat lower prediction limits in the same way,
and then a prediction interval can be formed
as the intersection of the rays
defined by upper and lower prediction limits.
(In several respects,
this is much more convenient than finding prediction intervals directly,
as shown in the context of conformal regression in \cite{Burnaev/Vovk:2014}
and discussed in \cite[Sect.~2.3.2]{Vovk/etal:2022book}.)
For simplicity, let us concentrate on e-prediction.

\iftoggle{FULL}{\bluebegin
  The expression ``prediction ray'' is not common,
  but it was used in \cite[Sects~2.3.2 and~13.4.2]{Vovk/etal:2022book}.
\blueend}{}%

Given a randomness e-predictor $E$,
the derived conformal e-predictor $E\tX$ is based on repeated averaging:
see \eqref{eq:tX}.
Therefore, we can expect $E\tX$ to be more regular.
We start from checking the regularity of $E\tX$ in a simple case.

Let us say that a randomness e-predictor $E$ is \emph{monotonic}
if $E(z_1,\dots,z_{n+1})$ is increasing in $y_{n+1}$
but decreasing in $y_i$ for each $i\in\{1,\dots,n\}$.
(This is analogous to monotonic conformity measures
as discussed in \cite[Sect.~7.2.3]{Vovk/etal:2022book}.)

\begin{lemma}\label{lem:monotonic}
  If a randomness e-predictor $E\in\ER$ is monotonic,
  then its conformal version $E\tX\in\EtX$,
  defined by \eqref{eq:tX},
  is increasing in $y_{n+1}$.
\end{lemma}

Lemma~\ref{lem:monotonic} says that the prediction function output by $E\tX$
is increasing.
It is clear that this lemma is also applicable to conformal p-predictors,
in the following sense:
if $P\in\PR$ is monotonic
(decreasing in $y_{n+1}$ and increasing in $y_i$, $i\ne n+1$),
then its conformalized version $1/(\delta P^{\delta-1})\tX$
outputs a decreasing prediction function.
Therefore, the conformalized version will output rays as its prediction sets.

\begin{proof}[Proof of Lemma~\ref{lem:monotonic}]
  The denominator of the fraction in \eqref{eq:tX}
  is the sum of its numerator,
  which is increasing in $y_{n+1}$,
  and addends that are decreasing in $y_{n+1}$.
  It remains to notice that the fraction
  \[
    \frac{f(y)}{f(y)+g(y)}
    =
    \frac{1}{1+g(y)/f(y)},
  \]
  where $f$ and $g$ are nonnegative functions that are increasing and decreasing,
  respectively,
  is increasing in $y$.
\end{proof}

The next proposition gives a lower bound on the conformalized version
of a particularly simple prediction set
output by a monotonic randomness e-predictor.
Namely, we consider an ``upper prediction ray'',
a prediction function of the form $f(y)=D1_{\{y\ge b\}}$,
where $b\in\R$ is the upper prediction limit
and $D>0$ reflects the confidence in this prediction.

\begin{proposition}\label{prop:bound}
  Let $B:\mathbf{Z}\hookrightarrow\mathbf{Y}$ be a Markov kernel.
  Suppose a monotonic randomness e-predictor $E$,
  given a training sequence $z_1,\dots,z_n$ and test object $x_{n+1}$,
  outputs a set prediction $f(y):=D1_{\{y\ge b\}}$
  for the label $y_{n+1}$,
  where $D>0$ and $b\in\R$.
  Let $F$ be the distribution function of $B(\cdot\mid z_{n+1})$.
  Then the function $h:\R\to\R$ defined by
  \begin{equation}\label{eq:bound}
    h(y)
    :=
    \frac{D(F(y)-F(b))^2}{\e g^2}
    1_{\{y\ge b\}},
  \end{equation}
  where $g:=G(z_1,\dots,z_{n+1})$ and $G$ is the randomness e-variable
  defined in Theorem~\ref{thm:e-main},
  is a lower bound on the prediction function output by $E\tX$.
\end{proposition}

The parameter $b$ of the prediction function $D1_{\{y\ge b\}}$ of $E$,
reflecting the precision of the upper prediction limit,
should ideally be greater than but close to $y_{n+1}$,
and then we could interpret $b-y_{n+1}$ as the precision.
The prediction function of $E\tX$ is increasing by Lemma~\ref{lem:monotonic}.
The bound~\eqref{eq:bound} is very weak,
which can be seen from $h(\infty)\le D/(\e g^2)$.
However, this is the best that can be derived from Theorem~\ref{thm:e-main}:
being competitive on average does not mean being competitive
at each individual label~$y$.

\begin{proof}[Proof of Proposition~\ref{prop:bound}]
  It suffices to consider only $y>b$ in \eqref{eq:bound}.
  Let $A$ be the value of the prediction function output by $E\tX$
  at some point $a>b$.
  Then the right-hand side of \eqref{eq:thm-e-main} is smallest
  if the prediction function $E\tX(z_1,\dots,z_n,x_{n+1},\cdot)$
  is $A$ at and to the left of $a$ and is $\infty$ to the right of $a$.
  Therefore, it is impossible to have
  \[
    \e^{-1/2}
    \int_b^a
    \sqrt{\frac{D}{A}}
    \dd F
    >
    g,
  \]
  i.e.,
  \[
    A
    <
    \frac{D(F(a)-F(b))^2}{\e g^2},
  \]
  which gives the lower bound~\eqref{eq:bound}.
\end{proof}

Let us see what the lower bound \eqref{eq:bound} becomes
for specific distributions,
e.g., the exponential ones centred at $y_{n+1}$,
\[
  F(y)
  :=
  \begin{cases}
    1-\exp(-\lambda(y-y_{n+1})) & \text{if $y>y_{n+1}$}\\
    0 & \text{otherwise},
  \end{cases}
\]
where $\lambda$ is a positive constant.
(Remember that $F$ is allowed to depend on $z_{n+1}$.)
The lower bound becomes
\[
  h(y)
  =
  \frac{D(\exp(-\lambda(b-y_{n+1}))-\exp(-\lambda(y-y_{n+1})))^2}{\e g^2}
\]
for $y>b$ and $b>y_{n+1}$.
In the homoscedastic, or nearly homoscedastic, regular case
we could choose the parameter $\lambda$ close
to the typical values of $1/(b-y_{n+1})$.
Then $h(2b-y_{n+1})$ would have the order of magnitude at least $D g^{-2}$
(the geometric interpretation of $2b-y_{n+1}$ is that $b$
is half-way between $y_{n+1}$ and $2b-y_{n+1}$).

\section{Conclusion}
\label{sec:conclusion}

These are some directions of further research:
\begin{itemize}
\item
  Can we connect any two of the classes $\PR$, $\PX$, and $\PtX$
  directly (in the spirit of Theorem~\ref{thm:p-main}),
  without a detour via e-values?
\item
  Our only optimality result,
  Theorem~\ref{thm:anti-Kolmogorov},
  covers Kolmogorov's step only.
  It would be ideal to have optimality results related
  to Theorems~\ref{thm:e-main} and~\ref{thm:p-main}.
\end{itemize}

% \clearpage % use when the last page is almost empty

\iftoggle{TR}{%
  \bibliographystyle{plain}
}{}%
\bibliography{local,%
  /doc/work/r/bib/AIT/AIT,%
  /doc/work/r/bib/general/general,%
  /doc/work/r/bib/math/math,%
  /doc/work/r/bib/prob/prob,%
  /doc/work/r/bib/stat/stat,%
  /doc/work/r/bib/vovk/vovk}
\end{document}
