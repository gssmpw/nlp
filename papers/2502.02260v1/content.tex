
\begin{abstract}
In the past decade, considerable research effort has been devoted to securing machine learning (ML) models that operate in adversarial settings. Yet, progress has been slow even for simple ``toy'' problems (e.g., robustness to small adversarial perturbations) and is often hindered by non-rigorous evaluations.
Today, adversarial ML research has shifted towards studying larger, general-purpose language models.
In this position paper, we argue that the situation is now even worse: \textbf{in the era of LLMs, the field of adversarial ML studies problems that are (1) less clearly defined, (2) harder to solve, and (3) even more challenging to evaluate.}
As a result, we caution that yet another decade of work on adversarial ML may fail to produce meaningful progress.
\end{abstract}


\section{Introduction}





When adversarial machine learning emerged as a field, it focused on attacking and defending simple models with well-defined objectives. For example, 
misclassifying a spam message as safe~\citep{graham2004beat} or images in deep learning models~\citep{biggio2013evasion,szegedy2013intriguing,goodfellow2014explaining}. These early problems were well-defined: the attack goals were clear (e.g., cause a misclassification), the target models were relatively simple (e.g., linear classifiers, small neural networks), the threat models were simple (e.g., perturb pixels by at most 8/255), and the evaluation metrics were straightforward (e.g., accuracy on a test set). 
Yet the field has struggled to develop robust solutions or even to fully understand why these vulnerabilities exist~\citep{barreno2006can,shafahi2018are}. Even fundamental ``toy'' problems like robustness to $\ell_p$-bounded perturbations,  remain largely unsolved to this day, and many defense evaluations still lack rigor~\citep{carlini2017towards,carlini2019evaluating,tramer2020adaptive}.

Recently, the focus of the field has since shifted towards studying adversarial problems with large language models (LLMs) and other generative models. \textbf{In this position paper, we argue that these new problems are significantly harder to define, solve and evaluate; making progress increasingly difficult to track}.

Due to their general-purpose nature, LLMs are not designed to solve any single well-defined ``task'' to be secured. Instead, the field now considers a more holistic notion of ``safety'', with adversarial objectives that are hard to define formally (e.g., making an LLM produce ``harmful'' responses)~\citep{christiano2017deep,ouyang2022training,bai2022training,casper2023open}. These safety properties are also often considered for unbounded threat models, thereby leading to much stronger adversaries (e.g., with the ability to adversarially fine-tune a model or to prompt it in arbitrary ways). 
Due to this large attack space---and the difficulty of directly optimizing over it~\citep{carlini2024aligned}---attacks are increasingly ad-hoc and human driven~\citep{li2024llm}. This further complicates the task for defenders, who cannot automatically search over strong, adaptive attacks. 

\begin{table*}[t]
\vskip -0.1in
\caption{Challenges faced in different research areas when defining and solving adversarial ML problems.}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l c c c c c c c@{}}
\toprule

& \multicolumn{7}{c}{\textbf{Challenges}} \\
\cmidrule(lr){2-8}
& \multicolumn{3}{c}{\textbf{Defining}} 
&
\multicolumn{2}{c}{\textbf{Solving}}
&
\multicolumn{2}{c}{\textbf{Evaluating}}
\\
\cmidrule(lr{5pt}){2-4}
\cmidrule(l{5pt}r{5pt}){5-6}
\cmidrule(l{5pt}r){7-8}
\makecell[lt]{\\ \\ \textbf{Research Area}}
& \makecell[t]{(\S\ref{sssec:success})\\ Defining\\ Success}
& \makecell[t]{(\S\ref{sssec:attackspace})\\ Bounding\\ Attacks}
& \makecell[t]{(\S\ref{sssec:databound})\\ Delimiting\\ Data}
& \makecell[t]{(\S\ref{sssec:searchattacks})\\ Attack\\ Search}
& \makecell[t]{(\S\ref{sssec:principleddefense})\\ Principled\\ Defenses}
& \makecell[t]{(\S\ref{sssec:measureharm})\\ Measuring\\ Harm \& Utility}
& \makecell[t]{(\S\ref{sssec:reproducibility})\\ Ensuring\\ Reproduc.} \\
\midrule

(\S\ref{sec:jailbreaks}) Jailbreaks 
& \checkmark 
& \checkmark 
&  
& \checkmark 
& \checkmark 
& \checkmark 
& \checkmark \\

(\S\ref{sec:unfinetune}) Un-finetunable Models
& \checkmark 
& \checkmark 
&  
&  \checkmark 
& \checkmark 
&  \checkmark 
& \\

(\S\ref{sec:poisoning}) Poisoning + Backdoors 
& \checkmark 
& \checkmark 
& \checkmark 
& \checkmark 
& \checkmark 
& \checkmark 
& \checkmark \\

(\S\ref{sec:pi}) Prompt Injections 
& \checkmark 
& \checkmark 
&  
& \checkmark 
& \checkmark 
& \checkmark 
& \checkmark \\

(\S\ref{sec:mi}) Membership Inference 
& \checkmark 
&  
& \checkmark 
&  
&  
&  
& \checkmark \\

(\S\ref{sec:unlearning}) Unlearning 
& \checkmark 
& \checkmark 
& \checkmark 
& \checkmark 
& \checkmark 
&  \checkmark 
& \\

\bottomrule
\end{tabular}%
} %
\label{tab:challenges}
\vspace{-5pt}
\end{table*}



Beyond making the technical problems harder, we argue that generative models have also made evaluation and benchmarking of attacks and defenses more challenging.
Measuring attack success is no longer as straightforward as measuring misclassification rates; it instead requires careful (human) evaluation of possible harms present in natural language outputs~\citep{mazeika2024harmbench,chao2024jailbreakbench}. In a similar vein, evaluating whether defenses preserve the utility of the original model has become more nuanced: 
instead of measuring test accuracy on a single task, we now have to determine whether a model maintains its general-purpose capabilities~\citep{cui2024or,mai2025canteatcaketoo}. 

Finally, reproducible benchmarking became harder as many state-of-the-art models are deployed via black-box APIs that may receive constant updates and patches as newer attacks are released. As these changes are often not reported, reproducing results or making meaningful comparisons between different approaches becomes nearly impossible.

In this position paper, we use several case studies of research areas in adversarial ML to illustrate the increasing complexity in both attacks and defenses. We first analyze how traditional research problems have evolved to become fundamentally harder to formally define and solve (Section~\ref{sec:define}). We then present case studies that illustrate these new challenges (Section~\ref{sec:case_studies}). Finally, we discuss our perspective on why these changes represent a fundamental challenge to progress in the field and alternative views on the evolution of adversarial ML (Section~\ref{sec:discussion}).

\section{New Challenges in Defining, Solving, and Evaluating Adversarial ML Problems}
\label{sec:define}

Traditional ML models were designed and trained for specific and narrow tasks---often classification. For example, computer vision models used to classify images into a fixed set of classes~\citep{krizhevsky2012imagenet}, and natural language processing models used to perform textual analysis on individual sentences~\citep{richardson2013mctest,rajpurkar2016squad}. Additionally, the training and test data were clearly delineated as inputs were discrete and bounded units (individual images or sentences). In these settings, adversarial objectives could be clearly specified. For example, misclassifying as many inputs as possible (i.e., adversarial examples~\citep{szegedy2013intriguing,goodfellow2014explaining}) or inferring if a given data point was used for training (i.e., membership inference~\citep{shokri2017membership}).

However, LLMs have fundamentally changed this landscape. Models no longer perform narrow tasks but serve as general-purpose systems that produce free-form and unbounded outputs. As a result, defining ``security'' or ``safety'' properties of the AI system has become more challenging, with the field focusing on very general definitions (e.g., a model should not produce outputs that can ``harm others''\footnote{\scriptsize{\url{https://openai.com/policies/usage-policies/}}}). Adversarial objectives related to training data (e.g., membership inference or unlearning) have also become more ill-defined, as the training set(s) of an LLM may span virtually the entire Internet~\citep{gao2020pile}, with no clear boundaries between data points or between train and test sets.

In this section, we identify three core challenges, each split into several sub-challenges, that make adversarial ML for LLMs \emph{harder to define}, \emph{harder to solve}, and \emph{harder to evaluate}. We provide a summary of the challenges faced in different research areas in~\Cref{tab:challenges}.

In Section~\ref{sec:case_studies}, we elaborate on how these challenges hinder progress by analyzing specific case studies: \emph{Jailbreaks} (Section~\ref{sec:jailbreaks}), 
\emph{Un-finetunable Models} (Section~\ref{sec:unfinetune}),
\emph{Poisoning and Backdoors} (Section~\ref{sec:poisoning}), \emph{Prompt Injections} (Section~\ref{sec:pi}), \emph{Membership Inference} (Section~\ref{sec:mi}), and \emph{Unlearning} (Section~\ref{sec:unlearning}). 

\subsection{Problems are Harder to Define}

\subsubsection{Defining Success of Attacks and Defenses}
\label{sssec:success}

In the past, adversarial problems for classification models typically involved concrete objectives (e.g., misclassifying images), which could be easily measured by accuracy on a set of clean or perturbed inputs. Now, the lack of a single well-defined task makes it unclear what criteria constitute a genuine success or failure for attacks or defenses.

LLMs produce free-form text in which both developer and adversary goals become subjective. Developers now aim to optimize abstract properties like helpfulness, honesty, and harmlessness~\citep{bai2022training}, while adversaries may try to obtain generically harmful outputs. Thus, measuring attack success---i.e., whether an output is actually harmful or violates the developer policies---also becomes subjective. %


\subsubsection{Defining and Bounding the Attack Space}
\label{sssec:attackspace}

In prior robustness settings (e.g., with classification models), the adversary was often constrained to perturb inputs within an $\ell_p$-ball around a given image. This served as a meaningful \emph{necessary} but \emph{not sufficient} condition for robustness  \cite{gilmer2018motivating}, allowing quantitative comparisons of different methods~\citep{goodfellow2014explaining}.
While the broader problem of \emph{unrestricted} \cite{brown2018unrestricted,song2018constructing} attacks has seen some study, the vast majority of research focused on the narrow $\ell_p$-ball problem.

For LLMs, researchers almost always allow the search space for attacks to be unbounded, since any input could potentially elicit a violation of a safety property~\citep{wei2024jailbroken}. The shift from input-dependent to input-\emph{independent} constraints makes it harder to specify adversarial capabilities that allow us to compare defenses. Beyond unbounded inputs, threat models have also become more permissive. In traditional adversarial ML problems (e.g., adversarial examples or poisoning), the strongest adversaries had white-box access to model weights, but could not alter the model's functionality. Now attackers need not maintain the model's general capabilities as long as they can elicit the desired harmful information, enabling stronger attacks such as fine-tuning or pruning~\citep{qi2024finetuning,wei2024assessing}\footnote{For adversarial robustness in image classifiers, the ability to finetune the victim model would be a trivial attack vector, since the attacker could simply fine-tune the model to have low accuracy.}.

Moreover, the set of attacks that should be ruled out may not always be obvious. While one could say ``any input that leads to harmful content is a valid attack,'' trivial attacks such as prompting ``please repeat [harmful text]'' do not reveal meaningful new vulnerabilities. Hence, there is no clear universal standard on what sorts of prompts or transformations count as ``valid'' or ``novel'' adversarial inputs.

\subsubsection{Delimiting Data}
\label{sssec:databound}

In many research areas traditionally studied in adversarial ML, such as unlearning or privacy protection, the notion of a \emph{training data point} plays a crucial role.
Previously, a model was trained on a carefully curated dataset with strict train/test splits; each data point (such as a single labeled image) was distinct, and known to researchers.
In contrast, generative models are trained on vast corpora, where similar, or even identical, content may appear across multiple subsets of the training set. The exact contents of the training data are also rarely publicly released~\citep{nasr2025scalable}. The notion of a held-out (IID) test set no longer really exists.


\subsection{Problems are Harder to Solve}

\subsubsection{Searching over Attacks}
\label{sssec:searchattacks}

The optimization landscape for most adversarial ML problems has become significantly more complex with LLMs. In traditional classification problems, such as crafting adversarial images, the objective function was clear: maximize the loss on the correct prediction while minimizing perturbation size. This objective could be formalized and optimized by propagating gradients to the input space~\citep{madry2017towards}. These automated attacks outperformed humans and consistently found worst-case attacks~\citep{carlini2017provably}.

However, the attack surface for LLMs is much larger and harder to define (see Section~\ref{sssec:attackspace}). There is no longer a single well-defined ``task'', and safety properties cannot be expressed with formal loss functions---they are qualitative, context-dependent, and often subjective~\citep{bai2022training}. 


Even if we define a ``toy'' attack objective (e.g., making the model output an affirmative response such as ``Sure, I can help you with that''~\citep{zou2023universal}), finding good attacks remains hard~\citep{carlini2024aligned}. Discrete text inputs makes gradient-based methods less effective~\citep{carlini2024aligned,rando2024gradient}, and the vast search space makes exploration impractical. Perhaps most telling, manual attacks still outperform automated methods at finding worst-case inputs~\citep{li2024llm}. Many successful attacks on LLMs exploit qualitative properties that are hard to optimize automatically, such as persona modulation~\citep{shah2023scalable}, multi-turn conversations~\citep{anil2024many}, and social engineering techniques~\citep{zeng2024johnny}. In contrast, current optimization methods typically generate gibberish inputs~\citep{zou2023universal,thompson2024flrt}.



\subsubsection{Building Principled Defenses}
\label{sssec:principleddefense}

In traditional adversarial tasks, researchers could devise \emph{certified} defenses~\citep{cohen2019certified} or well-motivated empirical defenses such as adversarial training~\citep{madry2017towards}, where key properties of the problem (like bounded input perturbations) were explicitly understood. Moreover, the performance of these defenses could be evaluated with strong, adaptive white-box attacks~\citep{tramer2020adaptive}. 

In contrast, for LLMs the adversarial objectives are typically not formally defined (see Section~\ref{sssec:success}) and the attack space is challenging to bound (see Section~\ref{sssec:attackspace}). 
As a result, there is little hope to build defenses upon principled foundations. Existing defenses rely on ad-hoc approaches, through either:  (1) adversarial training against \emph{known} successful attacks~\cite{bai2022training, wallace2024instruction}; (2) ``virtual'' adversarial training in the model's latent space~\cite{miyato2018virtual, casper2024defending,sheshadri2024latent}; (3) building external classifiers or detectors~\citep{inan2023llama}; (4) or random preprocessing~\citep{robey2023smoothllm}.
Crucially, none of these approaches  produce systems whose security can be analyzed or quantified in a well-defined formal. It is thus not too surprising that the original evaluations of some of these defenses overestimate their robustness~\citep{chi2024llama, qi2024evaluating, lucki2024adversarial}.

\subsection{Problems are Harder to Evaluate}
\label{ssec:evaluate}

\subsubsection{Measuring Attack Harm and Defense Utility}
\label{sssec:measureharm}

Since safety properties for LLMs are hard to formally define, it has become customary to use LLMs themselves as a fuzzy ``judge'' to determine harmfulness (e.g., when evaluating jailbreaks or prompt injections~\citep{mazeika2024harmbench}). But this approach suffers from a number of issues:

\begin{itemize}[leftmargin=10pt, topsep=0pt, itemsep=0pt]
    \item First, such judges still fall short of human judgment.\footnote{Even (non-expert) humans have a hard time judging harmfulness of model responses, e.g., when judging whether ``instructions for building a bomb'' truly yield a useful design.} For instance, many implementations often default to considering any non-refusal response as a successful attack even if the content is harmless~\citep{souly2024strongreject}.

    \item 
    Second, judges themselves may be vulnerable to attacks that make them misclassify model outputs~\citep{mangaokar2024prp,raina2024llm}.
    Thus, a strong attack could mistakenly be judged as ineffective because it also fools the LLM judge into classifying the outputs as harmless, or viceversa.

    \item Third, using LLMs-as-judges to evaluate defenses can create artificial correlations that bias evaluation results. For example, a defense that implements an output filter similar to the judge may achieve near-perfect scores without necessarily being effective against prompts where the judge itself fails. In the extreme, a defense could simply use the same LLM judge internally, and reject any output deemed unsafe by the judge~\citep{liu2024jailjudge}. Such a defense would, by definition, be judged as perfectly safe.
\end{itemize}

Measuring benign utility of defenses---whether they preserve other capabilities---is also non-trivial. Unlike classification tasks where accuracy on a fixed test set is standard, LLMs can be used for an open-ended array of tasks. A defense can trivially produce a safe-but-useless model by refusing all requests. Thus, any evaluation framework must somehow account for the model's usefulness to the end-user, which is subjective and context-dependent~\citep{cui2024or}.

\subsubsection{Reproducing and Comparing Results}
\label{sssec:reproducibility}

In earlier, more controlled research environments, practitioners had detailed information about a model’s architecture, training data, and training pipeline, enabling precise definitions of threats, defenses, and success criteria. This transparency made it straightforward to track progress.

Many influential LLMs are now closed-source and updated silently over time~\citep{chao2024jailbreakbench}, making it unclear which version of a system is being tested. Moreover, instead of investigating a single, well-defined model, one must analyze an entire system that may incorporate multiple pre-processing, post-processing, or other defense mechanisms.

This lack of transparency severely undermines reproducibility. Researchers cannot confirm whether observed behaviors persist across different snapshots of the system, nor can they reliably benchmark potential solutions. Consequently, adversarial ML problems become harder to define---let alone solve and evaluate. While black-box or discrete optimization approaches can help reveal some vulnerabilities, they provide only limited insight into the model’s internals,
leaving many critical security and privacy questions unanswered~\citep{casper2024black,carlini2024aligned}.

\section{Case Studies}
\label{sec:case_studies}

We now turn to detailed case studies that illustrate many of the difficulties faced as adversarial machine learning problems become increasingly challenging to define, solve and evaluate in today’s less well-defined environments.


\subsection{Jailbreaks} %
\label{sec:jailbreaks}

Jailbreaks illustrate many of the new challenges in adversarial research.
Jailbreaks are adversarial text inputs for language models that bypass safeguards and get the model to generate ``harmful'' content~\citep{wei2024jailbroken}.

\paragraph{``Harmful'' content has no formal definition.}
Defining success for an adversarial image is relatively easy: the perturbation is ``small'' under some given measure, and leads to a misclassification. With jailbreaks, however, success requires defining what it means for a model to output ``harmful'' or otherwise ``undesirable'' content. Early attempts used crude proxies based on simple substring matching~\citep{zou2023universal}.
This approach has largely been replaced by a more general use of an ``LLM-as-a-judge'', where the fuzzy task of defining harmfulness is given to another LLM~\citep{zheng2023judging,chao2023jailbreaking,shah2023scalable,mazeika2024harmbench}. The circularity of this definition leads to a number of issues, as illustrated in Section~\ref{sec:define}.


\paragraph{There are no meaningful bound on adversaries and attack strategies.} Although adversaries for image classification could also be unbounded, the fact that the safety property is dependent on the input (replacing a cat by a dog is not an interesting attack) made the community define an $l_p$ norm around the inputs as a proxy for preserving visual similarity. However, for jailbreaks, there is not such a meaningful bound as the safety property is \emph{independent} of the input (harmful generations should never occur). Researchers have come up with attacks that use semantic augmentations (e.g., role-playing or social engineering)~\citep{shah2023scalable,zeng2024johnny}, append high-perplexity suffixes~\citep{zou2023universal,thompson2024flrt} or even found that long inputs and random augmentations dilute safeguards~\citep{anil2024many,andriushchenko2024jailbreaking,hughes2024best}. Not only adversaries are now unbounded in the input space, but they can use additional methods such as fine-tuning~\citep{qi2024finetuning} or pruning~\citep{wei2024assessing}. This diversity of attacks illustrates the difficulty to define a narrow task, analogous to $\ell_p$ bounded robustness, that can be used to compare and benchmark attacks and defenses.

\paragraph{Optimizing for worst-case attacks is hard.} Optimizing attacks against classifiers is straightforward. You can set as objective misclassifying a given input and define it as the maximization of the model loss~\citep{szegedy2013intriguing}.
The loss gradient can be propagated all the way to the input to guide updates. However, LLMs do not provide any of the above: the optimization goal is unclear and optimization is not continuous nor over a finite input space. As a workaround, previous work has tried to optimize proxy objectives such as maximizing the probability of a compliance prefix (e.g. ``Sure, I can help you with that'')~\citep{zou2023universal,carlini2024aligned}. However, the input space is still discrete and virtually infinite. These challenges make discrete optimization extremely inefficient and close to random search~\citep{zou2023universal,andriushchenko2024jailbreaking}. Optimization challenges have made us shift from a field where the strongest attacks were found via white-box optimization, to one where the best attacks often come from human experts and cannot be found via optimization~\citep{li2024llm}. This challenges our ability to make progress in measuring worst-case performance of systems~\citep{carlini2024aligned}.

\paragraph{We cannot measure progress against continuously updated closed-source systems.} As soon as new attacks are reported, model developers often patch their models with additional filters or fine-tuning against that specific attack~\citep{chao2024jailbreakbench}. Although these updates are clearly beneficial to protect users from harmful content, they hinder the ability to track progress (see Section~\ref{sssec:reproducibility}).%

\subsection{Unfinetunable Models} 
\label{sec:unfinetune}

A recent research direction aims to design models that are not only robust to jailbreaks,
but \emph{also are robust to fine-tuning} \cite{tamirisa2024tamper,rosati2024representation}.
This threat model is motivated by the general observation that if a model does \emph{not} have the knowledge to perform some dangerous capability (such as giving instructions for how to perform a cyberattack or design a bioweapon), attacks will never be successful~\citep{li2024wmdp}.

\paragraph{The attacker is strictly more powerful than for adversarial examples.}
An adversarial example attacker has exactly one ability: to modify the input so the model produces an incorrect output.
When designing an un-finetunable model, we assume an attacker with \emph{strictly} more power: not only can they change the input arbitrarily, 
but they can also modify the model itself.
Indeed, recent work has already shown how the interplay between modifying the input and modifying the parameters can allow attackers to break many recently proposed defenses \cite{qi2024evaluating}.

\paragraph{The increased attack space makes it more difficult to evaluate.}
In the classical adversarial example literature, the evaluator must ensure exactly one thing is true: the input-space gradient is smooth and following it leads to adversarial examples.
In contrast, evaluating an unfinetunable model requires that the much higher \emph{parameter-space} gradients are smooth, something often $1000\times$ higher dimensional.
Moreover, the number of hyperparmeters in the evaluation increases significantly, introducing even more room for error~\citep{honig2024adversarial,qi2024evaluating}.

\paragraph{Defining what ``unfinetunable'' means is challenging.}
In one sense of the word, it's impossible to make a model unfinetunable---as long as the weights are available, an adversary can always in principle edit them.
What matters is whether or not those edits actually do anything useful.
So what attacking an ``unfinetunable model'' aims to achieve, then, is to teach the model something new without harming utility. %
But for the reasons discussed in the prior section, it is hard to actually \emph{quantify} whether or not something new has been taught, and whether or not utility has been harmed.


\subsection{Poisoning and Backdoors}
\label{sec:poisoning}

In poisoning attacks, adversaries modify a model's training data to affect its behavior on specific examples~\citep{huang2011adversarial} or inject backdoors~\citep{gu2019badnets}. The messy datasets and costly training runs for LLMs make the definition, optimization and evaluation of attacks more challenging.

\paragraph{Attack goals are hard to enumerate and conflict with intended functionality.} In classification models, adversaries injected training examples with specific triggers that correlated with an output label~\citep{gu2019badnets}. However, in generative models, adversaries trigger fuzzy and complex behaviors like producing harmful content or spreading misinformation~\citep{wan2023poisoning,rando2024universal,zhang2024persistent}. Not only are these behaviors harder to predict and specify formally, but they also fundamentally conflict with the model's intended functionality since the triggered behavior is often universally undesirable and explicitly trained against~\citep{zhang2024persistent}.

\paragraph{Attacks can come from multiple training stages and are hard to optimize over.} Traditional machine learning models had a single training stage on the entire dataset. However, LLMs are first pre-trained and then fine-tuned on (curated) data to turn them into helpful and harmless chatbots~\citep{bai2022training}. These different training stages have different properties, may enable different attacks, and can overwrite poisoning in previous stages~\citep{anwar2024foundational,zhang2024persistent}. Also, in LLMs there is no longer a good notion of what constitutes an effective poison nor we can optimize over them~\citep{goldblum2022dataset}.



\paragraph{Experiments with leading models are computationally infeasible.} Rigorous evaluation of backdoor attacks traditionally requires training models from scratch to understand both the effects of poisoned data and to establish clean baselines. However, this becomes infeasible for LLMs, where a single training run can cost millions of dollars~\citep{anwar2024foundational,zhang2024persistent}. The inability to perform comprehensive ablation studies or establish proper baselines makes it challenging to draw reliable conclusions.


\subsection{Prompt Injections} %
\label{sec:pi}

In a prompt injection attack~\citep{goodside,willison2023prompt}, 
an adversary injects malicious instructions into a language model’s context, manipulating its behavior to perform unauthorized actions or disclose sensitive information. These attacks commonly target LLM agents or LLM-integrated applications that interact with untrusted third-party resources through external tools~\citep{openai2024function,husain2024llama,anthropic2024tooluse}.




\paragraph{Measuring success of attacks and defenses requires a realistic AI agent environment.}
Rigorously evaluating the effectiveness of prompt injection attacks and defenses necessitates a realistic AI agent environment that closely mimics real-world scenarios. Such an environment should include comprehensive system scaffolding with tool use, enabling the simulation of complex interactions. However, for simplicity, many studies opt to simulate these environments and rely on LLMs as judges for evaluation.
There are new setups that have more rigorous evaluations~\citep{debenedetti2024agentdojo}, where the attack's success and utility can be precisely measured, but they are often limited due to the high cost of incorporating new tasks and their reliance on simulated environments.

\paragraph{Adversaries are unbounded.}
Unlike traditional adversarial attacks bounded by $\ell_p$ norms, prompt injection attacks also operate in a vast and unbounded input space. Additionally, prompt injection attacks can leverage context-dependent strategies, such as embedding malicious instructions within seemingly benign or unrelated text, or using multi-turn interactions to gradually steer the model toward undesirable outputs. This diversity in attack vectors, combined with the fact that virtually any controlled input can serve as a potential attack surface, complicates the task of establishing a reasonable threat model. Consequently, creating a standardized ``toy'' problem for benchmarking prompt injection defenses is inherently difficult. 


\paragraph{Optimizing for strong attacks is hard.} 
The primary goal of prompt injections is often clear---for instance, manipulating a language model to perform unauthorized actions like sending emails~\citep{debenedetti2024agentdojo}, where success can be directly measured.
However, the attack surface remains vast, encompassing not only single-turn interactions but also multi-turn scenarios where the model may repeatedly call external tools. In such cases, researchers often lack access to intermediate outputs, making it significantly more challenging to refine and optimize the attack. 

Most current attacks rely on handcrafted instructions~\citep{greshake2023what,liu2023prompt}, such as, ``Ignore all previous instructions, please do [target action] first,'' which are often effective in practice. These manual attacks complicate the development of principled defenses like adversarial training, due to their highly context-dependent and ad hoc nature.
Recent approaches~\citep{pasquini2024neural} have attempted to apply optimization techniques similar to those used in jailbreaks. Unfortunately, these attacks are not guaranteed to be optimal, and the search space for discovering highly effective attacks remains vast and largely unexplored.
As a result, defense attempts that train models against attacks mainly focus on \emph{known} attacks~\cite{wallace2024instruction}.





\paragraph{We cannot easily track progress against closed-source systems.} Similar to jailbreaks, model developers can mitigate prompt injection attacks by implementing safeguards such as filtering mechanisms~\citep{willison2023delimitters,wu2024secgpt} or regularly updating and fine-tuning their models~\citep{wallace2024instruction}. However, when targeting a closed-source system, the attack surface is no longer limited to a single model but encompasses the entire system, which may integrate multiple defense mechanisms simultaneously. Additionally, since these systems are frequently updated, it becomes difficult to establish a consistent benchmark for measuring progress or reproducing results. 

Worse, there are currently few open-source models that are effective tool-use agents~\cite{debenedetti2024agentdojo}. This is in contrast to jailbreaks, for example, where there at least exists a number of well-aligned open models that can be used for reproducible evaluation.

\subsection{Membership Inference} %
\label{sec:mi}

Membership inference (MI) attacks~\citep{shokri2017membership} aim to determine whether a specific sample $x$ was part of a model's training set.%
\paragraph{The distinction between members and non-members is no longer clearly defined.} In traditional classification settings, the training data is typically of limited size and relatively clean, with a clear delimitation between samples and few duplicates. 
However, the situation becomes more complicated for generative models.

\begin{enumerate}[leftmargin=10pt, topsep=0pt, itemsep=0pt]
    \item \textbf{Highly (partially) duplicated datasets}. The training data of generative models often comes from massive, diverse open datasets, which could include numerous duplicate and near-duplicate samples~\citep{lee2021deduplicating,tirumala2023d4}. Even if a model appears to memorize a particular sample (e.g., a piece of text or image), this does not necessarily prove that this sample itself was used during training. For example, a model might know much of the plot of Harry Potter without having been explicitly trained on the original book; it could have learned about the story indirectly through Wikipedia pages, reviews, fan discussions, or other online resources, which contain content highly similar to the original text. %
    Thus, the boundaries between members and non-members are blurred by the sheer scale and overlap of these datasets.
    
    \item \textbf{No IID train and test splits available}. A straightforward method for evaluating MI is to designate the training data as members and separate IID held-out data as non-members. However, for most generative models, the training datasets are typically not disclosed.%
    Some recent studies attempt to collect non-members post hoc for evaluation purposes~\citep{shi2023detecting,meeus2023did}, but these efforts often lead to misleading conclusions~\citep{duan2024membership,das2024blind} due to distribution shifts.
    
    \item \textbf{No fixed training data}. 
    Generative models are often updated through additional fine-tuning  that may incorporate new data beyond the initial cut-off date. For instance, some model developers, such as OpenAI, acknowledge that both pre-training and post-training datasets may include data from after the official data cutoff~\citep{achiam2023gpt}. Consequently, the ``cut-off date'' becomes somewhat arbitrary, and the distinction between training and non-training data is no longer clear-cut.
\end{enumerate}


\paragraph{We cannot build counterfactual scenarios for evaluation.} 
The core idea of MI is to demonstrate that the target model exhibits specific behavior---such as achieving a low loss on the data $x$---that is unlikely if the model had not been trained on $x$.  In traditional classification tasks (e.g., CIFAR-10), where the data generation process is fully controlled and models are relatively small, this process is relatively straightforward: one can quickly retrain the same models while excluding $x$, and then compare their statistical behaviors on $x$~\citep{carlini2022membership}. In the context of generative models, this approach is ill-defined and computationally impractical, thus it's infeasible to properly evaluate the success of an MI attack~\citep{zhang2024membership}.




\subsection{Machine Unlearning} %
\label{sec:unlearning}

Machine unlearning was originally formulated as a well-defined task: completely removing the influence of a specific datapoint $x$ from a model~\citep{bourtoule2021machine}. The goal was to produce a model that, after unlearning $x$, would be indistinguishable from one that was never trained on that point. In traditional classification settings with bounded inputs and outputs, and (often) deduplicated datasets with clear train-test splits, this objective could be precisely defined and evaluated. In fact, there exist exact solutions to unlearning~\citep{bourtoule2021machine}%
using membership inference attacks to determine if the sample to be unlearned could be detected as ``member'' of the training data of the model~\citep{shokri2017membership}.

\paragraph{Unlearning of ``concepts'' rather than individual data points is hard to define.}
However, generative models have fundamentally changed the nature of unlearning~\citep{cooper2024machine}. Instead of removing the influence of specific data points, the goal is to remove knowledge about entire concepts or topics that may be contained in one \emph{or more} data points (e.g., all dangerous knowledge about bioweapons~\citep{li2024wmdp} or copyrighted content from Harry Potter books~\citep{eldan2023s}). This has made it impossible to define unlearning in terms of a specific data point's influence, making both solutions and evaluations much more challenging.%

\paragraph{Unlearning goals conflict with other knowledge.} Developers may need to remove very specific knowledge (e.g., bioweapons) while maintaining the model's expertise in related fields (e.g., biology and virology)~\citep{li2024wmdp}. This tension between harmful and benign knowledge makes it inherently hard to define the goal of unlearning and to robustly evaluate safety and utility.

\paragraph{Threat models are overly strong.} Unlearning emerged as a white-box protection that would prevent \emph{any} adversary from accessing undesired capabilities in models~\citep{li2024wmdp}. This ambitious goal also enables stronger threat models where adversaries cannot only query the model, but also finetune it~\citep{hu2024jogging} and perform any white-box intervention~\citep{lucki2024adversarial}. Protecting against such a large attack surface is much harder~\citep{qi2024evaluating} as discussed in Section~\ref{sec:unfinetune}.

\paragraph{Measuring unlearning success is hard.} Measuring unlearning success has become significantly more challenging: training baseline models without specific datapoints is costly~\citep{eldan2023s} and membership inference has important limitations (see Section~\ref{sec:mi}). Recent studies have also demonstrated that even when a model cannot generate specific information, this does not reliably prove the underlying knowledge has been erased from its weights~\citep{patil2023can,lynch2024eight,lucki2024adversarial,shumailov2024ununlearning}. In practice, the search for adaptive evaluations is impractical and requires very careful tuning of the methodology for each scenario~\citep{lucki2024adversarial,qi2024evaluating}. Finally, \citet{shi2024muse} showed that measuring unintended effects of unlearning is challenging, as it can significantly affect other capabilities or even amplify privacy leakage.








\section{Discussion}
\label{sec:discussion}

\subsection{Alternative Views}


\paragraph{We are solving the right problem in the first place.}
We see increased complexity in adversarial ML because we are finally attempting to solve \emph{real} security challenges rather than toy academic problems. We knew that $\ell_p$-bounded perturbations were a simplified proxy~\citep{gilmer2018motivating}, but they were studied because they were challenging enough to drive progress and served as a \emph{necessary} condition for real-world robustness. We could similarly define toy problems for LLMs (e.g., jailbreaks limited to fixed-length prefixes or bounded sentence modifications), but the field has largely avoided such artificial constraints in favor of studying real-world unbounded adversaries. This shift might not indicate that problems have become fundamentally harder, but rather that the research community has decided to directly tackle the full complexity of real-world security.




\paragraph{Solving jailbreaks might be easier because we only need to prevent a behavior regardless of context.} Some researchers argue that certain problems have become conceptually simpler with LLMs. For instance, unlike adversarial examples where a model should maintain correct predictions in appropriate contexts (e.g., classify guacamole images as guacamole, but never cats as guacamole), jailbreak prevention has a simpler goal: the model should \emph{never} produce certain harmful outputs (e.g., instructions for building explosives) regardless of context. However, since there are many ways to express this knowledge (e.g., harmful requests can be decomposed into benign subquestions~\citep{glukhov2024breach}), defining and evaluating whether a model will \emph{never} produce harmful outputs remains a challenging problem.

Recent work, on representation engineering~\citep{arditi2024refusal,zou2024improving,tamirisa2024tamper} has aimed to identify specific directions in the model's representation space that can anticipate undesired behavior and prevent it universally. Yet, we know that adversarial images could also be detected by similar methods~\citep{carlini2017towards}, but these defenses ultimately proved vulnerable to newer attacks. Similarly, there are already works that show that representation engineering methods cannot robustly void undesired behaviors~\citep{li2024llm,qi2024evaluating}.

\paragraph{Scaffolding to reduce the probability of failure might be sufficient.} Given the difficulty of achieving robust safety guarantees, researchers and companies increasingly rely on complex defense systems~\citep{sharma2025constitutional} and security through obscurity to minimize risks. While this approach has demonstrated clear benefits in protecting users from harmful content, it prevents rigorous, reproducible and adaptive evaluations as systems become more complex and opaque~\citep{casper2024black}. This trend is particularly concerning given historical lessons from other security domains: preventing researchers from thoroughly analyzing systems can lead to severe real-world security breaches~\citep{swire2004model,mulligan2007magnificence,payne2020despite}. The apparent safety gains from obscurity and complexity may come at the cost of genuine security understanding.

\paragraph{We are already making progress on these problems.} A prevalent view in the field suggests that we are advancing security capabilities, pointing to newer models being demonstrably harder to attack than their predecessors~\citep{achiam2023gpt,zaremba2025trading}. While this observation might hold generally true, we caution that our inability to robustly evaluate defenses may be hindering our ability to track progress (see Section~\ref{ssec:evaluate}). Moreover, we must distinguish between progress in preventing average-case vulnerabilities and achieving \textbf{worst-case} security robustness. Although we might be making progress in the former, we have barely improved the latter and most models can still produce harmful generations under attacks. As the stakes increase with more capable models, the risks of rare yet successful attacks become significant~\citep{anthropicasl}.

\subsection{Suggestions for improvement}



We propose that there are (at least) two valid reasons for performing research on adversarial machine learning: (a) studying real-world security vulnerabilities and (b) advancing scientific understanding of adversarial ML. Papers should be explicit for what reason they are being written, and should be evaluated in this light. For real-world security, demonstrating attacks on fuzzy, ill-defined problems can be valuable when the potential harm is clear and immediate. For instance, it is valuable to show that language models can be manipulated to produce harmful content, even if we cannot precisely quantify ``harmfulness''. And when the objective is to advancing scientific understanding, we believe it is more productive to identify and focus on formal, well-defined sub-problems that can be rigorously studied, similar to how $\ell_p$-bounded perturbations provided a concrete framework for studying adversarial examples.

We acknowledge that even these well-defined sub-problems might still be challenging, just as achieving reliable $\ell_p$ robustness remains an open problem despite a decade of research. However, what we can definitely say is that if we cannot make progress on carefully scoped, formal problems, we have little hope of addressing the broader, fuzzier challenges of language model security. Moreover, working on well-defined problems enables rigorous scientific investigation: we can properly measure progress, compare different approaches, and build upon previous results. Attempting to solve the entire space of attacks without rigor is neither scientific nor likely to be productive.










