\section{RELATED WORK}
\subsection{View Planning in Manipulation}

View planning in robotics has been widely introduced in \cite{DBLP:journals/cvm/ZengWZL20}, \cite{chen2011active}, which seeks to determine the maximum information gain viewpoint and ensure the sequence of sensors. Among the various domains of view planning, one key area of interest is manipulation , which has been explored through various approaches to optimize task performance.
Arruda et al. \cite{ArrudaDexterous2016} proposed a geometry-based method that prioritizes object visibility and graspability, improving both the quality of reconstruction and the success of grasp. Alternatively, Jun Lv et al. \cite{lv2023samrlsensingawaremodelbasedreinforcement} introduced the differentiates between the manipulation arm and viewpoint arm, magnifying the operation area through viewpoint following to enhance grasping stability. 
Other methods~\cite{DBLP:journals/corr/abs-2409-17435,BreyerNBV2022,WangOTA2024} selected the view with the greatest information gain during operation to address issues such as occlusion.
In recent years, learning-based approaches have been increasingly used to optimize view planning. Some approaches~\cite{cheng2018reinforcement, ShangR2023, chen2020transferable} optimizes viewpoints planning process via reward functions during manipulation.
Additionally, Multi-View Picking~\cite{zaky2020active} applied a self-supervised state representation methods to focus on the target by changing views, enabling the completion of complex manipulation tasks.

%View planning for robotic manipulation has been explored through various approaches to optimize task performance. Arruda et al. \cite{ArrudaDexterous2016} proposed a geometry-based method that prioritizes object visibility and graspability, improving both reconstruction quality and grasp success. Subsequent work~\cite{morrison2019multi,BreyerNBV2022,WangOTA2024} focuses on maximizing information gain to address occlusion challenges. Recent advances leverage learning-based strategies: reinforcement learning~\cite{cheng2018reinforcement,ShangR2023,chen2020transferable} optimizes viewpoints via reward functions, while self-supervised multi-view picking~\cite{zaky2020active} dynamically adjusts perspectives to handle complex manipulation tasks.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/BFA-main-4.pdf}
    \vspace{-0.8cm}
    \caption{The overall pipeline of best feature aware fusion strategy applied in the end-to-end policy network. The multi-view images captured by the top-view and wrist cameras, are input to visual backbones for feature extraction. The multi-view features are then injected into a lightweight score network to produce the importance scores for each view. The importance scores are further used to reweight and fuse the multi-view features. The fused features are finally served as the input of the policy network to generate the action sequence for real-arm deployment. During training, the whole network is jointly optimized by the score loss and the policy loss.}
    \label{fig:BFA-main}
\end{figure*}



\subsection{Fine-Grained Robotic Manipulation}
Current methods often employ imitation learning strategies to complete fine-grained manipulation tasks. By leveraging expert demonstrations, imitation learning enables the agent to efficiently acquire complex skills.
% \cite{ZhaoALOHA2023, ChiDP2023, Ze2024DP3, NemecVMA2018, KramPass2018, KhorArm2020}. For instance
Some methods~\cite{AcT,rvt,rvt-2}, proposed an imitation learning framework based on transformer architecture \cite{VaswaniAttention2017}, leveraging multi-view information and joint data as demonstration inputs to predict future action sequences. Additionally, Some works~\cite{ChiDP2023,Ze2024DP3,zhao2024aloha} integrate the diffusion process into imitation learning. Moreover, Some works~\cite{bharadhwaj2024roboagent,haldar2024baku,ma2024hierarchical} have introduced a multi-task approach within these two paradigms, aiming to use a single model for handling multiple tasks.
However, all these methods integrate multi-view information by directly concatenating all visual representations, without considering the unequal information provided by different viewpoints.

% \begin{figure*}[t]
% \centering
% \includegraphics[width=\linewidth]{figs/framework.pdf}
% \vspace{-.2in}
% \caption{\textbf{Conceptual comparison of four depth supervision frameworks.} }
% \label{fig:framework}
% \end{figure*}