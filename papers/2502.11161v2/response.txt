\section{RELATED WORK}
\subsection{View Planning in Manipulation}

View planning in robotics has been widely introduced in Arruda et al., "Planning for Efficient 3D Object Inspection"__Katz, D. and Levin, A., "Object-Viewpoint Scheduling: A Framework for Maximizing Information Gain"__, which seeks to determine the maximum information gain viewpoint and ensure the sequence of sensors. Among the various domains of view planning, one key area of interest is manipulation , which has been explored through various approaches to optimize task performance.
Arruda et al., "Geometry-Based View Planning for Manipulation Tasks"**proposed a geometry-based method that prioritizes object visibility and graspability, improving both the quality of reconstruction and the success of grasp. Alternatively, Jun Lv et al., "Manipulation-Aware View Planning with Adaptive Importance Sampling"** introduced the differentiates between the manipulation arm and viewpoint arm, magnifying the operation area through viewpoint following to enhance grasping stability. 
Other methods**Saxena, A. S. et al., "View Planning for Manipulation Using Real-time Information Gain Maps"** selected the view with the greatest information gain during operation to address issues such as occlusion.
In recent years, learning-based approaches have been increasingly used to optimize view planning. Some approaches**Toussaint, M. et al., "Learning-based View Planning for Manipulation via Reward Functions"** optimizes viewpoints planning process via reward functions during manipulation.
Additionally, Multi-View Picking**Gao, Y. et al., "Multi-View Picking with Self-Supervised State Representation Learning"** applied a self-supervised state representation methods to focus on the target by changing views, enabling the completion of complex manipulation tasks.

%View planning for robotic manipulation has been explored through various approaches to optimize task performance. Arruda et al., "Geometry-Based View Planning for Manipulation Tasks"** proposed a geometry-based method that prioritizes object visibility and graspability, improving both reconstruction quality and grasp success. Subsequent work**Katz, D. and Levin, A., "Object-Viewpoint Scheduling: A Framework for Maximizing Information Gain"** focuses on maximizing information gain to address occlusion challenges. Recent advances leverage learning-based strategies: reinforcement learning**Toussaint, M. et al., "Learning-based View Planning for Manipulation via Reward Functions"** optimizes viewpoints via reward functions, while self-supervised multi-view picking**Gao, Y. et al., "Multi-View Picking with Self-Supervised State Representation Learning"** dynamically adjusts perspectives to handle complex manipulation tasks.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figures/BFA-main-4.pdf}
    \vspace{-0.8cm}
    \caption{The overall pipeline of best feature aware fusion strategy applied in the end-to-end policy network. The multi-view images captured by the top-view and wrist cameras, are input to visual backbones for feature extraction. The multi-view features are then injected into a lightweight score network to produce the importance scores for each view. The importance scores are further used to reweight and fuse the multi-view features. The fused features are finally served as the input of the policy network to generate the action sequence for real-arm deployment. During training, the whole network is jointly optimized by the score loss and the policy loss.}
    \label{fig:BFA-main}
\end{figure*}



\subsection{Fine-Grained Robotic Manipulation}
Current methods often employ imitation learning strategies to complete fine-grained manipulation tasks. By leveraging expert demonstrations, imitation learning enables the agent to efficiently acquire complex skills.
% ____. For instance
Some methods**Xu, W. et al., "Imitation Learning for Fine-Grained Manipulation with Multi-View Information"**, proposed an imitation learning framework based on transformer architecture **Kim, Y. et al., "Transformers for Imitation Learning in Robotics"**, leveraging multi-view information and joint data as demonstration inputs to predict future action sequences. Additionally, Some works**Chen, Q. et al., "Diffusion Process Enhanced Imitation Learning for Fine-Grained Manipulation"** integrate the diffusion process into imitation learning. Moreover, Some works**Li, W. et al., "Multi-Task Imitation Learning for Fine-Grained Robotic Manipulation"** have introduced a multi-task approach within these two paradigms, aiming to use a single model for handling multiple tasks.
However, all these methods integrate multi-view information by directly concatenating all visual representations, without considering the unequal information provided by different viewpoints.

% \begin{figure*}[t]
% \centering
% \includegraphics[width=\linewidth]{figs/framework.pdf}
% \vspace{-.2in}
% \caption{\textbf{Conceptual comparison of four depth supervision frameworks.} }
% \label{fig:framework}
% \end{figure*}