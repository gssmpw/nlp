\section{Related Work}
%Old related work

\textbf{Post-hoc sufficient reason techniques.} Some common post-hoc approaches include Anchors~(____), or SIS~(____), and typically seek to obtain concise subsets which uphold probabilistic sufficiency~(____) or baseline sufficiency~(____). A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(____) or \emph{prime implicants}~(____). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types, such as decision trees~(____), tree ensembles~(____), linear models~(____), or small-scale neural networks~(____).

\textbf{Self-explaining neural networks.} Unlike post-hoc methods, some techniques modify training to improve explanations~(____) or develop \emph{self-explaining} models that inherently provide interpretations for their decisions. Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, ____ and ____ describe model outputs by comparing them to relevant ``prototypes'', while____ derive concept coefficients from feature transformations. Other approaches, like those by____ and____, focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. ____ predict outcomes based on high-level concepts. Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(____). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(____). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(____). To our knowledge, this is the first self-explaining framework designed to generate such explanations.




%\textbf{Post-hoc sufficient reason techniques.} Some common post-hoc approaches include Anchors~(____), or SIS~(____), and typically seek to obtain concise subsets which uphold probablistic sufficiency~(____) or baseline sufficiency~(____). A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(____) or \emph{prime implicants}~(____). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types, such as decision trees~(____), KNNs____, tree ensembles~(____), linear models~(____), or small-scale neural networks~(____), typically obtained with the use of neural network verifiers____.

%\textbf{Self-explaining neural networks.} Unlike post-hoc methods, some techniques modify training to improve explanations~(____) or develop a self explaining neural network (SENN)____ architecture that inherently provides interpretations for its decisions____. Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, ____ and ____ describe model outputs by comparing them to relevant ``prototypes'', while____ derive concept coefficients from feature transformations. Other approaches, like those by____ and____, focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. ____ predict outcomes based on high-level concepts. 

%Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(____). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(____). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(____). To our knowledge, this is the first self-explaining framework designed to generate such explanations.


%Similar to the input-level perturbations in our method, other techniques also utilize perturbations for various tasks beyond interpretability. These tasks range from improving segmentation in weakly supervised localization tasks____, enhancing classification____, to acting as regularization____. Particularly related to our approach are the studies by Hase et al.____ and Vafa et al.____, which apply input perturbations to strengthen explanations against out-of-distribution counterfactuals in NLP. Unlike our method, these techniques do not inherently generate sufficient reasons within their outputs, avoiding the computational burden of obtaining such explanations post-training. Additionally, their counterfactual sampling is \emph{arbitrary}, which may inadvertently mask critical features, whereas our method intentionally only masks non-sufficient features at each iteration.
%\bigskip

%\textbf{Limitations.}