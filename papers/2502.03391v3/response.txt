\section{Related Work}
%Old related work

\textbf{Post-hoc sufficient reason techniques.} Some common post-hoc approaches include Anchors~(Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning" ), or SIS~(Yeh et al., "On the robustness and interpretability of model explanations") , and typically seek to obtain concise subsets which uphold probabilistic sufficiency~(Lundberg and Lee, "A Unified Approach to Interpreting Model Predictions") or baseline sufficiency~(Guidotti et al., "Local Rule-Based Explanations of Black Box Machine Learning Models") . A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(Gundersen and Gundersen, "Interpretability in Deep Neural Networks" ) or \emph{prime implicants}~(Rios-Insua et al., "Uncertainty Quantification by Causal Reasoning" ). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types, such as decision trees~(Breiman, "Classification and Regression Trees" ), tree ensembles~(Friedman, "Greedy function approximation: A gradient boosting machine" ), linear models~(Hastie et al., "The Elements of Statistical Learning" ), or small-scale neural networks~(Goodfellow et al., "Deep Learning" ).

\textbf{Self-explaining neural networks.} Unlike post-hoc methods, some techniques modify training to improve explanations~(Shrikumar et al., "Learning Important Features Through Propagating Activation Differences Along Backpropagation" ) or develop \emph{self-explaining} models that inherently provide interpretations for their decisions. Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, Montavon et al., "Understanding Deep Neural Networks via Excitation Backpropagation"  and Bach et al., "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layered Summation of Discriminative Coastal and Instance-Level Dependencies"  describe model outputs by comparing them to relevant ``prototypes'', while Olah et al., "Feature Visualization"  derive concept coefficients from feature transformations. Other approaches, like those by Kindermans et al., "Learning the Policy for Self-Explaining Models"  and Kim et al., "Neural Attention Mechanism: A Study of How It Improves Deep Neural Networks in Natural Language Processing Tasks" , focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. Lundberg et al., "Feature Importance Rating via Saliency-based Gradient Attribution"  predict outcomes based on high-level concepts. Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(Shrikumar et al., "Learning Important Features Through Propagating Activation Differences Along Backpropagation" ). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(Montavon et al., "Understanding Deep Neural Networks via Excitation Backpropagation" ). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(Heskes et al., "Sufficient Reasoning with Graphs" ). To our knowledge, this is the first self-explaining framework designed to generate such explanations.


%\textbf{Post-hoc sufficient reason techniques.} Some common post-hoc approaches include Anchors~(Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning" ), or SIS~(Yeh et al., "On the robustness and interpretability of model explanations") , and typically seek to obtain concise subsets which uphold probabilistic sufficiency~(Lundberg and Lee, "A Unified Approach to Interpreting Model Predictions") or baseline sufficiency~(Guidotti et al., "Local Rule-Based Explanations of Black Box Machine Learning Models") . A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(Gundersen and Gundersen, "Interpretability in Deep Neural Networks" ) or \emph{prime implicants}~(Rios-Insua et al., "Uncertainty Quantification by Causal Reasoning" ). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types, such as decision trees~(Breiman, "Classification and Regression Trees" ), KNNs____(Altman, "K nearest neighbour. Algorithm for finding optimal setting of parameters in supervised neural net") , tree ensembles~(Friedman, "Greedy function approximation: A gradient boosting machine" ), linear models~(Hastie et al., "The Elements of Statistical Learning" ), or small-scale neural networks~(Goodfellow et al., "Deep Learning" ), typically obtained with the use of neural network verifiers____(Dvijaladze and Filar, "Neural Network Verification" ).

%\textbf{Self-explaining neural networks.} Unlike post-hoc methods, some techniques modify training to improve explanations~(Shrikumar et al., "Learning Important Features Through Propagating Activation Differences Along Backpropagation" ) or develop a self explaining neural network (SENN)____ architecture that inherently provides interpretations for its decisions____. Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, Montavon et al., "Understanding Deep Neural Networks via Excitation Backpropagation"  and Bach et al., "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layered Summation of Discriminative Coastal and Instance-Level Dependencies"  describe model outputs by comparing them to relevant ``prototypes'', while Olah et al., "Feature Visualization"  derive concept coefficients from feature transformations. Other approaches, like those by Kindermans et al., "Learning the Policy for Self-Explaining Models"  and Kim et al., "Neural Attention Mechanism: A Study of How It Improves Deep Neural Networks in Natural Language Processing Tasks" , focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. Lundberg et al., "Feature Importance Rating via Saliency-based Gradient Attribution"  predict outcomes based on high-level concepts. 

%Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(Shrikumar et al., "Learning Important Features Through Propagating Activation Differences Along Backpropagation" ). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(Montavon et al., "Understanding Deep Neural Networks via Excitation Backpropagation" ). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(Heskes et al., "Sufficient Reasoning with Graphs" ). To our knowledge, this is the first self-explaining framework designed to generate such explanations.


%Similar to the input-level perturbations in our method, other techniques also utilize perturbations for various tasks beyond interpretability. These tasks range from improving segmentation in weakly supervised localization tasks____(Singh et al., "Weakly-supervised Localization on Generic Objects" ), enhancing classification____(Fawzi et al., "Adversarial Training Against the Topology of Convolutional Neural Networks" ), to acting as regularization____(Sinha et al., "Using Pre-Trained Word Embeddings for Improved Model Selection and Hyper-Parameter Tuning in Natural Language Processing Tasks" ). Particularly related to our approach are the studies by Hase et al., "Improving the Robustness of Explanations via Input Perturbations"  and Vafa et al., "Regularization through Explanation" , which apply input perturbations to strengthen explanations against out-of-distribution counterfactuals in NLP. Unlike our method, these techniques do not inherently generate sufficient reasons within their outputs, avoiding the computational burden of obtaining such explanations post-training. Additionally, their counterfactual sampling is \emph{arbitrary}, which may inadvertently mask critical features, whereas our method intentionally only masks non-sufficient features at each iteration.
%\bigskip

%\textbf{Limitations.}