\section{Related Work}
%Old related work

\textbf{Post-hoc sufficient reason techniques.} Some common post-hoc approaches include Anchors~(\cite{ribeiro2018anchors}), or SIS~(\cite{carter2019made}), and typically seek to obtain concise subsets which uphold probabilistic sufficiency~(\cite{wang2021probabilistic, ribeiro2018anchors, blanc2021provably}) or baseline sufficiency~(\cite{hase2021out, chockler2021explanations, deyoung2019eraser}). A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(\cite{ignatiev2019abduction, bassan2023formally}) or \emph{prime implicants}~(\cite{ignatiev2019abduction, shih2018symbolic}). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types, such as decision trees~(\cite{izza2020explaining}), tree ensembles~(\cite{izza2021explaining, ignatiev2022using}), linear models~(\cite{marques2020explaining}), or small-scale neural networks~(\cite{ignatiev2019abduction, la2021guaranteed, bassan2023towards}).

\textbf{Self-explaining neural networks.} Unlike post-hoc methods, some techniques modify training to improve explanations~(\cite{ismail2021improving, chen2019robust, hase2021out, vafa2021rationales, yan2023self}) or develop \emph{self-explaining} models that inherently provide interpretations for their decisions. Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, ~\cite{chen2019looks} and ~\cite{wang2021self} describe model outputs by comparing them to relevant ``prototypes'', while~\cite{alvarez2018towards} derive concept coefficients from feature transformations. Other approaches, like those by~\cite{agarwal2021neural} and~\cite{jain2020learning}, focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. ~\cite{koh2020concept} predict outcomes based on high-level concepts. Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(\cite{yeh2019fidelity, lundberg2017unified}). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(\cite{slack2020fooling, ribeiro2018anchors, yeh2019fidelity}). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(\cite{ribeiro2018anchors}). To our knowledge, this is the first self-explaining framework designed to generate such explanations.




%\textbf{Post-hoc sufficient reason techniques.} Some common post-hoc approaches include Anchors~(\cite{ribeiro2018anchors}), or SIS~(\cite{carter2019made, carter2021overinterpretation}), and typically seek to obtain concise subsets which uphold probablistic sufficiency~(\cite{wang2021probabilistic, ribeiro2018anchors, blanc2021provably}) or baseline sufficiency~(\cite{hase2021out, chockler2021explanations, deyoung2019eraser, chockler2024explaining}). A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(\cite{ignatiev2019abduction, bassan2023formally, ordyniak2023parameterized}) or \emph{prime implicants}~(\cite{ignatiev2019abduction, shih2018symbolic}). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types, such as decision trees~(\cite{izza2020explaining, huang2021efficiently, bounia2023approximating}), KNNs~\citep{barcelo2025explaining}, tree ensembles~(\cite{izza2021explaining, ignatiev2022using, audemard2022trading, audemard2022preferred, boumazouza2021asteryx}), linear models~(\cite{marques2020explaining}), or small-scale neural networks~(\cite{ignatiev2019abduction, wu2024verix, la2021guaranteed, bassan2023towards}), typically obtained with the use of neural network verifiers~\citep{wang2021beta, wu2024marabou, fel2023don}.

%\textbf{Self-explaining neural networks.} Unlike post-hoc methods, some techniques modify training to improve explanations~(\cite{ismail2021improving, chen2019robust, hase2021out, vafa2021rationales, yan2023self}) or develop a self explaining neural network (SENN)~\citep{alvarez2018towards} architecture that inherently provides interpretations for its decisions~\citep{lee2022self, shwartz2020unsupervised, rajagopal2021selfexplain, guyomard2022vcnet, guo2023counternet, zhang2022protgnn}. Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, ~\cite{chen2019looks} and ~\cite{wang2021self} describe model outputs by comparing them to relevant ``prototypes'', while~\cite{alvarez2018towards} derive concept coefficients from feature transformations. Other approaches, like those by~\cite{agarwal2021neural} and~\cite{jain2020learning}, focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. ~\cite{koh2020concept} predict outcomes based on high-level concepts. 

%Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(\cite{yeh2019fidelity, lundberg2017unified}). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(\cite{slack2020fooling, ribeiro2018anchors, yeh2019fidelity}). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(\cite{ribeiro2018anchors}). To our knowledge, this is the first self-explaining framework designed to generate such explanations.


%Similar to the input-level perturbations in our method, other techniques also utilize perturbations for various tasks beyond interpretability. These tasks range from improving segmentation in weakly supervised localization tasks~\cite{li2018tell, hou2018self, wei2017object, kumar2017hide}, enhancing classification~\cite{wang2019sharpen}, to acting as regularization~\cite{devries2017improved}. Particularly related to our approach are the studies by Hase et al.~\cite{hase2021out} and Vafa et al.~\cite{vafa2021rationales}, which apply input perturbations to strengthen explanations against out-of-distribution counterfactuals in NLP. Unlike our method, these techniques do not inherently generate sufficient reasons within their outputs, avoiding the computational burden of obtaining such explanations post-training. Additionally, their counterfactual sampling is \emph{arbitrary}, which may inadvertently mask critical features, whereas our method intentionally only masks non-sufficient features at each iteration.
%\bigskip

%\textbf{Limitations.}