\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{url}
%\usepackage{graphicx} % for including graphics
\usepackage{subcaption} % for subfigures
%\usepackage{subfig}
%\usepackage{subcaption}

%\usepackage[demo]{graphicx}



%\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2023}



\usepackage{bbm}
\usepackage{wrapfig}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
  %\usepackage[nonatbib]{neurips_2023}
%  \usepackage[nonatbib]{neurips_2024}




\usepackage{float}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{paralist}
\usepackage{diagbox} 
\usepackage{array}
%\usepackage{cite}

\newtheorem*{remark}{Remark}

%\newtheorem*{lemma}{Lemma}
\newtheorem{lemma}{Lemma}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}
\renewcommand\theproposition{\arabic{proposition}}

\newenvironment{propositionenum}
{\begin{enumerate}[1.]\renewcommand{\labelenumi}{\theproposition.\arabic{enumi}}}
{\end{enumerate}}




\newcommand{\mysubsection}[1]{\medskip\noindent\textbf{#1}}


\usepackage[inline]{enumitem}
% \setlist[enumerate,1]{label=\textit{\alph*)}}

% \newlist{proplist}{enumerate}{1}
% \setlist[proplist]{label=\textbf{Proposition \arabic*.}, ref=\arabic*, leftmargin=*}



\newcommand{\relu}{ReLU}
\newcommand{\true}{True}
\newcommand{\false}{False}
\newcommand{\yes}{\textit{Yes}}
\newcommand{\no}{\textit{No}}


\newcommand{\guy}[1]{\marginpar{\textcolor{orange}{GuyK: #1}}}
\newcommand{\ron}[1]{\marginpar{\textcolor{blue}{R: #1}}}
\newcommand{\shahaf}[1]{\marginpar{\textcolor{green}{Shahaf: #1}}}
\newcommand{\elad}[1]{\marginpar{\textcolor{red}{Elad: #1}}}
\newcommand{\todo}[1]{{\color{red}{\textbf{[TODO]} #1}}}

\newcommand{\PComplexity}{PTIME}
\newcommand{\NPComplexity}{NP}
\newcommand{\NPCompleteComplexity}{NP-complete}
\newcommand{\coNPComplexity}{co-NP}
\newcommand{\coNPCompleteComplexity}{co-NP-complete}
\newcommand{\StoPComplexity}{$\Sigma_{2}^{P}$} % S2P
\newcommand{\StoPCompleteComplexity}{$\Sigma_{2}^{P}$-complete} % S2P
\newcommand{\countPComplexity}{\#P}
\newcommand{\countPCompleteComplexity}{\#P-complete}
\newcommand{\bb}{\textbf{b}}
\newcommand{\z}{\textbf{z}}
\newcommand{\x}{\textbf{x}}
\newcommand{\X}{\{1,\dots,n\}}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
%\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
%\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays
\DeclareMathOperator*{\pr}{Pr} % thin space, limits underneath in displays


\usepackage{multirow} 

\usepackage{adjustbox}


\iclrfinalcopy

%\title{Small Subsets are All you Need:\\ Self Explaining Neural Networks with Minimal Sufficient Reasons}
%\title{\centering Small Subsets are All You Need:\\ Self-Explaining Neural Networks with Minimal Sufficient Reasons}

%\title{Concise, Efficient, and Faithful Sufficient Reasons using Self-Explaining Neural Networks}

%\title{Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons}
\title{\centering Explain Yourself, \textbf{Briefly!} \\ Self-Explaining Neural Networks with Concise Sufficient Reasons}

%\title{\centering Small Subsets are All You Need: \\ Self-Explaining Neural Networks with Minimal Sufficient Reasons}

%\title{\centering Self-Explaining Neural Networks with Minimal Sufficient Reasons}



\author{Shahaf Bassan$^{1,2}$, \ Ron Eliav$^{1,3}$, \ Shlomit Gur$^{1}$ \\
IBM Research$^1$, \ The Hebrew University of Jerusalem$^2$, \ Bar-Ilan University$^3$\\
\texttt{shahaf.bassan@mail.huji.ac.il}} %\\
%\And
%Shlomit Gur \\
%IBM Research \\
%\AND
%Ron Eliav \\
%Bar-Ilan University, IBM Research
%}



\begin{document}


\maketitle

\begin{abstract}
\emph{Minimal sufficient reasons} represent a prevalent form of explanation --- the smallest subset of input features which, when held constant at their corresponding values, ensure that the prediction remains unchanged. Previous \emph{post-hoc} methods attempt to obtain such explanations but face two main limitations: \begin{inparaenum}[(i)]
    \item Obtaining these subsets poses a computational challenge, leading most scalable methods to converge towards suboptimal, less meaningful subsets;
    \item These methods heavily rely on sampling out-of-distribution input assignments, potentially resulting in counterintuitive behaviors.
\end{inparaenum} 
To tackle these limitations, we propose in this work a self-supervised training approach, which we term \emph{sufficient subset training} (SST). 
Using SST, we train models to generate concise sufficient reasons for their predictions as an integral part of their output. Our results indicate that our framework produces succinct and faithful subsets substantially more efficiently than competing post-hoc methods, while maintaining comparable predictive performance. Code is available at: \url{https://github.com/IBM/SAX/tree/main/ICLR25}.\end{abstract} 

%










%With this method, we train models to generate concise sufficient reasons for their predictions as an integral part of their output. We employ a dual-propagation procedure during training, enabling any architecture to produce subsets that are both sufficient (faithful to the original prediction) and possess minimal cardinality. We conduct experiments with various sufficiency criteria and apply them to models from different domains. Our results demonstrate that our framework yields concise and faithful subsets significantly faster than competing post-hoc methods, all while maintaining comparable predictive performance.\end{abstract}

\section{Introduction}

Despite the widespread adoption of deep neural networks, understanding how they make decisions remains challenging. Initial attempts focused on additive feature attribution methods~(\cite{ribeiro2016should, lundberg2017unified,sundararajan2017axiomatic}), which often treat the model's behavior as approximately linear around the input of interest. A different line of techniques, such as Anchors~(\cite{ribeiro2018anchors}) and SIS~(\cite{carter2019made}), seek to pinpoint \emph{sufficient} subsets of input features that ensure that a prediction remains unchanged. Here, the goal often involves finding the \emph{smallest} possible sufficient subset, which is typically assumed to provide a better interpretation~(\cite{ribeiro2016should, carter2019made, ignatiev2019abduction, barcelo2020model, waldchen2021computational}).


%Initial attempts to interpret DNNs concentrated on feature attribution techniques~\cite{ribeiro2016should, lundberg2017unified, sundararajan2017axiomatic}. These methods typically assume that the model's behavior can be approximated as linear in a small local area around the input being interpreted. However, alternative techniques, such as Anchors~\cite{ribeiro2018anchors}, and SIS~\cite{carter2019made} aim to identify subsets of input features that are \emph{sufficient} to determine that a prediction remains unchanged. In this context, the objective frequently involves identifying \emph{cardinally minimal} subsets of features that remain sufficient for a prediction.

Building on this foundation, we adopt the common notation of a \emph{sufficient reason}~(\cite{barcelo2020model, darwiche2020reasons, arenas2022computing}) to refer to any subset of input features $S$ that ensures the prediction of a classifier remains constant. We categorize previous approaches into three broad categories of sufficiency, namely: \begin{inparaenum}[(i)] \item \emph{baseline} sufficient reasons, which involve setting the values of the complement $\overline{S}$ to a fixed baseline; \item \emph{probabilistic} sufficient reasons, which entail sampling $\overline{S}$ from a specified distribution; and \item \emph{robust} sufficient reasons, which aim to confirm that the prediction does not change for any possible assignment of values to $\overline{S}$ within a certain domain\end{inparaenum}.

Several methods have been proposed for obtaining sufficient reasons for ML classifiers (baseline, probabilistic, and robust)~(\cite{ribeiro2018anchors, carter2019made, ignatiev2019abduction, wang2021probabilistic, chockler2021explanations}). These methods are typically \emph{post-hoc}, meaning their purpose is to interpret the model's behavior \emph{after} training. There are two major issues concerning such post-hoc approaches: \begin{inparaenum}[(i)] \item First, identifying cardinally minimal sufficient reasons in neural networks is computationally challenging~(\cite{barcelo2020model, waldchen2021computational}), making most methods either unscalable or prone to suboptimal results. \item Second, when assessing if a subset $S$ is a sufficient reason, post-hoc methods typically fix $S$'s values and assign different values to the features of $\overline{S}$. This mixing of values may result in \emph{out-of-distribution} (OOD) inputs, potentially misleading the evaluation of $S$'s sufficiency. This issue has been identified as a significant challenge in such methods~(\cite{hase2021out, vafa2021rationales, yu2023eliminating, amir2024hard}), frequently leading to the identification of subsets that do not align with human intuition~(\cite{hase2021out}).\end{inparaenum}


%\begin{wrapfigure}{t!}{0.53\textwidth}
%    \vspace{-0.6cm}  
%    \begin{center}
%        \includegraphics[width=0.53\textwidth]{figures/teaser.png}
%        \caption{An inherent sufficient reason generated by a model trained with \emph{sufficient subset training} (\emph{SST}).}
%        \label{text_example}
%    \end{center}
%    \vspace{-0.4cm}  
%\end{wrapfigure}
\begin{figure}[t]
  \centering
    \includegraphics[height=8cm]{figures/new_teaser_2.png}
  \caption{An example of a sufficient reason generated by a model trained with \emph{sufficient subset training} (SST) on the IMAGENET dataset, compared to those generated by post-hoc methods on standard-trained models. While explanations from Anchors and GS are larger, those from SIS are less faithful and lack subset sufficiency (details in the experiments in Section~\ref{experiments_section_main}). SST generates explanations that are both concise and faithful, while performing this task with significantly improved efficiency. Additional examples appear in appendix~\ref{supplementary_results_appendix}.}
  \label{teaser_image}
\end{figure}


\textbf{Our contributions.} We start by addressing the previously mentioned intractability and OOD concerns, reinforcing them with new computational complexity findings on the difficulty of generating such explanations: \begin{inparaenum}[(i)]
    \item First, we extend computational complexity results that were limited only to the \emph{binary} setting~(\cite{barcelo2020model, waldchen2021computational}) to any discrete or \emph{continuous} domain, showcasing the intractability of obtaining cardinally minimal explanations for a much more general setting;
    \item Secondly, we prove that the intractability of generating these explanations holds for different \emph{relaxed} conditions, including obtaining cardinally minimal \emph{baseline} sufficient reasons as well as \emph{approximating} the size of cardinally minimal sufficient reasons. These results reinforce the computational intractability of obtaining this form of explanation in the classic post-hoc fashion.
\end{inparaenum}

Aiming to mitigate the intractability and OOD challenges, we propose a novel self-supervised approach named \emph{sufficient subset training} (\emph{SST}), designed to inherently produce concise sufficient reasons for predictions in neural networks. SST trains models with two outputs: \begin{inparaenum}[(i)] \item the usual predictive output and \item an extra output that facilitates the extraction of a sufficient reason for that prediction. \end{inparaenum} The training integrates two additional optimization objectives: ensuring the extracted subset is sufficient concerning the predictive output and aiming for the subset to have minimal cardinality.

We integrate these objectives using a dual propagation through the model, calculating two additional loss terms. The first, the \emph{faithfulness loss}, validates that the chosen subset sufficiently determines the prediction. The second, the \emph{cardinality loss}, aims to minimize the subset size. We employ various masking strategies during the second propagation to address three sufficiency types. These include setting $\overline{S}$ to a constant baseline for baseline sufficient reasons, drawing $\overline{S}$ from a distribution for probabilistic sufficient reasons, or using an adversarial attack on $\overline{S}$ for robust sufficient reasons.

Training models with SST reduces the need for complex post-hoc computations and exposes models to relevant mixed inputs, thereby reducing the sensitivity of subsets to OOD instances. Moreover, our method's versatility enables its use across any neural network architecture. We validate SST by testing it on multiple architectures for image and language classification tasks, using diverse masking criteria. We compare our training-generated explanations with those from popular post-hoc methods on standard (i.e., classification only) models. Our results show that SST produces sufficient reasons that are concise and faithful, more efficiently than post-hoc approaches, while preserving comparable predictive performance. Figure~\ref{teaser_image} illustrates an example of a sufficient reason generated by SST, in comparison to post-hoc explanations.









%fine-tune this rationale to ensure it is not only consistent with the initial classification decision but also succinct in its composition. Moreover, by training the model in scenarios where only a subset of the input features are set to specific values, we enhance the model's robustness, making it less susceptible to out-of-distribution (OOD) data perturbations.




%In this work, we provide a self-supervised approach for training self-explaining neural networks that inherently produce sufficient reasons by-design. Our training technique incorporates two outputs - the prediction output, as well as a second output, from which a sufficient reason can be extracted. During training, we perform a dual forward-propagation step over which the model learns . We minimize a loss function that incorporates both \begin{inparaenum}[(i)]
%    \item The appropriate prediction loss,
%    \item A second prediction loss for the masked sufficient reason output, as well as a
%    \item a cardinality loss, which maintains the minimality of the corresponding subset.
%\end{inparaenum}

%optimize this subset to be both faithful to the original classification prediction, as well as with minimal cardinality. Additionally, the exposure of the model to instances where only part of the input features are fixed to their values, the model becomes significantly less sensitive to OOD assignments.



%Deep neural networks (DNNs) are extensively used in a variety of different tasks. However, explaining the decisions made by DNNs remains challenging. Many of the most common XAI techniques do not provide formal guarantees that the underlying explanation satisfies. This is may often result in counterintuitive explanations that the case in sailency methods or feature attribution based methods,may result in counterintuitive  to interpret the decisions of a DNN relies on pinpointing important input features for determining a decision. While methods such as sailency maps or feature attribution techniques are useful, they often lack some formal critiria that they justify, which may result in explanations that are unintuitive

%A well familiar \emph{formal} critiria of explanations is \emph{sufficiency}. In it's most classic form, a sufficient reason denotes a subset of features that when fixed to their value, determine that the prediction remains the same. There  methods use sufficiency either as an evaluation metric, or as an optimization goal. In other Indeed, 
%Post-hoc sufficiency based methods suffer from two major limitations:\begin{inparaenum}[(i)]
 %   \item First, 
%\end{inparaenum}

%To overcome this concern, some recent work has been conducted on methods for producing sufficient reasons with . However,  

\section{Forms of Sufficient Reasons}

In this work, our focus is on \emph{local} interpretations of classification models. Specifically, for a model $f:\mathbb{R}^n\to\mathbb{R}^c$ and an input $\x\in\mathbb{R}^n$, we aim to explain the prediction $f(\x)$. Formally, we define a \emph{sufficient reason}~(\cite{barcelo2020model, darwiche2020reasons, arenas2022computing}) as any subset of features $S\subseteq [n]$, such that fixing $S$ to their values in $\x$, determines the prediction $f(\x)$. We specifically want the prediction $f(\x_S;\z_{\Bar{S}})$ to align with $f(\x)$, where $(\x_S;\z_{\Bar{S}})$ denotes an input for which the values of $S$ are drawn from $\x\in\mathbb{R}^n$ and the values of $\overline{S}$ are drawn from $\z\in\mathbb{R}^n$. If $S$ includes all input features, i.e., $S := \{1, \ldots, n\}$, it is trivially sufficient. However, it is common to seek more concise sufficient reasons~(\cite{ribeiro2018anchors, carter2019made, ignatiev2019abduction, barcelo2020model, blanc2021provably, waldchen2021computational}). Another consideration is the choice of values for the complement set $\overline{S}$. We classify previous methods into three general categories:

%Another factor is choosing the values for the complementary set $\overline{S}$, specifically the selection of $\mathbf{z}_{\Bar{S}}$ for $f(\x_S;\z_{\Bar{S}})$. We categorize previous approaches that were sugested for this task into three general forms of sufficient reasons, starting with \emph{baseline} sufficient reasons:

\begin{definition}
Given a model $f$, an instance $\mathbf{x}\in\mathbb{R}^n$ and some baseline $\mathbf{z}\in\mathbb{R}^n$, we define $S\subseteq \{1,\ldots,n\}$ as a \emph{baseline sufficient reason} of $\langle f,\mathbf{x}\rangle$ with respect to $\mathbf{z}$ iff it holds that: 
\end{definition}

\begin{equation}
\argmax_j \ f(\mathbf{x}_{S};\mathbf{z}_{\Bar{S}})_j=\argmax_j \ f(\mathbf{x})_j
\end{equation}

The baseline $\mathbf{z}$ represents the ``missingness'' of the complementary set $\overline{S}$, but its effectiveness is highly dependent on choosing a \emph{specific} $\mathbf{z}$. While it is feasible to use a small number of different baselines to mitigate this dependency, such an approach is not practical across broader domains. However, in certain cases, employing baselines may be intuitive, especially when they carry specific significance in that scenario. For instance, in the context of language classification tasks where $f$ represents a transformer architecture, $\z$ could be assigned the widely recognized \textsc{MASK} token, which is utilized during pre-training phases to address the concept of ``missingness'' in tokens~(\cite{devlin2018bert}).

Conversely, sufficiency can imply that the values of $S$ dictate the prediction, regardless of \emph{any} assignment to $\overline{S}$ in some domain. This definition is particularly useful in cases where a strict sufficiency of $S$ is imperative, such as in safety-critical systems~(\cite{marques2022delivering, ignatiev2019abduction, wu2024verix}). There, the presence of any counterfactual assignment over $\overline{S}$ may be crucial. While in some instances the domain of $\overline{S}$ is unlimited~(\cite{ignatiev2019abduction, marques2022delivering}), a more feasible modification is to consider a limited continuous domain surrounding $\x$~(\cite{wu2024verix, la2021guaranteed, izza2024distance}). We define this kind of sufficiency rationale as \emph{robust} sufficient reasons:


%In the opposite scenario, sufficiency can be defined such that fixing the values of $S$ determines the prediction, regardless of \emph{any} assignment to $\overline{S}$ under some given domain. This definition can be particularly useful in cases where strict sufficiency of $S$ is required, for example in safety critical systems~\cite{marques2022delivering, ignatiev2019abduction, wu2024verix}, where the existence of even one counterfactual assignment over $\overline{S}$ may be important to detect. In some cases, the domain of the complementary set is taken to be the entire feasible input domain~\cite{ignatiev2019abduction, marques2022delivering} (these are often refered as \emph{abductive explanations}~\cite{ignatiev2019abduction}, but a more practical relaxation of this definition considers some bounded continuous environment around the interpreted input $\x$~\cite{wu2024verix, la2021guaranteed}. We define this type of sufficient reasons as \emph{robust} sufficient reasons:

\begin{definition}
Given a model $f$ and an instance $\mathbf{x}\in\mathbb{R}^n$, we define $S\subseteq \{1,\ldots,n\}$ as a \emph{robust sufficient reason} of $\langle f,\mathbf{x}\rangle$ on an $\ell_p$-norm ball $B_p^{\epsilon_p}$ of radius $\epsilon_p$ iff it holds that: 
\end{definition}

\begin{equation}
\begin{aligned}
\label{explanation_definition}
\forall{\mathbf{z}\in B_p^{\epsilon_p}}(\mathbf{x}). \quad [\argmax_j \ f(\mathbf{x}_{S};\mathbf{z}_{\Bar{S}})_j=\argmax_j \ f(\mathbf{x})_j],  \\
%\forall{\mathbf{z}\in
s.t \quad B_p^{\epsilon_p}(\mathbf{x}):=\{\mathbf{z}\in \mathbb{R}^{n} | \ \ ||\mathbf{x}-\mathbf{z}||_p\leq \epsilon_p\}
\end{aligned}
\end{equation}

However, it is not always necessary to assume that fixing the subset $S$ guarantees a consistent prediction across an entire $B^{\epsilon_p}_p(\x)$ domain. Rather, when $S$ is fixed, the classification may be maintained with a certain probability (\cite{ribeiro2018anchors, blanc2021provably, waldchen2021computational, wang2021probabilistic}). We refer to these as \emph{probabilistic} sufficient reasons. Under this framework, fixing $S$ and sampling values from $\overline{S}$ according to a given distribution $\mathcal{D}$ ensures that the classification remains unchanged with a probability greater than $1-\delta$:

\begin{definition}
Given a model $f$ and some distribution $\mathcal{D}$ over the input features, we define $S\subseteq \{1,\ldots,n\}$ as a \emph{probabilistic sufficient reason} of $\langle f,\mathbf{x}\rangle$ with respect to $\mathcal{D}$ iff: 
\end{definition}

\begin{equation}
\begin{aligned}
\label{explanation_definition}
\pr_{\z\sim \mathcal{D}} [\argmax_j \ f(\z)_j= \argmax_j \ f(\x)_j \ | \ \z_S=\x_S]\geq1-\delta
\end{aligned}
\end{equation}

where $\z_S=\x_S$ denotes fixing the features of $S$ in the vector $\z$ to their corresponding values in $\x$. 

%\begin{equation}
%\begin{aligned}
%\label{explanation_definition}
%\pr_{\mathbf{z}_{\Bar{S}}\sim D} [\argmax_j \ f(\mathbf{x}_{S};\mathbf{z}_{\Bar{S}})_j=t]\geq1-\delta
%\end{aligned}
%\end{equation}


\section{Problems with post-hoc minimal sufficient reason computations}

\textbf{Intractability.} Obtaining cardinally minimal sufficient reasons is known to be computationally challenging~(\cite{barcelo2020model, waldchen2021computational}), particularly in neural netowrks~(\cite{barcelo2020model, adolfi2024computational, bassanlocal}). Two key factors influence this computational challenge --- the first is finding a \emph{cardinally minimal} sufficient reason, which may be intractable when the input space is large. This tractability barrier is common to all of the previous forms of sufficiency (baseline, probabilistic, and robust). A second computational challenge can emerge when verifying the sufficiency of even \emph{a single} subset proves to be computationally difficult. For instance, in \emph{robust} sufficient reasons, confirming subset sufficiency equates to robustness verification, a process that by itself is computationally challenging~(\cite{katz2017reluplex, salzer2021reachability}). The intractability of obtaining cardinally minimal sufficient reasons is further exemplified by the following theorem:

\begin{theorem}
Given a neural network classifier $f$ with ReLU activations and $\x\in\mathbb{R}^n$, obtaining a cardinally minimal sufficient reason for $\langle f,\x\rangle$ is \begin{inparaenum}[(i)] \item NP-Complete for baseline sufficient reasons \item $\Sigma^{P}_{2}$-Complete for robust sufficient reasons and \item $\text{NP}^{\text{PP}}$-Hard for probabilistic sufficient reasons.
\end{inparaenum} 
\end{theorem}

The highly intractable complexity class $\Sigma^{P}_{2}$ encompasses problems solvable in NP time with constant-time access to a coNP oracle, whereas $\text{NP}^{\text{PP}}$ includes problems solvable in NP time with constant-time access to a probabilistic PP oracle. More details on these classes are available in Appendix~\ref{background_appendix}. For our purposes, it suffices to understand that NP $\subsetneq\Sigma^{P}_{2}$ and NP $\subsetneq\text{NP}^{\text{PP}}$ is widely accepted~(\cite{arora2009computational}), highlighting their substantial intractability (problems that are significantly harder than NP-Complete problems).

\emph{Proof sketch.} The full proof is relegated to Appendix~\ref{first_proof_appendix_section}. We begin by extending the complexity results for \emph{robust} and \emph{probabilistic} sufficient reasons provided by~(\cite{barcelo2020model, waldchen2021computational}), which focused on either CNF classifiers or multi-layer perceptrons with \emph{binary} inputs and outputs. We provide complexity proofs for a more general setting considering neural networks over any discrete or \emph{continuous} input or output domains. Membership outcomes to continuous domains stem from realizing that instead of guessing certificate assignments for binary inputs, one can guess the activation status of any ReLU constraint. For hardness, we perform a technical reduction that constructs a neural network with a strictly stronger subset sufficiency. We also provide a novel complexity proof, which indicates the intractability of the \emph{relaxed} version of computing cardinally minimal \emph{baseline} sufficient reasons, via a reduction from the classic CNF-SAT problem.

To further emphasize the hardness of the previous results, we prove that their \emph{approximation} remains intractable. We establish this through approximation-preserving reductions from the Shortest-Implicant-Core~(\cite{umans1999hardness}) and Max-Clique~(\cite{haastad1999clique}) problems (proofs provided in Appendix~\ref{second_proof_appendix_section}):

\begin{theorem}
Given a neural network $f$ with ReLU activations and  $\x\in\mathbb{R}^n$, \begin{inparaenum}[(i)] \item $\forall\epsilon>0$ approximating cardinally minimal robust sufficient reasons with $n^{\frac{1}{2}-\epsilon}$ factor is $\Sigma^P_2$-Hard, \item $\forall\epsilon>0$ approximating cardinally minimal probabilistic or baseline sufficient reasons with factor $n^{1-\epsilon}$ is NP-Hard.
\end{inparaenum}
\end{theorem}

These theoretical limitations highlight the challenges associated with achieving exact computations or approximations and do not directly apply to the practical execution time of methods that employ heuristic approximations. Nevertheless, many such methods, including Anchors~(\cite{ribeiro2018anchors}) and SIS~(\cite{carter2019made}), face issues with scalability and often rely on significant assumptions. For instance, these methods may substantially reduce the input size to improve the tractability of features~(\cite{ribeiro2018anchors, carter2019made}) or concentrate on cases with a large margin between the predicted class's output probability and that of other classes~(\cite{carter2019made}).

\textbf{Sensitivity to OOD counterfactuals.} Methods computing concise sufficient reasons often use algorithms that iteratively test inputs of the form $(\x_S;\z_{\Bar{S}})$, comparing the prediction of $f(\x_S;\z_{\Bar{S}})$ with $f(\x)$ to determine $S$'s sufficiency~(\cite{ribeiro2018anchors, carter2019made, chockler2021explanations}). A key issue is that $(\x_S;\z_{\Bar{S}})$ might be OOD for $f$, potentially misjudging S's sufficiency. Iterative approaches aimed at finding \emph{cardinally minimal} subsets may exacerbate this phenomenon, leading to significantly suboptimal or insufficient subsets. A few recent studies focused on NLP domains~(\cite{hase2021out, vafa2021rationales}), aimed to enhance model robustness to OOD counterfactuals by arbitrarily selecting subsets $S$ and exposing models to inputs of the form $(\x_S;\z_{\Bar{S}})$ during training. However, this arbitrary masking process lacks clarity on two important questions --- \begin{inparaenum}[(i)] \item \emph{which} features should be masked, and \item how \emph{many} features should be masked
\end{inparaenum}. This, in turn, poses a risk of inadvertently masking critical features and potentially impairing model performance.

%The work of Hase et. al.~\cite{hase2021out} categorizes the OOD problem of sufficiency based explanations more formally, and refers to it as \emph{social misalignment}, which means that the explanation provides different information than what is expected by humans.


%\begin{equation}
%\label{explanation_definition}
%P_{\mathbf{z}\sim D}(f(\mathbf{z})=f(\mathbf{x})|\mathbf{z}_{S}=\mathbf{x}_S))>1-\delta
%\end{equation}

%\begin{equation}
%\label{explanation_definition}
%P(f(\mathbf{x}_{S};\mathbf{z}_{\Bar{S}})=f(\mathbf{x}))>1-\delta
%\end{equation}

\section{Self-Explaining Neural Networks with Sufficient Reasons}
\label{sec:architercutre_section}

To address the earlier concerns, we train self-explaining neural networks that inherently provide concise sufficient reasons along with their predictions. This eliminates the need for costly post-computations and has the benefit of exposing models to OOD counterfactuals during training.

\subsection{Learning Minimal Sufficient Reasons}


Our model is trained to produce two outputs: the initial prediction and an explanation vector that pinpoints a concise sufficient reason for that prediction. Each element in the vector corresponds to a feature. Through a Sigmoid output layer, we select a feature subset $S$ where outputs surpass a threshold $\tau$. More formally, we learn some: $h: \mathbb{R}^n\to \mathbb{R}^c\times [0,1]^n$ where for simplicity we denote: $h(\mathbf{x}):=(h_1(\mathbf{x}),h_2(\mathbf{x}))$, for which $h_1$ denotes the \emph{prediction} component and $h_2$, derived from a final Sigmoid layer, denotes the \emph{explanation} component; both share the same hidden layers. The sufficient reason is taken by identifying $S:=\{i \ | \ h_2(\mathbf{x})_i \geq \tau \}$. To ensure $S$ is sufficient, we perform a dual propagation through $h$, wherein the second forward pass, a new \emph{masked input} is constructed by fixing the features in $S$ to their values in $\x$ and letting the features in $\overline{S}$ take some other values $\z$. The masked input $(\x_S;\z_{\Bar{S}})$ is then propagated through $h$. %by computing $h^2(\x_S;\z_{\Bar{S}})$
This process is illustrated in Figure~\ref{architecture}.


%Our model is trained to generate two outputs: the initial prediction and an explanation vector that identifies a concise sufficient reason for the prediction. Each element in the explanation vector represents a feature. Using a Sigmoid output layer, we select a subset of features $S$ where the output exceeds a threshold $\tau$.


%We train our model to provide two outputs: the initial prediction and a second output that allows for the extraction of the minimal sufficient reason for that prediction. The second output is in the size of the input dimension where each value in this vector represents its corresponding input feature. We use a final Sigmoid output layer for the explanation vector and choose a subset of features $S$ whose output value is larger than some threshold $\tau$.



\begin{figure}[hbt!]
\centering
    \includegraphics[height=5.5cm]{Picture1.png}
  \caption{An illustration of the dual propagation incorporated during sufficient subset training}
  \label{architecture}
\end{figure}


Besides the prediction task, we optimize two additional objectives to ensure that $S$ is both sufficient and concise. Our training incorporates three distinct loss terms: \begin{inparaenum}[(i)] \item the prediction loss, $L_{pred}$, \item a faithfulness loss, $L_{faith}$, enhancing the sufficiency of $S$ concerning $\langle h_1, \x \rangle$, and a cardinality loss, $L_{card}$, minimizing the \emph{size} of $S$. \end{inparaenum} We optimize these by minimizing the combined loss:

%In addition to the prediction task, we require two additional optimization objectives - both ensuring that the subset $S$ is \emph{sufficient} with respect to the original prediction, as well as ensuring that $S$ is as small as possible. Specifically, our training is comprised of three different loss terms: \begin{inparaenum}[(i)]
%    \item the original prediction loss, $L_{pred}$, 
%    \item a faithfulness loss, $L_{faith}$, responsible for enhancing the sufficiency of $S$ in relation to $\langle h_1^1, \x \rangle$, and 
%    \item a cardinality loss, $L_{card}$, designed to encourage $S$ to have the smallest possible cardinality.
%\end{inparaenum} Our entire training objective is then learned by minimizing:

\begin{equation}
\begin{aligned}
L_{\theta}(h):= L_{pred}(h)+
\lambda L_{faith}(h)+
\xi L_{card}(h)
\end{aligned}
\end{equation}
\label{submission}

Here, we use the standard cross entropy (CE) loss as our \emph{prediction loss}, i.e., $L_{pred}(h):= L_{CE}(h_1(\x),t)$, where $t$ is the ground truth for $\x$. We use CE also in the \emph{faithfulness loss}, which ensures the sufficiency of $S$ by way of minimizing the discrepancy between the predicted probabilities of the masked input $h_1(\x_S;\z_{\Bar{S}})$ and the first prediction $h_1(\x)$: 

\begin{equation}
\begin{aligned}
L_{faith}(h):= L_{CE}(h_1(\x_S;\z_{\Bar{S}}),\argmax_{j} h_1(\x)_j), \ \ s.t \ \ S:=\{i \ | \ h_2(\x)_i \geq \tau \}
\end{aligned}
\end{equation}


%\ron{Do we want to explain that in equation 5, the reason of the $argmax_j{h_1(x)}_j$ is that we take one-hot vector?}
Training a model solely to optimize prediction accuracy and faithfulness may often result in larger sufficient reasons, as larger subsets are more likely to be sufficient. Specifically, the largest subset $S:= \{1,\ldots,n\}$ always qualifies as sufficient, irrespective of the model or instance. Conversely, excessively small subsets may compromise prediction performance, leading to a bias towards larger subsets when these are the only objectives considered.


Hence, to train our model to produce \emph{concise} sufficient reasons, we must include a third optimization objective aimed at minimizing the size of $S$. We incorporate this by implementing a \emph{cardinality loss} which encourages the sparsity of $S$, by utilising the standard $L_1$ loss, i.e., $L_{card}(h):= ||h_2(\x)||_{1}$.

%\begin{equation}
%\begin{aligned}
%L_{card}(f):= ||g_{\theta}(\mathbf{x})||_{1}
%\end{aligned}
%\end{equation}
%\label{submission}

%$L_1$ because it encourages sparsity.


%\begin{equation}
%\begin{aligned}
%L_{\theta}(f):= L_{pred}+
%\lambda L_{faith}+
%\xi L_{card}
%\end{aligned}
%\end{equation}
%\label{submission}

%\textbf{Limitations.} 


%\begin{equation}
%\begin{aligned}
%L_{\theta}(f):= L_{CE}(f(\mathbf{x}),t)+\\
%L_{CE}(f_{\theta}(\mathbf{x_S};\mathbf{x'_{\Bar{S}}}),\argmax_{j} f_{\theta}(\mathbf{x})_j))+\\
%||g(\mathbf{x})||_{1}
%\end{aligned}
%\end{equation}
%\label{submission}


%\begin{equation}
%\begin{aligned}
%    \argmin_{\theta}\mathbb{E}_D[\ \min_{||S||} \ [\max_{\mathbf{x'_{\Bar{S}}}\in B_p^{\epsilon_p}(\mathbf{x_{\Bar{S}}})}L_{CE}(f_{\theta}(\mathbf{x_S};\mathbf{x'_{\Bar{S}}}),t)]]
%\end{aligned}
%\end{equation}
%\label{submission}

%\begin{equation}
%\begin{aligned}
%    \argmin_{\theta}\mathbb{E}_D[\ \min_{||S||} \ [L_{CE}(f_{\theta}(\mathbf{x_S};\mathbf{x'_{\Bar{S}}}),t)]]
%\end{aligned}
%\end{equation}
%\label{submission}

\subsection{Masking} 
\label{masking_subsection}
As noted earlier, during the model's second propagation phase, a masked input $(\x_S; \z_{\Bar{S}})$ is processed by our model's prediction layer $h_1$. The faithfulness loss aims to align the prediction of $h_1(\x_S;\z_{\Bar{S}})$ closely with $h_1(\x)$. A key question remains on how to select the $\z$ values for the complementary features $\overline{S}$. We propose several masking methods that correspond to the different forms of sufficiency:

\begin{enumerate}
    \item \textbf{Baseline Masking.} Given a fixed baseline $\z\in\mathbb{R}^n$, we fix the features of $\overline{S}$ to their corresponding values in $\z$ and propagate $h_1(\x_S;\z_{\Bar{S}})$, optimizing $S$ to constitute as a \emph{baseline} sufficient reason with respect to $\z$.
    \item \textbf{Probabilistic Masking.} Given some distribution $\mathcal{D}$, we sample the corresponding assignments from $\mathcal{D}(\z|\z_S=\x_S)$ and propagate $h_1(\x_S;\z_{\Bar{S}})$, optimizing $S$ to constitute as a \emph{probabilistic} sufficient reason concerning $\mathcal{D}$.
    \item \textbf{Robust Masking.} To ensure the sufficiency of $S$ across the \emph{entire} input range of $B_p^{\epsilon_p}(\x)$, we conduct an adversarial attack on $\x$. Here, we fix the values of $S$ and perturb only the features in $\overline{S}$. This approach involves identifying the ``hardest'' instances, denoted by $(\x'_S;\z'_{\Bar{S}})\in B_p^{\epsilon_p}(\x_S;\z_{\Bar{S}})$, and optimizing the prediction of $h_1(\x'_S;\z'_{\Bar{S}})$ to assimilate that of $h_1(\x)$. We achieve this by initially setting $(\x_S;\z_{\Bar{S}})_{0}$ randomly within $B_p^{\epsilon_p}(\x_S;\z_{\Bar{S}})$ and updating it over $N$ projected gradient descent (PGD) steps~(\cite{madry2017towards}):
\begin{equation}
    (\x_S;\z_{\Bar{S}})_{n+1} = \Pi_{B^{\epsilon}(\x)}(\x_S;\z_{\Bar{S}})_n+\alpha \cdot \text{sign}(\nabla_{(\x_S;\z_{\Bar{S}})_n} L_{pred}(h_1(\x_S;\z_{\Bar{S}})_n,t))
\end{equation}
where $\alpha$ is the step size, and $\Pi$ is the projection operator. This process is similar to adversarial training~(\cite{shafahi2019adversarial}), with the difference of not perturbing the features in $S$.
\end{enumerate}


\section{Experiments}
\label{experiments_section_main}

We assess SST across image and language classification tasks, comparing the sufficient reasons generated by our approach to those from state-of-the-art \emph{post-hoc} methods. Following common conventions~(\cite{wu2024verix, ignatiev2019abduction, bassan2023towards}), we evaluate these subsets on three metrics: \begin{inparaenum}[(i)]
    \item the mean \emph{time} taken to generate a sufficient reason, \item the mean \emph{size} of the sufficient reasons, and \item the mean \emph{faithfulness}, i.e., how sufficient the subsets are.
\end{inparaenum} Particularly, when evaluating faithfulness, we categorize the results into three distinct types of sufficiency: baseline, probabilistic, and robust faithfulness. Each category measures the proportion of test instances where the derived subset $S$ remains sufficient under baseline, probabilistic, or robust masking.

\subsection{Sufficient Subset Training for Images}

We train SST-based models on three prominent image classification tasks: \textsc{MNIST}~(\cite{deng2012mnist}) digit recognition using a feed-forward neural network, \textsc{CIFAR-10}~(\cite{krizhevsky2009learning}) using Resnet18~(\cite{he2016deep}), and \textsc{IMAGENET} using a pre-trained Resnet50 model~(\cite{he2016deep}). To simplify the training process, we set the threshold at $\tau=0.5$ and the faithfulness coefficient at $\lambda=1$, conducting the grid search exclusively over the cardinality coefficient $\xi$. Full training details can be found in Appendix~\ref{model_specifications_appendix}. We assess the inherent sufficient reasons generated by SST and compare them with those from standard-trained models using post-hoc methods: \emph{Anchors}~(\cite{ribeiro2018anchors}), \emph{SIS}~(\cite{carter2019made}), and a vision-based technique~(\cite{fong2017interpretable}) which we term \emph{gradient search} (\emph{GS}).


%We train the models with all three masking techniques that were mentioned. First, we incorporate \emph{probabilistic} masking using a uniform sampling of the values of the complementary $\Bar{S}$ around the interpreted instance $\x$, in other words, a uniform sampling from $B_p^{\epsilon_p}(\z|\z_S=\x_S)$. For robust masking, we perform a gradient PGD $\ell_{\infty}$ attack over $\Bar{S}$. We incorporate \emph{baseline} masking only in MNIST, by zeroing out the pixels in $\Bar{S}$. This baseline is natural for MNIST, since these images are incorporated of a large portion of ``zeroed-out'' pixels in the background of images. This is, as opposed to CIFAR10 and IMAGENET which do not offer a generalized baseline due to the high diversity of images.

\textbf{Robust Sufficient Reasons in Images.} In image classification tasks, it is common to evaluate the \emph{robust} sufficiency of the identified sufficient reasons, by ensuring they remain sufficient across a continuous domain~(\cite{wu2024verix, ignatiev2019abduction, bassan2023towards}). We aim to explore the ability of SST-based models to generate such subsets, initially training all models using robust masking with a PGD $\ell_{\infty}$ ($\epsilon=0.12$) attack on the features in $\Bar{S}$, as described in section~\ref{masking_subsection}. Further details on the selection of $\epsilon = 0.12$ can be found in Appendix~\ref{model_specifications_appendix}. We subsequently assess these results against subsets produced by the aforementioned post-hoc methods over standard-trained models (i.e., with only a prediction output, and no explanation) and examine their capacity to uphold robust sufficiency. Figure~\ref{plot_images_vision} illustrates visualizations of these results, while Table~\ref{table:vision_robust_results} provides a summary of the empirical evaluations.
%\ron{Why $\epsilon=0.12$? What are the cons and the pros of different values of $\epsilon$?}
\begin{table*}[h]
	\centering
	%\def\arraystretch{0.5}% 
	%\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	\caption{SST with robust masking vs. post-hoc methods. Results depict average explanation size (\%), 10-minute timeouts (\%), average time (seconds), and robust faithfulness (\%).}
 \begin{adjustbox}{width=1\textwidth}
	\begin{tabular}{lcccc|cccc|cccc}
		\\
		\toprule
		%\hline
		\multirow{3}{*}{} &
            \multicolumn{4}{c}{MNIST} &
            \multicolumn{4}{c}{CIFAR-10} &
            \multicolumn{4}{c}{IMAGENET}
            \\
            {} & Size & T/O & Time & Faith. & 
            Size & T/O & Time & Faith. &
            Size & T/O & Time & Faith. \\
		\midrule
		%\hline
		Anchors  & 8.98 & 0  & 0.11 & 97.51 & 20.88  & 0 & 0.8 & 86.47 & 20.29 & 1.09 & 118.6 & 88.94  \\
		SIS &  2.35 & 0 &  4.34 & 98.08 &  0.87 & 0.14 & 195.25 & 84.64 & 0.15 & 0 & 262.42 & 77.32      \\
  		GS &  40.36 & 0  & 1.13 & 97.56 & 60.7 & 0 & 14.61 & 92.41 & 78.97 & 0 & 266.5 & 90.92   \\
            \textbf{SST} & 1.42 & 0 & $\mathbf{\approx}$$\mathbf{10^{-6}}$ & 99.28 & 12.99 & 0 & $\mathbf{\approx}$$\mathbf{10^{-5}}$ & 90.43 & 0.46 & 0 & $\mathbf{\approx}$$\mathbf{10^{-4}}$ & 80.88  \\
		\bottomrule
	\end{tabular}
 \end{adjustbox}
	\label{table:vision_robust_results}
\end{table*}


\begin{figure}[hbt!]
    %\vspace{-0.5cm}  
    \centering
        \includegraphics[height=8.5cm]{figures/experiments_comparison_crop.png} 
    \caption{Examples of sufficient reasons produced by SST compared to the ones generated by post-hoc approaches for MNIST, CIFAR-10, and IMAGENET. Additional examples appear in appendix~\ref{supplementary_results_appendix}.}
    \label{plot_images_vision}
\end{figure}

% \subfloat{\includegraphics[height=2cm]{MNIST_PLOT.jpeg}} \\
% \subfloat{\includegraphics[height=2.2cm]{image2.jpeg}}\\
% \subfloat{\includegraphics[height=2.2cm]{picture_2.jpeg}}

Results in Table~\ref{table:vision_robust_results} indicate that across all configurations, SST generated explanations with substantially greater efficiency than post-hoc methods. SST typically produced smaller explanations, especially in all comparisons with GS and Anchors, and in MNIST against SIS. Additionally, SST explanations often demonstrated greater faithfulness, notably in MNIST against all post-hoc methods, and in CIFAR-10 compared to Anchors and SIS. Typically, SIS offered concise subsets but at the cost of low faithfulness, while GS provided faithful explanations that were considerably larger. Anchors yielded more consistent explanations, yet they were often either significantly larger or less faithful compared to SST. Separately, concerning the accuracy shift for SST, MNIST showed a minor increase of $0.15\%$, whereas CIFAR-10 experienced a decrease of $1.96\%$, and IMAGENET saw a reduction of $4.7\%$. Of course, by reducing the impact of the cardinality and faithfulness loss coefficients, one can maintain accuracy on all benchmarks, but this may compromise the quality of the explanations.


\textbf{Different Masking Strategies for images.} We aim to analyze different masking strategies for SST in image classification, focusing on MNIST. We utilize two additional masking methods: \emph{probabilistic} masking, where the assignments of features in the complement $\overline{S}$ are sampled from a uniform distribution to enhance probabilistic faithfulness, and \emph{baseline} masking, where all the features in $\overline{S}$ are set to zero (using the trivial baseline $\z:=\mathbf{0}_n$), hence optimizing baseline faithfulness. This specific baseline is intuitive in MNIST due to the inherently zeroed-out border of the images. We compare these approaches to the same post-hoc methods as before, evaluating them based on their baseline, probabilistic, or robust faithfulness, and present these results in Table~\ref{mnist_comparison}.

\begin{table*}[h]
	\centering
	%\def\arraystretch{0.5}% 
	%\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	\caption{SST (different masks) vs. post-hoc methods: Mean explanation size (\%), accuracy gain (\%), mean inference time (seconds), and baseline, probabilistic, and robust faithfulness (\%).}
	\begin{tabular}{lccccccc}
		\\
		\toprule
		%\hline
		\multirow{6}{*}{} &
  		\multicolumn{1}{c}{Masking} &
		\multicolumn{1}{c}{Size} &
            \multicolumn{1}{c}{Acc. Gain} &
            \multicolumn{1}{c}{Time} &

            \multicolumn{3}{c}{Faithfulness} \\
            & {} & {} & {} & {} & {\ \ Robust} & {\ Probabilistic} & {Baseline}\\ 
		\midrule
		%\hline
		Anchors &\textbf{---} & 8.98 & ---  & 0.11 & 97.51 & 91.06  & 16.06  \\
		SIS &\textbf{---} & 2.35 & --- & 4.34 & 98.08 &  90.72 & 46.1    \\
  		GS &\textbf{---} & 40.36 & ---  & 1.13 & 97.56 & 94.11 & 16.72   \\
        \midrule
		& \textbf{robust} & 1.42 & +0.15 & $\mathbf{\approx}$$\mathbf{10^{-6}}$ & 99.28 & --- & ---  \\
		%\hline
  		\textbf{SST} & \textbf{baseline} & 23.69 &  -1.26 & $\mathbf{\approx}$$\mathbf{10^{-6}}$ & --- & --- &  96.52  \\
		%\hline
    	& \textbf{probabilistic} & 0.65 & +0.36 & $\mathbf{\approx}$$\mathbf{10^{-6}}$ & --- & 99.11 &  --- \\
		\bottomrule
	\end{tabular}
	\label{mnist_comparison}
\end{table*}


Results in Table~\ref{mnist_comparison} demonstrate the impact of SST with various masking strategies. Each strategy improved compared to post-hoc methods in its respective faithfulness measure. Robust and probabilistic masking strategies produced very small subsets and even slightly \emph{increased} accuracy. In contrast, the baseline masking strategy resulted in larger subsets and a larger accuracy drop. This likely occurs because setting the complement $\overline{S}$ to a fixed baseline strongly biases the model towards OOD instances, whereas uniform sampling or a bounded $\epsilon$-attack are less prone to this issue.

\begin{wrapfigure}{t}{0.45\textwidth}
    \vspace{-0.5cm}  
    \begin{center}
        \includegraphics[width=0.45\textwidth]{figures/gamma_effect.pdf}
        \caption{The faithfulness-cardinality tradeoff in baseline-masking SST models for MNIST with varying cardinality loss coefficients, $\xi$, shows that higher $\xi$ increases mask size $\overline{S}$ but reduces faithfulness, and vice versa.}
        \label{gamma_example}
    \end{center}
    %\vspace{-0.6cm}  
\end{wrapfigure}

%\vspace{0.2cm}
\textbf{Faithfulness-Cardinality Trade-off.} Optimizing models trained with SST involves a trade-off between two primary factors: \begin{inparaenum}[(i)] \item the size of the generated subset and \item the faithfulness of the model. \end{inparaenum} For instance, a full set explanation ($S:=\{1,\ldots, n\}$) scores low in terms of the optimization for small subset sizes but guarantees perfect faithfulness. Conversely, an empty set explanation ($S:=\emptyset$) achieves the best score for subset size due to its minimal cardinality but generally exhibits poor faithfulness. In our experiment, we aim to illustrate this trade-off, which can be balanced by altering the cardinality loss coefficient $\xi$. Greater $\xi$ values emphasize the subset size loss component, leading to smaller subsets at the cost of their faithfulness. Conversely, smaller $\xi$ values reduce the component's impact, resulting in larger subsets with better faithfulness. Figure~\ref{gamma_example} provides a comparative analysis of this dynamic using the MNIST dataset with baseline masking. For greater $\xi$ values, the cardinality of the mask is maximal. As $\xi \to 0$, the explanation size converges to $50\%$ of the input. We believe this happens because the threshold is set to the default value of $\tau := 0.5$ (see Appendix~\ref{model_specifications_appendix} for more details), resulting in the arbitrary selection of a $50\%$ sufficient subset due to random initialization at the beginning of training. This subset is likely sufficient to determine the prediction on its own (as demonstrated in Table~\ref{mnist_comparison}, even $23.69\%$ of the input may be sufficient for a prediction). Consequently, as the cardinality loss coefficient gets very close to zero, the parameters of the explanation component (i.e., $h_2$, as discussed in Section~\ref{sec:architercutre_section})
 remain nearly unchanged throughout training, with the explanation size fixed at $50\%$.

















%We believe that this happens since the threshold is set to the default value of $\tau := 0.5$ (see Appendix~\ref{model_specifications_appendix} for more details), causing the size of the sufficient subset to be selected arbitrarily due to random initialization at the start of training. During an early stage of the training, the model can readily find configurations with 50\% of the input dimension (as shown in Table~\ref{mnist_comparison}, even 23.69\% of the input can suffice to determine the prediction). Consequently, the explanation parameters then simply remain constant during the entire training (and the explanation size fixed at 50\%), since the cardinality loss coefficient is set to zero.










%As $\xi\to 0$, the explanation size becomes 50\% of the input. This occurs because the threshold is set to the default value of $\tau := 0.5$ (refer to appendix~\ref{model_specifications_appendix} for further details), which leads to an arbitrary selection of the sufficient subset in terms of its size, due to the random initialization at the beggining of the training. During training, since the model can easily find configurations with $50\%$ of the input dimension (see in Table~\ref{mnist_comparison} that even 23.69\% of the input can be sufficient to determine the prediction), this paramter stays constant As a result, when the cardinality loss coefficient is set to zero, the default explanation size becomes half of the input.












%We note that the optimization of models trained with SST requires balancing bbetween two main components: \begin{inparaenum}[(i)]
%    \item the cardinality of the generated subset and \item the faithfulness.
%\end{inparaenum} The trivial explanation ($S:=\{1,\ldots, n\}$ has a low cardinality scoring due but will always have a perfect faithfulness. On the other hand, $S:=\emptyset$ has a high score with respect to cardinality (the smallest possible subset), but will tend to have low faithfulness. In this experiment we seek to demonstrate this tradeoff which can be controled with varying values of the cardinality loss $\xi$: high values enhance the dependence on the cardinality component, and will hence produce smaller subsets on account on faithfulness. However, low $\xi$ values will mitiagte its influence, and will hence produce larger subsets, with improved faithfulness. Figure~\ref{gamma_example} depicts such a comparison over the MNIST benchmark, with baseline masking.





%\begin{figure}[h]
    %\vspace{-0.5cm}  
%    \begin{center}
%\includegraphics[width=0.55\textwidth]{figures/gamma_effect.png}
%        \caption{The tradeoff between faithfulness and cardinality in models trained with SST for MNIST, employing baseline masking and varying cardinality loss coefficients, $\xi$. Higher values of $\xi$ enhance the optimization of the explanation size, resulting in a larger cardinality of the mask $\overline{S}$, but at the cost of reduced faithfulness. Conversely, lower $\xi$ values improve the faithfulness of the models, however, this leads to a lower masked cardinality.}
%        \label{gamma_example}
%    \end{center}
    %\vspace{-0.31cm}  
%\end{figure}


%\begin{table*}[h]
%	\centering
%	\def\arraystretch{1}% 
%	\setlength{\tabcolsep}{0.4em} % for the horizontal padding
%	\caption{Comparing sufficient subset training (SST) to post-hoc sufficient reasons obtained over standard-trained models for MNIST, CIFAR10, and IMAGENET. Results depict average explanation size (\%), accuracy drop or gain (\%), instances solved (\%), and average faithfulness (\%).}
%	\begin{tabular}{lccccccc}
%		\\
%		\toprule
		%\hline
%		\multirow{6}{*}{} &
 %           \multicolumn{1}{c}{Masking} &
%		\multicolumn{1}{c}{Size} &
 %           \multicolumn{1}{c}{Drop/Gain} &
  %          \multicolumn{1}{c}{Solved} &

%            \multicolumn{3}{c}{Faithfulness} \\
%            & {} & {} & {} & {\ \ robust} & {\ Uniform} & {Baseline}\\ 
%		\midrule
		%\hline
%		Anchors & 8.98 & ---  & 21.47 & 97.51 & 91.06  & 16.06  \\
%		SIS & 2.35 & --- &  59.71 & 98.08 &  90.72 & 46.1    %\\
%  		GS & 40.36 & ---  & 14.13 & 97.56 & 94.11 & 16.72   \\
%		\textbf{SST} & \textbf{1.42} & \textbf{+0.15} & &\textbf{99.28} & \textbf{---} & \textbf{---}  \\
		%\hline
 % 		\textbf{SST - baseline} & \textbf{23.69} & \textbf{-1.26} & \textbf{100} & \textbf{---} & \textbf{---} &  \textbf{96.52}  \\
		%\hline
%    	\textbf{SST - probabilistic} & \textbf{0.65} & \textbf{+0.36} & &\textbf{---} & \textbf{99.11} &  \textbf{--%-} \\
%     \midrule
%        \larger{\textbf{CIFAR10}} \\
%		Anchors & 20.88 & ---  & 60.35 & 86.47  & 93.01  \\
%		SIS & 0.87 & --- & 99.86   & 84.64 & 93.01  \\
%  		GS & \textbf{60.7} & ---  & 100 & 92.41 & 93.64   \\
%    \midrule
%		\textbf{SST - robust} & \textbf{12.99} & \textbf{-1.96} & \textbf{100} & \textbf{90.43} & \textbf{---} &   \\
		%\hline
%    	\textbf{SST - probabilistic} & \textbf{37.98} & \textbf{-0.64} &  & \textbf{---} & \textbf{93.05}  \\
%		\bottomrule
%	\end{tabular}
%	\label{table:paramterizedresults}
%\end{table*}

%\vspace{8.0cm}  

\subsection{Sufficient Subset Training for Language}

\begin{table*}[h]
	\centering
	%\def\arraystretch{0.5}% 
	%\setlength{\tabcolsep}{1.0em} % for the horizontal padding
	\caption{Baseline/probabilistic SST (B-SST, P-SST) vs. post-hoc techniques: Mean inference time (seconds), 10-minute timeouts (\%), mean explanation size (\%), and probabilistic and baseline faithfulness (\%)}
 \begin{adjustbox}{width=1\textwidth}
	\begin{tabular}{lccccc|ccccc}
		\\
		\toprule
		%\hline
		\multirow{2}{*}{} &
            \multicolumn{5}{c}{IMDB} &
            \multicolumn{4}{c}{SNLI} &
            \\
            {} & Time & T/O & Size & P-Faith. & B-Faith. & Time & T/O &
            Size & P-Faith. & B-Faith. \\
		\midrule
		%\hline
		Anchors  &  22.42  & 20.05 & 1.12 & 89.1 & 39.6   & 94.78 & 3.6 & 10 &  35.24 & 40.24 \\
   		S-Anchors &  19.65  & 12.26 & 1.16 & 89.14 & 23.37  & 160.92 & 9.76 & 18 & 28.55 & 33.94 \\
		SIS &  14.9 &  0  & 22.57 & 88.97 &  97.89  & 23.13 & 0 & 12.74 & 44.84 & 49.29      \\
 
            \textbf{B-SST} &  $\mathbf{\approx}$$\mathbf{10^{-3}}$ &  0 & 28.96 &  --- & \textbf{98.05} &  $\mathbf{\approx}$$\mathbf{10^{-3}}$ & 0  & 39 & --- & \textbf{95.88} \\
           \textbf{P-SST} &  $\mathbf{\approx}$$\mathbf{10^{-3}}$ & 0 & 49.7 & \textbf{95.67} & ---  & $\mathbf{\approx}$$\mathbf{10^{-3}}$ & 0 & 53.69 & \textbf{95.35} & --- \\
		\bottomrule
	\end{tabular}
 \end{adjustbox}
	\label{table:language_table}
\end{table*}

We assess our results using two prevalent language classification tasks: \textsc{IMDB} sentiment analysis~(\cite{maas2011learning}) and \textsc{SNLI} (\cite{bowman2015large}). All models are trained over a BERT base (\cite{devlin2018bert}). We implement two typical masking strategies for testing sufficiency in language classification~(\cite{hase2021out, vafa2021rationales}): 
\begin{inparaenum}[(i)] \item using the pre-trained MASK token from BERT~(\cite{devlin2018bert}) as our baseline (baseline masking) and \item randomly sampling features uniformly (probabilistic masking) \end{inparaenum}. We evaluate the performance of SST-generated subsets against post-hoc explanations provided by either Anchors or SIS. In the Anchors library, an additional method assesses sufficiency for text domains by sampling words similar to those masked, leading us to two distinct configurations which we denote as: \emph{Anchors} and \emph{Similarity-Anchors} (\emph{S-Anchors}).






%\begin{figure}
%\centering
%\subfloat{\includegraphics[height=3cm]{language.jpeg}} \\
%\label{bla}
%\end{figure}

Table~\ref{table:language_table} shows that in the language setting, similarly to vision, SST obtains explanations much more efficiently than post-hoc methods. Although SST produces larger subsets than post-hoc methods, these subsets are significantly more faithful. For instance, SST achieves a baseline faithfulness of $98.05\%$ on IMDB, compared to only $23\%$ for S-Anchors. The convergence of SST towards larger subsets likely stems from our optimization of both faithfulness \emph{and} minimal subset cardinality. Increasing the cardinality loss coefficient may yield smaller subsets but could decrease faithfulness. Separately, SST maintained comparable accuracy levels for both of these tasks --- the decrease in accuracy for IMDB was $0.87\%$ and $0.89\%$ with probabilistic and baseline masking, respectively. For SNLI, probabilistic masking resulted in a $0.61\%$ drop, whereas baseline masking saw a smaller decrease of $0.32\%$.

\begin{figure}[h]
    %\vspace{-0.5cm}  
    \begin{center}
\includegraphics[width=0.9\textwidth]{figures/new_image_ooo.png}
        \caption{Explanations generated by SST using baseline vs. probabilistic masking. When each token in the complement $\overline{S}$ is replaced with the MASK token, the prediction stays negative. In the probabilistic setting, the prediction remains negative when values from $\overline{S}$ are randomly sampled. Further examples can be found in Appendix~\ref{supplementary_results_appendix}.}
        \label{text_example}
    \end{center}
    %\vspace{-0.31cm}  
\end{figure}

Explanations generated using probabilistic masking were larger on average compared to those generated using baseline masking. This is likely because random token perturbations present a more challenging constraint than fixing the MASK token. Figure~\ref{text_example} illustrates two explanations produced by SST using probabilistic or baseline masking for IMDB. Although both are intuitive, the baseline sufficient reason suffices by fixing only specific relevant terms like ``don't'' and ``off''. These terms are enough to preserve the negative classification when all other tokens are fixed to the MASK token. However, the probabilistic subset has larger negating phrases like ``wandering off the TV'' or ``really hard'' because it involves random perturbations of the complementary set, which may potentially shift the classification to positive.


%\begin{table*}[h]
%	\centering
%	\def\arraystretch{1}% 
%	\setlength{\tabcolsep}{0.4em} % for the horizontal padding
%	\caption{MNIST - Comparing Sufficient Subset Training to Post-hoc approaches}
%	\begin{tabular}{lccccccc}
%		\\
%		\toprule
		%\hline
%		\multirow{5}{*}{} &
%		\multicolumn{1}{c}{Size} &
 %           \multicolumn{1}{c}{Drop/Gain} &
  %          \multicolumn{1}{c}{Solved} &

%            \multicolumn{2}{c}{Faithfulness} \\
%            & {} & {} & {} &{\ Uniform} & {Mask-Baseline}\\ 
%		\midrule
		%\hline
%            \larger{\textbf{IMDB}} \\
%		Anchors & 0.68 & ---  & 80.56 & 89.1 & 39.6  \\
%		S-Anchors & 0.26 & --- &  88.35 & 89.14 &  23.37   \\
%  		SIS & 20.61 & ---  & 100 & 88.97 & 97.89   \\
%        \midrule
		%\hline
%  		\textbf{SST - baseline} & \textbf{54.81} & \textbf{-0.88} & \textbf{100} & \textbf{---} & \textbf{98.05}   \\
		%\hline
%    	\textbf{SST - probabilistic} & \textbf{49.59} & \textbf{-1.13} & &\textbf{95.67} & \textbf{---}  \\
%     \midrule
%        \larger{\textbf{SNLI}} \\
%		Anchors & 2.43 & ---  & 60.35 & 35.24  & 40.24  \\
%		S-Anchors & 3.8 & --- & 99.86   & 28.55 & 33.94    \\
%  		SIS & 50.9 & ---  & 100 & 44.84 & 49.29    \\
%    \midrule
		%\hline
 %   		\textbf{SST - baseline} & \textbf{40.9} & \textbf{-1.26} & \textbf{100} & \textbf{---} & \textbf{95.88}   \\
%    	\textbf{SST - probabilistic} & \textbf{49.56} & \textbf{-0.8} & \textbf{---} & \textbf{95.35}  & \textbf{---}   \\
%		\bottomrule
%	\end{tabular}
%	\label{table:paramterizedresults}
%\end{table*}

%\subsection{Random Noise Vs. Mask Token} As mentioned earlier, in language models a common technique for evaluating sufficiency is by incorporating the \emph{MASK} token as our baseline. However, as mentioned earlier as well, one problem with using a specific baseline is that it may not generalize to other baselines. Here, we check how much sufficient subset training that is used for language models with either probablistic mask sampling, or fixing the baseline to the \emph{MASK} token, generalizes well to the other setting.  

\section{Related Work} 

%Old related work

\textbf{Post-hoc sufficient reason techniques.} Some common post-hoc approaches include Anchors~(\cite{ribeiro2018anchors}), or SIS~(\cite{carter2019made}), and typically seek to obtain concise subsets which uphold probabilistic sufficiency~(\cite{wang2021probabilistic, ribeiro2018anchors, blanc2021provably}) or baseline sufficiency~(\cite{hase2021out, chockler2021explanations, deyoung2019eraser}). A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(\cite{ignatiev2019abduction, bassan2023formally}) or \emph{prime implicants}~(\cite{ignatiev2019abduction, shih2018symbolic}). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types, such as decision trees~(\cite{izza2020explaining}), tree ensembles~(\cite{izza2021explaining, ignatiev2022using}), linear models~(\cite{marques2020explaining}), or small-scale neural networks~(\cite{ignatiev2019abduction, la2021guaranteed, bassan2023towards}).

\textbf{Self-explaining neural networks.} Unlike post-hoc methods, some techniques modify training to improve explanations~(\cite{ismail2021improving, chen2019robust, hase2021out, vafa2021rationales, yan2023self}) or develop \emph{self-explaining} models that inherently provide interpretations for their decisions. Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, ~\cite{chen2019looks} and ~\cite{wang2021self} describe model outputs by comparing them to relevant ``prototypes'', while~\cite{alvarez2018towards} derive concept coefficients from feature transformations. Other approaches, like those by~\cite{agarwal2021neural} and~\cite{jain2020learning}, focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. ~\cite{koh2020concept} predict outcomes based on high-level concepts. Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(\cite{yeh2019fidelity, lundberg2017unified}). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(\cite{slack2020fooling, ribeiro2018anchors, yeh2019fidelity}). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(\cite{ribeiro2018anchors}). To our knowledge, this is the first self-explaining framework designed to generate such explanations.




%\textbf{Post-hoc sufficient reason techniques.} Some common post-hoc approaches include Anchors~(\cite{ribeiro2018anchors}), or SIS~(\cite{carter2019made, carter2021overinterpretation}), and typically seek to obtain concise subsets which uphold probablistic sufficiency~(\cite{wang2021probabilistic, ribeiro2018anchors, blanc2021provably}) or baseline sufficiency~(\cite{hase2021out, chockler2021explanations, deyoung2019eraser, chockler2024explaining}). A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(\cite{ignatiev2019abduction, bassan2023formally, ordyniak2023parameterized}) or \emph{prime implicants}~(\cite{ignatiev2019abduction, shih2018symbolic}). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types, such as decision trees~(\cite{izza2020explaining, huang2021efficiently, bounia2023approximating}), KNNs~\citep{barcelo2025explaining}, tree ensembles~(\cite{izza2021explaining, ignatiev2022using, audemard2022trading, audemard2022preferred, boumazouza2021asteryx}), linear models~(\cite{marques2020explaining}), or small-scale neural networks~(\cite{ignatiev2019abduction, wu2024verix, la2021guaranteed, bassan2023towards}), typically obtained with the use of neural network verifiers~\citep{wang2021beta, wu2024marabou, fel2023don}.

%\textbf{Self-explaining neural networks.} Unlike post-hoc methods, some techniques modify training to improve explanations~(\cite{ismail2021improving, chen2019robust, hase2021out, vafa2021rationales, yan2023self}) or develop a self explaining neural network (SENN)~\citep{alvarez2018towards} architecture that inherently provides interpretations for its decisions~\citep{lee2022self, shwartz2020unsupervised, rajagopal2021selfexplain, guyomard2022vcnet, guo2023counternet, zhang2022protgnn}. Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, ~\cite{chen2019looks} and ~\cite{wang2021self} describe model outputs by comparing them to relevant ``prototypes'', while~\cite{alvarez2018towards} derive concept coefficients from feature transformations. Other approaches, like those by~\cite{agarwal2021neural} and~\cite{jain2020learning}, focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. ~\cite{koh2020concept} predict outcomes based on high-level concepts. 

%Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(\cite{yeh2019fidelity, lundberg2017unified}). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(\cite{slack2020fooling, ribeiro2018anchors, yeh2019fidelity}). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(\cite{ribeiro2018anchors}). To our knowledge, this is the first self-explaining framework designed to generate such explanations.


%Similar to the input-level perturbations in our method, other techniques also utilize perturbations for various tasks beyond interpretability. These tasks range from improving segmentation in weakly supervised localization tasks~\cite{li2018tell, hou2018self, wei2017object, kumar2017hide}, enhancing classification~\cite{wang2019sharpen}, to acting as regularization~\cite{devries2017improved}. Particularly related to our approach are the studies by Hase et al.~\cite{hase2021out} and Vafa et al.~\cite{vafa2021rationales}, which apply input perturbations to strengthen explanations against out-of-distribution counterfactuals in NLP. Unlike our method, these techniques do not inherently generate sufficient reasons within their outputs, avoiding the computational burden of obtaining such explanations post-training. Additionally, their counterfactual sampling is \emph{arbitrary}, which may inadvertently mask critical features, whereas our method intentionally only masks non-sufficient features at each iteration.
%\bigskip

%\textbf{Limitations.}
\section{Limitations}
A primary limitation of SST is the need to tune additional hyperparameters due to multiple optimization objectives. We simplified this by conducting a grid search solely over the cardinality loss coefficient $\xi$, as detailed in Appendix~\ref{model_specifications_appendix}. Additionally, the dual-propagation training phase is twice more computationally demanding than standard training, and this complexity escalates with challenging masking strategies, such as robust masking (see Appendix~\ref{training_time_section} for more details). However, this investment in training reduces the need for complex \emph{post}-training computations, as highlighted in Section~\ref{experiments_section_main}. %which are both theoretically and practically challenging. 
Lastly, overly optimizing our interpretability constraints can sometimes reduce predictive accuracy. Our experiments showed this was true for IMAGENET, though all other benchmarks retained comparable accuracy levels (within $2\%$, and under $1\%$ for all language-model benchmarks). Interestingly, accuracy even improved under some configurations for MNIST classification. Future research endeavors could further investigate the conditions under which SST can be utilized not only for its inherent explanations but also to enhance predictive accuracy.

\section{Conclusion} Obtaining (post-hoc) minimal sufficient reasons is a sought-after method of explainability. However, these explanations are challenging to compute in practice because of the computationally intensive processes and heavy dependence on OOD assignments. We further analyze these concerns and propose a framework to address them through \emph{sufficient subset training} (\emph{SST}). SST generates small sufficient reasons as an inherent output of neural networks with an optimization goal of producing subsets that are both \emph{sufficient} and \emph{concise}. Results indicate that, relative to post-hoc methods, SST provides explanations that are consistently smaller or more faithful (or both) across different models and domains, with significantly greater efficiency, while preserving comparable predictive accuracy.

\section*{Acknowledgments} This research was funded by the European Union's Horizon Research and Innovation Programme under grant agreement No. 101094905 (AI4GOV). We also extend our sincere thanks to Guy Amit, Liat Ein Dor, and Beat Buesser from IBM Research for their valuable discussions.


\bibliography{iclr2025_conference.bib}
\bibliographystyle{iclr2025_conference}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\appendix
\setcounter{definition}{0}
\setcounter{theorem}{0}
\begin{center}\begin{huge} Appendix\end{huge}\end{center}    
\noindent{The following appendix is organized as follows:}


\newlist{MyIndentedList}{itemize}{4}
\setlist[MyIndentedList,1]{%
	label={},
	noitemsep,
	leftmargin=0pt,
}
% \setlist[MyIndentedList]{%
	%     label={},
	%     noitemsep,
	%     }


\begin{MyIndentedList}
\item 
	\item \textbf{Appendix~\ref{extended_related_work}} contains extended background on sufficient explanations and related work.
    \item \textbf{Appendix~\ref{background_appendix}} contains background for the computational complexity proofs.
  	\item \textbf{Appendix~\ref{first_proof_appendix_section}} contains the proof of theorem~\ref{regular_complexity_classes_appendix}.
     \item \textbf{Appendix~\ref{second_proof_appendix_section}} contains the proof of theorem~\ref{second_theorem_appendix}.
      \item \textbf{Appendix~\ref{model_specifications_appendix}} contains technical specifications, related to the models, and training.
    \item \textbf{Appendix~\ref{training_time_section}} contains information regarding the training time of SST compared to standard training.
    \item \textbf{Appendix~\ref{generalizability_section_appendix}} includes an experiment on the generalization of various masks across different sufficiency settings.
    \item \textbf{Appendix~\ref{simplifed_input_section_appendix}} includes an experiment on the extension of SST to simplified inputs.
    \item \textbf{Appendix~\ref{ablation_results_appendix}} contains additional ablation results.
      \item \textbf{Appendix~\ref{supplementary_results_appendix}} contains supplementary results.
\end{MyIndentedList}

\section{Extended Related Work and Background}
\label{extended_related_work}

\textbf{Sufficiency-based explanations.} Sufficiency-based explanations are a widely sought-after approach in explainability, with numerous methods proposed to achieve them (e.g.,~\citep{ribeiro2018anchors, carter2019made, ignatiev2019abduction, bassan2023towards}). The core idea is to identify a minimal subset of features that determine a prediction, allowing one to focus on the essential ``reason'' for the outcome while excluding non-relevant features. Notably, unlike the more prevalent additive attribution methods~\citep{ribeiro2016should, lundberg2017unified, sundararajan2017axiomatic}, these explanations do not depend on approximate linearity assumptions~\citep{ribeiro2018anchors, yeh2019fidelity}. From a human perspective, prior research has demonstrated their effectiveness; for instance, seeing multiple sufficiency-based explanations helps users predict model decisions more accurately compared to relying on additive attributions~\citep{ribeiro2018anchors}.


%Importantly, in contrast to the more commonly used additive attributions~\citep{ribeiro2016should, lundberg2017unified, sundararajan2017axiomatic}, these explanations do not rely on approximate linearity assumptions~\citep{ribeiro2018anchors, yeh2019fidelity}. From the human perspective, methods previous research has demonstrated their utility as well. For example, observing multiple explanations for predictions helps humans better predict model decisions compared to relying on additive forms~\citep{ribeiro2018anchors}.

The concept of sufficient explanations has been extensively explored in the literature. The general notion of a sufficient explanation is closely linked to ideas from actual causality~\citep{halpern2016actual} and has been examined in several works~\citep{chockler2021explanations, chockler2024explaining, chockler2024causal, kelly2023you, watson2021local}. Furthermore, the \emph{robust} class of sufficient explanations studied in this work also connects to research in formal logic~\citep{marques2023logic, darwiche2021quantifying}. In the broader case where the $\epsilon_p$ perturbation is unbounded, such explanations are sometimes referred to as \emph{abductive explanations}~\citep{ignatiev2019abduction, ordyniak2023parameterized, bassan2023formally}. A related but distinct concept is that of a \emph{prime implicant}~\citep{shih2018symbolic, ignatiev2019abduction}. Given the high computational complexity of deriving such explanations~\citep{barcelo2020model}, significant research has focused on identifying simplified models where explanation computation remains tractable~\citep{cooper2023tractability, audemard2021computational}. Examples of such models include decision trees~\citep{izza2020explaining, huang2021efficiently, bounia2023approximating}, KNNs~\citep{barcelo2025explaining}, tree ensembles~\citep{izza2021explaining, ignatiev2022using, audemard2022trading, audemard2022preferred, boumazouza2021asteryx}, linear models~\citep{marques2020explaining, subercaseaux2024probabilistic}, monotonic models~\citep{marques2021explanations, el2023cardinality}, and small-scale neural networks~\citep{ignatiev2019abduction, wu2024verix, la2021guaranteed, bassan2023towards}. These explanations are often facilitated by neural network verifiers~\citep{wang2021beta, wu2024marabou, fel2023don}.  


%The notions of sufficient explanations have a long history of references within the literature. The general sufficient-explanation term is rooted to terms related to actual causality~\citep{halpern2016actual}, as explored in a few different works~\citep{chockler2021explanations, chockler2024explaining, chockler2024causal, kelly2023you, watson2021local}. Moreover, the \emph{robust} category of sufficient explanations explored in this work, are also related to literature within formal logic~\citep{marques2023logic, darwiche2021quantifying}. In the more general case where the $\epsilon_p$ pertubation is not bounded, such explanations are sometimes refered to as \emph{abductive explanations}~\citep{ignatiev2019abduction, ordyniak2023parameterized, bassan2023formally}. A similar and relevant (though slightly different) notion is that of a \emph{prime implicant}~\citep{shih2018symbolic, ignatiev2019abduction}. Due to the high computational complexity of obtaining such explanations~\citep{barcelo2020model}, key research has focused on finding simple models where computing these explanation remains feasible~\citep{cooper2023tractability, audemard2021computational}. Examples include such as decision trees~(\cite{izza2020explaining, huang2021efficiently, bounia2023approximating}), KNNs~\citep{barcelo2025explaining}, tree ensembles~(\cite{izza2021explaining, ignatiev2022using, audemard2022trading, audemard2022preferred, boumazouza2021asteryx}), linear models~(\cite{marques2020explaining, subercaseaux2024probabilistic}), monotonic models~\citep{marques2021explanations, el2023cardinality}, or small-scale neural networks~(\cite{ignatiev2019abduction, wu2024verix, la2021guaranteed, bassan2023towards}), typically obtained with the use of neural network verifiers~\citep{wang2021beta, wu2024marabou, fel2023don}.

%Some common post-hoc approaches include Anchors~(\cite{ribeiro2018anchors}), or SIS~(\cite{carter2019made, carter2021overinterpretation}), and typically seek to obtain concise subsets which uphold probablistic sufficiency~(\cite{wang2021probabilistic, ribeiro2018anchors, blanc2021provably}) or baseline sufficiency~(\cite{hase2021out, chockler2021explanations, deyoung2019eraser, chockler2024explaining}). A different set of works is dedicated to obtaining robust sufficient reasons, where sufficiency is determined for some entire discrete or continuous domain. These are often termed \emph{abductive explanations}~(\cite{ignatiev2019abduction, bassan2023formally, ordyniak2023parameterized}) or \emph{prime implicants}~(\cite{ignatiev2019abduction, shih2018symbolic}). Due to the complexity of producing such explanations, they are commonly obtained on simpler model types

\textbf{Self-explaining models.}
The concept of a self-explaining neural network (SENN) was first introduced by~\citep{alvarez2018towards} and advocates for training architectures that inherently provide interpretations for their decisions~\citep{lee2022self, shwartz2020unsupervised, rajagopal2021selfexplain, guyomard2022vcnet, guo2023counternet, zhang2022protgnn, ji2025comprehensive}. This idea is closely related to other research on learning human-understandable ``concepts''~\citep{koh2020concept, blazek2021explainable, espinosa2022concept, marconato2022glancenets, kim2023probabilistic} and representative training-data ``prototypes''\citep{chen2019looks, keswani2022proto2proto, hong2023protorynet}. Additionally, related work explores training interventions designed to improve feature selection capabilities~\citep{lemhadri2021lassonet, zhang2024comprehensive, jethani2021have, jiang2024protogate, yang2022locally, yamada2020feature}.


%The concept of a self explaining neural network (SENN) was first proposed by~\citep{alvarez2018towards}, and suggests training architectures that inherently provides interpretations for their decisions~\citep{lee2022self, shwartz2020unsupervised, rajagopal2021selfexplain, guyomard2022vcnet, guo2023counternet, zhang2022protgnn, ji2025comprehensive}. These also relate more broadly to works discussing the learning of human understanadable ``concepts''~\citep{koh2020concept, blazek2021explainable, espinosa2022concept, marconato2022glancenets, kim2023probabilistic} or representative training-data ``prototypes''~\citep{chen2019looks, keswani2022proto2proto, hong2023protorynet}. Other related works consider training-intervention to enhance the capabilities of feature selection related tasks~\citep{lemhadri2021lassonet, zhang2024comprehensive, jethani2021have, jiang2024protogate, yang2022locally, yamada2020feature}. 

%Self-explaining models typically provide inherent \emph{additive feature attributions}, assigning importance weights to individual or high-level features. For example, ~\cite{chen2019looks} and ~\cite{wang2021self} describe model outputs by comparing them to relevant ``prototypes'', while~\cite{alvarez2018towards} derive concept coefficients from feature transformations. Other approaches, like those by~\cite{agarwal2021neural} and~\cite{jain2020learning}, focus on feature-specific neural networks or apply classifiers to snippets for NLP explanations, respectively. ~\cite{koh2020concept} predict outcomes based on high-level concepts. 

%Due to their focus on \emph{additive} attributions, these methods generally operate under the implicit or explicit assumption that the model's behavior can be approximated as \emph{linear} under certain conditions~(\cite{yeh2019fidelity, lundberg2017unified}). This assumption is crucial as it underpins the approach of decomposing a model’s output into independent, additive contributions from individual features, and may hence overlook nonlinear interactions among features~(\cite{slack2020fooling, ribeiro2018anchors, yeh2019fidelity}). In contrast, we propose a self-explaining framework that provides a distinct form of explanation --- sufficient reasons for predictions --- which do not depend on assumptions of underlying linearity~(\cite{ribeiro2018anchors}). To our knowledge, this is the first self-explaining framework designed to generate such explanations.

\section{Computational Complexity Background}
\label{background_appendix}

In this section, we will introduce formal definitions and establish a computational complexity background for our proofs.

\textbf{Computational Complexity Classes.} We start by briefly introducing the complexity classes that are discussed in our work. It is assumed that readers have a basic knowledge of standard complexity classes which include polynomial time (PTIME) and nondeterministic polynomial time (NP and coNP). We also will mention the common complexity class within the second order of the polynomial hierarchy, \StoPComplexity. This class encompasses problems solvable within \NPComplexity{}, provided there is access to an oracle capable of solving \coNPComplexity{} problems in $O(1)$ time. It is generally accepted that \NPComplexity{} and \coNPComplexity{} are subsets of \StoPComplexity, though they are believed to be strict subsets (\NPComplexity{}, \coNPComplexity{} $\subsetneq$ \StoPComplexity)~(\cite{arora2009computational}). Furthermore, the paper discusses the complexity class \emph{PP}, which describes the set of problems that can be solved by a \emph{probabilistic} Turing machine with an error probability of less than $\frac{1}{2}$ for all underlying instances.

\textbf{Multi Layer Perceptrons}. Our hardness reductions are performed over neural networks with ReLU activation functions. We assume that our neural network is some function $f:\mathbb{R}^{n}\to\mathbb{R}^c$ where $n$ is the number of input features and $c$ is the number of classes. The classification of $f$ is chosen by taking $\argmax_{j} f(\x)_j$. For us to resolve cases where two or more distinct classes are assigned the same value, we assume that the set of $c$ classes is \emph{ordered} ($i_1\succ i_2  \ldots \succ i_c$), and choose the first maximal valued class. In other words, if there exist a set of ordered classes $i_1,\ldots, i_k\in[c]$ for which $i_1\succ i_2 \succ \ldots \succ i_k$ and $f(\x)_{i_1}=\ldots =f(\x)_{i_k}=\argmax_{j}f(\x)_j$ then, without loss of generality, $i_1$ is chosen as the predicted class.  

In the described MLP, the function $f$ is defined to output $f \coloneqq g^{(t)}$. The first (input) layer $g^{(0)}$ is $\x \in \mathbb{R}^n$. The dimensions of both the biases and the weight matrices are represented by a sequence of positive integers $\{d_{0}, \ldots, d_{t}\}$. We focus on weights and biases that are \emph{rational}, or in other words, $W^{(j)}\in \mathbb{Q}^{d_{j-1}\times d_{j}}$ and $b^{(j)}\in \mathbb{Q}^{d_{j}}$. These parameters were obtained during the training of $f$. From our previous definitions of the dimensions of $f$ it holds that $d_{0} = n$ and $d_{t} = c$. We focus here on \emph{ReLU} activation functions. In other words, $\sigma^{(i)}$ is defined by $\text{ReLU}(x) = \max(0, x)$.


%except for the output layer, where a \emph{sigmoid} function is typically used for the classification. Since our focus is only on the \emph{post-hoc} interpretation of the corresponding model, we will equivalently assume the existence of a step function for the final layer activation, where we denote $step(\z) = 1 \iff \z \geq 0$.


\section{Proof of Theorem~\ref{regular_complexity_classes_appendix}}
\label{first_proof_appendix_section}
\begin{theorem}
Given a neural network classifier $f$ with ReLU activations, and $\x\in\mathbb{R}^n$, obtaining a cardinally minimal sufficient reason for $\langle f,\x\rangle$ is \begin{inparaenum}[(i)] \item NP-Complete for baseline sufficient reasons \item $\Sigma^{P}_{2}$-Complete for robust sufficient reasons and \item $\text{NP}^{\text{PP}}$-Hard for probabilistic sufficient reasons.
\end{inparaenum} 
\label{regular_complexity_classes_appendix}
\end{theorem}


\emph{Proof.} We follow computational complexity conventions~(\cite{barcelo2020model, arenas2022computing}),  focusing our analysis on the complexity of the \emph{decision} problem which determines if a set of features constitutes a cardinally minimal sufficient reason.
Specifically, the problem involves determining whether a subset of features $S\subseteq \{1,\ldots n\}$ qualifies as a sufficient reason while also satisfying $|S|\leq k$. This formulation is consistent with and extends the problems addressed in earlier research~(\cite{barcelo2020model, arenas2022computing, waldchen2021computational}). The complexity problem is defined as follows:

\noindent\fbox{%
	\parbox{\columnwidth}{%
		\mysubsection{MSR (Minimum Sufficient Reason)}:
		
		\textbf{Input}: A neural network $f$, and an input instance $\x\in\mathbb{R}^n$.
		
		\textbf{Output}: 
		\yes{} if there exists some $S\subseteq \{1,\ldots,n\}$ such that $S$ is a sufficient reason with respect to $\langle f,\x\rangle$, and \no{} otherwise
	}%
}   


We observe that earlier frameworks evaluating the complexity of the \emph{MSR} query concentrated on cases where $\x\in\{0,1\}^n$, a more straightforward scenario. Our focus extends to broader domains encompassing both discrete and continuous inputs and outputs. We are now set to define our queries, aligning with the definitions of sufficiency outlined in our work:

\noindent\fbox{%
	\parbox{\columnwidth}{%
		\mysubsection{P-MSR (Probabilistic Minimum Sufficient Reason)}:
		
		\textbf{Input}: A neural network $f$, some distribution $\mathcal{D}$ over the features, $0\leq\delta\leq 1$, and $\x$. %$\x\in\mathbb{R}^n$.
		
		\textbf{Output}: 
		\yes{} if there exists some $S\subseteq \{1,\ldots,n\}$ such that $S$ is a probabilistic sufficient reason with respect to $\langle f,\x\rangle$, $\mathcal{D}$ and $\delta$, and \no{} otherwise
	}%
}   

\noindent\fbox{%
	\parbox{\columnwidth}{%
		\mysubsection{R-MSR (Robust Minimum Sufficient Reason)}:
		
		\textbf{Input}: A neural network $f$, an input instance $\x\in\mathbb{R}^n$, and (possibly) some $\epsilon>0$.
		
		\textbf{Output}: 
		\yes{} if there exists some $S\subseteq \{1,\ldots,n\}$ such that $S$ is a robust sufficient reason with respect to $\langle f,\x\rangle$ (over the $\epsilon$-ball surrounding $\x$, or over an unbounded domain) and \no{} otherwise
	}%
}  


\noindent\fbox{%
	\parbox{\columnwidth}{%
		\mysubsection{B-MSR (Baseline Minimum Sufficient Reason)}:
		
		\textbf{Input}: A neural network $f$, an input instance $\x\in\mathbb{R}^n$, and some baseline $\z\in\mathbb{R}^n$.
		
		\textbf{Output}: 
		\yes{} if there exists some $S\subseteq \{1,\ldots,n\}$ such that $S$ is a baseline sufficient reason with respect to $\langle f,\x\rangle$, and $\z$, and \no{} otherwise
	}%
}  

We will begin by presenting the complexity proof for the \emph{R-MSR} query:

\begin{lemma}
\label{r_msr_proof_appendix}
Solving the R-MSR query over a neural network classifier $f$, an input $\x\in\mathbb{R}^n$, and (possibly), some $\epsilon>0$, where $f$ has either discrete or continuous input and output domains is $\Sigma^P_2$-Complete.
\end{lemma}

\emph{Proof.} Our proof is an extension of the one provided by the work of ~\cite{barcelo2020model}, who provided a similar result for multi-layer perceptrons that are restricted to binary inputs and outputs. We expand on this proof and show how it can hold for any possible input and output discrete or continuous domains. 

\textbf{Membership.} In the binary instance, proving membership in $\Sigma^P_2$ can be done by guessing some subset of features $S$ and then using a coNP oracle for validating that this subset is indeed sufficient concerning $\langle f,\x\rangle$. When dealing with binary inputs, validating that a subset of features is in coNP is trivial since one can guess some assignment $\z\in\{0,1\}^n$ and polynomially validate whether $f(\x_S;\z_{\Bar{S}})\neq f(\x)$ holds.

When addressing continuous domains, the process of guessing $S$ remains consistent, but the method for confirming its sufficiency changes. This alteration arises because the guessed certificate $\z$ may not be polynomially bounded by the input size and could be unbounded. When guessing values in $\mathbb{R}$, it becomes crucial to consider a limit on their size, which ties back to the feasibility of approximating these values with sufficient precision.

To determine the complexity of obtaining cardinally minimal sufficient reasons, it is often helpful to first determine the complexity of a \emph{relaxed} version of this query, which simply asks for the complexity of validating whether one specific given subset is a sufficient reason. We will use this as a first step in solving our unresolved inquiry for proving membership for continuous domains. The problem of validating whether one subset is sufficient is formalized as follows:

\noindent\fbox{%
	\parbox{\columnwidth}{%
		\mysubsection{CSR (Check Sufficient Reason)}:
		
		\textbf{Input}: A neural network $f$, a subset $S\subseteq \{1,\ldots,n\}$, and an input instance $\x\in\mathbb{R}^n$, and possibly some $\epsilon>0$.
		
		\textbf{Output}: 
		\yes{}, if $S$ is a sufficient reason with respect to $\langle f,\x\rangle$ (over some $\epsilon$ ball, or over an unbounded domain), and \no{} otherwise
	}%
} 

The process of obtaining a cardinally minimal sufficient reason (the \emph{MSR} query) is computationally harder than validating whether one specific subset is sufficient (the \emph{CSR} query). However, it is often helpful to determine the complexity of \emph{CSR}, as a step for calculating the complexity of \emph{MSR}.

We adopt the proof strategy from \cite{salzer2021reachability}, which demonstrated the complexity that the complexity of the \emph{NN-Reachability} problem is NP-Complete --- an intricate extension of the initial proof in \cite{katz2017reluplex}. Intuitively, in \emph{continuous} neural networks, rather than guessing \emph{binary} inputs as our witnesses, we can guess the \emph{activation status} of each ReLU constraint. This approach allows us to address the verification of neural network properties efficiently using linear programming. We begin by outlining the \emph{NN-Reachability} problem as defined by \cite{salzer2021reachability}:

\begin{definition}
    Let $f:\mathbb{R}^n\to\mathbb{R}^m$ be a neural network. We define the specification $\phi$ as a property that is a conjunction of linear constraints $\phi_{in}$ on the input variables $\mathbf{X}$ and a set of linear constraints $\phi_{out}$ on the output variables $\mathbf{Y}$. In other words, $\phi:=\phi_{in}(\mathbf{X})\wedge \phi_{out}(\mathbf{Y})$.
\end{definition}

\noindent\fbox{%
	\parbox{\columnwidth}{%
		\mysubsection{NNReach (Neural Network Reachability)}:
		
		\textbf{Input}: A neural network $f$, an input specification $\phi_{in}(\x)$, and an output specification $\phi_{out}(\mathbf{y})$
		
		\textbf{Output}: 
		\yes{} if there exists some $\x\in\mathbb{R}^n$ and some $\mathbf{y}\in\mathbb{R}^m$ such that the specification $\phi$ holds, i.e., $\phi_{in}(\x)$ is true and $\phi_{out}(\mathbf{y})$ is true, and \no{} otherwise
	}%
}   

\begin{lemma}
    \label{reachability_lemma}
    Solving the CSR query for neural networks with continuous input and output domains can be polynomially reduced to the $\overline{\textit{NNReach}}$ problem.
\end{lemma}

\emph{Proof.} We will begin by demonstrating the unbounded version (where no $\epsilon$ is provided as input), followed by an explanation of how we can extend this proof to a specific $\epsilon$-bounded domain. Given an instance $\langle f,S,\x\rangle$ we can construct an instance $\langle f,\phi_{in},\phi_{out}\rangle$ for which we define $\phi_{in}$ as a conjunction of linear constraints: $\mathbf{X}_i=\x_i$ for each $i\in S$. We use $\mathbf{X}_i=\x_i$ to denote an assignment of the input variable $\mathbf{X}_i$ to the assignment $\x_i\in\mathbb{R}$. Assume that $f(\x)$ is classified to some class $t$ (i.e., it holds that $f(\x)_t\geq f(\x)_i$ for all $i\neq t$). Then, we define $\phi_{out}$ as a \emph{disjunction} of linear constraints of the form: $\mathbf{Y}_t< \mathbf{Y}_i$ for all $i\neq t$.

$S$ is a sufficient reason with respect to $\langle f,\x\rangle$ if and only if setting the features $\mathbf{X}$ to there corresponding values in $\x$ determines that the prediction $f(\x)$ always stays $t$. Hence, $S$ is not a sufficient reason if and only if there exist some assignments $\x\in\mathbb{R}^n$ and an assignment $\mathbf{y}\in\mathbb{R}^m$ for which all features in $\mathbf{X}$ are set to their values in $\x$ and $f(\x)$ is \emph{not} classified to $t$, implying that for all $i\neq t$ it holds that $f(\mathbf{x})_t<f(\mathbf{x})_i$. This concludes the reduction. This reduction can be readily adapted for scenarios where sufficiency is defined within a \emph{bounded} $\epsilon$ domain. This involves specifying that for all $i\in\overline{S}$, we include constraints such as: $\mathbf{X}_i\leq \x_i+\epsilon$ and $\mathbf{X}_i\geq \x_i-\epsilon$.

$\qedsymbol{}$



%\begin{lemma}
%    Solving the MSR query for a neural network classifier $f$ with either binary or continuous input and output values is $\Sigma^P_2$-Complete.
%\end{lemma}

\textbf{Finishing the Membership Proof.} Membership of \emph{MSR} for continuous input domains can now be derived from the fact that we can guess some partial assignment to some subset $S\in\{1,\ldots n\}$ of size $k$. Then we can utilize Lemma~\ref{reachability_lemma} to incorporate a coNP-oracle which validates whether $S$ is sufficient concerning $\langle f,\x\rangle$ over the continuous domain $\mathbb{R}^n$.

\textbf{Hardness.} Prior work~(\cite{barcelo2020model}) demonstrated that \emph{MSR} is $\Sigma^P_2$-hard for MLPs limited to \emph{binary} inputs and outputs. To comprehend how this proof might be adapted to continuous domains, we will initially outline the key elements of the proof in the binary scenario. This involves a reduction from the \emph{Shortest-Implicant-Core} problem, which is known to be $\Sigma^P_2$-Complete~(\cite{umans2001minimum}). We begin by defining an \emph{implicant} for Boolean formulas:

\begin{definition}
    Let $\phi$ be a boolean formula. An implicant $C$ for $\phi$ is a partial assignment of the variables of $\phi$ such that any assignment to the remaining variables evaluates to true. 
\end{definition}

The reduction also makes use in the following Lemma (whose proof appears in~\cite{barcelo2020model}):

\begin{lemma}
    \label{boolean_circuit_mlp_lemma}
    Any boolean circuit $\phi$ can be encoded into an equivalent MLP over the binary domain $\{0,1\}^n\to \{0,1\}$ in polynomial time.
\end{lemma}

We will now begin by introducing the reduction for \emph{binary}-input-output MLPs from the \emph{Shortest-Implicant-Core} problem~(\cite{barcelo2020model}). The \emph{Shortest-Implicant-Core} problem is defined as follows:

\vspace{0.5em} % Add vertical space 

\noindent\fbox{%
    \parbox{\columnwidth}{%
\mysubsection{Shortest Implicant Core}:

\textbf{Input}: A formula in disjunctive normal form (DNF) $\phi:=t_1\vee t_2 \ldots \vee t_n$, an implicant $C$ of $\phi$, and an integer $k$.

\textbf{Output}: \yes{}, if there exists an implicant $C'\subseteq t_n$ of $\phi$ of size $k$, and \no{} otherwise
    }%
}

Initially, we identify $X_c$ as the set of features not included in $t_n$. Subsequently, each variable $x_j\in X_c$ can be divided into $k+1$ distinct variables $x_j^1,\ldots x_j^{k+1}$. For each $i\in\{1,\ldots, k+1\}$, we can construct a new formula $\phi^{(i)}$ by substituting every instance of the variable $x_j$ in $X_c$ with $x_j^i$.  Finally, we can define $\phi'$ as the conjunction of all the $\phi^{(i)}$ formulas. In summary:

\vspace{0.5em} % Add vertical space 

\begin{equation}
    \begin{aligned}
            \phi^{(i)}:=\phi[x_j\to x_j^i ,\forall x_j\in X_c], \\
            \phi':=\bigwedge_{i=1}^{k+1}\phi^{(i)}
    \end{aligned}
\end{equation}

The reduction then applies Lemma~\ref{boolean_circuit_mlp_lemma} to transform $\phi'$ into an MLP $f$ with binary inputs and outputs. It also constructs a vector $\x$, setting the value to $1$ for all features in $t_n$ that are positive literals, and $0$ for the rest. According to the reduction in~\cite{barcelo2020model}, then $\langle \phi, C,k\rangle\in$ \emph{Shortest-Implicant-Core} if and only if there exists a subset $S$ of size $k$ such that, when $S$ is fixed to its values in $\x$, then $f(\x_S;\z_{\Bar{S}})$ consistently predicts $1$, for any assignment $\z\in\{0,1\}^n$. This implies that $\langle \phi, C,k\rangle\in$ \emph{Shortest-Implicant-Core} if and only if $\langle f,\x,k\rangle\in$\emph{MSR} for MLPs with binary inputs and outputs, demonstrating that \emph{MSR} for binary-input-output MLPs is $\Sigma^P_2$-Hard.

However, it is important to note that the previous lemma may not apply when the MLP in question does not exclusively handle binary inputs and outputs. We will now demonstrate how we can transform $f$ to another MLP $f'$, in polynomial time. We then will prove that when $f$ is defined such that $f:\{0,1\}^n\to \{0,1\}$ and $f'$ is defined such that $f':\mathbb{R}^n\to\mathbb{R}^2$, a sufficient reason of size $k$ exists for $\langle f,\x\rangle$ if and only if one exists for $\langle f',\x\rangle$. This reduction will extend the reduction from the \emph{Shortest-Implicant-Core} to \emph{MSR} for MLPs with non-binary inputs and outputs, thereby providing a proof of $\Sigma^P_2$-hardness for non-binary inputs and output MLPs.

Typically, $f'$ will be developed based on $f$, but it will include several additional hidden layers and linear transformations. To begin, we will introduce several constructions that are crucial for this development. We will specifically demonstrate how these constructions can be implemented using only linear transformations or ReLU activations, allowing them to be integrated into our MLP framework. The initial construction we will discuss was proposed by~\cite{salzer2021reachability}:

\begin{equation}
    z := \text{ReLU}(\frac{1}{2}-x)+\text{ReLU}(x-\frac{1}{2})-\frac{1}{2}
\end{equation}

In~\cite{salzer2021reachability}, it is demonstrated that the construction is equivalent to:

\begin{equation}
    \begin{aligned}
        z = \begin{cases}
        \ -x & if \ \ x<\frac{1}{2} \\
    x-1 & otherwise
    \end{cases}
\end{aligned}
\end{equation}

From this, it can be directly inferred that $z=0$ if and only if $x=0$ or $x=1$, and $z\neq 0$ for any other values of $x\in\mathbb{R}$. This construction will prove useful in our reduction. We will also utilize a second construction, which is described as follows:

\begin{equation}
    \begin{aligned}
        z: = \text{ReLU}(x) + (-1)\cdot \text{ReLU}(-x) = |x|
\end{aligned}
\end{equation}

\textbf{The reduction.} We will now explain how to convert $f$ into the appropriate $f'$ to ensure the validity of our reduction. We begin with the foundational structure of  $f$ as the skeleton for $f'$. Initially, we add extra neurons to the first hidden layer and link them to the input layer as follows. Each input feature $\x_i$ is processed through a series of computations, which, leveraging the two constructions previously mentioned, can be performed solely using ReLU activations. Specifically, for each feature $\x_i$, we compute:

\begin{equation}
\begin{aligned}
    \x'_i := \text{ReLU}(\text{ReLU}(\frac{1}{2}-\x_i)+\text{ReLU}(\x_i-\frac{1}{2})-\frac{1}{2})+ \\ (-1)\cdot \text{ReLU}(-\text{ReLU}(\frac{1}{2}-\x_i)-\text{ReLU}(\x_i-\frac{1}{2})+\frac{1}{2})
    = \\
    |\text{ReLU}(\frac{1}{2}-\x_i)+\text{ReLU}(\x_i-\frac{1}{2})-\frac{1}{2}|
    \end{aligned}
\end{equation}


The existing structure of $f$ (and thus $f'$) features a \emph{single} output neuron, which, when given an input from the domain $\x\in\{0,1\}^n$, returns a single \emph{binary} value from $\{0,1\}$. Let us denote the value of this output neuron as $o_1$. To develop $f'$, we introduce additional hidden layers that connect sequentially after $o_1$. Initially, we link $o_1$
to two new output neurons, denoted $o_{1,1}$ and $o_{1,2}$. These neurons are configured as follows: $o_{1,2}$ is directly tied to $o_1$ with a linear transformation of value $1$, effectively making $o_{1,2}=o_{1}$. For the other output, we establish the weights and biases through the following computation:

\begin{equation}
    o_{1,1}:=\text{ReLU}(o_1)-1
\end{equation}

It is evident that the following equivalence is maintained:

\begin{equation}
    \begin{aligned}
        o_{1,1} = \begin{cases}
         1 & if \ \ o_1=0 \\
    0 & o_1=1
    \end{cases}
\end{aligned}
\end{equation}

We will now outline the construction of the \emph{output} layer of $f'$, which consists of two outputs: $o_{2,1}$ and $o_{2,2}$. Adhering to our formalization of MLP, we assume the following order: $o_{2_1}\succ o_{2,2}$, without loss of generality. This means that in cases where a specific assignment $\z\in\mathbb{R}^n$ is processed through $f'$ and results in $o_{2_1}=o_{2,2}$, then $f'$ is classified under class $o_{2,1}$. Conversely, $f'$ is classified as class $o_{2,2}$ only if the value at $o_{2,2}$ is \emph{strictly} larger than that at $o_{2,1}$. The constructions for $o_{2,1}$ and $o_{2,2}$ are as follows:

\begin{equation}
\begin{aligned}
    o_{2,1}:=\text{ReLU}(o_{1,1})+(-1)\cdot \text{ReLU}(-o_{1,1})+\sum_{i\in [m]}\x'_i,\\
    o_{2,2}:=\text{ReLU}(o_{1,2})+(-1)\cdot \text{ReLU}(-o_{1,2})+\sum_{i\in [m]}\x'_i+o_{2,1}
    \end{aligned}
\end{equation}

Which is equivalent to:

\begin{equation}
\begin{aligned}
    o_{2,1}:=|o_{1,1}|+\sum_{i\in [m]}\x'_i,\\
    o_{2,2}:=|o_{1,2}|+\sum_{i\in [m]}\x'_i+o_{2,1}
    \end{aligned}
\end{equation}

\textbf{Reduction Proof.} We will now establish the correctness of the reduction. Once more, we will begin by establishing correctness for the \emph{unbounded} domain (where no $\epsilon$ is provided as input), before outlining the necessary adjustments for the bounded domain. Initially, by design, we note that $\x\in\{0,1\}^n$ because the input vector, derived from the \emph{Shortest-Implicant-Core}, contains only binary features. Following the specified construction of $\x$ where all positive features in $t_n$ within the formula $\phi'$ are set to $1$, and all other features are set to $0$, it results in $t_n$ evaluating to True. Consequently, $\phi'$ evaluates to true, thereby causing $f$ to evaluate to $1$. This means that when $\x$ is processed through $f$, the single output value $o_1$ is determined to be $1$. Since the connections from the input layer to the output $o_1$ in $f'$ remain unchanged from those in $f$, when $\x$ is input into $f'$, $o_1$ also evaluates to $1$.

From the design of $f'$, we observe that $o_{1,1}=0$ and $o_{1,2} = 1$. Given that $\x$ comprises only binary values, our earlier construction ensures that all $\x'_i$ values are equal to $0$. Consequently, this results in: $o_{2,1}=0$ and $o_{2,2}=1$, leading to the classification of $f$ under class $o_{2,2}$.

Additionally, we observe that by definition, $\x'_i\geq 0$ for all $i$, which implies that it invariably holds that:

\begin{equation}
\begin{aligned}
    [o_{2,1}=|o_{1,1}|+\sum_{i\in [m]}\x'_i\geq 0] \ \ \ \wedge \ \ \
    [o_{2,2}=|o_{1,2}|+\sum_{i\in [m]}\x'_i+o_{2,1}\geq 0]
    \end{aligned}
\end{equation}

Since the precedence $o_{2,1}\succ o_{2,2}$ is established, it follows that $f'$ is classified under class $o_{2,1}$ if and only if $o_{2,1}=0$. This condition arises because if $o_{2,1}\neq 0$, then it necessarily means that $o_{2,2}>o_{2,1}$ resulting in $f'$ being classified under $o_{2,2}$. Conversely, if $o_{2,2}=0$, then $o_{2,1}=o_{2,2}$, and thus $f'$ is classified under $o_{2,1}$ by virtue of $o_{2,1}\succ o_{2,2}$.

Assuming that $\langle \phi,C,k\rangle\in$ \emph{Shortest-Implicant-Core}, as previously mentioned, it follows that there is a subset $S$ of features, each of size $k$, where fixing the features in $S$ to their values in $\x$ results in the output $o_1$ in $f'$ consistently remaining at $1$, irrespective of how the features in $\Bar{S}$ are set within the binary domain $\{0,1\}^n$. However, we need to establish a more robust claim —-- that this sufficiency extends to any values assigned to the features of $\Bar{S}$ from the continuous domain $\z\in\mathbb{R}^n$. Specifically, we need to demonstrate that when the values of the features in $S$ are fixed as they are in $\x$, the prediction of $f'$ consistently remains at $o_{2,2}$, no matter what \emph{real} values are assigned to the features in $\Bar{S}$.

%We need to prove that the sufficiency of $S$ holds when the features of $\Bar{S}$ can be sampled from any $\z\in \mathbb{R}^n$. Let us first assume that $\z\in\{0,1\}^n$. From our construction, this means that the value of $\x'_i=0$ for all $i$. Hence, this also implies that $o_{2,1}=o_{1,1}$ and $o_{2,2}= o_{1,2}+o_{2,1}= o_{1,1}+o_{1,2}$.

We have established that fixing the values of $S$ to those in $\x$ results in $ o_1 $ in $f'$ consistently equaling $1$, irrespective of how the values of $\Bar{S}$ are set within $\{0,1\}^n$. Given that $\x'_i \neq 0$ for all $ i$ under our conditions, it follows that both $ o_{2,1}$ and $o_{2,2}$ are greater than $0$. We have previously demonstrated that if $ o_{2,1} > 0$, then $ f' $ is classified as $o_{2,2}$. Therefore, for any potential assignment of the features in $\Bar{S}$ within the domain $ \{0,1\}^n $, $ f' $ is classified under $ o_{2,2} $. Since the original prediction is $ o_{2,2} $, this confirms that there is indeed a subset $S$ of size  $k$, which, when fixed at their values, ensures that the prediction of $f'$ remains consistently at $o_{2,2}$.

%We already know that if we fix the values of $S$ to $\x$, then regardless of setting the values of $\Bar{S}$ to any $\{0,1\}^n$, then the value of $o_{1}$ in $f'$ remains equal to $1$. Since we have that $\x'_i\neq 0$ for all $i$ under our current conditions, then it directly holds that $o_{2,1}, o_{2,2}>0$. We have already shown that if $o_{2,1}>0$, then $f'$ is classified as $o_{2,2}$. We hence get that for any possible assignment of the features $\Bar{S}$ to the domain $\{0,1\}^n$, we get that $f'$ is classified to $o_{2,2}$. Since the original prediction is $o_{2,2}$, this implies that there indeed exists a subset $S$ of size $k$, which when held at their values, determines that the prediction of $f'$ remains constant at $o_{2,2}$.

Let us consider the scenario where the values for the complementary set $\Bar{S}$ are sourced not exclusively from the $\{0,1\}^n$ domain, meaning they are derived from $\z \in \mathbb{R}^n \setminus \{0,1\}^n $. In more precise terms, there exists at least one feature $ i$ within $\Bar{S}$ such that for the assignment $ \z \in \mathbb{R}^n $, $ \z_i \neq 1 $ and $ \z_i \neq 0 $. We must demonstrate that under these circumstances, the prediction for $ f' $ still stabilizes at $ o_{2,2} $. For this particular feature $\z_i $, it is confirmed that:

%Let us now assume that the values of the complementary $\Bar{S}$ are not necessarily taken from the $\{0,1\}^n$ domain, or in other words are taken from from $\z\in\mathbb{R}^n\setminus \{0,1\}$. More formally, there exists at least one feature $i\in \Bar{S}$ such that for the assignment $\z\in\mathbb{R}^n$ it holds that $\z_i\neq 1$ and $\z_i\neq 0$. We need to prove that the prediction still remains constant at $o_{2,2}$ in this case as well. We have that for our specific feature $\z_i$ it satisfies that: 

\begin{equation}
       \text{ReLU}(\frac{1}{2}-\z_i)+\text{ReLU}(\z_i-\frac{1}{2})-\frac{1}{2}\neq 0
\end{equation}

This also suggests that within the newly constructed hidden layers of $ f' $, there are some $ \x'_i $ values such that:\begin{equation}
       \x'_i = |\text{ReLU}(\frac{1}{2}-\z_i)+\text{ReLU}(\z_i-\frac{1}{2})-\frac{1}{2}| >0
\end{equation}

We now can conclude that:

\begin{equation}
    o_{2,1}=|o_{1,1}|+\sum_{i\in [m]}\x'_i > 0
\end{equation}

Consequently, $ f $ is classified under class $ o_{2,2}$ even when the features in $ \Bar{S}$ are assigned values from the domain $\mathbb{R}^n \setminus \{0,1\}^n$. In summary, this indicates that there exists a subset $S$ which consistently remains sufficient for $ \langle f', \x \rangle$ when assessing sufficiency across any possible value in $\mathbb{R}^n$.

For the converse direction of the reduction, let us assume that $\langle \phi, C, k \rangle \notin$ \emph{Shortest-Implicant-Core}. This implies that there is no subset $S$ of size $ k $ or smaller such that $ f $ consistently evaluates to class $o_1$ when the values of $ \mathbf{z}$ come solely from the binary domain $\{0,1\}^n$. This also means that for any subset $S$ of size $k$ or smaller, there exists some $\mathbf{z} \in \{0,1\}^n$ where $ f(\mathbf{x}_S; \mathbf{z}_{\Bar{S}})$ is evaluated to $ 0 $ (resulting in a misclassification). Since $ (\mathbf{x}_S; \mathbf{z}_{\Bar{S}}) $ is entirely binary, all hidden neurons $ \mathbf{x}'_i $ evaluate to $ 0 $, leading to $ o_{2,1} = 0$ and thus, $ f' $ is misclassified to $o_{2,1} $. This overall demonstrates that there is no subset $ S $ of size $k $ or less that is sufficient concerning $\langle f', \mathbf{x} \rangle$, when the sufficiency of $S$ is assessed relative to $\mathbb{R}^n$. This completes the reduction.

We now need to demonstrate how to adjust the following reduction when $\epsilon$ is an input parameter, with the sufficiency of $S$ restricted to an $\epsilon$-ball rather than being unbounded. The reduction can simply set $\epsilon>1$. Since $\x\in\{0,1\}^n$, the sufficiency of $S$ is then defined over a continuous $\epsilon$-ball that includes all inputs in $\{0,1\}^n$. Let's refer to this domain as $\mathbb{F}$. In our reduction, we have shown that any contrastive assignment achievable in $f'$ must also be achieved over the domain $\{0,1\}^n$. Thus, sufficiency for $S$ in the domain $\mathbb{R}^n\setminus \{0,1\}^n$ is equivalent to sufficiency in $\mathbb{F}\setminus \{0,1\}^n$, which implies that our reduction is applicable even in such a \emph{bounded} domain.

$\qedsymbol{}$


%For the other direction of the reduction, we now assume that $\langle \phi,C,k\rangle\not\in$ \emph{Shortest-Implicant-Core}. It hence holds that there does not exist any subset $S$ of size $k$ or less for which $f$ is always evaluated to class $o_1$ when the values of $\z$ are only taken from the binary domain $\{0,1\}^n$. This also equivalently implies that for any subset of size $k$ or smaller, there exists some $\z\in\{0,1\}^n$ for which $f(\x_S;\z_{\Bar{S}})$ is evaluated to $0$ (is misclassified). Since $(\x_S;\z_{\Bar{S}})$ is entirely binary, we get that all hidden neurons $\x'_i$ are evaluated to $0$. We hence get that $o_{2,1}-0$, and hence it holds that $f'$ is classified to $o_{2,1}$ (is misclassified). We, overall get that there does not exist a subset $S$ of size $k$ or less which is sufficient with respect to $\langle f',\x\rangle$, when the sufficiency of $S$ is evaluated concerning $\mathbb{R}^n$. This concludes the reduction.

\begin{lemma}
Solving the B-MSR query over a neural network classifier $f$ with either continuous or discrete input and output domains is NP-Complete.
\end{lemma}

\emph{Proof.} We begin by proving membership. Membership in NP is established because one can guess a subset of features $S$ and then verify whether $f(\x_S;\x'_{\Bar{S}}) \neq f(\x)$ and whether $|S| \leq k$.

\textbf{Hardness.} We demonstrate hardness by presenting a reduction from the classic \emph{CNF-SAT} problem, which is known to be NP-Complete and defined as follows:

\vspace{0.5em} % Add vertical space 

\noindent\fbox{%
    \parbox{\columnwidth}{%
\mysubsection{CNF-SAT}:

\textbf{Input}: A formula in conjunctive normal form (CNF): $\phi$.

\textbf{Output}: \yes{}, if there exists an assignment to the $n$ literals of $\phi$ such that $\phi$ is evaluated to True,  and \no{} otherwise
    }%
}

 We denote $\mathbf{1}_n$ as a vector of size $n$ containing only $1$'s, and $\mathbf{0}_n$ as a vector of size $n$ containing only $0$'s.  Given some input $\phi$, we can first assign $\mathbf{1}_n$ to $\phi$ (setting all variables to True). If $\phi$ evaluates to True, then there exists a Truth assignment to $\phi$, so the reduction can construct some trivially correct encoding. We can hence assume that $\phi(\mathbf{1}_n)=0$. We now denote the following:

 \begin{equation}
 \begin{aligned}
 \phi_2: = (x_1 \wedge x_2 \wedge \ldots x_n), \\
          \phi':=\phi \vee \phi_2
 \end{aligned}
 \end{equation}
 
 $\phi'$, is no longer a CNF, however, we can still use Lemma~\ref{boolean_circuit_mlp_lemma} to transform $\phi'$ into an MLP $f$ which behaves equivalently to $\phi'$ under the domain $\{0,1\}^n$. The reduction then finally constructs $\langle f,\x:=\mathbf{1}_n,\x':=\mathbf{0}_n,k=n-1\rangle$. 

 We first note that $\phi_2(\x) = 1$, and hence both $\phi'(\x) = 1$ and $f(\x) = 1$. If $\langle \phi \rangle \in$ \emph{CNF-SAT}, then there exists an assignment of variables that evaluates $\phi$ to True. This means there is some assignment in $\{0,1\}^n$ that evaluates $f$ to True. Since we have assumed that this is not the assignment $\x = \mathbf{1}_n$, it must be some assignment $\x' \in \{0,1\}^n \neq \x$. This implies that there exists an assignment of size $k < n$ that evaluates $\mathbf{1}_n$ to True. Consequently, there exists a subset $S$ where $|S| \leq k$ and $f(\mathbf{1}_S; \mathbf{0}_{\Bar{S}}) = f(\x_S; \x'_{\Bar{S}}) = 1 = f(\mathbf{1}_n)$.

%We first note that $\phi_2(\x)=1$ and hence both $\phi'(\x)=1$ and $f(\x)=1$. If $\langle \phi\rangle\in$ \emph{CNF-SAT} then there exists some assignment to variables which evaluates $\phi$ to True. This implies that there exists some assignment over $\{0,1\}^n$ which evaluates $f$ to true. We have assumed that this is not the assignment $\x=\mathbf{1}_n$, and hence it is some assignment $\x'\in\{0,1\}^n\neq \x$. This implies that there exists some assignment to $\mathbf{1}_n$ of size $k<n$ or which evaluates to true, and hence, there exists a subset $S$ for which $|S|\leq k$ and $f(\mathbf{1}_S;\mathbf{0}_{\Bar{S}})=f(\x_S;\x'_{\Bar{S}})=1=f(\mathbf{1}_n)$.

For the second direction, we assume that $\langle \phi \rangle \not\in$ \emph{CNF-SAT}. This implies that no assignment evaluates $\phi$ to True. Consequently, there is no assignment with fewer than $n$ variables that evaluates $\phi_2 = (x_1 \wedge x_2 \wedge \ldots \wedge x_n)$ to True. Therefore, there is no assignment of size $k = n-1$ or less that evaluates either $\phi$ or $\phi_2$ to True. From this, we conclude that there is no subset $S \subseteq [n]$ of size $k$ or less for which $f(\x_S; \x'_{\Bar{S}}) \neq f(\x) = f(\mathbf{1}_n) = 1$, thus completing the reduction.

%For the second direction, we assume that $\langle \phi\rangle\not\in$ \emph{CNF-SAT}. This implies that no assignment evaluates to True in $\phi$. There clearly does not exist any assignment with strictly less than $n$ assignments that evaluate $\phi_2=(x_1 \wedge x_2 \wedge \ldots x_n)$ to True. Hence, we get that there is no assignment of size $k=n-1$ or less that evaluates either $\phi$ or $\phi_2$ to true. From here, we get that there is no subset $S\in[n]$ of size $k$ or less for which $f(\x_S;\x'_{\Bar{S}}) \neq f(\x)=f(\mathbf{1}_n)=1$, which concludes the reduction.

$\qedsymbol{}$


\begin{lemma}
Solving the P-MSR query on a neural network classifier $f$ is $\text{NP}^{\text{PP}}$-Hard.
\label{probabilistic_lemma_regular_appendix}
\end{lemma}

\emph{Proof.} The reduction is derived by integrating the proof from~\cite{waldchen2021computational} with Lemma~\ref{boolean_circuit_mlp_lemma}. The work in~(\cite{waldchen2021computational}) established that finding a cardinally minimal probabilistic sufficient reason for a CNF classifier, given a discrete uniform distribution over $\{0,1\}^n$, is $\text{NP}^{\text{PP}}$-Hard. Using Lemma~\ref{boolean_circuit_mlp_lemma}, we can transform $\psi$ into an MLP $f$, applicable to either discrete or continuous domains. This allows us to construct $\langle f,\x':=\x,k'=k\rangle$ and define $\mathcal{D}$ as the uniform distribution over $\{0,1\}^n$. Given that $\x$ is an input for a CNF classifier, it directly follows that $\x\in\{0,1\}^n$. It hence follows that a cardinally minimal probabilistic sufficient reason for $\psi$ exists if and only if one exists for $\langle f,\x\rangle$ concerning the distribution $\mathcal{D}$, thereby completing the reduction.

$\qedsymbol{}$

We observe that since the previous proof establishes \emph{hardness} over the uniform distribution, this hardness persists under any distributional assumption that includes this distribution, such as independent distributions or more expressive ones, like the Markovian distributions examined by~\citep{marzouktractability}, or the Hidden Markov modeled distributions examined by~\citep{marzouk2025computational}.  

\section{Proof of Theorem~\ref{second_theorem_appendix}}
\label{second_proof_appendix_section}

\begin{theorem}
Given a neural network classifier $f$ with ReLU activations, and  $\x\in\mathbb{R}^n$, \begin{inparaenum}[(i)] \item $\forall\epsilon>0$ approximating cardinally minimal robust sufficient reasons with $n^{\frac{1}{2}-\epsilon}$ factor is $\Sigma^P_2$-Hard, \item $\forall\epsilon>0$ it is NP-Hard to approximate cardinally minimal probabilistic or baseline sufficient reasons with factor $n^{\frac{1}{2}-\epsilon}$.
\label{second_theorem_appendix}
\end{inparaenum}
\end{theorem}

\emph{Proof.} We follow common conventions and when discussing the inapproximability of obtaining cardinally minimal sufficient reasons, refer to the \emph{optimization}-version of these problems, or in other words obtaining a cardinally minimal sufficient reason, instead of asking whether there exists such a subset of size $k$ (which is the decision version). To avoid confusion, we will refer to these problems as $\textit{R-MSR}^*$, $\textit{B-MSR}^*$ and $\textit{P-MSR}^*$. We start by proving the first complexity class:

\begin{lemma}
    Given a neural network classifier $f$ with ReLU activations, and  $\x\in\mathbb{R}^n$, $\forall\epsilon>0$ approximating cardinally minimal robust sufficient reasons with $n^{\frac{1}{2}-\epsilon}$ factor (i.e., solving the  $\textit{R-MSR}^*$ query) is $\Sigma^P_2$-Hard
\end{lemma}

\emph{Proof.} We first note a known inapproximability result for the \emph{Shortest-Implicant-Core} problem~(\cite{umans1999hardness}) which will be used to prove the inapproximability result for our case: 

\begin{lemma}
    Given a DNF formula $\psi$, then for all $\epsilon>0$, approximating the Shortest Implicant Core of $\psi$ to within factor $n^{1-\epsilon}$ is $\Sigma^P_2$-Hard.
\end{lemma}

We acknowledge, however, that despite the difficulty of the \emph{MSR} query being established through a reduction from the \emph{Shortest Implicant Core} problem (as proven in~\cite{barcelo2020model} and discussed in Lemma~\ref{r_msr_proof_appendix}), this approach cannot be directly utilized because the reduction is not necessarily \emph{approximation preserving}. Specifically, the reduction discussed in Lemma~\ref{r_msr_proof_appendix}, which follows~\cite{barcelo2020model}, involves an MLP with an input space considerably larger than that of the boolean formula. If the input size of the boolean circuit is $n$, then the input size of the constructed MLP in the worst-case scenario will be $n\cdot (k+1)$.

Note that if $k = n$, the problem has a trivial solution. Therefore, we can assume the problem is addressed for cases where $n > k$, or equivalently $ n \geq k + 1 $. Let us denote the optimal solution for the \emph{MSR} query in MLPs as $ \text{OPT}_{\text{MSR}}$ and the optimal solution for the shortest implicant core as $\text{OPT}_{\text{CORE}}$. Given that $k$ remains constant in both reductions, it follows that $\text{OPT}_{\text{MSR}} = \text{OPT}_{\text{CORE}}$. However, we still need to demonstrate that the same approximation ratios are applicable.

Let us suppose there exists some $\epsilon > 0$ such that an algorithm can solve the optimization problem of obtaining a cardinally minimal sufficient reason for MLPs with an approximation factor of $n^{\frac{1}{2} - \epsilon}$. We will denote the solution provided by this algorithm as $k'$. Consequently, we observe that:

\begin{equation}
\begin{aligned}
    k'\leq\text{OPT}_{\text{MSR}}\cdot (n\cdot (k+1))^{\frac{1}{2}-\epsilon}=\text{OPT}_{\text{CORE}}\cdot (n\cdot (k+1))^{\frac{1}{2}-\epsilon}\leq \\ \text{OPT}_{\text{CORE}}\cdot (n\cdot n)^{\frac{1}{2}-\epsilon}\leq \text{OPT}_{\text{CORE}}\cdot n^{1-2\cdot \epsilon}\leq \text{OPT}_{\text{CORE}}\cdot n^{1-\epsilon}
    \end{aligned}
\end{equation}

This results in an $n^{1-\epsilon}$ factor approximation algorithm for the shortest implicant core problem. Consequently, we conclude that approximating the optimization problem of obtaining robust cardinally minimal sufficient reasons is $\Sigma^P_2$-Hard to approximate within a factor of $n^{\frac{1}{2}-\epsilon}$.

$\qedsymbol{}$


\begin{lemma}
    Given a neural network classifier $f$ with ReLU activations, and  $\x\in\mathbb{R}^n$, $\forall\epsilon>0$ approximating cardinally minimal probabilistic sufficient reasons with $n^{1-\epsilon}$ factor is NP-Hard
\end{lemma}

This result can be extracted from the inapproximability results for obtaining cardinally minimal probabilistic sufficient reasons for CNF classifiers that were proven by~\cite{waldchen2021computational}. This result is as follows:

\begin{lemma}
    Given a CNF classifier, $\psi$, and some instance $\x$, for all $\epsilon>0$ obtaining a probabilistic sufficient reason concerning $\langle \psi,\x\rangle$ under the uniform distribution defined over $\{0,1\}^n$, is NP-Hard to approximate within factor $n^{1-\epsilon}$.
\end{lemma}

Using Lemma~\ref{boolean_circuit_mlp_lemma}, we can replicate the process described under Lemma~\ref{probabilistic_lemma_regular_appendix}. We begin with $\psi$ and develop an MLP $f$, ensuring that a cardinally minimal sufficient reason applicable to $f$ is also valid for $\psi$. This reduction is approximation preserving, as both $k':=k$ and $n':=n$, indicating that the same approximation ratio is preserved. Consequently, the hardness results established for CNF classifiers are equally applicable to any MLP with discrete or continuous input domains.

\begin{lemma}
    Given a neural network classifier $f$ with ReLU activations, and  $\x\in\mathbb{R}^n$, $\forall\epsilon>0$ approximating cardinally minimal baseline sufficient reasons with $n^{1-\epsilon}$ factor is NP-Hard
\end{lemma}

We will perform an approximation preserving reduction from the \emph{Max-Clique} problem, which is known to be hard to approximate. The problem is defined as follows:


\vspace{0.5em} % Add vertical space 

\noindent\fbox{%
    \parbox{\columnwidth}{%
\mysubsection{Max-Clique}:

\textbf{Input}: A graph $G:=(V,E)$

\textbf{Output}: a \emph{clique} in $G$ (i.e., a subset of vertices $C\subseteq V$ where each two vertices in $C$ are adjacent), which has maximal cardinality.
    }%
}

The following inapproximability result is known for the Max-Clique problem~(\cite{haastad1999clique}):

\begin{lemma}
    Given a graph $G=(V,E)$, for any $\epsilon>0$ it is NP-Hard to approximate the maximum clique of $G$ with approximation factor $n^{1-\epsilon}$.  
\end{lemma}

We now present an (approximation-preserving) reduction from the Max-Clique problem to $\textit{B-MSR}^*$ for neural networks. Consider a graph $G = (V, E)$. We define a Boolean formula $\psi$ that corresponds to $G$. $\psi$ will have $|V|$ variables: $x_1, \ldots, x_V$ (each variable representing a vertex in the graph). We define $\psi$ as follows:

\begin{equation}
    \psi:=\bigwedge_{(u,v)\not\in E} (\neg x_u \vee \neg x_v) 
\end{equation}

Using Lemma~\ref{boolean_circuit_mlp_lemma}, we encode $\psi$ into an MLP $f$. We assert that a subset $C \subseteq V$ constitutes a maximal clique if and only if $S := E \setminus C$ is a cardinally minimal baseline sufficient reason for $\langle f, \mathbf{0}_n \rangle$ with respect to the baseline $\mathbf{1}_n$.

First, we note that $f(\mathbf{0}_n) = 1$, since all variables are set to True in this case. Therefore, a sufficient reason for $f$ concerning the baseline $\mathbf{1}_n$ would be a subset of features $S$ such that setting the features in $\overline{S}$ to $1$ keeps the classification as $1$. We will prove that $C$ is a clique in $G$ if and only if $S = E \setminus C$ is a sufficient reason for $\langle f, \mathbf{0}_n \rangle$ concerning the baseline $\mathbf{1}_n$.

If $C$ is \emph{not} a clique in $G$, there exist vertices $u, v \in C$ such that $(u, v) \notin E$. Therefore, the clause $\neg x_u \vee \neg x_v$ is included in $\psi$. Given that $u, v \in C$, they also belong to $\overline{S}$, and their features are modified from a $0$ assignment to a $1$ assignment. As a result, $\neg x_u \vee \neg x_v = \text{False}$, leading to $\psi = \text{False}$ and $f(\mathbf{0}_S; \mathbf{1}_{\Bar{S}}) = 0$. This demonstrates that in this case, $S$ is not sufficient concerning the baseline $\mathbf{1}_n$. Conversely, if $C$ is a clique in $G$, then for any $(u, v) \in C$, $(u, v) \in E$. This ensures that for any clause $\neg x_u \vee \neg x_v$ in $\psi$, both $x_u$ and $x_v$ remain fixed at the value $0$, resulting in $\neg x_u \vee \neg x_v = \text{True}$ and thus $f(\mathbf{0}_S; \mathbf{1}_{\Bar{S}}) = 1$. This confirms that $S$ is a sufficient reason concerning the baseline $\mathbf{1}_n$.

% $C$ is \emph{not} a clique of $G$, there exists some $u,v\in C$ such that $(u,v)\not\in E$. Hence it holds that $\neg x_u \vee \neg x_v$ is contained in $\psi$, and hence since we have that $u,v\in C$, then $u,v\in\overline{S}$ and hence these features are altered from a $0$ assignment to a $1$ assignment. It hence holds that $\neg x_u \vee \neg x_v = False$, so hence $\psi=False$ so $f(\mathbf{0}_S;\mathbf{1}_{\Bar{S}})=0$. This shows that in this scenario $S$ is \emph{not} a sufficient concerning the baseline $\mathbf{1}_n$. However, if $C$ is a clique in $G$, we have that for any $(u,v)\in C$ we have that $(u,v)\in E$. Hence we have that for any $(u,v)\in E$, then for any clause $\neg x_u \vee \neg x_v$ in $\psi$, both $x_u$ and $x_v$ remain fixed at the value $0$. We hence get that $\neg x_u \vee \neg x_v=True$ so $f(\mathbf{0}_S;\mathbf{1}_{\Bar{S}})=1$. We hence get that $S$ is a sufficient reason concerning the baseline $\mathbf{1}_n$.

From the previous claim, it follows directly that a sufficient reason $S$ is of minimal cardinality if and only if the cardinality of $E \setminus C$ is minimal (when $C$ is a clique in $G$). This is equivalent to requiring that $C$ has maximal cardinality, thereby concluding our proof.

$\qedsymbol{}$


\section{Technical Specifications}
\label{model_specifications_appendix}


We provide detailed model specifications for reproducibility purposes. While our evaluations are conducted on specific benchmarks, the SST methodology can be adapted to any neural network classifier. We compare the sufficient reasons derived from SST-trained models with explanations from post-hoc methods on traditionally trained models. To ensure a fair comparison, we perform a separate grid search for each configuration to determine the optimal model. For traditional models, we optimize based solely on validation predictive accuracy, whereas for SST models, we consider a combination of accuracy, faithfulness, and subset cardinality. For SST-based models, we consistently set the threshold value $\tau:=\frac{1}{2}$ and the faithfulness coefficient $\lambda:=1$, conducting the grid search focusing only on the learning rate $\alpha$ and the cardinality coefficient $\xi$.

We begin by outlining general training details applicable to either all image or all language classification tasks, followed by a discussion of benchmark-specific implementations.

\subsection{Image Classification Experiments} All image classification configurations underwent a grid search across various learning rates $\alpha$, with values $\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}, 10^{-7}\}$ for both standard and SST training. For SST, additional grid searches were conducted for the cardinality coefficient 
$\xi$ options: $\{10^{-5}, 10^{-6}, 10^{-7}, 10^{-8}, 10^{-9}, 10^{-10}, 10^{-11}\}$. All models were trained using the Adam optimizer, a batch size of $64$, and held-out validation and test sets. For robust masking, a PGD $\ell_{\infty}$ attack with $\epsilon:=0.12$ was used, consisting of $10$ steps each of size $\alpha':=10^{-2}$. The value of $\epsilon := 0.12$ was selected to strike a balance between allowing a sufficiently large perturbation for learning, while avoiding excessive distortion of the image. On one hand, this creates a challenging task for learning (as opposed to smaller perturbations). On the other hand, it prevents excessive distortion (as might occur with larger perturbations), which could result in the complementary set $\overline{S}$ containing images that are entirely outside the desired distribution. Lastly, we apply a black background to mask the MNIST images, both for SST and post-hoc approaches, leveraging their naturally zeroed-out borders, which provides a clearer interpretation of the portions of the digit that fall within the sufficient reason. Moreover, for CIFAR-10 and IMAGENET images, we select images from these benchmarks where a specific object is highlighted against a white background. The sufficient reasons for these images are depicted over the same white background, allowing the explanation to remain focused on the object itself. This approach is applied consistently for both SST and the post-hoc methods.


\subsection{Language Classification Experiments} All language classification experiments utilized a pre-trained Bert-base~(\cite{devlin2018bert}) model (applicable to both standard training and SST). The grid search focused on learning rates $\alpha:=\{2e^{-5},3e^{-5},5e^{-5}\}$, which are the typical values used for optimizing pre-trained Bert models~(\cite{devlin2018bert}). For SST, an additional grid search was conducted for the cardinality coefficient 
$\xi$ options: $\{10^{-4}, 10^{-5}, 10^{-6}, 10^{-7}, 10^{-8}\}$. Optimization was performed using the standard AdamW optimizer, with a batch size of $32$ and held-out validation and test sets.

\subsection{MNIST}

We trained a simple feed-forward neural network consisting of two hidden layers with sizes $128$ and $64$, respectively. The experiments were conducted using four Intel(R) Xeon(R) Gold 6258R @ 2.70GHz CPUs. For the standard training scenario, the optimal configuration selected was $\alpha:=10^{-4}$. For the SST-based models, the optimal configurations were: for robust masking $\alpha:=10^{-4}$ and $\xi:=10^{-7}$, and for both probabilistic masking and baseline masking, $\alpha:=10^{-3}$ and $\xi:=10^{-7}$.

\subsection{CIFAR-10} We train a ResNet18 architecture~(\cite{he2016deep}) (which is \emph{not} pre-trained) as our base model using four Intel(R) Xeon(R) Gold 6258R @ 2.70GHz CPUs and one Nvidia A100-SXM4-80GB GPU. We use a batch size of 64 and the Adam optimizer. For the robust masking-based SST configuration, the chosen hyperparameters are $\alpha:=10^{-3}$ and $\xi:=10^{-8}$.

\subsection{IMAGENET}
We train an SST-based model on a \emph{pre-trained} ResNet50~(\cite{he2016deep}) for IMAGENET classification using nine Intel(R) Xeon(R) Gold 6258R @ 2.70GHz CPUs and one Nvidia A100-SXM4-80GB GPU. The optimal configuration for robust masking was $\alpha:=10^{-6}$ and $\xi:=10^{-9}$.

\subsection{SNLI}


We train our models using a pre-trained Bert-base~(\cite{devlin2018bert}) on the SNLI classification task. The training is conducted on 16 Intel(R) Xeon(R) Gold 6258R CPUs at 2.70GHz and one Nvidia A100-SXM4-80GB GPU. For standard training on SNLI, the optimal learning rate was set at $\alpha:=2e^{-5}$. In the SST scenario, the best configuration with probabilistic masking was $\alpha:=2e^{-5}$ and $\xi:=10^{-7}$, whereas, for baseline masking, it was $\alpha:=2e^{-5}$ and $\xi:=10^{-6}$.

\subsection{IMDB}


Our models were trained using a Bert-base~(\cite{devlin2018bert}) pre-trained on IMDB sentiment analysis. The setup included 16 Intel(R) Xeon(R) Gold 6258R CPUs at 2.70GHz and one Nvidia A100-SXM4-80GB GPU. The optimal setting for standard training on IMDB was established at $\alpha:=2e^{-5}$. For SST, the most effective configuration with probabilistic masking was $\alpha:=2e^{-5}$ and $\xi:=10^{-7}$, while for baseline masking, it was $\alpha:=2e^{-5}$ and $\xi:=10^{-6}$.

\section{Training Time}
\label{training_time_section}
One drawback of our approach is its higher computational demand compared to conventional training. Specifically, the dual-propagation procedure we employ doubles the training cost. This increase is more pronounced with complex masking techniques like \emph{robust} masking, which involves an adversarial attack at each iteration, slowing down the process and potentially complicating convergence. These issues are similar to those encountered in adversarial training~(\cite{shafahi2019adversarial}). However, we argue that this additional training effort reduces the necessity for computationally intensive \emph{post}-computations to derive concise sufficient reasons, as highlighted in our paper. Furthermore, future studies could investigate various strategies to improve the efficiency of our training method, drawing on techniques used in adversarial training~(\cite{shafahi2019adversarial}).

We now will highlight the training time gain incorporated by SST for each particular benchmark, as opposed to a standard training configuration. 

\subsection{Image classification tasks}
For MNIST, Standard training required $311.03$ seconds (over 43 epochs), whereas baseline-masking SST was completed in just $26.55$ seconds (over 2 epochs). This significant speed increase in the training, despite involving a dual-propagation process, was primarily due to the optimal learning rate for SST with baseline masking being set at $10^{-3}$, compared to $10^{-4}$ for standard training. For probabilistic masking, the training duration was $180.83$ seconds (over 11 epochs), where the enhanced efficiency stemmed, again, from achieving optimal results at a higher learning rate. However, robust masking, which is more computationally demanding, took substantially longer, clocking in at $1796.99$ seconds (over 49 epochs). For IMAGENET, standard training using the robust masking configuration ran for $287056.81$ seconds (over $22$ epochs) while the parallel standard-training configuration for IMAGENET ran for $74141.85$ seconds, over $43$ epochs (around 4 times faster). For CIFAR-10, SST using the robust masking that was mentioned in the paper took $3159.15$ seconds (over 53 epochs) compared to standard training which took 477.42 seconds (over 49 epochs). 

\subsection{Language Classification tasks} For IMDB sentiment analysis, standard training ran for $4974.33$ seconds (over $1$ epoch), while SST with baseline masking ran for $9713.42$ seconds (over $2$ epochs) and SST with probabilistic masking clocked at $21516.37$ seconds (over $3$ epochs). For SNLI classification, standard training ran for $5353.71$ seconds (over $2$ epochs), while SST with a probabilistic masking ran for $6084.66$ seconds (over $1$ epoch), SST with a baseline masking ran for $10440$ seconds (over $2$ epochs).

\section{Generalization between Sufficiency Settings}
\label{generalizability_section_appendix}

%In this section, we explore the ability of SST to generalize between different forms of sufficiency configurations. In our work we addressed different forms of sufficiency --- including baseline, probabilistic and robust making configurations. A user can choose the desired masking procedure based on the desired form of sufficiency that is maintained. Stricter sufficiency forms will naturally push the model to select larger subsets while looser forms will select smaller ones. Moreover, clearly, training SST with one masking configuration addressing one form of sufficiency is not necessairily guaranteed to generalize well to other forms. One way to mitigate this is by training SST with a few different masking configrations (e.g., using a different masking setting at each batch), which is an interesting direction to explore within future work. However, already when trained with one masking configuration, we still expect in certain scenarios that one form of sufficiency may imply the other, mainly if they subsume other forms (for example, a uniform sampling in some bounded pertubation is subsumed by a robust masking over that same pertubation). 


In this section, we examine SST's ability to generalize across different types of sufficiency configurations. Our work considers various sufficiency forms, including baseline, probabilistic, and robust masking configurations. Users can select the masking procedure that aligns with their preferred sufficiency criterion. Stricter sufficiency constraints typically lead the model to select larger subsets, while looser constraints result in smaller ones. However, training SST with a specific masking configuration for one form of sufficiency does not necessarily ensure strong generalization to other forms. A potential approach to improve generalization is to train SST with multiple masking configurations, such as varying the masking setting across batches --- an avenue worth exploring in future work. Nevertheless, even when trained with a single masking configuration, there are cases where one sufficiency form may implicitly imply another, particularly when it encompasses other forms. For instance, uniform sampling within a bounded perturbation may be naturally subsumed by a robust masking strategy applied to the same perturbation.

We conducted an additional experiment in which we trained each model from the experiments in Tables~\ref{mnist_comparison} and~\ref{table:language_table} (covering MNIST, SNLI, and IMDB) to evaluate the impact of different masking configurations on various forms of sufficiency. The results are presented in Table~\ref{generalizability_appendix_table}.

\begin{table*}[h]
	\centering
    \small
	%\def\arraystretch{0.5}% 
	%\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	\caption{The generalization of various masking configurations to different sufficiency conditions.}    \label{generalizability_appendix_table}        
	\begin{tabular}{lccccccc}
		\\
		\toprule
		%\hline
		\multirow{3}{*}{} &
  		\multicolumn{1}{c}{Masking} &
            \multicolumn{3}{c}{Faithfulness} \\
             & {} & {\ \ Robust} & {\ Probabilistic} & {Baseline}\\ 
		\midrule
		%\hline
		%Anchors &\textbf{---} & 8.98 & ---  & 0.11 & 97.51 & 91.06  & 16.06  \\
		%SIS &\textbf{---} & 2.35 & --- & 4.34 & 98.08 &  90.72 & 46.1    \\
  	%	GS &\textbf{---} & 40.36 & ---  & 1.13 & 97.56 & 94.11 & 16.72   \\
        %\midrule
		& \textbf{robust} & 99.28 & 99.32 & 11.82  \\
		%\hline
  		\textbf{MNIST} & \textbf{baseline} & 98.91 & 98.38 &  96.52  \\
		%\hline
    	& \textbf{probabilistic} & 98.85 & 99.11 & 8.16 \\
        		\midrule

  		\raisebox{-1.5ex}{\textbf{SNLI}} & \textbf{baseline} & --- & 44.81 &  95.88  \\
		%\hline
    	& \textbf{probabilistic} & --- & 95.35 & 93.12 \\
                		\midrule

  		\raisebox{-1.5ex}{\textbf{IMDB}} & \textbf{baseline} & --- & 75.7 &  98.05  \\
		%\hline
    	& \textbf{probabilistic} & --- & 95.67 & 77.7 \\
		\bottomrule
	\end{tabular}
\end{table*}

Table~\ref{generalizability_appendix_table} presents results highlighting key generalization patterns across different tasks. For language tasks, we observe the following: In SNLI, probabilistic masking demonstrates high baseline faithfulness, whereas baseline masking exhibits considerably lower probabilistic faithfulness, suggesting superior generalization under probabilistic constraints. In IMDB, both probabilistic and baseline masking achieve moderate faithfulness in the opposite masking configuration. In the MNIST experiment, a more intricate dynamic emerges. Robust masking and probabilistic sampling remain within a bounded $\epsilon_p$ domain, while the baseline configuration is significantly out-of-distribution (OOD), making the baseline task more challenging. This is reflected in the larger subset size (23.69\%) produced by SST for the baseline compared to the robust and probabilistic settings. As a result, the larger sufficient reason derived from the baseline generalizes well across other configurations, maintaining high faithfulness. Conversely, subsets from probabilistic and robust masking generalize effectively to each other but perform poorly on the baseline, likely due to the baseline's pronounced OOD nature. These findings emphasize the complex interaction between domain selection, sampling distribution, and baseline properties in influencing generalization. We believe further exploration into the generalization capabilities of SST across masks presents an exciting research direction.

\subsection{Discussion on variations of the faithfulness metric} 

Similar to the challenges that arise when generalizing across different sufficiency configurations, a similar concern extends to the \emph{evaluation} of explanations --- specifically, the computation of faithfulness. As with many explanation frameworks, defining a sufficient reason can be ambiguous, particularly due to the varying interpretations of ``missingness'' in the complement. Our work addresses this inherent limitation, which is prevalent in many post-hoc methods, by evaluating and training explanations across diverse sufficiency formulations. However, we recognize that no single definition can fully encapsulate a model’s internal behavior, and this limitation also applies to faithfulness metrics assessing the sufficiency of subsets. This issue is widespread in many XAI methods, including SHAP~\citep{sundararajan2020many}, and metrics such as infidelity~\citep{yeh2019fidelity}, where different treatments of ``missing'' features lead to variations in explanation definitions and evaluation metrics. By exploring diverse sufficiency criteria, our work contributes to a more comprehensive understanding of this class of explanations.

%Results in Table~\ref{generalizability_appendix_table} indicate that for the language tasks, we observe the following: For SNLI, probabilistic masking achieves high baseline faithfulness, while baseline masking achieves only considerably low probabilistic faithfulness, indicating better generalization under probabilistic constraints. For IMDB, both probabilistic masking and basline masking reaches moderate faitfulness for the opposite masking configuration. In the MNIST experiment, a more nuanced dynamic emerges. Both robust masking and probabilistic sampling occur within a bounded $\epsilon_p$ domain, while the baseline configuration is significantly OOD, making the baseline task more challenging. This is reflected in the larger subset size (23.69\%) produced by SST for the baseline, compared to the robust and probabilistic settings. Consequently, the baseline sufficient reason, which is larger, generalizes well to the other configurations, maintaining high faithfulness. However, subsets from probabilistic and robust masking generalize well to each other  but poorly to the baseline, likely due to the baseline's significant OOD nature. This underscores the interplay between domain choice, sampling distribution, and baseline properties in shaping generalization. We believe that further research into the generalizabiltiy capabilities of SST is an excitig research avenue.

\section{Extensions to Simplified Input Spaces}
\label{simplifed_input_section_appendix}

It is evident that SST can be trained using simplified input spaces instead of individual features such as pixels or tokens. These simplified spaces --- such as super-pixels, sentences, or other meaningful units --- can, in some cases, enhance the human interpretability of the resulting explanations, as explored in previous works (e.g.,~\citep{ribeiro2016should, ribeiro2018anchors}). To evaluate this approach, we conducted an experiment on MNIST and CIFAR-10, using a straightforward simplified input setting of $2\times2$ pixel patches. Accordingly, we adjusted the output vector to match the number of super-pixels and applied masking at the level of these simplified inputs. 

For consistency, we used the same hyperparameter choices as in the original (non-simplified input space) grid search. However, a dedicated grid search for this specific configuration could yield improved results. Our findings indicate that, for MNIST, the average explanation size was 4.26\%, with 90.53\% faithfulness and a minimal accuracy drop of -0.34\%. In contrast, for CIFAR-10, we observed a -4.54\% accuracy drop, 91.74\% faithfulness, and an explanation size of 22.93\%. These results suggest that, under this particular setting, the original (non-simplified) input space produced better overall performance metrics. However, from a human interpretability standpoint, simplified inputs can provide more intuitive and comprehensible explanations. We include examples of these simplified explanations alongside their regular counterparts in Figure~\ref{fig:superpixels}.

\begin{figure}[hbt!]
\centering
\subfloat{\includegraphics[height=4cm]{figures/superp-pixels.png}}
\caption{Examples of comparisons between explanations generated by SST using superpixels with a $2\times 2$ patch versus those based on individual pixels.} \label{fig:superpixels}
\end{figure}


\section{Additional Ablation Experiments}
\label{ablation_results_appendix}

In this section, we provide additional ablation results for our experiments. Specifically, we examine the effects of varying both the $\epsilon_p$ perturbation and the $\alpha$ step size in the robust masking experiments, as well as adjusting the $\tau$ selection threshold parameter. The results are summarized in~\Cref{tab:table_1_ablation,tab:table_2_ablation,tab:table_3_ablation}, with a detailed discussion of each experiment provided separately.

\subsection{Varying the $\epsilon_p$-bound in robust masking} We perform an ablation study by varying the $\epsilon_p$ attack in robust masking on CIFAR-10, using the same configurations as in our main experiment. In general, increasing $\epsilon_p$ makes it more challenging to satisfy the sufficiency conditions of the explanation. Consequently, users can select different values based on the desired "degree" of sufficiency. A higher $\epsilon_p$ can lead to larger explanation sizes and/or reduced faithfulness, or it may result in decreased model accuracy. The specific metric affected depends on the relative weighting of different loss coefficients during training, which influence the optimization objectives.

As shown in Table~\ref{tab:table_1_ablation}, increasing $\epsilon_p$ within our configurations generally leads to lower faithfulness and larger explanation sizes. However, we note that excessively large values of $\epsilon_p$ can reduce the effectiveness of gradient perturbations, a well-known issue in adversarial training~\citep{addepalli2022scaling}. Thus, it is not surprising that at very high perturbations (e.g., above $0.25$), model accuracy deteriorates significantly. Addressing the challenges posed by such large perturbations, which is also relevant to adversarial training, remains an open problem for future research.

\subsection{Varying the step size $\alpha$ in robust masking.} We conduct an ablation study by adjusting the step size $\alpha$ in adversarial training on CIFAR-10, while keeping the adversarial perturbation $\epsilon_p$ fixed at $0.12$. The results are presented in Table~\ref{tab:table_2_ablation}. Similar to adversarial training, smaller values of $\alpha$ yield adversarial examples that are closer to the original input, whereas larger values generate more distant examples. Importantly, the faithfulness measure we present quantifies the proportion of test instances that remain sufficient under a robust mask for each configuration, all computed using the same attack strategy (i.e., with the same $\alpha$ value). These findings demonstrate that our model can adapt to different $\alpha$ values, producing relatively compact and faithful explanations. However, a substantial increase in step size leads to a decline in accuracy.

 
\begin{table}
\begin{minipage}{0.45\textwidth}
\small 
\centering
    \parbox{1.05\textwidth}{
    \caption{\centering An ablation study examining the effect of varying $\epsilon_p$ perturbations in robust masking on the CIFAR-10 benchmark.}\label{tab:table_1_ablation}}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{$\epsilon_p$ (\%)} & \textbf{Acc. (\%)} & \textbf{Faith. (\%)} & \textbf{Size (\%)}  \\ \hline
        0.01 & -0.13 & 99.31 & 10.2 \\ \hline
        0.05 & +0.7 & 95.09 & 0.91 \\ \hline
        0.1 & -1.58 & 91.46 & 9.78 \\ \hline
        0.12 & -1.96 & 90.43 & 12.99 \\ \hline
        0.15 & -1.45 & 88.03 & 11.91 \\ \hline
        0.2 & -3.32 & 85.34 & 25.49 \\ \hline
        0.25 & -4.58 & 79.9 & 17.48 \\ \hline 
        0.3 & -9.92 & 78.88 & 35.9 \\ \hline
        0.35 & -5.97 & 70.94 & 19.2 \\ \hline
        0.4 & -10.25 & 64.95 & 34.53 \\ \hline
        0.45 & -14.68 & 64.95 & 34.53 \\ \hline
        0.5 & -14.09 & 68.05 & 31.99 \\ \hline
    \end{tabular}
\end{minipage}%
\hfill
\begin{minipage}{0.45\textwidth}
\small
        \centering
    \parbox{1.05\textwidth}{
    \caption{\centering An ablation study examining the effect of varying $\alpha$: the step size in robust masking (with a fixed $\epsilon_p:=0.12$ perturbation) on the CIFAR-10 benchmark.}\label{tab:table_2_ablation}}

    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{$\alpha$ (\%)} & \textbf{Acc. (\%)} & \textbf{Faith. (\%)} & \textbf{Size (\%)}  \\ \hline
        1/255 & -0.66 & 89.47 & 12.01 \\ \hline
        2/255 & -2.22 & 89.9 & 19.7 \\ \hline
        3/255 & -1.13 & 89.74 & 10.93 \\ \hline
        4/255 & -2.12 & 90.83 & 17.28 \\ \hline
        5/255 & -1.96 & 90.43 & 12.99 \\ \hline 
        6/255 & -0.08 & 89.25 & 7.2 \\ \hline
        7/255 & -1.32 & 90.21 & 9.78 \\ \hline
        8/255 & -1.91 & 90.44 & 15.09 \\ \hline
        9/255 & -1.68 & 90.27 & 12.89 \\ \hline
        10/255 & -2.71 & 89.15 & 17.27 \\ \hline
    \end{tabular}
\end{minipage}
\end{table}

\begin{table}[H]
\small
    \centering
        \parbox{0.55\textwidth}{ \caption{\centering An ablation study examining the effect of varying the selection threshold $\tau$ in probabilistic masking on the CIFAR-10 benchmark.}\label{tab:table_3_ablation}}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{$\tau$ (0-1 range)} & \textbf{Acc. Gain (\%)} & \textbf{Faith. (\%)} & \textbf{Size (\%)}  \\ \hline
        0.1 & +0.39 & 73.88 & 3.96 \\ \hline
        0.2 & -0.61 & 81.22 & 5.81 \\ \hline
        0.3 & -0.23 & 81.26 & 2.55 \\ \hline
        0.4 & -1.57 & 84.68 & 9.80 \\ \hline
        0.5 & -0.09 & 87.54 & 9.08 \\ \hline 
        0.6 & -1.62 & 88.32 & 12.26 \\ \hline
        0.7 & -1.79 & 96.44 & 11.80 \\ \hline
        0.8 & -2.08 & 86.97 & 12.63 \\ \hline
        0.9 & -1.24 & 86.43 & 6.65 \\ \hline
    \end{tabular}
\end{table}

\subsection{Varying the selection threshold $\tau$} Lastly, we performed an ablation study on different values of $\tau$ trained with probabilistic masking on CIFAR-10. The results are presented in Table~\ref{tab:table_3_ablation}. Overall, as observed in the results, our method adapts to varying values of $\tau$ by optimizing the model to learn different weights within the explanation component output vector. However, excessively high or low $\tau$ values tend to be more sensitive and may negatively impact certain key metrics. For instance, a low setting of $\tau:=0.1$ resulted in reduced faithfulness ($73.88\%$), while a high setting of $\tau:=0.8$ led to lower accuracy ($-2.08\%$). In contrast, setting $\tau:=0.5$ yielded comparable accuracy ($-0.09\%$).

%\vspace{7em}
\section{Supplementary Results}
\label{supplementary_results_appendix}
In this section, we will present additional results to support our evaluations. 

\subsection{MNIST}

We begin by offering more image samples to contrast post-hoc explanations generated over MNIST with SST explanations trained using robust masking, for comparison purposes.


\begin{figure}[hbt!]
\centering
\subfloat{\includegraphics[height=12cm]{figures/mnist_posthoc_appendix.png}}
\caption{Examples of comparisons between explanations produced by \emph{SST} compared to post-hoc approaches for MNIST} \label{fig:mnist_posthoc_appendix}
\label{plot_mnist_images}
\end{figure}

\subsection{CIFAR-10}

We now transition to showcasing further comparison findings for CIFAR-10, contrasting post-hoc explanations with those generated using SST and robust masking techniques.

\begin{figure}[hbt!]
\centering
 %return this Shahaf

\subfloat{\includegraphics[height=13cm]{figures/cifar_posthoc_appendix.png}} \\
\caption{Examples of comparisons between explanations produced by \emph{SST} compared to post-hoc approaches for CIFAR-10} \label{fig:cifar_posthoc_appendix}
\end{figure}


%\subsection{Post-Hoc Methods}
%\subsubsection{Vision}
%Comparison of the SST results for post-hoc methods for MNIST, CIFAR10 and IMAGENT datasets.

%We start by presenting additional examples of images comparing the results of SST to those of the aforementioned post-hoc methods: Anchors~\cite{ribeiro2018anchors},  \emph{gradient search} (\emph{GS})~\cite{fong2017interpretable}, and SIS~\cite{carter2019made}. MNSIT dataset comparison is presented in Figure \ref{fig:mnist_posthoc_appendix}. CIFAR10 comparison is presented in Figure \ref{fig:cifar_posthoc_appendix}. A comparison for a sample from The IMAGENET datatet is presented in Figure \ref{fig:imagenet_posthoc_appendix}.


To further illustrate examples of explanations under various masking configurations, we conducted an ablation study using SST-based explanations on CIFAR-10. We implemented baseline masking by zeroing out the complement $\overline{S}$ (the trivial baseline: $\z:=\mathbf{0}n$), \emph{probabilistic} masking, where we sampled features over $\overline{S}$ from a uniform distribution either over the entire input domain ($\epsilon:=1$) or within a bounded region surrounding $\x$ ($\epsilon:=0.12$). Additionally, we performed robust masking by executing a PGD $\ell{\infty}$ attack within an $\epsilon:=0.12$ ball with $10$ steps each of size $\alpha':=10^{-2}$.

Regarding average explanation sizes: robust masking accounted for $12.99\%$, probabilistic masking with $\epsilon=0.12$ was $9.40\%$, probabilistic masking with $\epsilon:=1$ was $50.75\%$, and baseline masking was $50.8\%$. The average faithfulness was $89.77\%$ for baseline masking, $87.92\%$ for bounded-probabilistic masking, $90.43\%$ for robust masking, and only $15.95\%$ for unbounded-probabilistic masking. Results are presented in Figure \ref{fig:cifar_ablation_appendix}.

\begin{figure}[hbt!]
\centering
 %return this Shahaf
\subfloat{\includegraphics[height=20cm]{figures/cifar_ablation_appendix.png}}% \\
\caption{An ablation study comparing between different masking techniques for CIFAR-10} \label{fig:cifar_ablation_appendix}
\end{figure}

\subsection{IMAGENET}


Lastly, we present a comparative analysis of SST-based models and post-hoc approaches for IMAGENET. 
Since IMAGENET inputs are high-dimensional and individual pixels are too small to be discernible to the human eye, we patch each pixel with the surrounding $5\times 5$ pixels, centering the corresponding pixel for visualization purposes. This approach is applied in all visualizations of IMAGENET image results, including both SST and post-hoc methods.

\begin{figure}[hbt!]
\centering
 %return this Shahaf
\subfloat{\includegraphics[height=21cm]{figures/imagenet_posthoc_appendix.png}} \\
\caption{Examples of comparisons between explanations produced by \emph{SST} compared to post-hoc approaches for IMAGENET} \label{fig:imagenet_posthoc_appendix}
\end{figure}

In this segment, we also perform an ablation study to evaluate the performance of IMAGENET using different masking techniques. We train IMAGENET using either robust masking through a PGD $\ell{\infty}$ attack within an $\epsilon:=0.12$ ball, taking $10$ steps each of size $\alpha':=10^{-2}$, or probabilistic masking by sampling features uniformly from the same $\epsilon:=0.12$ ball around $\x$. The average explanation size was $0.07\%$ for probabilistic masking (compared to $0.46\%$ for robust masking), with a faithfulness score of $80.05\%$ (compared to $80.88\%$ for robust masking).


%\subsubsection{Text}
%Comparison of the SST results for post-hoc methods for IMDB and SBLI datasets.


%\subsection{Ablations}
%In this section we will present our method cross multiple options for the completion input. i.e., the value of the non-sufficient input features. We use a Baseline (zero-value), Robust (adversarial features), and Uniform (random values from uniform distribution between $+-\epsilon$ from the real feature value. We test the Uniform method with $\epsilon=0.12$ (low epsilon) and with $\epsilon=1$ (high epsilon).
%Samples of the comparison for CIFAR dataset are presented in Figure \ref{fig:cifar_ablation_appendix}.

%Ablations for the IMAGENET

%Ablation for IMDB dataset, can be found in Figure \ref{fig:imdb_ablation_appendix}


\begin{figure}[hbt!]
\centering
% return this Shahaf
\subfloat{\includegraphics[height=20cm]{figures/imagenet_ablation_appendix.png}} \\
\caption{An ablation study comparing different masking techniques for IMAGENET} \label{fig:imagenet_ablation_appendix}
\end{figure}


%\begin{figure}
%\centering
%\subfloat{\includegraphics[height=20cm]{figures/imagenet_ablation_appendix.png}} \\
%\caption{Examples of comparisons between different completion approaches for IMAGENET} \label{fig:imagenet_ablation_appendix}
%\end{figure}

\subsection{IMDB}

We now offer more examples of sufficient reasons produced by SST-based models, beginning with cases of IMDB sentiment analysis. As noted in the main paper, subsets derived via probabilistic masking tend to be larger compared to those from baseline masking. This difference arises because the randomness of perturbing various tokens imposes a stricter constraint than simply applying a fixed MASK token. Figure~\ref{fig:imdb_appendix} provides further comparisons between explanations obtained through probabilistic and baseline masking methods.

\begin{figure}[hbt!]
  \centering
    \includegraphics[width=1.0\linewidth]{figures/imdb_5.png}
    \caption{IMDB sentiment analysis sufficient reasons that were inherently generated using SST (baseline vs. probabilistic)}
  \label{fig:imdb_appendix}
  \end{figure}



%\end{figure}
\newpage
\subsection{SNLI}
Here, we illustrate examples of sufficient reasons generated by SST-based models for the SNLI classification tasks. In this task, the input contains both a \emph{premise} and a \emph{hypothesis} and the classification task is to determine the nature of their relationship, categorized as follows:
\begin{inparaenum}[(i)]
\item \emph{entailment}, where the hypothesis is a logical consequence of the premise;
\item \emph{contradiction}, where the hypothesis is logically inconsistent with the premise; and
\item \emph{neutral}, where the hypothesis neither logically follows from nor contradicts the premise.
\end{inparaenum}

\begin{figure}[hbt!]
  \centering
    \includegraphics[width=1.0\linewidth]{figures/snli_5_new.png}
    \caption{Sufficient reasons that were inherently generated using SST for the SNLI benchmark}
    \label{Fig2a}
  \end{figure}






% \subsection{MNIST}

% \textbf{Comparison to post-hoc methods.} We start by presenting additional examples of images comparing the results of SST to those of the aforementioned post-hoc methods: Anchors~\cite{ribeiro2018anchors},  \emph{gradient search} (\emph{GS})~\cite{fong2017interpretable}, and SIS~\cite{carter2019made}.

% \subsection{Post-Hoc Methods}


% \begin{figure}
% \centering
% \subfloat{\includegraphics[height=10cm]{figures/mnist_posthoc_appendix.png}} \\
% % \subfloat{\includegraphics[height=2.5cm]{figures/mnist_appendix_2.png}}
% \caption{Examples of comparisons between explanations produced by \emph{SST} compared to post-hoc approaches for MNIST}
% \label{plot_mnist_images}
% \end{figure}

% \textbf{Ablation results.} We start by presenting additional examples of images comparing the results of SST to those of the aforementioned post-hoc methods: Anchors~\cite{ribeiro2018anchors}, SIS~\cite{carter2019made}, and \emph{gradient search} (\emph{GS})~\cite{fong2017interpretable}.

% \begin{figure}
% \centering
% \subfloat{\includegraphics[height=2.5cm]{figures/mnist_appendix_1.png}} \\
% \subfloat{\includegraphics[height=2.5cm]{figures/mnist_appendix_2.png}}
% \caption{Examples of comparisons between explanations produced by \emph{SST} compared to post-hoc approaches for MNIST, CIFAR10, and IMAGENET.}
% \label{plot_images_vision}
% \end{figure}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}



\end{document}
