\section{Related Works}
\label{sec:related_work}

\noindent
\textbf{Single-modality 3D Object Detection.}
% Camera-based 3D object detection has seen significant developments, categorized into image-view-based \textbf{Long, "Image-View-Based Single-Modality 3D Object Detection"} and bird-eye-view (BEV)-based approaches \textbf{Wang, "Bird-Eye View Based 3D Object Detection"}.
% Recent advancements, including DETR3D and PETR, employ transformer models to convert 2D features into 3D space, with BEVDet and BEVDepth predicting depth distributions to map image features to a 3D frustum meshgrid \textbf{Wang, "Transformer-Based Single-Modality 3D Object Detection"}.
Given that cameras offer significant cost advantages over LiDAR sensors, many researchers have focused on developing methods that leverage camera-based systems for 3D object detection using image-only inputs \textbf{Tateno, "Image-Based 3D Object Detection Using Camera Systems"}.
% In image-based 3D object detection, the absence of direct depth information poses a challenge. To address this, several studies \textbf{Xu, "Depth Estimation for Image-Based 3D Object Detection"} have employed depth estimation techniques to create pseudo 3D point cloud representations or lift 2D features into the 3D space for subsequent object detection.
% Recently, transformer-based architectures have been introduced to exploit 3D object queries and 3D-2D correspondences in the detection process \textbf{Wang, "Transformer-Based Single-Modality 3D Object Detection"}.
Image-based 3D object detection faces challenges due to the lack of direct depth information. To overcome this challenge, several studies \textbf{Tateno, "Image-Based 3D Object Detection Using Camera Systems"} have employed depth estimation techniques to generate pseudo 3D point clouds or elevate 2D features into 3D space for object detection, alongside proposals for transformer-based architectures \textbf{Wang, "Transformer-Based Single-Modality 3D Object Detection"} that utilize 3D object queries and 3D-2D correspondences. Nonetheless, accurately estimating depth from images remains difficult, resulting in image-based methods performing worse than LiDAR-based approaches.


% Despite these advancements, the information provided by a single modality remains constrained, impacting overall detection performance.


LiDAR-based 3D object detection can reliably estimate 3D bounding boxes using point clouds captured from LiDAR sensors. Current LiDAR-based detection methods can be divided into three categories based on different point cloud encoding formats: point-based methods \textbf{Chen, "Point-Based LiDAR Object Detection"} , voxel-based methods \textbf{Tang, "Voxel-Based LiDAR Object Detection"} , and point-voxel fusion networks \textbf{Liu, "Point-Voxel Fusion for LiDAR Object Detection"} . While LiDAR detection is advantageous in various conditions, it often struggles in areas with sparse LiDAR data. Therefore, integrating the geometric benefits of point clouds with the semantic richness of images remains crucial for improving performance.

\begin{figure*}[t]
\begin{center}
\vspace{-2pt}
\includegraphics[width=.95\linewidth]{figures/overall_framework_v7.pdf}
\vspace{-7pt}
\caption{The overall pipeline for event-based 3D object detection (Ev-3DOD). (a): At active timestamp 0, LiDAR and image data are available. Therefore, we utilize an RGB-LiDAR Region Proposal Network (RPN) to extract voxel features and 3D bounding boxes at the active timestamp. (b): To predict the bounding box during blind time, $0\leq t<1$, we estimate the 3D motion and confidence score using event features. For computational efficiency, we design the process to compute (a) only once before the next active timestamp while performing iterative computations solely for (b).
}
\label{fig:overall_method}
\end{center}
\vspace{-10pt}
\end{figure*}
% For this purpose, we propose Virtual 3D Event Fusion (V3D-EF), which projects and fuses event data into 3D space.

\noindent
\textbf{Multi-modality 3D Object Detection.} Multi-modal sensor fusion \textbf{Li, "Multimodal Sensor Fusion for 3D Object Detection"} , where different sensors complement each other, typically utilizes a combination of cameras and LiDAR. Multimodal 3D object detection has gained attention for improving accuracy over unimodal methods. Fusion approaches are categorized into early \textbf{Kim, "Early Fusion of Camera and LiDAR Data"} , middle \textbf{Tang, "Middle Fusion of Camera and LiDAR Data"} , and late fusions \textbf{Liu, "Late Fusion of Camera and LiDAR Data"} , with middle fusion now preferred due to its robustness to calibration errors and enhanced feature interaction. Cross-modal fusion has achieved significant performance improvements by enhancing the sparse features of point clouds with semantic information from images. However, there are still unexplored areas concerning safety in autonomous driving. LiDAR and cameras have limited time resolution (\ie,~10-20 Hz) due to the relatively high bandwidths. To enhance safety, algorithms that operate at high speeds are essential. To address this, we introduce an asynchronous event camera with high temporal resolution for 3D object detection for the first time. The proposed method performs multi-modal fusion between synchronized sensors and asynchronous events to increase the operational frequency of 3D detection.

% cross-modal fusion은 point cloud의 부족한 sparse한 feature를 이미지의 semantic으로 부터 enhance할 수 있어 큰 성능 향상을 달성하였지만, 자율주행의 safety 측면에서 아직 덜 탐구된 영역이 있다. 라이다와 카메라는 상대적으로 높은 bandwidth로 time resolution이 높지 않으며, 안전성을 높이려면 고속도로 작동하는 알고리즘이 요구된다. 이를 위해, 우리는 처음으로 high temporal resolution을 가지는 asynchronous한 event camera를 3D object detection에 도입하여, 다른 센서와 multi-modal fusion을 하는 방식을 제안하여 모델 작동의 frequency를 향상한다.



\noindent
\textbf{Event-based Object Detection.} Event cameras break through the limitations of bandwidth requirements and perceptual latency with the trade-off of sparse data and loss of intensity information. To make the most of these characteristics, prior work has designed models to adapt to event data using graph neural networks \textbf{Chen, "Graph Neural Networks for Event-Based Object Detection"} , spiking neural networks \textbf{Kim, "Spiking Neural Networks for Event-Based Object Detection"} , and sparse neural networks \textbf{Tang, "Sparse Neural Networks for Event-Based Object Detection"} . Recently, effective dense neural networks \textbf{Li, "Dense Neural Networks for Event-Based Object Detection"} and transformer architectures \textbf{Wang, "Transformer Architectures for Event-Based Object Detection"} have been developed to leverage event cameras, aiming to achieve both low latency and high performance. These advances enable real-time operation with impressive accuracy, demonstrating the practical applicability of event cameras in real-world scenarios. To the best of our knowledge, we are the first to extend event-based object detection from 2D to 3D space. By employing a multi-modal fusion approach of the event camera and 3D sensors (\ie,~LiDAR), we aim to overcome the latency and persistent interval time barriers of 3D object detections.


% 우리가 알기로, 우리는 처음으로 이러한 2D 상에서 event-based object detection을 3D space로 확장하고자 한다. 우리는 라이다와 같은 3D 센서와 함께 멀티 모달 퓨전 접근을 통해, 기존 3D object detection의 latency와 고질적인 interval time 장벽을 부수고자 한다.