\section{Event-based 3D Object Detection Datasets}

This study is the first to explore the utilize event camera for 3D detection. To validate our model's effectiveness, we introduce two event 3D detection datasets: Ev-Waymo, which consists of synthetic events, and DSEC-3DOD with real events. We first obtained accurate 3D bounding box annotations for the active time, when both LiDAR and image data are available, and then processed the blind time.





%본 연구는 event camera를 3d detection에 사용하는 첫 번째 시도임. 따라서 우리는 두 개의 multi-modality event 3d detection dataset을 introduce 하여, 우리 모델의 effectiveness를 증명함. Interframe interval에서의 3d detection을 실험하기 위해, 라이다와 이미지가 부재한 시간에서의 3D box annotation을 제작하였다. 

\subsection{Annotation for Active Time at 10 FPS}

% \subsection{Event Waymo Open Dataset}
\noindent
\textbf{Ev-Waymo Dataset} is a synthetic event dataset based on the Waymo Open Dataset (WOD)~\cite{Sun2019ScalabilityIP}. WOD provides sequential pairs of synchronized LiDAR, image data, and 3D box annotations at 10 FPS. Event streams are generated using a widely-adopted event simulator~\cite{Gehrig_2020_CVPR}.
% We added annotations at blind time divided evenly into 10 points between consecutive frames, resulting in temporally dense annotations at 100 FPS. 
% Ev-Waymo evaluation uses the same metric as WOD. Average precision weighted by heading (APH) and average precision (AP) are calculated for 3 classes: vehicle, pedestrian, and cyclist. All 100 FPS GT boxes are used for evaluation. Also, objects are categorized into two difficulty levels depending on the number of LiDAR points inside the bounding box.
The Ev-Waymo evaluation follows the same metrics as WOD, calculating both average precision (AP) and average precision weighted by heading (APH).
% for three classes: vehicle, pedestrian, and cyclist. 
% Additionally, objects are classified into two difficulty levels (Level 1 (L1), Level 2 (L2)) based on the number of LiDAR points within each bounding box.
% Furthermore, objects are divided into two difficulty levels, Level 1 (L1) and Level 2 (L2), determined by the number of LiDAR points within each bounding box.
% DSEC setting과 동일하게 하기 위해, 우리는 front camera와 해당 field-of-view의 LiDAR를 crop하여 사용한다.
% To align with the DSEC-3DOD setting, Ev-waymo uses the front camera and crop the LiDAR to the corresponding field of view.
% To conform to the DSEC-3DOD setting, Ev-Waymo utilizes the front camera and crops the LiDAR to match the corresponding field of view.
Adhering to the DSEC-3DOD setting, Ev-Waymo employs the front camera and a cropped LiDAR aligned to the corresponding field of view.


% Event Waymo Open Dataset (Ev-WOD)는 Waymo Open Dataset 기반의 synthetic event 3d detection datset임. WOD는 10HZ의 싱크로나이즈된 라이다와 이미지, 그리고 3D box ground truth를 제공함. 우리는 연속된 프레임 사이 interval을 10개로 균일하게 분할한 시점에서의 annotation을 추가하였으며, 결과적으로 100fps의 temporally dense한 high quality annotaion을 제공함. 

%라이다와 이미지가 없는 시점에서 정확한 bounding box ground truth를 제공하기 위헤, state-of-the art video frame interpolation과 pointcloud interpolation 모델을 사용하여 interval time에서의 pseudo sensor data를 생성하였음. 10fps의 bounding box를 interpolation한 후, data annotation expert를 고용하여 pseudo sensor data를 기반으로 bounding box를 refine하여 높은 퀄리티의 100fps event 3d detection dataset을 제작함. Ev-Waymo dataset은 총 16k개의 labeled scene으로 이루어진 80개의 sequence로 이루어져있음.


\begin{figure}[t]
\begin{center}
\includegraphics[width=.99\linewidth]{figures/dsec_sample_v1.pdf}
\vspace{-6pt}
\caption{DSEC 3D Object Detection Dataset samples. Event data, label overlaid image, and accumulated LiDAR from left to right.}
\label{fig:data_sample}
\end{center}
\end{figure}


\setlength{\aboverulesep}{-1.5pt}
\setlength{\belowrulesep}{0pt}
\setlength{\tabcolsep}{2.4pt}
\renewcommand{\arraystretch}{1.1}
% \setlength{\extrarowheight}{.75ex}
\begin{table*}[t]
\begin{center}
\caption{Performance comparison on the Ev-Waymo dataset for 3D vehicle (IoU = 0.7), pedestrian (IoU = 0.5) and cyclist (IoU = 0.5) detection following previous protocols. Evaluated on 100 FPS ground-truth annotation for $t=\{0, 0.1, \ldots, 0.9 \}$. Offline represents results obtained by interpolating data using sensor data from active timestamps 0 and 1. Online represents results obtained using data accessible at present. Specifically, online evaluation of previous works uses only the data from active timestamp 0, while we also utilize event data from active timestamp 0 up to the present time.}
\vspace{-7pt}
\label{tab:main_waymo}
\resizebox{.99\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|cc|cc|cc}
\toprule
% \hline
% \hline
% & & \multicolumn{14}{c}{Target Sequences} \\
% \cline { 3 - 16 }
& \multirow{3}{*}{\thead{Interpolation\\Methods}} & \multirow{3}{*}{Methods} & \multirow{3}{*}{\thead{3D Detection\\Modality}} &  ALL & 
\multicolumn{2}{c|}{\multirow{2}{*}{VEH (AP/APH)}} & \multicolumn{2}{c|}{\multirow{2}{*}{PED (AP/APH)}} & \multicolumn{2}{c}{\multirow{2}{*}{CYC (AP/APH)}} \\
% \hline
&  &  &  &  (mAP/mAPH) & & & & & & \\
\cline{5-11}
 & &  &  & L2 & L1 & L2 & L1 & L2 & L1 & L2 \\
\hline
\multirow{6}{*}{Offline} & \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci}} & 
VoxelNeXt~\cite{chen2023voxelnext} & L & 53.61/50.67 & 62.96/62.51 & 59.10/58.67 & 68.43/60.77 & 62.85/55.69 & 44.55/43.15 & 38.88/37.65 \\
& & HEDNet~\cite{zhang2024hednet} & L & 50.52/47.51 & 59.57/59.09 &	55.90/55.45 & 63.28/55.32 & 58.03/50.62 & 43.14/41.80 & 37.63/36.46 \\
\cline{2-11}
& \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci} + EMA~\cite{zhang2023extracting}} & Focals Conv~\cite{chen2022focal} & L + I & 42.53/40.41 & 55.49/55.02 & 54.45/53.99 & 48.07/43.06 & 46.54/41.69 & 27.70/26.60 & 26.59/25.54 \\
& & LoGoNet~\cite{li2023logonet} & L + I & 53.29/50.49 & 63.48/63.05 & 60.15/59.73 & 67.85/60.28 & 62.43/55.37 & 42.40/41.37 & 37.29/36.38 \\
\cline{2-11}
& \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci} + CBM~\cite{kim2023event}} & Focals Conv~\cite{chen2022focal} & L + I & 42.51/40.39 & 55.48/55.01 & 54.44/53.98 & 48.07/43.06 & 46.54/41.68 & 27.67/26.57 &	26.56/25.50 \\
& & LoGoNet~\cite{li2023logonet} & L + I & 53.57/50.75 & 63.60/63.16 & 60.27/59.85 &	68.14/60.53 & 62.77/55.67 &	42.85/41.81 & 37.66/36.74 \\
\noalign{\vskip -0.6pt}
\hline
\hline
\noalign{\vskip 1pt}
\rowcolor{LG}
 &  & VoxelNeXt~\cite{chen2023voxelnext} & L & \underline{33.32}/31.70 & \underline{44.40}/\underline{44.10} & \underline{41.78}/\underline{41.49} &	\underline{40.52}/\underline{36.23} & \underline{36.93}/\underline{32.96} &	24.40/23.73 & 21.24/20.66 \\
 \rowcolor{LG}
 &  & HEDNet~\cite{zhang2024hednet} & L & 31.57/29.90 & 42.03/41.71 & 39.32/39.02 &	38.86/34.43 & 35.67/31.53 &	22.64/21.99 & 19.72/19.14 \\
\rowcolor{LG}
& & Focals Conv~\cite{chen2022focal} & L + I & 26.27/25.01 & 37.31/37.01 & 36.60/36.31 &	29.20/26.16 & 28.41/25.44 &	14.30/13.78 & 13.79/13.29  \\
\rowcolor{LG}
& & LoGoNet~\cite{li2023logonet} & L + I  & 33.27/\underline{31.75} & 44.14/43.87 & 41.73/41.47 &	39.98/35.84 & 36.48/32.67 &	\underline{24.71}/\underline{24.15} & \underline{21.59}/\underline{21.10}\\
\rowcolor{LG}
\multirow{-5}{*}{Online}& \multirow{-5}{*}{N/A} & Ev-3DOD (Ours) &  L + I + E &\textbf{48.06}/\textbf{45.60} & \textbf{60.30}/\textbf{59.95} & \textbf{59.19}/\textbf{58.85} & \textbf{57.40}/\textbf{50.78} & \textbf{55.30}/\textbf{48.93} & \textbf{31.08}/\textbf{30.38} & \textbf{29.69}/\textbf{29.03} \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-15pt}
\end{table*}





% \subsection{DSEC 3D Object Detection Dataset}
\noindent
\textbf{DSEC-3DOD.} 
The DSEC dataset~\cite{gehrig2021dsec} is a stereo event dataset that includes LiDAR, image, and real event data captured in challenging environments. We sampled a portion of the ``zurich\_city" sequence, providing  LiDAR and image data at 10 FPS. We hired annotation experts to label 3D bounding boxes at 10 FPS, ensuring alignment with the sensor data.
% From this dataset, we extracted LiDAR and image data at 10 FPS and hired annotation experts to label 10 FPS 3D bounding boxes aligned with the sensor data. 
% Ground-truth 3D bounding boxes were generated based on the accumulated LiDAR and image data.
% The ground-truth 3D bounding boxes were generated using accumulated LiDAR and image data.
% To achieve 100 FPS annotation, we interpolated the 10 FPS annotations to 100 FPS and refined them based on the results of event frame interpolation~\cite{Kim2023EventbasedVF} and point cloud interpolation~\cite{zheng2023neuralpci}. The DSEC 3D dataset comprises 178 scenes and includes 54K labeled scenes.
As shown in Fig.~\ref{fig:data_sample}, in the annotation process, we used accumulated LiDAR points to enable the precise generation of ground-truth 3D bounding boxes.
% DSEC 3D detection shares evaluation metrics with the Ev-Waymo dataset. However, it contains vehicle and pedestrian classes as the cyclist objects are insufficient. Also, all objects are considered as a single difficulty level. 
For evaluation on DSEC-3DOD, we used the same evaluation metrics as Ev-Waymo but include only vehicle and pedestrian classes due to a lack of cyclist data, with all objects categorized under a single difficulty level.

% DSEC dataset은 challeging한 환경에서 취득한 라이다, 이미지, 그리고 real 이벤트 데이터를 포함하고 있는 stereo event dataset임. "zurich_city" sequence 중 일부를 sampling하여 사용함. 우리는 데이터셋에서 10fps의 라이다와 이미지 데이터를 추출한 후, annotation 전문가를 고용하여 센서와 align된 10fps 3D bounding box를 annotation함. Accumulated LiDAR와 이미지 데이터를 기반으로 3d bounding box ground truth를 생성함.

%100fps annoatation을 위해서 10fps annotation을 100fps로 interpolation한 후, event frame interpolation과 pointcloud interpolation 결과를 기반으로 refine함. DSEC 3D dataset은 178개의 scene과 54k개의 label된 scene을 포함하고 있음.


\begin{figure*}[t]
\begin{center}
\includegraphics[width=.99\linewidth]{figures/waymo_qaul_v4.pdf}
\vspace{-5pt}
\caption{Qualitative comparisons of our method with other offline and online evaluations on the Ev-Waymo dataset. $t=0$ represents the active time, while $t = 0.2, 0.4, 0.6, 0.8$ denotes the blind times. The \textcolor{blue}{blue} box represents the ground truth, while the \textcolor{red}{red} box shows the prediction results of each method. For easier understanding, images at active timestamps 0 and 1 are overlaid.}
\label{fig:waymo_qual}
\end{center}
\vspace{-13pt}
\end{figure*}

% \subsection{Generation of Annotations for Blind Time}
\subsection{Annotation for Blind Time at 100 FPS}

For training and evaluation on 3D detection during blind time, we created 3D box annotations for periods when LiDAR and images were absent.
% We added annotations at blind times evenly divided into 10 intervals between consecutive frames, resulting in temporally dense annotations at 100 FPS.
% For 100 FPS annotation, we interpolated the original 10 FPS annotations to 100 FPS
% We linearly interpolated annotations at blind times, evenly dividing them into 10 intervals between consecutive frames, resulting in temporally dense annotations at 100 FPS. 
We linearly interpolated the original 10 FPS annotations, dividing blind times into 10 evenly spaced intervals between consecutive frames, resulting in temporally dense annotations at 100 FPS. To ensure accurate bounding box ground-truths in instances where LiDAR and image data are unavailable, we employed event-based video frame interpolation~\cite{kim2023event} and point cloud interpolation~\cite{zheng2023neuralpci} methods to generate sensor data for blind time. Data annotation experts refined interpolated bounding boxes based on the generated sensor data. The Ev-Waymo dataset consists of 80 sequences comprising a total of 157K labeled scenes, providing 100 FPS detection ground-truth. The DSEC 3D dataset consists of 178 sequences and includes 54K labeled scenes. For evaluation in blind time, we utilize all ground truth boxes at 100 FPS. More details of the datasets are described in the supplementary material.

% , refining them using event frame interpolation~\cite{Kim2023EventbasedVF} and point cloud interpolation~\cite{zheng2023neuralpci}. 



 