
% \setlength{\aboverulesep}{-1.5pt}
% \setlength{\belowrulesep}{0pt}
% \setlength{\tabcolsep}{4.0pt}
% \renewcommand{\arraystretch}{1.2}
% % \setlength{\extrarowheight}{.75ex}
% \begin{table*}[t]
% \begin{center}
% \caption{Performance comparison on the Ev-Waymo dataset for 3D vehicle (IoU = 0.7), pedestrian (IoU = 0.5) and cyclist (IoU = 0.5) detection.}
% \vspace{-10pt}
% \label{tab:main_continuous_result}
% \resizebox{.99\linewidth}{!}{
% \begin{tabular}{c|c|c|c|cc|cc|cc}
% \toprule
% % \hline
% % \hline
% % & & \multicolumn{14}{c}{Target Sequences} \\
% % \cline { 3 - 16 }
% & \multirow{2}{*}{Methods} & \multirow{2}{*}{Modality} & ALL (mAPH) & \multicolumn{2}{c|}{VEH (AP/APH) } & \multicolumn{2}{c|}{PED (AP/APH)} & \multicolumn{2}{c}{CYC (AP/APH)} \\
% \cline{4-10}
%  &  &  &  L2 & L1 & L2 & L1 & L2 & L1 & L2 \\
% \hline
% \multirow{3}{*}{Offline} & VoxelNeXt~\cite{chen2023voxelnext} & L \\
% & Focals Conv~\cite{chen2022focal} & L + I\\
% & LoGoNet~\cite{li2023logonet} & L + I \\
% & Focals Conv~\cite{chen2022focal} & L + I + E\\
% & LoGoNet~\cite{li2023logonet} & L + I +E \\
% \noalign{\vskip -0.6pt}
% \hline
% \hline
% \noalign{\vskip 1pt}
% \rowcolor{LG}
%  & VoxelNeXt~\cite{chen2023voxelnext} & L \\
% \rowcolor{LG}
% & Focals Conv~\cite{chen2022focal} & L + I & 24.64 &  36.60/36.27 & 34.23/33.92 & 27.72/24.32 & 25.00/21.91 & 19.74/18.84 & 18.97/18.10  \\
% \rowcolor{LG}
% & LoGoNet~\cite{li2023logonet} & L + I & 32.60 & 45.88/45.54 & 42.44/42.12 & 42.16/37.39 & 37.18/32.96 & 24.59/23.77 & 23.53/22.73\\
% \rowcolor{LG}
% \multirow{-4}{*}{Online}& Ev-3DOD (Ours) &  L + I + E\\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-17pt}
% \end{table*}

% \setlength{\aboverulesep}{-1.5pt}
% \setlength{\belowrulesep}{0pt}
% \setlength{\tabcolsep}{3.6pt}
% \renewcommand{\arraystretch}{1.2}
% % \setlength{\extrarowheight}{.75ex}
% \begin{table*}[t]
% \begin{center}
% \caption{Performance comparison on the Ev-Waymo dataset for 3D vehicle (IoU = 0.7), pedestrian (IoU = 0.5) and cyclist (IoU = 0.5) detection following previous protocols.}
% \vspace{-7pt}
% \label{tab:main_continuous_result}
% \resizebox{.99\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|cc|cc|cc}
% \toprule
% % \hline
% % \hline
% % & & \multicolumn{14}{c}{Target Sequences} \\
% % \cline { 3 - 16 }
% & Interpolation & \multirow{2}{*}{Methods} & 3D Detection &  ALL (mAPH) & \multicolumn{2}{c|}{VEH (AP/APH) } & \multicolumn{2}{c|}{PED (AP/APH)} & \multicolumn{2}{c}{CYC (AP/APH)} \\
% \cline{5-11}
%  & Methods &  & Modality & L2 & L1 & L2 & L1 & L2 & L1 & L2 \\
% \hline
% \multirow{5}{*}{Offline} & N-PCI~\cite{zheng2023neuralpci} & 
% VoxelNeXt~\cite{chen2023voxelnext} & L \\
% \cline{2-11}
% & N-PCI~\cite{zheng2023neuralpci}  & Focals Conv~\cite{chen2022focal} & L + I\\
% & + EMA~\cite{zhang2023extracting} & LoGoNet~\cite{li2023logonet} & L + I \\
% \cline{2-11}
% & N-PCI~\cite{zheng2023neuralpci} & Focals Conv~\cite{chen2022focal} & L + I\\
% & + CBM~\cite{kim2023event} & LoGoNet~\cite{li2023logonet} & L + I \\
% \noalign{\vskip -0.6pt}
% \hline
% \hline
% \noalign{\vskip 1pt}
% \rowcolor{LG}
%  &  & VoxelNeXt~\cite{chen2023voxelnext} & L \\
% \rowcolor{LG}
% & & Focals Conv~\cite{chen2022focal} & L + I & 24.64 &  36.60/36.27 & 34.23/33.92 & 27.72/24.32 & 25.00/21.91 & 19.74/18.84 & 18.97/18.10  \\
% \rowcolor{LG}
% & & LoGoNet~\cite{li2023logonet} & L + I  & 32.60 & 45.88/45.54 & 42.44/42.12 & 42.16/37.39 & 37.18/32.96 & 24.59/23.77 & 23.53/22.73\\
% \rowcolor{LG}
% \multirow{-4}{*}{Online}& \multirow{-4}{*}{N/A} & Ev-3DOD (Ours) &  L + I + E\\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-17pt}
% \end{table*}

% \setlength{\aboverulesep}{-1.5pt}
% \setlength{\belowrulesep}{0pt}
% \setlength{\tabcolsep}{2.8pt}
% \renewcommand{\arraystretch}{1.2}
% % \setlength{\extrarowheight}{.75ex}
% \begin{table*}[t]
% \begin{center}
% \caption{Performance comparison on the Ev-Waymo dataset for 3D vehicle (IoU = 0.7), pedestrian (IoU = 0.5) and cyclist (IoU = 0.5) detection following previous protocols.}
% \vspace{-7pt}
% \label{tab:main_continuous_result}
% \resizebox{.99\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|cc|cc|cc}
% \toprule
% % \hline
% % \hline
% % & & \multicolumn{14}{c}{Target Sequences} \\
% % \cline { 3 - 16 }
% & Interpolation & \multirow{2}{*}{Methods} & 3D Detection &  ALL (mAP/mAPH) & \multicolumn{2}{c|}{VEH (AP/APH) } & \multicolumn{2}{c|}{PED (AP/APH)} & \multicolumn{2}{c}{CYC (AP/APH)} \\
% \cline{5-11}
%  & Methods &  & Modality & L2 & L1 & L2 & L1 & L2 & L1 & L2 \\
% \hline
% \multirow{6}{*}{Offline} & \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci}} & 
% VoxelNeXt~\cite{chen2023voxelnext} & L & 53.61/50.67 & 62.96/62.51 & 59.10/58.67 & 68.43/60.77 & 62.85/55.69 & 44.55/43.15 & 38.88/37.65 \\
% & & HEDNet~\cite{zhang2024hednet} & L & 50.52/47.51 & 59.57/59.09 &	55.90/55.45 & 63.28/55.32 & 58.03/50.62 & 43.14/41.80 & 37.63/36.46 \\
% \cline{2-11}
% & \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci} + EMA~\cite{zhang2023extracting}} & Focals Conv~\cite{chen2022focal} & L + I & 42.53/40.41 & 55.49/55.02 & 54.45/53.99 & 48.07/43.06 & 46.54/41.69 & 27.70/26.60 & 26.59/25.54 \\
% & & LoGoNet~\cite{li2023logonet} & L + I & 53.29/50.49 & 63.48/63.05 & 60.15/59.73 & 67.85/60.28 & 62.43/55.37 & 42.40/41.37 & 37.29/36.38 \\
% \cline{2-11}
% & \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci} + CBM~\cite{kim2023event}} & Focals Conv~\cite{chen2022focal} & L + I & 42.51/40.39 & 55.48/55.01 & 54.44/53.98 & 48.07/43.06 & 46.54/41.68 & 27.67/26.57 &	26.56/25.50 \\
% & & LoGoNet~\cite{li2023logonet} & L + I & 53.57/50.75 & 63.60/63.16 & 60.27/59.85 &	68.14/60.53 & 62.77/55.67 &	42.85/41.81 & 37.66/36.74 \\
% \noalign{\vskip -0.6pt}
% \hline
% \hline
% \noalign{\vskip 1pt}
% \rowcolor{LG}
%  &  & VoxelNeXt~\cite{chen2023voxelnext} & L & \underline{33.32}/31.70 & \underline{44.40}/\underline{44.10} & \underline{41.78}/\underline{41.49} &	\underline{40.52}/\underline{36.23} & \underline{36.93}/\underline{32.96} &	24.40/23.73 & 21.24/20.66 \\
%  \rowcolor{LG}
%  &  & HEDNet~\cite{zhang2024hednet} & L & 31.57/29.90 & 42.03/41.71 & 39.32/39.02 &	38.86/34.43 & 35.67/31.53 &	22.64/21.99 & 19.72/19.14 \\
% \rowcolor{LG}
% & & Focals Conv~\cite{chen2022focal} & L + I & 26.27/25.01 & 37.31/37.01 & 36.60/36.31 &	29.20/26.16 & 28.41/25.44 &	14.30/13.78 & 13.79/13.29  \\
% \rowcolor{LG}
% & & LoGoNet~\cite{li2023logonet} & L + I  & 33.27/\underline{31.75} & 44.14/43.87 & 41.73/41.47 &	39.98/35.84 & 36.48/32.67 &	\underline{24.71}/\underline{24.15} & \underline{21.59}/\underline{21.10}\\
% \rowcolor{LG}
% \multirow{-5}{*}{Online}& \multirow{-5}{*}{N/A} & Ev-3DOD (Ours) &  L + I + E &\textbf{48.06}/\textbf{45.60} & \textbf{60.30}/\textbf{59.95} & \textbf{59.19}/\textbf{58.85} & \textbf{57.40}/\textbf{50.78} & \textbf{55.30}/\textbf{48.93} & \textbf{31.08}/\textbf{30.38} & \textbf{29.69}/\textbf{29.03} \\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-17pt}
% \end{table*}



\begin{table*}[t]
% \setlength{\aboverulesep}{-1.5pt}
% \setlength{\belowrulesep}{0pt}
\setlength{\tabcolsep}{12.0pt}
\renewcommand{\arraystretch}{1.0}
\begin{center}
\caption{Results on the DSEC-3DOD dataset for 3D vehicle (IoU = 0.7), and pedestrian (IoU = 0.5) detection.}
\vspace{-8pt}
\label{tab:main_dsec}
\resizebox{.99\linewidth}{!}{
\begin{tabular}{c|c|c|c|cc|cc|cc}
\toprule
% \hline
% \hline
% & & \multicolumn{14}{c}{Target Sequences} \\
% \cline { 3 - 16 }
& Interpolation & \multirow{2}{*}{Methods} & 3D Detection &  \multicolumn{2}{c|}{ALL} & \multicolumn{2}{c|}{VEH} & \multicolumn{2}{c}{PED} \\
\cline{5-10}
 & Methods &  & Modality & mAP & mAPH & AP & APH & AP & APH \\
\hline
\multirow{6}{*}{Offline} & \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci}} & 
VoxelNeXt~\cite{chen2023voxelnext} & L & 36.29&	32.43&	45.35&	44.93&	27.23&	19.93\\
& & HEDNet~\cite{zhang2024hednet} & L &37.73&	30.73&	41.82&	41.35&	33.64&	20.1\\
\cline{2-10}
& \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci} + EMA~\cite{zhang2023extracting}} & Focals Conv~\cite{chen2022focal} & L + I & 30.29&	25.85&	35.52&	35.04&	25.05&	16.65\\
& & LoGoNet~\cite{li2023logonet} & L + I & 37.30&	31.39&	44.64&	44.06&	29.96&	18.72\\
\cline{2-10}
& \multirow{2}{*}{N-PCI~\cite{zheng2023neuralpci} + CBM~\cite{kim2023event}} & Focals Conv~\cite{chen2022focal} & L + I & 30.28&	25.86&	35.56&	35.08&	24.99&	16.64\\
& & LoGoNet~\cite{li2023logonet} & L + I &37.35&	31.45&	44.76&	44.17&	29.94&	18.73 \\
\noalign{\vskip -0.6pt}
\hline
\hline
\noalign{\vskip 1pt}
\rowcolor{LG}
 &  & VoxelNeXt~\cite{chen2023voxelnext} & L & \underline{13.94}&	 \underline{12.90} &   \underline{21.42} &  \underline{21.23} &	6.46&	4.57 \\
 \rowcolor{LG}
 &  & HEDNet~\cite{zhang2024hednet} & L & 13.75&	11.68&	18.48&	18.29&	\underline{9.02} &	 \underline{5.07} \\
\rowcolor{LG}
& & Focals Conv~\cite{chen2022focal} & L + I & 10.81&	9.61&	15.74&	15.55&	5.88&	3.67 \\
\rowcolor{LG}
& & LoGoNet~\cite{li2023logonet} & L + I  & 13.29&	11.91&	19.50&	19.32&	7.07&	4.49\\
\rowcolor{LG}
\multirow{-5}{*}{Online}& \multirow{-5}{*}{N/A} & Ev-3DOD (Ours) &  L + I + E & \textbf{31.17}  & \textbf{26.54} & \textbf{41.65} & \textbf{41.20} & \textbf{20.68} & \textbf{11.88}\\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-14pt}
\end{table*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=.92\linewidth]{figures/dsec_qual_v4.pdf}
\vspace{-11pt}
\caption{Qualitative comparisons of our method with other offline and online evaluations on the DSEC-3DOD dataset. $t=0$ represents the active time, while $t = 0.2, 0.4, 0.6, 0.8$ denotes the blind times. The \textcolor{blue}{blue} box represents the ground truth, while the \textcolor{red}{red} box shows the prediction results of each method. For easier understanding, images at active timestamps 0 and 1 are overlaid.}
\label{fig:dsec_qual}
\end{center}
\vspace{-19pt}
\end{figure*}

\section{Experiments}





% \subsection{Datasets}
% \noindent
% \textbf{Ev-Waymo.}


% \noindent
% \textbf{DSEC-3DOD.}

\subsection{Experiment Setup} % Setup?

% \noindent
% \textbf{Evaluation Setup.}

We target causal 3D detections to detect objects during blind time.
% Although direct comparison is not feasible, 
For comparison, we present two types of evaluation methods for conventional models. First is the online evaluation, where the network is inferred at any given blind time using only accessible data. Conventional methods can only process the most recent sensor data at active timestamp 0. In contrast, our approach additionally incorporates event data from active timestamp 0 up to the current blind time.
% to this end, 우리는 두 종류의 평가 방식을 present 한다. 첫번째로, online result로, straight forward하게 임의의 blind 시점에서 accesible한 데이터를 이용하여 네트워크가 inference하여 평가하는 것이다. 기존 방식의 경우, 가장 최근의 active timestmap 0의 센서 데이터만 사용이 가능하고, 우리의 방식은 추가적으로 active timestamp 0에서 현재 blind 시점까지의 이벤트 데이터도 함께 사용이 가능하다. 
% 다른 하나는, 기존 방식의 경우 fixed frame rate를 가지고 있어, direct하게 apply하여 blind time에 적용하면 어려움을 겪기 때문에, future data를 함께 사용하여 추론 및 평가하는 방식이다. In other words, data for the blind time is interpolated using active time stamps 0 and 1, which is the future data.
Second, offline evaluation addresses the challenge of applying conventional fixed-frame-rate approaches directly to blind time by utilizing future data. In other words, data for the blind time is interpolated using active timestamps 0 and 1, where timestamp 1 represents future data. Consequently, these offline evaluations serve as a performance oracle for online methods. More implementation details are provided in the supplementary material.


% Existing methods based on fixed frame rate sensors face difficulties in directly applying detection to these blind times. 

% To this end, we present offline evaluation results using future data as a performance reference. 

% For offline results, 



% 우리는 라이다와 이미지가 존재하지 않은 blind time에서 event data를 이용하여 3D detection이 가능함. 지금까지의 fixed frame rate sensor를 이용하는 방법론들은 blind time detection을 직접적으로 적용하기에 어려움이 있음. 비록 직접적으로 비교해야 하는 결과는 아니지만, 성능적인 참고를 위해 future 데이타를 사용한 offline evaluation 결과를 제시함. Offline 결과는 미래의 데이터를 이용하여 blind time의 pseudo data를 생성한 후 평가함. 따라서 offline evaluation들이 online 방법론의 성능적인 oracle임.


% \noindent
% \textbf{Implementation Details}

% Batch size 4.
% $S=6$ in ROI pooling
% event voxel grid bin size = 5




\subsection{Experimental Results}
% 우리는 기존 방법들과 비교하기 위해 최근 state-of-the-art 방법들인, camera-based와 camera-LiDAR 기반의 multi-modal approach들을 사용한다. 
We employ recent state-of-the-art approaches, including LiDAR-based~\cite{chen2023voxelnext, zhang2024hednet} and LiDAR-camera-based multi-modal methods~\cite{chen2022focal, li2023logonet}. For the offline image interpolation, we utilize frame-based, EMA~\cite{zhang2023extracting}, and the event-based method, CBM~\cite{kim2023event}.  NeuralPCI~\cite{zheng2023neuralpci} is adopted for point cloud interpolation.


\noindent
\textbf{Ev-Waymo.}
Table~\ref{tab:main_waymo} shows the quantitative result of Ev-Waymo. 
% Previous methods cannot detect new objects during blind time, resulting in reduced performance when evaluated with 100 FPS ground truth as they fail to capture scene changes. 
Comparing methods in the online setting, previous methods~\cite{li2023logonet, chen2022focal, zhang2024hednet, chen2023voxelnext}, which rely on fixed-frame-rate sensors (\ie,~LiDAR and Image), cannot detect object motion during blind times. This limitation leads to reduced performance when evaluated with 100 FPS ground-truth. In contrast, our approach, leverages high-temporal resolution of event data to estimate object motion during blind time, significantly outperforming existing online methods. Notably, Ev-3DOD is comparable to offline oracles and even surpasses the performance of a certain approach~\cite{chen2022focal}.

% Additionally, our method demonstrates comparable performance even when compared to offline oracle methods that use sensors interpolated with future data.

Figure~\ref{fig:waymo_qual} shows comparisons between the results of online, offline, and our method during blind time. Existing online methods fail to align prediction with the ground truth during blind time. In contrast, Ev-3DOD, despite being an online approach, aligns well with the ground-truth and demonstrates performance comparable to offline methods.



% The offline method, by interpolating sensor data, accurately predicts the actual object location during blind time. In contrast, e


% 이전 방법론들은 blind time에서 새로운 object를 detection 하는 것이 불가능하다. 따라서 100 FPS ground truth에서 평가를 했을 때 scene의 변화를 반영하지 못해 성능이 떨어진다. 반면, 우리 방법론은 이벤트 데이터를 이용하여 blind time에서 object motion을 추정하고 3D detection이 가능하며, 기존 online 방법론들을 큰 폭으로 outperform했다. 또한 미래의 데이터로 interpolation된 sensor를 사용한 offline oracle 방법론들과 비교했을 때도 comparable한 성능인 것을 확인할 수 있다.
% 아래 그림은 blind time에서 online method, offline method, 그리고 ours를 비교한 결과이다. 이해의 편의를 위해 0번과 1번 이미지를 겹쳐 표현하였다. 파란 박스는 ground truth, 빨간 박스는 prediction 결과이다. Offline 방법은 sensor data를 interpolation 하여 blind time에서 실제 물체 위치를 잘 prediction 하였다. 반면 기존 online 방법론은 active timestamp의 데이터만을 사용하여 blind time에서의 detection은 grount truth와 align 되어있지 않다. 



\noindent
\textbf{DSEC-3DOD.}
As shown in Table~\ref{tab:main_dsec}, we evaluate our model on the DSEC-3DOD real event dataset, which includes fast motion and challenging lighting conditions that significantly degrade performance during blind time. Our model effectively estimates the 3D motion of objects from events, achieving a substantial improvement over other online methods. Notably, it demonstrates performance comparable to offline oracle methods that utilize future information. The analysis of computational complexity is provided in the supplementary material.

Figure~\ref{fig:dsec_qual} shows blind time detection inference for fast-moving objects in the DSEC dataset. Under low-light conditions and rapid motion, the quality of the interpolated pseudo-sensor data may deteriorate, occasionally resulting in detection failures, even when future information is utilized. In contrast, the proposed Ev-3DOD accurately detects objects in blind time, predicting motion from events.



\begin{table}[t]
\begin{center}
\caption{Ablation study of the proposed components.}
% MCE denotes the motion confidence estimator (Sec.~\ref{sec:mce}).}
\label{tab:able_components}
\vspace{-6pt}
\renewcommand{\tabcolsep}{12pt}
% \renewcommand{\arraystretch}{0.9}
\resizebox{.72\linewidth}{!}{
\begin{tabular}{cc|cc}
\Xhline{2.5\arrayrulewidth}
\multirow{2}{*}{V3D-EF} & \multirow{2}{*}{MCE} & \multicolumn{2}{c}{L2} \\
& & mAP & mAPH \\
\hline 
 & & 33.30 & 31.68 \\
\darkgreencheck & & \underline{46.55} & \underline{44.22} \\
% &  \darkgreencheck &  &  \\
\darkgreencheck & \darkgreencheck & \textbf{48.06} & \textbf{45.60} \\
% \bottomrule
% \hline
% \hline
\Xhline{2.5\arrayrulewidth}
\end{tabular}
}
\end{center}
\vspace{-13pt}
\end{table}



\begin{table}[t]
\begin{center}
\caption{Effectiveness of non-empty mask in the V3D-EF module.}
\label{tab:able_mask}
\vspace{-8pt}
\resizebox{.72\linewidth}{!}
{
\renewcommand{\tabcolsep}{10pt}
\begin{tabular}{lcc}
\Xhline{2.5\arrayrulewidth}
\multirow{2}{*}{Method} & \multicolumn{2}{c}{L2} \\
& mAP & mAPH \\
\hline
w/o Non-empty Mask & 42.57 & 40.50\\
w/ Non-empty Mask & \textbf{46.55} & \textbf{44.22}\\
\Xhline{2.5\arrayrulewidth}
\end{tabular}
}
\end{center}
\vspace{-18pt}
\end{table}

% 우리 모델을 DSEC-3DOD real event 데이터셋에서 평가하였다. DSEC은 fast motion과 challenging한 lighting condition을 포함하고 있어서 blind time에서의 performance 하락이 크다. Our 모델은 event로부터 object의 3D motion을 잘 추정하여 높은 폭으로 SOTA 성능을 달성하였다. Future information을 사용한 offline inference method와 유사한 성능을 보이는 점이 notable하다. 
% DSEC에서 fast object에 대한 blind time detection inference 결과이다. Low light condition과 fast motion에 의해 interpolated된 pseudo sensor가 degrade될 경우, 미래의 정보를 사용함에도 detection을 fail하는 경우가 발생한다. 반면, proposed model은 motion을 정확하게 prediction하여 ground truth와 거의 비슷한 위치에 bounding box를 predict 한다.

% \setlength{\aboverulesep}{-1.5pt}
% \setlength{\belowrulesep}{0pt}
% \setlength{\tabcolsep}{4.0pt}
% \renewcommand{\arraystretch}{1.2}
% % \setlength{\extrarowheight}{.75ex}
% \begin{table*}[t]
% \begin{center}
% \caption{Performance comparison on the DSEC-3DOD dataset for 3D vehicle (IoU = 0.7), pedestrian (IoU = 0.5) and cyclist (IoU = 0.5) detection.}
% \vspace{-10pt}
% \label{tab:main_continuous_result}
% \resizebox{.99\linewidth}{!}{
% \begin{tabular}{c|c|c|c|cc|cc|cc|c}
% \toprule
% % \hline
% % \hline
% % & & \multicolumn{14}{c}{Target Sequences} \\
% % \cline { 3 - 16 }
% & \multirow{2}{*}{Methods} & \multirow{2}{*}{Modality} & ALL (mAPH) & \multicolumn{2}{c|}{VEH (AP/APH) } & \multicolumn{2}{c|}{PED (AP/APH)} & \multicolumn{2}{c|}{CYC (AP/APH)} & \multirow{2}{*}{FPS} \\
% \cline{4-10}
%  &  & &  L2 & L1 & L2 & L1 & L2 & L1 & L2 \\
% \hline
% \multirow{3}{*}{Offline} & VoxelNeXt~\cite{chen2023voxelnext} & L \\
% & Focals Conv~\cite{chen2022focal} & L + I\\
% & LoGoNet~\cite{li2023logonet} & L + I \\
% \noalign{\vskip -0.6pt}
% \hline
% \hline
% \noalign{\vskip 1pt}
% \rowcolor{LG}
%  & VoxelNeXt~\cite{chen2023voxelnext} & L \\
% \rowcolor{LG}
% & Focals Conv~\cite{chen2022focal} & L + I & 24.64 &  36.60/36.27 & 34.23/33.92 & 27.72/24.32 & 25.00/21.91 & 19.74/18.84 & 18.97/18.10  \\
% \rowcolor{LG}
% & LoGoNet~\cite{li2023logonet} & L + I & 32.60 & 45.88/45.54 & 42.44/42.12 & 42.16/37.39 & 37.18/32.96 & 24.59/23.77 & 23.53/22.73\\
% \rowcolor{LG}
% \multirow{-4}{*}{Online}& Ev-3DOD (Ours) &  L + I + E\\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-17pt}
% \end{table*}

% \setlength{\aboverulesep}{-1.5pt}
% \setlength{\belowrulesep}{0pt}
% \setlength{\tabcolsep}{4.0pt}
% \renewcommand{\arraystretch}{1.2}
% \setlength{\extrarowheight}{.75ex}
% \begin{table*}[t]
% \begin{center}
% \caption{Performance comparison on the DSEC-3DOD dataset for 3D vehicle (IoU = 0.7), pedestrian (IoU = 0.5) and cyclist (IoU = 0.5) detection.}
% \vspace{-10pt}
% \label{tab:main_continuous_result}
% \resizebox{.99\linewidth}{!}{
% \begin{tabular}{c|c|c|cc|cc|cc|cc|c}
% \toprule
% % \hline
% % \hline
% % & & \multicolumn{14}{c}{Target Sequences} \\
% % \cline { 3 - 16 }
% % & \multirow{2}{*}{Methods} & \multirow{2}{*}{Modality} & ALL (mAPH) & \multicolumn{2}{c|}{VEH (AP/APH) } & \multicolumn{2}{c|}{PED (AP/APH)} & \multicolumn{2}{c|}{CYC (AP/APH)} & \multirow{2}{*}{FPS} \\
% % \cline{4-10}
% & \multirow{2}{*}{Methods} & \multirow{2}{*}{Modality} & \multicolumn{2}{c|}{ALL} & \multicolumn{2}{c|}{VEH} & \multicolumn{2}{c|}{PED} & \multirow{2}{*}{FPS} \\
% \cline{4-9}
%  &  & &  mAP & mAPH & AP & APH & AP & APH \\
% \hline
% \multirow{3}{*}{Offline} 
% & VoxelNeXt~\cite{chen2023voxelnext} & L & 36.29 & 32.43 & 45.35 & 44.93 & 27.33 & 19.93 \\
% & HEDNet~\cite{zhang2024hednet} & L & 37.73 & 30.72 & 41.82 &	41.35 &	33.64 & 20.10 \\
% & Focals Conv~\cite{chen2022focal} & L + I & 30.29 & 25.85	& 35.52	& 35.04	& 25.05	& 16.65\\
% & LoGoNet~\cite{li2023logonet} & L + I &  31.39 & 44.60 & 44.06 & 29.96 & 18.72\\
% \noalign{\vskip -0.6pt}
% \hline
% \hline
% \noalign{\vskip 1pt}
% \rowcolor{LG}
%  & VoxelNeXt~\cite{chen2023voxelnext} & L & 12.9	& 21.42  & 21.23 & 6.46 & 4.57\\
% \rowcolor{LG}
% & HEDNet~\cite{zhang2024hednet} & L & 11.68	& 18.48 & 18.29 & 9.02 & 5.07\\
% \rowcolor{LG}
% & Focals Conv~\cite{chen2022focal} & L + I & 24.64 &  36.60/36.27 & 34.23/33.92 & 27.72/24.32 & 25.00/21.91 & 19.74/18.84 & 18.97/18.10  \\
% \rowcolor{LG}

% % & Focals Conv~\cite{chen2022focal} & L + I & 24.64 &  36.60/36.27 & 34.23/33.92 & 27.72/24.32 & 25.00/21.91 & 19.74/18.84 & 18.97/18.10  \\
% % \rowcolor{LG}
% & LoGoNet~\cite{li2023logonet} & L + I & 32.60 & 45.88/45.54 & 42.44/42.12 & 42.16/37.39 & 37.18/32.96 & 24.59/23.77 & 23.53/22.73\\
% \rowcolor{LG}
% \multirow{-4}{*}{Online}& Ev-3DOD (Ours) &  L + I + E\\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-17pt}
% \end{table*}





\subsection{Ablation Study and Analysis}
We present an ablation study on key components and various analyses on the Ev-Waymo dataset. In V3D-EF analyses, the motion confidence estimator (MCE) module is excluded for clear comparisons.






\noindent
\textbf{Effectiveness of Each Component.}
In Table~\ref{tab:able_components}, we conduct the ablation study on the key components of Ev-3DOD. We evaluated a baseline and progressively added modules to observe improvements. The motion confidence estimator is dependent on V3D-EF, so it was not evaluated separately. V3D-EF, which explicitly estimates motion, has a direct impact on performance, resulting in significant improvements of +13.25\% in mAP and +12.54\% in mAPH. Additionally, the motion confidence estimator refines prediction scores at each blind time, demonstrating its effectiveness with further gains of +1.51\% and +1.38\%, respectively.

\noindent
\textbf{Non-empty Mask in V3D-EF.}
In the proposed V3D-EF, we employ a non-empty mask to achieve accurate and efficient alignment between sparse events and voxel features. To demonstrate the effectiveness of this non-empty mask, we conduct a comparative experiment in which events are projected onto all voxels within the V3D-EF module. The results in Table~\ref{tab:able_mask} confirm that the non-empty mask enables effective alignment between the two modalities.

\noindent
\textbf{Variations of the V3D-EF module.}
Our core component, the V3D-EF module, offers several design choices. To analyze these, we designed various variants, presenting the results accordingly. Our core component, the V3D-EF module, provides multiple design options. To analyze these configurations, we developed and evaluated several variants, with the results summarized in Table~\ref{tab:able_variants}. We experimented with generating the implicit motion field using each modality, \ie,~LiDAR and event sensors, and found a performance drop compared to the multi-modal approach. This decline is likely due to the limitations of each modality: LiDAR alone cannot capture motion information during blind times, and event features cannot accurately estimate refined 3D motion. Additionally, multi-modal fusion before pooling decreased performance. We believe this is because fusing before pooling leads to integration between voxels and events outside of box proposal regions, resulting in the use of redundant and possibly misaligned information.


% Although we generated an implicit motion field based on a multi-modal approach using both LiDAR and event data, we conducted experiments by varying to a single modality.

\noindent
\textbf{Detection Performance per Elapsed Blind Time.} Figure~\ref{fig:frame_ablation} shows the detection performance over time elapsed since the active timestamp. Due to object motion, the performance of online detection deteriorates rapidly as the time distance from the active timestamp increases. 
In contrast, offline methods sustain robust performance throughout the blind time by leveraging future data, with only a slight performance drop at the center of the blind time as it moves further from the active timestamps located at both ends.
% , attributable to the limitations of interpolation. 
The proposed model, relying solely on event data, achieves a temporal degradation pattern comparable to that of the offline method.

% Object motion에 의해서 active timestamp에서 시간이 지날수록 online conventional detection 성능이 급격히 떨어짐. Offline은 future data를 사용해서 모든 blind time에 거쳐 좋은 성능을 유지했으며, interpolation의 nature에 의해 blind time의 중심에서 성능이 가장 낮음. Proposed model은 future data 없이도 event data만을 사용하여 offline method와 비슷한 정도의 성능 degradation이 있음을 확인함. 


\begin{table}[t]
\begin{center}
\caption{Comparison of results across different modalities and fusion variants of the V3D-EF module.}
\label{tab:able_variants}
\vspace{-8pt}
\setlength{\tabcolsep}{8.6pt}
\resizebox{.95\linewidth}{!}
{
% \renewcommand{\tabcolsep}{6pt}
\begin{tabular}{lcc}
\Xhline{2.5\arrayrulewidth}
\multirow{2}{*}{Method} & \multicolumn{2}{c}{L2} \\
& mAP & mAPH \\
\hline
Base Model & 33.30 & 31.68 \\
+ LiDAR & 37.04 & 35.25 \\
+ Event & 42.23 & 40.17 \\
+ LiDAR + Event (fusion before pooling) & \underline{43.95} & \underline{41.87} \\
+ LiDAR + Event (fusion after pooling) & \textbf{46.55} & \textbf{44.22}\\
\Xhline{2.5\arrayrulewidth}
\end{tabular}
}
\end{center}
\vspace{-18pt}
\end{table}


\begin{figure}[t]
\begin{center}
\includegraphics[width=.96\linewidth]{figures/per_frame_ablation_v4.pdf}
\vspace{-10pt}
\caption{Detection performance over time elapsed since the active timestamp. We compare ours with offline and online approaches.}
\label{fig:frame_ablation}
\end{center}
\vspace{-20pt}
\end{figure}

% \setlength{\aboverulesep}{-1.5pt}
% \setlength{\belowrulesep}{0pt}
% \setlength{\tabcolsep}{4.0pt}
% \renewcommand{\arraystretch}{1.2}
% % \setlength{\extrarowheight}{.75ex}
% \begin{table*}[t]
% \begin{center}
% \caption{Performance comparison on the Ev-Waymo dataset for 3D vehicle (IoU = 0.7), pedestrian (IoU = 0.5) and cyclist (IoU = 0.5) detection.}
% \vspace{-10pt}
% \label{tab:main_continuous_result}
% \resizebox{.99\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|cc|cc|cc}
% \toprule
% % \hline
% % \hline
% % & & \multicolumn{14}{c}{Target Sequences} \\
% % \cline { 3 - 16 }
% & \multirow{2}{*}{Methods} & 3D Detection & Interpolation & ALL (mAPH) & \multicolumn{2}{c|}{VEH (AP/APH) } & \multicolumn{2}{c|}{PED (AP/APH)} & \multicolumn{2}{c}{CYC (AP/APH)} \\
% \cline{5-11}
%  &  & Modality & Modality & L2 & L1 & L2 & L1 & L2 & L1 & L2 \\
% \hline
% \multirow{5}{*}{Offline} & VoxelNeXt~\cite{chen2023voxelnext} & L & L \\
% & Focals Conv~\cite{chen2022focal} & L + I & L + I\\
% & LoGoNet~\cite{li2023logonet} & L + I & L + I\\
% & Focals Conv~\cite{chen2022focal} & L + I & L + I + E\\
% & LoGoNet~\cite{li2023logonet} & L + I & L + I + E\\
% \noalign{\vskip -0.6pt}
% \hline
% \hline
% \noalign{\vskip 1pt}
% \rowcolor{LG}
%  & VoxelNeXt~\cite{chen2023voxelnext} & L \\
% \rowcolor{LG}
% & Focals Conv~\cite{chen2022focal} & L + I & & 24.64 &  36.60/36.27 & 34.23/33.92 & 27.72/24.32 & 25.00/21.91 & 19.74/18.84 & 18.97/18.10  \\
% \rowcolor{LG}
% & LoGoNet~\cite{li2023logonet} & L + I & $-$ & 32.60 & 45.88/45.54 & 42.44/42.12 & 42.16/37.39 & 37.18/32.96 & 24.59/23.77 & 23.53/22.73\\
% \rowcolor{LG}
% \multirow{-4}{*}{Online}& Ev-3DOD (Ours) &  L + I + E\\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \vspace{-17pt}
% \end{table*}








