\section{Introduction}
% \begin{figure*}[h!]
% \begin{center}
% \includegraphics[width=.9\linewidth]{figures/teaser_v1.pdf}
% % \vspace{-10pt}
% \caption{\textbf{TBD}}
% \label{fig:teaser}
% \end{center}
% % \vspace{-10pt}
% \end{figure*}
\def\thefootnote{*}\footnotetext{The first two authors contributed equally.}\def\thefootnote{\arabic{footnote}} 

\label{sec:intro}
3D object detection is a crucial task in autonomous driving. Multi-modal sensors like LiDAR and cameras are commonly used to understand scenes and handle complex road scenarios. This sensor setup consists of synchronized sensors with a fixed frame rate and has limited speed due to bandwidth limitations.
As shown in Fig.~\ref{fig:teaser}~(a), the blind time, which is the period when sensor information is unavailable (\eg,~100 milliseconds), is critical as cars can experience large motions as 3 meters during this time. Therefore, temporal resolution is a crucial factor that can directly influence accident risk.

% 센서의 정보가 부재한 시간인 blind 시간 (\eg, 100ms) 동안 차와 같은 object들은 3 meter 이상을 움직일 수 있기에, temporal resolution은 굉장히 중요한 문제이다.

% 최근에는 neuromorphic sensor인 이벤트 카메라를 이용하여 perceptual research에 새로운 페러다임을 제시하였다. 이벤트 카메라는 brightness changes를 stream 형태의 데이터로 제공하여, sub-milisecond latency를 가져서 motion blur~\cite{yang2024latency}에 강건하고, 낮은 bandwidth를 가진다~\cite{}.
% 이벤트 카메라는 bandwith와 blind time에 대한 trade-off를 극복한 대신, intensity에 대한 정보를 희생한다는 단점이 있으며, 최근 work들은 이러한 이벤트 카메라의 단점을 보완하기 위해 fixed  camera와 함께 사용하여 low-level vision과 autonomous driving에서 이벤트 카메라의 sparsity를 보완함과 동시에 센서의 temporal limit을 극복하였다.


Recently, event cameras~\cite{gallego2020event} have introduced a new paradigm in perceptual research. Event cameras provide brightness changes as a stream of data in sub-millisecond latency, which makes them robust to motion blur~\cite{yang2024latency} and have low bandwidth~\cite{Gehrig2022RecurrentVT, tulyakov2022time}. Event cameras overcome the trade-off between bandwidth and blind time but have the drawback of sacrificing intensity information. To address this, recent work has combined event cameras with frame cameras to complement the sparsity of event cameras while overcoming the fixed frame rate sensor's limits. 
% Apart from this line of research, event cameras are 2D image-based sensors, and as such, they are not inherently optimized for performing 3D perception on their own.
% 이러한 흐름과는 별개로 이벤트 카메라는 이미지 기반의 센서이기 때문에 그 자체만으로 3D perception을 수행하는데 최적이지는 않다. 

In this paper, we aim for accurate 3D object detection in blind time through a multi-modal approach that combines LiDAR and frame-based sensors with an event camera, leveraging its high temporal resolution. % Since LiDAR and cameras have a fixed frame rate, blind time inevitably occurs between the active timestamps, which refer to the moments when sensor data is acquired. 
% 이러한 세팅에서는 blind time 동안에는 이벤트 데이터만 존재하고, LiDAR와 image data는 존재하지 않으며, 가장 최근 active timestamp의 LiDAR와 이미지 데이터만 존재한다.
% 즉, 이미지와 LiDAR 데이터와 현재의 시점은 timestamp가 어긋나 있으며, 우리는 이벤트 카메라를 통해 현재 시점에 맞게 과거 정보를 옮겨주는 전략이 필요하다.
As shown in Fig.~\ref{fig:teaser}~(b), event data is available during blind times, while the LiDAR and image data from the most recent active timestamp are accessible. Therefore, there is a misalignment in timestamps between the fixed frame sensor data and the current blind time. Then, this raises the question: \textit{how can we estimate 3D motion using only event data, especially in cases where data from other sensors aligned with the current time is unavailable?}


To address this, we propose a virtual 3D event fusion (V3D-EF) that transfers past 3D information to align with the present time using the event camera. 
% 이벤트 카메라는 2D image plane 상에서 만의 pixel 단위의 움직임을 가지고 있기에, 3D motion을 추정하기 위해 우리는 3D voxel 공간에 이벤트를 project하여 가상의 3D event feature를 생성한다. 그 과정에서, point cloud가 존재하는 공간에 대해서만 event를 project하여 point cloud와 align하는 과정을 거친다. 다른 기존 연구에서 많이 다루었뜻이, 이벤트 데이터로 생성된 feature는 그 자체로 motion을 포함하지만, 우리의 연구에서는 이벤트로 3D motion을 추정해야한다.
As extensively covered in previous research~\cite{kim2024cross, shiba2022secrets, gehrig2024dense}, event data inherently contain motion information on an image plane. Based on this observation, we hypothesized that providing a 3D cue associated with the initial timestamp to the event information would enable accurate estimation of the 3D motion vector.
% Since the event camera captures only pixel-level motion on the 2D image plane, to estimate 3D motion, 
To this end, we project event features into a 3D voxel space to create virtual 3D event features. In this process, events are projected only onto regions where point cloud data exists, aligning with the point cloud data. 
% Then, we apply ROI pooling [9, 48] to generate the ROI-grid features.
Then, we aggregate the event features with the voxel features to estimate an implicit motion field that captures the motion inherent within the grid. The generated implicit motion field encapsulates both the 3D spatial information from voxels and the motion information from events, enabling the calculation of a 3D motion vector from the most recent active timestamp to the present blind time.

Another crucial factor in 3D object detection is the score for each bounding box. The model considers both box proposal scores at the active timestamp and the motion score of each predicted box in blind time.
In blind time, the model aligns 3D information of voxel features and motion information of events to learn the confidence of box motion. 


% event camera의 3D perception 연구의 장벽(허들)은 데이터셋의 부재이며, 이를 해결하기 위해 우리는 이 work에서 두 가지 데이터셋을 제시한다 synthetic event data를 포함하는 Ev-Waymo와 real event data를 포함하는 DSEC-3DOD 데이터셋을. 
A major hurdle in 3D detection research using event cameras is the lack of suitable datasets. To address this, we present two datasets: Ev-Waymo, which includes synthetic event data, and DSEC-3DOD, which contains real event data. Unlike existing public 3D object detection datasets, ours includes annotations not only for active timestamps with 3D LiDAR sensor data but also for blind times, providing annotation in 100 FPS frame rate. 

% , we can leverage this implicit motion field to estimate the movement of the 3D bounding box.


% However, in our study, we aim to further estimate 3D motion directly from event data.



% Specifically, we project event features from a 2D plane to 3D voxel space using point clouds 

% and then aggregate the event features with voxel features to estimate the implicit motion of objects



% In this work, we aim to enable 3D object detection during these blind time intervals between active timestamps using event cameras. 

% 이벤트 카메라는 bandwidth가 


% 우리는 fixed frame rate를 가지는 센서들을 이용하여 active timestamp 마다 기존 방식들과 동일하게 3D object detection을 수행한다. 그리고, blind time 동안에 이전 active timestamps에서 얻어진 네트워크의 산출물 (pointcloud feature, 3D box proposals)과 이벤트를 fusion하여 object의 blind time 동안의 움직임을 정확하게 추정할 수 있다. 3D object detection의 산출물들은 모두 3D space 상에 존재하기 때문에, 우리는 이벤트 feature를 2D plane에서 3D voxel space로 ray projection 하여, aggregate 하는 방식을 제안한다. 뿐만 아니라, 우리는 


% 우리는 이벤트 카메라



% 이를 위해 우리는 라이다 및 카메라 센서들을 함께 사용하는 multi-modal approach를 통해 정확한 3D obejct detection을 수행하고자 한다. 라이다와 카메라는 fixed frame rate를 가져서, 센서 데이터가 존재하지 않는 blind time이 필연적으로 생기는데, 우리는 이벤트 카메라를 이용하여 해당 interval 동안에도 inference가 가능한 방법을 제안한다. 구체적으로, 우리는 3D voxel 공간에 point cloud를 이용하여 event feature를 2D로부터 3D로 projection하고, 그 뒤에 3D 상의 이벤트 feature를 aggregate하여 implicit한 motion을 estimate한다.

% 이러한 움직임을 통해 우리는 image와 lidar가 존재하지 않는 blind time 동안의 물체 위치를 추정할 수 있으며, 또한 우리는 이전 생성된 image와 point cloud feature를 





% 우리는 기존 fixed frame rate 센서들의 정보가 부재한 상황에서도 이벤트 카메라를 이용하여 high frame rate의 perception이 가능하다는 것을 보여줌으로써, 이벤트 카메라의 자율주행에 대한 potential을 기대하게 만든다.


% event camera의 3D perception 연구의 장벽(허들)은 데이터셋의 부재이며, 이를 해결하기 위해 우리는 이 work에서 두 가지 데이터셋을 제시한다 synthetic event data를 포함하는 Ev-Waymo와 real event data를 포함하는 DSEC-3DOD 데이터셋을. 


% 
% 1. 3D detection에 대한 이야기
% 2. 안전한 자율 주행에 있어서, low latency의 필요성에 대한 이야기
% 


% % 

% The key contributions of our work are summarized as follows:

% \begin{itemize}[leftmargin=4mm, itemsep=0pt]
% \item We propose 
% \item We propose 
% \item We create the DSEC-3DOD dataset, 
% \end{itemize}
