

\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\setlength{\aboverulesep}{-1.5pt}
\setlength{\belowrulesep}{0pt}
\setlength{\tabcolsep}{3.0pt}
\renewcommand{\arraystretch}{1.2}
% \setlength{\extrarowheight}{.75ex}
\begin{table*}[!b]
\begin{center}
\caption{Performance and runtime comparison on the Ev-Waymo dataset. Evaluated at 100 FPS for $t = {0, 0.1, \ldots, 0.9}$. Offline results, which rely on sensor data from timestamp 1, future information, and additional interpolation algorithms, are excluded from evaluation.}
\vspace{2pt}
\label{tab:inference_time}
\resizebox{.99\linewidth}{!}{
\begin{tabular}{c|c|c|cc|cc|cc|c}
\toprule
\multirow{3}{*}{Methods} & \multirow{3}{*}{\thead{3D Detection\\Modality}} &  ALL & \multicolumn{2}{c|}{\multirow{2}{*}{VEH (AP/APH)}} & \multicolumn{2}{c|}{\multirow{2}{*}{PED (AP/APH)}} & \multicolumn{2}{c|}{\multirow{2}{*}{CYC (AP/APH)}} & \multirow{3}{*}{FPS}\\
&  & (mAP/mAPH) & & & & & & & \\
\cline{3-9}
 & & L2 & L1 & L2 & L1 & L2 & L1 & L2 \\
\hline 
\noalign{\vskip 1pt}
VoxelNeXt~\cite{chen2023voxelnext} & L & 33.32/31.70 & 44.40/44.10 & 41.78/41.49 &	40.52/36.23 & 36.93/32.96 &	24.40/23.73 & 21.24/20.66 & 17.34\\
HEDNet~\cite{zhang2024hednet} & L & 31.57/29.90 & 42.03/41.71 & 39.32/39.02 &	38.86/34.43 & 35.67/31.53 &	22.64/21.99 & 19.72/19.14 & 12.84\\
Focals Conv~\cite{chen2022focal} & L + I & 26.27/25.01 & 37.31/37.01 & 36.60/36.31 &	29.20/26.16 & 28.41/25.44 &	14.30/13.78 & 13.79/13.29  & 6.08\\
LoGoNet~\cite{li2023logonet} & L + I  & 33.27/31.75 & 44.14/43.87 & 41.73/41.47 &	39.98/35.84 & 36.48/32.67 &	24.71/24.15 & 21.59/21.10 & 10.68\\
\hline
Ev-3DOD (Ours) &  L + I + E &\textbf{48.06}/\textbf{45.60} & \textbf{60.30}/\textbf{59.95} & \textbf{59.19}/\textbf{58.85} & \textbf{57.40}/\textbf{50.78} & \textbf{55.30}/\textbf{48.93} & \textbf{31.08}/\textbf{30.38} & \textbf{29.69}/\textbf{29.03} & \underline{27.09}\\
Ev-3DOD-\textit{Small} (Ours) &  L + I + E & \underline{44.21}/\underline{42.01} & \underline{57.95}/\underline{57.62} & \underline{56.89}/\underline{56.57} & \underline{51.87}/\underline{45.91} &	\underline{49.94}/\underline{44.21}	& \underline{27.01}/\underline{26.44} &	\underline{25.80}/\underline{25.25} & \textbf{54.14}
\\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-15pt}
\end{table*}




In this supplementary material, we offer more details of our work, Ev-3DOD. Specifically, we provide
% 
\begin{itemize}
\item Runtime analysis of our framework, Ev-3DOD, in Section~\ref{sec:inf_time};
\item Detailed processing and annotation processes about event-based 3D detection datasets in Section~\ref{sec:dataset};
\item Additional qualitative results and video demo in Section~\ref{sec:qual_video};
\item Implementation details in Section~\ref{sec:implementation};
\item Hyper-parameter analysis in Section~\ref{sec:hyper};
\end{itemize}
\vspace{10pt}

\section{Inference Time}
\label{sec:inf_time}
To measure the inference time of our method and other approaches, we followed the speed measurement protocol from conventional event-based object detection~\cite{Gehrig2022RecurrentVT}, using the code provided at the given link\footnote{\url{https://github.com/uzh-rpg/RVT}}. We also performed GPU warm-up and ensured GPU-CPU synchronization using `torch.cuda.synchronize()' to accurately measure inference time. We measured inference time on a single NVIDIA A6000 GPU with a batch size of 1 and additionally designed a lightweight model, Ev-3DOD-\textit{Small}, to evaluate both performance and speed. Specifically, Ev-3DOD-\textit{Small} retains the overall structure of Ev-3DOD but replaces the event feature encoder with three simple convolution layers, reduces the number of pooling layers, and decreases the grid size in the Virtual 3D Event Fusion module. 

Table~\ref{tab:inference_time} compares performance and inference time among online methods. Looking at the performance metrics, our method, leveraging the event camera to infer during the blind time, achieves the best and second-best results across both approaches, with a significant margin over other methods. In terms of inference time, measured in FPS, even our full model (Ev-3DOD) achieves the fastest speed compared to other methods. This efficiency is attributed to our approach, which avoids recalculating point clouds and images during the blind time interval by explicitly leveraging events to update and reuse data at the present moment, making it highly cost-effective. Notably, when parameters are reduced, there is almost a twofold improvement in FPS with minimal performance degradation. This demonstrates that our method can effectively estimate 3D motion using event information without relying on a large number of parameters. We believe that the proposed Ev-3DOD, with its fast runtime using the high-frequency properties of an event camera, provides a promising direction for advancing future research in 3D object detection using event cameras.


\section{Event-based 3D Object Detection Datasets}
\label{sec:dataset}
In this section, we provide additional details about the dataset that may not have been fully covered in the main paper. Specifically, we delve into its structure, pre-processing steps, and unique characteristics critical for understanding the context and experimental results. By offering this comprehensive view, we aim to enhance the clarity and reproducibility of our work.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=.95\linewidth]{figures/supple_annotation_process_v3.pdf}
\vspace{-3pt}
\caption{The overall pipeline for annotation. To enhance data quality, we perform software-based alignment and generate fine-grained 100 FPS ground-truths through additional annotation and post-processing.}
\label{fig:supple_ann_process}
\end{center}
% \vspace{-19pt}
\end{figure*}

\subsection{DSEC-3DOD Dataset}

The DSEC~\cite{gehrig2021dsec} dataset provides LiDAR, stereo RGB images, and stereo events from diverse driving scenarios. To date, the DSEC dataset has been extensively studied for 2D perception tasks (\textit{e.g.} 2D object detection, semantic segmentation). In this study, we utilized this dataset for 3D perception for the first time and established the process of Fig.~\ref{fig:supple_ann_process} to provide fine-grained 100 FPS 3D detection ground truth. 

\noindent
\textbf{a. LiDAR-IMU SLAM}\\
For 3D detection annotation, a LiDAR sensor providing accurate depth information was designated as the reference sensor. The odometry of the reference sensor was estimated to synchronize LiDAR data with image timestamps, enabling accurate inter-modality alignment. Manual labeling was performed on a dense point cloud generated through pose-based LiDAR accumulation. To ensure precise LiDAR pose estimation, the LIO-Mapping~\cite{Ye2019TightlyC3} method was employed, consistent with the approach utilized in the DSEC dataset. The poses for the 10 FPS LiDAR data were subsequently obtained.

\begin{figure}[t]
\begin{center}
\includegraphics[width=.90\linewidth]{figures/image-lidar-sync-supple_v2.pdf}
\vspace{-8pt}
\caption{The time difference between images and closest LiDAR. To minimize temporal misalignment, 31 frames were sampled around the frame with the minimum time offset.}
\label{fig:supple_image_lidar_sync}
\end{center}
\vspace{-9pt}
\end{figure}

\noindent
\textbf{b. Sequence Sampling}\\
As mentioned in the main paper, we provide annotations for the ``zurich\_city" sequence. The DSEC provides images at 20 Hz and LiDAR data at 10 Hz. Images are sampled at 10 Hz following LiDAR. 
% The event data is aligned with the timestamps of the 10 Hz images. 
Although both the images and LiDAR data are sampled at the same 10 Hz rate, the lack of hardware time synchronization introduces temporal misalignment. To solve this problem, we utilize the sequence sampling strategy. Fig.~\ref{fig:supple_image_lidar_sync} illustrates the absolute time difference between the nearest image and LiDAR frames. Due to the periodic discrepancy between the two sensors, this misalignment repeats approximately every 237 frames, with a maximum time offset of half a sensor period (\textit{i.e.}, $50ms$). Therefore, we sampled 31 frames centered around the point of minimal time offset, reducing the maximum misalignment of $13 ms$.
As a result of sampling, the DSEC-3DOD dataset consists of 178 sequence chunks, each comprising 31 frames. Adjacent chunks are separated by a time gap of over 20 seconds, ensuring entirely different distributions in driving scenes. Table~\ref{tab:seq_train} and Table~\ref{tab:seq_test} show the train and test splits.


\noindent
\textbf{c. LiDAR-Image Synchronization}\\
Although the sequence sampling was designed to minimize time misalignment, synchronization errors still persist. Therefore, we further aligned the LiDAR data to the image timestamps using pose-based adjustments.

For an arbitrary RGB image $I_t$ at time $t$, the two closest-time LiDAR point clouds, $P_{t_0}$ and $P_{t_1}$, are identified, where $t_0 < t < t_1$. Assume the corresponding poses $X_{t_0}$ and $X_{t_1}$ are available from the mapping of Process \textbf{a}. Each pose is represented as a 3D coordinate and quaternion, denoted as $X = (x, y, z, Q)$, where $Q \in \mathbb{R}^4$. The image-aligned LiDAR pose $X_t$ is computed by interpolating $X_{t_0}$ and $X_{t_1}$. The position is interpolated linearly, while spherical linear interpolation (SLERP)~\cite{Shoemake1985AnimatingRW} is applied for the quaternions. 

The synchronized LiDAR point cloud $P_t$ is obtained using the transformation $P_t = T_t^{-1} T_{t'} P_{t'}$, where $T_t$ and $ T_{t'}$ are transformation matrices corresponding to the poses $X_t$ and $X_{t'}$, respectively. Here, $t'$ is the nearest time to $t$ as,
\begin{equation}
\begin{aligned}
t' =
\begin{cases} 
t_0 & \text{if } |t - t_0| < |t - t_1|, \\
t_1 & \text{otherwise.}
\end{cases}
\end{aligned}
\end{equation}
 
The effect of pose-based LiDAR synchronization is demonstrated in Fig.~\ref{fig:supple_image_lidar_alignment_compare}. LiDAR-image misalignment due to time offsets is most evident in scenes with large motions or significant rotations. By transforming the nearest LiDAR point cloud to the pose of the image timestamp, projection errors between sensors caused by time misalignment were minimized.

\begin{figure}[t]
\begin{center}
\includegraphics[width=.95\linewidth]{figures/image_lidar_compare_sync.pdf}
\vspace{-8pt}
\caption{LiDAR projection on images before pose-based LiDAR synchronization (left) and after synchronization (right).}
\label{fig:supple_image_lidar_alignment_compare}
\end{center}
\vspace{-19pt}
\end{figure}

\noindent
\textbf{d. Point Cloud Densification}\\
Raw LiDAR data is inherently sparse, which can lead to reduced accuracy in ground truth bounding boxes if used directly for annotation. To address this, we utilized an accumulated point cloud created by combining multiple LiDAR scans with their relative poses. As noted in the DSEC~\cite{gehrig2021dsec}, LiDAR accumulation does not effectively handle occlusions or moving objects, both of which are critical considerations in the labeling process. To mitigate these issues, we employed filtered results (\textit{i.e.}, disparity) during the annotation process.

\noindent
\textbf{e. Manual Annotation}\\
Annotation experts labeled 3D bounding boxes on the densified 10 FPS LiDAR data, ensuring accuracy by considering both the LiDAR data and images. The ground truth consists of three classes: vehicle, pedestrian, and cyclist. Detailed annotation rules were derived from the guidelines provided by the Waymo Open Dataset~\cite{Sun2019ScalabilityIP}. To maintain high-quality annotations, bounding boxes containing fewer than two points or positioned beyond 50 meters were excluded from the ground truth.\\
\noindent
\textbf{f. Annotation Interpolation}\\
To generate 100 FPS annotations from the manually labeled 10 FPS annotations, bounding boxes were interpolated to create ground truth for blind times where neither LiDAR nor images were available. Linear interpolation was applied to the bounding box pose and size, while SLERP interpolation was applied for rotations. The interpolated annotations were subsequently refined through the following process.\\
\noindent
\textbf{g. Sensor Data Interpolation}\\
In process \textbf{f}, the automatically interpolated annotations are generally accurate but may not fully capture the dynamics of real-world motion. To enhance the quality of these annotations, sensor data was generated for the blind times. For images, realistic video frames were produced using a recent event-based video frame interpolation method~\cite{kim2023event}. Similarly, the latest techniques~\cite{zheng2023neuralpci} were utilized to generate accurate and realistic intermediate point cloud data.

\noindent
\textbf{h. Annotation Filter and Refinement}\\
The interpolated sensor data was employed to refine the annotations for the blind times. In most cases with minimal motion, bounding box interpolation alone yielded ground truth that aligned well with the sensor data. In such instances, refinements were avoided to preserve smooth bounding box poses and ensure temporal consistency. However, if the interpolated labels were misaligned with the sensor data or if sensor data was unavailable, the affected labels were filtered out.

\subsection{Ev-Waymo Dataset}
\noindent
\textbf{Event Synthesis.} The Waymo Open Dataset (WOD) provides 10 FPS synchronized images, LiDAR, and 3D bounding box labels. To generate the events, we utilize a widely adopted event simulation model~\cite{Gehrig_2020_CVPR} to synthesize the events from video data. This enables us to utilize temporally dense events between image and LiDAR active timestamps for training and testing.

 
\noindent
\textbf{Annotation Interpolation and Refinement.}
Since WOD provides dense 10 FPS annotations, we can obtain 100 FPS ground truth through annotation interpolation and refinement. A process similar to the \textbf{f}, \textbf{g}, and \textbf{h} steps in DSEC-3DOD dataset processing was employed. We interpolated the 10 FPS bounding box information, including pose, dimensions, and heading, provided by WOD to generate 100 FPS data. In addition, we synthesized blind-time data of camera and LiDAR using an interpolation algorithm for refinement and filtering, ensuring higher quality.



% The pose, dimensions, and heading of the 10 FPS bounding boxes were interpolated, while synthetic blind-time images and point clouds were used for refinement and filtering.
















\section{More Qualitative Results and Videos}
\label{sec:qual_video}
To provide a more comprehensive understanding of the proposed model, we present additional qualitative results in Figures~\ref{fig:dsec_qual_1}, \ref{fig:dsec_qual_2}, and \ref{fig:dsec_qual_3}, showcasing its performance on the DSEC-3DOD dataset. The proposed method consistently predicts bounding boxes that closely align with the ground truth across various challenging environments. 

Figure~\ref{fig:dsec_qual_1} illustrates a challenging scene involving a bus, where size estimation is particularly difficult. The offline method, even with access to future information, fails to detect certain instances. Likewise, the online method demonstrates increasing errors compared to the ground truth as time progresses beyond the active timestamp. In contrast, the proposed method shows robust performance, generating predictions that closely align with the ground truth labels.

The introduced DSEC-3DOD dataset features challenging scenarios, including night scenes, as shown in Fig.~\ref{fig:dsec_qual_2}. Unlike the Ev-Waymo dataset, which cannot leverage challenging illumination conditions to generate synthetic events, our proposed real event dataset enables validation in such scenarios. In the night scene, the proposed method exhibits accurate box predictions compared to both the offline and online methods.

Figure~\ref{fig:waymo_qual}, \ref{fig:waymo_qual2}, and \ref{fig:waymo_qual3} present the results on the Ev-Waymo dataset, which features numerous complex sequences with high object density. The proposed model effectively predicts complex object motions, producing bounding boxes closely aligned with the ground truth. In such scenes, even with access to future data, interpolating sensor information during blind times remains challenging, which complicates precise bounding box predictions. Consequently, the qualitative results on Ev-Waymo demonstrate that the proposed method outperforms others by delivering more accurate bounding boxes.

We provide a short video to showcase the datasets used in the experiments and the results on sequential data. The proposed method demonstrates robust performance across various environments in both the DSEC-3DOD dataset and the Ev-Waymo dataset. Notably, it accurately estimates the motion of object bounding boxes even in challenging night scenes within the DSEC-3DOD dataset.

\section{Implementation Details}
\label{sec:implementation}
The model is trained using a two-stage strategy inspired by previous works~\cite{li2023logonet, Yin2024ISFusionIC} that leverage pre-trained encoders. In the first stage, 10 FPS LiDAR and images are utilized to train the active timestamp RGB-LiDAR Region Proposal Network. In the second stage, all sensor data are incorporated to train the blind time modules with 100 FPS ground truth bounding boxes.

Since only the front camera is used, we followed the KITTI~\cite{Geiger2012AreWR} methodology, utilizing only LiDAR point clouds and ground truth that fall within the camera's field of view. The point cloud spans $[0.0, 75.2m]$ along the $X$ axis, $[-75.2m, 75.2m]$ along the $Y$ axis, and $[-2m, 4m]$ along the $Z$ axis, with a voxel size of $(0.1m, 0.1m, 0.15m)$.
Ev-Waymo uses a resolution of $960 \times 640$, while DSEC-3DOD adopts $320 \times 240$ for both images and events using a downsample. The event stream is converted into a voxel grid with 5 bins. The DSEC-3DOD dataset provides sparse LiDAR data, which is processed by merging the preceding 9 LiDAR sweeps to enhance density. In contrast, Ev-Waymo utilizes only the current sweep without merging.

The voxel data is encoded using a 3D backbone~\cite{zhou2018voxelnet}, while the image and event data are processed using a common image encoder~\cite{Liu2021SwinTH}. 
 The small version of our model is discussed in Section~\ref{sec:inf_time}. In the Virtual 3D Event Fusion module, each box proposal is divided into $6\times6\times6$ sub-voxels. 

In the first stage, the Region Proposal Network is trained for 15 epochs with a batch size of 4 and a learning rate of $0.001$, using the Adam optimizer~\cite{kingma2014adam} with the scheduling strategy from \cite{smith2019super}. In the second stage, the blind time modules are trained for 15 epochs with a batch size of 3, maintaining the same learning rate of $0.001$. The loss function incorporates weighting factors $\lambda_1 = 1.0$ and $\lambda_2 = 1.0$.

% \noindent
% \textbf{Ev-Waymo}

% \noindent
% \textbf{DSEC-3DOD}

\section{Hyper-parameter Analysis}
\label{sec:hyper}
As shown in Table~\ref{tab:hyper}, we conduct an ablation study on the loss weights. The box regression loss and confidence prediction loss weights were set to 0.1, 1.0, and 10.0, respectively, during model training. The results demonstrated that the model remained robust, producing consistent outcomes despite changes in the loss magnitudes.



\begin{table}[t]
\caption{The result according to hyper-paramter in Eq.~(3) on Ev-Waymo LEVEL 2 (L2). $\lambda_1$ and $\lambda_2$ refer to the weight of box regression loss and binary cross entropy loss, respectively.}
\label{tab:hyper}
\vspace{-12pt}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.99\linewidth}{!}{\renewcommand{\tabcolsep}{6.0pt}
\begin{tabular}{c|cc|cc|cc}
% \hline
%                      & \multicolumn{3}{c|}{N-Caltech101} & \multicolumn{3}{c}{N-ImageNet~(Mini)} \\ 
\hline
\multirow{2}{*}{$\lambda_1$ \textbackslash{} $\lambda_2$} & \multicolumn{2}{c|}{0.1}
& \multicolumn{2}{c|}{1.0} 
 & \multicolumn{2}{c}{10.0}   \\
\cline{2-7}
& mAP & mAPH & mAP & mAPH & mAP & mAPH\\ 
\hline
0.1 & \gradient{47.32} & \gradienttwo{44.87} & 
\gradient{47.70} & \gradienttwo{45.22} & 
\gradient{47.14} & \gradienttwo{44.70} 
\\
% 0.05                & \gradient{83.52}    & \gradient{82.02}   & \gradient{81.66}   
% \\
1.0 & \gradient{47.72} & \gradienttwo{45.25}  &
\gradient{48.06} & \gradienttwo{45.60}  &
\gradient{46.98} & \gradienttwo{44.53} 
\\
% 0.5                 & \gradient{81.99}    & \gradient{82.39}   & \gradient{82.61}   
% \\
10.0  & \gradient{47.46} & \gradienttwo{45.02}  & 
\gradient{47.20} & \gradienttwo{44.74}  & 
\gradient{47.32} & \gradienttwo{44.86} 
\\ 
\hline

\end{tabular}}
\end{center}
\vspace{-5pt}
\end{table}

% \begin{table}[t]
% \centering
% \caption{The train/test sequence splits of DSEC-3DOD dataset.}
% \vspace{-7pt}
% \renewcommand{\arraystretch}{1.0}
% \setlength\tabcolsep{6.0pt}
% \resizebox{.99\linewidth}{!}{
% \begin{tabular}{l|l|ccc}
% \hline
% \textbf{Split} & \textbf{Time} & \textbf{Sequence} & \textbf{\# Frames} & \textbf{\# GT Scenes} \\ \hline
% \textbf{Train} & \textbf{Day} & 
%  zurich\_city\_00\_a & 31 & 301\\ 
% & &zurich\_city\_00\_b & 155 & 1,505\\ 
% & &zurich\_city\_01\_a & 62 & 602\\ 
% & &zurich\_city\_01\_b & 155 & 1,505\\ 
% & &zurich\_city\_01\_c & 124 & 1,204\\ 
% & &zurich\_city\_01\_d & 93 & 903\\ 
% & &zurich\_city\_01\_e & 217 & 2,107\\ 
% & &zurich\_city\_01\_f & 155 & 1,505\\ 
% & &zurich\_city\_02\_a & 31 & 301\\ 
% & &zurich\_city\_02\_b & 124 & 1,204\\ 
% & &zurich\_city\_02\_c & 279 & 2,709\\ 
% & &zurich\_city\_02\_d & 155 & 1,505\\ 
% & &zurich\_city\_02\_e & 186 & 1,806\\ 
% & &zurich\_city\_04\_a & 93 & 903\\ 
% & &zurich\_city\_04\_c & 93 & 903\\ 
% & &zurich\_city\_04\_d & 93 & 903\\ 
% & &zurich\_city\_04\_e & 31 & 301\\ 
% & &zurich\_city\_04\_f & 124 & 1,204\\ 
% & &zurich\_city\_05\_a & 217 & 2,107\\ 
% & &zurich\_city\_05\_b & 124 & 1,204\\ 
% & &zurich\_city\_06\_a & 186 & 1,806\\ 
% & &zurich\_city\_07\_a & 124 & 1,204\\ 
% & &zurich\_city\_08\_a & 62 & 602\\ 
% & &zurich\_city\_11\_a & 31 & 301\\ 
% & &zurich\_city\_11\_b & 93 & 903\\ 
% & &zurich\_city\_11\_c & 155 & 1,505\\ 
% \cline{2-5}
% & \textbf{Day Total} & & \textbf{3,193} & \textbf{31,003} \\
% \cline{2-5}
%  & \textbf{Night} & 
%  zurich\_city\_03\_a & 62 & 602\\ 
% & &zurich\_city\_09\_a & 217 & 2,107\\ 
% & &zurich\_city\_09\_b & 31 & 301\\ 
% & &zurich\_city\_09\_c & 155 & 1,505\\ 
% & &zurich\_city\_09\_d & 124 & 1,204\\ 
% & &zurich\_city\_09\_e & 93 & 903\\ 
% & &zurich\_city\_10\_a & 248 & 2,408\\ 
% & &zurich\_city\_10\_b & 217 & 2,107\\ 
% \cline{2-5}
% & \textbf{Night Total} & & \textbf{1,147} & \textbf{11,137} \\
% \cline{2-5} 
% & \textbf{Train Total} & & \textbf{4,340} & \textbf{42,140} \\
% \hline
% \hline
% \textbf{Test} & \textbf{Day} & 
% zurich\_city\_00\_a & 62 & 602\\ 
% & &zurich\_city\_00\_b & 31 & 301\\ 
% & &zurich\_city\_01\_e & 31 & 301\\ 
% & &zurich\_city\_01\_f & 62 & 602\\ 
% & &zurich\_city\_02\_b & 31 & 301\\ 
% & &zurich\_city\_02\_c & 93 & 903\\ 
% & &zurich\_city\_02\_d & 62 & 602\\ 
% & &zurich\_city\_04\_c & 31 & 301\\ 
% & &zurich\_city\_04\_d & 31 & 301\\ 
% & &zurich\_city\_05\_b & 62 & 602\\ 
% & &zurich\_city\_06\_a & 31 & 301\\ 
% & &zurich\_city\_07\_a & 62 & 602\\ 
% & &zurich\_city\_08\_a & 31 & 301\\ 
% & &zurich\_city\_11\_b & 155 & 1,505\\ 
% & &zurich\_city\_11\_c & 93 & 903\\ 
% \cline{2-5}
% & \textbf{Day Total} & & \textbf{868} & \textbf{8,428} \\
% \cline{2-5} 
% & \textbf{Night} &  
% zurich\_city\_03\_a & 31 & 301\\ 
% & &zurich\_city\_09\_a & 31 & 301\\ 
% & &zurich\_city\_09\_c & 31 & 301\\ 
% & &zurich\_city\_09\_d & 93 & 903\\ 
% & &zurich\_city\_10\_a & 31 & 301\\ 
% & &zurich\_city\_10\_b & 93 & 903\\ 
% \cline{2-5}
% & \textbf{Night Total} & & \textbf{310} & \textbf{3,010} \\
% \cline{2-5}
% & \textbf{Test Total} & & \textbf{1178} & \textbf{11,438} \\
% \hline
% \end{tabular}}
% \label{tab:seq}
% \end{table}

\vspace{100pt}


\begin{table}[p]
\centering
\caption{The train sequence splits of DSEC-3DOD dataset.}
\vspace{-7pt}
\renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{4.5pt}
\resizebox{.99\linewidth}{!}{
\begin{tabular}{l|l|ccc}
\hline
\textbf{Split} & \textbf{Time} & \textbf{Sequence} & \textbf{\# Frames} & \textbf{\# GT Scenes} \\ \hline
\textbf{Train} & \textbf{Day} & 
 zurich\_city\_00\_a & 31 & 301\\ 
& &zurich\_city\_00\_b & 155 & 1,505\\ 
& &zurich\_city\_01\_a & 62 & 602\\ 
& &zurich\_city\_01\_b & 155 & 1,505\\ 
& &zurich\_city\_01\_c & 124 & 1,204\\ 
& &zurich\_city\_01\_d & 93 & 903\\ 
& &zurich\_city\_01\_e & 217 & 2,107\\ 
& &zurich\_city\_01\_f & 155 & 1,505\\ 
& &zurich\_city\_02\_a & 31 & 301\\ 
& &zurich\_city\_02\_b & 124 & 1,204\\ 
& &zurich\_city\_02\_c & 279 & 2,709\\ 
& &zurich\_city\_02\_d & 155 & 1,505\\ 
& &zurich\_city\_02\_e & 186 & 1,806\\ 
& &zurich\_city\_04\_a & 93 & 903\\ 
& &zurich\_city\_04\_c & 93 & 903\\ 
& &zurich\_city\_04\_d & 93 & 903\\ 
& &zurich\_city\_04\_e & 31 & 301\\ 
& &zurich\_city\_04\_f & 124 & 1,204\\ 
& &zurich\_city\_05\_a & 217 & 2,107\\ 
& &zurich\_city\_05\_b & 124 & 1,204\\ 
& &zurich\_city\_06\_a & 186 & 1,806\\ 
& &zurich\_city\_07\_a & 124 & 1,204\\ 
& &zurich\_city\_08\_a & 62 & 602\\ 
& &zurich\_city\_11\_a & 31 & 301\\ 
& &zurich\_city\_11\_b & 93 & 903\\ 
& &zurich\_city\_11\_c & 155 & 1,505\\ 
\cline{2-5}
& \textbf{Day Total} & & \textbf{3,193} & \textbf{31,003} \\
\cline{2-5}
 & \textbf{Night} & 
 zurich\_city\_03\_a & 62 & 602\\ 
& &zurich\_city\_09\_a & 217 & 2,107\\ 
& &zurich\_city\_09\_b & 31 & 301\\ 
& &zurich\_city\_09\_c & 155 & 1,505\\ 
& &zurich\_city\_09\_d & 124 & 1,204\\ 
& &zurich\_city\_09\_e & 93 & 903\\ 
& &zurich\_city\_10\_a & 248 & 2,408\\ 
& &zurich\_city\_10\_b & 217 & 2,107\\ 
\cline{2-5}
& \textbf{Night Total} & & \textbf{1,147} & \textbf{11,137} \\
\cline{2-5} 
& \textbf{Train Total} & & \textbf{4,340} & \textbf{42,140} \\
\hline
\end{tabular}}
\label{tab:seq_train}
\end{table}

\begin{table}[p]
\centering
\caption{The test sequence splits of DSEC-3DOD dataset.}
\vspace{-7pt}
\renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{4.5pt}
\resizebox{.99\linewidth}{!}{
\begin{tabular}{l|l|ccc}
\hline
\textbf{Split} & \textbf{Time} & \textbf{Sequence} & \textbf{\# Frames} & \textbf{\# GT Scenes} \\ \hline
\textbf{Test} & \textbf{Day} & 
zurich\_city\_00\_a & 62 & 602\\ 
& &zurich\_city\_00\_b & 31 & 301\\ 
& &zurich\_city\_01\_e & 31 & 301\\ 
& &zurich\_city\_01\_f & 62 & 602\\ 
& &zurich\_city\_02\_b & 31 & 301\\ 
& &zurich\_city\_02\_c & 93 & 903\\ 
& &zurich\_city\_02\_d & 62 & 602\\ 
& &zurich\_city\_04\_c & 31 & 301\\ 
& &zurich\_city\_04\_d & 31 & 301\\ 
& &zurich\_city\_05\_b & 62 & 602\\ 
& &zurich\_city\_06\_a & 31 & 301\\ 
& &zurich\_city\_07\_a & 62 & 602\\ 
& &zurich\_city\_08\_a & 31 & 301\\ 
& &zurich\_city\_11\_b & 155 & 1,505\\ 
& &zurich\_city\_11\_c & 93 & 903\\ 
\cline{2-5}
& \textbf{Day Total} & & \textbf{868} & \textbf{8,428} \\
\cline{2-5} 
& \textbf{Night} &  
zurich\_city\_03\_a & 31 & 301\\ 
& &zurich\_city\_09\_a & 31 & 301\\ 
& &zurich\_city\_09\_c & 31 & 301\\ 
& &zurich\_city\_09\_d & 93 & 903\\ 
& &zurich\_city\_10\_a & 31 & 301\\ 
& &zurich\_city\_10\_b & 93 & 903\\ 
\cline{2-5}
& \textbf{Night Total} & & \textbf{310} & \textbf{3,010} \\
\cline{2-5}
& \textbf{Test Total} & & \textbf{1178} & \textbf{11,438} \\
\hline
\end{tabular}}
\label{tab:seq_test}
\end{table}



\begin{table*}[p]
\begin{center}
\caption{The train/test sequence splits of Ev-Waymo dataset.}
\vspace{-5pt}
\resizebox{0.99\linewidth}{!}{\renewcommand{\tabcolsep}{5.6pt}
\begin{tabular}{c||c|c|c}
\hline
\textbf{Dataset}   & \multicolumn{2}{c}{\textbf{Ev-Waymo} }  \\ \hline\hline
\multirow{3}{*}{\textbf{Split}}  & \multirow{3}{*}{\textbf{Sequence Name}} & \multirow{3}{*}{\thead{ \textbf{No.}\\ \textbf{Seq.}}} & \textbf{No.}  \\
& & & \textbf{Labeled} \\
& & & \textbf{Scenes} \\
\hline
Train &  
\begin{tabular}[c]{@{}c@{}}
segment-207754730878135627\_1140\_000\_1160\_000, 
segment-13840133134545942567\_1060\_000\_1080\_000, \\
segment-8327447186504415549\_5200\_000\_5220\_000,
segment-10964956617027590844\_1584\_680\_1604\_680, \\
segment-11918003324473417938\_1400\_000\_1420\_000,
segment-15448466074775525292\_2920\_000\_2940\_000,\\
segment-14830022845193837364\_3488\_060\_3508\_060,
segment-11379226583756500423\_6230\_810\_6250\_810, \\
segment-7861168750216313148\_1305\_290\_1325\_290,
segment-13506499849906169066\_120\_000\_140\_000, \\
segment-6229371035421550389\_2220\_000\_2240\_000,
segment-15882343134097151256\_4820\_000\_4840\_000,\\
segment-14098605172844003779\_5084\_630\_5104\_630,
segment-8582923946352460474\_2360\_000\_2380\_000, \\
segment-16485056021060230344\_1576\_741\_1596\_741,
segment-915935412356143375\_1740\_030\_1760\_030, \\
segment-3002379261592154728\_2256\_691\_2276\_691,
segment-4348478035380346090\_1000\_000\_1020\_000,\\
segment-2036908808378190283\_4340\_000\_4360\_000,
segment-15844593126368860820\_3260\_000\_3280\_000, \\
segment-5835049423600303130\_180\_000\_200\_000,
segment-15696964848687303249\_4615\_200\_4635\_200, \\
segment-7543690094688232666\_4945\_350\_4965\_350,
segment-16372013171456210875\_5631\_040\_5651\_040,\\
segment-14193044537086402364\_534\_000\_554\_000,
segment-550171902340535682\_2640\_000\_2660\_000, \\
segment-4641822195449131669\_380\_000\_400\_000,
segment-7239123081683545077\_4044\_370\_4064\_370, \\
segment-11928449532664718059\_1200\_000\_1220\_000,
segment-5100136784230856773\_2517\_300\_2537\_300,\\
segment-13182548552824592684\_4160\_250\_4180\_250,
segment-14004546003548947884\_2331\_861\_2351\_861, \\
segment-2570264768774616538\_860\_000\_880\_000,
segment-7440437175443450101\_94\_000\_114\_000, \\
segment-15717839202171538526\_1124\_920\_1144\_920,
segment-8148053503558757176\_4240\_000\_4260\_000,\\
segment-16977844994272847523\_2140\_000\_2160\_000,
segment-5451442719480728410\_5660\_000\_5680\_000, \\
segment-7290499689576448085\_3960\_000\_3980\_000,
segment-16801666784196221098\_2480\_000\_2500\_000, \\
segment-4916527289027259239\_5180\_000\_5200\_000,
segment-16202688197024602345\_3818\_820\_3838\_820,\\
segment-9758342966297863572\_875\_230\_895\_230,
segment-12161824480686739258\_1813\_380\_1833\_380,\\
segment-14369250836076988112\_7249\_040\_7269\_040,
segment-2752216004511723012\_260\_000\_280\_000,\\
segment-10444454289801298640\_4360\_000\_4380\_000,
segment-17388121177218499911\_2520\_000\_2540\_000,\\
segment-7885161619764516373\_289\_280\_309\_280,
segment-16561295363965082313\_3720\_000\_3740\_000,\\
segment-11199484219241918646\_2810\_030\_2830\_030,
segment-4575961016807404107\_880\_000\_900\_000,\\
segment-7566697458525030390\_1440\_000\_1460\_000,
segment-10275144660749673822\_5755\_561\_5775\_561,\\
segment-6193696614129429757\_2420\_000\_2440\_000,
segment-12251442326766052580\_1840\_000\_1860\_000, \\
segment-13271285919570645382\_5320\_000\_5340\_000,
segment-9015546800913584551\_4431\_180\_4451\_180, \\
segment-10596949720463106554\_1933\_530\_1953\_530, 
segment-15942468615931009553\_1243\_190\_1263\_190, \\
segment-15125792363972595336\_4960\_000\_4980\_000, 
segment-1422926405879888210\_51\_310\_71\_310, \\
segment-5576800480528461086\_1000\_000\_1020\_000,
segment-1255991971750044803\_1700\_000\_1720\_000 \\
\end{tabular} 
&  64 & 126,330 
\\
\hline
Test & 
\begin{tabular}[c]{@{}c@{}}
segment-18446264979321894359\_3700\_000\_3720\_000,  
segment-17152649515605309595\_3440\_000\_3460\_000, \\
segment-16213317953898915772\_1597\_170\_1617\_170,
segment-5183174891274719570\_3464\_030\_3484\_030, \\
segment-3126522626440597519\_806\_440\_826\_440,
segment-3077229433993844199\_1080\_000\_1100\_000,\\ 
segment-10289507859301986274\_4200\_000\_4220\_000, 
segment-30779396576054160\_1880\_000\_1900\_000, \\
segment-9243656068381062947\_1297\_428\_1317\_428, 
segment-2834723872140855871\_1615\_000\_1635\_000,\\ 
segment-2736377008667623133\_2676\_410\_2696\_410,
segment-15948509588157321530\_7187\_290\_7207\_290,\\ 
segment-9231652062943496183\_1740\_000\_1760\_000, 
segment-4854173791890687260\_2880\_000\_2900\_000, \\
segment-6324079979569135086\_2372\_300\_2392\_300,
segment-6001094526418694294\_4609\_470\_4629\_470 \\
\end{tabular} 
& 16 & 31,550 \\
\hline
\end{tabular}}
\end{center}
\vspace{-5pt}
\label{tab:ev_waymo_split}
\vspace{-10pt}
\end{table*}



\begin{figure*}[p]
\begin{center}
\includegraphics[width=.94\linewidth]{figures/supple_dsec_qual_v1.pdf}
\vspace{-11pt}
\caption{Qualitative comparisons with other offline and online methods on the DSEC-3DOD dataset. $t=0$ represents the active time, while $t = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$ denote the blind times. The \textcolor{blue}{blue} box indicates ground truth, and the \textcolor{red}{red} box shows predictions. For better understanding, we overlaid the results onto the images generated by the interpolation method~\cite{kim2023event}.
}
\label{fig:dsec_qual_1}
\end{center}
\vspace{-19pt}
\end{figure*}

\begin{figure*}[p]
\begin{center}
\includegraphics[width=.94\linewidth]{figures/supple_dsec_qual_v2.pdf}
\vspace{-11pt}
\caption{Qualitative comparisons with other offline and online methods on the DSEC-3DOD dataset. $t=0$ represents the active time, while $t = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$ denote the blind times. The \textcolor{blue}{blue} box indicates ground truth, and the \textcolor{red}{red} box shows predictions. For better understanding, we overlaid the results onto the images generated by the interpolation method~\cite{kim2023event}.
}
\label{fig:dsec_qual_2}
\end{center}
\vspace{-19pt}
\end{figure*}

\begin{figure*}[p]
\begin{center}
\includegraphics[width=.94\linewidth]{figures/supple_dsec_qual_v3.pdf}
\vspace{-11pt}
\caption{Qualitative comparisons with other offline and online methods on the DSEC-3DOD dataset. $t=0$ represents the active time, while $t = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$ denote the blind times. The \textcolor{blue}{blue} box indicates ground truth, and the \textcolor{red}{red} box shows predictions. For better understanding, we overlaid the results onto the images generated by the interpolation method~\cite{kim2023event}.
}
\label{fig:dsec_qual_3}
\end{center}
\vspace{-19pt}
\end{figure*}



\begin{figure*}[p]
\begin{center}
\includegraphics[width=.94\linewidth]{figures/supple waymo_qual_v1.pdf}
\vspace{-11pt}
\caption{Qualitative comparisons of our method with other offline and online evaluations on the Ev-Waymo dataset. $t=0$ represents the active time, while $t = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$ denote the blind times. The \textcolor{blue}{blue} box represents the ground truth, while the \textcolor{red}{red} box shows the prediction results of each method. For easier understanding, images at active timestamps 0 and 1 are overlaid.}
\label{fig:waymo_qual}
\end{center}
\vspace{-19pt}
\end{figure*}


\begin{figure*}[p]
\begin{center}
\includegraphics[width=.94\linewidth]{figures/supple waymo_qual_v2.pdf}
\vspace{-11pt}
\caption{Qualitative comparisons of our method with other offline and online evaluations on the Ev-Waymo dataset. $t=0$ represents the active time, while $t = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$ denote the blind times. The \textcolor{blue}{blue} box represents the ground truth, while the \textcolor{red}{red} box shows the prediction results of each method. For easier understanding, images at active timestamps 0 and 1 are overlaid.}
\label{fig:waymo_qual2}
\end{center}
\vspace{-19pt}
\end{figure*}




\begin{figure*}[p]
\begin{center}
\includegraphics[width=.94\linewidth]{figures/supple waymo_qual_v3.pdf}
\vspace{-11pt}
\caption{Qualitative comparisons of our method with other offline and online evaluations on the Ev-Waymo dataset. $t=0$ represents the active time, while $t = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$ denote the blind times. The \textcolor{blue}{blue} box represents the ground truth, while the \textcolor{red}{red} box shows the prediction results of each method. For easier understanding, images at active timestamps 0 and 1 are overlaid.}
\label{fig:waymo_qual3}
\end{center}
\vspace{-19pt}
\end{figure*}