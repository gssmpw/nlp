


\section{Method}

% \textbf{Probl}

\subsection{Problem Definition.}
% 우리의 goal은 LiDAR와 Image가 존재하는 active timestamp에 해당하는 0으로부터 다음 active timestamp인 1에 도달하기 전인 blind time $0 \leq t < 1$에서 3D bounding box를 추론하는 것이다.
LiDAR and camera data are only available at active timestamps 0 and 1, where 1 represents invalid future information in an online process. 
% Our goal is to predict 3D bounding boxes during \(0 \leq t < 1\), including blind time, when synchronized sensors are unavailable. 
Our goal is to predict 3D bounding boxes during \(0 \leq t < 1\), including the blind time when synchronized sensors are unavailable.
Given a pair of synchronized 3D points $P_0=\{(x_l, y_l, z_l)\}_{l=1}^N$ and image $I_0 \in \mathbb{R}^{3\times H \times W}$ at active timestamp 0, we leverage event stream $\mathcal{E}_{0 \rightarrow t}=\{(i, \ j, \ p, \ \tau) | 0 \leq \tau < t\}$ to detect 3D bounding box set $B_t$ at an arbitrary time $0 \leq t < 1$. 
% 본 연구는 fixed frame rate 라이다와 카메라 데이터가 부재한 interframe interval에서 aynchronous event를 이용하여 3d object를 detect 하는 것을 목적으로 한다. 우리의 모델은 




\subsection{Framework Overview}
As illustrated in (a) of Fig.~\ref{fig:overall_method}, at active timestamp 0, both LiDAR and image data are available. RGB-LiDAR Region Proposal Network (RPN) generates bounding box proposals from rich 3D information in active time. We begin by voxelizing the point cloud and generating voxel features, denoted as $\mathbf{V}_0 \in \mathbb{R}^{D_X \times D_Y \times D_Z \times C}$, where $D_X$, $D_Y$, and $D_Z$ represent the grid size, and $C$ denotes the channel dimension, using a 3D backbone~\cite{zhou2018voxelnet}. Then, using existing region proposals~\cite{yin2021center} and refinement networks~\cite{li2023logonet} with image features, we generate bounding box proposals, $B_0 = \{ B_0^1, B_0^2, \ldots, B_0^n \}$ and box proposal confidence $p_0=\{p^1_0, p^2_0, \ldots ,p^n_0\} \in [0, 1]$ for the active timestamp 0. RGB-LiDAR-based multi-modal networks have been widely studied, leading to many advanced models capable of accurate estimation at active timestamps. We tuned the model with fewer parameters and a reduced channel dimension to improve efficiency.

Our emphasis is on achieving precise estimation throughout the blind time, spanning the interval from active timestamp 0 to the subsequent timestamp 1. To infer bounding boxes at an arbitrary inter-frame time \( t \), we first convert the event stream, $\mathcal{E}_{0 \rightarrow t}$ into voxel grids~\cite{zhu2019unsupervised} as $\mathbb{E}_{t}$. Then, we use a feature encoder to produce event features, $\mathbf{E}_{t} \in \mathbb{R}^{H/4 \times W/4 \times C}$, where $H$ and $W$ is the spatial dimensions of the events. Our objective is to estimate the 3D motion of objects during blind time. To achieve this, we utilize the bounding box set \( B_0 \) and voxel features \( \mathbf{V}_0 \) from the active timestamp 0. During blind times, we share the voxel features, $\mathbf{V}_{0}$, generated at the active timestamp, ensuring that voxel features are computed only once to maintain a cost-effective structure. Through the Virtual 3D Event Fusion (V3D-EF), we generate an implicit motion field for each bounding box by integrating event and voxel features, encapsulating motion information. Using this field, we compute the 3D motion and confidence score, enabling the estimation of 3D boxes at the blind time \( t \).


% During blind times, we reuse the voxel features, $\mathbf{V}_{0}$, generated at the active timestamp to maintain a cost-effective structure.








% \subsection{Event-based Motion Prediction Module}

\subsection{Virtual 3D Event Fusion for Motion Prediction}
\label{Sec:v3d_ef}

% 이벤트 feature는 2D image plane 상에서 각 픽셀의 motion 정보를 시간에 대해 연속적으로 담고 있어, 2D motion을 추정하는데 굉장히 특화되어 있다. 
The event feature continuously captures each pixel's motion information over time on the 2D image plane. However, estimating 3D motion using only events is a challenge. We hypothesize that providing each event feature with adequate 3D information would allow for effective 3D motion estimation. To achieve this, we aim to incorporate a 3D cue into the event features. 
% To match the sparse event features with sparse voxel features, it is crucial to fully utilize the information from both modalities.
Since both event features and voxel features are sparse, it is crucial to align each voxels with the corresponding events accurately.
Directly using the voxel features would cause misalignment with the actual 3D geometry. Thus, we adopt the voxel point centroid method employed in previous works~\cite{hu2022point}. 


% 우리는 sparse한 event feature를 sparse한 voxel feature와 matching을 해야하기 때문에, 두 modality의 information에서 나오는 정보를 적극적으로 활용해야 한다. 단순히, voxel feature를 그대로 사용하면, 실제 geometric한 coordinate와 다르기 때문에, 우리는 기존 work에서 사용한 voxel point centorid 방법을 채택한다.

% Inspired by grid-subsampling in KPConv~\cite{31, 32}, the voxel point centroid localization module spatially locates non-empty voxel features for aggregation in density-aware RoI grid pooling. 

Specifically, we first consider the non-empty voxel features as $\mathbf{V}_0 = \{ V^k_0 = \{ h^{k}_0, f^{k}_0 \} \ | \ k = 1, \ldots, N_\mathcal{V} \}$, where $h_0^k$ is the 3D voxel index, $f_0^k$ is the associated voxel feature vector, and $N_\mathcal{V}$ is the number of non-empty voxels. Then, points within each voxel are grouped into a set \( \mathcal{N}(V_0^k) \) by determining their voxel index \( h_0^k \) based on their spatial coordinates \( X_i=(x_i, y_i, z_i) \) of points. The centroid of each voxel feature is subsequently computed as:
\begin{equation}
\begin{aligned}
c_0^k = \frac{1}{|\mathcal{N}(V_0^k)|} \sum_{X_i \in \mathcal{N}(V_0^k)} X_i.
\label{equ:cost}
\end{aligned}
\end{equation}
The centroid \( c_0^k \) is calculated for non-empty voxels, ensuring it corresponds only to indices with existing points, with the goal of aligning these centroids to the event data. As shown in the top of Fig.~\ref{fig:v3d_ef}, we propose a method to project events into a virtual voxel space based on the coordinates of the centroids. The projection onto the image plane from 3D space can be obtained as:
$\mathbf{p}_0^k = K \cdot E \cdot c_0^k,$
% \begin{equation}
% \begin{aligned}
% \label{equ:project}
% \end{aligned}
% \end{equation}
where $K$ and $E$ denote the intrinsic and extrinsic parameters, $\cdot$ operation is matrix multiplication.
Therefore, we calculate the event coordinates corresponding to each centroid, $c_0^k$, and gather the event features, $\mathbf{E}_{t}$, at these coordinates, $\mathbf{p}_0^k$, to create virtual 3D event features, $\mathbf{V}_t^E = \{\mathbf{E}_t(\mathbf{p}_0^k) \}_{k=1}^{N_\mathcal{V}} \in \mathbb{R}^{N_\mathcal{V} \times C}$.

% 이러한 centroid는 non-empty voxel에 대해 고려하였으므로, 모두 point가 존재하는 index에 해당하며, 우리는 해당 centroid에 대하여 이벤트와 matching하고자 한다.

\begin{figure}[t]
\begin{center}
\includegraphics[width=.98\linewidth]{figures/v3d_ef_v4.pdf}
\vspace{-2pt}
\caption{Virtual 3D Event Fusion (V3D-EF). We generate an implicit motion field for each bounding box by applying the ROI pooling  separately to the virtual 3D event features, which represent the 3D projection of events, and the voxel features.}
\label{fig:v3d_ef}
\end{center}
\vspace{-16pt}
\end{figure}




% 우리는 centroid의 coordinate를 기반으로 가상의 voxel 공간에 이벤트를 프로젝션하는 방식을 제안한다.

% 우리는 active timestamp 0에 추정된 bounding box에 대한 3D motion을 추정하고자 하기 때문에, box proposal에 대해서만 feature를 fusion 한다.

% ROI-grid feature를 생성한다.
Since we aim to estimate the 3D motion of bounding boxes from the active timestamp 0 up to the current time \( t \), we consider fusion only for the features corresponding to the box proposal. Given the bounding box proposals, $B_0$, at active timestamp 0, we divide box proposals into $S \times S \times S$ regular sub-voxels. 
% Then, we perform the ROI pooling on 
% Then, 우리는 각 sub-voxel에 해당하는 virtual 3D event features와 voxel features에 대해 ROI pooling~\cite{} operation을 각각 적용하여, bounding box들에 해당하는 feature를 생성한다.
% 그리고 생성한 feature들을 concatenate하고, MLP를 거쳐 implicit motion field를 생성해낸다.
% Then, we apply the ROI pooling~\cite{deng2021voxel} operation separately to the virtual 3D event features, $\mathbf{V}_t^E$, and voxel features, $\mathbf{V}_0$, corresponding to each sub-voxel, generating features for the bounding boxes as $\tilde{\mathbf{V}}_t^E, \tilde{\mathbf{V}}_0 \in \mathbb{R}^{n \times S^3 \times C}$, respectively, where $n$ is the number of box proposals.
Then, we independently perform the ROI pooling~\cite{deng2021voxel} on the virtual 3D event features $\mathbf{V}_t^E$, and the voxel features $\mathbf{V}_0$, corresponding to each sub-voxel. This generates grid features for the bounding boxes, denoted as $\tilde{\mathbf{V}}_t^E \in \mathbb{R}^{n \times S^3 \times C} $ and $ \tilde{\mathbf{V}}_0 \in \mathbb{R}^{n \times S^3 \times C}$, where $n$ is the number of box proposals. The generated features are then concatenated and passed through an MLP to produce the implicit motion field, $\mathcal{M}_t=\{\mathcal{M}_t^0,\mathcal{M}_t^1,\dots,\mathcal{M}_t^n\}$. The implicit motion field is created by aligning 3D events with their corresponding voxel features, encapsulating motion information for the bounding box. Using an MLP, we estimate the explicit 3D motion. The explicit box motion $\mathbf{M}_{0 \rightarrow t}^i=(dx^i, dy^i, dz^i, d\alpha^i)$, defined in box local coordinate of $B_0^i$, contains spatial shift and yaw rotation. The final box location is calculated as $B_t^i= g(B_0^i, \mathbf{M}_{0 \rightarrow t}^i)$, where $g$ is a box shifting operation.

% $ 




% 우리는 non-empty voxel에 대한 



% Since the voxels in the convolution layers are in a sparse format, an intermediate hash table is used to efficiently map each calculated voxel point centroid to its corresponding feature vector. As shown in Figure~3, both voxel point centroids and sparse voxel features are associated with a shared voxel index. The intermediate hash table links the centroid \( c^{V^l}_k \) with \( V^l_k \) using the matching voxel index \( h^{V^l}_k \).




% centroid~\cite{hu2022point}

%EMP module

% ray, virtual, voxel 이런게 넣고 싶긴 한 욕구가 있음.

% \subsection{Dynamic Region Proposal Module}
% new object handling 해주는 모듈

% \subsection{Confidence Refinement}

% \subsection{Persistence Scoring Module}
% removal이 강조되면 좋을듯

\subsection{Motion Confidence Estimator}
\label{sec:mce}
% Our framework estimates the motion of box proposals predicted at the active timestamp 0 throughout the blind time. 
As discussed in previous studies~\cite{zhou2018voxelnet, yin2021center}, self-awareness of the confidence of predicted bounding boxes is a key factor. The simplest choice of final confidence score in our setting would be the score of active timestamp 0. However, the final score should take into account the object's motion prediction during the blind time. If the object experiences challenging motion during the blind time, a reliable bounding box at the active timestamp 0  could become less reliable. Therefore, the final score should consider two different sources. The first aspect is the score estimated at the active timestamp, based on LiDAR and images. The second aspect is the confidence score based on motion during the blind time using events.

% Sec.~\ref{sec:}에서 다룬, implicit motion field는 point cloud와 event 사이에 align을 통해, 3D motion 정보를 담고 있어, 우리는 confidence만을 위한 MLP를 추가로 사용하여 motion confidence를 계산한다.

The implicit motion field from Sec.~\ref{Sec:v3d_ef} contains 3D motion information through alignment between the point cloud and events. We additionally employ an MLP specifically for calculating motion confidence. Box-wise motion confidence is estimated and normalized, resulting in $p_{0 \rightarrow t}=\{p^1_{0 \rightarrow t}, p^2_{0 \rightarrow t}, \ldots ,p^n_{0 \rightarrow t}\} \in [0, 1]$. The final confidence score for box proposal $i$ is calculated by considering both the score at the active timestamp and the motion confidence:
\begin{equation}
\begin{aligned}
p_t^i=p_0^i \cdot p_{0 \rightarrow t}^i.
% \label{equ:final_score}
\end{aligned}
\end{equation}


\subsection{Training Objectives}
% \noindent
% \textbf{Motion Estimation.}

For RGB-LiDAR RPN in the active timestamp, we follow RPN loss from ~\cite{Yan2018SECONDSE, li2023logonet, yin2021center}.
For blind time prediction, box regression loss from \cite{yin2021center} is applied between the predicted 3D box $B_t^i$ and the ground-truth $\hat{B}_t^i$. 
% \noindent
% \textbf{Motion Confidence.}
The target assigned to the motion confidence branch is an IoU-related value, as:
\[
\hat{p}^i_{0 \rightarrow t}(\text{IoU}_i) = 
\begin{cases} 
0 & \text{if IoU}_i < \theta_L, \\
\frac{\text{IoU}_i - \theta_L}{\theta_H - \theta_L} & \text{if } \theta_L \leq \text{IoU}_i < \theta_H, \\
1 & \text{if IoU}_i \geq \theta_H,
\end{cases}
\]
 $\text{IoU}_i$ is the IoU between the $i$-th prediction and the corresponding ground truth box, while $\theta_H$ and $\theta_L$ are foreground and background IoU thresholds. Binary Cross Entropy Loss is exploited here for confidence prediction. The losses of our detect head are computed as:

 \begin{equation}
\begin{aligned}
\mathcal{L}= \mathcal{L}_{RPN}
+\lambda_1 \cdot \mathcal{L}_{reg}
+\lambda_2 \cdot \mathcal{L}_{score}
\end{aligned}
\end{equation}