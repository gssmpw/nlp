\section{Related Works}
\label{sec2}


Within the domain of deep learning, CNNs as introduced in \cite{krizhevsky2017imagenet}, 
have been established as the foundational architecture for a variety of computer vision tasks. 
CNNs adeptly capture localized image features via their convolutional layers, 
securing outstanding achievements in image classification \cite{lecun1989backpropagation}, 
object detection \cite{faster2015towards}, and semantic segmentation \cite{long2015fully}. 
Despite these successes, CNNs encounter challenges in capturing long-range dependencies and integrating global contextual information within images.

The ViT detailed in \cite{dosovitskiy2020image}, represents a novel architectural shift by adopting the Transformer model's \cite{vaswani2017attention} mechanisms 
from NLP and applying them to the computer vision field. 
ViT processes images by segmenting them into patches, treating these as sequence inputs for the Transformer, 
and has demonstrated notable performance across a spectrum of visual tasks. 
This review will concisely examine the models related to CNNs and Vision Transformers, highlighting their evolution, applications, 
and interplay within the landscape of computer vision research.

\subsection{Vision Transformer}
\label{subsec:vit}

The Transformer model, as introduced in \cite{vaswani2017attention}, has achieved remarkable success in NLP.
This success has inspired researchers to explore the application of Transformer concepts in visual tasks. 
Dosovitskiy et al. \cite{dosovitskiy2020image} proposed the Vision Transformer (ViT), a model designed for image recognition.
ViT operates by partitioning an image into a series of patches, which are then processed as sequence data by the Transformer architecture, leading to superior performance in image classification tasks.
Despite these advancements, ViT's training demands a larger dataset and more substantial computational resources compared to CNNs.
The DeiT (Data-efficient Image Transformer) \cite{touvron2021training} addresses this challenge by integrating attention training and knowledge distillation techniques, 
demonstrating that effective training of ViT can be achieved using solely the ImageNet dataset.
DeiT leverages a pre-trained CNN, such as ResNet, in a teacher-student configuration, where the CNN provides guidance to enhance the Transformer's self-attention mechanisms, 
thereby boosting the overall model performance.
The Swin Transformer, an evolution of the ViT, introduces a novel hierarchical structure and a shifted window mechanism, setting it apart from its predecessor \cite{liu2021swin}.
This model optimizes computational efficiency by implementing a local window self-attention mechanism, which confines self-attention calculations to local image windows, 
thereby significantly curtailing the computational demands.
The Swin Transformer has made a mark in the field of computer vision, delivering exceptional results in diverse tasks such as object detection, 
semantic segmentation, image generation, and video action recognition, showcasing its versatility and robustness in handling complex visual challenges.

\subsection{CNN-Transformer Hybrid Model}
\label{subsec:cnn_transformer_hybrid}

The CNN-Transformer hybrid model is an innovative neural network architecture that merges the prowess of CNNs in local feature extraction with the comprehensive 
global context understanding of Transformer models. These models aim to harness the advantages of both architectures to significantly boost performance in visual tasks.

In the Context Vision Transformer (CvT) \cite{hassani2021escaping}, the authors present a distinctive structural approach by incorporating a series of convolutional layers prior 
to each attention mechanism. This design effectively decreases the spatial resolution of feature maps across layers while simultaneously expanding their feature dimensions. 
The authors report that CvT achieves superior performance on the ImageNet-1k dataset with a lower parameter count and reduced computational demand compared to 
the ViT and the DeiT.

Conversely, the Cross-Encoded Image Transformer (CeiT) \cite{yuan2021incorporating} employs an alternative strategy, 
utilizing convolutional layers for initial image downsampling before processing through ViT. 
This method capitalizes on CNNs' proficiency in low-level feature extraction and curtail computational expenses by diminishing the length of the 
image patch sequence subjected to self-attention mechanisms.

\subsection{End to End Image Comparison}
\label{subsec:end_to_end_comparison}

CompressAI, introduced in \cite{begaint2020compressai}, is a PyTorch-based library and evaluation framework 
meticulously crafted to facilitate end-to-end image compression research.
The platform incorporates multiple state-of-the-art end-to-end image compression models\cite{balle2018variational,minnen2018joint,cheng2020learned} that leverage CNNs.
These models have demonstrated compression capabilities competitive with established JPEG and PNG algorithms, 
thereby highlighting the promising prospects of deep learning in advancing image compression technology.
\subsection{Comparison}
\label{subsec:comparison}

ViT and DeiT process images in their original dimensions as inputs to the self-attention layer, 
which results in a significant escalation of computational requirements with increased model complexity and higher image resolutions.
While the Swin Transformer has effectively minimized the number of FLOPs through its pioneering local window mechanism, 
its modifications to the internal structure of ViT might hinder its adaptability for multimodal research and scalability.
Alternative models, such as the CvT, CeiT, and other CNN-Embedding based architectures, 
have adopted the technique of leveraging feature maps derived from CNNs as inputs to the self-attention layer, thereby curtailing computational costs.
Nevertheless, the downsampling inherent in this approach can result in the loss of fine-grained visual information, which is pivotal for tasks that demand high image fidelity, 
such as semantic segmentation and pose estimation. In these pixel-intensive applications, the subtleties of the image are especially critical.

Various approaches within the CNN-Embedding paradigm commonly incorporate feature maps extracted from pre-trained CNN architectures like ResNet-50, 
which are often originally trained on the ImageNet dataset. The efficacy of these models when generalized to datasets outside of ImageNet may be inconsistent, 
thus rendering the performance of ViT model substantially dependent on the proficiency of the CNN-Embedding training process.
Additionally, the CNN-Embedding segment typically necessitates fine-tuning during the ViT training phase. 
This process escalates the computational expense and complicates the attribution of optimization advancements, 
obscuring whether improvements stem from the CNN-Embedding segment or the ViT architecture itself.
The pursuit of novel structure for ViT and the execution of ablation studies are further complicated by this variability. 

In this research, we propose the CI2P-ViT model, integrating end-to-end compression technology derived from CNNs to perform downsampling on input images, 
effectively reducing their dimensions while striving to retain essential visual information. 
This innovative approach allows the ViT to gain a more nuanced understanding of the intricacies present within images.

Distinct from conventional CNN-Embedding techniques, which are often tailored to specific visual tasks, 
the image compression component of CI2P is trained autonomously, enhancing its versatility. 
A notable aspect of the CI2P-ViT model's training regimen is the freezing of the image compression model's parameters. 
This strategic decision minimizes the computational expenses associated with training.

we implement image compression techniques to reduce the volume of image patches processed by the ViT self-attention layer by a factor of four, 
leading to a marked reduction in the model's computational load, measured in FLOPs. 
The integration of the CI2P module introduces CNN inductive biases into the model, enhancing its precision and accelerating the training phase.
When juxtaposed with alternative models like the Swin Transformer and other CVT variants, 
the CI2P-ViT model distinguishes itself by preserving the pristine internal architecture of ViT, eschewing any modifications. 
This strategic approach facilitates the broader application and extension of the Transformer framework within the multimodal research domain.