\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figs/intro_framework.pdf}
    % \captionsetup{aboveskip=-15em, belowskip=-15em}
    \caption{The illustration of a command executing failed and ``polluting'' the environment.}
\label{figs:intro_framework}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figs/intro_example.pdf}
    % \captionsetup{aboveskip=-10em, belowskip=-10em}
    \caption{Examples of the failed command causing ``polluted'' environment.}
\label{figs:intro_example}
\end{figure}


% Code environment configuration is typically considered the first step in running a code repository. When a developer attempts to run the code, the environment needs to be properly configured.
% Configuring code environments is typically the first step for developers to run and test their code. With the advancement of Large Language Models (LLMs) and their applications in automated programming, many techniques now require observation of code behavior and running tests in actual runtime environments to achieve accurate code editing. However, previous approaches often relied on manually configuring environments, writing individual setup scripts for each task, which is both time-consuming and labor-intensive. This method is also highly dependent on the specific system environment, meaning that what works on one operating system (e.g., Windows) may not work on another (e.g., Linux).
Configuring the environment is typically the first step for developers to run and test the code. Meanwhile, Large Language Models (LLMs) are increasingly being integrated into tools such as chatbots and coding assistants, showcasing their potential to accomplish various software engineering tasks~\cite{llm1, llm2, llm3}. As a result, the research community explores how LLMs can be leveraged to assist with more complex real-world tasks in software development. 
Furthermore, LLM-based agents~\cite{agent1, agent2, agent3} are also increasingly being utilized in software engineering.
% For example, some LLM-based agents~\wxc{[lack of the introduction of LLM-based agents?]} are designed to interact with the code repository environment, performing actions like reading, editing files, and running commands to automate various aspects of development.
With the advancement of LLM-based agents and their applications in automated programming, many techniques now require running tests in the runtime environment to acquire more comprehensive information. However, previous approaches often rely on manually configuring environments~\cite{jain2024r2e}, and writing individual configuration scripts for each task, which is both time-consuming and labor-intensive. 
% This method is also highly dependent on the specific system environment, meaning that what works on one operating system (e.g., Windows) may not work on another (e.g., Linux).

We conduct an empirical study with eight professional developers from Internet companies, all with an average of seven years of development experience. The results indicate that when dealing with unfamiliar code repositories, five out of eight developers spend a great amount of time configuring the environment. Moreover, there is a strong willingness among them to use automated tools to alleviate the burden of environment setup. Details of the empirical study are presented in Appendix \ref{manual_experiment}.

In response to the growing complexity and time required for manual environment configuration, containerization tools like Docker have become a good solution. Docker allows developers to create isolated and consistent environments across different systems, thus eliminating the common ``it works on my machine'' problem~\cite{valstar2020using}. Dockerfiles are commonly used to configure Docker environments, which include two parts: the base image and the configuration process. The base image is a pre-packaged environment (e.g., python:3.10) that provides a standardized, isolated environment for development without the need for local deployment. The configuration process involves additional steps performed on top of the base image.

% To address the inconsistency and laborious aspect of manual configuration, containerization tools like Docker have emerged as a powerful solution. Docker allows developers to create isolated and consistent environments across different systems, thus eliminating the ``it works on my machine'' problem~\cite{valstar2020using}. By packaging the application along with its dependencies into a container, Docker ensures that the code runs the same way regardless of where it is executed.

% In software development, inconsistencies often arise when different tools or versions are used, leading to cases where software works on the developer's machine but fails elsewhere. This problem is commonly known as the ``it works on my machine'' issue~\cite{valstar2020using}. To tackle these issues, containerization tools like Docker have emerged as a powerful solution.

Inspired by the idea of the Docker container, we design our LLM agent for environment configuration, \tool, with a fundamental concept: utilize an LLM agent to configure the environment within an isolated Docker container. If the LLM successfully configures the environment, we only need to record the configuration steps and transfer them into a Dockerfile.
During the environment configuration process, we mainly face the following two major challenges:
% and our proposed solutions:

% \textbf{Lack of automated framework for building arbitrary repositories.} In the current environment configuration methods, facing unknown code repositories often lack frameworks and automated configuration tools. Manual configuration is not only time-consuming but also prone to errors. This issue becomes particularly prominent when switching between projects or modifying environments, significantly increasing workload and the risk of errors.

% \textbf{Uncertainty in tracking environment changes.} In the current environment settings, tracking and recording environmental changes is filled with uncertainty. Existing methods often fail to provide clear records of the configuration process, which can lead to incomplete records when errors occur halfway. This can result in the environment being left in an incomplete or inconsistent state, making subsequent debugging and fixing difficult.

\textbf{Ensure the LLM agent configures the environment within a Docker container.} One major challenge is ensuring that the LLM agent can accurately configure the environment inside the Docker container. This involves selecting an appropriate base image, installing dependencies, resolving conflicts, and running tests, etc. If we do not provide the LLM agent with effective tools, the LLM agent will find it difficult to handle the various complex issues encountered during configuration.
% The key objective is to streamline the process to minimize errors and maximize success rates in diverse and unknown code repositories.
% Dual-Environment Architecture
% To achieve this, we implement an \textbf{internal environment} (Docker container sandbox) for environment configuration, allowing the LLM agent to perform six types of actions, including test running and dependency installation. In addition, we develop an \textbf{external environment} to assist in the configuration of the internal environment. It facilitates the action-observation iterations of the LLM agent and supports operations such as changing the base image of the internal environment.

\textbf{Ensure the successful configuration process is recorded and accurately transferred to a runnable Dockerfile.} Another major challenge is maintaining an accurate and comprehensive record of the environment configuration process.
As illustrated in Figure~\ref{figs:intro_framework}, a failed executed command ``\texttt{commandA}'' may cause irreversible "pollution".
Although ``\texttt{commandA}'' fails to run, it still causes "pollution" to the original environment, potentially leading to unexpected changes to packages, files, or directories in the system. To accurately reproduce such system changes, we need to add the ``\texttt{RUN commandA}'' statement in the Dockerfile. However, this will cause the Dockerfile to build fail.

Figure \ref{figs:intro_example} shows two practical examples.
As shown in Figure~\ref{figs:intro_example} (a), when we run ``\texttt{pip install tensorflow-gpu}'', the command fails to execute because the package \texttt{tensorflow-gpu} has been deprecated. However, it still introduces the ``\texttt{python-version}'' package which is not present in the original environment. In Figure~\ref{figs:intro_example} (b), when we run the ``\texttt{rm -rf /path/to/logs}'' command to delete the directory, some files fail to be deleted due to a lack of permission. However, the files that have already been deleted (e.g., \texttt{log1.txt} and \texttt{log2.txt}) are not restored. Such ``pollution'' caused by failed commands may make the environment enter an \textbf{uncertain state}. This state cannot be reached by directly adding commands to the Dockerfile, as doing so would cause the Dockerfile build process to fail.

To address the aforementioned two challenges, we propose \textbf{\tool}~based on \textbf{atomic configuration synthesis}, which consists of a dual-environment architecture and a Dockerfile generator. To the best of our knowledge, this is the first work that leverages the LLM agent for automated Dockerfile generation and environment configuration in a completely automated manner.

The dual-environment architecture comprises both an \textbf{internal} and an \textbf{external} environment. The internal environment serves as a Docker container sandbox, allowing the LLM agent to execute various commands. The external environment assists in configuring the environment in the internal environment. It facilitates the action-observation interactions of the LLM agent and supports operations such as changing the base image of the internal environment. 
In addition, we design a rollback mechanism that performs a rollback when a command fails, preventing the system from entering an uncertain state. This ensures that we can execute commands ``atomically'', meaning each command either executes fully or not at all.

The Dockerfile generator designs a set of rules to transfer the successfully executed commands in the internal environment into statements in the Dockerfile without errors. Combined with the rollback mechanism, we finally ensure that all generated Dockerfiles are successfully built.

% Our paper introduces \tool, an LLM-based agent that is designed to automatically generate the Dockefile to configure the environment for an arbitrary code repository to execute the tests in the repository. To the best of our knowledge, this is the first work that leverages the LLM agent for automated Dockerfile generation and environment configuration in a completely automated manner.~\wxc{[I think the introduction of the method should after the challenges?]}

In summary, the contributions of this paper are as follows:
% For instance, consider an LLM agent attempting to configure a development environment: if a failed command like \texttt{pip install tensorflow-gpu} inadvertently installs partial dependencies (Figure~\ref{figs:intro_example}-Left), subsequent commands may inherit this corrupted state. Crucially, traditional methods naively record all commands into Dockerfiles—including failed ones like \texttt{commandA} in Fig.~\ref{figs:intro_framework}—which then break the entire build process. Second, as shown in Fig.~\ref{figs:intro_example}-Right, even successful commands (e.g., \texttt{rm -rf}) may exhibit system-dependent behaviors, rendering generated Dockerfiles non-portable.
% This involves capturing all necessary steps and commands executed by the LLM agent and converting them into a Dockerfile. This conversion process must be precise to ensure that the recreated environment matches the original setup, thus enabling consistency and reproducibility across different systems and setups.

% \textbf{Low Fault Tolerance in Batch Dependency Installation.} In practical environment configuration, batch dependency installation often faces issues like conflicts or dependency failures. Any failure in one part of the process can interrupt the entire configuration and make subsequent steps difficult to proceed.

$\bullet$ We propose Repo2Run, the first LLM-based agent designed to automate environment configuration and generate Dockerfiles for arbitrary Python repositories, enabling the execution of tests within these repositories.

$\bullet$ We propose atomic configuration synthesis, which includes a dual-environment architecture and a Dockerfile generator, ensuring that all Dockerfiles generated by \tool~are runnable.

$\bullet$ We create a benchmark to evaluate the performance of \tool~by crawling 420 popular Python repositories with unit tests from GitHub released within 2024. \tool~successfully configures the environment for 361 of these repositories, achieving a success rate of 86.0\%, which is 63.9\% higher than the best baseline.