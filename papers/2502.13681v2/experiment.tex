
We evaluate the effectiveness of \tool~on 420 Python code repositories. As the popular option, we select \texttt{gpt-4o-2024-05-13} for subsequent experiments, with the temperature uniformly set to 0.2.

\subsection{Benchmark}

To the best of our knowledge, there is no prior work similar to \tool~that constructs an environment for arbitrary Python repositories.
% Therefore, we create our new benchmark.
To validate the capability of \tool~in environment configuration, we create our new benchmark consisting of selected Python repositories from GitHub based on the following criteria:


$\bullet$ \textbf{Creation date}: To minimize the impact of data contamination, we carefully selected repositories created in 2024, ensuring they are not part of the training data for mainstream large language models.


$\bullet$ \textbf{Star count}: To ensure the quality of the repositories, we only select those with a star count greater than 100.


$\bullet$ \textbf{Test directory}:
% \pc{How about PyTest-based Test Case Presence? This is in line with the second evaluation metrics.}
\texttt{Pytest} is a leading Python testing framework that is compatible with many other Python testing tools, such as \texttt{unittest}. It identifies and runs test files and test functions that are named with a ``\texttt{test\_}'' prefix or a ``\texttt{\_test}'' suffix. Python repositories usually place their test files in the ``\texttt{test}'' or ``\texttt{tests}'' directory. To effectively filter repositories with unit tests, we keep repositories with the ``\texttt{test}'' or ``\texttt{tests}'' directory in their root directory to filter the repositories that most probably have unit tests.
% that the collected repositories have runnable tests.

Through the selection process, we crawl all 449 repositories that meet all the above requirements\footnote{The crawling was performed in December 2024.}. Then, we filter 420 repositories containing at least one unit test. These repositories constitute our benchmark for subsequent experiments. The details of the benchmark are presented in Appendix \ref{benchmark}.


\subsection{Evaluation metrics}


$\bullet$ \textbf{Dockerfile Generation Success Rate (DGSR)}:
It indicates the percentage of attempts where the method successfully generates a \textbf{runnable} Dockerfile. To be considered successful, the generated Dockerfile must be able to build without errors. If the Dockerfile for a code repository successfully builds, it is regarded as a successful generation. Generating runnable Dockerfile is fundamental for successfully configuring the environment.


$\bullet$ \textbf{Environment Configuration Success Rate (ECSR)}:
It represents the percentage of attempts where the method successfully configures environments. For a successful configuration, the generated Dockerfile must not only build successfully but also allow tests to run by ``\texttt{pytest}'' in the Docker container. We are only concerned with whether tests can be executed, regardless of whether they pass or fail, as outcomes of tests may inherently vary within the repository.

\input{tables/compare}


\subsection{Baselines}


$\bullet$ \textbf{pipreqs}\footnote{\url{https://github.com/bndr/pipreqs}}: It is an automated tool that generates a ``\texttt{requirements.txt}'' file by analyzing the import statements in the Python scripts and identifying the necessary dependencies without LLM. Using the \texttt{requirements.txt} file generated by pipreqs, we create a Dockerfile. The detail is provided in Appendix \ref{baseline_settings}.


$\bullet$ \textbf{LLM generator}:
The ``\texttt{README}'' file in a code repository usually contains environment configuration instructions. Therefore, we directly drive the LLM to read the ``\texttt{README}'' file and generate an executable Dockerfile accordingly.


$\bullet$ \textbf{SWE-agent}~\cite{yang2024swe}:
SWE-agent establishes a custom agent-computer interface (ACI) that facilitates the LLM agent's interaction with the repository environment by allowing actions such as reading files, editing files, and executing bash commands. Initially intended as an LLM agent for bug fixing, we preserve its framework and default settings, adjust its prompts, and use it as a baseline.


\subsection{Experimental Results}


% Results are shown in Table \ref{tab:compare}.
The results of different baselines are presented in Table \ref{tab:compare}. We observe that \tool~consistently outperforms other baselines on both DGSR and ECSR. Repo2Run ultimately completed environment configuration for 361 code repositories, achieving an ECSR of 86.0\%. It is 63.9\% higher than the highest rate achieved by other methods,  demonstrating great advantages.
Due to the design of atomic configuration synthesis, \tool~successfully generates Dockerfiles that can be built successfully for all 420 code repositories, which other tools cannot guarantee.

For pipreqs, the main failures come from two reasons. First, generating the requirements.txt fails when there are issues within the repository, such as encoding errors or syntax errors in the files. This happens in 30 repositories (7.1\%). Second, even when requirements.txt is generated, it might not download properly due to package version conflicts. This occurs in 265 repositories (63.1\%). Besides, both the LLM generator and SWE-agent fail to ensure that the generated Dockerfile can be successfully built due to the lack of an ensuring mechanism. Surprisingly, the ability of SWE-agent, a general agent framework, to generate Dockerfiles is even weaker than simply letting the LLM read the ``\texttt{README}'' file. This indicates that a general agent framework cannot guarantee the generation of runnable Dockerfiles. Ensuring mechanisms like atomic configuration synthesis are necessary to effectively use the interactive information from the agent to generate runnable Dockerfiles.

\input{tables/ablation}


\subsection{Ablabtion of~\tool}


% To investigate the impact of different components of \tool on the effectiveness of environment configuration, we conduct two ablation experiments: removing the Dockerfile generator and removing the internal environment, separately. We then evaluate the success rate of them.
To investigate the impacts of the dual-environment architecture and Dockerfile generator separately, as two parts of atomic configuration synthesis, we separately remove each component of them. For the experiment without the dual-environment architecture, we retain only the internal environment's bash commands as the most basic interface and remove all other tools. For the experiment without the Dockerfile generator, we directly instruct the LLM to generate a runnable Dockerfile. 

Experimental result of the ablation study is shown in Table~\ref{tab:ablation}. We observe that removing the dual-environment architecture and retaining only bash commands results in a 7.6\% decrease in DGSR. The main reason for this drop is the removal of rollback and other designs, making the system more prone to entering uncertain states and subsequently failing to reproduce. In addition, ECSR shows a 44.3\% decrease, primarily because the simplification of design makes it more difficult for the LLM agent to complete configuration in the internal environment. Besides, removing the Dockerfile generator directly leads to an 80.5\% drop in DGSR. This indicates that having the LLM directly generate Dockerfiles is unlikely to fully follow the event history, resulting in Dockerfiles that fail to build successfully. This also directly causes a sharp decline in ECSR.

It is also observed that \tool~without the Dockerfile generator is outperformed by the LLM generator. This is because the LLM generator leverages the ``\texttt{README}'' file, which provides a clear, simple and high-level overview of the environment configuration, allowing for more accurate Dockerfile generation. In contrast, the event history-based approach lacks this context, making it harder for the LLM to fully understand the configuration goals. However, the Dockerfile generator effectively utilizes the detailed event history, highlighting the complementary roles of both components of \tool~in generating a reliable Dockerfile.

% To remove the rollback mechanism, we modify the original setup so that upon encountering a command running failed, the LLM agent continues to perform actions in the uncertain state without executing a rollback and record all the commands running in the internal environment.
% To remove the Dockerfile generator, we directly drive the LLM to generate a runnable Dockerfile, rather than generating the Dockerfile based on the actual event stream.
% To remove the internal environment, we retain only the bash commands within the internal environment for the LLM agent to interact with, while removing all other types of actions.



% \subsection{Case study}