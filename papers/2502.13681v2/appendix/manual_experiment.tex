\subsection{Settings}
To ensure a fair comparison among participants, we conducted training and demonstrated examples before the experiment to ensure everyone understood the procedure. Additionally, to minimize discrepancies in time consumption due to network factors, all participants conducted the experiment in the same network environment.

\subsection{Survey}
We selected eight technical staff from internet companies to participate in the experiment and conducted a survey regarding their backgrounds prior to the experiment.
Their development experience ranges from 4 to 11 years, with an average of 7 years in software development and 3.8 years in Python development. Seven participants have experience in complex development projects, while one has experience in multiple small-scale projects.

Regarding environment configuration, three participants indicated that they spend a significant amount of time configuring the environment when faced with an unfamiliar code repository; another three stated that, although they spend a long time, it is generally manageable; two participants reported spending minimal time.

In terms of successful environment configuration in their regular work, three participants mentioned they only fail with extremely complex environments, while five indicated they can build environments for most medium-scale repositories.
As for their confidence in successfully configuring unfamiliar environments, six participants expressed that they are usually successful, and two said they are sometimes successful.

When it comes to the amount of time they are willing to wait to configure an unfamiliar repository, four participants are willing to wait for over 90 minutes, two are willing to wait 40-60 minutes, one is willing to wait 20-40 minutes, and one is only willing to wait 10-20 minutes.

Participants' overall evaluation of configuring code running environments is diverse: one finds it very troublesome with many issues, four find it somewhat troublesome, two indicate moderate difficulty, and one finds it relatively simple.

Additionally, all participants expressed a high or very high willingness to use a tool that could automatically configure the environment for an unfamiliar repository.

\subsection{Experiment guideline}
We request all participants to conduct the experiment following the guidelines below:

\textbf{1. Environment Setup}

Configure the Docker environment. Verify the installation is successful: If installed correctly, you should be able to use the following command (python:3.10 is just an example; select the base image according to your requirements):

\texttt{docker run -it python:3.10 bash}

\textbf{2. Overall Procedure}

Our objective is to install the given package in a Docker container, configure its environment, and be able to run its internal tests. During the process, record the time developers spend configuring the environment and eventually save the logs using docker logs.

\textbf{2.1 Review the Repository to be Configured (Optional)}

Review the GitHub repository that needs to be configured.

\textbf{2.2 Determine the Docker Base Image (Generally start with python:3.10)}

Select the Docker base image based on the repository (all repositories use Python as the main language). Common base images include: Note, in this experiment, it is generally sufficient to use the official Python series images. The main concern is the version; if uncertain, you can start with newer versions like 3.10, 3.11, 3.12, or select the recommended Python version based on the repository's README.

Python series (the number after python indicates the version): python:3.10, python:3.12, python:3.9, python:3.6, python:3.9...
PyTorch series: pytorch/pytorch, pytorch/pytorch:1.9.0-cuda10.2-cudnn7-runtime...
Anaconda series: continuumio/anaconda3...
Note: If you find the selected version is incorrect later, you can exit and reselect.

\textbf{2.3 Create and Enter the Container}

Using the determined base image name (e.g., python:3.10), enter the container with the following command:

\texttt{docker run -it --name mytest python:3.10 bash}

Here, mytest is the container name, recorded for log export later. It can be freely named, just keep track of it to avoid losing it later. Note: If any issues arise here, check if Docker is correctly installed and if the image name is valid, and troubleshoot accordingly.

\textbf{2.4 Install Relevant Tools}

APT tool downloads:

\texttt{apt-get update \&\& apt-get install -y curl}

Download pytest:

\texttt{pip install pytest}

\textbf{2.5 Download the Repository}

Select the repository to be configured and download its GitHub repository to a location (generally directly in the root directory):

\texttt{git clone https://github.com/\{full\_name\}.git}

\textbf{2.6 Configure the Environment}

Now, use your skills to configure: First, enter the downloaded file directory, for example:

\texttt{cd wddbfs}

Switch to the specified branch SHA (refer to the corresponding SHA of the repository), for example:

\texttt{git checkout 5c68aa}

Our goal is to successfully run pytest (not necessarily to pass all tests, just to run them). A simple criterion is to successfully run:

\texttt{pytest --collect-only -q}

At this point, you can use your experience and information from the repository documentation and debugging error messages to configure. However, there are a few restrictions:

Do not directly edit the test files! (Files starting with test\_ or ending with \_test.py).
Do not directly delete test files!
Editing the original repository files is not recommended.

During this process, you may perform various operations, including but not limited to pip, apt-get, and other tool downloads, as well as searching online or using GPT for debugging help.

Additionally, if there are long download times requiring waiting, you may decide according to your situation whether to leave this running and do other things (just donâ€™t forget about this task).

\textbf{2.7 Completion and Logging}

A task can conclude in two scenarios:

Scenario One: If ``\texttt{pytest --collect-only -q}'' runs without issues, you can then execute pytest. If pytest completes successfully, the task is done.
Scenario Two: If you feel the package is extremely difficult to configure, for example exceeding your patience threshold (refer to your usual development habits), you may also terminate.

Once finished, input exit to exit. Make sure to save your output logs with the following command (replace {container\_name} with the container name you recorded earlier, if you forget, you can use docker ps to check):

\texttt{docker logs {container\_name} -t > wddbfs.log}

\textbf{2.8 Fill Out the Form and Record Information}

You need to fill out the form according to your feelings.
\begin{longtable}{|l|p{10cm}|}
\hline
\textbf{Question} & \textbf{Description} \\
\hline
Is it successful? & Yes or No - whether the task successfully passed ``\texttt{pytest --collect-only -q}'' without errors and eventually ran ``\texttt{pytest}''. \\
\hline
Final base image used & E.g., python:3.10 \\
\hline
Reason for failure & Summarize the main reasons for failure (if failed), including:
\begin{itemize}
    \item Long download time
    \item Difficulty handling repository dependencies
    \item Unresolvable bug
    \item Errors in repository tests
    \item Lengthy test durations
    \item Other reasons (please specify)
\end{itemize} \\
\hline
Waiting time & Approximate value, indicating the time spent waiting for dependencies to download. Does not include time spent on decision-making and research. Provide an approximate range: 
\begin{itemize}
    \item <3 minutes
    \item 3-5 minutes
    \item 6-10 minutes
    \item 11-20 minutes
    \item 20-40 minutes
    \item 40-60 minutes
    \item 60-90 minutes
    \item 90+ minutes
\end{itemize} \\
\hline
Longest time-consuming process & Describe the most time-consuming configuration process, such as:
\begin{itemize}
    \item Downloading a specific dependency
    \item Resolving a specific error
    \item Incorrect Python version selection
\end{itemize} \\
\hline
Tolerance level & Based on your subjective feeling and development experience. Rate the process (1-5):
\begin{itemize}
    \item 1: Extremely unbearable
    \item 2: Somewhat unbearable, but manageable
    \item 3: Neutral, tolerable
    \item 4: Comfortable, no significant discomfort
    \item 5: Very comfortable, highly satisfactory configuration experience
\end{itemize} \\
\hline
Configuration difficulty & Based on the complexity of the configuration process (ignoring time spent): Rate the process (1-5):
\begin{itemize}
    \item 1: Very simple, completed with intuition and experience, no reference materials needed
    \item 2: Fairly simple, referred to basic materials (e.g., README), simple overall steps
    \item 3: Moderate difficulty, encountered some issues, but manageable
    \item 4: Difficult, required extensive debugging and configuration
    \item 5: Very difficult, needed numerous references and encountered unresolved or time-consuming issues
\end{itemize} \\
\hline
Materials referenced & List the materials used for reference (e.g., README, internal configuration files, online searches, GPT). If none, mention ``Directory". \\
\hline
Biggest challenge during the process & Describe the most troublesome aspect of the configuration process, such as long wait times, unclear error messages, dependency version conflicts, inability to find required software versions, etc. \\
\hline
\end{longtable}

\textbf{Example 1}:

\begin{longtable}{|l|l|}
\hline
\textbf{Question} & \textbf{Answer} \\ 
\hline
Is it successful? & Yes \\ 
\hline
Final base image used & python:3.10 \\ 
\hline
Reason for failure & Successful \\ 
\hline
Waiting time & 3-5 minutes \\ 
\hline
Longest time-consuming process & Downloading poetry \\ 
\hline
Tolerance level & 5 \\ 
\hline
Configuration difficulty & 1 \\ 
\hline
Materials referenced & Directory \\ 
\hline
Biggest challenge during the process & Waiting for configuration and installation \\ 
\hline
\end{longtable}

\textbf{Example 2}:

\begin{longtable}{|l|l|}
\hline
\textbf{Question} & \textbf{Answer} \\ 
\hline
Is it successful? & No \\ 
\hline
Final base image used & python:3.11 \\ 
\hline
Reason for failure & Unresolvable bug, provide bug image \\ 
\hline
Waiting time & 40-60 minutes \\ 
\hline
Longest time-consuming process & Resolving FileNotFoundError, ImportError, incorrect Python version selection \\ 
\hline
Tolerance level & 2 \\ 
\hline
Configuration difficulty & 5 \\ 
\hline
Materials referenced & README, GPT, StackOverflow \\ 
\hline
Biggest challenge during the process & Dependency version conflicts, long wait times \\ 
\hline
\end{longtable}

\subsection{Repository assignment}
We randomly assigned each participant four unique code repositories that were successfully configured by \tool. Additionally, each participant was assigned two code repositories that \tool~failed to configure. To avoid chance occurrences, each failed repository was assigned to two different participants. Below is the list of selections:

\textbf{Successfully configured}:

[alexwlchan/safari-webarchiver, ManiMozaffar/aioclock, mixedbread-ai/batched, mobiusml/gemlite, circlemind-ai/fast-graphrag, knowsuchagency/promptic, mbodiai/embodied-agents, modelscope/agentscope, Adibvafa/CodonTransformer, kennethreitz/simplemind, lmstudio-ai/venvstacks, mlecauchois/micrograd-cuda, IST-DASLab/PanzaMail, MetaGLM/zhipuai-sdk-python-v4, openai/mle-bench, RealOrangeOne/django-tasks, basf/MolPipeline, dai-motoki/zoltraak, lucidrains/alphafold3-pytorch, mistralai/mistral-common, BMPixel/moffee, DataformerAI/dataformer, jahwag/ClaudeSync, volfpeter/htmy, Genentech/gReLU, OpenNLPLab/lightning-attention, paradigmxyz/spice, reagento/dishka, arcee-ai/fastmlx, KyanChen/RSMamba, neuralmagic/guidellm, simonw/files-to-prompt]

\textbf{Failed to configure}:

[zhuqinfeng1999/Samba, dongxuyue/Open-ReplaceAnything, LazyAGI/LazyLLM, jialuechen/deepfolio, KOSASIH/pi-nexus-autonomous-banking-network, AARG-FAN/Yolo\_for\_Wukong, plinder-org/plinder, expectedparrot/edsl]

\subsection{Results}
Based on the times shown in the logs, we calculated that the average configuration time for each repository is 21.33 minutes. Furthermore, none of the repositories that \tool~failed to configure were successfully configured manually. Additionally, in the manual experiment, five environments that were successfully configured by \tool~were not successfully configured, representing 15.6\% of the total successfully configured assignments.

