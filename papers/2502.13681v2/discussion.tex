

\subsection{Tool usage frequency}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figs/tool_frequency.pdf}

    \caption{Tool usage frequency of the success configuration.}
\label{figs:tool_frequency}

\end{figure}



As shown in Figure~\ref{figs:tool_frequency}, we analyze the invocation times of various action types in 361 successfully configured projects, including the five actions within the internal environment and the action of changing the base image.  Bash commands are the most frequently used action, as they encompass the majority of instructions. Additionally, we observe that the LLM agent tends to call dependency installation quite frequently, averaging about 18 times per configuration, which means roughly 18 dependencies are installed per configuration on average. Moreover, the LLM agent calls test running approximately 3.5 times per configuration on average, which typically helps the agent better identify issues. Among successful configurations, we see 48 instances of changing the base image, accounting for 13.3\%~ of success cases, indicating that the initial selection of the base image is often incorrect and requires subsequent adjustments.


\subsection{Time consumption}


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/frequency.pdf}

\caption{The time distribution of each successful configuration.}

\label{figs:frequency}
\end{figure}

Figure~\ref{figs:frequency} shows the distribution of time spent successfully configuring 361 code repositories. The average time for successful configuration using \tool~is 29.03 minutes. 111 (30.7\%) of the repositories are successfully configured in less than 10 minutes. Additionally, our empirical study through sampling indicates an average manual configuration time of 21.33 minutes (Appendix~\ref{manual_experiment}). Considering network differences and randomness, \tool~achieves a time consumption comparable to manual configuration. Additionally, for complex issues, \tool~shows greater advantages over manual configuration. \tool~successfully configures all the cases that were manually successful in our empirical study. Moreover, the cases where Repo2Run fails are also not successfully configured manually.


\subsection{Case study}


For the code repositories that fail to configure, we manually inspect the reasons for failures and find that most are due to issues within the repositories themselves. Figure \ref{figs:case_study}~illustrates a ``module not found'' error when configuring the repository ``\texttt{jialuechen/deepfolio}''. In this case, the issue arises from the absence of updating the unit tests. The test file ``\texttt{test\_stats.py}'' attempts to import modules from ``\texttt{deepfolio.stats}'', but ``\texttt{stats}'' does not exist in ``\texttt{deepfolio}''. Consequently, no matter how the LLM agent operates, it cannot directly run this test. This highlights the importance for developers to continuously update existing unit tests as the code repository evolves.

