\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% \usepackage{tikz}
\usepackage{amsmath}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{array}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{marvosym}
\usepackage{adjustbox}
\usepackage{booktabs}
% \usepackage{caption}
% \setlist{leftmargin=8mm}
% \setlength{\itemsep}{0.4em}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
% \usepackage{cite}

\usepackage{color}
\usepackage{xcolor}
\def\red#1{\textcolor{red}{#1}}
\def\orange#1{\textcolor{orange}{#1}}
\def\blue#1{\textcolor{blue}{#1}}
\long\def\comment#1{}
\def\x{\bm{x}}
\def\t{\bm{t}}
\def\ie{$i.e.$}
\def\eg{$e.g.$}
\def\etal{\textit{et al.} }

\newtheorem{case}{Case}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
% \thanks{Corresponding author: Yiming Li.}
\title{REFINE: Inversion-Free Backdoor Defense via Model Reprogramming}
\author{Yukun Chen$^{1,2,}$\thanks{The first two authors contributed equally to this work. \textsuperscript{\Letter} Corresponding author: Yiming Li. Our code is available at \url{https://github.com/WhitolfChen/REFINE} and \href{https://github.com/THUYimingLi/BackdoorBox}{\texttt{BackdoorBox}}.}\ \ , Shuo Shao$^{1,2,*}$, Enhao Huang$^{1,2}$, Yiming Li$^{1,3,}$\textsuperscript{\Letter}, \textbf{Pin-Yu Chen}$^{4}$, \\ 
\textbf{Zhan Qin}$^{1,2}$, \ \textbf{Kui Ren}$^{1,2}$ \\
\textsuperscript{1} State Key Laboratory of Blockchain and Data Security, Zhejiang University \\
\textsuperscript{2} Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security \\
\textsuperscript{3} Nanyang Technological University \ \textsuperscript{4} IBM Research \\
\texttt{\{yukunchen, shaoshuo\_ss, huangenhao, qinzhan, kuiren\}@zju.edu.cn;}\\
\texttt{liyiming.tech@gmail.com; pin-yu.chen@ibm.com}
% \footnote{\textsuperscript{\Letter} Corresponding author: Yiming Li.}
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}

% \newcommand{\fix}{\marginpar{FIX}}
% \newcommand{\new}{\marginpar{NEW}}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Backdoor attacks on deep neural networks (DNNs) have emerged as a significant security threat, allowing adversaries to implant hidden malicious behaviors during the model training phase. Pre-processing-based defense, which is one of the most important defense paradigms, typically focuses on input transformations or backdoor trigger inversion (BTI) to deactivate or eliminate embedded backdoor triggers during the inference process. However, these methods suffer from inherent limitations: transformation-based defenses often fail to balance model utility and defense performance, while BTI-based defenses struggle to accurately reconstruct trigger patterns without prior knowledge. In this paper, we propose REFINE, an inversion-free backdoor defense method based on model reprogramming. REFINE consists of two key components: \textbf{(1)} an input transformation module that disrupts both benign and backdoor patterns, generating new benign features; and \textbf{(2)} an output remapping module that redefines the model's output domain to guide the input transformations effectively. By further integrating supervised contrastive loss, REFINE enhances the defense capabilities while maintaining model utility. Extensive experiments on various benchmark datasets demonstrate the effectiveness of our REFINE and its resistance to potential adaptive attacks. 
% Our code is available at \url{https://github.com/WhitolfChen/REFINE}.
% Our code is available at \href{https://github.com/THUYimingLi/BackdoorBox}{\texttt{BackdoorBox}} and \href{https://github.com/WhitolfChen/REFINE}{Github repository}.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Deep neural networks (DNNs) have been widely deployed across various domains~\citep{he2023finer, liu2024differentially, he2024difficulty, zhang2024text}. To develop a high-performance DNN, developers necessitate not only high-quality data samples but also substantial computational resources. Consequently, developers frequently and directly rely on third-party models for follow-up development. However, the utilization of third-party DNNs can introduce security threats, particularly with regard to backdoor attacks~\citep{gu2019badnets, li2022few, dong2023one,gao2024backdoor}.

Backdoor attacks aim to implant hidden backdoors into the model during training~\citep{gu2019badnets}. After the attack, the backdoored model functions normally on benign inputs. However, when a specific trigger is present, the model will produce intentionally incorrect outputs. Backdoor attacks pose a severe threat to critical applications where model reliability is essential, highlighting the urgent need for effective backdoor defense strategies to safeguard AI systems~\citep{li2022backdoor}. 

Currently, several backdoor defenses~\citep{huang2022backdoor, li2024purifying, li2024nearest, hou2024ibd} have been developed to tackle the threat of backdoor attacks. Among these, pre-processing-based defenses~\citep{villarreal2020confoc,qiu2021deepsweep} are particularly notable because they only apply certain modifications to input samples before model inference, without altering the original model structure and weights. Currently, there are two main types of pre-processing-based defenses. The first type of defenses relies on input transformations~\citep{li2021backdoor,sun2023mask, shi2023black}. These defenses aim to mismatch or eliminate potential trigger patterns by performing certain transformations to the input samples.
%However, these methods often fall short of achieving the desired defensive outcomes. 
The second type is based on backdoor trigger inversion (BTI)~\citep{wang2019neural, wang2023unicorn, xu2024towards}, which attempts to reconstruct the attacker's trigger patterns and remove them before the data is processed by the model. 

In this paper, we revisit the aforementioned pre-processing-based backdoor defenses. We reveal that they both have intrinsic limitations. Specifically, transformation-based defenses face a trade-off between model utility and defense performance: more extensive transformations can achieve lower attack success rates but may negatively impact the model's benign accuracy. This occurs because these defenses lack information about backdoor-related features, forcing them to modify all features indiscriminately, including those critical for benign accuracy. The close coupling of benign and backdoor features makes it difficult to apply stronger transformations without significantly compromising model utility. On the other hand, BTI-based defenses can `break' the trade-off by first obtaining the information of backdoor triggers via trigger inversion. However, due to the inherent difficulties of BTI (\eg, lack of prior knowledge about the implanted backdoor and poisoned samples), existing BTI methods struggle to accurately invert trigger patterns. This limitation makes it difficult to purify the backdoored input from the poisoned domain to the benign domain, leading to limited effectiveness of BTI-based defenses. Accordingly, an intriguing and important question arises: \textit{Could we break the curse of this trade-off without relying on backdoor trigger inversion?}
% On the other hand, BTI-based defenses can break the trade-off by first obtaining the \blue{information} of backdoor triggers via trigger inversion but often fall short of achieving the desired defensive outcomes. This is mostly because their effectiveness heavily relies on the quality of inverted trigger patterns, whereas the inverted trigger patterns are often significantly different from the ones predefined by the adversary due to the inherent difficulties of BTI (\eg, lack of \blue{prior knowledge} about the implanted backdoor and poisoned samples). Accordingly, an intriguing and important question arises: \textit{Could we break the curse of this trade-off without relying on backdoor trigger inversion?}
% On the other hand, BTI-based defenses can break the trade-off by first obtaining the information of backdoor triggers via trigger inversion. However, due to the inherent difficulties of BTI (\eg, lack of prior knowledge about the implanted backdoor and poisoned samples), existing BTI methods struggle to invert the ground-truth trigger. This limitation makes it difficult to purify the backdoor input from the poisoned domain to the benign domain, causing BTI-based defenses to fall short of achieving the desired outcomes.

%Specifically, for defenses that use backdoor trigger inversion (BTI) to remove trigger patterns from suspicious samples, the lack of prior knowledge about the backdoor attack, such as trigger type and target class, often results in the inverted trigger patterns being significantly different from those predefined by the attacker. 
%For defenses that rely on input transformations, the defender attempts to eliminate trigger patterns from the input samples while fixing the model's output domain, defined by the order of output classes in the final layer. To ensure that the predicted class of the sample remains consistent before and after defense, it is crucial to maintain the integrity of the benign features throughout the transformation process. However, since benign patterns are tightly coupled with trigger patterns on the input domain, it is challenging for the defender to apply more extensive transformations without compromising the benign accuracy of the original model.




To tackle the above challenge, we first provide a theoretical analysis showing that the effectiveness of backdoor defenses is bounded by the distance between output features before and after pre-processing. Accordingly, the ineffectiveness of existing defenses is mostly due to their underlying assumption of having a fixed output domain. Based on the above understandings, inspired by model reprogramming~\citep{chen2024model}, we propose REFINE, a \underline{RE}programming-based \underline{I}nversion-\underline{F}ree backdoor defe\underline{N}se m\underline{E}thod, as shown in Figure~\ref{fig:inference}. By allowing changes to the output domain, REFINE can significantly alter the input domain while largely maintaining model accuracy. Specifically, our REFINE involves an input transformation module and an output mapping module to reprogram the backdoored model and eliminate backdoor triggers. We utilize a trainable autoencoder as the input transformation module and redefine the model's output domain through a hard-coded remapping function. This adjustment to the output domain enables more extensive and effective input transformations. Besides, we enhance our method by applying supervised contrastive loss~\citep{khosla2020supervised}, ensuring that transformed samples of the same class remain closely aligned. %Our method can significantly reduce the success rate of backdoor attacks while preserving the benign accuracy of the original model. The defense process of our REFINE is illustrated in Figure~\ref{fig:inference}.

\begin{figure}[!t]
    \vspace{-3.5em}
    \centering
\includegraphics[width=0.90\linewidth]{figures/inference.pdf}
    \vspace{-1em}
    \caption{The defense process of our REFINE. The label remapping in the model's output domain significantly enhances the flexibility of input transformations while maintaining consistent sample predictions, effectively mitigating the trade-off often encountered in transformation-based pre-processing defenses. During prediction, the input sequentially passes through the well-trained input transformation module, the fixed backdoored model, and the pre-defined output mapping module, ultimately yielding the expected ground-truth (instead of the malicious target) label.}
    \label{fig:inference}
    \vspace{-1.5em}
\end{figure}

Our contributions are three-fold. \textbf{(1)} We revisit existing pre-processing-based backdoor defenses and reveal their limitations. \textbf{(2)} Based on the empirical and theoretical analysis, we propose a simple yet effective defense (\ie, REFINE). Our REFINE introduces trainable input transformation and output mapping modules for reprogramming and incorporates cross-entropy and supervised contrastive losses to enhance defense performance. \textbf{(3)} Extensive experiments on diverse benchmark datasets demonstrate the effectiveness of REFINE and its resistance to potential adaptive attacks.

%The main contributions of this paper are three-fold. \textbf{(1)} We reveal two key issues with existing pre-processing-based backdoor defenses: the difficulty in recovering the true trigger patterns and the challenge of applying substantial and effective transformations to the input samples, along with their primary causes. \textbf{(2)} Based on the understanding and the theoretical analysis, we propose REFINE, which introduces a trainable input transformation module and an output remapping module before and after the model for reprogramming, thereby enhancing backdoor defense performance. \textbf{(3)} Extensive experiments on standard benchmark datasets demonstrate the effectiveness of REFINE.

% \vspace{-5em}
% \newpage
% \vspace{-5em}
\section{Background}
\label{sec:related}

\subsection{Backdoor Attacks}
Backdoor attacks~\citep{gao2020backdoor, li2022backdoor} involve embedding hidden malicious behaviors into a model, typically by manipulating the training process with a small subset of poisoned data containing adversary-specified trigger patterns. Whenever the trigger appears in the input during inference, the model executes the attacker’s intended behavior, such as misclassifying the input to a target label. In the absence of the trigger, the model functions normally, rendering the backdoor hard to detect. Backdoor attacks pose serious threats in AI-enpowered systems.
%These attacks present serious risks, particularly in systems where AI models are deployed for critical decision-making.

The formulation of backdoor attacks is typically presented as follows. Given a training dataset $\mathcal{D}=\{(\bm{x}_i, \bm{y}_i)\}_{i=1}^N$, the attacker manipulates the training process of the model $\mathcal{F}$ by introducing a poisoned subset $\tilde{\mathcal{D}}=\{(\tilde{\bm{x}}_i, \bm{y}_t)\}_{i=1}^M$, where $\tilde{\bm{x}}_i = G(\bm{x}_i)$ with $G(\cdot)$ as a certain trigger injection function and $\bm{y}_t$ being the chosen target label, or by altering the training loss directly. During inference, the model behaves normally on benign samples, where $\bm{y}_j = \mathcal{F}(\bm{x}_j)$, while exhibiting backdoor behavior on poisoned samples, such as misclassifying to the target label $\bm{y}_t = \mathcal{F}(G(\bm{x}_j))$.

Generally, existing attacks can be classified into two types: \textbf{(1)} \emph{Visible backdoor attacks}, which typically employ trigger patterns that are visible to humans, such as specific white-black squares~\citep{gu2019badnets}, physical attacks~\citep{li2021backdoor}, or adaptive attacks~\citep{qi2023revisiting}. \textbf{(2)} \emph{Invisible backdoor attacks}, which introduce imperceptible triggers to enhance the stealth and evasiveness of attacks~\citep{chen2017targeted}, including sample-specific attacks~\citep{li2021invisible}, trainable noise attacks~\citep{doan2021lira}, and sample rotation attacks~\citep{xu2023batt}). 
% More details are in Appendix~\ref{sec:appen:background}.

%(1) \textit{Visible Backdoor Attacks.} These attacks typically employ patterns that are visible to humans as triggers. BadNets~\citep{gu2019badnets} is the first backdoor attack technique that injects simple but visually noticeable patterns into the training data, such as white squares or specific marks. Subsequently, researchers have proposed various visible attacks, including physical attacks~\citet{li2021backdoor} in real-world scenarios, adaptive attacks~\citet{qi2023revisiting}, and robust clean-label attacks~\citet{gao2023not}. (2) \textit{Invisible Backdoor Attacks.} To enhance the stealth of backdoor attacks, ~\citet{chen2017targeted} was the first to introduce the use of triggers that are imperceptible to humans, aiming to evade detection by basic data filtering techniques or human inspection. After that, sample-specific attacks~\citep{nguyen2021wanet, li2021invisible}, trainable noise attacks~\citep{doan2021lira}, and sample rotation attacks~\citep{xu2023batt} have also garnered significant research attention. Recently, a few works~\citep{wang2022bppattack, liang2024badclip, wei2024pointncbw, chou2024villandiffusion, yang2024stealthy} also provided novel and comprehensive discussions on invisible triggers from various perspectives. More details are in Appendix \ref{sec:appen:background}.

\subsection{Backdoor Defenses}

Currently, there are various backdoor defense methods designed to mitigate backdoor threats. These methods can generally be divided into three main paradigms~\citep{li2022backdoor}: \textbf{(1)} \emph{pre-processing-based defenses}~\citep{liu2017neural, li2021backdoor, shi2023black}. \textbf{(2)} \emph{backdoor elimination}~\citep{li2021neural, huang2022backdoor,xu2024towards}, which involves adjusting model parameters through fine-tuning, pruning or reconstruction to remove the backdoor. \textbf{(3)} \emph{trigger elimination}, also known as testing sample filtering~\citep{gao2019strip, javaheripi2020cleann, li2023ntd}. In this paper, we focus on pre-processing-based defenses since we consider scenarios where only fixed third-party models are accessible and defenders require to obtain the correct final results of all samples. 
% Detailed discussion about the backdoor defenses can be found in Appendix \ref{sec:appen:background}.

\noindent\textbf{Pre-processing-based Defenses.}
Generally, pre-processing-based defenses can be categorized into two types: (1) \textit{Transformation-based defenses}. Classical methods~\citep{liu2017neural, li2021backdoor, qiu2021deepsweep} typically involve applying simple transformations to input, aiming to disrupt trigger patterns and prevent the model from exhibiting backdoor behavior. More Recently, many methods have leveraged the powerful reconstruction capabilities of generative models, such as diffusion models~\citep{shi2023black, may2023salient} and masked autoencoders~\citep{sun2023mask}, intending to retain the original benign features while minimizing the presence of backdoor-related features. However, there is a trade-off between removing backdoor patterns and restoring benign patterns, which remains a pressing issue to address. (2) \textit{BTI-based defenses}~\citep{wang2019neural, xu2024towards, wang2023unicorn}, which focus on inverting the pre-injected triggers and utilizing them to purify the input samples. However, these methods may face issues with inaccuracies in the inverted triggers, which may lead to suboptimal purification of the input. How to design an effective pre-processing-based defense is still an important open question.


\subsection{Model Reprogramming}
Model reprogramming~\citep{kloberdanz2021improved, neekhara2022cross, jing2023deep} is a technique that extends the application of a pre-trained model from a source domain to a target domain. This technique involves adapting the input from the target domain to match that of the source domain. 
%This approach does not require fine-tuning or modifying the parameters of the source model; instead, it only involves adapting the input from the target domain to match that of the source domain. 
Specifically, model reprogramming introduces an input transformation module $\mathcal{T}(\bm{x}|\bm{\theta})$ and an output mapping module $\mathcal{M}(\bm{y}|\bm{\beta})$, where $\bm{\theta}$ and $\bm{\beta}$ are the trainable parameters of these two modules, respectively~\citep{chen2024model}. Given a pre-trained model $\mathcal{F}(\cdot)$ and an input sample $\bm{x}$, model reprogramming first transforms $\bm{x}$ to $\tilde{\bm{x}}$ leveraging the input transformation module. Then input $\tilde{\bm{x}}$ into the pre-trained model $\mathcal{F}(\cdot)$ and get the output $\tilde{\bm{y}}=\mathcal{F}(\tilde{\bm{x}})$. Finally, the output mapping module is used to map $\tilde{\bm{y}}$ into the final output $\bm{y}$. Through fine-tuning the input transformation module and the output mapping module (\ie, optimizing $\bm{\theta}$ and $\bm{\beta}$), model reprogramming can efficiently turn the pre-trained model from the source domain to a target domain. Compared to transfer learning, model reprogramming does not necessitate modifying the parameters of the pre-trained model. As such, it is more efficient and flexible. More details about related work are in Appendix~\ref{sec:appen:background}.
% As such, it is more efficient and flexible. Detailed descriptions of model reprogramming are in Appendix~\ref{sec:appen:background}.
% More details about model reprogramming can be found in Appendix~\ref{sec:appen:background}.
%Following the most classical setting in existing model reprogramming methods~\citep{chen2024model}, there is a well-developed pre-trained model $\mathcal{F}$ with fixed parameters and a target domain training dataset $\mathcal{D}^t=\{(\bm{x}^t_i,\bm{y}^t_i\})\}_{i=1}^N$. Before inputting the target domain data $\bm{x}^t_i$ into $\mathcal{F}$, the transformed input data $\bm{x}^s_i=\mathcal{T}(\bm{x}^t_i|\theta)$ needs to be obtained, where $\theta$ represents the set of trainable parameters for the input transformation. After obtaining the prediction output $\bm{y}^s_i=\mathcal{F}(\bm{x}^s_i)$, it needs to be mapped to the target domain output $\bm{y}^t_i=\mathcal{M}(\bm{y}^s_i|\beta)$, where $\beta$ denotes the set of trainable parameters for the output mapping. Typically, the output mapping is a specified source-target label mapping function. Therefore, the final output can be expressed as: $\bm{y}^t_i = \mathcal{M}(\mathcal{F}(\mathcal{T}(\bm{x}^t_i|\theta))|\beta)$. 

%Inspired by this, we can enhance the flexibility of input transformations while keeping the original model's accuracy intact by leveraging model reprogramming under the condition that the model parameters remain fixed. More details about model reprogramming can be found in Appendix~\ref{sec:appen:background}.

\begin{figure}[t]
    \vspace{-3.5em}
    \centering
\includegraphics[width=0.95\linewidth]{figures/revisiting1.pdf}
    \vspace{-0.6em}
    \caption{(a-1)\&(b-1): The ASR and BA for ShrinkPad (the first row) and BDMAE (the second row) with different transformation intensities. (a-2)$\sim$(a-4)\&(b-2)$\sim$(b-4): The t-SNE plots of the features of benign and backdoor samples under no defense (dubbed ``ND''), low transformation intensity (dubbed ``Low''), and high transformation intensity (dubbed ``High''). Squares and solid circles represent the centroids of benign sample distributions and backdoor sample distributions. As the transformation intensity increases, the features of benign samples deviate from the origin. The results demonstrate the tradeoff faced by the transformation-based backdoor defense methods.}
    \label{fig:revisiting1}
    \vspace{-1em}
\end{figure}

\section{Revisiting Existing Pre-processing-based Backdoor Defenses}
\label{sec:revisiting}

%In this section, we first introduce the threat model of pre-processing-based backdoor defenses. Subsequently, we provide empirical studies on both transformation-based and backdoor-trigger-inversion-based (BTI-based) defenses to analyze their intrinsic limitations.
% analyze the limitations of the existing BTI-based and transformation-based pre-processing defenses separately.

\subsection{Threat Model}
This paper focuses on tackling the issue of pre-trained backdoored models via pre-processing-based backdoor defense. The defender may buy or acquire a pre-trained model from third-party platforms. However, there exists a threat that the pre-trained model is backdoored. Due to the limitations of computational resources, the defender seeks to mitigate the backdoor in an efficient and low-cost way (\eg, without altering the parameters of the pre-trained model). Following prior works~\citep{liu2017neural, li2021backdoor}, we make the following assumptions. For adversaries, they can implant the backdoor into the pre-trained model in any way (\eg, by poisoning the training data or intervening in the training process). For defenders, we assume that they have access to an \emph{unlabeled} dataset that is independent and identically distributed to the training dataset of the pre-trained model. 
%In this paper, we focus on pre-processing-based defenses against backdoor attacks. Defenders may acquire DNN classification models from untrusted third-party platforms and seek to mitigate backdoor threats without altering the original model parameters, often due to local computational limitations or time constraints. Pre-processing defenses offer a promising approach by eliminating or mismatching potential trigger patterns in the input, thereby preventing the model from exhibiting backdoor behavior. Generally, we assume that the defender has access to an unlabeled dataset of the same size as the training set of the third-party suspicious model, which can be utilized to design or train a pre-processing framework.

\subsection{The Limitations of Transformation-based Defenses}

Transformation-based defenses aim to mismatch or eliminate triggers by applying specific transformations to test samples. This type of defense method can be categorized into two types: random perturbations and generator reconstruction. Specifically, random perturbations involve the defender mismatching the trigger pattern through techniques such as scaling or rotation, while generator reconstruction leverages a pre-trained generative model to erase the trigger pattern. However, \emph{the transformation-based backdoor defense methods face a trade-off between the utility of the model and the effectiveness of the backdoor elimination}, making them ineffective in practice.

\label{subsec:limit_trans}
% \textbf{Settings.}
In this section, we present the empirical results to support the above claim. We implement two representative transformation-based methods, ShrinkPad~\citep{li2021backdoor} (dubbed ``SP'') and BDMAE~\citep{sun2023mask} (dubbed ``BD''), to defend the BadNets attack~\citep{gu2019badnets} on CIFAR-10. Specifically, ShrinkPad applies simple spatial transformations to the input, while BDMAE employs a trained masked autoencoder for data cleansing. We use ``Pad Size'' (dubbed ``S''), which refers to the padding size applied around shrunk images, and ``Mask Ratio'' (dubbed ``R''), which represents the masking rate applied to images before reconstruction, to control the transformation intensity for ShrinkPad and BDMAE, respectively. We aim to analyze how these transformations impact the model's benign accuracy (BA) and attack success rate (ASR) of the backdoor. Additionally, we treat the original model as a feature extractor. We then visualize how transformation intensity affects the differences in feature distribution between benign and poisoned samples of the same class.

As shown in Figure~\ref{fig:revisiting1} (a-1) and (b-1), increasing the intensity of input transformation, which enlarges the feature distance between the original and transformed samples, reduces the backdoor ASR. However, it also leads to a decline in the model's BA. As depicted in Figure~\ref{fig:revisiting1} (a-2)$\sim$(a-4) and (b-2)$\sim$(b-4), higher transformation intensity causes greater changes in the feature distribution of backdoored samples within the same class, indicating that higher transformation levels effectively mismatch or remove trigger patterns. Nevertheless, the difficulty of decoupling benign patterns from backdoor patterns in the input domain results in that such transformations inevitably affect the benign features. It causes a shift in the centroid of the benign sample feature distribution (visualized as solid circles in Figure~\ref{fig:revisiting1}). The primary cause of this trade-off is the consistent output domain of the DNN before and after defenses, which forces the input transformation module to achieve two conflicting goals: \textbf{(1)} removing trigger patterns effectively and \textbf{(2)} maintaining benign patterns of samples while ensuring their correct classification. This conflict inspires us to consider that adjusting the model's output domain may help mitigate this issue.



\subsection{The Limitations of BTI-based Defenses}

BTI-based defenses can `break' the trade-off between model utility and defense performance by incorporating the information of backdoor attacks via trigger inversion. In the pre-processing-based defense paradigm, BTI-based defenses typically involve two steps: trigger inversion and input purification. Specifically, the defender first exploits several data to invert the pre-injected trigger, then trains a generator to purify the input samples using the inverted trigger. The effectiveness of BTI-based defenses highly relies on the quality of the inverted trigger. However, we argue that \emph{the inherent challenge of achieving high-quality trigger inversion, due to the lack of prior knowledge, hinders effective input purification}, ultimately limiting the performance of BTI-based defenses.


We implement the state-of-the-art BTI-based defense, BTI-DBF~\citep{xu2024towards}, to invert the backdoor triggers of BadNets~\citep{gu2019badnets} and Blended~\citep{chen2017targeted} on CIFAR-10. As shown in the Figure~\ref{fig:revisiting2}, BTI-DBF effectively reverses the trigger pattern of the BadNets attack and purifies the poisoned sample. However, for the Blended attack, the trigger pattern reversed by BTI-DBF significantly differs from the pre-injected one, leading to poor purification of the poisoned sample. This illustrates that the effectiveness of BTI-based defenses largely depends on the quality of trigger inversion, which is the inherent challenge of such defenses. Moreover, BTI-based defenses often identify ``pseudo-triggers'' inherent in DNNs \citep{ya2023towards}, which usually arise from the model's vulnerability to adversarial perturbations. When defenders attempt to use such triggers to train purification generators, they may disrupt the benign features of the samples, while leaving the backdoor patterns largely unaffected. If the quality and authenticity of the inverted trigger patterns cannot be guaranteed, BTI-based defenses may potentially yield adverse outcomes. 

In conclusion, achieving BTI is a challenging endeavor due to the lack of prior knowledge about the implanted backdoor and poisoned samples, highlighting the need for an inversion-free backdoor defense to resolve this trade-off.

\begin{figure}[!t]
    \vspace{-3em}
    \centering
\includegraphics[width=0.6\linewidth]{figures/revisiting2.pdf}
    \vspace{-0.6em}
    \caption{The visualization of BTI-DBF in inverting backdoor triggers under both BadNets and Blended attacks. We display the poisoned, inverted, and purified samples, respectively. }
    \label{fig:revisiting2}
    \vspace{-1em}
\end{figure}


\section{Methodology}

\subsection{Motivation and Inspiration}

% Theorem here.
In Section~\ref{sec:revisiting}, we empirically evaluate existing pre-processing-based defenses and analyze why they are ineffective. In this section, we present a theoretical analysis and the inspiration to design an effective and efficient backdoor defense method. Given a pre-processing method $\mathcal{T}(\cdot)$ and a pre-trained model $\mathcal{F}(\cdot)$, we have the following theorem.
% \vspace{-0.6em}
\begin{theorem}
\label{thm:defense}
    Given a $K$-class pre-trained deep learning model $\mathcal{F}(\cdot)=s(f(\cdot))$ where $s(\cdot)$ is the softmax function and $f(\cdot)$ is the feature extractor, and a pre-processing method $\mathcal{T}(\cdot)$, $\bm{x}$ is the data from a specific domain $\mathcal{D}$ (\ie, $\bm{x}\sim \mathcal{D}$) and $\tilde{\bm{x}}=\mathcal{T}(\bm{x})\sim \tilde{\mathcal{D}}$. Let $\Phi_{\mathcal{D}}(\bm{x})$ and $\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})$ denotes the probability density function of $\mathcal{D}$ and $\tilde{\mathcal{D}}$, we have
    \begin{equation}
    \label{eq:theorem}
        \mathbb{E}_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\|\mathcal{F}(\bm{x})-\mathcal{F}(\tilde{\bm{x}})\|_2 \leq 2\alpha\sqrt{K}\cdot\mathcal{W}_1(\mu, \tilde{\mu}),
    \end{equation}
    where $\mathcal{W}_1(\mu, \tilde{\mu})$ is the Wasserstein-1 distance between $\mu$ and $\tilde{\mu}$, $\mu$ and $\tilde{\mu}$ are the probability measures of the representations $f(\bm{x})$ and $f(\tilde{\bm{x}})$, and $\alpha = \max [\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}}|\bm{x}) / \Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})]$.
\end{theorem}
\vspace{-0.3em}

Theorem~\ref{thm:defense} indicates why existing defenses are ineffective. Assuming $\bm{x}$ is the poisoned sample, the left part of Eq.~(\ref{eq:theorem}) means the distance between the prediction of the transformed poisoned sample and the original poisoned sample. Theorem~\ref{thm:defense} demonstrates that the distance is bounded by the Wasserstein-1 distance between the probability measures $\mu, \tilde{\mu}$ of the output representations. Thus, to maintain model utility, existing pre-processing-based defenses tend to retain the output representations, limiting their effectiveness against backdoors. Otherwise, they have to compromise the model utility to achieve greater backdoor defense performance. The proof is in Appendix~\ref{sec:appen:proof}.

Following the above theorem, we can enhance the upper bound by increasing the distance between $\mu,\tilde{\mu}$. Inspired by model reprogramming techniques~\citep{chen2024model}, we propose REFINE, a reprogramming-based inversion-free backdoor defense method. Our REFINE can significantly transform the input domain to destroy trigger patterns while maintaining model utility for it also changes the output domain. Specifically, we introduce an input transformation module to modify inputs, and an output mapping module to remap original classes to new shuffled ones. We also employ a supervised contrastive loss to further enlarge the distances among different classes. The technical details of our REFINE are illustrated in the following parts.

% \subsection{Preliminaries}
% \noindent\textbf{Poisoning-based Backdoor Attacks.}
% There is a benign dataset $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N$ containing $N$ samples, where $x_{i}\in\mathcal{X}$ denotes the $i$-th sample and $y_{i}\in\mathcal{Y}=\{1, 2, 3, ...,K\}$ represents the label of the $i$-th sample, with $K$ being the number of classes. The adversary selects a subset $\mathcal{D}_{adv}$ from $\mathcal{D}$ to poison, i.e., $\mathcal{D}_{adv}=\{(\tilde{\bm{x}}, y')|\tilde{\bm{x}}=\mathcal{F}_{X}(x), y'=\mathcal{F}_{Y}(y)\}$, where the function $\mathcal{F}_X(\cdot)$ and $\mathcal{F}_Y(\cdot)$ represent the poisoned sample generation function and the poisoned label generation function, respectively. The dataset $\mathcal{D}_p$ used to train the backdoored model is then created by combining the benign subset $\mathcal{D}_b$ that was not selected by the adversary with $\mathcal{D}_{adv}$, i.e., $\mathcal{D}_p = \mathcal{D}_{adv}\cup\mathcal{D}_b$. Additionally, $\gamma=\mathcal{D}_{adv} / \mathcal{D}$ represents the poisoning rate.

% \noindent\textbf{Model Reprogramming.}
% Following the most classical setting in existing model reprogramming methods~\citep{chen2024model}, there is a well-developed pre-trained model $\mathcal{F}_s$ with fixed parameters and a target domain training dataset $\mathcal{D}_t=\{(x_t^{(i)},y_t^{(i)})\}_{i=1}^N$. Before inputting the target domain data $x_t^{(i)}$ into $\mathcal{F}_s$, the transformed input data $x_s^{(i)}=\mathcal{T}(x_t^{(i)}|\theta)$ needs to be obtained, where $\theta$ represents the set of trainable parameters for the input transformation. After obtaining the prediction output $y_s^{(i)}=\mathcal{F}_s(x_s^{(i)})$, it needs to be mapped to the target domain output $y_t^{(i)}=\mathcal{M}(y_s^{(i)}|\beta)$, where $\beta$ denotes the set of trainable parameters for the output mapping. Typically, the output mapping is a specified source-target label mapping function. Therefore, the final output can be expressed as: 
% $$y_s^{(i)} = \mathcal{M}(\mathcal{F}_s(\mathcal{T}(x_t^{(i)}|\theta))|\beta)$$

% \noindent\textbf{Threat Model.}
% In this paper, we focus on pre-processing-based backdoor defense methods. The defender has partial access to a third-party model, including the ability to obtain confidence scores for all output classes and the gradient values of the model's parameters during inference. However, the defender cannot retrain the model or modify its parameters; the model remains fixed throughout the defense process. Besides, assume that the defender has access to an unlabeled benign dataset of the same size as the training set of the backdoored source model.

% \noindent\textbf{Defender's Goals.}
% Generally, the defender have to achieve two key objectives: effectiveness and efficiency. \textit{Effectiveness} requires the defense method to successfully remove potential backdoor triggers from the input data. This reduces the attack success rate while ensuring that the model performs normally on all inputs and maintains accuracy close to that of the original model. \textit{Efficiency} means the defense method should have reasonable computational overhead. The training and inference of the pre-processing module must be completed within acceptable time and resource limits, well below the cost of training the original model from scatch.

\subsection{REFINE: REprogramming-based Inversion-Free backdoor defeNse mEthod}
% \subsubsection{Overview}
In general, REFINE consists of two essential components: \textbf{(1)} the input transformation module $\mathcal{T}$, which disrupts the benign and backdoor patterns of input samples through transformations and generates new benign features; \textbf{(2)} the label mapping module $\mathcal{M}$, which formulates the specified source-target hard-coded label remapping function and maps the original classes to new shuffled classes. Additionally, we integrate the cross-entropy loss $\mathcal{L}_{ce}$ and the supervised contrastive loss $\mathcal{L}_{sup}$ to steer the optimization of $\mathcal{T}$. The illustration of our REFINE is shown in Figure~\ref{fig:pipeline}.

\begin{figure}[t]
    \vspace{-3em}
    \centering
 \includegraphics[width=0.9\linewidth]{figures/pipeline.pdf}
    \vspace{-0.4em}
    \caption{The main optimization pipeline of our REFINE. There are two main components: input transformation module $\mathcal{T}$ and output mapping module $\mathcal{M}$. Specifically, after obtaining the fixed pre-trained model, the defender first specifies a particular hard-coded mapping $\mathcal{M}$ and then optimizes $\mathcal{T}$ guided by the loss function $\mathcal{L}$, using the unlabeled benign dataset. The loss function $\mathcal{L}$ consists of the cross-entropy loss $\mathcal{L}_{ce}$ which aims to maintain the model's utility, and the supervised contrastive loss $\mathcal{L}_{sup}$ to enhance the defense capability via forcing orderly sample aggregation.}
    \label{fig:pipeline}
    \vspace{-10pt}
\end{figure}

\subsubsection{Input Transformation Module}
To effectively alter potential trigger patterns in the input samples, we need to modify the input domain of the original model. Traditional model reprogramming methods~\citep{elsayed2019adversarial, tsai2020transfer} add the optimized universal adversarial perturbation around the input samples, while trigger patterns still remain intact on backdoored images to some extent. In contrast, we utilize a trainable autoencoder (e.g., UNet) as the foundational structure for our input transformation module. Arguably, this module not only preserves the consistency of sample dimension before and after transformation, but also affords greater flexibility in sample manipulation compared to conventional reprogramming methods. Upon inputting a batch of data, the input transformation module will encode the pixel features from the images and then decode them to produce new samples. The transformed samples $\tilde{\bm{X}}$ can be described as follows:
\begin{equation}
    \tilde{\bm{X}} = \mathcal{T}(\bm{X}, \theta),
\end{equation}
where $\bm{X}$ is a batch of input samples, and $\mathcal{T}(\cdot, \theta)$ is the input transformation module with $\theta$ as its trainable parameters. During this transformation process, both benign and backdoor patterns are disarranged, effectively removing potential triggers and causing the generation of new benign features orderly clustered by their respective classes.

\subsubsection{Output Mapping Module}
Once the input samples are transformed into new samples via the input transformation module, they are subsequently processed by the original backdoored model, which generates confidence scores for each class, as expressed below:
\begin{equation}
    \tilde{\bm{Y}} = \mathcal{F}(\tilde{\bm{X}}),
\end{equation}
where $\mathcal{F}(\cdot)$ is the original backdoored model. As demonstrated in Section~\ref{subsec:limit_trans}, fixing the model's output domain leads to a trade-off between model utility and defense performance. To address this issue, we introduce an output mapping module at the model's output end, aiming to alter the output domain and mitigate the aforementioned challenges. Specifically, the output mapping module redefines the class order of the model's output layer, which hard-codes a one-to-one label remapping function $f_L:\tilde{l}\mapsto l$, where $\tilde{l}, l \in L, \tilde{l} \neq l$, $L$ is the set of labels. The confidence scores generated by the original model can be remapped into new scores through $\mathcal{M}$, as follows:
\begin{equation}
    {\bm{Y}} = \mathcal{M}(\tilde{\bm{Y}}).
\end{equation}
%where $\mathcal{M}(\cdot)$ is the output mapping module with a pre-defined hard-code label remapping function. 
The final predictions for the samples can be derived from the confidence scores $\bm{Y}$ outputted by $\mathcal{M}$.

\subsubsection{Optimizing REFINE Modules}
To maximize the flexibility of input transformations for removing trigger patterns while maintaining the original model's accuracy, we incorporate two crucial loss functions, the cross-entropy loss and the supervised contrastive loss, to guide the optimization of the input transformation module. The formulation of the combined loss function can be expressed as follows:
\begin{equation}
\label{eq:loss}
    \mathop{\min}\limits_{\theta}\mathcal{L}_{refine} = \mathcal{L}_{ce} + \lambda\mathcal{L}_{sup}.
\end{equation}
In Eq.~(\ref{eq:loss}), $\mathcal{L}_{ce}$ and $\mathcal{L}_{sup}$ indicate the cross-entropy loss and the supervised contrastive loss, respectively. $\lambda$ is a scalar temperature parameter, and $\theta$ represents the set of parameters in the input transformation module to be optimized during training. Since Theorem~\ref{thm:defense} does not guarantee the model performance on clean samples, adding $\mathcal{L}_{ce}$ to maintain the utility of the model is necessary.

In our threat model, the dataset available to the defender is unlabeled. Therefore, before calculating these loss functions, it is necessary to obtain the pseudo-labels $\Bar{\bm{Y}}$ for the current batch of unlabeled samples $\bm{X}$, predicted by the original model (without any additional modules), as follows:
\begin{equation}
    \Bar{\bm{Y}} = \mathop{\arg\max}(\mathcal{F}(\bm{X})).
\end{equation}

\textbf{Leveraging Cross-entropy Loss to Maintain the Utility.}
Due to the substantial modification of the original model's output domain facilitated by the output mapping module, the input transformation module is no longer constrained by the requirement to preserve the original benign features of the samples. Nevertheless, the model must retain its original performance within the new output domain, which necessitates the employment of cross-entropy loss to effectively guide the sample transformation process. The cross-entropy loss is typically formalized as follows:
\begin{equation}
    \mathcal{L}_{ce}=-\frac1N\sum_{i=1}^N\Bar{\bm{y}}_{i}\log(\bm{y}_{i}),
\end{equation}
where $N$ represents the number of samples in the current data batch $\bm{X}$. $\Bar{\bm{y}}_{i}\in\Bar{\bm{Y}}$ denotes the pseudo-label for sample $\bm{x}_i\in\bm{X}$ (typically a one-hot encoded vector), and $\bm{y}_{i}\in\bm{Y}$ indicates the predicted probability remapped by the output mapping module for sample $\bm{x}_i$.

\textbf{Utilizing Supervised Contrastive Loss to Enhance Backdoor Defense.}
%Guided by the cross-entropy loss, the input transformation module is able to effectively eliminate potential trigger patterns within the samples. However, 
Arguably, relying solely on cross-entropy loss is insufficient to maintain the original model's benign accuracy and mitigate the backdoor. Therefore, we introduce supervised contrastive loss~\citep{khosla2020supervised}, where ``supervised'' refers to the original model as the supervisor. Specifically, the supervised contrastive loss aims to ensure that features of transformed samples from the same class are more similar, while those from different classes are further apart. It can be defined as follows.
\begin{equation}
    \mathcal{L}_{sup} = \sum_{i \in I}\frac{-1}{|P(i)|}\sum_{p\in P(i)}\log\frac{\exp\left(\tilde{\bm{x}}_i\boldsymbol{\cdot}\tilde{\bm{x}}_p/\tau\right)}{\sum_{a\in A(i)}\exp\left(\tilde{\bm{x}}_i\boldsymbol{\cdot}\tilde{\bm{x}}_a/\tau\right)}, 
\end{equation}
where $I\equiv\{1, 2, ..., N\}$ represents indices of all samples in current data batch, $\tilde{\bm{x}}_{i} = \mathcal{T}(\bm{x}_{i}, \theta) \in \tilde{\bm{X}}$, the $\boldsymbol{\cdot}$ symbol denotes the inner (dot) product, $\tau$ is a scalar temperature parameter, and $A(i)\equiv I\setminus\{i\}$. The set $P(i) \equiv \{p\in A(i) : \Bar{\bm{y}}_p = \Bar{\bm{y}}_i\}$ contain indices of all positives in the batch distinct from $i$, and $|P(i)|$ is its cardinality. The pseudo-code for the optimization process can be found in Appendix~\ref{sec:appen:code}.

\subsubsection{Utilizing REFINE for Model Inference}
During the model inference phase, we can apply the aforementioned well-trained modules to achieve high-performance and secure predictions. The input samples are sequentially processed through the input transformation module $\mathcal{T}(\cdot, \theta)$, the original pre-trained model $\mathcal{F}(\cdot)$, and the output mapping module $\mathcal{M}(\cdot)$. This process ultimately yields the predicted confidence scores, with all parameters remaining constant. The inference process can be formally expressed as follows.
\begin{equation}
    \bm{y} = \mathcal{M}(\mathcal{F}(\mathcal{T}(\bm{x}, \theta))),
\end{equation}
where $\bm{x}$ represents the sample to be predicted. The detailed process is illustrated in Figure \ref{fig:inference}.

% In the optimization phase of REFINE, the defender optimizes the input transformation module using a benign dataset, as detailed in Algorithm \ref{refine-algo}. Specifically, REFINE aims to achieve two objectives: (1) reducing the attack success rate (ASR) of the backdoor in the source model; (2) maintaining the benign accuracy (BA) of the source model.


% \noindent\textbf{Reduce ASR.}
% We aim for a significant difference between the input samples before and after transformation to effectively eliminate backdoor features. Therefore, we use the output mapping module and cross-entropy loss to guide the input transformation module in generating new samples with different classes from the original ones. Specifically, the output mapping module redefines the class order of the model's output layer, which hard-codes a one-to-one label remapping function: 
% $$f_L:l_S\mapsto l_T$$

% where $l_S, l_T \in L, l_S \neq l_T$, $L$ is the set of labels. Additionally, the cross-entropy loss $\mathcal{L}_{ce}$ is calculated between the mapped outputs, representing the predicted class confidence of the transformed samples, and the pseudo-labels $\hat{\mathcal{Y}}$ of the original samples.


% \noindent\textbf{Preserve BA.}
% The new model, composed of the input transformation module (to be trained), the output mapping module (with fixed mapping), and the source model (with fixed parameters), needs to reconverge under the new output layer. For the propose of making the new model accurately predict the input samples, we not only use cross-entropy loss to guide the optimization process but also employ supervised contrastive loss~\citep{khosla2020supervised} to further constrain it. Specifically, the supervised contrastive loss aims to ensure that features of transformed samples from the same class are more similar, while those from different classes are further apart. It can be defined as follows:

% $$\mathcal{L}_{sup} = \sum_{i\in I}\frac{-1}{|P(i)|}\sum_{p\in P(i)}\log\frac{\exp\left(\boldsymbol{z}_i\boldsymbol{\cdot}\boldsymbol{z}_p/\tau\right)}{\sum_{a\in A(i)}\exp\left(\boldsymbol{z}_i\boldsymbol{\cdot}\boldsymbol{z}_a/\tau\right)}$$

% Here, $I = \{(\boldsymbol{x}_i, \boldsymbol{y}_i)\}_{i=1}^M$ represents the set of all samples in the current data batch. $\boldsymbol{z}_{\boldsymbol{l}} = Input-Transform(\boldsymbol{x}_{\boldsymbol{l}})$, the $\boldsymbol{\cdot}$ symbol denotes the inner (dot) product, $\tau\in\mathcal{R}^+$ is a scalar temperature parameter, and $A(i)\equiv I\setminus(\boldsymbol{x}_i, \boldsymbol{y}_i)$. The set $P(i) \equiv \{p\in A(i) : \boldsymbol{y}_p = \boldsymbol{y}_i\}$ contain indices of all positives in the batch distinct from $i$, and $|P(i)|$ is its cardinality. 

% In summary, when optimizing $Input-Transform(\cdot, \theta)$, the cross-entropy loss and the supervised contrastive loss will jointly guide the gradient descent optimization of $\theta$, i.e., 
% $$\mathop{\arg\min}\limits_{\theta}\mathcal{L} = \mathcal{L}_{ce} + \lambda\mathcal{L}_{sup}$$
% where $\lambda$ is a scalar temperature parameter. 

\section{Experiments}
\label{sec:evaluation}

In this section, we evaluate the effectiveness of our REFINE compared with different existing backdoor defenses. We also conduct an ablation study and evaluate the resistance to potential adaptive attacks. The analysis of the overhead of REFINE is in Appendix~\ref{sec:appen:overhead} and the implementation of REFINE in the black-box scenario is in Appendix~\ref{sec:appen:black-box}.

\subsection{Experimental Settings}
\label{subsec:settings}
\textbf{Datasets and Models.}
We conduct experiments on two classical benchmark datasets, including CIFAR-10 ~\citep{krizhevsky2009learning} and (a subset of) ImageNet~\citep{deng2009imagenet} containing 50 classes. We evaluated our method with ResNet-18~\citep{he2016deep} on both datasets. We also validate the effectiveness of REFINE on other models in Appendix~\ref{sec:appen:ablation}. Note that our goal is to evaluate the effectiveness of backdoor defense methods instead of training a SOTA model. Therefore, the benign accuracies of our models may be lower than the SOTA models. We exploit U-Net~\citep{ronneberger2015u} as the structure of the input transformation module. 

\textbf{Attack Setup.}
We utilize 7 representative advanced backdoor attacks, including \textbf{(1)} BadNets~\citep{gu2019badnets}, \textbf{(2)} Blended~\citep{chen2017targeted}, \textbf{(3)} WaNet~\citep{nguyen2021wanet}, \textbf{(4)} PhysicalBA (dubbed ‘Physical’)~\citep{li2021backdoor}, \textbf{(5)} BATT~\citep{xu2023batt}, \textbf{(6)} LabelConsistent (dubbed ‘LC’)~\citep{turner2019label}, and \textbf{(7)} Adaptive-Patch (dubbed ‘Adaptive’)~\citep{qi2023revisiting}, to comprehensively evaluate the performance of different defenses. 
%We implemented these methods using the open-source BackdoorBox~\citep{li2023backdoorbox} framework, setting the poisoning rate to 10\% for all attacks. Besides, the attack target is set to be the 0th label. 

\textbf{Defense Setup.}
We compare the defense performance of REFINE with both types of pre-processing-based defenses. For transformation-based defenses, we utilize three advanced methods, including \textbf{(1)} ShrinkPad~\citep{li2021backdoor}, \textbf{(2)} BDMAE~\citep{sun2023mask}, \textbf{(3)} ZIP~\citep{shi2023black}. For BTI-based defenses, we employ three methods as baseline, including \textbf{(1)} Neural Cleanse (dubbed `NC')~\citep{wang2019neural}, \textbf{(2)} UNICORN~\citep{wang2023unicorn}, \textbf{(3)} BTI-DBF(P)~\citep{xu2024towards}. 
%For all defense methods, we assume that the defender has access to an unlabeled dataset of the same size as the training set of the backdoored source model.

\textbf{Evaluation Metrics.} 
Consistent with the standard evaluation metrics in backdoor-related studies~\citep{li2022backdoor}, we utilize benign accuracy (BA) and attack success rate (ASR) to assess all defense methods. BA and ASR are the accuracies of the benign samples and the poisoned samples, respectively. An effective defense is indicated by a higher BA and a lower ASR. 

\subsection{Main Results}
As shown in Tables \ref{tab:trans}-\ref{tab:bti}, our REFINE successfully mitigates backdoor threats in all cases while preserving high benign accuracy. Specifically, the ASRs of our method are lower than 3\% ($<2\%$ in most cases). For the BA, the models under REFINE experience less than 3\% drop on the CIFAR-10 dataset compared to the undefended models. On the ImageNet dataset, the BA even improves, due to the increased depth of the original models introduced by the input transformation module. In contrast, other baseline defenses may fail in certain cases, with BA drop or ASR $>10\%$. 
% Specifically, for the Adaptive-Patch attack~\citep{qi2023revisiting}, all baseline defenses perform poorly, because of the difficulty of inverting the adaptive trigger patterns for BTI-based defenses or disrupting the trigger patterns for transformation-based defenses.

% \textbf{Comparing REFINE with BTI-based Pre-processing Defenses.}
% As shown in Table \ref{tab:bti}

% \textbf{Comparing REFINE with Transformation-based Pre-processing Defenses.}
% As shown in Table \ref{tab:trans}, our REFINE is capable of successfully removing trigger patterns through sample pre-processing across all cases, preventing backdoor behaviors while preserving a high benign accuracy of the model. 



\begin{table*}[!t]
    \vspace{-25pt}
    \tabcolsep=3mm
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{The performance (\%) of REFINE and the transformation-based backdoor defenses. The best results are \textbf{boldfaced}, while all failed cases (BA drop or ASR $> 10\%$) are marked in \red{red}.}
    \vspace{-0.8em}
    \label{tab:trans}
    \scalebox{0.76}{
        \begin{tabular}{cccccccccccc}
        \toprule[1.5pt]
        \multirow{2}{*}{Dataset} 
        & Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{ShrinkPad} & \multicolumn{2}{c}{BDMAE} & \multicolumn{2}{c}{ZIP} & \multicolumn{2}{c}{\textbf{REFINE}} \\
        \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}\cmidrule(lr){11-12}
        & Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        \multirow{7}{*}{CIFAR-10} 
        & BadNets & 91.24 & 100 & 84.51 & \red{13.37} & 89.53 & 3.18 & 81.95 & \red{19.06} & \textbf{90.43} & \textbf{0.78}  \\
        & Blended & 91.04 & 100 & 83.95 & 5.94 & 89.08 & \red{84.80} & 81.54 & 3.72 & \textbf{89.85} & \textbf{1.73}  \\
        & WaNet & 91.15 & 99.97 & 84.45 & \red{34.79} & 87.45 & \red{99.92} & 81.79 & 7.58 & \textbf{90.33} & \textbf{0.97}  \\
        & Physical & 93.77 & 99.99 & 90.07 & \red{13.67} & \textbf{93.07} & 3.76 & \red{78.32} & \red{25.24} & 90.82 & \textbf{1.69}  \\
        & BATT & 92.48 & 100 & 86.13 & \red{100} & \textbf{91.71} & \red{100} & \red{82.27} & \red{98.89} & 90.54 & \textbf{1.21}  \\
        & LC & 92.12 & 95.95 & 85.87 & 9.61 & 90.18 & 4.62 & 81.93 & \red{90.08} & \textbf{90.97} & \textbf{0.80}  \\
        & Adaptive & 91.34 & 97.17 & 84.42 & 9.34 & 89.14 & \red{10.30} & 81.65 & \red{78.49} & \textbf{90.30} & \textbf{0.81}  \\
        \midrule
        \multirow{7}{*}{ImageNet} 
        & BadNets & 65.42 & 99.55 & 61.44 & 2.65 & \red{53.64} & 3.51 & 59.12 & \red{11.43} & \textbf{66.27} & \textbf{1.81}  \\
        & Blended & 66.15 & 98.93 & 59.84 & \red{24.53} & \red{54.80} & \red{96.08} & 59.32 & \red{92.98} & \textbf{66.59} & \textbf{1.11}  \\
        & WaNet & 67.11 & 98.81 & 59.44 & \red{40.12} & \red{52.12} & \red{94.82} & 57.92 & \textbf{0.82} & \textbf{66.23} & 1.36  \\
        & Physical & 71.64 & 99.80 & \textbf{71.80} & \red{56.73} & \red{58.40} & 8.98 & 64.00 & \red{17.63} & 67.11 & \textbf{1.97}  \\
        & BATT & 67.76 & 100 & \textbf{70.40} & \red{100} & 58.88 & \red{100} & 65.92 & \red{98.57} & 66.19 & \textbf{2.71}  \\
        & LC & 67.44 & 80.96 & 61.48 & 0.86 & \red{54.84} & \red{10.65} & 60.24 & \red{77.02} & \textbf{66.43} & \textbf{0}  \\
        & Adaptive & 66.76 & 93.20 & 62.44 & 6.37 & \red{56.16} & \red{69.40} & 61.28 & \red{94.57} & \textbf{66.99} & \textbf{1.48}  \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
    % \vspace{-8pt}
\end{table*}


\begin{table*}[t]
    % \vspace{-10pt}
    \tabcolsep=3mm
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{The performance (\%) of REFINE and the BTI-based backdoor defenses. The best results are \textbf{boldfaced}, while all failed cases (BA drop or ASR $> 10\%$) are marked in \red{red}.}
    \label{tab:bti}
    \vspace{-0.8em}
    \scalebox{0.76}{
        \begin{tabular}{cccccccccccc}
        \toprule[1.5pt]
        \multirow{2}{*}{Dataset} & Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{NC} & \multicolumn{2}{c}{UNICORN} & \multicolumn{2}{c}{BTI-DBF(P)} & \multicolumn{2}{c}{\textbf{REFINE}} \\
        \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}\cmidrule(lr){11-12}
        & Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        \multirow{7}{*}{CIFAR-10} 
        & BadNets & 91.24 & 100 & \red{76.44} & \red{37.40} & 86.08 & \red{22.96} & 89.14 & 5.60 & \textbf{90.43} & \textbf{0.78}  \\
        & Blended & 91.04 & 100 & 87.28 & \red{89.47} & 84.09 & \red{53.46} & 87.44 & \red{61.52} & \textbf{89.85} & \textbf{1.73}  \\
        & WaNet & 91.15 & 99.97 & 83.81 & 6.24 & 85.59 & 6.67 & 88.64 & 4.37 & \textbf{90.33} & \textbf{0.97}  \\
        & Physical & 93.77 & 99.99 & 89.00 & \red{52.36} & 90.04 & \red{41.04} & \textbf{91.74} & 9.53 & 90.82 & \textbf{1.69}  \\
        & BATT & 92.48 & 100 & \red{72.28} & 5.88 & \red{74.99} & \textbf{0.87} & 89.89 & 4.80 & \textbf{90.54} & 1.21  \\
        & LC & 92.12 & 95.95 & 83.60 & \red{38.66} & \red{75.10} & 1.89 & 89.52 & \red{88.92} & \textbf{90.97} & \textbf{0.80}  \\
        & Adaptive & 91.34 & 97.17 & 85.45 & \red{31.49} & \red{68.79} & 9.59 & 88.94 & \red{45.99} & \textbf{90.30} & \textbf{0.81}  \\
        \midrule
        \multirow{7}{*}{ImageNet} 
        & BadNets & 65.42 & 99.55 & 63.44 & \red{61.93} & \red{51.96} & \red{88.90} & 64.32 & 6.17 & \textbf{66.27} & \textbf{1.81}  \\
        & Blended & 66.15 & 98.93 & 62.68 & \red{96.90} & 60.64 & \red{98.00} & 65.44 & \red{97.67} & \textbf{66.59} & \textbf{1.11}  \\
        & WaNet & 67.11 & 98.81 & 62.20 & \red{91.80} & 61.64 & \red{94.86} & 65.56 & \red{92.17} & \textbf{66.23} & \textbf{1.36}  \\
        & Physical & 71.64 & 99.80 & 71.00 & \red{98.74} & 67.00 & \red{56.26} & \textbf{73.60} & 7.60 & 67.11 & \textbf{1.97}  \\
        & BATT & 67.76 & 100 & 62.92 & \textbf{0.65} & 68.56 & \red{41.86} & \textbf{72.00} & 6.94 & 66.19 & 2.71  \\
        & LC & 67.44 & 80.96 & 62.88 & \red{66.99} & 58.92 & \red{28.52} & 65.96 & \red{73.81} & \textbf{66.43} & \textbf{0}  \\
        & Adaptive & 66.76 & 93.20 & 62.80 & \red{91.92} & 61.04 & \red{90.13} & \textbf{67.32} & \red{93.35} & 66.99 & \textbf{1.48}  \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-10pt}
\end{table*}

\subsection{Ablation Study}
% \textbf{Settings.}
There are three important components in our methods, including \textbf{(1)} input transformation method, \textbf{(2)} hard-coded remapping function (HRF for short) in the output mapping module, and \textbf{(3)} supervised contrastive loss (SCL for short) of transformed samples. In this section, we present an ablation study on the former two modules and verify their effectiveness. We also test different architectures of the input transformation module and conduct additional ablation studies in Appendix~\ref{sec:appen:ablation}.
%The experiments are conducted on the CIFAR-10 dataset. Additional ablation studies about important hyper-parameters can be found in Appendix~\ref{sec:appen:ablation}.%Specifically, we conduct experiments on CIFAR-10. Unless otherwise specified, all settings are the same as those stated in Section \ref{subsec:settings}. 

% \textbf{Results.}
As shown in Table~\ref{tab:ablation}, we evaluate the defense performance of REFINE without the hard-coded remapping function (w/o HRF) or without the supervised contrastive loss (w/o SCL). Experimental results indicate that without the hard-coded remapping function, REFINE successfully preserves the BA of the original model, but struggles to reduce the ASR of the backdoor. This is because, without the hard-coded remapping function, the output domain of the model remains unchanged. Subsequently, it encounters the same trade-off problem as other transformation-based defenses, and is difficult to find a balance between transformation intensity and defense performance. Also, in the absence of supervised contrastive loss, REFINE can effectively reduce ASR with the help of the hard-coded remapping function. However, it encounters difficulties in restoring the BA of the original model, which may adversely affect the model's inference capabilities.

% When REFINE is implemented without the hard-coded remapping function, meaning the output domain of the model remains unchanged, it encounters the same trade-off problem as other transformation-based pre-processing defenses. Therefore, it is difficult to find a balance between transformation extent and defense performance. In the absence of the supervised contrastive loss, while REFINE can effectively reduce ASR with the help of the hard-coded remapping function, it encounters difficulties in restoring BA of the original model, which may adversely affect the model's inference capabilities.

\begin{table}[t]
    \vspace{-25pt}
    \tabcolsep=4mm
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{The performance (\%) of REFINE with/without the hard-coded remapping function (HRF) or with/without the supervised contrastive loss (SCL).}
    \label{tab:ablation}
    \vspace{-0.8em}
    \scalebox{0.82}{
        \begin{tabular}{ccccccccc}
        \toprule[1.5pt]
        Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{w/o HRF} & \multicolumn{2}{c}{w/o SCL} & \multicolumn{2}{c}{\textbf{REFINE}} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
        Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\        
        \midrule
        BadNets & 91.70 & 100 & 91.23 & 70.76 & 89.26 & 1.43 & 90.92 & 0.68 \\
        Blended & 91.10 & 98.76 & 90.59 & 75.30 & 90.38 & 0.10 & 90.65 & 0.51 \\
        WaNet & 91.09 & 99.98 & 91.03 & 99.53 & 89.08 & 1.45 & 90.45 & 0.88 \\
        Physical & 93.59 & 100 & 92.86 & 1.60 & 88.63 & 1.97 & 90.92 & 1.36 \\
        BATT & 92.43 & 99.91 & 91.67 & 72.46 & 88.82 & 5.87 & 90.89 & 1.97 \\
        LC & 92.30 & 99.74 & 91.88 & 69.15 & 90.37 & 0.59 & 90.57 & 1.25 \\
        Adaptive & 90.54 & 100 & 89.77 & 62.94 & 88.06 & 0.32 & 90.17 & 0.27 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
    % \vspace{-5pt}
\end{table}

\subsection{Resistance to Potential Adaptive Attacks}
In this section, we examine whether the adversary can circumvent our defenses if they have full knowledge of the process of our REFINE. After training the original backdoored model, the adversary can fine-tune it utilizing an input transformation module, along with a randomly initialized hard-coded output mapping module, to simulate our REFINE. During fine-tuning, the loss function for model optimization can be expressed as follows:
\begin{equation}
\label{eq:adaptive}
    \min_{\delta}\mathcal{L}_{adap} = \mathcal{L}_{b} + \gamma\mathcal{L}_{refine},
\end{equation}
where $\mathcal{L}_{b}$ indicates the cross-entropy loss function in the original training phase of the backdoored model, and $\mathcal{L}_{refine}$ represents the loss function of REFINE. $\gamma$ is a scalar temperature parameter, and $\delta$ denotes the trainable parameters of the backdoored model. Ideally, the adversary can achieve the backdoor target with a low value of $\mathcal{L}_{refine}$ by optimizing Eq.~(\ref{eq:adaptive}). Consequently, the REFINE may not work well since the $\mathcal{L}_{refine}$ is already low.

As shown in Table \ref{tab:adaptive}, REFINE is still highly effective with high BAs (BA drop $<1.5\%$) and low ASRs ($<1.5\%$). It is mostly because defenders can arbitrarily specify the output mapping function and train an input transformation module that may entirely differ from the attacker's. Besides, the original backdoored model experiences a decrease in BA after undergoing adaptive attack training, due to the inherent difficulty of optimizing multiple loss functions simultaneously. As such, these results demonstrate that our REFINE is resistant to adaptive attacks. 
%the original backdoored model experiences a decrease in BA after undergoing adaptive attack training, indicating that the attacker have to account for a certain decline in model performance. In terms of defense, REFINE is still highly effective with high BA and low ASR, demonstrating that our method is resistant to adaptive attacks.

\begin{table}[t]
    % \vspace{-20pt}
    \tabcolsep=4mm
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{The performance (\%) of REFINE against potential adaptive attacks.}
    \label{tab:adaptive}
    \vspace{-0.8em}
    \scalebox{0.82}{
        \begin{tabular}{ccccccccc}
        \toprule[1.5pt]
        Setting & \multicolumn{4}{c}{Normal Attack} & \multicolumn{4}{c}{Adaptive Attack} \\
        \cmidrule(lr){2-5}\cmidrule(lr){6-9}
        Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{\textbf{REFINE}} & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{\textbf{REFINE}} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
        Dataset & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ \\        
        \midrule
        CIFAR-10 & 91.74 & 100 & 90.71 & 1.07 & 84.53 & 100 & 83.05 & 0.98 \\
        ImageNet& 66.94 & 99.59 & 69.00 & 0.70 & 58.39 & 100 & 60.53 & 1.09 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-10pt}
\end{table}

% \subsection{Case Study}
% A. Sample size of unlabeled dataset: from 100\% to 10\%.

% B. Different U-Net structures: Change the number of neurons in the UNet hidden layer.

% C. Disjoint dataset: When the third-party model provider is tasked with both model training and dataset provision, the dataset owned by the user is not intersected with the original model training dataset (e.g., CINIC10).


\section{Conclusion}

In this paper, we revisited existing pre-processing-based backdoor defense methods, including backdoor-trigger-inversion-based (BTI-based) defenses and transformation-based defenses. We revealed the limitations of the two defense methods. Subsequently, according to the empirical and theoretical analysis, we proposed REFINE, a reprogramming-based inversion-free backdoor defense method. This method was motivated by the insight that increasing the distances of the feature representations before and after the transformation may lead to a better effectiveness of backdoor defense. Specifically, we introduced an input transformation module and an output mapping module. We also utilized the supervised contrastive loss to enhance the defense performance. Results on benchmark datasets verified the effectiveness of our REFINE and the resistance to the adaptive attack. We hope our REFINE can provide a new angle to facilitate the design of more effective backdoor defenses.

\newpage

\section*{Acknowledgements}

This research is supported in part by the National Key Research and Development Program of China under Grant 2021YFB3100300 and the National Natural Science Foundation of China under Grants (62441238, 62072395, and U20A20178). Pin-Yu Chen is not supported by any external fundings. This work was mostly done when Yiming Li was a Research Professor at the State Key Laboratory of Blockchain and Data Security, Zhejiang University, China. He is currently at College of Computing and Data Science, Nanyang Technological University, Singapore.

\section*{Ethics Statement}

This paper proposes an inversion-free backdoor defense method, REFINE. Our method can be utilized to mitigate the effect of the backdoor. Therefore, our REFINE is a defensive method and our work does not discover any new threat. Our research also does not include any human subjects. Accordingly, this paper does not raise ethical issues.

\section*{Reproducibility Statement}

The details of our implementations and experiments can be found in Appendix~\ref{sec:appen:implement}. We provide the official implementation of REFINE at \url{https://github.com/WhitolfChen/REFINE}. Additionally, we also integrate REFINE into \href{https://github.com/THUYimingLi/BackdoorBox}{\texttt{BackdoorBox}} for easy access and usage. 




\bibliographystyle{iclr2025_conference}
%{\small \bibliography{ref}}
\bibliography{ref}


\newpage
\appendix

\setcounter{theorem}{0}
\setcounter{equation}{0}


\section*{Appendix}
% You may include other additional sections here.
\section{The Proof of Theorem~\ref{thm:defense}}
\label{sec:appen:proof}

\begin{theorem}
\label{thm:appen:defense}
    Given a $K$-class pre-trained deep learning model $\mathcal{F}(\cdot)=s(f(\cdot))$ where $s(\cdot)$ is the softmax function and $f(\cdot)$ is the feature extractor, and a pre-processing method $\mathcal{T}(\cdot)$, $\bm{x}$ is the data from a specific domain $\mathcal{D}$ (\ie, $\bm{x}\sim \mathcal{D}$) and $\tilde{\bm{x}}=\mathcal{T}(\bm{x})\sim \tilde{\mathcal{D}}$. Let $\Phi_{\mathcal{D}}(\bm{x})$ and $\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})$ denotes the probability density function of $\mathcal{D}$ and $\tilde{\mathcal{D}}$, we have
    \begin{equation}
        \mathbb{E}_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\|\mathcal{F}(\bm{x})-\mathcal{F}(\tilde{\bm{x}})\|_2 \leq 2\alpha\sqrt{K}\cdot\mathcal{W}_1(\mu, \tilde{\mu}),
    \end{equation}
    where $\mathcal{W}_1(\mu, \tilde{\mu})$ is the Wasserstein-1 distance between $\mu$ and $\tilde{\mu}$, $\mu$ and $\tilde{\mu}$ are the probability measures of the representations $f(\bm{x})$ and $f(\tilde{\bm{x}})$, and $\alpha = \max [\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}}|\bm{x}) / \Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})]$.
\end{theorem}

Following similar approaches in \citep{yang2021voice2series}, the proof of Theorem~\ref{thm:appen:defense} is as follows.
\begin{proof}
    Let $[K]$ represents the set of the first $K$ positive integers, \ie, $[K]=\{1, 2, 3, ..., K\}$. According to the definition of mathematical expectation, we have
    \begin{equation}
    \begin{aligned}
        &\mathbb{E}_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\|\mathcal{F}(\bm{x})-\mathcal{F}(\tilde{\bm{x}})\|_2 \\
        =\enspace\enspace &\int_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\|\mathcal{F}(\bm{x})-\mathcal{F}(\tilde{\bm{x}})\|_2 \Phi_{\mathcal{D}, \tilde{\mathcal{D}}}(\bm{x}, \tilde{\bm{x}})d\bm{x}d\tilde{\bm{x}} \\
        =\enspace\enspace &\int_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\|\mathcal{F}(\bm{x})-\mathcal{F}(\tilde{\bm{x}})\|_2 \Phi_{\mathcal{D}}(\bm{x})\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}}|\bm{x})d\bm{x}d\tilde{\bm{x}} \\
        \leq\enspace\enspace &\alpha\int_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\|\mathcal{F}(\bm{x})-\mathcal{F}(\tilde{\bm{x}})\|_2 \Phi_{\mathcal{D}}(\bm{x})\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})d\bm{x}d\tilde{\bm{x}},
        \end{aligned}
    \end{equation}
    where $\alpha = \max [\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}}|\bm{x}) / \Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})]$. Assuming $\bm{x}\in \mathbb{R}^d$ is a $d$-dimension vector and $\bm{x}_i$ denotes the $i$-th element of $\bm{x}$, we have
    \begin{equation}
        \|\bm{x}\|=\sqrt{\sum_{i=1}^d\bm{x}_i^2}\leq\sqrt{d\cdot \max_{i\in[d]}[\bm{x}_i^2]}=\sqrt{d}\cdot\max_{i\in[d]}[|\bm{x}_i|].
    \end{equation}
    
     Since $\mathcal{F}(\cdot)$ is a $K$-class pre-trained model, we have
    \begin{equation}
        \begin{aligned}
            &\alpha\int_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\|\mathcal{F}(\bm{x})-\mathcal{F}(\tilde{\bm{x}})\|_2 \Phi_{\mathcal{D}}(x)\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})d\bm{x}d\tilde{\bm{x}} \\
        \leq\enspace\enspace &\alpha\sqrt{K}\int_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\max_{k\in [K]}|[\mathcal{F}(\bm{x})]_k - [\mathcal{F}(\tilde{\bm{x}})]_k|\cdot \Phi_{\mathcal{D}}(\bm{x})\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})d\bm{x}d\tilde{\bm{x}} \\
        =\enspace\enspace & \alpha\sqrt{K}\int_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\max_{k\in [K]}|[s(f(\bm{x}))]_k - [s(f(\tilde{\bm{x}}))]_k|\cdot \Phi_{\mathcal{D}}(\bm{x})\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})d\bm{x}d\tilde{\bm{x}}.
        \end{aligned}
    \end{equation}
    After that, we define $k^+$ and $k^-$ as the following equations. 
    \begin{equation}
    \left\{\begin{aligned}
        k^+=\arg\max_{k\in [K]}[s(f(\bm{x}))]_k - [s(f(\tilde{\bm{x}}))]_k \\
        k^-=\arg\max_{k\in [K]}[s(f(\tilde{\bm{x}}))]_k - [s(f(\bm{x}))]_k
        \end{aligned}
        \right..
    \end{equation}
    Because the output of $s(\cdot)$ is a probability logit and the sum total is $1$, there exist at least one $k_1$ such that $[s(f(\bm{x}))]_{k_1} - [s(f(\tilde{\bm{x}}))]_{k_1} \geq 0$ and also at least one $k_2$ leading to $[s(f(\tilde{\bm{x}}))]_{k_2} - [s(f(\bm{x}))]_{k_2} \geq 0$. Therefore,
    \begin{equation}
    \label{eq:max}
        \begin{aligned}
            &\max_{k\in [K]}|[s(f(\bm{x}))]_k - [s(f(\tilde{\bm{x}}))]_k| \\
            =\enspace\enspace & \max_{k\in [K]}\{[s(f(\bm{x}))]_k - [s(f(\tilde{\bm{x}}))]_k, [s(f(\tilde{\bm{x}}))]_k - [s(f(\bm{x}))]_k \\
            \leq\enspace\enspace & [s(f(\bm{x}))]_{k^+} - [s(f(\tilde{\bm{x}}))]_{k^+} +[s(f(\tilde{\bm{x}}))]_{k^-} - [s(f(\bm{x}))]_{k^-}.
        \end{aligned}
    \end{equation}
    According to Eq.~(\ref{eq:max}), we have
    \begin{equation}
        \begin{aligned}
            &\int_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\max_{k\in [K]}|[s(f(\bm{x}))]_k - [s(f(\tilde{\bm{x}}))]_k|\cdot \Phi_{\mathcal{D}}(\bm{x})\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})d\bm{x}d\tilde{\bm{x}} \\
            \leq\enspace &\int_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}([s(f(\bm{x}))]_{k^+} - [s(f(\tilde{\bm{x}}))]_{k^+} +[s(f(\tilde{\bm{x}}))]_{k^-} - [s(f(\bm{x}))]_{k^-})\cdot \Phi_{\mathcal{D}}(\bm{x})\Phi_{\tilde{\mathcal{D}}}(\tilde{\bm{x}})d\bm{x}d\tilde{\bm{x}} \\
            =\enspace &\mathbb{E}_{\bm{x}\sim\mathcal{D}}[[s(f(\bm{x}))]_{k^+} - [s(f(\bm{x}))]_{k^-}] - \mathbb{E}_{\tilde{\bm{x}}\sim\tilde{\mathcal{D}}}[[s(f(\tilde{\bm{x}}))]_{k^-} - [s(f(\tilde{\bm{x}}))]_{k^+}.
        \end{aligned}
    \end{equation}
    Based on the fact that $[s(\cdot)]_k$ is $1$-Lipschitz continuous for any $k\in [K]$~\citep{gao2017properties} and thus $[s(\cdot)]_{k^+} - [s(\cdot)]_{k^-}$ is $2$-Lipschitz continuous, we have
    \begin{equation}
        \begin{aligned}
            &\mathbb{E}_{\bm{x}\sim\mathcal{D}}[[s(f(\bm{x}))]_{k^+} - [s(f(\bm{x}))]_{k^-}] - \mathbb{E}_{\tilde{\bm{x}}\sim\tilde{\mathcal{D}}}[[s(f(\tilde{\bm{x}}))]_{k^-} - [s(f(\tilde{\bm{x}}))]_{k^+}\\
            \leq\enspace\enspace&2\cdot \sup_{g:\mathbb{R}^K\mapsto \mathbb{R}, {\rm Lip}(g)\leq 1} \mathbb{E}_{\bm{x}\sim\mathcal{D}}[g(f(\bm{x}))] - \mathbb{E}_{\tilde{\bm{x}}\sim\tilde{\mathcal{D}}}[g(f(\tilde{\bm{x}}))].
        \end{aligned}
    \end{equation}
    Following the Kantorovich-Rubinstein theorem of the dual representation of the Wasserstein-1 distance, finally, we have
    \begin{equation}
    \begin{aligned}
        &\mathbb{E}_{\bm{x}\sim \mathcal{D}, \tilde{\bm{x}}\sim \tilde{\mathcal{D}}}\|\mathcal{F}(\bm{x})-\mathcal{F}(\tilde{\bm{x}})\|_2 \\
        \leq\enspace\enspace& 2\alpha\sqrt{K}\cdot \sup_{g:\mathbb{R}^K\mapsto \mathbb{R}, {\rm Lip}(g)\leq 1} \mathbb{E}_{\bm{x}\sim\mathcal{D}}[g(f(\bm{x}))] - \mathbb{E}_{\tilde{\bm{x}}\sim\tilde{\mathcal{D}}}[g(f(\tilde{\bm{x}}))] \\
        =\enspace\enspace& 2\alpha\sqrt{K}\cdot \mathcal{W}_1(\mu, \tilde{\mu}),
    \end{aligned}
    \end{equation}
    where $\mu$ and $\tilde{\mu}$ are the probability measures of the representations $f(\bm{x})$ and $f(\tilde{\bm{x}})$.
\end{proof}
% \newpage

\section{The Pseudo-code of REFINE}
\label{sec:appen:code}

The pseudo-code of our REFINE optimization process is shown in Algorithm~\ref{algo:refine}.

\begin{figure}[h]
\vspace{-10pt}
  \begin{algorithm}[H]
  \setstretch{1.2}
  \caption{REFINE Optimization Process}
  \label{algo:refine}
  \flushleft
      {\bf Input:} The backdoored model $\mathcal{F}$, the unlabeled benign dataset $\bm{D}=\{\bm{x}_i\}_{i=1}^M$, the randomly initialized input transformation module $\mathcal{T}(\cdot, \theta)$, the specified output mapping module $\mathcal{M}(\cdot)$. \\
      {\bf Output:} The input transformation module parameters $\bm{\theta}$.
      \begin{algorithmic}[1]
      \For{data batches $\bm{X}=\{\bm{x}_i\}_{i=1}^N$ in $\bm{D}$}
        \State Obtain the transformed input $\tilde{{\bm{X}}} = \mathcal{T}(\bm{X}, \theta)$.
        \State Obtain the original model output $\tilde{{\bm{Y}}} = \mathcal{F}(\tilde{{\bm{X}}})$.
        \State Obtain the mapped output $\bm{Y} = \mathcal{M}(\tilde{{\bm{Y}}})$.
        \State Obtain the predicted labels $\Bar{\bm{Y}} = \mathop{\arg\max}(\mathcal{F}(\bm{X}))$.
        \State Compute the supervised contrastive loss $\mathcal{L}_{sup}(\tilde{{\bm{X}}}, \Bar{\bm{Y}})$.
        \State Compute the cross-entropy loss $\mathcal{L}_{ce}(\Bar{\bm{Y}}, \bm{Y})$.
        \State Optimize $\bm{\theta}$ with the composite loss: $\mathop{\arg\min}\limits_{\bm{\theta}}\mathcal{L} = \mathcal{L}_{ce} + \lambda\mathcal{L}_{sup}$.
      \EndFor
      \State \Return $\bm{\theta}$
    \end{algorithmic}
    \end{algorithm}
    \vspace{-15pt}
\end{figure}


\section{Implementation Details}
\label{sec:appen:implement}

\subsection{Details of the Experimental Settings}

\textbf{Details of Datasets.}
\textbf{(1)}~\textit{CIFAR-10.} The CIFAR-10 dataset~\citep{krizhevsky2009learning} contains 50,000 training samples and 10,000 testing samples in total. The dataset has 10 classes and each class has 5,000 training samples and 1,000 testing samples. Tbe size of each image sample is 3$\times$32$\times$32. \textbf{(2)}~\textit{ImageNet.} The ImageNet dataset ~\citep{deng2009imagenet} consists of 1,000 classes containing over 14 million manually annotated images. In this paper, we select a subset with 50 different classes and each class contains 500 training samples and 100 testing samples with size 3$\times$224$\times$224.

\textbf{Details of Training Backdoored Models.}
We utilize the SGD with a momentum of 0.9 and a weight decay of $5\times10^{-4}$ as the optimizer for training all backdoored DNNs. The batch size is set to 128 on both of CIFAR-10 and ImageNet. We set the initial learning rate as 0.1 and train all models for 150 epochs, with the learning rate reduced by a factor of 0.1 at the 100-th and 130-th epoch.

\textbf{Details of Optimization.}
For training the input transformation module, we employ SGD with a momentum of 0.9 and a weight decay of $5\times10^{-4}$ as the optimizer. The initial learning rate is set to 0.01, and the batch size is set to 256 for CIFAR-10 and 64 for ImageNet. The input transformation module is trained for 150 epochs, with the learning rate decayed by a factor of 0.8 at the 100-th and 130-th epochs. For the training loss function, we set the temperature parameter as 0.1. For the output mapping module, we randomly assign a hard-coded remapping function before each defense.

\textbf{Computational Resources.}
In our implementations, we utilize PyTorch as the deep learning framework. All our experiments are implemented with RTX 3090 GPUs.

\subsection{Details of the Adopted Backdoor Attacks}
In our experiments, we adopt 7 representative backdoor attacks to evaluate the defense performance of our REFINE and other baseline backdoor defense methods. We implement all 7 backdoor attacks using BackdoorBox~\citep{li2022backdoor}. We hereby provide a detailed introduction to these backdoor attacks, as follows.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/attacks.pdf}
    \caption{The illustration of the adopted backdoor attacks.}
    \label{fig:attacks}
    \vspace{-1.0em}
\end{figure}


\begin{itemize}[leftmargin=*]
    \item \textbf{BadNets}: \cite{gu2019badnets} introduced the earliest poisoning-based backdoor attack that aims to poison the training dataset using a visible, distinctive pixel pattern. In this paper, we utilize a 3$\times$3 random square as the trigger pattern on the bottom right of samples in CIFAR-10 and a 20$\times$20 square on ImageNet. The poisoning rate is set to 0.1.
    \item \textbf{Blended}: To evade human visual detection of poisoned samples, \cite{chen2017targeted} designed a covert data poisoning method known as Blended, which attempts to embed triggers implicitly within the samples. In this paper, we utilize an image of Hello Kitty as the trigger pattern and set the blending rate and poisoning rate to 0.1 across both datasets.
    \item \textbf{WaNet}: WaNet~\citep{nguyen2021wanet} is another type of invisible attacks that employs a warp-based trigger. We follow its default settings and set the poisoning rate to 0.1.
    \item \textbf{PhysicalBA}: \cite{li2021backdoor} demonstrated that DNNs applied in physical scenarios could also be vulnerable to backdoor threats and proposed backdoor attacks that simulate physical transformations. We follow its default settings and set the poisoning rate to 0.1.
    \item \textbf{BATT}: \cite{xu2023batt} noted that simple transformations specific to samples can pose significant backdoor threats to models and introduced the Backdoor Attack with Transformation-based Triggers (BATT). We follow its default settings and set the poisoning rate to 0.1.
    \item \textbf{Label-consistent Attack (LC)}: To address the issue of easily identifiable mislabeled poisoned data in poisoning datasets, \cite{turner2019label} proposed clean-label backdoor attacks, which aim to poison samples of specific classes to inject backdoors. We employ projected gradient descent (PGD) to generate adversarial samples, setting the maximum perturbation size to $\epsilon=8$. The trigger patterns utilized are identical to those employed in BadNets. The poisoning rate is set to 0.25 on the CIFAR-10 dataset and 1.0 on the ImageNet dataset.
    \item \textbf{Adaptive-Patch}: \cite{qi2023revisiting} observed that models trained on poisoned datasets often learn distinct latent representations for poisoned and clean samples, and they proposed adaptive backdoor attacks to mitigate this separation phenomenon. In this paper, we follow the default settings utilized in its original paper. For CIFAR-10, the poisoned rate and covered rate are set to 0.01 and 0.02, respectively; for ImageNet, they are set to 0.03 and 0.06, respectively.
\end{itemize}

The poisoned samples of these backdoor attacks are depicted in Figure \ref{fig:attacks}. 

\subsection{Details of The Adopted Backdoor Defenses}
In our experiments, we compare REFINE with two types of pre-processing-based defense methods, namely transformation-based defenses and BTI-based defenses. Each type of defense includes three specific baseline defenses. Specifically, for transformation-based defenses, we utilize three representative and advanced methods, including \textbf{(1)} ShrinkPad~\citep{li2021backdoor}, \textbf{(2)} BDMAE~\citep{sun2023mask}, \textbf{(3)} ZIP~\citep{shi2023black}. We implement this type of defenses using their open-source codes. For BTI-based defenses, we employ three methods as baseline, including \textbf{(1)} Neural Cleanse (dubbed `NC')~\citep{wang2019neural}, \textbf{(2)} UNICORN~\citep{wang2023unicorn}, \textbf{(3)} BTI-DBF(P)~\citep{xu2024towards}. For NC and UNICORN, we utilize their open-source code to implement the first step of the BTI-based defense, which is trigger inversion. Then, following the method outlined in BTI-DBF, we utilize the trigger patterns inverted by NC or the backdoor image generator inverted by UNICORN to train a purification generator, thereby completing the second step of the defense, input purification. For BTI-DBF(P), we implement it using its open-source code.



% \section{Additonal Results on More Attacks}

% \subsection{Results against Clean-image Attack}

% In this section, we test the clean-image backdoor attack~\citep{rong2024clean} (dubbed 'Clean-image'). We train a ResNet-18 backdoor model on CIFAR10 and applied defenses using BDMAE, BTI-DBF(P), and REFINE. For each defense method, we test the model's benign accuracy (BA), natural attack success rate (ASR-n), and manual attack success rate (ASR-m). ASR-m represents the ASR of poisoned samples that naturally contain the backdoor trigger, while ASR-m represents the ASR of manually generated poisoned samples.



% \begin{table}[t]
%     % \vspace{-20pt}
%     \tabcolsep=1mm
%     \renewcommand{\arraystretch}{1.1}
%     \centering
%     \caption{The performance (\%) of BDMAE, BTI-DBF(P) and REFINE against Clean-image. The best results are \textbf{boldfaced}.}
%     \label{tab:clean-image}
%     \scalebox{0.90}{
%         \begin{tabular}{ccccccccccccc}
%         \toprule[1.5pt]
%         Defense$\rightarrow$ & \multicolumn{3}{c}{No Defense} & \multicolumn{3}{c}{BDMAE} & \multicolumn{3}{c}{BTI-DBF(P)} & \multicolumn{3}{c}{REFINE} \\
%         \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
%         Attack$\downarrow$ & BA & ASR-n & ASR-m & BA & ASR-n & ASR-m & BA & ASR-n & ASR-m & BA & ASR-n & ASR-m \\
%         \midrule
%         Clean-image & 87.78 & 82.93 & 4.73 & 86.73 & 58.54 & 4.73 & 78.95 & 86.59 & 9.53 & \textbf{88.94} & \textbf{53.57} & \textbf{4.63}\\
%         \bottomrule[1.5pt]
%         \end{tabular}
%     }
%     \vspace{-0.3em}
% \end{table}



% As shown in Table~\ref{tab:clean-image}, existing pre-processing-based defenses are unable to effectively reduce the ASR-n of the clean-image attack. Compared to BDMAE and BTI-DBF(P), REFINE shows better defense performance. However, we believe that the Clean-image attack, from certain perspectives, extends beyond the typical scope of backdoor attacks. Specifically, the stealthiness of backdoor attacks usually requires the model to behave normally on benign samples, while the clean-image attack may cause misclassifications on some benign samples (i.e., naturally poisoned samples), which significantly impacts the model's normal inference. We will discuss how to defend against this type of attack in our future work.

\begin{table}[t]
    \vspace{-10pt}
    \tabcolsep=3mm
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{The performance (\%) of REFINE and two baseline defenses on different model architectures. The best results are \textbf{boldfaced}.}
    \label{tab:bdmae-resnet18}
    \vspace{-0.8em}
    \scalebox{0.9}{
        \begin{tabular}{cccccccccc}
        \toprule[1.5pt]
        \multirow{2}{*}{Model}
        & Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{BDMAE} & \multicolumn{2}{c}{BTI-DBF(P)} & \multicolumn{2}{c}{REFINE} \\
        \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}
        & Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        \multirow{7}{*}{VGG16}
        & BadNets & 86.47 & 99.53 & 84.87 & 2.83 & 84.95 & 4.44 & \textbf{87.62} & \textbf{1.47} \\
        & Blended & 86.45 & 98.70 & 84.29 & 82.95 & 83.90 & 36.97 & \textbf{87.34} & \textbf{2.30} \\
        & WaNet & 87.12 & 99.77 & 84.33 & 97.34 & 84.71 & 2.91 & \textbf{87.57} & \textbf{1.50} \\
        & Physical & 87.85 & 99.98 & 88.79 & 6.15 & 88.09 & 9.15 & \textbf{89.62} & \textbf{1.69} \\
        & BATT & 88.40 & 99.99 & 87.28 & 99.99 & 86.07 & 13.13 & \textbf{89.24} & \textbf{0.81} \\
        & LC & 87.46 & 72.87 & 85.24 & 10.19 & 85.26 & 42.06 & \textbf{88.60} & \textbf{1.77} \\
        & Adaptive & 86.49 & 87.57 & 83.76 & 27.17 & 83.94 & 33.05 & \textbf{87.70} & \textbf{2.30} \\
        \midrule
        \multirow{7}{*}{DenseNet121}
        & BadNets & 86.36 & 99.99 & 84.81 & 1.44 & 85.04 & 2.76 & \textbf{88.51} & \textbf{0.96} \\
        & Blended & 86.90 & 99.90 & 84.92 & 51.03 & 84.74 & 74.51 & \textbf{88.58} & \textbf{1.09} \\
        & WaNet & 86.34 & 99.34 & 83.92 & 96.93 & 84.29 & \textbf{0.86} & \textbf{88.04} & 1.72 \\
        & Physical & 86.69 & 97.89 & 85.66 & 3.64 & 83.96 & 9.20 & \textbf{87.33} & \textbf{1.84} \\
        & BATT & 86.45 & 100 & 85.63 & 100 & 84.04 & 3.63 & \textbf{88.45} & \textbf{0.11} \\
        & LC & 86.79 & 57.72 & 85.42 & 12.01 & 84.88 & 21.98 & \textbf{88.82} & \textbf{1.23} \\
        & Adaptive & 86.70 & 54.19 & 85.07 & 25.05 & 84.83 & 25.33 & \textbf{88.74} & \textbf{1.10} \\
        \midrule
        \multirow{7}{*}{ViT}
        & BadNets & 66.98 & 99.97 & 64.16 & 6.13 & 64.18 & 6.84 & \textbf{75.97} & \textbf{1.89} \\
        & Blended & 66.01 & 99.57 & 63.30 & 96.17 & 63.74 & 98.67 & \textbf{76.23} & \textbf{2.23} \\
        & WaNet & 66.17 & 98.03 & 61.44 & 43.71 & 63.63 & 4.17 & \textbf{76.15} & \textbf{2.51} \\
        & Physical & 67.09 & 99.92 & 63.91 & 6.25 & 62.66 & 8.00 & \textbf{76.28} & \textbf{2.40} \\
        & BATT & 68.66 & 99.99 & 67.01 & 99.97 & 66.33 & 98.73 & \textbf{72.75} & \textbf{3.35} \\
        & LC & 67.39 & 92.25 & 63.75 & 5.85 & 64.28 & 74.16 & \textbf{80.36} & \textbf{1.60} \\
        & Adaptive & 66.38 & 83.01 & 63.85 & 14.67 & 64.48 & 45.72 & \textbf{77.16} & \textbf{2.51} \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}


\section{Additional Ablation Study}
\label{sec:appen:ablation}

\subsection{Results on Additional Model Architectures}

In this section, we conduct experiments on three additional model architectures, including VGG-16~\citep{simonyan2014very}, DenseNet-121~\citep{huang2017densely}, and ViT~\citep{dosovitskiy2021an}. We conduct experiments on the CIFAR-10 dataset. We compare the defense performance of our REFINE with the most advanced transformation-based defense ($i.e.$, BDMAE) and the SOTA BTI-based defense ($i.e.$, BTI-DBF(P)).

As shown in Table \ref{tab:bdmae-resnet18}, REFINE effectively defends against 7 representative attacks across 3 different model architectures, significantly outperforming baseline defenses. Specifically, under the REFINE defense, the benign accuracy shows a slight improvement, while the backdoor attack success rate is reduced to below 3.5\%. The additional experimental results verify the effectiveness of REFINE.



\subsection{Effect of the Unlabeled Benign Dataset Size}
\label{sec:appen:size}
In this section, we evaluate the defense performance of REFINE under different sizes of the unlabeled benign dataset. We train a backdoored classification model on CIFAR-10 using the BadNets attack on a ResNet-18 architecture. For defense, we use different proportions (100\% to 20\%) of the CIFAR-10 dataset as the unlabeled benign dataset. As shown in Table \ref{tab:size}, the results indicate that as the number of unlabeled samples decreases, the BA of REFINE experiences a slight decline, while the ASR remains consistently low.


\begin{table}[t]
    %\vspace{-20pt}
    \tabcolsep=1.8mm
    \renewcommand{\arraystretch}{1.2}
    \centering
    \caption{Performance (\%) of REFINE under different sizes of the unlabeled benign dataset.}
    \label{tab:size}
        \vspace{-0.8em}
    \scalebox{0.84}{
        \begin{tabular}{ccccccccccccc}
        \toprule[1.5pt]
        Proportion & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{100\%} & \multicolumn{2}{c}{80\%} & \multicolumn{2}{c}{60\%} & \multicolumn{2}{c}{40\%} & \multicolumn{2}{c}{20\%} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
        Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        BadNets & 92.31 & 100 & 91.20 & 0.86 & 90.22 & 1.05 & 89.53 & 1.21 & 87.81 & 1.11 & 83.93 & 2.21 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}


\subsection{Effect of the Scalar Temperature Parameter $\lambda$}
In this section, we evaluate the defense performance of REFINE under different values of temperature parameters $\lambda$. The attack setup is consistent with that in Section \ref{sec:appen:size}. During the defense, we test various temperature parameters ranging from 1 to 0.2. As shown in Table \ref{tab:lambda}, the results indicate that the value of temperature parameter has minimal impact on the defense performance of REFINE.

\begin{table}[t!]
    %\vspace{-20pt}
    \tabcolsep=1.8mm
    \renewcommand{\arraystretch}{1.2}
    \centering
    \caption{Performance (\%) of REFINE under different values of temperature parameters $\lambda$.}
    \label{tab:lambda}
        \vspace{-0.8em}
    \scalebox{0.90}{
        \begin{tabular}{ccccccccccccc}
        \toprule[1.5pt]
        $\lambda$ & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{1.0} & \multicolumn{2}{c}{0.8} & \multicolumn{2}{c}{0.6} & \multicolumn{2}{c}{0.4} & \multicolumn{2}{c}{0.2} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}
        Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        BadNets & 91.74 & 100 & 90.83 & 0.92 & 90.87 & 0.76 & 90.60 & 0.60 & 91.03 & 0.51 & 90.69 & 1.27 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}

\begin{table}[t]
    %\vspace{-20pt}
    \tabcolsep=1.8mm
    \renewcommand{\arraystretch}{1.2}
    \centering
    \caption{Performance (\%) of REFINE under different number of channels in UNet hidden layers.}
    \label{tab:channel}
        \vspace{-0.8em}
    \scalebox{0.82}{
        \begin{tabular}{ccccccccccc}
        \toprule[1.5pt]
        Channels & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{32} & \multicolumn{2}{c}{48} & \multicolumn{2}{c}{64} & \multicolumn{2}{c}{80} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
        Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        BadNets & 91.74 & 100 & 89.49 & 1.09 & 90.61 & 0.64 & 90.18 & 1.43 & 91.07 & 0.78 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
    % \vspace{-1em}
\end{table}

\subsection{Effect of the Number of Channals in UNet Hidden Layers}
We hereby evaluate the performance of REFINE using UNet models with varying numbers of hidden layer channels. Specifically, the dimensionality of the encoded features can be adjusted by altering the number of output channels in the first layer of the UNet encoder. The attack setup is consistent with that in Section \ref{sec:appen:size}. For the defense, we tested different channel numbers, including 32, 48, 64, and 80. As shown in Table \ref{tab:channel}, the number of channels in the UNet hidden layers has minimal impact on the defense performance of REFINE, with both BA and ASR remaining at an optimal level.


\subsection{Effect of the Data Distribution Used for Defense}

In our main experiments, we assume that the defender can acquire independent and identically distributed (i.i.d.) unlabeled datasets. In this section, we explore the defense performance under different data distributions. We train a ResNet-18 model on the CIFAR10 dataset using the BadNets attack. For defense, we trained the input transformation module of REFINE using CINIC10~\citep{darlow2018cinic}, a dataset with the same categories as CIFAR10 but a different data distribution.

As shown in Table~\ref{tab:distribution}, REFINE is still highly effective in reducing the attack success rate (ASR $<1.5\%$) while maintaining the model's benign accuracy (BA drop $<3\%$). This favorable result is due to the fact that REFINE first assigns pseudo-labels to the unlabeled benign samples using the original model, and then trains the input transformation module based on these pseudo-labels.

\begin{table}[t]
    %\vspace{-20pt}
    \tabcolsep=4mm
    \renewcommand{\arraystretch}{1.1}
    \centering
    \caption{The performance (\%) of REFINE in scenarios with different data distribution.}
        \vspace{-0.8em}
    \label{tab:distribution}
    \scalebox{0.95}{
        \begin{tabular}{ccccc}
        \toprule[1.5pt]
        Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{REFINE} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}
        Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        BadNets & 91.18 & 100.00 & 88.39 & 1.40 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}

\subsection{Effect of Improved Transformation Module}

In this section, we conduct additional defense experiments using traditional model reprogramming methods~\citep{elsayed2019adversarial} (dubbed 'T-MR'). We select three representative types of backdoor attacks, including BadNets, WaNet, and BATT. We train backdoor ResNet-18 models on the CIFAR-10 dataset. We compare the defense performance of REFINE with T-MR.

As shown in Table~\ref{tab:T-MR}, the T-MR defense has a significant impact on the model's BA (BA drop $>15\%$) but fails to effectively reduce the ASR under the WaNet attack. This is because traditional model reprogramming methods only add a universal adversarial perturbation around the image, while the trigger pattern remains unchanged on the backdoor image to some extent.

\begin{table}[t!]
    %\vspace{-20pt}
    \tabcolsep=4mm
    \renewcommand{\arraystretch}{1.1}
    \centering
    \caption{The performance (\%) of REFINE and T-MR. The best results are \textbf{boldfaced}.}
        \vspace{-0.8em}
    \label{tab:T-MR}
    \scalebox{0.88}{
        \begin{tabular}{ccccccc}
        \toprule[1.5pt]
        Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{T-MR} & \multicolumn{2}{c}{REFINE} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
        Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        BadNets & 91.18 & 100.00 & 75.51 & 3.36 & \textbf{90.50} & \textbf{1.05}  \\
        WaNet & 91.29 & 99.91 & 74.49 & 25.76 & \textbf{90.64} & \textbf{1.93} \\
        Adaptive & 92.54 & 99.93 & 75.49 & 5.87 & \textbf{90.87} & \textbf{1.76} \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
    \vspace{-1em}
\end{table}




\section{REFINE in the Black-box Scenario}
\label{sec:appen:black-box}

In our main experiments, we assume that we can obtain white-box access to the pre-trained backdoored models. We hereby initially explore how to implement our REFINE in the black-box scenario where defenders can only get black-box access to the backdoored model. In this scenario, only the class confidence scores are accessible and it is hard to calculate the gradients to optimize the REFINE modules. To tackle the aforementioned challenge, we leverage the surrogate model technique. Specifically, we distill a surrogate model from the original black-box model using an unlabeled dataset $D$. We employ the mean squared error (MSE) loss to align the output confidence scores between the black-box model $\mathcal{F}(\cdot)$ and the surrogate model $\mathcal{F}_s(\cdot)$, as follows:
\begin{equation}
    \mathcal{L}_{distill}=\frac{1}{|D|}\sum_{\bm{x}\in D}[\mathcal{F}(\bm{x})-\mathcal{F}_s(\bm{x})]^2.
\end{equation}

The surrogate model is then leveraged to replace the pre-trained model in our REFINE and optimize the input transformation module. Subsequently, the trained input transformation and output mapping modules are subsequently applied to the original black-box model.
%which is subsequently employed as a pre-processing step for the original black-box model.

% For a black-box model where only the class confidence scores are accessible and the model's architecture and parameters remain unknown, we first distill a surrogate model using an unlabeled dataset. This surrogate model is then leveraged to train the input transformation module, which is subsequently employed as a pre-processing step for the original black-box model. 

% \textbf{Settings.}
To validate the feasibility of our REFINE in the black-box scenario, we employ the backdoored ResNet-50 pre-trained on the CIFAR-10 dataset as the black-box model and ResNet-18 as the surrogate model. As shown in Table \ref{tab:black}, we evaluate both the black-box original model and the surrogate model in terms of BA and ASR before and after applying the REFINE defense. The ASRs of our REFINE are all below $4\%$. The results indicate that even though the input transformation module is trained using the surrogate model, our REFINE is still capable of achieving high performance of backdoor defense for the black-box original model.
%During the distillation process, we employ only the mean squared error (MSE) loss to align the output confidence scores between the black-box model and the surrogate model. 

\begin{table}[t]
    \tabcolsep=3mm
    \renewcommand{\arraystretch}{1.2}
    \centering
    \caption{Performance (\%) of REFINE in defending against attacks in black-box scenarios.}
    \label{tab:black}
    \vspace{-0.8em}
    \scalebox{0.82}{
        \begin{tabular}{ccccccccc}
        \toprule[1.5pt]
        Defense & \multicolumn{4}{c}{No Defense} & \multicolumn{4}{c}{\textbf{REFINE}} \\
        \cmidrule(lr){2-5}\cmidrule(lr){6-9}
        Model & \multicolumn{2}{c}{Black-box} & \multicolumn{2}{c}{Surrogate} & \multicolumn{2}{c}{Surrogate} & \multicolumn{2}{c}{Black-box} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
        Attack & BA & ASR & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        BadNets & 90.60 & 100 & 91.20 & 1.24 & 88.21 & 0.92 & 88.17 & 0.36 \\
        Blended & 91.08 & 96.94 & 90.69 & 2.23 & 88.34 & 0.62 & 87.75 & 0.18 \\
        WaNet & 91.50 & 99.93 & 90.92 & 99.84 & 88.77 & 3.37 & 87.44 & 0.04 \\
        Physical & 93.61 & 100 & 92.21 & 2.56 & 90.18 & 1.52 & 89.84 & 2.23 \\
        BATT & 93.24 & 99.89 & 92.76 & 4.30 & 90.86 & 2.01 & 89.21 & 3.72 \\
        LC & 91.95 & 93.06 & 91.53 & 1.11 & 89.04 & 0.87 & 88.69 & 1.05 \\
        Adaptive & 90.15 & 100 & 90.36 & 1.57 & 88.41 & 0.32 & 87.91 & 0.44 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
    % \vspace{-10pt}
\end{table}

\section{The Overhead of our REFINE}
\label{sec:appen:overhead}
In this section, we evaluate the overhead of our REFINE. Specifically, we measure the training time of the input transformation module and the model inference time on the CIFAR-10 using the ResNet-18 model. We employ a UNet with 32 hidden layer channels as the structure for the input transformation module. During training, we employ SGD with a momentum of 0.9 and a weight decay of $5\times10^{-4}$ as the optimizer. The initial learning rate is set to 0.01, with a batch size of 256. The input transformation module is trained for 150 epochs, with the learning rate decaying by a factor of 0.8 at the 100-th and 130-th epochs. For the training loss function, the temperature parameter is set to 0.1. We conduct all training using a single RTX 3090 GPU. For the output mapping module, a hard-coded remapping function is randomly assigned before each defense. Here, we compare REFINE's time consumption with that of BDMAE and BTI-DBF(P), which are the representative of SOTA transformation-based and BTI-based defenses, respectively.

\begin{table}[t]
    %\vspace{-20pt}
    \tabcolsep=4mm
    \renewcommand{\arraystretch}{1.1}
    \centering
    \caption{The overhead (minutes) of REFINE compared with BDMAE and BTI-DBF(P).}
    \label{tab:overhend}
    \vspace{-0.8em}
    \scalebox{0.9}{
        \begin{tabular}{cccc}
        \toprule[1.5pt]
        Defense & BDMAE & BTI-DBF(P) & REFINE \\
        \midrule
        Overhead & 39.67 & 15.49 & 36.70 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}

As shown in Table \ref{tab:overhend}, the overall time overhead of our REFINE is on par with SOTA baselines. Moreover, training the transformation module is a one-time process and can be done offline, although the pre-processing and model inference happen online. Once training of our REFINE is complete, inference on 10,000 images from CIFAR-10 takes 6.31 seconds, with the cost per image being nearly 0. Although REFINE introduces some additional overhead, we believe this cost is reasonable and acceptable. 

\section{Combining REFINE with Existing Defenses}

Arguably, our method can be used in conjunction with existing (model reconstruction-based) defenses to further enhance their effectiveness. To demonstrate this, we first applied model fine-tuning defense (dubbed 'FT') to a ResNet-18 model subjected to the BadNets attack on CIFAR-10, followed by the REFINE defense. As shown in Table~\ref{tab:FT}, the FT+REFINE defense effectively reduces the backdoor ASR while maintaining the model's BA.

\begin{table}[t]
    \vspace{-10pt}
    \tabcolsep=4mm
    \renewcommand{\arraystretch}{1.1}
    \centering
    \caption{The performance (\%) of FT and FT+REFINE on ResNet-18.}
    \label{tab:FT}
    \vspace{-0.8em}
    \scalebox{0.95}{
        \begin{tabular}{ccccccc}
        \toprule[1.5pt]
        Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{FT} & \multicolumn{2}{c}{FT+REFINE} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
        Attack & BA & ASR & BA($\uparrow$) & ASR($\downarrow$) & BA($\uparrow$) & ASR($\downarrow$) \\
        \midrule
        BadNets & 91.18 & 100.00 & 91.89 & 91.67 & 90.42 & 0.87 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}

\section{Related Work}
\label{sec:appen:background}

\subsection{Backdoor Attack}

\noindent\textbf{Visible Backdoor Attacks.}
This type of attack typically employs patterns that are visible to humans as triggers. BadNets~\citep{gu2019badnets} is the first backdoor attack technique that injects samples with simple but visually noticeable patterns into the training data, such as white squares or specific marks. \citet{li2021backdoor} then proposed a transformation-based enhancement that strengthens the attack's resilience and establishes its applicability to physical scenarios. To address the issue of latent feature separation in backdoor attacks, \citet{qi2023revisiting} employed asymmetric trigger planting strategies and developed adaptive backdoor poisoning attacks. Besides, \citet{gao2023not} revealed that clean-label attacks were difficult due to the conflicting effects of `robust features' in poisoned samples and proposed a simple yet effective method to improve these attacks by targeting ‘hard’ samples instead of random ones.

\noindent\textbf{Invisible Backdoor Attacks.}
To enhance the stealth of backdoor attacks, ~\citet{chen2017targeted} was the first to introduce the use of triggers that are imperceptible to humans, aiming to evade detection by basic data filtering techniques or human inspection. They proposed a blending strategy that generates poisoned images by subtly merging the backdoor trigger with benign images. After that, a series of studies focused on designing invisible backdoor attacks. WaNet~\citep{nguyen2021wanet} and ISSBA~\citep{li2021invisible} employed warping-based triggers and perturbation-based triggers, respectively, introducing sample-specific trigger patterns during training; LIRA~\citep{doan2021lira} formulated the learning of an optimal, stealthy trigger injection function as a non-convex constrained optimization problem, where the trigger generator function is trained to manipulate inputs using imperceptible noise; BATT~\citep{xu2023batt} utilized images rotated to a specific angle as triggers, representing a new attack paradigm where triggers extend beyond basic pixel-wise manipulations. 

A few existing literature also provided novel and comprehensive discussions on backdoor attacks from various domains and applications, such as diffusion models~\citep{chou2024villandiffusion}, 3D point clouds~\citep{wei2024pointncbw}, ViTs~\citep{yang2024not}, code generation~\citep{yang2024stealthy}, audio \citep{zhai2021backdoor,cai2024towards}, and federated learning~\citep{shao2024fedtracker}. Moreover, some existing works also explore utilizing the backdoor attack for good purposes, such as copyright protection~\citep{li2022untargeted,li2023black,guo2023domain,guo2024zero,li2025reliable} and XAI evaluation~\citep{ya2023towards}.

% ~\citep{wang2022bppattack, liang2024badclip, wei2024pointncbw, chou2024villandiffusion, yang2024stealthy}

\subsection{Backdoor Defenses}

Currently, there are various backdoor defense methods~\citep{li2024purifying, li2024nearest} designed to mitigate backdoor threats. These methods can generally be divided into three main paradigms~\citep{li2022backdoor}: (1) trigger-backdoor mismatch, which primarily refers to pre-processing-based defenses~\citep{liu2017neural, li2021backdoor, shi2023black}. (2) backdoor elimination~\citep{li2021neural,zhao2020Bridging,zeng2021rethinking,zeng2022adversarial,huang2022backdoor,xu2024towards}, such as model reconstruction~\citep{wang2020practical,li2021neural, zeng2022adversarial}, poison suppression~\citep{huang2022backdoor,tang2023setting}, and training sample filtering~\citep{hayase2020spectre, zeng2021rethinking}. (3) trigger elimination, also known as testing sample filtering~\citep{gao2019strip,xie2024badexpert,yi2025probe}.

\textbf{Pre-processing-based Defenses.}
These methods incorporate a pre-processing module prior to feeding samples into DNNs, altering the trigger patterns present in the samples. Consequently, the modified triggers no longer align with the hidden backdoor, thereby preventing the backdoor activation. AutoEncoderDefense~\citep{liu2017neural} is the first pre-processing-based backdoor defense by employing a pre-trained autoencoder as the pre-processing module. Based on the idea that trigger regions have the most significant impact on predictions, Februus~\citep{doan2020februus} effectively mitigates backdoor attacks by removing potential trigger artifacts and reconstructing inputs, all while preserving performance for both poisoned and benign samples. \cite{li2021backdoor} observed that poisoning-based attacks with static trigger patterns degrade sharply with slight changes in trigger appearance or location and proposed spatial transformations (e.g., shrinking, flipping) as an efficient defense with minimal computational cost. Deepsweep~\citep{qiu2021deepsweep} proposes a unified defense that (1) fine-tunes the infected model using a data augmentation policy to remove backdoor effects and (2) pre-processes input samples with another augmentation policy to disable triggers during inference. Recently, many pre-processing-based defenses utilize the generative model, such as the diffusion model and the masked autoencoder, to purify suspecious samples. ZIP~\citep{shi2023black} applies linear transformations, such as blurring, to poisoned images to disrupt backdoor patterns and subsequently employs a pre-trained diffusion model to recover the semantic information lost during the transformation. BDMAE~\citep{sun2023mask} detects potential triggers in the token space by evaluating image structural similarity and label consistency between test images and MAE restorations, refines these results based on trigger topology, and finally adaptively fuses the MAE restorations into a purified image for prediction.  

\textbf{Backdoor Elimination Defenses.}
In contrast to pre-processing-based defenses, backdoor elimination methods typically mitigate backdoor threats by directly modifying model parameters or prevent backdoor injection by controlling the model training process. \cite{li2021anti} identified two key weaknesses of backdoor attacks: (1) models learn backdoored data significantly faster than clean data, and (2) the backdoor task is associated with a specific target class. Consequently, they proposed Anti-Backdoor Learning (ABL), which introduces a two-stage gradient ascent mechanism: (1) isolating backdoor examples in the early training phase, and (2) breaking the correlation between backdoor examples and the target class in the later training phase. Inspired by the phenomenon where poisoned samples tend to cluster together in the feature space of the attacked DNN model, \cite{huang2022backdoor} proposed a novel backdoor defense by decoupling the original end-to-end training process into three stages. \cite{yang2023backdoor} removed backdoors by suppressing the skip connections in key layers identified by their method and fine-tuned these layers to restore high BA and further reduce the ASR. Neural Polarizer~\citep{zhu2023neural} achieved effective defense by training an additional linear transformation, called neural polarizer, using only a small portion of clean data without modifying the model parameters. DataElixir~\citep{zhou2024dataelixir} detects target labels by quantifying distribution discrepancies, selects purified images based on pixel and feature distances, and determines their true labels by training a benign model. \citet{xu2024towards} discovered that even in the feature space, the triggers generated by existing BTI methods differ significantly from those used by the adversary. Consequently, they proposed BTI-DBF, which decouples benign features instead of directly decoupling backdoor features. This method primarily involves two key steps: (1) decoupling benign features, and (2) triggering inversion by minimizing the differences between benign samples and their generated poisoned versions while maximizing the differences of the remaining backdoor features.



\textbf{Trigger Elimination Defenses.}
These defenses filter out malicious samples during the inference process rather than during training. As a result, the deployed model exclusively predicts benign test samples or purified attack samples, thereby preventing backdoor activation by removing trigger patterns. STRIP~\citep{gao2019strip} perturbs the input samples and observes the randomness in predicted classes from the deployed model for these perturbed inputs. If the entropy of the predicted classes is low, this violates the input-dependence characteristic of a benign model, indicating the presence of malicious features within the input. \cite{du2020robust} demonstrated that applying differential privacy can enhance the utility of outlier detection and novelty detection, and further extended this approach for detecting poisoned samples in backdoor attacks. Besides, CleaNN~\citep{javaheripi2020cleann} leverages dictionary learning and sparse approximation to characterize the statistical behavior of benign data and identify triggers, representing the first end-to-end framework capable of online mitigation against backdoor attacks in embedded DNN applications.

\subsection{Model Reprogramming}

\citet{elsayed2019adversarial} first proposed adversarial reprogramming, which aims to repurpose a classifier trained on ImageNet-1K for tasks such as classifying CIFAR-10 and MNIST images and counting the number of squares in an image. BAR~\citep{tsai2020transfer} extended model reprogramming to black-box scenarios and applied it to the bio-medical domain. Driven by advancements in deep speech processing models and the fact that speech data is a univariate time signal, Voice2Series~\citep{yang2021voice2series} learns to reprogram acoustic models for time series classification and output label mapping through input transformations. \cite{neekhara2022cross} analyzed the feasibility of adversarially repurposing image classification neural networks for natural language processing (NLP) and other sequence classification tasks. They developed an effective adversarial program that maps a series of discrete tokens onto an image, which can then be classified into the desired category by an image classification model. \cite{li2023exploring} found that combining Visual Prompting (VP) with PATE—a state-of-the-art differential privacy training method that utilizes knowledge transfer from a team of teachers—achieves a cutting-edge balance between privacy and practicality with minimal expenditure on privacy budget. More Recently, a novel application~\citep{dey2024enhancing} of model reprogramming repurposed models originally designed for able-bodied individuals to predict joint movements in amputees, significantly enhancing assistive technologies and improving mobility for amputees. Currently, model reprogramming has been shown to outperform transfer learning and training from scratch in many applications~\citep{tsai2020transfer, yang2021voice2series, vinod2023reprogramming}, without altering the original model's parameters.


\begin{figure}[!t]
    \vspace{-2em}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/visual.pdf}
    % \vspace{-0.4em}
    \caption{The visualization of transformed samples $\tilde{\bm{x}}$. We display the benign and poisoned samples and transformed benign and poisoned samples for each class. For each class of small areas, the upper left corner represents the benign sample, the upper right corner represents the transformed benign sample, the bottom left corner represents the poisoned sample and the bottom right corner represents the poisoned sample after transformations.}
    \label{fig:visual}
    % \vspace{-10pt}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/visualize2.pdf}
    % \vspace{-0.4em}
    \caption{The visualization of transformed samples $\tilde{\bm{x}}$ for classes `automobile' and `bird' of CIFAR-10. For each class, we display five input images and their transformed images.}
    \label{fig:visualize2}
    \vspace{-1em}
\end{figure}



\section{The Visualization of the Transformed Samples $\tilde{\bm{x}}$}
In this section, we visualize the transformed benign and poisoned samples $\tilde{\bm{x}}$ generated by the UNet of our REFINE. We train a backdoored ResNet-18 model on CIFAR-10 using the BadNets attack with a specified $3\times3$ trigger patterns at the bottom right corner of images, and the hard-coded remapping function $f_L$ of the output mapping module $\mathcal{M}$ is defined as follows:
\begin{equation}
    f_L = \tilde{l}\mapsto l,
\end{equation}
where
\begin{equation}
    \tilde{l} = \{airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\},
\end{equation}
and
\begin{equation}
    l = \{cat, deer, automobile, ship, frog, bird, horse, truck, airplane, dog\}.
\end{equation}

As shown in Figure \ref{fig:visual}, for both benign and poisoned samples, the transformed sample patterns are very similar, and the transformed pattern of the poisoned sample effectively removes the trigger. This further illustrates the effectiveness of our REFINE in mitigating backdoor threats.

As shown in Figure \ref{fig:visualize2} and \ref{fig:visualize3}, samples from the same class exhibit visual similarities after transformation. However, the transformed samples do not contain any human-recognizable information. This phenomenon occurs because the input transformation module maps the samples to a new benign feature space, and the constraint imposed by the supervised contrastive loss ensures that transformed samples from the same class exhibit more similar benign features.

\section{The Visualization of the Feature Distribution before and after REFINE}

We hereby visualize the changes in the feature distribution of the input samples before and after REFINE. Specifically, we trained a backdoor ResNet-18 model on CIFAR-10 using the BadNets attack and extracted the features from the input of the model's fully connected (FC) layer as the feature values of the input samples.

\begin{figure}[t!]
    % \vspace{-20pt}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/visual_imagenet.pdf}
    % \vspace{-0.4em}
    \caption{The visualization of transformed samples $\tilde{\bm{x}}$ for classes `koalas' and `golden retriever' of ImageNet. For each class, we display five input images and their transformed images.}
    \label{fig:visualize3}
    % \vspace{-10pt}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/feature_1.png}
    % \vspace{-0.4em}
    \caption{The t-SNE plots of the feature distribution of samples in Class 1 before and after REFINE. ND\_Benign and ND\_Poison represent the features of benign and poisoned samples under the No Defense (ND) scenario, respectively. RF\_Benign and RF\_Poison represent the features of benign and poisoned samples after applying REFINE, respectively.}
    \label{fig:feature_1}
    \vspace{-10pt}
\end{figure}

As shown in Figure \ref{fig:feature_1}, before applying REFINE, the feature distributions of benign and poisoned samples are clustered in two distinct locations. After applying REFINE, the feature distributions of benign and poisoned samples are interwoven and clustered in the same new location. This indicates that REFINE effectively removes the trigger patterns from the poisoned samples and maps samples of the same class to a new benign distribution.

\begin{figure}[t!]
    % \vspace{-20pt}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/feature_all.png}
    % \vspace{-0.4em}
    \caption{The t-SNE plots of the feature distribution of benign samples from different classes, both before and after REFINE.}
    \label{fig:feature_all}
    \vspace{-10pt}
\end{figure}

As shown in Figure \ref{fig:feature_all}, before applying REFINE, the benign samples of each class form distinct clusters in the feature space. After applying REFINE, the benign samples, adjusted by the input transformation module and output mapping module, form new clusters in different positions. This empirically demonstrates that REFINE is able to maintain the model's benign accuracy.


\section{Soceital Impact}

This paper aims to design an effective and efficient backdoor defense method and have a positive societal impact. Specifically, we propose a novel pre-processing-based backdoor defense method, REFINE, based on model reprogramming. REFINE can mitigate the backdoor behaviors injected into the third-party pre-trained models. Therefore, our REFINE can assist in ensuring the stable and reliable operation of the AI models, mitigating the potential threat of backdoors, and facilitating the reuse and deployment of the models. Moreover, the application of our REFINE may also facilitate the emergence of new business models such as model trading.

On the other hand, in this paper, we propose to leverage the model reprogramming techniques to build the input transformation and output mapping modules to mitigate the backdoors. The insight of our method can also be applied to the use of the pre-trained model in an unauthorized way. For instance, an adversary might use the model for an unauthorized task via model reprogramming, leading to copyright infringement~\citep{shao2025explanation, wang2022non}. However, we argue that the negative societal impact is negligible. The model developer can employ several existing protection methods, such as non-transfer learning ~\citep{wang2022non}, to prevent such misbehaviors. Moreover, although we do not find effective adaptive attacks against our REFINE, an adversary may design a more advanced adaptive attack to circumvent our proposed method since its effectiveness lacks of theoretical guarantees. Even so, the model users and developers can still prevent the backdoor threat from the source by only using trusted pre-trained models.

\section{Potential Limitations and Future Directions}

Firstly, as outlined in our threat model, the goal of our defense is to protect against pre-trained models from third-party platforms. Specifically, similar to other baseline methods, we assume that the defender possesses a certain amount of unlabeled sample datasets. To explore the effectiveness of REFINE in few-shot scenarios, we conduct additional experiments using 10\% unlabeled clean data. We apply the REFINE defense to a ResNet-18 model trained on the CIFAR-10 dataset, which is subjected to the BadNets attack. In this case, the unlabeled training set for REFINE used only 10\% of the CIFAR-10 training set.

\begin{table}[t]
    %\vspace{-20pt}
    \tabcolsep=4mm
    \renewcommand{\arraystretch}{1.1}
    \centering
    \caption{The performance (\%) of REFINE in the 10\% unlabeled data scenario on ResNet-18.}
    \label{tab:lessunlabel}
    \scalebox{0.95}{
        \begin{tabular}{ccccc}
        \toprule[1.5pt]
        Defense & \multicolumn{2}{c}{No Defense} & \multicolumn{2}{c}{REFINE} \\
        \cmidrule(lr){2-3}\cmidrule(lr){4-5}
        Attack & BA & ASR & BA$\uparrow$ & ASR$\downarrow$ \\
        \midrule
        BadNets & 91.18 & 100.00 & 78.02 & 2.90 \\
        Blended & 90.64 & 98.18 & 77.89 & 2.59 \\
        WaNet & 91.29 & 99.91 & 78.79 & 1.83 \\
        PhysicalBA & 93.67 & 100.00 & 79.87 & 2.34 \\
        \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}

As shown in Table \ref{tab:lessunlabel}, even with only 10\% unlabeled data, REFINE is still effective to some extent. REFINE effectively reduces the ASR, although it does have some impact on the model's BA. Therefore, in cases where the defender lacks the number of unlabeled samples, it becomes impossible to train the input transformation module, thereby hindering the execution of the intended defense. Currently, with the widespread application of generative models, obtaining a sufficient amount of unlabeled samples is no longer a challenging task. In the future, we will continue to explore how to maintain the effectiveness of our REFINE in few-shot scenarios.

Secondly, we need to train a local input transformation module, which requires certain computational resources and time. While this overhead is somewhat higher than that of pre-processing defenses based on random transformations, it is significantly lower than the overhead associated with pre-processing defenses based on generative models and BTI-based methods, as presented in Appendix~\ref{sec:appen:overhead}. This overhead is considered acceptable compared to retraining a DNN from scratch.

Finally, our method primarily focuses on backdoor defense for image classification models. Fortunately, existing researchs~\citep{yang2021voice2series, neekhara2022cross} have demonstrated that model reprogramming techniques can yield favorable results in fields such as text and audio. We will explore the reprogramming-based backdoor defense in other modalities and tasks in our future work.
% 1. 相比于transformation-based方法，开销大一些，但是能接受
% 2. 本文的实验主要针对图像分类模型，其他模态和任务的模型作为未来工作

\section{Discussion on Adopted Data}

In our experiments, we only use open-source dataset (\ie, CIFAR-10~\citep{krizhevsky2009learning} and ImageNet~\citep{deng2009imagenet}) for evaluation. Our research strictly obeys the open-source licenses of these datasets and does not lead to any privacy issues. The ImageNet dataset may include some personal elements. For instance, data about human faces is available in the ImageNet dataset. Nevertheless, our work treats all objects equally and does not intentionally exploit or manipulate these elements. As such, our work complies with the requirements of these datasets and should not be construed as a violation of personal privacy.



%\theendnotes
\end{document}







