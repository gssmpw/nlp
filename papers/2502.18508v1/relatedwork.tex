\section{Related Work}
\label{sec:appen:background}

\subsection{Backdoor Attack}

\noindent\textbf{Visible Backdoor Attacks.}
This type of attack typically employs patterns that are visible to humans as triggers. BadNets~\citep{gu2019badnets} is the first backdoor attack technique that injects samples with simple but visually noticeable patterns into the training data, such as white squares or specific marks. \citet{li2021backdoor} then proposed a transformation-based enhancement that strengthens the attack's resilience and establishes its applicability to physical scenarios. To address the issue of latent feature separation in backdoor attacks, \citet{qi2023revisiting} employed asymmetric trigger planting strategies and developed adaptive backdoor poisoning attacks. Besides, \citet{gao2023not} revealed that clean-label attacks were difficult due to the conflicting effects of `robust features' in poisoned samples and proposed a simple yet effective method to improve these attacks by targeting ‘hard’ samples instead of random ones.

\noindent\textbf{Invisible Backdoor Attacks.}
To enhance the stealth of backdoor attacks, ~\citet{chen2017targeted} was the first to introduce the use of triggers that are imperceptible to humans, aiming to evade detection by basic data filtering techniques or human inspection. They proposed a blending strategy that generates poisoned images by subtly merging the backdoor trigger with benign images. After that, a series of studies focused on designing invisible backdoor attacks. WaNet~\citep{nguyen2021wanet} and ISSBA~\citep{li2021invisible} employed warping-based triggers and perturbation-based triggers, respectively, introducing sample-specific trigger patterns during training; LIRA~\citep{doan2021lira} formulated the learning of an optimal, stealthy trigger injection function as a non-convex constrained optimization problem, where the trigger generator function is trained to manipulate inputs using imperceptible noise; BATT~\citep{xu2023batt} utilized images rotated to a specific angle as triggers, representing a new attack paradigm where triggers extend beyond basic pixel-wise manipulations. 

A few existing literature also provided novel and comprehensive discussions on backdoor attacks from various domains and applications, such as diffusion models~\citep{chou2024villandiffusion}, 3D point clouds~\citep{wei2024pointncbw}, ViTs~\citep{yang2024not}, code generation~\citep{yang2024stealthy}, audio \citep{zhai2021backdoor,cai2024towards}, and federated learning~\citep{shao2024fedtracker}. Moreover, some existing works also explore utilizing the backdoor attack for good purposes, such as copyright protection~\citep{li2022untargeted,li2023black,guo2023domain,guo2024zero,li2025reliable} and XAI evaluation~\citep{ya2023towards}.

% ~\citep{wang2022bppattack, liang2024badclip, wei2024pointncbw, chou2024villandiffusion, yang2024stealthy}

\subsection{Backdoor Defenses}

Currently, there are various backdoor defense methods~\citep{li2024purifying, li2024nearest} designed to mitigate backdoor threats. These methods can generally be divided into three main paradigms~\citep{li2022backdoor}: (1) trigger-backdoor mismatch, which primarily refers to pre-processing-based defenses~\citep{liu2017neural, li2021backdoor, shi2023black}. (2) backdoor elimination~\citep{li2021neural,zhao2020Bridging,zeng2021rethinking,zeng2022adversarial,huang2022backdoor,xu2024towards}, such as model reconstruction~\citep{wang2020practical,li2021neural, zeng2022adversarial}, poison suppression~\citep{huang2022backdoor,tang2023setting}, and training sample filtering~\citep{hayase2020spectre, zeng2021rethinking}. (3) trigger elimination, also known as testing sample filtering~\citep{gao2019strip,xie2024badexpert,yi2025probe}.

\textbf{Pre-processing-based Defenses.}
These methods incorporate a pre-processing module prior to feeding samples into DNNs, altering the trigger patterns present in the samples. Consequently, the modified triggers no longer align with the hidden backdoor, thereby preventing the backdoor activation. AutoEncoderDefense~\citep{liu2017neural} is the first pre-processing-based backdoor defense by employing a pre-trained autoencoder as the pre-processing module. Based on the idea that trigger regions have the most significant impact on predictions, Februus~\citep{doan2020februus} effectively mitigates backdoor attacks by removing potential trigger artifacts and reconstructing inputs, all while preserving performance for both poisoned and benign samples. \cite{li2021backdoor} observed that poisoning-based attacks with static trigger patterns degrade sharply with slight changes in trigger appearance or location and proposed spatial transformations (e.g., shrinking, flipping) as an efficient defense with minimal computational cost. Deepsweep~\citep{qiu2021deepsweep} proposes a unified defense that (1) fine-tunes the infected model using a data augmentation policy to remove backdoor effects and (2) pre-processes input samples with another augmentation policy to disable triggers during inference. Recently, many pre-processing-based defenses utilize the generative model, such as the diffusion model and the masked autoencoder, to purify suspecious samples. ZIP~\citep{shi2023black} applies linear transformations, such as blurring, to poisoned images to disrupt backdoor patterns and subsequently employs a pre-trained diffusion model to recover the semantic information lost during the transformation. BDMAE~\citep{sun2023mask} detects potential triggers in the token space by evaluating image structural similarity and label consistency between test images and MAE restorations, refines these results based on trigger topology, and finally adaptively fuses the MAE restorations into a purified image for prediction.  

\textbf{Backdoor Elimination Defenses.}
In contrast to pre-processing-based defenses, backdoor elimination methods typically mitigate backdoor threats by directly modifying model parameters or prevent backdoor injection by controlling the model training process. \cite{li2021anti} identified two key weaknesses of backdoor attacks: (1) models learn backdoored data significantly faster than clean data, and (2) the backdoor task is associated with a specific target class. Consequently, they proposed Anti-Backdoor Learning (ABL), which introduces a two-stage gradient ascent mechanism: (1) isolating backdoor examples in the early training phase, and (2) breaking the correlation between backdoor examples and the target class in the later training phase. Inspired by the phenomenon where poisoned samples tend to cluster together in the feature space of the attacked DNN model, \cite{huang2022backdoor} proposed a novel backdoor defense by decoupling the original end-to-end training process into three stages. \cite{yang2023backdoor} removed backdoors by suppressing the skip connections in key layers identified by their method and fine-tuned these layers to restore high BA and further reduce the ASR. Neural Polarizer~\citep{zhu2023neural} achieved effective defense by training an additional linear transformation, called neural polarizer, using only a small portion of clean data without modifying the model parameters. DataElixir~\citep{zhou2024dataelixir} detects target labels by quantifying distribution discrepancies, selects purified images based on pixel and feature distances, and determines their true labels by training a benign model. \citet{xu2024towards} discovered that even in the feature space, the triggers generated by existing BTI methods differ significantly from those used by the adversary. Consequently, they proposed BTI-DBF, which decouples benign features instead of directly decoupling backdoor features. This method primarily involves two key steps: (1) decoupling benign features, and (2) triggering inversion by minimizing the differences between benign samples and their generated poisoned versions while maximizing the differences of the remaining backdoor features.



\textbf{Trigger Elimination Defenses.}
These defenses filter out malicious samples during the inference process rather than during training. As a result, the deployed model exclusively predicts benign test samples or purified attack samples, thereby preventing backdoor activation by removing trigger patterns. STRIP~\citep{gao2019strip} perturbs the input samples and observes the randomness in predicted classes from the deployed model for these perturbed inputs. If the entropy of the predicted classes is low, this violates the input-dependence characteristic of a benign model, indicating the presence of malicious features within the input. \cite{du2020robust} demonstrated that applying differential privacy can enhance the utility of outlier detection and novelty detection, and further extended this approach for detecting poisoned samples in backdoor attacks. Besides, CleaNN~\citep{javaheripi2020cleann} leverages dictionary learning and sparse approximation to characterize the statistical behavior of benign data and identify triggers, representing the first end-to-end framework capable of online mitigation against backdoor attacks in embedded DNN applications.

\subsection{Model Reprogramming}

\citet{elsayed2019adversarial} first proposed adversarial reprogramming, which aims to repurpose a classifier trained on ImageNet-1K for tasks such as classifying CIFAR-10 and MNIST images and counting the number of squares in an image. BAR~\citep{tsai2020transfer} extended model reprogramming to black-box scenarios and applied it to the bio-medical domain. Driven by advancements in deep speech processing models and the fact that speech data is a univariate time signal, Voice2Series~\citep{yang2021voice2series} learns to reprogram acoustic models for time series classification and output label mapping through input transformations. \cite{neekhara2022cross} analyzed the feasibility of adversarially repurposing image classification neural networks for natural language processing (NLP) and other sequence classification tasks. They developed an effective adversarial program that maps a series of discrete tokens onto an image, which can then be classified into the desired category by an image classification model. \cite{li2023exploring} found that combining Visual Prompting (VP) with PATE—a state-of-the-art differential privacy training method that utilizes knowledge transfer from a team of teachers—achieves a cutting-edge balance between privacy and practicality with minimal expenditure on privacy budget. More Recently, a novel application~\citep{dey2024enhancing} of model reprogramming repurposed models originally designed for able-bodied individuals to predict joint movements in amputees, significantly enhancing assistive technologies and improving mobility for amputees. Currently, model reprogramming has been shown to outperform transfer learning and training from scratch in many applications~\citep{tsai2020transfer, yang2021voice2series, vinod2023reprogramming}, without altering the original model's parameters.


\begin{figure}[!t]
    \vspace{-2em}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/visual.pdf}
    % \vspace{-0.4em}
    \caption{The visualization of transformed samples $\tilde{\bm{x}}$. We display the benign and poisoned samples and transformed benign and poisoned samples for each class. For each class of small areas, the upper left corner represents the benign sample, the upper right corner represents the transformed benign sample, the bottom left corner represents the poisoned sample and the bottom right corner represents the poisoned sample after transformations.}
    \label{fig:visual}
    % \vspace{-10pt}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/visualize2.pdf}
    % \vspace{-0.4em}
    \caption{The visualization of transformed samples $\tilde{\bm{x}}$ for classes `automobile' and `bird' of CIFAR-10. For each class, we display five input images and their transformed images.}
    \label{fig:visualize2}
    \vspace{-1em}
\end{figure}