% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{graphicx}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}  % for '\mathbb' and other symbols
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{multirow}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{From Retrieval to Generation: Comparing Different Approaches}
%\title{From Retrieval to Generation: Evaluating the Best Approach}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{
    \textbf{Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani,} \\
    \textbf{Mohammed Ali, Adam Jatowt} \\
    University of Innsbruck \\
    \texttt{\{abdelrahman.abdallah, jamshid.mozafari, bhawna.piryani,} \\
    \texttt{mohammed.ali, adam.jatowt\}@uibk.ac.at}
}


\begin{document}
\maketitle

\begin{abstract}

Knowledge-intensive tasks, particularly open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, require a balance between retrieval accuracy and generative flexibility. Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR) efficiently retrieve from large corpora but often lack semantic depth. Generative models like GPT-4-o provide richer contextual understanding but face challenges in maintaining factual consistency.  In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our results show that dense retrievers, particularly DPR, achieve strong performance in ODQA with a top-1 accuracy of 50.17\% on NQ, while hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document reranking. Additionally, we analyze language modeling tasks using WikiText-103, showing that retrieval-based approaches like BM25 achieve lower perplexity compared to generative and hybrid methods, highlighting their utility in retrieval-augmented generation.  By providing detailed comparisons and practical insights into the conditions where each approach excels, we aim to facilitate future optimizations in retrieval, reranking, and generative models for ODQA and related knowledge-intensive applications\footnote{The code and the dataset will be available after acceptance of the paper.}.


%Knowledge-intensive tasks like open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling require balancing retrieval accuracy with generative flexibility. While retrieval models like BM25 and DPR efficiently extract relevant documents, they lack deep semantic understanding, whereas generative models like GPT-3.5 provide richer context but struggle with factual consistency. In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our evaluation shows that DPR achieves a top-1 accuracy of 50.17\% on NQ, while hybrid models significantly improve document reranking, raising nDCG@10 on BEIR from 43.42 (BM25) to 52.59. Additionally, retrieval-based approaches, particularly BM25, outperform generative and hybrid models in language modeling, achieving lower perplexity on WikiText-103. These insights highlight the strengths and limitations of each approach, guiding future optimizations in retrieval, reranking, and generative modeling for ODQA and related tasks.\footnote{The code and the dataset will be available after acceptance of the paper.}








\end{abstract}


\input{sections/introduction}
\input{sections/related_work}
\input{sections/methods}
\input{sections/experimental-setup}
\input{sections/results}
\section*{Limitations} While our study demonstrates promising results in open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, several limitations warrant further attention:

\begin{enumerate} \item The computational complexity of hybrid models, which combine retrieval and generation, increases with both the size of the corpus and the length of documents. This can lead to slower processing times, especially for large-scale datasets. \item The effectiveness of dense retrievers like DPR is highly dependent on the quality and diversity of the corpus used for training. Poorly representative datasets may lead to reduced performance in real-world applications. \item While hybrid models show significant improvements in document reranking, they are sensitive to the interplay between the retrieval and generation components. Inconsistent alignment between these components could lead to suboptimal performance in certain scenarios. 
\item Our evaluation is primarily limited to standard benchmarks, such as NQ and BEIR, which may not fully capture the diverse nature of real-world knowledge-intensive tasks. Besides other types of questions and retrieval tasks, the analysis should be extended to domain-specific scenarios, especially ones with low tolerance for errors and hallucinations like Medical \cite{kim-etal-2024-medexqa} or Legal QA \cite{abdallah2023exploring}. \end{enumerate}


\section*{Ethical Considerations and Licensing}

Our research utilizes the GPT models, which is available under the OpenAI License and  Apache-2.0 license, and the Llama model, distributed under the Llama 3 Community License Agreement provided by Meta. We ensure all use cases are compliant with these licenses. Additionally, the datasets employed are sourced from repositories permitting academic use. We are releasing the artifacts developed during our study under the MIT license to facilitate ease of use and adaptations by the research community. We have ensured that all data handling, model training, and dissemination of results are conducted in accordance with ethical guidelines and legal stipulations associated with each used artifact.
\bibliography{custom}
\appendix


\input{sections/Appendix}
\end{document}
