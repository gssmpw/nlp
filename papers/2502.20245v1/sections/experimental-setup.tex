\section{Experimental Setup}\label{sec:exp-setup}

\begin{table*}[ht]
\centering
 % Resize the table to fit the page width
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|cccccc|cccccc|cccccc}
\hline
\multirow{2}{*}{\textbf{Retriever}} & \multicolumn{6}{c|}{\textbf{NQ}} & \multicolumn{6}{c|}{\textbf{TQA}} & \multicolumn{6}{c}{\textbf{WEBQ}} \\
 & Top-1 & Top-5 & Top-10 & Top-20 & Top-50 & Top-100 & Top-1 & Top-5 & Top-10 & Top-20 & Top-50 & Top-100 & Top-1 & Top-5 & Top-10 & Top-20 & Top-50 & Top-100 \\
 \midrule
    \multicolumn{19
    }{c}{\textit{Unsupervised Retrievers}} \\
    \midrule

MSS         & 19.28 & 41.25 & 51.27 & 59.97 & 69.56 & 75.57 & 30.76 & 52.65 & 60.52 & 67.18 & 74.58 & 79.11 & 11.66 & 29.04 & 39.12 & 49.21 & 61.12 & 68.36 \\

BM25        & 22.11 & 43.77 & 54.46 & 62.94 & 72.74 & 78.25 & 46.30 & 66.28 & 71.74 & 76.41 & 80.56 & 83.15 & 18.90 & 41.83 & 52.17 & 62.40 & 71.70 & 75.49 \\
Contriever  & 22.16 & 47.29 & 58.73 & 67.87 & 76.01 & 80.55 & 34.16 & 59.49 & 68.00 & 73.91 & 79.84 & 82.94 & 19.98 & 43.45 & 56.40 & 65.70 & 74.85 & 80.12 \\
\midrule
\multicolumn{19}{c}{\textit{Supervised Retrievers}} \\
\midrule
DPR         & 48.67 & 68.78 & 74.54 & 79.20 & 83.71 & 85.71 & 57.47 & 72.40 & 76.50 & 79.77 & 82.97 & 85.10 & 44.83 & 65.01 & 70.62 & 74.61 & 78.69 & 81.64 \\

MSS-DPR     & \textbf{50.17} & \textbf{71.88} & \textbf{77.48} &\textbf{ 81.44} & \textbf{85.98} & \textbf{88.14} & 61.64 & 75.21 & 79.15 & 81.85 & 84.92 & 86.58 & 44.24 & 65.01 & 71.65 & 76.92 & 81.84 & 84.55 \\

\midrule
\multicolumn{19}{c}{\textit{Generator}} \\
\midrule
GenRead &  45.76 & 65.32 & 71.22 & - & - & - & \textbf{69.41} & \textbf{79.72} & \textbf{82.85} & - & - & - & \textbf{51.03} & \textbf{69.05} & \textbf{73.23} & - & - & - \\
\midrule

\end{tabular}}
\caption{Performance of different retrieval and generative models on Natural Questions (NQ), TriviaQA (TQA), and WebQuestions (WEBQ). 
%The table reports Top-1, Top-5, Top-10, Top-20, Top-50, and Top-100 accuracies for each model. The unsupervised models include BM25, MSS, and Contriever, while the supervised models include DPR and MSS-DPR. GenRead is a generative approach without external document retrieval.
}
\label{tab:retreivier_generator}
\end{table*}
\subsection{Datasets}\label{sec:datasets}
The evaluation is conducted for ODQA on Natural Questions (NQ)~\cite{kwiatkowski2019natural}, TriviaQA~\cite{joshi2017triviaqa} and WebQuestions (WebQ)~\cite{berant2013semantic}, following the same setup as in \cite{yu2022generate,izacard2020leveraging,lee2019latent}, while for Information Retrieval we use TREC~\cite{craswell2020overview} and  BEIR~\cite{thakur2021beir} and finally WikiText-103~\cite{merity2016pointer} for Language modelling.  %(see Tab. 1).

\textbf{ODQA datasets:} NQ is derived from real user queries made through Google Search, where answers are extracted as spans from Wikipedia articles. The dataset consists of 79,168 examples for training, 8,757 for development, and 3,610 for testing. TriviaQA, a dataset constructed from trivia and quiz-league websites, contains open-domain questions with well-defined answers. For ODQA, we use its unfiltered version, which includes 78,785 training examples, 8,837 development examples, and 11,313 test examples. WebQ consists of questions sourced via the Google Suggest API, with answers mapped to Freebase entities. It includes 3,417 training, 361 development, and 2,032 test examples.
%NQ is derived from real user queries made through Google Search. It includes questions paired with answers that are specific spans from Wikipedia articles. The dataset comprises approximately 79,168 examples in its training set, 8,757 examples in the development set, and 3,610 examples in the test set. TriviaQA comprises questions collected from trivia and quiz-league websites. For ODQA tasks, we utilize the unfiltered version of this dataset. It contains 78,785 examples in the training set, 8,837 examples in the development set, and 11,313 examples in the test set. WebQ is a dataset consisting of questions sourced using the Google Suggest API, with answers mapped to entities from the Freebase knowledge graph. It includes approximately 3,417 examples in the training set, 361 examples in the development set, and 2,032 examples in the test set.

\begin{comment}
    

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}l |c c c@{}}
 \toprule
 \textbf{Dataset}  & \textbf{ Train}  & \textbf{Dev} & \textbf{Test} \\
 \midrule
  NQ                 & 58,880 & 8,757 & \phantom{0}3,610 \\
 TriviaQA           & 60,413 & 8,837 & 11,313 \\
 WebQ              & \phantom{0}2,474 & \phantom{00}361 &  \phantom{0}2,032 \\
 \bottomrule
 \end{tabular}
\caption{ODQA datasets statistics. }
\label{tab:dataset_stats}
%\vspace{-6pt}
\end{table}
\end{comment}

\textbf{Evidence Passages:}
For ODQA, we used the preprocessed English Wikipedia dump from December 2018, as released by Karpukhin et al.~\cite{karpukhin2020dense}. Each Wikipedia article is split into non-overlapping passages of 100 words. This corpus contains over 21 million passages and serves as the evidence set from which relevant documents are retrieved for answering questions in QA tasks, we downloaded and used this corpus from Rankify, as described in \cite{abdallah2025rankify}.

    

\textbf{Information Retrieval (IR):} TREC~\cite{craswell2020overview} is a well-established benchmark dataset used in IR. For our evaluation, we utilize the test sets from the 2019 and 2020 TREC Deep Learning (DL) tracks: (i) TREC-DL19, which includes 43 queries, and (ii) TREC-DL20, comprising 54 queries. BEIR Benchmark~\cite{thakur2021beir}: is a heterogeneous benchmark covering 18 retrieval tasks, including fact-checking, question answering, and domain-specific retrieval (biomedical, scientific, etc.). %It presents a challenging task for models that need to generalize across diverse domains.


\textbf{Language Modeling:} WikiText-103~\cite{merity2016pointer}: is a large-scale dataset of over 100 million tokens sourced from long, context-rich Wikipedia articles. This dataset is a standard for evaluating language modeling tasks.

All experiments and dataset processing were conducted using the Rankify\footnote{\url{https://github.com/DataScienceUIBK/Rankify}} framework, which provides a unified toolkit for retrieval, re-ranking, and retrieval-augmented generation~\cite{abdallah2025rankify}.

\subsection{\textbf{Retrieval and Generative Models}}
\textbf{Retrieval Models:}\label{sec:retrieval-models}
We used five retrieval models in our experiments: \textbf{BM25}, a sparse vector-based method; \textbf{DPR}, a dense dual-encoder model that maximizes similarity between questions and relevant passages; \textbf{MSS}, a dense retriever pre-trained on predicting masked spans like named entities; \textbf{MSS-DPR}, combining MSS pre-training with DPRâ€™s fine-tuning for improved performance; and \textbf{Contriever}, an unsupervised dense retriever optimized for zero-shot performance through contrastive learning.
%We employed five different retrieval models in our experiments, spanning both sparse and dense retrieval methods: \textbf{BM25}: A sparse vector-based retrieval method.  \textbf{DPR}: A dense retrieval model using a dual-encoder architecture to encode both questions and passages into dense vectors which is trained to maximize the similarity between a question and its corresponding passage (positive example), while minimizing the similarity between the question and other non-relevant passages (negative examples).  \textbf{MSS }: A dense retriever trained by predicting masked spans like named entities. It is pre-trained using salient spans and is shown to improve retrieval performance. \textbf{MSS-DPR}: Combines MSS pre-training with supervised fine-tuning, similar to DPR, resulting in better performance through the combination of pre-training and fine-tuning strategies. \textbf{Contriever}: An unsupervised dense retrieval model trained using contrastive learning on text paragraphs, optimized for zero-shot performance across multiple benchmarks.



\begin{table}[t]
\centering
%\begin{minipage}{0.65\textwidth}  % First table taking 60% width
    \centering
    \resizebox{0.40\textwidth}{!}{  % Resize the table to fit the page width
    \begin{tabular}{l|cccccc|cccccc}
    \hline
    \multirow{2}{*}{\textbf{Retriever}} & \multicolumn{6}{c|}{\textbf{NQ}} & \multicolumn{6}{c}{\textbf{WEBQ}} \\
     & Top-1 & Top-5 & Top-10 & Top-20 & Top-50 & Top-100 & Top-1 & Top-5 & Top-10 & Top-20 & Top-50 & Top-100 \\
     \midrule
    \multicolumn{13}{c}{\textit{Generator+UPR}} \\
    \midrule
    GenRead &  44.76 &  64.18 & 71.22 & - & - & - & 51.72 & 67.13 & 73.23  & - & - & - \\
    \midrule
    \multicolumn{13}{c}{\textit{Retriever+UPR}} \\
    \midrule
    MSS        & 35.90 & 60.91 & 66.70 & 71.44 & 74.10 & 75.57 &  29.97 & 51.03 & 58.46 & 63.19 &  66.93 &  68.36 \\
    BM25        & 36.84 & 61.72  &68.45 & 72.63  & 76.68 & 78.25 & 33.12 & 56.45 & 63.73 &69.14 & 73.92 & 75.49 \\
    Contriever  &36.73 & 63.49 & 71.69 &76.32& 79.50& 80.55 & 33.96 & 59.94 & 67.18 & 73.08 & 78.20 & 80.12 \\
    DPR        &44.21 & 71.86 &  78.92 &82.16& 84.90 &  85.71 &  41.44 & 67.18 &72.93  &77.12 & 80.41 &   81.64 \\
    MSS-DPR    &  43.74 & 72.88 & 80.44 &84.71&87.26 & 88.14  &  39.96 & 65.40 & 73.08 & 78.30 & 82.63 &84.55  \\
    \midrule
    \multicolumn{13}{c}{\textit{Combined+UPR}} \\
    \midrule
    MSS        & 43.16 & 64.68 &  75.32 & 82.58 & 85.37 &86.29 & 48.87 & 66.49 &  72.83 & 78.89 &  81.20 &  82.33 \\
    BM25       & 43.46 & 64.52 &  75.73 &  82.96 & 86.09 & 87.09  & 48.28 & 66.78 & 73.33 & 78.94 &  82.38 & 83.76 \\
    Contriever & 43.27 & 64.99 & 76.34 &  83.93 &  87.15 & 87.78 & 47.93 & 67.67 & 73.97 & 79.48 &  83.56 & 84.99 \\
    DPR        & 44.07 &  65.90 &  78.17 &  86.68 & 89.39 & 90.11 &  49.46 &  68.06 &  74.90 &  81.89 &   85.19 &  86.27 \\
    MSS-DPR    &  43.96 &  65.60 &  77.87 &  87.51 &\textbf{ 90.42} &\textbf{ 91.27}  &  48.13 &  68.11 &  74.46 &  81.64 &  85.48 & \textbf{87.40} \\
    \midrule
    \multicolumn{13}{c}{\textit{Generator+RankGPT}} \\
    \midrule
    GenRead    &  50.97 &  64.74 & 71.22 & - & - & -  &  55.71 & 67.77 & 73.23 & - & - & - \\
    \midrule
    \multicolumn{13}{c}{\textit{Retriever+RankGPT}} \\
    \midrule
    MSS        &  43.52 & 63.19 & 68.34 & 70.28 & 73.85 & 75.57 &  35.58 & 53.30 & 58.76  &  61.91 & 65.8 & 68.36  \\
    BM25       &  48.98 & 66.76 & 70.86 & 73.71 & 76.40 & 78.25 &    42.27 &60.19   & 65.75 & 69.39 &  73.43  & 75.49 \\
    Contriever &  46.87 &  67.09 &71.58  &75.29  &  78.98 & 80.55 &   41.93&  63.24&   68.8&  73.23&77.12  &  80.12 \\
    DPR        &  50.47 & 75.24 & 80.00 & 82.71 & 84.88 & 85.71  &   48.28 &  68.85 &   74.26 &  77.31 & 79.82 & 81.64  \\
    MSS-DPR    & 54.88 & 75.35 & 81.47 & 84.88& 87.20 &88.14   &   49.56 &  69.83 &   75.15 &  79.28 & 82.43 & 84.55  \\
    \midrule
    \multicolumn{13}{c}{\textit{Combined+RankGPT}} \\
    \midrule
    MSS        & 53.60 & 68.37 & 76.95 & 81.88& 83.96 &86.12 &  54.77 & 68.6 & 73.72  & 76.43 & 79.53 & 82.04  \\
    BM25       & 53.46 & 68.48 & 76.98 & 82.22 &84.65  &  86.87 &  55.07 & 69.29 & 75.34  & 78.35 & 81.5 & 83.86  \\
    Contriever &  53.05 & 68.25 & 77.26 & 83.05 & 85.84 & 87.67&  55.86 & 68.90 & 74.51  & 78.74 &  82.38 &84.84  \\
    DPR        & 56.15 &  69.70 &  80.08 & 86.93 & 88.92 & 90.06 &  56.10 & \textbf{70.52} & \textbf{78.30}  & 82.33 & 84.25 & 85.93  \\
    MSS-DPR    & \textbf{56.79} & \textbf{70.55} &\textbf{80.58}  & \textbf{87.73} & 89.81& 91.16 &  \textbf{56.50} & 69.59 &  77.90  &  \textbf{82.53} & \textbf{85.63} &  87.30  \\
    \midrule
    \end{tabular}}
    \caption{Performance comparison of various retrieval and generation methods combined with UPR and RankGPT for reranking on NQ and WebQ datasets.}
    \label{tab:combined_reranking}
\end{table}
\textbf{Generative Models:}\label{sec:generative-models} For the generation-based retrieval, we employ GenRead~\cite{yu2022generate} a generative model designed for open-domain QA tasks, which first generates contextual documents based on the query and then predicts the final answer using those generated documents.

\subsection{Language Models}\label{sec:language-models}

We tested a range of LLMs in our experiments, focusing on both generation and reranking tasks: \textbf{GPT-2}~\cite{radford2019language}: A transformer-based autoregressive language model trained on WebText. We experimented with the small (110M), medium (345M), large (774M), and extra-large (1.5B) versions of GPT-2 to observe how model size impacts performance. \textbf{OPT}~\cite{zhang2022opt}: We experimented with various OPT models ranging from 125M to 13B parameters to analyze their performance across retrieval and generation tasks. \textbf{GPT-Neo}~\cite{gpt-neo}: An autoregressive language model trained on the Pile dataset~\cite{leo-etal-pile}. We evaluated its performance on WikiText-103 using both retrieval and generation configurations.
\begin{table*}[t]
\centering
\resizebox{0.7\textwidth}{!}{
%\setlength\tabcolsep{3pt}
\begin{tabular}{@{}l|c|ccc|ccc|ccc|ccc|ccc|ccc@{}}
\toprule
 \multirow{2}{*}{\textbf{Retriever} } & \multirow{2}{*}{\textbf{\# Mode}} & \multicolumn{3}{c|}{\textbf{LLama-3.3 8B}} & \multicolumn{3}{c|}{\textbf{LLama-3.1 8B}} & \multicolumn{3}{c|}{\textbf{Gemma-2-2b}} & \multicolumn{3}{c|}{\textbf{Gemma-2-9b}} & \multicolumn{3}{c|}{\textbf{Llama-2-13b-hf}} & \multicolumn{3}{c}{\textbf{Mistral-7B-v0.1}} \\

 & & \textbf{NQ} & \textbf{TriviaQA} & \textbf{WebQA} & \textbf{NQ} & \textbf{TriviaQA}& \textbf{WebQA} & \textbf{NQ} & \textbf{TriviaQA}& \textbf{WebQA} & \textbf{NQ} & \textbf{TriviaQA}& \textbf{WebQA} & \textbf{NQ} & \textbf{TriviaQA}& \textbf{WebQA} & \textbf{NQ} & \textbf{TriviaQA}& \textbf{WebQA} \\
\midrule
%Question Only& 0&  &  &  & &  &  &  & &  & & & 18.99 &11.30 &39.10  &11.90&&  &  \\
& G & 24.68 & 52.23 & 15.45 & 21.49 & 48.74 & 14.51 & 27.01 & 59.91 & 19.34 & 28.28 & 63.02 & 18.65 & 28.06 & 62.64 & 20.32 & 27.01 & 62.64 & 16.09 \\

\midrule
\multirow{6}{*}{BM25 } 
& R & 14.90 & 42.10 & 10.23 & 12.82 & 40.13 & 9.25 & 14.02 & 43.28 & 14.71 & 19.81 & 57.55 & 14.96 & 21.14 & 57.90 & 19.54 & 11.19 & 52.85 & 6.40 \\


& R+G & 25.51 & 53.29 & 15.05 & 22.29 & 49.69 & 13.97 & 28.39 & \textbf{59.89} & 19.29 & 28.45 & \textbf{63.50} & 19.05 & 26.62 & 61.35 & 19.00 & 25.68 & 60.45 & 15.65 \\

& G+R & 25.67 & 53.24 & 15.00 & 23.29 & 50.29 & 13.43 & 28.50 & 59.87 & 19.73 & 28.42 & 62.94 & 19.34 & 26.79 & \textbf{\underline{62.01} } & 19.24 & 23.71 & 58.56 & 13.44 \\

& UPR & 23.49 & \textbf{\underline{55.85} }& 17.27 & 24.24 & \textbf{55.22} & 17.67 & 26.23 & 58.71 & 19.78 & 23.41 & 58.74 & 15.94 & 27.59 & 61.60 & 20.37 & 25.18 & \textbf{\underline{59.64} } & \textbf{17.18} \\

& RankGPT & \textbf{28.45} & - & \textbf{19.73} & \textbf{26.65} & - & 18.55 & \textbf{30.36} & - & \textbf{21.11} & \textbf{30.75} & - & \textbf{\underline{21.06}} & \textbf{29.22} & - & \textbf{21.99} & \textbf{25.35} & - & 17.18 \\

\midrule
\multirow{6}{*}{MSS}
& R & 12.82 & 31.90 & 7.38 & 11.19 & 30.74 & 6.88 & 13.96 & 33.05 & 14.71 & 19.78 & 50.93 & 14.96 & 21.52 & 51.75 & 20.62 & 11.08 & 42.69 & 6.40 \\

%& G & 24.95 & 51.69 & 15.84 & 21.46 & 47.98 & 14.12 & 27.06 & 59.27 & 19.34 & 27.95 & 62.67 & 18.65 & 28.01 & 60.66 & 19.44 & 27.15 & 61.28 & 16.09 \\

& R+G & 25.54 & 51.39 & 14.96 & 21.57 & 47.75 & 14.46 & 28.48 & \textbf{58.63} & 19.29 & 28.56 & 62.53 & 18.80 & 26.81 & 58.64 & 19.05 & \textbf{\underline{25.90}} & 56.28 & 14.57 \\

& G+R & 25.31 & 51.81 & 15.20 & 22.68 & 48.75 & 13.43 & 28.42 & 58.47 & 19.78 & 28.31 &\textbf{ 61.78} & 18.55 & 27.48 & \textbf{59.56 }& 18.75 & 23.10 & 52.15 & 13.09 \\

& UPR & 23.35 & \textbf{53.78} & 17.08 & 24.43 & \textbf{54.97} & 16.70 & 26.20 & 58.15 & 19.73 & 23.10 & 57.50 & 15.85 & 27.29 & 59.23 & 20.72 & 23.77 & \textbf{57.24} & \textbf{\underline{17.42} }\\

& RankGPT & \textbf{28.17} & - & \textbf{19.05} & \textbf{25.84} & - & \textbf{17.37} & \textbf{29.17} & - & \textbf{19.93} & \textbf{29.97} & - & \textbf{19.64} & \textbf{27.56} & - & \textbf{20.77} & 23.77 & - & 16.98 \\

\midrule
\multirow{6}{*}{Contriever} 
& R & 15.29 & 36.25 & 10.67 & 13.24 & 35.29 & 9.35 & 13.96 & 33.05 & 14.71 & 19.78 & 50.93 & 14.96 & 20.47 & 42.69 & 19.98 & 11.08 & 42.69 & 6.40 \\

%& G & 24.70 & 51.48 & 15.89 & 21.32 & 48.39 & 14.61 & 27.06 & 59.27 & 19.34 & 27.95 & 62.85 & 19.64 & 26.79 & 61.28 & 19.69 & 27.15 & 61.28 & 16.09 \\

& R+G & 25.59 & 51.78 & 15.10 & 22.18 & 48.38 & 13.87 & 28.78 & \textbf{58.86} & 20.28 & 28.84 & \textbf{62.85} & 19.59 & 27.45 & 56.43 & 19.59 & \textbf{25.35} & 56.43 & 15.31 \\

& G+R & 25.70 & 52.07 & 15.50 & 23.13 & 48.91 & 13.87 & 28.75 & 58.64 & 20.13 & 28.37 & 61.97 & 19.09 & 27.17 & 52.66 & 19.88 & 23.02 & 54.39 & 13.44 \\

& UPR & 23.24 & \textbf{53.48} & 17.32 & 24.21 & \textbf{55.71} & \textbf{17.62} & 26.26 & 58.37 & 19.83 & 23.21 & 57.64 & 16.14 & 26.57 & \textbf{59.60} & \textbf{20.72} & 25.10 & \textbf{57.26} & \textbf{17.27} \\

& RankGPT & \textbf{30.55} & - & \textbf{19.78} & \textbf{28.86} & - & \textbf{17.62} & \textbf{32.11} & - & \textbf{20.67} & \textbf{32.44} & - & \textbf{19.88} & \textbf{30.39} & - & \textbf{20.72} & 25.10 & - & \textbf{17.27} \\

\midrule
\multirow{6}{*}{DPR} 
& R & 28.08 & 45.88 & 19.83 & 23.21 & 43.62 & 14.32 & 13.99 & 33.05 & 14.71 & 19.78 & 50.93 & 14.96 & 21.94 & 51.07 & 19.83 & 11.11 & 42.69 & 6.40 \\

%& G & 25.06 & 51.66 & 21.41 & 21.41 & 48.21 & 14.76 & 27.06 & 59.27 & 19.34 & 27.92 & 62.67 & 18.65 & 28.12 & 60.85 & 20.47 & 27.15 & 61.28 & 16.09 \\

& R+G & 28.94 & 54.41 & 24.62 & 24.62 & 50.61 & 14.32 & 30.25 &\textbf{ 60.16 }& 20.18 & 30.83 & \textbf{\underline{63.72}} & 19.93 & 28.81 & 58.00 & 20.32 & 27.70 & 58.00 & 16.44 \\

& G+R & 28.14 & \textbf{54.50} & \textbf{25.51} & 25.01 & 51.65 & 14.81 & 27.92 & 60.17 & 20.72 & 29.92 & 63.18 & 19.39 & 27.92 & 60.01 & 20.72 & 25.01 & 54.64 & 14.71 \\

& UPR & 23.60 & 53.41 & 18.06 & 24.74 & \textbf{\underline{56.07}} & \textbf{\underline{19.64}} & 26.51 & 58.71 & 19.78 & 23.38 & 57.85 & 16.04 & 27.45 & 60.05 & 20.62 & 25.25 & \textbf{57.53 }& 17.17 \\

& RankGPT & \textbf{31.74 }& - & 20.42 & \textbf{29.58} & - & \textbf{\underline{19.64}} & \textbf{\underline{34.16}} & - & \underline{\textbf{22.15} } & \underline{\textbf{34.04} }& - & \textbf{21.01} & \textbf{32.77} & - & \underline{\textbf{22.15} }& \textbf{25.26} & - &\textbf{ 17.18} \\

\midrule
\multirow{6}{*}{MSS+DPR}
& R & 28.17 & 47.69 & 23.68 & 23.68 & 45.72 & 13.92 & 13.96 & 33.05 & 14.71 & 19.78 & 50.93 & 14.96 & 21.47 & 51.35 & 19.83 & 11.08 & 42.69 & 6.40 \\

%& G & 24.73 & 51.64 & 21.80 & 21.80 & 47.98 & 14.27 & 27.06 & 59.27 & 19.34 & 27.15 & 61.28 & 16.09 & 28.20 & 60.44 & 20.13 & 27.15 & 61.28 & 16.09 \\

& R+G & 29.41 & \textbf{54.53} & 24.09 & 24.09 & 50.72 & 14.96 & 28.48 & \textbf{\underline{60.56} } & 19.29 & 28.45 & \textbf{62.53 }& 18.96 & 28.45 & 59.50 & 19.78 & 27.40 & 58.03 & \textbf{16.44} \\

& G+R & 28.61 & 54.48 & \textbf{\underline{25.54} }& 25.07 & 52.20 & 14.76 & 28.61 & 60.25 & 19.78 & 28.31 & 61.78 & 18.55 & 28.73 & 59.96 & 20.77 & 25.07 & 54.39 & 14.96 \\

& UPR & 23.10 & 53.65 & 16.68 & 25.24 & \textbf{55.52} & 18.98 & 23.10 & 58.64 & 16.68 & 25.24 & 57.47 & 16.09 & 23.10 & \textbf{60.00} & 19.98 & 25.23 & \textbf{57.47 }& 16.08 \\

& RankGPT & \textbf{\underline{31.94}} & - & 21.41 & \textbf{\underline{29.95}} & - & \textbf{18.99} & \textbf{32.51} & - & \textbf{21.41} & \textbf{32.13} & - & \textbf{18.99} & \textbf{\underline{32.61} }& - & \textbf{21.95} & \textbf{25.24} & - & 16.09 \\

\bottomrule
\end{tabular}
}
\caption{Zero-shot results of in-context learning on
the test set of NQ, TriviaQA, and WebQ measured by exact match.  
\textbf{Bold values} indicate the best performance within each retriever, while  
\underline{\textbf{underlined values}} represent the best overall performance. Please refer to the Appendix for additional evaluation metrics (e.g., Recall and F1-score) presented in Tables~\ref{tab:qa_LLama_3_3.1}, \ref{tab:qa_Gemma}, and \ref{tab:qa_Llama-2-13b}. }
\label{tab:em_comparison}
\end{table*}
\subsection{Reranking Methods}\label{sec:reranking-techniques}

We explored two reranking techniques to optimize the combination of retrieved and generated documents: \textbf{UPR}: Based on the T5 series~\cite{raffel2020exploring}, which consists of encoder-decoder transformers pre-trained on text denoising tasks. We used the T5-lm-adapt~\cite{lester2021power} and T0~\cite{sanh2022multitask} models for reranking. \textbf{RankGPT}: A reranking approach using GPT-3.5 and LLama 3 (70B) to evaluate the relevance of both retrieved and generated documents. These models dynamically rank documents to ensure the most relevant results are presented in QA tasks.

