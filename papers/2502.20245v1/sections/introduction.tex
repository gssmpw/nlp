\section{Introduction}


%The increasing complexity of knowledge-intensive tasks, particularly open-domain question answering (ODQA) and retrieval-augmented applications, necessitates more advanced approaches to efficiently retrieve and generate relevant information. Traditionally, retrieval-based methods have played a central role in these tasks, with models like BM25~\cite{robertson2009probabilistic} serving as foundational tools for extracting relevant documents from large corpora. The limitations of keyword matching prompted the development of dense retrieval models such as Dense Passage Retrieval (DPR)~\cite{karpukhin2020dense} and Contriever~\cite{izacard2021unsupervised}, leveraging transformer-based architectures like BERT~\cite{devlin2019bertpretrainingdeepbidirectional} to encode both queries and documents into dense vectors. 

The increasing complexity of knowledge-intensive tasks, particularly open-domain question answering (ODQA) and retrieval-augmented applications, necessitates advanced approaches to efficiently retrieve and generate relevant information. Traditionally, retrieval-based methods have played a central role in these tasks, with models like BM25~\cite{robertson2009probabilistic} serving as foundational tools for extracting relevant documents. However, the limitations of keyword-based retrieval prompted the development of dense retrieval models such as Dense Passage Retrieval (DPR)~\cite{karpukhin2020dense} and Contriever~\cite{izacard2021unsupervised}, which leverage transformer-based architectures to encode queries and documents into dense representations. While dense retrieval models improve over sparse methods, they introduce new challenges. First, retrieval corpora are typically divided into fixed chunks~\cite{karpukhin2020dense}, which can lead to retrieving irrelevant content. Second, dual-encoder architectures encode queries and documents separately, limiting direct interaction between them~\cite{khattab2021relevance}. Finally, dense retrieval models require pre-encoding and storing document embeddings, which constrains scalability and hinders their ability to leverage large language models (LLMs)~\cite{levine2022standing}.



%While dense retrieval models have advanced beyond traditional sparse methods, they introduce a range of complexities and challenges. Firstly, the document corpus for retrieval is pre-processed into fixed chunks~\cite{karpukhin2020dense}, introducing noise as irrelevant content may still be present in the retrieved documents. Secondly, in dual-dense retrieval models, the embeddings of queries and documents are usually generated separately, leading to minimal interaction between the document and query~\cite{khattab2021relevance}. This method of encoding limits the depth of semantic understanding possible between the query and its corresponding documents, which is critical for achieving precise retrieval outcomes. Lastly, the operational demands of Information Retrieval across expansive corpora require that all candidate documents are pre-encoded and their representations stored. This requirement constrains the scalability of dense retrievers and restricts the utility of embeddings, particularly in leveraging the extensive contextual and inferential capabilities that are characteristic of large language models (LLMs)~\cite{levine2022standing}.


To address these limitations, generative models such as GPT-3.5 and InstructGPT~\cite{brown2020language,ouyang2022training} offer an alternative by directly generating contextualized responses instead of retrieving existing documents. Approaches like GenRead~\cite{yu2022generate} first generate relevant text and then use it for answer prediction. However, generative models often struggle with factual consistency and may hallucinate information~\cite{huang2023survey}, making them less reliable for knowledge-intensive tasks.
%The limitations of retrieval-based models led to the exploration of generation-based approaches, where the focus shifted from retrieving pre-existing documents to generating contextually relevant information on the fly. LLMs like GPT-3.5 and InstructGPT~\cite{brown2020language,ouyang2022training} have demonstrated the ability to generate text, and detailed responses to complex queries without relying on external corpora. This approach, known as GenRead~\cite{yu2022generate}, generates contextual documents based on the query and then conditions the final answer prediction on these generated documents. Generative models can generate content that aligns with the specific needs of the question. Despite their potential, generative models face significant challenges, particularly for factual accuracy. Unlike retrieval-based models, which can cite external sources, generative models may hallucinate information~\cite{huang2023survey}, creating responses that are plausible but incorrect. Thus, while generative approaches offer flexibility and contextual richness, they also introduce risks that need to be carefully managed.
Given the trade-offs between retrieval and generation, hybrid models have emerged to integrate the strengths of both approaches. Merging Generator and Retriever (MGR)~\cite{abdallah2023generator,zhang2023merging} combines generated and retrieved documents, allowing models to refine answers while maintaining factual accuracy. However, hybrid models introduce challenges in document selection, requiring effective reranking strategies to prioritize relevant information. Recent work in Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL)~\cite{lewis2020retrieval,ram-etal-2023-context} highlights the importance of reranking techniques in improving answer quality.


%Given these trade-offs, hybrid models that combine retrieval and generation have emerged as promising solutions. Merging Generator and Retriever~\cite{abdallah2023generator,zhang2023merging} aims to integrate the strengths of both methods, balancing the factual grounding of retrieval with the depth and flexibility of generation. The process begins by generating contextual documents using LLM and then retrieving additional documents from an external corpus that directly relate to the query. Both the generated and retrieved documents are combined and processed by another LLM, which gives the final answer, leveraging the factual accuracy from retrieval and the contextual richness from generation. These hybrid methods face challenges in document selection, as models must balance between the precision of retrieved documents and the richer, yet potentially less accurate, content of generated responses. The success of these models in RAG and In-context Learning ~\cite{lewis2020retrieval,ram-etal-2023-context}, highlights the importance of effective reranking strategies that rerank documents most likely to contain accurate relevant information, making hybrid methods a powerful tool in improving open-domain question answering.


This paper extends prior work on retrieval-augmented language models~\cite{ram-etal-2023-context}, by incorporating generated documents into the retrieval process. We evaluate retrieval, generation, and hybrid models with a primary focus on ODQA and retrieval-augmented tasks, examining their impact on document selection, answer generation, and language modeling. Figure~\ref{fig:intro} provides an overview of these tasks and the experimental setup, illustrating how different approaches are compared in terms of retrieval effectiveness, generative capability, and reranking strategies. First, we compare the accuracy of retrieval models such as BM25~\cite{robertson2009probabilistic}, MSS~\cite{sachan2021end}, MSS-DPR~\cite{sachan2021end}, Contriever~\cite{izacard2021unsupervised}, and DPR~\cite{karpukhin2020dense} with generation-based models. Next, we examine how combining retrieval and generation methods affects performance in hybrid models. We also investigate strategies for reranking documents—an essential step in hybrid models—to determine how best to select the most relevant content for downstream tasks. Additionally, we investigate how hybrid models combining retrieval and generation perform in ODQA and Information Retrieval (IR) tasks, evaluating datasets like BEIR~\cite{thakur2021beir} and TREC~\cite{craswell2020overview}. Finally, we evaluate the impact of these methods on language modelling in reducing perplexity. In general, we try to answer a question: \emph{Which approach—retriever, generator, or hybrid—is best suited for ODQA, language modeling and information retrieval tasks?} Our contributions are as follows:

\begin{enumerate}
    \item We provide a comparison of retrieval and generation-based models, focusing on their effectiveness in ODQA and retrieval-augmented applications by incorporating generated documents into the retrieval process. 
    
    \item  We evaluate the impact of combining retrieval and generation methods in hybrid models, examining how these models perform across different tasks.
    
    \item We explore advanced document reranking methods, demonstrating how reranking enhances Information Retrieval (IR) accuracy and improves the performance of hybrid models in ODQA tasks.

    \item We provide practical insights into the strengths and limitations of retrieval, generation, and hybrid approaches in ODQA and retrieval-augmented applications
\end{enumerate}


\begin{figure}[t]

    %\centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/GRG_Figure.drawio.pdf}
        \caption{Overview of experimental setup across the three tasks of open-domain question answering (QA), Information Retrieval, and Language Modeling. 
        }
         \label{fig:intro}
    \end{minipage}\hfill
   
\end{figure}

