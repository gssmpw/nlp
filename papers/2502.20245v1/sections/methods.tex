\section{Approaches}

\subsection{Retriever Models}
Let $\mathcal{D} = {\mathbf{d}_1, \mathbf{d}_2, \dots, \mathbf{d}_M}$ be a collection of evidence documents representing a retrieval corpus. Given a query $\mathbf{q}$, an Information Retrieval (IR) model selects a subset of relevant passages $\mathcal{Z} \subset \mathcal{D}$, one or more of which will ideally contain the correct answer to $\mathbf{q}$. Our setup supports passages obtained from any retriever, whether based on sparse representations like BM25 ~\cite{robertson2009probabilistic} or dense representations such as DPR~\cite{karpukhin2020dense}, MSS~\cite{sachan2021end}, MSS-DPR~\cite{sachan2021end}, and Contriever~\cite{izacard2021unsupervised}.  BM25 is a traditional sparse retriever that ranks documents based on term frequency-inverse document frequency (TF-IDF).  DPR encodes queries and documents into dense vector representations using pre-trained transformers. The similarity between a query $\mathbf{q}$ and a document $\mathbf{d}$ is calculated as the dot product of their dense embeddings, i.e., $\text{sim}(\mathbf{q}, \mathbf{d}) = E_Q(\mathbf{q})^\top E_P(\mathbf{d})$, where $E_Q$ and $E_P$ are the encoders for the query and document, respectively. In addition to DPR, we test models such as MSS, which focuses on masked salient span prediction, and MSS-DPR, which extends DPR with additional pre-training using MSS. Another dense retriever, Contriever, is trained in an unsupervised manner using contrastive learning on text paragraphs. We assume that each retriever provides the top-K most relevant passages, denoted as $\mathcal{Z} = \{\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_K\}$.

\subsection{Generator Model}
The generator approach is an alternative to the traditional retriever. Rather than relying on retrieving documents from an external corpus, it prompts an LLM to generate contextual documents based on a query.  Formally, given a question $\mathbf{q}$, the model generates a set of contextual documents $\mathcal{G} = \{\mathbf{g}_1, \mathbf{g}_2, \dots, \mathbf{g}_n\}$, where each document $\mathbf{g}_i$ is generated by a large language model, such as InstructGPT~\cite{ouyang2022training}, conditioned on the query $\mathbf{q}$. %Unlike traditional retrieval models, the generator model directly generates relevant documents, potentially addressing issues related to noise in retrieved documents and the limitations of pre-existing corpora.

\subsection{Zero-Shot LLM Reranking}
\label{sec:method-zero-shot-reranking}

Zero-shot retrieval reranking~\cite{abdallah2025asrank,abdallah2025dynrank} aims to reorder retrieved or generated documents within an Information Retrieval (IR) pipeline, without relying on training data specific to the task or dataset. We explore two methods for reranking: using LLMs to generate a question from a document and a zero-shot ranking approach based on semantic matching.

In the first approach, inspired by the Unsupervised Passage Re-ranking (UPR)~\cite{sachan2022improving}, a pre-trained LLM estimates the likelihood of generating a question given the passage. Formally, for each passage $\boldsymbol{z}_i$ from the set of top-K retrieved passages $\mathcal{Z}$, we compute the relevance score $p(\boldsymbol{q} \mid \boldsymbol{z}_i)$, where $\boldsymbol{q}$ is the input question.
The score is estimated by calculating the likelihood of generating the question $\boldsymbol{q}$ given the passage $\boldsymbol{z}_i$. The LLM computes the average log-likelihood of question tokens: \(\log p(\boldsymbol{q} \mid \boldsymbol{z}_i) = \frac{1}{|\boldsymbol{q}|} \sum_{t} \log p(q_t \mid \boldsymbol{q}_{<t}, \boldsymbol{z}_i; \Theta)\) where $\Theta$ are the parameters of the LLM. The generated question serves as a query, and we rank the passages based on how well the passage can produce the question. 
The second approach, called RankGPT~\cite{sun-etal-2023-chatgpt}, utilizes a permutation-based approach for re-ranking top-$K$ retrieved documents using LLMs like GPT-3.5 and GPT-4. Unlike traditional methods that evaluate documents independently, RankGPT optimizes the order of all retrieved documents by considering their relevance to a query.
RankGPT inputs a set of retrieved documents into an LLM, where each document is tagged with an identifier (e.g., $[1], [2]$, etc.). The LLM generates a ranking by outputting a permutation of these identifiers based on their relevance to the input query. This process directly produces a ranked list without relying on intermediate scoring, leveraging the LLM’s understanding of instructions for ranking.  %To accommodate the token limitations of LLMs, RankGPT uses a sliding window technique for re-ranking larger sets of documents. This method involves re-ranking smaller subsets of documents sequentially. %For example, RankGPT processes the last few documents, then moves backwards in steps, re-ranking the documents until the entire set is covered. 


\subsection{Retrieval Augmented Generation Language model}
\label{sec:in-context-learning}

In-context learning enables LLMs to utilize external knowledge without altering their internal parameters. This approach enriches the context by incorporating both retrieved and generated documents directly into the model's input sequence. Given a sequence of tokens \(x_1, \ldots, x_n\), where \(x_1, \ldots, x_{i-1}\) represents the tokens preceding the current token \(x_i\), the goal is to predict \(x_i\). The standard formulation is:
 \(
p(x_1, \ldots, x_n) = \prod_{i=1}^n p_{\theta}(x_i | x_{<i}),
\)
where \(p_{\theta}(x_i | x_{<i})\) is the conditional probability of generating \(x_i\) based on its prefix \(x_{<i}\), with \(\theta\) representing the model’s parameters. We extend this by incorporating a set of retrieved documents \(\mathcal{R}(x_{<i})\) and a set of generated documents \(\mathcal{G}(q)\), where \(q\) is the query derived from the input prefix. The generation probability is:
 \(
p(x_1, \ldots, x_n) = \prod_{i=1}^n p_{\theta}(x_i | [x_{<i}; \mathcal{R}(x_{<i}); \mathcal{G}(q)]),
\)
where \([a; b; c]\) denotes the concatenation of sequences \(a\), \(b\), and \(c\). This setup allows the LLM to condition its output on both retrieved knowledge and newly generated content, providing a richer context for generation.


For each query \(q\) derived from the prefix tokens \(x_{<i}\), a set of top-$k$ relevant documents \(\{d_1, d_2, \ldots, d_k\}\) is retrieved, forming \(\mathcal{R}(q)\). Simultaneously, the model generates a set of contextual documents \(\mathcal{G}(q) = \{\mathbf{g}_1, \mathbf{g}_2, \ldots, \mathbf{g}_n\}\), where each \(\mathbf{g}_i\) is conditioned on \(q\). These documents serve as additional context to inform the answer generation. The combined influence of retrieved and generated documents allows the model to maximize the likelihood of generating the next token \(x_i\):
 \(
    d_{i^*}, g_{j^*} = \arg\max_{d \in \mathcal{R}(q), g \in \mathcal{G}(q)} p_{\theta}(x_i | [x_{<i}; d; g]),
\)
where \(d_{i^*}\) is the most relevant retrieved document and \(g_{j^*}\) is the most informative generated document for the given prefix and query. By balancing the factual accuracy of retrieved documents with the creative and contextual insights from generated documents, this hybrid approach enhances the model’s ability to address complex information needs. 
Building upon~\cite{ram-etal-2023-context}, which focuses on retrieval-augmented language models (RALMs), we extend the framework by incorporating generated documents into the retrieval process alongside retrieved passages. Unlike prior work, which primarily relies on external retrieval, we explore the trade-offs between retrieved and generated context within hybrid models, analyzing their impact on factual consistency and response diversity. Our analysis in Section~\ref{sec:exp-setup} reveals that while this hybrid retrieval-augmented generation approach improves recall in QA tasks, it introduces challenges related to retrieval redundancy, factual consistency, and ranking biases in hybrid models.

