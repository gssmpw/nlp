\begin{comment}
\section{Related Work}
Recent advancements in Open-Domain Question Answering (ODQA), Document Reranking, and Language Modeling (LM) have led to the development of diverse architectures aimed at improving accuracy and efficiency. These methods broadly fall into three categories: retriever-based models, generator-based models, and hybrid approaches that combine retrieval with generation. Additionally, retrieval-augmented techniques have been explored in language modeling to enhance factual consistency. In this section, we provide an overview of the most relevant approaches.

\subsection{Retriever-Based Methods}
Retriever-based methods serve as a foundation for many knowledge-intensive tasks by identifying relevant documents before further processing. Classical sparse retrieval methods, such as TF-IDF and BM25~\cite{robertson2009probabilistic}, use lexical matching to rank documents but often struggle with capturing semantic relationships. To overcome these limitations, dense retrieval models such as Dense Passage Retrieval (DPR)~\cite{karpukhin2020dense} and Contriever~\cite{izacard2021unsupervised} leverage transformer-based architectures to encode queries and documents into dense vector representations, allowing for more effective retrieval in high-dimensional space. %DPR, for example, trains a dual-encoder model with contrastive learning to improve retrieval precision. 
Other works, such as ANCE~\cite{xiong2020approximate}, ColBERT~\cite{khattab2021relevance}, and MSS-DPR~\cite{sachan2021end}, further optimize dense retrieval. %by incorporating dynamic indexing and late interaction mechanisms. 
Some retriever-only approaches treat QA as phrase retrieval, bypassing full-document retrieval. Dense phrase retrieval models~\cite{lee2020learning,seo2019real} learn to extract answer spans directly from indexed passages. However, despite their efficiency, retrieval-only methods often lack reasoning capabilities, necessitating the use of generative models.


\subsection{Generator-Based Methods}
Generator-based models take a different approach by directly generating responses instead of retrieving pre-existing documents. Large Language Models (LLMs) like GPT-3.5 and InstructGPT~\cite{brown2020language,ouyang2022training} have demonstrated strong performance in open-ended question answering and text generation. These models are capable of generating responses without external retrieval, making them suitable for tasks requiring abstraction and reasoning. A key advancement in this area is the Generator-Reader paradigm, where synthetic documents are generated before answering queries. GenRead~\cite{yu2022generate} and DocGen~\cite{askari-etal-2023-expand} expand queries by generating contextual documents and then extracting answers from them. This approach improves answer diversity but often struggles with factual consistency since generative models can hallucinate information~\cite{huang2023survey}. Some works mitigate these limitations by fine-tuning models with external knowledge. T5-RC~\cite{raffel2020exploring} and BART-based retriever-reader models~\cite{lewis2020retrieval} improve factual reliability by pretraining on structured datasets. However, without retrieval grounding, generative models may still produce plausible yet incorrect responses.

\subsection{Hybrid Approaches}
To leverage the factual accuracy of retrieval and the flexibility of generation, hybrid models have emerged, integrating retrieval into the generation process. Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval} first retrieves relevant documents and then conditions generation on these documents. This approach has been extended by models such as FiD (Fusion-in-Decoder)~\cite{izacard2020leveraging}, which improves multi-document conditioning, and Merging Generator and Retriever (MGR)~\cite{abdallah2023generator,zhang2023merging}, which dynamically selects between retrieved and generated content. One of the key challenges in hybrid approaches is document reranking. Since retrieval produces multiple documents, reranking methods are needed to prioritize the most relevant ones. Neural rerankers such as MonoBERT~\cite{nogueira2019passage} and DuoBERT~\cite{nogueira2020document} refine rankings by re-scoring retrieved documents using transformer-based architectures. Re-ranking plays a critical role in ODQA, ensuring that models prioritize the most informative sources over noise. Hybrid methods have shown particular success in In-Context Learning (ICL)~\cite{ram-etal-2023-context}, where retrieval helps LLMs recall relevant knowledge dynamically. Instead of relying solely on pretrained knowledge, models can retrieve context-specific examples from external databases, enhancing their ability to answer complex queries.

\subsection{Retrieval-Augmented Language Modeling} 
Beyond question answering, retrieval is also used to improve language modeling by incorporating external knowledge into next-token prediction. One prominent approach is $k$NN-LM~\cite{knn-lm}, which refines predictions by interpolating between a language modelâ€™s output and the nearest neighbors retrieved from a large corpus. While this method enhances factuality, it suffers from scalability issues due to large storage requirements~\cite{he-etal-2021-efficient,retomaton}. Another major direction is Retrieve-and-Read models, where documents are retrieved in real-time to inform text generation. REALM~\cite{realm} enhances masked language modeling by retrieving supporting documents, while RETRO~\cite{retro} improves LLM performance by integrating retrieved chunks via cross-attention mechanisms. These retrieval-enhanced models bridge the gap between traditional LMs and knowledge-grounded systems, allowing for more accurate and contextually aware responses.

\end{comment}

\section{Related Work}
Advancements in Open-Domain Question Answering (ODQA), Document Reranking, and Language Modeling (LM) have led to three main approaches: retriever-based, generator-based, and hybrid models, along with retrieval-augmented techniques for improving factual consistency in text generation.

Retriever-based methods focus on identifying relevant documents before processing. Sparse models like BM25~\cite{robertson2009probabilistic} rely on lexical matching, while dense retrieval methods such as DPR~\cite{karpukhin2020dense} and Contriever~\cite{izacard2021unsupervised} encode queries and documents into dense vectors for improved retrieval. Further refinements include ANCE~\cite{xiong2020approximate}, ColBERT~\cite{khattab2021relevance}, and MSS-DPR~\cite{sachan2021end}. Some models bypass full-document retrieval, using dense phrase retrieval~\cite{lee2020learning,seo2019real} to extract answer spans directly.

Generator-based models generate responses rather than retrieving documents. LLMs like GPT-3.5 and InstructGPT~\cite{brown2020language,ouyang2022training} perform well in open-ended QA but struggle with factual accuracy, often hallucinating information~\cite{huang2023survey}. Models like GenRead~\cite{yu2022generate} and DocGen~\cite{askari-etal-2023-expand} generate contextual documents before extracting answers. Pretrained models like T5-RC~\cite{raffel2020exploring} and BART-based retriever-reader architectures~\cite{lewis2020retrieval} attempt to improve reliability, but consistency issues persist.

Hybrid approaches integrate retrieval and generation, balancing factual grounding with contextual flexibility. RAG~\cite{lewis2020retrieval} conditions generation on retrieved documents, while FiD~\cite{izacard2020leveraging} enhances multi-document conditioning. Merging Generator and Retriever (MGR)~\cite{abdallah2023generator,zhang2023merging} dynamically selects between retrieved and generated content. Neural rerankers like MonoBERT~\cite{nogueira2019passage} and DuoBERT~\cite{nogueira2020document} refine retrieval results. Hybrid models have also been effective in In-Context Learning (ICL)~\cite{ram-etal-2023-context}, allowing LLMs to retrieve external knowledge dynamically.

Retrieval-augmented language models improve factual consistency by conditioning text generation on retrieved knowledge. $k$NN-LM~\cite{knn-lm} enhances predictions using nearest-neighbor retrieval but faces scalability challenges~\cite{he-etal-2021-efficient,retomaton}. Retrieve-and-Read models like REALM~\cite{realm} and RETRO~\cite{retro} integrate retrieval into masked modeling or cross-attention to improve factual reliability. These approaches bridge the gap between traditional LMs and knowledge-grounded systems, enhancing generated response accuracy.