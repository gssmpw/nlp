
%\section{Results}
\begin{table}[ht]

%\begin{minipage}{0.45\textwidth} 
%\end{minipage}\hfill
%\begin{minipage}{0.3\textwidth}  % Second table taking 40% width
    \centering
    \setlength\tabcolsep{5pt}
    \resizebox{0.40\textwidth}{!}{
    \begin{tabular}{l|ccc}
    \toprule
    \multirow{1}{*}{Models}& \multicolumn{1}{c}{NQ}  &  \multicolumn{1}{c}{TriviaQA} & \multicolumn{1}{c}{WebQ}   \\
    % & test & test & test  \\  
    \midrule
    \multicolumn{4}{c}{\textbf{Retriever Only}} \\
    \midrule
    \textbf{REALM}~\cite{guu2020retrieval}&   40.4  &  - &  40.7    \\
    \textbf{DPR}~\cite{karpukhin2020dense}&   41.5   & 56.8 &  41.1  \\
    \textbf{RAG}~\cite{lewis2020retrieval}& 44.5   & 56.1 &45.2  \\
    \textbf{FiD-l}~\cite{izacard2020leveraging}&46.7   & 61.9 & 48.1 \\
    \textbf{FiD-xl}~\cite{izacard2020leveraging}&50.1   & 66.3 & 50.8 \\
    \textbf{FiD-xl}~\cite{izacard2020leveraging} &45.0  &70.1 &53.6   \\
    \textbf{FiD}~\cite{izacard2020leveraging}&51.4  &67.6 &50.5    \\
    \textbf{EMDR}~\cite{singh2021end}& 52.5  & 71.4 & 48.7  \\
    \textbf{RFiD-large}~\cite{wang2023rfid} &  54.3   & 72.6 & -  \\
    \textbf{DensePhrases}~\cite{lee2020learning}&14.5  &34.4 & 17.3   \\
    \textbf{DensePhrases}~\cite{lee2021learning}& 41.3  &53.5 & 41.5  \\
    \midrule
    \multicolumn{4}{c}{\textbf{Generator}} \\
    \midrule
    \textbf{GenRead (FiD-l)}~\cite{yu2022generate}  & 40.3 & 67.8 & 51.5   \\
    \textbf{GenRead (FiD-l)}~\cite{yu2022generate}   & 43.5& 70.2 & 53.3   \\
    \textbf{GenRead (FiD-xl)}~\cite{yu2022generate}   & 42.6& 69.6 & 52.6  \\
    \textbf{GenRead (FiD-xl)}~\cite{yu2022generate}  & 45.6  & 71.6 &\textbf{ 54.4 } \\
    \midrule
    \multicolumn{4}{c}{\textbf{Generator and Retriever}} \\
    \midrule
    Combine Document &  \textbf{57.4}  &  \textbf{75.7}  & 53.6   \\
    \bottomrule
    \end{tabular}}
    \caption{Exact match (EM) performance of Llama 2-7B trained on retrieved and generated documents compared to baseline models across NQ, TriviaQA, and WebQ datasets using DPR and Generated Documents.}
    \label{tab:odqaresult}
\end{table}
\section{Open-Domain QA Results}
\subsection{Retrieval and Generation Results:}




In this section, we evaluate the performance of various retrieval-based models and a generative model, GenRead, on three open-domain QA datasets: \texttt{NQ}, \texttt{TriviaQA}, and \texttt{WebQ}. We compare unsupervised retrievers (BM25, MSS, and Contriever), supervised retrievers (DPR and MSS-DPR), and the GenRead generative model to understand their effectiveness in different retrieval settings. Table \ref{tab:retreivier_generator} presents the results for Top-1, Top-5, Top-10, Top-20, Top-50, and Top-100 retrieval accuracies across the three datasets. For the unsupervised retrieval models, Contriever achieves the highest Top-1 accuracy on NQ, outperforming BM25 and MSS by capturing deeper semantic relationships between queries and passages, consistent with findings by~\cite{izacard2021unsupervised}. However, the supervised retrievers show a clear advantage, with MSS-DPR achieving the highest Top-1 accuracy of 50.17\% on NQ, demonstrating the impact of training on specific QA datasets like NQ~\cite{sachan2021end}.
\begin{table}[!t]
\centering
\small
\setlength\tabcolsep{2pt}
\begin{adjustbox}{width=0.5\textwidth,center}
\begin{tabular}{l cc | cccccccc | c }

\toprule
\textbf{Method} & DL19 & DL20 & Covid &  NFCorpus &  Touche &  DBPedia & SciFact &  Signal & News &  Robust04 & BEIR (Avg) \\

\midrule



BM25
& 50.58 & 47.96 & 59.47 & 30.75 & 44.22 & 31.80 & 67.89 & 33.05 & 39.52 & 40.70 & 43.42\\


\midrule
\textbf{Supervised} \\
\midrule

monoBERT (340M)
& 70.50 & 67.28 & 70.01 & 36.88 & 31.75 & 41.87 & 71.36 & 31.44 & 44.62 & 49.35 & 47.16
\\

monoT5 (220M)
& 71.48 & 66.99 & 78.34 & 37.38 & 30.82 & 42.42 & 73.40 & 31.67 & 46.83 & 51.72 & 49.07
\\

monoT5 (3B)
& 71.83 & 68.89 & 80.71 & \textbf{\underline{38.97} } & \textbf{32.41} & \textbf{44.45} &  \underline{\textbf{76.57}} & 32.55 & 48.49 & \textbf{56.71} & \textbf{51.36}
\\

TART-Rerank (FLAN-T5-XL)
& -&- &75.10  &36.03 & 27.46 & 42.53 & 74.84 & 25.84 & 40.01 & 50.75 & -
\\

Cohere Rerank-v2
& \textbf{\underline{73.22} } & \textbf{67.08} & 81.81 & 36.36 & 32.51 & 42.51 & 74.44 & 29.60 & 47.59 & 50.78 & 49.45
\\

 Cohere Embedding-large&-&-
 & 80.10 & 34.70 & 27.60 & 37.20 & 72.10 & 30.60 & 46.10 & 48.90 & 47.16
 \\

 OpenAI Embedding-ada&-&-
 & \textbf{81.30} & 35.80 & 28.00 & 40.20 & 73.60 & \textbf{32.90} & \textbf{49.50} & 50.90 & 49.02
\\




\midrule
\textbf{Unsupervised} \\
\midrule

UPR (FLAN-T5-XL)
& 53.85 & 56.02 & 68.11 & 35.04 & 19.69 & 30.91 & 72.69 & 31.91 & 43.11 & 42.43 & 42.99
\\

% SGPT-CE (6.1B)
 %& 79.10 & 34.70 & 23.40 & 37.00 & 68.20 & 32.30 & 46.60 & 48.00 & 46.16
 %\\

%InPars (monoT5-3B)
%& - & 66.12 &  78.35 & - & - & - & - & - & - & - & -
%\\

% HyDE (text-davinci-003)
% & 59.30 & - & 27.30 & - & 44.00 & - & 46.60 & - & 36.80  & 69.10&-
% \\

 Promptagator++ (zero-shot)$^\dagger$
 & - & - & 76.0 & 36.0 & 27.8 & 41.3 & 73.6 & - & - & - & -
 \\

Promptagator++ (few-shot)
& - & - & 76.2 & 37.0 & 38.1 & 43.4 & 73.1 & - & - & - & -
\\




BM25+RankGPT & 65.80 & 62.91&  76.67&  \textbf{35.62} &\textbf{\underline{36.10} }& 44.47 & 70.43 & 32.12 & 48.85 &50.62 & 49.37\\

BM25+GenRead+RankGPT & \textbf{72.21} & \textbf{\underline{68.93}} & \textbf{\underline{82.81} } &  38.59 &  35.82 & \textbf{\underline{45.64}} &  \textbf{75.46} &  \textbf{\underline{33.64} }& \textbf{\underline{50.35} }& \textbf{\underline{58.48} }& \textbf{\underline{52.59} }\\


%BM25+G+rankgpt& 72.10 & 68.68 & 83.97 &  44.28 & 34.72 &  42.60&  40.11 & 32.05 & 50.12 & 58.46 & \\

\bottomrule
\end{tabular}
\end{adjustbox}

\caption{Results (nDCG@10) on TREC and BEIR. Best performing unsupervised and overall system(s) are marked bold. For generating documents we used LLama-3.1 70B.
}
\label{table:beir_benmark}

\end{table}


The generative model, GenRead, achieves competitive performance, particularly in TriviaQA, where it outperforms all retrievers with a Top-1 accuracy of 69.41\%. This indicates the model's ability to generate contextually relevant passages even when traditional retrieval may fall short. However, generating more passages like retrieving documents (such as Top-50 or Top-100) increases the computational costs associated with generating documents and the risk of repetitive content~\cite{ganguli2022predictability}. Generative models require significant resources to create contextually relevant documents, which becomes increasingly demanding as the number of generated documents increases. Additionally, there is a tendency for generative models to produce similar or redundant outputs when tasked with generating numerous responses for a single query~\cite{ganguli2022predictability}. 





\subsection{ Re-ranking Results}







In this section, we evaluate the impact of combining retrieval and generation through reranking methods, specifically using UPR and RankGPT, to refine document selection for open-domain tasks. Table \ref{tab:combined_reranking} illustrates the performance of different retrieval models paired with UPR and RankGPT across NQ and WebQ datasets. UPR enhances the precision of document ranking, as evidenced by DPR's improvement in Top-10 accuracy from 74.54\% to 80.44\% on the NQ dataset when combined with UPR. RankGPT, which leverages the semantic capabilities of large language models for reranking, achieves further gains, particularly for hybrid methods. For example, MSS-DPR with RankGPT achieves a Top-10 accuracy of 81.47\% on NQ, compared to 77.87\% without reranking. Combining retrieval and generative outputs, especially with methods like \texttt{Combined+RankGPT}, provides a balance between broad retrieval coverage and specific contextual information from LLMs. This is demonstrated by the MSS-DPR method achieving a Top-100 accuracy of 91.16\% on NQ when paired with RankGPT. Additionally, on the TriviaQA dataset as shown in Figure \ref{fig:tqa_retreiver_combined_reranking} (see Appendix~\ref{app:trivialqa}), the UPR method's performance comparison reveals that the GenRead+UPR retriever reaches the highest Top-1 accuracy at 69.74\%, while DPR+GenRead+UPR and MSS-DPR+Gen+UPR follow closely with Top-1 accuracies of 67.50\% and 67.30\%, respectively. Hybrid methods like BM25+Gen+UPR and DPR+Gen+UPR excel in Top-10 accuracy, achieving 84.41\% and 85.03\%, respectively, showing the benefit of combining generative context with retrieval outputs. However, we note that RankGPT's reranking process is computationally expensive, costing over $1,500$ for evaluations across NQ and WebQA, which limited its application to only UPR for the TriviaQA dataset in our experiments. 


%\end{minipage}
%\end{minipage}\hfill
%\begin{minipage}{0.5\textwidth}





\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/Perplexity.pdf}
    \caption{Perplexity Comparison for Language Modeling with Retrieval, Generation, and Hybrid Context Strategies using Different Generator Documents. The figure is divided into two subplots, each representing a different document generator used for providing context: (a) Llama-3 70B Generator Document and (b) GPT3.5 Generator Document. In both subplots, language model perplexity is evaluated under several context strategies: 'No Context' (baseline), 'R' (Retrieval-only using BM25 from Wikipedia), 'G' (Generation-only, context from a generated document), 'R+G' (Retrieval followed by Generation), and 'G+R' (Generation followed by Retrieval).  %Note that the 'Retrieval (R)' context using Llama-3 70B Generator is displayed in both subplots for direct comparison. Lines represent the trend of perplexity (lower is better) across different language models for each context strategy.  The figure illustrates that while retrieval-based context ('R') often improves perplexity compared to 'No Context', hybrid approaches (R+G, G+R) do not consistently outperform retrieval-only methods. Furthermore, the choice of generator document (Llama-3 70B vs. GPT3.5) impacts the performance of generation-based and hybrid strategies, although the 'Retrieval-only' strategy generally shows more consistent and competitive perplexity.
    }
    \label{fig:Perplexity-language-model}
\end{figure*}





%\vspace{-5mm}
\subsection{In-Context for Open-Domain QA}

In this section ,we evaluate a Retrieval-Augmented Generation (RAG) setting, where retrieval-based models (BM25, MSS-DPR) are combined with generative models (e.g., InstructGPT, GenRead). The retrieved documents serve as input to a large language model, which generates the final response. %We employ several retrievers (BM25, MSS, Contriever, and DPR) and generation models (GPT3.5) across NQ, TriviaQA, and WebQA.
Similar to prior works~\cite{karpukhin-etal-2020-dense,izacard2020leveraging}, our setup involves retrieval (R), generation (G), and hybrid models (R+G, G+R). Furthermore, we implement reranking techniques such as UPR and RankGPT to refine the document selection process. Our reader model (LLama-3.3 8B) gets the question along with its corresponding retrieved documents and returns the answer. Reader models are simply a frozen large LM (not pre-trained, fine-tuned).  Table~\ref{tab:em_comparison}, shows that hybrid models (R+G, G+R) when combined with reranking approaches such as RankGPT offer a more balanced approach across various datasets. For example, \texttt{BM25 R+G} achieves a score of 25.51 on NQ, in comparison with both retrieval-only (14.90) and generation-only (24.68) models in the \texttt{LLama V3.3 8B} setup. In particular, RankGPT consistently enhances performance, with BM25+RankGPT achieving 28.45 on NQ and 19.73 on WebQA, which highlights the effectiveness of reranking in refining document selection for question answering.


\vspace{-2mm}
\subsection{Supervised open-domain QA}


In this section, we evaluate the Fusion-in-Decoder (FiD) model~\cite{izacard2020leveraging} using LLama-2 7B. We integrate retrieved documents from DPR with generated content, leveraging both retrieval and generative for enhanced question-answering EM.  We compare Finetuned LLama-2 7B with other Retrieve-Read models, including DPR~\cite{karpukhin2020dense}, REALM~\cite{guu2020retrieval}, RAG~\cite{lewis2020retrieval}, and  GENREAD~\cite{yu2022generate}. Table \ref{tab:odqaresult} presents the EM for Llama 2-7B trained on retrieved and generated documents across the benchmarks. As seen, DPR combined with the generative model achieves competitive results, with an EM score of 57.4 on the NQ test set, 75.7 on TriviaQA, and 53.6 on WebQ. This performance is compared with baseline models such as REALM, and the FiD variants, showing improvements in most cases. For instance, the Trained LLama Model on generated and retrieved outperforms FiD-xl (50.1 EM on NQ) with a 7.3\% increase when using DPR.


\vspace{-4mm}

\section{Language Modeling Performance}\label{sec:5}
%\vspace{-3mm}

In this section, we explore the performance of retrieval (R), generation (G), and hybrid models (R+G and G+R) for language modeling on their impact on perplexity, a widely used evaluation metric for language models. The models are evaluated on the \texttt{WikiText-103} test set using \texttt{GPT2}, \texttt{OPT} and \texttt{GPT-Neo}, as presented in Figure~\ref{fig:Perplexity-language-model} (see Table~\ref{tab:language_model_retrieval_results} in the Appendix). We aim to analyse how retrieving documents from Wikipedia affects perplexity and can generate documents that can help also like in the ODQA task. Table \ref{tab:language_model_retrieval_results} shows that using GPT-2 with BM25 retrieval achieves a perplexity of 29.56, outperforming the generation-based (Llama-3.3 70, GPT3.5) model, which yields a perplexity of 42.20 and 39.27. The perplexity of retrieval models is $\text{Perplexity} = \exp\left(- \frac{1}{N} \sum_{i=1}^{N} \log p_\theta(w_i | \text{context}) \right)$, where $N$ represents the total number of words and $p_\theta(w_i | \text{context})$,
where $w_i$ is the $i$-th word in the sequence, and $p_\theta(w_i | \text{context})$ represents the probability assigned by the model $\theta$ given the retrieved context.  Hybrid models that combine retrieval and generation in two configurations do not outperform retrieval-only models as seen in Table \ref{tab:language_model_retrieval_results}. For instance, in LLama-3.3 70B, the R+G setup yields a perplexity of 32.14, compared to the retrieval-only model's 24.91. 

\section{Information Retrieval Performance}

Finally, we present an evaluation of the \texttt{BM25+GEN+RankGPT} method against state-of-the-art supervised and unsupervised models for information retrieval on the TREC and BEIR benchmarks. We focus on nDCG@10 scores across BEIR datasets. The supervised baselines include monoBERT~\cite{nogueira2019passage}, monoT5~\cite{nogueira2020document}, and Cohere Rerank. The unsupervised baselines include UPR~\cite{sachan2022improving}, and Promptagator++~\cite{dai2022promptagator}. Table \ref{table:beir_benmark} presents that the \texttt{BM25+GEN+RankGPT} consistently outperforms BM25 across all benchmarks, achieving the highest nDCG@10 scores on BEIR and TREC datasets. For example, on Robust04 dataset, \texttt{BM25+GEN+RankGPT} achieves an nDCG@10 score of 58.48, compared to 43.42 for BM25. Similarly, on SciFact, the hybrid model reaches 45.64, outperforming both supervised and unsupervised baselines like monoT5 (44.45) and UPR (30.91).






\section{Conclusion}
This study compares retrieval-based, generation-based, and hybrid models across QA, reranking, information retrieval, and language modeling. Retrieval models like BM25 and DPR excel in factual accuracy, while generative models provide contextual richness but struggle with consistency. Hybrid models effectively balance retrieval and generation, enhancing QA and IR performance. However, in language modeling, hybrid and generative approaches do not consistently outperform retrieval-based methods, underscoring the importance of retrieval for factual accuracy. %Future work should refine retrieval-augmented generation and reranking strategies to optimize the integration of external knowledge with generative models.



%Our study provides a comparative analysis of retrieval-based, generation-based, and hybrid models across open-domain QA, document reranking, information retrieval, and language modeling tasks. Retrieval-based models, such as BM25 and DPR, excel in tasks requiring factual accuracy and efficient document retrieval, making them well-suited for information retrieval and reranking. Generative models, while effective in producing contextually rich responses, struggle with factual consistency, particularly in knowledge-intensive tasks. Hybrid approaches, which integrate retrieval and generation, demonstrate strong performance in open-domain QA and information retrieval by balancing factual grounding with contextual flexibility.

%However, in language modeling tasks, hybrid and generative models still fail to consistently outperform retrieval-based approaches, highlighting the continued importance of retrieval mechanisms for maintaining factual accuracy. These findings emphasize the need for further research into refining retrieval-augmented generation and advanced reranking strategies, ensuring models can leverage both external knowledge and generative capabilities effectively.





