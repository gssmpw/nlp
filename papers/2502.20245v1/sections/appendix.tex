\section{Appendix}

This appendix provides detailed insights into the retrieval and reranking results discussed in the main paper. We present performance evaluations of different retrieval methods, including BM25, DPR, MSS, and hybrid approaches (R+G, G+R) across multiple models. The results demonstrate the impact of reranking and retrieval-augmented generation (RAG) techniques on various question-answering benchmarks.



\subsection{Retrieval Performance on TriviaQA}
\label{app:trivialqa}

Retrieval plays a fundamental role in the TriviaQA dataset, where models must extract relevant information from large document collections to answer trivia-based, open-domain questions. This section provides a detailed comparison of various retrieval methods, including sparse retrievers like BM25, dense retrievers such as DPR, and generator models. The retrieval effectiveness of these models is measured using Top-1, Top-5, and Top-10 accuracy, which represent the percentage of cases in which a correct document appears within the top-k retrieved results.

The results show that DPR achieves the highest Top-1 accuracy at 75.4\%, significantly outperforming BM25, which achieves only 54.0\%. This indicates that sparse retrieval methods struggle with the complexity of trivia-style questions, while dense retrieval models that leverage learned representations of queries and documents exhibit superior retrieval effectiveness. MSS-DPR follows closely, with a Top-1 accuracy of 73.5\%, suggesting that additional pretraining techniques further enhance retrieval performance. Generative augmentation also proves valuable, as GenRead achieves a Top-1 accuracy of 69.7\%, surpassing BM25 and approaching the effectiveness of dense retrievers.

The advantages of generator approaches become more evident in the Top-5 and Top-10 accuracy metrics. MSS-DPR+Gen leads with an 85.0\% Top-5 accuracy, followed closely by DPR+Gen at 84.4\%, indicating that the combination of retrieval and generation improves ranking effectiveness. BM25+Gen also sees significant improvements, achieving 84.4\% in Top-5 accuracy, compared to BM25 alone at 73.6\%. In the Top-10 retrieval setting, hybrid models consistently outperform retrieval-only methods, with DPR+Gen reaching 85.2\% and MSS-DPR+Gen achieving 85.0\%. These findings confirm that hybrid approaches, which integrate retrieval with generative document expansion, provide more robust and reliable retrieval for complex QA tasks.


\begin{figure*}[t!]
    \centering
    \begin{minipage}{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/tqa_retriever_reranking_bold_fixed.pdf}
        \caption{Comparison of retrieval methods using the UPR approach on the TriviaQA dataset, highlighting Top-1, Top-5, and Top-10 accuracy (Gen refers to GenRead method). The results showcase the impact of different retrieval and reranking strategies.
        }
        \label{fig:tqa_retreiver_combined_reranking}
    \end{minipage}\hfill
\end{figure*}
\subsection{Language Model Perplexity Evaluation}
This section evaluates the impact of retrieval on language modeling performance, measured using perplexity (PPL). Lower perplexity values indicate a model's ability to generate more fluent and contextually appropriate text, making it a key metric for evaluating generative language models. We compare various configurations, including retrieval-based (R), generative (G), and hybrid retrieval-generation approaches (R+G, G+R), across different model sizes, including GPT-2, OPT, and GPT-Neo, on the WikiText-103 benchmark.

The results indicate that retrieval (R) consistently improves language model performance, leading to substantial reductions in perplexity. For instance, GPT-2 Small achieves a perplexity of 37.5 without any augmentation, which drops to 29.56 with BM25-based retrieval, representing a 21.2\% improvement. Similarly, GPT-2 XL sees its perplexity reduced from 20.0 to 16.56 when retrieval is applied, highlighting the benefits of factual grounding in reducing language modeling uncertainty. Larger models such as OPT-13B achieve the lowest perplexity, with a reduction from 12.7 to 10.83 when retrieval is used.

Generative (G), in contrast, does not always lead to improvements in perplexity. In several cases, generating context without retrieval increases perplexity, as seen with GPT-2 XL, where perplexity rises from 20.0 to 24.91 when using generation alone. This suggests that generative models may introduce hallucinations, making their predictions less certain and increasing the likelihood of generating inaccurate text. The same trend is observed in smaller models, such as OPT-125M, where perplexity worsens from 40.1 to 48.12 with generation.

Hybrid approaches that combine retrieval and generation yield mixed results. The retrieval-first strategy (R+G) consistently outperforms the generation-first approach (G+R), as seen in GPT-2 XL, where R+G achieves a perplexity of 19.22 compared to 20.94 for G+R. The results confirm that retrieval should precede generation for maximum benefit, ensuring that the generative model conditions its output on factually accurate retrieved information. 

\begin{table}[t]
\centering
\setlength\tabcolsep{5pt}
\begin{adjustbox}{width=0.5\textwidth,center}
\begin{tabular}{@{}l|c|c|ccc|ccc@{}}
\toprule
\multirow{2.6}{0pt}{\textbf{Model}} & \multirow{2}{*}{No Context} & &\multicolumn{3}{c|}{\textbf{LLama-3.3 70B}  } & \multicolumn{3}{c}{\textbf{GPT3.5} }\\
\cline{4-9}
&  & R & G & R+G  & G+R  & G & R+G  & G+R\\
\midrule
\textbf{GPT-2 S} &  37.5& 29.56 & 42.20 & 32.14& 34.77   & 39.27 &30.92 & 31.56\\
\midrule
\textbf{GPT-2 M} &  26.3 & 21.45 &  33.23 & 24.46 & 27.12 & 28.68 & 22.78 & 23.48\\
\midrule
\textbf{GPT-2 L}  & 22.0 & 18.08 & 27.61 & 20.89  & 23.09 & 23.83 & 19.17 &  19.69\\
\midrule
\textbf{GPT-2 XL}  &   20.0&   16.56 &  24.91 & 19.22 & 20.94 & 21.50 &17.51 & 17.91\\
\midrule
\textbf{opt-125m}  &   40.1&  32.03  &  48.12 & 36.56 & 39.39 & 43.58 & 34.23 & 34.88\\
\midrule
\textbf{opt-350m}  &  30.4 &  24.41  & 37.22  &  28.20 & 30.57 & 32.32  & 25.71 &  26.27 \\
\midrule
\textbf{opt-1.3b}  &   19.2&  15.91  &  23.74 &  18.60 & 20.25 & 20.34& 16.68 &17.01 \\
\midrule
\textbf{opt-2.7b}  &   16.4& 13.80   &  20.94 & 16.87 &  17.84 & 17.54 &  14.67 & 14.80\\
\midrule
\textbf{opt-6.7b}  &   13.8&  11.72  &  17.52 &  14.13 & 15.07 & 14.63 & 12.31 &  12.47 \\
\midrule
\textbf{opt-13b}  &   12.7&  10.83  & 16.39  & 13.35 &  14.23 &13.46 & 11.41 & 11.52 \\
\midrule
\textbf{gpt-neo-1.3B}    & 17.5 &  14.62  &  20.45 & 16.53 & 17.38 &  18.83 & 15.53 & 15.70 \\
\midrule
\textbf{gpt-neo-2.7B}    & 15.1 &  12.85  & 18.35  & 14.97 &  15.76 & 16.35 & 13.70 & 13.86\\
\midrule
\textbf{gpt-neo-6B}    & 11.6 &  10.05  &  13.73  & 11.58 &  12.09 & 12.32 &  10.58 &  10.73\\

\bottomrule
\end{tabular}
\end{adjustbox}
\caption{
Perplexity results on the WikiText-103 test set for various models (GPT-2, OPT, GPT-Neo) using retrieval (R), generation (G), and hybrid approaches (R+G and G+R). Llama-3 70B and GPT3.5 used for generated Documents.
}
\label{tab:language_model_retrieval_results}
%\end{minipage}
\end{table}


\subsection{QA performance comparison on multiple benchmarks}

This section presents a detailed zero-shot evaluation of various retrieval and reranking methods within a retrieval-augmented generation (RAG) framework. The performance of these methods is assessed across three widely used question-answering benchmarks: Natural Questions (NQ), TriviaQA, and WebQuestions (WebQ). The study compares different retriever and reranker models, including BM25, Multi-Stage Search (MSS), Contriever, Dense Passage Retrieval (DPR), and hybrid approaches such as retrieval + generation (R+G) and generation + retrieval (G+R). The results are reported for multiple state-of-the-art large language models, including LLaMA-3 8B, LLaMA-3.1 8B, Gemma-2 (2B and 9B), LLaMA-2-13B, and Mistral-7B-v0.1.


\subsubsection{Retrieval and RAG Performance on LLaMA-3 8B and LLaMA-3.1 8B}
The performance results for LLaMA-3 8B and LLaMA-3.1 8B across different retrieval approaches are presented in Table \ref{tab:qa_LLama_3_3.1}. The models are evaluated using three primary metrics: Exact Match (EM), Recall, and Consistency (Con). The EM score measures the percentage of responses that exactly match the ground truth, Recall represents the proportion of relevant documents retrieved, and Consistency assesses the stability of the modelâ€™s generated answers across multiple retrieval settings.

For LLaMA-3 8B, the DPR+G+R hybrid method achieves the highest EM score on TriviaQA (54.50\%) and NQ (28.14\%), while for WebQ, the BM25+G and DPR+G methods perform comparably with 15.45\% and 15.50\% EM, respectively. In contrast, traditional BM25 retrieval alone exhibits significantly lower performance, achieving only 14.90\% EM on NQ, 42.10\% on TriviaQA, and 10.23\% on WebQ. The R+G (Retriever first, then Generator) approach consistently outperforms G+R, with DPR+R+G reaching 28.94\% EM on NQ and 37.31\% on TriviaQA. This demonstrates that conditioning retrieval before generation is a superior strategy for answer synthesis.

For LLaMA-3.1 8B, a general performance improvement is observed over LLaMA-3 8B. The DPR+R+G method improves EM on NQ to 30.83\% and TriviaQA to 37.89\%, indicating that a refined retriever-generator interaction further enhances retrieval-augmented generation. The MSS+DPR approach achieves a strong balance between retrieval recall and consistency, with Recall reaching 49.23\% on NQ and 73.67\% on TriviaQA, while maintaining high consistency.

\subsubsection{Performance of RAG on Gemma-2 (2B and 9B) Models}
The retrieval and reranking evaluation on Gemma-2-2B and Gemma-2-9B is presented in Table \ref{tab:qa_Gemma}. The larger Gemma-2-9B model significantly outperforms its 2B counterpart across all benchmarks, demonstrating the benefits of model scaling in retrieval-augmented question answering.

On Gemma-2-2B, DPR+R+G achieves 30.25\% EM on NQ and 37.73\% on TriviaQA, while the traditional BM25 retriever falls behind, achieving 14.02\% EM on NQ and 43.28\% on TriviaQA. The generative augmentation (G) alone leads to better recall but performs worse in terms of EM than retrieval-augmented methods, with 46.85\% Recall on NQ and 72.99\% on TriviaQA.

The Gemma-2-9B model exhibits substantial improvements, with DPR+R+G achieving 30.83\% EM on NQ and 37.89\% on TriviaQA, mirroring the performance trends observed with LLaMA-3.1 8B. Interestingly, BM25+G surpasses the retrieval-only (BM25 R) approach by achieving 63.02\% Recall on TriviaQA, reinforcing that generative augmentation benefits sparse retrieval methods. However, the best-performing approach remains MSS+DPR+R+G, which achieves a Reciprocal Rank of 49.23\% on NQ and 73.67\% on TriviaQA, emphasizing the importance of hybrid search.

\subsubsection{Retrieval-Augmented QA with LLaMA-2-13B and Mistral-7B-v0.1}
Table \ref{tab:qa_Llama-2-13b} presents the results for LLaMA-2-13B and Mistral-7B-v0.1, two widely used open-source models. LLaMA-2-13B achieves better retrieval performance than Mistral-7B, especially when combining dense retrieval with reranking.

For LLaMA-2-13B, DPR+R+G consistently achieves the highest scores, with 30.91\% EM on NQ and 38.12\% on TriviaQA, surpassing the BM25+R baseline (21.14\% EM on NQ and 57.90\% on TriviaQA). However, BM25+G shows better recall (73.63\% on TriviaQA) compared to dense retrieval methods, supporting the argument that generative augmentation enhances sparse retrieval. The hybrid approach (DPR+G+R) further improves retrieval, attaining EM scores of 30.66\% on NQ and 37.98\% on TriviaQA, slightly trailing behind DPR+R+G but still outperforming retrieval-only baselines.

For Mistral-7B-v0.1, the results indicate that BM25 alone is highly ineffective, achieving only 11.19\% EM on NQ and 52.85\% on TriviaQA. The DPR+G+R hybrid model achieves 25.07\% EM on NQ and 28.92\% on TriviaQA, demonstrating that retrieval-first approaches remain more effective than generation-first pipelines. Interestingly, UPR (Unsupervised Passage Reranking) achieves 25.24\% EM on NQ and 31.25\% on TriviaQA, proving to be a strong alternative to traditional DPR reranking.

\begin{table*}[!ht]
%\addtolength{\tabcolsep}{-0.65pt}
%\small
\centering
\resizebox{1.0\textwidth}{!}{  % Resize the table to fit the page width
\setlength\tabcolsep{3pt}
\begin{tabular}{@{}l|c | ccc| ccc | ccc |ccc | ccc | ccc @{}}
\toprule
 & & \multicolumn{9}{c|}{\textbf{LLama V3.3 8B}} & \multicolumn{9}{c}{\textbf{LLama V3.1 8B}}\\

\multirow{3}{*}{\textbf{Retriever} }& \multirow{3}{*}{\textbf{\# Doc}} & \multicolumn{3}{c}{\textbf{NQ}} & \multicolumn{3}{c}{\textbf{TriviaQA}} & \multicolumn{3}{c|}{\textbf{WebQ}} & \multicolumn{3}{c}{\textbf{NQ}} & \multicolumn{3}{c}{\textbf{TriviaQA}} & \multicolumn{3}{c}{\textbf{WebQ}} \\

                    &  & EM & Recall  & Con & EM & Recall  & Con & EM & Recall  & Con  & EM & Recall  & Con & EM & Recall  & Con & EM & Recall  & Con \\
\midrule
\multirow{4}{*}{BM25 } &       R  &    14.90  &  26.68  & 19.91  & 42.10 & 56.90 & 50.11 &  10.23 & 24.95 & 16.92 & 12.82 & 27.14 & 20.27 & 40.13 &58.40 &51.40 & 9.25 & 24.92 & 17.22 \\

&       G  &  24.68   &  45.43   &  33.85 &  52.23 &66.86 & 59.61 & 15.45  & 45.34 &35.28  &21.49  & 44.92 &33.65 & 48.74 &66.27 &  58.68 & 14.51& 45.73 & 35.03\\

&       R+G  & 25.51 & 45.22 & 34.29 & 53.29  & 70.04 & 62.75 & 15.05 & 42.75 & 32.77  & 22.29&45.46 & 34.37&  49.69 & 70.45&  62.98 & 13.97 & 43.91&33.56\\

&       G+R  &  25.67 & 45.93 &  34.70 & 53.24 & 69.54& 62.34 & 15.00 & 44.09 & 33.56  & 23.29 &46.10 &34.93 & 50.29 & 70.50 & 63.00& 13.43 & 43.53 & 32.72 \\

&   UPR   & 23.49  & 41.02 & 31.55 & 55.85 & 69.59 & 62.65 & 17.27 & 44.15 & 32.68 & 24.24 & 40.68 & 31.47 & 55.22 & 68.14 & 61.78 & 17.67 & 41.68 & 30.71 \\

&   RankGPT   & 28.45  & 42.73 & 34.02 & - & - & - & 19.73 & 40.57 & 29.82 & 26.65 & 40.89 & 32.44 & - & - & - & 18.55 & 37.92 & 28.10 \\

\midrule
\multirow{4}{*}{MSS} &       R  &  12.82 & 22.96 & 17.36 & 31.90 &44.27  & 37.75 &  7.38 & 18.9 &11.81  & 11.19 & 23.37 & 17.81 & 30.74 & 45.82 &38.91 & 6.88 & 19.30 & 12.40\\

&       G  & 24.95  &  45.82 & 34.15 &  51.69 &  66.58 & 59.22 & 15.84 &46.06 &35.77  &21.46 & 45.29 & 33.90 & 47.98& 65.57& 57.84 & 14.12 & 45.25 &45.25 \\

&       R+G  &  25.54 &   44.79 &33.96 & 51.39 &67.52 & 59.99 & 14.96 & 43.14 & 32.48 & 21.57 & 44.88 & 33.93 & 47.75& 68.22 & 60.41 & 14.46 & 44.46& 33.31 \\

&       G+R  &  25.31  & 45.34 & 34.43 & 51.81 &  67.72 & 60.12 &   15.20 &43.75  & 33.12 & 22.68 & 45.55 &34.68 & 48.75 & 68.02 & 60.27 & 13.43 & 44.14 &33.07\\

&   UPR   & 23.35  & 40.52 & 31.55 & 53.78 & 68.67 & 61.63 & 17.08 & 43.97 & 32.14 & 24.43 & 41.08 & 31.77 & 54.97 & 68.46 & 61.73 & 16.70 & 42.20 & 31.30 \\

&   RankGPT    & 28.17 & 41.67 & 33.21 & - & - & - & 19.05 & 39.33 & 29.58 & 25.84 & 39.07 & 31.41 & - & - & - & 17.37 & 36.38 & 27.21 \\

\midrule
\multirow{4}{*}{Contriever}   &       R  &  15.29  & 27.36 & 20.99 & 36.25 &  49.52 &  49.52 &  10.67 & 28.28 &  20.22 &13.24 & 27.68 &  21.88 &35.29  & 50.96 &44.19 & 9.35 &28.71 & 20.32\\

&       G  &  24.70 &  46.02 & 34.48 & 51.48  &  66.44 & 59.03 & 15.89  & 45.59 &35.23  &21.32 & 45.53 & 34.04 &  48.39 &65.82 &58.03 &  14.61 &45.67 &34.84\\

&       R+G  & 25.59  & 45.28 & 34.32 & 51.78 & 67.90 & 60.45 &   15.10 & 43.49 & 32.87 &22.18 & 45.26 & 34.45 & 48.38 & 68.84 & 61.47& 13.87 &45.07 &33.80\\

&       G+R  &  25.70 & 46.07   & 35.04  & 52.07 &  68.21& 60.98 & 15.50  &  44.08 & 33.21 & 23.13 &46.18  & 35.12 & 48.91 & 68.32 & 60.82 & 13.87 & 44.87 &34.10\\

&   UPR   & 23.24  & 41.01 & 31.61 & 53.48 & 68.70 & 61.82 & 17.32 & 44.11 & 32.97 & 24.21 & 40.99 & 31.63 & 55.71 & 68.74 & 61.98 & 17.62 & 37.73 & 28.44 \\

&   RankGPT     & 30.55  & 44.25 & 35.40 & - & - & - & 19.78 & 41.18 & 31.10 & 28.86 & 42.42 & 33.68 & - & - & - & 17.62 & 37.73 & 28.44 \\

\midrule
\multirow{4}{*}{DPR}  &       R  &   28.08 & 45.40 & 36.37 & 45.88 & 61.24 & 54.58 &  19.83 & 40.27   & 30.98  & 23.21 & 44.99 & 36.03 &43.62 & 62.61& 55.97 & 14.32 & 38.27 &28.98 \\

&       G  & 25.06 & 45.81 & 34.34 &  51.66  &  66.51 &59.08 &  15.45  & 46.32   & 36.59  &21.41 & 45.37 & 34.15 & 48.21&65.76 &57.96 & 14.76 & 45.32 &34.59\\

&       R+G  &  28.94 & 48.82 & 37.31 &   54.41 & 70.83 & 63.68 & 24.50 & 47.45  &  35.94 &24.62 & 48.50 & 37.28 & 50.61 & 71.30 & 63.99 & 14.32 & 46.55 & 34.94 \\

&       G+R  &  28.14 & 48.87 & 37.03 & 54.50& 70.53 & 63.42 &25.51 & 48.60  &  37.50&25.51 & 50.28 & 38.53 & 51.65 & 71.66 &64.30 &14.81 & 46.59& 35.23\\

&   UPR   & 23.60  & 41.18 & 31.77 & 53.41 & 68.60 & 61.60 & 18.06 & 44.01 & 32.48 & 24.74 & 41.32 & 31.75 & 56.07 & 69.14 & 62.41 & 19.64 & 39.77 & 29.87 \\

&   RankGPT    & 31.74 & 46.41 & 36.76 & - & - & - & 20.42 & 40.84 & 31.00 & 29.58 & 43.92 & 34.65 & - & - & - & 19.64 & 39.77 & 29.87 \\

\midrule
\multirow{4}{*}{MSS+DPR}&       R  &   28.17 & 46.72 & 37.00 &  47.69 & 63.66 & 57.08 &  13.92 &  40.87   & 30.57 & 23.68 & 46.70 &37.53 & 45.72 & 65.24 & 58.53 & 13.92 &38.97 & 29.67\\

&       G  &  24.73  &  45.46 & 33.85 & 51.64  &66.90 & 59.40&  14.51  & 48.74   &  37.73 & 21.80 & 45.28 & 33.90 &  47.98 & 65.51 & 57.83 &14.27 &46.14 &35.03\\

&       R+G  &  29.41 &   49.73 & 38.25 &   54.53 &70.86  &63.82 & 14.96 & 49.48  & 38.84 &24.09 & 48.80 & 37.34 & 50.72 & 71.67 & 64.35 & 14.96 &46.40 & 34.94\\

&       G+R  &  28.61 & 49.27 & 37.36 &  54.48 & 71.16  & 63.91& 14.76 &  48.80  & 37.77 & 25.54 &50.30 &38.61 & 52.20 & 72.19 & 64.86 & 14.76 &46.42 & 35.33\\

&   UPR   & 23.10  & 40.81 & 31.25 & 53.65 & 68.61 & 61.69 & 16.68 & 44.07 & 32.48 & 25.24 & 41.17 & 31.83 & 55.52 & 68.76 & 61.90 & 18.99 & 40.76 & 30.22 \\

&   RankGPT   & 31.94 & 45.95 & 36.59 & - & - & - & 21.41 & 42.54 & 32.14 & 29.95 & 45.00 & 36.49 & - & - & - & 18.99 & 40.76 & 30.22 \\

\bottomrule
\end{tabular}
}
\caption{Zero-shot results of in-context learning on
The test set of NQ, TriviaQA, and WebQ uses the LLama 3/3.1 8B Model as RAG }
\label{tab:qa_LLama_3_3.1}
\end{table*}

\begin{table*}[!ht]
%\addtolength{\tabcolsep}{-0.65pt}
%\small
\centering
\resizebox{1.0\textwidth}{!}{  % Resize the table to fit the page width
\setlength\tabcolsep{3pt}
\begin{tabular}{@{}l|c | ccc| ccc | ccc |ccc | ccc | ccc @{}}
\toprule
 & & \multicolumn{9}{c|}{\textbf{Gemma-2-2b}} & \multicolumn{9}{c}{\textbf{Gemma-2-9b}}\\

\multirow{3}{*}{\textbf{Retriever} }& \multirow{3}{*}{\textbf{\# Doc}} & \multicolumn{3}{c}{\textbf{NQ}} & \multicolumn{3}{c}{\textbf{TriviaQA}} & \multicolumn{3}{c|}{\textbf{WebQ}} & \multicolumn{3}{c}{\textbf{NQ}} & \multicolumn{3}{c}{\textbf{TriviaQA}} & \multicolumn{3}{c}{\textbf{WebQ}} \\

&  & EM & Recall  & Con & EM & Recall  & Con & EM & Recall  & Con  & EM & Recall  & Con & EM & Recall  & Con & EM & Recall  & Con \\
\midrule

\multirow{6}{*}{BM25 } &       R   & 14.02 & 25.55 & 18.53 & 43.28 & 52.78 & 47.01 & 14.71 & 37.74 & 27.21 & 19.81 & 26.95 & 22.05 & 57.55 & 65.53 & 60.29 & 14.96 & 24.87 & 20.13 \\

&       G   & 27.01 & 46.85 & 35.35 & 59.91 & 72.99 & 66.36 & 19.34 & 50.19 & 38.93 & 28.28 & 46.46 & 36.04 & 63.02 & 75.33 & 68.83 & 18.65 & 50.57 & 39.37 \\

&       R+G   & 28.39 & 46.41 & 35.62 & 59.89 & 73.39 & 66.53 & 19.29 & 47.44 & 35.93 & 28.45 & 45.09 & 35.54 & 63.50 & 75.23 & 68.50 & 19.05 & 45.38 & 34.94 \\

&       G+R   & 28.50 & 46.50 & 35.68 & 59.87 & 72.99 & 66.21 & 19.73 & 48.22 & 36.47 & 28.42 & 45.67 & 35.96 & 62.94 & 75.44 & 68.64 & 19.34 & 45.83 & 35.53 \\

&   UPR   & 26.23 & 44.37 & 33.99 & 58.71 & 71.59 & 64.79 & 19.78 & 48.16 & 36.47 & 23.41 & 41.52 & 32.24 & 58.74 & 71.92 & 65.20 & 15.94 & 46.16 & 34.15 \\

&   RankGPT   & 30.36 & 45.96 & 36.09 & - & - & - & 21.11 & 45.13 & 34.35 & 30.75 & 43.48 & 35.46 & - & - & - & 21.06 & 39.32 & 31.15 \\

\midrule
\multirow{6}{*}{MSS} &       R   & 13.96 & 25.41 & 18.50 & 33.05 & 42.24 & 36.23 & 14.71 & 37.74 & 27.21 & 19.78 & 26.86 & 22.08 & 50.93 & 58.57 & 53.28 & 14.96 & 24.92 & 20.18 \\

&       G   & 27.06 & 46.87 & 35.29 & 59.27 & 72.32 & 65.49 & 19.34 & 50.19 & 38.93 & 27.95 & 46.77 & 36.01 & 62.67 & 74.93 & 68.41 & 18.65 & 50.57 & 39.37 \\

&       R+G   & 28.48 & 46.36 & 35.48 & 58.63 & 71.48 & 64.65 & 19.29 & 47.05 & 35.78 & 28.56 & 45.27 & 35.65 & 62.53 & 73.97 & 67.27 & 18.80 & 44.44 & 34.01 \\

&       G+R   & 28.42 & 46.18 & 35.57 & 58.47 & 71.27 & 64.46 & 19.78 & 47.79 & 36.07 & 28.31 & 45.53 & 35.37 & 61.78 & 73.76 & 67.07 & 18.55 & 45.11 & 35.09 \\

&   UPR   & 26.20 & 44.14 & 33.77 & 58.15 & 71.02 & 64.10 & 19.73 & 48.92 & 36.96 & 23.10 & 41.49 & 32.05 & 57.50 & 71.15 & 64.33 & 15.85 & 47.20 & 34.65 \\

&   RankGPT   & 29.17 & 45.00 & 35.32 & - & - & - & 19.93 & 44.61 & 34.10 & 29.97 & 42.59 & 34.85 & - & - & - & 19.64 & 36.91 & 29.72 \\

\midrule
\multirow{6}{*}{Contriever}   &       R   & 13.96 & 25.41 & 18.50 & 33.05 & 42.24 & 36.23 & 14.71 & 37.74 & 27.21 & 19.78 & 26.86 & 22.08 & 50.93 & 58.57 & 53.28 & 14.96 & 24.92 & 20.18 \\

&       G   & 27.06 & 46.87 & 35.29 & 59.27 & 72.32 & 65.49 & 19.34 & 50.19 & 38.93 & 27.95 & 46.77 & 36.01 & 62.67 & 74.93 & 68.41 & 18.65 & 50.57 & 39.37 \\

&       R+G   & 28.78 & 46.98 & 36.12 & 58.86 & 71.68 & 64.81 & 20.28 & 48.42 & 37.30 & 28.84 & 45.33 & 35.90 & 62.85 & 73.89 & 67.21 & 19.64 & 46.01 & 35.43 \\

&       G+R   & 28.75 & 46.70 & 35.98 & 58.64 & 71.33 & 64.57 & 20.13 & 48.24 & 36.86 & 28.37 & 45.46 & 36.07 & 61.97 & 73.87 & 67.07 & 19.09 & 45.95 & 36.17 \\

&   UPR   & 26.26 & 44.26 & 33.82 & 58.37 & 71.17 & 64.23 & 19.83 & 48.32 & 36.61 & 23.21 & 41.46 & 32.08 & 57.64 & 71.20 & 64.36 & 16.14 & 46.12 & 34.01 \\

&   RankGPT   & 32.11 & 47.86 & 38.31 & - & - & - & 20.67 & 45.77 & 34.89 & 32.44 & 44.83 & 36.79 & - & - & - & 19.88 & 38.57 & 30.02 \\

\midrule
\multirow{6}{*}{DPR}  &       R   & 13.99 & 25.44 & 18.53 & 33.05 & 42.24 & 36.23 & 14.71 & 37.74 & 27.21 & 19.78 & 26.86 & 22.08 & 50.93 & 58.57 & 53.28 & 14.96 & 24.92 & 20.18 \\

&       G  & 27.06 & 46.87 & 35.29 & 59.27 & 72.32 & 65.49 & 19.34 & 50.19 & 38.93 & 27.92 & 46.75 & 35.98 & 62.67 & 74.93 & 68.41 & 18.65 & 50.57 & 39.37 \\

&       R+G   & 30.25 & 48.89 & 37.73 & 60.16 & 73.19 & 66.28 & 20.18 & 48.36 & 36.66 & 30.83 & 47.93 & 37.89 & 63.72 & 74.99 & 68.27 & 19.93 & 46.16 & 35.88 \\

&       G+R   & 30.72 & 48.78 & 37.45 & 60.17 & 72.98 & 66.22 & 20.52 & 48.93 & 36.86 & 29.92 & 47.48 & 37.12 & 63.18 & 75.00 & 68.29 & 19.39 & 46.59 & 36.32 \\

&   UPR   & 26.51 & 44.62 & 34.16 & 58.71 & 71.72 & 64.78 & 19.78 & 48.62 & 36.86 & 23.38 & 41.80 & 32.38 & 57.85 & 71.54 & 64.75 & 16.04 & 46.56 & 34.50 \\

&   RankGPT   & 34.16 & 50.26 & 39.42 & - & - & - & 21.21 & 46.41 & 35.48 & 34.04 & 47.14 & 38.64 & - & - & - & 21.01 & 40.41 & 31.69 \\

\midrule
\multirow{6}{*}{MSS+DPR}&       R   & 13.96 & 25.41 & 18.50 & 33.05 & 42.24 & 36.23 & 14.71 & 37.74 & 27.21 & 19.78 & 26.86 & 22.08 & 50.93 & 58.57 & 53.28 & 14.96 & 24.92 & 20.18 \\

&       G   & 27.06 & 46.87 & 35.29 & 59.27 & 72.32 & 65.49 & 19.34 & 50.19 & 38.93 & 28.25 & 46.47 & 36.01 & 62.67 & 74.93 & 68.41 & 18.60 & 46.85 & 36.32 \\

&       R+G   & 30.91 & 49.23 & 38.12 & 60.56 & 73.67 & 66.82 & 20.72 & 48.67 & 36.91 & 30.58 & 47.85 & 37.73 & 63.76 & 75.20 & 68.48 & 19.93 & 46.62 & 36.32 \\

&       G+R  & 30.66 & 49.15 & 37.98 & 60.25 & 73.53 & 66.68 & 21.26 & 49.67 & 37.70 & 29.94 & 47.61 & 37.40 & 63.17 & 75.19 & 68.37 & 20.13 & 46.90 & 37.01 \\

&   UPR   & 26.54 & 44.67 & 34.21 & 58.64 & 71.50 & 64.55 & 19.49 & 48.03 & 36.02 & 23.46 & 41.75 & 32.38 & 57.75 & 71.34 & 64.48 & 15.94 & 45.96 & 33.56 \\

&   RankGPT   & 32.51 & 49.77 & 39.53 & - & - & - & 21.41 & 47.58 & 36.52 & 32.13 & 46.55 & 38.29 & - & - & - & 21.06 & 40.72 & 32.04 \\

\bottomrule
\end{tabular}
}
\caption{
Zero-shot results of in-context learning on
The test set of NQ, TriviaQA, and WebQ uses the Gemma Model as RAG.
}
\label{tab:qa_Gemma}
\end{table*}
\begin{table*}[!ht]
%\addtolength{\tabcolsep}{-0.65pt}
%\small
\centering
\resizebox{1.0\textwidth}{!}{  % Resize the table to fit the page width
\setlength\tabcolsep{3pt}
\begin{tabular}{@{}l|c | ccc| ccc | ccc |ccc | ccc | ccc @{}}
\toprule
 & & \multicolumn{9}{c|}{\textbf{Llama-2-13b-hf}} & \multicolumn{9}{c}{\textbf{Mistral-7B-v0.1}}\\

\multirow{3}{*}{\textbf{Retriever} }& \multirow{3}{*}{\textbf{\# Mode}} & \multicolumn{3}{c}{\textbf{NQ}} & \multicolumn{3}{c}{\textbf{TriviaQA}} & \multicolumn{3}{c|}{\textbf{WebQ}} & \multicolumn{3}{c}{\textbf{NQ}} & \multicolumn{3}{c}{\textbf{TriviaQA}} & \multicolumn{3}{c}{\textbf{WebQ}} \\

&  & EM & Recall  & Con & EM & Recall  & Con & EM & Recall  & Con  & EM & Recall  & Con & EM & Recall  & Con & EM & Recall  & Con \\
\midrule

\multirow{6}{*}{BM25 } 
&       R   & 21.14 & 30.82 & 24.46 & 57.90 & 65.27 & 59.57 & 19.54 & 37.38 & 27.51 & 11.19 & 13.45 & 11.80 & 52.85 & 58.11 & 53.82 & 6.40 & 8.46 & 6.84 \\

&       G   & 28.06 & 44.60 & 34.21 & 62.64 & 73.63 & 67.13 & 20.32 & 45.54 & 34.40 & 27.01 & 41.30 & 32.19 & 62.64 & 72.63 & 66.19 & 16.09 & 33.01 & 24.70 \\

&       R+G   & 26.62 & 41.96 & 32.47 & 61.35 & 73.01 & 66.34 & 19.00 & 43.61 & 32.53 & 25.68 & 37.68 & 30.11 & 60.45 & 69.76 & 63.49 & 15.65 & 29.42 & 22.54 \\

&       G+R   & 26.79 & 43.16 & 33.35 & 62.01 & 73.14 & 66.67 & 19.24 & 43.70 & 32.68 & 23.71 & 34.64 & 27.87 & 58.56 & 67.61 & 61.63 & 13.44 & 26.74 & 20.08 \\

&   UPR   & 27.59 & 42.65 & 32.99 & 61.60 & 71.62 & 65.00 & 20.37 & 44.09 & 33.76 & 25.18 & 40.47 & 31.11 & 59.64 & 69.91 & 63.20 & 17.18 & 40.89 & 30.46 \\

&   RankGPT   & 29.22 & 43.00 & 34.21 & - & - & - & 21.99 & 41.25 & 31.15 & 25.35 & 40.47 & 31.36 & - & - & - & 17.18 & 40.90 & 30.46 \\

\midrule
\multirow{6}{*}{MSS} 
&       R   & 21.52 & 30.92 & 24.18 & 51.75 & 58.58 & 53.35 & 20.62 & 39.45 & 29.68 & 11.08 & 13.33 & 11.66 & 42.69 & 47.12 & 43.40 & 6.40 & 8.46 & 6.84 \\

&       G   & 28.01 & 43.22 & 33.74 & 60.66 & 72.01 & 65.43 & 19.44 & 44.80 & 34.20 & 27.15 & 41.40 & 32.19 & 61.28 & 71.06 & 64.71 & 16.09 & 33.01 & 24.70 \\

&       R+G   & 26.81 & 42.48 & 33.02 & 58.64 & 70.39 & 63.87 & 19.05 & 42.74 & 32.14 & 25.90 & 38.34 & 30.58 & 56.28 & 64.82 & 59.03 & 14.57 & 27.91 & 20.96 \\

&       G+R   & 27.48 & 43.97 & 33.88 & 59.56 & 71.14 & 64.63 & 18.75 & 42.48 & 31.64 & 23.10 & 33.75 & 27.26 & 52.15 & 59.83 & 54.59 & 13.09 & 24.77 & 18.31 \\

&   UPR   & 27.29 & 42.28 & 32.63 & 59.23 & 69.93 & 63.56 & 20.72 & 44.50 & 33.96 & 23.77 & 40.41 & 30.97 & 57.24 & 67.67 & 61.04 & 17.42 & 42.17 & 31.20 \\

&   RankGPT   & 27.56 & 41.77 & 33.05 & - & - & - & 20.77 & 44.50 & 33.96 & 23.77 & 39.08 & 29.28 & - & - & - & 16.98 & 39.87 & 29.43 \\

\midrule
\multirow{6}{*}{Contriever}   
&       R   & 20.47 & 30.13 & 23.85 & 42.69 & 47.12 & 43.40 & 19.98 & 37.74 & 27.61 & 11.08 & 13.33 & 11.66 & 42.69 & 47.12 & 43.40 & 6.40 & 8.46 & 6.84 \\

&       G   & 26.79 & 43.16 & 33.35 & 61.28 & 71.06 & 64.71 & 19.69 & 44.88 & 33.76 & 27.15 & 41.40 & 32.19 & 61.28 & 71.06 & 64.71 & 16.09 & 33.01 & 24.70 \\

&       R+G   & 27.45 & 42.89 & 33.27 & 56.43 & 65.00 & 59.20 & 19.59 & 43.25 & 32.19 & 25.35 & 37.57 & 30.08 & 56.43 & 65.00 & 59.20 & 15.31 & 27.88 & 20.82 \\

&       G+R   & 27.17 & 43.64 & 33.74 & 52.66 & 60.52 & 55.33 & 19.88 & 44.21 & 33.61 & 23.02 & 33.53 & 27.29 & 54.39 & 62.52 & 57.18 & 13.44 & 25.79 & 19.24 \\

&   UPR   & 26.57 & 41.99 & 32.44 & 59.60 & 70.02 & 63.58 & 20.72 & 44.65 & 33.56 & 25.10 & 40.43 & 31.02 & 57.26 & 67.63 & 61.11 & 17.27 & 40.48 & 29.97 \\

&   RankGPT   & 30.39 & 44.60 & 36.09 & - & - & - & 20.72 & 44.65 & 33.56 & 25.10 & 40.48 & 31.02 & - & - & - & 17.27 & 40.48 & 29.97 \\

\midrule
\multirow{6}{*}{DPR}  
&       R   & 21.94 & 31.36 & 24.88 & 51.07 & 57.97 & 52.71 & 19.83 & 37.35 & 28.05 & 11.11 & 13.36 & 11.69 & 42.69 & 47.12 & 43.40 & 6.40 & 8.46 & 6.84 \\

&       G  & 28.12 & 44.10 & 34.32 & 60.85 & 71.87 & 65.44 & 20.47 & 45.05 & 34.15 & 27.15 & 41.40 & 32.19 & 61.28 & 71.06 & 64.71 & 16.09 & 33.01 & 24.70 \\

&       R+G   & 28.81 & 44.73 & 34.82 & 58.00 & 67.03 & 61.14 & 20.32 & 45.14 & 34.25 & 27.70 & 40.40 & 32.47 & 58.00 & 67.03 & 61.14 & 16.44 & 32.14 & 24.31 \\

&       G+R   & 27.92 & 44.15 & 34.27 & 60.01 & 71.44 & 64.93 & 20.72 & 45.02 & 34.15 & 25.01 & 35.90 & 29.06 & 54.64 & 62.57 & 57.27 & 14.71 & 28.39 & 21.75 \\

&   UPR   & 27.45 & 42.93 & 33.02 & 60.05 & 70.49 & 63.96 & 20.62 & 44.81 & 33.91 & 25.26 & 40.71 & 31.36 & 57.53 & 67.80 & 61.31 & 17.18 & 41.20 & 30.66 \\

&   RankGPT   & 32.77 & 47.56 & 38.06 & - & - & - & 22.15 & 44.37 & 33.81 & 25.26 & 40.71 & 31.36 & - & - & - & 17.18 & 41.20 & 30.66 \\

\midrule
\multirow{6}{*}{MSS+DPR}
&       R   & 21.47 & 31.26 & 24.60 & 51.35 & 58.26 & 53.01 & 19.83 & 37.37 & 27.61 & 11.08 & 13.33 & 11.66 & 42.69 & 47.12 & 43.40 & 6.40 & 8.46 & 6.84 \\

&       G   & 28.20 & 43.75 & 34.07 & 60.44 & 71.60 & 65.09 & 20.13 & 45.50 & 34.30 & 27.15 & 41.40 & 32.19 & 61.28 & 71.06 & 64.71 & 16.09 & 33.01 & 24.70 \\

&       R+G   & 28.45 & 45.21 & 35.21 & 59.50 & 71.17 & 64.79 & 19.78 & 44.57 & 33.46 & 27.40 & 39.95 & 31.94 & 58.03 & 67.43 & 61.49 & 16.44 & 31.62 & 23.97 \\

&       G+R  & 28.73 & 44.67 & 34.79 & 59.96 & 71.90 & 65.57 & 20.77 & 45.09 & 34.10 & 25.07 & 35.95 & 28.92 & 54.39 & 62.52 & 57.18 & 14.96 & 29.13 & 22.00 \\

&   UPR   & 27.45 & 42.91 & 33.10 & 60.00 & 71.52 & 64.07 & 19.98 & 44.81 & 33.32 & 25.24 & 40.59 & 31.25 & 57.47 & 67.80 & 61.12 & 16.09 & 44.33 & 33.32 \\

&   RankGPT   & 32.61 & 48.90 & 40.19 & - & - & - & 21.95 & 43.73 & 33.32 & 25.24 & 40.59 & 31.25 & - & - & - & 16.09 & 44.33 & 33.32 \\

\bottomrule
\end{tabular}
}
\caption{
Zero-shot results of in-context learning on
the test set of NQ, TriviaQA, and WebQ  using Llama-2-13b-hf and Mistral-7B-v0.1 as RAG
}
\label{tab:qa_Llama-2-13b}
\end{table*}
