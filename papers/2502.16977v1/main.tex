\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage[dvipsnames]{xcolor}
\usepackage{ulem}
\usepackage{bbold}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue,    
    urlcolor=red,
    pdfpagemode=FullScreen,
    }
\usepackage{graphicx}
\usepackage{array}
\usepackage{stmaryrd}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\ee}{\mathrm{e}}
\newcommand{\ind}[1]{\mathds{1}_{\{#1\}}}
\newcommand{\iind}[1]{\mathds{1}_{#1}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\tr}{\mathrm{tr}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{conjecture}{Conjecture}

\title{Convergence of Shallow ReLU Networks\\ on Weakly Interacting Data}
\date{}

\author[1]{Léo Dana}
\author[2]{Loucas Pillaud-Vivien}
\author[1]{Francis Bach}

\affil[1]{Sierra, Inria.} 
\affil[2]{CERMICS, ENPC.}

\usepackage[authoryear]{natbib}
\setcitestyle{round}
\bibliographystyle{plainnat}
\usepackage[a4paper, margin=1in]{geometry}


\begin{document}

\maketitle

\begin{abstract}
    We analyse the convergence of one-hidden-layer ReLU networks trained by gradient flow on $n$ data points. Our main contribution leverages the high dimensionality of the ambient space, which implies low correlation of the input samples, to demonstrate that a network with width of order $\log(n)$ neurons suffices for global convergence with high probability. Our analysis uses a Polyak–Łojasiewicz viewpoint along the gradient-flow trajectory, which provides an exponential rate of convergence of $\frac{1}{n}$. When the data are exactly orthogonal, we give further refined characterizations of the convergence speed, proving its asymptotic behavior lies between the orders $\frac{1}{n}$ and $\frac{1}{\sqrt{n}}$, and exhibiting a phase-transition phenomenon in the convergence rate, during which it evolves from the lower bound to the upper, and in a relative time of order $\frac{1}{\log(n)}$.
\end{abstract}

\section{Introduction}

Understanding the properties of models used in machine learning is crucial for providing guarantees to downstream users. Of particular importance, the convergence of the training process under gradient methods stands as one of the first issues to address in order to comprehend them. If, on the one hand, such a question for linear models and convex optimization problems~\citep{bottou2018optimization,bach2024learning} are well understood, this is not the case for neural networks, which are the most used models in large-scale machine learning. This paper focuses on providing quantitative convergence guarantees for a one-hidden-layer neural network.

Theoretically, such global convergence analysis of neural networks has seen two main achievements in the past years: (i) the identification of the \textit{lazy regime}, due to a particular initialization, where convergence is always guaranteed at the cost of being essentially a linear model \citep{jacot2018neural,arora2019fine,chizat2019lazy}, and (ii) the proof that with an infinite amount of hidden units a two-layer neural network converges towards the global minimizer of the loss \citep{mei2018mean,chizat2018global,rotskoff2018parameters}. However, neural networks are trained in practice outside of these regimes, as neural networks are known to perform \textit{feature learning}, and experimentally reach global minimum with a large but finite number of neurons. Quantifying in which regimes neural networks converge to a global minimum of their loss is still an important open question.

This article aims to identify such a regime, characterized by low-correlated inputs, under which we can rigorously prove the convergence of shallow neural networks trained via gradient flow. The previous assumptions needed for convergence relied on special initialization scales \citep{chizat2019lazy, boursier2022gradient}, on an {infinite number of neurons} \citep{jacot2018neural, chizat2018global}, or on the {exact orthogonality} of the input data \citep{boursier2022gradient, frei2023implicit}. In this article, we aim to simply fix the dimension~$d$, and ask how many randomly sampled examples can be interpolated by the network with high probability. This \textit{high dimensionality} regime, corresponding to~$d$ being larger than~$n^2$, is our main assumption to prove convergence of shallow ReLU neural networks. In addition, we are interested in understanding the general dynamics of the system, and in particular the system's convergence speed to a global minimizer of the loss.

We summarize our contributions in the analysis of the learning dynamics of a one-hidden-layer ReLU network on a finite number of data $n$ via gradient flow.

\begin{itemize}
    \item Our main contribution is that shallow neural networks in high dimension $d\geq Cn^2$ interpolates exactly random whitened data with high probability as soon as the neural network has more that $\log(n)$ neurons, for any initialization scale. We also show that the loss converges to zero exponentially fast with a rate at least of order $\frac{1}{n}$.
    \item Then, when the inputs are orthogonal, we refine our analysis in order to characterize the range of possible asymptotic speeds, which we find to be at most of order $\frac{1}{\sqrt{n}}$. Moreover, we conjecture that this speed is always of the highest order $\frac{1}{\sqrt{n}}$ with high probability and verify empirically this claim.
    \item Finally, for orthonormal inputs and a special initialization of the network, we highlight a phase transition in the convergence rate during the system's evolution, and compute the associated cut-off time and transition period.
\end{itemize}

\section{Problem Setup}
\label{section:setup}

\paragraph{Notations.} We use $||v||$ to denote the euclidean norm of a vector $v$, $\langle\cdot|\cdot\rangle$ its scalar product, and $||M||$ for the operator norm associated with $||\cdot||$ of a matrix $M$. Moreover, let $\bar{v} = \frac{v}{||v||}$.

\paragraph{Loss function.} Let $(x_i,y_i)_{i=1:n}\in(\mathbb{R}^{d} \times \R)^n$ be a sample of input vectors and real outputs. Let $d\in \N^*$ be the dimension of the vector space and $n\in \N^*$ the number of data points. In order to learn the regression problem of mapping $x_i$ to $y_i$, we use one-hidden-layer ReLU neural networks\footnote{This model can accommodate to having a bias by adding an extra dimension to the data, identical for all $x_i$ and small enough to ensure our statements stay true.}, which we write:

\begin{equation}
    h_{\theta}(x) = \frac{1}{p}\sum_{j=1}^pa_j\sigma(\langle w_j|x\rangle)\,,
\end{equation}

where $p \in \N^*$ is the number of units, $\sigma (x) = \max\{0,x\}$ for $x \in \R$ is the rectified linear unit (ReLU), and the parameters are gathered in $\theta = (a_j,w_j)_{1\leq j\leq p} \in (\R \times \R^d)^p $. To simplify the ReLU notation, we define $\sigma(\langle w_j|x_i\rangle) = \langle w_j|x_i\rangle_+$ and $\mathbb{1}_{\langle w_j|x_i\rangle>0} = 1_{j,i}$. When mentioning neurons of the network, we refer to $\langle w_j|x_i\rangle_+$, while second layer neurons refer to $a_j$. Neurons can be activated if $\langle w_j|x_i\rangle_+ >0$, and are correctly activated if moreover $a_jy_i>0$. Upon this prediction class and data, we analyse the regression loss with square error,

\begin{equation}
   L(\theta) := \frac{1}{2n} \sum_{ i = 1}^n (y_i - h_{\theta}(x_i))^2~.
\end{equation}

As soon as $d \geq n$, ${(x_i)_{i=1:n}}$ can form a free family, in which case the set of minima of $L$, which consists of all interpolators, is non-empty.

\paragraph{Gradient flow.} In order to understand a simplified version of the optimization dynamics of this neural network, we study the continuous-time limit of gradient descent. We initialize $\theta_{t = 0} = \theta_0$ and follow for all $t \geq 0$ the ordinary differential equation

\begin{equation}
\label{eq:gradient_flow}
    \frac{d}{dt}\theta_t = - p\nabla_{\theta_t} L(\theta_t)\,,
\end{equation}

where we choose a particular element of the sub-differential of the ReLU $\sigma'(x) = \mathbb{1}_{x>0}$, for any $x \in \R$. This choice is motivated by both prior empirical work from \cite{bertoin2021numerical} and theoretical work from \citet[Proposition 2]{boursier2022gradient} and \cite{jentzen2023convergence}. We also decided to accelerate the dynamics by a factor $p$ as only this scaling gives a consistent mean field limit for the gradient flow when the number of neurons tends to infinity (see Definition 2.2 by \cite{chizat2018global}).

\paragraph{Weight invariance.}The 1-homogeneity of the ReLU provides a continuous symmetry in the function $\theta \mapsto h_\theta$ and hence the loss\footnote{Indeed, the subspace built from all parameters $\theta_{\gamma} = (\frac{a_j}{\gamma_j},\gamma_j w_j)_{1\leq j\leq p}$, when $\gamma$ varies in $(\R^*_+)^p$, maps to the same network, i.e., $h_{\theta_{\gamma}} = h_{\theta_{1}}$.}. This feature is known to lead automatically to invariants in the gradient flow as explained generally by \cite{marcotte2024abide}. The following lemma is not new \citep[p.11]{wojtowytsch2020convergence}, and shows that, from this invariance, we deduce that the two layers have balanced contributions throughout the dynamics.

\begin{lemma}
    \label{lem:homogeneity}
    For all $j \in \llbracket1,p \rrbracket$, for all $t \geq 0$,  $|a_j(t)|^2 - ||w_j(t)||^2 = |a_j(0)|^2 - ||w_j(0)||^2$ , and thus, if $|a_j(0)| \geq ||w_j(0)||$, then $a_j(t)$ maintains its sign and $|a_j(t)| \geq ||w_j(t)||$.
\end{lemma}

\paragraph{Initialization.} Throughout the paper, we initialize the network's weights $w_j$ and $a_j$ from a joint distribution where both marginals are non-zero, centered, rotational-invariant, have finite covariance, and we take norms of $a_j$ and $w_j$ independent of $d, n, p$. Moreover, we need an assumption of asymmetry of the norm at initialization.

\begin{assumption}[Asymmetric norm initialization] 
    \label{assump:asymmetry}
    We assume that the weights of the network at initialization satisfy $\forall j\in \llbracket 1, p\rrbracket ,\ |a_j(0)| \geq ||w_j(0)||$.
\end{assumption}

Articles by \citet{boursier2024early,boursier2024simplicitybiasoptimizationthreshold} already used this assumption to study two-layer neural networks in order to use the property described in Lemma~\ref{lem:homogeneity}\footnote{Note that Pytorch's default initialization does not follow the assumptions of our initialization, in particular since $|a_j(0)| \geq ||w_j(0)||$ for all $j$ is very unlikely. One way to initialize the network, as we do in Section \ref{section:experiments}, is to first sample $w_j$ and then sample $a_j$ conditionally on the norm asymmetry.}.

\paragraph{Data.} We define the data matrix $X = (x_1, \hdots, x_n) \in \R^{d \times n}$. Denote $C_x^- = \min_{i}||x_i||$ and $C_y^- = \min_{i}|y_i|$; in what follows, we suppose that $C_x^- > 0$ and $C_y^- > 0$, i.e., the input and output data are bounded away from the origin. Similarly, we also let $C_x^+ = \max_i||x_i||$ and $C_y^+ = \max_i|y_i|$. We note $C_{x,y}^{+,-}$ to refer to the set of these constants. Finally, we introduce the following hypothesis on the low correlation between the inputs.

\begin{assumption}[Low correlated inputs]
    \label{assump:data}
    We assume that the data satisfy
    
    \begin{equation}
        ||X^T X-D_X|| < \frac{(C_x^-)^2}{\sqrt{n}}\frac{C_y^-}{C_y^+}~,
    \end{equation}
    
    where $D_X$ denotes the diagonal matrix with coefficients $||x_i||^2$.
\end{assumption}

The term $||X^TX-D_X||$ is a control on the magnitude of the correlations $(\langle x_i, x_j \rangle)_{i \neq j}$. As an extreme case, when it equals zero, the inputs are orthogonal. This assumption is purely deterministic at this stage. Later, we show that this weak interaction between the inputs is highly likely to occur for random whitened vectors in high dimensions (see Corollary~\ref{cor:conv_high_dim}).

\paragraph{Dimensions.} Throughout the paper, even if the results provided are all non-asymptotic in nature, the reader can picture that the numbers $n,p,d$ (respectively data, neurons and dimension) are all large. Moreover, they verify the following constraint: $n$ is less than~$d$, and~$p$ can be thought of the order~$\log(n)$, meaning only a ‘‘low'' number of neurons is required.

\subsection{Related works}
\label{section:related_works}
\paragraph{Convergence of neural networks.} Neural networks are known to converge under specific data, parameter, or initialization hypotheses, among which: the neural tangent kernel regime studied by \citet{jacot2018neural,arora2019fine,du2018gradient,allen2019convergence}, that has been shown to correspond in fact to a \textit{lazy regime} where there is no feature learning because of the initialization scale. Another field of study is the \textit{mean-field} regime, where feature learning can happen but where the optimization has been shown to converge only in the infinite width case \citep{mei2018mean,chizat2018global,rotskoff2018parameters}. Note that it is also possible to produce generic counter examples, where convergence does not occur~\citep{boursier2024simplicitybiasoptimizationthreshold}. Beyond these, there have been attempts to generalize convergence results under local PL (or local curvature) conditions as shown by \citet{chatterjee2022convergence,liu2022loss,zhou2021local}, but they remain unsatisfactory to explain the good general behavior of neural networks due to the constraint it imposes on the initialization. Convergence theorems similar in spirit to Theorem~\ref{thm:CV_hig_dim} can be found in an article by \citet{chen2022feature}. The main difference relies on two features: only the inner weights are trained and their result necessitates a large value of outer weights when $n$ is large, which is the regime of interest of the present article. Finally, it is worth mentioning other works on neural networks dynamics, e.g., the study of the implicit bias either for regression~\citep{boursier2022gradient} or classification~\citep{Lyu2020Gradient,ji2020directional}, or sample complexity to learn functions in a specific context~\citep{glasgow2023sgd}.

\paragraph{Polyak-Łojasiewicz properties.} Dating back from the early sixties, Polyak derived a sufficient criterion for a smooth gradient descent to converge to a global minimizer~\citep{Polyak1964}. This corresponds to the later-called Polyak-Łojasiewicz (PL) constant $\mu$ of a function $f:\mathbb{R}^d\rightarrow\mathbb{R}_+$, that  can be defined as the best exponential rate of convergence of gradient flow over all initializations, or equivalently to the following minimum ratio $\mu = \min_{x\in\mathbb{R}^d}\frac{||\nabla f(x)||^2}{f(x)}$. This has found many applications in non-convex optimization, as it is the case for neural network optimization, and is very popular for optimization in the space of measures~\citep{gentil2020entropie}. Other notions of PL conditions have emerged in the literature to characterize local convergence, by bounding the PL constant over a ball $\mu^*(z,r) = \min_{x\in\mathcal{B}(z,r)}\frac{||\nabla f(x)||^2}{f(x)}$ \citep{chatterjee2022convergence, liu2022loss} and comparing it to $f(z)$. We use a notion of PL which is local and trajectory-wise to prove lower bounds valid on each trajectory.

\section{Convergence in high dimension}
\label{section:CV}
 In this first section, our goal is to understand when the gradient flow converges toward a global minimizer of the loss. Note that the parametrization of the prediction function $h_\theta$ by a neural network often implies the non-convexity of the objective $L$ and prevents any direct application of convex tools in order to ensure global convergence. Generally speaking, even if gradient flows are expected to converge to critical points of the parameter space~\citep{lee2016gradient}, such that $\nabla_\theta L(\theta) = 0$, they might become stuck in local minimizers that do not interpolate the data.

\subsection{Local PL-curvature}

Convexity is not the only tool that provides global convergence: as known in the optimization community, showing that $\frac{||\nabla L(\theta)||^2}{L(\theta)}$ is uniformly lower bounded suffices. As mentioned in Section \ref{section:related_works}, this is known as the Polyak-Lojasiewicz condition~\citep{Polyak1964}. Taking a dynamical perspective on this, we define a trajectory-wise notion of this ``curvature'' condition which we name the \textbf{local-PL curvature} of the system, and define for all $t \geq 0$,

\begin{equation}
    \mu(t) := p\frac{\|\nabla L(\theta_t)\|^2}{L(\theta_t)} = -\frac{\frac{d}{dt}L(\theta_t)}{L(\theta_t)}
\end{equation}

with the second equality being a property of the gradient flow. Intuitively, this coefficient describes the curvature in parameter space that $\theta_t$ ``sees'' at time $t \geq 0$. The following lemma is classical and shows how it can be used to prove the global convergence of the system, as well as a quantification on the rate. 
\begin{lemma}
    \label{lem:average_PL}
    Let $\langle \mu(t)\rangle := \frac{1}{t}\int_{0}^{t}\mu(u)du$ the time average of the local-PL curvature, which we name the \textbf{average-PL curvature}. We have L$(\theta_t) = L(\theta(0))e^{-\langle \mu(t)\rangle t}$.
\end{lemma}

Hence, if the \textbf{total average-PL curvature} $\langle\mu_\infty\rangle := \lim_{t \to \infty} \langle \mu(t)\rangle$ is strictly positive, we can deduce an upper bound on the loss and convergence to $0$ at the exponential speed $\langle\mu_\infty\rangle$. This shows that the average-PL curvature is actually the instantaneous exponential decay rate of the loss, and thus controls the speed at which the system converges.

\subsection{Global convergence of neural networks for weakly correlated inputs}

We are ready to state the main theorem of the paper on the minimization of the loss.

\begin{theorem}
\label{thm:CV_hig_dim}
    Let $\varepsilon >0$, $p\geq 4\log(\frac{n}{\varepsilon})$, and suppose Assumption \hyperref[assump:asymmetry]{1}. We fix the data $(x_i,y_i)_{1\leq i\leq n}$ and suppose it satisfies Assumption \hyperref[assump:data]{2}. Then with probability at least $1-\varepsilon$ over the initialization of the network, the loss converges to 0 with $\langle \mu_{\infty}\rangle \geq \frac{C}{n}$, 
    where we define $C = \frac{(C_x^-)^2}{C_x^+}C_y^-$. Moreover, for any $t \geq 0$, we have the lower bound
    
    \begin{equation}
    \label{eq:lower_bound_PL_deter}
        \begin{split}
            \mu(t) &\geq \frac{C}{n}\min_i\left|1-\frac{r_i(t)}{y_i}\right|.
        \end{split}
    \end{equation}
    
\end{theorem}

Note that, at best, the number of neurons required in Theorem \ref{thm:CV_hig_dim} is logarithmic. This finiteness stands in contrast with the infinite number required in the \textit{mean-field regime}, and the polynomial dependency typical of the neural tangent kernel (NTK) regime \citep{jacot2018neural, allen2019convergence}. In the orthogonal case, the ReLU makes the $\log(n)$ dependency necessary and sufficient, as shown in Lemma~\ref{lem:all_act_big}, as the residual $r_i$ goes to zero if and only if a neuron gets initialized as $a_jy_i>0$ and $\langle w_j|x_i\rangle >0$ for each $i$.

Assumption~\hyperref[assump:data]{2} is crucial for this proof: it means that the examples are insufficiently correlated with each other for the weights to collapse onto a single direction. As proved by \citet[Theorem 1]{boursier2024early}, the direction $\bar{w}^* = \arg\min_{\theta = \{\bar{w},a\}} L(\theta)$ will attract all neurons if it is accessible from anywhere on the initialization landscape\footnote{This accessibility condition is in fact the absence of saddle point for some function of normed neurons, which imply that neurons can rotate from anywhere on the sphere to $\bar{w}^*$.}. This phenomenon known as \textit{early alignment} and first described by \cite{maennel2018gradient}, will prevent interpolation if examples are highly correlated \citep[Theorem 2]{boursier2024early}. The fact that our result holds for any initialization scale shows that near-orthogonal inputs prevent accessibility to $\bar{w}^*$ and make the early alignment phenomenon benign, as found by \cite{boursier2022gradient, frei2023implicit}.

Note finally that our norm-asymmetric initialization (Assumption \hyperref[assump:asymmetry]{1}) is sufficient for global convergence with high probability, but may not be necessary. That said, we present in Appendix \ref{app:collapse_a_j} a detailed example of interpolation failure when the assumption is not satisfied.

\paragraph{Convergence in high dimension.} In this paragraph we assume that the data $(x_i,y_i)_{i=1:n}$ are generated i.i.d. from some distribution $\mathcal{P}_{X,Y}$. We first show that, with high probability, Assumption~\hyperref[assump:data]{2} is almost always valid if the dimension is larger than the square root of the number of data points. Additionally, we assume that the law anti-concentrates at the origin. These two features are gathered in the following lemma.

\begin{lemma}
    \label{lem:hyp_data}
    Let $(x_i,y_i)_{1\leq i\leq n}$ be generated i.i.d. from a probability distribution $\mathcal{P}_{X,Y}$ such that the marginal $\mathcal{P}_X$ is sub-Gaussian, has zero-mean, and satisfy $\mathbb{E}_{x\sim\mathcal{P}_X}[xx^T] = \frac{\lambda^2}{d}I_d$ for some $\lambda$ independent of $d,n$, while, on $\mathbb{R}^*$, the marginal law $\mathcal{P}_Y$ has compact support.\footnote{Note that this means that $\mathcal{P}_Y$ can have an atom at 0.} There exists $C>0$ depending only on the constants $C_{x,y}^{+,-}$ and the initialization weights, such that, if $d\geq C\left(n^2+n\log\left(\frac{1}{\varepsilon}\right)\right)$, then, with probability $1-\varepsilon$, Assumption \hyperref[assump:data]{2} is satisfied.
\end{lemma}

The hypothesis in the previous lemma is satisfied by standard distributions like Gaussians $\mathcal{N}(0,\frac{1}{d}I_d)$ for the inputs. The following corollary restates Theorem~\ref{thm:CV_hig_dim} for data that are generically distributed as in Lemma \ref{lem:hyp_data}, and when the dimension is large enough.

\begin{corollary}
    \label{cor:conv_high_dim}
    Let $\varepsilon>0$. Suppose Assumption \hyperref[assump:asymmetry]{1} and that $(x_i,y_i)_{1\leq i\leq n}$ are i.i.d. generated from a probability distribution satisfying the same properties as in Lemma \ref{lem:hyp_data}. There exists a constant $C>0$ depending only on the constants $C_{x,y}^{+,-}$ such that, if the network has $p\geq 8\log\left(\frac{n}{\varepsilon}\right)$ neurons in dimension $d\geq C\left(n^2+n\log\left(\frac{1}{\varepsilon}\right)\right)$, then, with probability at least $1-\varepsilon$ over the initialization of the network and the data generation, the loss converges to 0 at exponential speed of rate at least $\frac{1}{n}$.
\end{corollary}

Beyond the high-dimensionality of the inputs, Corollary \ref{cor:conv_high_dim} does not require any initialization specificity (small or large), and the number of neurons required to converge can be as low as $\log(n)$. Hence, let us put emphasis on the fact that the global nice structure of the loss landscape comes from the high-dimensionality: this does not come from a specific region in which the network is initialized as in the NTK (or lazy) regime~\citep{chatterjee2022convergence}, nor rely on the infinite number of neurons~\citep{wojtowytsch2020convergence}.

Remark that, under the near-orthogonality assumption, in the large $d$ limit, the largest amount of data that ``fits'' in the vector space is only $d$, and corresponds to a perturbation of the canonical basis\footnote{In Appendix \ref{app:antipodality}, we show that our method can in fact accommodate $2d$ examples: an orthogonal basis and their antipodal vectors.}. On average, Corollary~\ref{cor:conv_high_dim} finally states that the average number of data points for which we can show convergence is of the order $\sqrt{d}$. Trying to push back this limit up to order $d$ is an important question for future research and seems to ask for other techniques. Experiments underlying this question are presented in Section~\ref{section:experiments} (Figure~\ref{fig:proba_CV}).

\subsection{Sketch of Proof}

The proof of convergence relies on three key points: \textbf{(i)} the loss strictly decreases as long as each example is activated by at least a neuron, \textbf{(ii)} for a data point, if there exists a neuron which is activated at initialization, then at least one neuron remains activated throughout the dynamics, \textbf{(iii)} At initialization, condition (ii) is satisfied with large probability. Let us detail shortly how each item articulates with one another.
\medbreak
\paragraph{(i).} First, Lemma \ref{lem:bounds_on_mu}, stated and proved in Appendix, shows that, by computing the derivatives of the loss, we get a lower bound on the curvature

\begin{equation}
    \mu(t) \geq \frac{2}{n}((C_x^-)^2 - ||X^TX-D_X||)\min_i\left\{\frac{1}{p}\sum_{j=1}^p|a_j|^21_{j,i}\right\}.
\end{equation}

To prove the strict positivity, one needs to show that $||X^TX-D_X||$ is small enough, and that for each data $i$, there exists $j$ such that $|a_j|^21_{j,i}$ is strictly positive. Thanks to the initialization of the weights, $|a_j|^2 \geq |a_j(0)|^2-||w_j(0)||^2>0$, and to Assumption \hyperref[assump:data]{2}, $\frac{1}{\sqrt{2}}(C_x^-)^2 > ||X^TX-D_X||$. Thus, we have convergence if at any time, for any data input, one neuron remains active, i.e., formally, for all $t\geq0$, and all $i \in \llbracket 1, n \rrbracket$, there exists $j \in \llbracket 1 , p \rrbracket$ such that $\langle w_j(t)| x_i\rangle_+>0$. Hence, the loss decreases as long as one neuron remains active per data input. We see next how to show this crucial property.

\paragraph{(ii).} Let us fix the data index $i \in \llbracket 1 , n\rrbracket$, and $y_i>0$ without loss of generality. Let us define $j^*_i~=~\arg\max_{a_jy_i>0}\langle w_j(t)|x_i\rangle$ the index of the largest correctly initialized neuron. Since $a_j$ cannot change sign thanks to Assumption \hyperref[assump:asymmetry]{1}, $\langle w_{j_i^*}(t)|x_i\rangle$ is continuous, and has a derivative over each constant segment of $j_i^*$. The strict positivity of this neuron is an invariant of the dynamics: if $r_i \geq y_i$, the derivative of the neuron shows it increases, and if $r_i < y_i$, the residual has decreased, which implies that the $\langle w_{j_i^*}(t)|x_i\rangle$ is strictly positive. Thus, if a neuron is correctly initialized for the data point $i$, a neuron stays active throughout the dynamics.

\paragraph{(iii).} Finally, Lemma \ref{lem:all_act_big} shows $\mathbb{P}(\forall i, \exists j, \langle w_j(0)|x_i\rangle >0\cap a_jy_i> 0) \geq 1-n\left(\frac{3}{4}\right)^p$, which implies that for $p\geq 4\log(\frac{n}{\varepsilon})$, the network is well initialized with probability at least $1-\varepsilon$.

\section{Orthogonal Data}
\label{section:orthogonal}

In this section, we go deeper on the study of the gradient flow, assuming that the input data are perfectly orthogonal, or equivalently that $||X^TX-D_X|| = 0$. Since most of the intuition for the convergence is drawn from the orthogonal case, it offers stronger results which we detail. In particular, we are able to closely understand the local-PL curvature $(\mu(t))_{t \geq 0}$ evolution and asymptotic behaviour.

\subsection{Asymptotic PL curvature}
Theorem~\ref{thm:CV_hig_dim} has shown that the local-PL curvature is lower bounded by a term of order $\frac{1}{n}$, allowing us to show an exponential convergence rate of this order. The following proposition shows that in the orthogonal case the curvature can also be upper bounded.

\begin{proposition}
    \label{prop:upper_bound_mu}
    Let $\varepsilon>0$. Given orthogonal inputs, Assumption \hyperref[assump:asymmetry]{1}, $d$ large enough, and $p\geq 4\log(\frac{n}{\varepsilon})$, with probability $1-\varepsilon$, we have an upper-bound on the local-PL curvature: there exists $C>0$ depending only on the initialization and the constants $C_{x,y}^{+,-}$ such that for all $t \geq Cn$,
    \begin{equation}
        \mu(t) \leq C\sqrt{\frac{p}{n}}\max_i\left|1 - \frac{r_i(t)}{y_i}\right| + \frac{C}{n}.\\ 
    \end{equation}
\end{proposition}

This upper bound uses two properties that are characteristic of the orthogonal case. First, once a neuron is inactive on some data input, then, it can never re-activate again. The second property is that for an initialization scale independent on $n$, there is a phase during which correctly initialized neurons increase while the others decrease to 0. This \textit{extinction phase}, proved in Lemma \ref{lem:extinction_in_finite_time}, is short in comparison to the time needed to fit the residuals, and leaves the system decoupled between positive and negative outputs $y_i$.

In the limit where $n$ goes to infinity, Proposition \ref{prop:upper_bound_mu} shows that the network does not learn since the local-PL is 0. This is an artifact of the orthogonality of the inputs: the interaction between inputs should accelerate the dynamics. However, although all quantities have well defined limits as $n\rightarrow+\infty$, the limits cannot be understood as a gradient descent in an infinite dimensional space\footnote{One would like to write the loss as an expectation over the data point, yet it is impossible as there is no uniform distribution on $\mathbb{N}$.}.

Proposition \ref{prop:upper_bound_mu} is in fact valid for $p$ fixed, and an initialization of the weights for which every data is correctly initialized by a neuron. In that case, Proposition \ref{prop:upper_bound_mu} shows that the asymptotic curvature cannot be larger than the order $\frac{1}{\sqrt{n}}$. While the local-PL curvature is between the order $\frac{1}{n}$ and $\frac{1}{\sqrt{n}}$, the next proposition shows that any intermediate order $\frac{1}{n^{\alpha}}$, for $\alpha\in[\frac{1}{2},1]$, can be reached asymptotically, with strictly positive probability, using a particular initialization of the network. 

\paragraph{Group initialization.} In the following, we use $p_n$ to denote the number of neurons, and partition the $n$ data points in $p_n$ groups of cardinality $k_n$ (note that $p_n k_n = n$). We re-index the examples per group as by $(x_i^j,y_i^j) = (x_{i+(j-1)k_n},y_{i+(j-1)k_n})$, for all $i \in \llbracket 1, k_n\rrbracket$ and $j \in \llbracket 1, p_n\rrbracket$. Moreover, we use a special initialization of the network such that for all $j,q \in \llbracket 1 , p_n\rrbracket $, $i \in \llbracket 1, k_n\rrbracket $,
\begin{equation}
    \bigg\{  
    \begin{matrix}
        \langle w_j | x_i^q\rangle >0\text{ if }j=q\\
        \langle w_j | x_i^q\rangle \leq 0\text{ if }j\neq q\\
    \end{matrix}
    \ \ \text{ and } \ \  a_j = s_j||w_j||~,
\end{equation}
i.e., $w_j$ is correctly activated on the group $j$ only.

\begin{proposition}
    \label{prop:group_examples}
    Suppose the group initialization described above, with orthonormal inputs, i.e., $X^TX = I$, and that the signs of all outputs of the group $j$ are equal to $s_j$. We fix $k_n = n^{2(1-\alpha)}$ with $\alpha\in[\frac{1}{2},1]$. Then, for $t\geq Cn^{3\alpha-1}\log\left(Cn\right)$, the local-PL curvature satisfies
    \begin{equation}
        \frac{K_1}{n^{\alpha}} \leq \mu(t) \leq \frac{K_2}{n^{\alpha}},
    \end{equation}
    where $C = \max(\alpha C_y^-,(\frac{1}{2C_y^-})^{\frac{1}{\alpha}})$, $K_1 = 2C_y^-\min_j\frac{||w_j(0)||^2}{2+||w_j(0)||^2}$ and $K_2 = 4C_y^+$.
\end{proposition}

Proposition \ref{prop:group_examples} states that any asymptotic value $\langle \mu_{\infty}\rangle\in[\frac{K_1}{n}, \frac{K_2}{\sqrt{n}}]$ can be achieved with strictly positive probability using group initialization. But of what order is the most likely limit of the curvature for standard initialization? Experiment \ref{exp2} in Section~\ref{exp2} suggest that, with high probability, the asymptotic curvature is always of the order $\frac{1}{\sqrt{n}}$.

\begin{conjecture}
    \label{conjecture}
    Let $\varepsilon >0$. There exist $C_1,C_2>0$ depending only on the data and the initialization, such that for $p\geq C_1\log(\frac{n}{\varepsilon})$ and for orthogonal examples, with probability at least $1-\varepsilon$ over the initialization of the network, we have convergence of the loss to 0 and
    \begin{equation}
        \langle\mu_{\infty}\rangle = \frac{C_2}{\sqrt{n}}.
    \end{equation}
\end{conjecture}

\subsection{Phase transition in the PL curvature}

In the previous section, we emphasized the asymptotic order of the local-PL curvature with respect to $n$ and hypothesized that it is of the order $\frac{1}{\sqrt{n}}$ in most cases. In this section, we are interested in the evolution of the local-PL curvature during the dynamics. Lemma \ref{lem:speed_at_init} below computes the local-PL curvature at initialization in the large $p$ regime, and shows that initially it is of order $\frac{1}{n}$.

\begin{lemma}
    \label{lem:speed_at_init}
    At initialization, the local-PL curvature $\mu(0)$ is a random variable which satisfy $\sqrt{p}(\frac{2}{n}\mu(0)-\beta_0)\underset{p\rightarrow+\infty}{\longrightarrow}\mathcal{N}\left(0,\gamma_0^2\right)$, and with $\beta_0,\gamma_0$ depending only on the data and the distributions of the network's neurons.
\end{lemma}

The constant $\beta_0$ is strictly positive as soon as the limit network does not directly equal the labels, which is natural to assume since they are unknown a priori. Thus the exponential rate of decrease of the loss in the early times of the dynamics is of order $\frac{1}{n}$. Importantly, Proposition \ref{prop:group_examples} with a single group has an asymptotic speed of order $\frac{1}{\sqrt{n}}$, meaning that the local-PL curvature transitions between $\frac{1}{n}$ and $\frac{1}{\sqrt{n}}$. If Conjecture \ref{conjecture} is true, then this phenomenon happens with high probability during the dynamics.

Let us study this phenomenon through the example of Proposition \ref{prop:group_examples}, with a fixed number of neurons $p$. In this case, the following theorem shows that there are exactly $p$ phase transitions of the loss, which each corresponds to a data group being fitted. To be precise, let us define $L_{\infty}(t) =\lim_{n\rightarrow+\infty}L_n(t)$, with $L_n(t) = L(\theta(t\times{t_n}))$, $t_n = \frac{\sqrt{np}}{4}\log(np)$, and $p$ fixed ($k_n=\frac{n}{p}$). We prove that $L_{\infty}$ is constant by parts with at most $p$ parts.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.65\linewidth]{Phase_Transition.png}
    \caption{Simulation of the loss trajectory of a network with 2 neurons and group initialization, each activated separately on half the data points. $L_{n}$ is the rescaled loss for $n$ examples, and $L_{\infty}$ is its limit as $n$ goes to infinity. We can see two phase transitions in the very high-dimensional regime.}
    \label{fig:phase_transition}
\end{figure}

\begin{theorem}
    \label{thm:phase_transition}
    Suppose the same data hypothesis and initialization as Proposition \ref{prop:group_examples}. We define $||D_j^n||^2 = \frac{1}{k_n}\sum_{i=1}^{k_n}(y_i^j)^2$ for each cluster, and suppose its limit $||D_j^{\infty}||^2$ finite. Then, the function $L_{\infty}$ is constant by parts with at most $p$ parts, and the transitions happen at each time $t^j = \frac{1}{||D_j^{\infty}||}$. Moreover, for all $\varepsilon\in]0,1[$, there exist times $t_n^j({\varepsilon})$ satisfying
    
    \begin{equation}
        L^j(t_n^j({\varepsilon})) =\frac{\varepsilon}{2} ||D_j^n||^2 ,\,\,\frac{t_n^j({\varepsilon})}{t_n^j(1-{\varepsilon})} \sim_n 1 \text{  and  }
        \frac{t_n^j(1-{\varepsilon}) - t_n^j({\varepsilon})}{t_n} \sim_n \frac{1}{2||D_j^{\infty}||}\frac{\log\left(C^j(\varepsilon)\right)}{\log(n)},
    \end{equation}
    
    where $L^j$ is the part of the loss corresponding to the group $j$, and $C^j(\varepsilon)>1$ depends on $\varepsilon$ and the initializations and data of the group $j$.
\end{theorem}

The theorem shows that each transition of $L_n$ occurs in the time frame which decreases as $\frac{1}{\log(n)}$. Note that these transitions are subtle: one needs extremely large dimensions in order to differentiate two close transitions as shown on Figure \ref{fig:phase_transition} in Appendix. The phase transitions of the loss are in fact associated with transitions $||w_j||^2$ from a constant order to an order $\sqrt{n}$, and by Lemma \ref{lem:bounds_on_mu} with transitions on the local-PL from order $\frac{1}{n}$ to order $\frac{1}{\sqrt{n}}$.

\section{Experiments}
\label{section:experiments}

In this Section, we aim to perform deeper experimental investigations on the system, which we could not do formally. Precisely, we want to answer two questions:

\begin{enumerate}
    \item What is the probability that the loss reaches 0 for $n$ data points in dimension $d$, under the distributional hypotheses of Lemma \ref{lem:hyp_data} (sub-Gaussian, zero-mean and whitened data)? What is the maximum $n$ for a fixed $d$ such that global convergence holds with high probability ?
    \item In the orthogonal case, is the asymptotic exponential convergence rate of order $\frac{1}{\sqrt{n}}$ (on average over the initialization) as stated in Conjecture~\ref{conjecture}?
\end{enumerate}

The data and weights distribution which have been used for the experiments below can be found in Appendix \ref{app:experiments}, and the code is available on \href{https://github.com/leodana2000/Convergence-High-Dimension}{GitHub}.

\subsection{Probability of Convergence}
\label{exp1}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{CV_probability.png}
    \caption{\textbf{Left:} Probability that a network trained on $n$ data converges to 0 loss. We observe a transition at $n=3000$, from likely to unlikely convergence.\\
    \textbf{Right:} Loss at convergence normalized by the loss at initialization. For $n\geq 3000$, the loss increases to $0.6\%$, which is equivalent to fitting all but one example.}
    \label{fig:proba_CV}
\end{figure}

This section aims to test the limit in which Corollary \ref{cor:conv_high_dim} holds when the number of data points increase. Intuitively, as the number of examples $n$ grows, the neural network becomes less and less overparametrized, and hence is expected to fail to globally converge. Knowing if and when this occurs with high probability is important for us to understand how much our current threshold $C\sqrt{d}$ can be improved. We thus plot the probability of convergence, as well as the loss at convergence to obtain additional information when the probability is zero. We train $500$ one-layer neural networks with the normalization presented in Section~\ref{section:setup}, dimension $d=100$, $n$ ranging from $2500$ to $3500$, and $p_n = C\log(n)$ neurons. Additional details on the training procedure can be found in Appendix \ref{app:experiments}.

Figure \ref{fig:proba_CV} shows that for $n\leq 2900$, the probability of convergence is very likely, for $n\geq 3100$ the probability is almost zero, and in between, there is a sharp transition. This sharp transition is visible for any value $d$ at some point $N(d,p)$, which we name the \textbf{convergence threshold}. By measuring the point for different values of $d$ and $p$, we see that the threshold scales like $N(d,p) \simeq C(p)d$, with $C(p)$ which is sub-linear (see Figure \ref{fig:proba_p} in Appendix \ref{app:experiments}). In particular, for $n\leq Cdp$, there exists a network that interpolates the data, meaning that the convergence threshold is not a threshold for the existence of a global minimum. The threshold's scaling is linear in $d$ which implies that proving convergence for $Cd$ data in dimension $d$ seems feasible.

\subsection{Empirical asymptotic local-PL curvature}
\label{exp2}

In this section we test Conjecture \ref{conjecture}, and to do so we measure $\mu(t)$ during the dynamics, and mostly at the end of the dynamics, since we know by Lemma \ref{lem:speed_at_init} that near 0 the local-PL curvature is of order $\frac{1}{n}$. To provide the strongest evidence for the conjecture, we measured the order of the local-PL curvature in three ways: by directly measuring the local-PL $\mu(t_{\infty}) = \log\left(\frac{L(t_{\infty}-1)}{L(t_{\infty})}\right)$ at the last epoch $t_{\infty}$, by measuring the average-PL curvature $\langle \mu_{\infty}\rangle = \frac{1}{t_{\infty}}\log\left(\frac{L(0)}{L(t_{\infty})}\right)$, and finally by mesuring the lower and upper bounds on the local-PL given in Lemma \ref{lem:bounds_on_mu}.

Following Conjecture \ref{conjecture}, all approximations should likely be decreasing in $\frac{1}{\sqrt{n}}$ as $n$ increases. To show this, we plot the log-log graph of each measure above. We train $500$ networks in dimension $d=2000$, with $n$ ranging from $1000$ to $2000$, and $p_n=C\log(n)$. All resulting plots appear linear in the log-log scale, with a slope close to $-\frac{1}{2}$ (see Figure \ref{fig:CV_speed} in Appendix \ref{app:experiments}), meaning that the scalings are indeed in $\frac{C}{\sqrt{n}}$. This empirically confirms our conjecture that the local-PL curvature has order $\frac{1}{\sqrt{n}}$ asymptotically.

\section{Conclusion}

We have studied the convergence of the gradient flow on a one-hidden-layer ReLU networks on finite datasets. Our analysis leverages a local Polyak-Łojasiewicz viewpoint on the gradient-flow dynamics, revealing that for a large dimension $d$ in the order of $n^2$ data points, we can \textbf{guarantee global convergence with high probability} using only $\log(n)$ neurons. The specificity of the system relies on the low-correlation between the input data due to the high dimension. Moreover, in the orthogonal setting the \textbf{loss's exponential rate of convergence} is at least of order $\frac{1}{n}$ and at most of order $\frac{1}{\sqrt{n}}$, which is also the average asymptotic order as experimentally verified. For a special initialization of the network, a \textbf{phase transition in this rate} occurs during the dynamics.

\paragraph{Future Directions.} We are most enthusiastic about proving the convergence of the networks for linear threshold $d\geq Cn$, which should require new proof techniques, as well as quantifying the impact of large amounts of neurons on the system, which has been overlooked in our study. Future work should also consider a using a teacher-network to generate the outputs, in order to link the probability or interpolation with the complexity, in terms of neurons, of the teacher.

\section{Acknowledgements}
This work has received support from the French government, managed by the National Research Agency, under the France 2030 program with the reference "PR[AI]RIE-PSAI" (ANR-23-IACL-0008).

\clearpage

\bibliography{refs}

\clearpage

\begin{center}
{ \scshape \Large  Organization of the Appendix}
\end{center}

The appendices of this article are structured as follows. Appendix \ref{app:proofs} contains the proofs of each of the 10 statements of the paper in an entitled subsection, with additional lemmas included in the relevant subsections. Only Corollary \ref{cor:conv_high_dim} doesn't have a complete proof as it is a simple combination of Lemma \ref{lem:hyp_data} and Theorem \ref{thm:CV_hig_dim}. Appendix \ref{app:experiments} contains additional details on the experiments that were performed in Section~\ref{section:experiments}, as well as graphs for the scaling laws. Finally, Appendix C contains general discussions about the possibility to provably learn $2d$ inputs in dimension $d$, and the possible collapse of the second-layer weights.

\appendix

\section{Proofs}
\label{app:proofs}

\subsection{Theorem \ref{thm:CV_hig_dim}}

Lemma \ref{lem:all_act_big} shows that a number of neurons of order $\log(n)$ is both necessary and sufficient to obtain the event $\mathcal{I}$, which corresponds to a nice initialization of the network.
\begin{lemma}
    \label{lem:all_act_big}
    Suppose $y_i\neq 0$, and let $\mathcal{I}$ the event: for all $i$, there exists $j$ such that, $\langle w_j(0)|x_i\rangle >0$ and $a_j(0)y_i>0$. For all $\varepsilon >0$, 
    \begin{itemize}
        \item if $p\geq 4\log(\frac{n}{\varepsilon})$, then $\mathbb{P}(\mathcal{I})\geq 1-\varepsilon$,
        \item if $p\leq 3\log(\frac{n}{\varepsilon})-2$, then $\mathbb{P}(\mathcal{I})\leq 1-\varepsilon$,
    \end{itemize}
    and thus, $\mathbb{P}(\mathcal{I})= 1-\varepsilon$ implies $p\in[3\log(\frac{n}{\varepsilon})-2, 4\log(\frac{n}{\varepsilon})]$.
\end{lemma}

\begin{proof}\textbf{of Lemma} \ref{lem:all_act_big}\\
    Let us note $\langle w_j(0)|x_i\rangle = W_{i,j}$ and $A_j = a_j(0)y_i$ random variables which are symmetric. $A_j$ are independent with all variables, while $W_{j,i}$ are independent with all variables $A_j$ and $W_{q,k}$ with $q\neq j$.
    \begin{equation}
        \begin{split}
            \mathbb{P}(\mathcal{I}) &= \mathbb{P}(\forall i, \exists j, \langle w_j(0)|x_i\rangle >0\cap a_jy_i> 0)\\ 
            &= \mathbb{P}\left(\bigcap_i \bigcup_j W_{i,j} >0 \cap A_j> 0\right)\\
            &= 1-\mathbb{P}\left(\bigcup_i \bigcap_j W_{i,j} \leq0 \cup A_j\leq 0\right)\\
            &\geq 1-n\mathbb{P}\left(\bigcap_j W_{i,j} \leq0 \cup A_j\leq 0\right)\\
            &= 1-n\mathbb{P}\left(W_{i,j} \leq0 \cup A_j\leq 0\right)^p\\
            &= 1-n(1-\mathbb{P}\left(W_{i,j} >0 \cap A_j> 0\right))^p\\
            &= 1-n\left(1-\mathbb{P}\left(W_{i,j}> 0\right)\mathbb{P}\left(A_j> 0\right)\right)^p\\
            &= 1-n\left(\frac{3}{4}\right)^p\\
        \end{split}
    \end{equation}
    Replacing the expression with $p = 4\log(\frac{n}{\varepsilon})\geq \frac{1}{\frac{1}{3}-\frac{1}{2}\frac{1}{3^2}}\log(\frac{n}{\varepsilon}) \geq\frac{\log(\frac{n}{\varepsilon})}{\log(\frac{4}{3})}$, we find that the probability is larger than $1-\varepsilon$. Now for the other bound,
    \begin{equation}
        \begin{split}
            \mathbb{P}(\mathcal{I})  &= \mathbb{P}(\forall i, \exists j, \langle w_j(0)|x_i\rangle >0\cap a_jy_i> 0)\\
            &= \mathbb{P}\left(\bigcap_i \bigcup_j W_{i,j} >0 \cap A_j> 0\right)\\
            &\leq \mathbb{P}\left(\bigcap_i \bigcup_j W_{i,j} >0\right)\\
            &= \mathbb{P}\left(\bigcup_j W_{1,j} >0\right)^n\\
            &= (1-\mathbb{P}\left(W_{1,1} >0\right)^p)^n\\
            &= (1-2^{-p})^n\\
            &\leq \left(1-4\left(\frac{\varepsilon}{n}\right)^{3\log(2)}\right)^n\\
            &\leq \left(1-4\frac{\varepsilon}{n}\right)^n\\
            &\leq 1-\varepsilon
        \end{split}
    \end{equation}
    where we use $(1-\frac{x}{n})^n\leq 1-\frac{x}{e}\leq 1-\frac{x}{4}$ valid on $x\in[0,1]$.
\end{proof}

\begin{lemma}
    \label{lem:bounds_on_mu}
    For any set of parameters $\theta = (a_j,w_j)_{j=1:p}$, the following bounds on the local-PL curvature hold.
    \begin{equation}
        \begin{split}
            \mu(t) &\leq \frac{2}{n}(C_x^+ + ||X^TX-D_X||)\max_i\frac{1}{p}\sum_{j=1}^p(|a_j|^2+||w_j||^2)1_{j,i}\\
            \mu(t) &\geq \frac{2}{n}(C_x^- - ||X^TX-D_X||)\min_i\frac{1}{p}\sum_{j=1}^p|a_j|^21_{j,i}
        \end{split}
    \end{equation}
\end{lemma}

\begin{proof}\textbf{of Lemma \ref{lem:bounds_on_mu}}\\
    Let us start by writing the derivatives of the two variables of the system. We define $P_j\in\mathbb{R}^{n\times n}$ the diagonal matrix with elements $1_{j,i}$.
    \begin{equation}
        \begin{split}
            &\frac{d}{dt}w_j = \frac{a_j}{n}\sum_{i=1}^nr_ix_i1_{j,i} = \frac{a_j}{n}XP_jR\\
            &\frac{d}{dt}a_j = \frac{1}{n}\sum_{i=1}^nr_i\langle w_j|x_i\rangle_+ = \frac{1}{n}w_j^TXP_jR
        \end{split}
    \end{equation}
    We now compute the derivatives for the residuals $r_i$. 
    \begin{equation}
        \begin{split}
        \frac{d}{dt}r_i &= -\frac{1}{p}\sum_{j=1}^p\left(\frac{d}{dt}a_j\right)\langle w_j|x_i\rangle_++a_j\left(\frac{d}{dt}\langle w_j|x_i\rangle_+\right)\\
        &=-\frac{1}{np}\sum_{j=1}^p1_{j,i}x_i^Tw_jw_j^TXP_jR+|a_j|^21_{j,i}x_i^TXP_jR\\
        \frac{d}{dt}R &= -\frac{1}{np}\left[\sum_{j=1}^pP_jX^Tw_jw_j^TXP_j + |a_j|^2P_jX^TXP_j\right]R\\
        \end{split}
    \end{equation}
    The matrix written between brackets is symmetric, and the local-PL curvature lies between its smallest and largest eigenvalues.
    \begin{equation}
        \frac{d}{dt}||R||^2 = -\frac{2}{np}\left[\sum_{j=1}^p(w_j^TXP_jR)^2 + |a_j|^2||XP_jR||^2\right]
    \end{equation}
    Since $0\preceq w_jw_j^T \preceq ||w_j||^2I_d$, we get the following bound on the derivative
    \begin{equation}
        -\frac{2}{np}\sum_{j=1}^p(||w_j||^2+|a_j|^2)||XP_jR||^2 \leq \frac{d}{dt}||R||^2 \leq -\frac{2}{np}\sum_{j=1}^p|a_j|^2||XP_jR||^2
    \end{equation}
    We can manipulate the term $||XP_jR||^2$ to make $||X^TX-D_X||$ appear.
    \begin{equation}
        \begin{split}
            ||XP_jR||^2 &= R^TP_j^TX^TXP_jR\\
            &= R^TP_j^T(D_X - (X^TX-D_X))P_jR\\
            &= ||\sqrt{D_X}P_jR||^2 - ||\sqrt{X^TX-D_X}P_jR||^2\\
            &\geq (C_x^-)^2||P_jR||^2 - ||X^TX-D_X||||P_jR||^2\\
            &\leq (C_x^+)^2||P_jR||^2 + ||X^TX-D_X||||P_jR||^2
        \end{split}
    \end{equation}
    Finally, by observing that $\mu(t) = \frac{\frac{d}{dt}||R||^2}{||R|^2}$, one has the lower bound
    \begin{equation}
        \begin{split}
            \mu(t) &\geq \frac{2}{np}\sum_{j=1}^p|a_j|^2\frac{||P_jR||^2}{||R||^2}\left((C_x^-)^2 - ||X^TX-D_X||\right)\\
            &\geq \frac{2}{n}\frac{1}{||R||^2}\left((C_x^-)^2 - ||X^TX-D_X||\right)\sum_{i=1}^n\frac{r_i^2}{p}\sum_{j=1}^p|a_j|^21_{j,i}\\
            &\geq \frac{2}{n}\left((C_x^-)^2 - ||X^TX-D_X||\right)\min_i\frac{1}{p}\sum_{j=1}^p|a_j|^21_{j,i},\\
        \end{split}
    \end{equation}
    and and the upper bound
    \begin{equation}
        \begin{split}
            \mu(t) &\leq \frac{2}{np}\sum_{j=1}^p|a_j|^2\frac{||P_jR||^2}{||R||^2}\left((C_x^+)^2 + ||X^TX-D_X||\right)\\
            &\leq \frac{2}{n}\left((C_x^+)^2 + ||X^TX-D_X||\right)\max_i\frac{1}{p}\sum_{j=1}^p(|a_j|^2+||w_j||^2)1_{j,i}.\\
        \end{split}
    \end{equation}
\end{proof}


\begin{proof}\textbf{of Theorem \ref{thm:CV_hig_dim}}\\
This is a precise proof based on the sketch visible in Section~\ref{section:CV}. The proof of convergence relies on three key points: 
\begin{enumerate}[label=(\roman*)]
    \item The loss strictly decreases as long as each example is activated by at least a neuron.
    \item For a data point, if there exists a neuron which is activated at initialization, then at least one neuron remains activated throughout the dynamics.
    \item At initialization, the previous condition is satisfied with large probability.
\end{enumerate}
We finish the proof with the lower bounds on $\mu(t)$ and $\langle \mu_{\infty}\rangle$.
\medbreak
\textbf{(i)} First, Lemma \ref{lem:bounds_on_mu} shows that, by computing the derivatives of the loss, we get a lower bound on the curvature.
\begin{equation}
    \label{eq:lower_bound_mu}
    \mu(t) \geq \frac{2}{n}\left((C_x^-)^2 - ||X^TX-D_X||\right)\min_i\left\{\frac{1}{p}\sum_{j=1}^p|a_j|^21_{j,i}\right\}
\end{equation}
We want to show the strict positivity of this lower bound.  First, using Assumption \ref{assump:data}, we have for all $n\geq 2$ that 
\begin{equation}
    (C_x^-)^2 - ||X^TX-D_X|| \geq (C_x^-)^2\left(1-\frac{1}{\sqrt{n}}\frac{C_y^-}{C_y^+}\right) \geq \left(1-\frac{1}{\sqrt2}\right)(C_x^-)^2
\end{equation}
which also holds for $n=1$ since then $||X^TX-D_X||=0$. Moreover, thanks to the asymmetric initialization, we have $|a_j|^2 \geq |a_j(0)|^2-||w_j(0)||^2>0$, which means that $\mu(t)$ is bounded away from 0 as long as for all $i$ there exists $j$ satisfying $\langle w_j(t)| x_i\rangle_+>0$.

 \textbf{(ii)} Let us fix the data index $i \in \llbracket 1 , n\rrbracket$, and $y_i>0$ without loss of generality. Let us define $j^*_i~=~\arg\max_{a_jy_i>0}\langle w_j(t)|x_i\rangle$ the index of the largest correctly initialized neuron. Since $a_j$ cannot change sign thanks to Assumption \hyperref[assump:asymmetry]{1}, $\langle w_{j_i^*}(t)|x_i\rangle$ is continuous, and has a derivative over each constant segment of $j_i^*$. We can write the derivatives of this neuron as
    \begin{equation}
        \begin{split}
            \frac{d}{dt}\langle w_{j_i^*}|x_i\rangle &= \frac{a_{j_i^*}}{n}\sum_k^nr_k\langle x_i|x_k\rangle 1_{{j_i^*},k}\\
            &= \frac{a_{j_i^*}}{n}e_i^TX^TXP_jR\\
            &\geq \frac{|a_{j_i^*}|}{n}\left[r_i||x_i||^21_{{j_i^*},i}s_{j_i^*} - ||X^TX-D_X||||R||\right]\\
            &= \frac{|a_{j_i^*}|}{n}\left[r_i||x_i||^21_{{j_i^*},i}s_{j_i^*} - ||X^TX-D_X||\sqrt{2nL(\theta_t)}\right]\\
            &\geq \frac{|a_{j_i^*}|}{n}\left[r_i||x_i||^21_{{j_i^*},i}s_{j_i^*} - ||X^TX-D_X||\sqrt{2nL(\theta(0))}\right]\\
        \end{split}
    \end{equation}
    With the condition $||X^TX - D_X|| < \frac{1}{\sqrt{n}}(C_x^-)^2\frac{C_y^-}{C_y^+}$ and the fact that $s_{j_i^*}y_i>0$, we get the inequality
    \begin{equation}
        \label{neuron 1}
        \begin{split}
            \frac{d}{dt}\langle w_{s_{j_i^*}}|x_i\rangle &> \frac{|a_{s_{j_i^*}}|C_y^-(C_x^-)^2}{n}\left[\frac{r_i}{y_i}1_{s_{j_i^*},i} - 1\right]\\
        \end{split}
    \end{equation}
Now, the strict positivity of $\langle w_{j_i^*}|x_i\rangle$ is an invariant of the system: if $\frac{r_i}{y_i}\geq1$, then $\langle w_{j_i^*}|x_i\rangle$ strictly increases, and otherwise we have
\begin{equation}
    0 < \frac{y_i-r_i}{y_i} = \frac{1}{p}\sum_{j=1}^p\frac{a_j}{y_i}\langle w_j|x_i\rangle_+ \leq \langle w_{j_i^*(t)} | x_i \rangle\frac{1}{p}\sum_{j=1}^p\frac{|a_j|}{|y_i|}
\end{equation}
Which implies that $\langle w_{j_i^*}|x_i\rangle$ stays strictly positive throughout the dynamics.

\textbf{(iii)} As shown in Lemma \ref{lem:all_act_big}, for $p\geq 4\log(\frac{n}{\varepsilon})$, we have the strict positivity with probability $1-\varepsilon$.
\begin{equation}
    \mathbb{P}(\forall i, \exists j, \langle w_j(0)|x_i\rangle >0\cap a_jy_i> 0) \geq 1-\varepsilon.
\end{equation}
\medbreak
Finally, we prove the lower bounds on the PL. Let us recall that $|a_j|\geq ||w_j||$ and that $1_{j,i} \geq \langle \bar{w}_j(t)| \bar{x}_i\rangle_+$, which gives us
\begin{equation}
    \frac{1}{p}\sum_{j=1}^p|a_j|^21_{j,i} \geq \left|\frac{1}{p}\sum_{j=1}^pa_j\langle w_j|\bar{x}_i\rangle_+\right| = \left|\frac{y_i-r_i(t)}{C_x^+}\right|.
\end{equation} 
We can plug these into equation \ref{eq:lower_bound_mu} to obtain
\begin{equation}
    \mu(t) \geq \frac{2-\sqrt2}{n}\frac{(C_x^-)^2}{C_x^+}C_y^-\min_i\left|1-\frac{r_i(t)}{y_i}\right|
\end{equation}
From this last equation, by integration, we obtain
\begin{equation}
    \frac{1}{t}\int_0^t\mu(u)du \geq \frac{2-\sqrt2}{n}\frac{(C_x^-)^2}{C_x^+}C_y^- \left(1 - \frac{1}{t}\int_0^t\max_i\left|\frac{r_i(u)}{y_i}\right|du\right)
\end{equation}
Let $t_{\delta}$ satisfying $\max_i|r_i(t)|\leq \delta$ for all $t\geq t_{\delta}$. $t_{\delta}$ exists and is finite since the loss reaches 0. Thus, we have for any $\delta>0$ that 
\begin{equation}
    \frac{1}{t}\int_0^t\mu(u)du \geq \frac{2-\sqrt2}{n}\frac{(C_x^-)^2}{C_x^+}C_y^- \left(1 - \frac{t_{\delta}}{t}\max_{i}\frac{\sqrt{2nL(\theta_0)}}{\left|y_i\right|\sqrt{p}} - \frac{t-t_{\delta}}{t}\delta\right)
\end{equation}
whic in the limit $t\rightarrow +\infty$ gives
\begin{equation}
    \langle \mu_{\infty}\rangle \geq \frac{2-\sqrt2}{n}\frac{(C_x^-)^2}{C_x^+}C_y^-(1-\delta)
\end{equation}
Taking $\delta\rightarrow 0$ gives the desired bound.
\end{proof}

\subsection{Lemma \ref{lem:hyp_data}}
\begin{proof}\textbf{of Lemma \ref{lem:hyp_data}}\\
    This proof will heavily rely on the result of \citet[Remark 5.59]{vershynin2010introduction} on the concentration of sub-Gaussian random variables. It states that if $A\in\mathbb{R}^{n\times d}$ is a matrix, the columns of which are $n$ independent centered, whitened\footnote{In this article,  \citet[Remark 5.59]{vershynin2010introduction} uses the isotrop of the columns, but defines it as $\mathbb{E}[xx^T]=I_d$, which we rather refer to as a whitened distribution.}, sub-Gaussian random variables in dimension $d$, then with probability $1-2e^{-t^2}$, 
    \begin{equation}
        \left|\left|\frac{1}{d}A^TA - I_d\right|\right| \leq C\sqrt{\frac{n}{d}}+\frac{t}{\sqrt{d}}
    \end{equation}
    with $C>0$ depending only on $\max_i||A_i||_{\psi_2}$ the sub-Gaussian norm of the columns. We use this property with $A_i=\frac{\sqrt{d}}{\lambda}x_i$ which satisfies every hypothesis, in particular it is whitened since by assumption $\mathbb{E}[x_ix_i^T]=\frac{\lambda^2}{d}I_d$, and $t= \sqrt{\log\left(\frac{2}{\varepsilon}\right)}$, which gives
    \begin{equation}
         \left|\left|\frac{1}{\lambda^2}X^TX-I_d\right|\right| = \left|\left|\frac{1}{d}A^TA - I_d\right|\right|\leq C\sqrt{\frac{n}{d}}+\sqrt{\frac{\log\left(\frac{2}{\varepsilon}\right)}{d}}
    \end{equation}
    Moreover, note that 
    \begin{equation}
        (C_x^-)^2 \geq 1 + \min_i(||x_i||^2-1) \geq 1 - ||X^TX-I_d||
    \end{equation}
    Thus, the condition in Assumption \ref{assump:data} is satisfied if 
    \begin{equation}
         \left|\left|X^TX - \lambda^2I_d\right|\right|\leq \lambda\frac{C_y^-}{C_y^-+\sqrt{n}C_y^+}
    \end{equation}
    Now, since $C_y^-,C_y^+$ are independent of $n$ since $\mathcal{P}_Y$ has compact support away from 0, taking
    \begin{equation}
        d> 16n\max\left(C^2n, \log\left(\frac{2}{\varepsilon}\right)\right)\left(\frac{C_y^-}{C_y^+}\right)^2
    \end{equation}
    is sufficient for Assumption \ref{assump:data} with probability $1-\varepsilon$.
\end{proof}

The proof of Corollary \ref{cor:conv_high_dim} is then the combination of the proof of Theorem \ref{thm:CV_hig_dim}, using $\left|\left|X^TX - \lambda^2I_d\right|\right|$ instead of $||XX^T-D_X||$, with $\frac{\varepsilon}{2}$.

\subsection{Proposition \ref{prop:upper_bound_mu}}

\begin{lemma}
    \label{lem:extinction_in_finite_time}
    Suppose orthogonal inputs and convergence of the system to 0 loss, then with probability $1-\varepsilon$ for all $t\geq t_n$ with $t_n = 2n\max_{j,i}\frac{\langle w_j(0)|x_i\rangle_+}{|y_i|||x_i||^2(|a(0)|^2-||w_j(0)||^2)}$,
    \begin{equation}
        \begin{split}
            a_j(0)y_i >0 &\implies \langle w_j(t) | x_i \rangle_+ \geq \langle w_j(0) | x_i \rangle_+\\
            a_j(0)y_i <0 &\implies \langle w_j(t) | x_i \rangle_+ \leq 0\\
        \end{split}
    \end{equation}
\end{lemma}
This Lemma states that, for orthogonal data, incorrectly initialized neuron vanish, and cannot become active again. Thus, after time $t_n$, the system is decoupled between the positive and negative labels, and only correctly initialized neuron, which are useful to the prediction, persist.

\begin{proof}\textbf{of Lemma \ref{lem:extinction_in_finite_time}}\\
    We start by computing the derivative of a neuron in the orthogonal setting.
    \begin{equation}
        \begin{split}
            \frac{d}{dt}\langle w_j|x_i\rangle_+ &= \frac{a_jr_i1_{j,i}||x_i||^2}{n}\\
            \frac{d}{dt}a_j &= \frac{1}{n}\sum_{i=1}^nr_i\langle w_j|x_i\rangle_+
        \end{split}
    \end{equation}
    Let us only discuss the case of neurons that are positive at initialization since otherwise nothing happens. We observe that, as long as all $r_i\neq 0$, then the neurons vary monotonously. This implies that if $a_jy_i<0$, then the corresponding neuron will reach 0 in finite time. Let $t^*_n$ the first time any $|r_i-y_i|=\frac{y_i}{2}$, which is finite with high probability. For $j,i$ such that $a_jy_i<0$, we have
    \begin{equation}
        \begin{split}
            \frac{d}{dt}\langle w_j|x_i\rangle_+ &\leq -\frac{\sqrt{|a_j(0)|^2 - ||w_j(0)||^2}|y_i|}{2n}||x_i||^21_{j,i}\\
            \langle w_j|x_i\rangle_+&\leq  \langle w_j(0)|x_i\rangle_+- 1_{j,i}\frac{|y_i|}{2n}||x_i||^2C_jt
        \end{split}
    \end{equation}
    where $C_j^2 = |a_j(0)|^2 - ||w_j(0)||^2$. Let $\tilde{t}_n = \max_{j,i}\frac{2n\langle w_j(0)|x_i\rangle_+}{|y_i||x_i||^2C_j}$, if $\tilde{t}_n\leq t^*_n$, then we have extinction in finite time, i.e., the incorrectly initialized neurons have reached 0. Let us show that at $\tilde{t}_n$, residuals have almost not moved. First, we bound the second-layer neurons $a_j$,
    \begin{equation}
        \begin{split}
            \frac{d}{dt}a_j &\leq \frac{1}{n}\sqrt{\sum_{i=1}^nr_i^2}\sqrt{\sum_{i=1}^n\langle w_j|x_i\rangle_+^2}\\
            &\leq \sqrt{2}L(\theta_t)\max_{i}\langle w_j|x_i\rangle_+\\
            &\leq \sqrt{2}L(\theta_0)\max_{i}\langle w_j|x_i\rangle_+\\
            a_j-a_j(0) &\leq \sqrt{2}L(\theta_0)\int_0^t\max_{i}\langle w_j(u)|x_i\rangle_+du
        \end{split}
    \end{equation}
    Let $C_1 = \frac{3}{2n}\max_i|y_i|||x_i||^2$, $C_2 = \max_{i}\langle w_j(0)|x_i\rangle_+^2+ a_j(0)^2\frac{C_1}{2}$, and $C_3 = C_1\left(\frac{L(\theta_0)}{\sqrt{2}}+\frac{1}{2}\right)$. The upper bound on $a_j$ turns into an upper bound on $w_j$,
    \begin{equation}
        \begin{split}
            \frac{d}{dt}\max_{i}\langle w_j(u)|x_i\rangle_+ &\leq C_1\left(a_j(0)+\sqrt{2}L(\theta_0)\int_0^t\max_{i}\langle w_j(u)|x_i\rangle_+du\right)\\
            \max_{i}\langle w_j(t)|x_i\rangle_+^2-\max_{i}\langle w_j(0)|x_i\rangle_+^2 &\leq C_1a_j(0)\int_0^t\max_{i}\langle w_j(u)|x_i\rangle_+du+\\
            &+\frac{C_1}{\sqrt{2}}L(\theta_0)\int_0^t\max_{i}\langle w_j(u)|x_i\rangle_+^2du\\
            \max_{i}\langle w_j(t)|x_i\rangle_+^2 &\leq C_2 + C_3\int_0^t\max_{i}\langle w_j(u)|x_i\rangle_+^2du\\
        \end{split}
    \end{equation}
    Let us pose $A_j(t) = \int_0^t\max_{i}\langle w_j(u)|x_i\rangle_+^2du$, We can solve the differential inequality
    \begin{equation}
        \begin{split}
            A_j'(t) &\leq C_2 + C_3A_j(t)\\
            A_j(t)+\frac{C_2}{C_3} &\leq (A_j(0)+\frac{C_2}{C_3})e^{C_3t}
        \end{split}
    \end{equation}
    In the end, we obtain
    \begin{equation}
        \max_{i}\langle w_j|x_i\rangle_+ \leq C_2e^{C_3t}
    \end{equation}
    Importantly, since $C_3\tilde{t}_n$ is a constant of $n$, we have that at there exists $K_1,K_2$ which doesn't depend on $n$ such that at $\tilde{t}_n$ for all $j,i$,
    \begin{equation}
        \begin{split}
            \langle w_j(\tilde{t}_n)|x_i\rangle_+ &\leq K_1\langle w_j(0)|x_i\rangle_+\\
            |a_j| &\leq K_2|a_j(0)|\\
        \end{split}
    \end{equation}
    Thus, we can bound the deviation of the residuals at the beginning of the dynamics.
    \begin{equation}
        \begin{split}
            |r_i(\tilde{t}_n)-y_i| &= \left|\frac{1}{p}\sum_{j=1}^pa_j\langle w_j|x_i\rangle_+\right|\\
            &\leq\frac{1}{p}\sum_{j=1}^p\left|a_j\right|\langle w_j|x_i\rangle_+\\
            &\leq K_1K_2\frac{1}{p}\sum_{j=1}^p\left|a_j(0)\right|\langle w_j(0)|x_i\rangle_+
        \end{split}
    \end{equation}
    Finally, since $a_j,w_j$ are initialized with norms independent of $d$, it means that by rotational invariance $\langle w_j(0)|x_i\rangle$ has variance $\frac{1}{\sqrt{d}}$ and mean 0. Thus, in large dimension, we get that $|r_i(\tilde{t}_n)-y_i| \underset{d\rightarrow+\infty}{\rightarrow} 0$. This shows that for $d$ sufficiently large, we have $
    \tilde{t}_n \leq t^*_n$.
\end{proof}

\begin{proof}\textbf{of Proposition \ref{prop:upper_bound_mu}}\\
    Thanks to Lemma \ref{lem:extinction_in_finite_time}, there exists $t_n$ such that for $t\geq t_n$, each example has only correctly activated neurons. Without loss of generality suppose that all labels are positive. Then the network only has positive contributions $a_j(t)\langle w_j|x_i\rangle \geq 0$ for all i. Let $N(j,i)$ the number of indices $q$ such that $a_j(t)\langle w_j|x_i\rangle \leq a_q(t)\langle w_q|x_i\rangle$. We have
    \begin{equation}
        \frac{N(j,i)}{p}a_j\langle w_j|x_i\rangle \leq \frac{1}{p}\sum_{q=1}^pa_q\langle w_q|x_i\rangle = y_i - r_i
    \end{equation}
    Thus, we can bound the norm of $w_j$,
    \begin{equation}
        \begin{split}
        ||w_j||^4&\leq a_j^2||w_j||^2\\
        &= \sum_{i=1}^na_j^2\langle w_j|\bar{x}_i\rangle^2\\
        &\leq \sum_{i=1}^n\frac{p^2}{N(j,i)^2}\left(\frac{y_i - r_i}{||x_i||}\right)^2\\
        &\leq \max_i\left(\frac{y_i - r_i}{C_x^-}\right)^2\sum_{i=1}^n\frac{p^2}{N(j,i)^2}
        \end{split}
    \end{equation}
    This helps us majorate the sum of $|a_j|^2+||w_j||^2$
    \begin{equation}
        \begin{split}
            \frac{1}{p}\sum_{j=1}^p\left(|a_j|^2+||w_j||^2\right)1_{j,i} &\leq \frac{1}{p}\sum_{j=1}^p\left(|a_j(0)|^2 -||w_j(0)||^2\right)1_{j,i} + \frac{2}{p}\sum_{j=1}^p||w_j||^2\\
            &\leq \bar{C} + \frac{2}{p}\sum_{j=1}^p\frac{\max_i(y_i - r_i)}{C_x^-}\sqrt{\sum_{i=1}^n\frac{p^2}{N(j,i)^2}}\\
            &\leq \bar{C} + 2\frac{\max_i(y_i - r_i)}{C_x^-}\sum_{j=1}^p\sqrt{\sum_{i=1}^n\frac{1}{N(j,i)^2}}\\
            &\leq \bar{C} + \pi\sqrt{\frac{2}{3}}\frac{\max_i(y_i - r_i)}{C_x^+}\sqrt{np}\\
            &\leq \bar{C} + 6\frac{\max_i(y_i - r_i)}{C_x^-}\sqrt{np}\\
        \end{split}
    \end{equation}
    which we can use in the bound from Lemma \ref{lem:bounds_on_mu} to end the proof. Thus, the constant $C$ from the Proposition statement is
    \begin{equation}
        \begin{split}
            C = \max\bigg(& 2\max_{j,i}\frac{\langle w_j(0)|x_i\rangle_+}{|y_i|||x_i||^2(|a(0)|^2-||w_j(0)||^2)},\\ 
            &2(C_x^+)^2\frac{1}{p}\sum_{j=1}^p\left(|a_j(0)|^2 -||w_j(0)||^2\right)1_{j,i}, \\
            &12\frac{(C_x^+)^2}{C_x^-}C_y^+\bigg)
        \end{split}
    \end{equation}
\end{proof}

\subsection{Proposition \ref{prop:group_examples}}

\begin{proof}\textbf{of Proposition} \ref{prop:group_examples}\\
    Recall that the number of neurons is the number of groups $p_nk_n=n$, that we have for $q\neq j$ that $\langle w_j | x_i^q\rangle \leq 0$ at all time, meaning that $h_{\theta}(x_i^j) = \frac{a_j}{p_n}\langle w_j | x_i^j\rangle_+ = \frac{s_j}{p_n}||w_j||_+\langle w_j | x_i^j\rangle_+$, and $s_j$ does not change by Lemma \ref{lem:homogeneity}. This implies that the dynamics is decoupled: $w_j$ and $w_q$ can be studied separately.
    
     Let us compute the dynamics for the neuron $j$. We let $D_j^n = \frac{1}{k_n}\sum_{i=1}^{k_n}y_i^jx_i^j$, $||w_j||_+^2 = \sum_{i=1}^n\langle w_j| x_i\rangle_+^2$, and $\bar{x}^+ = \frac{x}{||x||_+}$. We first consider the alignment between $w_j$ and $D_j^n$:
    \begin{equation}
        \begin{split}
            \frac{d}{dt}\langle \bar{D}_j^n|s_j\bar{w}^+_j \rangle &= \left\langle \bar{D}_j^n \big| I_d - \bar{w}^+_j(\bar{w}^+_j)^T\big|\frac{1}{||w_j||_+}\frac{d}{dt}w_j \right\rangle \\
            & = \frac{1}{n}\langle\bar{D}_j^n | I_d - \bar{w}^+_j(\bar{w}^+_j)^T| R \rangle\\
            & = \frac{1}{n}\langle\bar{D}_j^n | I_d - \bar{w}^+_j(\bar{w}^+_j)^T| k_nD_j^n - \frac{s_j}{p_n}||w_j||_+w_j \rangle\\
            & = \frac{\sqrt{k_n}||D_j^n||}{n}(1 - \langle \bar{D}_j^n|s_j\bar{w}^+_j \rangle^2)\\
        \end{split}
    \end{equation}
    This equation can be solved closed-form, and we obtain
    \begin{equation}
        \begin{split}
            \langle \bar{D}_j^n|s_j\bar{w}^+_j \rangle &= \frac{\sinh\left(c_n^jt\right) + \langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\cosh\left(c_n^jt\right)}{\cosh\left(c_n^jt\right) + \langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\sinh\left(c_n^jt\right)}\\
            &= \frac{1}{c_n^j}\frac{d}{dt}\left[\log\left(\cosh\left(c_n^jt\right) + \langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\sinh\left(c_n^jt\right)\right)\right]
        \end{split}
    \end{equation}
    with $c_n^j = \frac{2\sqrt{k_n}||D_j^n||}{n}$. Now we can compute the norm of the neuron.
    \begin{equation}
        \begin{split}
            \frac{d}{dt}||w_j||_+^2 &= 2||w_j||_+\frac{s_j}{n}\sum_{i=1}^kr_i\langle w_j|x_i\rangle_+\\
            &= \frac{2}{n}||w_j||_+^2(\langle D_j^n|s_j\bar{w}^+_j \rangle - \frac{1}{p_n}||w_j||_+^2)\\
            ||w_j||_+^2e^{\frac{2}{np_n}\int_0^t||w_j(u)||_+^2du} &= ||w_j(0)||_+^2e^{\frac{2}{n}\int_0^t\langle D_j^n|s_j\bar{w}^+_j(u) \rangle du}\\
            e^{\frac{2}{np_n}\int_0^t||w_j(u)||_+^2du} -1 &= \frac{2}{np_n}||w_j(0)||_+^2\int_0^t e^{\frac{2}{n}\int_0^u\langle D_j^n|s_j\bar{w}^+_j(v) \rangle dv}du\\
            ||w_j(t)||_+^2 &= \frac{||w_j(0)||_+^2e^{\frac{2}{n}\int_0^t\langle D_j^n|s_j\bar{w}^+_j(u) \rangle du}}{1+\frac{2}{np_n}||w_j(0)||_+^2\int_0^t e^{\frac{2}{n}\int_0^u\langle D_j^n|s_j\bar{w}^+_j(v) \rangle dv}du}
        \end{split}
    \end{equation}
    Finally, we can replace the expression of the correlation.
    \begin{equation}
        \label{norm}
        ||w_j(t)||_+^2 = \frac{p_n\sqrt{k_n}||D_j^n||||w_j(0)||_+^2
        \left(\cosh\left(c_n^jt\right) + \langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\sinh\left(c_n^jt\right)\right)}{p_n\sqrt{k_n}||D_j^n||+||w_j(0)||_+^2 
        \left(\sinh\left(c_n^jt\right) + \langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle(\cosh\left(c_n^jt\right)-1)\right)}
    \end{equation}
    We use this equation in Lemma \ref{lem:bounds_on_mu}, and easily obtain the upper bound thanks to the monotonicity of $||w_j(t)||^2$.
    \begin{equation}
    \mu(t) \leq \frac{4}{np_n}\max_i\sum_j^p||w_j(t)||_+^21_{j,i} = \frac{4\max_j||w_j(t)||_+^2}{np_n} \leq \frac{4C_y^+\sqrt{k_n}}{n} = \frac{4C_y^+}{n^{\alpha}}
    \end{equation}
    For the lower bound, we have the bound for $t\geq \frac{\alpha}{2C_y^-}n^{3\alpha-1}\log\left(n(C_y^+)^{\frac{1}{\alpha}}\right)\geq \frac{1}{c_n^j}\log(p_n\sqrt{k_n}||D_j^n||)$ by monotonicity. Indeed,
    \begin{equation}
        \cosh\left(c_n^jt\right) + \langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\sinh\left(c_n^jt\right) \geq \frac{1}{2}e^{c_n^jt}\left(1+\langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\right)
    \end{equation}
    and 
    \begin{equation}
        \sinh\left(c_n^jt\right) + \langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle(\cosh\left(c_n^jt\right)-1) \leq \frac{1}{2}e^{c_n^jt}\left(1+\langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\right)
    \end{equation}
    which implies that 
    \begin{equation}
        \begin{split}
        ||w_j(t)||_+^2 &\geq \frac{p_n\sqrt{k_n}||D_j^n||||w_j(0)||_+^2e^{c_n^jt}\left(1+\langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\right)
        }{2p_n\sqrt{k_n}||D_j^n||+||w_j(0)||_+^2e^{c_n^jt}\left(1+\langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\right) 
        }\\
        &\geq p_n\sqrt{k_n}||D_j^n||\frac{||w_j(0)||_+^2\left(1+\langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\right)}{2+||w_j(0)||_+^2\left(1+\langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\right)}\\
        &\geq p_n\sqrt{k_n}||D_j^n||\frac{||w_j(0)||_+^2}{2+||w_j(0)||_+^2}
        \end{split}
    \end{equation}
    since $\langle \bar{D}_j^n|s_j\bar{w}^+_j(0) \rangle\geq 0$. Finally, we obtain the desired lower bound.
    \begin{equation}
        \mu(t) \geq \frac{2}{np_n}\min_i\sum_j^p||w_j(t)||_+^21_{j,i} = \frac{2\min_j||w_j(t)||_+^2}{np_n} \geq \frac{2C_y^-}{n^{\alpha}}\min_j\frac{||w_j(0)||_+^2}{2+||w_j(0)||_+^2}
    \end{equation}
\end{proof}

\subsection{Lemma \ref{lem:speed_at_init}}

\begin{proof}\textbf{of Lemma \ref{lem:speed_at_init}}\\
    Let us recall the equation of the local-PL curvature on the system.
\begin{equation}
    \mu(0) = \frac{2}{np}\sum_{j=1}^p(w_j(0)XP_jR(0))^2+|a_j(0)|^2||XP_jR(0)||^2
\end{equation}
Here there are 3 types of variables: $w_j$, $a_j$ and $P_j$ which is the diagonal matrix with diagonal $1_{j,i}$. We apply the Central Limit Theorem on the average on $p$ to find the limit law of $\mu(0)$. Let us first look at the residual limit at $t=0$ for large $p$.
\begin{equation}
         R(0) = Y - \frac{1}{p}\sum_{j=1}^pa_jw_j^TXP_j = Y - \mathbb{E}[aw^TXP] - \frac{\xi_p}{\sqrt{p}} = \tilde{Y} - \frac{\xi_p}{\sqrt{p}}
\end{equation}
With $\xi_p\underset{p\rightarrow+\infty}{\longrightarrow}\xi\sim\mathcal{N}(0,\mathbb{V}(aw^TXP))$. We split the residuals in the expression of $\mu(0)$.
\begin{equation}
   \begin{split}
            (w_j^TXP_jR)^2 &= (w_j^TXP_j\tilde{Y})^2 - \frac{2}{\sqrt{p}}\tilde{Y}^T(w_j^TXP_j)^T(w_j^TXP_j)\xi_p + \frac{1}{p^2}(w_j^TXP_j\xi_p)^2\\
            ||XP_jR||^2&= ||XP_j\tilde{Y}||^2 - \frac{2}{\sqrt{p}}\tilde{Y}^T(XP_j)^T(XP_j)\xi_p + \frac{1}{p^2}||XP_j\xi_p||^2\\
        \end{split}
\end{equation}
This means that we can compute the average
\begin{equation}
         \mu(0)\underset{p\rightarrow+\infty}{\longrightarrow}\frac{2}{n}\mathbb{E}[(w^TXP\tilde{Y})^2+|a|^2||XP\tilde{Y}||^2] = \frac{2}{n}\beta_0
    \end{equation}
    and the deviation, as the deviation of the central terms and the first order deviations of the residuals,
    \begin{equation}
         \sqrt{p}(\mu(0)-\frac{2}{n}\beta_0)\underset{p\rightarrow+\infty}{\longrightarrow}\mathcal{N}(0,\gamma_0^2)
    \end{equation}
    with 
    \begin{equation}
        \begin{split}
            \gamma_0^2 &= \mathbb{V}((w^TXP\tilde{Y})^2+|a|^2||XP\tilde{Y}||^2)\\ 
            &+ \left|\left|\sqrt{\mathbb{V}(\xi)}\mathbb{E}[(w^TXP)^T(w^TXP)]\tilde{Y}\right|\right|^2\\ 
            &+ \left|\left|\sqrt{\mathbb{V}(\xi)}\mathbb{E}[|a|^2(XP)^T(XP)]\tilde{Y}\right|\right|^2.\\
        \end{split}
    \end{equation} 
    where the first term is the deviation of the mean, and the two other come from the deviation of the residuals.
\end{proof}

\subsection{Theorem \ref{thm:phase_transition}}

\begin{proof}\textbf{of Theorem \ref{thm:phase_transition}}\\
    We consider the setting of Proposition \ref{prop:group_examples} but with a fixed number of neuron $p$, and as in its proof, we focus on one specific neuron $j$ for which we suppose $s_j=1$. We can rewrite equation \ref{norm}.
    \begin{equation}
        ||w_j(t)||_+^2 = \frac{p\sqrt{k_n}||D_j^n||||w_j(0)||_+^2
        \left(\cosh\left(c_n^jt\right) + \langle \bar{D}_j^n|\bar{w}_j^+(0) \rangle\sinh\left(c_n^jt\right)\right)}{p\sqrt{k_n}||D_j^n||+||w_j(0)||_+^2 
        \left(\sinh\left(c_n^jt\right) + \langle \bar{D}_j^n|\bar{w}_j^+(0) \rangle(\cosh\left(c_n^jt\right)-1)\right)}
    \end{equation}
    Let us rewrite the loss of the group $j$.
    \begin{equation}
        \begin{split}
        L^j(t) &= \frac{1}{2k_n}\sum_{i=1}^{k_n}(r_i^j)^2\\
        &= \frac{1}{2k_n}\sum_{i=1}^{k_n}\left(y_i^j - \frac{||w_j||_+}{p}\langle \bar{w}_j^+|x_i^j\rangle\right)^2\\
        &= \frac{1}{2}\left[ ||D_j^n||^2 - \frac{2}{\sqrt{k_n}p}||D_j^n||\langle \bar{D}_j^n|\bar{w}^+_j\rangle ||w_j||^2_+ + \frac{1}{k_np^2}||w_j||^4_+\right]
        \end{split}
    \end{equation}
    Let $t_n^j(\kappa) = \frac{1}{c_n^j}\log(\kappa p\sqrt{k_n}||D_j^n||)$, where $c_n^j=\frac{2||D_j^n||}{\sqrt{np}}$ which depends on the variable $\kappa>0$, we have
    \begin{equation}
        ||w_j(t_n^j(\kappa))||_+^2 = p\sqrt{k_n}||D_j^n|| \frac{\kappa||w_j(0)||_+^2\left(1 + \frac{K(j,n)^2}{\kappa^2} + \langle \bar{D}_j^n|\bar{w}^+_j(0)\rangle\left(1-\frac{K(j,n)^2}{\kappa^2}\right)\right)}{2+\kappa||w_j(0)||_+^2\left(1 - \frac{K(j,n)^2}{\kappa^2} + \langle \bar{D}_j^n|\bar{w}^+_j(0)\rangle\left(1-\frac{K(j,n)}{\kappa}\right)^2\right)}
    \end{equation}
    with $K(j,n) = \frac{1}{\sqrt{k_n}p||D_j^n||}$. Moreover, we have 
    \begin{equation}
        \langle \bar{D}_j^n|s_j\bar{w}^+_j(t_n^j(\kappa)) \rangle = \frac{1 - \frac{K(j,n)^2}{\kappa^2} + \langle \bar{D}_j^n|\bar{w}^+_j(0)\rangle\left(1+\frac{K(j,n)^2}{\kappa^2}\right)}{1 + \frac{K(j,n)^2}{\kappa^2} + \langle \bar{D}_j^n|\bar{w}^+_j(0)\rangle\left(1-\frac{K(j,n)^2}{\kappa^2}\right)}.
    \end{equation}
    Thus, by taking $n$ large enough, there exists $\kappa(j,n,\varepsilon)$ such that $L^j(t_n^j(\kappa(j,n,\varepsilon))) = L^j(t_n^j(\varepsilon)) = \varepsilon ||D_j^n||^2$. Moreover, $\kappa(j,n,\varepsilon) \rightarrow \kappa^j(\varepsilon)$ when $n$ goes to infinity. Indeed,
    \begin{equation}
        L^j(t_n^j(\varepsilon)) \rightarrow \frac{1}{2}\left[||D_j^{\infty}|| - \frac{\kappa^j(\varepsilon)||w_j(0)||_+^2}{2+\kappa^j(\varepsilon)||w_j(0)||_+^2}\right]^2 = \frac{\varepsilon}{2} ||D_j^{\infty}||^2
    \end{equation}
    which means $\kappa^j(\varepsilon) = \frac{2}{||w_j(0)||_+^2}\frac{||D_j^{\infty}||\left(1-\sqrt{\varepsilon}\right)}{1+||D_j^{\infty}||\left(1-\sqrt{\varepsilon}\right)}$. Thus, we have a phase transition since
    \begin{equation}
        \frac{t_n^j(\varepsilon)}{t_n} = 4\frac{\log(\kappa(j,n,\varepsilon) p\sqrt{k_n}||D_j^n||)}{c_n^j\sqrt{np}\log(n)} \sim_n \frac{1}{||D_j^{\infty}||}
    \end{equation}
    and the cutoff window is 
    \begin{equation}
        \frac{t_n^j(\varepsilon)-t_n^j(1-\varepsilon)}{t_n} = \log\left(\frac{\kappa(j,n,\varepsilon)}{\kappa(j,n,1-\varepsilon)}\right)\frac{1}{c_n^jt_n} \sim_n \frac{1}{2||D_j^{\infty}||}\log\left(\frac{\kappa^j(\varepsilon)}{\kappa^j(1-\varepsilon)}\right)\frac{1}{\log(n)}
    \end{equation}
    where we recall that $t_n = \frac{\sqrt{np}}{4}\log(n)$. We conclude that the normalized loss thus has at most $p$ phase transition at times $\frac{1}{||D_j^{\infty}||}$. Moreover, the constant in the Theorem is
    \begin{equation}
        C^j(\varepsilon) = \frac{\left(1-\sqrt{\varepsilon}\right)}{1+||D_j^{\infty}||\left(1-\sqrt{\varepsilon}\right)}\frac{1+||D_j^{\infty}||\left(1-\sqrt{1-\varepsilon}\right)}{\left(1-\sqrt{1-\varepsilon}\right)}
    \end{equation}
\end{proof}

\subsection{Other results}
\begin{proof}\textbf{of Lemma \ref{lem:homogeneity}}\\
    Let us write the two derivatives of the parameters.
    \begin{equation}
        \begin{split}
            \frac{d}{dt}w_j &= \frac{a_j}{n}\sum_{i=1}^nr_ix_i1_{j,i}\\
            \frac{d}{dt}a_j &= \frac{1}{n}\sum_{i=1}^nr_i\langle w_j|x_i\rangle_+\\
        \end{split}
    \end{equation}
    We verify that $\langle\frac{d}{dt}w_j|w_j\rangle = a_j\frac{d}{dt}a_j$, which gives the desired property once integrated.
\end{proof}

\begin{proof}\textbf{of Lemma \ref{lem:average_PL}}\\
    By definition, we have $\mu(t) = \frac{|\nabla L(\theta_t)|^2}{L(\theta_t)}$, and by property of the gradient flow, $|\nabla L(\theta_t)|^2 = -\frac{d}{dt}L(\theta_t)$. Thus, 
    \begin{equation}
        \begin{split}
            \frac{d}{dt}L(\theta_t) &= -\mu(t)L(\theta_t)\\
            \log(L(\theta_t)) - \log(L(\theta(0))) &= -\int_0^t\mu(x)dx\\
            L(\theta_t) &= L(\theta(0))e^{-\langle \mu(t)\rangle t}
        \end{split}
    \end{equation}
\end{proof}

\section{Experiments}
\label{app:experiments}
This Appendix contains additional details on the experiments done in Section~\ref{section:experiments}. Data generation and weight initialization were performed as follows: we initialize all neurons independently as $w_j\sim\mathcal{N}(0,\frac{1}{d}I_d)$ as well as $\frac{a_j}{|a_j|}\sim\mathcal{U}(\{-1,1\})$ and $|a_j| - ||w_j|| \sim \text{Exp}(1)$ which implies $|a_j(0)| \geq ||w_j(0)||$. For the data, we consider $y_i\sim\mathcal{U}([-2,-1]\cup[1,2])$ and $||x_i||\sim\mathcal{U}([1,2])$ in order to control the constants $C_x^-, C_y^- \geq 1$ and $C_x^+,C_y^+ \leq 2$. Finally, in order to fall within the assumptions of Lemma \ref{lem:hyp_data}, we let $\frac{x_i}{||x_i||}\sim\mathcal{U}(\mathbb{S}^{d-1})$ in Section~\ref{exp1}, and $\frac{x_i}{||x_i||}$ be an orthogonal family in Section~\ref{exp2}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\linewidth]{Convergence_Threshold_d.png}
    \caption{This graph shows the scaling law of the convergence threshold for a fixed number of neurons. It suggests that the scaling is linear in $d$: $N(d,p) = C(p)d$.}
    \label{fig:proba_d}
\end{figure}

\paragraph{Experiment 1.} For the experiment in Figure \ref{fig:proba_CV}, we trained 500 networks in dimension 100, with $n$ between 2500 and 3500, with 25 runs for each value of $n$. We used $p=\lfloor\frac{\log(\frac{n}{\varepsilon})}{\log(\frac{4}{3})}\rfloor+1$ neurons for each experiment with $\varepsilon=0.05$, since this is the optimal threshold obtained in Lemma \ref{lem:all_act_big}. We trained the networks with gradient descent using a learning rate of $1$ for a total time $t_{\infty} = 1.5\times\frac{\sqrt{np}}{4}\log(np)$ and thus $e=\frac{t_{\infty}}{\text{lr}}$ epochs. 

We considered that a network converged as long as its loss went below $\frac{C_y^-}{2n}$, which then guarantees convergence to 0. We thus early stopped the training and declared the loss was exactly 0. Otherwise, the convergence went for all epochs and the network was assumed to not be able to reach 0 loss. In doted line, we interpolate the probability plot using a sigmoid function, and learned automatically the convergence threshold $N(d,p)$. 

For the scaling law on $d$, we fixed $p$ at $30$, and trained networks with dimension varying from 10 to 100, and $n$ ranging from $N(d,p)-15d$ to $N(d,p)+15d$, with step $d$. For each dimension, we interpolate the probability graph using a sigmoid, and plotted the linear trend on Figure \ref{fig:proba_d}. For the scaling in $p$, we fixed $d=30$, and varied $p$ from 50 to 400, and plotted the trend on Figure \ref{fig:proba_p} which shows that the scaling in $p$ is sub-linear.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{Convergence_Threshold_p.png}
    \caption{This graph show the scaling law of the convergence threshold for a fixed number of neurons. It suggests that the scaling is not linear in $d$, but it is hard to differentiate between a sub-linear polynomial growth or a logarithmic growth.}
    \label{fig:proba_p}
\end{figure}

\paragraph{Experiment 2.} For this experiment, we trained 500 networks in dimension 2000, with $n$ between 1000 and 2000, with 25 runs for each value of $n$. We used the same number of neurons, learning rates, and epochs as in experiment 1. Let us recall the 4 measures we plotted on the Figure \ref{fig:CV_speed}:
\begin{enumerate}
    \item The instantaneous local-PL curvature at the end of the training, $\mu(t_{\infty}) = \log\left(\frac{L(t_{\infty}-1)}{L(t_{\infty})}\right)$,
    \item The average-PL curvature throughout the training, $\langle \mu_{\infty}\rangle = \log\left(\frac{L(0)}{L(t_{\infty})}\right)$,
    \item The lower bound on the local-PL at the end of the training, $\mu_{\text{low}} = \frac{2}{n}\min_{i}\frac{1}{p}\sum_{j=1}^p|a_j|^21_{j,i}$,
    \item The upper bound on the local-PL at the end of the training, $\mu_{\text{upp}} = \frac{16}{n}\max_{i}\frac{1}{p}\sum_{j=1}^p|a_j|^21_{j,i}$.
\end{enumerate}
Each of the slope being close to $-\frac{1}{2}$, we conclude from this log-log graph that $\langle \mu_{\infty}\rangle = \frac{K}{\sqrt{n}}$ as foreseen in Conjecture \ref{conjecture}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{Asymptotic_log_mean_PL.png}
    \caption{Scaling laws in log-log for different measures of the local-PL curvature in dimension 2000. Each curve is in fact linear with slope close to $-\frac{1}{2}$, which is expected by Conjecture \ref{conjecture}.}
    \label{fig:CV_speed}
\end{figure}
\medbreak
Each plot's related experiments were performed on a MacBook Air under 2 hours without acceleration materials.



\section{Additional results}
\subsection{Collapse of the second layer}
\label{app:collapse_a_j}

Similar to the early alignment phenomenon described by \citet[Theorem 2]{boursier2024early, boursier2024simplicitybiasoptimizationthreshold}, where the neurons can rotate and collapse to align on a single vector preventing minimization of the loss, the weights $a_j$ of the second layer can also collapse on a single direction. Under the hypothesis that $|a_j(0)| \geq ||w_j(0)||$, the scalar $a_j$ cannot change sign, which prevents this scenario in the article's results. But if $|a_j(0)| < ||w_j(0)||$, they can change sign, and prevent global minimization even when the neurons are correctly initialized. Proposition \ref{prop:counter_ex} gives an example of such collapse in low dimension.

\begin{proposition}
    \label{prop:counter_ex}
    Suppose that $d=n=p=2$. Let $(x_1,x_2)$ be the canonical basis of $\mathbb{R}^2$, with the outputs satisfying $y_1y_2<0, \lambda = |\frac{y_2}{y_1}|$. Let $|a_1(0)|, |a_2(0)| \leq \delta$, and let $\min_{j,i}\langle w_j(0)|x_i\rangle >0$. Then, for $\delta$ small enough, $y_1$ large enough, and 
    \begin{equation}
        \sqrt{\max_{j,i}\langle w_j(0)|x_i\rangle\frac{8}{y_1}}\leq \lambda \leq \frac{\min_{j,i}\langle w_j(0)|x_i\rangle}{\max_{j,i}\langle w_j(0)|x_i\rangle}
    \end{equation} 
    we have $\lim_{t\rightarrow+\infty}L(\theta_t) >0$.
\end{proposition}

The proof relies on the ratio between outputs being large $\lambda$, in order to steer the $a_j$ to change signs, but not too large to then make the neuron go extinct before the signs of $a_j$ may change again. This traps the network in a state of sub-optimal loss, and if $a_j$ were initialized as large as the vectors, this collapse could not have happened.

\begin{proof}\textbf{of Proposition \ref{prop:counter_ex}}
    Without loss of generality, let us suppose $y_1>0$ and $y_2<0$. We will show that there are values of $\lambda,\varepsilon$ for which the system will not converge. The derivatives of $a_j$ at the beginning of the dynamics writes
    \begin{equation}
        \begin{split}
            \frac{d}{dt}a_j &= \frac{1}{4}(r_1\langle w_j|x_1\rangle + r_2\langle w_j|x_2\rangle) \\
            \left|\frac{d}{dt}a_1 - \frac{y_1}{4}(\langle w_1|x_1\rangle - \lambda\langle w_1|x_2\rangle)\right|&\leq \max(|a_1|, |a_2|)\max(||w_1||,||w_2||)\\
            \left|\frac{d}{dt}a_2 - \frac{y_1}{4}(\langle w_2|x_1\rangle - \lambda\langle w_2|x_2\rangle)\right|&\leq \max(|a_1|, |a_2|)\max(||w_1||,||w_2||)
        \end{split}
    \end{equation}
    and the derivatives of $\langle w_j|x_i\rangle $ are
    \begin{equation}
        \begin{split}
            \frac{d}{dt}\langle w_j|x_i\rangle &= \frac{a_jr_i\mathbb{1}_{j,i}}{2}\\
            \left|\frac{d}{dt}\langle w_j|x_1\rangle - \frac{a_jy_1\mathbb{1}_{j,1}}{2}\right| &\leq \max(|a_1|, |a_2|)\max(||w_1||,||w_2||)\\
            \left|\frac{d}{dt}\langle w_j|x_2\rangle + \lambda\frac{a_jy_2\mathbb{1}_{j,2}}{2}\right| &\leq \max(|a_1|, |a_2|)\max(||w_1||,||w_2||)\\
        \end{split}
    \end{equation}
    Now suppose that for $t\leq T$, $|a_j|,||w_j||\leq M$ and $\langle w_j|x_i\rangle\geq m>0$, we have 
    \begin{equation}
        \begin{split}
            a_j(t) &\geq -\delta + \left(\frac{y_1}{4}(m-\lambda M) - M^2\right)t\\
            a_j(t) &\leq \delta + \left(\frac{y_1}{4}(M-\lambda m) + M^2\right)t\\
        \end{split}
    \end{equation}
    Thus, for $T> \frac{\delta}{\left(\frac{y_1}{4}(m-\lambda M) - M^2\right)}$ with $\lambda \geq \frac{m}{M}$ and $y_1\geq \frac{4M^2}{m-\lambda M}$, we have $a_j(T)> 0$. We now wish to find the constants $M,m$ such that the previous equation will hold. To find the constraint on $m$ and $M$, let us write
    \begin{equation}
        \begin{split}
            \langle w_j|x_1\rangle &\geq \langle w_j(0)|x_1\rangle - \left(\frac{1}{2}y_1M + M^2\right)t\\
            \langle w_j|x_2\rangle &\geq \langle w_j(0)|x_2\rangle - \left(\frac{\lambda}{2}y_1M + M^2\right)t\\
            \langle w_j|x_1\rangle &\leq \langle w_j(0)|x_1\rangle + \left(\frac{1}{2}y_1M + M^2\right)t\\
            \langle w_j|x_2\rangle &\leq \langle w_j(0)|x_2\rangle + \left(\frac{\lambda}{2}y_1M + M^2\right)t\\
        \end{split}
    \end{equation}
    Thus, the constraints are
    \begin{equation}
        \begin{split}
            m &\geq \min\left(\min_{j,i}\langle w_j(0)|x_2\rangle, \delta\right) -\delta\frac{2y_1M + 4M^2}{y_1(m-\lambda M) - 4M^2} \\
            M &\leq \max\left(\max_{j,i}\langle w_j(0)|x_2\rangle,\delta\right)+\delta\frac{2y_1 M+ 4M^2}{y_1(m-\lambda M) - 4M^2}
        \end{split}
    \end{equation}
    We see that the constraint are satisfied with $m\geq \min_{j,i}\langle w_j(0)|x_2\rangle-2\delta>0$ and $M\leq\max_{j,i}\langle w_j(0)|x_2\rangle+2\delta$ if: $\delta$ is small enough, $y_1$ is large enough, and $\lambda < \frac{\min_{j,i}\langle w_j(0)|x_2\rangle}{\max_{j,i}\langle w_j(0)|x_2\rangle}$. Thus, there exists $T>0$ such that at time $T$, we have $a_1(T),a_2(T)> 0$, and no neurons went extinct. 
    
    Now, let us show that neurons $\langle w_j|x_2\rangle$ will go to 0 for some time $\mathcal{T}>0$, while the neurons $a_j$ stay positive. We can use the same equations as before, with this time $|a_j|,||w_j||\leq N$ for $t\leq \mathcal{T}$, and get 
    \begin{equation}
        \langle w_j|x_2\rangle \leq \langle w_j(0)|x_2\rangle - \left(\frac{\lambda}{2}y_1N - N^2\right)t
    \end{equation}
    Thus, for $\mathcal{T} = \frac{2\max_j\langle w_j(0)|x_2\rangle}{\lambda y_1N - 2N^2}$ and $\lambda y_1 \geq 2N$, we have extinction of the neurons. To find the constraint on $N$, use the bounds on the growth of $a_j$ and $\langle w_j(0)|x_1\rangle$. The constraint is
    \begin{equation}
        N \leq \max\left(\max_j\langle w_j(0)|x_1\rangle, \delta\right)+2\max_j\langle w_j(0)|x_2\rangle\frac{y_1 + 2N}{\lambda y_1 - 2N}
    \end{equation}
    Thus, the constraints are satisfied with $N \leq \max_{j,i}\langle w_j(0)|x_i\rangle(1 + \frac{3}{\lambda})$ as long as $y_1$ is large enough, and 
    \begin{equation}
        \lambda \geq \sqrt{\max_{j,i}\langle w_j(0)|x_i\rangle\frac{8}{y_1}}.
    \end{equation}
    After time $\mathcal{T}$, the neurons $\langle w_j(0)|x_2\rangle$ went extinct, and thus we have $L(\theta_t) \geq \frac{y_2^2}{4}>0$.
\end{proof}

\subsection{Antipodality}
\label{app:antipodality}

In this section, we give a trick to show how to make the network learn $2d$ vectors in dimension $d$. Indeed, one can see, if $\langle \bar{x}_i|\bar{x}_k\rangle = -1$, then there is no problem of large interaction between inputs as $1_{j,i}1_{j,k}=0$, since a vector $w_j$ is either activated on $x_i$ or activated on $x_k$. Thus, if we note $\bar{x}_{-i} = -\bar{x}_i$, we can replace the condition in Assumption \hyperref[assump:data]{2}, by
\begin{equation}
    N_2=\left|\left|X^TX - \left(\begin{array}{c|c} D_X^+ & -\sqrt{D_X^+D_X^-} \\ \hline -\sqrt{D_X^+D_X^-} & D_X^- \end{array} \right) \right|\right| \leq \frac{(C_x^-)^2}{\sqrt{n}}\frac{C_y^-}{C_y^+}
\end{equation}
with $D_X^+$ the diagonal matrix with diagonal terms $||x_i||^2$, and $D_X^-$ the diagonal matrix with diagonal terms $||x_{-i}||^2$. Thus, adding antipodal vectors will result in the same equations:
\begin{equation}
    \begin{split}
        \frac{d}{dt}\langle w_{j_i^*}|x_i\rangle &\geq \frac{|a_{j_i^*}|}{n}\left[r_i||x_i||^21_{{j_i^*},i}s_{j_i^*} - r_{-i}||x_{i}||||x_{-i}|| 1_{{j_i^*},-i}- N_2\sqrt{2nL(\theta(0))}\right]\\
        &= \frac{|a_{j_i^*}|}{n}\left[r_i||x_i||^21_{{j_i^*},i}s_{j_i^*}- N_2\sqrt{2nL(\theta(0))}\right]
    \end{split}
\end{equation}
Since, by definition of $j_i^*$, $\langle w_{j_i^*}|x_i\rangle \geq0$, thus $1_{{j_i^*},-i}=0$. And 
\begin{equation}
    \begin{split}
        ||XP_jR||^2 &\geq (C_x^-)^2\left[||P_jR||^2-2(R^+P_j^+)^TR^-P_j^-\right] - N_2||P_jR||^2\\
        &\geq (C_x^-)^2||P_jR||^2 - N_2||P_jR||^2\\
    \end{split}
\end{equation}
where $R^+_i = r_i$, $R^-_i = r_{-i}$, $(P_j^+)_i = 1_{j,i}$ and $(P_j^-)_i = 1_{j,-i}$. As said $P_j^+P_j^- = 0$. These two equations being the most important for the proof of Theorem \ref{thm:CV_hig_dim}, it holds with antipodal input data.

This shows a weakness of the proof: we expect this to generalize when the data are perturbed by $\varepsilon$ small, yet here going from exactly antipodal to nearly-antipodal make the proof incorrect. To accommodate for this, one has to control the small zone were interaction can exists and prevent vectors from entering that zone at all time. 

\end{document}