\section{Related works}
\label{section:related_works}
\paragraph{Convergence of neural networks.} Neural networks are known to converge under specific data, parameter, or initialization hypotheses, among which: the neural tangent kernel regime studied by ____, that has been shown to correspond in fact to a \textit{lazy regime} where there is no feature learning because of the initialization scale. Another field of study is the \textit{mean-field} regime, where feature learning can happen but where the optimization has been shown to converge only in the infinite width case ____. Note that it is also possible to produce generic counter examples, where convergence does not occur____. Beyond these, there have been attempts to generalize convergence results under local PL (or local curvature) conditions as shown by ____, but they remain unsatisfactory to explain the good general behavior of neural networks due to the constraint it imposes on the initialization. Convergence theorems similar in spirit to Theorem~\ref{thm:CV_hig_dim} can be found in an article by ____. The main difference relies on two features: only the inner weights are trained and their result necessitates a large value of outer weights when $n$ is large, which is the regime of interest of the present article. Finally, it is worth mentioning other works on neural networks dynamics, e.g., the study of the implicit bias either for regression____ or classification____, or sample complexity to learn functions in a specific context____.

\paragraph{Polyak-Łojasiewicz properties.} Dating back from the early sixties, Polyak derived a sufficient criterion for a smooth gradient descent to converge to a global minimizer____. This corresponds to the later-called Polyak-Łojasiewicz (PL) constant $\mu$ of a function $f:\mathbb{R}^d\rightarrow\mathbb{R}_+$, that  can be defined as the best exponential rate of convergence of gradient flow over all initializations, or equivalently to the following minimum ratio $\mu = \min_{x\in\mathbb{R}^d}\frac{||\nabla f(x)||^2}{f(x)}$. This has found many applications in non-convex optimization, as it is the case for neural network optimization, and is very popular for optimization in the space of measures____. Other notions of PL conditions have emerged in the literature to characterize local convergence, by bounding the PL constant over a ball $\mu^*(z,r) = \min_{x\in\mathcal{B}(z,r)}\frac{||\nabla f(x)||^2}{f(x)}$ ____ and comparing it to $f(z)$. We use a notion of PL which is local and trajectory-wise to prove lower bounds valid on each trajectory.