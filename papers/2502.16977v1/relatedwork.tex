\section{Related works}
\label{section:related_works}
\paragraph{Convergence of neural networks.} Neural networks are known to converge under specific data, parameter, or initialization hypotheses, among which: the neural tangent kernel regime studied by \citet{jacot2018neural,arora2019fine,du2018gradient,allen2019convergence}, that has been shown to correspond in fact to a \textit{lazy regime} where there is no feature learning because of the initialization scale. Another field of study is the \textit{mean-field} regime, where feature learning can happen but where the optimization has been shown to converge only in the infinite width case \citep{mei2018mean,chizat2018global,rotskoff2018parameters}. Note that it is also possible to produce generic counter examples, where convergence does not occur~\citep{boursier2024simplicitybiasoptimizationthreshold}. Beyond these, there have been attempts to generalize convergence results under local PL (or local curvature) conditions as shown by \citet{chatterjee2022convergence,liu2022loss,zhou2021local}, but they remain unsatisfactory to explain the good general behavior of neural networks due to the constraint it imposes on the initialization. Convergence theorems similar in spirit to Theorem~\ref{thm:CV_hig_dim} can be found in an article by \citet{chen2022feature}. The main difference relies on two features: only the inner weights are trained and their result necessitates a large value of outer weights when $n$ is large, which is the regime of interest of the present article. Finally, it is worth mentioning other works on neural networks dynamics, e.g., the study of the implicit bias either for regression~\citep{boursier2022gradient} or classification~\citep{Lyu2020Gradient,ji2020directional}, or sample complexity to learn functions in a specific context~\citep{glasgow2023sgd}.

\paragraph{Polyak-Łojasiewicz properties.} Dating back from the early sixties, Polyak derived a sufficient criterion for a smooth gradient descent to converge to a global minimizer~\citep{Polyak1964}. This corresponds to the later-called Polyak-Łojasiewicz (PL) constant $\mu$ of a function $f:\mathbb{R}^d\rightarrow\mathbb{R}_+$, that  can be defined as the best exponential rate of convergence of gradient flow over all initializations, or equivalently to the following minimum ratio $\mu = \min_{x\in\mathbb{R}^d}\frac{||\nabla f(x)||^2}{f(x)}$. This has found many applications in non-convex optimization, as it is the case for neural network optimization, and is very popular for optimization in the space of measures~\citep{gentil2020entropie}. Other notions of PL conditions have emerged in the literature to characterize local convergence, by bounding the PL constant over a ball $\mu^*(z,r) = \min_{x\in\mathcal{B}(z,r)}\frac{||\nabla f(x)||^2}{f(x)}$ \citep{chatterjee2022convergence, liu2022loss} and comparing it to $f(z)$. We use a notion of PL which is local and trajectory-wise to prove lower bounds valid on each trajectory.