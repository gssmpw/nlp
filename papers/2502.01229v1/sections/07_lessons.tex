\section{Recommendations for Cost Models} \label{sec:lessons}
In this paper, we comprehensively evaluated \lcms against traditional cost models for query optimization. 
In the following, we summarize our results and recommendations.
As a main outcome, in \textit{none} of the tasks we analyzed, \lcms can significantly beat traditional approaches for cost modeling -- even though \lcms provide higher accuracy over query workloads.
The total execution time of plans selected by the \lcms in query optimization was, in fact, often even higher than with traditional models.
However, we still believe that \lcms have a high potential, but the focus \textit{only} on their accuracy has resulted in the position we are in today.
In the following, we distill recommendations based on our main findings to provide future directions to unlock the full potential of \lcms.

\noindent \textbf{R1: Consider Model Architectures and Features.}
As shown in the classification of models in \Cref{subsec:taxonomy}, \lcms largely vary in input features, query representation and model architecture.
We summarize the most critical learnings:
(1) Learning from the query plan is absolutely necessary, but using the SQL string as input alone is unsuitable
(2) Simple model architectures like \flatvector often perform relatively well, making it questionable whether very complex architectures are necessary. 
(3) DB-agnostic \lcms often outperformed DB-specific models, as they were trained on a larger variety of query workloads and data distributions.
(4) While precise cardinality estimates help solve the downstream tasks, it remains unclear to what extent statistics and sample bitmaps help. 
%------------------------

\noindent \textbf{R2: Use Appropriate Metrics.}
Another key finding of this paper is that traditional evaluation strategies are insufficient to assess how good \lcms are for query optimization. 
Therefore, we recommend using the metrics presented in this paper that show how well an \lcm selects from multiple plan candidates, how well it can rank these plans, and what speed-up it provides for a given query optimization task.
Ideally, these metrics influence the design and learning approach of future \lcms.
For example, one direction could be to apply ranking-based approaches, which have been used recently for end-to-end learned optimizers \cite{behr2023, chen2023, zhu2023} to cost models.

\noindent \textbf{R3: Diversify Your Training Data.}
The third key finding of this study is that traditional training strategies for \lcms induce fundamental biases stemming from how training data is collected.
Specifically, \lcms are typically trained on \textit{pre-optimized queries}, as these have already been executed by a database system  (cf. \Cref{fig:learning_procedure}\circles{B}) to generate the corresponding labels.
Another reason for biases in training data is timeouts, which are necessary to execute training queries within a limited, reasonable time.
Query plans with expensive operators like nested-loop joins are thus likely to timeout before completion. 
Consequently, only the cases where these operators are beneficial are included in the training data, leading to a bias in the models, as for access path selection (\Cref{sec:access_path_selection}). 
A similar bias was observed for learned cardinality estimation in \cite{reiner2023}.
Overall, the traditional training strategies are thus particularly ill-suited for query optimization, as they lead to a substantial divergence between the training data distribution and non-optimized queries that occur during query optimization.
To resolve this bias, training data should be diversified so that both good and bad query plans can be learned from.

We demonstrate the effect of how diversification can help with the task of access path selection in a small experiment.
In particular, we fine-tune \lcms with additional training data consisting of 500 randomized scan queries with different selectivities.
For each query, we enforce two executions: One with an \texttt{IndexScan} and one with a \texttt{SeqScan}, and we do not enforce timeouts.
We report the balanced accuracy $B$ over the columns from IMDB of \Cref{tab:scan_costs_over_datasets} \textit{before} and \textit{after} fine-tuning in \Cref{fig:retraining}~ \circles{A}.
As we can see, using diversified training data is highly beneficial in most cases as accuracy improves.
Even more importantly (see \Cref{fig:retraining}~\circles{B}), this also leads to improvements of up to 45\% in the total runtime across all \lcms.
Interestingly, \zeroshot is the only cost model that outperforms the runtime of \postgresx (95s vs. 116s), which shows that \lcms can outperform traditional approaches for downstream tasks and training data plays a crucial role.
The results for \dace typically do not improve because it seems to learn mainly from PostgreSQL costs as a signal, as discussed in \Cref{subsec:access_path_selectivity}.

However, while this diversification of training data has already been leveraged in the domain of learned query optimization (e.g., by cardinality injection \cite{zhu2023, doshi23} or exploration \cite{yang_balsa_2022}), its application for \lcms is not trivial.
One challenge is that each query plan would result in many possible candidates (e.g., join order permutations).
Sophisticated strategies are required to select meaningful training queries to maximize information gain and still keep the training data execution costs reasonable.
Related promising ideas are data-efficient training strategies \cite{agnihotri2024}, pseudo-label generation \cite{liu2022}, geometric learning \cite{reiner2023}, or simulation \cite{yang_balsa_2022}.
Another challenge is extremely long-running query plans, which cannot be executed in a practical sense as one plan would take days.
For this, other strategies are needed to reflect also timeout queries in the training procedure.
This is again non-trivial, as simply using a high constant value representing timeout queries leads to difficulties, as the \lcm cannot really learn meaningful costs from this.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/retraining.pdf}
    \caption{Fine-tuning for access path selection on IMDB columns. 
    \circles{A} For most columns, balanced accuracy increases after fine-tuning.
    \circles{B} Also the total runtime improved, so that \zeroshot outperforms \postgres.}
    \label{fig:retraining}
\end{figure}

\noindent \textbf{R4: Do Not Throw Expert Knowledge Away.}
We observed that using estimates from PostgreSQL is beneficial for \lcms, as demonstrated by the comparably good results of \dace and \qppnet.
Such a hybrid approach combines both the expert knowledge incorporated in traditional approaches and the strength of ML, which can learn arbitrarily complex functions.
While \dace and \qppnet inherently use these estimates as training features, recent work explicitly combines traditional cost functions with learned query-specific coefficients as a hybrid approach \cite{yang2023}.
This hybrid approach is particularly promising when looking at situations where \lcms still face major challenges, and traditional models are more reliable, like string-matching operations or user-defined functions.