\section{Task 1: Join Ordering} \label{sec:join_order}

Optimizing join order is a crucial task in query optimization as it improves query performance, especially in complex queries involving joins over multiple tables.
In the following, we analyze \emph{how reliably current \lcms reason about join orders} by first describing the detailed experimental setting and additional metrics before presenting the results of several experiments.
%---------------------------------------------
\subsection{Evaluation Setup}\label{subsec:join_order_setup}
\noindent\textbf{Experimental Setting.}
Unlike previous evaluation strategies that evaluate the prediction accuracy on a single plan candidate, we compare how well \lcms can be used to pick a join order.
For this, we exhaustively generate \textit{all} possible join permutations for a given workload and let all \lcms predict their execution costs (as a traditional query optimizer would do).
We decided to use exhaustive enumeration to separate concerns and avoid a bad plan being picked based on the enumeration strategy (e.g., by only enumerating left-deep plans).
That way, we analyze how well \lcms provide costs (i.e., runtime) that enable an optimizer to pick good join orders, not how well the enumeration strategy works.
As a workload in this study, we use the JOB-light benchmark \cite{leis_how_2015, kipf2019}, which operates on the IMDB dataset consisting of 70 SPJA-Queries, as it is specifically designed to evaluate the task of join ordering.
In contrast to other datasets like TPC-H, this dataset contains diverse correlations and non-uniform data distributions, increasing the difficulty of the task.
%--------------------------------------------------------

\noindent\textbf{Experimental Metrics.}
As explained before, we define new metrics to evaluate \lcms for the join ordering task as follows:

\begin{enumerate}[leftmargin=*, nosep]
\item To evaluate how \lcms affect the outcome of query optimization in terms of runtime, we introduce the \textit{selected runtime}:
\begin{definition} \label{def:selected_runtime}
\textit{Selected Runtime ($r$)}:
Given a set of plan candidates, the selected runtime, $r$, is defined as the actual runtime $y$ of the plan that the \lcm would choose, i.e., where the prediction $\hat{y}$ is minimal.
Formally, it is:
$r = y_{\arg\min_{i} \hat{y}}$
\end{definition}

\item To report how well \lcms are able to select the optimal out of multiple plans, we introduce \textit{surpassed plans}.
\begin{definition}
\textit{Surpassed Plans ($s_p$)}:
In relation to a selected plan with actual runtime $y$, surpassed plans refer to the proportion of query plans that actually have a longer actual runtime.
$s_p=100\%$ therefore means that the optimal plan was selected, while $s_p=0\%$ stands for the worst choice.
Formally, it is defined as: $s_p = 100 \% * (1/n) * \sum_{i=1}^{n} \text{bool}(y_i > r)$ with total $n$ plans with $s_p \in [0\%, 100\%]$
\end{definition}
    
\item Moreover, it's crucial for \lcms to order query plans by their costs, as the optimizer needs to select from different candidates.
Thus, we report the correlation of actual and predicted runtimes of different query plans with the \textit{rank correlation} that asses the ranking ability of the \lcm towards the query plans \cite{spearman1904}.
\begin{definition}
\textit{Rank Correlation ($\rho$)}:
For our study, we use Spearman's Correlation which is given by: $\rho = 1 - \frac{6 \sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}$, with $\rho \in [0\%, 100\%]$, where $d_i$ is the difference between the ranks of corresponding runtimes, and $n$ is the number of plans.
\end{definition}
    
\item Finally, to evaluate the likelihood of \lcms to pick a non-optimal plan, we report under- and overestimation.
For instance, if the span of \lcm  under- and overestimation is high, it is more likely to select a plan that is, in fact, sub-optimal.
\begin{definition} \textit{Maximal Relative Under- /Overestimation($m_u, m_o$)}:
These metrics indicate by which factor a query plan candidate is over- or underestimated in the worst case. 
For a set of predictions $\hat{y}$ and labels $y$, they are defined as $m_u = min_{i} (y_i / \hat{y}_i)$ and  $m_o = max_{i} (\hat{y}_i / y_i)$.
\end{definition}
\end{enumerate}

\noindent\textbf{Metric Discussion:}
As we show for all experiments, the focus on accuracy alone is not sufficient to evaluate cost models for query optimization, and therefore, we propose various other metrics. 
However, it is important to note that each metric has limitations in what it can show, and only the combination of multiple metrics can help to understand the overall behavior.
For example, for join ordering, correlation metrics are good in helping to understand how well cost models rank plans. 
Still, they fail to analyze the severity of the effect if plans are not ranked well, which can lead to sub-optimal plan selections. 
For this reason, its is crucial to look at the percentage of surpassed plans and the total selected runtime.


\subsection{Example Query \& Metrics}\label{subsec:join_order_anecdotal}
We first report the results of an example query (Query $33$ from JOB-Light) in \Cref{fig:join_order_example} (at the top), as its results are representative of the full study that we conduct in \Cref{sec:join_order_full} and to illustrate our metrics introduced above.
As this query has four tables, there are 120 possible join permutations.
We discuss the results of each subplot (\circles{A} - \circles{G}) in the following:

\noindent\textbf{\circles{A} Model Predictions.}
We sort the plans with the different join permutations by their actual runtime, as presented by the black curve. 
Intuitively, the leftmost plan is optimal, with the shortest runtime of 2.20s, while the worst plan requires 11.17s.
Moreover, we show the predictions of all \lcms in the same plot to get a qualitative impression of their predictions. 
On the first view, the predictions of \lcms often show a behavior where predicted costs do not grow regularly with the actual costs.
In general, this is undesirable behavior, as it reinforces the choice of unfavorable plans since undesirable, false minima can occur in the cost predictions. 
In contrast, both \postgresx (gray curve) and \postgresxvi (gold curve) show that the predicted cost grows with the actual cost.

\noindent\textbf{\circles{B} Median Q-Error.}
To assess the overall prediction accuracy, we report the Q-Error ($Q_{50}$) of the predictions over all join permutations.
Interestingly, for this particular query, \postgresxvi shows the best results with $Q_{50}$=1.23, followed by \postgresx with  $Q_{50}$=1.42.
Moreover, another interesting observation is when looking at \mscn.
While the Q-error is not that bad (with $Q_{50} = 2.23$), the predictions are not helpful for an optimizer at all, as they predict the very same runtime value for all plans. 
Due to its SQL-based featurization, \mscn cannot reason about the join order of the query plan or the query operators and thus, it cannot be used for join ordering or physical plan selection.
Still, we included \mscn, which is often used as a naive baseline for cost predictions \cite{liu2022, hilprecht2022, zhao2022}.

\begin{figure*}[]
    \centering
    \includegraphics[width=\linewidth]{figures/join_order_examples.pdf}
    \caption{Example Query Nr. 33 from JOB-Light for join ordering. 
    We report model predictions (\circles{A}), overall accuracy (\circles{B}), outcome on query optimization (\circles{C}),  model optimality (\circles{D}), model ranking (\circles{E}) as well as under- and overestimation (\circles{E} \& \circles{F}).
    In this query, \postgresx picks the optimal plan and outperforms all \lcms for most of the metrics.}
    \label{fig:join_order_example}
\end{figure*}

%----------------------------------------------------


\textbf{\circles{C} Selected Runtime.}
To evaluate how much \lcms affect query optimization, we report the selected runtime ($r$), as this reflects the query runtime of the chosen plan when relying on the cost estimates of a given \lcm.
Here, the best cost model is \postgresx, where a query runtime of $r$=2.20s is achieved.
In contrast, the worst case is when using \queryformer; an optimizer would lead to a plan with a much longer runtime of $r$=5.56s, which is more than twice as optimal.
All other \lcms end up between \postgresx and \queryformer; i.e., no \lcm provides a better plan choice than \postgresx.
Interestingly, \postgresxvi selects a slightly worse plan than \postgresx, which is, however, still near-optimal. 

\textbf{\circles{D} Surpassed Plans.}
Next, we evaluate the surpassed plans ($s_p$).
This shows the fraction of plans outperformed by the selected plan from a given \lcm and thus shows the relative rank of the plan a model picks; i.e., if a model picks a plan on rank 5 of 10 plans, it surpasses $5/10=50\%$ of the plans.
Consistent with the previous results, \postgres beats the other models as it selects the optimal plan in this example ($s_p$=100$\%$).
In contrast, \queryformer (worst \lcm) only surpasses $s_p$=39\% of the plan candidates.

\textbf{\circles{E} Rank Correlation.}
As \lcms crucially need to determine the \textit{rank} of query plans by their costs, it is important to analyze the rank faithfulness of a cost model\footnote{
This observation led to \textit{ranking-based cost models} as an interesting alternative \cite{behr2023, chen2023, zhu2023}.}.
We report the ranking correlation ($\rho$) between the actual and predicted runtimes to evaluate how well the models perform in the ranking task.
In the given example query, both \postgresx and \postgresxvi have the best correlation ($\rho=0.79/ \rho=0.72$) indicating a successful ranking.
In contrast, all \lcm competitors are worse, down to $\rho = -0.23$ for \queryformer. 
Critically, the negative value means that as the actual runtime increases, the predicted runtime tends to decrease, indicating a failure of the ranking task.
This shows that cost predictions of traditional models provide a better ranking of query plans than \lcms for this query.

\textbf{\circles{F} \& \circles{G} Under- and Overestimation.}
We now report underestimation and overestimation of \lcms.
Overall, underestimation and overestimation indicate the probability of a query optimizer not picking the optimal plan.
For example, when significantly underestimating the runtime, a plan might be chosen that is, in fact, a long-running plan.
When looking at \circles{F} \& \circles{G}, we make two interesting observations:
(1) Under- \& overestimation is generally larger with \lcms than for \postgresx and \postgresxvi.
(2) Generally, PostgreSQL tends towards systematic underestimation of execution costs, which similarly has been shown by \cite{leis_how_2015}.
A typical reason for this is the assumption of filter attribute independence, leading to underestimates. 
However, \lcms are prone towards both under- and overestimation.
Overall, this wider range of over- and underestimation of \lcms, in fact, increases the likelihood of picking a non-optimal plan.

\subsection{Full Results on Join Order}\label{sec:join_order_full}
To report more representative results, we aggregate the previously discussed metrics over the full JOB-Light benchmark. 
The results can be seen in \Cref{fig:join_order_act_cards}.
Most interestingly, when looking at the total runtime of selected plans, we see that \postgresx still provides the shortest total selected runtime of $r=518s$, followed by \zeroshot with $r=530s$.
The optimal runtime for the workload is at $r$=446s, which assumes a perfect plan selection for every query. 
The worst model again is \queryformer, where the selected runtime for all queries is $r=830s$.
When looking at surpassed plans, \postgresx and \postgresxvi also outperform their \lcm competitors.
Regarding the ranking of plans, as shown by ranking correlation $\rho$, \zeroshot as \lcm actually returns the best results, outperforming traditional approaches.
This is surprising, as \zeroshot still does not lead to plans that outperform them in total runtime.
However, when looking at under- and overestimation, we can see that in line with the anecdotal result, \zeroshot tends to both under- and overestimate at the same time.
This leads to worse plan selections.
Interestingly, the best three performing \lcms are all \textit{database-agnostic} and \flatvector shows a good (often third place) performance, although it uses a simplistic model structure.

\begin{figure*}
    \centering
\includegraphics[width=\linewidth]{figures/join_order_full.pdf}
    \caption{Full Join Ordering Results on JOB-Light benchmark. 
    Traditional models often still outperform \lcms in terms of join ordering across all metrics.
    Using actual cardinalities helps to improve the results of \lcms significantly.}
    \label{fig:join_order_act_cards}
\end{figure*}
%----------------------------------------------------------

\subsection{Impact of Improved Cardinalities}
The optimal order of joins is impacted significantly by intermediate cardinalities, which in turn influence query costs. 
So far, many of the proposed \lcms use \textit{estimated cardinalities} as input derived from PostgreSQL to provide cost estimates for a plan. 
However, the PostgreSQL cardinality estimator frequently produces inaccuracies, sometimes deviating by several orders of magnitude. 
It was shown that better cardinality estimates substantially improve the cost estimation results \cite{leis_how_2015}. 
This raises the question of their impact on cost estimates of \lcms to make better optimizer decisions. 
To investigate this, we train and evaluate all \lcms, that utilize cardinalities as input features (i.e., \flatvector, \qppnet, \zeroshot, \dace, cf. \Cref{tab:taxonomy}), with actual, observed cardinalities rather than estimated ones.
This aims to isolate the effects of cost estimation from that of cardinality estimation.
We repeat the same evaluation from the previous experiment from \Cref{sec:join_order_full} with actual cardinalities instead and report the results in \Cref{fig:join_order_act_cards}, where the new model variants are represented by the lighter-colored bars.
It can be seen that, indeed, perfect cardinalities have a positive impact on the overall results.
For instance, the total runtime of \qppnet decreases from $r$=765s to $r$=644s.
Still, the best \lcm variant, which is \flatvector here, is outperformed by \postgresx, which still relies on estimated cardinalities.
However, as a promising result, we would like to highlight ranking correlation and the underestimation, where adding perfect cardinalities significantly improves all \lcm variants.

\subsection{Accurate Cardinalities for PostgreSQL}\label{subsec:optimizer_comparison}
Finally, we analyze how optimal the selected plans of \lcms are in relation to the total runtime when providing accurate cardinality estimates for classical cost models. 
We compare \postgresx, \postgresxvi and the best performing \lcm, from the previous experiment (i.e., \zeroshot) and report the relative slow-down of runtime that they achieve compared to the optimal workload runtime for the JOB-Light benchmark.
The results are shown in \Cref{fig:optimizer_runtime} where we see that overall, the execution is 18.9\% slower than the optimum when using \zeroshot.
In contrast, \postgresx achieves a slow-down of $16.23\%$ and \postgresxvi $24.9\%$. 
Importantly, when providing actual cardinalities to the PostgreSQL models, their performance improves drastically towards near-optimal plan selections (1.0\% and 0.8\% slow-down).
In contrast, \zeroshot only gets slightly better, indicating that it cannot yet make full use of information provided by cardinality estimates; i.e. \zeroshot still over- and underestimates costs for individual plans, leading to non-optimal plan selections.

\subsection{Summary \& Takeaways}
The analysis of join ordering indicates that the traditional model, \postgres, continues to outperform \lcms in selecting plans with low runtimes. 
In fact, only in terms of the ability of \textit{ranking} of plans, the traditional models are outperformed by \zeroshot, which demonstrates that \lcms have the ability to improve query optimization.
However, traditional models are not without flaws, as they also still fail to identify faster plans in some cases.
For this task, the three best performing \lcms are all DB-agnostic, and the simple model \flatvector performed relatively well.
Interestingly, \lcms tend to be more precise in predicting costs for query plans close to the optimal plan, while they have more significant prediction errors (over-and under-estimates) for less optimal plans with higher actual runtimes. 
One reason is that training data typically contains more optimal than sub-optimal plans, which comes from the biased training strategy that we discuss in \Cref{sec:lessons}.
As such, in the future, we suggest that \lcms should focus on mitigating over-and under-estimates for such plans by systematically including signals from these plans in the training of \lcms.
Another direction is to extend \lcms to predict not only runtime but also their confidence.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/optimizer_runtimes.pdf}
    \caption{Relative runtime slow-down vs. the optimal execution. PostgreSQL models are closer to the optimum than \lcms, especially when using perfect cardinalities}
    \label{fig:optimizer_runtime}
\end{figure}