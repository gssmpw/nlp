\section{Evaluation Methodology} \label{sec:methodology}
In this section, we discuss the selection of \lcms for this study and then explain our evaluation strategy.
Afterwards, we discuss which downstream tasks we include in our study and why we select those tasks.
Finally, we explain the experimental setup. 
%--------------------------------------------------
\begin{table*}
\resizebox{\linewidth}{!}{
\begin{tabular}{lr|cccccc|c|c|c}
\multicolumn{2}{c|}{} & \multicolumn{6}{c|}{\textbf{Input Features}} &  &  &  \\
\multicolumn{2}{c|}{\textbf{Model}} & \begin{tabular}[c]{@{}c@{}}SQL\\ String\end{tabular} & \begin{tabular}[c]{@{}c@{}}Physical\\ Plan\end{tabular} & Cardinalities & \begin{tabular}[c]{@{}c@{}}DB Cost \\ Estimates\end{tabular} & \begin{tabular}[c]{@{}c@{}}DB\\ Statistics\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sample \\ Bitmaps\end{tabular} & \textbf{\begin{tabular}[c]{@{}c@{}}Query \\ Representation\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Database-\\ Dependency\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Model\\ Architecture\end{tabular}} \\ \hline
\cellcolor[HTML]{8c613c}\textcolor{white}{\textbf{\flatvector}} & \cellcolor[HTML]{8c613c}\textcolor{white}{\textbf{\cite{ganapathi2009}}} &  & \checkmark & \checkmark &  &  &  & Flat & DB-agnostic & Regression Tree \\ \hline
\cellcolor[HTML]{956cb4}\textcolor{white}{\textbf{\mscn}} & \cellcolor[HTML]{956cb4}\textcolor{white}{\textbf{\cite{kipf2019}}} & \checkmark &  &  &  &  & \checkmark & Flat & DB-specific & Deep Sets \\ \hline
\cellcolor[HTML]{d65f5f}\textcolor{white}{\textbf{\etoe}} & \cellcolor[HTML]{d65f5f}\textcolor{white}{\textbf{\cite{sun2019}}} &  & \checkmark &  &  & \checkmark & \checkmark & Graph & DB-specific & Tree Structured NN \\ \hline
\cellcolor[HTML]{6acc64}\textcolor{white}{\textbf{\qppnet}} & \cellcolor[HTML]{6acc64}\textcolor{white}{\textbf{\cite{marcus2019}}} &  & \checkmark & \checkmark & \checkmark & & \checkmark & Graph & DB-specific & Neural Units \\ \hline
\cellcolor[HTML]{4878d0}\textcolor{white}{\textbf{\queryformer}} & \cellcolor[HTML]{4878d0}\textcolor{white}{\textbf{\cite{zhao2022}}} &  & \checkmark &  &  & \checkmark & \checkmark & Graph & DB-specific & Transformer \\ \hline
\cellcolor[HTML]{dc7ec0}\textcolor{white}{\textbf{\zeroshot}} & \cellcolor[HTML]{dc7ec0}\textcolor{white}{\textbf{\cite{hilprecht2022}}} &  & \checkmark & \checkmark &  & \checkmark &  & Graph & DB-agnostic & Graph Neural Networks \\ \hline
\cellcolor[HTML]{ee864a}\textcolor{white}{\textbf{\dace}} & \cellcolor[HTML]{ee864a}\textcolor{white}{\textbf{\cite{zibo_liang_dace_2024}}} &  & \checkmark & \checkmark & \checkmark &  &  & Graph & DB-agnostic & Transformer \\
\end{tabular}
}
\caption{Selection and main dimensions of \lcms for our study}
\label{tab:taxonomy}
\end{table*}

\subsection{Selection of \lcms}
Existing \lcms differ in various dimensions, which might affect query optimization.
For this paper, we thus select a representative, broad set of recent state-of-the-art \lcms covering various approaches as shown in \Cref{tab:taxonomy}.
Moreover, we focus on models where artifacts were available to make the results of this study reproducible. 
Below, we discuss the model selection briefly.

\begin{enumerate}[leftmargin=*, nosep]
    \item \flatvector \cite{ganapathi2009}:
    This is one of the earliest approaches published more than 15 years ago and serves as a simple baseline for more recent and complex approaches.
    This paper aims to represent the physical query plan as a fixed-size vector with one entry per operator type (e.g., hash join, nested loop join, sort-based aggregate, hash aggregate). 
    Each entry then contains the sum of the intermediate cardinalities per operator type.
    To predict the runtime of a plan, a state-of-the-art regression model LightGBM \cite{lightgbm} is trained, which uses such a flat vector as input.

    \item \mscn \cite{kipf2019}:
    As a second model, we choose \mscn as one of the first models of the more recent generation.
    While initially developed for cardinality prediction, the model has also been used for cost estimation \cite{hilprecht2022, yang2023, sun2019}. 
    We include this model as the only one that uses the SQL-String as input instead of the physical query plan.
    To represent the SQL query, \mscn uses one-hot encoded flat feature vectors to describe tables, join conditions, and predicates used in a query.
    Moreover, to learn from the data distribution, \mscn uses \textit{sample bitmaps} as model input, which indicates for the filter conditions of a given query which rows of the base tables qualify for selection.

    \item \etoe \cite{sun2019}:
    As the third approach, we select \etoe, as this was the first proposed \lcm which explicitly models the plan structure. 
    As a model architecture, a tree-structured neural network is used for which it combines different \ac{mlp} for encoding input features and aggregating them over the query graph. 
    %A final estimation layer is used for the cost prediction.

    \item \qppnet \cite{marcus2019}:
    This approach is also aware of the plan structure but uses a more modular approach of so-called \textit{neural units}, which are \ac{mlp}, trained per operator type (e.g., one for hash joins, one for nested loop joins).
    Each neural unit considers a set of operator-related features and predicts a per-operator runtime and hidden states, passed along the query graph as input signal for the following \ac{mlp}. 
    In addition, it learns from estimated cardinalities and estimated operator costs.

\item \queryformer \cite{zhao2022}:
    Different from \qppnet and \etoe, which are based on simple \ac{mlp}, this model employs a learning architecture using \textit{transformers} to estimate query costs. 
    Most importantly, it introduces a tree-based self-attention mechanism, which is designed to learn from long query plans with many operators.
    Furthermore, it also incorporates bitmap samples and a richer feature set compared to other \lcms, including histograms on base tables.
    
\item \zeroshot\cite{hilprecht2022}:
    This is the first \emph{database-agnostic} model proposed that can generalize across databases.
    To achieve this, it learns from so-called transferable features such as table size and does not encode database-specific features (e.g., attribute \& table names) as the models did before. 
    As a model architecture, variations of \textit{graph neural networks} are used. 

\item \dace \cite{zibo_liang_dace_2024}:
    Finally, \dace represents one of the most recent models, which combines transformers with a database-agnostic approach. 
    As it is based on transformers, it uses self-attention \cite{vaswani2017} and \textit{tree-structured attention} mechanisms.
    In contrast to other models, it uses a reduced feature set, mainly learning from the operator tree and the cost estimates provided by a traditional cost model (i.e., PostgreSQL cost). 
\end{enumerate}

\subsection{Tasks of Query Optimization}
In this study, we evaluate the selected \lcms against \textit{three} query optimization tasks that each rely on precise cost estimates but stress different abilities of cost models for plan selection.

\noindent\textbf{Join Ordering.}
The join order describes the sequence in which base tables are joined in a query plan.
It is decisive for the query runtime as joins are the most expensive operations in a query plan, and a sub-optimal join order can significantly increase the runtime as the intermediate cardinalities explode.
To solve this task during query optimization, \lcms are combined with plan enumeration techniques such as dynamic programming to select the plan with the estimated lowest cost for execution.

\noindent\textbf{Access Path Selection.}
In addition to the join order, the correct choice of access path (index vs. table scan) is another decisive factor for the runtime of the final query plan \cite{selinger1979}. 
Using indexes like B-trees for accessing data in queries can massively accelerate access when selected correctly by a cost model.
However, the cost of index-based access vs. scan-based access depends on several factors, such as selectivity or data distribution.
\lcms must fundamentally understand these to provide reliable cost estimates for access paths.

\noindent\textbf{Physical Operator Selection.}
Another critical task for query optimization is to select a physical implementation of query operators.
For instance, most database systems support various join implementations like hash join, sort-merge join, or nested-loop join.
Again, the optimal selection of physical operators depends on several factors, such as intermediate sizes or the sortedness of data, which are not all explicitly included in many \lcms.
 
%-----------------------------------------------------
\subsection{Experimental Setup} \label{subsec:setup}
In the following, we explain the experimental setup of this paper that is common for all these tasks.

\noindent\textbf{Training and Evaluation Data.}
To train and test the selected \lcms, we leverage the benchmark proposed by \cite{hilprecht2022} that contains 20 real-world databases and a workload generator that provides SPJA-queries supported by all presented \lcms.
The generator reflects the state-of-the-art workload generation used by recent \lcms to create training queries.
We generate and execute 10,000 queries per database to learn from a broad range of query patterns and set the timeout to 30s, which is sufficient to show the effects of selecting good plans.
As we will demonstrate, this runtime can show already significant trade-offs for query optimization.
Moreover, this is a realistic scenario for cloud providers \cite{renen2023, renen2024}.
We use the same dataset and workloads for all cost models. 
For the execution of queries, we use PostgreSQL v10.23 on bare-metal instances of type \texttt{c8220} on CloudLab as an academic resource \cite{duplyakin2019} to generate both our training and evaluation data.
Each query is executed three times, and the runtime is averaged for a stable ground truth.
For some experiments in our study, we enforce the plan selection with \texttt{pg\_hint\_plan}.
We make all models, evaluation results and training data publicly available\textsuperscript{\ref{data_link}}.
%-------------------------------------------------

\noindent\textbf{Model Training.}
For the model training, we need to differentiate between database-specific and database-agnostic \lcms:

\textit{Database-specific} models are trained for a specific database (i.e., set of tables).
For these models, we ran 10,000 queries on a single database and divided them into a training, evaluation, and test set (80\%, 10\%, 10\%).
To ensure that database-specific models are sufficiently trained, we provided all of them with 10.000 training queries and validated that adding more training data does not improve the quality of their predictions.

\textit{Database-agnostic} models are trained across various databases. 
Thus, they typically require more training data but then generalize out-of-the-box to unseen datasets.
We trained all database-agnostic \lcms on 19 training databases, each with 5,000 queries, and evaluated on an unseen target database, according to the strategies proposed in \cite{hilprecht2022, zibo_liang_dace_2024}.
Training with more training data also did not yield significant improvements in terms of accuracy.
Finally, each \lcm is trained three times with different seeds on the weights initialization and the train/test-split.
We average the model predictions across the seeds for all evaluation results reported.
%-----------------------------------------------

\textbf{Traditional Baselines.}
As a traditional model, we use PostgreSQL's cost model in this study\footnote{While other even more sophisticated traditional models in commercial DBMS such as Microsoft SQL Server exist, we already see as a result of this study that \lcms cannot (yet) improve over PostgreSQL}.
Here, we included two different versions (10.23 and 16.4) to allow a broader comparison and also analyze how traditional cost models evolved over time
In PostgreSQL, the cost of an operator is determined by a weighted sum of the number of disk pages accessed and the amount of data processed in memory.
Note that the cost estimates from PostgreSQL do not represent actual execution times.
Still, they are designed to represent the execution cost and can thus be used for all query optimization tasks.
As such, to make the predictions comparable with the \lcms, we scale the logical costs to the actual runtime with a linear regression model and refer to this approach as \postgresx and \postgresxvi, which is similarly used in other papers \cite{yang2023, zibo_liang_dace_2024, hilprecht2022, zhao2022}.

\textbf{\lcm Implementations.}
For all \lcms, we relied on published source code, which we refer to in our repository\textsuperscript{\ref{code_link}}. 
However, we needed to re-implement some details, such as their ability to work with the same training datasets. 
For instance, \queryformer was hard-coded to work with IMDB only, as it assumed a fixed set of tables and filters. 
We addressed this by first standardizing the various inputs of \lcms (cf. \Cref{tab:taxonomy}), like physical plans, database statistics and sample bitmaps. 
In addition, we maintain feature statistics for each dataset that help to normalize the inputs and to create one-hot encodings (e.g., to encode table or column names) that are required for some \lcms.
We further unified the \lcm training and evaluation pipelines to collect consistent metrics for all models.
However, it is important that all our changes did not change the internal behavior of these models.
%-------------------------------------------
\subsection{The Need for New Metrics}
Most works focus on the prediction accuracy over an unseen test dataset by typically reporting the median Q-Error to evaluate how well \lcms predict the execution costs. Often, the median as well as percentiles are reported.
It is defined as follows:
\begin{definition}
\textit{Q-Error ($Q_{50}$)}: The Q-Error is defined as the relative, maximal ratio of an observed label $y$ and its prediction $\hat{y}$ with $Q = \max \left( \hat{y}/{y}, y/\hat{y} \right)$ where $Q=1$ indicates a perfect prediction. 
\end{definition}

However, we argue that this strategy is \textit{not sufficient} to evaluate the applicability of \lcms in query optimization due to two reasons:

\begin{enumerate}[leftmargin=*, nosep]
\item \textbf{Focus on Single Plan Candidates.}
The traditional strategy typically only evaluates \textit{one plan per query} in a workload.
However, the typical task in query optimization is to select one plan out of multiple candidates.
As such, for a study that aims to understand the quality of plan choices, an evaluation methodology needs to enumerate multiple plans for the same query and report metrics that show us how well \lcms can pick the best plan (see next). 

\item \textbf{Focus on Accuracy as only Metric.}
Traditional strategies focus mainly on the overall prediction accuracy of runtimes.
However, the ability of the model to \textit{select the right plan} and to \textit{rank the plan candidates} is much more critical.
Thus, we introduce novel metrics for the corresponding tasks to evaluate the ranking and selection properties of \lcms in the later sections. 
\end{enumerate}
