\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% \usepackage{titlesec}
% \renewcommand{\thesection}{\arabic{section}}
% \renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
% \renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

% % Ensuring that subsections are numbered with Arabic numerals
% \titleformat{\section}[block]{\bfseries}{\thesection}{1em}{}
% \titleformat{\subsection}[block]{\bfseries}{\thesubsection}{1em}{}
% \titleformat{\subsubsection}[block]{\bfseries}{\thesubsubsection}{1em}{}




% Adding space after section titles
% \titlespacing*{\section}{0pt}{*2}{*1}
% \titlespacing*{\subsection}{0pt}{*2}{*1}
% \titlespacing*{\subsubsection}{0pt}{*2}{*1}
% \renewcommand\citepunct{, } %FIXES THIS [1],[2] -> [1,2]

% \usepackage{caption}
\pdfminorversion=4
\usepackage{subcaption}

\usepackage{scalerel}
\usepackage{multirow} 
\usepackage{booktabs}
\usepackage[table]{xcolor}
% \pagestyle{plain}
% \usepackage{cite}
\usepackage[sort&compress, numbers]{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}

\usepackage{float}\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{flushend}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{tcolorbox}
\usepackage[inline]{enumitem}
%for code syntax highlight
\usepackage{tabularx} % To use the tabularx environment

\usepackage{xcolor}  % Required for custom colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}




\usepackage{listings}
\lstdefinestyle{mystyle}{
  % language=Java,
  aboveskip=3mm,
%   belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  numbers=none,
  backgroundcolor=\color{backcolour},
  commentstyle=\color{codegreen},
 keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\scriptsize\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=false,                 
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    escapeinside=``
}
\lstset{style=mystyle}


% \captionsetup{
%   font=normalsize,
%   % labelfont=bf,
%   % textfont=bf
% }
\usepackage{natbib}

\usepackage{hyperref}


\newcommand{\cdiff}{$C_{\textsc{diff}}$}
\newcommand{\rnl}{$R_{\textsc{nl}}$}

% \newcommand{\tvalid}{${\texttt{valid}}\ $}
% \newcommand{\tnoisy}{${\texttt{noisy}}\ $}
\newcommand{\tvalid}{\texttt{valid}}
\newcommand{\tnoisy}{\texttt{noisy}}

\newcommand{\cleangpt}{$\textsc{Cleaned}_{\textsc{GPT-3.5}}$}
\newcommand{\cleanllama}{$\textsc{Cleaned}_{\textsc{Llama3}}$}

\newcommand{\controlgpt}{$\textsc{Controlled}_{\textsc{GPT-3.5}}$}
\newcommand{\controlllama}{$\textsc{Controlled}_{\textsc{Llama3}}$}
        
% \newcommand{\pbasic}{$\text{P}_{\textsc{Basic}}\ $}
% \newcommand{\paux}{$\text{P}_{\textsc{Guideline}}\ $}
\newcommand{\pdef}{$\text{P}_{\textsc{Definition}}$}
\newcommand{\paux}{$\text{P}_{\textsc{Auxiliary}}$}

% \newcommand{\ch}[1]{{\color{red}{\bf{C:}}\emph{#1}}}

% \newcommand{\red}[1]{{\color{red}#1}}

%space after section is large 
% \usepackage{titlesec}
% \titlespacing*{\section}{left}{before}{after}
% \titlespacing*{\section}{0pt}{1ex}{1ex}
% \titlespacing*{\subsection}{0pt}{1ex}{1ex}
% \titlespacing*{\subsubsection}{0pt}{1ex}{1ex}


\begin{document}

% \title{Too Noisy To Learn: Towards Automated Noise Removal for Code Review Comment Generation}

\title{Too Noisy To Learn: Enhancing Data Quality for Code Review Comment Generation}
% \\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

 \author{\IEEEauthorblockN{Chunhua Liu}
 \IEEEauthorblockA{
 \textit{The University of Melbourne}\\
 chunhua.liu1@unimelb.edu.au}
  \and
 \IEEEauthorblockN{Hong Yi Lin}
 \IEEEauthorblockA{
 \textit{The University of Melbourne}\\
 holin2@student.unimelb.edu.au}
 \and
 \IEEEauthorblockN{Patanamon Thongtanunam}
 \IEEEauthorblockA{
 \textit{The University of Melbourne}\\
 patanamon.t@unimelb.edu.au}
 }
 


\maketitle

\begin{abstract}
% Modern code review has been widely used in collaborative environment in both industry and research to ensure the quality of software. In the cycle of code review, reviewers assess the code changes made by developers and provide review comments in natural language for developers to improve the code. Therefore, these comments serve as bridge between reviewers and developers to communicate and contribute the the software development cycle.
% Most recent work on automate code review focus on training neural models on large-scale open-source code review datasets to generate comments. 
% However, the datasets quality often vary, containing various comments that is . 
% Little work has been focusing on evaluating the quality of code review datasets and study the the impact of nosiness on comment generation models. In this work, we delve deep into the quality of comments and identified that 36\% of the comments in CodeReviewer comment generation dataset is noisy and we propose to identify the noisy comments by prompting large language models (LLMs) and we found that LLM can remove 10-20\% of noisy comments. Furthermore, we clean the dataset and re-train them on stat-of-the-art code review models and found that the the models can generate more accurate comments when compared to ground truth comments provided human reviewers, and on xx types of comments. Our research highlights the importance of high-quality datasets on code review comment generation, we encourage future research to . 
% Modern code review, essential for software quality in both industry and academia, relies on reviewers' natural language comments on code changes to facilitate developer communication. 
%%%%%%%%%%%#####Old one
% Code review is an important practice in software development, yet time-consuming and requires substantial effort. Recent advancements have leveraged large-scale datasets mined from open-source projects to train neural models to automate code review tasks, including review comment generation. Despite the availability of large-scale datasets, they may be susceptible to noise (e.g., vague review comments) in the training data, which may lead the neural models to generate low-quality review comments. In this paper, we investigate the impact of such noise on the performance of review comment generation. We employ large language models (LLMs) to classify valid and noisy comments, achieving 66-85\% precision in identifying valid comments. When using our cleaned dataset to fine-tune the state-of-the-art code review models, they generate review comments that are 7.5-13\% more similar to the human-written review comments compared to the models trained on the original dataset. We also find that the cleaned models can generate more informative and relevant comments than the original model. Our findings underscore the critical impact of dataset quality on the performance of review comment generation. We advocate for further research into cleaning training data to enhance the practical utility and quality of automated code review.

%###########
Code review is an important practice in software development, yet it is time-consuming and requires substantial effort. While open-source datasets have been used to train neural models for automating code review tasks, including review comment generation, these datasets contain a significant amount of noisy comments (e.g., vague or non-actionable feedback) that persist despite cleaning methods using heuristics and machine learning approaches. Such remaining noise may lead models to generate low-quality review comments, yet removing them requires a complex semantic understanding of both code changes and natural language comments. In this paper, we investigate the impact of such noise on review comment generation and propose a novel approach using large language models (LLMs) to further clean these datasets. Based on an empirical study on a large-scale code review dataset, our LLM-based approach achieves 66-85\% precision in detecting valid comments. Using the predicted valid comments to fine-tune the state-of-the-art code review models (cleaned models) can generate review comments that are 13.0\% - 12.4\% more similar to valid human-written comments than the original models. We also find that the cleaned models can generate more informative and relevant comments than the original models. Our findings underscore the critical impact of dataset quality on the performance of review comment generation. We advocate for further research into cleaning training data to enhance the practical utility and quality of automated code review.

\end{abstract}

\begin{IEEEkeywords}
Automated Code Review, Review Comment Generation, Dataset Quality
\end{IEEEkeywords}

\section{Introduction}

% The Significance and Challenges of Code Review
Code review is a critical practice in software development, providing multiple benefits such as identifying logic errors~\cite{Bacchelli-etal-2013-Expectations,Rigby-etal-2013-Convergent}. At the core of this process is the reviewers' comments on code changes, which serve as the primary means for providing feedback and suggestions. 
Despite its advantages, the process is often time-consuming and requires substantial effort from reviewers~\cite{Bosu-etal-2017-Process}. To alleviate this burden, recent research has focused on automatically generating review comments by training neural models on large-scale datasets mined from open-source platforms such as GitHub and Gerrit. 
While these models show promise in generating code review comments by training neural models on large-scale datasets~\cite{Tufano2022PretrainedModels,Li2022CodeReviewer,CCT5}, the gap between model-generated and human-written comments remains significant, limiting their practical use. 
% \textcolor{red}{please revise}For example, Tufano et al.~\cite{Tufano2022PretrainedModels} collected 382K Java instances from GitHub and Gerrit and Li et al.~\cite{Li2022CodeReviewer} mined 1,161 projects with 138K instances from GitHub with nine programming languages. 
% A wide range of neural models have been trained and evaluated using these datasets \cite{ Li2022CodeReviewer,LLaMA-Reviewer,CCT5}, 


\input{tables/example_reviewer_comments_valid_noisy}

% The Problem of Dataset Quality
Despite the availability of large-scale datasets, the quality of review comments varies due to various factors such as reviewer experience, limited review time~\cite{Oleksii-etal-2016-CodeReviewQuality, Lin-etal-2024-Improving}, diverse communicative intentions~\cite{ebert2018communicative}, and different standards for code review across different projects. Prior work has attempted to clean these datasets by removing bots and filtering out noisy comments using heuristics~\cite{tufano2021towards, Tufano2022PretrainedModels}. More recently, Li et. al. have attempted to clean their widely-used CodeReviewer benchmark using heuristic rules and a machine learning classifier (SVM)~\cite{Li2022CodeReviewer}. 
Yet concerns about data quality persist. A recent work by Tufano et al.~\cite{tufano2024code} revealed that a substantial proportion (32\%) of comments in the test set of this widely-used dataset still contains noise. The noise includes vague, difficult-to-understand comments, and comments that merely seek clarification rather than suggest an improvement to the code (see Fig. \ref{fig:code_reviews_stacked} Top).
% The persistence of such noise, even after applying multiple cleaning techniques, highlights the limitations of existing approaches and the need for more sophisticated methods that can better understand the semantic relationship between code changes and review comments.
% This persistence of noise highlights the need for more sophisticated methods capable of semantic understanding between code changes and review comments

% Recent work by Tufano et al.~\cite{tufano2024code} has observed that a substantial proportion (32\%) of comments remains noisy in the test set of this widely-used dataset during their manual inspection. 
 


We argue that such noisy comments may also exist in the training set. Neural models trained on noisy datasets inevitably internalize and potentially propagate low-quality reviews into the generated review comments. Such poor reviews can be considered as less useful in practical settings~\cite{bosu2015characteristics,Oleksii-etal-2016-CodeReviewQuality, turzo2024makes} or even negatively impact software quality~\cite{Mcintosh-etal-2016-ImpactCodeReiew}. To develop models that generate high-quality review comments—providing clear suggestions for code improvements that assist code authors and automated code refinement—it is crucial to improve the quality of the training data.

% To facilitate code improvement by code authors or automated code refinement, the generated review comments should provide clear suggestions for improving code changes.

% This concern aligns with findings by McIntosh et al.~\cite{Mcintosh-etal-2016-ImpactCodeReiew}, who suggest that poorly-reviewed code negatively impacts software quality. 
% Furthermore, existing models often rely on human review comments as ground truth, but noisy comments can distort the evaluation, failing to reflect the actual effectiveness of the models.


% Challenges in Noise Removal
Identifying and removing noisy comments remaining in the code review datasets presents significant challenges due to the complexity of the task. It requires an understanding of both the technical context of code changes and the review comments written in natural language. Therefore, interpreting code review comments can be highly ambiguous and inference-heavy~\cite{ebert2018communicative,Ebert-etal-2021-ConfusionCodeReview}. Simple heuristics such as keywords matching and sentence length used in prior works~\cite{tufano2021towards,Tufano2022PretrainedModels} fall short in analyzing these complexities and nuances in the review comments, urging the need for a more semantic and context-aware method to effectively clean code review datasets. 


% Advancements in Large Language Models (LLMs) 
To address these challenges, this work explores a novel approach to improve the quality of the review datasets and studies the impact of noise on the performance of automated review comment generation.  
As large language models (LLMs) have shown promising performance in understanding both code and natural language \cite{chen2021evaluating,hendrycks2020measuring}, and potential data annotation tasks in other domains \cite{Gilardi-etal-2023-ChatGPTAnnotation,ding-etal-2023-gptAnnotator}, we investigate the feasibility of using LLMs to classify noisy and valid comments, then retain the valid ones. Finally, we evaluate the performance of the state-of-the-art models (i.e., CodeReviewer~\cite{Li2022CodeReviewer} and CodeT5~\cite{wang-etal-2021-codet5}) trained on the cleaned datasets. 


% Leveraging the power of LLMs to clean noisy data with minimal annotations presents an opportunity to improve dataset quality in code review.
% Motivation and Study Overview
% Given the critical importance of data quality, this study aims to systematically understand LLMs' capability of detecting noisy comments, evaluate the the impact of noise on code review models, and assess the quality of generated comments post-noise removal. 

Through an empirical study on the widely-used code review benchmark (CodeReviewer~\cite{Li2022CodeReviewer}), we find that
only 64\% of the sampled comments in the training set of CodeReviewer benchmark are valid.  Our approach using LLMs achieved a precision of 66\% - 85\% in identifying valid comments and achieved a recall of 51\% - 89\% in identifying noisy comments.
These results suggest that 
% LLMs can identify up to 89\% of the noisy comments and 
the proportion of valid comments in the dataset can be improved from 64\% to 85\% by LLMs. By retaining only the valid comments predicted by LLMs (i.e., cleaned datasets), the training size is 25\% - 66\% smaller than the original dataset.
Nonetheless, the smaller data did not negatively impact the performance of comment generation models.   
% When using the valid comments predicted by LLMs (i.e., cleaned data) to fine-tune the models,  
% using LLMs to filter out noisy comments and re-train comment generation models, 
Instead, the models fine-tuned on the cleaned datasets achieved BLEU-4 scores 7.5\% - 13\% higher than those trained on the original dataset, with a 12.4\% - 13.0\% increase specifically on valid comments in test sets.
% When using the review dataset cleaned by LLMs to train the code review models, the generated comments achieved BLEU-4 higher than the ones generated by the original models. 
% Specifically, we observe an 7.5\% - 13\% improvement in BLEU-4 scores for actual valid comments. 
Moreover, we also found that the quality of comments generated from the cleaned models is significantly improved, with up to a 24\% increase in information score and an 11\% increase in relevance score. These results highlight that valid and noisy review comments can be detected by LLMs and cleaning review data can improve the performance of the review comment generation models.

\textbf{Novelty and Contribution.} To the best of our knowledge, we are the first to
\begin{enumerate*} 
    \item present an automated approach to clean the large-scale review dataset using LLMs,
    \item demonstrate the capability of LLMs to automatically classify valid and noisy review comments, and
    % We identified that 36\% of comments in the CodeReviewer dataset are noisy and demonstrated that 11.5\% to 21.4\% of this noise can be effectively removed using LLMs. 
    \item highlight the impact of data quality on the performance of automated review comment generation, \item demonstrate an improvement in model performance and comment quality with a cleaned dataset despite its significantly smaller size than the original data (e.g., 117K vs 39K), \item employ a semi-automated method to approximate the quality of generated review comments at scale.
    % We empirically show that removing noisy comments improves the performance of code review comment generation models, resulting in up to a 13\% improvement in BLEU-4 scores.
    % \item We demonstrate that the quality of comments generated from models trained on cleaned datasets is significantly improved, with up to a 24\% increase in information score and an 11\% increase in relevance score.
\end{enumerate*}


\textbf{Open Science.}
To facilitate reproducibility and future work, we provide a replication package that includes the cleaned datasets, experimental results, and scripts.\footnote{\url{https://zenodo.org/records/13150598}}

\textbf{Paper Organisation.}
The remainder of this paper is structured as follows: 
Section~\ref{sec:related_work} discussed related work. 
Section~\ref{sec:study_design} outlines an overview of the study design. 
Sections~\ref{sec:identifying_noisy_comments_rq1} - \ref{sec:noisy_impact_on_generated_comment_quality_rq3} present our study approaches and
results of each research question. 
% presents our approach to identifying noisy comments in code review datasets. Section~\ref{sec:noise_impact_on_comment_generation_models_rq2} explores the impact of noisy comments on comment generation models, while Section~\ref{sec:noisy_impact_on_generated_comment_quality_rq3} evaluates the quality of comments generated by models. 
% We discuss our findings and their implications in 
Section~\ref{sec:discussion} discusses the findings. Section~\ref{sec:threats_to_validity} addresses potential threats to the validity. Finally, Section~\ref{sec:conclusion} draws a conclusion.

\section{Background \& Related Work}
\label{sec:related_work}
% structure of related work
% P1: Code Review Automation Literature. Ending with why generating review comments is important. \\  
% P2: Review comment generation . Key Message: all models are trianed on code review datasets  mined from open-source platforms \\ 
% P3: Review comment quality. Key Message:  the quality review comments varies, not all comments are suitble for machine learning \\ 
% P4: Existing approaches for Dataset Quality.  Key message: some people trying to remove the noisy in different context, but cannot be applied to the code review context; no one has done this before. 

\textbf{Automated Code Reviews.}
To help alleviate the cognitive burden of the modern code review process~\cite{compositechanges,workingmemory}, automated code review research introduced three sequential tasks in 1) code change quality estimation~\cite{Li2022CodeReviewer}, 2) review comment generation~\cite{Tufano2022PretrainedModels} and 3) code refinement~\cite{tufano2019learning,thongtanunam2022autotransform,pornprasit2023d}, which mirrors the human process of 1) assessing if a code change is problematic, 2) providing a review comment in natural language that details issues and relevant fixes and 3) addressing the review by revising the code. 
Recent study have leveraged large-scale datasets from open-source platforms to train models to perform these tasks~\cite{tufano2019learning,tufano2021towards,Li2022CodeReviewer}.   
Despite advancements achieved, automating the code review process remains challenging, particularly in the review comment generation task~\cite{tufano2024code}. 

% \textcolor{red}{Aside from the wide variety in content, these code review comments are also conveyed in natural language with diverse expressions, adding an additional layer of complexity.}

\textbf{Review Comment Generation.}
Review comment generation is formulated as a sequence-to-sequence generation task, where models generate natural language review comments (\rnl) based on code changes (\cdiff). Initially, Tufano et al.~\cite{Tufano2022PretrainedModels} trained a T5 transformer~\cite{raffel2020} on Java (\cdiff, \rnl) pairs, demonstrating that review comments possess latent statistical properties that can be learned. Li et al.~\cite{Li2022CodeReviewer} introduced a large-scale dataset called CodeReviewer which includes nine programming languages for code review-specific pre-training and fine-tuning. Subsequent research using the CodeReviewer dataset explored various solutions, such as joint-training on multiple code review tasks~\cite{sghaier24} and prompting LLMs~\cite{tufano2024code,LLaMA-Reviewer}. Although these models show improvements to the state-of-the-art, their generated comments still fall short when compared to human reviews, raising concerns about their practicality. While these studies focus on developing new techniques, the quality of underlying datasets has been overlooked. This importance of data quality is indeed underscored by a case study at Google~\cite{google2024}, which highlights the potential to build practical code review tools using high-quality data. 


\textbf{Review Comment Quality.}
%Code review is a critical practice in software development, fostering collaboration and improving code quality. In real-world scenarios, code review is inherently conversational, often involving multiple rounds of dialogue between contributors and reviewers \cite{meyers2018dataset}. This conversational characteristic enriches the review process, allowing reviewers to clarify points and contributors to respond. Consequently, reviewer comments serve multiple communication intentions, from suggesting code changes to seeking further information or expressing attitudes \cite{ebert2018communicative}.
Review comments serve multiple communication intentions, from suggesting code changes to seeking further information or expressing attitudes~\cite{ebert2018communicative}.
Such multi-faceted nature of these comments may hinder automated code review. 
Recent studies~\cite{frommgen2024resolving, tufano2021towards} have highlighted that not all review comments are suitable for training machine learning and artificial intelligence models. This issue is compounded by the fact that datasets mined from open-source platforms often contain noise, such as unclear or non-actionable comments, even after processing with heuristic rules or removing bot-posted comments~\cite{tufano2024code}.
%The presence of noisy or irrelevant comments in training datasets could potential significantly impact the effectiveness of automated code review systems. 
%A positive correlation between comment quality and code refinement task performance on a small test set \cite{guo2023exploring}, demonstrating that models perform better with more concrete and explainable comments. 
%This underscores the critical need for high-quality comments in training data. 
Although the importance of data quality has been recognized, most studies do not focus on identifying and removing noisy review comments at scale, and instead build on them directly. 
% This noise in training set might hinder models' performance in training \cite{tufano2024code} and limit their practical usage. 

\textbf{Existing Approaches for Dataset Quality.} Existing approaches for cleaning code review datasets have primarily relied on manually crafted heuristic rules, which are often dataset-specific and limited in scope. Early work by \cite{gupta2018intelligent} used keywords to develop regular expressions  for C\# to remove non-actionable comments. Similarly, Tufano et al.~\cite{tufano2021towards, Tufano2022PretrainedModels} designed a set of heuristic rules for Java that focused on surface-level features like comment length and keywords (e.g., \textit{less than 5 words and contains `pr'} is considered as noisy). These approaches, however, lack semantic understanding and cross-language generalizability.

More recent efforts have attempted to combine rule-based approaches with supervised machine learning. In developing the CodeReviewer dataset, Li et al.~\cite{Li2022CodeReviewer} applied existing heuristic rules \cite{Tufano2022PretrainedModels} and supplemented them with a machine learning classifier (SVM) trained on manually labeled data.\footnote{Details are provided in their supplementary material (Section A.1 and A.2) at \url{https://arxiv.org/pdf/2203.09095v1}.} Despite these advances, a recent manual analysis ~\cite{tufano2024code} reveals that substantial noise persists in current benchmark datasets including CodeReviewer~\cite{Li2022CodeReviewer}. 
Particularly, the remaining noisy comments require semantic understanding of both code changes and review comments - a capability beyond existing approaches.
% the remaining noisy comments require a semantic understanding of review comments and their relationship to code changes.This highlights the urgent need for a semantic and context-aware approach that can effectively clean code review datasets across multiple programming languages. Our work directly addresses this need by exploring methods that are both generalizable and scalable for automatically cleaning code review data.

% Tufano's notebook: https://github.com/RosaliaTufano/code_review/blob/master/is_relevant.ipynb


% Data quality issues have plagued many automated software engineering research, such as code summarisation~\cite{shi2022we} and software vulnerability datasets~\cite{croft2023data}. However, their noise detection approaches are crafted specifically for their tasks. For example, the rule-based approach of~\cite{sun2022importance} is designed based on manual analysis for code search, but is limited in coverage and unable to capture semantics.

% For automated code review research, past work has either relied on (1)heuristic rules~\cite{gupta2018intelligent,tufano2021towards} or (2) classifiers trained on manually labeled samples~\cite{Li2022CodeReviewer}. However, these methods are seldom effective given that (1) identifying noisy comments require a deep understanding of both code and natural language semantics, and (2) manual labeling is prohibitively costly and limited in coverage. Indeed, these approaches have been employed to clean the code review dataset like CodeReviewer~\cite{Li2022CodeReviewer}. Yet, a recent study~\cite{tufano2024code} points out that a substantial proportion of noise still exists. This highlights the need for a more semantic and context-aware method to effectively clean code review datasets.


% message: automatic cleaning methods are non-existent 
% add bosu later if a better argument comes up 
% For automated code review research, prior work primary focus on  manually craft heuristic rules for each specific dataset to remove noisy comments ~\cite{gupta2018intelligent,tufano2019learning, tufano2021towards}. \cite{gupta2018intelligent} manually design a set regular expressions filters for C\# language based on their training data to remove comments that are non-actionable, require code context and longer reviews, yet, their rules are not public-available and might not be generalize to other datasets. \citep{tufano2019learning, tufano2021towards} manually designed a set of rules for Jave language, based on the comment lengths (less than 5 words -> noisy) and key words, which might not generalise to classifying the noise in semantic.\footnote{We ran the heuristic rules of \citep{tufano2021towards} but none of noisy comments are detected.}

% When creating the CodeReviewer dataset, \cite{Li2022CodeReviewer} applied these rules from \cite{tufano2019learning, tufano2021towards} to first remove noisy comments. In additional, they used classifiers trained on in-house manually labeled samples~\cite{Li2022CodeReviewer} to clean the dataset.  \footnote{See details provided in their supplementary material (Section A.1 and A.2 https://arxiv.org/pdf/2203.09095v1).} The effort in data cleaning reflect the level of dataset cleanness that can be cleaned using the state-of-the-art cleaning approaches. Yet, a recent study~\cite{tufano2024code} points out that a substantial proportion of noise still exists, which often often require understand the semantic meaning of review comments and/or it's relationship with the code change. This highlights that the need for a more semantic and context-aware method to effectively clean code review datasets. However, up to date, there is no approach that can be directly used to clean code review datasets in multiple languages and in semantic level, our study fill in this gap. 



% 
% The keywords by \cite{gupta2018intelligent} are not publicly available

% The 10-lines rule and other signals by \cite{bosu2015characteristics} requires the additional information in the code review process, e.g., comments triggering changes and response from code authors, which are unfortunately not available and traceable in the CodeReviewer dataset. In addition, to ensure our approach is easily adoptable for any datasets in the future, we focus on linguistic information in the comments, avoiding reliance on data available only after the review is complete.


% However, these methods are seldom effective given that (1) identifying noisy comments require a deep understanding of both code and natural language semantics, and (2) manual labeling is prohibitively costly and limited in coverage. Indeed, these approaches have been employed to clean the code review dataset like CodeReviewer~\cite{Li2022CodeReviewer}. Yet, a recent study~\cite{tufano2024code} points out that a substantial proportion of noise still exists. This highlights the need for a more semantic and context-aware method to effectively clean code review datasets.


% we used the majority class to reflect the remaining noisy ratio in the dataset.





\begin{figure*}
    \centering
\includegraphics[height=4.5cm, keepaspectratio]{figures/code_review_noisy_impact_framework.drawio_msr.pdf} %width=\textwidth,
    \caption{An overview of the pipeline of our study.}
    \label{fig:framework}
\end{figure*}

\section{Study Design}
\label{sec:study_design}

Our study aims to explore an automated approach that automatically distils human code review comments before using them to train automated comment generation models. 
By focusing on comments that directly contribute to code improvements, we can enhance the quality of training data and, consequently, improve the performance of automated code review comment generation models.

% \textcolor{red}{rephrase}
We consider valid and noisy comments using the following definitions, which align with definitions of the prior work~\cite{tufano2024code}.

\begin{itemize}
    \item \textbf{Valid Comments} refer to the review comments that should provide clear suggestions aimed at improving the source code. Given the submitted code change (i.e., code diff), the valid comment should explicitly express the issues, and clearly outline necessary actions to improve the code. The type of requested actions should also be clear, such as refactoring the code to improve code quality (regarding documentation, style, programming conventions and more), writing tests, aligning with good object-oriented design principles, fixing bugs, enhancing logging, or addressing other specific needs.
    
    \item \textbf{Noisy comments} refer to the review comments that do not request direct and applicable actions to refine the code, or the message expressed is unclear and difficult to understand. This includes comments that do not explicitly ask for specific changes, merely justifying the submitted code change, or are of low quality due to vagueness, ambiguity, or other factors that hinder understanding.
\end{itemize}
% \textcolor{blue}{
This definition is consistent with characteristics of useful comments perceived by practitioners~\cite{Bacchelli-etal-2013-Expectations,bosu2015characteristics,turzo}, widely adopted in prior work~\cite{meyers-etal-2018-dataset,Rahman2017Predicting} and corresponds with established taxonomies for comment type classification~\cite{Mantyla-Lassenius-2009-TypesofDefects,tufano2024code}.
% }




% Our approach to filtering and classifying comments goes beyond simple heuristics. We propose a comprehensive framework that considers the context of the code change, the specificity of the suggested actions, and the potential impact on code quality. By implementing this more nuanced approach, we aim to create a dataset of high-quality comments that more accurately represent valuable code review feedback, while addressing the challenges of scale and domain expertise.



% Our work not only contributes to improving the quality of training data but also paves the way for more effective automated code review systems that can better assist developers in improving code quality.

%This often involves multiple decision-makings, i.e., assessing whether the comment raises an issue relevant to the code patch, identifying what actions are being asked to improve the code.  

% The quality of code review comments has been 
% review comment usefulenss, 
% review comment communication intention 

% On a high-level, our labels contains two classes: valid or noisy.  

% \subsection{Overall Pipeline}


\subsection{Research Questions}
% empirically

The goal of this study is to examine the feasibility of Large Language Models (LLMs) to identify valid and noisy review comments and to investigate the impact of noisy comments on the performance of automated review comment generation models.
% quantify the level of noise present in CodeReviewer dataset, assess the potential for its reduction, and examine the subsequent impacts on  models trained for comment generation. 
To this end, we address three research questions.

% RQ: movition, methods 
\textbf{RQ1: To what degree can large language models semantically clean code review comments?} 
While heuristics have been applied to clean the review datasets~\cite{gupta2018intelligent,tufano2021towards,Tufano2022PretrainedModels}, they have missed noisy comments requiring a deeper understanding of semantics~\cite{tufano2021towards}. Despite the recent advancement of LLMs in various code-related tasks~\cite{zan-etal-2023-large}, their ability to determine the quality of
code review comments has not yet been investigated. Our RQ1 aims to bridge this gap by empirically assessing the effectiveness of LLMs in classifying valid and noisy code review comments, shedding light on an efficient way of improving code review data quality. 

% \textbf{RQ2: Does noisy comment removal impact the accuracy of code review comment generation models?}
\textbf{RQ2: Does semantic data cleaning impact the accuracy of code review comment generation models?}
Recent work~\cite{tufano2024code} has observed that a substantial proportion (32\%) of the comments remain noisy. 
% in widely used datasets, raising concerns about dataset quality. 
While retaining only predicted valid comments could improve data quality, it also reduces the training size, potentially affecting model performance. Therefore, we set out RQ2 to examine the impact of data cleaning on the performance of comment generation models.
Specifically, we reassess the accuracy of code review generation models when trained on original versus cleaned datasets.
% providing empirical evidence on the trade-off between dataset quantity and quality.
% “Does noisy comment removal impact the accuracy of code review comment generation models in terms of textual similarity?” to emphasize the focus on quantitative evaluation.

% \textbf{RQ2: How do noisy comments impact the accuracy of code review comment generation models?} 
% \textbf{RQ2: Does noisy comment removal impact the accuracy of  code review comment generation models?}
% Recent work~\cite{tufano2024code} has observed that a substantial proportion of noisy comments remains in widely-used datasets. Yet, the extent of their impact on the performance of comment generation models remains unclear. 
% Therefore, we set out RQ2 to understand the significance of high-quality datasets in the context of code review generation models.
% Specifically, we investigate the accuracy improvement in review comment generation when the models are trained on original and cleaned datasets.

\textbf{RQ3: Does semantic data cleaning improve the quality of code review comment generation models?} As the quality of training data improved (i.e., a higher ratio of valid comments), the review comment generation models would learn more on valid examples, thus they are more likely to generate high-quality review comments.
Yet, it is unclear to what degree can the quality of generated review comments be improved using cleaned datasets.
% Due to the diversity of review comments, comparing generated comments purely with human references often fails to capture the full range of their quality~\cite{Haque-etal-semantic}. 
Therefore, in RQ3, we aim to examine the improvement in the quality of comments generated from models trained on original and cleaned datasets.
We evaluate the quality in two main aspects, i.e., information and relevance using both manual and semi-automated analyses.

\subsection{Dataset}
In this study, we use the CodeReviewer dataset~\cite{Li2022CodeReviewer} as our subject of study. We selected this dataset because it is the largest real-world and widely used code review dataset, representing typical code review data mined from public platforms like GitHub. This dataset exhibits several key characteristics: (a) it is large in scale, including approximately 116K samples in the training set, along with about 10K samples each in the validation and test sets; (b) it is diverse, containing projects from over 1,000 software repositories and covering nine commonly used programming languages.

\subsection{Study Overview}
Figure~\ref{fig:framework} presents an overview of our study. To investigate the degree to which valid and noisy comments can be automatically classified by LLMs, RQ1 evaluates the accuracy of different prompts and LLMs based on manually annotated samples.  
% leverage large language models (LLMs) for classification using zero-shot prompting with chain-of-thought reasoning. We sample a subset from the CodeReviewer training set and manually annotate instances as either \textit{valid} or \textit{noisy}. These samples, along with their definitions, serve as prompt inputs for LLMs to perform classification. RQ1 evaluates the capabilities of several LLMs, 
The most effective approach (prompt strategy and LLM) will be selected to clean the whole training dataset.

RQ2 and RQ3 aim to study the impact of semantic data cleaning on the performance of the comment generation models. We employ the LLM to clean the training data by (a) labeling all instances as valid or noisy, and (b) retaining only instances predicted as valid for a clean dataset. Subsequently, we fine-tune the comment generation models on both the cleaned and original datasets. We then compare their performance in terms of accuracy and quality.
RQ2 evaluates the accuracy of the generated comments by comparing them with actual review comments written by human reviewers.
RQ3 evaluates the quality of the generated comments by examining the information they provide and their relevance to the corresponding code diff. We conduct a manual analysis on a subset of sampled comments and perform a semi-automated analysis to estimate the quality for the overall test set. Below, we describe the details of our experimental design and present experimental results for each RQ (Sections \ref{sec:identifying_noisy_comments_rq1} - \ref{sec:noisy_impact_on_generated_comment_quality_rq3}).

% By removing noisy comments, we reduce the number of training instances in RQ2. This comparison allows us to examine whether models trained on smaller, high-quality datasets outperform those trained on larger, noisier datasets. Through this approach, we gain insights into the relationship between dataset scale and quality in code review comment generation models. \ch{ this paragraph can be optional here.}

% Our study addresses key aspects of automated code review, from noise identification to its impact on model performance and output quality. By systematically evaluating these factors, we provide a comprehensive view of noise issues in code review and offer effective strategies for their mitigation.



\section{Semantic Data Cleaning via LLMs (RQ1)}
\label{sec:identifying_noisy_comments_rq1}
To assess the effectiveness of large language models (LLMs) in classifying valid and noisy comments, we frame this as a binary classification task. Given a natural language review comment \rnl{} and a code change \cdiff, an LLM classifies whether the comment is valid or noisy based on the definitions described in Section~\ref{sec:study_design}. Due to the lack of an evaluation set, we randomly sampled a subset from the training data and manually categorized it with \tvalid \ and \tnoisy \ labels (Sec~\ref{ssec:rq1:data_labeling}). Using this labeled dataset, we then assessed several LLMs with various prompts to evaluate their performance on comment classification (Sections~\ref{ssec:rq1:experimental_setup} - ~\ref{ssec:rq1:experimental_results}).


\subsection{Data Labeling} 
\label{ssec:rq1:data_labeling}
% Introduce the manual labeling: label taxonomy, sample size, human annotation process, annotator background, agreement, final statistics 

% In the first stage, we follow the guideline provided in \cite{tufano2024code} to annotate whether a comment is valid or noisy to  
% We randomly sampled a subset of 35 samples from \cite{tufano2024code} labeled set. 
% Our annotation includes three stages, stage 1 aims to familiarise our annotators with the task and 



To address RQ1, we manually labeled a subset of 270 samples that were randomly selected from the training dataset, constituting a statistically significant sample size with a confidence level of 90\% and a margin of error of $\pm$5\%.

The labeling was conducted by two annotators (i.e., the authors of the paper) who have backgrounds in computer science and software engineering with more than five years of programming and software development experience.
% . Annotator 1 possessed a computer science background with over ten years of programming experience. Annotator 2 is a PhD student in software engineering with more than five years of experience in software development. 
In the initial labeling phase, Annotators 1 and 2 independently annotated 50 of the 270 samples based on the definitions of valid and noisy described in Section~\ref{sec:study_design}.
% using a detailed annotation guideline. This guideline provides the task definition and aspect-based criteria along with examples to determine whether a comment is valid or noisy. 
The initial annotation resulted in a Cohen's kappa coefficient of 0.57, indicating moderate agreement.
After discussions on the disagreed cases, the Cohen's kappa coefficient improved to 0.83, signifying near-perfect agreement. The remaining disagreements were resolved by involving Annotator 3 who is a senior researcher with extensive expertise in software engineering. 


After the initial phase, the definitions of valid comments were elaborated to become guidelines for determining valid comments based on the shared understanding of the two annotators.\footnote{The complete guideline is available in the replication package~\cite{our_replication_package}.} 
Following this guideline, Annotator 1 proceeded to label the remaining 220 samples, consulting with Annotators 2 and 3 on ambiguous cases to finalize the labels.
Figure~\ref{fig:code_reviews_stacked} provides examples of valid and noisy review comments that we annotated.


Ultimately, out of 270 review comments in the training dataset, the sample comprised 172 valid comments and 98 noisy comments.
The proportion of noisy comments (36\%) is similar to the manual annotation in the test set of CodeReviewer by Tufano et al.~\cite{tufano2024code}, i.e., 32\%,\footnote{{
While Tufano et al. reported a 25\% overall noise ratio across all three benchmarks in their study, our 32\% specifically refers to the CodeReviewer benchmark, which was obtained from their replication package.}} suggesting that our manual annotation is consistent with the prior work.
% . \textcolor{orange}{This consistency in findings affirms the reliability of our manual labeling.}

% The consistency between these results underscores the substantial presence of noise within code review datasets, highlighting the importance of addressing this issue in the automated code review process.

% {\color{red}Question: where should we introduce the annotation on the test set? Later on? or merge all data  annotation and move forward?}



\subsection{Review Comment Classification}
\label{ssec:rq1:experimental_setup}
\textbf{Large-language Models (LLMs).}
% To assess the capability of large language models (LLMs) in identifying noisy code review comments, we selected three state-of-the-art models capable of understanding both code and natural language. 
We focused on state-of-the-art LLMs with instruction-following capabilities and exposure to code-related tasks, as this allows the models to follow our classification guidelines and understand the code change.
We selected one commercial model (GPT-3.5) and two open-source models (CodeLlama~\cite{roziere2023code} and Llama 3~\cite{llama3}).
\begin{itemize}
     \item \textbf{GPT-3.5} is an LLM that has been pre-trained on extensive natural language and code corpora. It has been widely used in many code-relevant tasks~\cite{Zheng2024, guo2023exploring}. We employed the gpt-3.5-turbo-0125 in our experiments. 
     % For example, prior work employed this model to conduct code review comment generation directly using prompting  \cite{tufano2024code}, but found that the model struggled to raise points aligned with human reviewers and under-performed compared to fine-tuned code review models like CodeReviewer. In this study, instead of using this model to generate comments directly, we use it to evaluate the quality of comments, which requires less external context for reasoning. We choose the gpt-3.5-turbo-0125 in our experiments. 
    % is a state-of-the-art commercial code generation model with billions of trainable parameters and has been pre-trained on an extensive code corpus
    \item \textbf{CodeLlama} is a variant of the open-source model Llama2~\cite{touvron2023llama}, tailored for code generation and understanding. We chose the CodeLlama-34b-Instruct version which is fine-tuned to follow human instructions, enabling various tasks across multiple programming languages without task-specific training.
    % We chose the CodeLlama-34b-Instruct version in our experiments.
    % the model is equipped with the instruction following ability in a zero-shot manner for programming tasks for multiple languages. 
    % It supports most of the widely use programming languages such as  Python, C++, Java and etc.
    % We chose the 34 billion parameter version, i.e., CodeLlama-34b-Instruct, the largest model our infrastructure can support. 
    
    
    \item \textbf{Llama3} is a recent large open-source model.
    % , improving upon Llama2 with larger pre-training data, longer context sequences, and more efficient inference. 
    Despite not being specifically code-trained like CodeLlama,
    % with substantially larger high-quality pre-training data (including four times more code data), longer context sequences, and more efficient inference. 
    Llama3's performance is optimized for coding and reasoning,
    % and achieves state-of-the-art performance in both code and natural language benchmarks,
    which is suitable for our classification task. We used the Llama-3-8B-Instruct version to enable instruction following in our experiments.
    % Despite not being specifically code-trained like CodeLlama, its extensive code data exposure and state-of-the-art performance in both code and natural language benchmarks at this scale make it our chosen model. We choose the Llama-3-8B-Instruct in our experiments.
    % Available in both 8B and 70B versions, the instruction-fine-tuned Llama-3-8B-Instruct outperforms the Llama-2-70B model on several benchmarks, such as HumanEval \cite{chen2021evaluating}. 
    % featuring an improved efficiency and inference on longer sequences . 
\end{itemize}
{Our model selection considers (a) open-source and closed-source; (b) model size; (c) general vs code-focused. Although CodeLLama and Llama3 are from the same family, they were trained for different purposes, i.e., CodeLLama represents code-focused and Llama3 represents general-purpose LLMs.}

%S2: the model we chose are all instrucional tuned with human feedback, which means they are trained to follow human instructions even when facing with new task instructions.  


% logic: what's prompting, 
% how to desgin good prompts/rationale: 
% survey paper, OpenAI blog: , the apache prompting format, 
% what's our practice 

\begin{figure}
    \centering
    % \begin{minipage}{0.45\textwidth}
    \includegraphics[width=0.8\linewidth]{figures/prompting_strategies.v2.drawio.pdf}
    % \end{minipage}
    \caption{The prompt template for noisy classification using \pdef{} with context.}
    \label{fig:prompt_template}
\end{figure}

\textbf{Prompt Design}. A prompt is used to instruct LLMs to perform specific tasks~\cite{Liu-etal-2023-PromptSurvey}. 
Typically, a prompt comprises two components: the system prompt, which guides the models' overall behaviours, and the user prompt, which provides specific input for each query. Well-crafted and informative prompts in both components can effectively elicit relevant and accurate responses from LLMs. For our review comment classification task, we developed a comprehensive prompt template that draws upon strategies from OpenAI's gpt-best-practices~\cite{openai2023gptbestpractices} and those that have proven effective in software engineering tasks such as software vulnerability detection~\cite{Zhang-etal-2024-PromptEnhanced} and security code review~\cite{yu2024securitycodereviewlarge}. Our system prompt incorporates task instruction details including role assignment, key definitions, and classification criteria. 
The user prompt provides the context of the code change, the specific review comment to be classified, and a structured format for the model's response. Figure~\ref{fig:prompt_template} provides an example of our prompt template. 
% Our prompt incorporates task instruction details (i.e., role assignment, definitions, and criteria) in the system prompt, as well as a context prompt and a review comment in the user prompt. Figure~\ref{fig:prompt_template} provides an example of our prompt template. 

%P2: introduce the variants 
To investigate the effectiveness of different components of the prompts on the review comment classification task, we designed four prompts by varying the task instruction in the system prompt and the user prompt. For task instructions, we varied between (a) \pdef: a prompt that includes the definitions of valid and noisy comments as we used in manual annotation, and {(b) \paux: a prompt that supplements the \pdef{} with seven auxiliary rules that concretize the same criteria in Fig~\ref{fig:prompt_template}. These auxiliary rules were developed 
based on our discussions and shared understanding during our initial labelling phase, and were used consistently to guide annotations throughout both RQ1 and RQ2. Complete prompts for \pdef{} and \paux{} for our experiments are in our replication package~\cite{our_replication_package}.}
% and (b) \paux: a prompt that supplements the \pdef{} with \textcolor{red}{seven} auxiliary rules derived from common mislabeling comments observed during manual annotation, \sout{replacing} \textcolor{red}{covering} the criteria in Fig~\ref{fig:prompt_template}. These rules provide additional information that further distinguishes between valid and noisy comments. 
For the user prompt, we varied between (a) providing only a review comment \rnl{} as input, and (b) providing both the code diff patch \cdiff{} and the corresponding comment \rnl{} together as input. We experimented with these prompts on three LLMs, resulting in 12 experiments. For all models, we set the temperature to a low value of 0.1 to ensure consistency.  


% \ch{Instruct models: these models are instructional tuned models, which takes instructions as input, and respond; we construct the instruct based on the following; (what we are doing is consistent with the models)}

% We use zero-shot prompting to test these models due to the lack of labeled data, which means that the LLM needs to identify noisy comments based on the prior knowledge that they have learnt during their training stages. 


% inspired by the chain-of-thought prompting methodology
% We design task-specific prompt
% Our prompt includes several components: 
%  role prompting with task instruction, definitions of valid and noisy comments, evaluation criteria, context information, answer prompt to ensure the output is in correct json format, 
% self-explanations 

% {\color{red} TODO: explain the method of construct the prompts; e.g., did we used the chain-of-thought}

% To understand the impact of different prompts on noisy classification performance and identify optimal prompts for cleaning comments, we design two prompt groups by considering the \textbf{task instruction} and the \textbf{context information}. For \textbf{task instructions}, we compare (a) \pdef: a prompt used in prior work for identifying code changes in comments with definitions \cite{tufano2024code}, and (b) \paux: a prompt aligned with our annotation guidelines, detailing aspects for judging comment validity. This setup impacts the system prompt for each model. For \textbf{context information}, we investigate (a) providing only a review comment \rnl as input, and (b) providing both the code diff patch \cdiff{} and the corresponding comment \rnl together as input. This setup will impact how the user prompt is configured for each model. See Table~\ref{tab:prompt_templates} for prompt templates. We experiment with these combinations on three LLMs, resulting in 12 experiments. For all models, we set the temperature to a low value of 0.1 to ensure consistency.  
% A complete prompt template is available on in our replication package.\footnote{\ch{Put the prompt template link here}} 



\subsection{Evaluation}
To assess the performance of LLMs in classifying valid and noisy comments, we employ \textbf{precision}, \textbf{recall}, and \textbf{F1} metrics. 
To provide a comprehensive view of classification performance, we report class-wise metrics, treating both valid and noisy as positive labels in separate evaluations. This evaluation offers insights into each LLM's ability to identify valid and noisy comments.
We also measure an overview performance considering both valid and noisy classes.
Given the imbalanced nature of our dataset between valid and noisy classes, we report weighted overall performance metrics, where weights are proportional to the sample size of each class.
% , formulated as follows: 
% \begin{align}
%     \text{M}_{\text{weighted}} = \frac{\sum_{i}^{\{\mathrm{valid,noisy}\}} (\text{M}_i \cdot w_i)}{\sum_{i}^{\{\mathrm{valid,noisy}\}} w_i}, 
% \end{align}
% where $\text{M}_i \in \{precision, recall, F1\}$ for the class $i$ and $w_i$ is the proportion of true instances for the class $i$.

\input{tables/RQ1_results}

\textbf{Baseline.}
We use the cleaning approach of Li et al.\cite{Li2022CodeReviewer} as a baseline.
When constructing the CodeReviewer dataset, they applied extensive cleaning methods using both heuristic rules~\cite{Tufano2022PretrainedModels} and SVM classifiers.\footnote{The data cleaning approach described in their appendix: \url{https://arxiv.org/pdf/2203.09095v1}}
Consequently, all instances in the CodeReviewer dataset were considered valid according to the approach of Li et al., which implies that any sampled instances are also considered valid by their approach. This baseline is appropriate because state-of-the-art cleaning approaches were already applied; thus,  the remaining noisy ones represent cases that prior techniques could not detect, reflecting the upper bound performance of prior approaches. Other techniques~\cite{tufano2021towards,bosu2015characteristics} are not suitable as they either require additional information that is not available in the CodeReviewer dataset~\cite{bosu2015characteristics}, or are limited to the specific context~\cite{tufano2021towards}.
For example, we applied the heuristic rules of Tufano et al.~\citep{tufano2021towards}, and none of the comments in the CodeReviewer dataset are identified as noisy.
% , indicating that the remaining noise is too challenging for existing methods to identify. Given this context, we used Majority as our baseline to establish a performance threshold that advanced models must surpass.

% For the baseline, we used the Majority baseline, which predicts the dominant class (\tvalid{}) for all instances in our labeled samples. The result of this baseline is equivalent to the current heuristic cleaning approaches~\cite{Li2022CodeReviewer} where all the instance in the training dataset are considered as valid. This sets a baseline performance threshold that advanced models should surpass, especially in accurately predicting valid comments while also identifying noisy ones.



% \textcolor{red}{need elaboration.} 
% (1) Precision refers to the ratio of correct predictions to the total number of predictions; (2) Recall refers to the ratio of correct predictions to the total number of samples in the golden test set; and (3) F1 is the harmonic mean of precision and recall.

\subsection{Experimental Results}
\label{ssec:rq1:experimental_results}
Table~\ref{tab:rq1_main_results} presents our experimental results on the noisy comments classification task. 
The precision of the baseline on \tvalid{} comments suggests that the original dataset has a valid comment ratio of 63.7\%.

% I tune this down to avoid the impression that we gave to ICSE reviwers: LLMs are super-powerful and our baseline is not strong enough to compare against
Table \ref{tab:rq1_main_results} shows that
 LLMs achieve an overall F1 of up to 71.7\%. Among the studied LLMs, Llama3 and GPT-3.5 achieved comparable overall precision (71.8\% and 70.3\% respectively), with Llama3 and CodeLlama showing similar performance in recall (72.6\% and 65.6\%). For identifying valid comments, GPT-3.5 and Llama3 achieve a precision of 85.1\% and 75.3\% respectively. This suggests that by retaining only instances predicted as valid, the valid comment ratio is improved by 21.4 and 11.6  percentage points from the original dataset (63.7\%). These findings suggest that LLMs can  distinguish between valid and noisy code review comments.
% that were not identified by prior methods. 

% (1) overall performance and trend (ICSE version) 
% Our results show that LLMs can classify valid and noisy comments, achieving an overall F1 of up to 71.7\%, which is 49.6\% higher than the majority baseline. Llama3 and GPT-3.5 achieve an overall precision of 71.8\% and 70.3\%, respectively, which is more than 73\% higher than the Majority baseline. Llama3 and CodeLlama achieve an overall recall of 72.6\% and 65.6\%, respectively. For identifying valid comments, Llama3 and GPT-3.5 achieve a precision of 75.3\% and 85.1\% respectively. These results highlight the potential of LLMs in identifying valid code review comments.

% Overall, all tested LLMs outperformed the baseline in precision and F1 score, demonstrating their potential for identifying noisy comments. Meanwhile, their performance vary across the board under difference scenarios, with  precision improvements range from 7\% to 31.2\% and F1 improvements range from 2.7\% to 22.1\%. Specifically, among the three LLMs, Llama3-8b emerged as the most robust model, achieving the highest overall precision at 71.8\%, recall at 72.6\%, and F1 score at 71.7\%. 
% CodeLlama-34b showed comparable performance but with more variability across different prompts and input contexts. GPT-3.5-Turbo exhibited a significant trade-off between precision and recall, with similar precision as Llama3-8b but lower recall. 

% \pdef vs \paux
% Merge two paragraph to save space
{Table~\ref{tab:rq1_main_results} shows that the {} prompt with only the review comment (\rnl) as input generally performed best across valid and noisy classes. It achieves the highest precision on overall and \tvalid, and the second highest precision for \tnoisy. Specifically, for Llama3, the performance remains similar whether using \pdef{} or \paux{} prompt. 
% when using the \paux{} prompt is similar to the performance when using the \pdef{} prompt. On the other hand, GPT-3.5's performance dropped when using \paux. 
% Using the \paux{} prompt which includes supplementary rules does not always result in performance improvements. Specifically, for Llama3, the performance when using the \paux{} prompt is similar to the performance when using the \pdef{} prompt. On the other hand, GPT-3.5's performance dropped when using \paux. 
For CodeLlama, we observe a performance improvement when only the review comment (\rnl) is provided with the \paux{} prompt. 
Similarly, providing additional code context (\cdiff) alongside review comments (\rnl) does not improve the model performance. 
For instance, CodeLlama has a precision drop of 9.9\% - 24\% when \cdiff{} is provided. 
This may be because the additional details of \paux{} and code context increase the prompt length and distract the models, potentially leading them to overlook crucial criteria in longer prompts~\cite{liu2024lost}. For instance, we observed that GPT-3.5 returns unexpected labels or empty responses for three samples with long context.
% \footnote{\textcolor{red}{The long context also occasionally causes GPT-3.5 to return three unexpected labels or empty responses out of 270 samples.}}
These findings highlight the importance of concise, focused inputs for optimal LLM performance.
% The increased prompt length of \paux{} may impact LLMs' processing capacity, potentially leading them to overlook crucial criteria in longer prompts~\cite{liu2024lost}, necessitating a balance between simple and detailed instructions.
% The findings suggest a non-linear relationship between prompt complexity and model performance, 

% Merge with the above paragraph because they have same core reason and save space.
% \rnl vs. \rnl + \cdiff
% Surprisingly, providing additional code context (\cdiff) alongside review comments (\rnl) does not improve the model performance. 
% For instance, CodeLlama has a precision drop of 9.9\% - 24\% when \cdiff{} is provided. 
% This may be because the additional code context \textcolor{red}{increases the context length} and distracts the models, potentially causing misinterpretation of comments based on superficial code similarities rather than adherence to provided definitions. These findings highlight the importance of concise, focused inputs for optimal LLM performance.

% One unexpected finding occurred when comparing the performance of providing different context information, i.e., \rnl vs. \rnl + \cdiff. We found that simpler input contexts, such as using only review comments (\rnl), generally lead to better performance. Adding \cdiff into the prompt led to a significant negative impact on performance, e.g., resulting in a drop in precision of 9.9\% - 24\% on CodeLlama-34b.
% This demonstrates that providing code as extra context does not help models and can even hinder their performance. One potential reason could be that the code provides more distractions for the models, causing them to misinterpret comments that share similar code pieces with the code patch as a signal of validity rather than follow the definitions provided in the prompt. Additionally, adding \cdiff increases the length of the prompt, and LLMs may lose the ability to effectively process longer inputs. Prior work has shown that LLMs tend to ignore content in the middle of long inputs [cite]; in our case, the criteria for classifying valid and noisy comments are buried in the middle.
% resulting in a drop in precision of 9.9\% - 24\% on CodeLlama-34b, 4.7\% - 6.4\% on GPT3.5-Turbo, and 
% 7.7\% - 9.2\% on Llama3-8b. Similarly, there was a drop in F1 score of 5.7\% - 14.4\% on CodeLlama-34b, 4.7\% - 6.4\% on GPT3.5-Turbo, and 8.4\% - 11.9\% on Llama3-8b, with the exception of an 8.1\% F1 improvement on GPT3.5-Turbo when using \pdef.

% When we turn our attention to the class-wise performance, we observe a discrepancy between the two classes. The performance is largely higher for \tvalid \ and lower for \tnoisy, indicating that identifying noisy comments is harder than identifying valid ones.
% Among the three models, the Llama series models exhibit similar behaviors with high recall, showing a tendency to predict all comments as \tvalid, especially for CodeLlama-34b, which has the highest recall at 94.8\%. In contrast, GPT3.5-Turbo tends to predict most comments as \tnoisy, achieving the highest recall of 88.8\%. On the precision side, GPT3.5-Turbo often achieves higher precision, followed by Llama3-8b.


% performance on Valid and Noisy label differences 
% We observed a consistent pattern in class-wise performance across models: all performed better on \tvalid\ than \tnoisy\ comments, indicating that identifying noisy comments is more challenging. Llama-based models showed high recall for \tvalid\ comments, with CodeLlama-34b reaching 94.8\%, suggesting a tendency to overpredict this class. Conversely, GPT3.5-Turbo leaned towards \tnoisy\ predictions, achieving 88.8\% recall for this class. In terms of precision, GPT3.5-Turbo generally outperformed the others, followed closely by Llama3-8b.

% \subsection{Discussions}
% \label{ssec:rq1:discussions}
% Do these models exhibit complementary results? 

% TODO: add some qualitative analysis on these cases where humans labeled as Noisy but models fail to identify them as Valid, are they common across the these models? 
\begin{tcolorbox}[size=title]
{\textbf{Answer to RQ1:}} LLMs show promising potential in classifying code review comments, with an overall F1 up to 71.7\%. The precision in identifying valid comments highlights that the proportion of valid comments can be improved from 64\% on the original dataset to 85\% on our cleaned datasets.
% Llama3 and GPT-3.5 achieve 71.8\% and 70.3\% overall precision respectively in classifying valid and noisy comments. 
% For valid comments, LLMs achieve a precision of up to 85\%.
\end{tcolorbox}




\section{Impact on Comment Generation Accuracy (RQ2)}
\label{sec:noise_impact_on_comment_generation_models_rq2}
To investigate the impact of semantic data cleaningon comment generation models, we employ the LLMs from RQ1 to clean the dataset. Then, we fine-tune the pre-trained models with the cleaned dataset and evaluate the performance of the models in generating review comments. 




\subsection{Semantic Data Cleaning}
To clean the dataset, we use the LLMs to predict the \tvalid\ and \tnoisy\ classes and retain the \tvalid\ instances (i.e., removing the \tnoisy\ instances). We used (1) \textbf{GPT-3.5} and (2) \textbf{Llama3} using the \pdef{} prompt with \rnl\ since these two models exhibit strong performance in retaining a high ratio of \tvalid\ comments (with 85.1\% and 75.3\% precision, respectively) while exhibiting complementary characteristics in recall. This allows us to obtain cleaned datasets with varying degrees of a valid comment ratio and training size.


We use the LLMs to clean the training and validation sets of the studied dataset. Table~\ref{tab:dataset_statistics} shows the statistics summary of cleaned training and validation set sizes with different LLMs. The \textsc{Original} row shows the size of the original training and validation set of CodeReviewer~\cite{Li2022CodeReviewer}.
The \textsc{Cleaned} rows show the size of the training and validation cleaned by \textbf{GPT-3.5} and  \textbf{Llama3} using the \pdef{} prompt with \rnl{}.
% datasets from \textbf{GPT-3.5-Turbo} and \textbf{Llama3-8b}, referred as to denoted as \textsc{Clean\_Llama3} and \textsc{Clean\_GPT-3.5}; and 
The \textsc{Controlled} are the training and validation sets that are randomly sampled to have the same number of instances as the two cleaned datasets.
% the control groups of \textsc{Control\_Llama3} and \textsc{Control\_GPT-3.5}, which is randomly sampled from the original dataset to have the same number of instances as the two cleaned datasets. 
These control groups are designed to account for the impact of reduced data in clean datasets on performance, as the removal of noisy data results in fewer instances for training and validation.


% {\color{green} Question: do we want to mention that we annotate the 35 samples from Tufano's paper? The concern is that these samples are from test set, we might give reviewer the idea that we looked at test already in the first stage, is this a problem?}


\begin{table}[!t]
    \centering
    \caption{Statistics of Datasets for Comment Generation.}
    \begin{tabular}{@{}lcc|c@{}}
        \toprule
        \textbf{Dataset} & \textbf{Training set} & \textbf{Validation set} & \textbf{Test set} \\
        \midrule
        \textsc{Original} & 117,739 & 10,319 & \multirow{5}{*}{10,169} \\ \cmidrule{1-3}
        % \hline
        \cleangpt & 39,625 & 3,395 & ~\\
        \cleanllama & 87,872 & 7,571 &  ~\\ \cmidrule{1-3} %\hline
        \controlgpt & 39,625 & 3,395 &  ~\\
        \controlllama & 87,872 & 7,571 & ~ \\
        \bottomrule
    \end{tabular}
    \label{tab:dataset_statistics}
\end{table}
\subsection{Comment Generation Models}
% To investigate the impact of dataset quality on the review comment generation, we will need to fine-tune the models using our cleaned datasets.

To address RQ2, we focused on models that can generate code review comments. Our study requires code review models with reproducibility for a fair comparison of model performance between original and cleaned datasets under the same settings. Specifically, we need models that provide publicly available checkpoints and fine-tuning scripts. While prior work has fine-tuned various code models~\cite{auger,CCT5,sghaier24,Li2022CodeReviewer} and LLMs~\cite{LLaMA-Reviewer} for this task, many do not meet our criteria. Therefore, we selected two widely known models for code review automation, i.e., CodeT5~\cite{wang-etal-2021-codet5} and CodeReviewer \cite{Li2022CodeReviewer} that met our criteria.


\textbf{CodeT5} is a general-purpose encoder-decoder Transformer model pre-trained on both programming and natural languages, which demonstrated effectiveness across multiple downstream tasks. 
% Its training on eight common programming languages and various code-related tasks such as code summarization has demonstrated effectiveness across multiple downstream tasks \cite{Lu-etal-2021-CodeXGLUE}. 
% CodeT5's capability of understanding multilingual programming languages and generating natural language makes it well-suited for code review tasks, which require code comprehension and textual output.
%CodeReviewer: a code review specialized model 
\textbf{CodeReviewer} is a state-of-the-art model pre-trained on tasks relevant to code changes. It leverages the pre-trained weights from CodeT5 for model initialization and continues pre-training on datasets pertinent to code reviews.
% , performing tasks such as predicting code diff tags (+/-) and generating review comments for given code diffs. 
This specialized pre-training allows CodeReviewer to demonstrate superior performance on the comment generation task. 

{We select CodeT5 and CodeReviewer as representative models because we aim to compare the impact of data quality between a general code-pretrained model (CodeT5) and a code review-specific pretrained model (CodeReviewer). This comparison allows us to understand whether the benefits of data cleaning generalize across different models and training data.
We select CodeT5 over alternatives such as CodeBERT because CodeT5 has consistently demonstrated superior performance across various code-related tasks~\cite{wang-etal-2021-codet5}. 
% While CodeReviewer was initially released in 2022, 
For CodeReviewer, recent studies have validated its continued competitiveness in the code review domain. 
For instance, a recent study by Google researchers~\cite{frommgen2024resolving} highlighted CodeReviewer as ``perhaps the closest recent result" to their code review assistant trained on their high-quality industrial datasets. Furthermore, Fan et al.~\cite{fan2024exploring} found that fine-tuning more recent large language models like Llama2 and CodeLlama sometimes did not outperform CodeReviewer, with BLEU-4 score differences ranging from -1.28 to 0.42. These findings demonstrate that CodeReviewer remains a competitive baseline for code review automation, making it an appropriate choice for evaluating the impact of data quality on model performance.}


% To answer RQ2, we focus on models that can generate code review comments.
% While prior work fine-tuned various code models~\cite{auger,CCT5,sghaier24,Li2022CodeReviewer} and LLMs~\cite{LLaMA-Reviewer} to generate code review comments, we need approaches that can be replicated in order to investigate the impact of dataset quality on their performance in review comment generation.Therefore, we selected two widely known models for code review automation, i.e., CodeT5\cite{wang-etal-2021-codet5} and CodeReviewer \cite{Li2022CodeReviewer} that provide publicly available checkpoints and fine-tuning scripts.

% 

% \textcolor{red}{Justification why these 2 are selected. Why not others.}

% why these two: checkpoints are publicaly available and easy to replicate, 
% not replicable: Auger (pointed out by Tufano et. 2024)
% no checkpoints: CCT5 \cite{CCT5}, DISCOREV \cite{sghaier24} 
%LlaMA-Reviewer: relying on instruction-tuning, our fine-tuned CodeReviewe achieved higher performance than LlaMa-reviewer 
%cite Google paper: CodeReviewer achieved the closest performance to  Google's most recent  codereview assistant \cite{frommgen2024resolving}

% representative, 
%CodeT5: a general pre-trained code model 
% why codeT5 is a suitable model for code review comment generation: it's an encoder-decoder model pre-trained on xx code data, so it has good understanding of code and are able to generate natural language comments 




% , and its model checkpoints are publicly available.

% \ch{needs revision here}
% CodeT5 wasn't pre-trained on a lot of code review data specifically, which allows us to understand the impact of dataset quality directly 
% \textbf{CodeT5} \cite{wang-etal-2021-codet5} is a general pre-trained code model that utilizes an encoder-decoder Transformer architecture. The training aims at learning both code semantics and natural language semantics, involving a diverse set of pre-training tasks, including masked span prediction, identifier tagging and prediction, and code-text bi-modal generation tasks. CodeT5 has been used as a baseline model in follow-up studies, often fine-tuned for specific code tasks such as code review \cite{Li2022CodeReviewer, LLaMA-Reviewer}. In this study, we employ the CodeT5-base version, which has a model size of 220M with 12 layers of encoder and decoder. {\color{red} {\small Q: why don't we run CodeT5-large and see whether larger model with higher quality outperforms CodeReviewer-220M? or data size and model size which one impacts more. Calling for an update of codereview baseline, upgrading to larger models.}}
% Notably, the model is trained with both programming language and natural language so it can align cross-modality knowledge. 

\textbf{Fine-tuning}: We fine-tuned the CodeReviewer and CodeT5 models using original, cleaned, and controlled datasets. All experiments were conducted on four NVIDIA H100-80GB GPUs.We followed the hyperparameters specified in the original CodeReviewer, with one exception: we adjusted the batch size from 64 to 32, which resulted in an improved BLEU score from 5.3 to 5.7 and improved the training efficiency. To avoid over-fitting, we used an early stopping criterion that ended training after 5 epochs without improvement on validation set.
% We re-ran CodeT5 and CodeReviwer on original dataset on our set up and achieved higher performance than previous reported in Li et al.~\citep{Li2022CodeReviewer}. 

% \begin{table}[]
%     \centering
%     \caption{Dataset statistics}
%     \begin{tabular}{llll}
%         \toprule
%         ~ & Training set & Validation set  \\
%         \midrule 
%         Original &  117,739 & 10,319 \\ 
%         Clean\_{Llama3} & 87,872 & 7,571\\ 
%         Clean\_{GPT-3.5} & 39,625 & 3,395\\ 
%         \midrule \midrule 
%         & Test set \\ 
%          & Valid & Noisy & Total\\  
%         Tufano's & 234 & 135  & 369\\ 
%         Ours & 223 & 148 & 371 \\ 
%         Combined & 452 & 274 & 726\\ 
%         \bottomrule
%     \end{tabular}
    
%     \label{tab:dataset_statistics}
% \end{table}


% \begin{table}[t]
%     \centering
%     \setlength{\tabcolsep}{8pt} 
%     \caption{Statistics of Three Labeled Test Subsets.}
%     % \begin{tabular}{@{}lccc@{}}
%     \begin{tabular}{lccc}
%         \toprule
%         \textbf{Source} & \textbf{Valid} & \textbf{Noisy} & \textbf{Total} \\
%         \midrule
%         Tufano’s & 234 & 135 & 369 \\
%         Ours & 223 & 148 & 371 \\
%         Our\&Tufano & 452 & 274 & 726 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:three_test_set_splits}
% \end{table}

\begin{table*}[h]
        % \small 
        \centering
        \caption{Model Performance (BLEU-4) on Comment Generation Models.}
        \label{table:comment_gen_main_results}
        \setlength{\tabcolsep}{5pt}  % Adjust the spacing between columns here
        % \resizebox{0.98\textwidth}{!}{
        \begin{tabular}{ll|l|ll|ll|ll}
        \toprule 
        \textbf{$M$} & \textbf{Dataset} & \textbf{Test} & \textbf{Valid$_{\text{Our\&Tufano}}$} & \textbf{Noisy$_{\text{Our\&Tufano}}$} & \textbf{Valid$_{\text{Our}}$} & \textbf{Noisy$_{\text{Our}}$} & \textbf{Valid$_{\text{Tufano}}$} & \textbf{Noisy$_{\text{Tufano}}$} \\ \midrule
          % \multicolumn{9}{c@{}}{\textbf{BLEU-4 }} \\ \midrule 
        \multirow{5}{*}{\rotatebox{90}{CodeReviewer}} & \textsc{Original} & 5.73 & 6.17 & 5.41 & 5.45 & 5.17 & 7.12 & 5.60  \\
        ~ & \cleangpt &  \textbf{6.04} \ 5.4\%↑$^{*}$ & \textbf{6.97} \ \textbf{13.0\%}↑$^{*}$  & 5.02 \ 7.2\%↓ & \textbf{5.93} \ 8.8\%↑ & 5.19 \ 0.4\%↑ & \textbf{7.99} \ \textbf{12.2\%}↑$^{*}$ & 4.83 \ 13.8\%↓ \\
        ~ & \controlgpt & 5.63 & 6.20 & 5.43 & 5.21 & 5.13 & 7.39 & 5.70 \\ 
        &  \cleanllama & \underline{5.97} \ 4.2\%↑$^{*}$ & \underline{6.63} \ 7.5\%↑$^{*}$  & 5.18 \ 4.3\%↓ & 5.64 \ 3.5\%↑& 5.11 \ 1.2\%↓ & \underline{7.71} \ \underline{8.3\%}↑$^{*}$ & 5.14  \ 8.2\%↓ \\
         ~ & \controlllama & 5.63 & 6.18 & 5.66 & 5.12 & 5.36 & 7.45 & 5.86 \\
        \midrule 
        \multirow{5}{*}{\rotatebox{90}{CodeT5}} & \textsc{Original}&  5.19 & 5.34 & 5.04 & 4.84 & 5.09 & 5.85 & 6.03  \\ 
        ~ & \cleangpt & 5.67 \ \textbf{9.2\%}↑$^{*}$ & 6.00 \ \underline{12.4\%}↑$^{*}$  & 5.23 \ 3.8\%↑$^{*}$  & \underline{5.88} \ \textbf{21.5\%}↑$^{*}$ & 5.27 \ 3.5\%↑ & 6.06 \ 3.6\%↑ & 5.15 \ 14.6\%↓ \\
        ~ & \controlgpt & 5.20 & 5.34 & 5.30 & 5.17 & 5.39 & 5.45 & 5.41  \\ 
         ~ & \cleanllama& 5.54 \ \underline{6.7\%}↑$^{*}$ & 5.74 \ 7.5\%↑$^{*}$  & 5.33 \ 5.8\%↑ & 5.32 \ \underline{9.9\%}↑$^{*}$ & 5.14 \ 1.0\%↑ & 6.09 \ 4.1\%↑ & 5.46 \ 9.5\%↓ \\
        ~ & \controlllama& 5.21 & 5.19 & 5.12 & 4.95 & 5.26 & 5.38 & 5.01 \\
        % \midrule \midrule
        %    \multicolumn{9}{c@{}}{\textbf{BLEU-4 (W/O Stop Words)}}  \\ 
        %  \cmidrule(lr){1-9}  % This line adds the horizontal rule
        % \multirow{5}{*}{\rotatebox{90}{CodeReviewer}} & \textsc{Original} & 7.33 & 8.04 & 6.96 & 6.80 & 6.32 & 9.55 & 7.75 \\
       
        % ~ & \cleangpt &  \underline{7.41} \ 1.1\%↑ & \textbf{8.75} \ 8.8\%↑ & 6.08 \ 12.6\%↓ & \textbf{7.40} \ 8.8\%↑ & 6.17 \ 2.4\%↓ & \underline{10.07} \ 5.4\%↑ & 5.98  \ 22.8\%↓ \\
        % ~ & \controlgpt &  7.26 & 7.89 & 7.37 & 6.48 & 6.63 & 9.54 & 8.21 \\ 
        %  ~ & \cleanllama  & \textbf{7.44} \ 1.5\%↑ & \underline{8.57} \ 6.6\%↑ & 6.56 \ 5.7\%↓ & 7.02 \ 3.2\%↑ & 6.49 \ 2.7\%↑ & \textbf{10.25} \ 7.3\%↑ & 6.52 \ 15.9\%↓ \\
        % ~ & \controlllama & 7.22 & 7.95 & 7.06 & 6.48 & 6.77 & 9.67 & 7.34 \\
        % \midrule 
        % \multirow{5}{*}{\rotatebox{90}{CodeT5}} &  \textsc{Original} &  6.64 & 6.59 & 6.75 & 5.82 & 6.88 & 6.83 & 7.79 \\ 
        % ~ & \cleangpt & 6.79 \ 2.3\%↑ & 7.32 \ 11.1\%↑ & 5.76 \ 14.7\%↓ & \underline{7.17} \ 23.2\%↑ & 5.92 \ 14.0\%↓ & 7.41 \ 8.5\%↑ & 5.54 \ 28.9\%↓ \\
        % ~ & \controlgpt &  6.45 & 6.67 & 6.33 & 6.20 & 6.68 & 7.03 & 6.29 \\ 
        % ~ & \cleanllama& 6.87 \ 3.5\%↑ & 7.24 \ 9.9\%↑ & 6.52 \ 3.4\%↓ & 6.63 \ 13.9\%↑ & 6.10 \ 11.3\%↓ & 7.76 \ 13.6\%↑ & 6.79 \ 12.8\%↓ \\
        % ~ & \controlllama&  6.45 & 6.20 & 6.55 & 5.66 & 6.98 & 6.70 & 6.06 \\
        \bottomrule 
        \multicolumn{9}{l}{\footnotesize The highest and second-highest results are in bold and underlined, respectively. $^{*}$ indicates the statistical significance (p-value $<$ 0.05).
        % The $^{*}$ signifies that the \textsc{Cleaned} models significantly outperform the \textsc{Original} models (p-value $<$ 0.05).
        } \\
        % \multicolumn{9}{l}{\footnotesize }
        \end{tabular}
        % }
        \end{table*}

    
\subsection{Evaluation} 
We evaluate the fine-tuned models using the original test set.
We do not clean the test set because our goal is to assess the impact of different training sets on comment generation. 
Nevertheless, there may be noisy comments in the test set.
Thus, we evaluate models' performance on valid and noisy samples. To be consistent with RQ1, we manually labeled a subset of review comments from the test set as valid or noisy using our guidelines described in Section~\ref{ssec:rq1:data_labeling}. We randomly sampled 371 review comments, ensuring a significant sample size with a confidence level of 95\% and a margin of error of $\pm$5\%. The annotation was conducted by Annotator 1, with ambiguous cases discussed with Annotator 2 to reach a consensus. As a result, this sample includes 223 valid samples and 148 noisy samples. To further increase the generalisation of the results, we obtained a manually labeled subset sampled from the test set by Tufano et al~\cite{tufano2024code}, which includes 234 valid comments and 135 noisy comments that were randomly sampled from the same CodeReviewer test set that we used. We combined the two labeled test sets, resulting in a total of 726 samples, comprising 452 valid and 274 noisy comments.

To evaluate the quality of generated comments, we conduct automatic evaluation using the BLEU (Bilingual Evaluation Understudy) metric~\cite{papineni-etal-2002-bleu}, which quantifies the n-gram lexical overlap between the generated comments and the ground truth comments from human reviewers. Following prior work~\cite{Li2022CodeReviewer}, we use the BLEU-4 variant to calculate the overlap of up to 4-grams between generated and ground truth comments. We evaluate model performance on the entire test set and on manually labeled valid and noisy subsets. We use the one-sided Wilcoxon signed-rank test to statistically examine the difference in BLEU-4 between the original and cleaned models.

% The Wilcoxon signed rank tests show significant difference between cleaned and original in both information and relevance (p-value < 0.001) with a medium effect size (0.35-0.45). The cleaned models also achieved BLEU statistically higher BLEU than the original models with a small effect size. We can add these statistical tests to complement our results and discussion.  


\subsection{Experimental Results}
Table~\ref{table:comment_gen_main_results} presents our experimental results. Overall, despite the substantial reduction in training data (i.e., 66\% smaller using GPT-3.5 and 25\% smaller using LLAMA3), the performance of comment generation models is not negatively impacted. Instead, when training using \textsc{Cleaned} datasets, the BLEU-4 increases by 4.2\%-5.4\% for CodeReviewer models and 6.7\%-9.2\% for CodeT5 models compared to the \textsc{Original} dataset. The Wilcoxon signed rank tests also confirm that BLEU-4 of the cleaned models is statistically higher than the original models for all test instances. The improvement aligns with previous work; e.g., $\Delta$BLEU(CodeReviewer, CodeT5) = 0.49~\cite{Li2022CodeReviewer}, and fine-tuning LLMs also demonstrates similar gains, e.g., $\Delta$BLEU(Llama-Reviewer, CodeReviewer) = 0.4~\cite{LLaMA-Reviewer}.
However, merely reducing the training data does not improve performance, as evidenced by the results of the \textsc{Controlled} groups. This indicates that training data quality is as important as data quantity. 

% Test columns: impact of training set on test performance 
% Table~\ref{table:comment_gen_main_results} presents our experimental results. Overall, the performance of models improves by training on clean datasets, leading to 4.2\%-5.4\% increase for CodeReviewer and 6.7\%-9.2\% increase for CodeT5, compared to the \textsc{Original} dataset.  This suggests that training on \textsc{Cleaned} data improves model performance despite the reduced data sizes. However, merely reducing the training data does not improve performance, as evidenced by \textsc{Original} and \textsc{Controlled} groups. This demonstrates that noisy data hinders model performance and the development of accurate comment generation models.

% Note P2: Use one paragraph to discuss Valid, implication (point to the Combined columns)
Models trained on cleaned datasets demonstrate consistent improvements in the valid subsets.
When considering both our and Tufano's valid subsets (\textbf{Valid$_{\text{Our\&Tufano}}$}), CodeReviewer exhibits substantial gains, with increases ranging from 7.5\% to 13.0\% in BLEU-4 scores. Similarly, CodeT5 achieves increases between 7.5\% and 12.4\%. 
Table~\ref{table:comment_gen_main_results} also shows a consistent increase in BLEU-4 for \textbf{Valid$_{\text{Our}}$} and \textbf{Valid$_{\text{Tufano}}$} independently.
These results strongly indicate that models trained on cleaned data generate comments that more closely align with valid human review comments.
On the other hand, the performance of \textsc{Cleaned} models on the noisy subsets is inconsistent. 
Nonetheless, these BLEU-4 score variations may not accurately reflect the true quality of generated comments due to the inherent noise in the noisy dataset. 
Therefore, we conduct a manual evaluation in RQ3.  


% Note P3: discrepancy on noisy, we cannot draw conclusions, point to RQ3 
%valid and noisy columns 
% A consistent pattern emerged across all models and metrics: performance was notably higher on \textbf{Valid$_{\text{Our\&Tufano}}$} compared to \textbf{Noisy$_{\text{Our\&Tufano}}$}. 
% A substantial discrepancy between \tvalid {} and \tnoisy {}  was observed across models and metrics, with higher performance on \textbf{Valid$_{\text{C}}$} and lower performance on \textbf{Noisy$_{\text{C}}$}. 
% For models trained on \textsc{Original} dataset, the BLEU-4 score differences were 14\% for CodeReviewer and 5\% for CodeT5, respectively.
% For models trained on cleaned datasets, the \cleangpt{} CodeReviewer, in particular, showed the largest discrepancy, with a difference of 38.4\% in BLEU-4. In addition, 

% While we observe a consistent increase in BLEU-4 scores for valid subsets, the performance of \textsc{Cleaned} models on the noisy subsets is inconsistent.
% The BLEU-4 scores on the noisy subsets decrease for CodeT5 but increase for CodeReviewer. Nonetheless, these BLEU-4 score variations may not accurately reflect the true quality of generated comments due to the inherent noise in the noisy dataset. 
% Therefore, we conduct a manual evaluation in RQ3. 
% These observations highlight the challenge of evaluating model performance using a single, potentially noisy test set, suggesting that such evaluations may not accurately reflect true model capabilities.

% This inconsistency might be due to the generated comments containing more similar stop words, as the performance on \tnoisy \ are consistently dropped when evaluated without stop words. 

% P4: Regarding the two models used for data cleaning, both achieved comparable performance. Given GPT-3.5 is commercialised, our results show the promise of open-source code LLMs like Llama3 as a tool for creating high quality dataset. 

% Regarding the two models used for data cleaning, both GPT-3.5 and Llama3 achieved comparable performance. \cleangpt{} consistently achieved the highest performance on CodeReviewer across all metrics, showing the largest gain of 13.0\% on \textbf{Valid$_{\text{C}}$} in BLEU-4. While \cleanllama{} showed the highest scores on test for BLEU-4 (W/O Stop) on CodeT5 and comparable performance gains against \cleangpt{} on \textbf{Valid$_{\text{C}}$}. These results show that while both models are effective for data cleaning. Despite GPT-3.5's commercial nature and slightly better results, the substantial improvements from the open-source Llama3 model are promising. These findings suggest that freely available models like Llama3 can be effective tools for creating high-quality datasets, potentially offering a cost-effective alternative for researchers and developers.
% in code analysis and generation tasks.




\begin{tcolorbox}[size=title]
{\textbf{Answer to RQ2:}}
Despite the training size reduction, the performance of comment generation models is improved when using cleaned datasets.
% Removing noisy comments from training data improves model performance, even with a substantial reduction in training data.} 
% Models fine-tuned on the datasets cleaned by our LLM-based approach outperform those trained on original data, with 7.5-13\% increases in BLEU-4 scores on the test set. 
Specifically, the cleaned models perform consistently better on valid comments in test sets, leading to even higher increases in BLEU-4 scores of 13.0\% - 12.4\% compared to the original models.
% for CodeReviewer and 12.4\% for CodeT5.
% Models fine-tuned on the datasets cleaned by our LLM-based approach outperform those trained on original data, with 7.5-13\% increases in BLEU-4 scores on the test set. Specifically, the cleaned models perform consistently better on valid test sets, leading to even higher increases in BLEU-4 scores of 13.0\% for CodeReviewer and 12.4\% for CodeT5.
\end{tcolorbox}




\section{Impact on Generated Comment Quality (RQ3)}
\label{sec:noisy_impact_on_generated_comment_quality_rq3}
While BLEU-4 evaluates accuracy in terms of lexical correspondence, it does not account for the diversity in how similar intents can be expressed~\cite{Stapleton-etal-2020-humanstudy}. 
% For instance, a comment like \textit{``\$status\_allowedlist is not a good variable name. \$status\_whitelist is better."} and a generated comment suggesting \textit{``May be just \$allowed\_statuses?''} both aim to improve the variable name but differ in their wording, resulting in low BLEU-4 score. 
% This diversity in expression means that BLEU-4 might not fully capture the quality of the generated comments.
% In addition, the quality 
% .. <Motivation about the quality>...
In addition, the quality of comments extends beyond lexical similarity to human reviews, incorporating factors such as informativeness and contextual relevance to code changes are crucial for code review~\cite{Oleksii-etal-2016-CodeReviewQuality,Rahman2017Predicting}. This is particularly important given the potential noise in human review comments. Therefore, in RQ3, we evaluate the quality of generated comments.

We assess the quality based on how much information a generated comment provides, and how relevant the comment is to the code change \cdiff{} (Sec~\ref{ssec:comments_quality_aspects}). We compare the generations of the models trained by the \textsc{Original} and \textsc{Cleaned} datasets. We focus the CodeReviewer models in this RQ given their superior performance over the CodeT5 models on comment generation. Our analysis focuses on comments generated by three model variants, i.e., trained by the \textsc{Original} dataset and two cleaned versions —\cleangpt{} and \cleanllama{} datasets — to evaluate comment quality.

We conduct a two-fold evaluation: (a) a manual evaluation of a sampled subset of  generated comments (Sec~\ref{ssec:manual_quality_evaluation}); and (b) an overall quality evaluation of the entire test set (Sec~\ref{ssec:overall_quality_estimation}).


\subsection{Comments Quality Measures} 
\label{ssec:comments_quality_aspects}
% following similar definitions used in the CodeReviewer paper \cite{Li2022CodeReviewer}.
Following the definitions used in the CodeReviewer paper~\cite{Li2022CodeReviewer}, we evaluate the information and relevance of generated comments as follows.

For \textbf{information}, we evaluate how informative the comment is for the code author to revise the code change. 
Each comment will be labeled with an information score of one to five, where five indicates very informative.
For example, comments explicitly point out the issues and provide concrete suggestions (e.g., \textit{``Shouldn't this be an assert instead of a throw?''}) will have a higher information score than those purely seeking clarification (e.g., \textit{``Why do we need to change this?''}).  

For \textbf{relevance}, we evaluate to what extent the review comment is related to the corresponding code change. Each comment will be labeled with a relevance score of one to three, where three indicates high relevance. Comments that explicitly point out the location of issues in the code changes will receive high relevance scores, while comments that implicitly indicate the issue location or are not related to the code changes will receive low relevance scores. 
% Note that we refer to relevance as whether an issue pointed out in a comment can be mapped to an explicit location in the code patch. 
Note that we did not evaluate the logical correctness of comments in our relevance metric. This aspect often requires context beyond the code change and lacks a definitive ground truth which is prone to uncertainty. Thus, we focus on objective factors such as the explicitness of issue location within the code change. 




\subsection{Manual Evaluation} 
\label{ssec:manual_quality_evaluation}
% p1: introduce how did we sample and label the 



% The 100 samples consist of 50 human reviews labeled as \tvalid{} and 50 labeled as \tnoisy{}.
We conducted a manual evaluation on a sampled subset of the generated comments.
We randomly sampled 100 instances from our entire labeled test set.
Each instance received three comments generated by the three model variants (\textsc{Original}, \cleangpt, and \cleanllama{}). Thus, we evaluated a total of 300 generated comments. 
To ensure a reliable assessment of comment quality across our sample, the evaluations were manually conducted in two rounds.  
Initially, Annotators 1 and 2 independently evaluated 50 generated comments for both information and relevance according to the guidelines with definitions for each score.\footnote{See information and relevance definitions in our replication package~\cite{our_replication_package}.}
The first round of annotation achieved a Cohen's kappa of 0.71 (substantial agreement) for information and 0.42 (moderate agreement) for relevance. 
Following a discussion to resolve disagreements, a second round of independent annotation of another 50 comments was conducted for relevance, improving the Cohen's kappa to 0.60 (substantial agreement). 
Annotators 1 and 2 annotated the remaining 200 comments independently and then discussed to resolve remaining disagreements. 

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/information_score_distribution_overall.pdf}
        \caption{Information score (manual)}
        \label{fig:information_score_distribution_overall}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/relevance_score_distribution_overall.pdf}
        \caption{Relevance Score (manual)}
        \label{fig:relevance_score_distribution_overall}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topic_information_score_distribtuion.pdf}
        \caption{Information Score (overall)}
        \label{fig:topic_information_score_distribtuion}
    \end{subfigure}
     \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topic_relevance_score_distribtuion.pdf}
        \caption{Relevance Score (overall)}
        \label{fig:topic_information_score_distribtuion_relevance}
    \end{subfigure}
    \caption{Distribution of information and relevance scores on tests across CodeReviewer models trained on different training sets.}
    \label{fig:information_score_distributions}
\end{figure*}
\input{tables/information_relevance_scores}


\subsection{Overall Evaluation}
\label{ssec:overall_quality_estimation}

In addition to manual evaluation which is feasible only for a limited number of samples due to its labor-consuming nature, we conducted a semi-automated method to estimate the information and relevance of generated comments for the entire test setTo do so, we employed topic modeling to cluster comments generated by each model,
% comments generated by each of the three models, 
then we manually annotated the information and relevance scores for each cluster.

\textbf{Topic Modeling.} We employed BERTopic~\cite{grootendorst2022bertopic}, a widely adopted topic modeling technique that outperforms traditional methods like LDA, to extract meaningful clusters from the generated comments. BERTopic is an embedding-based approach that leverages a transformer-based model to represent each comment as a contextual embedding and applies clustering to these embeddings. This method effectively captures semantic similarities in comments and group similar comments in clusters. 
To generate embeddings, we used the recent code model, CodeT5+~\cite{wang-etal-2023-codet5+}, which performs effective bi-modal representation tasks involving both code and natural language. For the clustering model, we used agglomeration hierarchical clustering, which assigns each comment into its own cluster and iteratively merges the closest pairs of clusters until a stopping criterion is met.

We measured the cluster quality using the mean coherence score~\cite{roder-etal-2015-topic-coherence}, which measures how semantically similar the comments within a topic are to each other. 
%We selected the number of clusters at 50 which 
% balances between the manual annotation and the cluster quality.
%achieved the coherence scores for all clusters, across all three models, above 0.67. This high coherence indicates that the clusters are generally well-formed and internally consistent, regardless of which model generated the comments.
% \textcolor{red}{
We set the number of clusters to 50, which achieved coherence scores above 0.67 for all three sets of comment clusters. The high coherence indicates that the clusters are generally well-formed and internally consistent, indicating cohesive clusters for manual analysis.
% regardless of which model generated the comments.



\textbf{Annotating Comment Quality.} 
We evaluated the quality of generated comments as follows. For each cluster, BERTopic identified the top three representative comments that are most semantically similar to its cluster representation, using cTF-IDF and cosine similarity \citep{roder-etal-2015-topic-coherence}. 
Then, we annotated the information and relevance scores following our established guidelines (Sec \ref{ssec:comments_quality_aspects}). Similar to the manual evaluation, Annotators 1 and 2 independently conducted the evaluations and resolved disagreements through discussion. 
Given the relatively high coherence scores of the clusters, we considered the average information and relevance scores of the representative comments as \textit{approximated} quality scores for all comments in the corresponding cluster.
% Additionally, we assessed the relevance of each cluster by examining how explicitly the representatives pointed out the issue locations in the code changes.
% The information score, combined with the cluster size, provides an estimate of the information distribution across the test set. 



% {\color{red}TODO: justify why choosing 50  (usign elbow? or other metrics) and how is the representative docs being selected.}

\begin{figure}
    \centering
    % \begin{minipage}{0.45\textwidth}
    \includegraphics[width=\linewidth]{figures/example_generated_comments.drawio.pdf}
    % \end{minipage}
    \caption{Example comments generated by original and cleaned models with information (Info) and Relevance (Rel) scores.}
    \label{fig:example_generated_comments}
\end{figure}

\subsection{Experimental Results}
% P1: 
Table~\ref{tab:information_relevance_scores} provides the average information and relevance scores. Figure~\ref{fig:information_score_distributions} illustrates the distribution of these scores.

% P1: manual evaluation
Based on the manual evaluation of 100 sampled instances, the \textsc{Cleaned} models achieve substantial improvement over the \textsc{Original} model. 
For example, \cleangpt{} achieves a 24\% increase in information score and an 11\% increase in relevance score. 
% Similarly, \cleanllama{} achieves an 18\% increase in information score and a 10\% increase in relevance score. 
Figure~\ref{fig:information_score_distributions}~(a) and (b) show a clear trend towards higher information and higher relevance for the \textsc{Cleaned} models compared to the \textsc{Original} model. 
The most notable change is a 73-80\% decrease in low information (scores 1 and 2) and a 61-72\% decrease in low relevance (score 1) for both cleaned datasets.
These results highlight a substantial improvement in the quality of comments after cleaning.

We observe that the cleaned models tend to generate comments including code tokens related to the code change, making comments more specific and relevant to the code under review.
The examples in Figure~\ref{fig:example_generated_comments} show that a comment from the \cleangpt{} model points out an issue more directly relevant to a changed code, compared to the comment from the \textsc{Original} model.
The improved relevance likely benefits from the characteristics of valid comments (i.e., more actionable and context-aware) in the training data. 

% \input{tables/example_generated_comments}

%P2: 
The results of an overall evaluation for the whole test set align with the manual evaluation results, as shown in Table~\ref{tab:information_relevance_scores}.
% The \textsc{Cleaned} models achieve higher information and relevance scores than the \textsc{Original} model. 
For example, the approximated information and relevance scores increase by 18\% and 11\% for \cleangpt{}.
% , while \cleanllama{} increases the information score by 4\% and relevance score by 0.8\%. 
% Figure~\ref{fig:information_score_distributions}~(c) and (d) also show a clear trend towards higher information and higher relevance for the whole test set.   
% \cleangpt outperforms \cleanllama{} in both average information and relevance scores.
% \textcolor{red}{need revise:}This performance difference can be attributed to the distribution of scores (Figure~(c) and (d)). 
% \cleangpt{} comments predominantly achieve the highest scores (5 for information and 3 for relevance), while \cleanllama{} comments are concentrated in the middle range (4 for information and 2 for relevance). 
Similar to our manual evaluation, we observe a 35-66\% decrease in low information and a 55-88\% decrease in low relevance for both \textsc{Cleaned} models.
The results suggest that the cleaned datasets improve the quality of generated comments in terms of information and relevance.

% \cleanllama{} and \cleangpt achieved reductions of 37\%-58\% on information score 1 and 30\% respectively. The the improvement was even more pronounced on information score 2: \cleangpt reduced these by 30\%, whereas \cleangpt nearly eliminated them with a 95\% reduction.  Both models reducedlow-relevance comments (score 1) by 55\% to 88\%.}
% \ch{TODO: needs some re-examining of why}

%4.38 vs 3.85 
% 2.63 vs 2.38 

\begin{tcolorbox}[size=title]
{\textbf{Answer to RQ3:}}The quality of generated review comments can be improved when using our cleaned datasets. 
Our manual evaluation shows an increase up to 24\% in informativeness scores and 11\% in relevance scores. 
% The overall evaluation shows consistent results on the improvement in the quality of generated comments for the whole test set.
% This improvement is confirmed by estimates across the entire test set. The findings indicate a direct correlation between the cleanliness of the training data and the quality of generated comments: cleaner datasets yield higher-quality review comments.
\end{tcolorbox}





\section{Discussions}
\label{sec:discussion}
In this section, we discuss the benefits, limitations, and costs of using LLMs to clean the review dataset.
% discuss the potential and limitations of using LLMs to clean noisy review comments and the impact of cleaning on generated comments.
% evaluates the cost-benefit trade-offs of our noise removal process, and analyzes how cleaned datasets influence the quality and characteristics of generated review comments. 

% note: emphasize that the clean can be done by LLMs automatically 
% what kind of noise can be, what cannot be identified by LLMs; why, 
\textbf{LLMs' Capability:} \textit{To what extent LLMs can classify valid and noisy code review comments? } 
Our RQ1 has shown promising results on leveraging LLMs to classify valid and noisy comments, paving a crucial step towards automating dataset cleaning. Nonetheless, LLMs sometimes struggle to identify noisy comments. We observe that LLMs often incorrectly classify comments including domain-specific terms but do not provide improvement suggestions as valid.
For example, 
% \textit{``Fixing `lint' warnings that have existed for a while''} and 
\textit{``Why `preexec\_fn' is not set in the previous version?'' includes `preexec\_fn'.} 
% \textit{``Why preexec\_fn' is not set in the previous version?''} includes preexec\_fn'.
% justifying code changes  or comments related to clarification requests as valid. Justification-type comments typically explain the rationale for code changes without suggesting improvement, such as \textit{``Fixing `lint' warnings that have existed for a while''}. Clarification request comments like  \textit{``Why `preexec\_fn' is not set in the previous version?''}  often include code tokens (e.g., `preexec\_fn'), making LLMs misunderstand as valid. 
This may be because LLMs tend to preserve their learned knowledge (i.e., code tokens), consequently failing to adhere to the classification instruction that valid comments must explicitly address code changes.
This underscores the complexity of noisy review comments that future research can address to improve the performance of noise removal in code review datasets. {Since our current study examines models individually, future research could explore ensemble approaches that leverage common predictions across multiple models to enhance classification accuracy.}
% revealing the need for more sophisticated approaches that consider both code context and better align model judgements with criteria described in instructions. 


% We acknowledge that some comments classified as ``noisy'' may still hold value in certain contexts, particularly in the iterative nature of real-world code reviews. However, for the purpose of training automated systems to generate actionable, single-round review comments, focusing on explicit and direct feedback is essential. 
% Advocating The need for sophisticated approach to clean the datasets  

\textbf{Cost-Performance Trade-off}: \textit{What are the costs of removing noisy comments using LLMs, and what benefits can it bring?} We evaluated the efficiency of our LLM-based approach in terms of both time and cost, as well as its impact on the quality of comment generation. 
In terms of costs, GPT-3.5 required \$50 USD and 39 hours to clean the entire dataset and the open-source Llama3 took 15 hours. This cost is lower than manual annotation, which would cost \$25,600 USD based on crowdsourcing rates (\$8/hour)\footnote{\url{https://www.prolific.com/calculator}} assuming 2,000 man-hours (one minute/comment) to annotate the entire training and validation sets.
% Assume 40 people for 3000 mins to annotate 122058 comments, i.e., 1 min/person/comment

% using an NVIDIA H100 GPU with a batch size 16.  
Given the cost of LLMs, the benefits of cleaned datasets are substantial.
RQ2 shows that the cleaned models achieve an 13\% and 12.4\% increase in BLEU-4 for valid comments and RQ3 shows that the quality of the generated comments substantially increases.
% Although GPT-3.5 slightly outperforms in identifying valid comments, Llama3 presents a compelling, cost-effective alternative for cleaning. 
% , with the specialized CodeReviewer model achieving a 4.2\%-5.4\% increase in BLEU-4 score, while the general-purpose CodeT5 model showed an even larger increase of 6.7\%-9.2\%.
Moreover, we observe that the general-purpose CodeT5 model with a cleaned dataset achieved comparable performance (BLEU-4 of 5.67) to the original CodeReviewer (BLEU-4 of 5.73) while using far fewer resources. CodeReviewer is a code-review specific model which was further pre-trained on CodeT5 with 463.2GB of code review data with 2,481k comments over 250k steps. In contrast, the CodeT5 model trained on \cleangpt{} used only 39k comments (98.4\% fewer) and 7k training steps (97.2\% fewer). 
% This means the cleaned CodeT5 model reached similar performance as original CodeReviewer using 97.2\% fewer training steps and 98.4\% less data. 
This highlights the benefits of high-quality data for model efficiency, potentially reducing computational costs and environmental impact in large-scale training.

% we observed a 6.6\%-13\% improvement in model performance when evaluated on valid test set ( Table~\ref{table:comment_gen_main_results}), up to 24\% increase in comment informativeness, and 11\% in relevance (Table~\ref{tab:information_relevance_scores}). 
% Our findings strongly suggest that the benefits of noise removal is clear. This analysis offers valuable insights for the impact of dataset quality in automated code review systems. 
% \ch{discuss the reduced dataset size but higher performance}

% How do review comments characteristics change when models are trained on cleaner dataset?
% \textbf{Impact of Dataset Cleaning on Generated Review Comments} \textit{What are they characteristics of comments generated from cleaned models?} Our results in Table~\ref{table:comment_gen_main_results} and Figure~\ref{fig:information_score_distributions} shows that comments generated by \textsc{Cleaned} models are more aligned with human reviews, and exhibit higher levels of informativeness and relevance. During our manual inspection of generated comments, we observed a consistent pattern: these comments tend to share more code pieces with the original code patch, indicating a closer association between the models and the code under review. Table~\ref{tab:code_reviews_generated_comments} illustrates this improvement with two examples, comparing comments generated by the \textsc{Original} model to those from the \textsc{Cleaned} models. 
% For example, a comment from \cleangpt{} is \textit{Shouldn’t this be ‘Task StartAsync(object hint =
% null)‘?}, which points out an issue more directly relevant to a changed code line \lstinline|+ Task Start(object hint = null);|, compared to the  comment from the \textsc{Original} model \textit{why is this change needed?}.
% This increased alignment with the code patch suggests that cleaned datasets enable models to generate comments that are more specific and relevant to the code under review. The improved relevance likely benefits from the characteristics of valid comments (i.e., more actionable and context-aware) in the training data. Despite these improvements, we also observed instances of potential hallucination, where models fabricated non-existent code tokens by combining existing ones in the code patch. This suggests the importance of future work on developing more comprehensive evaluation metrics to assess both relevance and factual correctness, as well as advanced training strategies to reduce hallucinations.

% \input{tables/example_generated_comments}


% \textbf{Limitations}


% \ch{TODO:add one/two examples to show the differences of comments generated from original model and clean model}




\section{Threats to Validity}
\label{sec:threats_to_validity}

% definition of Valid and Noisy 
% focus on chunk level of code review 

\textbf{Construct Validity.}
% \textcolor{red}{we don't consider the usefulness or applications of comments. to indicate the scope of 'valid'}.
{We define `valid' comments as those that are non-noisy (i.e., not vague, difficult to understand, or seeking clarification). It is possible that these valid comments can be technically incorrect or considered not useful by practitioners. However, assessing such technical correctness requires project-specific expertise to validate the comments.} The classification performance of LLMs may vary with different prompts and hyperparameter settings. 
Different prompt strategies and LLMs might yield different results.
% In addition, other or newer LLMs may also yield different performance.
However, our primary goal was not to find the best LLMs nor optimize their hyperparameter settings, but to investigate the feasibility of automatically cleaning the review data using LLMs.
In addition, we only evaluate the quality of generated comments in terms of informativeness and relevance. 
There might be a risk of incorrectness, where models fabricated non-existent code tokens by combining existing ones in the code change.
However, evaluating correctness and hallucination is a non-trivial and manual-intensive task that requires a deep understanding of the system and code change.  
% In this work, we consider actionable and improvement-oriented comments as valid. 
% The classification performance of LLMs might be different if the definition focuses on other aspects. 
% Nevertheless, our definition 
% is based on characteristics of useful comments perceived by practitioners~\cite{Bacchelli-etal-2013-Expectations,bosu2015characteristics,turzo}, widely adopted in prior work \cite{meyers-etal-2018-dataset,Rahman2017Predicting} and corresponds with established taxonomies for comment type classification \cite{Mantyla-Lassenius-2009-TypesofDefects,tufano2024code}. 
% the study of comment usefulness \cite{meyers-etal-2018-dataset}.

% \ch{add more practitioner papers; actionable feedback}
% Therefore, we believe our approach provides a solid foundation for understanding the impact of comment quality on code review automation. While alternative definitions might yield different results, by focusing on actionable and improvement-oriented comments, our definition directly addresses the primary goal of code reviews: improving code quality. 
% This practical orientation [cite] strengthens the applicability of our results to real-world code review processes and automated review systems.

% \ch{metric for measuring the concept: }
% \cite{guo2023exploring}u sed similar definitions in code refinement tasks, showing that providing more relevant and informative comments can improve model performance on refining code.



\textbf{Internal Validity.} The manual labeling and evaluation are subject to cognitive biases.
To mitigate these, the annotations were conducted independently, and inter-rater agreements were measured.
In addition, the results were reviewed blindly without knowing which models generated the comments to ensure that the quality scores do not purposefully favor any particular models.
% In RQ2, our evaluation of model performance on the overall test set potentially introduces confounding factors, as it contains both valid and noisy comments. To mitigate this threat, we conducted additional evaluation on manually labeled valid and noisy subsets to disentangle the impact of noisy comments. However, we acknowledge that the limited size of these subsets could potentially affect the precision and range of our observed performance scores. Future work could benefit from larger-scale manual validation to further strengthen the reliability of these results.
%We use topic modeling to cluster generated comments then assign quality scores based on a subset of comments from each cluster. 
%It is possible that the scores may not be accurate.
%Nevertheless, this overall evaluation is consistent with the manual evaluation of the samples.
%It is important to note that this evaluation is for approximation to complement our manual evaluation.
% \textcolor{red}{
{While the annotation of test samples by us and Tufano et al. could potentially be different, merging these two samples is reasonable as (1) the noisy/valid definitions and the criteria are derived from both Tufano's and other works, and (2) we evaluate models separately on each labeled test set and on the combined set.}
We use topic modeling to cluster the generated comments, and then assign quality scores based on subsets of each cluster. 
It is possible that the scores are inaccurate.
Nevertheless, this overall evaluation is consistent with the manual evaluation of the samples.
It is important to note that this evaluation is just an approximation to complement our manual evaluation. {LLMs used for classification in this work were trained on GitHub data. Thus, they may be susceptible to data leakage. 
Nevertheless, they were not specifically trained to classify noisy code review comments. Therefore, we believe the impact of potential data leakage on our classification task is minimal.}

% }
% comment quality for the entire test set is an approximation. 

% However, our manual evaluation on the sampled subset provides an accurate evaluation of the quality of generated comments. The alignment between the manual evaluation and the estimated results shows the validity of our estimation.

% \textcolor{red}{RQ3: It is important to note that this evaluation is an approximation as it does not consider the full code context and relies on a subset of comments from each cluster.
% Nonetheless, the key goal of this analysis is to complement our manual evaluation of samples.}
% The manual labeling process for the subset used in RQ1 involved subjective judgements. Although we used two annotators and resolved disagreements, there's still a possibility of inconsistency.


% remove noisy for training? 

% Our approach of removing noisy instances from the training set resulted in a reduction of dataset size, which could potentially confound the effects of noise removal on model performance. To address this, we introduced controlled groups in our experiments, randomly sampled to match the size of the cleaned datasets. This design allows us to differentiate between the impact of noise removal and the effect of reduced dataset size. By comparing performance across original, cleaned, and size-controlled datasets, we mitigate the risk of misattributing improvements solely to noise removal when dataset size reduction might also play a role. While this approach strengthens our findings, we acknowledge that other confounding variables may still influence our results, such as variations in programming language distribution, code change complexity and review styles.

% \textcolor{red}{Results applied to the other models. We selected these models for their publicly available checkpoints, ensuring reproducibility, while excluding alternatives like AUGER \cite{auger}, CCT5 \cite{CCT5}, and DISCOREV \cite{sghaier24} due to replication constraints.}
% \textcolor{red}{CodeReviewer stood out by demonstrating comparable performance to Google's practical code review assistant in real-world scenarios \cite{frommgen2024resolving} and outperforming its descendant LLaMA-Reviewer \cite{LLaMA-Reviewer} in our preliminary tests.}

\textbf{External Validity}
Our study is based on two widely-known models (i.e., CodeT5 and CodeReviewer). 
The findings may not generalize to other code review models and datasets.
% Unfortunately, other existing code review models~\cite{auger,CCT5,sghaier24, LLaMA-Reviewer} cannot be replicated due to missing model checkpoints or fine-tuning scripts. 
{Unfortunately, other existing code review models are not suitable for our study for various reasons, for example, some models~\cite{CCT5,sghaier24, LLaMA-Reviewer} cannot be replicated due to missing model checkpoints or fine-tuning scripts, while ~\cite{auger} was pretrained only with Java examples, which is not comparable with CodeReviewer's multi-language dataset.} {While we acknowledge that newer models continue to emerge, our primary objective was to demonstrate the fundamental impact of data quality on code review generation, rather than achieving state-of-the-art performance. The consistent improvements we observed across both models suggest that our data cleaning approach offers benefits that likely extend beyond a specific model. Furthermore, our study on identifying high-quality datasets remains valuable for emerging LLM-based code review systems in several key ways: (1) enhancing retrieval-augmented generation by validating the quality of retrieved examples, and (2) improving benchmark dataset quality by providing more reliable test sets for model evaluation.}
% Nevertheless, these approaches use datasets mined from the same source, i.e., GitHub, which potentially include noise like CodeReviewer dataset.
Therefore, we believe our key finding — high-quality datasets improve comment generation models — remains valid and applicable to a broad range of code review contexts.





\section{Conclusion}
\label{sec:conclusion}
In this paper, we address the critical issue of data quality in code review automation. 
We explore a novel method leveraging large language models (LLMs) to identify and remove noisy comments.
% , leading to substantial improvements in the quality of generated review comments. Our findings reveal that approximately 36\% of comments in the CodeReviewer dataset are noisy, and we demonstrated 
% \textcolor{red}{Our results show that LLMs can effectively identify 11\%-21\% of noisy comments out of 36\%.} 
Our results show that LLMs can achieve 66-85\% precision in identifying valid comments, improving the proportion of valid comments from 64\% in the original dataset to up to 85\% in our cleaned datasets. By training code review models on cleaned datasets, we observe substantial improvements in review comment generation quality, with up to a 13\% increase in BLEU-4 scores and a 24\% improvement in informativeness. 
% These results highlight the importance of dataset quality in developing effective code review automation tools. 
Our work demonstrates the feasibility of automatically cleaning review datasets and offers insights into how data quality affects model performance in automated code review. Future research could investigate advanced cleansing techniques for complex comments.

\section{Acknowledgement}
This research was supported by The University of Melbourne’s Research Computing Services and the Petascale Campus Initiative. Patanamon Thongtanunam was supported by the Australian Research Council’s Discovery Early Career Researcher Award (DECRA) funding scheme (DE210101091).

% Our work sheds light on the feasibility of automatically cleaning the review dataset, providing valuable insights into the impact of the data quality and model performance in the context of automated code review. Future research could explore more advanced data cleansing techniques for complex code review comments.

\bibliographystyle{IEEEtran}
\bibliography{reference.bib}



\end{document}
