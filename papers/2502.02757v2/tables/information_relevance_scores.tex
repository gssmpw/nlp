\begin{table}[t!]
% \small 
\setlength{\tabcolsep}{4pt}
\centering
\caption{\label{tab:information_relevance_scores}Information and Relevance Scores for Different Training Sets on CodeReviewer model.}
\begin{tabular}{l|ll|ll}
\toprule
\multirow{2}{*}{\textbf{Training set}} & \multicolumn{2}{c|}{\textbf{Manual}} & \multicolumn{2}{c}{\textbf{Overall}}  \\
% \cline{2-8}
% \midrule
\cmidrule(lr){2-5} %\cmidrule(lr){5-8}
&  Information & Relevance & Information & Relevance \\
\midrule
\textsc{Original} & 3.44 &  2.48 & 3.69  & 2.36 \\
$\textsc{Cleaned}_{\textsc{GPT-3.5}}$ & \textbf{4.27}$^{*}$ & \textbf{2.77}$^{*}$ & \textbf{4.38}$^{*}$  & \textbf{2.63}$^{*}$\\
$\textsc{Cleaned}_{\textsc{Llama3}}$  & 4.08$^{*}$ & 2.72$^{*}$ & 3.85$^{*}$ &  2.38\\
\bottomrule
\multicolumn{5}{p{8.5cm}}{\footnotesize Information scores range from 1-5; relevance scores from 1-3. The $^*$ indicates that the score from cleaned model is significantly higher from the original model (p-value $<$0.05 using Wilcoxon signed rank tests).}
\end{tabular}
\end{table}


 