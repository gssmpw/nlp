% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{Gusfield:97,
  author  = {Dan Gusfield},
  title   = {Algorithms on Strings, Trees, and Sequences},
  year    = {1997},
  journal = {Cambridge University Press}
}

// Controllable Expressive Speech Generation

// Text-to-Speech

@article{yang2024instructtts,
  title={Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt},
  author={Yang, Dongchao and Liu, Songxiang and Huang, Rongjie and Weng, Chao and Meng, Helen},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2024},
  publisher={IEEE}
}

@inproceedings{guo2023prompttts,
  title={Prompttts: Controllable text-to-speech with text descriptions},
  author={Guo, Zhifang and Leng, Yichong and Wu, Yihan and Zhao, Sheng and Tan, Xu},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{leng2023prompttts,
  title={Prompttts 2: Describing and generating voices with text prompt},
  author={Leng, Yichong and Guo, Zhifang and Shen, Kai and Tan, Xu and Ju, Zeqian and Liu, Yanqing and Liu, Yufei and Yang, Dongchao and Zhang, Leying and Song, Kaitao and others},
  journal={arXiv preprint arXiv:2309.02285},
  year={2023}
}

@inproceedings{shimizu2024prompttts++,
  title={PromptTTS++: Controlling speaker identity in prompt-based text-to-speech using natural language descriptions},
  author={Shimizu, Reo and Yamamoto, Ryuichi and Kawamura, Masaya and Shirahata, Yuma and Doi, Hironori and Komatsu, Tatsuya and Tachibana, Kentaro},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={12672--12676},
  year={2024},
  organization={IEEE}
}

@article{liu2023promptstyle,
  title={Promptstyle: Controllable style transfer for text-to-speech with natural language descriptions},
  author={Liu, Guanghou and Zhang, Yongmao and Lei, Yi and Chen, Yunlin and Wang, Rui and Li, Zhifei and Xie, Lei},
  journal={arXiv preprint arXiv:2305.19522},
  year={2023}
}

@inproceedings{ji2024textrolspeech,
  title={Textrolspeech: A text style control speech corpus with codec language text-to-speech models},
  author={Ji, Shengpeng and Zuo, Jialong and Fang, Minghui and Jiang, Ziyue and Chen, Feiyang and Duan, Xinyu and Huai, Baoxing and Zhao, Zhou},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={10301--10305},
  year={2024},
  organization={IEEE}
}

@article{du2024cosyvoice,
  title={Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens},
  author={Du, Zhihao and Chen, Qian and Zhang, Shiliang and Hu, Kai and Lu, Heng and Yang, Yexin and Hu, Hangrui and Zheng, Siqi and Gu, Yue and Ma, Ziyang and others},
  journal={arXiv preprint arXiv:2407.05407},
  year={2024}
}

@misc{lyth2024natural,
      title={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},
      author={Dan Lyth and Simon King},
      year={2024},
      eprint={2402.01912},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

// ParlerTTS
@misc{lacombe-etal-2024-parler-tts,
  author = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},
  title = {Parler-TTS},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/parler-tts}}
}

// Voice Conversion
@inproceedings{kuan2023towards,
  title={Towards General-Purpose Text-Instruction-Guided Voice Conversion},
  author={Kuan, Chun-Yi and Li, Chen-An and Hsu, Tsu-Yuan and Lin, Tse-Yang and Chung, Ho-Lam and Chang, Kai-Wei and Chang, Shuo-Yiin and Lee, Hung-yi},
  booktitle={2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}

@article{niu2024hybridvc,
  title={HybridVC: Efficient Voice Style Conversion with Text and Audio Prompts},
  author={Niu, Xinlei and Zhang, Jing and Martin, Charles Patrick},
  journal={arXiv preprint arXiv:2404.15637},
  year={2024}
}


// Pre-trained Classification Models

// emotion2vec for emotion recognition
@article{ma2023emotion2vec,
  title={emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation},
  author={Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie},
  journal={arXiv preprint arXiv:2312.15185},
  year={2023}
}

// gender recognition
@inproceedings{burkhardt2023speech,
  title={Speech-based Age and Gender Prediction with Transformers},
  author={Burkhardt, Felix and Wagner, Johannes and Wierstorf, Hagen and Eyben, Florian and Schuller, Bj{\"o}rn},
  booktitle={Speech Communication; 15th ITG Conference},
  pages={46--50},
  year={2023},
  organization={VDE}
}

// wav2vec 2.0
@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

// Whisper
@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

// Dataset and Benchmarl.
// EXPRESSO
@inproceedings{nguyen2023expresso,
  title={Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis},
  author={Nguyen, Tu Anh and Hsu, Wei-Ning and d'Avirro, Antony and Shi, Bowen and Gat, Itai and Fazel-Zarani, Maryam and Remez, Tal and Copet, Jade and Synnaeve, Gabriel and Hassid, Michael and others},
  booktitle={INTERSPEECH 2023},
  pages={4823--4827},
  year={2023},
  organization={ISCA}
}

//
@inproceedings{zhao2018gender,
  title={Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  pages={15--20},
  year={2018}
}

// GPT-4o
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

//
@article{bansal2022well,
  title={How well can text-to-image generative models understand ethical natural language interventions?},
  author={Bansal, Hritik and Yin, Da and Monajatipoor, Masoud and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2210.15230},
  year={2022}
}

// chi-square test.
@Inbook{Pearson1992,
author="Pearson, Karl",
title="On the Criterion that a Given System of Deviations from the Probable in the Case of a Correlated System of Variables is Such that it Can be Reasonably Supposed to have Arisen from Random Sampling",
bookTitle="Breakthroughs in Statistics: Methodology and Distribution",
year="1992",
publisher="Springer New York",
address="New York, NY",
pages="11--28",
abstract="Let x1, x2 {\ldots} xnbe a system of deviations from the means of n variables with standard deviations $\sigma$1, $\sigma$2 {\ldots} $\sigma$nand with correlations r12, r13, r23 {\ldots} rnâˆ’1,n.",
isbn="978-1-4612-4380-9",
doi="10.1007/978-1-4612-4380-9_2",
url="https://doi.org/10.1007/978-1-4612-4380-9_2"
}

// Part of Datasets.
// LibriTTS-R Dataset.
@ARTICLE{Koizumi2023-hs,
  title         = "{LibriTTS-R}: A restored multi-speaker text-to-speech corpus",
  author        = "Koizumi, Yuma and Zen, Heiga and Karita, Shigeki and Ding,
                   Yifan and Yatabe, Kohei and Morioka, Nobuyuki and Bacchiani,
                   Michiel and Zhang, Yu and Han, Wei and Bapna, Ankur",
  month         =  may,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2305.18802",
journal={arXiv preprint arXiv:2305.18802}
}

// MLS Dataset.
@article{pratap2020mls,
  title={Mls: A large-scale multilingual dataset for speech research},
  author={Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
  journal={arXiv preprint arXiv:2012.03411},
  year={2020}
}

//aGender Dataset.
@inproceedings{burkhardt2010database,
  title={A Database of Age and Gender Annotated Telephone Speech},
  author={Burkhardt, Felix and Eckert, Martin and Johannsen, Wiebke and Stegmann, Joachim},
  booktitle={Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10)},
  year={2010}
}

// CommonVoice Dataset.
@inproceedings{commonvoice:2020,
  author = {Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G.},
  title = {Common Voice: A Massively-Multilingual Speech Corpus},
  booktitle = {Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)},
  pages = {4211--4215},
  year = 2020
}

// TIMIT
@misc{garofolo1993timit,
  title = {TIMIT Acoustic-Phonetic Continuous Speech Corpus},
  author = {John S. Garofolo and Lori F. Lamel and William M. Fisher and Jonathan G. Fiscus and David S. Pallett and Nancy L. Dahlgren and Victor Zue},
  year = {1993},
  publisher = {Linguistic Data Consortium, Philadelphia},
  howpublished = {Web Download},
  doi = {10.35111/17gk-bn40},
  note = {LDC93S1, ISBN: 1-58563-019-5}
}

// VoxCeleb 2
@article{chung2018voxceleb2,
  title={VoxCeleb2: Deep speaker recognition},
  author={Chung, J and Nagrani, A and Zisserman, A},
  journal={Interspeech 2018},
  year={2018},
  publisher={International Speech Communication Association}
}

