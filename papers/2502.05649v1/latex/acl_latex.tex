% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[final]{acl}
% \usepackage[preprint]{acl}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{cite}
\usepackage{natbib}
\usepackage{multicol} % For two-column layout
\usepackage{tabularx} % For table with adjustable width
\usepackage{lipsum}   % Dummy text
\definecolor{oldlace}{HTML}{FFF4E4}
\definecolor{vanilla}{HTML}{F7E8A4}
\definecolor{mistryrose}{HTML}{FFEBE7}
\definecolor{tearose}{HTML}{E9BCB7}
\definecolor{uranianblue}{HTML}{ABDAFC}
\definecolor{cambriggeblue}{HTML}{8FC0A9}
\definecolor{azure}{HTML}{E5FCFF}
\definecolor{teacolor}{HTML}{E2F5D7}
\definecolor{oldrose}{HTML}{F2D4D5}
\definecolor{frenchgray}{HTML}{E7E9EC}
\usepackage{ulem}
\usepackage{xurl}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{diagbox}
\makeatletter
\newcommand{\hlc}[2][yellow]{{%
  \colorlet{foo}{#1}%
  \sethlcolor{foo}\hl{#2}}%
}
\makeatother
\usepackage{booktabs}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{color}
\usepackage{cleveref}

\definecolor{blue1.0}{HTML}{E0F7FA}
\definecolor{blue2.0}{HTML}{B3E5FC}
\definecolor{blue3.0}{HTML}{81D4FA}
\definecolor{blue4.0}{HTML}{4FC3F7}
\definecolor{blue5.0}{HTML}{29B6F6}
\definecolor{blue6.0}{HTML}{03A9F4}
\definecolor{blue7.0}{HTML}{039BE5}
\definecolor{blue8.0}{HTML}{0288D1}
\definecolor{blue9.0}{HTML}{0277BD}
\definecolor{blue10.0}{HTML}{01579B}

\definecolor{blue1}{HTML}{E0F7FA}
\definecolor{blue2}{HTML}{B3E5FC}
\definecolor{blue3}{HTML}{81D4FA}
\definecolor{blue4}{HTML}{4FC3F7}
\definecolor{blue5}{HTML}{29B6F6}
\definecolor{blue6}{HTML}{03A9F4}
\definecolor{blue7}{HTML}{039BE5}
\definecolor{blue8}{HTML}{0288D1}
\definecolor{blue9}{HTML}{0277BD}
\definecolor{blue10}{HTML}{01579B}

\definecolor{orange1.0}{HTML}{FFF3E0}
\definecolor{orange2.0}{HTML}{FFE0B2}
\definecolor{orange3.0}{HTML}{FFCC80}
\definecolor{orange4.0}{HTML}{FFB74D}
\definecolor{orange5.0}{HTML}{FFA726}
\definecolor{orange6.0}{HTML}{FF9800}
\definecolor{orange7.0}{HTML}{FB8C00}
\definecolor{orange8.0}{HTML}{F57C00}
\definecolor{orange9.0}{HTML}{EF6C00}
\definecolor{orange10.0}{HTML}{E65100}

\definecolor{orange1}{HTML}{FFF3E0}
\definecolor{orange2}{HTML}{FFE0B2}
\definecolor{orange3}{HTML}{FFCC80}
\definecolor{orange4}{HTML}{FFB74D}
\definecolor{orange5}{HTML}{FFA726}
\definecolor{orange6}{HTML}{FF9800}
\definecolor{orange7}{HTML}{FB8C00}
\definecolor{orange8}{HTML}{F57C00}
\definecolor{orange9}{HTML}{EF6C00}
\definecolor{orange10}{HTML}{E65100}

\definecolor{green1}{HTML}{E8F5E9}
\definecolor{green2}{HTML}{C8E6C9}
\definecolor{green3}{HTML}{A5D6A7}
\definecolor{green4}{HTML}{81C784}
\definecolor{green5}{HTML}{66BB6A}
\definecolor{green6}{HTML}{4CAF50}
\definecolor{green7}{HTML}{43A047}
\definecolor{green8}{HTML}{388E3C}
\definecolor{green9}{HTML}{2E7D32}
\definecolor{green10}{HTML}{1B5E20}

\definecolor{purple1}{HTML}{F3E5F5}
\definecolor{purple2}{HTML}{E1BEE7}
\definecolor{purple3}{HTML}{CE93D8}
\definecolor{purple4}{HTML}{BA68C8}
\definecolor{purple5}{HTML}{AB47BC}
\definecolor{purple6}{HTML}{9C27B0}
\definecolor{purple7}{HTML}{8E24AA}
\definecolor{purple8}{HTML}{7B1FA2}
\definecolor{purple9}{HTML}{6A1B9A}
\definecolor{purple10}{HTML}{4A148C}

\definecolor{red1}{HTML}{FFEBEE}
\definecolor{red2}{HTML}{FFCDD2}
\definecolor{red3}{HTML}{EF9A9A}
\definecolor{red4}{HTML}{E57373}
\definecolor{red5}{HTML}{EF5350}
\definecolor{red6}{HTML}{F44336}
\definecolor{red7}{HTML}{E53935}
\definecolor{red8}{HTML}{D32F2F}
\definecolor{red9}{HTML}{C62828}
\definecolor{red10}{HTML}{B71C1C}


\definecolor{lightgray}{HTML}{D3D3D3}
% \usepackage[table]{xcolor} % 控制表格背景顏色
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{GenBias: Unmasking Gender Stereotypes in Text-Guided Speech Generation}
% \title{Gender Stereotypes in Instruction-Guided Speech Synthesis Models}
\title{Gender Bias in Instruction-Guided Speech Synthesis Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Chun-Yi Kuan \\
  National Taiwan University \\
  Taiwan \\
  \texttt{chunyi.kuan.tw@gmail.com} \\\And
  Hung-yi Lee \\
  National Taiwan University \\
  Taiwan \\
  \texttt{hungyilee@ntu.edu.tw} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Recent advancements in controllable expressive speech synthesis, especially in text-to-speech (TTS) models, have allowed for the generation of speech with specific styles guided by textual descriptions, known as style prompts. 
While this development enhances the flexibility and naturalness of synthesized speech, there remains a significant gap in understanding how these models handle vague or abstract style prompts. 
This study investigates the potential gender bias in how models interpret occupation-related prompts, specifically examining their responses to instructions like ``Act like a nurse''. We explore whether these models exhibit tendencies to amplify gender stereotypes when interpreting such prompts.
% Our experiments involve generating speech with prompts related to gender-stereotyped occupations, such as nurses and engineers, while keeping the content of the speech neutral. 
% The findings highlight the importance of identifying hidden biases in TTS models to promote the development of fair and inclusive voice technologies.
Our experimental results reveal the model's tendency to exhibit gender bias for certain occupations. 
Moreover, models of different sizes show varying degrees of this bias across these occupations.
% Our experimental results reveal the model's tendency to over-represent or under-represent a particular gender for certain occupations. 
% Models of different sizes show varying degrees of bias for certain occupations.
% As the number of model parameters increases, the gender bias becomes more pronounced.
% , suggesting that these models may amplify gender stereotypes when interpreting such prompts.
% 模型大小對結果的影響
\end{abstract}

\section{Introduction}

Recent years have witnessed significant advancements in controllable expressive speech synthesis technology, particularly in text-to-speech (TTS) models. 
These models have shown remarkable ability to generate speech with specific styles based on textual descriptions, known as style prompts. 
For instance, a prompt like ``speak in a cheerful boyish voice'' can guide the model to produce speech with corresponding characteristics. 
This development has opened new possibilities in natural language processing and artificial intelligence, offering more nuanced and context-appropriate speech output.
% 該如何轉折

\input{figures/overview}

However, a critical gap exists in our understanding of these models' behavior when faced with vague or abstract style prompts. 
Of particular concern is the potential for these models to exhibit or amplify gender biases, a problem that remains largely unexplored. 
This study aims to address this gap by investigating how these models interpret and respond to ambiguous style prompts, with a specific focus on gender bias manifestation.
% Our research centers on two key questions.
% First, we investigate how these models interpret and create speech styles when given occupation-related prompts like ``Act like a nurse''. 
% % occupation-related prompts
% Second, we examine whether the models show specific tendencies, especially gender bias, in their interpretation process. 
Our research focuses on a key question: How do these models interpret and generate speech styles when given occupation-related prompts such as ``Act like a nurse''? 
Within this main inquiry, we examine whether these models display specific tendencies, particularly gender bias, in their interpretation process.
By exploring these questions, we hope to gain insights into how speech synthesis models process and respond to ambiguous style instructions and whether they exhibit any biases in doing so.
% \begin{enumerate}
%     \item {How do speech systhesis models interpret and create speech styles when given vague style prompts, such as ``Act like a nurse''?}
%     \item {Do these models exhibit specific tendencies, particularly gender bias, in their interpretation process?}
%     % \item {In the absence of explicit style guidance, how do content prompts influence the style of generated speech?}
% \end{enumerate}

To explore these questions, we design style prompt experiments\footnote{\href{https://github.com/kuan2jiu99/GenderBias-TTS-Dataset}{github.com/kuan2jiu99/GenderBias-TTS-Dataset}}, as shown in Fig.~\ref{fig:overview}. 
We create a series of prompts in the format like \textit{Act like a <specific occupation>}, choosing occupations with varied gender stereotypes, such as nurse, engineer, teacher, and police officer. 
In addition, we use several consistent and neutral content prompts, which is the content of the synthesized speech, we generate multiple speech samples, varing only the style prompts.
% Our evaluation datasets are available\footnote{\url{github.com/kuan2jiu99/audio-hallucination}}.
% Since current speech gender recognition models are trained on datasets that focus on biological sex (male or female), this study will first concentrate on these two categories.
% We focus on finding hidden gender biases in these systems to help create fairer and more inclusive voice technologies. 
% As speech synthesis becomes common in daily life, it's vital to address ethical issues. 
Our contributions are outlined as follows:
% \vspace{-5pt}
\begin{enumerate} 
    \item Investigating gender bias in instruction-guided text-to-speech models' interpretation of occupation-related prompts.
    We find that models tend to favor one gender over another for certain occupations. 
    % \vspace{-5pt}
    % \item Identifying and mitigating bias in occupational associations. 
    % % We find that models tend to favor one gender over another for certain occupations. 
    % Proposed mitigation methods reduce but do not fully eliminate these biases, sometimes leading to overcompensation. 
    \vspace{-3pt}
    \item We observe that using only prompt engineering as a mitigation method is not entirely sufficient to eliminate bias in occupational associations. 
    In some cases, this approach can even lead to overcompensation.
\end{enumerate}

\section{Related Work}

\subsection{Instruction-guided Speech Synthesis}
% Instruction-guided Speech Generation 讓傳統的 text-to-speech 和 voice conversion 等語音生成的任務有了新的面目，藉由加入 style prompt 的設定，使得生成的語音可以依靠自然語言的描述而變換風格。

% 過去很多人想要用 prompt 來減輕 bias，對 instruciton-TTS 沒辦法這麼做。
% 單純修改 prompt 並不足以消弭 bias 的問題，反而會加深。

% 在 text-to-speech 上，Q 除了原有的 systhesized content 外，也利用文字的 instruction 當作 style prompt，引導語音合成時的風格。
% 在 voice conversion 上，K 利用文字的 instruction 當作 style prompt 取代傳統 voice conversion 需要一段 reference speech 的設定。

% Instruction-guided Speech Generation 好處是能透過自然語言的彈性與豐富度，操控語音的風格。
Instruction-guided speech generation has brought a new dimension to traditional speech generation tasks like text-to-speech and voice conversion. 
By adding style prompts, we can change speech styles using natural language descriptions.
In text-to-speech systems,~\citet{yang2024instructtts, guo2023prompttts, leng2023prompttts, shimizu2024prompttts++, liu2023promptstyle, ji2024textrolspeech, du2024cosyvoice, lyth2024natural} introduce an improvement. 
Along with the usual synthesized content, they use text instructions as style prompts to guide the style of the generated speech.
For voice conversion, ~\citet{kuan2023towards, niu2024hybridvc} replaces the traditional need for a reference speech sample with text instructions as style prompts.
The main advantage of instruction-guided speech synthesis is its flexibility. 
It allows us to control speech styles using the rich and versatile nature of natural language. 
% This approach opens up new possibilities for creating diverse and customized speech outputs.
However, there has been little research exploring whether these models exhibit specific biases when generating speech based on style prompts. 
Threrefore, our work aims to provide an initial investigation into this issue.

% \subsection{Gender Bias in Generative Models}
% This is part of gender bias in generative models.

\section{Method}

\subsection{Design of Style and Content Prompt}
%In the field of instruction-guided speech synthesis, we define two types of prompts: style prompts and content prompts. 
A typical instruction-guided speech synthesis model takes two prompts as input: one is a style prompt, and the other is a content prompt.
Style prompts guide the style of the synthesized speech, such as ``speak in a sorrowful and deep voice''. 
Content prompts, on the other hand, determine what the speech will say.
In our work, we creat five style prompt templates:
(1) \texttt{Act like a <occupation>}, 
(2) \texttt{Take on the role of a <occupation>}, 
(3) \texttt{Imagine yourself as a <occupation>}, 
(4) \texttt{Think and respond like a <occupation>}, and
(5) \texttt{Do what a <occupation> would do}, where \texttt{<occupation>} can be replaced by a specific occupation.
%We also develop ten neutral content prompts using GPT-4o. 
%These prompts don't involve any specific speech style. 
For the content prompts, we have ten that do not involve any specific speech style or occupation.
% In this way, 對於每一個 style prompt template 和 neutral content prompts 的組合，我們各別生成 10 句語音，所以對於每個 occupation，總共會生成 500 句語音。
For each combination of a style prompt template and a neutral content prompt, we generate 10 speech samples. 
As a result, for each occupation, we produce a total of 500 speech samples. 
For the occupations selection, we source from~\citet{zhao2018gender} and also ask GPT-4o~\citep{achiam2023gpt} to generate possible occupations. 
In this way, we have 109 occupations in total.
The details of the process and methods mentioned above are presented in Appendix \ref{appendix:prompt_design}.

% We recognize that instruction-guided TTS models might have their own inherent bias when generating speech, even without a specific style prompt. 
% To address this, we design three control groups with different style prompts to observe the model's tendencies: 
% \textbf{Control group 1}: An empty string (no style prompt), 
% \textbf{Control group 2}: General prompts such as ``Act like a person'', ``Act like an ordinary person'', and ``Act like an average person'' and 
% \textbf{Control group 3}: Neutral sentences that don't refer to any specific speech style, emotion, or gender. For example, ``Morning dew sparkled on the grass, catching the first rays of sunlight''. The details are shown in Appendix~\ref{appendix:content_prompt}.
% % We design several candidate sentences using GPT-4o and randomly chose ten sentences as our neutral sentences for style prompts. 
% % The complete process is shown in Appendix~\ref{appendix:content_prompt}.
% When the TTS model encounters these style prompts, it lacks clear guidance for speech generation. The purpose of this process is to generate audio samples without incorporating any specific style prompts to identify inherent biases in TTS models. To examine whether occupation-related style prompts introduce gender bias, we then use these three control groups as a baseline for comparison.
% %This approach allows us to determine how a specific style prompt affects the model's speech generation tendencies compared to these neutral conditions.

% \subsection{Baseline Models}
% We select four versions of models from the \texttt{Parler-TTS} project for our study.
% Parler-TTS is developed by~\citet{lacombe-etal-2024-parler-tts}, reproducing the work originally presented by~\citet{lyth2024natural} on natural language guidance for high-fidelity text-to-speech.
% The selected models include \texttt{Parler-TTS Large v1}, \texttt{Parler-TTS Mini v1}, \texttt{Parler-TTS Mini v0.1}, and \texttt{Parler-TTS Mini Expresso}.
% These models vary in size and training data, with detailed specifications provided in Appendix~\ref{appendix:baseline_models_intro}.

% Parler-TTS Large v1 is a 2.2B-parameters Parler checkpoint, trained on 45K hours of audio data.
% Parler-TTS Mini v1 is a 880M parameters Parler checkpoint, trained on 45K hours of audio data.
% Parler-TTS Mini v0.1 is a 880M parameters Parler checkpoint, trained on 10.5K hours of audio data.
% Parler-TTS Mini Expresso is a fine-tuned version of Parler-TTS Mini v0.1 on the Expresso dataset.

\subsection{Speech Attribute Measurement}
To study whether there is bias in the generated samples, we need to automatically identify speech attributes, including gender\footnote{In our study, gender specifically refers to biological sex (male or female) due to training datasets. 
We recognize that gender identity and expression are more complex topics.}, emotion, and speaking rate, from the samples.
For gender classification, we use the speech-based gender recognition model proposed by~\citet{burkhardt2023speech}. 
This model is based on the pre-trained wav2vec 2.0~\citep{baevski2020wav2vec}. 
For speech emotion recognition, we we select the emotion2vec~\citep{ma2023emotion2vec}. 
To measure speaking speed, we focus on three metrics: phonemes , words, and syllables per second, respectively. 
More details about these models and process are in Appendix~\ref{appendix:baseline_models_intro}.
% In our study, gender prediction specifically refers to biological sex (male or female) due to the training datasets. 
% We recognize that gender identity and expression are more complex topics. 

\subsection{Control Group for Inherent Bias}
We recognize that instruction-guided TTS models might have their own inherent bias when generating speech, even without a specific style prompt. 
In addition, gender recognition systems used to analyze the speech output may also introduce bias. 
To address these potential sources of bias, we design three control groups with different style prompts:
\textbf{Control group 1}: An empty string (no style prompt), 
\textbf{Control group 2}: General prompts such as ``Act like a person'', ``Act like an ordinary person'', and ``Act like an average person'' and 
\textbf{Control group 3}: Neutral sentences that don't refer to any specific speech style, emotion, or gender. For example, ``Morning dew sparkled on the grass, catching the first rays of sunlight''. 
Details of these style prompts are provided in Appendix~\ref{appendix:prompt_design}.

These control groups serve two purposes. 
First, they generate audio samples without specific style guidance, helping identify inherent biases in TTS models.
Second, they account for potential biases in both the TTS model and the gender recognition system used for analysis.
By using these control groups as a baseline, we can better examine whether occupation-related style prompts introduce additional gender bias beyond any inherent biases in the TTS and gender recognition systems.

% Original 寫法
% When the TTS model encounters these style prompts, it lacks clear guidance for speech generation. The purpose of this process is to generate audio samples without incorporating any specific style prompts to identify inherent biases in TTS models. To examine whether occupation-related style prompts introduce gender bias, we then use these three control groups as a baseline for comparison.

\subsection{Bias Analysis}
% We analyze potential gender bias by comparing two groups: a control group (unconditional) and a group subject to a specific style prompt (conditional). 
We analyze potential gender bias by comparing two groups: a control group and a group subject to a specific style prompt. 
This approach allows us to examine how the style prompt might influence gender representation in the results.
The process begins by organizing input data, which consists of counts for male and female voices in both the control group and the style-prompted group. 
From these counts, we calculate probabilities for each category, giving us a clear picture of the gender distribution in both scenarios, where full results is demonstrated in~\Cref{appendix:results-gender-part1,appendix:results-gender-part2,appendix:results-gender-part1-content-prompt,appendix:results-gender-part2-content-prompt}.

To assess potential bias, we use a chi-square test~\citep{Pearson1992}. 
This statistical method helps us determine if there's a significant difference between the expected frequencies and the observed frequencies in our data, comparing the control group to the style-prompted group.
After performing the chi-square test, we look at the p-value. 
If it's less than 0.05, we conclude that there's a statistically significant difference in the data. 
This suggests the presence of bias introduced by the style prompt.

To understand the direction and magnitude of the bias, we examine the standardized residuals. 
These residuals show us how much the observed data in the style-prompted group deviates from what we would expect based on the control group.
A positive residual value means the observed frequency is higher than expected, while a negative value indicates it's lower. 
% The larger the absolute value of the standardized residual, the greater the difference between observed and expected frequencies.
% We compare the standardized residuals for males and females. 
% The larger absolute value indicates which gender is more affected by the bias. 
% If the residual is positive, it means that gender is overrepresented in the style-prompted group; if negative, it's underrepresented.
% The direction of bias is determined by which gender has the larger absolute residual value and whether it's positive or negative. 
% For example, if the male residual is larger and positive, we say the bias is towards males in the style-prompted condition.
% The magnitude of bias is represented by the absolute value of the larger residual. This gives us a measure of how strong the effect of the style prompt is on gender representation.
We use standardized residuals to analyze gender bias by comparing the values for males and females. 
A positive residual means that gender is overrepresented in the style-prompted group, while a negative one means it's underrepresented. 
% The direction of bias is determined by which gender has the larger absolute residual and its sign. 
The bias direction is determined by which gender has the positive residual value. 
This indicates which gender the bias favors.
For instance, if males have a positive residual, we say there's a bias towards males in the style-prompted condition. 
The magnitude of this residual indicates the strength of the bias. 
By analyzing these factors, we can provide a clear picture of whether the style prompt introduces gender bias, in which direction it leans, and how strong it is. 

\input{tables/results_bias_stat_selected}

\section{Experimental Results}

\subsection{Experimental Setups}

We select four models from the \texttt{Parler-TTS} project~\citet{lacombe-etal-2024-parler-tts}, which builds on the work of~\citet{lyth2024natural} in natural language-guided high-fidelity TTS. 
The selected models include \texttt{Parler-TTS Large v1}, \texttt{Parler-TTS Mini v1}, \texttt{Parler-TTS Mini v0.1}, and \texttt{Parler-TTS Mini Expresso}, each differing in size and training data.
\texttt{Large v1} has 2.2 billion parameters and is trained on 45,000 hours of audio. \texttt{Mini v1} has 880 million parameters and is trained on the same amount of data. \texttt{Mini v0.1} has 880 million parameters but is trained on 10,500 hours of audio. \texttt{Mini Expresso} is a fine-tuned version of \texttt{Mini v0.1} on the Expresso dataset. 
Further details are provided in Appendix~\ref{appendix:baseline_models_intro}.

% We select four versions of models from the \texttt{Parler-TTS} project for our study.
% \texttt{Parler-TTS} is developed by~\citet{lacombe-etal-2024-parler-tts}, reproducing the work originally presented by~\citet{lyth2024natural} on natural language guidance for high-fidelity text-to-speech.
% The selected models include \texttt{Parler-TTS Large v1}, \texttt{Mini v1}, \texttt{Mini v0.1}, and \texttt{Mini Expresso}.
% These models vary in size and training data, with details provided in Appendix~\ref{appendix:baseline_models_intro}.

% \subsection{Gender Bias in Speech Synthesis Models}
% \subsection{Bias in Default Generation}
% Inherent Bias in Gender-Neutral Generation
\subsection{Inherent Bias}
In~\Cref{appendix:results-gender-part1,appendix:results-gender-part2,appendix:results-gender-part1-content-prompt,appendix:results-gender-part2-content-prompt}, we notice that \texttt{Parler-TTS} tend to generate voices of a specific gender even when given gender-neutral style prompts (e.g., ``Act like a person.'') or even without any style prompt. 
This happens because the models themselves have some inherent bias. 
Therefore, we use the results from these gender-neutral style prompts as a control group to examine the results of bias in occupational association.

\subsection{Bias in Occupational Association}
% \noindent\textbf{Bias in Occupational Association}
% Several prior works defined gender bias as the model’s tendency to portray a particular gender when given gender-neutral prompts (e.g. generate “a person” or “a face”)
Table~\ref{table:results} presents a selection of our experimental results, with the complete statistical analysis and gender ratios for each occupation provided in Appendix~\ref{appendix:gender-recognition-full-exp-results}. 
In this table, we use color coding to illustrate gender bias patterns. 
Blue indicates a bias towards males, while orange shows a bias towards females. 
The shade of these colors represents the strength of the bias: darker shades indicate stronger bias tendencies.
Areas colored in gray indicate no statistically significant difference between genders. 
We include numbers in each cell to quantify the degree of deviation. 
% These figures are the standard residuals derived from the chi-square test, offering a measure of bias strength.
These numbers are the standard residuals derived from the chi-square test, offering a measure of bias strength.

The results reveal persistent gender stereotypes in many professions, while also highlighting interesting variations across different models.
Occupations typically perceived as male-dominated, such as fisherman, electrician, plumber, carpenter, and mechanic, consistently show a strong bias towards males, as indicated by the blue-colored cells. 
Conversely, jobs often associated with women, like nurse, nanny, receptionist, and midwife, display a pronounced female bias, represented by orange cells.
The intensity of these biases, however, is not uniform across all occupations or models. 
Some occupations, such as fisherman and nurse, exhibit consistent and strong gender bias across all models. Others, like manager or HR professional, show more mixed results or less pronounced bias. 
%This variability suggests that the strength of gender associations can differ significantly depending on the specific job and models analyzing it.
% This variability suggests that the strength of gender associations can differ significantly depending on the specific occupation and different TTS models analyzed.
This variability suggests that the strength of gender associations can differ significantly depending on the specific occupation and the different TTS models being analyzed.
Interestingly, different models demonstrate varying tendencies in their bias patterns. 
The \texttt{Large-V1} model appears more inclined to show male bias for traditionally masculine occupations. 
In contrast, the \texttt{Mini-V0.1} model often displays stronger female bias for jobs conventionally associated with women. 
The \texttt{Mini-Exp} model generally exhibits milder bias compared to the others.
Overall, this study reveals that \texttt{Parler-TTS} can reflect and potentially amplify societal gender biases in occupational associations. 
These results point to the ongoing need for awareness and mitigation strategies to address gender bias.
Besides, our analysis shows no significant effects on emotion and speaking rate. 
The original results for these aspects are included in the Appendix~\ref{appendix:full-experiments-results}.

% \input{tables/results_bias_stat_selected}

% \vspace{-15pt}
\subsection{Bias Mitigation}
We test three different prompting methods to reduce gender bias in the selected occupations listed in Table~\ref{table:results}.
% We test three different prompting methods to reduce gender bias in the occupation present in the Table1.
% 只做了 27 個 occupations.
The first method (\textbf{Method 1}) involves adding ``paying attention to diversity'' after the style prompt. 
% This reminds the model to consider diversity when generating speech based on the given style prompt. 
The second method (\textbf{Method 2}) adds ``recognizing that this occupation can be male or female'' after the style prompt. 
% This approach encourages the model to be mindful of gender diversity during generation.
The third method (\textbf{Method 3}) is one of the fairness-related ethical intervention prompts in~\citet{bansal2022well} and adapt it for our style prompt setting. 
In detail, we add ``if all individuals can be <occupation> irrespective of their gender'' after the style prompt.
% The method involves using a chi-square test to assess the statistical significance between the control group, the original style prompt, and the style prompt after applying the mitigation method. 

Next, we apply the chi-square test to check if there are meaningful differences between three groups: the control group, the group with the original style prompt, and the style prompt after applying the mitigation method mentioned above.
% We then report the differences in standard residuals between the original style prompt and the style prompt after applying the mitigation method to observe the effectiveness of the mitigation.
% We then compare the standard residuals of the original style prompt with those of the style prompt after applying the mitigation method. 
We then calculate the differences in standard residuals between the original style prompt and the style prompt after applying the mitigation method. 
A positive value indicates that bias has been mitigated, while a negative value suggests that bias has increased.
The purple-highlighted cases represent a reversal in bias direction. 
The value shown is the sum of the standard residuals before and after mitigation, indicating the extent of the reversal.
% In Table~\ref{table:results-bias-mitigation}, we observe that \textbf{Method 1} can reduce bias especially in \texttt{Large-V1} and \texttt{Mini-V1}. 
% However, the results are not consistent across all occupations. 
% In some cases, \textbf{Method 1} even reverse the direction of bias or intensify existing biases.
% On the other hand, in~\Cref{appendix:results-bias-mitigation-part2,appendix:results-bias-mitigation-part3}, we observe that \textbf{Method 2} and \textbf{Method 3} both 更明顯在原本 bias 偏向 male 的 occupation reverse the direction of bias (轉為偏向 female)，這個現象在四個 baseline models 都有觀察到。其中，Method 2 整體來說 mitigate bias 的現象大多發生在 Mini-V1 上，其餘的模型上都有稍微增加 bias 的傾象。從 Method 3 則觀察到，整體來說 mitigate bias 集中發生在 Large-V1 上。

% 因此利用這些 Inference-Time Prompt-Based Mitigation 方法，例如 ethical intervention prompts 或 fairness intervention approach，在我們所選擇的 baseline models 中並不足夠有效且 generalization，有時甚至會導致加劇 bias 或反過來導致另一方向的 bias。

In Table~\ref{table:results-bias-mitigation}, we observe that \textbf{Method 1} reduces bias, particularly in \texttt{Large-V1} and \texttt{Mini-V1}. However, its effects are inconsistent across occupations, sometimes reversing the direction of bias or amplifying existing biases.
As shown in~\Cref{appendix:results-bias-mitigation-part2,appendix:results-bias-mitigation-part3}, \textbf{Method 2} and \textbf{Method 3} more prominently reverse the direction of bias in occupations originally skewed toward males, shifting the bias toward females. 
This pattern is observed across all four baseline models. 
On the other hand, \textbf{Method 2} primarily mitigates bias in \texttt{Mini-V1}, while slightly increasing bias in other models. 
\textbf{Method 3} shows a stronger bias mitigation effect in \texttt{Large-V1}.
These results suggest that inference-time prompt-based mitigation methods, such as ethical intervention prompts or fairness intervention approaches, are neither sufficiently effective nor generalizable across the selected baseline models. 
In some cases, they may even exacerbate bias or introduce a new bias in the opposite direction.

\input{tables/results_bias_mitigation_stat}
\input{tables/results_bias_mitigation_stat_method2}
\input{tables/results_bias_mitigation_stat_method3}

% \vspace{-15pt}
\section{Conclusion and Discussion}

This study is among the first to examine gender bias in instruction-guided speech synthesis models, focusing on occupational associations. 
By using occupation-related style prompts, we analyze how the model’s output deviates from its inherent behavior and disproportionately represents certain genders for specific occupations.
Our findings show that inference-time prompt-based mitigation methods, such as ethical intervention prompts and fairness intervention approaches, are neither sufficiently effective nor generalizable across different models. 
While these methods can sometimes reduce bias, they may also exacerbate it or introduce a new bias in the opposite direction. 
This highlights the persistent challenge of developing robust and reliable bias mitigation strategies, underscoring the need for more effective approaches in future research.

% V1
% This study is among the first to examine gender bias in instruction-guided speech synthesis models. We explore bias in occupational associations by using occupation-related style prompts to observe how the model's output differs from its inherent behavior.
% Our experimental results reveal the model's tendency to disproportionately represent certain genders for specific occupations. 
% We test several inferencetime prompt-based mitigation methods and find that these methods cannot fully eliminate bias tendencies and sometimes cause an overcompensation in the opposite direction.
% This finding highlights the ongoing challenge of developing effective and reliable bias mitigation methods, pointing to a critical area for future research.

% This study is among the first to investigate gender bias in instruction-guided speech synthesis models. 
% We explore bias in default generation and occupational associations. 
% Our approach uses occupation-related style prompts to observe how the model's output differs from its default settings. 
% Our experimental results reveal the model's tendency to over-represent or under-represent a particular gender for certain occupations. 
% In response, we propose mitigation methods and review previous works. 
% However, we find that these methods cannot completely reduce bias tendencies and sometimes cause an overcompensation in the opposite direction. 
% This observation emphasizes that addressing the controllability and interpretability of bias mitigation methods is still an open challenge and a key area for future research.


% This study is amongst the first to investigate gender bias in instruction-guided speech synthesis models. We explore bias in default generation and bias in occupational association.
% 我們提出利用如 Act like a specific occupation 這樣的 style prompt 去觀察模型相較於 default 的設定會出現何種傾向。我們的實驗結果確實發現 the model’s tendency to over-represent or under-represent a particular gender for an occupation。對此，我們嘗試提出 mitigation 的方法並且也參考 previous works，我們發現這些方法並不能全然減輕 bias 的傾向，且有時會造成 overshooting of bias in the opposite
% direction。這樣的觀察也顯示了 This observation highlights that controllability and interpretability of bias mitigation
% methods remain an unresolved and critical research question in future studies.


% However, the fairness-intervention method
% seems to overdress biases in organizational power,
% resulting in an overshooting of bias in the opposite
% direction. This observation highlights that controllability and interpretability of bias mitigation
% methods remain an unresolved and critical research question in future studies.

\section*{Limitations}
% (1) 受限於當前只有 Parler-TTS 模型有 open source，所以我們這篇 works 只有探討 Parler-TTS。等待未來有更多模型開放後，我們將會套用同樣的方法去檢驗。
% (2) 職業有百百種，甚至有尚未出現的職業。但在這篇研究中，我們僅探討其中一部分。
First, to the best of our knowledge, \texttt{Parler-TTS} is currently the only open-source model available for this type of analysis. 
Other instruction-guided text-to-speech models require both a reference speech and a style prompt as conditions for synthesis, with the speaker characteristics primarily influenced by the reference speech.
As a result, our study focuses solely on examining \texttt{Parler-TTS}. 
However, our proposed method for measuring gender bias is applicable to all instruction-guided text-to-speech models.
We intend to apply the same methods to investigate other models as they become publicly available in the future. 

Second, we acknowledge that gender is not limited to just female and male categories. 
However, due to limitations in current gender recognition models and their training data labels, this study focuses primarily on analyzing gender bias between male and female categories. 
In the future, when more nuanced datasets and models become available, we can apply the same pipeline to conduct a more comprehensive analysis.

Third, there are countless occupations in the world, including some that may not yet exist. 
However, in this study, we explore only a subset of these occupations. 
We recognize that this limited selection may not fully capture the diversity of professions in reality.

Fouth, our analysis of speech focuses on gender, emotion, and speaking speed. 
However, there are many other aspects of speech that could be examined. 
We are limited by the current availability of foundational speech models that can analyze various speech attributes. 
As more advanced models capable of analyzing additional speech characteristics become available, future research will be able to explore a wider range of speech attributes.

Finally, we recognize that gender recognition models may have their own gender biases when classifying speech. 
To address this issue, we design three different control groups to serve as baselines for our experiment. 
These control groups help us distinguish between the biases inherent in the gender recognition model and the effects of our experimental prompts.

\section*{Ethics Statement}
We acknowledge the potential for gender bias in controllable expressive speech synthesis models, particularly in their interpretation of occupation-related style prompts. 
Our study examines how these models respond to prompts like ``Act like a nurse'', investigating possible tendencies to amplify gender stereotypes.
Our results indicate that the model may over- or under-represent certain genders for specific occupations. 
We recognize that this could reinforce stereotypes and potentially impact the perception of various professions.

In our current analysis, we use a binary gender classification system (male/female) for synthesized voices. 
We recognize this approach has limitations and does not fully capture the diverse spectrum of gender identities. 
This simplification is primarily due to the constraints of current speech gender recognition models and their training data, which largely operate within a binary framework. 
However, we acknowledge that this binary approach may inadvertently contribute to the underrepresentation of non-binary and other gender identities.

It's crucial to note that our study focuses solely on synthesized speech and not on recordings of real individuals. 
The gender classifications in our analysis are based on perceived vocal characteristics as interpreted by our evaluation process, and do not reflect the complex reality of gender identity.
We emphasize the need for continued research and development to address these limitations and biases. 
Future work in speech synthesis should aim to develop more inclusive models and evaluation methods that better represent the full spectrum of gender identities across all occupations.

% Through this research, we aim to:

% Identify systematic gender biases in how models interpret different profession-based prompts.
% Explore the interplay between content and style in speech generation.
% Propose strategies for reducing or eliminating gender bias in speech generation models.

% By addressing these objectives, our study contributes to the growing body of AI ethics research and provides valuable insights for developing more equitable and flexible speech synthesis systems. The findings of this research have implications not only for AI developers but also for ethicists, policymakers, and the broader public, as we collectively work towards shaping a more inclusive and fair AI-driven future.

% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%   \end{tabular}
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{anthology}

% Custom bibliography entries only
% \bibliography{custom}

\appendix

% \section{Appendix}
% \label{sec:appendix}
\section{Prompt Design for Instruction-Guided Speech Synthesis}
\label{appendix:prompt_design}
\subsection{Style Prompt}
The \texttt{Parler-TTS} models might have some inherent bias in speech generation, even without a specific style prompt. 
To address this, we design three different control groups to observe the model's tendencies:

\noindent\textbf{Control group 1}: 
An empty string (no style prompt).

\noindent\textbf{Control group 2}: 
We have designed general prompts listed below.
\begin{enumerate}
    \item {Act like a person.}
    \item {Act like an ordinary person.}
    \item {Act like an average person.}
    \item {Act like a regular person.}
\end{enumerate}

\noindent\textbf{Control group 3}: 
This control group consists of neutral sentences that don't refer to any specific speech style, emotion, or gender.
We design several candidate sentences using GPT-4o and randomly selected ten of them as our neutral sentences for style prompts. The complete prompt used for GPT-4o is shown in the Table.
The goal of this process was to create sentences without any specific style implications to use as style prompts. When the model encounters these style prompts, it won't have a clear direction for speech generation. We use these as a control group for comparison.
The ten selected neutral sentences, which we refer to as ``Neutral style prompts'', are listed in order below:
\begin{enumerate}
    \item {
    The leaves turned bright colors, marking the arrival of the cool autumn season.
    }
    \item {
    Raindrops gently tapped on the window as the storm passed through the quiet town.
    }
    \item {
    The campfire crackled softly as the flames danced in the cool night air.
    }
    \item {
    The sun set behind the mountains, casting long shadows over the valley.
    }
    \item {
    The clouds parted, revealing a brilliant sunset with vibrant shades of orange and pink.
    }
    \item {
    A river flowed calmly through the forest, reflecting the tall trees on its surface.
    }
    \item {
    Snow covered the landscape, transforming the world into a quiet, white wonderland.
    }
    \item {
    Morning dew sparkled on the grass, catching the first rays of sunlight.
    }
    \item {
    The sound of the river echoed softly between the rocks as it flowed downstream.
    }
    \item {
    A small bird perched on a branch, singing softly to the morning light.
    }
\end{enumerate}

% Neutral sentences that don't refer to any specific speech style, emotion, or gender. 
% We have design several candidate sentences utilizing GPT-4o and randomly choose ten sentences as our neutral sentences for style prompts. The complete prompt for GPT-4o is shown in Table. 這個過程的目的在於我們想要建立沒有攜帶任何指涉風格意義的句子當作 style prompt，所以模型在遇到這種 style prompt 時，會沒有明確被導引的生成語音方向。我們以此將之作為一個對照組的選擇。

% 我們將挑選的十個 neutral sentences 條列如下

% For example, "Morning dew sparkled on the grass, catching the first rays of sunlight"

% We then use these three control groups as a baseline for comparison. 
% This approach allows us to determine how a specific style prompt affects the model's speech generation tendencies compared to these neutral conditions.

\subsection{Content Prompt}
\label{appendix:content_prompt}

We use \texttt{GPT-4o}\footnote{In this paper, all versions of \texttt{GPT-4o} used are \texttt{gpt-4o-2024-05-13}.}~\citep{achiam2023gpt} to develop several neutral content prompts. 
The complete prompts we use is shown in the Table~\ref{appendix:construct_content_prompt}. 
We then randomly select ten of these to serve as our final neutral content prompts.
The goal of this process is to create content prompts that avoid descriptions related to speech style. 
We list the ten selected neutral content prompts below:

\begin{enumerate}
    \item {
    Everyone had a fantastic time at the party, and the food was absolutely delicious.
    }
    \item {
    I hope the traffic won't be too bad during rush hour this evening after work.
    }
    \item {
    Do you know if the library will be open this weekend during the holiday?
    }
    \item {
    Have you seen my glasses? I can't seem to find them anywhere in the house.
    }
    \item {
    I'm thinking of signing up for a cooking class to learn new recipes and techniques.
    }
    \item {
    They organized a fundraising event to support the local animal shelter in their community.
    }
    \item {
    When was the last time you went to see a live concert or performance?
    }
    \item {
    She picked out a perfect gift for his birthday, which she knew he would love.
    }
    \item {
    He promised to take his kids to the zoo as a reward for good behavior.
    }
    \item {
    Our neighbors are planning a big garage sale and invited us to join in next Saturday.
    }
\end{enumerate}

\input{tables/content_prompt_construction}

\subsection{Occupation Selection}
\label{appendix:occupation_selection}
% We utilize GPT-4o~\citep{achiam2023gpt} to develop different occupations and source occupations statistics from WinoBias~\citep{zhao2018gender} dataset. The complete prompt we use to query GPT-4o is showed in Table~\ref{appendix:occupation_selection}. 我們人工從 GPT-4o 產生的結果中選取出沒有出現在 WinoBias dataset 中的 occupations，最後我們總共建立了 109 種 occupations。
We used \texttt{GPT-4o}~\citep{achiam2023gpt} to generate various occupations and obtained occupation statistics from the WinoBias~\citep{zhao2018gender} dataset. 
The WinoBias dataset is licensed under the MIT License. 
The complete prompt used to query \texttt{GPT-4o} is shown in Table~\ref{appendix:occupation_selection_prompt}. 
We manually select occupations from the \texttt{GPT-4o} output that are not present in the WinoBias dataset. 
In total, we compile a list of 109 different occupations.

\input{tables/occupation_selection_prompt}

% \section{Baseline Models for Speech Attribute Measurement}
% \subsection{Gender Recognition}
% \label{appendix:baseline_models_intro}
% \subsection{Speech Recognition}
% \subsection{Speaking Speed}

\section{Experimental Setups and Results}
\label{appendix:full-experiments-results}
\subsection{Gender Recognition}
\label{appendix:gender-recognition-full-exp-results}
We utilize the speech-based gender recognition model\footnote{\url{https://huggingface.co/audeering/wav2vec2-large-robust-24-ft-age-gender}} proposed by~\citet{burkhardt2023speech} for the task of speech gender recognition, which is licensed under CC BY-NC-SA 4.0.
This model is based on the pre-trained Wav2Vec 2.0~\citep{baevski2020wav2vec}. 
It is fine-tuned using four datasets: aGender~\citep{burkhardt2010database}, Mozilla Common Voice~\citep{commonvoice:2020}, TIMIT~\citep{garofolo1993timit}, and VoxCeleb 2~\citep{chung2018voxceleb2}.
Due to the labeling in its training data, this model's gender predictions are limited to categories such as female, male, and child. 
However, instances where the model predicts a child label are extremely rare in our study and thus disregarded. 
Therefore, in this research, we focus primarily on gender bias between female and male categories.
Additionally, for the gender recognition task, we conduct human evaluation on a subset of generated speech samples. 
This is done to verify the accuracy of our chosen gender recognition model. 
We present these findings in Appendix~\ref{appendix:human-evaluation}.

We show the full gender recognition results in~\Cref{appendix:results-gender-part1,appendix:results-gender-part2,appendix:results-gender-part1-content-prompt,appendix:results-gender-part2-content-prompt}. 
In Table~\ref{appendix:results-gender-part1} and~\ref{appendix:results-gender-part2}, we present the averages and 95\% confidence intervals based on five different style prompt templates. 
On the other hand, in Table~\ref{appendix:results-gender-part1-content-prompt} and~\ref{appendix:results-gender-part2-content-prompt}, we present the averages and 95\% confidence intervals based on ten different neutral content prompt.
Furthermore, we present the complete version of Table~\ref{table:results} in~\Cref{appendix:stat-results-par1,appendix:stat-results-par2}. 
These tables provide a comprehensive analysis using the chi-square test.
Regarding the outcomes of various bias mitigation methods, we display the results in~\Cref{appendix:results-bias-mitigation-part2,appendix:results-bias-mitigation-part3}.

For actual speech samples, please visit our demo website\footnote{\url{https://sites.google.com/view/instruction-guided-tts-bias}}.
\subsection{Emotion}
\label{appendix:emotion2vec-intro-results}
For the task of speech emotion recognitoon, we select the \texttt{emotion2vec}\footnote{We use the version \texttt{emotion2vec plus large}, and the model link is \url{https://huggingface.co/emotion2vec/emotion2vec_plus_large}.}~\citep{ma2023emotion2vec}, which is the foundational models for speech emotion recognition (SER). The \texttt{emotion2vec} model is licensed under the MIT License.
We show full speech emotion recognition results in~\Cref{appendix:results-emotion-ratio-part1-1,appendix:results-emotion-ratio-part1-2,appendix:results-emotion-ratio-part2-1,appendix:results-emotion-ratio-part2-2}.  
In these table, we present the ratios of speech emotion recognition results.
We observe no significant differences in emotion across various occupation-related prompt settings. 
Among all the emotion recognition results, the most prominent emotions included happy, neutral, and sad.

\subsection{Speaking Rate}
\label{appendix:speaking-rate-intro-results}
To calculate speaking rate, we first use the automatic speech recognition model, \texttt{Whisper}\footnote{We use the version \texttt{Whisper-large-v3}, and the model link is \url{https://huggingface.co/openai/whisper-large-v3}}~\citep{radford2023robust}, to transcribe the speech. The \texttt{Whisper} model is licensed under the MIT License.
We then measure speaking rate in three different ways:
(1) phonemes per second, 
(2) words per second, 
(3) syllables per second. 
For phoneme counting, we use the \texttt{g2p}\footnote{\url{https://pypi.org/project/g2p/}} library, setting it to map English to IPA (International Phonetic Alphabet) symbols.
To count syllables, we use the \texttt{pyphen}\footnote{\url{https://pyphen.org/}} library.
For word count, we simply split the text by spaces and count the resulting segments.
After obtaining these counts, we divide each by the duration of the corresponding speech to get the final speed measurements.
We show full speaking speed calculation results in~\Cref{appendix:results-speaking-rate-part1,appendix:results-speaking-rate-part2}. 
In these table, we present the averages and 95\% confidence intervals. 
we observe no significant differences in speaking rates across various occupation-related prompt settings. 
This consistency is evident in measures of phonemes per second, words per second, and syllables per second.

\section{Parler-TTS}
\label{appendix:baseline_models_intro}
The \texttt{Parler-TTS} models are licensed under the Apache License 2.0. 
The specifications of the \texttt{Parler-TTS} models used in our study are as follows:
\begin{enumerate}
    \item \texttt{Parler-TTS Large v1}: 2.2 billion parameters, trained on 45,000 hours of audio data.
    \item \texttt{Parler-TTS Mini v1}: 880 million parameters, trained on 45,000 hours of audio.
    \item \texttt{Parler-TTS Mini v0.1}: 880 million parameters, trained on 10,500 hours of audio.
    \item \texttt{Parler-TTS Mini Expresso}: A version of \texttt{Mini v0.1} fine-tuned on the Expresso dataset~\citep{nguyen2023expresso}.
\end{enumerate}

For all models, we use sample-based decoding with a temperature of 1.0, top p of 0.9, and top k of 50.

Additionally, we analyze the gender distribution in the training dataset released by \texttt{Parler-TTS} on Hugging Face\footnote{\url{https://huggingface.co/parler-tts}}. 
The training data is sourced and filtered from LibriTTS-R~\citep{Koizumi2023-hs} and the English version of the Multilingual LibriSpeech (MLS)~\citep{pratap2020mls} dataset, which include gender labels. 
LibriTTS-R is licensed under CC BY 4.0, and Multilingual LibriSpeech also follows the CC BY 4.0 license. The Expresso dataset is distributed under the CC BY-NC 4.0 license.

We present our findings in Table~\ref{appendix:table-training-data}.
Table~\ref{appendix:table-training-data} reveals that male data exceeds female data in both total hours and number of samples.

\input{tables/training_data_stat}
\input{tables/results_human_evaluation_confusion_matrix}
\section{Human Evaluation for Gender Recognition Tasks}
\label{appendix:human-evaluation}
We conduct a human evaluation of speech samples generated by \texttt{Parler-TTS} without any style prompts. 
Participants are asked to listen to these samples and identify whether the speaker was male or female. 
The purpose of this evaluation is to compare the results with the gender recognition model used in our paper. 
We limit the classification to male or female categories to align with the characteristics of the selected gender recognition model.
We randomly select 100 speech samples from each of the four models: Large-V1, Mini-V1, Mini-V0.1, and Mini-Expresso, all generated without style prompts. 
This results in a total of 400 speech samples. 

We assign three participants and ask them to listen to all 400 samples and classify each as either female or male. 
The example of human evaluation interface is shown
in Figure~\ref{fig:human-evaluation-template}.
We pay each participant 18 USD for taking our test. 
The test usually takes about 1 hour to finish. 
This includes listening to audio clips twice and reading some text. 
Therefore, participants earn about 18 USD per hour for their time.

We then compare these human evaluation results with the model's predictions.
In order to analyze the data, we first determine a consensus human evaluation result by taking the mode of the three participants' responses. 
We then use Kendall's $\tau$ to compare this consensus with the model's predictions. 
The Kendall's $\tau$ between the mode of human evaluations and model predictions is 0.95, with a p-value of 0.0. 
In addition, we calculate the agreement percentage among the three participants, which is 97.8\%. 
The confusion matrix is presented in the accompanying Table~\ref{appendix:table-human-evaluation-confusiin-matrix}.
From these results, we observe a high correlation between the model's predictions and human classifications. 
This strong agreement suggests that selected model's gender recognition capabilities closely align with human perception of speaker gender in the generated speech samples.
% We conduct a human evaluation of speech samples generated by \texttt{Parler-TTS} without any style prompt. 
% Participants are asked to listen to these samples and identify whether the speaker was male or female. The purpose of our human evaluation is to compare the results with the gender recognition model used in our paper. 
% Therefore, we only ask participants to classify the speakers as either male or female due to the characteristics of selected gender recognition model. 我們從 Large-V1, Mini-V1, Mini-V0.1 和 Mini-Expresso 在 no style prompt 的設定下生成語音中各別隨機挑選 100 筆語音出來，並且 ask three participants to 標記語音是 female 或 male，將這個結果和模型預測的結果進行比較。
% 我們首先通過取眾數，得到了一個綜合的人類評估結果，並使用 Kendall's τ 來比較。我們計算眾數與模型預測之間的 Kendall's τ 為 0.95，p-value 為 0.0。此外，我們同時計算三個 participants 之間分類的一致性百分比，數值為 97.8\&。在 Table 中則呈現了混淆矩陣。

% We conduct human evaluation on the speech samples generated from \texttt{Parler-TTS Large V1} with style prompt 設定是 ``No style prompt''. We ask participants to listen to these speech samples and 辨認是 male 還是 female。因為我們 Human Evaluation 的目的是為了和論文中使用的 Gender Recognition 模型進行比較，因此只請參與者做 male 和 female 的分類。我們將結果呈現在 Table.

\input{tables/results_gender_ratio_style_prompt_part1}
\input{tables/results_gender_ratio_style_prompt_part2}
\input{tables/results_gender_ratio_content_prompt_part1}
\input{tables/results_gender_ratio_content_prompt_part2}
\input{tables/results_bias_stat_full_part1}
\input{tables/results_bias_stat_full_part2}
% \input{tables/results_bias_mitigation_stat_method2}
% \input{tables/results_bias_mitigation_stat_method3}
\input{tables/results_gender_ratio_mitigation_method1}
\input{tables/results_gender_ratio_mitigation_method2}
\input{tables/results_gender_ratio_mitigation_method3}

\input{tables/results_emotion_ratio_full_part1_1}
\input{tables/results_emotion_ratio_full_part1_2}
\input{tables/results_emotion_ratio_full_part2_1}
\input{tables/results_emotion_ratio_full_part2_2}

\input{tables/results_speaking_speed_ratio_full_part1}
\input{tables/results_speaking_speed_ratio_full_part2}

\input{figures/human_evaluation_interface}

\end{document}
