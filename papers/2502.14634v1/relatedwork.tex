\section{Related Work}
\subsection{Reasoning in LLMs}

Recent research has explored various techniques to enhance the reasoning capabilities of LLMs. CoT prompting \cite{brown2020language, kojima2022large} improves multi-step reasoning by generating structured intermediate steps, leading to more transparent and interpretable solutions. Self-consistency \cite{wang2022self} further enhances accuracy by sampling multiple reasoning paths and selecting the most consistent answer. In parallel, question decomposition methods \cite{zhou2022least, dua-etal-2022-successive, khot2022decomposed, ling2023deductive, weng-etal-2023-large} improve coherence by breaking complex queries into simpler sub-questions, though it introduces additional computational overhead. Another promising direction involves search and planning-based methods \cite{wang2023hypothesis, wang2024planning, yao2023tree, besta2024got, xue2025decompose, yang2024buffer}, which systematically explore multiple reasoning trajectories to improve problem-solving. Lastly, integrating external tools—such as web search engines and Python interpreters—extends the model’s capabilities, enabling more precise and efficient task execution across diverse domains \cite{lu2023chameleon, yao2023react, kim2024husky, chen2022program}.
As our approach is grounded in uncertainty estimation, we begin by reviewing existing uncertainty estimation methods, followed by an introduction to uncertainty-aware reasoning techniques, which are the most pertinent to our research.

\subsection{Uncertainty Estimation}
Uncertainty estimation methods can be broadly classified into two categories: black-box \cite{zhang-etal-2023-sac3, xiong2024can, lin2023generating, manakul-etal-2023-selfcheckgpt, chen-mueller-2024-quantifying} and white-box \cite{kuhn2023semantic, duan-etal-2024-shifting, fadeeva-etal-2024-fact, huang2023look} approaches. One approach to uncertainty estimation is training-based confidence estimation \cite{cohen2024don, lin2022teaching, azaria-mitchell-2023-internal}, which improves calibration by incorporating uncertainty estimation directly into the training process. These methods modify the training objective, introduce auxiliary loss functions, or leverage additional supervision to produce more reliable confidence estimates. 
%A notable example is uncertainty-aware pretraining, such as IDK-tuning \cite{cohen2024don}, which incorporates an explicit [IDK] token into the model’s vocabulary. During continued pretraining, the model learns to allocate probability mass to this token, enabling it to express uncertainty when lacking sufficient knowledge for confident predictions. 
Another approach is verbal-based confidence estimation \cite{tian-etal-2023-just, kadavath2022language}, which prompts the model to explicitly express its confidence through natural language statements. 
%By leveraging LLMs ability to self-assess uncertainty, these methods allow for direct human interpretation of self-verbalized confidence scores. 
Finally, semantic-based uncertainty estimation methods \cite{nikitin2024kernel, kuhn2023semantic, qiu2024semantic, wang2024clue} cluster outputs or reasoning chains that are semantically equivalent, quantifying uncertainty based on the variability of responses within these clusters.

\subsection{Uncertainty-aware reasoning}
An emerging trend leverages uncertainty estimation as a tool to enhance various components of reasoning. One application is in improving few-shot prompting, where uncertainty estimation helps automate the selection of demonstrations \cite{gonen-etal-2023-demystifying, huang2024unlocking, margatina-etal-2023-active}, reducing the need for manually intensive prompt engineering. Another key contribution of uncertainty estimation in reasoning is its role in selecting the most reliable reasoning chain based on confidence \cite{murray-chiang-2018-correcting, kadavath2022language, malinin2020uncertainty}. In such cases, uncertainty acts as a guiding signal, identifying the chain where the model exhibits the highest confidence. Our approach builds on this intuition by enabling a weighted voting mechanism to select the final answer. More importantly, instead of applying our uncertainty estimation function to every token, we focus only on critical tokens, specifically the intermediate answers in a CoT chain.

% Another contribution of uncertainty estimation in reasoning is its role in directly enhancing accuracy. 
%Specifically, CoT-decoding \cite{wang2024chain} leverages uncertainty to identify the reasoning path by considering only the final answer phrase in the generated text without requiring CoT prompting. In contrast, our method guides the LLM to generate multi-step reasoning paths while exploring different uncertainty estimation functions for both step-wise and path-wise aggregation to determine the optimal configuration. More precisely, while CoT-decoding focuses on achieving CoTs without needing a CoT prompt, we propose a confidence-based weighing of paths which takes into account critical points throughout the entire chain.