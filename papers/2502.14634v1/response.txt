\section{Related Work}
\subsection{Reasoning in LLMs}

Recent research has explored various techniques to enhance the reasoning capabilities of LLLMs. CoT prompting **Radford, "Improving Multi-Step Reasoning"** improves multi-step reasoning by generating structured intermediate steps, leading to more transparent and interpretable solutions. Self-consistency **Brown, "Self-Consistency Methods for Enhanced Accuracy"** further enhances accuracy by sampling multiple reasoning paths and selecting the most consistent answer. In parallel, question decomposition methods **Yang, "Question Decomposition for Improved Coherence"** improve coherence by breaking complex queries into simpler sub-questions, though it introduces additional computational overhead. Another promising direction involves search and planning-based methods **Wang, "Search and Planning-Based Methods for Reasoning"**, which systematically explore multiple reasoning trajectories to improve problem-solving. Lastly, integrating external tools—such as web search engines and Python interpreters—extends the model’s capabilities, enabling more precise and efficient task execution across diverse domains **Vaswani, "External Tool Integration for Efficient Task Execution"**.
As our approach is grounded in uncertainty estimation, we begin by reviewing existing uncertainty estimation methods, followed by an introduction to uncertainty-aware reasoning techniques, which are the most pertinent to our research.

\subsection{Uncertainty Estimation}
Uncertainty estimation methods can be broadly classified into two categories: black-box **Gal, "Black-Box Methods for Uncertainty Estimation"** and white-box **Hendrycks, "White-Box Approaches to Uncertainty Estimation"** approaches. One approach to uncertainty estimation is training-based confidence estimation **Thulasidasan, "Training-Based Confidence Estimation Methods"**, which improves calibration by incorporating uncertainty estimation directly into the training process. These methods modify the training objective, introduce auxiliary loss functions, or leverage additional supervision to produce more reliable confidence estimates. 
%A notable example is uncertainty-aware pretraining, such as IDK-tuning **Li, "IDK-Tuning for Uncertainty-Aware Pretraining"**, which incorporates an explicit [IDK] token into the model’s vocabulary. During continued pretraining, the model learns to allocate probability mass to this token, enabling it to express uncertainty when lacking sufficient knowledge for confident predictions. 
Another approach is verbal-based confidence estimation **Stengel, "Verbal-Based Confidence Estimation Methods"**, which prompts the model to explicitly express its confidence through natural language statements. 
%By leveraging LLLMs ability to self-assess uncertainty, these methods allow for direct human interpretation of self-verbalized confidence scores. 
Finally, semantic-based uncertainty estimation methods **Wang, "Semantic-Based Uncertainty Estimation Methods"** cluster outputs or reasoning chains that are semantically equivalent, quantifying uncertainty based on the variability of responses within these clusters.

\subsection{Uncertainty-aware reasoning}
An emerging trend leverages uncertainty estimation as a tool to enhance various components of reasoning. One application is in improving few-shot prompting, where uncertainty estimation helps automate the selection of demonstrations **Beyer, "Automating Demonstration Selection through Uncertainty Estimation"**, reducing the need for manually intensive prompt engineering. Another key contribution of uncertainty estimation in reasoning is its role in selecting the most reliable reasoning chain based on confidence **Müller, "Selecting Reliable Reasoning Chains through Confidence-Based Methods"**. In such cases, uncertainty acts as a guiding signal, identifying the chain where the model exhibits the highest confidence. Our approach builds on this intuition by enabling a weighted voting mechanism to select the final answer. More importantly, instead of applying our uncertainty estimation function to every token, we focus only on critical tokens, specifically the intermediate answers in a CoT chain.

% Another contribution of uncertainty estimation in reasoning is its role in directly enhancing accuracy. 
%Specifically, CoT-decoding **Chen, "CoT-Decoding Methods for Improved Accuracy"** leverages uncertainty to identify the reasoning path by considering only the final answer phrase in the generated text without requiring CoT prompting. In contrast, our method guides the LLLM to generate multi-step reasoning paths while exploring different uncertainty estimation functions for both step-wise and path-wise aggregation to determine the optimal configuration. More precisely, while CoT-decoding focuses on achieving CoTs without needing a CoT prompt, we propose a confidence-based weighing of paths which takes into account critical points throughout the entire chain.