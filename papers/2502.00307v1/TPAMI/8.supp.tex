\setcounter{theorem}{0}
\setcounter{lemma}{0}

\appendices


\section{Derivatives of training and inference processes}

\subsection{Algorithms of training and inference processes}

In this part, we provide the algorithms of training and inference processes.
%
\sqq{Notably,} both training and inference procedures in \Cref{alg:paired_train,alg:paired_infer} resemble the corresponding processes of DDPM respectively, where $\epsilon_{\phi}$ is the pre-trained DDPM with parameter $\phi$, and $\sigma_i$ is the variance of the distribution $\epsilon_{\phi}(y_{i-1}|y_i)$.
%
The training learns to transfer between the intermediate diffusion results $x_t$ and $y_t$ while DDPM approximator intends to predict the noise $\epsilon$ from $x_t$.

\revise{
Furthermore, we provide the algorithms \sqq{for the} training and inference process\sqq{es} of the generalized asymmetric pipelines in \Cref{alg:paired_train_asymm,alg:paired_infer_asymm}.
}

\begin{algorithm}[H]
\setstretch{1.2}
\caption{Training}\label{alg:paired_train}
\begin{algorithmic}[1]
\Repeat
\State $x_0\sim q(x_0),y_0\sim q(y_0|x_0)$
\State $z_t\sim\mathcal N(0, I)$
\State $x_t\leftarrow \sqrt{\balpha{t}}x_0+\sqrt{1-\balpha{t}}z_t$
\State $y_t\leftarrow \sqrt{\balpha{t}}y_0+\sqrt{1-\balpha{t}}z_t$
\State Take gradient descent step on
$$\nabla_{\theta}\|f_{\theta}(x_t)-y_t\|^2$$
\Until converged
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\setstretch{1.2}
\caption{Inference}\label{alg:paired_infer}
\begin{algorithmic}[1]
\State $x_0\sim q(x_0)$
\State $z_t,z\sim\mathcal N(0, I)$
\State $x_t\leftarrow \sqrt{\balpha{t}}x_0+\sqrt{1-\balpha{t}}z_t$
\State $y_t\leftarrow f_{\theta}(x_t)-\sqrt{1-\balpha{t}}z_t+\sqrt{1-\balpha{t}}z$
\For {$i=t,t-1,\cdots,1$}
\State $\epsilon_i\sim\mathcal N(0, I)$ if $i>1$, else $\epsilon_i=0$
\State $y_{i-1}=\frac{(y_i-\frac{1-\alpha_i}{\sqrt{1-\bar\alpha_i}}\epsilon_{\phi}(y_i,i))}{\sqrt{\alpha_i}}+\sigma_i\epsilon_i$
\EndFor
\State \Return $y_0$
\end{algorithmic}
\end{algorithm}

\revise{
\begin{algorithm}[H]
\setstretch{1.2}
\caption{Training of the generalized asymmetric pipeline}\label{alg:paired_train_asymm}
\begin{algorithmic}[1]
\Repeat
\State $x_0\sim q(x_0),y_0\sim q(y_0|x_0)$
\State $z\sim\mathcal N(0, I)$
\State $x_s\leftarrow \sqrt{\balpha{s}}x_0+\sqrt{1-\balpha{s}}z$
\State $y_t\leftarrow \sqrt{\balpha{t}}y_0+\sqrt{1-\balpha{t}}z$
\State Take gradient descent step on
$$\nabla_{\theta}\|f_{\theta}(x_s)-y_t\|^2$$
\Until converged
\end{algorithmic}
\end{algorithm}
}


\revise{
\begin{algorithm}[H]
\setstretch{1.2}
\caption{Inference of the generalized asymmetric pipeline}\label{alg:paired_infer_asymm}
\begin{algorithmic}[1]
\State $x_0\sim q(x_0)$
\State $z_1,z_2\sim\mathcal N(0, I)$
\State $x_s\leftarrow \sqrt{\balpha{s}}x_0+\sqrt{1-\balpha{s}}z_1$
\State $y_t\leftarrow f_{\theta}(x_s)-\sqrt{1-\balpha{t}}z_1+\sqrt{1-\balpha{t}}z_2$
\For {$i=t,t-1,\cdots,1$}
\State $\epsilon_i\sim\mathcal N(0, I)$ if $i>1$, else $\epsilon_i=0$
\State $y_{i-1}=\frac{(y_i-\frac{1-\alpha_i}{\sqrt{1-\bar\alpha_i}}\epsilon_{\phi}(y_i,i))}{\sqrt{\alpha_i}}+\sigma_i\epsilon_i$
\EndFor
\State \Return $y_0$
\end{algorithmic}
\end{algorithm}
}

\begin{table*}[ht]
\begin{tabular}{cc}
\begin{minipage}[t]{0.48\textwidth}
\begin{algorithm}[H]
\setstretch{1.15}
\caption{Pseudo-code of \method in a PyTorch-like style.}
\label{alg:code}
\begin{lstlisting}[language=python]
import torch

def forward_step(x_0, y_0, t, T):
    """Defines the forward process of one training step.
    
    Args:
        x_0: Source inputs, with shape [B, C, H, W].
        y_0: Target outputs, with shape [B, C, H, W].
        t: The preset timestep to perform distribution shift.
        T: The translator module to learn.
    """
    # Compute the cumulated variance until timestep t.
    bar_alpha_t = cum_var(t)
    
    # Adding noise (i.e., diffusion) to images from both domains.
    z_t = torch.randn_like(x_0)
    x_t = torch.sqrt(bar_alpha_t) * x_0 + torch.sqrt(1 - bar_alpha_t) * z_t
    y_t = torch.sqrt(bar_alpha_t) * y_0 + torch.sqrt(1 - bar_alpha_t) * z_t
    
    # Learn the translator.
    loss = (T(x_t) - y_t).square().mean()
    
    return loss
\end{lstlisting}
\end{algorithm}
\end{minipage}
&
\begin{minipage}[t]{0.48\textwidth}
\renewcommand\arraystretch{1.0}
\begin{algorithm}[H]
\caption{\revise{Pseudo-code of \method in a PyTorch-like style.}}
\label{alg:code_asymm}
\begin{lstlisting}[language=python]
import torch

def forward_step(x_0, y_0, s, t, T):
    """Defines the forward process of one training step.
    
    Args:
        x_0: Source inputs, with shape [B, C, H, W].
        y_0: Target outputs, with shape [B, C, H, W].
        s: The preset timestep to perform distribution shift for x_0.
        t: The preset timestep to perform distribution shift for y_0.
        T: The translator module to learn.
    """
    # Compute the cumulated variance until timestep s and t.
    bar_alpha_s = cum_var(s)
    bar_alpha_t = cum_var(t)
    
    # Adding noise (i.e., diffusion) to images from both domains.
    z = torch.randn_like(x_0)
    x_s = torch.sqrt(bar_alpha_s) * x_0 + torch.sqrt(1 - bar_alpha_s) * z
    y_t = torch.sqrt(bar_alpha_t) * y_0 + torch.sqrt(1 - bar_alpha_t) * z
    
    # Learn the translator.
    loss = (T(x_s) - y_t).square().mean()
    
    return loss
\end{lstlisting}
\end{algorithm}
\end{minipage}
\end{tabular}
\end{table*}

\subsection{Pseudo-code of training process}
Our proposed diffusion model translator (\method) achieves image-to-image translation (I2I) based on a pre-trained DDPM via simply learning a distribution shift at a certain diffusion timestep.
%
Accordingly, it owns a \textit{highly efficient} implementation, which is even \textit{independent} of the DDPM itself.
%
In this part, we provide the pseudo-code of the training process in \cref{alg:code,alg:code_asymm}.
%
% It is noteworthy that the whole training process if extremely simple and efficiency, compared to the Pix2Pix~\cite{isola2017image}, which introduces a discriminator during training simultaneously.



\section{Proofs of main results}

\begin{lemma}
We have an upper bound of the negative log-likelihood of $-\log p_{\theta}(y_0|x_0)$ by
%
\begin{align}
-\log p_{\theta}(y_0|x_0)\leqslant\mathbb E_q\left[\log\frac{q(y_{1:t},x_{1:t}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:t}|x_0)}\right],
\end{align}
%
where $q=q(y_{1:t},x_{1:t}|y_0, x_0).$
\end{lemma}

\begin{proof}
\begin{align}
&-\log p_{\theta}(y_0|x_0) \\
\leqslant& -\log p_{\theta}(y_0|x_0) + \nonumber \\
& \qquad D_{KL}\left(q(y_{1:t},x_{1:t}|y_0, x_0) \| p_{\theta}(y_{1:t},x_{1:t}|y_0, x_0)\right) \\
=& -\log p_{\theta}(y_0|x_0)+\nonumber  \\
& \qquad \mathbb E_{q(y_{1:t},x_{1:t}|y_0, x_0)}\left[\log\frac{q(y_{1:t},x_{1:t}|y_0, x_0)}{p_{\theta}(y_{1:t},x_{1:t}|y_0, x_0)}\right] \\
=& -\log p_{\theta}(y_0|x_0)+ \nonumber \\
& \qquad \mathbb E_{q(y_{1:t},x_{1:t}|y_0, x_0)}\left[\log\frac{q(y_{1:t},x_{1:t}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:t}|x_0)/p_{\theta}(y_0|x_0)}\right] \\
%=& -\log p_{\theta}(y_0|x_0)+\mathbb E_{q(y_{1:t},x_{1:t}|y_0, x_0)}\left[\log\frac{q(y_{1:t},x_{1:t}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:t}|x_0)}+\log p_{\theta}(y_0|x_0)\right] \\
=& \mathbb E_{q(y_{1:t},x_{1:t}|y_0, x_0)}\left[\log\frac{q(y_{1:t},x_{1:t}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:t}|x_0)}\right].
\end{align}
\end{proof}

\begin{theorem}[Closed-form expression]%\label{theorem:1}
% The loss function in \cref{eq:3.3} has a closed-form representation.
The loss function in Equation (13) in the main paper has a closed-form representation.
%
The training is equivalent to optimizing a KL-divergence up to a non-negative constant, \textit{i.e.},
%
\begin{align}
\mathcal L_{VLB}=\mathbb E_{q(y_0,x_t|x_0)}\left[D_{KL}(q(y_t|y_0)\|p_{\theta}(y_t|x_t))\right] + C\sqq{.}%\label{eq:3.4}
\end{align}
\end{theorem}

\begin{proof}
% By the factorization in \cref{eq:3.1} and \cref{eq:3.2}, we observe that
By the factorization in Equation (6) and (11) in the main paper, we observe that
%
\begin{align}
\mathcal L_{VLB}&=\mathbb E_{q(y_{0:t},x_{1:t}|x_0)}\left[\log\frac{q(y_{1:t},x_{1:t}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:t}|x_0)}\right] \\
&=\mathbb E_{q(y_{0:t},x_{1:t}|x_0)}\left[\log\frac{1}{p_{\theta}(y_t|x_t)}+\sum_{j=1}^t\log\frac{q(y_j|y_{j-1})}{q(y_{j-1}|y_j)}\right].
\end{align}
%
Using Bayes' rule, for any $j=1,2,\cdots,t$, we have
%
\begin{align}
%\frac{q(y_j|y_{j-1})}{q(y_{j-1}|y_j)}&=\frac{q(y_j|y_{j-1})\cdot q(y_j)\cdot q(y_{j-1})}{q(y_{j-1}|y_j)\cdot q(y_j)\cdot q(y_{j-1})}\\
%&=\frac{q(y_j,y_{j-1})q(y_j)}{q(y_{j-1},y_j)q(y_{j-1})}=\frac{q(y_j)}{q(y_{j-1})}.
\frac{q(y_j|y_{j-1})}{q(y_{j-1}|y_j)}=\frac{q(y_j)}{q(y_{j-1})}, \quad \frac{q(y_t)}{q(y_0)}=\frac{q(y_t|y_0)}{q(y_0|y_t)}.
\end{align}

Hence\sqq{,} it is equivalent to optimizing the KL-divergence up to a non-negative constant $C$:
%
\begin{align}
\mathcal L_{VLB}&=\mathbb E_{q(y_{0:t},x_{1:t}|x_0)}\left[\log\frac{q(y_t|y_0)}{p_{\theta}(y_t|x_t)}+\log\frac{1}{q(y_0|y_t)}\right] \\
&=\mathbb E_{q(y_0,x_t|x_0)}\left[D_{KL}(q(y_t|y_0)\|p_{\theta}(y_t|x_t))\right] + C,
\end{align}
%
where $C=\mathbb E_{q(y_t)}\left[H(q(y_0|y_t))\right]\geqslant0$ and $H$ is the entropy of a distribution.
%
Since $q(y_t|y_0)$ follows a Gaussian distribution, then so is optimal $p_{\theta}(y_t|x_t)$.
\end{proof}


% \begin{theorem}[Optimal solution to \cref{eq:3.4}]%\label{theorem:2}
\begin{theorem}[Optimal solution to Equation (15) in the main paper]%\label{theorem:2}
The optimal $p_{\theta}(y_t|x_t)$ follows a Gaussian distribution with mean $\mu_{\theta}$ being
%
\begin{align}
\mu_{\theta}(x_t) = \sqrt{\balpha{t}}y_0.
\end{align}
\end{theorem}

\begin{proof}
% To minimize the KL-divergence in \cref{eq:3.4}, we first notice that $q(y_t|y_0)$ follows a Gaussian distribution, i.e.,
To minimize the KL-divergence in Equation (15) in the main paper, we first notice that $q(y_t|y_0)$ follows a Gaussian distribution, \textit{i.e.},
%
\begin{align}
q(y_t|y_0)\sim\mathcal N(y_t;\sqrt{\balpha{t}}y_0,(1-\balpha{t})I),\quad \mu_t(y_t)=\sqrt{\balpha{t}}y_0,
\end{align}
%
which implies that $
p_{\theta}(y_t|x_t)\sim\mathcal N(y_t;\mu_{\theta}(x_t),\Sigma_{\theta}(x_t))$
%
with mean $\mu_{\theta}(x_t)=\mu_t(y_t)=\sqrt{\balpha{t}}y_0$.
\end{proof}

\revise{
\begin{lemma}
We have an upper bound of the negative log-likelihood of $-\log p_{\theta}(y_0|x_0)$ by
%
\begin{align}
-\log p_{\theta}(y_0|x_0)\leqslant\mathbb E_q\left[\log\frac{q(y_{1:t},x_{1:s}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:s}|x_0)}\right],
\end{align}
%
where $q=q(y_{1:t},x_{1:s}|y_0, x_0).$
\end{lemma}
}

\revise{
\begin{proof}
\begin{align}
&-\log p_{\theta}(y_0|x_0) \\
\leqslant& -\log p_{\theta}(y_0|x_0) + \nonumber \\
& \qquad D_{KL}\left(q(y_{1:t},x_{1:s}|y_0, x_0) \| p_{\theta}(y_{1:t},x_{1:s}|y_0, x_0)\right) \\
=& -\log p_{\theta}(y_0|x_0)+\nonumber  \\
& \qquad \mathbb E_{q(y_{1:t},x_{1:s}|y_0, x_0)}\left[\log\frac{q(y_{1:t},x_{1:s}|y_0, x_0)}{p_{\theta}(y_{1:t},x_{1:s}|y_0, x_0)}\right] \\
=& -\log p_{\theta}(y_0|x_0)+ \nonumber \\
& \qquad \mathbb E_{q(y_{1:t},x_{1:s}|y_0, x_0)}\left[\log\frac{q(y_{1:t},x_{1:s}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:s}|x_0)/p_{\theta}(y_0|x_0)}\right] \\
=& \mathbb E_{q(y_{1:t},x_{1:s}|y_0, x_0)}\left[\log\frac{q(y_{1:t},x_{1:s}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:s}|x_0)}\right].
\end{align}
\end{proof}
}

\revise{
\begin{theorem}[Closed-form expression]
The loss function in Equation (23) in the main paper has a closed-form representation.
%
The training is equivalent to optimizing a KL-divergence up to a non-negative constant, \textit{i.e.},
%
\begin{align}
\mathcal L_{VLB}=\mathbb E_{q(y_0,x_s|x_0)}\left[D_{KL}(q(y_t|y_0)\|p_{\theta}(y_t|x_s))\right] + C
\sqq{.}
\end{align}
\end{theorem}
}

\revise{
\begin{proof}
By the factorization in Equation (20) and (21) in the main paper, we observe that
%
\begin{align}
\mathcal L_{VLB}&=\mathbb E_{q(y_{0:t},x_{1:s}|x_0)}\left[\log\frac{q(y_{1:t},x_{1:s}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:s}|x_0)}\right] \\
&=\mathbb E_{q(y_{0:t},x_{1:s}|x_0)}\left[\log\frac{1}{p_{\theta}(y_t|x_s)}+\sum_{j=1}^t\log\frac{q(y_j|y_{j-1})}{q(y_{j-1}|y_j)}\right].
\end{align}
%
Using Bayes' rule, for any $j=1,2,\cdots,t$, we have
%
\begin{align}
%\frac{q(y_j|y_{j-1})}{q(y_{j-1}|y_j)}&=\frac{q(y_j|y_{j-1})\cdot q(y_j)\cdot q(y_{j-1})}{q(y_{j-1}|y_j)\cdot q(y_j)\cdot q(y_{j-1})}\\
%&=\frac{q(y_j,y_{j-1})q(y_j)}{q(y_{j-1},y_j)q(y_{j-1})}=\frac{q(y_j)}{q(y_{j-1})}.
\frac{q(y_j|y_{j-1})}{q(y_{j-1}|y_j)}=\frac{q(y_j)}{q(y_{j-1})}, \quad \frac{q(y_t)}{q(y_0)}=\frac{q(y_t|y_0)}{q(y_0|y_t)}.
\end{align}
%
Hence\sqq{,} it is equivalent to optimizing the KL-divergence up to a non-negative constant $C$:
%
\begin{align}
\mathcal L_{VLB}&=\mathbb E_{q(y_{0:t},x_{1:s}|x_0)}\left[\log\frac{q(y_t|y_0)}{p_{\theta}(y_t|x_s)}+\log\frac{1}{q(y_0|y_t)}\right] \\
&=\mathbb E_{q(y_0,x_s|x_0)}\left[D_{KL}(q(y_t|y_0)\|p_{\theta}(y_t|x_s))\right] + C,
\end{align}
%
where $C=\mathbb E_{q(y_t)}\left[H(q(y_0|y_t))\right]\geqslant0$ and $H$ is the entropy of a distribution.
%
Since $q(y_t|y_0)$ follows a Gaussian distribution, then so is the optimal $p_{\theta}(y_t|x_s)$.
\end{proof}
}


\revise{
\begin{theorem}[Optimal solution to Equation (25) in the main paper]
The optimal $p_{\theta}(y_t|x_s)$ follows a Gaussian distribution with mean $\mu_{\theta}$ being
%
\begin{align}
\mu_{\theta}(x_s) = \sqrt{\balpha{t}}y_0.
\end{align}
\end{theorem}
}

\revise{
\begin{proof}
% To minimize the KL-divergence in \cref{eq:3.4}, we first notice that $q(y_t|y_0)$ follows a Gaussian distribution, i.e.,
To minimize the KL-divergence in Equation (25) in the main paper, we first notice that $q(y_t|y_0)$ follows a Gaussian distribution, \textit{i.e.},
%
\begin{align}
q(y_t|y_0)\sim\mathcal N(y_t;\sqrt{\balpha{t}}y_0,(1-\balpha{t})I),\quad \mu_t(y_t)=\sqrt{\balpha{t}}y_0,
\end{align}
%
which implies that $
p_{\theta}(y_t|x_s)\sim\mathcal N(y_t;\mu_{\theta}(x_s),\Sigma_{\theta}(x_s))$
%
with mean $\mu_{\theta}(x_s)=\mu_t(y_t)=\sqrt{\balpha{t}}y_0$.
\end{proof}
}


\input{tables/multi-step.tex}
\input{tables/ablation_s_t_star.tex}


\section{\newrevise{Comparison between multi-step and asymmetric \method}}


\newrevise{
\yr{\textbf{Multi-step \method.}} To implement the multi-step \method, due to the use of the vanilla DDPM, which is only capable of inputting a 3-channel input intermediate noisy image, we train an auxiliary UNet model to fuse the $y_{t/2}$ transformed from $x_{t/2}$ together with the $y'_{t/2}$ denoised from the $y_{t}$.
}


\newrevise{
In order to train the fusion UNet processing the intermediate noisy images $y_{t/2}$ and $y'_{t/2}$, we first need to prepare the dataset.
%
In more details, given a paired data $(x_0,y_0)$, the preset timestep $t$, noise $z$ under standard Gaussian distribution, the pre-trained \method $G$, and the pre-trained diffusion model, we first apply the diffusion forward process onto both $x_0$ and $y_0$ with noise $z$ until timestep $t$ and $t/2$, \textit{i.e.}, we acquire the $x_{t/2}$, $x_{t}$, $y_{t/2}$, and $y_{t}$.
%
Next, we utilize the pre-trained \method model to obtain the transformed $G(x_{t/2})$ and $G(x_{t})$.
%
Then, we apply the reverse process via the pre-trained diffusion model to achieve the denoised result from $G(x_{t})$, denoted by $D(G(x_{t}))$.
%
Finally, repeating the process above with different paired $(x_0,y_0,z)$, we are able to acquire the dataset for the fusion UNet.
%
Note that similar to the training of \method, the fusion UNet are aimed to deal with \textit{noisy} images.
%
That is to say, empirically we need much more data samples and training epochs since CNN may easily fail on noisy data.
%
For the CelebA-HQ dataset with 30,000 images, we train the fusion UNet with more than 200,000 samples.
%
As a comparison, we train \method within only 60 epochs with 27,000 data samples.
}


\newrevise{
The comparisons between multi-step and our single-step design are reported in \cref{tab:metric} and \cref{tab:time}.
%
From \cref{tab:metric} we observe that under the same timestep, the FID score of multi-step \method is worse than single-step in most cases, and multi-step \method does not significantly benefit from the additional step of performing the fusion UNet.
%
As for SSIM score, there is indeed performance improvement to some extent compared to the vanilla DMT, which is mainly due to doubling the information during denoising process and taking advantage of the fusion UNet.
%
Considering the dramatic additional time cost discussed below, we regard that the vanilla single-step DMT is an adequate solution to the I2I task. 
%
It is noteworthy that for multi-step \method, both the training (including \method and Fusion training) and inference costs increase significantly, as shown in \cref{tab:time}, which confirms the superiority of the single-step \method proposed in the paper.
}


\newrevise{
\yr{\textbf{Asymmetric \method.}}
As for asymmetric \method, in addition to the theoretical analysis in the main paper, we also conducted a comprehensive ablation study focusing on the performance at various timestep pairs $(s,t)$ near $s=t$.
%
As shown in \cref{tab:ablation_s_t_sstar,tab:ablation_s_t_tstar}, our proposed strategy (pair with $s=t$) is capable of achieving on-par or even superior performance across various $(s,t)$ alternatives.
}


\section{More results}

This part shows more qualitative results and compares our \method with existing I2I approaches, including Pix2Pix (GAN-based)~\cite{isola2017image}, TSIT (GAN-based)~\cite{jiang2020tsit} and Palette (DDPM-based)~\cite{saharia2021palette}.
%
We perform evaluation on the tasks of stylization (\cref{fig:portrait_qmupd0-1-0}), image colorization (\cref{fig:afhq}), segmentation to image (\cref{fig:celeba}), and sketch to image (\cref{fig:edges2handbags}), using our handcrafted Anime dataset, AFHQ~\cite{choi2020stargan}, CelebA-HQ~\cite{karras2018progressive}, and Edges2handbags~\cite{zhu2016generative,xie15hed}, respectively.
%
Our method surpasses the other three competitors with higher fidelity (\textit{e.g.}, clearer contours\newrevise{, less artifacts} and more realistic colors \newrevise{as highlighted}), suggesting that our \method manages to bridge the content information provided by the input \revise{condition} and the domain knowledge contained in the pre-trained DDPM.


\input{figures/fig_portrait_qmupd0-1-0/fig.tex}
\input{figures/fig_afhq/fig.tex}
\input{figures/fig_celeba/fig.tex}
\input{figures/fig_edges2handbags/fig.tex}

