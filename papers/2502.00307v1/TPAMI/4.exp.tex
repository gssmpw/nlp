\section{Experiments}\label{sec:exp}


In this section, we evaluate the proposed \method \sqq{on} four different I2I tasks: image stylization, colorization, segmentation to image, and sketch to image.
%
We first show that the \method is capable of mapping translation between the two domains of an I2I task in \cref{subsec:Quality}.
%
Then\sqq{,} we compare the DMT with \sqq{several} representative methods to demonstrate its superior efficiency and performance in \cref{subsec:Comparison}.
%
Finally, we provide an ablation study on the effect of the timestep $t$ for training in \cref{subsec:Ablation}.

\subsection{Experimental setups}\label{subsec:Setup}

\noindent\textbf{Datasets and tasks.}
%
We train the I2I task on four datasets: our handcrafted Portrait dataset using CelebA-HQ by QMUPD~\cite{YiLLR22}, AFHQ~\cite{choi2020stargan}, CelebA-HQ~\cite{karras2018progressive}, and Edges2handbags~\cite{zhu2016generative,xie15hed}.
%
All the images are resized to $256\times256$ resolution.
%
Our Portrait dataset consists of 27,000 \sqq{images for training} and 3,000 images for inference; all these images are generated from \sqq{the} CelebA-HQ dataset using \sqq{a} pretrained QMUPD model.
%
The AFHQ dataset consists of 14,630 images for training and 1,500 images for inference, \sqq{encompassing a variety of cats, dogs, and wild animal images.}
%
For \sqq{the} CelebA-HQ dataset, we randomly choose 27,000 images together with their segmentation masks as the paired training data, while the remaining 3,000 images are used as test data.
%
As for Edges2handbags, we use \sqq{all} 138,567 images as training data and the 200-image test data for inference.

\noindent\textbf{Evaluation metrics.} 
%
We use Fr\'{e}chet Inception Distance (FID)~\cite{heusel2017gans}, Structure Similarity Index Measure (SSIM)~\cite{wang2004image}, LPIPS~\cite{zhang2018unreasonable}, $L1$ and $L2$ metrics to evaluate the fidelity of the generated images and how well the content information is kept after the translation.
%
Besides, we compare all the methods in a user study, where users were asked to score the image quality from 1 to 5.
%
We also compare the training and inference efficiency of all the methods by comparing the number of total training epochs, training speed for 1,000 images, and inference time for generating an image.


\noindent\textbf{Baselines.}
%
We compare our proposed \method algorithm with \revise{five} representative I2I algorithms:  Pix2Pix~\cite{isola2017image}, TSIT~\cite{jiang2020tsit}, SPADE~\cite{park2019SPADE}, QMUPD~\cite{YiLLR22}, and Palette~\cite{saharia2021palette}.
%
\revise{The alternatives can be divided into two categories: GAN-based and DDPM-based algorithms.}
%
Pix2Pix is a classic cGAN-based method involving $L_1$ and adversarial loss.
%
TSIT is a \revise{GAN-based} versatile framework using specially designed normalization layers and coarse-to-fine feature transformation.
%
SPADE is a \revise{GAN-based} specially-designed framework for semantic image synthesis with spatially-adaptive normalization.
%
QMUPD \revise{is also GAN-based, which} is specially designed for portrait stylization by unpaired training. \sqq{We train} the model with paired data for fair comparison.
%
Palette introduces the DDPM~\cite{ho2020denoising} framework into the I2I task and injects the input constraint to each step of \sqq{the} denoising process.

\noindent\textbf{Implementation details.}
%
We train the proposed \method module on the platform of PyTorch~\cite{paszke2019pytorch}, in a Linux environment with an NVIDIA Tesla A100 GPU.
%
We set total timestep $T=1000$ for all the experiments, the same setting as in~\cite{ho2020denoising}.
%
We train the reverse denoising process of the DDPM using a U-Net backbone together with the Transformer sinusoidal embedding~\cite{ronneberger2015u,vaswani2017attention}, following~\cite{dhariwal2021diffusion}.
%
The DDPM is frozen during the training of the \method module.
%
To train the \method module, we use the Pix2Pix~\cite{isola2017image} and TSIT~\cite{jiang2020tsit} model.  We remove the discriminator model and train only the generator block to ensure that the translator $f_{\theta}$ has approximately the same functional form as the real mapping.
%
Note that our \method employs the DDPM denoising process during sampling, which employs hundreds of iterative function evaluations for denoising and can be \sqq{time-consuming}.
%
Therefore, we apply DDIM~\cite{song2020denoising} for acceleration, which realizes high-quality synthesis within \sqq{10 function evaluations} (NFE = 10).


\setlength{\tabcolsep}{5pt}
\input{tables/spade.tex}
\input{figures/fig_spade/fig.tex}


\subsection{Qualitative evaluation on various tasks}\label{subsec:Quality}

\setlength{\tabcolsep}{5pt}
\input{tables/qmupd.tex}
\input{figures/fig_qmupd/fig.tex}

\input{figures/fig_ablation_t_qualitative/fig.tex}
\input{tables/comparison.tex}

The process of inferring images \sqq{with} \method consists of the following three simple steps.
%
\begin{itemize}
\item[1)] We apply the forward diffusion process to the input image $x_0$ until the pre-selected timestep $t$ to obtain $x_t$, which can be written as $x_t=\sqrt{\balpha{t}}x_0+\sqrt{1-\balpha{t}}z_t$;
%
\item[2)] By obtaining the mean by the functional approximator $f_{\theta}$ according to \cref{eq:3.5}, we infer the approximated $y_t$ by adding another Gaussian noise;
%
\item[3)] Using $y_t$ as the intermediate result, sampling with the given pre-trained DDPM by the reverse process achieves the required output.
\end{itemize}

We conducted four experiments to evaluate our proposed \method on four datasets, \textit{i.e.}, our handcrafted Portrait dataset, AFHQ~\cite{choi2020stargan}, CelebA-HQ~\cite{karras2018progressive}, and Edges2handbags~\cite{zhu2016generative,xie15hed}.
%
In training\sqq{,} we use 40 epochs for the sketch-to-image task, and 60 epochs for the other three tasks.%, which are the same setting as used in Pix2Pix~\cite{isola2017image} and TSIT~\cite{jiang2020tsit} model.
%
As shown in \cref{fig:teaser}, our method is capable of learning the cross-domain translation mapping and generates high-quality images.
%
For example, in the stylization task, the shared encoder is able to distinguish the two different forward diffusion processes of the two domains.
%
In the other tasks, our method can still extract the \revise{input} feature and generate photo-realistic images with high diversity \sqq{even with little input \revise{condition} information}
%
More results can be found in Appendix C.

\input{figures/fig_comparison/fig.tex}

\subsection{Comparisons}\label{subsec:Comparison}

We qualitatively and quantitatively compare our method with the four classic I2I methods:  Pix2Pix~\cite{isola2017image}, TSIT~\cite{jiang2020tsit}, SPADE~\cite{park2019SPADE}, QMUPD~\cite{YiLLR22}, and the DDPM-based conditional generation method Palette~\cite{saharia2021palette}.
%
First, we compare with SPADE~\cite{park2019SPADE}.
%
It requires category-wise segmentation masks, limiting its application \sqq{to} most I2I tasks.
%
Note that our proposed \method introduces the shared encoder by gradually adding noise onto the original images, which \sqq{corrupts} the semantic information from the category-wise segmentation masks.
%
Hence\sqq{,} we only compare with SPADE on segmentation-to-image task, without applying the \method on top of it.
%

\sqq{T}he results are shown in \cref{fig:spade} and \cref{tab:spade}.

We also compare with the specially-designed stylization algorithm QMUPD~\cite{YiLLR22}.
%
It introduces a quality metric guidance for portrait generation using unpaired training data.
%
We train QMUPD with paired data for fair comparison, which reduces the training difficulty and achieves a stronger baseline. 
%
\sqq{The results, presented in \cref{fig:qmupd} and \cref{tab:qmupd}, demonstrate that our approach achieves performance that is on par with, or even surpasses, existing standards.}
% The results are shown in \cref{fig:qmupd} and \cref{tab:qmupd}, from which one can see that we reach on-par or even better performance.

Then, we compare with Palette~\cite{saharia2021palette} using the open source implementation\footnote{https://github.com/Janspiry/Palette-Image-to-Image-Diffusion-Models}.
%
As shown in \cref{fig:comparison}, we observe that the results of Palette fail to extract the segmentation feature of CelebA-HQ and Edges2handbags dataset\sqq{. Consequently, this leads to an inability to accurately generate details in the background of human images or replicate the horse pattern on the bags.}
% , and therefore cannot generate the details of the background of an image of a human or the pattern of horse on the bag.
%
As a comparison, our proposed \method can generate \sqq{high-quality} images \sqq{and preserve} the semantic information of the input \revise{condition}, even \sqq{when} given little input semantic information.

Next\sqq{,} we compare with Pix2Pix~\cite{isola2017image}.
%
We observe that our method can generate images of much higher quality than the Pix2Pix method.
%
For instance, the generated images of Pix2Pix suffer from severe artifacts over the facial region in the CelebA-HQ datasets, while our method consistently produces high\sqq{-}quality results.
%
Moreover, the \sqq{feature extraction performance} is significantly improved by the shared encoder and the well-prepared DDPM model in our method.

We finally compare with TSIT~\cite{jiang2020tsit}.
%
Although TSIT introduces a coarse-to-fine feature transformation block and hence can synthesize high-quality images in most cases, it fails to produce results with sufficient and satisfying semantics and textures when given very little inference information (\textit{e.g.}, hair and forehead region of segmentation).
%
In contrast, the results of \method have clear \sqq{boundaries} at the forehead and hair region, together with rich texture.

The quantitative results are reported in \cref{tab:quantitative_comparison}, showing that our method has the best image fidelity (FID), the lowest perceptual loss (LPIPS), and comparable structural similarity (SSIM).
%
Regarding the training and inference speed, our method uses the smallest number of training epochs and has \sqq{the} fastest training speed for generating 1,000 images, because it only needs to train one translation module.
%
The \method is also 40x $\sim$ 80x faster than Palette~\cite{saharia2021palette} due to starting the sampling process at an intermediate step (4x $\sim$ 8x faster) and the use of the fast sampling algorithm DDIM ($\sim$10x faster).

\subsection{Ablation study on the timestep for domain translation}\label{subsec:Ablation}

\input{tables/ablation.tex}


\begin{table*}[t]
\begin{minipage}[t]{0.44\textwidth}
\scriptsize
\setlength{\tabcolsep}{10pt}
\caption{
    \revise{
    \textbf{Ablation study} of $t$ near $t^*$ on AFHQ dataset.
    }
}
\label{tab:ablation_t_star_afhq}
\vspace{-10pt}
\begin{tabular}{l|ccccc}
\toprule
\revise{$t$} & \revise{0} & \revise{5 ($t^*$)} & \revise{10} & \revise{15} & \revise{25} \\
\midrule
\revise{\bf FID$\downarrow$} & \revise{13.28} & \revise{\textbf{13.03}} & \revise{13.61}  & \revise{13.92}  & \revise{14.62}  \\
\revise{\bf SSIM$\uparrow$}  & \revise{\textbf{0.708}} & \revise{0.684} & \revise{0.680}  & \revise{0.620}  & \revise{0.537}  \\
\bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{0.55\textwidth}
\scriptsize
\setlength{\tabcolsep}{8pt}
\caption{
    \revise{
    \textbf{Ablation study} of $t$ near $t^*$ on CelebA-HQ dataset.
    }
}
\label{tab:ablation_t_star_celebahq}
\vspace{-10pt}
\begin{tabular}{l|ccccccc}
\toprule
\revise{$t$}                 &   \revise{180} &   \revise{190} &       \revise{195} & \revise{200 ($t^*$)} &   \revise{205} &   \revise{210} &       \revise{220} \\
\midrule
\revise{\bf FID$\downarrow$} & \revise{37.93} & \revise{38.48} &     \revise{36.46} &       \revise{36.78} & \revise{37.09} & \revise{39.62} & \revise{\bf 36.04} \\
\revise{\bf SSIM$\uparrow$}  & \revise{0.450} & \revise{0.438} & \revise{\bf 0.455} &       \revise{0.446} & \revise{0.420} & \revise{0.418} &     \revise{0.432} \\
\bottomrule
\end{tabular}
\end{minipage}
\end{table*}

\sqq{In} the \method algorithm\sqq{,} we first gradually add noise \sqq{for both $x_0$ and $y_0$ using a shared decoder} until some preset timestep $t$.
%
Here, the timestep $t$ plays a critical role \sqq{in} the performance of the translator $f_{\theta}$ as well as the quality of the generated images.
%
As discussed in \cref{subsec:Optimal}, we proposed a simple method to determine an adequate timestep before training, denoted by $t=t^*$, by pre-computing the distance between $(x_0, x_t)$ and between $(x_t, y_t)$.
%
In this section, we compare the generation quality using different timesteps $t$ and show that the timestep $t^*$ selected using our method in \cref{subsec:Optimal} offers the optimal performance.

\sqq{In} \cref{fig:ablation_t_qualitative}, \sqq{we} observe that:
%
(1) As the translation timestep $t$ increases, the input \revise{condition} provides weaker constraint to the output generation.
%
For instance, the face poses of the results in row 1 and row 3 begin to change in an unwarranted way when $t>400$;
%
(2) When the translation timestep $t$ is small, the translation mapping can hardly approximate the real distribution (\newrevise{\textit{e.g.}}, the hair texture of the segmentation to image task in row 3, column 3).

We also present quantitative comparison results in \cref{tab:quantitative_ablation}, from which we see the trade-off between the strength of the input \revise{condition} and the difficulty \sqq{of learning} the translation mapping.
%
\sqq{Significantly, our} method for selecting an appropriate timestep achieves performance comparable to using the optimal $t$ shown in \cref{tab:quantitative_ablation}. This confirms the effectiveness of our simple selection strategy.

\revise{
We conduct further ablation study on the performance of timestep $t$ near the preset timestep $t^*$, \sqq{in order} to \sqq{demonstrate} the strong robustness of our strategy.
%
As shown in \cref{tab:ablation_t_star_afhq,tab:ablation_t_star_celebahq}, despite the significant performance drop when using different timesteps, our strategy is still able to search an adequate timestep for \method.
}

\subsection{Limitations}\label{subsec:Discussion}

Our \method method has several limitations \sqq{that are} interesting avenues for future research.
%
First, our algorithm is based on the assumption that both the forward and the reverse process satisfy the Markovian property, but this assumption holds only for the DDPM or its extension. %[{\bf Please name a model, as an example, that does not meet this assumption.} ]
%
Second, the \method is designed to train with paired data due to its reliance on using  Pix2Pix~\cite{isola2017image} or TSIT~\cite{jiang2020tsit} module as the translation mapping $f_{\theta}$.
%
Hence, our method cannot be applied to unpaired training data and related I2I tasks.
%
Third, our \method is not applicable to tasks whose condition (source domain) and the target domain are almost identical. \sqq{We briefly explain this limitation next.}
%
Following \cref{eq:3.3}, when $x_0$ equals $y_0$, we have $q(y_0|x_0) = \delta_{x_0}(y_0)$, which is the Dirac distribution.
%
Then\sqq{,} \cref{eq:3.3} becomes
%
\begin{align}
\mathcal L_{CE}=\log p_\theta(x_0|x_0) = 0,
\end{align}
%
which is a constant independent of the model parameter $\theta$. Therefore\sqq{,} the model cannot be optimized.
% Third, on the tasks of "segmentation2image" and "sketch2image", the input itself lacks diversity, which may cause poor divergence of synthesis.
%
% One feasible solution is to introduce the spatial stochasticity into the intermediate features with some network designs.
%
% For example, we can encode a spatial noise and use it to modulate the features with spatial adaptive instance normalization (AdaIN)~\cite{huang2017adain}.
%
% Future research will devote to how to accomplish the approximation of the translation mapping trained on unpaired datasets and tasks in which the condition and the target domain are almost the same.
%, and improve the synthesis divergence.

% [{\bf Several different words, "shift", "transfer", and "translation", are used to mean the same thing in the paper. Could you try to stick to one word, as much as possible, to make it more consistent? Translate, translation, etc, would be the best choice, to echo Translator in the title.} ]



