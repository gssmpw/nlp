\section{Method}\label{sec:method}

\subsection{Markov process of translation mappings}\label{subsec:Markov_Process}

For an I2I task, traditional DDPM methods directly approximate the real distribution $q(y_0|x_0)$ in which $x_0, y_0$ are paired data from the source domain ${\mathcal D}_x$ and the target domain ${\mathcal D}_y$, respectively.
%
In contrast, we construct a translation module $p_{\theta}(y_t|x_t)$, which bridges the input \revise{condition} and the pre-trained DDPM.
%
\sqq{Accordingly}, we can approximate the $q(y_0|x_0)$ using the learned intermediate translation module.
%
Specifically, given a noise-adding schedule of the forward variance process $\beta_i\in[0,1],t=1,2,\cdots T$, $\alpha_i=1-\beta_i$ and $\balpha{t}=\prod_{i=1}^t\alpha_t$, we first generalize the forward Markov process to the joint distribution of $(x_{1:t}, y_{1:t})$ as below:
%
\begin{align}
q(y_{1:t},x_{1:t}|y_0, x_0)& =\prod_{i=1}^t q(x_i|x_{i-1})\prod_{j=1}^t q(y_j|y_{j-1}), \label{eq:3.1} \\
q(x_i|x_{i-1})& \sim\mathcal N(x_i;\sqrt{\alpha_i}x_{i-1},\beta_i I), \\
q(x_t|x_0)& \sim\mathcal N(x_t;\sqrt{\balpha{t}}x_0,(1-\balpha{t}) I), \\
q(y_j|y_{j-1})& \sim\mathcal N(y_j;\sqrt{\alpha_i}y_{i-1},\beta_i I), \\
q(y_t|y_0)& \sim\mathcal N(y_t;\sqrt{\balpha{t}}y_0,(1-\balpha{t}) I).
\end{align}

The corresponding DDPM trained on the target domain provides a reverse Markov process to approximate $q(y_0)$ from a sample $y_T$ \sqq{drawn} from the standard Gaussian distribution, \newrevise{\textit{i.e.},} $y_T\sim\mathcal N(y_T;0,I)$.
%
Note that during the denoising process, $y_i$ is only determined by $y_{i+1}$ and irrelevant to $x_{0:t}$ for $i\in[0,t-1]$. We choose to construct the translation mapping at some specified step\footnote{The selection of this specified step is discussed in Section \ref{sec:exp}.} of the diffusion forward process using $p_{\theta}(y_t|x_t)$, which induces the following Markov process:
%
% \begin{align}
% p_{\theta}(y_{i:t},x_{1:t}|x_0)&=q(y_i|y_{i+1})p_{\theta}(y_{i+1:t},x_{1:t}|x_0),
% \end{align}

\begin{align}
p_{\theta}(y_{0:t},x_{1:t}|x_0)=p_{\theta}(y_t|x_t)\prod_{i=1}^t q(x_i|x_{i-1})\prod_{j=1}^t q(y_{j-1}|y_j), \label{eq:3.2}
\end{align}
%
where $q(y_{j-1}|y_j)$ is the denoising process of the pre-trained DDPM. 
%[{\bf Is it true the denoising step is normally denoted by $p(y_{j-1}|y_j)$, rather than $q(y_{j-1}|y_j)$, in the literature?} ]

\subsection{Translation mappings of DDPM}\label{subsec:Transfer_Mappings}

\sqq{Let} $p_{\theta}(y_0|x_0)=\int p_{\theta}(y_{0:t},x_{1:t}|x_0)dy_{1:t}dx_{1:t}$ \sqq{represent} the sampling distribution of $q(y_0|x_0)$\sqq{, where} $p_{\theta}(y_t|x_t)$ \sqq{serves to bridge} the two domains.
%
By making use of the variational lower bound to optimize the negative log-likelihood, we have the following lemma:
%
\begin{lemma}\label{lem:1}
The negative log-likelihood of $-\log p_{\theta}(y_0|x_0)$ has the following upper bound,
%
\begin{align}
-\log p_{\theta}(y_0|x_0)\leqslant\mathbb E_q\left[\log\frac{q(y_{1:t},x_{1:t}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:t}|x_0)}\right],
\end{align}
%
where $q=q(y_{1:t},x_{1:t}|y_0, x_0).$
\end{lemma}
%
In other words, the translation mapping can be learned by optimizing the variational lower bound:
%
\begin{align}\label{eq:3.3}
\mathcal L_{CE}&=\mathbb -\mathbb E_{q(y_0|x_0)}\left[\log p_{\theta}(y_0|x_0)\right]\\
&\leqslant\mathbb E_{q(y_{0:t},x_{1:t}|x_0)}\left[\log\frac{q(y_{1:t},x_{1:t}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:t}|x_0)}\right]:=\mathcal L_{VLB}.
\end{align}


First, we claim that the optimal $p_{\theta}(y_t|x_t)$ follows a Gaussian distribution up to a non-negative constant of \cref{eq:3.3}.

\begin{theorem}[Closed-form expression]\label{theorem:1}
The loss function in \cref{eq:3.3} has a closed-form representation.
%
The training is equivalent to optimizing a KL-divergence up to a non-negative constant, \textit{i.e.},
%
\begin{align}
\mathcal L_{VLB}=\mathbb E_{q(y_0,x_t|x_0)}\left[D_{KL}(q(y_t|y_0)\|p_{\theta}(y_t|x_t))\right] + C\sqq{.}\label{eq:3.4}
\end{align}
\end{theorem}

For the given closed-form expression in \cref{eq:3.4}, the optimal $p_{\theta}(y_t|x_t)$ follows a Gaussian distribution and its mean $\mu_{\theta}$ has an analytic form, as summarized in the \Cref{theorem:2} below:


\begin{theorem}[Optimal solution to \cref{eq:3.4}]\label{theorem:2}
The optimal $p_{\theta}(y_t|x_t)$ follows a Gaussian distribution with its mean being
%
\begin{align}
\mu_{\theta}(x_t) = \sqrt{\balpha{t}}y_0.
\end{align}
\end{theorem}

Detailed proofs of the above lemma and theorems are provided in Appendix B.

\subsection{Reparameterization of $\mu_{\theta}$}\label{subsec:Analytic}

Given the DDPM trained on the target domain, we first apply the same diffusion forward process on both $x_0$ and $y_0$ as a shared encoder \sqq{to represent the mean $\mu_{\theta}(x_t)$}:
%
\begin{align}\label{eq:3.6}
x_t=\sqrt{\balpha{t}}x_0+\sqrt{1-\balpha{t}}z_t,\quad y_t=\sqrt{\balpha{t}}y_0+\sqrt{1-\balpha{t}}z_t.
\end{align}

\Cref{theorem:2} reveals that $\mu_{\theta}$ needs to approximate the expression $\sqrt{\balpha{t}}y_0$ with $x_t$ as the only available input. Then\sqq{,} we apply the following parameterization,
%
\begin{align}\label{eq:3.5}
\mu_{\theta}(x_t)=f_{\theta}(x_t)-\sqrt{1-\balpha{t}}z(x_t),
\end{align}
%
where $f_{\theta}$ is a trainable function and $z(x_t)=z_t$\sqq{,} which is set to the shared noise component of $x_0$ and $y_0$. % derived from the shared encoder.
%
The KL-divergence in \cref{eq:3.4} is optimized by minimizing the difference between the two means together with the variance $\Sigma_{\theta}$ of $p_{\theta}(y_t|x_t)$.
%
Noting that $\Sigma_{\theta}=(1-\balpha{t})I$, the objective function then has the following form,
%
\begin{align}
\mathcal L_t
%&=\mathbb E_q\left[\frac{1}{2(1-\balpha{t})}\|f_{\theta}(x_t)-(\sqrt{\balpha{t}}y_0+\sqrt{1-\balpha{t}}z_t)\|^2\right] \\
&=\mathbb E_q\left[\frac{1}{2(1-\balpha{t})}\|f_{\theta}(x_t)-y_t\|^2\right].
\end{align}

\cref{eq:3.5} implies that inferring $y_t\sim p_{\theta}(y_t|x_t)$ is to compute $f_{\theta}(x_t)-\sqrt{1-\balpha{t}}z_t+\sqrt{1-\balpha{t}}z$, where $z\sim\mathcal N(0,I)$.


\subsection{Determining an appropriate timestep for translation}\label{subsec:Optimal}

\input{figures/fig_ablation_t_quantitative/fig.tex}

Recall that we encode the same forward diffusion process onto both $x_0$ and $y_0$ using a shared encoder (ref. to \cref{eq:3.6}),
%
where $z_t$ is independent of $x_0$ and $y_0$.
%
As $t$ tends to $T$, $x_t$ and $y_t$ will converge to the same Gaussian noise simultaneously, since  $x_t,y_t\rightarrow z_T\sim\mathcal N(0,I)$.
%
Hence, as $t$ increases, the distance between $(x_t,y_t)$ will decrease and the distance between $(x_0,x_t)$ will increase.
%
In other words, the training of \method faces a trade-off between the gap between the two potential domains and the strength of the \revise{condition} signal.
%
The larger timestep $t$ makes it easier for the \method to learn the translation mapping, while the strength of inference information will be weakened since the injected noise corrupts the origin signal.


\newrevise{
To address this trade-off issue, we provide a theoretical analysis below.
%
Recall that our proposed diffusion-model-based I2I system consists of three sub-systems: (1) the forward diffusion process from $x_0$ to $x_t$, (2) \method from $x_t$ to $y_t$, and (3) the denoising process via pre-trained diffusion model from $y_t$ to $y_0$.
%
Our analysis is based on the following observation: the complexity $C$ of the whole system $S$ is determined by the maximal one among the complexities of three sub-systems $(S_1,S_2,S_3)$, \textit{i.e.}, $C(S)=\max\{C(S_1),C(S_2),C(S_3)\}$.
%
Given a timestep $t$, let $C(S_1)=f(t)$, $C(S_2)=g(t)$, $C(S_3)=h(t)$, where $f(t)$, $g(t)$ and $h(t)$ are complexity curves of diffusing $x_0$ to $x_t$, translating $x_t$ to $y_t$, and denoising $y_t$ to $y_0$ w.r.t. the timestep $t$, respectively.
%
First, we assume\footnote{\newrevise{This assumption is reasonable because the diffusion and denoising processes are reciprocal at the same time step, although in different domains.}} $f(t)\approx h(t)$.
%
Then $C(S)=\max\{f(t),g(t)\}$.
%
Second, we assume\footnote{\newrevise{This assumption is reasonable because the larger the time step, the greater the complexity of forward diffusion and the lower the complexity of \method.}} that $f(t)$ and $g(t)$ are monotone curves.
%
Then we have the conclusion that {\it $C(S)$ takes the minimum value at the intersection point of two monotone curves $f(t)$ and $g(t)$}.
}


\newrevise{
Accordingly, we propose a simple and effective strategy to determine an appropriate timestep $t$ before training.}
% To determine an appropriate timestep $t$ before training, we propose a simple and effective strategy.
%
We calculate the \revise{$L_1$, $L_2$, Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable}, Fr\'{e}chet Inception Distance (FID)~\cite{heusel2017gans}, and }Structure Similarity Index Measure (SSIM)~\cite{wang2004image} between $(x_t,y_t)$ and between $(x_0, x_t)$\revise{, among which SSIM achieves the timestep with the best performance.}
%
\sqq{The results shown in \cref{fig:ablation_t_quantitative} are consistent with our aforementioned findings}: the distance between $(x_t,y_t)$ drops rapidly, while the distance between $(x_0, x_t)$ grows monotonically as the timestep $t$ grows.
%
Note that the intersection point of the two curves \newrevise{offers a good approximation for the minimum of system complexity.} %between the distance of $(x_t,y_t)$ and that of $(x_0,x_t)$.
%
This observation provides us with a pre-selecting strategy that chooses the timestep $t$ of this intersection point as an appropriate timestep $t$ for domain transfer\sqq{.}
%
We demonstrate in \cref{subsec:Ablation} the performance of using the timestep $t$ thus chosen by this pre-selecting method.

To summarize, we train the \method module in the same way as a simple I2I task.
%
First, we gradually apply the same diffusion forward process onto both the input \revise{condition} and the desired output until a pre-selected timestep.
%
Then\sqq{,} we train the function approximator $f_{\theta}$ using a reparameterization strategy to reformulate the objective function.
%
We theoretically prove the feasibility of the simple \method module and show that the approximator $f_{\theta}$ resembles the reverse process mean function approximator in DDPM~\cite{ho2020denoising}.
%
We \sqq{verify} the efficiency of the \method in \cref{sec:exp} with comprehensive experiments on a wide range of datasets, and provide the algorithms and the pseudo-codes in Appendix A.


\input{figures/fig_method/fig_generalization.tex}


\subsection{\revise{Further discussion of \method}}\label{subsec:generalization}


\setlength{\tabcolsep}{10pt}
\begin{table*}[t]
% \renewcommand\arraystretch{1.2}
\centering
\caption{
    \revise{
    \textbf{Ablation study} on the preset timestep pair $(s,t)$ in our proposed \method on the two I2I tasks under different $\lambda$ defined in \cref{eq:s_t}. 
    %
    SSIM is used to evaluate the distance between samples.
    }
}
\label{tab:ablation_s_t}
\vspace{-2pt}
\begin{tabular}{cccc}
\toprule
\revise{Stylization} & \revise{Colorization} & \revise{Segmentation} & \revise{Sketch} \\
\midrule
\revise{$(s,t)=(50, 50)$} & \revise{$(s,t)=(5,5)$} & \revise{$(s,t)=(200, 200)$} & \revise{$(s,t)=(20, 20)$} \\
\bottomrule
\end{tabular}
\end{table*}


\newrevise{
Recall that} \revise{we introduce the shared encoder by diffusing both $x_0$ and $y_0$ with the identical timestep $t$.
%
\sqq{To} address the trade-off between the strength of content information and domain gap, we propose a strategy to automatically preset an adequate timestep $t^*$ to achieve equilibrium between the distances of $(x_0,x_t)$ and $(x_t,y_t)$.
}
%
\newrevise{
Therefore, one could reasonably consider to use \yr{(1)} multi-step translation results from \method to facilitate the denoising precess, or \yr{(2)} diffusion processes with distinct timesteps for the source and target domains\yr{,} as a strategy to mitigate trade-offs and achieve improved performance.
%
In this subsection, we discuss these two interesting alternatives, by fusing the \method results at \yr{multiple} timestep\yr{s} \yr{(\textit{e.g.},} $t$ and $t/2$) (\textit{i.e.}, \cref{fig:method_generalization} (a)), together with \sqq{using} the \textit{asymmetric} timestep pair $(s,t)$ (\textit{i.e.}, \cref{fig:method_generalization} (b)), \sqq{where} $x_0$ and $y_0$ \sqq{are diffused at timesteps} $s$ and $t$, $s\neq t$ respectively.
%
Given the results analyzed in this section, we conclude that the former multi-step method significantly increases training time cost while degrading the FID performance, and that the latter more complicated pipeline practically coincides with our proposed \method method, since the optimal timestep pair $(s,t)$ appears to be the same.
}


\newrevise{
To implement the multi-step \method, due to the use of the vanilla DDPM, which is only capable of inputting a 3-channel input intermediate noisy image, we train an auxiliary UNet model to fuse the $y_{t/2}$ transformed from $x_{t/2}$ together with the $y'_{t/2}$ denoised from the $y_{t}$.
%
However, we argue that the additional UNet significantly increases the training cost, while degrading the FID performance, due to additional error from the UNet.
%
Detailed experimental setups and quantitative comparison are provided in \supp.
}


\newrevise{
As for the asymmetric setting,
}
%
\revise{
\sqq{we define} the disjoint distribution of the forward Markov process of $(x_{1:s}, y_{1:t})$ as below:
%
\begin{align}
q(y_{1:t},x_{1:s}|y_0, x_0)& =\prod_{i=1}^s q(x_i|x_{i-1})\prod_{j=1}^t q(y_j|y_{j-1}), \label{eq:3.7} \\
p_{\theta}(y_{0:t},x_{1:s}|x_0)&=p_{\theta}(y_t|x_s)\prod_{i=1}^s q(x_i|x_{i-1})\prod_{j=1}^t q(y_{j-1}|y_j)\sqq{.} \label{eq:3.x}
\end{align}
%
We first claim the feasibility of this pipeline, whose proofs are addressed in \supp.
%
Similar to \Cref{lem:1}, \Cref{theorem:1,theorem:2}, we have
%
\begin{lemma}\label{lem:2}
The negative log-likelihood of $-\log p_{\theta}(y_0|x_0)$ has the following upper bound,
%
\begin{align}
-\log p_{\theta}(y_0|x_0)\leqslant\mathbb E_q\left[\log\frac{q(y_{1:t},x_{1:s}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:s}|x_0)}\right],
\end{align}
%
where $q=q(y_{1:t},x_{1:s}|y_0, x_0).$
\end{lemma}
%
We accordingly define the $\mathcal L_{VLB}$ as below:
%
\begin{align}\label{eq:3.8}
\mathcal L_{CE}&=\mathbb -\mathbb E_{q(y_0|x_0)}\left[\log p_{\theta}(y_0|x_0)\right]\\
&\leqslant\mathbb E_{q(y_{0:t},x_{1:s}|x_0)}\left[\log\frac{q(y_{1:t},x_{1:s}|y_0, x_0)}{p_{\theta}(y_{0:t},x_{1:s}|x_0)}\right]:=\mathcal L_{VLB}.
\end{align}
%
Then we have the re-claimed \Cref{theorem:1}:
%
\begin{theorem}[Closed-form expression]\label{theorem:3}
The loss function in \cref{eq:3.8} has a closed-form representation.
%
The training is equivalent to optimizing a KL-divergence up to a non-negative constant, \textit{i.e.},
%
\begin{align}
\mathcal L_{VLB}=\mathbb E_{q(y_0,x_s|x_0)}\left[D_{KL}(q(y_t|y_0)\|p_{\theta}(y_t|x_s))\right] + C\sqq{.}\label{eq:3.9}
\end{align}
\end{theorem}
%
For the given closed-form expression in \cref{eq:3.9}, the optimal $p_{\theta}(y_t|x_s)$ follows a Gaussian distribution and its mean $\mu_{\theta}$ has an analytic form, as summarized in the \Cref{theorem:2} \sqq{above.}
%
\begin{theorem}[Optimal solution to \cref{eq:3.9}]\label{theorem:4}
The optimal $p_{\theta}(y_t|x_s)$ follows a Gaussian distribution with its mean being
%
\begin{align}
\mu_{\theta}(x_s) = \sqrt{\balpha{t}}y_0\sqq{.}
\end{align}
\end{theorem}
%
By applying the diffusion forward process on both $x_0$ and $y_0$ with identical random noise at asymmetric timestep $s$ and $t$, respectively, \sqq{we have the following:}
%
\begin{align}
x_s=\sqrt{\balpha{s}}x_0+\sqrt{1-\balpha{s}}z,\quad y_t=\sqrt{\balpha{t}}y_0+\sqrt{1-\balpha{t}}z.
\end{align}
%
\Cref{theorem:4} reveals that $\mu_{\theta}$ needs to approximate the expression $\sqrt{\balpha{t}}y_0$ with $x_s$ as the only available input. Then we apply the following parameterization,
%
\begin{align}\label{eq:3.10}
\mu_{\theta}(x_s)=f_{\theta}(x_s)-\sqrt{1-\balpha{t}}z,
\end{align}
%
where $f_{\theta}$ is a trainable function.
%
The KL-divergence in \cref{eq:3.9} is optimized by minimizing the difference between the two means together with the variance $\Sigma_{\theta}$ of $p_{\theta}(y_t|x_s)$.
%
Formally, we have the simplified objective:
%
\begin{align}
\mathcal L_{s,t}=\mathbb E_q\left[\frac{1}{2(1-\balpha{t})}\|f_{\theta}(x_s)-y_t\|^2\right].
\end{align}
}


\revise{
To determine an adequate timestep pair $(s,t)$} \newrevise{for the asymmetric diffusion process, similar to the theoretical analysis about original \method, the complexity of our I2I system is characterized by $C(S)=\max\{C(S_1),C(S_2),C(S_3)\}$.
%
For I2I with the asymmetric \method, the three sub-systems are (1) the forward diffusion process from $x_0$ to $x_s$ with the complexity $f(s)$, (2) \method from $x_s$ to $y_t$ with the complexity $g(s,t)$, and (3) the denoising process via pre-trained diffusion model from $y_t$ to $y_0$ with the complexity $h(t)$.
%
$f(s)$ and $h(t)$ are monotone w.r.t. $s$ and $t$, respectively; but $g(s,t)$ does not have to be monotone.
%
If $s\neq t$, the diffusion process from $x_0$ to $x_s$ and denoising process from $y_t$ to $y_0$ are no longer reciprocal, so we need to consider both $f(s)$ and $h(t)$.
%
Then the complexity of $C(S)$ can be represented as $C(S)=C(s,t)=\max\{f(s),g(s,t),h(t)\}$.
%
Our target is to search the timestep pair $(s,t)$ minimizing $\min_{s,t}C(s,t)$.
%
We have
%
\begin{align}
\max_{i=1,2,3}d_i&= \max\{\max\{d_1,d_2\},d_3\}\\
&=\max\{\frac{d_1+d_2}{2}+\frac{|d_1-d_2|}{2},d_3\} \\
&\geqslant\max\{\frac{d_1+d_2}{2},d_3\} \\
&\geqslant\frac{1}{3}(2\cdot\frac{d_1+d_2}{2}+d_3)=\frac{1}{3}(d_1+d_2+d_3),
\end{align}
%
where the equality holds if and only if $|d_1-d_2|=0$ and $\frac{d_1+d_2}{2}=d_3$, \textit{i.e.}, $d_1=d_2=d_3$.
%
That means $C(s,t)=\max\{f(s),g(s,t),h(t)\}$ reaches its minimum when $s=t$.
%
In practice, we add the regularity term $\mathrm{SSIM}(x_0,x_s)+\mathrm{SSIM}(x_s,y_t)+\mathrm{SSIM}(y_0,y_t)$ to help search the global minimum.}
%
\newrevise{
Formally, we calculate the weighted sum of SSIM distances defined below, in which the smaller the result the better the performance.
}
%
\revise{
\begin{align}\label{eq:s_t}
\mathrm{dist}(s,t)=&|\mathrm{SSIM}(x_0,x_s)-\mathrm{SSIM}(x_s,y_t)|\nonumber\\
&\qquad+|\mathrm{SSIM}(x_s,y_t)-\mathrm{SSIM}(y_0,y_t)|\nonumber\\
&\qquad+|\mathrm{SSIM}(x_0,x_s)-\mathrm{SSIM}(y_0,y_t)|\nonumber\\
&\qquad+\lambda\mathrm{SSIM}(x_0,x_s)\nonumber\\
&\qquad+\lambda\mathrm{SSIM}(x_s,y_t)\nonumber\\
&\qquad+\lambda\mathrm{SSIM}(y_0,y_t)).
\end{align}
%
By setting the weight $\lambda=0.5$, we acquire an appropriate timestep pair as in \cref{tab:ablation_s_t}.
%
Notably, the preset timestep pair $(s,t)$ of this generalized pipeline coincide with the original pipeline theoretically and empirically, \textit{i.e.}, the asymmetric timestep pair appears to be identical.
}

