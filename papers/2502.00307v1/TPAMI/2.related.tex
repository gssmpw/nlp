\section{Related Work}\label{sec:related-work}


\noindent\sqq{In a forward diffusion process,} a \textbf{Diffusion probabilistic model (DPM)}~\cite{sohl2015deep,ho2020denoising} transforms a given data distribution into a simple latent distribution, such as a Gaussian distribution. 
%
Due to its strong capabilities, DPM has achieved great success in various fields, including speech synthesis~\cite{chen2020wavegrad,kong2020diffwave}, video synthesis~\cite{ho2022video,ho2022imagen}, image super-resolution~\cite{saharia2021image,li2022srdiff}, conditional generation~\cite{choi2021ilvr,wang2022pretraining}\sqq{,} and image-to-image translation~\cite{saharia2021palette,sasaki2021unit}.
%
Denoising diffusion probabilistic model (DDPM)~\cite{ho2020denoising} \revise{assumes} the Markovian property of the forward diffusion process.
For a dataset of images, the forward diffusion process is realized by corrupting each image $x_0$ \sqq{through the addition of} standard Gaussian noise to reduce it into a completely random noise image. 
%
Formally, given the variance schedules $\alpha_t\in[0,1],t=1,2,\cdots,T, \beta_t=1-\alpha_t$, we can write the Markov chain as:
%
\begin{align}
& q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1}),\\
& q(x_t|x_{t-1})\sim\mathcal N(x_t;\sqrt{\alpha_t}x_{t-1},\beta_t I),
\end{align}
%
where $x_T\sim\mathcal N(x_T;0,I)$ and $I$ is the identity matrix.


When reversing this diffusion process, DDPM serves as a generator for data generation in the form $p_{\theta}(x_0) = \int p_{\theta}(x_{0:T})dx_{1:T}$ starting from $x_T$:
%
\begin{align}
& p_{\theta}(x_{0:T})=p_{\theta}(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t),\\
& p_{\theta}(x_{t-1}|x_t)\sim\mathcal N(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t))\sqq{,}
\end{align}
so that any sample $x_T$ in the latent distribution will be mapped back to $x_0$ in the original data distribution. 
To achieve its reverse process for image synthesis, DDPM parameterizes the mean $\mu_{\theta}(x_t,t)$ by a time-dependent model $\epsilon_{\theta}(x_t,t)$ and optimizes the following simplified objective function:
%
%\begin{align}
%\mu_{\theta}(x_t,t) = \frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\balpha{t}}}\epsilon_{\theta}(x_t,t)\right).
%\end{align}
%
%The reverse process can then be trained using the variational lower bound:
%
%\begin{align}\label{eq:2.1}
%\mathbb E_q\left[D_{KL}(q(x_T|x_0)\|p(x_T)+\sum_{t>1}D_{KL}(q(x_{t-1}|x_t,x_0)\|p_{\theta}(x_{t-1}|x_t)-\log p_{\theta}(x_0|x_1)\right].
%\end{align}
%
%Ho~\cite{ho2020denoising} simplified the training process of \cref{eq:2.1} to an analytical objective function below by a reparameterization trick:
\begin{align}
\mathcal{L} = \mathbb{E}_{q(x_0, t, \epsilon)}\left[\|\epsilon - \epsilon_{\theta}(\sqrt{\balpha{t}}x_0 + \sqrt{1 - \balpha{t}}\epsilon, t)\|^2 \right].
\end{align}

\input{figures/fig_teaser/fig.tex}

\noindent\textbf{Faster DPM} attempts to explore shorter trajectories rather than the complete reverse process, while ensuring that the synthesis performance is comparable to the original DPM.
%
Some existing methods seek the trajectories using the grid search~\cite{chen2020wavegrad}.
%
However, this is only suitable for short reverse processes because its time complexity grows exponentially.
%
Other methods try to find optimal trajectories by solving a least-cost-path problem with a dynamic programming (DP) algorithm~\cite{watson2021learning,bao2022analyticdpm}.
%
Another representative category of fast sampling methods use\sqq{s} high-order differential equation (DE) solvers~\cite{jolicoeur2021gotta,Liu0LZ22,PopovVGSKW22,tachibana2021taylor,lu2022dpm}.
%
Some GAN-based methods also consider larger sampling step size\sqq{. For instance,} \cite{xiao2022DDGAN} \sqq{demonstrates learning a multi-modal distribution within a conditional GAN using a larger step size.}
% a multi-modal distribution is learned in a conditional GAN with a large step size.

\noindent\textbf{Image-to-image translation} (I2I) aims to translate an input image from a given source domain to another image in a given target domain, with input-output paired training data~\cite{isola2017image}.
%
To this end, the conditional generative adversarial network (cGAN) is designed to inject the information of the input image into the generation decoder with the adversarial loss~\cite{mirza2014conditional,goodfellow2014generative}.
%
The cGAN-based algorithms has demonstrated high quality on many I2I tasks~\cite{dong2017semantic,kaneko2017generative,karacan2016learning,ledig2017photo,sangkloy2017scribbler,wang2016generative,zhang2017age,jiang2020tsit,park2019SPADE,Zhu_2020_CVPR}.
%
However, due to their training instability and the severe mode collapse issue, it is hard for the cGAN-based methods to generate diverse high-resolution images.
%
Recently, DPM has been applied to the I2I task.
%
Palette~\cite{saharia2021palette} introduces the novel DPM framework to the I2I task by injecting the input \sqq{into} each sampling step for refinement.
%
Some methods use pre-trained image synthesis models for the I2I task~\cite{wang2022pretraining}.
%
Despite the high quality of synthesized images, the generation process of these existing methods is extremely time-consuming. 
%
Our \revise{work} tackles this issue by proposing a new DDPM method for the I2I task that works efficiently, without the time-consuming requirement of having to inject the input source information in every \sqq{denoising} step. 
%
\sqq{Although unpaired data are more accessible for translation tasks, the advantages of paired image-to-image (I2I) tasks, such as reduced data demands and enhanced synthesis quality, have made them a significant research focus.}
% As for the translation task, although unpaired data are easier to access, paired I2I tasks \sqq{offer} their own advantages such as \sqq{lower} data demand and \sqq{higher} synthesis quality, and have been well recognized as an important research topic.

