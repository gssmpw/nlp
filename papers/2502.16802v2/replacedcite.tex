\section{Related Work}
\subsection{Data Mixing} 
The quality of pre-training data has been demonstrated to play a critical role in model performance, as highlighted in several studies ____. 
One natural and intuitive approach to improving data quality involves adjusting the weights assigned to different data domains. 
Data mixing methods aim to optimize the distribution of attribute weights within pre-training datasets. 
For example, methods such as DoReMi ____ and DOGE ____ utilize small proxy models to generate domain weights, while DMLaw ____ and RegMix ____ determine domain weights by training a set of smaller models.
More recently, Llama-3.1 ____ employs downsampling to reduce the proportion of data from the arts and entertainment domain, and ____ investigates effective training strategies by adjusting topic weights. 
However, these studies lack details of domain weights and primarily explore data mixing from a domain-level perspective.
In this paper, we propose incorporating topic modeling to control data weights at a finer granularity, enabling more precise adjustments to the pre-training data and enhancing LLM's capabilities.

\subsection{Topic Model} 
Topic modeling is an unsupervised method used to uncover abstract topics within documents in the field of Natural Language Processing ____. 
Traditional approaches, such as Latent Dirichlet Allocation (LDA), typically rely on probabilistic techniques to generate topics ____. 
BERTopic ____ leverages transformer-based architectures to enhance traditional topic modeling processes.
More recent research has explored the use of LLMs for topic modeling, particularly by utilizing their text summarization capabilities to automatically assign descriptive labels to clusters of words. 
For instance, ____ demonstrated that approximately half of the topic labels generated by ChatGPT were considered useful by human evaluators. 
Additionally, other studies ____ have conducted extensive experiments to improve the performance of LLMs in topic modeling through prompt engineering.
However, these methods become computationally expensive and impractical when applied to large-scale datasets. 
In contrast to these approaches, we propose utilizing topic model for data mixing, enabling more fine-grained control over data weights. 
This approach not only improves the efficiency of pre-training LLMs but also provides valuable insights into the role of topic-level adjustments in optimizing data distributions for enhanced model performance.