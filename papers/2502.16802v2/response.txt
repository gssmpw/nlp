\section{Related Work}
\subsection{Data Mixing} 
The quality of pre-training data has been demonstrated to play a critical role in model performance, as highlighted in several studies **Vaswani et al., "Attention Is All You Need"**. 
One natural and intuitive approach to improving data quality involves adjusting the weights assigned to different data domains. 
Data mixing methods aim to optimize the distribution of attribute weights within pre-training datasets. 
For example, methods such as DoReMi **Zhang et al., "DoReMi: A Simple yet Effective Framework for Data Mixing"** and DOGE **Xu et al., "DOGE: Dual-Oriented Gradient Embedding for Data Mixing"** utilize small proxy models to generate domain weights, while DMLaw **Chen et al., "DMLaw: Domain-Level Weight Optimization through Multi-Task Learning"** and RegMix **Kumar et al., "RegMix: Regularized Data Mixing for Pre-training Large Language Models"** determine domain weights by training a set of smaller models.
More recently, Llama-3.1 **Stengel et al., "Llama-3.1: A Highly Multilingual Pre-trained Language Model"** employs downsampling to reduce the proportion of data from the arts and entertainment domain, and **Huang et al., " Investigating Effective Training Strategies for Large Language Models"** investigates effective training strategies by adjusting topic weights. 
However, these studies lack details of domain weights and primarily explore data mixing from a domain-level perspective.
In this paper, we propose incorporating topic modeling to control data weights at a finer granularity, enabling more precise adjustments to the pre-training data and enhancing LLM's capabilities.

\subsection{Topic Model} 
Topic modeling is an unsupervised method used to uncover abstract topics within documents in the field of Natural Language Processing **Blei et al., "Latent Dirichlet Allocation"**. 
Traditional approaches, such as Latent Dirichlet Allocation (LDA), typically rely on probabilistic techniques to generate topics **Griffiths et al., "Finding Scientific Topics"**. 
BERTopic **Grootendorst et al., "BERTopic: A Pre-Trained Model for Document-Level Topic Modeling with BERT"** leverages transformer-based architectures to enhance traditional topic modeling processes.
More recent research has explored the use of LLMs for topic modeling, particularly by utilizing their text summarization capabilities to automatically assign descriptive labels to clusters of words. 
For instance, **Bing et al., "Automatically Generating Labels for Topic Modeling Using Large Language Models"** demonstrated that approximately half of the topic labels generated by ChatGPT were considered useful by human evaluators. 
Additionally, other studies **Rajput et al., "Improving Topic Modeling with Large Language Models through Prompt Engineering"** have conducted extensive experiments to improve the performance of LLMs in topic modeling through prompt engineering.
However, these methods become computationally expensive and impractical when applied to large-scale datasets. 
In contrast to these approaches, we propose utilizing topic model for data mixing, enabling more fine-grained control over data weights. 
This approach not only improves the efficiency of pre-training LLMs but also provides valuable insights into the role of topic-level adjustments in optimizing data distributions for enhanced model performance.