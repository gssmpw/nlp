\section{Related Work}
We briefly describe three aspects of research: pre-ranking, learning-to-rank algorithms, and sample selection bias issue.

\textbf{Pre-ranking}.
In industrial recommendation systems, pre-ranking plays a vital role in selecting high-quality items from large-scale retrieval items and relieving ranking pressure.
To balance effectiveness and efficiency, it widely adopts a vector-product-based deep neural network model____.
However, lightweight model architectures degrade the model expression ability. 
%To further enhance the learning of user-item interaction information, Li et al. proposed IntTower____, which adopts a fine-grained and early feature interaction in the two-tower model method.
%Some other researchers proposed a new generation using fully connected DNN to improve the prediction accuracy, such as COLD____ and FSCD____.
% Recent works focus on enhancing critical calibration and ranking capabilities.
For critical calibration ability, recent works design model architectures to improve effectiveness under a computing power constraint, such as COLD____, FSCD____, and IntTower____.
Some works____ focus on knowledge distillation. 
Despite some advancements, they optimize the pre-ranking model as an independent entity, neglecting the consistency with ranking, leading to sub-optimal outcomes.
% and hinders the system's effectiveness. 
For ranking ability, some  methods____ propose the importance of improving the consistency with ranking.
% introduce the importance of simultaneously optimizing calibration and ranking abilities of pre-ranking models. 
Some works propose strict score alignment via point-wise distillation loss____, MSE loss____, or ListNet distillation loss____ to align with raw scores predicted by ranking. 
COPR____ proposes a rank alignment module and a chunk-based sampling module to optimize the consistency with ranking. 
However, they treat all items equally, neglecting accuracy discrepancies between head and tail items predicted by ranking.
% In addition, pair-wise approaches focus on relative differences and may only achieve local-optimal, while list-wise considers an entire sequence order from a global perspective.
Moreover, pre-ranking has larger-scale candidates than ranking and the only utilization of ranking logs can not adapt to changes in retrieval distribution, leading to a sample selection bias problem.

\textbf{Learning-To-Rank (LTR) Algorithm}.
% To address the problem of ranking ability optimization, LTR algorithms are widely used to learn a scoring function for computing the degree of relevance, such as point-wise, pair-wise, and list-wise method. 
LTR algorithms are extensively used to enhance ranking abilities by approximating relevance degree, which can be categorized into point-wise, pairwise, and listwise.
Point-wise____ approaches usually treat ranking as regression or classification problems, which assign score for each item independently.
% in which items are independently assigned scores.
Considering mutual information, pairwise____ methods achieve local optimality by predicting relative orders of item pairs but ignoring the overall order.
Listwise methods____ consider the entire sequence quality from a global perspective, aligning with ranking objectives.
% To the end, list-wise____ method is proposed to learn the whole ranking sequence, which is exactly consistent with the ranking goal. 
To preserve calibration and ranking quality, some studies combine the above losses____.
% combine point-wise and non-point-wise losses.

\textbf{Sample Selection Bias (SSB) Problem}.
% Previous works on reducing SSB issues____ usually focus on the retrieval stage.
As the magnitude of retrieval items increases, SSB issues____ in pre-ranking receive growing attention.
Some works sample negatives through a pre-defined distribution____ or in-batch sampling mechanisms____.
Since the diversity of negatives sampled by in-batch methods is constrained by batch size, some works propose cross-batch sampling based on memory banks____.
In addition to negative sampling techniques, some approaches enhance the learning of long-tail items by cross-decoupling network____ or modified losses, such as the modified softmax loss integrated with multi-positive samples____.
We propose an approach that leverages entire stream data to alleviate SSB issues and enhance adaptability when retrieval distribution shifts.

\vspace{-1mm}