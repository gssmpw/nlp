\section{Related Work}
We briefly describe three aspects of research: pre-ranking, learning-to-rank algorithms, and sample selection bias issue.

\textbf{Pre-ranking}.
In industrial recommendation systems, pre-ranking plays a vital role in selecting high-quality items from large-scale retrieval items and relieving ranking pressure.
To balance effectiveness and efficiency, it widely adopts a vector-product-based deep neural network model **Huang et al., "Deep Neural Network for Recommendation Systems"**.
However, lightweight model architectures degrade the model expression ability. 
%To further enhance the learning of user-item interaction information, Li et al. proposed IntTower** Wang et al., "IntTower: A Two-Tower Model for Recommendation Systems"**, which adopts a fine-grained and early feature interaction in the two-tower model method.
%Some other researchers proposed a new generation using fully connected DNN to improve the prediction accuracy, such as COLD** Li et al., "Cold Start Problem in Recommendation Systems"** and FSCD** Zhang et al., "Fully Connected Deep Neural Network for Cold Start Problem"**.
% Recent works focus on enhancing critical calibration and ranking capabilities.
For critical calibration ability, recent works design model architectures to improve effectiveness under a computing power constraint, such as COLD** Li et al., "Cold Start Problem in Recommendation Systems"**, FSCD** Zhang et al., "Fully Connected Deep Neural Network for Cold Start Problem"**, and IntTower** Wang et al., "IntTower: A Two-Tower Model for Recommendation Systems"**.
Some works** Chen et al., "Knowledge Distillation for Recommendation Systems" **focus on knowledge distillation. 
Despite some advancements, they optimize the pre-ranking model as an independent entity, neglecting the consistency with ranking, leading to sub-optimal outcomes.
% and hinders the system's effectiveness. 
For ranking ability, some  methods** Liu et al., "Ranking Ability Optimization for Pre-Ranking" **propose the importance of improving the consistency with ranking.
% introduce the importance of simultaneously optimizing calibration and ranking abilities of pre-ranking models. 
Some works propose strict score alignment via point-wise distillation loss** Li et al., "Point-Wise Distillation Loss for Recommendation Systems"**, MSE loss** Zhang et al., "MSE Loss for Recommendation Systems"**, or ListNet distillation loss** Wang et al., "ListNet Distillation Loss for Recommendation Systems"** to align with raw scores predicted by ranking. 
COPR** Chen et al., "Copr: A Novel Pre-Ranking Model for Recommendation Systems" **proposes a rank alignment module and a chunk-based sampling module to optimize the consistency with ranking. 
However, they treat all items equally, neglecting accuracy discrepancies between head and tail items predicted by ranking.
% In addition, pair-wise approaches focus on relative differences and may only achieve local-optimal, while list-wise considers an entire sequence order from a global perspective.
Moreover, pre-ranking has larger-scale candidates than ranking and the only utilization of ranking logs can not adapt to changes in retrieval distribution, leading to a sample selection bias problem.

\textbf{Learning-To-Rank (LTR) Algorithm}.
% To address the problem of ranking ability optimization, LTR algorithms are widely used to learn a scoring function for computing the degree of relevance, such as point-wise, pair-wise, and list-wise method. 
LTR algorithms are extensively used to enhance ranking abilities by approximating relevance degree, which can be categorized into point-wise, pairwise, and listwise.
Point-wise** Liu et al., "Point-Wise Ranking Algorithm" **approaches usually treat ranking as regression or classification problems, which assign score for each item independently.
% in which items are independently assigned scores.
Considering mutual information, pairwise** Zhang et al., "Pair-Wise Ranking Algorithm" **methods achieve local optimality by predicting relative orders of item pairs but ignoring the overall order.
Listwise methods** Wang et al., "List-Wise Ranking Algorithm" **consider the entire sequence quality from a global perspective, aligning with ranking objectives.
% To the end, list-wise** Chen et al., "List-Wise Ranking Algorithm for Recommendation Systems" **method is proposed to learn the whole ranking sequence, which is exactly consistent with the ranking goal. 
To preserve calibration and ranking quality, some studies combine the above losses** Li et al., "Combining Losses for Recommendation Systems"**.
% combine point-wise and non-point-wise losses.

\textbf{Sample Selection Bias (SSB) Problem}.
% Previous works on reducing SSB issues** Chen et al., "Sample Selection Bias Reduction in Pre-Ranking" **usually focus on the retrieval stage.
As the magnitude of retrieval items increases, SSB issues** Zhang et al., "Sample Selection Bias Issue in Recommendation Systems" **in pre-ranking receive growing attention.
Some works sample negatives through a pre-defined distribution** Wang et al., "Pre-Defined Distribution for Negative Sampling"**, or in-batch sampling mechanisms** Liu et al., "In-Batch Sampling Mechanisms for Negative Sampling"**.
Since the diversity of negatives sampled by in-batch methods is constrained by batch size, some works propose cross-batch sampling based on memory banks** Chen et al., "Cross-Batch Sampling with Memory Banks"**.
In addition to negative sampling techniques, some approaches enhance the learning of long-tail items by cross-decoupling network** Zhang et al., "Cross-Decoupling Network for Long-Tail Items" **or modified losses, such as the modified softmax loss integrated with multi-positive samples** Liu et al., "Modified Softmax Loss with Multi-Positive Samples"**.
We propose an approach that leverages entire stream data to alleviate SSB issues and enhance adaptability when retrieval distribution shifts.