@inproceedings{1994inferring,
author = {Gey, Fredric C.},
title = {Inferring probability of relevance using the method of logistic regression},
year = {1994},
isbn = {038719889X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {222–231},
numpages = {10},
location = {Dublin, Ireland},
series = {SIGIR '94}
}

@inproceedings{2004ssb,
author = {Zadrozny, Bianca},
title = {Learning and Evaluating Classifiers under Sample Selection Bias},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015425},
doi = {10.1145/1015330.1015425},
abstract = {Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {114},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{2005RankNet,
author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
title = {Learning to rank using gradient descent},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102351.1102363},
doi = {10.1145/1102351.1102363},
abstract = {We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {89–96},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}

@inproceedings{2007onlylist,
author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
title = {Learning to rank: from pairwise approach to listwise approach},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273513},
doi = {10.1145/1273496.1273513},
abstract = {The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as 'instances' in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as 'instances' in learning. The paper proposes a new probabilistic method for the approach. Specifically it introduces two probability models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {129–136},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{2008listmle1,
author = {Xia, Fen and Liu, Tie-Yan and Wang, Jue and Zhang, Wensheng and Li, Hang},
title = {Listwise approach to learning to rank: theory and algorithm},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390306},
doi = {10.1145/1390156.1390306},
abstract = {This paper aims to conduct a study on the listwise approach to learning to rank. The listwise approach learns a ranking function by taking individual lists as instances and minimizing a loss function defined on the predicted list and the ground-truth list. Existing work on the approach mainly focused on the development of new algorithms; methods such as RankCosine and ListNet have been proposed and good performances by them have been observed. Unfortunately, the underlying theory was not sufficiently studied so far. To amend the problem, this paper proposes conducting theoretical analysis of learning to rank algorithms through investigations on the properties of the loss functions, including consistency, soundness, continuity, differentiability, convexity, and efficiency. A sufficient condition on consistency for ranking is given, which seems to be the first such result obtained in related research. The paper then conducts analysis on three loss functions: likelihood loss, cosine loss, and cross entropy loss. The latter two were used in RankCosine and ListNet. The use of the likelihood loss leads to the development of a new listwise method called ListMLE, whose loss function offers better properties, and also leads to better experimental results.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1192–1199},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@techreport{2010lambdarank,
author = {Burges, Chris J.C.},
title = {From RankNet to LambdaRank to LambdaMART: An Overview},
year = {2010},
month = {June},
abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very successful algorithms for solving real world ranking problems: for example an ensemble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and reports, and so here we give a self-contained, detailed and complete description of them.},
url = {https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/},
number = {MSR-TR-2010-82},
}

@inproceedings{2013_global_neg,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {Distributed representations of words and phrases and their compositionality},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3111–3119},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{2013dssm,
author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
title = {Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2505665},
doi = {10.1145/2505515.2505665},
abstract = {Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information \& Knowledge Management},
pages = {2333–2338},
numpages = {6},
keywords = {deep learning, semantic model, web search, clickthrough data},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@ARTICLE{2015RankingSVM,
  author={Gu, Bin and Sheng, Victor S. and Tay, Keng Yeow and Romano, Walter and Li, Shuo},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Incremental Support Vector Learning for Ordinal Regression}, 
  year={2015},
  volume={26},
  number={7},
  pages={1403-1416},
  keywords={Support vector machines;Training;Algorithm design and analysis;Vectors;Training data;Educational institutions;Convergence;Incremental learning;online learning;ordinal regression (OR);support vector machine (SVM);Incremental learning;online learning;ordinal regression (OR);support vector machine (SVM)},
  doi={10.1109/TNNLS.2014.2342533}}

@misc{2015lstmdssm,
      title={Semantic Modelling with Long-Short-Term Memory for Information Retrieval}, 
      author={H. Palangi and L. Deng and Y. Shen and J. Gao and X. He and J. Chen and X. Song and R. Ward},
      year={2015},
      eprint={1412.6629},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{2018_rank_distillation,
author = {Tang, Jiaxi and Wang, Ke},
title = {Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220021},
doi = {10.1145/3219819.3220021},
abstract = {We propose a novel way to train ranking models, such as recommender systems, that are both effective and efficient. Knowledge distillation (KD) was shown to be successful in image recognition to achieve both effectiveness and efficiency. We propose a KD technique for learning to rank problems, called ranking distillation (RD). Specifically, we train a smaller student model to learn to rank documents/items from both the training data and the supervision of a larger teacher model. The student model achieves a similar ranking performance to that of the large teacher model, but its smaller model size makes the online inference more efficient. RD is flexible because it is orthogonal to the choices of ranking models for the teacher and student. We address the challenges of RD for ranking problems. The experiments on public data sets and state-of-the-art recommendation models showed that RD achieves its design purposes: the student model learnt with RD has less than an half size of the teacher model while achieving a ranking performance similar tothe teacher model and much better than the student model learnt without RD.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2289–2298},
numpages = {10},
keywords = {knowledge transfer, learning to rank, model compression, recommender system},
location = {London, United Kingdom},
series = {KDD '18}
}

@misc{2019LDRER,
      title={Learning Dense Representations for Entity Retrieval}, 
      author={Daniel Gillick and Sayali Kulkarni and Larry Lansing and Alessandro Presta and Jason Baldridge and Eugene Ie and Diego Garcia-Olano},
      year={2019},
      eprint={1909.10506},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{2020Meta-KD,
  title={Meta-KD: A meta knowledge distillation framework for language model compression across domains},
  author={Pan, Haojie and Wang, Chengyu and Qiu, Minghui and Zhang, Yichang and Li, Yaliang and Huang, Jun},
  journal={arXiv preprint arXiv:2012.01266},
  year={2020}
}

@inproceedings{2020_negative_gold,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@article{2020cold,
  author       = {Zhe Wang and
                  Liqin Zhao and
                  Biye Jiang and
                  Guorui Zhou and
                  Xiaoqiang Zhu and
                  Kun Gai},
  title        = {{COLD:} Towards the Next Generation of Pre-Ranking System},
  journal      = {CoRR},
  volume       = {abs/2007.16122},
  year         = {2020},
  url          = {https://arxiv.org/abs/2007.16122},
  eprinttype    = {arXiv},
  eprint       = {2007.16122},
  timestamp    = {Mon, 03 Aug 2020 14:32:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-16122.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{2020privileged,
      title={Privileged Features Distillation at Taobao Recommendations}, 
      author={Chen Xu and Quan Li and Junfeng Ge and Jinyang Gao and Xiaoyong Yang and Changhua Pei and Fei Sun and Jian Wu and Hanxiao Sun and Wenwu Ou},
      year={2020},
      eprint={1907.05171},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{2021_cross_batch,
author = {Wang, Jinpeng and Zhu, Jieming and He, Xiuqiang},
title = {Cross-Batch Negative Sampling for Training Two-Tower Recommenders},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463032},
doi = {10.1145/3404835.3463032},
abstract = {The two-tower architecture has been widely applied for learning item and user representations, which is important for large-scale recommender systems. Many two-tower models are trained using various in-batch negative sampling strategies, where the effects of such strategies inherently rely on the size of mini-batches. However, training two-tower models with a large batch size is inefficient, as it demands a large volume of memory for item and user contents and consumes a lot of time for feature encoding. Interestingly, we find that neural encoders can output relatively stable features for the same input after warming up in the training process. Based on such facts, we propose a simple yet effective sampling strategy called Cross-Batch Negative Sampling (CBNS), which takes advantage of the encoded item embeddings from recent mini-batches to boost the model training. Both theoretical analysis and empirical evaluations demonstrate the effectiveness and the efficiency of CBNS.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1632–1636},
numpages = {5},
keywords = {recommender systems, neural networks, information retrieval},
location = {Canada},
series = {SIGIR '21}
}

@inproceedings{2021fscd,
author = {Ma, Xu and Wang, Pengjie and Zhao, Hui and Liu, Shaoguo and Zhao, Chuhan and Lin, Wei and Lee, Kuang-Chih and Xu, Jian and Zheng, Bo},
title = {Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462979},
doi = {10.1145/3404835.3462979},
abstract = {In real-world search, recommendation, and advertising systems, the multi-stage ranking architecture is commonly adopted. Such architecture usually consists of matching, pre-ranking, ranking, and re-ranking stages. In the pre-ranking stage, vector-product based models with representation-focused architecture are commonly adopted to account for system efficiency. However, it brings a significant loss to the effectiveness of the system. In this paper, a novel pre-ranking approach is proposed which supports complicated models with interaction-focused architecture. It achieves a better tradeoff between effectiveness and efficiency by utilizing the proposed learnable Feature Selection method based on feature Complexity and variational Dropout (FSCD). Evaluations in a real-world e-commerce sponsored search system for a search engine demonstrate that utilizing the proposed pre-ranking, the effectiveness of the system is significantly improved. Moreover, compared to the systems with conventional pre-ranking models, an identical amount of computational resource is consumed.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2036–2040},
numpages = {5},
keywords = {pre-ranking, feature selection, efficiency, effectiveness},
location = {Canada},
series = {SIGIR '21}
}

@inproceedings{2022inttower,
author = {Li, Xiangyang and Chen, Bo and Guo, Huifeng and Li, Jingjie and Zhu, Chenxu and Long, Xiang and Li, Sujian and Wang, Yichao and Guo, Wei and Mao, Longxia and Liu, Jinxing and Dong, Zhenhua and Tang, Ruiming},
title = {IntTower: The Next Generation of Two-Tower Model for Pre-Ranking System},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557072},
doi = {10.1145/3511808.3557072},
abstract = {Scoring a large number of candidates precisely in several milliseconds is vital for industrial pre-ranking systems. Existing pre-ranking systems primarily adopt the two-tower model since the "user-item decoupling architecture" paradigm is able to balance the efficiency and effectiveness. However, the cost of high efficiency is the neglect of the potential information interaction between user and item towers, hindering the prediction accuracy critically. In this paper, we show it is possible to design a two-tower model that emphasizes both information interactions and inference efficiency. The proposed model, IntTower (short for Interaction enhanced Two-Tower), consists of Light-SE, FE-Block and CIR modules. Specifically, lightweight Light-SE module is used to identify the importance of different features and obtain refined feature representations in each tower. FE-Block module performs fine-grained and early feature interactions to capture the interactive signals between user and item towers explicitly and CIR module leverages a contrastive interaction regularization to further enhance the interactions implicitly. Experimental results on three public datasets show that IntTower outperforms the SOTA pre-ranking models significantly and even achieves comparable performance in comparison with the ranking models. Moreover, we further verify the effectiveness of IntTower on a large-scale advertisement pre-ranking system. The code of IntTower is publicly available https://gitee.com/mindspore/models/tree/master/research/recommend/IntTower.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
pages = {3292–3301},
numpages = {10},
keywords = {neural networks, pre-ranking system, recommender systems},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{2022point_mix2,
author = {Yan, Le and Qin, Zhen and Wang, Xuanhui and Bendersky, Michael and Najork, Marc},
title = {Scale Calibration of Deep Ranking Models},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539072},
doi = {10.1145/3534678.3539072},
abstract = {Learning-to-Rank (LTR) systems are ubiquitous in web applications nowadays. The existing literature mainly focuses on improving ranking performance by trying to generate the optimal order of candidate items. However, virtually all advanced ranking functions are not scale calibrated. For example, rankers have the freedom to add a constant to all item scores without changing their relative order. This property has resulted in several limitations in deploying advanced ranking methods in practice. On the one hand, it limits the use of effective ranking functions in important applications. For example, in ads ranking, predicted Click-Through Rate (pCTR) is used for ranking and is required to be calibrated for the downstream ads auction. This is a major reason that existing ads ranking methods use scale calibrated pointwise loss functions that may sacrifice ranking performance. On the other hand, popular ranking losses are translation-invariant. We rigorously show that, both theoretically and empirically, this property leads to training instability that may cause severe practical issues.In this paper, we study how to perform scale calibration of deep ranking models to address the above concerns. We design three different formulations to calibrate ranking models through calibrated ranking losses. Unlike existing post-processing methods, our calibration is performed during training, which can resolve the training instability issue without any additional processing. We conduct experiments on the standard LTR benchmark datasets and one of the largest sponsored search ads dataset from Google. Our results show that our proposed calibrated ranking losses can achieve nearly optimal results in terms of both ranking quality and score scale calibration.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4300–4309},
numpages = {10},
keywords = {sponsored search, ranking scale calibration, learning-to-rank},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{2022rankflow,
author = {Qin, Jiarui and Zhu, Jiachen and Chen, Bo and Liu, Zhirong and Liu, Weiwen and Tang, Ruiming and Zhang, Rui and Yu, Yong and Zhang, Weinan},
title = {RankFlow: Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532050},
doi = {10.1145/3477495.3532050},
abstract = {Building a multi-stage cascade ranking system is a commonly used solution to balance the efficiency and effectiveness in modern information retrieval (IR) applications, such as recommendation and web search. Despite the popularity in practice, the literature specific on multi-stage cascade ranking systems is relatively scarce. The common practice is to train rankers of each stage independently using the same user feedback data (a.k.a., impression data), disregarding the data flow and the possible interactions between stages. This straightforward solution could lead to a sub-optimal system because of the sample selection bias (SSB) issue, which is especially damaging for cascade rankers due to the negative effect accumulated in the multiple stages. Worse still, the interactions between the rankers of each stage are not fully exploited. This paper provides an elaborate analysis of this commonly used solution to reveal its limitations. By studying the essence of cascade ranking, we propose a joint training framework named RankFlow to alleviate the SSB issue and exploit the interactions between the cascade rankers, which is the first systematic solution for this topic. We propose a paradigm of training cascade rankers that emphasizes the importance of fitting rankers on stage-specific data distributions instead of the unified user feedback distribution. We design the RankFlow framework based on this paradigm: The training data of each stage is generated by its preceding stages while the guidance signals not only come from the logs but its successors. Extensive experiments are conducted on various IR scenarios, including recommendation, web search and advertisement. The results verify the efficacy and superiority of RankFlow.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {814–824},
numpages = {11},
keywords = {recommendation, information retrieval, cascade ranking systems},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@misc{2022rankingConsistency,
      title={On Ranking Consistency of Pre-ranking Stage}, 
      author={Siyu Gu and Xiangrong Sheng},
      year={2022},
      eprint={2205.01289},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{2023ELIRCDN,
author = {Zhang, Yin and Wang, Ruoxi and Cheng, Derek Zhiyuan and Yao, Tiansheng and Yi, Xinyang and Hong, Lichan and Caverlee, James and Chi, Ed H.},
title = {Empowering Long-tail Item Recommendation through Cross Decoupling Network (CDN)},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599814},
doi = {10.1145/3580305.3599814},
abstract = {Industry recommender systems usually suffer from highly-skewed long-tail item distributions where a small fraction of the items receives most of the user feedback. This skew hurts recommender quality especially for the item slices without much user feedback. While there have been many research advances made in academia, deploying these methods in production is very difficult and very few improvements have been made in industry. One challenge is that these methods often hurt overall performance; additionally, they could be complex and expensive to train and serve.In this work, we aim to improve tail item recommendations while maintaining the overall performance with less training and serving cost. We first find that the predictions of user preferences are biased under long-tail distributions. The bias comes from the differences between training and serving data in two perspectives: 1) the item distributions, and 2) user's preference given an item. Most existing methods mainly attempt to reduce the bias from the item distribution perspective, ignoring the discrepancy from user preference given an item. This leads to a severe forgetting issue and results in sub-optimal performance.To address the problem, we design a novel Cross Decoupling Network (CDN) to reduce the two differences. Specifically, CDN (i) decouples the learning process of memorization and generalization on the item side through a mixture-of-expert architecture; (ii) decouples the user samples from different distributions through a regularized bilateral branch network. Finally, a new adapter is introduced to aggregate the decoupled vectors, and softly shift the training attention to tail items. Extensive experimental results show that CDN significantly outperforms state-of-the-art approaches on popular benchmark datasets. We also demonstrate its effectiveness by a case study of CDN in a large-scale recommendation system at Google.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5608–5617},
numpages = {10},
keywords = {decoupling, memorization and generalization, recommendation},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{2023GCL_MOPPR,
  title={Graph Contrastive Learning with Multi-Objective for Personalized Product Retrieval in Taobao Search},
  author={Li, Longbin and Zhang, Chao and Li, Sen and Zhong, Yun and Liu, Qingwen and Zeng, Xiaoyi},
  journal={arXiv preprint arXiv:2307.04322},
  year={2023}
}

@inproceedings{2023_copr_ndcg,
author = {Zhao, Zhishan and Gao, Jingyue and Zhang, Yu and Han, Shuguang and Lou, Siyuan and Sheng, Xiang-Rong and Wang, Zhe and Zhu, Han and Jiang, Yuning and Xu, Jian and Zheng, Bo},
title = {COPR: Consistency-Oriented Pre-Ranking for Online Advertising},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615465},
doi = {10.1145/3583780.3615465},
abstract = {Cascading architecture has been widely adopted in large-scale advertising systems to balance efficiency and effectiveness. In this architecture, the pre-ranking model is expected to be a lightweight approximation of the ranking model, which handles more candidates with strict latency requirements. Due to the gap in model capacity, the pre-ranking and ranking models usually generate inconsistent ranked results, thus hurting the overall system effectiveness. The paradigm of score alignment is proposed to regularize their raw scores to be consistent. However, it suffers from inevitable alignment errors and error amplification by bids when applied in online advertising. To this end, we introduce a consistency-oriented pre-ranking framework for online advertising, which employs a chunk-based sampling module and a plug-and-play rank alignment module to explicitly optimize consistency of ECPM-ranked results. A ΔNDCG-based weighting mechanism is adopted to better distinguish the importance of inter-chunk samples in optimization. Both online and offline experiments have validated the superiority of our framework. When deployed in Taobao display advertising system, it achieves an improvement of up to +12.3\% CTR and +5.6\% RPM.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4974–4980},
numpages = {7},
keywords = {consistency, cascading architecture, pre-ranking},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@misc{2023_list_distillation,
      title={Calibration-compatible Listwise Distillation of Privileged Features for CTR Prediction}, 
      author={Xiaoqiang Gui and Yueyao Cheng and Xiang-Rong Sheng and Yunfeng Zhao and Guoxian Yu and Shuguang Han and Yuning Jiang and Jian Xu and Bo Zheng},
      year={2023},
      eprint={2312.08727},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{2023jdRethink,
author = {Song, Jinbo and Huang, Ruoran and Wang, Xinyang and Huang, Wei and Yu, Qian and Chen, Mingming and Yao, Yafei and Fan, Chaosheng and Peng, Changping and Lin, Zhangang and Hu, Jinghe and Shao, Jingping},
title = {Rethinking Large-Scale Pre-Ranking System: Entire-Chain Cross-Domain Models},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557683},
doi = {10.1145/3511808.3557683},
abstract = {Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with L0 regularization to reduce computational costs. Evaluations on real-world large-scale traffic logs demonstrate that our pre-ranking models outperform SOTA methods while time consumption is maintained within an acceptable level, which achieves better trade-off between efficiency and effectiveness.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
pages = {4495–4499},
numpages = {5},
keywords = {recommendation system, pre-ranking, cross-domain},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{2023joint_rank_cali,
author = {Sheng, Xiang-Rong and Gao, Jingyue and Cheng, Yueyao and Yang, Siran and Han, Shuguang and Deng, Hongbo and Jiang, Yuning and Xu, Jian and Zheng, Bo},
title = {Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599851},
doi = {10.1145/3580305.3599851},
abstract = {Despite the development of ranking optimization techniques, pointwise loss remains the dominating approach for click-through rate prediction. It can be attributed to the calibration ability of the pointwise loss since the prediction can be viewed as the click probability. In practice, a CTR prediction model is also commonly assessed with the ranking ability. To optimize the ranking ability, ranking loss (e.g., pairwise or listwise loss) can be adopted as they usually achieve better rankings than pointwise loss. Previous studies have experimented with a direct combination of the two losses to obtain the benefit from both losses and observed an improved performance. However, previous studies break the meaning of output logit as the click-through rate, which may lead to sub-optimal solutions. To address this issue, we propose an approach that can Jointly optimize the Ranking and Calibration abilities (JRC for short). JRC improves the ranking ability by contrasting the logit value for the sample with different labels and constrains the predicted probability to be a function of the logit subtraction. We further show that JRC consolidates the interpretation of logits, where the logits model the joint distribution. With such an interpretation, we prove that JRC approximately optimizes the contextualized hybrid discriminative-generative objective. Experiments on public and industrial datasets and online A/B testing show that our approach improves both ranking and calibration abilities. Since May 2022, JRC has been deployed on the display advertising platform of Alibaba and has obtained significant performance improvements.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4813–4822},
numpages = {10},
keywords = {calibration, click-through rate prediction, hybrid model},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

