\section{Related Work}
We briefly describe three aspects of research: pre-ranking, learning-to-rank algorithms, and sample selection bias issue.

\textbf{Pre-ranking}.
In industrial recommendation systems, pre-ranking plays a vital role in selecting high-quality items from large-scale retrieval items and relieving ranking pressure.
To balance effectiveness and efficiency, it widely adopts a vector-product-based deep neural network model\cite{2013dssm,2015lstmdssm}.
However, lightweight model architectures degrade the model expression ability. 
%To further enhance the learning of user-item interaction information, Li et al. proposed IntTower\cite{2022inttower}, which adopts a fine-grained and early feature interaction in the two-tower model method.
%Some other researchers proposed a new generation using fully connected DNN to improve the prediction accuracy, such as COLD\cite{2020cold} and FSCD\cite{2021fscd}.
% Recent works focus on enhancing critical calibration and ranking capabilities.
For critical calibration ability, recent works design model architectures to improve effectiveness under a computing power constraint, such as COLD\cite{2020cold}, FSCD\cite{2021fscd}, and IntTower\cite{2022inttower}.
Some works\cite{2020privileged,2020Meta-KD} focus on knowledge distillation. 
Despite some advancements, they optimize the pre-ranking model as an independent entity, neglecting the consistency with ranking, leading to sub-optimal outcomes.
% and hinders the system's effectiveness. 
For ranking ability, some  methods\cite{2022rankingConsistency,2023jdRethink} propose the importance of improving the consistency with ranking.
% introduce the importance of simultaneously optimizing calibration and ranking abilities of pre-ranking models. 
Some works propose strict score alignment via point-wise distillation loss\cite{2018_rank_distillation,2023rethinking}, MSE loss\cite{2022rankflow}, or ListNet distillation loss\cite{2023_list_distillation} to align with raw scores predicted by ranking. 
COPR\cite{2023_copr_ndcg} proposes a rank alignment module and a chunk-based sampling module to optimize the consistency with ranking. 
However, they treat all items equally, neglecting accuracy discrepancies between head and tail items predicted by ranking.
% In addition, pair-wise approaches focus on relative differences and may only achieve local-optimal, while list-wise considers an entire sequence order from a global perspective.
Moreover, pre-ranking has larger-scale candidates than ranking and the only utilization of ranking logs can not adapt to changes in retrieval distribution, leading to a sample selection bias problem.

\textbf{Learning-To-Rank (LTR) Algorithm}.
% To address the problem of ranking ability optimization, LTR algorithms are widely used to learn a scoring function for computing the degree of relevance, such as point-wise, pair-wise, and list-wise method. 
LTR algorithms are extensively used to enhance ranking abilities by approximating relevance degree, which can be categorized into point-wise, pairwise, and listwise.
Point-wise\cite{1994inferring} approaches usually treat ranking as regression or classification problems, which assign score for each item independently.
% in which items are independently assigned scores.
Considering mutual information, pairwise\cite{2005RankNet,2015RankingSVM} methods achieve local optimality by predicting relative orders of item pairs but ignoring the overall order.
Listwise methods\cite{2007onlylist,2008listmle1,2010lambdarank} consider the entire sequence quality from a global perspective, aligning with ranking objectives.
% To the end, list-wise\cite{2011LTR_for_IR,2008listmle1,2019BinaryRelevance} method is proposed to learn the whole ranking sequence, which is exactly consistent with the ranking goal. 
To preserve calibration and ranking quality, some studies combine the above losses\cite{2022point_mix2,2023joint_rank_cali}.
% combine point-wise and non-point-wise losses.

\textbf{Sample Selection Bias (SSB) Problem}.
% Previous works on reducing SSB issues\cite{2004ssb} usually focus on the retrieval stage.
As the magnitude of retrieval items increases, SSB issues\cite{2004ssb} in pre-ranking receive growing attention.
Some works sample negatives through a pre-defined distribution\cite{2013_global_neg} or in-batch sampling mechanisms\cite{2019LDRER,2020_negative_gold}.
Since the diversity of negatives sampled by in-batch methods is constrained by batch size, some works propose cross-batch sampling based on memory banks\cite{2020cross_cvpr,2021_cross_batch}.
In addition to negative sampling techniques, some approaches enhance the learning of long-tail items by cross-decoupling network\cite{2023ELIRCDN} or modified losses, such as the modified softmax loss integrated with multi-positive samples\cite{2023GCL_MOPPR}.
We propose an approach that leverages entire stream data to alleviate SSB issues and enhance adaptability when retrieval distribution shifts.

\vspace{-1mm}