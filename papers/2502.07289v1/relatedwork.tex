\section{Related Work}
% 第一段介绍深度补全任务和传统方法，第二段介绍一般方法，第三段介绍propagation-based方法

\paragraph{Traditional Methods}
The depth completion task involves reconstructing a dense, accurate depth map from sparse measurements. These sparse inputs can originate from range sensors like LiDAR, Time-of-Flight (ToF), and structured light, or from computational techniques such as Structure-from-Motion (SfM) and binocular stereo. Traditional methods for depth completion often employ conventional techniques to interpolate missing depth values. For instance, Depth SEEDS \cite{depth_seeds} leveraged an extended version of the SEEDS \cite{van2015seeds} superpixel technique, incorporating available depth measurements to recover absent depth information. IP-Basic \cite{ip_basic} applied a series of traditional image processing operations, including dilation, closure, bilateral filtering, and hole filling, \etc, to densify sparse depth maps. Zhang \etal \cite{zhang2018deep} utilized surface normal and occlusion boundary priors to solve for a dense depth map from sparse observations by minimizing an objective function that balances geometric and smoothness constraints. Zhao \etal \cite{zhao2021surface} introduced a surface geometry model enhanced by an outlier removal algorithm to fill the missing depth values based on a local surface sharing assumption. 

\paragraph{Deep Methods}
The development of deep learning has shifted the focus towards neural network-based, data-driven approaches to enhance prediction accuracy and scalability. Handling sparse and irregularly spaced input data presents a significant challenge in depth completion. To tackle this, Uhrig \etal \cite{uhrig2017sparsity} introduced a sparse invariant convolutional operator that explicitly accounts for missing values during convolution, thus better processing irregularly spaced sparse depths. Huang \etal \cite{huang2019hms} enhanced this with HMS-Net, an encoder-decoder architecture employing novel sparsity-invariant operations to fuse multi-scale features from various layers. Eldesokey \etal \cite{eldesokey2020uncertainty} introduced an algebraically-constrained normalized convolution layer for managing highly sparse depth data. Additionally, some approaches adopt a multi-stage framework, initially filling in sparse depth map values to circumvent direct convolution on sparse data. Chen \etal \cite{chen2018estimating} employed nearest interpolation on the sparse depth map before feeding it into a deep network; Liu \etal \cite{liu2021learning} used a differentiable kernel regression layer in place of manual interpolation, and BP-Net \cite{bp_net} aggregated information from nearest neighbors for filling missing depth values through bilateral propagation.

\paragraph{Multi-modal Fusion}
Another research avenue emphasizes the effective integration of features from both RGB images and sparse depth maps. Chen \etal \cite{chen2019learning} proposed to fuse 2D and 3D features extracted through 2D convolutions on images and continuous convolutions on 3D points, respectively. DeepLiDAR \cite{deeplidar} used surface normals as an intermediate representation for fusion with sparse depth. GuideNet \cite{guidenet} utilized features from both modalities as guidance to generate context-dependent, spatially-variant convolution kernels for adaptive feature extraction from sparse depth maps. RigNet \cite{rignet} and Wang \etal \cite{wang2023decomposed} further improved \cite{guidenet} by introducing repetitive architecture and novel decomposition schemes, respectively. Recent works have also explored attention mechanisms for multi-modal feature fusion and innovative projection views for enhancing geometric representation capabilities. For example, GuideFormer \cite{guideformer} introduced a guided-attention module for multi-modal fusion, BEV@DC \cite{bev_dc} employed a bird's-eye view for training, leveraging the rich geometric details of LiDAR, whereas TPVD \cite{tpvd} used a tri-perspective view decomposition to model 3D geometry explicitly.

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figs/framework.pdf}
    \caption{Overall Framework of LP-Net. MFP, RH and SDF stand for the Multi-path Feature Pyramid module, Regression Head and Selective Depth Filtering module, respectively. $S$ represents the input sparse depth, while $\hat{S}^{(1)}\sim \hat{S}^{(4)}$ are its progressively lower-resolution versions, obtained through a weighted pooling operation. $F_{d}^{0}\sim F_{d}^{4}$ indicate the decoder feature maps. The prediction of the final depth map $\hat{D}$ is structured into five progressive steps, beginning with a direct regression and confidence-based fusion with $\hat{S}^{(4)}$ to produce the low-frequency residual $\hat{D}^{(4)}$. Subsequently, $\hat{D}^{(4)}$ undergoes iterative upsampling, fusion with the corresponding sparse measurements, and refinement via the SDF module to yield more accurate, higher-resolution depth maps.}
    \label{fig.framework}
\end{figure*} 

\paragraph{Propagation Methods}
Propagation-based methods have significantly advanced recent depth completion strategies. CSPN \cite{cspn} introduced a convolutional spatial propagation network, initiating with a coarse depth prediction and progressively refining it through learned affinity among fixed neighbors in a recurrent convolutional process. CSPN++ \cite{cspn++} extent this by combining results from various kernel sizes and iteration steps. NLSPN \cite{nlspn} improved upon this by learning to select relevant neighbors through predicted offsets from a regular grid. DySPN \cite{dyspn, lin2023dyspn} shifted to an attention-based dynamic approach to learn affinity, addressing issues like fixed affinity and over-smoothing. GraphCSPN \cite{graphcspn} integrated 3D information into a graph neural network to estimate neighbors during updates, and LRRU \cite{lrru} introduced a lightweight network that adjusts kernel scope from large to small to capture dependencies from long to short range. Despite their wide adoption for post-processing to enhance prediction accuracy, these propagation-based methods are often computationally intensive and lack a comprehensive understanding of global scenes. In contrast, our proposed method constructs a multi-scale scheme that progressively upsamples initial predictions and recovers high-frequency details at each scale using a novel selective filtering technique, thereby achieving state-of-the-art performance and high computational efficiency.