\section{Related Work}
% 第一段介绍深度补全任务和传统方法，第二段介绍一般方法，第三段介绍propagation-based方法

\paragraph{Traditional Methods}
The depth completion task involves reconstructing a dense, accurate depth map from sparse measurements. These sparse inputs can originate from range sensors like LiDAR, Time-of-Flight (ToF), and structured light, or from computational techniques such as Structure-from-Motion (SfM) and binocular stereo. Traditional methods for depth completion often employ conventional techniques to interpolate missing depth values. For instance, Depth SEEDS ____ leveraged an extended version of the SEEDS ____ superpixel technique, incorporating available depth measurements to recover absent depth information. IP-Basic ____ applied a series of traditional image processing operations, including dilation, closure, bilateral filtering, and hole filling, \etc, to densify sparse depth maps. Zhang \etal ____ utilized surface normal and occlusion boundary priors to solve for a dense depth map from sparse observations by minimizing an objective function that balances geometric and smoothness constraints. Zhao \etal ____ introduced a surface geometry model enhanced by an outlier removal algorithm to fill the missing depth values based on a local surface sharing assumption. 

\paragraph{Deep Methods}
The development of deep learning has shifted the focus towards neural network-based, data-driven approaches to enhance prediction accuracy and scalability. Handling sparse and irregularly spaced input data presents a significant challenge in depth completion. To tackle this, Uhrig \etal ____ introduced a sparse invariant convolutional operator that explicitly accounts for missing values during convolution, thus better processing irregularly spaced sparse depths. Huang \etal ____ enhanced this with HMS-Net, an encoder-decoder architecture employing novel sparsity-invariant operations to fuse multi-scale features from various layers. Eldesokey \etal ____ introduced an algebraically-constrained normalized convolution layer for managing highly sparse depth data. Additionally, some approaches adopt a multi-stage framework, initially filling in sparse depth map values to circumvent direct convolution on sparse data. Chen \etal ____ employed nearest interpolation on the sparse depth map before feeding it into a deep network; Liu \etal ____ used a differentiable kernel regression layer in place of manual interpolation, and BP-Net ____ aggregated information from nearest neighbors for filling missing depth values through bilateral propagation.

\paragraph{Multi-modal Fusion}
Another research avenue emphasizes the effective integration of features from both RGB images and sparse depth maps. Chen \etal ____ proposed to fuse 2D and 3D features extracted through 2D convolutions on images and continuous convolutions on 3D points, respectively. DeepLiDAR ____ used surface normals as an intermediate representation for fusion with sparse depth. GuideNet ____ utilized features from both modalities as guidance to generate context-dependent, spatially-variant convolution kernels for adaptive feature extraction from sparse depth maps. RigNet ____ and Wang \etal ____ further improved ____ by introducing repetitive architecture and novel decomposition schemes, respectively. Recent works have also explored attention mechanisms for multi-modal feature fusion and innovative projection views for enhancing geometric representation capabilities. For example, GuideFormer ____ introduced a guided-attention module for multi-modal fusion, BEV@DC ____ employed a bird's-eye view for training, leveraging the rich geometric details of LiDAR, whereas TPVD ____ used a tri-perspective view decomposition to model 3D geometry explicitly.

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figs/framework.pdf}
    \caption{Overall Framework of LP-Net. MFP, RH and SDF stand for the Multi-path Feature Pyramid module, Regression Head and Selective Depth Filtering module, respectively. $S$ represents the input sparse depth, while $\hat{S}^{(1)}\sim \hat{S}^{(4)}$ are its progressively lower-resolution versions, obtained through a weighted pooling operation. $F_{d}^{0}\sim F_{d}^{4}$ indicate the decoder feature maps. The prediction of the final depth map $\hat{D}$ is structured into five progressive steps, beginning with a direct regression and confidence-based fusion with $\hat{S}^{(4)}$ to produce the low-frequency residual $\hat{D}^{(4)}$. Subsequently, $\hat{D}^{(4)}$ undergoes iterative upsampling, fusion with the corresponding sparse measurements, and refinement via the SDF module to yield more accurate, higher-resolution depth maps.}
    \label{fig.framework}
\end{figure*} 

\paragraph{Propagation Methods}
Propagation-based methods have significantly advanced recent depth completion strategies. CSPN ____ introduced a convolutional spatial propagation network, initiating with a coarse depth prediction and progressively refining it through learned affinity among fixed neighbors in a recurrent convolutional process. CSPN++ ____ extent this by combining results from various kernel sizes and iteration steps. NLSPN ____ improved upon this by learning to select relevant neighbors through predicted offsets from a regular grid. DySPN ____ shifted to an attention-based dynamic approach to learn affinity, addressing issues like fixed affinity and over-smoothing. GraphCSPN ____ integrated 3D information into a graph neural network to estimate neighbors during updates, and LRRU ____ introduced a lightweight network that adjusts kernel scope from large to small to capture dependencies from long to short range. Despite their wide adoption for post-processing to enhance prediction accuracy, these propagation-based methods are often computationally intensive and lack a comprehensive understanding of global scenes. In contrast, our proposed method constructs a multi-scale scheme that progressively upsamples initial predictions and recovers high-frequency details at each scale using a novel selective filtering technique, thereby achieving state-of-the-art performance and high computational efficiency.