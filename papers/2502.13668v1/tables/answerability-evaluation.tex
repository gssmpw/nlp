\begin{table*}[!h]
\small
\centering
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
 &  & \multicolumn{3}{c}{Answerable ($N=383$)} & \multicolumn{3}{c}{Unanswerable ($N=112$)} & \multicolumn{3}{c}{Average} \\
 \cmidrule(lr){3-5}
 \cmidrule(lr){6-8}
 \cmidrule(lr){9-11}
Model & Ctx. & Prec. & Recall & F1 & Prec. & Recall & F1 & Acc. & W-F1 & M-F1 \\ \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Llama-3\\ IT-8B-8k\end{tabular}} 
 & G    & \textbf{1.0000} & 0.4517 & 0.6223 & --     & --     & --     & 0.4517 & 0.6223 & 0.3112 \\
 & 10   & 0.8407 & 0.3995 & 0.5416 & 0.2652 & \textbf{0.7411} & 0.3906 & 0.4768 & 0.5074 & 0.4661 \\
 & 20   & \textbf{0.8796} & 0.2480 & 0.3870 & 0.2558 & \textbf{0.8839} & 0.3968 & 0.3919 & 0.3892 & 0.3919 \\
 & 50   & 0.7907 & 0.1775 & 0.2900 & 0.2298 & \textbf{0.8393} & 0.3608 & 0.3273 & 0.3060 & 0.3254 \\
 & 100  & 0.7667 & 0.1802 & 0.2918 & 0.2247 & \textbf{0.8125} & 0.3520 & 0.3232 & 0.3054 & 0.3219 \\
 & FT   & 0.8168 & 0.5587 & 0.6636 & 0.2747 & 0.5714 & 0.3710 & 0.5616 & 0.5974 & 0.5173 \\
 \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Llama-3\\IT-8B-32k\end{tabular}} 
 & G & \textbf{1.0000} & 0.4047 & 0.5762 & -- & -- & -- & 0.4047 & 0.5762 & 0.2881 \\
 & 10 & 0.8326 & 0.4804 & 0.6093 & 0.2737 & 0.6696 & 0.3886 & 0.5232 & 0.5593 & 0.4989 \\
 & 20 & 0.8182 & 0.4700 & 0.5970 & 0.2618 & 0.6429 & 0.3721 & 0.5091 & 0.5461 & 0.4846 \\
 & 50 & 0.8056 & 0.3786 & 0.5151 & 0.2444 & 0.6875 & 0.3607 & 0.4485 & 0.4802 & 0.4379 \\
 & 100 & 0.7984 & 0.2585 & 0.3905 & 0.2345 & 0.7768 & 0.3602 & 0.3758 & 0.3837 & 0.3754 \\
 & FT & 0.8488 & 0.3812 & 0.5261 & 0.2663 & 0.7679 & 0.3954 & 0.4687 & 0.4965 & 0.4608 \\
  \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Mistral\\ IT-v02-7B-32k\end{tabular}} 
 & G    & \textbf{1.0000} & \textbf{0.8877} & \textbf{0.9405} & --     & --     & --     & \textbf{0.8877} & \textbf{0.9405} & \textbf{0.4703} \\
 & 10   & 0.7854 & \textbf{0.9269} & \textbf{0.8503} & \textbf{0.3488} & 0.1339 & 0.1935 & \textbf{0.7475} & \textbf{0.7017} & 0.5219 \\
 & 20   & 0.7790 & \textbf{0.9295} & \textbf{0.8476} & 0.2895 & 0.0982 & 0.1467 & \textbf{0.7414} & 0.6890 & 0.4971 \\
 & 50   & 0.7768 & \textbf{0.9086} & \textbf{0.8375} & 0.2553 & 0.1071 & 0.1509 & \textbf{0.7273} & 0.6822 & 0.4942 \\
 & 100  & 0.7824 & \textbf{0.9295} & \textbf{0.8496} & \textbf{0.3250} & 0.1161 & 0.1711 & \textbf{0.7455} & \textbf{0.6961} & 0.5103 \\
 & FT   & 0.7803 & \textbf{0.9739} & \textbf{0.8664} & \textbf{0.4118} & 0.0625 & 0.1085 & \textbf{0.7677} & \textbf{0.6949} & 0.4875 \\
  \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Command-R\\ v01-34B-128k\end{tabular}} 
 & G    & \textbf{1.0000} & 0.7232 & 0.8394 & -- & -- & -- & 0.7232 & 0.8394 & 0.4197 \\
 & 10   & 0.7985 & 0.8172 & 0.8077 & 0.3204 & 0.2946 & 0.3070 & 0.6990 & 0.6944 & \textbf{0.5574} \\
 & 20   & 0.8025 & 0.8381 & 0.8199 & \textbf{0.3474} & 0.2946 & 0.3188 & 0.7152 & \textbf{0.7065} & \textbf{0.5694} \\
 & 50   & 0.8031 & 0.8198 & 0.8114 & \textbf{0.3365} & 0.3125 & 0.3241 & 0.7051 & \textbf{0.7011} & 0.5677 \\
 & 100  & 0.7949 & 0.8094 & 0.8021 & 0.3048 & 0.2857 & 0.2949 & 0.6909 & 0.6873 & 0.5485 \\
 & FT   & 0.8113 & 0.7520 & 0.7805 & 0.3214 & 0.4018 & 0.3571 & 0.6727 & 0.6847 & \textbf{0.5688} \\
 \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}GPT-3.5\\ Turbo-0613-16k\end{tabular}} 

 & G    & \textbf{1.0000} & 0.4935 & 0.6608 & --     & --     & --     & 0.4935 & 0.6608 & 0.3304 \\
 & 10   & 0.8107 & 0.4360 & 0.5671 & 0.2526 & 0.6518 & 0.3641 & 0.4848 & 0.5211 & 0.4656 \\
 & 20   & 0.8248 & 0.5039 & 0.6256 & 0.2720 & 0.6339 & 0.3807 & 0.5333 & 0.5702 & 0.5032 \\
 & 50   & 0.8168 & 0.5587 & 0.6636 & 0.2747 & 0.5714 & 0.3710 & 0.5616 & 0.5974 & 0.5173 \\
 & 100  & 0.8507 & 0.4465 & 0.5856 & 0.2789 & 0.7321 & 0.4039 & 0.5111 & 0.5445 & 0.4948 \\
 & FT   & 0.8348 & 0.2507 & 0.3855 & 0.2447 & \textbf{0.8304} & 0.3780 & 0.3818 & 0.3838 & 0.3818 \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}GPT-4o\\0806-128k\end{tabular}} 
 & G    & \textbf{1.0000} & 0.4465 & 0.6173 & --     & --     & --     & 0.4465 & 0.6173 & 0.3087 \\
 & 10   & \textbf{0.8439} & 0.5222 & 0.6452 & 0.2907 & 0.6696 & \textbf{0.4054} & 0.5556 & 0.5909 & 0.5253 \\
 & 20   & 0.8560 & 0.5744 & 0.6875 & 0.3151 & 0.6696 & \textbf{0.4286} & 0.5960 & 0.6289 & 0.5580 \\
 & 50   & \textbf{0.8604} & 0.5953 & 0.7037 & 0.3261 & 0.6696 & \textbf{0.4386} & 0.6121 & 0.6437 & \textbf{0.5712} \\
 & 100  & \textbf{0.8543} & 0.5666 & 0.6813 & 0.3112 & 0.6696 & \textbf{0.4249} & 0.5899 & 0.6233 & \textbf{0.5531} \\
 & FT   & \textbf{0.8458} & 0.5300 & 0.6517 & 0.2941 & 0.6696 & \textbf{0.4087} & 0.5616 & 0.5967 & 0.5302 \\

\bottomrule
\end{tabular}
\captionof{table}{Evaluation results on the answerability task of various LLMs, with different context settings (G = Gold Evidence, FT = Full-Text, 10/20/50/100 = Top-k passages). Note that the class distribution is imbalanced. There are a total of 383 answerable and 112 unanswerable questions. W-F1 is Weighted F1, M-F1 is Macro F1.}\label{tbl:answerability-evaluation}
\end{table*}


