\begin{table*}[!h]
\scalebox{0.98}{
\begin{minipage}{\linewidth}
\centering
\begin{adjustbox}{angle=270} 
\tiny
\begin{tabularx}{0.93\textheight}{@{}p{0.1cm}p{3.1cm}Xp{3.25cm}p{6cm}@{}}
\toprule
 & Question & Answer Evidence & Free-Form Answer & GPT-4 Augmented Answer \\ \midrule
\multirow{1}{*}{\rotatebox{90}{ICLR 23}} 
& What benefits does improving the upper bound for the Information Gain evaluation metric provide in practice? & to measure the quality of such models An interpretable alternative evaluation metric for likelihood-based models is Normalized Information Gain ( NInGa ) & interpretable and robust evaluation of full likelihood models & Improving the upper bound for the Information Gain evaluation metric provides the benefit of enabling a more interpretable and robust evaluation of full likelihood models in practice. \\ \midrule
\multirow{1}{*}{\rotatebox{90}{ICLR 22}} & Does the expected label always match the most probable label given the noisy data? & This makes it clear that, overall, the pattern zig-zags, first moving towards the unknown true label before eventually turning to memorize the wrong label & No, that is why zig-zag might happen for the hard labels. & No, the expected label does not always match the most probable label given the noisy data. This is indicated by the observed zig-zag pattern, where the process initially moves towards the true label but eventually turns to memorize the incorrect label. \\ \midrule
\multirow{1}{*}{\rotatebox{90}{NeurIPS 22}} & What criteria were used to select the videos and classes for the study? & This is inspired by the fact that complex human activities in real-world settings are usually hierarchical and compositional across space and time. & We selected videos and classes that contain complex human activities that are hierarchical and compositional. & The criteria for selecting the videos and classes for the study were based on the presence of complex human activities that are hierarchical and compositional in nature.\par \\ \midrule
\multirow{1}{*}{\rotatebox{90}{ARR 22}} & What impact does the layer norm approximation have on the performance of the tasks discussed in the paper? & Another challenge of THE-X is the attention score input of layer normalization.\par In most cases, the scale of multi-head attention output is very dense around [-1,1].\par However, before normalization, we also observe the attention scores are scarily sparse, with some extreme values reaching 1e4, which is difficult for our LN-distill stage.\par To prevent the overflow attention scores, we use the weight decay of Adam optimizer as regularization.\looseness=-1 & Layernorm approximation may lead to attention overflow, which dramastically affect the performance of downstream tasks & The approximation of layer normalization can potentially lead to attention overflow. This can significantly impact the performance of the downstream tasks discussed in the paper. \\ \midrule
\multirow{1}{*}{\rotatebox[origin=c]{90}{COLING 20}} & What measures were taken to ensure that paths were not shared between the training and test sets? & To avoid giving an unfair advantage to the path-based models, we filtered both validation and test sets to only include hyponym queries that are unseen anywhere in the full taxonomy paths of the training data.\par We also exclude hyponyms from the test and validation sets which appear as hyponyms in the training set7 to prevent the models from merely copying. & All queries observed during path training and all hyponyms in the training set that also appear as hyponyms in the test set are eliminated. & The authors implemented measures to ensure that paths were not shared between the training and test sets by filtering both validation and test sets to only include hyponym queries that were not present in the full taxonomy paths of the training data. Additionally, any hyponyms that appeared in the training set were excluded from the test and validation sets to prevent the models from simply replicating the data. \\ \midrule
\multirow{1}{*}{\rotatebox{90}{ACL 17}} & What would be the impact of not having access to pre-trained embeddings on the performance of this method? & Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting. When we include the LM embeddings in our system overall performance increases from 90.87\% to 91.93\% F 1 for the CoNLL 2003 NER task, a more then 1\% absolute F1 increase, and a substantial improvement over the previous state of the art.  We also establish a new state of the art result (96.37\% F 1 ) for the CoNLL 2000 Chunking task. Importantly, the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 F 1 in the NER and Chunking tasks, respectively. & Performance decreases by about 1\% on the CoNLL 2003 NER task, and by 1.4\% on the CoNLL 2000 Chunking task when removing pretrained language model embeddings. & The absence of pre-trained language model embeddings would result in a decrease in performance by approximately 1\% on the CoNLL 2003 NER task, and by around 1.4\% on the CoNLL 2000 Chunking task. \\ \midrule
\multirow{1}{*}{\rotatebox{90}{CoNLL 16}} & Why were post-editing rates chosen over prediction (h)ter for intrinsic uncertainty evaluation? & Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort ( Koponen et al., 2012 ).\par Additionally, time is more directly applicable in real translation environments – where uncertainty estimates could be useful, as it relates directly to productivity measures. & The authors state that normalised post-editing times (i.e. post-editing rates) are a more realistic measure with respect to operational effort, and can be adapted to different application scenarios. & The authors opted for post-editing rates over prediction (h)ter for intrinsic uncertainty evaluation because they believed that time is a more comprehensive measure of post-editing effort. This measure not only captures technical effort, like HTER, but also cognitive effort. Furthermore, time is more directly applicable in real translation environments, where uncertainty estimates could be beneficial, as it correlates directly to productivity measures. Thus, post-editing rates provide a more realistic measure of operational effort and can be adapted to various application scenarios.\looseness=-1 \\ \midrule
\multirow{1}{*}{\rotatebox{90}{ESD 23}} & What is the impact on detection or signal-to-noise ratio (SNR) if RR-based fingerprint construction is not applied? & robust detection and attribution (D\&A) of forced change in the water cycle are impaired by low signal-to-noise ratios.\par The concept of detection and attribution is to use climate model simulations in which the applied forcings are known and internal variability can be reduced by averaging multiple realisations, to estimate a so-called fingerprint that represents the effect of the applied forcings on climate variables of interest. & detection won't be possible if no SNR-maximising method/process is applied to the noise observational data & If the RR-based fingerprint construction is not applied, it could potentially impair the detection process due to the low signal-to-noise ratios. This could make detection impossible without the application of a method or process that maximizes the signal-to-noise ratio in the observational data.\\ \midrule
\multirow{1}{*}{\rotatebox{90}{ESurf 23}} & Does the orientation of the grid, with potentially more north-facing slopes in the Elwha area than in the Quinault area, affect the absorption of solar radiation, thereby influencing glacier distribution? & Our flowline model captures the variability in the direction of the steepest precipitation gradients but neglects variability across the valleys that may be important, especially for spatially variable drivers of ablation, such as topographic shading. & We do not include topographic shading as a factor influencing melting in the model - so the answer to this question is no - not in our model.  In the real world, the north-facing slopes of the Elwha headwaters may have been important in the mass balance of past glaciers. & The authors' model does not account for topographic shading as a factor influencing melting, thus the orientation of the grid does not affect the absorption of solar radiation in their model. However, in reality, the north-facing slopes of the Elwha headwaters may have played a significant role in the mass balance of past glaciers. \\ \midrule
\multirow{1}{*}{\rotatebox{90}{F1000 22}} & How was the conceptual model in Figure 2 developed? & 
Participants were recruited from their schools and colleges through the activity recruitment poster and “ Word-of-Mouth ” from the teachers and lecturers.\par 
Then, their crafts will be evaluated by art teachers and ranked on a leaderboard.\par The participants who were ranked on the leaderboard will receive certificates and prizes.\par After experiencing two-level EGL, 29 students were purposively selected as FGD participants.\par
Figure 2 shows the overview of the EGL activity flow. & It is based on the procedures of the EGL activity from participation to focus group discussion. & The conceptual model depicted in Figure 2 was developed based on the procedures of the Experiential Group Learning (EGL) activity. This process ranged from participant recruitment to the focus group discussion.\\ 
\bottomrule
\end{tabularx}
\end{adjustbox}
\caption{Exemplary questions, answer evidence, and free-form answers of the PeerQA dataset from all venues.}\label{tbl:examplary-annotation}
\end{minipage}
}
\end{table*}