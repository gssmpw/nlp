
\begin{table}[!hp]
\begin{center}
\small

\begin{tabular}{@{}lccccccccc@{}}
\toprule
 &  & \multicolumn{3}{c}{Rouge-L} & \multicolumn{3}{c}{AlignScore} & \multicolumn{2}{c}{Prometheus} \\ 
\cmidrule(lr){3-5}
\cmidrule(lr){6-8}
\cmidrule(lr){9-10}
Model & Ctx. & AE & FF & GPT-4 FF & AE & FF & GPT-4 FF & FF & GPT-4 FF \\
\midrule

\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Llama-3\\ IT-8B-8k\end{tabular}} 
 & G    & 0.1683 & 0.2295 & 0.2569 & 0.5731 & 0.1098 & 0.2643 & 3.1102 & 3.1593 \\
 & 10   & 0.1670 & 0.2113 & \textbf{0.2479} & 0.3839 & 0.1107 & 0.2107 & 3.1347 & 3.1828 \\
 & 20   & 0.1771 & 0.2074 & 0.2458 & 0.3719 & 0.1041 & 0.1965 & 3.1878 & 3.2454 \\
 & 50   & 0.1621 & 0.2050 & 0.2357 & 0.3402 & 0.1062 & 0.1958 & 3.0122 & 3.0313 \\
 & 100  & 0.1418 & 0.2069 & 0.2278 & 0.3255 & 0.1067 & 0.2184 & 2.8082 & 2.7885 \\
 & FT   & 0.1484 & 0.1736 & 0.2037 & 0.2719 & 0.0653 & 0.1159 & 2.7510 & 2.9321 \\

\midrule

\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Llama-3\\ IT-8B-32k\end{tabular}}
& G   & 0.1648 & 0.2286 & 0.2567 & 0.5778 & 0.1016 & 0.2436 & 3.1673 & 3.1749 \\
& 10  & 0.1513 & \textbf{0.2258} & 0.2464 & 0.3970 & 0.1142 & 0.2177 & 3.1388 & 3.1410 \\
& 20  & 0.1558 & 0.2204 & 0.2425 & 0.4001 & 0.1115 & 0.2109 & 3.1388 & 3.1227 \\
& 50  & 0.1546 & 0.2061 & 0.2397 & 0.3750 & 0.0999 & 0.2011 & 3.0571 & 3.1358 \\
& 100 & 0.1664 & 0.2099 & 0.2412 & 0.3785 & 0.1037 & 0.2008 & 3.0000 & 3.2010 \\
& FT  & 0.1835 & 0.1948 & 0.2260 & 0.3311 & 0.0711 & 0.1450 & 3.1959 & 3.2167 \\
 \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Mistral\\ v02-7B-32k\end{tabular}}
 & G    & \textbf{0.2442} & 0.1922 & 0.2432 & 0.6407 & 0.0827 & 0.1977 & 3.4245 & \textbf{3.4517} \\
 & 10   & \textbf{0.1967} & 0.1667 & 0.2032 & 0.3573 & 0.0612 & 0.1094 & 3.2490 & 3.3629 \\
 & 20   & \textbf{0.2039} & 0.1670 & 0.2011 & 0.3449 & 0.0505 & 0.1107 & 3.2408 & 3.2663 \\
 & 50   & \textbf{0.2023} & 0.1572 & 0.1943 & 0.3211 & 0.0496 & 0.1017 & 3.1306 & 3.1958 \\
 & 100  & \textbf{0.2023} & 0.1593 & 0.1927 & 0.3142 & 0.0634 & 0.1209 & 3.0245 & 3.0809 \\
 & FT   & \textbf{0.1883} & 0.1344 & 0.1678 & 0.2599 & 0.0328 & 0.0750 & 2.9796 & 3.1227 \\
 \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Command-R\\ v01-34B-128k\end{tabular}}
 & G    & 0.1310 & 0.2294 & 0.2081 & 0.5604 & 0.1362 & 0.3059 & 3.0571 & 3.0052 \\
 & 10   & 0.1211 & 0.2104 & 0.1973 & 0.3767 & 0.1221 & 0.2275 & 3.1551 & 3.1723 \\
 & 20   & 0.1220 & 0.2164 & 0.1978 & 0.3823 & 0.1245 & 0.2213 & 3.0490 & 3.0052 \\
 & 50   & 0.1229 & 0.2188 & 0.1941 & 0.3872 & 0.1223 & 0.2247 & 3.1224 & 3.0026 \\
 & 100  & 0.1244 & 0.2200 & 0.1853 & 0.3688 & 0.1112 & 0.1976 & 3.0245 & 3.0052 \\
 & FT   & 0.1230 & \textbf{ 0.2085} & 0.1859 & 0.3530 & \textbf{0.1015} & \textbf{0.1939} & 2.9020 & 2.9869 \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}GPT-3.5\\ Turbo-0613-16k\end{tabular}}
 & G    & 0.1540 & \textbf{0.2414} & 0.2688 & 0.5596 & \textbf{0.1378} & \textbf{0.3175} & 3.0408 & 3.0705 \\
 & 10   & 0.1342 & 0.2212 & 0.2462 & \textbf{0.4410} & \textbf{0.1412} & \textbf{0.2531} & 2.9184 & 3.0313 \\
 & 20   & 0.1388 & \textbf{0.2211} & \textbf{0.2465} & \textbf{0.4255} & \textbf{0.1446} & \textbf{0.2394} & 2.9714 & 3.0888 \\
 & 50   & 0.1365 & \textbf{0.2205} & \textbf{0.2437} & 0.4159 & \textbf{0.1356} & \textbf{0.2374} & 2.9918 & 3.0914 \\
 & 100  & 0.1297 & \textbf{0.2207} & \textbf{0.2437} & 0.4092 & \textbf{0.1360} & \textbf{0.2301} & 2.9102 & 3.0470 \\
 & FT   & 0.1162 & 0.1895 & 0.2188 & 0.3341 & 0.0771 & 0.1524 & 2.7143 & 2.9060 \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}GPT-4o\\ 0806-128k\end{tabular}}
 & G    & 0.1992 & 0.2266 & \textbf{0.2739} & \textbf{0.6410} & 0.1224 & 0.2802 & \textbf{3.4612} & 3.4308 \\
 & 10   & 0.1765 & 0.2048 & 0.2455 & 0.4055 & 0.0884 & 0.1963 & \textbf{3.5143} & \textbf{3.5222} \\
 & 20   & 0.1798 & 0.2039 & 0.2453 & 0.4094 & 0.0963 & 0.1830 & \textbf{3.5510} & \textbf{3.5927} \\
 & 50   & 0.1771 & 0.2058 & 0.2433 & \textbf{0.4164} & 0.0971 & 0.1926 & \textbf{3.5592} & \textbf{3.6423} \\
 & 100  & 0.1793 & 0.2036 & 0.2436 & \textbf{0.4120} & 0.0936 & 0.1886 & \textbf{3.5714} & \textbf{3.5614} \\
 & FT   & 0.1821 & 0.1981 & \textbf{0.2372} & \textbf{0.3900} & 0.0713 & 0.1790 & \textbf{3.5673} & \textbf{3.6057} \\
 \bottomrule
\end{tabular}
\captionof{table}{
Evaluation results on the answer generation task of various LLMs, with different context settings (G = Gold Evidence, FT = Full-Text, 10/20/50/100 = Top-k passages) and the metric computed against different ground truths (AE = Answer Evidence Paragraph, FF = Free-Form Answer, GPT-4 FF = GPT-4 rephrased Free-Form Answer). Rouge-L measures lexical overlap; AlignScore measures factual consistency; Prometheus measures answer correctness using an LLM-as-a-judge approach between the generation and the annotated Free-Form Answer or Answer Evidence.}\label{tbl:answer-generation-evaluation}
\end{center}
\end{table}