\begin{table*}[!ht]
\footnotesize
\centering
\begin{tabularx}{\textwidth}{p{0.2cm}p{0.7cm}X}
\toprule
\# & Size & Representative Questions \\
\midrule
1 & 35.4\% & Why does the baseline have significantly better performance on ACE 2004 and ACE 2005 compared to Yu et al. (2020), but similar performance on OntoNotes 5 and CoNLL 2003? Does the proposed alternate way to use linear by computing the mean absolute value of the weights associated with it differ from the original linear model proposed by Dalvi et al. (2019)? Is the design of the proposed method arbitrary for all layers of a given VIT model, or are some layers fixed? \\ \midrule
2 & 10.9\% & How does your method differ from existing methods for visual and language understanding with multilinguality, such as VQA, captioning, and retrieval? What other downstream tasks, such as natural language inference, question answering, and semantic role labeling, have been tested using an encoder that has been transferred from language 1 to language 2 without any parameter updates? How can the authors ensure that the natural language sentences produced from the "ground truth" activity graphs accurately describe the scene? \\ \midrule
3 & 4.8\% & Is the authors' conclusion about the accuracy of the CMIP6 climate models in simulating the processes based on the agreement between the observed data and the models' predictions in terms of the residual variability? Do the authors assume that iron is sourced from the platform when considering the feasibility of coastal seaweeds, which have a very low surface-to-volume ratio, competing for iron against the typically small and specialized open ocean phytoplankton that have a high surface-to-volume ratio? Can we assume that coastal seaweeds, which have a very low surface-to-volume ratio, would be competitive in iron uptake against the mostly small and specialized open ocean phytoplankton that have a high surface-to-volume ratio, especially in iron-limited areas? \\ \midrule
4 & 4.3\% & Can the authors provide a justification for why only four datasets were used to evaluate the visual search models, rather than a more diverse collection of datasets? Why should associations that are solvable by AI be kept in the framework, when the purpose of the framework is to collect associations that are difficult for models but solvable by humans? Does the proposed approach address issues related to assigning different attributions to features that have the same effect on the model or assigning positive attributions to features with no effect? \\ \midrule
5 & 4.0\% & How does the paper incorporate section titles into the BOS representation? What is the purpose of multiplying the scalar sc(omega, q) by the inner product of omega and q in equation 5? What is the difference between the \verb|\odot| and \verb|\cdot| symbols in the equation for computing the overall source mask from the k masks? \\ \midrule
6 & 3.8\% & Is it possible to consistently find perturbations to empirically robust adversarial examples that result in a correctly classified image? How does the paper define the concept of an "adversarial L2 ball" when it appears to suggest that every sample should have the same classification as \verb|\tilde{x}|, contrary to the expectation that each sample within the ball should have a different classification compared to x? Could the authors provide further justification for their claim that the gradient-based attack is responsible for the shift between test and training data observed in the adversarial attack? \\ \midrule
7 & 3.6\% & What is the performance of larger GLM models compared to state-of-the-art results, given that hardware resources do not appear to be a constraint? What is the expected relationship between the performance of the algorithms and the number of updates per sample, memory size, and batch size? What could explain the difference in performance between the DICTA test set and the new test set, particularly the difference between the cha and wor scores? \\ \midrule
8 & 3.3\% & What protocol did you use to decide when to stop training and to select hyperparameters for each dataset when no labeled target data is available? Does label smoothing always improve performance, or are there cases where it can degrade performance? Does label smoothing always improve the performance of the hyperparameter-fine tuning procedure? \\ \midrule
9 & 3.1\% & What are the vertical uncertainty bars in Figure 13? What would be the correct classification for the image in Figure 1 where the space bar is hidden? What is the reason for the sudden change in the green and blue curves in Figure 2 at epoch 90? \\ \midrule
10 & 3.1\% & What is the impact of adjusting $\delta$ on the results of Table 1? Is the coreference resolution pipeline depicted in Table 1 universally accepted in the field of coreference resolution? What is the difference between the results in Table 1 and Table 2(a)? \\ \midrule
11 & 2.8\% & Is there an optimal number of MSD points to use in order to minimize the error on the estimated parameters, and is there an option to automatically determine this number? Why is a new metric, concept purity, introduced instead of using the same set of metrics provided in Yuan et al. (2020)? What benefits does improving the upper bound for the Information Gain evaluation metric provide in practice? \\
\midrule
12 & 2.4\% & Are all participants in the trial pregnant women who are less than 36 weeks gestation? What is the rationale for changing the data distribution if the KB was compiled by medical papers? What ethical considerations were taken into account when selecting the data for the dataset? \\ 
\bottomrule
\end{tabularx}
\caption{Questions clustered into the top 12 topics by BERTopic. \textit{Representative Questions} are automatically identified by BERTopic.}
\label{tbl:question-topics}
\end{table*}