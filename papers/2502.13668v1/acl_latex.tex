\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{float}
\usepackage{multirow} 
\usepackage{adjustbox} 
\usepackage{makecell} 
\usepackage{enumitem}
\usepackage[colaction]{multicol}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{subfiles}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{rotating}
\usepackage{makecell}


\newcommand\multicollinenumbers{%
 \linenumbers
 \def\makeLineNumber{\docolaction{\makeLineNumberLeft}{}{\makeLineNumberRight}}}


\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\newcolumntype{C}{>{\Centering\arraybackslash}X} % centered "X" column

\newenvironment{Table}
  {\par\bigskip\noindent\minipage{\columnwidth}\centering}
  {\endminipage\par\bigskip}


\title{PeerQA: A Scientific Question Answering Dataset from Peer Reviews}

\author{
Tim Baumg√§rtner,$^{1}$ 
Ted Briscoe,$^{2}$ 
Iryna Gurevych$^{1,2}$ \\
$^{1}$Ubiquitous Knowledge Processing Lab (UKP Lab), \\ Department of Computer Science and Hessian Center for AI (hessian.AI), \\Technical University of Darmstadt \\
$^{2}$Mohamed bin Zayed University of Artificial Intelligence\\
\url{www.ukp.tu-darmstadt.de}
}


\begin{document}
\maketitle
\begin{abstract}
We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health.
PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. 
We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens.\footnote{Our code and data is available at \url{https://github.com/UKPLab/peerqa}.}

\end{abstract}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.89\columnwidth]{figs/peer-qa-overview-with-note.pdf}
\caption{Overview of the PeerQA data collection process. From the peer review process (in green), we extract and process questions from the reviews. Given the published version of the article and a question, an expert (in our case, the original paper authors) (1) checks the question and modifies or discards it, (2) annotates whether it is answerable or not (i.e. if there is sufficient information in the paper), and if so (3) highlights the evidence to answer the question and finally (4) provides a free-form answer to the question.}
\label{fig:peer-qa-process}
\end{figure}

\section{Introduction}
The number of scientific articles is increasing exponentially \citep{fire-over-optimization-2019,growth-rate-modern-science}, leading to an increase in review work and leaving researchers with an ever-expanding number of publications to read to keep up with their field. Therefore, novel tools are required to support reviewing work and enable readers to consume information from scientific articles more efficiently \citep{brainard2020scientists,kuznetsov2024natural}.
\input{tables/related-work}
Automatic Question Answering (QA) systems can provide such support, allowing researchers and reviewers to productively extract information from an article, particularly if integrated directly into the reading and reviewing interface \citep{zyska-etal-2023-care, lo-semantic-reader-2023}. QA systems can also improve the quality of peer review, e.g., by avoiding questions in a review that are addressed in the article but potentially overlooked by a reviewer. However, the development of QA models is limited by the availability of high-quality and realistic datasets in the scientific domain to measure the performance of methods. Collecting scientific QA data is challenging because it requires expert annotators who are difficult to recruit. Furthermore, naturally occurring questions are difficult to source compared to the general domain, where search engine logs can be used \citep{nguyen-msmarco-2016,kwiatkowski-etal-2019-natural}. Previous work resorted to recruiting practitioners or graduate students and focused only on Machine Learning (ML) or Natural Language Processing (NLP) domains \citep{dasigi-etal-2021-dataset, qasa-lee-2023}. Annotators of these datasets have various degrees of knowledge, e.g., having read only the abstract, skimmed the paper, or sometimes read the paper fully. Collecting questions from annotators has the downside of questions not being realistic, such as asking questions that would not be raised naturally or being generic when the questioner has superficial knowledge of the paper.

To this end, we introduce PeerQA, a real-world, scientific, document-level Question Answering dataset. PeerQA supports three crucial tasks for QA over scientific articles: Given a question and a paper, evidence sentences relevant to the question need to be retrieved. Based on these, the answerability of the question can be decided. Finally, the dataset contains free-form reference answers addressing the question. We leverage peer reviews to source questions, and answers are annotated by the authors of the respective papers. While most questions are from ML and NLP papers, 10\% of questions come from other scientific domains, including Geoscience and Public Health. Figure~\ref{fig:peer-qa-process} provides an overview of our data collection process. To summarize, our contributions are the following:\looseness=-1

1. We release PeerQA, a QA dataset over scientific articles with questions sourced from peer reviews and answers annotated by authors. We release a set of 579 annotated samples (from 208 papers), as well as 12k unlabeled questions (from 2.6k papers). We show the properties of the collected data, including various statistics, question topics, and classes.

2. We establish baselines for all three tasks in PeerQA: Evidence Retrieval, Question Answerability, and Free-Form Answer Generation, and outline which factors contribute to model performance.

\section{Related Work}
\paragraph{Peer Review} Many tasks and applications leverage peer reviews as a data source, including argument mining \citep{hua-etal-2019-argument, cheng-etal-2020-ape, kennard-etal-2022-disapere}, helpfulness and score prediction \citep{xiong-litman-2011-automatically, gao-etal-2019-rebuttal}, review generation \citep{auto-peer-review,darcy-marg-2024}, tagging and linking review comments with the paper \citep{kuznetsov-etal-2022-revise, darcy-etal-2024-aries}, rebuttal generation \citep{purkayastha-etal-2023-exploring}, the study and analysis of peer review \citep{kang-etal-2018-dataset, ghosal2022peer} and more general contexts such as document revision \citep{ruan-etal-2024-re3}. In PeerQA, we utilize peer reviews to source a scientific QA dataset.

\paragraph{Scientific QA} QA datasets in the scientific domain can generally be categorized as larger-scale datasets that are (semi-) automatically created and small expert-annotated datasets. 

\input{tables/dataset-size}

Among the larger-scale but (semi-) automatically created QA datasets are PubMedQA \citep{jin-etal-2019-pubmedqa}, in which questions are sourced from article titles that are phrased as questions. Answers are either yes, no, or maybe, and a subset is expert-annotated. SciDefinition \citep{august-etal-2022-generating} uses templates to generate questions about the definition of scientific terms. \citet{kulshreshtha-etal-2021-back} create a dataset in the ML and Biomedicine domain with questions sourced from Google's "People also ask" suggestions and answers from the search engine's span extraction feature. 
\citet{wan2024sciqag} generate a large-scale, scientific QA dataset by distilling a generation model from \mbox{GPT-4} instructed to output QA pairs given a paper. 
\citet{auer-et-al-2023-sciqa} develop question templates to automatically generate questions that are answerable from the Open Research Knowledge Graph \citep{jaradeh-et-al-2019-orkg} covering factoid questions, e.g., about the metadata of a paper, or questions that require inference over multiple papers. The questions in PeerQA are all focused on a single publication and the content of it, and our baselines use only the unstructured text of the article. PeerQA is an expert-annotated QA resource, where questions are sourced from human-written peer reviews and answers are annotated by paper authors.\looseness=-1

Regarding expert annotated datasets, the BioASQ challenge \citep{bioasq,krithara-et-al-2022-bioasq-qa} is an open-domain QA dataset from biomedical experts. Experts come up with questions and corresponding answers (yes/no, factoid, list, and free-form), which are additionally grounded in sentences from abstracts of publications on PubMed. While this is one of the greatest available resources for biomedical QA, annotating answers only in abstracts limits the question and answer complexity. Compared with PeerQA, questions are also more general, i.e., they are not asked within the context of a specific paper, and answers can be found in various articles.
Most similar to our work are the QASPER \citep{dasigi-etal-2021-dataset} and QASA \citep{qasa-lee-2023} datasets. In QASPER, NLP practitioners have read the abstract of a paper and raised questions about the paper. This leads to generic questions applicable to many papers (e.g., "Which baselines did they compare?") and questions that are easy to answer from the full paper. QASA takes this a step further by giving question annotators access to the full paper, instructing them to either skim or read it in more detail. In both these datasets, annotators create questions and answers; in contrast, our questions are based on peer reviews, i.e., they have been naturally raised by a reviewer, a domain expert who has read the paper in detail. Besides the questions, the answers in PeerQA are provided by experts, i.e., the authors of the respective papers. Table~\ref{tbl:related-work-dataset-comparision} provides an overview of these differences. To summarize, PeerQA is the first scientific QA resource with natural questions and all QA pairs annotated by paper authors.


In concurrent work, \citet{singh-etal-2024-scidqa} also explore extracting questions from peer reviews in the ML domain. Unlike PeerQA, their approach uses the authors' responses provided during the rebuttal to obtain reference answers. To identify supporting evidence from the paper for each answer, they employ a hybrid approach that combines manual and automated mapping of the answers to relevant information in the paper.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/peer-qa-stats-venue_pretty.pdf}
    \caption{Statistics of the PeerQA dataset. The color coding shows the distribution per venue and by the scientific community (i.e., blue colors for ML, orange for NLP, green for Geosciences, and purple for mixed). The gray dotted line indicates the average. The leftmost histogram shows a paper distribution, while the others show a distribution of questions. We measure the number of tokens using the \texttt{Llama-3} tokenizer.}
    \label{fig:dataset-stats}
\end{figure*}

\paragraph{Long-Context QA} Dialogue and QA systems grounded in a document have recently gained traction \citep{dialdoc-2023-dialdoc}. 
In this vein, NarrativeQA \citep{kocisky-etal-2018-narrativeqa} contains questions about movie scripts and books with an average length of 63k tokens. \citet{pang-etal-2022-quality} construct a multiple-choice dataset over books and articles with an average length of 5k tokens focusing on questions that require reading the article in detail. \citet{reddy-etal-2024-docfinqa} extend FinQA \citep{chen-etal-2021-finqa} to financial documents with an average of 123k words. ConditionalQA~\citep{sun-etal-2022-conditionalqa} is a dataset of government documents with an average length of 1.5k tokens and answers tied to certain input conditions. PeerQA serves as another resource for long-context QA, with documents having an average length of 12k tokens and 30\% of questions requiring combining information from more than one location in the paper.


\section{PeerQA}
\subsection{Data Collection}\label{sec:data-collection} 
Figure~\ref{fig:peer-qa-process} provides an overview of the data collection process. We use papers and peer reviews from NLPeer \citep{dycke-etal-2023-nlpeer} and extend this set with journals and conferences that publish peer reviews and camera-ready versions publicly. Specifically, the data from ARR 2022 (containing papers published at ACL and NAACL 2022), COLING 2020, ACL 2017, CoNLL 2016, and F1000 was curated in NLPeer, partially based on previous data collections \citep{kang-etal-2018-dataset, kuznetsov-etal-2022-revise} and published under a CC-BY-NC-SA 4.0 license. The data from the Geoscience domain is published under a CC-BY 4.0 license in two journals: Earth System Dynamics\footnote{\url{https://www.earth-system-dynamics.net}} (ESD) and Earth Surface Dynamics\footnote{\url{https://www.earth-surface-dynamics.net}} (ESurf). For ICLR 2022/2023 and NeurIPS Datasets and Benchmark Track 2022, we retrieve papers and reviews from OpenReview. Since they are without any license, we do not publish them in our release but provide a download and processing script. All questions and answers in PeerQA are published under CC-BY-NC-SA 4.0.

\paragraph{Paper Processing.} We extract the full text of the camera-ready version of a publication, including equations and captions, using GROBID~0.8~\citep{GROBID}, which also groups sentences into paragraphs, which we use later in our experiments.

\paragraph{Question Processing.} From the peer reviews of each paper, we extract an initial set of questions using all sentences ending in a question mark, resulting in 17910 questions.\footnote{In preliminary experiments, we extracted questions based on syntax. However, this resulted in many false positives.} The resulting questions comprise three problems: First, they are noisy as peer reviews often contain spelling or grammar mistakes. Second, they are contextualized into the preceding sentences of the review, i.e., their actual meaning can only be understood from the context of the review but not in isolation. Third, some questions contain compounds of multiple or follow-up questions after applying the decontextualization step. We deemed this problematic for our annotations as it would obfuscate which evidence aligns with which part of the question. To address these issues, we conduct two preprocessing steps: First, we create a clean and contextualized version of a question using InstructGPT\footnote{We use \texttt{text-davinci-003}. However, when we added the Geoscience subset, \texttt{text-davinci-003} was no longer available. Thus, we resorted to \texttt{gpt-4-0125-preview}.} \citep{ouyang-instruction-2022}. For this, we prompt the model with the preceding three sentences of the review and the extracted question to generate a single question that is context-independent. Conveniently, due to the good fluency of Large Language Models (LLM), this also addresses the noisiness of the original question. To detect multiple or follow-up questions, we employ a constituency parser \citep{kitaev-klein-2018-constituency, kitaev-etal-2019-multilingual} and flag questions with root-level conjunctions. We then decompose these questions adopting InstructGPT again.

Finally, we manually filter all resulting questions to include only information-seeking types of questions and discard questions that contain errors due to the preprocessing steps or not being relevant for a QA dataset. Specifically, we ensure that questions address the \emph{content} of the paper (e.g., we discard questions of rhetorical nature or about typos and layout) and are \textit{decontextualized} correctly (i.e., we discard questions that are ambiguous, contain hallucinations or references such as line numbers that are not present in a camera-ready version).\footnote{This filtering step has largely been done by a graduate NLP student supported by the paper authors.} In this step, we remove 30\% of the questions, yielding the final set of 12546 questions.

\paragraph{Answer Annotation.} 
Our questions were asked based on the submitted article. However, answers are annotated in the final publication. Hence, our annotation process relies on authors incorporating reviews into the final version for questions to be answerable. Questions might also be answerable when reviewers overlooked details in the submission that already answer their questions.
For each paper, we contact paper authors via email requesting their voluntary participation in answering the questions (see \S\ref{sec:appendix-contact-email}).\footnote{For the 5 CoNLL papers, we were unsuccessful in contacting the authors. Therefore, the annotations were performed by a senior NLP professor and co-author of this paper.} We implement multiple layers to instruct authors on how to complete the task. First, we provide a high-level description of the task in the initial email and a link to the detailed annotation guideline. We updated the annotation guideline during data collection with common questions we received. Moreover, we explain the annotation interface and demonstrate the task in a short video. Finally, our annotation interface (see \S\ref{sec:appendix-annotation-interface}) also contains UI elements that provide hints to the authors explaining the task. The annotation task comprises 4 steps: First, authors can provide feedback on a question, e.g., to remove or update it. Second, the authors highlight any text in the PDF of the final paper that is relevant to answering the question, which we refer to as Answer Evidence. Third, the authors provide free-form text that directly answers the question. Alternatively, questions can also be flagged as unanswerable. Unanswerable questions can, for example, occur when a question from a reviewer has been answered in the rebuttal but was not incorporated into the final publication. While we ask authors to perform all steps, some questions only have answer evidence or a free-form answer, but not both. The annotated evidence is mapped to the text extracted from the PDF. We notice that GROBID occasionally misses paragraphs that can not be mapped to the annotated evidence. We publish the raw annotated data and the mapped data, allowing future research with access to better PDF extraction tools to use the full dataset.\looseness=-1

\paragraph{Quality Control} Besides manually filtering questions and removing low-quality or irrelevant ones, we also provide the experts with a way to improve the dataset's quality. In our annotation interface, authors can leave feedback for a question, e.g., if they find it imprecise and wish to correct or remove it. All feedback has been manually processed, and the questions have been updated or removed.
Finally, we notice a high variance in the free-form answer quality. While some answers are clear and concise, others are more succinct and provide less detail. Although we give detailed guidelines on how to write the free-form answer to the authors, since we only engage briefly with them, it is challenging to enforce a similar quality. To counter this, we augment the collected answers with rephrases from GPT-4 \citep{openai-gpt-4}.\footnote{See \S\ref{sec:appendix-answer-augmentation-with-evidence} and \S\ref{sec:appendix-answer-augmentation-without-evidence} for prompts.}

Following this process, we obtained 579 answers from 208 papers. Table~\ref{tbl:dataset-nums} reports the number of annotations per venue. We also release the remaining 11967 questions from 2623 papers that have not been answered.\footnote{The number of mapped evidence from the noisy text extraction is reported in \S\ref{sec:appendix-dataset-numbers-with-extraxted}. Examples are provided in \S\ref{sec:appendix-examplary-annotation}. \S\ref{sec:appendix-unlabeled-data} reports a breakdown by venue for the unlabeled questions.}


\subsection{Analysis}\label{sec:analysis}
We report distributional statistics of the dataset in Figure~\ref{fig:dataset-stats}. Notably, the average paper length is 11723 tokens, which provides an interesting benchmark for long-context generative models. Furthermore, questions are relatively long, with an average of $20.2$ tokens (the average length in BioASQ, QASPER, and QASA is $13.2$, $10.2$, and $17.7$, respectively). One reason for this is the question processing pipeline, particularly the decontextualization step. Reviewers construct questions potentially consisting of multiple sentences. During preprocessing, the question has been rephrased to contain all this information. We analyze the semantic similarity between the final and original questions, finding that $90\%$ of questions have a similarity of more than $0.6$ and $50\%$ more than $0.82$.\footnote{\S\ref{sec:cosine-similarity-question-context} provides a detailed analysis of the similarities.} This shows that our processed questions remain highly similar to the original questions in the review. On average, questions have 3.8 annotated answer evidence sentences. Besides, 30\% of questions have non-consecutive answer evidence, i.e., the evidence is distributed non-contiguously over the paper.\footnote{\S\ref{sec:appendix-answer-evidence-stats} reports more answer evidence statistics.}\looseness=-1

\input{tables/evidence-retrieval}

We run a topic model to understand which questions are contained in PeerQA, specifically BERTopic \citep{grootendorst-bertopic-2022}. We find community-specific clusters (e.g., mentions of \textit{language} or \textit{annotation} for NLP; \textit{carbon} or \textit{soil} for Geoscience), topics about specific elements of the paper (e.g., figures, tables, or equations) or specialized clusters (e.g., adversarial attacks or fine-tuning/hyperparameter related questions).\footnote{A list of topics and their size can be found in \S\ref{sec:appendix-question-topcis}. We also apply the topic model to the unlabeled questions.} While the topic analysis clusters questions semantically, we also sample 100 questions randomly and sort them into one of 8 question classes: Methods, Data, Implications, Definitions, Comparisons, Analysis, Justification, and Evaluation.\footnote{The annotation was performed by two graduate students, reaching a substantial agreement of $0.68$ Cohens Kappa.} We find that 44\% of questions aim to clarify the methods or data, followed by 12\% of questions asking the authors to justify a decision.\footnote{Class definitions and the distribution can be found in \S\ref{sec:appendix-question-classes}.}

\section{Experiments}
\subsection{Answer Evidence Retrieval}\label{sec:experiments-answer-evidence-retrieval}
We set up the answer evidence retrieval task as an information retrieval problem: Given a query, the model computes a score for each passage in the paper, where a passage can be a paragraph or sentence. To evaluate the answer evidence retrieval task, we test models of various architectures, including cross-encoder \citep{nogueira-bert-reranking-2019}, dense retrieval \citep{reimers-gurevych-2019-sentence}, multi-vector dense retrieval \citep{khattab-colbert-2020}, sparse \citep{zamani-snrm-2018} and lexical models. Specifically, a cross-encoder model concatenates the query and passage and outputs a relevance score. In contrast, dense approaches encode query and passage independently by the same or individual models, obtaining a high-dimensional representation for each. A score is computed via dot-product or cosine-similarity between the two representations. Multi-vector approaches represent a query and passage not by a single but by many representations, e.g., for each token. The relevance score is computed by taking the sum of the maximum score between each query and passage token. Lexical approaches use term matching and weighting between the query and passage. Building upon this, sparse models perform a semantic query and/or document expansion to overcome the lexical gap. Concretely, we evaluate: \texttt{MiniLM-L12-v2} \citep{minilm, beir}, \texttt{Contriever} \citep{izacard-contriever-2022}, \texttt{Dragon+} \citep{lin-etal-2023-train}, \texttt{GTR} \citep{ni-etal-2022-large}, \texttt{ColBERTv2} \citep{santhanam-etal-2022-colbertv2}, \texttt{BM25} \citep{robertson-bm25-2009} and \texttt{SPLADEv3} \citep{lassance-spladev3-2024}.\looseness=-1


Besides various models, we investigate the impact of retrieving paragraphs or sentences. We use the paragraphs extracted by GROBID and mark any paragraph as relevant that contains a relevant sentence. Furthermore, we investigate a baseline to improve the decontextualization by prepending the title, which has been shown beneficial in cases where decontextualization is required \citep{wang-etal-2024-dapr}.\looseness=-1

We evaluate using Mean Reciprocal Rank (MRR) \citep{craswell-mrr-2009}, which considers the first relevant passage in a ranked list. While a typical question in PeerQA often has multiple answer evidence sentences (cf. Figure~\ref{fig:dataset-stats}), they frequently belong to the same paragraph or are close to each other. Therefore, pointing a user to the respective paragraph in a real-world application would already be useful as further relevant information usually clusters around the same location. We also measure the quality of the entire ranking by evaluating Recall@10. We chose 10, as most questions have fewer relevant sentences (cf. Figure~\ref{fig:answer-evidence-stats}).


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/metrics-unans-clf-macro-f1-all-concise3.pdf}
    \captionof{figure}{Answerability scores (y-axis) with different contexts (x-axis). In the \textit{Gold} setting, the model is only provided with the annotated, relevant paragraphs (i.e., no unanswerable questions are available in this setting); in \textit{Full Text}, the entire paper is provided in the context (and potentially truncated); otherwise, the top-scoring passages by \texttt{SPLADEv3} are provided. The Precision and Recall plots show the Answerable (- -) and Unanswerable ($\cdot\cdot$) classes.}
    \label{fig:results-un-answerable}
\end{figure*}

\subsection{Answerability and Answer Generation}\label{sec:experiments-answerability-answer-generation}
We set up the answerability task as a binary classification problem: given a question and context, a model predicts whether a question is answerable or not. We label all questions as answerable with annotated answer evidence and all as unanswerable, which the authors flagged as such. The answer generation task is set up as a sequence-to-sequence task, i.e., given the question and the context, the answer needs to be generated. For both tasks, we employ instruction-tuned LLMs. For the answerability task, we prompt the model to either answer the question if sufficient evidence is provided or to generate \textit{No Answer}. However, to obtain generations for all answerable questions, we remove the instruction to generate \textit{No Answer} from the prompt for the answer generation task (see \S\ref{sec:appendix-answerability-prompt} and \S\ref{sec:appendix-answer-generation-prompt} for the prompts). We experiment with providing as context the gold passages (ablating retrieval errors), the top-$k$ retrieved paragraphs (where $k \in \{10, 20, 50, 100\}$), and the full text. This is a Retrieval Augmented Generation (RAG) \citep{lewis-rag-2020} setup, except we retrieve from a single, long document instead of a corpus.We truncate the paragraphs if required by the maximum context size of the models and decode greedily from the models. Specifically, we use \texttt{Llama-3-8B-Instruct} \citep{llama3modelcard}, which we also extend to a 32k context size with dynamic rope-scaling, \texttt{Command-R}\footnote{\url{https://docs.cohere.com/docs/command-r}}, \texttt{Mistral-7B-Instruct-v0.2} \citep{jiang-mistral-2023}, \texttt{GPT-3.5-Turbo-0613-16k} and \texttt{GPT-4o-0806} \citep{openai-gpt-4}. We evaluate the answerability task as a binary classification problem. We evaluate with macro-F1 to counter the imbalance between the number of answerable (383) and unanswerable (112) questions.

Evaluating generative AI for long-form QA is a challenging, ongoing research topic by itself \citep{krishna-etal-2021-hurdles, xu-etal-2023-critical}. We chose a diverse set of evaluation metrics, including \mbox{Rouge-L} \citep{lin-2004-rouge}, AlignScore \citep{zha-etal-2023-alignscore} and Prometheus-2 \citep{kim-etal-2024-prometheus}. AlignScore is a model-based metric trained on a broad range of text alignment data, among others, on QA. AlignScore breaks the reference into passages of roughly 350 words and the generation into sentences. The model is trained to measure how much each generated sentence is aligned with the information in the reference passage. In practice, we notice that free-form answers provided by the authors can contain information that is not present in the paper. Therefore, besides using only the free-form answer as ground truth, we also compare the generation to the concatenated answer evidence paragraphs. The Prometheus-2 model is an LLM-as-a-judge model \citep{zheng2023judging} fine-tuned on feedback and judgment data generated by GPT-4 on a large set of custom score rubrics. We provide a scoring rubric that measures the correctness of the generated answer given the reference on a scale from 1-5.\footnote{See \S\ref{sec:appendix-evaluation-metric-details} for the Prometheus prompt and score rubric.}\looseness=-1

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\textwidth]{figs/gen-eval-rouge-alignscore-prometheus-all-concise3.pdf}
    \caption{Rouge-L F1, AlignScore and Prometheus Correctness metrics between 
    the annotated free-form answer (1. column), the GPT-4 augmented answer (2. column), the annotated evidence passages (3. column), and the generated answer.\looseness=-1}
    \label{fig:answer-generation-eval}
\end{figure*}

\section{Results}
\subsection{Answer Evidence Retrieval}
Table~\ref{tbl:evidnce-retrieval-results} reports the retrieval results. Across models, we find that retrieving the paragraph yields higher scores than the sentence. Appending the title to the paragraph further improves results (except \texttt{ColBERTv2}'s MRR), showing that decontextualizing the paragraphs from the paper helps. However, we find that MRR and Recall remain the same for most models when prepending the title on a sentence level. Since sentences are short, we conjecture that adding a title influences the overall representation too much, while on a paragraph level, the title only accounts for a fraction of the overall tokens. Overall, we find that \texttt{MiniLM-L12-v2}, \texttt{Dragon+}, and \texttt{SPLADEv3} perform the best. We proceed with \texttt{SPLADEv3} for the RAG experiments as it achieves the highest recall.

\subsection{Answerability}\label{sec:answerability-results}
We report precision, recall, and macro-F1 on the answerability task in Figure~\ref{fig:results-un-answerable}. We observe similar precision for all model and context settings. Precision for answerable questions is much higher than for unanswerable ones. When looking at recall, we find notable differences between the models. While \texttt{Mistral} and \texttt{Command-R} obtain relatively high recall on answerable questions and low recall on unanswerable questions, the \texttt{Llama} and \texttt{GPT} models obtain high recall on unanswerable questions and lower recall on answerable questions. This pattern can be explained: \texttt{Mistral} and \texttt{Command-R} tend to predict an answer more often, while \texttt{Llama} and \texttt{GPT} tend to predict the question is unanswerable, showing that all models have a bias towards one of the classes. \texttt{Command-R} and \texttt{GPT-4o} provide the best trade-off, shown by the highest macro-F1.


\subsection{Answer Generation}\label{sec:results-answer-generation}

Figure~\ref{fig:answer-generation-eval} reports the evaluation metrics comparing the generated answers to either the free-form reference answer, the GPT-4 augmented answer, or the gold paragraphs. Generally, models perform best with the gold answer evidence. Therefore, the annotated evidence provides a strong signal to answer the question. The scores achieved with the gold evidence represent an upper bound. However, higher scores might be possible with more context to better understand the gold answer evidence (or potentially unannotated but useful passages). Upon manual inspection of model errors, we find that lower performance is caused by evaluation failures or information in the free-form answer that is not supported by the evidence (i.e., information that is coming from the author's knowledge that might be general about the field or specific to the paper and did not make it into the camera-ready version). Generally, LLMs perform better in RAG, with fewer but relevant contexts, than in the full-text setting on PeerQA. This shows that despite LLMs' large context sizes, it is more effective to employ a retriever filtering relevant information than leaving this step to the internal workings of the LLM. A notable exception is GPT-4o, which exhibits stable performance with increasing context sizes and increasing performance on answer correctness. GPT-4o is also the most recent and powerful model in our evaluation, demonstrating the improved abilities of state-of-the-art models on long-context tasks. We further analyze the answer generation performance of the RAG setting by measuring the correlation between the retriever recall and the generation metric. We find mostly positive correlations between the retrieval and generation performance across models. While the correlation is not very strong (up to $r=0.42$), it confirms that with increased retrieval performance, the generation improves \cite{salemi-erag-2024}.\footnote{\S\ref{sec:appendix-correlation-generation-recall} reports correlations across all metrics and contexts.}

\begin{table}[!t]
\small
\centering
\begin{tabularx}{0.75\columnwidth}{lS[table-format=3.2]}
\toprule
{Error Class} & \\
\midrule
Correct / Evaluation Error & 43.75\% \\
Partially Correct & 12.50\% \\
Reasoning Error & 10.00\% \\
Implicit Evidence Only & 7.50\% \\
Insufficient Context & 11.25\% \\
Insufficient Evidence & 12.50\% \\
Insufficient Free-Form Answer & 3.75\% \\
\bottomrule
\end{tabularx}
\caption{Error analysis of \texttt{GPT-3.5}'s generations with gold evidence. \S\ref{sec:appendix-error-analysis} provides definitions and examples for error classes.}
\label{tbl:error-analysis}
\end{table}

\paragraph{Error Analysis.} We analyze the lowest performing 80 generations\footnote{Specifically, we sort by the minimum performance of all metrics, considering all questions that have both evidence and free-form annotations and use the \textit{gold} evidence as context.} of \texttt{GPT-3.5} to better understand the errors and report them in Table~\ref{tbl:error-analysis}. We find many low-scoring generations are correct despite at least one of the evaluation metrics providing a low score, for example, when the generation is more verbose or expresses the correct answer differently (\textit{Evaluation Error}). However, we find the metric with the highest score for these generations to be above the 50th percentile in 91\% of the cases. This shows that using different metrics against different ground truths is plausible and catches the alleged failures. Further, we observe the model is only \textit{partially correct} when the free-form answer contains important additional details. In other cases, the model fails to reason correctly over the evidence, e.g., it arrives at an opposite conclusion than the correct answer. Similarly, when the evidence is only implicit or requires expert domain knowledge, the model fails.
Lastly, there are also a few errors in the data. In $11.25\%$ of cases, the gold evidence is not self-sufficient, i.e., more context from the paper would be required, e.g., to understand previously introduced concepts. These errors can likely be recovered through additional retrieval. Other times the answer by the authors is not entailed by the evidence (\textit{Insufficient Evidence}) or the free-form answer only reports the element in the article, but not an actual answer (\textit{Insufficient Free-Form Answer}).\looseness=-1

\section{Conclusion}
We introduced the PeerQA dataset to advance and study question answering on scientific documents. We sourced PeerQA's questions from peer reviews and obtained answer annotations from the paper authors. Our dataset supports three crucial tasks for developing QA systems: evidence retrieval, answerability, and answer generation. We analyzed the collected data and established baseline systems for all three tasks. For evidence retrieval, we find that decontextualization is key to improving performance. On the answerability task, we find that models tend to either over- or under-answer, showing a bias for one of the classes. Further, although models can fit the entire paper into context in the answer generation task, providing the model with the top passages from a retriever outperforms the full-text setting. We also show that with increased retrieval performance, the answer generation improves. Finally, our error analysis highlights the need for better evaluation metrics and model reasoning abilities.\looseness=-1

\section{Limitations}
\paragraph{Dataset Size.} General domain QA datasets usually comprise up to three magnitudes more data than PeerQA (e.g., NQ has 323k samples). However, collecting high-quality data in the scientific domain is challenging due to the requirement for expert annotators. Since science has many domains, it is impractical to collect training data for each of them. Instead, models need to generalize in an unsupervised manner, at most leveraging few-shot examples. Therefore, we introduce PeerQA as an evaluation resource to test the generalizability of models. The size is in line with other recent datasets such as HumanEval \citep{human-eval-chen-et-al-2021} (164 examples), TruthfulQA \citep{lin-etal-2022-truthfulqa} (817), and GPQA \citep{gpqa-rein-et-al-2023} (448). In addition, we release the unlabeled data, comprising 12k questions from 2.6k papers, that can be used for more annotations, unsupervised learning, and further study of reviews. Small evaluation datasets also have the advantage of reduced iteration time over experimental settings, lesser use of compute resources, and a smaller environmental impact.\looseness=-1

\paragraph{Science Domains.}While PeerQA covers more scientific domains compared to prior work, there is a limited amount of data beyond the ML and NLP domains. A major challenge in data collection is the availability of public peer reviews with openly licensed scientific articles \citep{dycke-etal-2022-yes}. We call on the scientific community to further transform reviewing practices to an open format.

\paragraph{English-Only.}PeerQA is limited to English since it is dominant in scientific writing. Nevertheless, publications in other languages exist, and our data collection framework can be applied to any language. The evaluated retrievers are English-only models (except \texttt{BM25}, which is language-agnostic). Some retrieval models have multi-lingual counterparts (e.g., \texttt{mContriever}); however, due to a lack of multi-language data, their performance remains unclear. Some of the evaluated generative models are also multilingual; the performance in other languages is likely to be different than in English.

\paragraph{Free-Form Annotations.}While authors possess the ultimate expertise in their papers, they usually have knowledge beyond the information in their publications. Some free-form answers contain information not included in the answer evidence. For this reason, we also compare the generated answer with the annotated answer evidence, measuring if the model can produce answers entailed by the information in the paper.

\paragraph{Long-Form QA Evaluation.}Evaluating free-form answers is challenging and an ongoing area of research. To evaluate different aspects, we use three metrics against two ground truths. Ideally, we would have multiple free-form answer references; however, even collecting a single response has proven to be challenging. In the hope of better metrics, we also publish the generated answers of our baselines to facilitate adaptation to future, improved methods.

\paragraph{Methods.}Many LLMs and methods \citep{zhao-llm-survey-2023} exist that could be applied to the tasks in PeerQA. Therefore, more sophisticated and specialized methods might exceed the reported performances. However, we focus on introducing the dataset and establishing baseline systems with widely used retrieval and generative models.

\section{Ethical Considerations}
All annotators in PeerQA are authors of accepted articles at conferences or in journals. We do not collect any of their personal information or who has provided the answers. By the nature of our data collection protocol, we only contact authors who have provided their email publicly along with their publication and contact each author individually. Authors have participated voluntarily in the data collection, and we try to keep their workload low by only asking few questions (on average $2.8$). Furthermore, the authors have largely already answered questions during peer review (see \S\ref{sec:cosine-similarity-question-context}), making them familiar with the questions and answers, further reducing their workload.

One objective of PeerQA is to advance the study of peer review, including developing methods and tools to facilitate the authoring and reviewing of scientific articles. Particularly, LLMs have the potential to support authors and reviewers in their work \citep{kuznetsov2024natural}. However, these models also have biases and weaknesses. For example, in our question answerability task, we clearly observe that some models are biased towards one class, i.e., predicting the question as answerable or unanswerable (see \S\ref{sec:answerability-results}). Therefore, these methods can only be used as assistants that support humans. PeerQA sheds light on these issues, raising awareness of potential weaknesses in these models and their careful application in science.

\section*{Acknowledgements}
This work has been funded by the German Research Foundation (DFG) as part of the QASciInf project (grant GU 798/18-3). Further, we gratefully acknowledge the support of Microsoft with a grant for access to the OpenAI GPT models via the Azure cloud (Accelerate Foundation Model Academic Research).

We thank the anonymous reviewers for their helpful suggestions for improving this paper and Sukannya Purkayastha, Max Eichler, and Haritz Puerto for their insightful feedback throughout the paper-writing process. Our gratitude also goes to Maike Nowatzki for reviewing the questions in the Geoscience domain, to Richard Eckart de Castilho for his support with our annotation platform, and to Sebastian Alles for his assistance in establishing the compute and annotation infrastructure. Finally, we are grateful to all authors who have voluntarily participated in creating this dataset.


\bibliography{custom,anthology}

\clearpage

\appendix

\input{appendix}
\clearpage
\input{appendix_one_col}

\end{document}
