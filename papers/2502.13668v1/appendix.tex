\section{PDF extraction of Answer Evidence}\label{sec:appendix-dataset-numbers-with-extraxted}
Text extraction from PDF is not a perfect process. Unfortunately, this means that some annotated answer evidence (and therefore also answerable questions) must be discarded in our experiments since their evidence has not been extracted correctly. Table~\ref{tbl:dataset-numbers-with-extraxted} shows the number of annotated answer evidence (Evidence), as well as the number of questions whose evidence has been extracted correctly (Evidence Mapped). We nevertheless make the complete dataset public so future research with better PDF processing tools can leverage more annotations.
\begin{table}[!h]
\small
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
Venue & Questions & Evidence & Ev. Mapped \\ \midrule

ICLR 23 & 153 & 103 & 93 \\
ICLR 22 & 137 & 108 & 99 \\
NeurIPS 22 & 79 & 56 & 55 \\ 
ARR 22 & 87 & 60 & 55 \\
COLING 20 & 31 & 25 & 23 \\
ACL 17 & 20 & 16 & 16 \\
CoNLL 16 & 12 & 7 & 4 \\
ESD 23 & 17 & 10 & 10 \\
ESurf 23 & 16 & 16 & 16 \\
F1000 22 & 27 & 14 & 12 \\
\midrule
Total & \textbf{579} & \textbf{414} & \textbf{383} \\ \bottomrule
\end{tabular}
\caption{Number of questions with answer evidence that could be mapped to the PDF extracted text.}
\label{tbl:dataset-numbers-with-extraxted}
\end{table}

\section{Pre- \& Post-Processing Prompts}
\subsection{Question Clean-Up \& Decontextualization}\label{sec:appendix-question-processing-clean-up-and-decontextualization}
Given the extracted question and previous sentences (\texttt{context}) from the peer review, we use the following prompt to decontextualize the question:\\
\texttt{This is part of a scientific peer review where the reviewer raises a question regarding the paper.\\
"""\\
\{context\} \{question\}\\
"""\\
Write the last question such that it can be comprehended independently without the context of the review. Resolve any references to the review. Respond with a single question.
}
\subsection{Question Decomposition}\label{sec:appendix-question-processing-decompoistion}
In case the constituency parser detects a conjunction, we use the following prompt to decompose the question:\\
\texttt{This is a sentence from a peer review containing two questions.\\
"""\\
\{question\}\\
"""\\
Write the questions such that each can be comprehended independently without the context of the other question. Resolve any references in the second question. Therefore, the fundamental question information needs to be duplicated in each question.\\
}

\subsection{Answer Free-Form Augmentation with Evidence}\label{sec:appendix-answer-augmentation-with-evidence}
To ensure a similar quality and verbosity of answers, we augment the free-form answers provided by the authors using the prompt below in case the question has annotated evidence. If it does not have annotated evidence, we use the prompt in \S\ref{sec:appendix-answer-augmentation-without-evidence}.\\
\texttt{You are a helpful scientific research assistant. Your task is to write clean answers, given noisy answers from a scientific question answering dataset. The question has been asked during a peer review of a scientific article. Given the question, background information extracted from the paper, and a noisy answer, your task is to write a clean answer. Write a concise answer that directly answers the question. Make sure all information in your answer is covered by the background. Incorporate additional information from the original answer. Write the answer neutrally, i.e., as a third person (and not the author) answering the question. For example, use "The authors" instead of "We".\\
Question: \{question\}\\
Background: \{evidence\}\\
Original Answer: \{answer\}\\
Rephrased Answer:\\
}

\subsection{Answer Free-Form Augmentation without Evidence}\label{sec:appendix-answer-augmentation-without-evidence}
\texttt{You are a helpful scientific research assistant. Your task is to write clean answers, given noisy answers from a scientific question answering dataset. The question has been asked during a peer review of a scientific article. Given the question and a noisy answer, your task is to write a clean answer. Write a concise answer that directly answers the question. Incorporate the information from the original answer. Write the answer neutrally, i.e., as a third person (and not the author) answering the question. For example, use "The authors" instead of "We".\\
Question: \{question\}\\
Original Answer: \{answer\}\\
Rephrased Answer:\\
}

\section{Question Grounding}\label{sec:cosine-similarity-question-context}
Figure~\ref{fig:cosine-similarity-question-context} visualizes the similarity between the processed question and original review sentences. We use \texttt{all-MiniLM-L6-v2}\footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}} to compute the similarity. As detailed in \S\ref{sec:data-collection}, we extract questions from the peer review and contextualize them with the preceding three sentences from the review. To understand whether our preprocessing has altered the original question or not, we compute the maximum similarity between the final processed question and the four sentences of the review (i.e., the question and the three proceeding questions). We find that $90\%$ of questions have a similarity of at least $0.60$, and $50\%$ are more than $0.82$ similar to the final processed question. This shows the quality of our cleaning, decontextualization, and decomposition steps: Questions are generally highly similar and, therefore, grounded in the original peer review.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\columnwidth]{figs/cosine-similarity-question-context-ecdf.pdf}
    \caption{Empirical cumulative distribution function of the cosine similarity between the processed question and the sentences in the review. \textit{Context n} refers to the n-th preceding sentence before the raw, unprocessed \textit{Review Question}. \textit{Max. Similarity} takes the max operation over these four similarity scores, i.e., reports the similarity the processed question is most similar to.}
    \label{fig:cosine-similarity-question-context}
\end{figure}

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=\textwidth]{figs/peer-qa-stats-num-evidence.pdf}
    \caption{Number of evidence sentences (left), paragraphs (middle), and non-consecutive chunks (right) per question with annotated answer evidence.}
    \label{fig:answer-evidence-stats}
\end{figure*}

\section{Unlabeled Data}\label{sec:appendix-unlabeled-data}
\begin{table}[!h]
\small
\centering
\begin{tabular}{lrr}
\toprule
 Venue & Questions & Papers \\
\midrule
ICLR 23 & 5199 & 1188 \\
ICLR 22 & 3987 & 824 \\
NeurIPS 22 & 1186 & 110 \\
ARR 22 & 470 & 188 \\
COLING 20 & 70 & 33 \\
ACL 17 & 147 & 54 \\
CoNLL 16 & 3 & 3 \\
ESurf 23 & 312 & 51 \\
ESD 23 & 246 & 48 \\
F1000 22 & 347 & 124 \\
\midrule
\textbf{Total} & 11967 & 2623 \\
\bottomrule
\end{tabular}
\caption{Number of unlabeled questions and papers per venue.}
\label{tbl:appendix-unlabled-questions-count}
\end{table}


Besides the 579 questions with answer annotations, we additionally release all preprocessed and filtered 12k questions from 2.6k papers that have not been answered. Table~\ref{tbl:appendix-unlabled-questions-count} shows the breakdown per venue.


\section{Answer Evidence Statistics}\label{sec:appendix-answer-evidence-stats}
Figure~\ref{fig:answer-evidence-stats} reports the number of answer evidence depending on the retrieval unit. Note that this only includes answer evidence that we could map into the text extracted from the PDF. Non-consecutive chunks are essentially the number of different locations in the paper with answer evidence.

While the answer evidence for most questions comes from a single place, 30\% of questions have more than one location in the paper that addresses the question. While requiring to retrieve from multiple sources is related to multi-hop question answering \citep{welbl-etal-2018-constructing, yang-etal-2018-hotpotqa}, our setup is slightly different. We have also investigated the performance of questions with single vs multiple answer evidence chunks and have not found consistent differences. The information in the different chunks is not necessarily complementary, but it can also be that similar information is contained in each chunk, or a single chunk is sufficient to answer the question. 

\section{RAG Recall@k}
Figure~\ref{fig:recall-at-k-splade-v3} shows the recall at various cutoffs $k$ for \texttt{SPLADEv3}, the best-performing model answer evidence retrieval task. This model is used as a retrieval model for the retrieval augmented answer generation experiments.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=\columnwidth]{figs/recall-at-k-spladev3.pdf}
    \caption{Recall@k of \texttt{SPLADEv3} on the answer evidence retrieval task in the paragraph setting. The paragraphs retrieved by \texttt{SPLADEv3} are used in the RAG experiments.}
    \label{fig:recall-at-k-splade-v3}
\end{figure}


\section{Answerability Prompts}\label{sec:appendix-answerability-prompt}
We use the following prompts to determine whether a question is answerable or not in the setting where we provide the full text (\S\ref{sec:appendix-answerability-prompt-full-text}), the gold or retrieved paragraphs (\S\ref{sec:appendix-answerability-prompt-rag}).
\subsection{Full-Text}\label{sec:appendix-answerability-prompt-full-text}
\texttt{Read the following paper and answer the question. If the paper does not answer the question, answer with "No Answer".\\
Question: \{question\}\\
Paper: \{paper\}\\
Answer:
}
\subsection{RAG}\label{sec:appendix-answerability-prompt-rag}
\texttt{Read the following paragraphs of a paper and answer the question. If the paragraphs do not provide any information to answer the question, answer with "No Answer".\\
Question: \{question\}\\
Paragraphs: \{paragraphs\}\\
Answer:
}

\section{Answer Generation Prompts}\label{sec:appendix-answer-generation-prompt}
We use the following prompts to generate answers in the full-text (\S\ref{sec:appendix-answer-generation-prompt-full-text}) and RAG (\S\ref{sec:appendix-answer-generation-prompt-rag}) setting.
\subsection{Full-Text}\label{sec:appendix-answer-generation-prompt-full-text}
\texttt{Read the following paper and answer the question.\\
Question: \{question\}\\
Paper: \{paper\}\\
Answer:
}
\subsection{RAG}\label{sec:appendix-answer-generation-prompt-rag}
\texttt{Read the following paragraphs of a paper and answer the question.\\
Question: \{question\}\\
Paragraphs: \{paragraphs\}\\
Answer:
}

\section{Model Sizes and Computational Resources}
\paragraph{Answer Retrieval}
The number of parameters for each retrieval model is reported in Table~\ref{tbl:ir-models-parameters}. The retrieval experiments have been conducted on a Titan RTX 24GB.
\begin{table}[!h]
\small
\centering
\begin{tabular}{@{}lr@{}}
\toprule
Model & Parameters (M) \\ \midrule
MiniLM-L12-v2 & 33 \\
Contriever & 110 \\
Dragon+ & 110 \\
GTR-XL & 1240 \\
ColBERTv2 & 110 \\
BM25 & -- \\
SPLADEv3 & 110 \\ \bottomrule
\end{tabular}
\caption{Number of parameters in Millions of the evaluated retrieval models.}\label{tbl:ir-models-parameters}
\end{table}

\paragraph{Answerability \& Answer Generation}
Sizes for the models used in the answerability and answer generation task are reported with the model names. The number of parameters for the proprietary \texttt{GPT-3.5} and \texttt{GPT-4o} models are unknown, and we use it via the Azure API. We deploy the other models on a single A100 80GB GPU, except \texttt{Command-R} for which we require 2 A100 GPUs to fit also the longest paper fully into memory. All generation experiments use greedy decoding and use the \texttt{vllm} library \citep{kwon2023efficient} in version 0.4.2.

\section{Evaluation Metric Details}\label{sec:appendix-evaluation-metric-details}
\paragraph{Answer Evidence Retrieval}
To evaluate the answer evidence retrieval task, we use the mean reciprocal rank and recall implemented by the \texttt{pytrec\_eval} \citep{VanGysel2018pytreceval} package in version 0.5.
\paragraph{Un/Answerability} To compute the precision, recall, accuracy and F1 scores of the question answerability task, we use the classification report provided by \texttt{scikit-learn} \citep{scikit-learn} version 1.4.0.
\paragraph{Free-Form Answer Generation} The generated answers are evaluated with Rouge \citep{lin-2004-rouge}, AlignScore \citep{zha-etal-2023-alignscore} and Prometheus \citep{kim-etal-2024-prometheus}. 
For Rouge, we use the longest common subsequence (Rouge-L) between the generated answer and the reference answer. We use the \texttt{rouge-score} package in version 0.1.2 via Hugging Face's \texttt{datasets} package \cite{lhoest-etal-2021-datasets}. We also stem the generated and reference answer before computing the metric with the Porter Stemmer. All reported Rouge-L scores are F1 metrics.
For AlignScore, we use the fine-tuned checkpoint based on RoBERTa-large \citep{roberta} and use the \texttt{nli\_sp} mode, which splits the generation into sentences and uses a 3-way classification head to obtain scores. We use the original implementation in version 0.1.3.\footnote{\url{https://github.com/yuh-zha/AlignScore/commit/a0936d5afee642a46b22f6c02a163478447aa493}} For Prometheus, we use the \texttt{prometheus-eval} package with version 0.1.20\footnote{\url{https://github.com/prometheus-eval/prometheus-eval/releases/tag/v0.1.20}} and the  7B-v2.0 model\footnote{\url{https://huggingface.co/prometheus-eval/prometheus-7b-v2.0}} and instruct the model to evaluate the correctness with respect to the reference answer. Following \citet{kim-etal-2024-prometheus} and the score rubric proposed by \citet{rag-rubic-haystack} we use the following prompt:\\
\texttt{
\#\#\#Task Description:\\
An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\\
1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\\
2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\\
3. The output format should look as follows: "(write a feedback for criteria) [RESULT] (an integer number between 1 and 5)"\\
4. Please do not generate any other opening, closing, and explanations.\\
\#\#\#The instruction to evaluate:\\
Your task is to evaluate the generated answer against the reference answer for the question: \{question\}\\
\#\#\#Response to evaluate:\\
\{generation\}\\
\#\#\#Reference Answer (Score 5):\\
\{reference answer\}\\
\#\#\#Score Rubrics:\\
Correctness\\
Score 1: The answer is not relevant to the question and does not align with the reference answer.\\
Score 2: The answer is relevant to the question but deviates significantly from the reference answer.\\
Score 3: The answer is relevant to the question and generally aligns with the reference answer but has errors or omissions.\\
Score 4: The answer is relevant to the question and closely matches the reference answer but is less concise or clear.\\
Score 5: The answer is highly relevant, fully accurate, and matches the reference answer in both content and clarity.\\
\#\#\#Feedback:
}

\paragraph{Human Evaluation of AlignScore}
\citet{zha-etal-2023-alignscore} has evaluated correlation with human judgments extensively, particularly on factual consistency datasets. To show the reliability of AlignScore on our data, we manually label 100 randomly generated answers. Specifically, we compare the free-form and generated answer on a 1-5 Likert scale, evaluating whether the generation matches the free-form answer. This yields a significant (p<0.01) Spearman correlation of 0.449, indicating a moderate alignment between Human evaluation and the AlignScore metric.