\section{Related Work}
\paragraph{Peer Review} Many tasks and applications leverage peer reviews as a data source, including argument mining ____, helpfulness and score prediction ____, review generation ____, tagging and linking review comments with the paper ____, rebuttal generation ____, the study and analysis of peer review ____ and more general contexts such as document revision ____. In PeerQA, we utilize peer reviews to source a scientific QA dataset.

\paragraph{Scientific QA} QA datasets in the scientific domain can generally be categorized as larger-scale datasets that are (semi-) automatically created and small expert-annotated datasets. 

\input{tables/dataset-size}

Among the larger-scale but (semi-) automatically created QA datasets are PubMedQA ____, in which questions are sourced from article titles that are phrased as questions. Answers are either yes, no, or maybe, and a subset is expert-annotated. SciDefinition ____ uses templates to generate questions about the definition of scientific terms. ____ create a dataset in the ML and Biomedicine domain with questions sourced from Google's "People also ask" suggestions and answers from the search engine's span extraction feature. 
____ generate a large-scale, scientific QA dataset by distilling a generation model from \mbox{GPT-4} instructed to output QA pairs given a paper. 
____ develop question templates to automatically generate questions that are answerable from the Open Research Knowledge Graph ____ covering factoid questions, e.g., about the metadata of a paper, or questions that require inference over multiple papers. The questions in PeerQA are all focused on a single publication and the content of it, and our baselines use only the unstructured text of the article. PeerQA is an expert-annotated QA resource, where questions are sourced from human-written peer reviews and answers are annotated by paper authors.\looseness=-1

Regarding expert annotated datasets, the BioASQ challenge ____ is an open-domain QA dataset from biomedical experts. Experts come up with questions and corresponding answers (yes/no, factoid, list, and free-form), which are additionally grounded in sentences from abstracts of publications on PubMed. While this is one of the greatest available resources for biomedical QA, annotating answers only in abstracts limits the question and answer complexity. Compared with PeerQA, questions are also more general, i.e., they are not asked within the context of a specific paper, and answers can be found in various articles.
Most similar to our work are the QASPER ____ and QASA ____ datasets. In QASPER, NLP practitioners have read the abstract of a paper and raised questions about the paper. This leads to generic questions applicable to many papers (e.g., "Which baselines did they compare?") and questions that are easy to answer from the full paper. QASA takes this a step further by giving question annotators access to the full paper, instructing them to either skim or read it in more detail. In both these datasets, annotators create questions and answers; in contrast, our questions are based on peer reviews, i.e., they have been naturally raised by a reviewer, a domain expert who has read the paper in detail. Besides the questions, the answers in PeerQA are provided by experts, i.e., the authors of the respective papers. Table~\ref{tbl:related-work-dataset-comparision} provides an overview of these differences. To summarize, PeerQA is the first scientific QA resource with natural questions and all QA pairs annotated by paper authors.


In concurrent work, ____ also explore extracting questions from peer reviews in the ML domain. Unlike PeerQA, their approach uses the authors' responses provided during the rebuttal to obtain reference answers. To identify supporting evidence from the paper for each answer, they employ a hybrid approach that combines manual and automated mapping of the answers to relevant information in the paper.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/peer-qa-stats-venue_pretty.pdf}
    \caption{Statistics of the PeerQA dataset. The color coding shows the distribution per venue and by the scientific community (i.e., blue colors for ML, orange for NLP, green for Geosciences, and purple for mixed). The gray dotted line indicates the average. The leftmost histogram shows a paper distribution, while the others show a distribution of questions. We measure the number of tokens using the \texttt{Llama-3} tokenizer.}
    \label{fig:dataset-stats}
\end{figure*}

\paragraph{Long-Context QA} Dialogue and QA systems grounded in a document have recently gained traction ____. 
In this vein, NarrativeQA ____ contains questions about movie scripts and books with an average length of 63k tokens. ____ construct a multiple-choice dataset over books and articles with an average length of 5k tokens focusing on questions that require reading the article in detail. ____ extend FinQA ____ to financial documents with an average of 123k words. ConditionalQA____ is a dataset of government documents with an average length of 1.5k tokens and answers tied to certain input conditions. PeerQA serves as another resource for long-context QA, with documents having an average length of 12k tokens and 30\% of questions requiring combining information from more than one location in the paper.