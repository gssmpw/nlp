\onecolumn

\section{Annotation Instructions and Interface}
\subsection{Contact Email}\label{sec:appendix-contact-email}
\begin{multicols}{2}
Figure~\ref{fig:email} shows an instance of an email that invited the authors to participate in the data collection. Email addresses have been extracted from papers or, in the case of EGU and F1000, addresses to corresponding authors are provided online. To prevent spamming authors, we have ensured that no author received more than 3 emails (e.g., when they were listed as authors on multiple papers of a venue). Email addresses were only used to contact authors and are not part of the dataset. We also do not publicize the code to extract email addresses from papers.
\end{multicols}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/peer-qa-email-blurred.png}
    \caption{Exemplary contact email that has been sent to authors requesting their participation in answering the questions.}
    \label{fig:email}
\end{figure*}

\clearpage
\begin{multicols}{2}
\subsection{Annotation Interface}\label{sec:appendix-annotation-interface}
The annotation interface for providing answers is shown in Figure~\ref{fig:annotation-interface}. The camera-ready PDF of the publication is shown on the right-hand side, while answer annotations can be provided on the left side. In \textit{1.2 Question Feedback}, authors can leave free-form feedback about the question, e.g., if it should be removed or modified. By clicking on the \textit{Add} button in \textit{2.1 Answer Evidence}, text spans in the PDF can be highlighted. One highlight can span over several sentences or even pages. Multiple spans can be added by clicking the \textit{Add} button again. In \textit{2.2. Answer Free Text}, the free-form answer to the question can be given. Finally, in \textit{3.1.}, the authors can also mark the question as unanswerable or provide further feedback to the question. If none of the categories apply, feedback on why the question is unanswerable can also be provided in \textit{No Answer Reason Free Text}.
\end{multicols}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/peer-qa-annotation-interface.png}
    \caption{Screenshot of the annotation interface. The annotation consists of four parts. First, the annotator can provide feedback to the question, e.g., to correct its meaning or provide their interpretation. Second, answer evidence is annotated by highlighting sentences in the PDF. Third, a free-form answer can be provided, directly answering the question. Lastly, if a question is unanswerable or is of low quality, the interface provides an option to flag the question.}
    \label{fig:annotation-interface}
\end{figure*}

\clearpage
\begin{figure*}[!b]
    \centering
    \includegraphics[width=\textwidth]{figs/sunburst-sunset.pdf}
    \caption{Sunburst diagram of the 4-grams in the PeerQA questions.}
    \vspace{11em}
    \label{fig:sunburst-questions}
\end{figure*}
\begin{multicols*}{2}
\section{Question Sunburst}\label{sec:appendix-sunburst-questions}
We visualize the starting 4-grams of all questions in Figure~\ref{fig:sunburst-questions}. All words have been lowercased and lemmatized, and rare n-grams have been discarded.
\end{multicols*}



\clearpage


\begin{figure}[!b]
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/topic-keywords.pdf}
         \caption{Topics of Labeled Questions}
         \label{fig:question-topics-labelled}
     \end{subfigure}
     \hfill
     \vspace{0.5em}
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/topic-keywords-unlabelled.pdf}
         \caption{Topics of Unlabeled Questions}
         \label{fig:question-topics-unlabelled}
     \end{subfigure}
        \caption{Top 8 keywords for the top 12 question topics. The first topic contains all questions that could not be assigned during clustering. The bar shows the keyword's c-TF-IDF score. The top figure shows the topics for the labeled questions and the bottom for the unlabelled.}
        \label{fig:question-topic-keywords}
\end{figure}

\begin{multicols*}{2}
\section{Question Topics}\label{sec:appendix-question-topcis}
Figure~\ref{fig:question-topic-keywords} reports the result of applying BERTopic \citep{grootendorst-bertopic-2022} on the labeled (\ref{fig:question-topics-labelled}) and unlabeled (\ref{fig:question-topics-unlabelled}) questions. Table~\ref{tbl:question-topics} additionally shows representative questions for the topics. We use the standard \texttt{all-MiniLM-L6-v2} sentence transformer model to compute embeddings. After embedding, stopwords have been removed, and the words have been lemmatized using spacy \citep{spaCy2020} to improve the keyword extraction. In \S\ref{sec:analysis}, we analyzed the topics of the labeled questions cluster. We found them to be focused on the scientific community and its subtopics or related to elements in the paper. We observe similar clusters in the unlabeled data (e.g., topic \#4 for NLP, topic \#12 for Geoscience, \#6 focusing on (pre-)training, \#5 on figures and plots).
\end{multicols*}


\clearpage


\input{tables/question-topics}

\clearpage

\section{Example Annotations}\label{sec:appendix-examplary-annotation}
\input{tables/example-annotations}

\clearpage
\begin{multicols}{2}
\section{Question Classes}\label{sec:appendix-question-classes}
Table~\ref{tbl:question-classes} reports the number of questions per class and representative questions for that class. The definitions for each class are the following:
\paragraph{Method Clarification} Questions to better understand a specific detail (e.g., a parameter) or inner workings of a proposed or used method, including methods used for obtaining data or details about the experiment setup/process.
\paragraph{Data Clarification} Questions to understand the process of obtaining data or properties of the used data for an experiment, however, excluding questions about a method to obtain data.
\paragraph{Justification/Rationale} Questions that challenge an assumption, ask the authors to motivate a decision's reasoning or are critical towards a process/finding.
\paragraph{Analysis} Questions asking for a better understanding of a result, e.g., why a method works or questions asking about what factors contribute to a result/finding. 
\paragraph{Implication} Questions about potential real-world applications, transfers of the data/method/findings to other applications/domains/tasks, or wider-scoped consequences of the findings.
\paragraph{Definition} Questions about the (intended) meaning of a certain phrase or term used in the paper.
\paragraph{Comparison} Questions asking for comparisons or differences between methods/data or different studies.
\paragraph{Evaluation/Evidence} Questions asking for details about a result (excluding analysis of results), details of the evaluation process, or evidence to support a certain claim.
\end{multicols}

\input{tables/question-classes}

\clearpage

\begin{multicols}{2}
\section{Answerability Evaluation}\label{sec:answerability-evaluation}
Table~\ref{tbl:answerability-evaluation} shows detailed evaluation metrics for the answerability task, and Figure~\ref{fig:results-un-answerable} visualizes them. We report Precision, Recall, and F1-Score on both the answerable and unanswerable questions, as well as the average accuracy, weighted, and macro F1-Score.
\vfill\null
\columnbreak
\vfill\null
\end{multicols}
\input{tables/answerability-evaluation}
\clearpage


\begin{multicols}{2}
\section{Answer Generation Evaluation}\label{sec:answer-generation-evaluation}
Table~\ref{tbl:answer-generation-evaluation} reports the exact numbers of the free-form answer generation experiment for all models and contexts, corresponding to Figure~\ref{fig:answer-generation-eval}.
\vfill\null
\columnbreak
\vfill\null
\end{multicols}
\input{tables/answer-generation-evaluation}
\clearpage

\section{Answer Generation Error Analysis}\label{sec:appendix-error-analysis}
As outlined in \S\ref{sec:results-answer-generation}, we conducted an error analysis on GPT-3.5's generations. Table~\ref{tbl:appendix-error-class-definition} defines each error class, and Table~\ref{tbl:error-examples} provides an example for each class.
\subsection{Error Classes}
\begin{table}[!htpb]
\small
\begin{tabularx}{\textwidth}{lX}
\toprule
Error Class & Definition \\
\midrule
Evaluation Error & The generated answer is correct; however, at least one of the metrics provides a low score. \\
Partially Correct & The generated answer is correct; however, the free-form answer provides additional details that are not covered by the generation. \\
Reasoning Error & The generation is incorrect. The model fails to arrive at the same conclusion as the free-form answer. \\
Implicit Evidence Only & The generation is incorrect. The evidence only implies the correct answer, making it challenging for the model to infer it. \\
Insufficient Context & The answer is incorrect because further context is required to interpret the evidence correctly (e.g., abbreviations in the context are not resolved, or information established earlier in the paper is missing from the evidence).  \\
Insufficient Free-Form Answer & The author's free-form answer points only to an answer in the paper but does not contain an answer (e.g., "The architecture is depicted in Figures D.3 and D.4").\\
Insufficient Evidence & The highlighted context by the authors does not provide sufficient information to answer the question well. \\
\bottomrule
\end{tabularx}
\caption{Error classes definitions for analyzing the failure modes of the generation models.}
\label{tbl:appendix-error-class-definition}
\end{table}

\clearpage
\subsection{Error Examples}
\input{tables/error-examples}

\clearpage
\section{Answer Generation Correlation Analysis}
\subsection{Recall}\label{sec:appendix-correlation-generation-recall}
\begin{figure*}[!b]
    \centering
    \includegraphics[width=0.63\textwidth]{figs/correlation-generation-recall-all-concise3.pdf}
    \caption{Pearson correlation ($r$) with the corresponding $p$-value between the recall (x-axis) at $k$ (columns) and the answer generation performance (y-axis) according to different metrics (rows). Therefore, each circle represents a single QA pair of a specific model. We added 0.03 x-jitter to the markers to improve visibility.}
    \label{fig:correlation-generation-recall}
\end{figure*}

\begin{multicols*}{2}
Figure~\ref{fig:correlation-generation-recall} visualizes the relationship between the recall of the retrieval model (in this case \texttt{SPLADEv3}) at different cutoffs and the answer generation performance measured by different metrics.
\end{multicols*}

\clearpage
\begin{multicols}{2}
\subsection{Mean Evidence Position}\label{sec:appendix-correlation-evidence-poistion}
Figure~\ref{fig:correlation-evidence-poistion} visualizes the Pearson correlation between the answer generation metric (Rouge-L, AlignScore, or Prometheus-2 compared to either the answer evidence, the annotated free-form answer or the GPT-4 augmented free-form answer as ground truth) and the mean token position of the answer evidence. All generations are taken from the full-text setting, i.e., where the entire paper text was given as input to the model. To compute the mean token position for each answer evidence, we compute the number of tokens in the paper before the evidence sentence. If a question has multiple answer evidence, we take the average position. We only find a weak relationship that is statistically insignificant in many cases. Nevertheless, some p-values show statistical significance, indicating that for some settings, the generation performance declines when the answer evidence is relatively towards the end of the paper. This finding is also consistent with related work such as \citet{buchmann-etal-2024-attribute}.
\end{multicols}

\begin{figure*}[!hb]
    \centering
    \includegraphics[width=0.84\textwidth]{figs/correlation-evidence-poistion-all-concise3.pdf}
    \caption{Pearson correlation ($r$) with the corresponding $p$-value between the answer generation evaluation metric (y-axis) and the mean token position of the annotated answer evidence (x-axis).}
    \label{fig:correlation-evidence-poistion}
\end{figure*}

\clearpage

\begin{multicols}{2}
\section{Answer Generation Similarities}\label{sec:appendix-answer-generation-similarity}
We compute the average similarity of the generated answers between all models. We embed the generated answers with \texttt{all-MiniLM-L6-v2} and compute the cosine similarity between the generations of the models. Figure~\ref{fig:answer-generation-similarity} visualizes the similarities with the gold and retrieved evidence and full-text settings.
We find that all models produce fairly similar outputs for the gold setting, i.e., where the annotated answer evidence is provided. With increasing retrieved evidence as context (i.e., RAG-10 - RAG-100), the similarity between the model outputs decreases but remains relatively high.
\end{multicols}
\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{figs/answer-generation-similarity-heatmap-all-concise3.pdf}
    \caption{Semantic similarity of the generated answers between models with different context settings.}
    \label{fig:answer-generation-similarity}
\end{figure*}


\begin{multicols}{2}
\section{Attributable Question Answering}

\begin{Table}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
 & MRR & Recall@10 \\ \midrule
SPLADEv3 & 0.4536 & 0.6661 \\
GPT-3.5-Turbo-0613-16k & 0.2440 & 0.2762 \\ 
GPT-4o-0806-128k & 0.5429 & 0.5339 \\ 
\bottomrule
\end{tabular}
\captionof{table}{Evidence retrieval scores in the attributable question answering setting.}
\label{tbl:appendix-attributable-qa-evidence-retrieval-results}
\end{Table}


We have considered answer generation based on the top retrieved paragraphs (RAG) or using the full context (\S\ref{sec:experiments-answerability-answer-generation}). In the RAG setup, the answer generation can generally be attributed to the retrieved passages (assuming the model is faithful to the context). However, when using the full text as context, attribution to the passage level is not trivial. Recently, attributable question answering has gained momentum \citep{bohnet-etal-2022-attributed,gao-etal-2023-enabling,malaviya-etal-2024-expertqa}, where in addition to generating an answer, the model is supposed to cite evidence supporting it. Therefore, we also conduct an experiment where the model is conditioned on the full text of the paper and is tasked to "cite" any paragraphs on which the generated answer is based. We prepend an id before each paragraph and include an instruction on how to cite. Specifically, we use the following prompt:

\texttt{
Read the following paper and answer the question. Provide one or several evidence paragraphs that can be used to verify the answer. Give as few paragraphs as possible, but as many that provide evidence to the answer. Your answer must have the following format: "\textless answer\textgreater\ [X] [Y]". In your reply, replace \textless answer\textgreater\ with your answer to the question and add any references in square brackets. Your answer must be followed by the ids of the relevant segments from the document.
Question: \{question\}\\
Paper: \{paper\}\\
Answer:
} 
\\

This setting has the challenge that the model does not provide a ranked list of all paragraphs but an unordered list of what it considers relevant. Therefore, we rank the cited paragraphs in the order in which the LLM generates them.

Table~\ref{tbl:appendix-attributable-qa-evidence-retrieval-results} reports the results of the evidence retrieval with the attributable question answering setup. We find that for \texttt{GPT-3.5}, the scores fall far behind the performance of a dedicated retrieval model (e.g., \texttt{SPLADEv3}). For \texttt{GPT-4o}, the MRR outperforms \texttt{SPLADEv3}, however, the Recall@10 is inferior.

We further investigate the answer generation performance of the attributable QA setup, reporting the results in Table~\ref{tbl:appendix-attributable-qa-answer-generation-results}. Compared with the RAG setting using the top 20 paragraphs retrieved by \texttt{SPLADEv3}, the attributable QA setup performs worse. A RAG setup is also significantly more cost and compute-efficient, particularly considering the long context of papers. Specifically, the average paragraph in PeerQA has 94 tokens, leading to an average of 1880 tokens to encode in the RAG-20 setting. In contrast, on average, a paper has 11723 tokens. Therefore, the full-text setup is 6.24 times more expensive than the RAG-20 setting. 
\end{multicols}

\input{tables/answer-generation-evaluation-attributable}