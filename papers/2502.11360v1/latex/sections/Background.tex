\section{Background}
\dw{(Some parts of this section can be shortened or removed if necessary.)}

In this section, we formalize the PGPS process and describe the main recipes of our method such as the VLM and the CLIP.

\subsection{Plane geometry problem solving (PGPS)}

% The goal of PGPS is building a solver which generates valid solution steps for a given plane geometry problem, i.e., a pair of diagram and text.
% In other words, for a given diagram and text pair $(D, X)$, we aim to build a solver $f_\theta$ which generates solution steps $S = f_\theta(D, X)$.
% The solution steps $S$ can be reasoning steps written in natural language with computed answer~\citep{gllava,mavis} or code which can be executed by an engine which the execution result is the answer~\citep{geoqa,unigeo,pgps}.
% Here, we study the solvers which generate the solution steps as code.

The goal of PGPS is to implement a solver that produces valid solution steps for a given problem represented by a diagram–text pair. Formally, for a pair \((D, X)\) consisting of a diagram \(D\) and textual description \(X\), the solver should generate solution steps \(S = [s^{(1)}, \cdots, s^{\text{len}(S)}]\). These steps may take the form of natural language reasoning with an answer~\citep{gllava,mavis}, or code that can be executed by an engine to obtain the final answer~\citep{geoqa,unigeo,pgps}. In this work, we focus on solvers that produce solution steps as executable code.

The solver is trained to generate valid solution steps using a dataset \(\{(D_i, X_i, S_i)\}_{i=1}^N\), where each tuple consists of a diagram, a textual description, and a corresponding solution code. Formally, the training objective is to maximize the likelihood of the ground-truth solution code \(S_i\) conditioned on the diagram \(D_i\) and textual description \(X_i\) which is written as:
\begin{equation}
    \max_\theta \prod_{i=1}^{N} P_\theta\bigl(S_i \mid D_i, X_i\bigr),
    \label{eq:objective}
\end{equation}
where $\theta$ is the parameters of a solver.

\subsection{Vision-language model}

To integrate diagram interpretation and reasoning, PGPS works typically employ VLMs~\citep{need-citation}. A VLM comprises a vision encoder and a language model. The vision encoder $g$ produces a visual embedding \(h_D = g(D)\) from the diagram \(D\), and the language model then generates solution steps in an autoregressive manner, conditioned on the textual description \(X\) and the visual embedding \(h_D\) as:
\begin{equation}
s^{(i)} = \mathrm{LanguageModel}\bigl(h_D, X, s^{(0:i-1)}\bigr),
\label{eq:vlm}
\end{equation}
where \(s^{(1:i-1)}\) refers to previously generated tokens and \(s^{(0)}\) is a pre-defined start token.
The likelihood is then defined as:
\begin{equation*}
    P_\theta(S\mid D, X) = \prod_{i=1}^{\textrm{len}(S)}P_\theta\left(s^{(i)} \mid h_D, X, s^{(0:i-1)}\right),
\end{equation*}
where $\theta$ is the parameters of the language model and the vision encoder $g$.

To further improve the vision encoder’s comprehension of geometric diagrams, several studies introduce auxiliary tasks trained with the primary objective (\cref{eq:objective}). Some approaches utilize supervised learning to predict geometric primitives~\citep{}, knowledge points~\citep{}, or scene graphs~\citep{} describing geometric objects. Other approaches employ self-supervised learning strategies such as jigsaw location prediction~\citep{geoqa,geoqa-plus}, masked auto-encoding~\citep{mae}, and vector-quantized variational autoencoder (VQ-VAE)~\citep{vqvae}.

\subsection{Contrastive language-image pre-training}

Although auxiliary tasks tailored to geometric diagrams and self-supervised learning have attracted growing attention in the PGPS community, CLIP is widely adopted in VLMs across diverse domains~\citep{}.

Given a collection of diagram–caption pairs \(\mathcal{D} := \{(D_i, X_i)\}_{i=1}^N\), we train the vision encoder \(g\) and the text encoder \(h\) using the following contrastive learning objective:
\begin{align}
    &\mathcal{L}_{\textrm{CLIP}}(\mathcal{D}) := \nonumber \\
    &\,\,\,\,\mathbb{E}_{\mathcal{D}} \!\biggl[ -\log \frac{\exp \bigl( g(D_i)^T \, h(X_i) / \tau \bigr)}{\sum_{X \in \{X_i\}_i} \exp \bigl( g(D_i)^T \, h(X) / \tau \bigr)} \biggr],
    \label{eq:clip}
\end{align}
where \(\tau\) is a temperature parameter.

CLIP pre-trained models have shown outstanding performance, particularly in zero-shot classification with text labels~\citep{}. Moreover, they serve as highly effective pre-trained backbones for a range of vision tasks, including optical character recognition~\citep{}, geo-localization~\citep{}, and object recognition~\citep{}.
