\section{Experiments}

In this section, we evaluate the PGPS performance of our VLM equipped with the domain adapted \geoclip{} on MathVerse~\citep{mathverse}.
We compare its performance against established PGPS baselines. We also present ablation studies highlighting our VLM’s strong visual feature recognition and resilience to domain shifts, both of which are facilitated by the adapted vision encoder.
% For the remaining experiments, we designate GeoQA and PGPS9K as the target domains. 

\subsection{Experimental settings and training details}


\input{latex/tables/mathverse_results}
% \subsection{MathVerse}

% \paragraph{Datasets.}
\paragraph{Datasets.}
We use MathVerse~\citep{mathverse} to measure the performance of VLMs.
MathVerse is a benchmark designed to evaluate both the reasoning and visual-feature recognition capabilities of VLMs, covering plane geometry, solid geometry, and function problems. It is constructed by compiling problems from various sources, including Geometry3K~\citep{intergps}, GeoQA~\citep{geoqa}, and GEOS~\citep{geos}. Each problem is presented in five variants: \emph{text-dominant}, which provides all essential textual information for solving the problem; \emph{text-lite}, which omits descriptive details, e.g., object shapes, from the text; \emph{vision-intensive}, which removes certain textual conditions that can be inferred from remaining information; \emph{vision-dominant}, which relocates numerical measurements, such as angles and lengths, from the text to the diagram; and \emph{vision-only}, which offers only the diagram as input, embedding all text within the diagram. In the following experiments, we focus on plane geometry problems and exclude the vision-only task.

% \subsection{Training details}

% \paragraph{Training details.}
\paragraph{Training details.}
We describe the construction of our \textbf{geo}metric VLM with \textbf{d}omain-\textbf{a}gnostic visio\textbf{n} enc\textbf{o}der, named \geovlm{}. 
Based on \geoclip{} developed in \cref{sec:geoclip}, we apply the domain adaptation to GeoQA and Geometry3K datasets. For the domain adaptation, we randomly sample 50 diagrams and translate the diagram and caption styles following the procedure described in \cref{sec:domain_adaptation}. Finally, \geoclip{} is fine-tuned via \cref{eqn:da}.
We name the GeoQA and Geometry3K adapted \geoclip{} as \geoclip{}-DA.

We combine LLama-3-8b-Instruct~\citep{llama} and \geoclip{}-DA to construct a VLM. The combined model is then fine-tuned again with the training set of GeoQA and PGPS9K to predict the solution program. For PGPS9K, we use the Geometry3K split.  While previous works focusing on PGPS do not consider optical character recognition (OCR) from diagrams since the benchmark datasets, GeoQA and PGPS9K, provide necessary details in problem descriptions, numerical values can appear within diagrams in real-world settings. Therefore, we fine-tune \geovlm{} with additional OCR capability by modifying the problem statements.
Additional details about the modification process with hyper-parameter configurations can be found in \cref{sec:vlm_details}.

In addition, we unify the programming languages used in the solution programs of GeoQA and PGPS9K by converting GeoQA language into PGPS9K format. The unification makes the output of VLM consistent since both datasets use different types of formal languages.

% the following paragraphs need to be added in to appendix.

%However, our fine-tuning strategy differs slightly from previous works~\citep{unigeo,pgps,geox}. 
%Here, we clarify the difference between our approach and previous approaches. In previous works, the VLM is trained to produce the solution program given diagram and problem description as shown in \cref{fig:pgps_examples}. An interesting observation from GeoQA and PGPS9K datasets is that the numerical measurements, such as angles, lengths, and volumes, are not written in the problem description but given as additional conditions, and the numerals are substituted as a variable in the problem description as shown in \cref{fig:geoqa_example}. Therefore, the VLM only needs to produce the solution program without having optical character recognition (OCR) from the diagram. The variables are automatically substituted by the actual numbers when the program is executed. Therefore, the vision encoders do not need to learn OCR from the image.

% However, this approach cannot be generalized to a wider class of problems where the numerals are embedded in the diagram instead of written in the problem description. Some variants of MathVerse, such as the vision-dominant problems, fall into this category as well. To incorporate OCR into the solution of the problem, we modify some problem statements in the training set, such that the numerical measurements are only shown in the diagram and not in the statements. We further modify the solution problem so that the solution contains OCR results as a part of the final output. Finally, we unify the language of the solution programs used in GeoQA and PGPS9K by converting GeoQA programs into PGPS9K format. The unification makes the output of VLM consistent since both datasets use different types of formal languages.

% \cref{fig:training_data} shows examples of the modified input pairs and solutions, where the first problem statement does not have numerical measurements and the OCR results are in the part of the output solution program.

% \paragraph{Baselines.}
\paragraph{Baselines.}
We use two different types of baseline models for the experiments: PGPS \emph{specialist VLMs} and \emph{generalist VLMs}. Specialist VLMs produce a solution program as an output of a given problem, and generalist VLMs produce a natural language solution as an output.

For the specialist VLMs, we test PGPSNet~\citep{pgps}, NGS~\citep{geoqa}, SCA-GPS~\citep{scagps}, GeoFormer~\citep{unigeo}, UniMath-Flan-T5~\citep{unimath}, and GeoX~\citep{geox}. For GeoX, we use the two variants GeoX-Geo3K and GeoX-GeoQA, which are fine-tuned on Geometry3K and GeoQA, respectively.

For the generalist VLMs, we test two GPT-4o variants~\citep{gpt4o}: gpt-4o-2024-11-20 and gpt-4o-mini-2024-07-18, and the InternVL2.5 variants: 8B and 26B models~\citep{internvl2_5}.

% \paragraph{Evaluation metric.}
\paragraph{Evaluation metric.}
For each plane geometry problem, both the specialist VLMs and \geovlm{} generate 10 outputs via beam search. Following \citet{pgps}, we then use completion accuracy and top-10 accuracy as our primary evaluation metrics. The completion accuracy assesses whether the first successfully executed solution from the beam is correct; the solutions are reviewed in beam order, and success is recorded if the first executable solution produces the correct answer. Top-10 accuracy examines all ten beam outputs, counting a success if any of these solutions yield the correct result upon execution. Note that, as described before, the specialist VLMs do not have OCR capability. For the evaluation, we feed the correct values to the outputs of these models by using the parser developed in \citet{pgps}. For the models that are trained in Chinese, i.e., NGS and SCA-GPS, we use problem descriptions translated by GPT-4o~\citep{gpt4o}.

To measure the performance of the generalist VLMs, we use multiple-choice questions instead of open-ended questions due to the difficulty in parsing the final answer from free-form text. We use the multiple-choice question provided in MathVerse as an additional input to each problem. We ask VLMs to produce the answer in a pre-specified form. We report the top-1 accuracy of these models.
To compare \geovlm{} against the generalist models, we use the same protocol used in \citet{pgps} to measure the accuracy.

\subsection{Results}
\label{sec:experiments}

\paragraph{Performance against specialist VLMs.}
In \cref{tab:mathverse}, \geovlm{} shows the best performance in almost all the problem variants and metrics except the completion accuracy in the text-dominant task. Note that the specialist models cannot solve the vision-dominant problems since these problems do not contain variables representing numerical values, such as a length, in the problem description.
When comparing the performance between text and vision-dominant tasks, the top-10 accuracy of \geovlm{} on vision-dominant task is higher than the top-10 accuracy of the specialist models on text-dominant task except for GeoX-GeoQA. Given that the two tasks use the same problem set, the result implies that \geovlm{} performs better than the specialist models without having the geometric premises in the problem description. In other words, our vision encoder can extract geometric premises accurately from the visual information.


\input{latex/tables/llm_results}
\paragraph{Performance against generalist VLMs.}
\cref{tab:mathverse_llm_transposed} reports the performance of generalist VLMs and \geovlm{} on multiple choice questions. \geovlm{} outperforms proprietary closed models, i.e., GPT-4o variants, and open-sourced models, i.e., the InternVL2.5 variants. % except the vision-dominant task.
Especially, the performance gap between GeoDANO and InternVL2.5-26B reflects the parameter efficiency of our VLM.
While GeoDANO shows impressive results among the variants, the performance of GeoX-GeoQA degrades dramatically as the visual information moves from the text to the diagram.
% The previous specialist underperform the generalist VLMs because top-10 accuracy cannot exceed the accuracy reported in \cref{tab:mathverse_llm_transposed}. 
Our work is the first to show that the specialist can compete with the generalist in MathVerse.


\subsection{Ablation studies}
\label{sec:abl}

\paragraph{Variation of \geoclip{}.}
We perform a detailed empirical analysis to evaluate how effectively the \captionstyle{} captions and the proposed domain adaptation technique improve \geovlm{}’s performance. Specifically, we compare \geovlm{} against other VLMs trained on the \geoclip{} variants, including OpenCLIP~\citep{clip} and the GeoCLIP without domain adaptation.
We also test a variant of \geoclip{} trained with additional diagram-caption pairs from the target domains without having any filtering process. In this case, we utilize all the data in the training sets.

We show the experimental result in \cref{tab:mathverse}. 
\geovlm{}-OC and \geovlm{}-GC represent the VLM with OpenCLIP and GeoCLIP without domain adaptation, respectively. \geovlm{}-GCD represents the GeoCLIP with additional unfiltered domain captions. \geovlm{} outperforms other variants on most tasks, except the completion accuracy on the vision-intensive task.

\paragraph{OCR performance.}
We assess the accuracy of \geovlm{} and its variants in OCR on the MathVerse diagrams, focusing on the vision-dominant task. We evaluate the OCR performance of the first executable solution program in top-10 VLM predictions. \geovlm{}-OC, \geovlm{}-GC, \geovlm{}-GCD, and \geovlm{} achieve 1.84\%, 20.26\%, 13.95\%, and 46.58\% accuracy, respectively.
The result explains the accuracy improvement of \geovlm{} in the vision-dominant task against other variants.

\input{latex/tables/domain_adaptation_results}
\input{latex/figures/tsne}

\paragraph{Domain adaptation analysis.}
We examine how effectively GeoCLIP-DA generalizes to new domains with different diagram styles. For this experiment, we compare the embedding similarity between two diagrams representing the same structure in different styles. To create the paired dataset, we use a similar process described in \cref{sec:domain_adaptation}. Specifically, a total of 100 diagrams are sampled from the test sets of GeoQA and PGPS9K, and these samples are rendered in AlphaGeometry style through the diagram description.

For evaluation, we sample 100 diagrams from each of the target domain's training sets and compare the similarity against the original diagram via cosine similarity. We also compute the similarity between the style transferred diagram and the original diagram. 
We report two metrics for test diagrams: the mean rank (MR) and the mean average precision (mAP) of the style-transferred diagram.

As reported in \cref{tab:domain_adaptation}, \geoclip{}-DA produces similar embeddings for structurally equivalent diagrams, regardless of their stylistic differences. \cref{fig:tsne} visualizes the diagram embeddings of OpenCLIP and GeoCLIP-DA. As one can observe, the OpenCLIP embeddings are largely separated by the domain of the diagrams, whereas those of \geoclip{}-DA appear to capture and align with underlying visual features more effectively.

% \input{latex/figures/tsne}
