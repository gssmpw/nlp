\section{Improving the Vision Encoder Geometric Premises Recognition}

In this section, we first propose \geoclip{}, a new vision encoder designed to recognize geometric premises from diverse styles of diagrams.
To transfer the recognition to real-world PGPS benchmarks, we then propose a domain adaptation technique for \geoclip{} that leverages a small set of diagram–caption pairs from the target domains. 

% \subsection{Overall architecture}

% We begin by summarizing the architecture of our VLM, a combination of a vision encoder and a language model. For the vision encoder, we use \geoclip{}-DA, introduced in \cref{sec:domain_adaptation} with a two-layer MLP of GeLU activation as the projection layers following LLaVA-OneVision~\citep{llava-next}. For the language model, we employ LLama-3-8B-Instruct~\citep{llama}.
% For a given diagram and question pair in PGPS, we feed the vision encoder with the given diagram, and then the output of the encoder is used as an input token of LLM through the projection layer. The question text is then fed into the LLM, followed by the diagram embedding.
%The input to the language model is a concatenation of the diagram embedding, the textual input, and previously generated tokens, following a scheme similar to LLaVA~\citep{llava}.


\subsection{\geoclip{}}
% \subsection{Improving visual \geofeat{} recognition}
\label{sec:geoclip}

To make a vision encoder recognize geometric diagrams better, we propose a \geoclip{}, a vision encoder trained with CLIP objective with a newly developed 200,000 diagram-caption examples.
From the random diagram generator developed in \cref{sec:synthetic_data_engine}, we additionally sample 200,000 diagrams written in the formal language. Directly rendering these samples can result in a diagram that may not preserve the geometric properties. For example, the perpendicularity between two lines cannot be observed from the diagram without having the right angle sign, i.e., $\measuredrightanglewithsquare$. Therefore, we ensure to render the images containing all necessary geometric premises from its visual illustration.

For the caption of a diagram, we filter out some geometric properties from the original description of a diagram used to render the image. Specifically, we only keep the following four properties, concyclic, perpendicularity, angle measures, and length measures, from the visual premises shown in \cref{tab:alphageometry}. After that, we convert the remaining descriptions written in the formal language into natural language. We filter out some properties for two reasons.
First, some properties are not recognizable from the rendered diagram without additional information, e.g., congruency. These properties are listed as non-visual premises in \cref{tab:alphageometry}. Second, collinearity and parallelity occur so frequently that they can marginalize others.
Some examples of generated captions after filtering and translating are provided in the right-most column of \cref{fig:alphageometry}. We call the filtered caption as \emph{\captionstyle{} caption}.

With this dataset, we fine-tune OpenCLIP~\citep{clip} according to the CLIP objective which is formulated as:
% To be specific, given a collection of image–caption pairs \(\mathcal{D} := \{(D_i, X_i)\}_{i=1}^N\), we train the vision encoder \(g\) and the text encoder \(h\) using the following contrastive learning objective:
\begin{align}
    &\mathcal{L}_{\textrm{CLIP}}(\mathcal{D}, g, h) := \nonumber \\
    &\,\,\,\,\,\mathbb{E}_{\mathcal{D}} \!\biggl[ -\log \frac{\exp \bigl( g(D_i)^T \, h(X_i) / \tau \bigr)}{\sum_{X \in \{X_i\}_i} \exp \bigl( g(D_i)^T \, h(X) / \tau \bigr)} \biggr],
    \label{eq:clip}
\end{align}
where \(\mathcal{D} := \{(D_i, X_i)\}_{i=1}^N\) is the diagram-caption pairs, $g$ is the vision encoder, $h$ is the text encoder, and \(\tau\) is a temperature parameter.
% in \cref{eq:clip}, 
We named the resulting vision encoder as \geoclip{}. \cref{sec:hparams} provides the details, including hyper-parameters.

We compare the performance of \geoclip{} to other self-supervised approaches trained with the same dataset. We test three self-supervised approaches: Jigsaw~\citep{geoqa, geoqa-plus}, MAE~\citep{scagps, geox}, and VQ-VAE~\citep{unimath} used in previous work to improve the recognition performance of plane diagrams. We use the same architecture used for \geoclip{} for Jigsaw and MAE with the hyper-parameters used in the previous works. For VQ-VAE, we follow the architecture of \citet{unimath}. 
All model performances are measured through the linear probing used in \cref{sec:benchmakr_results}. 
% We measure the performances of the models through the linear probing used in \cref{sec:benchmakr_results}.

As shown in \cref{tab:linear_probing}, \geoclip{} recognizes geometric features better than existing baselines and self-supervised methods. The self-supervised approaches generally perform poorly for the benchmark, justifying the choice of the objective. We also compare the performance of \geoclip{} against other encoders such as OpenCLIP. Note that although we outperform the other encoders in difficult tasks such as SquareShape and AngleDetection, these results might be \emph{unfair} since the training set of \geoclip{} is similar to the diagrams in the benchmark.
The t-SNE plots of the embeddings from the vision encoders are illustrated at \cref{fig:tsne_benchmark}.

We further ablate the filtering process in \geoclip{}. To this end, we compare \geoclip{} with its two variants: \emph{GeoCLIP (F $\times$)}, which uses the captions generated without filtering. We also test \emph{GeoCLIP (2K)}, which is trained on only 2,000 pairs, to see the effectiveness of the large-scale dataset.
The results in \cref{tab:linear_probing} imply both the filtering and the training set size matter in enhancing geometric properties recognition.

% \paragraph{Solution program and numerical measurements prediction.}

% We then change the output of PGPS to predict not only the solution programs but also the numerical measurements in the diagram and text.
% While the previous PGPS models extract the numerical measurements from the text or assume as given features, our revised target enables problem solving even if the numerical measurements are only provided in the diagram. 

% Furthermore, the utilization of the vision encoder in the VLM would be enhanced due to the numerical measurements prediction from the diagram.

%To achieve this, we employ the diagram and geometric properties pairs obtained from the programs generated by the synthetic data engine.
%In forming the textual captions, we retain only the following geometric properties: concyclic, perpendicularity, and angle measure, which are basically visual features. Other properties are excluded for two reasons. First, certain properties cannot be inferred from the diagram without additional information, e.g., congruency, so those are considered non-visual features. Second, some visual features occur so frequently that they overshadow others, such as collinearity and parallelity. Consequently, those are removed as well.
% \sh{Examples of generated diagram and caption pairs are visualized at \cref{fig:geoclip}.}
% \todo{We need an example of this process in Appendix (which information is retained and removed from the original description.)}

%\cref{fig:alphageometry} shows the examples of sampled diagram-caption pairs.
%We produce a total of 200,000 diagram–caption pairs. 

%Using this dataset, we train OpenCLIP~\citep{clip}, whose architecture is ViT-L/14~\citep{vit} with an image resolution of $336 \times 336$, according to the CLIP objective at \cref{eq:clip}. Additional details regarding hyper-parameters are provided in \cref{sec:geoclip_hparams}.


\subsection{Domain adaptation of \geoclip{}}
% \subsection{Enhancing domain generalization capability}
\label{sec:domain_adaptation}

Although \geoclip{} enhances the geometric premises recognition on the benchmark set, the diagram styles in existing PGPS benchmarks differ, necessitating further adaptation. To overcome this challenge, we propose a domain adaptation method for \geoclip{}. To this end, we propose a few-shot domain adaptation method utilizing a few labeled diagrams.

% to transfer the knowledge from a source to a target domain. %To overcome this challenge, we propose a simple yet effective domain adaptation method that makes the vision encoder focus on important geometric information instead of irrelevant attributes, such as color and font family.
A domain-agnostic vision encoder must match the same diagrams drawn in different styles. To do so, we need a target domain diagram translated into the source domain style or the source diagrams translated into the target domain style. With these translated images, we can guide the model to focus on key geometric information instead of irrelevant attributes, such as color and font family. However, in practice, it is difficult to obtain the same diagrams with different styles. 

We develop a way to translate the target diagrams into source style.
Thankfully, since well-known PGPS datasets come with diagram captions written in formal languages~\citep{intergps}, we can easily convert them to the AlphaGeometry-style descriptions. 
Given the translated descriptions, we utilize the rendering engine of AlphaGeometry to translate the target domain images into the source domain. 
With the translation, we can generate the same diagram in the source domain style. 
\cref{fig:domain_adaptation_samples} provides examples of the diagram pairs with different styles. However, in some cases, the original description contains geometric premises that are unrecognizable from the diagram, such as \(\angle ACB = 35.0\) in \cref{fig:geoqa_example}. Therefore, we apply the same filtering process used in \geoclip{} to translate the AlphaGeometry-style descriptions into natural languages.

%In practice, however, not only do the diagrams have different styles, but also the captions. For example, the textual description of the GeoQA diagram in \cref{fig:geoqa_example} includes non-visual details, such as \(\angle ACB = 35.0\), which we removed from the dataset used to train \geoclip{}. Therefore, we apply the same filtering process used to curate captions for \geoclip{}. To this end, 

Formally, let $\mathcal{D}_{S} := \{(D_S^{(i)}, X_S^{(i)}) \}_{i=1}^{N_S}$ be the diagram-caption pairs from source domain $S$, e.g., the synthetic diagrams, and let $\mathcal{D}_{T_j} := \{(D_{T_j}^{(i)}, X_{T_j}^{(i)}) \}_{i=1}^{N_{T_j}}$ be the set of diagram-caption pairs of target domain $T_j$, e.g., the PGPS benchmarks. With the translation process described above, we can synthesize a style-transferred diagram-caption pair $(\hat{D}_{T_j}^{(i)}, \hat{X}_{T_j}^{(i)})$ for each diagram $D_{T_j}^{(i)}$ and caption $X_{T_j}^{(i)}$ in target domain $T_j$.

We perform domain adaptation by fine-tuning the vision encoder through the style-transferred diagram-caption pairs.
Let $\hat{\mathcal{D}}_{T_j}$ be a collection of the original diagram and style-transferred captions, i.e., $\hat{\mathcal{D}}_{T_j} = \{(D_{T_j}^{(i)}, \hat{X}_{T_j}^{(i)})\}_{i=1}^{N_{T_j}}$, and let $\hat{\mathcal{D}}_{T_jS}$ be a collection of the original and style transferred diagram pairs, i.e., $\hat{\mathcal{D}}_{T_jS}= \{(D_{T_j}^{(i)}, \hat{D}_{T_j}^{(i)}) \}_{i=1}^{N_{T_j}}$. The cross-domain adaptation objective is written as
%jointly training with diagram–caption pairs from both the source and target domains, as well as with the diagram pairs produced by the synthetic engine. Formally, we update the vision and text encoders $g$ and $h$ using the CLIP objective in \cref{eq:clip}, defined as:
\begin{align}
    &\mathcal{L}_{\textrm{CLIP-DA}}(\mathcal{D}_S, \{\mathcal{D}_{T_j}\}_j, g, h) := 
    \mathcal{L}_{\textrm{CLIP}}(\mathcal{D}_S, g, h) + \nonumber\\
    &\,\,\,\,\,\,\,\,\Sigma_j \mathcal{L}_{\textrm{CLIP}}(\hat{\mathcal{D}}_{T_j}, g, h) + \mathcal{L}_{\textrm{CLIP}}(\hat{\mathcal{D}}_{T_jS}, g, g),
    \label{eqn:da}
\end{align}
where $g$ and $h$ are the vision and text encoders of \geoclip{}, respectively. Note that we do not use the original captions from the target domain, since our goal is to adapt the vision encoder to the target domain, not the text encoder.
% We refer to the domain-adapted vision encoder as \geoclip{}-DA.
% Figure x illustrates the vision encoder pre-training phase.

% \paragraph{Unified solution program language.}

% Although GeoCLIP-DA is capable of capturing relevant visual \geofeat{}s across diverse diagram styles, the difference in solution program languages often restrict the VLM to fully leverage the enhanced visual embeddings. To address the gap, we unify the programming languages of the PGPS datasets, i.e., GeoQA and PGPS9K, by implementing a converter that transforms the solution programs from GeoQA into the PGPS9K format.
% We also convert the prediction target to predict both solution program and the numerical measurements as mentioned in \cref{sec:geoclip}.
% Examples of the training data are at \cref{fig:training_data}.

% \subsection{Training details - \dw{can we move this to experiments?}}

% For the remaining experiments, we designate GeoQA and PGPS9K as the target domains. In GeoQA, $N_{T_j}=50$ diagrams are sampled and manually annotated with \geoclip{}–style captions, followed by the generation of $N_{S_j}=100$ synthetic diagrams for each sampled diagram.
% In PGPS9K, where visual features are explicitly provided, the given features are converted into natural language to produce \geoclip{}–style captions. From this set, 50 diagram–caption pairs are chosen, and corresponding AlphaGeometry programs are generated. Subsequently, each sampled target diagram is supplemented with $N_{S_j}=100$ synthetic diagrams for domain adaptation.

% Next, we build the VLM using GeoCLIP-DA. We freeze the vision encoder and train the projection and language model on the revised training data of GeoQA and PGPS9K. Further details on hyper-parameters are provided in \cref{sec:vlm_hparams}.