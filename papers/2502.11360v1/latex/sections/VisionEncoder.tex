\section{Visual Geometric Premises Recognition Benchmark for Vision Encoders}
\label{sec:visual_feature}

In this section, we first develop a benchmark for evaluating a vision encoder's performance in recognizing geometric features from a diagram. We then report the performance of well-known vision encoders on this benchmark.

%\dw{We first examine how commonly used vision encoders in open-source VLMs recognize geometric primitives, such as points and lines, and geometric premises, such as perpendicularity, from a given diagram.} Because a VLM’s performance is typically measured by its final solution in PGPS, the result does not fully capture the encoder’s capacity to identify fundamental geometric structures. 
% Yet, as the encoder is the first component to observe the diagram, understanding its recognition ability is essential for further advancement.
% To address this gap, we introduce a benchmark that thoroughly evaluates vision encoders.

\subsection{Benchmark preparation}
\label{sec:synthetic_data_engine}

We design our benchmark as simple classification tasks. By investigating PGPS datasets, we identify that recognizing \emph{geometric primitives}, such as points and lines, and geometric properties representing \emph{relations between primitives}, such as perpendicularity, is important for solving plane geometry problems. Recognized information forms \emph{geometric premises} to solve the problem successfully. To this end, we carefully curate five classification tasks as follows:
\begin{itemize}
    \item \textbf{Concyclic}: A circle and four points are given. The task is to identify how many of those points lie on the circle.
    \item \textbf{TwoLines}: Two lines, AB and BC, are given alongside other geometric objects. The task is to determine whether AB and BC are perpendicular, collinear, or neither.
    \item \textbf{ObjectShape}: A given diagram includes one of the following geometric objects: a segment, triangle, square, or pentagon. The task is to classify which object is present.
    \item \textbf{SquareShape}: A diagram including a square ABCD and other geometric objects is given. The task is to classify whether the square is a trapezoid, parallelogram, or rectangle.
    \item \textbf{AngleDetection}: A diagram is given with at least three points: A, B, and C. The task is to classify the correct angle of ABC from \(\{15^\circ, 20^\circ, \ldots, 75^\circ\}\). 
\end{itemize}
An example of each task is provided in \cref{fig:benchmark}.

\input{latex/figures/benchmark}

Our benchmark is built on top of AlphaGeometry~\citep{alphageometry}, which is designed to solve IMO-style plane geometry problems. The program provides useful functions such as formal language describing plane diagrams. The language predefines a set of geometric premises listed in \cref{tab:alphageometry}, including all necessary properties to define our benchmark tasks. In addition, once a diagram description is given in formal language, the program renders a corresponding diagram with varying fonts, colors, widths, orientations, and resolutions, allowing us to have diagrams with diverse styles often observed in a real-world scenario.

We create question-and-answer pairs based on AlphaGeometry. To sample a diverse set of question-and-answers, we first establish a foundational geometric structure corresponding to the key problem of the task and then repeatedly add new points or lines with randomly selected geometric relationships to the existing diagram with the help of the formal language. The pseudo-code for the random question generation is presented in \cref{alg:sampling}. For each task, we generate 50,000, 10,000, and 10,000 question-and-answer pairs for training, validation, and testing, respectively.

\iffalse
We implement a data engine that randomly generates \emph{plane geometry problems}, complete with corresponding \emph{diagrams} and \emph{\geofeat{}s}, including the relationship between lines and measurements such as angles. Our engine is built on top of AlphaGeometry~\citep{alphageometry}, a proof assistant for IMO-style plane geometry problems. AlphaGeometry provides a formal language for representing plane geometry problems. 
\sh{
Given an AlphaGeometry problem, we can derive the \geofeat{}s and characteristics of the given problem such as perpendicularity and collinearity. For completeness, we provide the list of \geofeat{}s that AlphaGeometry can support in \cref{tab:alphageometry}.
In addition, we can render the diagram that represents the AlphaGeometry problem with different fonts, colors, widths, orientations, and resolutions, allowing us to have diagrams with diverse styles often observed in a real-world scenario.
}
% as an executable program. 
% AlphaGeometry can derive the solution when a problem statement is given in terms of the formal language. 
% Given an AlphaGeometry problem written in the formal language, we can render the diagram which expresses . AlphaGeometry supports diverse \geofeat{}s and characteristics of plane geometry diagrams, such as perpendicularity and collinearity. For completeness, we provide the list of \geofeat{}s that AlphaGeometry can support in \cref{tab:alphageometry}. %\cref{fig:alphageometry} illustrates the examples of the problems in the formal language with the rendered diagrams. %repeated below
% In addition, the diagram can be rendered with different fonts, colors, widths, orientations, and resolutions, allowing us to have diagrams with diverse styles often observed in a real-world scenario.

%We employ AlphaGeometry’s formal language to curate a synthetic dataset. 
To sample a diverse set of plane geometry problems, we randomly generate AlphaGeometry problems by first establishing a foundational geometric structure, and then repeatedly adding new points with randomly selected geometric relationships to existing points, drawn from the predefined set in AlphaGeometry. The pseudo-code for the sampling algorithm is presented in \cref{alg:sampling}, and randomly selected examples are provided in \cref{fig:alphageometry}. Note that samples consist of the AlphaGeometry problem and \geofeat{}s written in the text along with the corresponding diagram.

%\subsection{Benchmark for visual \geofeat{} recognition}
Based on the synthetic data engine, we develop a benchmark that evaluates how well the vision encoders recognize visual \geofeat{}s in plane geometry diagrams. In PGPS, recognizing characteristics such as collinearity, perpendicularity, and angle measures is crucial for solving geometry problems. Based on the elements, we define five diagram classification tasks that depend on accurate visual \geofeat{} perception:
\begin{itemize}
    \item \textbf{ObjectShape}: Each diagram includes one of the following geometric objects: a segment, triangle, square, or pentagon. The goal is to classify which object is present.
    \item \textbf{Concyclic}: Each diagram contains a circle and four points. The task is to identify how many of those points lie on the circle.  
    \item \textbf{TwoLines}: Each diagram includes two lines, AB and BC, alongside other geometric objects. The vision encoder must determine whether AB and BC are perpendicular, collinear, or neither.
    \item \textbf{SquareShape}: Each diagram features a square ABCD with additional objects. The goal is to classify whether the square is actually a trapezoid, parallelogram, or rectangle. 
    \item \textbf{AngleDetection}: Each diagram includes an angle ABC and other objects. The objective is to identify the measure of angle ABC from among the set \(\{15^\circ, 20^\circ, \ldots, 75^\circ\}\). 
\end{itemize}
We can generate samples used for benchmarking by adding additional conditions to the random problem generator. For example, we let the generator synthesize AlphaGeometry problems with a segment, triangle, square, or pentagon for the ObjectShape task.
For each task, we generate 50,000, 10,000, and 10,000 AlphaGeometry problems for training, validation, and test, respectively.
We then render the diagrams and corresponding labels from the generated problems, which is later used to train the vision encoders.
\cref{fig:benchmark} shows randomly generated diagrams for the classification tasks.
\fi


\subsection{Results}
\label{sec:benchmakr_results}
\input{latex/tables/linear_probing_results}


With the proposed benchmark, we evaluate four widely adopted vision encoders for the open-sourced VLMs: OpenCLIP~\citep{clip}, SigLIP~\citep{siglip}, DinoV2~\citep{dinov2}, and ConvNeXT~\citep{convnext}.

To evaluate the vision encoder, we adopt a linear probing approach. Specifically, we add a linear layer on top of each encoder as a prediction head and train the linear layer from scratch while freezing the parameters of the vision encoder. We use a training set to train the prediction head and report the test accuracy with the best validation performance. The details for the hyper-parameters are described in \cref{sec:hparams}.

As shown in \cref{tab:linear_probing}, many existing vision encoders well recognize the shape of objects but fail to recognize the correct angle between two lines. The encoders also show some difficulties in recognizing the shape of a square and the relationship between two lines. Although the result may seem satisfactory at a glance, these errors will propagate to the downstream tasks when combined with LLMs. Hence, it is important to improve the recognition performance of the vision encoder further. 
%The t-SNE plots of the vision encoder embeddings are illustrated at \cref{fig:tsne_benchmark}. %Let's move this to the next section
