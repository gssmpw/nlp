\section{Related Work}

In this section, we summarize the studies related to the benchmarks proposed to evaluate plane geometry problem solving (PGPS), the models trained for PGPS, and the contrastive learning methods used to enhance PGPS performance.

\subsection{PGPS benchmarks}
\label{sec:relatedwork_benchmark}

% \input{latex/tables/benchmarks_summary}

% PGPS benchmark
Several studies have introduced benchmarks for PGPS, including a set of diagrams and corresponding problem and solution descriptions~\citep{geoqa,intergps,pgps,unigeo}. The problem and solution descriptions are provided in natural languages or formal languages. Often, the solution steps are provided in the form of formal language. Given the dataset, the goal of PGPS is to train a model that produces a valid solution as an executable program.
\iffalse
As the problem is multimodal in nature, many works on PGPS combine vision and language models, resulting in a vision-language model. In many architectures, the \emph{vision encoder model} is expected to extract useful information from diagrams and send it to the language model as a form of embedding, and then, the \emph{language model} produces the solution program based on visual embeddings and problem description.
\fi

However, as recent work by \citet{mathverse} shows, the problem description contains too much information such that the model produces a valid solution program without having the diagram information as shown in \cref{fig:pgps_examples}. 
% In addition, the benchmarks provide numerical measurements, such as angles, lengths, and volumes, in both training and test sets, as shown in \cref{fig:pgps_examples}.
% This reduces the need for the vision encoder to perform optical character recognition (OCR) on the diagrams.
% cannot be obtained from the diagrams but only from the textual problem description as shown in \cref{fig:pgps_examples}.
%This observation ignites the development of a new set of benchmarks. 
MathVerse~\citep{mathverse} introduces modifications to existing PGPS benchmarks by directly encoding the geometric properties and relations into the diagrams. Therefore, it is impossible to produce a valid solution without recognizing the necessary information from diagrams. Despite the effort, it is still unclear to what extent the vision encoder recognizes the geometric conditions in a diagram as models are evaluated in an end-to-end fashion.

% To jointly evaluate the perception of visual \geofeat{} and reasoning capabilities of a model, MathVerse~\citep{mathverse} extends existing PGPS benchmarks by transferring the visual \geofeat{}s from the text to the diagram. While the modification allows for automatically assessing visual \geofeat{} perception of a model, the perception of the vision encoder, an essential component in PGPS systems, is difficult to estimate directly. In response, we introduce a benchmark designed specifically to evaluate how well the vision encoder perceives visual \geofeat{}s, and utilize it to assess the existing vision encoders.

% Several studies have introduced benchmarks for PGPS, accompanied by training data that includes diagrams, textual description, numerical measurements, and solution steps written in formal language~\citep{geos,geoqa,unigeo,intergps,pgps}. Here, numerical measurements represent actual values such as the degree between two lines or the length of a line. The numerical measurements may not be available from the diagram in some cases. The goal of PGPS is to produce a valid solution as an executable program based on the diagram and the textual description. \dw{To answer successfully, a model needs to reason based on retrieve \geofeat{}s from the diagram and the text.} 
% \sout{The prediction implies retrieving \geofeat{}s from the diagram and the text and reasoning based on the retrieved \geofeat{}s.} 
% The solution program can be executed with the numerical measurements, and the final answer can be obtained from the execution result.
% Formally, given a dataset \(\{(D_i, X_i, N_i, S_i)\}_{i}\) where $D_i, X_i, N_i, S_i$ are the diagram, text, numerical measurements, and solution programs to the $i$-th problem, the PGPS model is learned to predict the solution program $S_i = [s_i^{(i)}, \dots, s_i^{\text{len}(S_i)}]$ from the diagram $D_i$ and the text $X_i$, which can be later executed with the numerical measurements $N_i$.
% \sout{Although the datasets and benchmarks facilitate PGPS research, they do not confirm that trained models genuinely attend to the diagram, because the input to the models contains all diagram-related details, e.g., visually recognizable \geofeat{}s and the numerical values.}

% \dw{Although the datasets and benchmarks make PGPS research accessible for many researchers, the role of diagrams and the geometric feature recognition ability of PGPS models remains unclear until recently, as many problems can be solved solely by using the text and measurements without accessing diagrams. \citet{} categorize the problems into different types including text intensive, vision intensive, etc, and show some empirical evidence that the current vision models cannot recognize geometric features correctly. }
% and the numerical values are provided separately.

% To jointly evaluate the perception of visual \geofeat{} and reasoning capabilities of a model, MathVerse~\citep{mathverse} extends existing PGPS benchmarks by transferring the visual \geofeat{}s from the text to the diagram. While the modification allows for automatically assessing visual \geofeat{} perception of a model, the perception of the vision encoder, an essential component in PGPS systems, is difficult to estimate directly. In response, we introduce a benchmark designed specifically to evaluate how well the vision encoder perceives visual \geofeat{}s, and utilize it to assess the existing vision encoders.

\subsection{Program generation based PGPS}

A core challenge in program generation-based PGPS is processing both diagrams and text to interpret \geofeat{}. One approach tackles the challenge by converting a diagram into alternative representations such as lists of geometric primitives and relations that can be represented as text~\citep{geos,geos-plus,intergps, GOLD, pgdp, geodrl}. Although reducing the problem to a single modality can be effective, building such converters typically requires labeled diagrams, which are expensive to collect and eventually limit generalization across diverse diagram styles.

Another line of research typically employs vision-language models (VLMs), where a VLM comprises a vision encoder and a language model~\citep{pgps,geoqa,geoqa-plus,scagps,unigeo,unimath,geox,LANS}. 
The vision encoder produces a visual embedding from the diagram, and the language model then generates solution steps in an autoregressive manner, conditioned on the textual description and the visual embedding.
% \begin{equation}
% s^{(i)} = \mathrm{LanguageModel}\bigl(h_D, X, s^{(0:i-1)}\bigr),
% \label{eq:vlm}
% \end{equation}
% where \(s^{(1:i-1)}\) refers to previously generated tokens and \(s^{(0)}\) is a pre-defined start token.
While the VLMs apply to various diagram formats, the visual \geofeat{} perception of the VLMs remains underexplored due to the abundance of textual information in existing benchmarks. 
Moreover, the VLMs are often fine-tuned and tested on a single benchmark, leaving their domain generalization capabilities across different diagram styles unexamined. 
% In this paper, we conduct analysis on both visual \geofeat{} perception and generalization capabilities of the PGPS models by utilizing the existing benchmark constructed with muliple domains, i.e., MathVerse, and the newly proposed visual \geofeat{} perception benchmark.

\subsection{Contrastive learning in PGPS}

Contrastive learning is applied in diverse domains such as computer vision~\citep{facenet} and natural language processing~\citep{simcse}. 
% Especially, contrastive language-image pre-training (CLIP) is widely used in vision tasks to align the embeddings of a vision encoder and a text encoder by utilizing image-caption pairs.
% To be specific, given a collection of image–caption pairs \(\mathcal{D} := \{(D_i, X_i)\}_{i=1}^N\), we train the vision encoder \(g\) and the text encoder \(h\) using the following contrastive learning objective:
% \begin{equation}
%     \mathcal{L}_{\textrm{CLIP}}(\mathcal{D}, g, h) := \mathbb{E}_{\mathcal{D}} \!\biggl[ -\log \frac{\exp \bigl( g(D_i)^T \, h(X_i) / \tau \bigr)}{\sum_{X \in \{X_i\}_i} \exp \bigl( g(D_i)^T \, h(X) / \tau \bigr)} \biggr],
%     \label{eq:clip}
% \end{equation}
% where \(\tau\) is a temperature parameter.
% The vision encoder and text encoder are often used for zero-shot classification, i.e., finding relevant premises in the image with text labels~\citep{clip}.
In the context of PGPS, contrastive learning is employed to address domain-specific challenges. GeoX~\citep{geox} applies contrastive learning to the adapter layer of the VLM to enhance formal language comprehension. Other approaches train the vision encoder itself using the contrastive language-image pre-training (CLIP)~\citep{clip} objective: LANS~\citep{LANS} aligns patch embeddings from a vision Transformer (ViT) with text token embeddings if they describe the same point, and MAVIS~\citep{mavis} employs diagram–caption pairs generated by a synthetic engine for CLIP.
In this work, we examine how CLIP with varied caption styles influences the visual \geofeat{} recognition of the vision encoder. In addition, a contrastive learning framework is introduced to strengthen robustness against domain shifts in the styles of diagrams.

% \subsection{Domain adaptation of CLIP}

% CLIP shows impressive performance on zero-shot classification with text labels~\citep{}.
% Several works attempt to preserve the zero-shot classification performance of CLIP pre-trained model in different domains.
% CDTrans: pseudo-labeling and special Transformer architecture~\citep{cdtrans}. 
% PADCLIP: pseudo-labeing and catastrophic forgetting measurment.~\citep{padclip}. 
% ReCLIP: source-free, pseudo-labeling and projection layer for removing class-agnostic features~\citep{reclip}. 
% CDCL: source-free, pseudo-labeling~\citep{cdcl}. 
% => pre-defined sets of class texts.
% S-CLIP: optimal transport + combination of existing captions for unlabeled data~\citep{sclip}.
% Instead, we label unlabeled data with an image, not captions by utilizing synthetic data engine.
