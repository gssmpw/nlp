\section{\geovlm{}}
\label{sec:vlm_details}

\subsection{Modification of training data}

Our fine-tuning strategy differs slightly from previous works~\citep{unigeo,pgps,geox}. 
Here, we clarify the difference between our approach and previous approaches. In previous works, the VLM is trained to produce the solution program given diagram and problem description as shown in \cref{fig:pgps_examples}. An interesting observation from GeoQA and PGPS9K datasets is that the numerical measurements, such as angles, lengths, and volumes, are not written in the problem description but given as additional conditions, and the numerals are substituted as a variable in the problem description as shown in \cref{fig:geoqa_example}. Therefore, the VLM only needs to produce the solution program without having optical character recognition (OCR) from the diagram. The variables are automatically substituted by the actual numbers when the program is executed. Therefore, the vision encoders do not need to learn OCR from the image.

However, this approach cannot be generalized to a wider class of problems where the numerals are embedded in the diagram instead of written in the problem description. Some variants of MathVerse, such as the vision-dominant problems, fall into this category as well. To incorporate OCR into the solution of the problem, we modify some problem statements in the training set, such that the numerical measurements are only shown in the diagram and not in the statements. We further modify the solution problem so that the solution contains OCR results as a part of the final output. Finally, we unify the language of the solution programs used in GeoQA and PGPS9K by converting GeoQA programs into PGPS9K format. The unification makes the output of VLM consistent since both datasets use different types of formal languages.

\cref{fig:training_data} shows examples of the modified input pairs and solutions, where the first problem statement does not have numerical measurements and the OCR results are in the part of the output solution program.

% \paragraph{Solution program and numerical values prediction.}
% \cref{fig:training_data} reveals the training data for GeoDANO.

\input{latex/figures/training_data}

% \subsection{Architecture}
% \label{sec:vlm_arch}

% We begin by summarizing the architecture of our VLM, a combination of a vision encoder and a language model. For the vision encoder, we use \geoclip{}-DA, with a two-layer MLP of GeLU activation as the projection layers following LLaVA-OneVision~\citep{llava-next}. For the language model, we employ LLama-3-8B-Instruct~\citep{llama}.
% For a given diagram and question pair in PGPS, we feed the vision encoder with the given diagram, and then the output of the encoder is used as an input token of LLM through the projection layer. The question text is then fed into the LLM, followed by the diagram embedding.



% \subsection{Fine-tuning strategy.}
% \label{sec:vlm_details}

% \paragraph{Modification process.}
% Our fine-tuning strategy differs slightly from previous works~\citep{unigeo,pgps,geox}. 
% Here, we clarify the difference between our approach and previous approaches. In previous works, the VLM is trained to produce the solution program given diagram and problem description as shown in \cref{fig:pgps_examples}. An interesting observation from GeoQA and PGPS9K datasets is that the numerical measurements, such as angles, lengths, and volumes, are not written in the problem description but given as additional conditions, and the numerals are substituted as a variable in the problem description as shown in \cref{fig:geoqa_example}. Therefore, the VLM only needs to produce the solution program without having optical character recognition (OCR) from the diagram. The variables are automatically substituted by the actual numbers when the program is executed. Therefore, the vision encoders do not need to learn OCR from the image.

% However, this approach cannot be generalized to a wider class of problems where the numerals are embedded in the diagram instead of written in the problem description. Some variants of MathVerse, such as the vision-dominant problems, fall into this category as well. To incorporate OCR into the solution of the problem, we modify some problem statements in the training set, such that the numerical measurements are only shown in the diagram and not in the statements. We further modify the solution problem so that the solution contains OCR results as a part of the final output. Finally, we unify the language of the solution programs used in GeoQA and PGPS9K by converting GeoQA programs into PGPS9K format. The unification makes the output of VLM consistent since both datasets use different types of formal languages.

% \cref{fig:training_data} shows examples of the modified input pairs and solutions, where the first problem statement does not have numerical measurements and the OCR results are in the part of the output solution program.

\subsection{Training details}
We begin by summarizing the architecture of our VLM, a combination of a vision encoder and a language model. For the vision encoder, we use \geoclip{}-DA, with a two-layer MLP of GeLU activation as the projection layers following LLaVA-OneVision~\citep{llava-next}. For the language model, we employ LLama-3-8B-Instruct~\citep{llama}.
For a given diagram and question pair in PGPS, we feed the vision encoder with the given diagram, and then the output of the encoder is used as an input token of LLM through the projection layer. The question text is then fed into the LLM, followed by the diagram embedding.

With the modified training data, we apply supervised fine-tuning on the VLM, i.e., the gradient only flows through the prediction of numerical values and solution steps, not the diagram and text.

We train the VLM with AdamW optimizer~\citep{adamw} and cosine annealing scheduler with warmp up ratio 0.03 and maximum learning rate $1\text{e-}5$.
We use LoRA~\citep{lora} with rank 128.
We set the batch size to 16 and train with 5 epochs.
We train the VLM with four A100-80GB GPUs for approximately 24 hours.
% \paragraph{Hyper-parameters}

% We train the VLM with AdamW optimizer~\citep{adamw} and cosine annealing scheduler with warmp up ratio 0.03 and maximum learning rate $1\text{e-}5$.
% We use LoRA~\citep{lora} with rank 128.
% We set the batch size to 16 and train with 5 epochs.

% \subsection{Prediction results}

% \input{latex/figures/tsne}

% We provide quantitative analysis of GeoDANO on MathVerse.