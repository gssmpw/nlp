\section{Details of Benchmark}

\subsection{Training details}

\label{sec:hparams}

% \paragraph{Linear probing.}
To evaluate the visual feature perception of the vision encoder, we utilize a linear probing approach, which involves freezing the vision
encoder parameters and training a simple linear classifier on top of its features.

We train the linear classifier on the training set of each task for 50 epochs with batch size 128 and learning rate $1\text{e-}4$.
We use Adam optimizer for optimization.

\subsection{Visualization of the vision encoders}

We visualize the embeddings of the vision encoders used in \cref{sec:benchmakr_results} at \cref{fig:tsne_benchmark}.

\input{latex/figures/tsne_benchmark}


% \paragraph{GeoCLIP.}
% We start from OpenCLIP~\citep{clip}, a pre-trained model where the architecture is ViT-L/14 with image resolution $336\times 336$. To train OpenCLIP with GeoCLIP, we use total of 200,000 diagram-caption pairs generated with our synthetic data engine.
% We set the batch size and weight decay to 256 and 0.2, respectively.
% We optimize for 50 epochs using Adam optimizer~\citep{adam} and a cosine annealing scheduler with 2,000 warmup steps and the maximum learning rate is set to be $1\text{e-}4$.
% For the domain adaptation parts, i.e., applying CLIP on the diagram-caption pairs and the diagram pairs of target domains, we vary the batch size to 32.
