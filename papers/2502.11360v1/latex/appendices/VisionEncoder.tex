\section{GeoCLIP-DA}

\subsection{Domain adaptation data}

We adopt GeoCLIP to the two PGPS benchmarks: GeoQA~\citep{geoqa} and PGPS9K~\citep{pgps}.
For PGPS9K, we use the Geometry3K split.
\cref{fig:domain_adaptation_samples} shows the pairs used to adapt the domain of GeoCLIP.

\input{latex/figures/domain_adaptation_samples}

\subsection{Training details}

We start from OpenCLIP~\citep{clip}, a pre-trained model where the architecture is ViT-L/14 with image resolution $336\times 336$. To train OpenCLIP, we use total of 200,000 diagram-caption pairs generated with our synthetic data engine.
For the domain adaptation to GeoQA and Geometry3K datasets, we randomly sample 50 diagrams and translate the diagram and caption styles following the procedure described in \cref{sec:domain_adaptation}. Finally, \geoclip{} is fine-tuned via \cref{eqn:da}.
We name the GeoQA and Geometry3K adopted \geoclip{} as \geoclip{}-DA.

We set the batch size for the source domain diagram-caption pairs to 256. 
For the domain adaptation parts, i.e., applying CLIP on the diagram-caption pairs and the diagram pairs of target domains, we vary the batch size to 32.
We set weight decay to 0.2.
We optimize for 50 epochs using Adam optimizer~\citep{adam} and a cosine annealing scheduler with 2,000 warmup steps and the maximum learning rate is set to be $1\text{e-}4$.
We train the model with eight RTX3090 GPUs for approximately 24 hours.
