\section{Mechanistic Analysis\label{sec:mechanism}}

In the previous sections, we introduced preliminary studies regarding the generalization pitfalls of the moral foundations classification task (Section~\ref{sec:preliminary}), and the performance of fine-tuning LLMs for two moral reasoning tasks (Section~\ref{sec:moraltuning}).
In this section, we: 
%(1) reveal the generalization pattern of next-token prediction in the context of RoT generation and ethical judgment prediction; 
%(2) showcase the pragmatic dilemma necessitates distributionally similar training samples exist therefore LLMs can generalize to a test sample, but those training samples indicate different moral foundations; 
%(3) empirically indicate that this pragmatic dilemma can be alleviated by introducing moral foundation for RoT generation and ethical judgment prediction.
(1) propose the Representational Likelihood Algorithm (RLA) which can uncover supportive training samples for a given test sample;
(2) explore the characteristics of supportive training samples, demonstrating that the introduction of additional information to enhance generalization aligns with the generalization mechanism of the semantics-level task;
(3) showcase that the \textit{pragmatic dilemma} still holds even though fine-tuned LLMs perform better in RoT generation and ethical judgment prediction.

\textbf{Motivation.}
Our study builds on the representational learning nature of LLMs and the widely accepted principle in generalization theory that a well-trained machine learning model can generalize effectively when the training and test set distributions are closely aligned in the feature space~\cite{zhou2022domain,hupkes2022state}.
Since neural language models capture distributional semantics, representational similarity can be interpreted as equivalent to distributional similarity.
%For LLMs, the feature space is the representation space, our goal is to find training samples that are close to a given sample in the representation space. 
%However, the representation of a given sample might be biased and the bias level is determined by to what extent the fine-tuned LLM has fitted to it.
%Therefore we take the representation of and the likelihood of the given sample together to determine how close two samples are.
Recall from Section~\ref{sec:preliminary} that we highlighted the generalization pitfalls of the moral foundation classification task. We argue that similar pitfalls should also exist for RoT generation and ethical judgment prediction.
Our hypothesis is that \textit{for a given test sample, the LLM can generalize effectively only if highly similar training samples have been adequately learned during fine-tuning}. To test this hypothesis, we propose a novel algorithm to identify the training samples most conducive to the generalization of a given test sample within the representation space.
%Please refer to Section~\ref{subsec:hypo} for more details.
\subsection{Representational Likelihood Algorithm\label{subsec:hypo}}
In this section, we present our novel method for identifying training samples that contribute to the prediction of a given test sample. We refer to these training samples as \textit{generalization-supportive samples}\footnote{In this paper, we use generalization-supportive and supportive interchangeably.}. Our goal is to correlate representational similarity with LLM predictions, and then leverage this correlation to characterize the generalization mechanism of the considered morality acquisition tasks.
%Our goal is to correlate representational similarity with LLM predictions, which would allow us to link representational similarity and generalization, and then leverage this correlation to characterize the generalization mechanism of considered morality acquisition tasks.
%We firstly propose a simulation task to demonstrate the correlation exists, then leverage this correlation to analyze how the generalization is achieved in our considered pragmatic-level tasks.

Assume that a fine-tuned LLM $f_{\theta}$ has been trained on the training set $\mathcal{D}_{\text{train}}$, where each sample is represented as $x = [x_s,y_m,y_r,y_j]$,
following the annotation introduced in Section~\ref{subsec:learning-paradigm}.
We denote training samples as $x \sim \mathcal{D}_{\text{train}}$ and test samples as $x' \sim \mathcal{D}_{\text{test}}$.
The hidden states of $f_{\theta}$ are denoted by $\mathcal{H}_{\theta}(\cdot)$, and the conditional likelihood of a given input and output is represented as $\mathcal{P}_{\theta}(\cdot|\cdot)$.
Denote the cosine similarity function as $\text{cos}(\cdot)$.
\begin{algorithm}
\caption{RLA for Judgment Prediction}
\label{alg:simulation}
\begin{algorithmic}[1] % The [1] enables line numbering
    %\STATE \textbf{Input:} Data $X$, parameter $\theta$
    %\STATE \textbf{Output:} Processed data $Y$
    %\STATE \textbf{cosine similarity}: $\text{cos(\cdot)}$
    \STATE Initialize $r = 0$, $\mathbf{d}=\{\}$
    \FOR{each sample $x'$ in $\mathcal{D}_{\text{test}}$}
        \STATE Sampling $\mathcal{N}$ cases from $\mathcal{D}_{\text{train}}$ as\\ $\mathcal{X}=[x^1,x^2,\cdots,x^{\mathcal{N}}]$\\
        \FOR{each $x^t$ in $\mathcal{X}$} 
            \STATE $S^t$ = $ \overbrace{\text{cos}(\mathcal{H}_{\theta}(x_s^t),\mathcal{H}_{\theta}(x_s^{'}))}^{\text{\textbf{representational similarity}}}$ $\cdot$ $\overbrace{\mathcal{P}_{\theta}(x^t_{j}|x^{t}_{s})}^{\textbf{likelihood}}$
            % $\mathcal{P}_{\theta}(x^t_{j}|x^{'}_{s})$
            \STATE $\mathbf{d}[S^t]=\underbrace{\mathcal{P}_{\theta}(x^t_{j}|x^{'}_{s})}_{\text{\textbf{prediction}}}$
        \ENDFOR
        \STATE Sort $\mathbf{d}$ by key in \textit{ascending} order, return the value list as $\mathcal{V}$
        \IF{\text{MEAN}($\mathcal{V}$$[:\frac{\mathcal{N}}{2}]$) $<$ \text{MEAN}($\mathcal{V}[\frac{\mathcal{N}}{2}:]$) }
            \STATE $r$++ 
       
        \ENDIF
    \ENDFOR
    \STATE \textbf{return} $\frac{r}{\#\mathcal{D}_{\text{test}}}$
\end{algorithmic}
\end{algorithm}
Algorithm~\ref{alg:simulation} presents our proposed Representational Likelihood Algorithm (RLA) by taking the \texttt{judg} fine-tuning strategy ($y_j=f_{\theta}(x_s)$) as an instance. 
Specifically,\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item[1.] For each test case, we randomly sample $\mathcal{N}$ samples $\mathcal{X}$ from the training set (line 3).
    \item[2.] For each training sample $x^t$ in the sampled set $\mathcal{X}$, we calculate the \textbf{similarity score $S^t$} which comprises the: (1) cosine similarity between two hidden states $\mathcal{H}_{\theta}(x_s^t)$ and $\mathcal{H}_{\theta}(x_s^{'})$ (line 5) measuring the representational similarity, and (2) likelihood, the conditional probability $\mathcal{P}_{\theta}(x^t_{j}|x^{t}_{s})$ measuring how good $f_{\theta}$ fits $x^{t}$ (line 5). With this design, only those training samples that have been fitted well by $f_{\theta}$ would be considered in the process of measuring representational similarity.
    \item[3.] Compute the conditional probability of the training sample’s judgment given the test case’s situation (line 6).
    \item[4.] If $f_{\theta}$ becomes increasingly likely to assign $x_t$'s judgment $x^t_j$ to $x'_s$ as their representational similarity increases, then we can correlate representational similarity and prediction (lines 8-10).
\end{itemize}

In our experiments, we utilize the hidden states from the ${15}^{th}$ layer onward of the final token as the representation and compute the average cosine similarity across these layers to obtain the representational similarity score. This is because previous studies~\cite{geva2023dissecting,liu-etal-2024-intrinsic} indicate that the LLMs considered in this paper generally exhibit differences in the hidden state space from the ${15}^{th}$ layer onward.
\begin{table}[ht]
\small
    \centering
    \begin{tabular}{c c c}
        \toprule
        &Mistral & Llama3 \\ 
        \midrule
        Socialchem-rot&.920 & .924 \\ 
        \midrule
        Socialchem-judg& .998 & .996 \\ 
        \midrule
       MIC-rot& .926 & .912 \\ 
        \midrule
        MIC-judg& .990 & .971 \\ 
        \bottomrule
    \end{tabular}
    \caption{\small Experimental results for the simulation task show that all values exceed 0.9, indicating a strong correlation between representational similarity and prediction.}
    \label{tab:distri_hypo}
\end{table}
Table~\ref{tab:distri_hypo} presents the results of two baseline fine-tuning strategies, \texttt{rot} and \texttt{judg}, evaluated across various benchmarks and LLM models.
As shown, all experimental results exceed 0.9, particularly the \texttt{judg} fine-tuning strategy which is very close to 1.0, demonstrating that there exists correlation between representational similarity and prediction. 
In other words, for a given test sample, generalization-supportive training samples can be identified by assessing their representational similarity.
\subsection{Interpretation of Generalization}

Building on the method for identifying generalization-supportive training samples from Section~\ref{subsec:hypo}, this section interprets the generalization mechanism of the examined morality-relevant tasks by analyzing the characteristics of these supportive training samples\footnote{In this section, we provide a detailed analysis only for the fine-tuned Mistral, while the analysis for the fine-tuned Llama3 is presented in Appendix~\ref{appendix:mech_llama3}.}.

For each test sample, we collect the top-10 generalization-supportive training samples with the most highest similarity score $S^t$.
However, the similarity score $S^t$ is a high-level metric capturing the statistical correlation between representational similarity and predictions, making it insufficient for directly interpreting the underlying reasons for performance gains.
To have an in-depth analysis, we investigate (i) the cosine similarity of hidden states between the test sample's moral situation and the training sample's moral situation; (ii) the BertScore between the train sample's situation and the test sample's situation. 
%Since BertScore is derived from a BERT model, it reflects distributional similarity.
Figure~\ref{fig:mech_mistral} present these two analytical perspectives on the top 10 generalization-supportive training samples for the fine-tuned Mistral model across two benchmarks.

By zooming into the left four subfigures in Figure~\ref{fig:mech_mistral}, introducing moral foundation or RoT in the fine-tuning process can decrease the representational similarity, particularly the optimal fine-tuning strategies, e.g., \texttt{moral-rot} and \texttt{moral-judg}, lead to lower representational similarities than that of the baseline strategy (\texttt{rot} and \texttt{judg}).
This phenomenon aligns with our hypothesis that \textit{generalization in moral reasoning acquisition tasks requires a high degree of representational similarity between test and training samples}.

By referring to the curve of SST that also faces a lower representational similarity, we can conclude that additional information of moral foundation or RoT would alleviate the generalization pitfall of the baseline strategy that necessitates much similar training samples to generalize.
This is rather natural since those fine-tuning strategies not only capture the information of situations but also moral foundations and/or RoTs, newly introduced information would impact the characteristics of the representation space.

Additionally, we can observe decreased BertScore in the right four sub-figures, except for RoT generation in the MIC benchmark, where the BertScore for \texttt{moral-rot} remains close to that of the baseline \texttt{rot} strategy.
A decrease in BertScore suggests that the additional information reduces reliance on generalization-supportive training samples with high distributional similarity to the test sample.
Due to the association between distributional similarity and representational similarity in LLMs, those two observations are aligned.
\begin{figure}[t]
    \centering
    \subfloat{\includegraphics[width=0.99\linewidth]{mistral.socialchem.rot.regular.png}}
    \hfill
    \subfloat{\includegraphics[width=0.99\linewidth]{mistral.socialchem.judgement.regular.png}}
    \hfill
    \subfloat{\includegraphics[width=0.99\linewidth]{mistral.mic.rot.regular.png}}
    \hfill
    \subfloat{\includegraphics[width=0.99\linewidth]{mistral.mic.judgement.regular.png}}
    \hfill
    \caption{\small Top-10 generalization-supportive training samples analysis for fine-tuned Mistral with the SocialChem (upper two rows) and MIC (bottom two rows) benchmark. %Two analytical perspectives of representational similarity and BertScore between the given test sample's situation and the generalization-supportive training sample's situation are presented.
    }
    \label{fig:mech_mistral}
\end{figure}
It is not surprising that the performance gain arises from the generalization mechanism analogical to that of semantics-level tasks. 
A natural question is~\textit{does the incorporation of moral foundations or RoT alleviate the pragmatic dilemma of current learning paradigms in moral reasoning acquisition?}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mistral.socialchem.rot.same_moral.png}
        %\caption{Caption 1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mistral.socialchem.judgement.same_moral.png}
        %\caption{Caption 2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mistral.mic.rot.same_moral.png}
        %\caption{Caption 1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mistral.mic.judgement.same_moral.png}
        %\caption{Caption 2}
    \end{minipage}
    \hfill
    \caption{Ratio of generalization-supportive training situations with the same underlying moral foudation as the test situation. Upper two subfigures are for SocialChem and the bottom sub-figures are for MIC.}
    \label{fig:same_moral_mistral}
\end{figure}
%A natural question to this alleviation is \textit{to what extent it alleviate the pragmatic dilemma?}
An extreme case for the vanishment of the pragmatic dilemma is: \textit{for a given test situation, top-10 generalization-supportive training moral situations should have the same underlying moral foundations as the test moral situation}.
Therefore, we compute the ratio of the top-10 supportive training moral situations that share the same moral foundations as the test moral situation. 
Notably, we take the term training/test moral situation, for MIC and SocialChem, instead of training/test samples to emphasize that our analysis exactly focuses on moral situations.
For reference, we include SST and consider the sentiment label when calculating the ratio for SST.

Figure~\ref{fig:same_moral_mistral} presents the results for this ratio.
Interestingly, even for SST, which can be viewed as a binary classification task, only half of the supportive training samples share the same sentiment label as their corresponding test samples.
For both RoT generation and ethical judgment prediction, the optimal fine-tuning strategies (\texttt{moral-rot} and \texttt{moral-judg}) align with the baseline fine-tuning strategies (\texttt{rot} and \texttt{judg}), except for \texttt{moral-judg} on the SocialChem benchmark.
We believe this exception arises because the textual length of moral situations in SocialChem is relatively short, amplifying the influence of ethical judgment during fine-tuning.
On the other hand, we calculate the average conditional likelihoods of the top-10 supportive training situations, and note the optimal fine-tuning strategy does help LLMs fit training samples. These observations suggest that LLMs consider moral situations and additional information together to generalize, but still operate primarily within the realm of semantics.
\begin{table}[ht]
\small
\centering
\begin{tabular}{c c c }
\toprule
&\texttt{SocialChem} &\texttt{MIC}   \\ 
        \midrule
        rot&.389 & .659 \\ 
        moral-rot& .418 & .738 \\ 
        \midrule
        judg&.992 & .770\\ 
        moral-judg& .997 & .835  \\
        \bottomrule
\end{tabular}
\caption{\small The average conditional likelihoods of top-10 generalization-supportive training samples.}
\label{tab:likelihoods}
\end{table}

Recall that, in Section~\ref{sec:preliminary}, we demonstrate that the generalization and convergence behavior of moral foundation classification is different from SST due to the pragmatic delimma. 
Similarly, we also argue that the pragmatic nature of morality would be more negative to the language modeling capability of LLMs than that from SST.
Figure~\ref{fig:perplexisity} presents the perplexity evaluation results, acquired through the OpenWebText datset~\cite{Gokaslan2019OpenWeb}, of Mistral models fine-tuned with different strategies.
It is obvious that morality-relevant tasks introduce more perplexity than SST.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{Mistral.LM.png}
    \caption{\small Perplexity for Mistral. Baseline indicates the Perplexity of the LLMs without any fine-tuning.}
    \label{fig:perplexisity}
\end{figure}

In summary, \textit{while the optimal fine-tuning strategies improve performance on both tasks, this improvement remains within the realm of distributional semantics, and the pragmatic dilemma persists}.

