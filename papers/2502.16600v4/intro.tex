\section{Introduction and Related Works}
Given the widespread usage of LLMs across all facets of society, enabling such models with moral reasoning capabilities has become a significant research goal. Though AI alignment~\cite{bai2022training} has become the de-facto method to align LLMs with human values, its effectiveness has been debated~\cite{lin2023unlocking,qi2024safety}.
One significant complaint is that alignment with human preference does not allow LLMs to achieve intrinsic alignment, resulting in various safety issues, e.g., jailbreak attacks~\cite{xie2023defending} and propagation of social biases to downstream tasks~\cite{liu-etal-2024-intrinsic}.
%To investigate the ethical values embedded in LLMs, previous studies have designed cognition-grounding question-answering and sentence completion tasks.
%\citet{bonagiri2024sage} shows that model performance and moral consistency are independent among LLMs.
%\citet{abdulhai2023moral} explores whether LLMs has a bias towards a typical dimension of moral principles.
%A statistical method is proposed by~\citet{scherrer2024evaluating} to evaluate the encoded moral values in LLMs.
%~\citet{zhang2023measuring} proposes a metric to examine if LLMs understand ethical values with both “know what” and “know why”. 
%Those studies all demonstrate that LLMs do not have consistent moral or ethical orientations given various instances, this is against the moral consistency principle~\cite{arvanitis2020consistency}.
%Besides those efforts for evaluating ethical values in LLMs, there are also new benchmarks~\cite{forbes2020social,hendrycks2020aligning,ren-etal-2024-valuebench}.
However, enabling LLMs to develop moral reasoning capabilities is a non-trivial task; it is both a pragmatics-level task~\cite{awad2022computational}, as well as philosophically challenging, due to debate over the correct representation of human morals and ethics~\cite{zhixuan2024preferencesaialignment}.
%Whether LLMs can acquire morality and perform moral reasoning remains a topic of debate.
%\footnote{In this paper, we interchange \textit{morality} and \textit{ethics}, as they are philosophically similar for our purposes.}

\citet{jiang2021can} and~\citet{hendrycks2020aligning} represent pioneering efforts to enable LLMs to acquire ethical judgment capabilities by fine-tuning them on curated textual data that jointly depicts various moral situations alongside corresponding judgments.
\citet{zhou2024rethinking} introduces an in-context learning method to help LLMs perform moral reasoning, based on a top-down framework driven by the Moral Foundation Theory~\cite{anderson2007machine}.
~\citet{liutraining} introduce a social sandbox wherein LLMs can learn how to be moral through interactions.
~\citet{tennant2024moral} propose a moral alignment framework to make LLM agents behave morally through a newly designed intrinsic moral reward function based on the Iterated Prisoner's Dilemma\footnote{\url{https://en.wikipedia.org/wiki/Prisoner\%27s_dilemma}}.
In addition to those efforts proposing solutions, new benchmarks have also been proposed~\cite{forbes2020social,hendrycks2020aligning,ren-etal-2024-valuebench}.

%On the other hand, there are some studies which highlight the inefficiency of LLMs in morality-relevant tasks. 
There are also several studies which highlight the inefficiency of LLMs on tasks requiring moral reasoning. 
\citet{talat2022machine} has criticized the \citet{jiang2021can} work described above, because while their intended goal was normative ethics, they instead leveraged a bottom-up approach for learning descriptive ethics~\cite{vida-etal-2023-values,fraser2022does}.
\citet{jin2022make} empirically demonstrate that the current learning paradigm for moral reasoning tasks relies on a large training dataset. \citet{sap2022neural} also show the failure of LLMs on social intelligence tasks such as theory-of-mind.

In cognitive science, \citet{mahowald2024dissociating} suggest that while LLMs excel in formal language competence, they struggle with functional language competence—an essential requirement for acquiring moral reasoning capabilities.
More fundamentally,~\citet{bender2020climbing} and other studies in BERTology~\cite{rogers2021primer}, argue that Transformers cannot achieve true language acquisition, as it necessitates physical grounding and situated communicative intent~\cite{beuls2024humans}, which extends beyond the distributional semantics captured by Transformers~\cite{harris1954distributional,lenci2008distributional,boleda2020distributional}. 
Previous studies~\cite{bonagiri2024sage,zhang2023measuring} demonstrate that LLMs do not have consistent moral or ethical orientations across various instances, which is contrary to the moral consistency principle~\cite{arvanitis2020consistency}.
Appendix~\ref{sec:relatedworks} contains additional related works and motivation pertaining to machine ethics.
%which further demonstrate the inefficiency of current learning paradigms for moral reasoning acquisition in LLMs.
%However, there are some more recent studies shows that every large LLMs such as GPT-4 achieve good performance in moral reasoning~\cite{zhang2023measuring} and theory-of-mind~\cite{kosinski2024evaluating}, \textit{showing contradictory conclusions to that of previous studies and supporting LLMs in pragmatics-level tasks}.

To address this debate, in this paper we pursue a deeper understanding of the mechanisms underlying current learning paradigms for moral reasoning acquisition. We argue that while existing paradigms can improve LLMs’ performance on morality-related tasks, this enhancement: (1) primarily arises from distributional similarities between seen and unseen ethical situations, and (2) faces challenges in generalization due to the inherently pragmatic nature of morality.
We name this phenomenon as the \textit{pragmatic dilemma}~\cite{laverick2010ethical,sap2022neural} of moral reasoning acquisition, which arises from the inherent gap between the nature of distributional semantics in LLMs and the pragmatic nature of morality. Significant consequences of this pragmatic dilemma include poor generalization and a lack of intrinsic alignment.

Specifically, we employ three fundamental tasks, Moral Foundations classification, rule of thumb generation, and ethical judgment prediction, as downstream evaluations of moral reasoning acquisition. We then compare their generalization characteristics with a representative semantics-driven task, sentiment analysis.
Motivated by the distributional semantics hypothesis, we:
%we characterize generalization with the following steps: 
%(1) propose a analysis method bridging the moral decision and representational similarity;
%(2) seek helpful seen situations in the fine-tuning dataset that contributes to a given unseen situation by referring to representation similarity and negative likelihood;
%(3) analyze several features of helpful seen situations, e.g., the underlying moral foundation and lexical similarity;
%(4) force LLMs fine-tuned through current learning paradigm to self-refine immoral situations to be moral.
(1) empirically show the generalization and convergence pitfalls of Moral Foundations classification; (2) propose the novel \textit{Representational Likelihood Algorithm (RLA)} to statistically correlate representational similarity between seen and unseen moral pragmatics with the prediction likelihood of unseen morals; and
(3) using RLA, perform mechanistic analysis of LLM performance gains for unseen situations.
%Based on our analysis, we conclude that the pragmatic dilemma blocks the effectiveness of current learning paradigms.
% propose a method to statistically correlate representational similarity between seen and unseen situations with the prediction of unseen morals

Section~\ref{sec:preliminary} introduces the prevalent learning paradigm for moral reasoning acquisition and highlights the generalization challenges in fine-tuning masked language models for moral foundation prediction. Section~\ref{sec:moraltuning} presents experimental results across different learning paradigms, and Section~\ref{sec:mechanism} provides a detailed mechanistic analysis. Based on our experimental results, we conclude that the pragmatic dilemma blocks the effectiveness of current learning paradigms.
%Although there is no universally agreed-upon definition of morality acquisition, in this paper, we propose that morality acquisition can be evaluated through three key tasks\footnote{Details of these tasks are provided in Section~\ref{subsec:learning-paradigm}}: moral foundation prediction, Rule-of-Thumbs (RoTs) generation, and judgment prediction.
%Our argument is motivated by key findings and theories for the representation learning nature of LLMs. \textbf{Firstly} previous studies claiming the apparent intelligence of LLMs merely reflects the statistical nature of language models, rather than true understanding~\cite{van2023large,fui2023generative,mitchell2023debate,liu2024intrinsic}; \textbf{Secondly} the foundation of LLMs lies in their ability to learn representations through self-supervised learning. This process is rooted in vector semantics~\cite{boleda2020distributional}, which are based on the distributional hypothesis proposed in the 1950s~\cite{Joos1950DescriptionOL,harris1954distributional}. 
%Prior research~\cite{li2021implicit} has demonstrated that transformer-based models effectively encode information about the lexical entities within the input. Additionally, the linear representation hypothesis~\cite{park2023linear} further supports the notion that these representations are inherently semantics-dependent.

%On the basis of representative open-sourced LLMs and benchmarks, we fine-tune LLMs with ethical benchmarks for three tasks: moral foundation prediction, RoTs generation and judgment prediction.
%Then we show how the learning process and representation are different from that of semantic-level tasks.

%Debates have emerged regarding whether LLMs possess human-like intelligence; some studies argue that their apparent intelligence merely reflects the statistical nature of language models, rather than true understanding~\cite{van2023large,fui2023generative}.
%In this paper, we aim to address two research questions \textbf{(1)}~\textit{can LLMs acquire ethical values through next-token prediction?} and \textbf{(2)}~\textit{If they cannot, what are the underlying reasons?}
%We explore the first question through the lenses of induction and deduction pattern in cognitive science, taking the categorization over the hierarchy of human ethics as a signal to understand if LLMs can understand human ethics as humans do.
%Given the evaluation results based on categorization, we can tell the reason that LLMs can not acquire ethical values