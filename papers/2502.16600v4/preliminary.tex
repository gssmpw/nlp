\section{Preliminary Background\label{sec:preliminary}}
In this section, we begin by introducing the benchmarks and dataset annotation used in our study. We then present the prevailing learning paradigm for moral reasoning acquisition. Finally, we use the Moral Foundations prediction task with a Masked Language Model, specifically BERT~\cite{devlin2019bert}, as a case study, to illustrate the generalization challenges of this task by drawing comparisons to the semantics-level task of sentiment analysis.
\subsection{Benchmark and Dataset Annotation\label{subsec:dataset}}
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{l}
        \toprule
        \textbf{Situation}: Reminding my coworker who crashed \\into my car to pay to get it repaired. \\
        \midrule
        \textbf{Moral Foundation}: Fairness.\\
        \midrule
        \textbf{Rule of Thumb (RoT)}: If you crash into someone's car,\\ you should pay for their repairs.\\
        \midrule
        \textbf{(Ethical) Judgment}: You should.\\
        \bottomrule
    \end{tabular}
    \caption{\small Dataset Annotation. Given a moral situation describing a morality-relevant case, the corresponding Moral Foundation, RoT, and Judgment are presented.}
    \label{tab:dataset}
\end{table}
We employ two popular benchmarks: MIC~\cite{ziems2022moral} and SocialChem~\cite{forbes2020social}.
Table~\ref{tab:dataset} presents an overview of the dataset annotations used across both benchmarks.
Given a moral situation, the \textit{Moral Foundation}~\cite{haidt2004,haidt2007morality} represents the underlying social norm that the situation either adheres to or violates (please refer to Table~\ref{appendix:MFs} for more details of Moral Foundation Theory). The \textit{RoT (Rule of Thumb)} encapsulates a fundamental explanation of right and wrong behavior, serving as a guidance for the subsequent ethical judgment. The \textit{(Ethical) Judgment} then determines whether the given situation is deemed acceptable or unacceptable.
While a single moral situation may be associated with multiple moral foundations, this study focuses exclusively on cases where only one underlying moral foundation is present.
In the MIC, each prompt-reply pair is treated as a distinct situation.


\subsection{Learning Paradigms\label{subsec:learning-paradigm}}
Existing learning paradigms for moral reasoning acquisition generally fine-tune LLMs on curated textual data that depicts various moral situations alongside corresponding judgments or actions. 
In previous studies, ethical judgment prediction and RoT generation are the most popular tasks~\cite{bonagiri2024sage,ren-etal-2024-valuebench,hendrycks2020aligning,sorensen2024value}, and Moral Foundations classification is widely accepted in the area of computational social science~\cite{johnson2018classification,roy2021identifying}. 
Though there is no agreed-upon definition for moral reasoning acquisition, we consider Moral Foundations classification, RoT generation, and ethical judgment prediction as three downstream tasks indicative of moral reasoning capabilities.
Although some studies incorporate interactive sandboxes or multi-round feedback into learning paradigms~\cite{liutraining,wang-etal-2024-sotopia}, Moral Foundations classification, RoT generation, and ethical judgment prediction remain fundamental tasks, which when fine-tuned with LLMs form the preferred learning paradigms. 

\textbf{Notations.} We denote the moral situation as $x_s$, the moral foundation as $y_m$, the RoT as $y_r$, and the judgment as $y_j$.
Assuming an LLM $f$ is parameterized by $\theta$, RoT generation is formulated as $y_r=f_{\theta}(x_s)$ and judgment prediction is represented as $y_j=f_{\theta}(x_s)$.

\textbf{Fine-tuning Strategies.} Current learning paradigms of moral reasoning acquisition which aim to maximize conditional probabilities $\mathcal{P}_{\theta}(y_r|x_s)$ and $\mathcal{P}_{\theta}(y_j|x_s)$, typically apply a self-supervised fine-tuning or a reinforcement learning loss objective\footnote{Please note the choice of objective loss function does not impact our conclusion.}.
Given the causal relationships among moral foundations, RoT, and judgment, previous studies often integrate them into a unified prediction task, such as $y_r=f_{\theta}(x_s, y_m)$ and $y_j=f_{\theta}(x_s,y_m,y_r)$.
During fine-tuning, the input for RoT generation can be $x_s$ with or without $y_m$, while the input for ethical judgment prediction can be $x_s$ with or without $y_m$ and/or $y_r$.
%In Section~\ref{sec:mechanism} we will introduce the performance difference among those different strategies and the mechanistic reason for the performance gain.


\subsection{Pitfalls of Generalization\label{subsec:pitfall-generalization}}
In this section, we use the Moral Foundations classification task as an example to illustrate its generalization pitfalls by comparing it to the semantics-level task of sentiment analysis.
We argue that \textit{in moral classification tasks, there should be serious generalization issues since the classification model has to map semantically different situations into the same moral foundation label}. 
A direct consequence is that an unseen situation is likely to be predicted correctly only if a semantically similar sample exists in the training set. This similarity requirement is much stricter than that for the sentiment analysis task.
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{l}
        \toprule
        \textbf{Situation}: Kicking a kid out of his birthday party. \\
        \midrule
        \textbf{Situation}: Not telling my mom I smoke weed.\\
        \bottomrule
    \end{tabular}
    \caption{\small Situation Examples. Two moral situations with the same underlying moral foundation of authority-subversion.}
    \label{tab:situation2moral}
\end{table}

Our argument is driven by the gap between the distributional semantics captured by neural language models and the inherently pragmatic nature of morality.
For instance, Table~\ref{tab:situation2moral} presents two moral situations from the SocialChem benchmark; they are semantically different (\textit{distributional semantics}) but the underlying moral foundations are identical (\textit{pragmatics}).
If we force an MLM to map these two situations into the same moral foundation label, it would violate the captured distributional semantics during pre-training.  
To illustrate how the violation works, we refer to a semantics-level task of sentiment analysis using the SST dataset from the GLUE benchmark~\cite{wang2018glue}.

\textbf{Experimental Settings for Classification.} 
We have two settings for the moral classification tasks: classify moral situations to moral foundations and classify RoTs to moral foundations.
We use a fine-tuning dataset with 7500 randomly sampled cases and the bert-base-uncased\footnote{\url{https://huggingface.co/google-bert/bert-base-uncased}} model as the backbone model.
Beyond the backbone model, we insert a fully-connected layer as the classifier layer. More details about the hyperparameters setting is available in Appendix~\ref{appendix:hyperparams4clf}.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{preliminary.png}
    \caption{\small Training and Development Accuracy Over 10 Fine-tuning Epochs. The first four figures display results for moral foundation classification tasks, while the rightmost figure shows the results for the SST benchmark.}
    \label{fig:clf}
\end{figure*}

\textbf{Observations for Classification Performance.} Table~\ref{fig:clf} presents the classification performance on both the training and development set. 
Compared to the generalization behavior observed in SST (rightmost figure), the moral foundation classification tasks (first four figures) exhibit a significant performance gap between the training set and the development set. 
However, for \texttt{MIC-RoT} and \texttt{SocialChem-RoT}, because the training accuracy approaches 100\% and converges after only several epochs, this suggests that \textit{task difficulty is not the primary cause of the observed generalization gap}.
The difference in classification performance between Situation and RoT stems from the fact that RoT is constructed based on typical moral foundations, inherently conveying information about the corresponding moral foundation. However, the generalization gap between the training set and development set for all moral foundation classification settings is apparent.

To further analyze the generalization pitfall in moral foundation classification, we examine the convergence behavior with respect to training dataset size. 
We use the curve of development accuracy in SST as a reference to highlight the convergence issue observed in moral foundation classification tasks.
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{generalizationpitfall.png}
    \caption{\small Convergence Curve of Development Accuracy for Considered Classification Tasks. Only the development accuracy of SST increases with more training samples and finally approaches 1.0.}
    \label{fig:convergence}
\end{figure}

\textbf{Experimental Settings for Convergence.} 
SST is a binary classification task. To ensure a fair comparison, we re-categorize the moral foundation labels for MIC and SocialChem to convert them into a binary classification task (details are in Appendix~\ref{appendix:redist_moral}).
For each task setting, we incrementally increase the training set size from 1,000 to 210,000 in steps of 2,000 and report the best performance on the development set at each training size setting.


\textbf{Observations for Convergence.} Figure~\ref{fig:convergence} illustrates the curve of development accuracy across all evaluated classification tasks. 
For SST, accuracy improves as the number of training samples increases, eventually stabilizing and approaching 1.0.
In contrast, the development accuracies for moral foundation classification tasks show no improvement in SocialChem and only marginal gains in MIC.
We believe this disparity is due to the fact that moral situations in SocialChem are generally shorter than that of MIC.
The convergence behavior analysis again showcases the generalization pitfalls of the moral foundation classification task.

In summary, we: (1) introduce the current learning paradigms for moral reasoning acquisition; and (2) show the generalization pitfalls of the moral foundation classification task (a pragmatics-level task) by referring and comparing to the development accuracy of a semantics-level task.