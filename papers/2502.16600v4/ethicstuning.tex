
\section{Fine-tuning for Moral Reasoning Acquisition\label{sec:moraltuning}}
In this section, we introduce fine-tuning strategies and experimental results of existing learning paradigms for moral reasoning acquisition, focusing on the tasks of RoT generation and ethical judgment prediction.
\begin{table*}[t]
\small
\centering
\begin{tabular}{c c c c c| c c c c c}
\toprule
\texttt{SocialChem} &\text{\small BertScore} & \text{\small Rouge1} & \text{\small Rouge2} & \text{\small RougeL} & \texttt{MIC} &\text{\small BertScore} & \text{\small Rouge1} & \text{\small Rouge2} & \text{\small RougeL} \\ 
        \midrule
        rot&.777 & .229 & .096 & .213& rot&.768 & .175 & .077 & .168 \\ 
        \textbf{moral-rot}& .836 & .416 & .205 & .401& \textbf{moral-rot}&.826 & .393 & .192 & .379 \\ 
        \midrule
        judg&.7240 & .230 & .137 & .230 & judg&.671 & .071 & .000 & .071 \\ 
        \textbf{moral-judg}& .7632 & .464 & .346 & .464& \textbf{moral-judg}&.762 & .314 & .000 & .314 \\
        rot-judg&.7626 & .464 & .346 & .464& rot-judg&.660 & .061 & .000 & .061 \\
        moral-rot-judg&.7628 & .463 & .345 & .463& moral-rot-judg&.761 & .306 & .000 & .306 \\
        \bottomrule
\end{tabular}
\caption{\small Performance of Fine-tuned \texttt{Mistral} Model Across Various Fine-tuning Strategies for Each Benchmark, with the best strategy highlighted in \textbf{bold}. For both tasks, introducing more information, e.g., moral foundation, in the fine-tuning process would improve the performance. The~\texttt{moral-rot} achieves the optimal performance for both SocialChem and MIC. The~\texttt{moral-judg} and~\texttt{moral-judg} are the best strategy for SocialChem and MIC respectively, in terms of the judgment prediction task. Additional results for Llama3 are availabe in Table~\ref{tab:ethicaltuning4llama3}.}
\label{tab:ethicaltuning4mistral}
\end{table*}

\textbf{Experimental Settings.}
We take Mistral-7B\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-v0.1}} and Llama3-8B\footnote{\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}} as the backbone models and leverage the LoRA method to fine-tune them through a supervised fine-tuning loss.
For each benchmark, we employ two fine-tuning strategies for RoT generation and four fine-tuning strategies for ethical judgment prediction. For \textit{RoT generation}, we fine-tune LLMs: (1) to directly generate RoT according to the given situation (\text{rot}) and (2) first generate the moral foundation, then the RoT (\text{moral-rot}). For \textit{Judgment Prediction}, we fine-tune LLMs to: (1) directly predict judgment (\text{judg}) , and (2) firstly generate the moral foundation and/or RoT then the judgment (\text{moral-judg},~\text{rot-judg} and~\text{moral-rot-judg}).

The prompting format and LoRA fine-tuning settings are available in Appendix~\ref{appendix:ethicaltuning}. 
We consider 10000 samples with only one underlying moral foundation for analytical convenience. In the process of fine-tuning, we take the check point with the least loss on the  development set, and report its performance on the test set.
During inference, we prompt fine-tuned LLMs to first generate intermediate predictions before producing the final RoT or ethical judgment, following the same prompting strategy used during fine-tuning.
For example, in the~\texttt{moral-rot} strategy, LLMs are instructed to first predict the moral foundation based on the given situation and subsequently generate the RoT using both the situation and the predicted moral foundation. Following~\citet{ziems2022moral}, we report the performance of the BertScore~\cite{zhang2019bertscore}, Rouge-1, Rouge-2, and Rouge-L metrics.

RoT generation and ethical judgment prediction align with the core capabilities essential for morality-related scenarios and serve as prototypical formats for moral reasoning.
By incorporating moral foundations into RoT generation, we aim to guide LLMs to first identify the moral foundation associated with a given situation, thereby improving the quality of the generated RoT. RoTs serve as instances of evidence and explanation for ethical judgments, aligning with previous studies that seek to enhance LLMsâ€™ social intelligence through social interaction environments~\cite{liutraining,wang-etal-2024-sotopia}.

\textbf{Main Results.}
Table~\ref{tab:ethicaltuning4mistral} and Table~\ref{tab:ethicaltuning4llama3} present fine-tuning results for Mistral and Llama3, respectively\footnote{Note that this paper does not aim to achieve state-of-the-art performance but rather to investigate the underlying mechanisms behind these performance gains.}.
As shown in Table~\ref{tab:ethicaltuning4mistral}, introducing moral foundations in fine-tuning enhances performance across all experimental settings.
However, incorporating RoT information along into the ethical judgment prediction task has a negative impact to the MIC benchmark. We hypothesize that this is because judgments are significantly shorter than RoTs, and the added complexity of RoTs would introduce challenges for fine-tuning.