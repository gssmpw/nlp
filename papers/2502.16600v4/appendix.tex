
\subsection{Additional Related Works\label{sec:relatedworks}}
Machine ethics~\cite{anderson2011machine,tolmeijer2020implementations,nath2020problem,allen2006machine} has been a long-standing research topic for hardware and software systems, with the aim of maximizing their benefits while minimizing societal risks.
Recently, we have witnessed the progress of Artificial Intelligence (AI), particularly that associated with Large Language Models (LLMs), changing the world.
Ensuring LLMs will acquire an understanding of ethics to prevent them from making harmful decisions has become a serious research problem for both academia and industry.
Dating back to the 1940s, the Three Laws of Robotics~\cite{asimov1941three} were proposed to ensure that robots do not cause harm to humans.
Since then, machine ethics has been explored by researchers in philosophy, psychology, and cognitive science. However, it remains a significant challenge for AI, as even coherent and diverse language generation poses difficulties.
The widespread deployment of LLMs opens the door for AI researchers to pursue ethics acquisition due to their strong semantic modeling capability. 
%of LLMs. %and their widespread deployment.
% The emergence of LLMs

Numerous studies have attempted to evaluate the moral and ethical orientations encoded in LLMs through empirical experiments. \citet{bonagiri2024sage} demonstrates that model performance and moral consistency are independent of one another, while \citet{abdulhai2023moral} investigates whether LLMs exhibit biases toward specific moral principles. \citet{scherrer2024evaluating} proposes a statistical method to assess the moral values encoded in LLMs, and \citet{zhang2023measuring} introduces a metric to determine whether LLMs understand ethical values both in terms of “knowing what” and “knowing why.” Collectively, these studies highlight that LLMs lack consistent moral or ethical orientations across different scenarios.
Enabling LLMs to acquire ethical values is a formidable challenge, not only because ethical AI operates at the level of pragmatics~\cite{awad2022computational}, but also due to the philosophical complexities surrounding the proper representation of human ethics~\cite{zhixuan2024preferencesaialignment}. Progress has been made, albeit only partially. 

\subsection{Hyperparameters for the Bert Classifier\label{appendix:hyperparams4clf}}
Hyperparameters are available in Table~\ref{tab:optimhyperparam}.
\begin{table}[h]
    \centering
    \begin{tabular}{cc}
    \toprule
     \textbf{Hyperparameters} & \textbf{Setting} \\
     \midrule
     \textbf{Optimizer} & \text{AdamW}\\
     \textbf{Adam $\beta_1$} & \text{0.9} \\
     \textbf{Adam $\beta_2$} & \text{0.98} \\
     \textbf{Adam $\epsilon$} & \text{1e-3} \\
     \textbf{Learning rate for BERT} & \text{5e-5} \\
     \textbf{Learning rate for classifier layer} & \text{1e-2} \\
     \textbf{Maximum training epochs} & \text{10} \\
     \textbf{Weight decay} & \text{0.01} \\
     \textbf{Batch size} & \text{32}\\
     \textbf{Seed} & 1,2,3,4,5\\
     \bottomrule
    \end{tabular}
    \caption{Hyperparameter Settings for the AdamW Optimizer.}
    \label{tab:optimhyperparam}
\end{table}
\subsection{Re-categorization of Moral Foundation Labels\label{appendix:redist_moral}}
For \textbf{MIC}, we label samples with the moral foundation of \textit{Care} as 0, and those with the foundations of \textit{Fairness, Liberty, Authority}, and \textit{Loyalty} as 1.
For \textbf{SocialChem}, samples classified under \textit{Loyalty-Betrayal} are labeled as 0, while those falling under \textit{Fairness-Cheating}, \textit{Care-Harm}, \textit{Sanctity-Degradation}, and \textit{Authority-Subversion} are labeled as 1.
\subsection{Experimental Settings for Fine-tuning\label{appendix:ethicaltuning}}
\begin{center}
\begin{tcolorbox}[colback=black!2,colframe=black,width=7.5cm,arc=1mm,boxrule=0.5pt]
\small
Prompting format moral-rot-judgment \\ \\ Situation: \{\#SITUATION\}\\
Moral Foundation: \{\#MORAL\_FOUNDATION\}\\
Rule of Thumb: \{\#RoT\}\\
Ethical Judgment: \{\#judgment\}\\

LoRA hyperparameters\\
rank: 64\\
lora alpha: 16\\
lora dropout: 0.1\\
target modules: q\_proj, k\_proj, v\_proj, o\_proj\\
batch size: 16\\
learning rate: 5e-5
\end{tcolorbox}
\end{center}



\subsection{Mechanistic Analysis to Fine-tuned Llama3\label{appendix:mech_llama3}}
Table~\ref{fig:mech_llama3} introduces the fine-tuning results for the Llama3 model. 
Different from Mistral, introducing additional information of the moral foundations and RoT do not always contribute to better performance.
For the SocialChem benchmark, the baseline fine-tuning strategy outperforms other strategies, albeit by a very narrow margin.
This aligns with the generalization mechanism illustrated in Figure~\ref{fig:mech_llama3}. Unlike Mistral, the introduction of moral foundations and RoT does not reduce cosine similarity or BertScore.
Figure~\ref{fig:same_moral_llama3} shows the ratio of the same moral foundation among top 10 generalization-supportive training moral situations, and the behavior of Llama3 is the same as Mistral.
In summary, the pragmatic dilemma still persists for the Llama3 model and is even worse than that of the Mistral model.
\begin{table*}[h]
\small
\centering
\begin{tabular}{c c c c c| c c c c c}
\toprule
\texttt{SocialChem} &BertScore & Rouge1 & Rouge2 & RougeL & \texttt{MIC} &BertScore & Rouge1 & Rouge2 & RougeL \\ 
        \midrule
        \textbf{rot}&.8222 & .358 & .151 & .343& rot&.814 & .365 & .152 & .332 \\ 
        moral-rot&.8217 & .356 & .152 & .340 & \textbf{moral-rot}&.818 & .365 & .168 & .352 \\ 
        \midrule
        \textbf{judg}&.759 & .440 & .313 & .440 & judg &.684 & .109 & .000 & .109 \\ 
        \underline{moral-judg} & .757 & .411 & .285 & .411 & \underline{moral-judg} &.751 & .254 & .000 & .254 \\
        rot-judg & .755 & .400 & .264 & .400 & rot-judg &.660 & .061 & .000 & .061 \\
        moral-rot-judg&.752 & .370 & .248 & .370& \textbf{moral-rot-judg}&.762 & .314 & .000 & .314 \\
        \bottomrule
\end{tabular}
\caption{\small Performance of Fine-tuned \texttt{Llama3} Model Across Various Fine-tuning Strategies for Each Benchmark. The best fine-tuning strategy is highlighted in \textbf{bold} and the second best strategy is \underline{underlined}. For MIC, incorporating additional information, such as moral foundations, during fine-tuning enhances performance; however, this effect is not observed for SocialChem.}
\label{tab:ethicaltuning4llama3}
\end{table*}
%However, this effect is not observed in the SocialChem benchmark.



\begin{figure*}
    \centering
    \subfloat[\tiny RoT Generation for Socialchem]{\includegraphics[width=0.5\linewidth]{llama3.socialchem.rot.regular.png}}
    \hfill
    \subfloat[\tiny Ethical Judgment Prediction for SocialChem]{\includegraphics[width=0.5\linewidth]{llama3.socialchem.judgement.regular.png}}
    \hfill
    \subfloat[\tiny RoT Generation for MIC]{\includegraphics[width=0.5\linewidth]{llama3.mic.rot.regular.png}}
    \hfill
    \subfloat[\tiny Ethical Judgment Prediction for MIC]{\includegraphics[width=0.5\linewidth]{llama3.mic.judgement.regular.png}}
    \hfill
    \caption{\small Top-10 Generalization-Supportive Training Samples Analysis for Fine-tuned Llama3 Through the Introduced Fine-tuning Strategies.}
    \label{fig:mech_llama3}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{llama3.socialchem.rot.same_moral.png}
        \caption{\small RoT in SocialChem}
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{llama3.socialchem.judgement.same_moral.png}
        \caption{\small Ethical Judgment Prediction in SocialChem}
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{llama3.mic.rot.same_moral.png}
        \caption{\small RoT in MIC}
    \end{minipage}
    \hfill
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{llama3.mic.judgement.same_moral.png}
        \caption{\small Ethical Judgment Prediction in MIC}
    \end{minipage}
    \caption{\small Same Moral Ratio for Fine-tuned Llama3.}
    \label{fig:same_moral_llama3}
\end{figure*}

\begin{comment}
\subsection{Prompts for refinement\label{appendix:prompts4refine}}
Following the prompts used for refinement

\begin{center}
\begin{tcolorbox}[colback=black!2,colframe=black,width=7.5cm,arc=1mm,boxrule=0.5pt]
\small
\texttt{Baseline}\\

\textbf{Human:}\\

Question: If technology began attacking you right now, how would you die?\\
Answer: I would probably die from a heart attack.
\\

Your task is to generate a new answer that is more ethical than the answer above. \\
Please provide only your answer without any explanations. 
\\

\textbf{Assistant:} My new answer is
\end{tcolorbox}
\end{center}

\begin{center}
\begin{tcolorbox}[colback=black!2,colframe=black,width=7.5cm,arc=1mm,boxrule=0.5pt]
\small
\texttt{rot}\\

\textbf{Human:}\\

A Rule of Thumb is a fundamental judgment about right and wrong behavior.\\

A Rule of Thumb should explain the basics of good and bad behavior, should contain a judgment (e.g. “you should”) and an action (e.g. “give money to the poor”) and make a general rule but still provide enough detail such that it is understandable even out of context. 
\\


Question: If technology began attacking you right now, how would you die?\\
Answer: I would probably die from a heart attack.\\
Rule of Thumb: \{\#RoT\}
\\

Your task is to generate a new answer by referring to the rule of thumb derived from the answer above. \\
Please provide only your answer without any explanations. 
\\

\textbf{Assistant:} My new answer is
\end{tcolorbox}
\end{center}

\begin{center}
\begin{tcolorbox}[colback=black!2,colframe=black,width=7.5cm,arc=1mm,boxrule=0.5pt]
\small
\texttt{judg}\\

\textbf{Human:}\\

A Rule of Thumb is a fundamental judgment about right and wrong behavior.\\
A Rule of Thumb should explain the basics of good and bad behavior, should contain a judgment (e.g. “you should”) and an action (e.g. “give money to the poor”) and make a general rule but still provide enough detail such that it is understandable even out of context. 
\\


Question: If technology began attacking you right now, how would you die?\\
Answer: I would probably die from a heart attack.\\

Your ethical judgment to the answer above is: \{\#judgment\}
\\

Please generate a new answer by referring to your ethical judgment if you think your answer is not ethial, otherwise keep the same answer.
\\

\textbf{Assistant:} My new answer is
\end{tcolorbox}
\end{center}
\end{comment}

\begin{table*}[t]
\small
\centering  
\begin{tabular}{|l|l|}
\toprule
Moral Foundation Branches & Brief Description \\
\midrule
\makecell{Care\\Harm} & \makecell{Demonstrates care, generosity, compassion, and empathy, \\while showing sensitivity to others’ suffering and upholding the principle of avoiding harm.}\\ 
\midrule
\makecell{Fairness\\Cheating} & \makecell{Encompasses fairness, justice, reciprocity, altruism, rights, \\autonomy, equality, proportionality, and the rejection of cheating.} \\
\midrule
\makecell{Loyalty\\Betrayal} & \makecell{Emphasizes group affiliation, solidarity, patriotism, \\and self-sacrifice, while prohibiting betrayal.} \\
\midrule
\makecell{Authority\\Subversion}& \makecell{Upholding social roles, respecting authority and \\traditions, valuing leadership, and prohibiting rebellion.} \\
\midrule
\makecell{Purity (Sanctity)\\Degradation} & \makecell{Reverence for the sacred, purity, religious principles guiding life, \\and prohibitions against violating the sacred.} \\
\bottomrule
\end{tabular}

\caption{Brief Descriptions of the Moral Foundations. Each foundation has two aspects representing positive and negative perspectives of that moral foundation branch. Please refer to~\citet{atari2023morality} for the most up-to-date list of moral foundations and their descriptions.} 
\label{appendix:MFs}
\end{table*}