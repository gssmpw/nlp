\section{Prompt to Self-Refinement}
In this section, we show an experiment leveraging the fine-tuned LLMs to refine an unethical situation to be an ethical one through the self-correction framework~\cite{liu2024coverge,liu2024intrinsic,qi2024moral,ganguli2023capacity}.
The MIC benchmark provide a refined reply that is ethical compared to the original reply, there fore we prompt LLMs to refine the original reply and take the refine reply as the ground-truth.
We take the BertScore between the refined answer and the ground-truth answer as the evaluation metric.
We have three experimental settings:\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item[1.]~\texttt{baseline}. We directly prompt Mistral and Llama3 with an instruction ``Your task is to generate a new answer that is more ethical than the answer above". In this way, we employ the intrinsic self-correction capability within LLMs.
    \item[2.]~\texttt{rot}. We utilize the LLM fine-tuned with the RoT strategy to first generate a RoT and then refine the original answer accordingly.
    \item[3.]~\texttt{judg}. We use the LLM fine-tuned with the judg strategy to first predict its ethical judgment and then refine the original answer based on that judgment.
\end{itemize}
The used prompts are presented in Appendix~\ref{appendix:prompts4refine}.


\begin{table}[ht]
\small
    \centering
    \begin{tabular}{c c c c}
        \toprule
        & \texttt{baseline} & \texttt{rot} & \texttt{judg} \\ 
        \midrule
        Mistral& \textbf{.742} & .737 & .733 \\ 
        \midrule
        Llama3& \textbf{.741} & .735 & .734\\
        \bottomrule
    \end{tabular}
    \caption{\small Experimental results for the Self-Refinement task.}
    \label{tab:selfrefine}
\end{table}
Table~\ref{tab:selfrefine} presents the best performance achieved within five self-refinement rounds, where the prompt incorporates the refined answer from the previous round.
The performance gap between the baseline setting and others is not large, however it is obvious that fine-tuned LLMs do not outperform the intrinsic self-refinement that only leverage LLMs' internal knowledge acquired during fine-tuning.
We believe this result aligns with previous studies regarding the superficial hypothesis in LLMs's various capabilities, e.g., self-correction~\cite{liu2024intrinsic}, alignment~\cite{lin2023unlocking}.




