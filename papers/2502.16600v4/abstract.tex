Ensuring that Large Language Models (LLMs) return just responses which adhere to societal values is crucial for their broader application. 
Prior research has shown that LLMs often fail to perform satisfactorily on tasks requiring moral cognizance, such as ethics-based judgments. While current approaches have focused on fine-tuning LLMs with curated datasets to improve their capabilities on such tasks, choosing the optimal learning paradigm to enhance the ethical responses of LLMs remains an open research debate. In this work, we aim to address this fundamental question:~\textit{can current learning paradigms enable LLMs to acquire sufficient moral reasoning capabilities?} Drawing from distributional semantics theory and the pragmatic nature of moral discourse, our analysis indicates that 
performance improvements follow a mechanism similar to that of semantic-level tasks, and therefore remain affected by the pragmatic nature of morals latent in discourse, a phenomenon we name the \textit{pragmatic dilemma}. We conclude that this pragmatic dilemma imposes significant limitations on the generalization ability of current learning paradigms, making it the primary bottleneck for moral reasoning acquisition in LLMs. 

\textit{\small \textbf{Warning}: examples in this paper may be offensive.}

%We conclude that the inherently pragmatic nature of morality imposes significant limitations on the generalization ability of current learning paradigms, making the pragmatic dilemma the primary bottleneck for morality acquisition in LLMs. \textit{\small \textbf{Warning}: examples in this paper may be offensive.}
%We conducted our experiments using widely adopted open-source LLMs on representative morality-relevant prediction tasks of moral foundation, Rule-of-Thumbs and judgment, concluding that the pragmatic dilemma is the primary bottleneck for morality acquisition in LLMs.

%Prior research has explored fine-tuning with curated datasets to improve typical LLM capabilities on ethical judgment tasks. Other studies have shown that LLMs often fail to perform 
%Although there is ongoing debate regarding the correct learning paradigm for the enhancement of morally just responses from LLMs, prior research has explored fine-tuning LLMs on curated datasets to improve their typical capabilities, e.g. ethical judgment. 
%Meanwhile, some studies have shown that LLMs often fail to perform satisfactorily in morality-related tasks or, more broadly, in tasks requiring social intelligence.
