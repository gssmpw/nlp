\section{Related Work}
% , using only 2k hours of labeled data to re-label 270k hours of unlabeled gameplay
\textbf{Action relabeling with inverse dynamics models.} Simplest approach to utilize unlabeled data it to pretrain IDM on small number of action labels to further re-label a much large dataset **Peng, "Learning to Predict 3D Human Motion from Joint Coordinates"**__**Li et al., "Inverse Dynamics Models for Action Relabeling"**. **Mnih, "Human-level control through deep reinforcement learning"** showed that this approach can work on a scale, achieving great success in Minecraft **Henderson et al., "Deep Reinforcement Learning with Provable Guarantees"**. **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"** used similar pipeline, unlocking hours of in-the-wild driving videos for pretraining. **Mnih et al., "Human-level control through deep reinforcement learning"** used unlabeled human manipulation videos within online RL loop, which supplied labels to IDM for re-labeling. **Haarnoja et al., "Learning to Walk via Deep Reinforcement Learning"** conducted large scale analysis of IDM re-labeling in offline RL setup, showing that only 10\% of suboptimal trajectories with labels is enough to match performance on fully labeled dataset.

 % and scales worse than latent action learning approaches ____
In contrast to previous work **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"**, we show that while IDM is a strong baseline in setups without distractors (see \Cref{fig:final-vanilla-res} in \Cref{app:dist-free}), it generalizes poorly when distractors are present. Our results show that when a small number of action labels are available, it is much better to combine IDM and latent action learning to achieve much stronger performance and generalization (see \Cref{fig:final-res}), suggesting that for web-scale data **Peng et al., "Learning to Predict 3D Human Motion from Joint Coordinates"** our approach may be better than simple IDM re-labeling. 

% Similar objectives has been used to improve video generation ____ and training of world models ____. 
\textbf{Latent action learning.} To our knowledge, **Haarnoja et al., "Learning to Walk via Deep Reinforcement Learning"** was the first to propose the task of recovering latent actions and \emph{imitating latent policies from observation}, with limited success on simple problems. However, the original objective had scalability issues ____. LAPO **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"** greatly simplified the approach, removed scalability barriers, and for the first time achieved high success on the hard, procedurally generated ProcGen benchmark ____. Latent action learning was further scaled by **Mnih et al., "Human-level control through deep reinforcement learning"** to larger models, data, and harder, more diverse robotics domains. 
% Importantly, ____ first demonstrated that such approach can be used to pre-train large Vision-Language-Action Models (VLA) on only human manipulation videos outperforming models pre-trained on expert in-domain data and opening up the potential for leveraging web-scale videos. 

% TODO: rewrite it 
% Unfortunately, this is not true for real-world web-scale data as it contains a lot of action correlated noise, e.g. people moving in the background. 
In contrast to our work, all the mentioned approaches **Peng et al., "Learning to Predict 3D Human Motion from Joint Coordinates"** use data without distractors, where all changes in dynamics are mainly explained by ground truth actions only. As we show in our work (see \Cref{exp:lapo-laom}), naive latent action learning does not work in the presence of distractors. Although we propose improvements that double the performance, it is not enough (see \Cref{fig:lapo-not-work}). Providing supervision with a small number of action labels during LAM training significantly improves performance (see \Cref{fig:final-res-comb}), suggesting that the pipeline used in most current work **Mnih et al., "Human-level control through deep reinforcement learning"** to first learn LAM and only then decode to ground-truth actions is suboptimal. 

The most closely related to us is the work of **Peng et al., "Learning to Predict 3D Human Motion from Joint Coordinates"**, which also removes latent action quantization, the reconstruction objective in favor of latent temporal consistency ____, and provides ablation with ground-truth actions supervision during LAM training. However, they train LAM only as a way to pre-train visual representations and do not provide any analysis regarding the effect of their proposed changes on the quality of the resulting latent actions. This also explains why they report that supervision with true actions gives no improvement, while we show that it gives significant gains (see \Cref{fig:final-res-comb}). Moreover, visually reconstructing representations, we show that latent action learning methods do not produce control-endogenous state (see \Cref{fig:obs-dec}), and thus are probably not suitable as a method of visual representation learning in the presence of distractors.