%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{mathtools}
\usepackage{multirow}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Latent Action Learning Requires Supervision in the Presence of Distractors}

\begin{document}

\twocolumn[
\icmltitle{Latent Action Learning Requires Supervision in the Presence of Distractors}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

% Alexander Nikulin, Ilya Zisman, Denis Tarasov, Lyubaykin Nikita, Andrei Polubarov, Igor Kiselev, Vladislav Kurenkov 
\begin{icmlauthorlist}
    \icmlauthor{Alexander Nikulin}{airi,mipt}
    \icmlauthor{Ilya Zisman}{airi,skol}
    \icmlauthor{Denis Tarasov}{airi}
    \icmlauthor{Nikita Lyubaykin}{airi,inno}
    \icmlauthor{Andrei Polubarov}{airi,skol}
    \icmlauthor{Igor Kiselev}{acc}
    \icmlauthor{Vladislav Kurenkov}{airi,inno}    
\end{icmlauthorlist}

\icmlaffiliation{airi}{AIRI}
\icmlaffiliation{inno}{Innopolis University}
\icmlaffiliation{skol}{Skoltech}
\icmlaffiliation{mipt}{MIPT}
% \icmlaffiliation{hse}{HSE}
\icmlaffiliation{acc}{Accenture}

\icmlcorrespondingauthor{Alexander Nikulin}{nikulin@airi.net}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML, reinforcement learning, imitation learning, latent action model, latent actions, learning from observations}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by \textbf{8x}, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5\% of the full dataset, during latent action learning improves downstream performance by \textbf{4.2x} on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.
\end{abstract}

% TODO: add citations for latent action learning from oleh rybkin
\section{Introduction}
% During the past few years, pre-training massive models on massive web-scale datasets has led to the creation of highly capable and general foundation models \citep{bommasani2021opportunities, brown2020language, agarwal2024many}. Such success holds the promise that scaling can pave the way for similar breakthroughs in robotics and embodied AI \citep{lin2024data, sartor2024neural}. Unfortunately, even the largest Vision-Language-Action (VLA) models \citep{o2023open, khazatsky2024droid, kim2024openvla, team2024octo} remain far behind in terms of generalization, emergent abilities and reasoning \citep{guruprasad2024benchmarking}. The main bottleneck is the lack of appropriate data, as currently existing datasets \citep{o2023open, khazatsky2024droid} are quite limited in diversity, which is essential for generalization \citep{lin2024data}. One of the most promising directions is the use of existing video data from the Internet \citep{yang2024video}, as it is extremely large and diverse, encompassing many complex human-related activities in the real-world that is difficult to model accurately in simulators \citep{mccarthy2024towards}. However, despite its great potential, video data cannot be used immediately due to the lack of rewards and action labels needed for reinforcement and imitation learning \citep{mccarthy2024towards}.

% During the past few years, as predicted by scaling laws \citep{hestness2017deep, kaplan2020scaling, hoffmann2022training, zhai2022scaling}, the most effective and reliable strategy to advance in many areas of deep learning has been to increase model and data size. Pre-training massive models on massive web-scale datasets has led to the creation of highly capable and general foundation models that can perform new tasks zero-shot \citep{bommasani2021opportunities, brown2020language, agarwal2024many}. Such success holds the promise that scaling can pave the way for similar breakthroughs in robotics \citep{lin2024data} and embodied AI \citep{sartor2024neural}. 

% Unfortunately, even the largest Vision-Language-Action (VLA) models \citep{o2023open, khazatsky2024droid, kim2024openvla, team2024octo} remain far behind foundation models from other fields in terms of generalization, emergent abilities and reasoning \citep{guruprasad2024benchmarking}. Currently, one of the most limiting factors is the lack of appropriate data, as currently existing open-source datasets \citep{o2023open, khazatsky2024droid} are quite limited in diversity, which is essential for generalization \citep{lin2024data}. To unlock scaling, researchers are actively seeking new sources of diverse data, e.g. by creating more realistic simulators \citep{tao2024maniskill3} or developing new methods for faster data collection in the real-world \citep{chi2024universal, cheng2024open}. One of the most promising directions is the use of existing video data from the Internet \citep{yang2024video}, as it is extremely large and diverse, encompassing many complex human-related activities in the real-world that is difficult to model accurately in simulators \citep{mccarthy2024towards}. However, despite its great potential, video data cannot be used immediately due to the lack of rewards and action labels needed for reinforcement and imitation learning \citep{mccarthy2024towards}.

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth]{figures/final-averaged-res.pdf}}
        \caption{We show that in the presence of distractors, LAPO struggles to learn latent actions useful for pre-training and that simple BC or IDM are more effective. We propose LAOM, a simple modification that doubles the performance but still underperforms. Thus, we propose to reuse available ground-truth action labels to supervise latent action learning, which significantly improves the performance, achieving normalized score of 0.44. It recovers almost half the performance of BC with access to the full action-labeled dataset, while having access to only 2.5\%. Results are averaged over four environments from Distracting Control Suite, three random seeds each. We provide per-environment plots on \Cref{fig:final-res}. See \Cref{exp:setup} for the evaluation protocol, \Cref{exp:lapo-laom,exp:laom-supervision} for method details. }
        \label{fig:final-res-comb}
    \end{center}
    \vskip -0.4in
\end{figure}

% TODO: (1) remove margins somehow, (2) take diagrams closer to each other

\begin{figure*}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=0.8\textwidth]{figures/lapo-arc-comb-draft-v5.pdf}}
        \caption{Simplified architecture visualization of LAPO, and LAOM - our proposed modification. LAPO consists of IDM and FMD, both with separate encoders, uses latent action quantization and predict next observation in image space via the decoder in FDM. LAOM incorporates multi-step IDM, removes quantization and does not reconstruct images, relying on latent temporal consistency loss. Images are encoded by shared encoder, while IDM and FDM operate in compact latent space. When small number of ground-truth action labels is available, we use them for supervision, linearly predicting from latent actions. For detailed description see \Cref{exp:lapo-laom}.}
        \label{fig:lapo-arc-viz}
    \end{center}
    \vskip -0.2in
\end{figure*}


Recently, a new wave of approaches based on latent action learning has emerged \citep{edwards2019imitating}, demonstrating superior pre-training efficiency on datasets without action labels in large-scale robotics \citep{ye2024latent, chen2024moto, cui2024dynamo, bruce2024genie} and reinforcement learning \citep{schmidt2023learning}. Latent Action Models (LAM) infer latent actions between successive observations, effectively compressing observed changes. Under certain conditions, latent actions can even rediscover the ground truth action space \citep{schmidt2023learning, bruce2024genie}. After training, LAM can be utilized for imitation learning on latent actions to obtain useful behavioral priors. For example, LAPA \citep{ye2024latent} showed that latent action learning can be used to pre-train large model on only human manipulation videos, and despite the huge cross-embodiment gap, still outperform OpenVLA \citep{kim2024openvla}, which was pre-trained on expert in-domain data with available action labels. 

Despite the initial success and the promise of unlocking \emph{vast amounts of video available on the
web} \citep{schmidt2023learning, ye2024latent}, there is a critical shortcoming of previous work â€“ it uses distractor-free data, where all changes between observations are mainly and most efficiently explained by ground truth actions only, such as robot manipulation on a static background \citep{khazatsky2024droid}. Unfortunately, this is not true for real-world web-scale data, as it contains a lot of action-correlated noise \citep{misra2024towards}, e.g. people moving in the background. Such noise may better explain video dynamics and thus lead to latent actions unrelated to real actions. The phenomenon of overfitting to task-irrelevant information is not new and has been studied in model-based \citep{wang2024ad3} and representation learning \citep{lamb2022guaranteed, zhang2020learning, pmlr-v216-zhou23a}. However, the effect of distractors on latent action learning, which we aim to address in this work, has not been similarly investigated.

In this work we empirically investigate the effect of action-correlated distractors on latent action learning using Distracting Control Suite \citep{stone2021distracting}. We demonstrate that naive latent action learning based on quantization and reconstruction objectives, such as LAPO \cite{schmidt2023learning}, struggle in the precense of distractors (see \Cref{exp:lapo-laom}). We propose LAOM, a simple LAPO modification that improve the quality of latent actions by \textbf{8x}, as measured by linear probing, and double the downstream performance (see \Cref{fig:final-res}). However, even after this, the resulting performance is only slightly better than simple Behavioral Cloning on available ground-truth actions. Thus, as our core contribution, we show that providing supervision with a small number, as little as $2.5$\% of the complete dataset, of action labels during LAOM training improves the downstream performance by \textbf{4.3x} on average (see \Cref{exp:laom-supervision}), outperforming all baselines (see \Cref{fig:final-res}). Our findings suggest that the pipeline used in most current work \citep{ye2024latent, cui2024dynamo, chen2024moto} to first learn LAM and only then decode to ground-truth actions is suboptimal when distractors are present, as with supervision better result can be achieved using the same budget of actions labels. In addition, we show that latent action learning with supervision generalizes better in contrast to approaches based on inverse dynamics models \citep{baker2022video, zhang2022learning, zheng2023semi} but does not learn control-endogenous minimal state \citep{lamb2022guaranteed}.  

% \begin{figure*}[t]
%     \vskip 0.2in
%     \begin{center}
%         % \centerline{\includegraphics[width=\columnwidth]{figures/final-averaged-res.pdf}}
%         \centerline{\includegraphics[width=\textwidth]{figures/final_mean_result_comb.pdf}}
%         \caption{TODO}
%         \label{icml-historical}
%     \end{center}
%     \vskip -0.2in
% \end{figure*}


 % Collecting new high quality data that resembles the diversity of the real world is slow, costly and probably not scalable yet. 

% inital long intro

% \textbf{\textcolor{red}{NB:} this is not an actual intro, but rather my lengthy thoughts on motivation. I will rewrite it in a more focused manner later.}

% During the past few years, as predicted by scaling laws \citep{hestness2017deep, kaplan2020scaling, hoffmann2022training, zhai2022scaling}, the most effective and reliable strategy to advance in many areas of deep learning has been to increase model and data size. Pre-training massive models on massive web-scale datasets has led to the creation of highly capable and general multimodal foundation models \citep{bommasani2021opportunities} that can perform new tasks zero-shot or can be quickly fine-tuned to do so \citep{brown2020language, agarwal2024many}. Such models have enabled many breakthroughs in the understanding and generation of natural language \citep{brown2020language, achiam2023gpt}, image \citep{radford2021learning, dehghani2023scaling}, video \citep{madan2024foundation}, and audio \citep{defossez2024moshi}. 

% The success of foundation models holds the promise that scaling can pave the way for similar breakthroughs in embodied AI. There have already been early attempts to explore scaling laws in robotics \citep{sartor2024neural, pearce2024scaling} and reinforcement learning \citep{hilton2023scaling, springenberg2024offline}, discovering laws similar to those observed in language models. Based on these observations, \citet{o2023open} collected the largest robotics dataset to date and showed that the generalist agent trained on it significantly outperformed specialized methods in many tasks and domains, which was later further improved by \citet{khazatsky2024droid, kim2024openvla, team2024octo}. However, even the best and largest agents currently in existence \citep{kim2024openvla} remain far behind foundation models from other fields in terms of generalization, emergent abilities and reasoning \citep{guruprasad2024benchmarking}. 

% Embodied AI has many unique challenges that can stall progress, but the biggest is the lack of suitable data. While most new datasets, such as Open-X \citep{o2023open}, are orders of magnitude larger than those previously available, they are still tiny compared to the web-scale datasets used to train VLMs, which contain trillions of tokens of text alone \citep{soldaini2024dolma}. They are also quite limited in diversity, which is essential for generalization \citep{lin2024data}, and mostly involve simple tasks in laboratory-controlled settings far from the real world. Collecting new high quality data that resembles the diversity of the real world is slow, costly and probably not scalable yet. Moreover, the foundation models on which agents are usually built \citep{brohan2022rt, brohan2023rt} are themselves quite limited in their agentic capabilities \citep{paglieri2024balrog}, as they were not pre-trained with such an objective in mind and need to be adapted to fit in. 

% To unlock scaling for embodied AI, researchers are actively seeking new sources of data and ways to increase its diversity, for example by creating more realistic simulators \citep{tao2024maniskill3} or developing faster methods for collecting real-world demonstrations \citep{chi2024universal, cheng2024open}. Another promising direction is the use of existing video data from the Internet \citep{yang2024video}. It is extremely large and diverse, encompassing many complex human-related activities, real-world physics and dynamics that are difficult to model accurately in simulators \citep{mccarthy2024towards}. 

% However, despite its great potential, video data cannot be used immediately because it lacks the clear rewards needed for reinforcement learning and even the action labels needed for imitation learning. Moreover, \citet{misra2024towards} showed theoretically that the absence of actions is a serious limitation in a more general way, since in the presence of action-correlated noise, e.g. people moving in the background, the sample complexity of learning good representations from video data can be exponentially worse than from action-labeled data.

% Unfortunately, even the largest Vision-Language-Action (VLA) models \citep{o2023open, khazatsky2024droid, kim2024openvla, team2024octo} remain far behind foundation models from other fields in terms of generalization, emergent abilities and reasoning \citep{guruprasad2024benchmarking}. One of the most limiting factors is the lack of appropriate data. While recent open source datasets \citep{o2023open, khazatsky2024droid} are orders of magnitude larger than those previously available, they are still tiny compared to the web-scale datasets used to train foundation models and limited in diversity, which is essential for generalization \citep{lin2024data}, mainly involving simple tasks in laboratory-controlled settings. Moreover, the foundation models on which agents are usually built \citep{brohan2022rt, brohan2023rt} are themselves quite limited in their agentic capabilities \citep{paglieri2024balrog}, as they have not been pre-trained with such a goal in mind. 

% To unlock scaling, researchers are actively seeking new sources of diverse data, e.g. by creating more realistic simulators \citep{tao2024maniskill3} or developing new methods for faster data collection in the real-world \citep{chi2024universal, cheng2024open}. The most promising direction is the use of existing video data from the Internet \citep{yang2024video}, as it is extremely large and diverse, encompassing many complex human-related activities in the real-world that is difficult to model accurately in simulators \citep{mccarthy2024towards}. However, despite its great potential, video data cannot be used immediately due to the  lack of rewards and action labels needed for reinforcement and imitation learning \citep{mccarthy2024towards}.

% During the past few years, as predicted by scaling laws \citep{hestness2017deep, kaplan2020scaling, hoffmann2022training, zhai2022scaling}, the most effective and reliable strategy to advance in many areas of deep learning has been to increase model and data size. Pre-training massive models on massive web-scale datasets has led to the creation of highly capable and general foundation models that can perform new tasks zero-shot \citep{bommasani2021opportunities, brown2020language, agarwal2024many}. Such success holds the promise that scaling can pave the way for similar breakthroughs in embodied AI and robotics. 

% % Pre-training massive models on massive web-scale datasets has led to the creation of highly capable and general multimodal foundation models \citep{bommasani2021opportunities} that can perform new tasks zero-shot or can be quickly fine-tuned to do so \citep{brown2020language, agarwal2024many}. Such success holds the promise that scaling can pave the way for similar breakthroughs in embodied AI and robotics. 
% % They have enabled many breakthroughs in the understanding and generation of natural language, images, or video \citep{brown2020language, achiam2023gpt, radford2021learning, dehghani2023scaling}.

% Unfortunately, even the largest Vision-Language-Action (VLA) models \citep{o2023open, khazatsky2024droid, kim2024openvla, team2024octo} remain far behind foundation models from other fields in terms of generalization, emergent abilities and reasoning \citep{guruprasad2024benchmarking}. One of the most limiting factors is the lack of appropriate data. While recent open source datasets \citep{o2023open, khazatsky2024droid} are orders of magnitude larger than those previously available, they are still tiny compared to the web-scale datasets used to train foundation models, which contain trillions of tokens of text alone \citep{soldaini2024dolma}. They are also quite limited in diversity, which is essential for generalization \citep{lin2024data}, and mainly involve simple tasks in laboratory-controlled settings. Moreover, the foundation models on which agents are usually built \citep{brohan2022rt, brohan2023rt} are themselves quite limited in their agentic capabilities \citep{paglieri2024balrog}, as they have not been pre-trained with such a goal in mind. 

% To unlock scaling, researchers are actively seeking new sources of diverse data, e.g. by creating more realistic simulators \citep{tao2024maniskill3} or developing new methods for faster data collection in the real-world \citep{chi2024universal, cheng2024open}. The most promising direction is the use of existing video data from the Internet \citep{yang2024video}, as it is extremely large and diverse, encompassing many complex human-related activities in the real-world that is difficult to model accurately in simulators \citep{mccarthy2024towards}. However, despite its great potential, video data cannot be used immediately due to the  lack of rewards and action labels needed for reinforcement and imitation learning \citep{mccarthy2024towards}.

% \begin{figure*}[t]
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \centerline{\includegraphics[width=\columnwidth]{figures/lapo-arc-draft.pdf}}
%         \caption{}
%         \label{fig:}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \centerline{\includegraphics[width=\columnwidth]{figures/laom-arc-draft.pdf}}
%         \caption{}
%         \label{fig:}
%     \end{subfigure}
%     \label{fig:}
% \end{figure*}


\section{Preliminaries}

\textbf{Learning from observations.} Most methods in reinforcement learning require access to the dataset $\mathcal{D} \coloneq \{ \tau_n \}_{n=1}^N$ of $N$ trajectories, where each $\tau_n \coloneq \{ (o_i^n, a_i^n, r_i^n) \}_{i=1}^\tau$ contains observations, actions and rewards. Similarly, imitation learning requires access to trajectories $\tau_n \coloneq \{ (o_i^n, a_i^n) \}_{i=1}^\tau$ that contain actions. Unfortunately, most expert demonstrations in the real world, such as YouTube videos of some human activity \citep{aytar2018playing, baker2022video, zhang2022learning, ghosh2023reinforcement}, do not include rewards or action labels. Thus, researchers are actively exploring how to most effectively use the data $\tau_n \coloneq \{ (o_i^n) \}_{i=1}^\tau$ without action labels to accelerate the learning of embodied agents at scale \citep{torabi2019recent}. Still, we can often assume that a very small number of action labels are available. For example, previous work has explored ratios of up to 10\% \citep{zheng2023semi}, whereas in our work we allow a maximum of $\sim2.5$\% of labeled transitions.
 
\textbf{Latent action learning.} Latent action learning approaches \citep{edwards2019imitating, schmidt2023learning, chen2024moto, cui2024dynamo, ye2024latent} aim to infer latent actions $z_t$ such that they are maximally informative about each observed transition $(o_t, o_{t+1})$ while being minimal. After the latent action model (LAM) is pre-trained, we can train policies to imitate latent actions on full data to obtain useful behavioral priors. Finally, small decoder heads can be learned from latent to real actions of domain of interest.

We base our work on LAPO \citep{schmidt2023learning}, which is used in recent work \citep{chen2024moto, cui2024dynamo, ye2024latent}. LAPO uses two models in combination to infer latent actions. First is inverse dynamics model (IDM), which is given two consecutive observations predicts latent action $z_t \sim p_{\text{IDM}}(\cdot | o_t, o_{t+1})$. Second is forward dynamics model (FDM), which observes current observation and latent action, and predicts the next observation $\hat{o}_{t+1} \sim p_{\text{FDM}}(\cdot | o_t, z_t)$. Both models are trained jointly to minimize the next observation prediction loss $\| \hat{o}_{t+1} - o_{t+1} \|^2$. We illustrate the model architecture in \Cref{fig:lapo-arc-viz}.

Given the information bottleneck on latent actions, e.g. quantization via the VQ-VAE \citep{van2017neural}, IDM cannot simply copy the next observation into the FDM as is, so it will be forced to compress and encode the difference between observations to be most predictive of the next observation. Without the distractors, through simplicity bias \citep{shah2020pitfalls}, the latent actions will recover the ground truth actions as they are most predictive of the dynamics. However, due to the presence of distractors, it may be not true for real-world data. In this work, we empirically examine how well current LAM can recover true actions in such circumstances. 

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=0.8\columnwidth]{figures/envs-vis.pdf}}
        \caption{We visualize the environments from Distracting Control Suite used in our work. Top row: without any distractors, identical to the original DeepMind Control Suite. Bottom row: with distractors, which consists of dynamic background videos, agent color change and camera shaking.}
        \label{fig:envs-vis}
    \end{center}
    \vskip -0.4in
\end{figure}


\textbf{Control-endogenous minimal state.} 
% In this work we may use the terms \emph{endogenous} and \emph{exogenous} when describing noise, i.e. distractors, or representation properties. We borrow this terminology from the \citet{lamb2022guaranteed, islam2022agent, levine2024multistep, misra2024towards} line of research as it is closely related to latent action learning. 
\citet{lamb2022guaranteed} defines \emph{control-endogenous minimal state} as a representation that contains all the information necessary to control the agent, while completely discarding all irrelevant information. \citet{lamb2022guaranteed, levine2024multistep}, theoretically and practically show that to learn such minimal state multi-step IDM should be used, i.e. IDM that predicts action $a_t$ from states $s_t$ and $s_{t + k}$, where $k \in \{1, 2, 3, \dots, K\}$. However, as showed by \citet{misra2024towards}, in the presence of \emph{exogenous noise}, i.e. non-iid noise that is temporally action-correlated, the sample complexity of learning control-endogenous minimal state from video data can be exponentially worse than from action-labeled data. They hypothesized that this is true for latent action learning as well but did not provide any analysis regarding the quality of latent actions, which we tried to empirically address in this work. 

% Thus, latent action learning in the presence of distractors locked in the chicken-and-egg problem state. Given control-endogenous minimal state, inferring ground truth actions is trivial, as they are the most predictive of the next states. Given true actions, the minimal state can be trivially learned via multi-step IDM \citep{lamb2022guaranteed}. Unfortunately, we are given neither.


\section{Experimental Setup}
\label{exp:setup}

\textbf{Environments and datasets.} To decouple the effects of latent action quality and exploration on performance, we work in an offline setting. For our purposes, it is \emph{essential that the Behavior Cloning (BC) agent should recover most of the expert performance when trained on the full dataset with ground-truth actions revealed}, otherwise it would be difficult to understand the effect of latent action quality on pre-training. 

% For our purposes, it is \emph{essential that the Behavior Cloning (BC) agent trained on the full dataset with ground-truth actions revealed should recover most of the expert performance}, otherwise it would be difficult to understand the effect of latent action quality on pre-training. 

% Most researchers use the Distractor Control Suite (DCS) \citep{stone2021distracting} or its successors \citep{hansen2021softda, almuzairee2024recipe} as a benchmark for online learning and recently released DMC-VB \citep{ortiz2024dmc} for offline learning. 

As currently existing benchmarks with distractors \citep{stone2021distracting, ortiz2024dmc} are not yet solved, we collect new datasets with custom difficulty, based on Distracting Control Suite (DCS) \citep{stone2021distracting}. DCS uses dynamic background videos, camera shaking and agent color change as distractors (see \Cref{fig:envs-vis} for visualization). The complexity is determined by the number of videos as well as the scale for the magnitude of the camera and the color change. We empirically found that using 60 videos and a scale of 0.1 is the hardest setting when BC can still recover expert performance. We collect datasets with five thousand trajectories for four tasks: cheetah-run, walker-run, hopper-hop and humanoid-walk, listed in the order of increasing difficulty. See \Cref{app:data-collection} for additional details.
 
\textbf{Evaluation.} To access the quality of the latent actions, we use two methods. First, we follow the approach of \citet{zhang2022light} and use linear probing \citep{alain2016understanding}, which is a common technique used to evaluate the quality of learned representations by training a simple linear classifier or regressor on top the representations. Since we include ground truth actions in our datasets for debugging purposes, we train linear probes to predict them from latent actions simultaneously with the main method, e.g. LAPO \citep{schmidt2023learning}. We do not pass the gradient through the latent actions, so this does not affect the training. Second, following the most commonly used three-stage pipeline \citep{schmidt2023learning, chen2024moto, ye2024latent}, we first pre-train LAM, then train BC model to predict latent actions on the full dataset, and finally, we reveal a small number of labeled trajectories to train a small two-layer MLP decoder from latent to real actions. Using this decoder, we then evaluate the resulting agent in the environment for 25 episodes. To access scaling properties with different budgets of real actions, similar to \citet{schmidt2023learning}, we repeat this process for a variable number of labeled trajectories, from 2 to 128. All experiments are averaged over three random seeds.

\textbf{Baselines.} We use BC on true actions as our main baseline, since the main goal of latent action learning is to pre-train useful behavioral policies \citep{edwards2019imitating, schmidt2023learning}, which can be achieved by recovering true actions as accurately as possible. We use it in two ways. First, we try to get the best performance for each full dataset with true actions to use the final return for normalization. With such normalization, we can quantify how much performance we have recovered compared to if we had access to a fully action-labeled dataset. Second, we train BC from scratch on the same number of labels available to LAM, to evaluate the benefit of pre-training on large unlabeled data. Our last baseline is IDM, as it remains one of the most successful and simplest approaches to learn from action-free data at scale \citep{baker2022video, zhang2022learning, zheng2023semi}. For additional details, see \Cref{app:baselines}. 

We do not consider other possible types of unsupervised pre-training, as it was already extensively explored by other researchers \citep{tomar2021learning, zhang2022light, kim2024investigating}, even with distractors \citep{misra2024towards, ortiz2024dmc}. Our aim is not to compare latent action learning with existing approaches, but to investigate whether it works at all in the presence of action-correlated distractors.  

% , although it's already been done by \citet{ye2024latent}
% We use identical backbones when possible and tried our best to bring all methods equal in the number of trainable weights. 

\textbf{On hyperparameters tuning.} We tune the hyperparameters based on online performance for BC, on MSE to real actions on the full dataset for IDM, and on final linear probe MSE to real actions for latent action learning. In more practical tasks, we usually do not have this luxury, but since we are interested in estimating the upper bound performance of each method in a controlled setting, we believe that it is appropriate. For exact hyperparameters see \Cref{app:hps}.

\section{Latent Action Learning Struggle in the Presence of Distractors}
\label{exp:lapo-laom}

To access the effect of distractors on latent action learning we start by carefully reproducing and adapting LAPO \citep{schmidt2023learning} for our domain. We use similar architecture (see \Cref{fig:lapo-arc-viz}) with ResNet \citep{he2016deep} as observation encoders, borrowed from the open-source official LAPO implementation. Similar to \citet{schmidt2023learning} we resize observations to 64 height and width, stacking 3 consecutive frames. 

\textbf{Quantization hinders latent action learning.}
To validate our implementation, we first measured performance on distractor-free datasets, which should not cause any difficulty. Contrary to previous research \citep{schmidt2023learning, chen2024moto, ye2024latent, bruce2024genie}, we found that commonly used latent action quantization during training significantly hindered the resulting latent action quality. We initially hypothesized that the problem might be with the VQ-VAE used for quantizing. In conversation with \citet{schmidt2023learning} we confirmed that VQ-VAE is indeed susceptible to codebook collapse and requires extensive tuning. We tried the more modern FSQ \citep{mentzer2023finite}, which has already been used successfully in RL \citep{scannell2024iqrl} and does not suffer from codebook collapse. Unfortunately, even after tuning, we were unable to improve the results significantly, so we simply removed it. To our surprise, this resulted in a large positive improvement (see \Cref{fig:lapo-fsq-viz}), but only for datasets without distractors, while with distractors the action quality remained at almost the same level. 

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=0.8\columnwidth]{figures/lapo_res.pdf}}
        \caption{Quality of latent actions learned by LAPO. We show that quantization of latent actions significantly reduces the quality of actions, even on data without distractors, where LAPO should work without problems. Removing the quantization recovers the latent action quality, but additional modifications are needed to improve LAPO performance with distractors. Results are averaged across all four environments, each with three random seeds.}
        \label{fig:lapo-fsq-viz}
    \end{center}
    \vskip -0.4in
\end{figure}


\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth]{figures/lapo_improvements_res.pdf}}
        \caption{The individual effect of each proposed change in LAOM, our modification of LAPO, which overall improves latent action quality in the presence of distractors by a factor of 8. We describe the proposed changes in detail in \Cref{exp:lapo-laom} and visualize the final architecture in \Cref{fig:lapo-arc-viz}. Results are averaged across all four environments, each with three random seeds.}
        \label{fig:lapo-improvements}
    \end{center}
    \vskip -0.4in
\end{figure}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\columnwidth]{figures/lapo_vanilla_vs.pdf}}
        \caption{Performance evaluation of the LAPO and the proposed LAOM with and without distractors. As can be seen, large gap in performance remains in the presence of distractors. Results are averaged across all four environments, each with three random seeds.}
        \label{fig:lapo-not-work}
    \end{center}
    \vskip -0.4in
\end{figure}

One explanation for the result on \cref{fig:lapo-fsq-viz} may be that we are working with continuous actions, unlike the \citet{schmidt2023learning} which used discrete actions. However, we believe that there are more general reasons. The main motivation for quantizing latent actions was to prevent shortcut learning, i.e. IDM copying $o_{t+1}$ to FDM as is, and to incentivize IDM to learn simpler latents that capture only action-related changes. We observed no evidence for shortcut learning, suggesting that it is unlikely to occur with high-dimensional observations, similar to the unlikelihood of collapse in Siamese networks \citep{chen2021exploring}. More importantly, in the presence of action-correlated distractors, \emph{the information bottleneck may have the opposite effect, incentivising the IDM to encode noise into latent actions}. This noise can explain the dynamics more easily, so without guidance, the IDM has no way of distinguishing it from real actions. Therefore, we advise against the use of quantization for LAM training on real-world data.
% More importantly, in the presence of action-correlated distractors, \emph{the information bottleneck may have the opposite effect, incentivising the IDM to encode noise into latent actions} since it may explain the dynamics more easily, and without guidance, the IDM has no way of distinguishing noise from real actions.

\textbf{Latent action quality can be significantly improved.} As \Cref{fig:lapo-fsq-viz} shows, naive LAPO may not be able to learn good latent actions in the presence of distractors and further improvements are needed. Thus, we propose simple modifications to the LAPO architecture, which in combination improve latent action quality by \textbf{8x}, almost closing the gap with distractor-free setting (see \Cref{fig:lapo-improvements}). Interestingly, on distractor-free data improvements are marginal, further demonstrating the importance of the proposed changes to specifically help latent action learning in the presence of distractors. We visualize the resulting architecture, which we called Latent Action Observation Model (\textbf{LAOM}) in \Cref{fig:lapo-arc-viz} and describe changes in detail next:

\emph{Multi-step IDM.} Inspired by research on control-endogenous minimal state discovery \citep{lamb2022guaranteed, levine2024multistep} via multi-step IDM, we slightly modify our IDM objective to estimate latent action $z_t$ from $o_t$ and $o_{t + k}$, where $k \in \{1, 2, 3, \dots, K\}$, instead of just consecutive observations. During training, we sampled $k$ uniformly for each sample and found that $K \coloneq 10$ worked best. Multi-step IDM helps to learn representation which encodes control-endogenous information with respect to current latent actions, which in turn helps learn better latent actions. This simple change alone doubled the latent action quality. 

\emph{Increasing latent actions capacity.} So far we have used latent actions with $128$ dimensions, as in the original LAPO. However, for reasons similar to quantization removal, we significantly increased it to $8192$, as it allows better next-observation prediction. Since IDM cannot distinguish control-related features from noise, the best we can hope for in general is to learn the full dynamics of the environment as accurately as possible. In such a case, latent actions will by definition contain true actions and we will be able to extract them via the probe. This change gives an additional 2.5x improvement. 

% However, this does not automatically mean better downstream performance for a number of reasons that we will discuss later. 

\begin{figure*}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\textwidth]{figures/final_result_comb.pdf}}
        \caption{Performance evaluation of latent action learning approaches and baselines across different budgets of ground-truth action labels. As can be seen, LAPO struggles in the presence of distractors, being outperformed by simpler baselines. LAOM, our modification of LAPO, performs better, but not significantly. However, when we reuse the same labels used for decoding from latent to true actions to provide supervision during LAOM training (see \Cref{exp:laom-supervision}), we significantly improve downstream performance, outperforming baselines in all environments. Importantly, all methods were pre-trained on the same unlabeled datasets and had access to exactly the same action labels, differing only in their use. Results are averaged over three random seeds. For a detailed description of the evaluation pipeline, see \Cref{exp:setup}.}
        \label{fig:final-res}
    \end{center}
    \vskip -0.4in
\end{figure*}

\begin{figure*}[t]
    \vskip 0.2in
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{figures/lapo_vs_idm_gen.pdf}}
        \caption{Generalization to new distractors}
        \label{fig:idm-gen}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{figures/lapo_plus_act_mse_res.pdf}}
        \caption{Latent actions quality vs dimensionality}
        \label{fig:laom-act-mse}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{figures/lapo_plus_act_res.pdf}}
        \caption{Performance vs actions dimensionality}
        \label{fig:laom-act-score}
    \end{subfigure}
    \caption{(a) We show that latent action learning with supervision generalizes better than IDM to novel distractors for all considered budgets of ground-truth action labels available for pre-training. (b)-(c) Supervision with a small number of ground-truth actions during latent action learning allows for smaller action dimensionality without major performance degradation. Without supervision, the quality of latent actions, as well as performance, quickly degrades.}
    \label{fig:}
    \vskip -0.2in
\end{figure*}

\emph{Removing observation reconstruction.} The need to fully reconstruct the next observation forces latent actions to encapsulate changes in each pixel, which is not always related to true actions, e.g. video in the background. Thus, we use the latent temporal consistency loss \citep{schwarzer2020data, hansen2022temporal, zhao2023simplified} to predict next observation in compact latent space without reconstruction. IDM and FMD now operate on latent representation and consist of MLPs instead of ResNets (see \Cref{fig:lapo-arc-viz}). This brings additional benefits, as with such architecture we can get rid of expensive decoder, reducing model size and increasing training speed. For target next observation we use simple stop-grad as in \citet{chen2021exploring} or EMA encoder \citep{schwarzer2020data}. These change alone slightly increases probe MSE due to the instabilities. We fix them with the next change.  

\emph{Adding augmentations.} Augmentations are commonly used in conjunction with self-supervised objectives to stabilize training and avoid collapse \citep{schwarzer2020data, hansen2022temporal, zhao2023simplified}. Similarly, we found that augmentations help with stability and improve performance to even smaller probe MSE. We use the subset of augmentations from \citet{almuzairee2024recipe}, which consists of random shifts, rotations and changes of perspective. We apply then only during latent actions training and do not use in later stages. 

\textbf{The large gap in downstream performance remains.} 
% The question remains whether such a large increase in the quality of latent actions measured by probes (see \Cref{fig:lapo-improvements}) will transfer to downstream performance after LAM pre-training. 
As \Cref{fig:lapo-not-work} shows, our improvements partially transfer to downstream performance, as LAOM outperforms vanilla LAPO on all label budgets, improving performance by up to 2x. LAOM also outperforms LAPO on data without distractors, but not significantly. However, there remains a large gap in final performance with and without distractors. We should emphasize that this gap is not due to the fact that setting with distractors is more difficult for BC, for example. We normalize performance by the return achieved by BC trained on each full dataset with ground-truth actions. Thus, the difference in performance is relative to BC and is explained by a difference in the quality of the latent actions.

% To answer this question, we follow the three-stage pipeline from \citep{schmidt2023learning, ye2024latent}. After pre-training LAPO and LAOM on both datasets: with and without distractors, we train BC to imitate these latent actions on the same datasets. Finally, we reveal a small amount of true actions (no more than 2.5\% of the full dataset) and fine-tune a small decoder head on the main policy to map from latent to true actions. 

Unfortunately, linear probing has a major limitation - it can only tell us whether real actions are contained in latent actions or not. For example, by increasing the dimensionality of latent actions in LAOM, we have improved the quality according to the probe, but sacrificed their minimality, i.e. they additionally describe full dynamics, that is mostly unrelated to real actions. This can be detrimental as, during the BC stage, not only do we waste capacity predicting actions with higher dimensionality, but we also risk learning spurious correlations. This is probably the main reason for the poor performance, but it is the best we can do, otherwise latent actions will not contain true actions at all.

\section{Latent Action Learning Requires Supervision in the Presence of Distractors}
\label{exp:laom-supervision}

In previous sections, we proposed LAOM, an improved version of LAPO which almost doubled the downstream performance in the presence of distractors for all budgets of true action labels considered. However, overall performance remained quite low. Similar to unlikelihood of recovering the control-endogenous minimal state in the presence of distractors \citep{misra2024towards}, our results suggest that  without any supervision latent action learning may not be able to learn actions useful for efficient pre-training. What if we can provide supervision? Even the smallest number of true actions may ground latent action learning to focus on control-related features. We explore this in the following experiments.

% and initiate bootstrapping cycle: the more .

% the closer the latent actions are to ground-truth, the more minimal the state becomes, and the more minimal the state is, the easier it is to learn latent actions close to the present

% With LAOM we did our best to ensure that latent actions will contain true actions, although with no guarantee of minimality.

\textbf{Supervision significantly increases downstream performance.} Despite the fact that existing approaches \citep{schmidt2023learning, ye2024latent, chen2024moto} pre-train LAM without true actions, in practice we still need to have some number of labels to learn the action decoder as last stage. We reuse these labels to provide supervision by linearly predicting them from latent actions during LAOM training (see \Cref{fig:lapo-arc-viz} for the final architecture). We plot the resulting downstream performance for each environment in \Cref{fig:final-res} and summarize in \Cref{fig:final-res-comb}. As can be seen, LAOM+supervision outperforms all baselines and scales better with a larger budget of real actions. It achieves an average normalized score of 0.44, i.e. it recovers almost half the performance of BC with access to the full dataset of true actions, while using only 2.5\% of the action labels. Importantly, all methods have access to exactly the same number of action labels, differing only in how they use them. We provide results for distractor-free data in \Cref{app:dist-free}. 

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=0.9\columnwidth]{figures/mixed-res.pdf}}
        \caption{Evaluation of latent action learning approaches in cross-embodied pre-training in the presence of distractors, e.g. pre-training LAM on datasets from three environments and fune-tuning on action labeled data from the remaining one. Supervision during latent action pre-training improves downstream performance. However, overall performance is comparable to that of a simple BC trained from scratch on available action labels. Results are averaged across all four environments, each with three random seeds.}
        \label{fig:diff-emb}
    \end{center}
    \vskip -0.4in
\end{figure}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=0.65\columnwidth]{figures/hopper-decoder.pdf}}
        \caption{In contrast to IDM, latent action learning encode a lot of control-unrelated information, such as background videos, into the observation representations. This finding suggest that using latent action learning exclusively as a way to pre-train visual representations is not viable in the presence of distractors. We visualize the representations by training a separate decoder to reconstruct original observations.}
        \label{fig:obs-dec}
    \end{center}
    \vskip -0.4in
\end{figure}


\textbf{Latent action learning with supervision generalizes better that IDM.} Learning to predict true actions with IDM with a small number of labels and then relabeling larger datasets has recently been a quite successful approach \citep{baker2022video, zheng2023semi}. Unfortunately, IDM is greatly limited in its generalization capabilites as dataset with labels may not contain some distractors or cover all actions. LAOM+supervision on other hand pre-trains on full combined dataset and can adapt better to larger variety of distractors and actions. We confirm this intuition in \Cref{fig:idm-gen} measuring action prediction accuracy on evaluation dataset with never seen distractor background videos. IDM indeed generalizes worse than LAOM+supervision.

\textbf{Supervision enables compact latent actions without large performance degradation.} As we mentioned earlier very high dimensional latent actions are not optimal, as they may not be minimal, i.e. contain control-unrelated information and require larger BC models to imitate accurately. Similarly, LAPA \citep{ye2024latent} also reported that more compact latent action space increases pre-training efficiency. Unfortunately, the effectiveness of LAPO and even LAOM degrades dramatically when the dimensionality of latent actions is reduced. In \Cref{fig:laom-act-mse} and \Cref{fig:laom-act-score} we show that supervision can partially mitigate this effect. LAOM+supervision loses only 16\% of performance when reducing latent actions dimensionality from 8192 to 64, compared to 63\% loss for LAOM. We used 128 labeled trajectories for this experiment.

\textbf{Supervision improves cross-embodied pre-training.} So far we have used homogeneous datasets, which contain data from only one environment. However, in practice our hope is to pre-train LAM on large and diverse dataset from different embodiments, including humans \citep{mccarthy2024towards, ye2024latent}. To access performance in such a scenario, we assemble cross-embodied datasets in a leave-one-out fashion, e.g. for the cheetah-run, we sample 1666 trajectories (to get $
\sim 5$k) from other environments and combine them into a single dataset. We pre-train LAM and BC on them as usual and use the labeled data from the excluded environment for action decoding or supervision during LAOM training. As \Cref{fig:diff-emb} shows supervision during LAM pre-training yields a large performance improvement. However, the final performance is no better than training BC only on the provided labels from scratch. This is slightly concerning and further emphasizes the limitations of LAM methods in the presence of distractors.

\textbf{In contrast to IDM, latent action learning does not learn minimal state.} DynaMo \citep{cui2024dynamo} used latent action learning only as an objective to pre-train visual representations, not to obtain useful latent actions. To access the viability of such approach, we additionally train decoders to reconstruct original observations from the representations learned by LAM and IDM. What information does LAM encodes into its representations? As \Cref{fig:obs-dec} shows decoders were able to reconstruct original observations quite well, indicating that both LAOM and LAOM+supervision encode a lot of control-unrelated information, including distractors. In contrast, multi-step IDM truly learns control-endogenous minimal state as predicted by \citet{lamb2022guaranteed, islam2022agent, levine2024multistep}, fully ignoring control-unrelated information, such as background videos or agent color. This result appears to provide compelling evidence that using LAM exclusively as a way to obtain visual representations is not a viable approach in the presence of distractors.

% \subsection{LAOM Generalizes better than IDM}

% \subsection{}
% \section{Ablation Studies}

% \begin{figure}[h]
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \centerline{\includegraphics[width=\columnwidth]{figures/lapo_res.pdf}}
%         \caption{TODO}
%         \label{fig:}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \centerline{\includegraphics[width=\columnwidth]{figures/lapo_improvements_res.pdf}}
%         \caption{TODO}
%         \label{fig:}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \centerline{\includegraphics[width=\columnwidth]{figures/lapo_vanilla_vs.pdf}}
%         \caption{TODO}
%         \label{fig:}
%     \end{subfigure}
%     \caption{}
%     \label{fig:}
% \end{figure}

% \subsection{LAPO Struggle in the Presence of Distractors}

% \textbf{Latent action learning with distractors does not work.}

\section{Related Work}

% , using only 2k hours of labeled data to re-label 270k hours of unlabeled gameplay
\textbf{Action relabeling with inverse dynamics models.} Simplest approach to utilize unlabeled data it to pretrain IDM on small number of action labels to further re-label a much large dataset \citep{torabi2018behavioral}. \citet{baker2022video} showed that this approach can work on a scale, achieving great success in Minecraft \citep{pmlr-v176-kanervisto22a}. \citet{zhang2022learning} used similar pipeline, unlocking hours of in-the-wild driving videos for pretraining. \citet{schmeckpeper2020reinforcement} used unlabeled human manipulation videos within online RL loop, which supplied labels to IDM for re-labeling. \citet{zheng2023semi} conducted large scale analysis of IDM re-labeling in offline RL setup, showing that only 10\% of suboptimal trajectories with labels is enough to match performance on fully labeled dataset.

 % and scales worse than latent action learning approaches \citep{schmidt2023learning}
In contrast to previous work \citep{schmeckpeper2020reinforcement, baker2022video, zheng2023semi}, we show that while IDM is a strong baseline in setups without distractors (see \Cref{fig:final-vanilla-res} in \Cref{app:dist-free}), it generalizes poorly when distractors are present. Our results show that when a small number of action labels are available, it is much better to combine IDM and latent action learning to achieve much stronger performance and generalization (see \Cref{fig:final-res}), suggesting that for web-scale data \citep{baker2022video, zhang2022learning} our approach may be better than simple IDM re-labeling. 

% Similar objectives has been used to improve video generation \citep{menapace2021playable} and training of world models \citep{ye2023become}. 
\textbf{Latent action learning.} To our knowledge, \citet{edwards2019imitating} was the first to propose the task of recovering latent actions and \emph{imitating latent policies from observation}, with limited success on simple problems. However, the original objective had scalability issues \citep{struckmeier2023preventing}. LAPO \citep{schmidt2023learning} greatly simplified the approach, removed scalability barriers, and for the first time achieved high success on the hard, procedurally generated ProcGen benchmark \citep{cobbe2020leveraging}. Latent action learning was further scaled by \citet{bruce2024genie, cui2024dynamo, ye2024latent, chen2024moto} to larger models, data, and harder, more diverse robotics domains. 
% Importantly, \citet{ye2024latent} first demonstrated that such approach can be used to pre-train large Vision-Language-Action Models (VLA) on only human manipulation videos outperforming models pre-trained on expert in-domain data and opening up the potential for leveraging web-scale videos. 

% TODO: rewrite it 
% Unfortunately, this is not true for real-world web-scale data as it contains a lot of action correlated noise, e.g. people moving in the background. 
In contrast to our work, all the mentioned approaches \citep{schmidt2023learning, ye2024latent, cui2024dynamo, chen2024moto} use data without distractors, where all changes in dynamics are mainly explained by ground truth actions only. As we show in our work (see \Cref{exp:lapo-laom}), naive latent action learning does not work in the presence of distractors. Although we propose improvements that double the performance, it is not enough (see \Cref{fig:lapo-not-work}). Providing supervision with a small number of action labels during LAM training significantly improves performance (see \Cref{fig:final-res-comb}), suggesting that the pipeline used in most current work \citep{ye2024latent, cui2024dynamo, chen2024moto} to first learn LAM and only then decode to ground-truth actions is suboptimal. 

The most closely related to us is the work of \citet{cui2024dynamo}, which also removes latent action quantization, the reconstruction objective in favor of latent temporal consistency \citep{schwarzer2020data, zhao2023simplified}, and provides ablation with ground-truth actions supervision during LAM training. However, they train LAM only as a way to pre-train visual representations and do not provide any analysis regarding the effect of their proposed changes on the quality of the resulting latent actions. This also explains why they report that supervision with true actions gives no improvement, while we show that it gives significant gains (see \Cref{fig:final-res-comb}). Moreover, visually reconstructing representations, we show that latent action learning methods do not produce control-endogenous state (see \Cref{fig:obs-dec}), and thus are probably not suitable as a method of visual representation learning in the presence of distractors.

\section{Conclusion}

In this work, we empirically investigated the effect of action-correlated distractors on latent action learning. We showed that LAPO struggles to learn latent actions useful for pre-training. Although we proposed LAOM, a simple modification of LAPO, which doubled performance, it did not fully close the gap with the distractor-free setting. Crucially, we found that even minimal supervision - reusing as little as 2.5\% of the dataset's ground-truth action labels during latent action learning significantly improved downstream performance, challenging the conventional pipeline of first pre-training LAM and only then decoding from latent to real actions. Our findings suggest that integrating supervision is essential for robust latent action learning in real-world scenarios, paving the way for unlocking the vast amounts of video data available on the web for embodied AI. We discuss the limitations of our work in the \Cref{app:limitations}.

% To summarize, our findings reveal serious limitations of latent action learning in the presence of distractors that need to be addressed by future research in order to successfully scale LAM on web-scale data.


% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.


% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Limitations}
\label{app:limitations}

There are several notable limitations to our work. First, although we used the Distracting Control Suite \citep{stone2021distracting}, which allows us to precisely control the difficulty of distractors in a convenient way and clearly access generalization to new distractors, the overall distribution and noise patterns may be quite different compared to real-world videos on the web. Thus, our conclusions may not be fully applicable, e.g. it is possible that supervision is not as important for relevant to embodied AI data, or vice versa, it may turn out to be much more necessary for good results than we have used. Nevertheless, we believe that the overall conclusion about the need for some form of supervision is quite general. 

Second, the need for supervision for latent action learning is a serious limitation, as compared to our setup, which is more reminiscent of Minecraft \citep{pmlr-v176-kanervisto22a} or Nethack \citep{hambro2022dungeons}, where both labeled and unlabeled data are available, we have no chance to get real labels for already existing videos on the web or to fully cover their diversity with hand-crafted labels. Therefore, further research is needed to find out whether pre-training LAM on web data combined with supervision on robot data will achieve a similar effect, although our preliminary experiment on cross-embodied pre-training is pessimistic. It is quite possible that supervision can come in other forms than ground-truth actions, as we simply need a way to ground latent actions on control-related features of the observations. For example, for egocentric videos \citep{grauman2022ego4d} we can use hand tracking as a proxy action to supervise latent action learning.

Finally, similar to offline RL \citep{levine2020offline}, the problem of hyperparameter tuning remains, since without action labels there is currently no way to access the quality of latent actions. 

\section{Additional Related Work}
% TODO: shorted it, to long!
\textbf{Learning with distractors.} Distractors in various forms are commonly used in many sub-fields of reinforcement learning, such as: visual model-based learning, model-free learning, and representation learning.

In model-based learning, researchers explore ways to efficiently train world models that do not waste their capacity to model task-irrelevant details, either via decomposing world models to predict relevant and irrelevant parts separately \citep{fu2021learning, wang2022denoised, wan2023semail, wang2024ad3} or by avoiding reconstructing observations \citep{okada2021dreaming, deng2022dreamerpro, NEURIPS2023_6692e1b0, liu2023learning, burchi2024mudreamer}. In our work, we have a similar need to not model action irrelevant details, as this will result in latent actions that describe changes in exogenous noise, not changes cased by ground truth actions. Thus, we use the commonly occurring latent temporal consistency loss \citep{schwarzer2020data, hansen2022temporal, zhao2023simplified}.

In model-free learning, researchers explore various techniques to improve generalization to new distractors and domain shifts \citep{hansen2021generalization, hansen2021stabilizing, bertoin2022look, NEURIPS2022_802a4350, batra2024zero, almuzairee2024recipe}, which often revolves around the use of augmentations \citep{ma2022comprehensive}. In our work we also use augmentations, specifically a subset of ones proposed by \citet{almuzairee2024recipe}, to stabilize LAM training with latent temporal consistency loss \citep{schwarzer2020data}. 

In representation learning, researchers search for ways to obtain minimal representations that contain only task- \citep{yamada2022task}, reward- \citep{pmlr-v216-zhou23a} or control-related information \citep{zhang2020learning, lamb2022guaranteed, liu2023robust, ni2024bridging, levine2024multistep}, as this can greatly increase sample efficiency and generalization \citep{kim2024investigating}.  In our work, inspired by \citet{lamb2022guaranteed}, we incorporate the multi-step IDM into LAM and show that it can help learn better latent actions in the presence of exogenous noise. Moreover, when small number of ground truth actions is available for pre-training (see \Cref{fig:final-res-comb}), our model on them conceptually reduces to one proposed by \citet{levine2024multistep}, for which it has been theoretically shown that it can recover control-endogenous minimal state. This may explain why incorporating labels during LAM pre-training, rather than during final fine-tuning, brings so much benefit, since discovering true actions is trivial given a minimal state. We however, found a contradicting evidence, as \Cref{fig:obs-dec} shows that our proposed methods do not learn minimal state in practice.

Overall, although we were inspired by existing approaches, they have not previously been used to improve latent action learning, especially in combination, which, as we show (see \Cref{fig:lapo-improvements}) is essential for good performance in the presence of distractors. 

% longer vesion
% \textbf{Learning with distractors.} Distractors in various forms are commonly used in many sub-fields of reinforcement and imitation learning to model domain shifts and study different aspects of zero-shot generalization \citep{batra2024zero}, robustness \citep{stone2021distracting}, or representation learning \citep{zhang2020learning}. In general, there are three research branches that are most relevant to our work: visual model-based learning, model-free learning, and representation learning. 

% In model-based learning, researchers explore ways to efficiently train world models that do not waste their capacity to model task-irrelevant details. The most common approaches are either to decompose world models to predict relevant and irrelevant parts separately \citep{fu2021learning, wang2022denoised, wan2023semail, wang2024ad3}, to avoid reconstructing observations, and to model the dynamics in a compact latent space through different variations of contrastive learning \citep{okada2021dreaming, deng2022dreamerpro, NEURIPS2023_6692e1b0, liu2023learning, burchi2024mudreamer} or to focus the model capacity on action-relevant features \citep{NEURIPS2023_797be96e, hutson2024policy}. In our work, we have a similar need to not model action irrelevant details when predicting the next state in the LAM, as this will result in latent actions that describe changes in exogenous noise, not changes cased by ground truth actions. Thus, we use the commonly occurring latent temporal consistency loss \citep{schwarzer2020data, hansen2022temporal, zhao2023simplified} to avoid reconstruction in observation space.

% In model-free learning, researchers explore various techniques to improve generalization to new distractors and domain shifts \citep{hansen2021generalization, hansen2021stabilizing, bertoin2022look, NEURIPS2022_802a4350, batra2024zero, almuzairee2024recipe}, which often revolves around the use of augmentations and the stabilization of learning under them \citep{ma2022comprehensive}. In our work we also use augmentations, specifically a subset of ones proposed by \citet{almuzairee2024recipe}, to stabilize LAM training with latent temporal consistency loss \citep{schwarzer2020data}. 

% In representation learning, researchers search for ways to obtain minimal representations that contain only task- \citep{yamada2022task}, reward- \citep{pmlr-v216-zhou23a} or control-related information \citep{lamb2022guaranteed}, as this can greatly increase sample efficiency and generalization \citep{kim2024investigating}. Most approaches rely on self-predictive representations \citep{ni2024bridging}, bisimulation metrics \citep{zhang2020learning, liu2023robust, liu2023towards} or inverse-dynamics modeling \citep{lamb2022guaranteed, levine2024multistep}. In our work, inspired by research on control-endogenous minimal state discovery \citep{lamb2022guaranteed}, we incorporate the multi-step IDM into LAM and show that it can help learn better latent actions in the presence of exogenous noise. Moreover, when small amount of ground truth actions is available for pre-training (see \Cref{exp:labels}), our model on them conceptually reduces to one proposed by \citet{levine2024multistep}, for which it has been theoretically shown that it can recover control-endogenous minimal state. This partly explains why incorporating labels during LAM pre-training, rather than during final fine-tuning, brings so much benefit, since discovering true actions is trivial given a minimal state.

% Overall, although we were inspired by existing approaches, they have not previously been used to improve latent action learning, espesially in combination, which, as we show (see \Cref{fig:lapo-improvements}) is essential for good performance in the presence of distractors. 

\section{Data Collection}
\label{app:data-collection}

We used environments from the Distracting Control Suite (DCS), wrapped with Shimmy wrappers for compatibility with the Gymnasium API. For cheetah-run, walker-run and hopper-hop we used PPO \citep{schulman2017proximal}, adapted from the CleanRL \citep{huang2022cleanrl} library. For humanoid-walk, we used SAC \citep{haarnoja2018soft} from the stable-baselines3 \citep{stable-baselines3} library, as PPO from CleanRL was not able to solve it at the expert level. We used default hyperparameters and trained on 1M transitions in each environment, except for humanoid-walk, where we trained on 100k transitions. Importantly, for speed, all experts were trained with proprioceptive states and no distractors, we later rendered proprioceptive states to 64px images with or without distractors during data collection. For each environment, we collected 5k trajectories, with an additional 50 trajectories for evaluation with novel distractor videos (from the evaluation set in the DCS). As each trajectory consists of 1000 steps, the datasets contain 5M transitions. We include ground truth actions and states for debugging purposes. The datasets will be released together with the main code repository.

\begin{table}[h]
    \caption{Datasets statistics.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
        % \begin{sc}
            \begin{tabular}{l|r|r}
                \toprule
                Dataset & Average Return & Size (GB) \\
                \midrule
                cheetah-run       & 837.70 & 57.7 \\
                walker-run        & 739.79 & 57.8 \\
                hopper-hop        & 306.63 & 57.6 \\
                humanoid-walk     & 617.22 & 58.9 \\
                \bottomrule
            \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\section{Implementation Details}

All experiments were run on H100 GPUs, in single-gpu mode and PyTorch bf16 precision with AMP. For the visual encoder, we used ResNets from the open-source LAPO \citep{schmidt2023learning} codebase, which also borrowed from baselines originally provided as part of the \href{https://www.aicrowd.com/challenges/neurips-2020-procgen-competition}{ProcGen 2020} competition. For the action decoder, we used a two-layer MLP with 256 hidden dimensions and ReLU activations. 

In contrast to the commonly used cosine similarity, we used MSE for temporal consistency loss. We also found that projection heads degraded performance, so we did not use them. We use slightly non-standard MLP for latent IDM and FDM: we compose it from multiple MLP blocks inspired by Transformer architecture \citep{vaswani2017attention} and condition on latent action and observation representation on all layers instead of just the first. We have found that this greatly improves prediction, especially for latent actions. We also use ReLU6 activations instead of GELU, as it naturally bounds the activations, which helps with stability during training, similar to target networks in RL \citep{bhatt2019crossq}. Without supervision, we use the EMA target encoder. With supervision, we find that a simple stop-grad is sufficient to prevent any signs of collapse, a finding also reported by \citet{schwarzer2020data}.  

For all experiments we use cosine learning late schedule with warmup. We will publicly release the code, all configs and all Weights\&Biases logs after the review. For hyperparameters see \Cref{app:hps}.

\begin{table}[h]
    \caption{Methods training time summed from all stages (including online evaluation) for each method.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
        % \begin{sc}
            \begin{tabular}{l|r}
                \toprule
                Method & Training Time \\
                \midrule
                LAPO             & $\sim$ 7h 38m  \\
                LAOM             & $\sim$ 6h 43m  \\
                LAOM+supervision & $\sim$ 7h 6m  \\
                BC               & $\sim$ 1h 10m  \\
                IDM              & $\sim$ 5h 30m \\
                \bottomrule
            \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}


\section{Evaluation Details}
\label{app:baselines}

We outline the evaluation procedures used in our experiments for each method. First, we review the general setup. For each environment, we have a large dataset without action labels, with and without distractors. To decode the learned latent actions to ground truth for evaluation, we allow a small amount of action labeled data, in line with previous work \citep{schmidt2023learning, ye2024latent}. We sample it once from the existing dataset, revealing true actions, to ensure that all methods are on equal conditions. We use identical backbones where possible, and try our best to make all methods equal in the number of trainable weights. For hyperparameters, see \Cref{app:hps}. We report the scores achieved by BC trained on datasets with all actions revealed in \Cref{table:bc-scores}. We use these for normalization in all our experiments.

\textbf{BC.} We trained BC from scratch to predict ground-truth actions on available labels, i.e. on 2 or 128 trajectories.

\textbf{IDM.} We used two-staged pipeline. First, we trained IDM to predict actions on available labels, i.e. on 2 trajectories. Then, we trained BC on full unlabeled dataset, providing labels via pre-trained IDM. We report BC final return. 

\textbf{LAPO and LAOM.} We used three-stage pipline. First, we pre-train latent actions on full unlabeled datasets. Then, we trained BC, providing latent action labels via pre-trained LAM. Finally, we trained action decoder on small amount of labels, while freezing the rest of the policy weights.

\textbf{LAOM+supervision.} Almost like LAOM, with the difference being that we exactly aligned stages in terms of action labels used. While in LAOM we can pre-train it once and then re-use for later stages regardless of the number of action labels, in LAOM+supervision we trained separate LAM for each budget of labels. Thus, for LAOM+supervision trained with supervision from 32 trajectories of labels, on final stage the decoder was trained only on the same 32 trajectories. We repeat this process for all cases, from 2 to 128 trajectories.

\begin{table}[h]
    \caption{Evaluation returns of BC trained on full datasets with ground-truth actions revealed. We use them for normalization.}
    \label{table:bc-scores}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
        % \begin{sc}
            \begin{tabular}{l|r|r}
                \toprule
                Dataset & With distractors & Without distractors \\
                \midrule
                cheetah-run       & 823 & 840 \\
                walker-run        & 749 & 735 \\
                hopper-hop        & 253 & 300 \\
                humanoid-walk     & 428 & 601 \\
                \bottomrule
            \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\begin{table}[h]
    \caption{Total parameters for each method according to the hyperparameters used in \cref{app:hps}.}
    \label{table:bc-scores}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
        % \begin{sc}
            \begin{tabular}{l|r|r}
                \toprule
                Dataset & Total Parameters \\
                \midrule
                LAPO               & 211847849 \\
                LAOM               & 192307136 \\
                LAOM+supervision   & 192479189 \\
                BC (on all stages)    & 107541504 \\
                IDM                & 192258965 \\
                \bottomrule
            \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}


% todo:
% provide somewhere scores for normalization
% provide total number of parameters for each method

\clearpage
\section{Additional Figures}
\label{app:dist-free}

\begin{figure*}[h]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\textwidth]{figures/final_result_vanilla_comb.pdf}}
        \caption{Main results without distractors, analogously to our main result in \Cref{fig:final-res}. As can be seen, supervision help even without distractors, although all methods work good in this setting. Notably, IDM is a strong baseline.}
        \label{fig:final-vanilla-res-comb}
    \end{center}
    \vskip -0.2in
\end{figure*}

\begin{figure*}[h]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=0.5\columnwidth]{figures/final-vanilla-averaged-res.pdf}}
        \caption{Figure summarizing the results from \Cref{fig:final-vanilla-res-comb}, analogously to our main result in \Cref{fig:final-res-comb}. As can be seen, supervision help even without distractors, although all methods work good in this setting.}
        \label{fig:final-vanilla-res}
    \end{center}
    \vskip -0.2in
\end{figure*}

\begin{figure*}[h!]
    \vskip 0.2in
    \begin{center}
        \centerline{\includegraphics[width=\textwidth]{figures/mix-res-comb.pdf}}
        \caption{Mixed-embodied pre-training experiment results for each environment. For details see \Cref{fig:mix-res-comb}.}
        \label{fig:mix-res-comb}
    \end{center}
    \vskip -0.2in
\end{figure*}

\clearpage
\section{Hyperparameters}
\label{app:hps}
% lapo, laom, laom+supervision, idm, bc



\begin{table}[h]
    \caption{LAPO hyperparameters. We use the same hyperparameters for all experiments and explicitly mention any exceptions. Names are exactly follow the configuration files used in code.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
            % \begin{sc}
                \begin{tabular}{l|l|r}
                    \toprule
                    \textbf{Stage} & \textbf{Parameter} & \textbf{Value} \\
                    \midrule
                    \multirow{12}{*}{Latent actions learning}
                        & grad\_norm & None \\
                        & batch\_size & 512 \\
                        & num\_epochs & 10 \\
                        & frame\_stack & 3 \\
                        & encoder\_deep & False \\
                        & weight\_decay & None \\
                        & encoder\_scale & 6 \\
                        & learning\_rate & 0.0001 \\
                        & warmup\_epochs & 3 \\
                        & future\_obs\_offset & 10 \\
                        & latent\_action\_dim & 8192 \\
                        & encoder\_num\_res\_blocks & 2 \\
                    \midrule
                    \multirow{11}{*}{Latent behavior cloning}
                        & dropout & 0.0 \\
                        & use\_aug & False \\
                        & batch\_size & 512 \\
                        & num\_epochs & 10 \\
                        & frame\_stack & 3 \\
                        & encoder\_deep & False \\
                        & weight\_decay & None \\
                        & encoder\_scale & 32 \\
                        & learning\_rate & 0.0001 \\
                        & warmup\_epochs & 0 \\
                        & encoder\_num\_res\_blocks & 2 \\
                    \midrule
                    \multirow{8}{*}{Latent actions decoding} 
                        & use\_aug & False \\
                        & batch\_size & 512 \\
                        & hidden\_dim & 256 \\
                        & weight\_decay & None \\
                        & eval\_episodes & 25 \\
                        & learning\_rate & 0.0003 \\
                        & total\_updates & 2500 \\
                        & warmup\_epochs & 0.0 \\
                    \bottomrule
                \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\begin{table}[h]
    \caption{LAOM hyperparameters. We use the same hyperparameters for all experiments and explicitly mention any exceptions. Names are exactly follow the configuration files used in code.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
            % \begin{sc}
                \begin{tabular}{l|l|r}
                    \toprule
                    \textbf{Stage} & \textbf{Parameter} & \textbf{Value} \\
                    \midrule
                    \multirow{21}{*}{Latent actions learning}
                        & use\_aug & True \\
                        & grad\_norm & None \\
                        & batch\_size & 512 \\
                        & num\_epochs & 10 \\
                        & target\_tau & 0.001 \\
                        & frame\_stack & 3 \\
                        & act\_head\_dim & 1024 \\
                        & encoder\_deep & False \\
                        & obs\_head\_dim & 1024 \\
                        & weight\_decay & None \\
                        & encoder\_scale & 6 \\
                        & learning\_rate & 0.0001 \\
                        & warmup\_epochs & 3 \\
                        & encoder\_dropout & 0.0 \\
                        & act\_head\_dropout & 0.0 \\
                        & encoder\_norm\_out & False \\
                        & obs\_head\_dropout & 0.0 \\
                        & future\_obs\_offset & 10 \\
                        & latent\_action\_dim & 8192 \\
                        & target\_update\_every & 1 \\
                        & encoder\_num\_res\_blocks & 2 \\
                    \midrule
                    \multirow{11}{*}{Latent behavior cloning}
                        & dropout & 0.0 \\
                        & use\_aug & False \\
                        & batch\_size & 512 \\
                        & num\_epochs & 10 \\
                        & frame\_stack & 3 \\
                        & encoder\_deep & False \\
                        & weight\_decay & None \\
                        & encoder\_scale & 32 \\
                        & learning\_rate & 0.0001 \\
                        & warmup\_epochs & 0.0 \\
                        & encoder\_num\_res\_blocks & 2 \\
                    \midrule
                    \multirow{8}{*}{Latent actions decoding} 
                        & use\_aug & False \\
                        & batch\_size & 512 \\
                        & hidden\_dim & 256 \\
                        & weight\_decay & None \\
                        & eval\_episodes & 25 \\
                        & learning\_rate & 0.0003 \\
                        & total\_updates & 2500 \\
                        & warmup\_epochs & 0 \\
                    \bottomrule
                \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\begin{table}[h]
    \caption{LAOM+supervision hyperparameters. We use the same hyperparameters for all experiments and explicitly mention any exceptions. Names are exactly follow the configuration files used in code.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
            % \begin{sc}
                \begin{tabular}{l|l|r}
                    \toprule
                    \textbf{Stage} & \textbf{Parameter} & \textbf{Value} \\
                    \midrule
                    \multirow{23}{*}{Latent actions learning}
                        & use\_aug & True \\
                        & grad\_norm & None \\
                        & batch\_size & 512 \\
                        & num\_epochs & 10 \\
                        & target\_tau & 0.001 \\
                        & frame\_stack & 3 \\
                        & act\_head\_dim & 1024 \\
                        & encoder\_deep & False \\
                        & obs\_head\_dim & 1024 \\
                        & weight\_decay & 0.0 \\
                        & encoder\_scale & 6 \\
                        & learning\_rate & 0.0001 \\
                        & warmup\_epochs & 3 \\
                        & encoder\_dropout & 0.0 \\
                        & act\_head\_dropout & 0.0 \\
                        & encoder\_norm\_out & False \\
                        & obs\_head\_dropout & 0.0 \\
                        & future\_obs\_offset & 10 \\
                        & labeled\_loss\_coef & 0.01 (0.001, cheetah-run) \\
                        & latent\_action\_dim & 8192 \\
                        & labeled\_batch\_size & 128 \\
                        & target\_update\_every & 1 \\
                        & encoder\_num\_res\_blocks & 2 \\
                    \midrule
                    \multirow{11}{*}{Latent behavior cloning}
                        & dropout & 0.0 \\
                        & use\_aug & False \\
                        & batch\_size & 512 \\
                        & num\_epochs & 10 \\
                        & frame\_stack & 3 \\
                        & encoder\_deep & False \\
                        & weight\_decay & None \\
                        & encoder\_scale & 32 \\
                        & learning\_rate & 0.0001 \\
                        & warmup\_epochs & 0 \\
                        & encoder\_num\_res\_blocks & 2 \\
                    \midrule
                    \multirow{8}{*}{Latent actions decoding} 
                        & use\_aug & False \\
                        & batch\_size & 512 \\
                        & hidden\_dim & 256 \\
                        & weight\_decay & 0 \\
                        & eval\_episodes & 25 \\
                        & learning\_rate & 0.0003 \\
                        & total\_updates & 2500 \\
                        & warmup\_epochs & 0 \\
                    \bottomrule
                \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\begin{table}[h]
    \caption{IDM hyperparameters. We use the same hyperparameters for all experiments and explicitly mention any exceptions. Names are exactly follow the configuration files used in code.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
            % \begin{sc}
                \begin{tabular}{l|l|r}
                    \toprule
                    \textbf{Stage} & \textbf{Parameter} & \textbf{Value} \\
                    \midrule
                    \multirow{15}{*}{IDM learning}
                        & use\_aug & False \\
                        & grad\_norm & None \\
                        & batch\_size & 512 \\
                        & frame\_stack & 3 \\
                        & act\_head\_dim & 1024 \\
                        & encoder\_deep & False \\
                        & weight\_decay & None \\
                        & encoder\_scale & 12 \\
                        & learning\_rate & 0.0001 \\
                        & total\_updates & 10000 \\
                        & warmup\_epochs & 3 \\
                        & encoder\_dropout & 0.0 \\
                        & act\_head\_dropout & 0.0 \\
                        & future\_obs\_offset & 1 \\
                        & encoder\_num\_res\_blocks & 2 \\
                    \midrule
                    \multirow{11}{*}{Behavior cloning on IDM actions}
                        & dropout & 0.0 \\
                        & use\_aug & False \\
                        & batch\_size & 512 \\
                        & num\_epochs & 10 \\
                        & frame\_stack & 3 \\
                        & encoder\_deep & False \\
                        & weight\_decay & None \\
                        & encoder\_scale & 32 \\
                        & eval\_episodes & 25 \\
                        & learning\_rate & 0.0001 \\
                        & warmup\_epochs & 0 \\
                        & encoder\_num\_res\_blocks & 2 \\
                    \bottomrule
                \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\begin{table}[h]
    \caption{BC as baseline hyperparameters. We use the same hyperparameters for all experiments and explicitly mention any exceptions. Names are exactly follow the configuration files used in code.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
            % \begin{sc}
                \begin{tabular}{l|r}
                    \toprule
                    \textbf{Parameter} & \textbf{Value} \\
                    \midrule
                        dropout & 0.0 \\
                         use\_aug & false \\
                         batch\_size & 512 \\
                         frame\_stack & 3 \\
                         encoder\_deep & false \\
                         weight\_decay & 0 \\
                         encoder\_scale & 32 \\
                         eval\_episodes & 25 \\
                         learning\_rate & 0.0001 \\
                         total\_updates & 10000 \\
                         warmup\_epochs & 0 \\
                         cooldown\_ratio & 0 \\
                         encoder\_num\_res\_blocks & 2 \\
                    \bottomrule
                \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\begin{table}[h]
    \caption{BC for normalization hyperparameters. We use the same hyperparameters for all experiments and explicitly mention any exceptions. Names are exactly follow the configuration files used in code.}
    \label{sample-table}
    \vskip 0.15in
    \begin{center}
        % \begin{small}
            % \begin{sc}
                \begin{tabular}{l|r}
                    \toprule
                    \textbf{Parameter} & \textbf{Value} \\
                    \midrule
                        dropout & 0.0 \\
                         use\_aug & false \\
                         batch\_size & 512 \\
                         frame\_stack & 3 \\
                         encoder\_deep & false \\
                         weight\_decay & 0 \\
                         encoder\_scale & 32 \\
                         eval\_episodes & 25 \\
                         learning\_rate & 0.0001 \\
                         num\_epochs & 10 \\
                         warmup\_epochs & 0 \\
                         cooldown\_ratio & 0 \\
                         encoder\_num\_res\_blocks & 2 \\
                    \bottomrule
                \end{tabular}
            % \end{sc}
        % \end{small}
    \end{center}
    \vskip -0.1in
\end{table}


\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
