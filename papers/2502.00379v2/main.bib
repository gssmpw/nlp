@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12104--12113},
  year={2022}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{agarwal2024many,
  title={Many-shot in-context learning},
  author={Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Rosias, Luis and Chan, Stephanie and Zhang, Biao and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and others},
  journal={arXiv preprint arXiv:2404.11018},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={International Conference on Machine Learning},
  pages={7480--7512},
  year={2023},
  organization={PMLR}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{kovalevskiy2024alphafold,
  title={AlphaFold two years on: Validation and impact},
  author={Kovalevskiy, Oleg and Mateos-Garcia, Juan and Tunyasuvunakool, Kathryn},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={34},
  pages={e2315002121},
  year={2024},
  publisher={National Academy of Sciences}
}

@article{madan2024foundation,
  title={Foundation Models for Video Understanding: A Survey},
  author={Madan, Neelu and M{\o}gelmose, Andreas and Modi, Rajat and Rawat, Yogesh S and Moeslund, Thomas B},
  journal={arXiv preprint arXiv:2405.03770},
  year={2024}
}

@article{defossez2024moshi,
  title={Moshi: a speech-text foundation model for real-time dialogue},
  author={D{\'e}fossez, Alexandre and Mazar{\'e}, Laurent and Orsini, Manu and Royer, Am{\'e}lie and P{\'e}rez, Patrick and J{\'e}gou, Herv{\'e} and Grave, Edouard and Zeghidour, Neil},
  journal={arXiv preprint arXiv:2410.00037},
  year={2024}
}

@article{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@article{brohan2023rt,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  journal={arXiv preprint arXiv:2307.15818},
  year={2023}
}

@article{o2023open,
  title={Open x-embodiment: Robotic learning datasets and rt-x models},
  author={O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and others},
  journal={arXiv preprint arXiv:2310.08864},
  year={2023}
}

@article{sartor2024neural,
  title={Neural Scaling Laws for Embodied AI},
  author={Sartor, Sebastian and Thompson, Neil},
  journal={arXiv preprint arXiv:2405.14005},
  year={2024}
}

@article{pearce2024scaling,
  title={Scaling Laws for Pre-training Agents and World Models},
  author={Pearce, Tim and Rashid, Tabish and Bignell, Dave and Georgescu, Raluca and Devlin, Sam and Hofmann, Katja},
  journal={arXiv preprint arXiv:2411.04434},
  year={2024}
}

@article{hilton2023scaling,
  title={Scaling laws for single-agent reinforcement learning},
  author={Hilton, Jacob and Tang, Jie and Schulman, John},
  journal={arXiv preprint arXiv:2301.13442},
  year={2023}
}

@article{springenberg2024offline,
  title={Offline actor-critic reinforcement learning scales to large models},
  author={Springenberg, Jost Tobias and Abdolmaleki, Abbas and Zhang, Jingwei and Groth, Oliver and Bloesch, Michael and Lampe, Thomas and Brakel, Philemon and Bechtle, Sarah and Kapturowski, Steven and Hafner, Roland and others},
  journal={arXiv preprint arXiv:2402.05546},
  year={2024}
}

@article{kim2024openvla,
  title={OpenVLA: An Open-Source Vision-Language-Action Model},
  author={Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others},
  journal={arXiv preprint arXiv:2406.09246},
  year={2024}
}

@article{khazatsky2024droid,
  title={Droid: A large-scale in-the-wild robot manipulation dataset},
  author={Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and others},
  journal={arXiv preprint arXiv:2403.12945},
  year={2024}
}

@article{team2024octo,
  title={Octo: An open-source generalist robot policy},
  author={Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and others},
  journal={arXiv preprint arXiv:2405.12213},
  year={2024}
}

@article{guruprasad2024benchmarking,
  title={Benchmarking Vision, Language, \& Action Models on Robotic Learning Tasks},
  author={Guruprasad, Pranav and Sikka, Harshvardhan and Song, Jaewoo and Wang, Yangyue and Liang, Paul Pu},
  journal={arXiv preprint arXiv:2411.05821},
  year={2024}
}

@article{lin2024data,
  title={Data scaling laws in imitation learning for robotic manipulation},
  author={Lin, Fanqi and Hu, Yingdong and Sheng, Pingyue and Wen, Chuan and You, Jiacheng and Gao, Yang},
  journal={arXiv preprint arXiv:2410.18647},
  year={2024}
}

@article{paglieri2024balrog,
  title={Balrog: Benchmarking agentic llm and vlm reasoning on games},
  author={Paglieri, Davide and Cupia{\l}, Bart{\l}omiej and Coward, Samuel and Piterbarg, Ulyana and Wolczyk, Maciej and Khan, Akbir and Pignatelli, Eduardo and Kuci{\'n}ski, {\L}ukasz and Pinto, Lerrel and Fergus, Rob and others},
  journal={arXiv preprint arXiv:2411.13543},
  year={2024}
}

@article{soldaini2024dolma,
  title={Dolma: An open corpus of three trillion tokens for language model pretraining research},
  author={Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and others},
  journal={arXiv preprint arXiv:2402.00159},
  year={2024}
}

@article{tao2024maniskill3,
  title={Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai},
  author={Tao, Stone and Xiang, Fanbo and Shukla, Arth and Qin, Yuzhe and Hinrichsen, Xander and Yuan, Xiaodi and Bao, Chen and Lin, Xinsong and Liu, Yulin and Chan, Tse-kai and others},
  journal={arXiv preprint arXiv:2410.00425},
  year={2024}
}

@article{chi2024universal,
  title={Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots},
  author={Chi, Cheng and Xu, Zhenjia and Pan, Chuer and Cousineau, Eric and Burchfiel, Benjamin and Feng, Siyuan and Tedrake, Russ and Song, Shuran},
  journal={arXiv preprint arXiv:2402.10329},
  year={2024}
}

@article{cheng2024open,
  title={Open-television: Teleoperation with immersive active visual feedback},
  author={Cheng, Xuxin and Li, Jialong and Yang, Shiqi and Yang, Ge and Wang, Xiaolong},
  journal={arXiv preprint arXiv:2407.01512},
  year={2024}
}

@article{yang2024video,
  title={Video as the New Language for Real-World Decision Making},
  author={Yang, Sherry and Walker, Jacob and Parker-Holder, Jack and Du, Yilun and Bruce, Jake and Barreto, Andre and Abbeel, Pieter and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2402.17139},
  year={2024}
}

@article{mccarthy2024towards,
  title={Towards Generalist Robot Learning from Internet Video: A Survey},
  author={McCarthy, Robert and Tan, Daniel CH and Schmidt, Dominik and Acero, Fernando and Herr, Nathan and Du, Yilun and Thuruthel, Thomas G and Li, Zhibin},
  journal={arXiv preprint arXiv:2404.19664},
  year={2024}
}

@article{misra2024towards,
  title={Towards Principled Representation Learning from Videos for Reinforcement Learning},
  author={Misra, Dipendra and Saran, Akanksha and Xie, Tengyang and Lamb, Alex and Langford, John},
  journal={arXiv preprint arXiv:2403.13765},
  year={2024}
}

@article{batra2024zero,
  title={Zero-Shot Generalization of Vision-Based RL Without Data Augmentation},
  author={Batra, Sumeet and Sukhatme, Gaurav S},
  journal={arXiv preprint arXiv:2410.07441},
  year={2024}
}

@inproceedings{hansen2021generalization,
  title={Generalization in reinforcement learning by soft data augmentation},
  author={Hansen, Nicklas and Wang, Xiaolong},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={13611--13617},
  year={2021},
  organization={IEEE}
}

@article{hansen2021stabilizing,
  title={Stabilizing deep q-learning with convnets and vision transformers under data augmentation},
  author={Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={3680--3693},
  year={2021}
}

@article{almuzairee2024recipe,
  title={A Recipe for Unbounded Data Augmentation in Visual Reinforcement Learning},
  author={Almuzairee, Abdulaziz and Hansen, Nicklas and Christensen, Henrik I},
  journal={arXiv preprint arXiv:2405.17416},
  year={2024}
}

@article{zhang2022light,
  title={Light-weight probing of unsupervised representations for reinforcement learning},
  author={Zhang, Wancong and GX-Chen, Anthony and Sobal, Vlad and LeCun, Yann and Carion, Nicolas},
  journal={arXiv preprint arXiv:2208.12345},
  year={2022}
}

@article{stone2021distracting,
  title={The Distracting Control Suite--A Challenging Benchmark for Reinforcement Learning from Pixels},
  author={Stone, Austin and Ramirez, Oscar and Konolige, Kurt and Jonschkowski, Rico},
  journal={arXiv preprint arXiv:2101.02722},
  year={2021}
}

@inproceedings{hansen2021softda,
  title={Generalization in Reinforcement Learning by Soft Data Augmentation},
  author={Nicklas Hansen and Xiaolong Wang},
  booktitle={International Conference on Robotics and Automation},
  year={2021},
}

@article{ortiz2024dmc,
  title={DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors},
  author={Ortiz, Joseph and Dedieu, Antoine and Lehrach, Wolfgang and Guntupalli, Swaroop and Wendelken, Carter and Humayun, Ahmad and Zhou, Guangyao and Swaminathan, Sivaramakrishnan and L{\'a}zaro-Gredilla, Miguel and Murphy, Kevin},
  journal={arXiv preprint arXiv:2409.18330},
  year={2024}
}

@article{zhang2020learning,
  title={Learning invariant representations for reinforcement learning without reconstruction},
  author={Zhang, Amy and McAllister, Rowan and Calandra, Roberto and Gal, Yarin and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.10742},
  year={2020}
}

@article{ma2022comprehensive,
  title={A comprehensive survey of data augmentation in visual reinforcement learning},
  author={Ma, Guozheng and Wang, Zhen and Yuan, Zhecheng and Wang, Xueqian and Yuan, Bo and Tao, Dacheng},
  journal={arXiv preprint arXiv:2210.04561},
  year={2022}
}

@article{bertoin2022look,
  title={Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning},
  author={Bertoin, David and Zouitine, Adil and Zouitine, Mehdi and Rachelson, Emmanuel},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30693--30706},
  year={2022}
}

@inproceedings{NEURIPS2022_802a4350,
 author = {Huang, Yangru and Peng, Peixi and Zhao, Yifan and Chen, Guangyao and Tian, Yonghong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {20393--20406},
 publisher = {Curran Associates, Inc.},
 title = {Spectrum Random Masking for Generalization in Image-based Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/802a4350ca4fced76b13b8b320af1543-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{wan2023semail,
  title={SeMAIL: eliminating distractors in visual imitation via separated models},
  author={Wan, Shenghua and Wang, Yucen and Shao, Minghao and Chen, Ruying and Zhan, De-Chuan},
  booktitle={International Conference on Machine Learning},
  pages={35426--35443},
  year={2023},
  organization={PMLR}
}

@inproceedings{fu2021learning,
  title={Learning task informed abstractions},
  author={Fu, Xiang and Yang, Ge and Agrawal, Pulkit and Jaakkola, Tommi},
  booktitle={International Conference on Machine Learning},
  pages={3480--3491},
  year={2021},
  organization={PMLR}
}

@inproceedings{deng2022dreamerpro,
  title={Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations},
  author={Deng, Fei and Jang, Ingook and Ahn, Sungjin},
  booktitle={International conference on machine learning},
  pages={4956--4975},
  year={2022},
  organization={PMLR}
}

@article{wang2024ad3,
  title={AD3: Implicit Action is the Key for World Models to Distinguish the Diverse Visual Distractors},
  author={Wang, Yucen and Wan, Shenghua and Gan, Le and Feng, Shuai and Zhan, De-Chuan},
  journal={arXiv preprint arXiv:2403.09976},
  year={2024}
}

@article{burchi2024mudreamer,
  title={MuDreamer: Learning Predictive World Models without Reconstruction},
  author={Burchi, Maxime and Timofte, Radu},
  journal={arXiv preprint arXiv:2405.15083},
  year={2024}
}

@inproceedings{okada2021dreaming,
  title={Dreaming: Model-based reinforcement learning by latent imagination without reconstruction},
  author={Okada, Masashi and Taniguchi, Tadahiro},
  booktitle={2021 ieee international conference on robotics and automation (icra)},
  pages={4209--4215},
  year={2021},
  organization={IEEE}
}

@article{wang2022denoised,
  title={Denoised mdps: Learning world models better than the world itself},
  author={Wang, Tongzhou and Du, Simon S and Torralba, Antonio and Isola, Phillip and Zhang, Amy and Tian, Yuandong},
  journal={arXiv preprint arXiv:2206.15477},
  year={2022}
}

@inproceedings{NEURIPS2023_6692e1b0,
 author = {Zhu, Chuning and Simchowitz, Max and Gadipudi, Siri and Gupta, Abhishek},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {32445--32467},
 publisher = {Curran Associates, Inc.},
 title = {RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6692e1b0e8a31e8de84bd90ad4d8d9e0-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{liu2023learning,
  title={Learning world models with identifiable factorization},
  author={Liu, Yuren and Huang, Biwei and Zhu, Zhengmao and Tian, Honglong and Gong, Mingming and Yu, Yang and Zhang, Kun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={31831--31864},
  year={2023}
}

@article{hutson2024policy,
  title={Policy-shaped prediction: avoiding distractions in model-based reinforcement learning},
  author={Hutson, Miles and Kauvar, Isaac and Haber, Nick},
  journal={arXiv preprint arXiv:2412.05766},
  year={2024}
}

@inproceedings{NEURIPS2023_797be96e,
 author = {Tomar, Manan and Islam, Riashat and Taylor , Matthew  and Levine, Sergey and Bachman, Philip},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {38624--38641},
 publisher = {Curran Associates, Inc.},
 title = {Ignorance is Bliss: Robust Control via Information Gating},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/797be96e4481c3fe5d675c1ba5352969-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@article{schwarzer2020data,
  title={Data-efficient reinforcement learning with self-predictive representations},
  author={Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R Devon and Courville, Aaron and Bachman, Philip},
  journal={arXiv preprint arXiv:2007.05929},
  year={2020}
}

@inproceedings{zhao2023simplified,
  title={Simplified temporal consistency reinforcement learning},
  author={Zhao, Yi and Zhao, Wenshuai and Boney, Rinu and Kannala, Juho and Pajarinen, Joni},
  booktitle={International Conference on Machine Learning},
  pages={42227--42246},
  year={2023},
  organization={PMLR}
}

@article{hansen2022temporal,
  title={Temporal difference learning for model predictive control},
  author={Hansen, Nicklas and Wang, Xiaolong and Su, Hao},
  journal={arXiv preprint arXiv:2203.04955},
  year={2022}
}

@article{kim2024investigating,
  title={Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning},
  author={Kim, Donghu and Lee, Hojoon and Lee, Kyungmin and Hwang, Dongyoon and Choo, Jaegul},
  journal={arXiv preprint arXiv:2406.06037},
  year={2024}
}

@article{ni2024bridging,
  title={Bridging State and History Representations: Understanding Self-Predictive RL},
  author={Ni, Tianwei and Eysenbach, Benjamin and Seyedsalehi, Erfan and Ma, Michel and Gehring, Clement and Mahajan, Aditya and Bacon, Pierre-Luc},
  journal={arXiv preprint arXiv:2401.08898},
  year={2024}
}

@inproceedings{liu2023robust,
  title={Robust representation learning by clustering with bisimulation metrics for visual reinforcement learning with distractions},
  author={Liu, Qiyuan and Zhou, Qi and Yang, Rui and Wang, Jie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={7},
  pages={8843--8851},
  year={2023}
}

@article{liu2023towards,
  title={Towards Control-Centric Representations in Reinforcement Learning from Images},
  author={Liu, Chen and Zang, Hongyu and Li, Xin and Heng, Yong and Wang, Yifei and Fang, Zhen and Wang, Yisen and Wang, Mingzhong},
  journal={arXiv preprint arXiv:2310.16655},
  year={2023}
}


@InProceedings{pmlr-v216-zhou23a,
  title = 	 {Learning robust representation for reinforcement learning with distractions by reward sequence prediction},
  author =       {Zhou, Qi and Wang, Jie and Liu, Qiyuan and Kuang, Yufei and Zhou, Wengang and Li, Houqiang},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {2551--2562},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/zhou23a/zhou23a.pdf},
  url = 	 {https://proceedings.mlr.press/v216/zhou23a.html},
  abstract = 	 {Reinforcement learning algorithms have achieved remarkable success in acquiring behavioral skills directly from pixel inputs. However, their application in real-world scenarios presents challenges due to their sensitivity to visual distractions (e.g., changes in viewpoint and light). A key factor contributing to this challenge is that the learned representations often suffer from overfitting task-irrelevant information. By comparing several representation learning methods, we find that the key to alleviating overfitting in representation learning is to choose proper prediction targets. Motivated by our comparison, we propose a novel representation learning approach—namely, reward sequence prediction (RSP)—that uses reward sequences or their transforms (e.g., discrete time Fourier transform) as prediction targets. RSP can efficiently learn robust representations as reward sequences rarely contain task-irrelevant information while providing a large number of supervised signals to accelerate representation learning. An appealing feature is that RSP makes no assumption about the type of distractions and thus can improve performance even when multiple types of distractions exist. We evaluate our approach in Distracting Control Suite. Experiments show that our method achieves state-of-the-art sample efficiency and generalization ability in tasks with distractions.}
}

@article{yamada2022task,
  title={Task-induced representation learning},
  author={Yamada, Jun and Pertsch, Karl and Gunjal, Anisha and Lim, Joseph J},
  journal={arXiv preprint arXiv:2204.11827},
  year={2022}
}

@article{lamb2022guaranteed,
  title={Guaranteed discovery of control-endogenous latent states with multi-step inverse models},
  author={Lamb, Alex and Islam, Riashat and Efroni, Yonathan and Didolkar, Aniket and Misra, Dipendra and Foster, Dylan and Molu, Lekan and Chari, Rajan and Krishnamurthy, Akshay and Langford, John},
  journal={arXiv preprint arXiv:2207.08229},
  year={2022}
}

@article{levine2024multistep,
  title={Multistep Inverse Is Not All You Need},
  author={Levine, Alexander and Stone, Peter and Zhang, Amy},
  journal={arXiv preprint arXiv:2403.11940},
  year={2024}
}

@inproceedings{edwards2019imitating,
  title={Imitating latent policies from observation},
  author={Edwards, Ashley and Sahni, Himanshu and Schroecker, Yannick and Isbell, Charles},
  booktitle={International conference on machine learning},
  pages={1755--1763},
  year={2019},
  organization={PMLR}
}

@misc{
struckmeier2023preventing,
title={Preventing Mode Collapse When Imitating Latent Policies from Observations},
author={Oliver Struckmeier and Ville Kyrki},
year={2023},
url={https://openreview.net/forum?id=Mf9fQ0OgMzo}
}

@inproceedings{menapace2021playable,
  title={Playable video generation},
  author={Menapace, Willi and Lathuiliere, Stephane and Tulyakov, Sergey and Siarohin, Aliaksandr and Ricci, Elisa},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10061--10070},
  year={2021}
}

@inproceedings{
ye2023become,
title={Become a Proficient Player with Limited Data through Watching Pure Videos},
author={Weirui Ye and Yunsheng Zhang and Pieter Abbeel and Yang Gao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Sy-o2N0hF4f}
}

@article{schmidt2023learning,
  title={Learning to act without actions},
  author={Schmidt, Dominik and Jiang, Minqi},
  journal={arXiv preprint arXiv:2312.10812},
  year={2023}
}

@inproceedings{cobbe2020leveraging,
  title={Leveraging procedural generation to benchmark reinforcement learning},
  author={Cobbe, Karl and Hesse, Chris and Hilton, Jacob and Schulman, John},
  booktitle={International conference on machine learning},
  pages={2048--2056},
  year={2020},
  organization={PMLR}
}

@article{cui2024dynamo,
  title={DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control},
  author={Cui, Zichen Jeff and Pan, Hengkai and Iyer, Aadhithya and Haldar, Siddhant and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2409.12192},
  year={2024}
}

@article{ye2024latent,
  title={Latent action pretraining from videos},
  author={Ye, Seonghyeon and Jang, Joel and Jeon, Byeongguk and Joo, Sejune and Yang, Jianwei and Peng, Baolin and Mandlekar, Ajay and Tan, Reuben and Chao, Yu-Wei and Lin, Bill Yuchen and others},
  journal={arXiv preprint arXiv:2410.11758},
  year={2024}
}

@article{chen2024moto,
  title={Moto: Latent Motion Token as the Bridging Language for Robot Manipulation},
  author={Chen, Yi and Ge, Yuying and Li, Yizhuo and Ge, Yixiao and Ding, Mingyu and Shan, Ying and Liu, Xihui},
  journal={arXiv preprint arXiv:2412.04445},
  year={2024}
}

@inproceedings{bruce2024genie,
  title={Genie: Generative interactive environments},
  author={Bruce, Jake and Dennis, Michael D and Edwards, Ashley and Parker-Holder, Jack and Shi, Yuge and Hughes, Edward and Lai, Matthew and Mavalankar, Aditi and Steigerwald, Richie and Apps, Chris and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{baker2022video,
  title={Video pretraining (vpt): Learning to act by watching unlabeled online videos},
  author={Baker, Bowen and Akkaya, Ilge and Zhokov, Peter and Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and Houghton, Brandon and Sampedro, Raul and Clune, Jeff},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24639--24654},
  year={2022}
}


@InProceedings{pmlr-v176-kanervisto22a,
  title = 	 {MineRL Diamond 2021 Competition: Overview, Results, and Lessons Learned},
  author =       {Kanervisto, Anssi and Milani, Stephanie and Ramanauskas, Karolis and Topin, Nicholay and Lin, Zichuan and Li, Junyou and Shi, Jianing and Ye, Deheng and Fu, Qiang and Yang, Wei and Hong, Weijun and Huang, Zhongyue and Chen, Haicheng and Zeng, Guangjun and Lin, Yue and Micheli, Vincent and Alonso, Eloi and Fleuret, Fran\c{c}ois and Nikulin, Alexander and Belousov, Yury and Svidchenko, Oleg and Shpilman, Aleksei},
  booktitle = 	 {Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track},
  pages = 	 {13--28},
  year = 	 {2022},
  editor = 	 {Kiela, Douwe and Ciccone, Marco and Caputo, Barbara},
  volume = 	 {176},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--14 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v176/kanervisto22a/kanervisto22a.pdf},
  url = 	 {https://proceedings.mlr.press/v176/kanervisto22a.html},
  abstract = 	 {Reinforcement learning competitions advance the field by providing appropriate scope and support to develop solutions toward a specific problem. To promote the development of more broadly applicable methods, organizers need to enforce the use of general techniques, the use of sample-efficient methods, and the reproducibility of the results. While beneficial for the research community, these restrictions come at a cost—increased difficulty. If the barrier for entry is too high, many potential participants are demoralized. With this in mind, we hosted the third edition of the MineRL ObtainDiamond competition, MineRL Diamond 2021, with a separate track in which we permitted any solution to promote the participation of newcomers. With this track and more extensive tutorials and support, we saw an increased number of submissions. The participants of this easier track were able to obtain a diamond, and the participants of the harder track progressed the generalizable solutions in the same task.}
}

@article{torabi2018behavioral,
  title={Behavioral cloning from observation},
  author={Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  journal={arXiv preprint arXiv:1805.01954},
  year={2018}
}

@inproceedings{zhang2022learning,
  title={Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining},
  author={Zhang, Qihang and Peng, Zhenghao and Zhou, Bolei},
  booktitle={European Conference on Computer Vision},
  pages={111--128},
  year={2022},
  organization={Springer}
}

@article{schmeckpeper2020reinforcement,
  title={Reinforcement learning with videos: Combining offline observations with interaction},
  author={Schmeckpeper, Karl and Rybkin, Oleh and Daniilidis, Kostas and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2011.06507},
  year={2020}
}

@inproceedings{zheng2023semi,
  title={Semi-supervised offline reinforcement learning with action-free trajectories},
  author={Zheng, Qinqing and Henaff, Mikael and Amos, Brandon and Grover, Aditya},
  booktitle={International conference on machine learning},
  pages={42339--42362},
  year={2023},
  organization={PMLR}
}

@article{tomar2021learning,
  title={Learning Representations for Pixel-based Control: What Matters and Why?},
  author={Tomar, Manan and Mishra, Utkarsh A and Zhang, Amy and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2111.07775},
  year={2021}
}

@article{aytar2018playing,
  title={Playing hard exploration games by watching youtube},
  author={Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Thomas and Wang, Ziyu and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{torabi2019recent,
  title={Recent advances in imitation learning from observation},
  author={Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  journal={arXiv preprint arXiv:1905.13566},
  year={2019}
}

@inproceedings{ghosh2023reinforcement,
  title={Reinforcement learning from passive data via latent intentions},
  author={Ghosh, Dibya and Bhateja, Chethan Anand and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={11321--11339},
  year={2023},
  organization={PMLR}
}

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{shah2020pitfalls,
  title={The pitfalls of simplicity bias in neural networks},
  author={Shah, Harshay and Tamuly, Kaustav and Raghunathan, Aditi and Jain, Prateek and Netrapalli, Praneeth},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9573--9585},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{mentzer2023finite,
  title={Finite scalar quantization: Vq-vae made simple},
  author={Mentzer, Fabian and Minnen, David and Agustsson, Eirikur and Tschannen, Michael},
  journal={arXiv preprint arXiv:2309.15505},
  year={2023}
}

@article{scannell2024iqrl,
  title={iQRL--Implicitly Quantized Representations for Sample-efficient Reinforcement Learning},
  author={Scannell, Aidan and Kujanp{\"a}{\"a}, Kalle and Zhao, Yi and Nakhaei, Mohammadreza and Solin, Arno and Pajarinen, Joni},
  journal={arXiv preprint arXiv:2406.02696},
  year={2024}
}

@inproceedings{chen2021exploring,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15750--15758},
  year={2021}
}

@article{hambro2022dungeons,
  title={Dungeons and data: A large-scale nethack dataset},
  author={Hambro, Eric and Raileanu, Roberta and Rothermel, Danielle and Mella, Vegard and Rockt{\"a}schel, Tim and K{\"u}ttler, Heinrich and Murray, Naila},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24864--24878},
  year={2022}
}

@article{islam2022agent,
  title={Agent-controller representations: Principled offline rl with rich exogenous information},
  author={Islam, Riashat and Tomar, Manan and Lamb, Alex and Efroni, Yonathan and Zang, Hongyu and Didolkar, Aniket and Misra, Dipendra and Li, Xin and Van Seijen, Harm and Combes, Remi Tachet des and others},
  journal={arXiv preprint arXiv:2211.00164},
  year={2022}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@article{huang2022cleanrl,
  title={Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms},
  author={Huang, Shengyi and Dossa, Rousslan Fernand Julien and Ye, Chang and Braga, Jeff and Chakraborty, Dipam and Mehta, Kinal and Ara{\~A}{\v{s}}jo, Jo{\~A}{\c{G}}o GM},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={274},
  pages={1--18},
  year={2022}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18995--19012},
  year={2022}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{bhatt2019crossq,
  title={CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity},
  author={Bhatt, Aditya and Palenicek, Daniel and Belousov, Boris and Argus, Max and Amiranashvili, Artemij and Brox, Thomas and Peters, Jan},
  journal={arXiv preprint arXiv:1902.05605},
  year={2019}
}