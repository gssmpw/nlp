\section{Related Work}
% , using only 2k hours of labeled data to re-label 270k hours of unlabeled gameplay
\textbf{Action relabeling with inverse dynamics models.} Simplest approach to utilize unlabeled data it to pretrain IDM on small number of action labels to further re-label a much large dataset \citep{torabi2018behavioral}. \citet{baker2022video} showed that this approach can work on a scale, achieving great success in Minecraft \citep{pmlr-v176-kanervisto22a}. \citet{zhang2022learning} used similar pipeline, unlocking hours of in-the-wild driving videos for pretraining. \citet{schmeckpeper2020reinforcement} used unlabeled human manipulation videos within online RL loop, which supplied labels to IDM for re-labeling. \citet{zheng2023semi} conducted large scale analysis of IDM re-labeling in offline RL setup, showing that only 10\% of suboptimal trajectories with labels is enough to match performance on fully labeled dataset.

 % and scales worse than latent action learning approaches \citep{schmidt2023learning}
In contrast to previous work \citep{schmeckpeper2020reinforcement, baker2022video, zheng2023semi}, we show that while IDM is a strong baseline in setups without distractors (see \Cref{fig:final-vanilla-res} in \Cref{app:dist-free}), it generalizes poorly when distractors are present. Our results show that when a small number of action labels are available, it is much better to combine IDM and latent action learning to achieve much stronger performance and generalization (see \Cref{fig:final-res}), suggesting that for web-scale data \citep{baker2022video, zhang2022learning} our approach may be better than simple IDM re-labeling. 

% Similar objectives has been used to improve video generation \citep{menapace2021playable} and training of world models \citep{ye2023become}. 
\textbf{Latent action learning.} To our knowledge, \citet{edwards2019imitating} was the first to propose the task of recovering latent actions and \emph{imitating latent policies from observation}, with limited success on simple problems. However, the original objective had scalability issues \citep{struckmeier2023preventing}. LAPO \citep{schmidt2023learning} greatly simplified the approach, removed scalability barriers, and for the first time achieved high success on the hard, procedurally generated ProcGen benchmark \citep{cobbe2020leveraging}. Latent action learning was further scaled by \citet{bruce2024genie, cui2024dynamo, ye2024latent, chen2024moto} to larger models, data, and harder, more diverse robotics domains. 
% Importantly, \citet{ye2024latent} first demonstrated that such approach can be used to pre-train large Vision-Language-Action Models (VLA) on only human manipulation videos outperforming models pre-trained on expert in-domain data and opening up the potential for leveraging web-scale videos. 

% TODO: rewrite it 
% Unfortunately, this is not true for real-world web-scale data as it contains a lot of action correlated noise, e.g. people moving in the background. 
In contrast to our work, all the mentioned approaches \citep{schmidt2023learning, ye2024latent, cui2024dynamo, chen2024moto} use data without distractors, where all changes in dynamics are mainly explained by ground truth actions only. As we show in our work (see \Cref{exp:lapo-laom}), naive latent action learning does not work in the presence of distractors. Although we propose improvements that double the performance, it is not enough (see \Cref{fig:lapo-not-work}). Providing supervision with a small number of action labels during LAM training significantly improves performance (see \Cref{fig:final-res-comb}), suggesting that the pipeline used in most current work \citep{ye2024latent, cui2024dynamo, chen2024moto} to first learn LAM and only then decode to ground-truth actions is suboptimal. 

The most closely related to us is the work of \citet{cui2024dynamo}, which also removes latent action quantization, the reconstruction objective in favor of latent temporal consistency \citep{schwarzer2020data, zhao2023simplified}, and provides ablation with ground-truth actions supervision during LAM training. However, they train LAM only as a way to pre-train visual representations and do not provide any analysis regarding the effect of their proposed changes on the quality of the resulting latent actions. This also explains why they report that supervision with true actions gives no improvement, while we show that it gives significant gains (see \Cref{fig:final-res-comb}). Moreover, visually reconstructing representations, we show that latent action learning methods do not produce control-endogenous state (see \Cref{fig:obs-dec}), and thus are probably not suitable as a method of visual representation learning in the presence of distractors.