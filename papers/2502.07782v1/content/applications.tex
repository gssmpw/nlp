\section{Applications}\label{sec:apps}
Before moving on to experimental results, we specify applications of Flag Decomposition (FD), which enables reconstruction in the presence of data corruption, visualization, classification, and a novel prototype and distance for few-shot learning. 

\subsection{Reconstruction}
Consider the matrix $\D$ with an associated column hierarchy $\cA_1 \subset \cA_2 \subset \cdots\subset  \cA_k$. Suppose we have a corrupted version $\tilde{\mathbf{D}}$ and the feature hierarchy is known a priori. We use FD to recover $\D$ from $\tilde{\D}$. For example, $\tilde{\D}$ could be a (pixels $\times$ bands) flattened hyperspectral image with a hierarchy on the bands (see~\cref{ex:spectrum_hierarchy} and~\cref{fig:concept}) with outlier bands or additive noise. Another example includes a hierarchy of subjects: with images of subject $1$ inside those of subjects $1$ \& $2$ (see~\cref{fig:flag cartoon2}). A reconstruction using FD respects this hierarchy by preserving the identities of the two subjects.
\begin{figure}[t!]
    \centering
    \vspace{-4mm}
    \includegraphics[width=\linewidth]{figures/flag_faces.pdf}
    \footnotesize
    \caption{Images from the YFB~\cite{belhumeur1997eigenfaces} are flattened and horizontally stacked into $\D$. We use the hierarchy with the images of the first subject as $\cA_1$ and all images as $\cA_2$. We run FD (flag type $(1,2)$) and baselines (rank $2$). FD is the only method to correctly reconstruct the subjects. We plot the basis vectors (eigenfaces) on the right and find FD extracts basis elements that most closely resemble the subjects.}
    \vspace{-6mm}
    \label{fig:flag cartoon2}
\end{figure}


Suppose FD is computed by running~\algname~ on $\tilde{\D}$ to output $\Q, \bR, \bP$. Then we reconstruct $\hat{\D} = \Q \bR \bP^\top \approx \D$. This is a \textit{low-rank} reconstruction of $\tilde{\D}$ when $n_k < \mathrm{rank}(\tilde{\D})$. \emph{Unlike other reconstruction algorithms this application preserves the column hierarchy.}

% In addition, we reconstruct $\D$ using a \emph{low-rank} $\hat{\D}$ while preserving the column hierarchy of $\D$. This is done by running~\algname~on $\D$ with its column hierarchy and flag type $(n_1,n_2,\dots,n_k;n)$ with $n_k < p$ to output $\Q, \bR, \bP$. Then $\hat{\D} = \Q \bR \bP^\top \approx \D$ is a reconstruction of rank $n_k < p$.

%Baselines are chosen as SVD \& IRLS-SVD since SVD is a standard denoising algorithm, IRLS-SVD is robust to outliers, and SVD has been used for flag recovery~\cite{ma2022self}. 




\subsection{Leveraging the geometry of flags}
Consider a collection of $\mathcal{D} = \{\D^{(j)} \in \R^{n \times p_j}\}_{j=1}^N$ with column hierarchies $\cA_1^{(j)} \subset \cA_2^{(j)} \subset \cdots \subset \cA_k^{(j)}$. 
%Note that two different elements of $\mathcal{D}$ need not have the same column hierarchy. 
For example, $\mathcal{D}$ could be a collection of (band  $\times$ pixel) hyperspectral image patches. After choosing one flag type $(n_1,n_2,\dots,n_k;n)$ with $n_k \leq \mathrm{min}(p_1,\dots,p_N)$, we can use~\algname~with this flag type on each $\D^{(j)}$ to extract the collection of flags $ \mathcal{Q} = \{[\![\Q^{(j)}]\!]\}_{j=1}^N \subset \flag(n_1,n_2,\dots,n_k;n)$. Now, we can use chordal distance on the flag manifold $\flag(n_1,n_2,\dots,n_k;n)$ or the product of Grassmannians $Gr(m_1,n) \times Gr(m_2,n) \times \cdots \times Gr(m_k,n)$ to build an $N \times N$ distance matrix and run multidimensional scaling (MDS)~\cite{kruskal1978multidimensional} to visualize $\mathcal{D}$ or $k$-nearest neighbors~\cite{marrinan2021minimum} to cluster $\mathcal{D}$ (see~\cref{fig:concept}). Other clustering algorithms like $k$-means can also be used for clustering with means on products of Grassmannians (\eg,~\cite{fletcher2009geometric, draper2014flag, mankovich2022flag, mankovich2023subspace}) or chordal flag averages (\eg,~\cite{Mankovich_2023_ICCV}). Additionally, we can generate intermediate flags by sampling along geodesics between flags in $\mathcal{Q}$ using tools like {\tt manopt}~\cite{boumal2014manopt,townsend2016pymanopt} for exploration of the flag manifold between data samples.
% \tolga{what are flags between $\mathcal{Q}$?} \nate{re-worded above.}

\subsection{Few-shot learning}
In few-shot learning, a model is trained on very few labeled examples, a.k.a. `shots' from each class to make accurate predictions. 
Suppose we have a pre-trained feature extractor $f_\Theta: \mathcal{X} \rightarrow \R^{n}$, parameterized by $\Theta$. 
In few-shot learning, the number of classes in the training set is referred to as `ways.' We denote the feature representation of $s$ shots in class $c$ as $f_\Theta(\bm{x}_{c,1}),f_\Theta(\bm{x}_{c,2}),\dots, f_\Theta(\bm{x}_{c,s})$ where $\bm{x}_{c,i} \in \mathcal{X}$. The `support' set is the set of all shots from all classes. A few-shot learning architecture contains a method for determining a class representative (a.k.a. `prototype') in the feature space ($\R^n$) for each class using its shots. A test sample (`query') is then passed through the encoder, and the class of the nearest prototype determines its class. Overall, a classical few-shot learning architecture is comprised of three (differentiable) pieces: (1) a mapping of shots in the feature space to prototypes, (2) a measure of distance between prototypes and queries, and (3) a loss function for fine-tuning the pre-trained encoder.


\begin{figure}[t]
        \centering
        \includegraphics[width=\linewidth, trim= 38mm 0mm  38mm 0mm, clip]{figures/nn_hierarchy_more.pdf}
        \caption{To perform few shot-learning, we embed all $s=3$ shots ($\bm{x}_1, \bm{x}_2, \bm{x}_3$) of one class into one flag using FD. The light shapes are pre-trained and frozen feature extractors.}
        \label{fig:fewshot_flags}
    \end{figure}
\paragraph{Flag classifiers}~\label{sec:fewshot_flags} 
Take a feature extractor that decomposes into $k \geq 2$ functions. Specifically, $f_{\Theta} = f_{\Theta}^{(k)} \circ \cdots \circ f_{\Theta}^{(1)}:\mathcal{X} \rightarrow \R^n$,
where each $f_{\Theta}^{(i)}$ maps to $\R^n$.
%via flags with $k$ nested subspaces. 
We can generalize~\cref{ex:feature_hierarchy} to construct a $k$-part hierarchical data matrix. For simplicity, we consider the case where $k =2$. After constructing a data matrix with a corresponding column hierarchy, we use~\algname~to represent the support of one class, $c$, in the feature space as $[\![\Q^{(c)}]\!]\in \flag(n_1,n_2; n)$ (see~\cref{fig:fewshot_flags}). Now, each subspace $[\Q^{(c)}_1]$ and $[\Q^{(c)}_2]$ represents the features extracted by $f_{\Theta}^{(1)}$ and $f_{\Theta}$, respectively.

    Given a flag-prototype $[\![\Q^{(c)}]\!] \in \flag(n_1,n_2;n)$ and query $\left\{f^{(1)}_{\Theta}(\bm{x}), f_{\Theta}(\bm{x})\right\} \subset\R^n$, we measure distance between the them as
    % \begin{align}\label{eq: fewshot_dist}
    %     \begin{aligned}
    %     &\left \| f_{\Theta}^{(1)}(\bm{x}) - \Q^{(c)}_1 {\Q^{(c)}_1}^\top f_{\Theta}^{(1)}(\bm{x}) \right \|_2^2 \\
    %     &+ \left \| f_{\Theta}(\bm{x}) - \Q^{(c)}_2 {\Q^{(c)}_2}^\top f_{\Theta}(\bm{x}) \right \|_2^2
    %     \end{aligned}
    % \end{align}
    \begin{equation}\label{eq: fewshot_dist}
        \left \| \boldsymbol{\Pi}_{{\Q^{(c)}_1}^\perp}f_{\Theta}^{(1)}(\bm{x}) \right \|_2^2 + \left \| \boldsymbol{\Pi}_{{\Q^{(c)}_2}^\perp}f_{\Theta}(\bm{x}) \right \|_2^2
    \end{equation}
    where $\boldsymbol{\Pi}_{{\Q^{(c)}_i}^\perp} = \I - \Q^{(c)}_i {\Q^{(c)}_i}^\top$ for $i=1,2$.
    % \begin{equation}\label{eq: fewshot_dist}
    %     \left \| f_{\Theta}^{(1)}(\bm{x}) - \Q^{(c)}_1 {\Q^{(c)}_1}^\top f_{\Theta}^{(1)}(\bm{x}) \right \|_2^2
    %     + \left \| f_{\Theta}(\bm{x}) - \Q^{(c)}_2 {\Q^{(c)}_2}^\top f_{\Theta}(\bm{x}) \right \|_2^2
    % \end{equation}
    This is proportional to the squared chordal distance on $\flag(1,2;n)$ when the matrix $\left[ f_{\Theta}^{(1)}(\bm{x})| f_{\Theta}(\bm{x}) \right]$ is in Stiefel coordinates (\eg, has orthonormal columns). 
    %This happens exactly when the range of $f^{(1)}_{\Theta}$ is orthogonal to the range of $f_{\Theta}$. In this case, normalizing $f^{(1)}_{\Theta}(\bm{x})$ and $f_{\Theta}(\bm{x})$ creates a Stiefel coordinate representative.

    Flag classifiers are \emph{fully differentiable} enabling fine-tuning of the feature extractor with a flag classifier loss. We leave this as an avenue for future work.
    %So fine-tuning of the feature extractor can be done with any differentiable loss function (e.g., cross-entropy). 
    % It can also be adapted to address problems in continual learning. This is yet another promising direction for future work.
\begin{comment}
Take a feature extractor that decomposes into $k \geq 2$ functions. Specifically,
    \begin{equation}
        f_{\Theta} = f_{\Theta}^{(1)} \circ \cdots \circ f_{\Theta}^{(k)}:\mathcal{X} \rightarrow \R^n,
    \end{equation} 
    where each $f_{\Theta}^{(i)}$ maps to $\R^n$.
    %via flags with $k$ nested subspaces. 
    We generalize~\cref{ex:feature_hierarchy} to construct a $k$-part hierarchical data matrix. For simplicity, we will consider the case where $k =2$ even though the following procedure generalizes for $k$-part hierarchies. After constructing a data matrix with a corresponding column hierarchy, we use~\algname~to represent the support of one class, $c$, in the feature space as $[\![\Q^{(c)}]\!]\in \flag(n_1,n_2; n)$ (see~\cref{fig:fewshot_flags}). Now, each subspace $[\Q^{(c)}_1]$ and $[\Q^{(c)}_2]$ represents the features extracted by $f_{\Theta}$ and $f_{\Theta}^{(2)}$, respectively.

    Given a flag-prototype $[\![\Q^{(c)}]\!] \in \flag(n_1,n_2;n)$ and a query $\left\{f_{\Theta}(\bm{x}), f^{(2)}_{\Theta}(\bm{x})\right\} \subset\R^n$, we measure distance between the query and the prototype flag as
    \begin{align}\label{eq: fewshot_dist}
        \begin{aligned}
        &\left \| f_{\Theta}(\bm{x}) - \Q^{(c)}_1 {\Q^{(c)}_1}^\top f_{\Theta}(\bm{x}) \right \|_2^2 \\
        &+ \left \| f_{\Theta}^{(2)}(\bm{x}) - \Q^{(c)}_2 {\Q^{(c)}_2}^\top f_{\Theta}^{(2)}(\bm{x}) \right \|_2^2
        \end{aligned}
    \end{align}
    This is proportional to the squared chordal distance on $\flag(1,2;n)$ when the matrix $[f_{\Theta}(\bm{x})| f^{(2)}_{\Theta}(\bm{x})]$ is in Stiefel coordinates (\eg, has orthonormal columns). This happens exactly when the range of $f_{\Theta}$ is orthogonal to the range of $f^{(2)}_{\Theta}$. In this case, normalizing $f_{\Theta}(\bm{x})$ and $f^{(2)}_{\Theta}(\bm{x})$ creates a Stiefel coordinate representative.

    Flag classifiers are \emph{fully differentiable}. So fine-tuning of the feature extractor can be done with any differentiable loss function (e.g., cross-entropy). We leave this as an avenue for future work.
    % It can also be adapted to address problems in continual learning. This is yet another promising direction for future work.    
\end{comment}



