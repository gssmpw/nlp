\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\input{preamble_rebuttal}

\begin{document}

\maketitle
\thispagestyle{empty}
\appendix

\noindent We thank the reviewers for their useful comments. We are delighted that the reviewers find our paper \textbf{really good} (\Rtwo), well-written (\Rone~\& \Rthree), clearly explained (\Rone~\& \Rthree), interesting (\Rtwo), self-consistent \textbf{with no theoretical issues} (\Rone), and contains a novel (\Rfour) \& \textbf{sound methodology} (\Rtwo) along with a clear empirical evaluation (\Rthree). 
We thank the reviewers for pointing out the typos and minor issues which we have addressed thoroughly.
We have also restructured the paper for increased clarity. 
We now address some common concerns before delving into reviewer-specific responses.


\noindent \textbf{Intuition} (\Rone, \Rtwo,~\& \Rfour). Ex. 2.1-3 are examples of hierarchies appearing in data. We provide new examples in~\cref{fig:flag cartoon1,fig:flag cartoon2} for more intuition behind our method.
\begin{figure}[ht!]
    \centering
    \vspace{-3mm}
    \includegraphics[width=\linewidth]{figures/advantage0.pdf}
    \vspace{-8mm}
    \caption{Flag recovery. Columns of $\D$ from $\cA_1$ are in blue and $\cA_2 \setminus \cA_1$ are in orange. The only method that recovers both the entire hierarchy-preserving flag is FD. SVD only recovers the plane and QR only the line. \tolga{what is this supposed to show?} \nate{The data matrix is generated so that data in $\cA_1$ are all spanned by a 1d subspace (blue line) and data in $\cA_2$ are all spanned by a 2d subspace (orange plane). So we have a data matrix $\D$ with an associated column hierarchy $\cA_1 \subset \cA_2$ and the flag (blue line inside orange plane). We run FD, SVD, and QR to try to recover the `hidden flag.' FD is the only method that correctly recovers the hidden flag.}}
    \vspace{-4mm}
    \label{fig:flag cartoon1}
\end{figure}



\noindent \textbf{SOTA baselines} (\Rone~\& \Rfour) 
{We would like to emphasize that our baseline methods, SVD and QR, along with the proposed FD, are fundamental to computer vision and remain widely used across various applications. While more modern, task-specific algorithms may exist for each experiment, our primary objective is to demonstrate the effectiveness of FD compared to the de facto standards, SVD and QR, in the context of hierarchical data. That said, our evaluations also take application-specific algorithms into account. In particular, our few-shot experiments leverage an FD-based flag classifier and compare it to the two standard few-shot learning prototypes (e.g., means and subspaces). Our clustering experiments compare different methods for extracting flags from data using the same flag clustering algorithm. Thus we compare to the standard flag-extraction methods: QR and SVD. We leave the novel design of application-specific architectures / algorithms that build upon the proposed decomposition for a future study.} \nate{I like the way you wrote this. We can probably shorten it if needed.}
% Our clustering experiments compare different methods for extracting flags from data using the same flag clustering algorithm. Thus we compare to the standard flag-extraction methods: QR and SVD. For the denoising experiments, we use SVD as a baseline due to its simplicity and to highlight the advantage of the hierarchical structure in the FD as a generalization of the SVD. Finally, in the fewshot, experiment we propose the flag classifier from FD and compare it to the two standard fewshot learning prototypes (e.g., means and subspaces). Overall, the purpose of this paper is mainly conceptual; we aim to propose and analyze the novel FD. Therefore, we leave an FD applications paper with extensive SOTA comparisons and experimentation for future work.  

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/flag_faces.pdf}
    \vspace{-6mm}
    \caption{Reconstruction. Images from the YaleFaceDatabase are flattened and horizontally stacked into $\D$. We use the hierarchy with the images of the first subject as $\cA_1$ and all images as $\cA_2$. We run FD (flag type $(1,2)$) and baselines (rank $2$). FD is the only method to correctly reconstruct the subjects. We plot the basis vectors (eigenfaces) on the right and find FD extracts basis elements that most closely resemble the two subjects.}
    \vspace{-6mm}
    \label{fig:flag cartoon2}
\end{figure}
\noindent \textbf{Few-shot experiments (\Rtwo)}
The reviewer is right that the flag classifiers have an advantage over the baselines because they see features $f_{\Theta}$ and $f_{\Theta}^{(2)}$. Our baselines used those features to remain true to the SoTA standards.
Note, the hierarchy in Ex. 2.3 also places more relevance on the features of $f_{\Theta}$ because these data ($\cA_1$) appear in every other set of the hierarchy. Additionally, FD best reconstructs in the ``first'' part of the column hierarchy (columns of the matrix in $\mathcal{A}_1$).
For a more fair comparison, we have now run our baselines on stacked features from both $f_{\Theta}$ and $f_{\Theta}^{(2)}$ leading only to a slight improvement in classification accuracy for the baselines, while both flag and subspace classifiers outperform prototypical nets.
We emphasize that this evaluation mostly shows the usability of our FD in practically-relevant tasks such as few-shot learning. We thank the reviewer for bringing this up and will add this discussion and more results in the main paper. We leave a further improved utility of FD in few-shot learning as a promising future research.

% This indicates that our proposed flag prototypes using the FD are competitive with the baseline methods. Therefore, the potential utility of FD in fewshot learning is a promising avenue for future research.

% The hierarchy in Ex. 2.3 places more relevance on the features extracted by $f_{\Theta}$ because these data ($\cA_1$) appear in every other set of the hierarchy. Additionally, FD best reconstructs in the ``first'' part of the column hierarchy (columns of the data matrix in $\mathcal{A}_1$). Standard fewshot learning uses only the features from $f_{\Theta}$. Therefore our baselines only use these features. \Rtwo~correctly notes that flag classifiers have an advantage over the baselines because they see features from $f_{\Theta}$ and $f_{\Theta}^{(2)}$. For a more fair comparison, we run our baselines on stacked features from both $f_{\Theta}$ and $f_{\Theta}^{(2)}$. This change improves classification accuracy for the baselines in almost all experiments resulting in similar performance between flag and subspace classifiers while both methods generally outperform prototypical networks. This indicates that our proposed flag prototypes using the FD are competitive with the baseline methods. Therefore, the potential utility of FD in fewshot learning is a promising avenue for future research.



\noindent \textbf{On GS and stability (\Rone)}.
Our algorithm utilizes modified GS (MGS) and Block MGS (BMGS) that are designed to be more stable than GS. We have restructured the methods and applications and added tables to our Appendix, highlighting the stability and hierarchy-preserving properties of~\algname~and FD. 
Our future work will use Housholder transforms to further increase stability. 

\noindent \textbf{On MLMD (Ong and Lustig 2017) (\Rfour)}. MLMD models $\D = \sum_i \X_i$ where each block low-rank matrix $\X_i$ models finer-grained features than $\X_{i+1}$ consisting of more blocks. Suppose $\D = [\B_1|\B_2]\in \R^{n \times p}$ is of rank $n_k$ with columns sorted according to the hierarchy $\cA_1 \subset \cA_2$. The FD with flag type $(n_1,n_2;n)$ is $\D = \Q \bR$ where $\Q = [\Q_1 | \Q_2] \in St(n_k,n)$, $\Q_1 \in \R^{n \times n_1}$, and $\bR$ is block upper triangular. FD does not seek block low-rank representations for different scales, rather it extracts a hierarchy-preserving flag $[\![\Q]\!] \in \flag(n_1,n_2;n)$. Moreover, MLMD partitions $\D$ into column blocks requiring the block partition $P_2$ to be an `order of magnitude' larger than $P_1$ ($1$st par. Sec. II). Hence, it can run only when $|\cA_1| = |\cA_2|$. FD is more general being free of this restriction. Suppose $|\cA_1| = |\cA_2|$. MLMD models $\D = \X_1 + \X_2$ with $\X_i = \sum_{b \in P_i} R_b (\U_b \boldsymbol{\Sigma}_b \V^\top_b)$ where $R_b$ is a block reshaper. The output would be $3$ bases (in each $\U_b$), two for the columns of $\B_1$ and $\B_2$, and one for all of $\D$. These are neither mutually orthogonal nor guaranteed to be hierarchy-preserving. FD outputs one basis in the columns of $\Q$ containing the hierarchy-preserving flag: $[\Q_1]=[\B_1]$, and $[\Q] = [\D]$. We thank the reviewer for bringing MLMD to our attention, which we now discuss in related works.

%% OLD VERSION
% Multiscale low-rank matrix decomposition models $\D = \sum_i \X_i$ where each block low-rank matrix $\X_i$ models finer-grained features than $\X_{i+1}$ because it consists of more blocks. Suppose $\D = [\B_1|\B_2]\in \R^{n \times p}$ is of rank $n_k$ with columns sorted according to the column hierarchy $\cA_1 \subset \cA_2$. The FD model with flag type $(n_1,n_2;n)$ is $\D = \Q \bR$ where $\Q = [\Q_1 | \Q_2] \in St(n_k,n)$, $\Q_1 \in \R^{n \times n_1}$, and $\bR$ is block upper triangular. FD does not seek two block low-rank representations of $\D$ for different scales of features, rather it extracts a hierarchy-preserving flag $[\![\Q]\!] \in \flag(n_1,n_2;n)$. Additionally, MLMD partitions $\D$ into column blocks requiring the block partition $P_2$ to be an `order of magnitude' larger than $P_1$ ($1$st par. Sec. II). Therefore MLMD can run only when $|\cA_1| = |\cA_2|$. FD is more general and does not require this restriction. Suppose $|\cA_1| = |\cA_2|$ and we run MLMD. The output decomposition $\D = \X_1 + \X_2$ has $\X_i = \sum_{b \in P_i} R_b (\U_b \boldsymbol{\Sigma}_b \V^\top_b)$ where $R_b$ is the block reshape operator. In this case, MLMD would output $3$ bases (in each $\U_b$), one for the columns of $\B_1$, one for the columns of $\B_2$, and one for all of $\D$. These are not mutually orthogonal and none are guaranteed to be hierarchy-preserving. FD outputs one basis in the columns of $\Q$ that contains the hierarchy-preserving flag meaning $[\Q_1]=[\B_1]$, and $[\Q] = [\D]$. 
%We thank the reviewer for bringing MLMD to our attention, which we now discuss in related works.
%%

\clearpage
\newpage
\paragraph{Long version}

\noindent We thank the reviewers for their useful comments. We are delighted that the reviewers find our paper well-written (\Rone~\& \Rthree), clearly explained (\Rone~\& \Rthree), interesting (\Rtwo), self-consistent (\Rone),  and contains a novel (\Rfour) \& sound methodology (\Rtwo) along with a clear empirical evaluation (\Rthree). 

\noindent \textbf{Related work} (\Rfour) Multiscale low-rank matrix decomposition (MLMD) (Ong and Lustig 2017) models the data matrix as a sum of block-wise low-rank matrices (e.g., $\D = \sum_i \X_i$). $\X_i$ models finer-grained features than $\X_{i+1}$ because it consists of more blocks. Suppose we have $\D\ \in \R^{n \times p}$ (of rank $n_k$) with columns sorted according to the associated column hierarchy $\cA_1 \subset \cA_2$. This means $\D$ is structured in two blocks $\D = [\B_1|\B_2]$. The FD model with flag type $(n_1,n_2;n)$ is $\D = \Q \bR$ where $\Q = [\Q_1 | \Q_2] \in St(n_k,n)$, $\Q_1 \in \R^{n \times n_1}$, and $\bR$ is block upper triangular. FD does not seek two low-rank representations of $\D$ for different scales of features, rather it extracts a hierarchy-preserving flag $[\![\Q]\!] \in \flag(n_1,n_2;n)$, specifically $[\Q_1]=[\B_1]$, and $[\Q] = [\D]$. Additionally, MLMD partitions $\D$ into column blocks it requires the partition $P_1$ to be an `order of magnitude' larger than the previous scale $P_1$ (First paragraph section 2). This means, that MLMD can run only when $|\cA_1| = |\cA_2|$. FD is more general and does not require this restriction. If we suppose $|\cA_1| = |\cA_2|$ and we run MLMD, then we have the decomposition $\D = \X_1 + \X_2$ where $\X_i = \sum_{b \in P_i} R_b (\U_b \boldsymbol{\Sigma}_b \V^\top_b)$ where $R_b$ is the block reshape operator. In this case, MLMD would output three bases (in the $\U_b$), one for the columns of $\B_1$, one for the columns of $\B_2$, and one for all of $\D$. These are not mutually orthogonal and none are guaranteed to be hierarchy-preserving. FD outputs one basis in the columns of $\Q$ that contains the hierarchy-preserving flag meaning $[\Q_1]=[\B_1]$, and $[\Q] = [\D]$. FD requires flag type as a hyperparameter. In contrast, MLMD proposes an optimization procedure for determining these ranks. In future work, we are inspired by MLMD to investigate optimizations to extract a flag type from a data matrix.




% propose the following objective for determining flag type
% \begin{align*}
%     &\argmin_{\substack{\Q \in St(n_k,n)\\ \Q = [\Q_1|\cdots|\Q_k]}} \sum_{i=1}^k \lambda_i \|\Q_i\|_{\mathrm{nuc}}\\
%     &\text{subject to } \D = \Q \bR \bP^\top \text{ is an FD}
% \end{align*}
% where $\lambda_i$ are parameters.



\noindent \textbf{Intuition} (\Rone, \Rtwo,~\& \Rfour) Ex. 2.1-3 contain examples of hierarchies appearing in data. 

One cartoon in~\cref{fig:flag cartoon} and two toy examples in~\cref{fig:flag cartoon1,fig:flag cartoon2} are meant to provide more intuition behind our method.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth, trim={0 15mm 0 3mm}, clip]{figures/hierarchy_motivation.pdf}
    \caption{Consider $\D$ with the column hierarchy (top row: dark to light purple). The corresponding columns of $\D$ are plotted as points in $\R^n$ (bottom row). SVD and QR of $\D$ output a basis for the final subspace (bottom right corner), but do not output subspaces that isolate the data in $\cA_1, \cA_2$ (line and plane). A hierarchy-preserving FD of $\D$ outputs the flag of subspaces in the bottom row and effectively captures the data in $\cA_1$, $\cA_2$, and $\cA_3$.}
    \label{fig:flag cartoon}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/advantage0.pdf}
    \caption{Flag recovery using FD, SVD, and QR decomposition. We plot the columns of $\D$. from $\cA_1$ are in blue and $\cA_2 \setminus \cA_1$ are in orange. The only method that recovers both the line and plane of the hierarchy-preserving flag is FD. SVD only recovers the plane and QR only recovers the line.}
    \label{fig:flag cartoon1}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/flag_faces.pdf}
    \caption{FD on a hierarchy of faces from the YaleFaceDatabaseB. the $6$ images are flattened into $\D$ with rows as pixels and columns as faces. We use the hierarchy with the $3$ illuminations of the first subject as $\cA_1$ and all $6$ images as $\cA_2$. We run FD (flag type $(1,2)$), SVD (rank $2$), and QR (rank $2$) to reconstruct $\D$. Although SVD correctly organizes the different illuminations, FD is the only method to correctly reconstruct the subjects. We plot the eigenfaces (columns of $\Q$ for FD and QR and columns of $\U$ for SVD) on the right and find FD extracts basis elements that most closely resemble the two subjects.}
    \label{fig:flag cartoon2}
\end{figure}

\noindent \textbf{Stability} (\Rone) Modified GS (MGS) and Block MGS (BMGS) are designed to be more stable procedures. Of course, algorithms such as block Householder reflections might prove to be more stable than BMGS. We leave the exploration of more optimal FD algorithms to future work since our paper is meant to introduce the concept of a hierarchy-preserving FD.

\noindent \textbf{SOTA baselines} (\Rone~\& \Rfour) Our clustering experiments compare different methods for extracting flags from data using the same flag clustering algorithm. Thus we compare to the standard flag-extraction methods: QR and SVD. For the denoising experiments, we use SVD as a baseline due to its simplicity and to highlight the advantage of the hierarchical structure in the FD as a generalization of the SVD. Finally, in the fewshot, experiment we propose the flag classifier from FD and compare it to the two standard fewshot learning prototypes (e.g., means and subspaces). Overall, the purpose of this paper is mainly conceptual; we aim to propose and analyze the novel FD. Therefore, we leave an FD applications paper that compares it to the latest SOTA methods and conducts extensive experimentation for future work.  

\noindent \textbf{Fewshot experiments (\Rtwo)}
The hierarchy in Ex. 2.3 places more relevance on the features extracted by $f_{\Theta}$ because these data ($\cA_1$) appear in every other set of the hierarchy. Additionally, FD best reconstructs in the ``first'' part of the column hierarchy (columns of the data matrix in $\mathcal{A}_1$). 

This inspired us to experiment with changing the hierarchy to place the $f^{(2)}_{\Theta}$ features in the first part of the hierarchy instead. We were surprised to find small improvements in classification accuracy with this new hierarchy. We posit that the more generalizable features appear in $f_{\Theta}^{(2)}$ are perhaps more important than those closer to the classification head of alexnet. We will edit Ex. 2.3, the fewshot learning section, and the results to reflect this change in hierarchy order upon acceptance. Additionally, we run our all flag classifiers with a flag type $(s-1, 2(s-1))$ to be consistent with the subspace classifiers which are run with subspace dimension of $s-1$ where $s$ is the number of shots. A comparison between the old and new results is in~\cref{tab:fewshot_oldnew}.



Standard fewshot learning uses only the features from $f_{\Theta}$. Therefore our baselines only use these features. \Rtwo~correctly notes that flag classifiers have an advantage over the baselines because they see features from $f_{\Theta}$ and $f_{\Theta}^{(2)}$. For a more fair comparison, we run our baselines on stacked features from both $f_{\Theta}$ and $f_{\Theta}^{(2)}$. This change (see~\cref{tab:fewshot_unfair}) improves classification accuracy for the baselines in almost all experiments resulting in similar performance between flag (Flag) and subspace (Subsp.) classifiers while both methods generally outperform prototypical networks (Euc.). This indicates that our proposed flag prototypes using the FD are competitive with the baseline methods. Therefore, the potential utility of FD in fewshot learning is surely a promising avenue for future research.

Overall, we posit that the flag type and chosen hierarchy can affect the results and perhaps are dataset-specific. We propose to change Tab. 3 in the manuscript to~\cref{tab:fewshot_forpaper}. 






\noindent \textbf{Clarity} (\Rone)
Upon reviewer approval, we will restructure the methods and applications and add the tables to improve clarity~\cref{tab:alg_table1,tab:alg_table2} 
\begin{table}[ht!]
    \centering
    \caption{GS variants and their corresponding algorithm.}
    \footnotesize
    \label{tab:alg_table1}
    \begin{tabular}{c|cccc}
        \toprule
        Algorithm & GS & MGS & BMGS & \algname \\
        \midrule
        Stable & \xmark & \cmark & \cmark & \cmark \\ 
        Block-wise & \xmark & \xmark & \cmark & \cmark \\ 
        Hier.-pres. & \xmark & \xmark & \xmark & \cmark \\ 
        \bottomrule
    \end{tabular}    
\end{table}


\noindent \textbf{Typos} (\Rone,~\Rtwo~\&~\Rfour) We thank~\Rtwo~for pointing out the typo in section 5.3. We use the first $10$ left singular vectors. We have addressed all other typos in the manuscript.

\clearpage
\newpage
\paragraph{Tables \& Small Comments}

\begin{table}[ht!]
    \centering
    \caption{A comparison between old and new results.}
    \label{tab:fewshot_oldnew}
    \begin{tabular}{llcc}
    \toprule
    $s$ & Dataset & Old & New\\
    \midrule
    \multirow{3}{*}{$3$} & EuroSat & 77.4 & \textbf{77.7}\\
     & CIFAR-10 & 59.2 & \textbf{59.6} \\
     & Flowers102 & 89.4 & \textbf{90.2} \\
    \cline{1-4}
    \multirow{3}{*}{$5$} & EuroSat & 81.6 & \textbf{81.8} \\
     & CIFAR-10 & 65.1 & \textbf{65.2} \\
     & Flowers102 & 92.3 & \textbf{93.3} \\
    \cline{1-4}
    \multirow{3}{*}{$7$} & EuroSat & 83.6 & \textbf{83.9} \\
     & CIFAR-10 & \textbf{68.4} & 68.0 \\
     & Flowers102 & 93.5 & \textbf{94.5} \\
    \bottomrule
    \end{tabular}\vspace{-3mm}
\end{table}

\begin{table}[ht!]
    \centering
    \caption{\emph{Classification accuracy ($\uparrow$)} with $s$ shots, $5$ ways, and $100$ evaluation tasks each containing $10$ query images, averaged over $20$ random trials. Flag types for `Flag' are $(s-1,2(s-1))$ and the subspace dimension is $s-1$.}
    \label{tab:fewshot_forpaper}
    \begin{tabular}{llccc}
    \toprule
    $s$ & Dataset & Flag & Euc. & Subsp.\\
    \midrule
    \multirow{3}{*}{$3$} & EuroSat & \textbf{77.7} & 75.9 & 76.8 \\
     & CIFAR-10 & \textbf{59.6} & 58.4 & 58.5 \\
     & Flowers102 & \textbf{90.2} & 87.9 & 88.8 \\
    \cline{1-5}
    \multirow{3}{*}{$5$} & EuroSat & \textbf{81.8} & 79.8 & 80.8 \\
     & CIFAR-10 & \textbf{65.2} & 64.5 & 63.8 \\
     & Flowers102 & \textbf{93.3} & 91.1 & 92.0 \\
    \cline{1-5}
    \multirow{3}{*}{$7$} & EuroSat & \textbf{83.9} & 81.7 & 82.9 \\
     & CIFAR-10 & \textbf{68.0} & 67.9 & 66.7 \\
     & Flowers102 & \textbf{94.5} & 92.3 & 93.4 \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht!]
    \centering
    \caption{\emph{Classification accuracy ($\uparrow$)} with $s$ shots, $5$ ways, and $100$ evaluation tasks each containing $10$ query images, averaged over $20$ random trials. Flag types for `Flag' are $(s-1,2(s-1))$ and the subspace dimension is $s-1$. The features from $f_{\Theta}$ and $f^{(2)}_{\Theta}$ are concatenated for the Euclidean and subspace prototypes.}
    \label{tab:fewshot_unfair}
    \begin{tabular}{llccc}
    \toprule
     & s & Flag & Euc. & Subsp. \\
    \midrule
    \multirow[t]{3}{*}{3} & EuroSat & \textbf{77.7}  & 76.7  & 77.6  \\
     & CIFAR-10 & \textbf{59.6}  & 58.6  & \textbf{59.6}  \\
     & Flowers102 & \textbf{90.2}  & 88.2  & \textbf{90.2}  \\
    \cline{1-5}
    \multirow[t]{3}{*}{5} & EuroSat & \textbf{81.8}  & 80.7  & \textbf{81.8}  \\
     & CIFAR-10 & \textbf{65.2}  & \textbf{65.2}  & \textbf{65.2}  \\
     & Flowers102 & \textbf{93.3}  & 91.4 & 93.2  \\
    \cline{1-5}
    \multirow[t]{3}{*}{7} & EuroSat & \textbf{83.9}  & 82.6  & \textbf{83.9}  \\
     & CIFAR-10 & 68.0  & \textbf{68.6}  & 68.1  \\
     & Flowers102 & \textbf{94.5} & 92.7 & \textbf{94.5}  \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht!]
    \centering
    \caption{Flag extraction algorithms.}
    \label{tab:alg_table2}
    \begin{tabular}{c|ccccc}
        \toprule
        Decomp. & QR & SVD & IRLS-SVD & FD  & RFD\\
        \midrule
        Robust & \xmark & \xmark & \cmark & \xmark & \cmark \\ 
        Order-pres. & \cmark & \xmark & \xmark & \cmark & \cmark \\ 
        Flag-type & \xmark & \xmark & \xmark & \cmark & \cmark \\ 
        Hier.-pres. & \xmark & \xmark & \xmark & \cmark & \cmark \\ 
        \bottomrule
    \end{tabular}
    
\end{table}

\begin{itemize}
    \item (\Rone~\& \Rtwo) Please see the On flag type section in Sec. 3.2 for an explanation of methods for determining the flag type.
    \item (\Rtwo) We have changed the proof sketch of Prop. 2 to improve clarity
    \begin{proof}
        Define (for $i=2,3,\dots,k$) the projector onto the null space of $[\Q_1,\Q_2,\dots,\Q_i]$, as $\bP_{\Q_{:i}^\perp} = \I - \Q_{:i}\Q_{:i}^\top$. We use this to define $\C_i = \bP_{\Q_{:i-1}^\perp}\B_i$ and $\Q_i \in St(m_i,n)$ so that $[\Q_i] = [\C_i]$. We use mathematical induction to prove a series of small results ending in $\bP_{\Q_i^\perp} \cdots\bP_{\Q_1^\perp} \B_i = \bm{0}$, and $\Q_i \in St(m_i,n)$ with $m_i = n_i - n_{i-1}$ where $n_i = \mathrm{rank}(\D_{\cA_i})$.
    \end{proof}
    \item (\Rtwo) Eq. 14 is essentially a subspace recovery problem. We agree that most subspace recovery methods optimize $r,q \in \{1,2\}$. We do mention that some robust subspace recovery methods use $r=0$. Fixing $q=1$ and increasing $r$ from $0$ to $2$ results in less robustness to outliers.
    \item We thank \Rtwo~\&~\Rfour for pointing out type-os and have edited our manuscript accordingly. Specifically, we have added $\mathrm{rank}(\mathbf{D}_{\mathcal{A}_i})$ to proposition $2$, modified the reference to Eq 12, added block upper triangular matrix, fixed type-os in Fig. 2. On line 342 it is indeed $\tilde{\mathbf{D}}$. We use $\mathbf{P}^{(c)}$ to represent the Stiefel coordinates for the prototype flag of class $c$ to remain consistent with the notation for subspace classifiers for fewshot learning (Simon et al. 2020). We have added a reference to Figure 4 in the robustness to outliers section of 5.1. In 5.4, $\mathcal{A}_1$ is the center pixel, and $\mathcal{A}_2$ is all of the $3 \times 3$ image patch.
\end{itemize}




\end{document}
