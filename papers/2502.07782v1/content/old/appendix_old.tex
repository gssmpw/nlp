

\section{Theoretical Justifications \& Discussions}\label{sec:proofs}
\begin{prop}%\label{prop:leftsvs}
    The $i$th iteration of FlagRep solves 
    \begin{equation}
        \X_{i} = \argmin_{\substack{\Z^T\Z = \I \\ \text{rank}(\Z) = \text{rank}(\C_i)}} \| \Z \Z^T \C_i - \C_i \|_F.
    \end{equation}
\end{prop}
\begin{proof}
    This is a classical result from the eigenvalue decomposition: the solution is the left singular vectors of $\C_i$. Specifically, we can translate the optimization to
    \begin{equation}
        \argmax_{\substack{\Z^T\Z = \I \\ \text{rank}(\Z) = \text{rank}(\C_i)}} \tr\left( \Z^T \C_i\C_i^T \Z_i \right).
    \end{equation}
    Using the method of Lagrange multipliers (stored in the matrix $\mathbf{\Lambda}$, this is the eigenvalue problem $\C_i \C_i^T\Z = \mathbf{\Lambda} \Z$.
    This is solved by the first $\mathrm{rank}(\C_i)$ left singular vectors of $\C_i$, computable by the thin singular value decomposition.
\end{proof}

\begin{prop}\label{prop:stiefelcoords}
    The output of FlagRep is always in Stiefel coordinates.
\end{prop}
\begin{proof}
    Let $\X = \begin{bmatrix} \X_1 &\X_2 & \cdots & \X_k \end{bmatrix}$. We must verify $\X^T \X = \I$. This is equivalent to $\X_i^T\X_i = \I$ and $\X_i^T\X_j = \mathbf{0}$ for all $i\neq j$. First, $\X_i^T\X_i = \I$ because $\X_i = \U_i$ and left singular vectors are orthogonal.    
    % We begin by showing $\mathbf{P}_{1} \cdots \mathbf{P}_{j+\ell-1} \C_{j} = \mathbf{0}$ by induction on $\ell$. 
    To show $\X_i^T\X_j = \mathbf{0}$ for all $i\neq j$, it suffices to fix an arbitrary $j$ and show $\X_{j+\ell}^T\X_j$ for all $\ell > 1$ and $\ell < k-j$. 
    First we will show $\X_{j+\ell}^T\X_j$ is equivalent to $\C_{j+\ell}^T\C_{j}= \mathbf{0}$ for all $\ell > 1$ and $\ell < k-j$.  For each $i$, $\X_i = \U_i(1:\mathrm{end},1:m_i)$ where $\U_i \mathbf{\Sigma}_i \V_i = \C_i$ is the rank $m_i$-SVD where $m_i$ is the rank of $\C_i$. Thus
    \begin{align*}
        \mathbf{0} &= \C_{j+\ell}^T\C_{j}, \\
        \mathbf{0} &= (\U_{j+\ell} \mathbf{\Sigma}_{j+\ell} \V_{j+\ell}^T)^T\U_{j} \mathbf{\Sigma}_{j} \V_{j}, \\
        \mathbf{0} &= \mathbf{\Sigma}_{j+\ell}^{-1} \V_{j+\ell}^T \V_{j+\ell} \mathbf{\Sigma}_{j+\ell} \U_{j+\ell}^T \U_{j} \mathbf{\Sigma}_j \V_j^T \V_j \mathbf{\Sigma}_j^{-1},\\
        \mathbf{0} &= \U_{j+\ell}^T \U_j = \X_{j+\ell}^T \X_j. 
    \end{align*}
For any $j$, $\mathbf{P}_j = (\I - \X_j \X_j^T)$ projects into the null space of $\X_j$. Since the span of the columns of $\X_j$ is the same as the span of the columns of $\C_j$, $\mathbf{P}_j$ projects into the null space of $\C_j$. So, $\mathbf{P}_j \C_j= \mathbf{0}$
    
    We will proceed by induction on $\ell$ to show $\C_{j + \ell}^T\C_j = \mathbf{0}$. For the base case take $\ell = 1$. Then
    \begin{equation}
        \C_{j + 1}^T\C_j = \B_{j + 1}^T\mathbf{P}_1 \cdots \mathbf{P}_{j-1}\underbrace{(\mathbf{P}_j\C_j)}_{\mathbf{0}} = \mathbf{0}.
    \end{equation}
    Suppose $\C_{j + s}^T\C_j = \mathbf{0}$ for all $0 < s < \ell$. This implies $\mathbf{P}_{j + s}\C_j = \C_j$ because $\C_j$ is in the null space of $\C_{j + s}$ for all $0 < s < \ell$. Then
    \begin{align}
        \C_{j + \ell}^T\C_j &= \B_{j + \ell}^T\mathbf{P}_1 \cdots \mathbf{P}_j \underbrace{(\mathbf{P}_{j+1} \cdots \mathbf{P}_{j+\ell -1}\C_j)}_{\C_j}\\
        &= \B_{j + \ell}^T\mathbf{P}_1 \cdots \mathbf{P}_{j-1}\underbrace{(\mathbf{P}_j \C_j)}_{\mathbf{0}} = \mathbf{0}
    \end{align}
because $\mathbf{P}_{j+\ell -1}$ projects onto the null space of $\C_{j+\ell - 1}$.
\end{proof}

\begin{prop}
    Suppose there exists some $[\![ \X ]\!]$ such that $\mathbf{P}_i \cdots \mathbf{P}_1 \B_i = \mathbf{0}$ for all $i=1,\dots,p$. FlagRep decomposes a data matrix into a block-QR structure as
    \begin{equation}
        \D \boldsymbol{\Pi}
        = \underbrace{\begin{bmatrix} \X_1 & \cdots & \X_k \end{bmatrix}}_{n \times n_k}
        \underbrace{\begin{bmatrix}
            \mathbf{R}_{11} & \mathbf{R}_{12} & \cdots & \mathbf{R}_{1k}\\
            \mathbf{0} & \mathbf{R}_{22} & \cdots & \mathbf{R}_{2k}\\
            \vdots & \vdots & \ddots & \vdots\\
            \mathbf{0} & \mathbf{0} & \cdots & \mathbf{R}_{kk}
        \end{bmatrix}}_{n_k \times p},
    \end{equation}
    where $\D \boldsymbol{\Pi} = \begin{bmatrix} \B_1 & \cdots & \B_k \end{bmatrix}$ and
    \begin{equation}
        \mathbf{R}_{i,j} := 
        \begin{cases}
            % (\X_i^T-\X_i^T\mathbf{P}_{i-1}'\D_i, &i=j\\
            \X_i^T\mathbf{P}_{i-1}\cdots \mathbf{P}_1 \B_i, &i=j\\
            \X_i^T\mathbf{P}_{i-1}\cdots \mathbf{P}_1 \B_j, &i < j.
        \end{cases}
    \end{equation}
\end{prop}
\begin{proof}
    Define the recursion (with $\mathbf{P}'_0 = \mathbf{0}$)
    \begin{equation*}
        \mathbf{P}_i' = \X_i \X_i^T + (\I - \X_i \X_i^T) \mathbf{P}'_{i-1}
    \end{equation*}
    Notice for all $i=1,\dots,p$ we have
    \begin{align}
        \I -  \mathbf{P}_{i}' &= \I - \X_i \X_i^T - (\I - \X_i \X_i^T) \mathbf{P}'_{i-1},\\
         &=\mathbf{P}_i - \mathbf{P}_i \mathbf{P}'_{i-1},\\
        &=\mathbf{P}_i(\I - \mathbf{P}_{i-1}').
    \end{align}
    Since this is true for all $i$, we have
    \begin{equation}
        \I -  \mathbf{P}_{i}' = \mathbf{P}_i \cdots \mathbf{P}_1.
    \end{equation}
    Therefore
    \begin{equation}
        \mathbf{0} = \mathbf{P}_i \cdots \mathbf{P}_1\B_i = (\I - \mathbf{P}_i')\B_i= \B_i - \mathbf{P}_i' \B_i.
    \end{equation}
    For any $i$, we have
    \begin{equation}
        \mathbf{P}_i' = (\X_i \X_i^T + (\I - \X_i \X_i^T) \mathbf{P}'_{i-1}) = \X_i \X_i^T(\I - \mathbf{P}'_{i-1}) + \mathbf{P}'_{i-1} = \X_i \X_i^T\mathbf{P}_{i-1}\cdots \mathbf{P}_1 + \mathbf{P}'_{i-1}.
    \end{equation}
    Replacing every $\mathbf{P}_j'$ for each $j < i$ with $\X_i \X_i^T\mathbf{P}_{i-1}\cdots \mathbf{P}_1 + \mathbf{P}'_{i-1}$, we have
    \begin{equation}
        \mathbf{P}_i' = \X_i \X_i^T\mathbf{P}_{i-1}\cdots \mathbf{P}_1 + \X_{i-1} \X_{i-1}^T\mathbf{P}_{i-2}\cdots \mathbf{P}_1 + \cdots +\X_1 \X_1^T \mathbf{P}_1
    \end{equation}
    Then, we have 
    \begin{align}
        \B_i & = \mathbf{P}_i' \B_i\\
        &= (\X_i \X_i^T\mathbf{P}_{i-1}\cdots \mathbf{P}_1 + \X_{i-1} \X_{i-1}^T\mathbf{P}_{i-2}\cdots \mathbf{P}_1 + \cdots +\X_1 \X_1^T \mathbf{P}_1)\B_i
    \end{align}
    Stacking this into matrices for each $i$ gives us the desired result.
\end{proof}

\section{Extra Experiments}
% \subsection{Trivial Experiment}
We generate hierarchical data with $p=4$ features: $\mathcal{A}_1 = \{1\}$, $\mathcal{A}_2 = \{1,2,3\}$, and $\mathcal{A}_3 = \{1,2,3,4\}$. To this end, we sample $\mathbf{d}_1\sim\mathcal{N}(0,1)$, $\mathbf{d}_2\sim\mathcal{N}(0,2)$, and $\mathbf{d}_4\sim\mathcal{N}(0,3)$. We set $\mathbf{d}_3=2\mathbf{d}_2$ to induce correlation. The rank of the final data matrix $\mathbf{D} = \begin{bmatrix} \mathbf{d}_1 & \mathbf{d}_2  & \mathbf{d}_3 & \mathbf{d}_4 \end{bmatrix} \in \R^{10 \times 4}$ is $3$.
% These data are $n=10$ samples from feature $1$ from $\mathcal{N}(0,1)$, feature $2$ from $\mathcal{N}(0,2)$, and feature $4$ from $\mathcal{N}(0,3)$. Then we define feature $3$ to as twice feature $2$. Notice that the rank of the data matrix $\mathbf{D} = \begin{bmatrix} \mathbf{d}_1 & \mathbf{d}_2  & \mathbf{d}_3 & \mathbf{d}_4 \end{bmatrix} \in \R^{10 \times 4}$ is $3$ because $\mathbf{d}_3 = 2\mathbf{d}_2$. 
We then run $3$ different flag representation methods, \emph{FlagRep, QR decomposition}, and \emph{SVD} in Steifel coordinates for $\phi : (\mathcal{D}, \mathcal{A}) \rightarrow \flag(1,2,3;4)$. QR decomposition has been used in the past for flag representations of data~\cite{Mankovich_2023_ICCV}. The mapping $\phi$ is illustrated in~\cref{fig:concept}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/concept_cropped.pdf}\vspace{-3mm}
    \caption{Stiefel coordinates $\phi$ for a flag of type $(1,2,3;4)$ for data $\mathcal{D} \subset \R^{10}$ with a feature hierarchy $\mathcal{A}_1 \subset \mathcal{A}_2 \subset \mathcal{A}_3 $.\vspace{-2mm}}
    \label{fig:concept}
\end{figure}

Each of these flags is represented by a Steifel matrix of the form $\X = [\x_1 | \x_2 |\x_3]  \in \mathbb{R}^{10 \times 3}$. We expect $\x_1$ to be correlated with $\mathbf{d}_1$, $\x_2$ to be correlated with $\mathbf{d}_2$ and $\mathbf{d}_3$, and $\x_3$ to be correlated with $\mathbf{d}_4$. To test this, we compute the average absolute correlation over the data. Geometrically, this is the cosine of the acute angles between the subspaces spanned by the data and corresponding columns of the flag representative $\mathbf{X} \in St(3,10)$.
\cref{fig:synthetic} shows that the mean of these absolute correlations is higher for FlagRep than QR and SVD which indicates that FlagRep produces the best representation of these data as a flag of type $(1,2,3;4)$.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/flagrep_vs_qr_vs_svd2.pdf}
    \caption{$1000$ random trials on data with nested features. FlagRep, QR, and SVD are used to represent these data as a flag $[\![\X ]\!]$ of type $(1,2,3;10)$ with coordinates $\X = [\x_1 | \x_2 | \x_3]\in St(3,10)$. We plot the mean absolute correlation between the original data and the corresponding columns of flag representation.\vspace{-3mm}}
    \label{fig:synthetic}
\end{figure}


\flushcolsend
