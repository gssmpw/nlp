\section{Intro old}
\section{Introduction}\label{sec:intro}
A (real) flag encodes a hierarchical nested sequence of subspaces with increasing dimensions $n_1 < n_2 < \cdots < n_k < n$. This ordered set of increasing subspace dimensions is called the type or signature of the flag and is denoted $(n_1,\dots n_k;n)$. For example, a simple flag of type $(1,2;3)$, is a line inside a plane in $\R^3$.

Flag manifolds have been used to produce new variants of dimensionality reduction algorithms like Principal Component Analysis (PCA)~\cite{pennec2018barycentric, mankovich2024fun, szwagier2024curseisotropyprincipalcomponents} and Self-Organizing Mappings (SOM)~\cite{ma2022self}, compute robust averages of motions~\cite{Mankovich_2023_ICCV}, represent different face illuminations~\cite{draper2014flag, Mankovich_2023_ICCV}, represent the space of 3D motions~\cite{Mankovich_2023_ICCV}\footnote{the space of motions in $\R^3$ corresponds to the oriented flag manifold of type $(1,2,3;4)$} and parameterize $3$-dimensional shapes~\cite{ciuclea2023shape}. Additionally, flags have been used as prototypes for clusters in subspace clustering algorithms applied to datasets of video clips~\cite{marrinan2014finding, mankovich2022flag, Mankovich_2023_ICCV} and biological datasets~\cite{mankovich2023module,mankovich2023subspace}. Moreover, non-linear flags, a manifold of shapes in $\R^3$, is parameterized using a non-linear flag of curves inside surfaces~\cite{ciuclea2023shape}.

Hierarchies are commonly found in many data modalities in computer vision within pixel structures, multi-band imagery, and even within neural networks. Commonly used feature extraction and data reconstruction algorithms range from matrix decompositions (like the SVD-based PCA) to autoencoder architectures. These methods are limited because they do not always preserve data hierarchies. We offer a flag decomposition, inspired by Graham-Schmidt methods~\cite{jalby1991stability,ida2019qr,apriansyah2022parallel} and fueled by the hierarchical structure of flags, that decomposes hierarchical data into a flag. This method is used to recover hierarchy-preserving flags and reconstructions. After introducing flag decompositions, propose an algorithm for finding flag decompositions called FlagRep. FlagRep robustly recovers data contaminated with noise and outliers in simulations and is used for accurate flag recovery for downstream tasks like flag clustering. Then, FlagRep is applied to real data and is shown to improve hyperspectral image patch clustering and de-noising over off-the-shelf flag recovery methods like SVD. Finally, using flags as prototypes in a few-shot learning framework we improve classification accuracy on EuroSat, CIFAR-10, and Flowers102 datasets.




% Although flags are powerful, general, hierarchical structures that have proven useful in many computer vision applications, we have yet to scratch the surface of their potential. Previous applications of flags are task-specific and there is still no general and universally accepted algorithm for representing hierarchical datasets as flags. We aim to fill this gap with a flag decomposition for a matrix with a hierarchical column structure. 

%Specifically, we use the Stiefel coordinates of the flag manifold to propose a novel algorithm for hierarchical flag decomposition of arbitrary real-valued data.
% \begin{itemize}
%     \item Flags are useful in CV
%     \item Subspace recovery is useful in CV
% \end{itemize}

% We provide a framework for estimating a flag from data with a hierarchical structure. If these data are rank deficient, our method also estimates the flag type (signature).
% \paragraph{Low-rank block QR factorizations}
% Suppose $\D \in \R^{n \times p}$ A QR decomposition of $\D$ with pivoting produces
% \begin{equation}
%     \D = [\underbrace{\Q_1}_{n \times k}|\underbrace{\Q_2}_{n \times (n-k)}]\underbrace{\begin{bmatrix}
%         \bR_{1,1} & \bR_{1,2} \\
%         \bm{0} & \bR_{2,2}
%     \end{bmatrix}}_{n\times p}\underbrace{\bP}_{p\times p}.
% \end{equation}
% if $\D$ is of rank $k$, then $\bR_{2,2} = \bm{0}$.
% We call this ``rank-revealing''~\cite{chan1987rank} if $p<n$,$\bR_{1,1}$ is well conditioned, $\|\bR_{2,2}\|_2$ is small and $\bR_{1,1}$ is linearly dependent on $\bR_{1,1}$ with coefficients bounded by a low degree polynomial in $n$

% We call $\D \in \R^{n \times p}$ a block low-rank matrix if it is low rank and can be broken down into a $r \times s$ block matrix with square blocks of size $b \times b$ (this means $r = n/b$ and $s = p/b$). There are fast methods for rank-revealing QR decompositions of such matrices that exploit the block symmetries (see~\cite{apriansyah2022parallel}).

% The modified block Graham-Schmidt algorithm is here~\cite{jalby1991stability} and nicely summarized in Figure $2$ in~\cite{ida2019qr}. This performs flag decompositions when $p \leq n$ and when $|\mathcal{A}_i| = |\mathcal{A}_j|$ for all $i \neq j$.




% \paragraph{Contributions}
% \begin{itemize}
%     \item Novel decomposition of a data matrix into a flag in Steifel coordinates, weight matrix, and permutation matrix bridges gap between pecously used methods (e.g., QR and SVD)
%     \item Algorithm is general and admits any subspace recovery method
%     \item FlagRep can be used to estimate flag type
%     \item We coin the term ``flag recovery'' which generalized subspace recovery
%     \item We offer a novel few-shot learning algorithm that produces high accuracies when doing linear probing
%     \item FlagRep improves hyperspectral image de-noising over PCA
% \end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/Concept.pdf}
    \caption{(a) A flag decomposition, represented by the blue trapezoids, is used for a hierarchy-preserving representation and reconstruction. (b) The flag decomposition is used for reconstruction (e.g., de-noising). (c) The flag decomposition is used on image patches to produce a collection of hierarchy-preserving flags. These flags are then clustered using chordal distance. (d) Multiple samples of the same class are passed through a pre-trained feature extractor (light blue) to generate hierarchically structured data. Then a flag decomposition represents these samples with a single flag for tasks like few-shot learning.}
    \label{fig:concept}
\end{figure}

\clearpage
\newpage

\section{Methods old}

 $[\![\X]\!]$, weights $\bR$, and permutation matrix $\bP$ so that $\mathbf{D} \approx \hat{\D} = \X \bR\boldsymbol{\Pi}$. The essential notation in this section appears in~\cref{tab:symbol-definitions}.



\begin{prop}
    Suppose there exists some $[\![ \X ]\!]$ such that $\bP_i \cdots \bP_1 \B_i = \mathbf{0}$ for all $i=1,\dots,p$. FlagRep decomposes a data matrix into a block-QR structure as
    \begin{equation}\label{eq:reconstruction}
        \D 
        = \underbrace{\begin{bmatrix} \X_1 & \cdots & \X_k \end{bmatrix}}_{n \times n_k}
        \underbrace{\begin{bmatrix}
            \bR_{11} & \bR_{12} & \cdots & \bR_{1k}\\
            \mathbf{0} & \bR_{22} & \cdots & \bR_{2k}\\
            \vdots & \vdots & \ddots & \vdots\\
            \mathbf{0} & \mathbf{0} & \cdots & \bR_{kk}
        \end{bmatrix}}_{n_k \times p}\underbrace{\boldsymbol{\Pi}}_{p \times p},
    \end{equation}
    where $\D  = \begin{bmatrix} \B_1 & \cdots & \B_k \end{bmatrix}\boldsymbol{\Pi}$,
    \begin{align*}
        \bR_{i,j} &= 
        \begin{cases}
            % (\X_i^\top-\X_i^\top\bP_{i-1}'\D_i, &i=j\\
            \X_i^\top\bP_{i-1}\cdots \bP_1 \B_i, &i=j\\
            \X_i^\top\bP_{i-1}\cdots \bP_1 \B_j, &i < j
        \end{cases},\\
        \boldsymbol{\Pi} &= \left[\boldsymbol{\Pi}_1|\boldsymbol{\Pi}_2|\cdots|\boldsymbol{\Pi}_k\right]\\
        \boldsymbol{\Pi}_i &= \left[ \mathbf{e}_{b_1}, \mathbf{e}_{b_2}, \dots, \mathbf{e}_{b_{|\cB_i|}} \right],
    \end{align*}
    $\cB_i = \left \{b_1^{(i)}, \cdots b_{|\cB_i|}^{(i)}\right \}$, and $\mathbf{e}_{b_j^{(i)}}$ is a vector of all $0$s except a $1$ in the $b_j^{(i)}$th entry.
\end{prop}
\begin{proof}
    See supplementary material.
\end{proof}

\begin{remark}[Comparison to QR]
The FlagRep algorithm is essentially the Graham-Schmidt process for subspaces. The main difference between a block-QR decomposition is that FlagRep is a flag decomposition (e.g., hierarchy preserving) that can be used to recover a low-rank reconstruction of $\hat{\D}$. SVD reduces the rank of $\D$.


% The standard QR decomposition solves this problem when $p = n_k$. But, when $n_k < p$, vanilla QR decomposition does not work because it does not address the rank of each of the blocks $\D_i$, thus only FlagRep does the trick. Block QR decomposition (CITE) certainly exists but has never been approached from a flag perspective and does not address low-rank reconstructions that respect a hierarchical column structure in $\D$. %However, we can replace the SVD basis extraction and truncation steps with the QR decomposition followed by a truncation via the number of non-zero rows in $\bR$. 
\end{remark}

\begin{table}[ht!]
    \caption{Symbols used in this section.}\vspace{-2mm}
    \label{tab:symbol-definitions}
    \centering\small
    \begin{tabular}{l | @{\hspace{1em}}l@{}}
        Symbol & Definition \\ [0.5ex] \hline
        $\D \in \R^{n \times p}$ & data matrix\\
        $\cA_i\subset \{ 1, \dots, p \}$ & column subset\\
        $\cA_1 \subset \cdots \subset \cA_k$ & column hierarchy\\
        $\cB_i$ & $\cA_i \setminus \mathcal{A
        }_{i-1}$, with $\cB_1 = \cA_1$\\
        $\B_i$ & matrix with cols $\mathbf{d}_i$ s.t. $i \in \cB_i$\\
        $\bP_{\Q_i^\perp} = \I - \Q_i \Q_i^\top$ & proj. onto the null space of $\Q_i$\\
        % $\C_i = \bP_{i-1}\cdots \bP_1 \mathbf{B}_i$ & cols span the part of  $[\B_i]$ not in $[\C_j]$, $j < i$\\
        %$\U(:,1:m_i)$ & first $m_i$ columns of $\U$\\
        $\Q = [\Q_1 | \cdots | \Q_k]$ & Stiefel coords for flag $[\![\Q]\!]$
    \end{tabular}
    \vspace{-3mm}
\end{table}













%        
%The notation used in this section is summarized in~\cref{tab:symbol-definitions}. 



\noindent We now discuss in detail the utility of this representation.








\begin{remark}[The utility of FlagRep]\label{rem:utility}
Compared to traditional parameterizations based on QR decomposition or SVD,~\cref{alg:flagrep} brings several advantages:
\begin{itemize}
    \item \textbf{Data Reconstruction}. W
    \item \textbf{Flag recovery}. 
    \item \textbf{Few-shot learning}. 
\end{itemize}\vspace{-2mm}
\end{remark}


In principle, the closest flag to $\D$ with feature hierarchy $\cA_1 \subset \cdots \subset \cA_k$ would solve
\begin{equation}\label{eq: mother equation}
    \argmin_{[\![\X]\!] \in \flag(n_1,n_2, \dots, n_k;n)} \sum_{i=1}^k \sum_{j \in \cB_i}\| \bP_i \cdots \bP_1 \mathbf{d}_j \|_r^q
\end{equation}
for some $r,q \in \R$. We are interested in the case where $r=2$ and $q=1,2$. This problem is difficult, even after restricting $q$ and $r$, so we address the iterative optimization for each piece of the flag (e.g., $\X_i$) with FlagRep~\ref{alg:flagrep}. The general optimization problem solved at each iteration is
\begin{equation}\label{eq:iterative_opt}
    \X_i =  \argmin_{\Z \in St(m_i,n)} \sum_{j \in \cB_j}\| (\I - \Z \Z^\top) \bP_{i-1} \cdots \bP_1 \mathbf{d}_j \|_r^q.
\end{equation}
This is essentially a subspace recovery problem which the get\_basis function solves. 

\begin{remark}[get\_basis]
A study of all subspace recovery methods is out of the scope of this work. Therefore, we focus on two subspace recovery methods the Singular Value Decomposition (SVD) for $q=r=2$ and a naive Iteratively Re-weighted Least Squares-SVD (IRLS-SVD) method (CITE) for $q=2,r=1$. We call ``FlagRep'' the version with the SVD for get\_basis and ``Robust FlagRep'' the version with IRLS-SVD for get\_basis.
\end{remark}

FlagRep is a mapping that takes a data matrix, hierarchy, and flag type and outputs a flag and weight matrix with the following properties:
\begin{enumerate}
\item Each iteration of FlagRep outputs 
\begin{itemize}
    \item a truncated orthogonal matrix $\X_i$ whose projection, $\X_i\X_i^\top$, optimally recovers $\C_i := \bP_{i-1} \cdots \bP_1 \B_i$
    \item a flag $[\![\X ]\!]$ in Stiefel coordinates
\end{itemize}
% \begin{itemize}
% \item outputs a truncated orthogonal matrix $\X_i$ whose projection, $\X_i\X_i^\top$, \textit{optimally recovers} \\$\C_i := \bP_{i-1} \cdots \bP_1 \B_i$
% %See~\cref{prop:leftsvs}.
%         \item uses $\C_i$ as \textit{the closest matrix to $\B_i$} that is in the null space of $\C_1,\dots,\C_{i-1}$
%         \end{itemize}
% \item outputs %See~\cref{prop:stiefelcoords}. 
\item FlagRep provides a rank $n_k$ reconstruction of $\D$ as \\ $\hat{\D} = \X \bR \bm{\Pi}$ that respects the input hierarchy.
\item FlagRep reconstruction is invariant to block rotations of $\X$ with respect to flag type.
\end{enumerate}
\begin{prop}[Optimal Recovery]\label{prop:leftsvs}
    The $i$th iteration of FlagRep (with SVD for get\_basis) solves 
    \begin{equation}
        \X_{i} = \argmin_{\Z \in St(m_i,n)} \| \Z \Z^\top \C_i - \C_i \|_F.
    \end{equation}
\end{prop}
\begin{proof}
    This is a classical result of the eigenvalue decomposition.
\end{proof}
% Using properties of the Frobenius norm and 
% \begin{remark}[Relation to~\cref{eq: mother equation}]
%     FlagRep solves the iterative optimization
%     \begin{equation}\label{eq:iterative_opt}
%         \X_i = \argmin_{\Z \in St(m_i,n)}\sum_{j \in \cB_i} \| (\I - \Z \Z^\top) \bP_{i-1} \cdots \bP_1 \mathbf{d}_j \|_2^2
%     \end{equation}
%     with $\X_1 = \argmin_{\Z \in St(m_1,n)}\sum_{j \in \cB_1} \| (\I - \Z \Z^\top)\mathbf{d}_j \|_2^2$. This follows directly from~\cref{prop:leftsvs}. Thus, each $\X_i$ solves one term in the first sum of the objective minimized in~\cref{eq: mother equation}.
% \end{remark}



\begin{prop}[Stiefel Coordinates]\label{prop:stiefelcoords}
    The output of FlagRep is always in Stiefel coordinates.
\end{prop}
\begin{proof}
    See supplementary material.
\end{proof}





% \begin{prop}
%     The first sum of the squares of the first $m_i$ singular values of $\C_i$ are exactly $\| \X_i \X_i^\top \bP_{i-1} \cdots \bP_1 \B_i \|_2^2$.
% \end{prop}
% \begin{proof}
%     Standard linear algebra ...
% \end{proof}

\begin{remark}[Assumptions of FlagRep reconstruction]
    We find the weight matrix $\bR$, by assuming that there exists some optimal $\X \in St(n_k,n)$ so that $\bP_{i} \cdots \bP_1 \B_i=\mathbf{0}$ for each $i=1, \cdots, k$. Therefore the minimum objective value in~\cref{eq:iterative_opt} is $0$ for each $i$. In fact, this occurs when $\mathrm{col}(\B_i) \cap \mathrm{col}(\B_j) = 0$ for all $i<j$ and $\mathrm{col}(\B_i) \cap \mathrm{col}(\X_i)$ for all $i$.
\end{remark}




\begin{remark}[Flag type estimation]
    FlagRep can be used for flag type estimation. If $\D$ is a rank-deficient matrix and SVD is used for get\_basis, we can take the left singular vectors associated with only the non-zero singular values at each run of get\_basis. This will certainly result in $n_k < p$ and output a flag type without using it as an input to FlagRep. Sadly, such rank-deficient matrices are not common in the wild.
\end{remark}

\begin{remark}[Approximation error]
If we incorporate the approximation error matrix, 
\begin{equation}
    \mathbf{E} = \begin{bmatrix}
        \bP_1\B_1 & \bP_2 \bP_1 \B_2 & \cdots & \bP_k\cdots\bP_1\B_k  
    \end{bmatrix},
\end{equation}
for each $i$, we achieve equality for any $\D$
\begin{equation}
    \mathbf{D} = (\X \bR + \mathbf{E})\boldsymbol{\Pi} .
\end{equation}
\end{remark}

\begin{remark}[Comparison to QR]
The standard QR decomposition solves this problem when $p = n_k$. But, when $n_k < p$, vanilla QR decomposition does not work because it does not address the rank of each of the blocks $\D_i$, thus only FlagRep does the trick. Block QR decomposition (CITE) certainly exists but has never been approached from a flag perspective and does not address low-rank reconstructions that respect a hierarchical column structure in $\D$. %However, we can replace the SVD basis extraction and truncation steps with the QR decomposition followed by a truncation via the number of non-zero rows in $\bR$. 
\end{remark}

% \begin{align*}
%     \D \boldsymbol{\Pi} &= \X \bR \\
%     &= \X  \begin{bmatrix}
%             \X_1^\top \C_1 & \X_1^\top \B_2 & \cdots & \X_1^\top \B_k\\
%             \mathbf{0} & \X_2^\top \C_2 & \cdots & \X_2^\top \bP_1  \B_k\\
%             \vdots & \vdots & \ddots & \vdots\\
%             \mathbf{0} & \mathbf{0} & \cdots &  \X_k^\top\C_k
%     \end{bmatrix}
% \end{align*}

\begin{remark}[Robust FlagRep]
    The $i$th iteration of FlagRep solves~\cref{eq:iterative_opt} with $q=r=2$. A robust formulation of this optimization the same problem with $q=1$, $r=2$. This problem can be solved by IRLS or optimization on the Grassmannian (see~\cite{maunu2019well}).

    The last iteration of the IRLS algorithm is essentially an SVD on a weighted data matrix. Given a diagonal weighting matrix $\mathbf{W}_i$, this solves
    \begin{equation}
        \X_{i} = \argmin_{\substack{\Z^\top\Z = \I \\ \text{rank}(\Z) = \text{rank}(\C_i)}} \sum_{j=1}^{|\cB_i|} \|\Z \Z^\top \C_i\W_i - \C_i\W_i \|_2^2.
    \end{equation}
    Assuming $\Z \Z^\top \C_i\W_i - \C_i\W_i  = \mathbf{0}$ for all $i$ and multiplying the right-hand side by $\W_i^{-1}$ results in the same equation $\bP_i \cdots \bP_1 \B_i = \mathbf{0}$ for $i$, thus we can use the same reconstruction as was used for the SVD (non-robust) FlagRep formulation.
\end{remark}




% \begin{algorithm}[t]%\label{alg:train}
% \caption{FlagRep$^{-1}$}\label{alg:flagrep}
%  \textbf{Input}: {Feature hierarchy $\cA_1\subset \cdots \subset \cA_k$, flag representation $[\![\X]\!]\in \flag(n_1,n_2,\dots,n_k;n)$, $\bR \in \R^{n_k \times p}$}\\
%  \textbf{Output}: {A reconstructed data matrix $\hat{\mathbf{D}} \in \R^{n \times p}$} \\[0.25em]
%      $\cA_0 \gets \emptyset$;\\
%      \For{$i\gets1$ \KwTo $k$}{
%          $\cB_i \gets \cA_i \setminus \cA_{i-1}$;\\
%          $\hat{\mathbf{D}}(1:\text{end},\cB_i) \gets \sum_{j=1}^i \X_i \bR_{i,j}$;
%         }
% \end{algorithm}%\vspace{-2mm}




The decomposition procedure is summarized in \emph{FlagRep}~\cref{alg:flagrep}. 

\begin{algorithm}[ht!]%\label{alg:train}
\caption{Robust FlagRep}\label{alg:robust flagrep}
 \textbf{Input}: {A data matrix $\mathbf{D} \in \R^{n \times p}$, column hierarchy $\cA_1\subset \cdots \subset \cA_k$, flag type $(n_1,\cdots,n_k;n)$}\\
 \textbf{Output}: {Flag $[\![\Q]\!]$, weights $\bR$, perm. $\bP$} \\[0.25em]
     \For{$i\gets1$ \KwTo $k$}{
         $\cB_i \gets \cA_i \setminus \cA_{i-1}$; \\
         $\mathbf{B}_i  \gets \mathbf{D}(1:\mathrm{end},\cB_i) \in \R^{n \times |\cB_i|}$;\\
         $\bP_i \gets  \left[ \mathbf{e}_{b_{i,1}}| \mathbf{e}_{b_{i,2}}| \cdots| \mathbf{e}_{b_{i,|\cB_i|}} \right]$
        }
     \For{$i\gets1$ \KwTo $k$}{
         $\Q_i \gets \mathrm{get\_basis}(\bP_{\Q_{i-1}^\perp \cdots \Q_0^\perp} \B_i, n_i-n_{i-1})$;\\
         $\bR_i \gets 
             \begin{bmatrix} \mathbf{0} |\cdots|\mathbf{0}| \B_i | \cdots | \B_k \end{bmatrix}\bP_{\Q_{i-1}^\perp \cdots \Q_0^\perp}^\top\Q_i $;\\
         $\bP_{\Q_{i}^\perp \cdots \Q_0^\perp} \gets (\I - \Q_{i}\Q_{i}^\top)\bP_{\Q_{i-1}^\perp \cdots \Q_0^\perp}$;\\
        }
     $\Q \gets \begin{bmatrix}\Q_1 | \Q_2 | \cdots | \Q_k\end{bmatrix}$;\\
     $\bR \gets \begin{bmatrix}\bR_1 | \bR_2 | \cdots | \bR_k \end{bmatrix}^\top$;\\
     $\bP \gets \begin{bmatrix} \bP_1 | \bP_2 | \cdots | \bP_k \end{bmatrix}$;
     % $(n_1,n_2,\dots,n_k) =\mathrm{cumsum}(m_1,m_2,\dots,m_k)$;\\ % $n_i = \sum_{j=1}^{i-1}m_i$.
\end{algorithm}%\vspace{-2mm}