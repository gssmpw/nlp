\section{Brainstorm}


\paragraph{FlagRep}
\begin{itemize}
    \item Will not work with PSA because PSA outputs a flag type which does not result in a feature hierarchy. It results in a hierarchical subspace representation for data.
    \item  Use FlagRep to represent data, then perform downstream tasks. E.g., dimensionality reduction for visualization of a collection of datasets, classification of datasets, etc...
    \item We should apply it to some of the experiments from~\cite{mankovich2022flag,mankovich2023subspace}.
    \item A little bit more theory... take a data matrix $\mathbf{D}$, and a hierarchical feature structure $\mathcal{A}$, FlagRep seeks a flag $[\![\X]\!]$ and a flag type that is closest to $\mathbf{D}$ and respects the hierarchical structure encoded in $\mathcal{A}$. This optimization problem is something like
    \begin{equation}
        \argmin_{\lambda \in \Psi(\mathcal{A}), [\![\X]\!] \in \flag(\lambda)} \sum_{j} d([\X_j],\mathbf{D}_{\mathcal{A}_j}) 
    \end{equation}
    where $\psi(\mathcal{A})$ is the function that outputs the set of all possible flag types from $\mathcal{A}$ and $d$ is some dissimilarity measure that maps $d:\Gr(k_1,n)\times \R^{n \times k_2}\rightarrow \R$.
    \item Use PSA to find the flag type, then do PCA. Then do a flag representation of the PCs.
\end{itemize}





\paragraph{Rotationally invariant classifiers}
Do PCA, but then do a classifier which is rotationally invariant.
Or change pooling or add flags to this:~\url{https://www.ijcai.org/Proceedings/2024/0564.pdf}

\paragraph{Principled Flag Averaging}

\textbf{Averaging different flag types}
\begin{enumerate}
    \item A collection of datasets with $p^{(i)} \gg d_k^{(i)}$ features $\mathbf{D}^{(i)} \in \R^{d \times p^{(i)}}$
    \item Run PCA for $d_k^{(i)}$
    \item Find flag type $\lambda_i = (d_1^{(i)},\dots,d_k^{(i)};d)$ of each $i$ using principal subspaces
    \item Input a dataset of flags of different types
    \begin{equation*}
        \{[\![ \X^{(i)} ] \!] \in \flag(\lambda_i)\}
    \end{equation*}
    \item Define the average
    \begin{equation*}
        \argmin_{\lambda, [\![\Y]\!] \in \flag(\lambda)} \sum_{i} d_c^2 ([\![ \X^{(i)} ] \!],[\![\Y]\!])
    \end{equation*}
    \item On the Stiefel manifold, this is
    \begin{equation*}
        \argmin_{\lambda, \Y \in St(d_k,d)} \sum_{i=1}^p \sum_{j=1}^{k} d_j^{(i)} - \text{tr}(\Y_j^T\X_j^{(i)}{\X_j^{(i)}}^T\Y_j)
    \end{equation*}
    \item We approximating this by taking $\lambda= \hat{\lambda}$ as the most common flag type in $\{\lambda_i\}_{i=1}^p$
    \item Look closer at the objective function. Each chordal distance will be computed according to the flag type $i$
    \begin{equation*}
        \sum_{i=1}^p \sum_{j=1}^{k_i} \min(d_j^{(i)},\hat{d}_j)  - \text{tr}(\Y_j^T\X_j^{(i)}{\X_j^{(i)}}^T\Y_j)
    \end{equation*}
    \item Then we construct the $\mathbf{P}_j$s as before and we're done!
    \begin{equation*}
        \mathbf{P}_j = \sum_{i=1}^p\X_j^{(i)}{\X_j^{(i)}}^T
    \end{equation*}
    \end{enumerate}

    
\textbf{Flag type of a subspace average}
\begin{enumerate}
    \item Input a dataset of subspaces of the same dimension
    \begin{equation*}
        \{[ \X^{(i)} ]  \in Gr(d_k,d)\}
    \end{equation*}
    \item Find their average flag
    \begin{enumerate}
        \item Subspace optimization (easier)
        \begin{align*}
            &\argmin_{\lambda, \Y \in St(d_k,d)} \sum_{i=1}^p \sum_{j=1}^{k} d_j - \text{tr}(\I_j\Y^T\X^{(i)}{\X^{(i)}}^T\Y)\\
            &\argmin_{\lambda, \Y \in St(d_k,d)} \sum_{j=1}^{k} d_j - \text{tr}(\I_j\Y^T\mathbf{P}\Y)\\
            &\argmax_{\lambda, \Y \in St(d_k,d)} \text{tr}(\Y^T\mathbf{P}\Y)
        \end{align*}
        This is equivalent to doing an SVD of $\mathbf{P}$. Maybe we have to do this with maximum likelihood estimation... 
        \item Robust optimization: make flags out of the data (harder)
        \begin{equation*}
            \argmin_{\lambda, \Y \in St(d_k,d)} \sum_{i=1}^p \sum_{j=1}^{k} d_j - \text{tr}(\I_j\Y^T\X^{(i)}\I_j{\X^{(i)}}^T\Y)
        \end{equation*}
    \end{enumerate}
\end{enumerate}
Perhaps use the Hierarchical clustering strategy on a hierarchical subfamily of PSA models for the flag mean. Select the hierarchical subfamily as all those possible flag types that have maximum dimension selected via the order fitting rule for subspace averaging.


\textbf{Order fitting rule for chordal flag averaging}
Following \url{https://gtas.unican.es/files/pub/sspv6_final.pdf} ... but doesn't really work.

\paragraph{Replace pymanopt from chordal flag median}
Use a different Stiefel manifold solver. For example: \url{https://arxiv.org/pdf/2409.01770}


\paragraph{Flag optimization}
Second order optimization using parallel transport or other methods using flag optimization.~\url{https://github.com/tomszwagier/flag-manifold-distance/blob/main/flag-manifold-distance.py} and ~\url{https://drive.google.com/file/d/1GNPethyUszRmPAceluKV-cReDbnKG2AK/view}. BFGS, Gauss newton. 2nd order optimization using parallel transport. ~\url{https://arxiv.org/pdf/2408.06054} ~\url{https://github.com/dnguyend/par-trans/blob/main/examples/NumpyFlagParallel.ipynb}




\paragraph{FlagRep Inverse}
\begin{equation*}
   \text{FlagRep}(\mathbf{D}, \mathcal{A}_1 \subset \cdots \subset \mathcal{A}_k) =  [\![\X]\!] \in \flag(n_1,\dots,n_k;n)
\end{equation*}
Assume that the order of the columns of 
\begin{equation*}
    \mathbf{D} = [\mathbf{D}_1,\cdots,\mathbf{D}_k]
\end{equation*}
are ordered according to $\mathcal{A}_1 \subset \cdots \subset \mathcal{A}_k$ Then we can easily reconstruct our data as
\begin{equation*}
    [\![\X]\!] \mapsto \begin{bmatrix}\X_1 \tilde{\boldsymbol{\Sigma}}_1 \tilde{\V}_1^T & \cdots & \X_k \tilde{\boldsymbol{\Sigma}}_k \tilde{\V}_k^T \end{bmatrix}.
\end{equation*}
Where $\tilde{\boldsymbol{\Sigma}}_i, \tilde{\V}_i$ is the truncation of $\boldsymbol{\Sigma }_i, \V_i$ to the first $m_i$ values. Notice, when $m_i = |\mathcal{A}_i|$ for all $i$ this is the inverse of FlagRep.
\textcolor{blue}{How does this work for a bunch of random $\mathbf{D}$ and feature hierarchies and truncations. How does it compare to SVD and QR?}



\paragraph{Miscellaneous}
\begin{enumerate}
    \item \textbf{Projective flags}. Projecting flags onto flags. Take multiple pictures of a (infinite) line inside an (infinite) plane. Can we recover the orientation of the plane from the pictures? Dumb solution: triangulate each line, fit a plane to lines. Then project lines onto fitted plane. Then, can we use this in non-linear spaces (e.g., sphere + geodesics)?~\url{https://robots.ox.ac.uk/~vgg/publications/2004/Hartley04/hartley04.pdf}
    \item \textbf{Flag order estimation}. Nacho, iterative PCAs, iterative SVDs, etc... Dimension estimation of a dataset. Is it already answered?~\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2835171/}
    \item \textbf{Nonlinear flags}. \url{https://arxiv.org/pdf/2303.15184}, \url{https://link.springer.com/chapter/10.1007/978-1-4612-4162-1_14}, \url{https://math.berkeley.edu/~jawolf/publications.pdf/paper_133.pdf}
    \item \textbf{Affine flags.}~\url{https://openaccess.thecvf.com/content/CVPR2024/papers/Dogadov_Fitting_Flats_to_Flats_CVPR_2024_paper.pdf}
    \item \textbf{Apply flag PCA.} to this: \url{https://snap-research.github.io/weights2weights/}
    \item \textbf{A journal paper.} On the flag-perspective of PCA.
    \item \textbf{Use flag distances.} Instead of subspace distances or other distances? I'm not sure where this might be useful...
    \item \textbf{Flag of inliers} If you have data with a flag with the smallest dimension subspace containing the inliers then increasing dimensions contain more and more outliers. So we can frame it as ``robust flag recovery"~\url{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8425657}. But this might just be a re-formulation of Flag PCA and therefore not too novel...
\end{enumerate}






\subsection{To do}

\paragraph{Clustering of videos (Mind's eye + Youtube)}
Visualize, then LBG clustering


\paragraph{Data visualization}
papers \url{https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf}


\paragraph{In deep learning}
papers: \url{https://arxiv.org/abs/1905.05929}, \url{https://arxiv.org/abs/2311.17475}



