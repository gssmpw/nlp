\section{Introduction}\label{sec:intro}
Hierarchical structures are fundamental across a variety of fields: they shape taxonomies and societies~\cite{lane2006hierarchy}, allow us to study 3D objects~\cite{leng2024hypersdfusion}, underpin neural network architectures~\cite{yan2015hd}, and form the backbone of language~\cite{longacre1966hierarchy}. They reflect parts-to-whole relationships~\cite{taher2024representing} and how our world organizes itself compositionally~\cite{salthe1985evolving}.
However, when handling hierarchies in data, we often resort to the temptation to \textit{flatten} them for simplicity, losing essential structure and context in the process. This tendency is evident in standard dimensionality reduction techniques, like Principal Component Analysis, which ignore any hierarchy the data contains.

In this work, we advocate for an approach rooted in flags to preserve the richness of hierarchical linear subspaces. A flag~\cite{Mankovich_2023_ICCV,mankovich2024fun} represents a sequence of nested subspaces with increasing dimensions, denoted by its type or signature $(n_1,n_2,\dots,n_k;n)$, where $n_1$$<$$n_2$$<$$\dots$$<$$n_k$$<$$n$. For instance, a flag of type $(1,2;3)$ describes a line within a plane in $\mathbb{R}^3$. By working in flag manifolds—structured spaces of such hierarchical subspaces—we leverage the full complexity of hierarchical data. Flag manifolds have already shown promise in extending traditional methods like PCA~\cite{pennec2018barycentric, mankovich2024fun, szwagier2024curseisotropyprincipalcomponents}, Independent Component Analysis (ICA)~\cite{nishimori2006riemannian,nishimori2006riemannian0,nishimori2006riemannian1,nishimori2007flag,nishimori2008natural}, generalizing subspace learning~\cite{szwagier2025nestedsubspacelearningflags}, and Self-Organizing Maps (SOM)~\cite{ma2022self}. They enable robust representations for diverse tasks: averaging motions~\cite{Mankovich_2023_ICCV}, modeling variations in face illumination~\cite{draper2014flag, Mankovich_2023_ICCV}, parameterizing 3D shape spaces~\cite{ciuclea2023shape}, and clustering subspaces for video and biological data~\cite{marrinan2014finding, mankovich2022flag, Mankovich_2023_ICCV, mankovich2023module}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/Concept.pdf}
    \caption{A flag decomposition (center) is used for a hierarchy-preserving flag representation and reconstruction. This decomposition separates a data matrix $\D$ with an associated hierarchy of column indices $\cA_1 \subset \cA_2 \subset \cA_3$ into Stiefel coordinates $\Q$ for a flag $[\![\Q]\!]$, a block-upper triangular matrix $\bR$, and a permutation matrix $\bP$ (not pictured). Example applications 
    include denoising (green), clustering (yellow), and few-shot learning (orange).\vspace{-3mm}}
    \label{fig:concept}
\end{figure}
Our main contribution is a Flag Decomposition (FD) specifically designed to preserve hierarchical structures within data (see~\cref{fig:concept}). First, we formalize the notion of hierarchies in data and the definition of a flag (\S\ref{sec:bg}). Next, we provide a practical algorithm for deriving FDs (\S\ref{sec:methods}) and outline multiple promising applications (\S\ref{sec:apps}). Then we demonstrate its robustness in clustering tasks involving noisy and outlier-contaminated simulated data (\S\ref{sec:results}). Our approach outperforms standard methods, such as Singular Value Decomposition (SVD), in clustering and denoising hyperspectral satellite images. Finally, we show that using flags as prototypes in a few-shot framework improves classification accuracy on benchmark datasets. Final remarks are in (\S\ref{sec:conclusion}) and formal proofs are in the suppl. material. Our implementation is \url{https://github.com/nmank/FD}.


% \clearpage
% \newpage