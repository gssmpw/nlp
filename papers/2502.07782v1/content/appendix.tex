

% \section{Hierarchy examples}
% \tolga{}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{figures/pixel_sampling.pdf}
%     \caption{Neighborhood hierarchy.}
%     % \label{fig:enter-label}
% \end{figure}

We provide 
%examples for improved intuition behind the FD in~\cref{sec:intuition}, 
alternative methods for flag recovery in~\cref{sec:svd_and_qr},
proofs of each proposition in~\cref{sec:proofs}, a discussion of block matrix decompositions in~\cref{sec:alg_rev}, a formal presentation of the~\algname~algorithm in~\cref{sec:algs}, and additional details for the results in~\cref{sec:extra_experiments}.

\section{SVD and QR for Flag Recovery}\label{sec:svd_and_qr}
The SVD and QR decomposition recover individual subspaces of the flag $[\Q_i]$ and, in certain hierarchies, recover the entire flag $[\![\Q]\!]$. 

\paragraph{Subspace recovery} Recall the optimization problem:
\begin{equation}
    \Q_i =  \argmin_{\X \in St(m_i,n)} \sum_{j \in \cB_j}\| \boldsymbol{\Pi}_{\X^\perp} \boldsymbol{\Pi}_{\Q^\perp_{i-1}} \cdots \boldsymbol{\Pi}_{\Q^\perp_1} \tilde{\mathbf{d}}_j \|_r^q.
\end{equation}
For the QR decomposition with pivoting, we have $\boldsymbol{\Pi}_{\Q^\perp_{i-1}} \cdots \boldsymbol{\Pi}_{\Q^\perp_0} \B_i = \Q_i' \bR_i'{\boldsymbol{\Pi}_i'}^\top$, the columns of $\Q_i$ correspond to columns of $\Q_i'$ associated with non-zero rows of $\bR_i'$. For the case of SVD where $\boldsymbol{\Pi}_{\Q^\perp_{i-1}} \cdots \boldsymbol{\Pi}_{\Q^\perp_0} \B_i = \U_i \bm{\Sigma}_i \V_i^\top$, the columns of $\Q_i$ are the columns of $\U_i$ associated with the non-zero singular values.


\paragraph{Flag recovery} Both the SVD and QR decomposition can be used for flag recovery for certain column hierarchies and flag types.
\begin{exmp}[QR decomposition]
    For a tall and skinny ($p \leq n$) $\D$ with the column hierarchy $\{1,\dots,p_1\}\subset \{1,\dots,p_2\} \subset \cdots \subset \{1,\dots,p\}$ and the flag type is $(p_1,p_2,\dots,p;n)$, the QR decomposition $\D = \Q \bR$ outputs the hierarchy-preserving flag $[\![\Q]\!] \in \flag(p_1,p_2,\dots,p;n)$ because $[\mathbf{q}_1 | \mathbf{q}_2|\cdots |\mathbf{q}_i] = [\mathbf{d}_1 | \mathbf{d}_2|\cdots |\mathbf{d}_i]=[\D_{\cA_i}]$ for $i=1,2\dots,k$.
\end{exmp}

\begin{exmp}[SVD]
    Suppose $\D$ has the column hierarchy $\{1,\dots,p\}$ and the rank $n_k$. The SVD of $\D$ is $\D =\U \bm{\Sigma} \V^\top$. Let $\Q$ be the $n_k$ left singular vectors (columns of $\U$) associated with non-zero singular values. Then $[\![\Q]\!] \in \flag(n_k;n)$ is a hierarchy-preserving flag because $[\Q] = [\D]$.
\end{exmp}
% \section{Intuition}\label{sec:intuition}
% In addition to Ex. 2.1-3, we visualize the advantage of our FD in recovering a flag of given type from a hierarchical dataset in~\cref{fig:flag cartoon1}. \cref{fig:flag cartoon2} highlights the advantage of FD in reconstructing a hierarchical dataset. We refer reviewers to figure captions.  
% \begin{figure}[ht!]
%     \centering
%     \vspace{-4mm}
%     \includegraphics[width=\linewidth]{figures/advantage0.pdf}
%     \caption{\footnotesize \textbf{Flag recovery}. We recover a flag from $\D$ with hierarchy $\cA_1 \subset \cA_2$. Columns of $\D$ are plotted as points with $\cA_1$ in blue and $\cA_2 \setminus \cA_1$ in orange. FD is the only method that recovers the flag (line inside plane). SVD correctly recovers the plane but not the line whereas QR only recovers the line and the plane misses the orange points.}
%     \label{fig:flag cartoon1}
% \end{figure}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/flag_faces.pdf}
%     \footnotesize
%     \caption{\footnotesize \textbf{Reconstruction}. Images from the YaleFaceDatabase are flattened and horizontally stacked into $\D$. We use the hierarchy with the images of the first subject as $\cA_1$ and all images as $\cA_2$. We run FD (flag type $(1,2)$) and baselines (rank $2$). FD is the only method to correctly reconstruct the subjects. We plot the basis vectors (eigenfaces) on the right and find FD extracts basis elements that most closely resemble the subjects.}
%     \label{fig:flag cartoon2}
% \end{figure}

\section{Theoretical Justifications}\label{sec:proofs}
We prove each proposition from the Methods section. For the sake of flow, we re-state the propositions from the Methods section before providing the proofs. Throughout these justifications we use $\mathrm{rank}(\D)$ as the dimension of the column space of $\D$. This is equivalent to the dimension of the subspace spanned by the columns of $\D$, denoted $\mathrm{dim}([\D])$

\begin{prop}
    A data matrix $\D$ admits a flag decomposition of type $(n_1,n_2, \cdots, n_k; n)$ if and only if $\cA_1 \subset \cA_2 \subset \cdots \subset \cA_k$ is a column hierarchy for $\D$.
\end{prop}

\begin{proof}
    We first tackle the forward direction. Suppose $\D$ admits a flag decomposition with the hierarchy $\cA_1 \subset \cA_2 \subset \cdots \subset \cA_k$. Then $\D = \Q \bR \bP^\top$ and $\D \bP = \Q \bR$ because $\bP$ is a permutation matrix. 
    Define 
    \begin{equation}
        \B = [\B_1| \B_2 | \cdots | \B_k] = \D \bP = \Q\bR.
    \end{equation} 
    Since we have a flag decomposition, $[\![\Q]\!] \in \flag(n_1,n_2,\dots,n_k;n)$ with $\Q = [\Q_1|\Q_2|\cdots|\Q_k] \in St(n_k,n)$. Since $\Q$ is in Stiefel coordinates we have $\Q_{i-1}^T\Q_{i} = \bm{0} \in \R^{m_{i-1} \times m_i}$, so
    \begin{equation}\label{eq:q_ineq}
        \mathrm{dim}([\Q_1, \Q_2, \dots,\Q_{i-1}]) < \mathrm{dim}([\Q_1, \Q_2, \dots,\Q_i]).
    \end{equation} 
    Since $[\![\Q]\!]$ is hierarchy preserving, for $i=1,2,\dots,k$ we have $[\B_1,\B_2,\dots,\B_i] = [\Q_1, \Q_2, \dots,\Q_i]$. Using this and ~\cref{eq:q_ineq}, we have
    \begin{equation}\label{eq:dim_ineq_b}
        \mathrm{dim}([\B_1, \B_2, \dots,\B_{i-1}]) < \mathrm{dim}([\B_1, \B_2, \dots,\B_i]).
    \end{equation}
    By construction $\mathrm{dim}([\B_1,\B_2,\dots,\B_i]) = \mathrm{dim}([\D_{\cA_i}])$. So, using~\cref{eq:dim_ineq_b}, $\mathrm{dim}([\D_{\cA_{i-1}}]) < \mathrm{dim}([\D_{\cA_i}])$ proving $\cA_1 \subset \cA_2 \subset \cdots \subset \cA_k$ is a column hierarchy for $\D$.

    The backward direction is proved in~\cref{prop:stiefel_coords_app,prop:proj_prop_and_flags_app}. Specifically, given a data matrix with an associated column hierarchy,~\cref{prop:stiefel_coords_app} describes how to find a hierarchy-preserving flag. Then~\cref{prop:proj_prop_and_flags_app} shows how to find the permutation matrix $\bP$ from the column hierarchy and the weight matrix $\bR$ from $\Q$ so that $\D = \Q \bR \bP^\top$.
    \end{proof}


% \begin{prop}\label{prop:projection_property}
%     Suppose $\cA_1 \subset \cA_2 \subset \cdots \subset \cA_k$ is a column hierarchy for $\D$. Then there exists a $\Q= [\Q_1\,|\,\Q_2\,|\, \cdots\,|\,\Q_k] \in St(n_k,n)$ that satisfies (for $i=1,2\dots,k$) $[\Q_i] = [\boldsymbol{\Pi}_{\Q_{i-1}^\perp}\cdots \boldsymbol{\Pi}_{\Q_1^\perp}\B_i]$ and the \textbf{projection property}: 
%     \begin{equation}\label{eq:projection_property}
%     \boldsymbol{\Pi}_{\Q_i^\perp}\boldsymbol{\Pi}_{\Q_{i-1}^\perp}\cdots \boldsymbol{\Pi}_{\Q_1^\perp} \B_i = 0.
%     \end{equation}
% \end{prop}
\begin{prop}\label{prop:stiefel_coords_app}
    Suppose $\cA_1 \subset \cA_2 \subset \cdots \subset \cA_k$ is a column hierarchy for $\D$. Then there exists $\Q= [\Q_1\,|\,\Q_2\,|\, \cdots\,|\,\Q_k]$ that are coordinates for the flag $[\![\Q]\!]\in\flag(n_1,n_2,\dots,n_k;n)$ where $n_i = \mathrm{rank}(\D_{\mathcal{A}_i})$ that satisfies $[\Q_i] = [\boldsymbol{\Pi}_{\Q_{i-1}^\perp}\cdots \boldsymbol{\Pi}_{\Q_1^\perp}\B_i]$ and the \textbf{projection property} (for $i=1,2\dots,k$): 
    \begin{equation}\label{eq:projection_property_app}
    \boldsymbol{\Pi}_{\Q_i^\perp}\boldsymbol{\Pi}_{\Q_{i-1}^\perp}\cdots \boldsymbol{\Pi}_{\Q_1^\perp} \B_i = \bm{0}.
    \end{equation}
\end{prop}
\begin{proof}
    For $i=1$ define $m_1 = n_1 = \mathrm{rank}(\B_1) = \mathrm{rank}(\D_{\cA_1})$. Now define $\C_1 = \B_1$ and $\Q_1 \in St(m_1,n)$ whose columns are an orthonormal basis for the column space of $\C_1$, specifically $[\Q_1] = [\C_1]$.
    
    \textbf{For ease of notation, denote} $\Q_{:i} = [\Q_1|\Q_2|\cdots|\Q_{i}]$. Define (for $i=2,3,\dots,k$) the projector onto the null space of $[\Q_1,\Q_2,\dots,\Q_i]$, as 
    \begin{equation}
        \boldsymbol{\Pi}_{\Q_{:i}^\perp} = \I - \Q_{:i}\Q_{:i}^\top.
    \end{equation}
    We use this to define $\C_i$ through 
    \begin{equation}
        \C_i = \boldsymbol{\Pi}_{\Q_{:i-1}^\perp}\B_i
    \end{equation} 
    and $\Q_i \in St(m_i,n)$ so that $[\Q_i] = [\C_i]$.

    We use mathematical induction to prove the following:
        \begin{enumerate}
        \item \emph{Non-zero} $\C_i \neq \bm{0}$,
        \item \emph{Coordinates} $\Q_{:i} = [\Q_1|\Q_2| \cdots | \Q_i]$ is in Stiefel coordinates (\eg, $\Q_{:i}\Q_{:i}^\top = \I$),
        \item \emph{Hierarchy} $[\B_1,\B_2,\dots,\B_i] = [\Q_1,\Q_2, \dots, \Q_i]$,
        \item \emph{Projection property} $\boldsymbol{\Pi}_{\Q_i^\perp} \cdots\boldsymbol{\Pi}_{\Q_1^\perp} \B_i = \bm{0}$ and \\
        $\boldsymbol{\Pi}_{\Q_i^\perp} \cdots\boldsymbol{\Pi}_{\Q_1^\perp}= \boldsymbol{\Pi}_{\Q_{:i}^\perp}$, %\nate{Using $\boldsymbol{\Pi}_{[\Q_1|\cdots|\Q_i]^\perp}$ is more intuitive, computationally efficient. So, the projection property should really be stated with this.} 
        \item \emph{Dimensions} $\Q_i \in St(m_i,n)$ with $m_i = n_i - n_{i-1}$ where $n_i = \mathrm{rank}(\D_{\cA_i})$.
    \end{enumerate}

    We proceed with the base case $i=1$. (1) $\C_1 = \B_1 = \D_{\cA_1} \neq \bm{0}$. (2) $\Q_1 \in St(n_1,n)$ because its columns form an orthonormal basis for $[\C_1]$. (3) $[\B_1] = [\C_1] = [\Q_1]$. (4) Since $\boldsymbol{\Pi}_{\Q_1^\perp}$ projects into the nullspace of $\Q_1$ and $[\Q_1] = [\B_1]$, we have $\boldsymbol{\Pi}_{\Q_1^\perp} \B_1 = \bm{0}$. (5) Since $m_1 = n_1$, $n_1 = \mathrm{\dim}([\Q_1]) = \mathrm{dim}([\B_1]) = \mathrm{dim}([\D_{\cA_1}])$, and the columns of $\Q_1$ form an orthonormal basis, we have $\Q_1 \in St(m_1,n)$.
    
    Fix some $j \in \{2,3,\dots,k\}$. Suppose statements (1-5) hold true for all $i < j$.

    \paragraph{1. Non-zero} By way of contradiction, assume $\C_j = \bm{0}$. Then $\boldsymbol{\Pi}_{\Q_{:j-1}^\perp}\B_j=\bm{0}$. This means each column of $\B_j$ is in the column space of $\Q_{:j-1}$. In terms of subspaces, this implies
    \begin{equation}\label{eq:bj_subset_q}
        [\B_j] \subseteq [\Q_1,\Q_2,\dots,\Q_{j-1}] = [\B_1,\B_2,\dots,\B_{j-1}]
    \end{equation}
    where the second equality follows from the induction hypothesis part 3. \cref{eq:bj_subset_q} implies 
    \begin{equation}\label{eq:b_eq}
        \mathrm{dim}([\B_1,\B_2,\dots,\B_j])
       = \mathrm{dim}([\B_1,\B_2,\dots,\B_{j-1}]).
    \end{equation}
    By construction (see first paragraph of Methods section), 
    \begin{equation}\label{eq:span_constr}
        \mathrm{dim}([\D_{\cA_j}]) = \mathrm{dim}([\B_1,\B_2,\dots,\B_j]).
    \end{equation}
    So~\cref{eq:b_eq,eq:span_constr} imply $\mathrm{rank}(\D_{\cA_j}) = \mathrm{rank}(\D_{\cA_{j-1}})$. This contradicts the assumption of a column hierarchy for $\D$, namely $\mathrm{rank}(\D_{\cA_j}) > \mathrm{rank}(\D_{\cA_{j-1}})$.

    \paragraph{2. Coordinates} It suffices to show 
    \begin{equation}
        \Q_j^\top\Q_{:j-1} = [\Q_j^\top \Q_1| \Q_j^\top \Q_2| \cdots | \Q_j^\top \Q_{j-1}] = \bm{0}
    \end{equation} 
    which is equivalent to showing $[\Q_j]$ is orthogonal to $[\Q_1,\Q_2,\dots,\Q_{j-1}]$. By construction, 
    \begin{equation}
        [\Q_j] = [\C_j] = [\boldsymbol{\Pi}_{\Q_{:j-1}^\perp}\B_j]
    \end{equation} 
    which is orthogonal to $[\Q_1,\dots,\Q_{j-1}]$.

    \paragraph{3. Hierarchy} Using $\Q_j^\top\Q_{:j-1} = \bm{0}$, we have
    \begin{align}\label{eq:proj_prod_prop}
        \begin{aligned}
            \boldsymbol{\Pi}_{\Q_{:j}^\perp} &= \I -\Q_{:j}\Q_{:j}^\top \\
               &= \I -\sum_{\ell = 1}^j\Q_{\ell}\Q_{\ell}^\top\\
               &= \I - \Q_j \Q_j^\top - \Q_{:j-1}\Q_{:j-1}^\top \\
               &= \I - \Q_j \Q_j^\top - \Q_{:j-1}\Q_{:j-1}^\top \\
               &+ \Q_j \underbrace{\Q_j^\top \Q_{:j-1}}_{\bm{0}}\Q_{:j-1}^\top,\\
               &= (\I - \Q_j \Q_j^\top)(\I - \Q_{:j-1}\Q_{:j-1}^\top),\\
               &= \boldsymbol{\Pi}_{\Q^\perp_j} \boldsymbol{\Pi}_{\Q_{:j-1}^\perp}.\\
        \end{aligned}
    \end{align}
    By ~\cref{eq:proj_prod_prop} and the construction $[\Q_j] = [\boldsymbol{\Pi}_{\Q_{:j-1}^\perp}\B_j]$, we have
    \begin{align*}
        \bm{0} &= \boldsymbol{\Pi}_{\Q_j^\perp}\boldsymbol{\Pi}_{\Q_{:j-1}^\perp}\B_j\\
               &= \boldsymbol{\Pi}_{\Q_{:j}^\perp}\B_j,\\
               &= (\I - \Q_{:j}\Q_{:j}^\top)\B_j,\\
        \B_j   &= \Q_{:j}\Q_{:j}^\top \B_j.
    \end{align*}
    Thus $[\B_j] \subseteq [\Q_1,\Q_2,\cdots ,\Q_j]$. By the induction hypothesis (3), $[\B_1,\dots,\B_{j-1}] = [\Q_1,\dots,\Q_{j-1}]$. So $[\B_1,\B_2,\dots,\B_j] \subseteq [\Q_1,\Q_2, \dots, \Q_j]$.
    
    In contrast $[\B_j] \supseteq [\boldsymbol{\Pi}_{\Q_{:j-1}^\perp}\B_j] = [\C_j] = [\Q_j]$. So, also using $[\B_1,\dots,\B_{j-1}] = [\Q_1,\dots,\Q_{j-1}]$ (induction hypothesis 3),  we have $[\B_1,\B_2,\dots,\B_j] \supseteq [\Q_1,\Q_2,\dots,\Q_j]$. %Therefore $[\B_1,\B_2,\dots,\B_j] = [\Q_1,\Q_2,\dots,\Q_j]$.

    \paragraph{4. Projection property} Using~\cref{eq:proj_prod_prop} and the induction hypothesis (4) that $\boldsymbol{\Pi}_{\Q_{:j-1}^\perp} = \boldsymbol{\Pi}_{\Q^\perp_{j-1}} \cdots \boldsymbol{\Pi}_{\Q^\perp_1}$, we have
    \begin{align}\label{eq:proj_identity_proof}
    \begin{aligned}
         \boldsymbol{\Pi}_{\Q_{:j}^\perp} &= \boldsymbol{\Pi}_{\Q^\perp_j} \boldsymbol{\Pi}_{\Q_{:j-1}^\perp},\\
               &= \boldsymbol{\Pi}_{\Q^\perp_j} \boldsymbol{\Pi}_{\Q^\perp_{j-1}}\cdots \boldsymbol{\Pi}_{\Q^\perp_1}.
   \end{aligned}
    \end{align}
    By construction $[\Q_j] = [\boldsymbol{\Pi}_{\Q_{:j-1}^\perp}\B_j]$. Thus 
    \begin{equation*}
        \boldsymbol{\Pi}_{\Q^\perp_j}\boldsymbol{\Pi}_{\Q_{:j-1}^\perp}\B_j = \bm{0}.
    \end{equation*}
    Using~\cref{eq:proj_identity_proof}, we have
    \begin{equation*}
        \boldsymbol{\Pi}_{\Q^\perp_j} \boldsymbol{\Pi}_{\Q^\perp_{j-1}}\cdots \boldsymbol{\Pi}_{\Q^\perp_1}\B_j = \bm{0}.
    \end{equation*}    
    
    
    % \begin{align*}
    %     \bm{0} &= \boldsymbol{\Pi}_{[\Q_1|\cdots|\Q_j]^\perp}\B_j,\\
    %            &= (\I - \Q_j \Q_j^\top - [\Q_1|\dots|\Q_{j-1}][\Q_1|\dots|\Q_{j-1}]^\top \\
    %            &+ 2\Q_j \underbrace{\Q_j^\top [\Q_1|\dots|\Q_{j-1}]}_{\bm{0}}[\Q_1|\dots|\Q_{j-1}]^\top)\B_j,\\
    %            &= (\I - \Q_j \Q_j^\top)(\I - [\Q_1|\dots|\Q_{j-1}][\Q_1|\dots|\Q_{j-1}]^\top)\B_j,\\
    %            &= \boldsymbol{\Pi}_{\Q^\perp_j} \cdots \boldsymbol{\Pi}_{\Q^\perp_1}\B_j.
    % \end{align*}

    \paragraph{5. Dimensions} By the induction hypothesis (5), $\Q_i \in St(m_i,n)$ for $i=1,2,\dots,j-1$. So $\Q_{:j-1} \in St(n_{j-1},n)$ with $n_{j-1} = \sum_{i=1}^{j-1} m_i$. Let 
    \begin{align*}
        n_j &= \mathrm{rank}(\D_{\cA_j}), \\
        &= \mathrm{dim}([\B_1,\B_2,\dots,\B_j]),\\
        &= \mathrm{dim}([\Q_1,\Q_2,\dots,\Q_j]).
    \end{align*}
    Thus $\Q_{:j} \in \R^{n \times n_j}$ and $\Q_j \in \R^{n \times m_j}$ with $m_j = n_j - n_{j-1}$. $\Q_j$ has orthonormal columns by construction, so $\Q_j \in St(m_j,n)$.

    
    By way of mathematical induction, we have proven (1-5) for all $i=1,2,\dots,k$. Specifically, given a column hierarchy on $\D$, we have found coordinates for a hierarchy-preserving flag $[\![\Q]\!] \in \flag(n_1,n_2,\dots,n_k;n)$ that satisfies the projection property.
\end{proof}

Although we can write $\bR_{i,j} = \Q_i^\top\B_j$ for $j \geq i$, an equivalent definition is provided in~\cref{eq:R_and_P_app} because it is used in~\cref{alg:FD}.

\begin{prop}\label{prop:proj_prop_and_flags_app}
 Suppose $\cA_1 \subset \cA_2 \subset \cdots \subset \cA_k$ is a column hierarchy for $\D$. Then there exists some hierarchy-preserving 
 $[\![\Q]\!] \in \flag(n_1,n_2,\dots, n_k;n)$ (with $n_i = \mathrm{rank}(\D_{\mathcal{A}_i})$) 
 that satisfies the projection property of $\D$ and can be used for a flag decomposition of $\D$ with
    \begin{align}\label{eq:R_and_P_app}
    \bR_{i,j} &= 
        \begin{cases}
            \Q_i^\top\boldsymbol{\Pi}_{\Q_{i-1}^\perp}\cdots \boldsymbol{\Pi}_{\Q_1^\perp} \B_i, &i=j\\
            \Q_i^\top\boldsymbol{\Pi}_{\Q_{i-1}^\perp}\cdots \boldsymbol{\Pi}_{\Q_1^\perp} \B_j, &i < j
        \end{cases},\\
        \bP_i &= \left[ \,\mathbf{e}_{b_{i,1}}\,|\, \mathbf{e}_{b_{i,2}}\,|\, \cdots\,|\, \mathbf{e}_{b_{i,|\cB_i|}} \right]
    \end{align}
    where $\{b_{i,j}\}_{j=1}^{|\cB_i|} = \cB_i$ and $\mathbf{e}_{b}$ is the $b_{i,j}$$^{\mathrm{th}}$ standard basis vector.
\end{prop}
\begin{proof}
    We define the permutation matrix $\bP = [\bP_1|\bP_2|\cdots |\bP_k]$ in~\cref{eq:R_and_P_app}. Specifically, we assign the non-zero values in each column of $\boldsymbol{\Pi}_i$ to be the index of each element in $\mathcal{B}_i$. In summary, $\boldsymbol{\Pi}_i$ is defined so that $\D\bP= [\B_1|\B_2|\cdots|\B_k]$ and $\D = \B = [\B_1|\B_2|\cdots|\B_k]\bP^\top$.

    We find the coordinates $[\Q_1|\Q_2|\cdots|\Q_k] \in St(n_k,n)$ for the hierarchy-preserving flag $[\![\Q]\!] \in \flag(n_1,n_2,\dots,n_k;n)$ with $n_k = \mathrm{rank}(\D_{\cA_i})$ that satisfies the projection property using~\cref{prop:stiefel_coords_app}. 
    
    Now, we aim to find $\bR$ so that $\B = \Q \bR$. Using the projection property $\boldsymbol{\Pi}_{\Q_j}\cdots\boldsymbol{\Pi}_{\Q_1}\B_j = \bm{0}$ and the identity $\boldsymbol{\Pi}_{\Q_j}\cdots\boldsymbol{\Pi}_{\Q_1} = \boldsymbol{\Pi}_{[\Q_1|\cdots|\Q_j]^\perp}$ from~\cref{eq:proj_identity_proof}, we can write 
    \begin{equation}\label{eq:easy_r}
        \B_j = \sum_{i=1}^j\Q_i \Q_i^\top \B_j =  \sum_{i=1}^j\Q_i\bR_{i,j}.
    \end{equation}
    This is equivalent to the projection formulation of $\bR_{i,j}$ in~\cref{eq:R_and_P_app} because (for $i=1,2,\dots,k$),
    \begin{align}\label{eq:projection_qit}
    \begin{aligned}
        &\Q_i^\top\boldsymbol{\Pi}_{\Q_{i-1}^\perp}\cdots \boldsymbol{\Pi}_{\Q_1^\perp} \\
        &= \Q_i^\top\boldsymbol{\Pi}_{[\Q_{i-1}| \cdots|\Q_1]^\perp},\\
        &= \Q_i^\top(\I - [\Q_{i-1}| \cdots|\Q_1][\Q_{i-1}| \cdots|\Q_1]^\top),\\
        &= \Q_i^\top - \underbrace{\Q_i^\top [\Q_{i-1}| \cdots|\Q_1]}_{\bm{0}}[\Q_{i-1}| \cdots|\Q_1]^\top,\\
        &= \Q_i^\top. 
        \end{aligned}
    \end{align}
    Stacking the results from~\cref{eq:easy_r,eq:projection_qit} into block matrices gives $\B = \Q\bR$ with $\bR$ defined in~\cref{eq:R_and_P_app}.
\end{proof}

Recall the two optimization problems proposed in the Methods section:
\begin{equation}\label{eq:general_opt_app}
    [\![\Q]\!] = \argmin_{[\![\X]\!] \in \flag(n_1,n_2, \dots, n_k;n)} \sum_{i=1}^k \sum_{j \in \cB_i}\| \boldsymbol{\Pi}_{\X_i^\perp} \cdots \boldsymbol{\Pi}_{\X_1^\perp} \tilde{\mathbf{d}}_j \|_r^q,
\end{equation}
\begin{equation}\label{eq:iterative_opt_app}
    \Q_i =  \argmin_{\X \in St(m_i,n)} \sum_{j \in \cB_j}\| \boldsymbol{\Pi}_{\X^\perp} \boldsymbol{\Pi}_{\Q^\perp_{i-1}} \cdots \boldsymbol{\Pi}_{\Q^\perp_1} \tilde{\mathbf{d}}_j \|_r^q.
\end{equation}


\begin{prop}[Block rotational ambiguity]
    Given the FD $\D = \Q \bR \bP^\top$, any other Stiefel coordinates for the flag $[\![\Q]\!]$ produce an FD of $\D$ (via~\cref{prop:proj_prop_and_flags_app}). Furthermore, different Stiefel coordinates for $[\![\Q]\!]$ produce the same objective function values in~\cref{eq:general_opt_app,eq:iterative_opt_app} (for $i=1,\cdots,k$).
\end{prop}
\begin{proof}
    The flag manifold $\flag(n_1,n_2,\dots,n_k;n)$ is diffeomorphic to $St(n_k,n)/(O(m_1)\times \cdots \times O(m_k))$ where $m_i = n_i - n_{i-1}$. Suppose $\D = \Q \bR \bP^\top$ is a flag decomposition. Consider $\Q \mathbf{M} \in St(n_k,n)$ with $\mathbf{M} = \mathrm{diag}([\mathbf{M}_1|\mathbf{M}_2|\cdots | \mathbf{M}_k]) \in O(m_1)\times \cdots \times O(m_k)$, meaning $\M_i \in O(m_i)$ for $i=1,2,\dots,k$. 
    
    Notice $\Q$ and $\Q \mathbf{M}$ are coordinates for the same flag, $[\![\Q]\!]=[\![\Q\M]\!]$. 
    
    The key property for this proof is that right multiplication by $\M_i$ does not change projection matrices $\Q_i\Q_i^\top$. Specifically $\Q_i \Q_i^\top = \Q_i\M_i (\Q_i\M_i)^\top$ for $i=1,2,\dots,k$. 
    
    Both $\Q$ and $\M\Q$ satisfy the projection property relative to $\D$ because (for $i=1,2,\dots,k$)
    \begin{equation}\label{eq:proj_mat_equiv}
    \boldsymbol{\Pi}_{\Q_i^\perp} = \I - \Q_i \Q_i^\top = \I - \Q_i \mathbf{M}_i (\mathbf{M}_i \Q_i)^\top = \boldsymbol{\Pi}_{(\Q_i\mathbf{M}_i)^\perp}
    \end{equation}
    which implies that the objective function values in~\cref{eq:general_opt_app,eq:iterative_opt_app} (for $i=1,2,\dots,k$) are the same for $\Q$ and $\Q\mathbf{M}$. Additionally,~\cref{eq:proj_mat_equiv} implies
    \begin{align}
        \bm{0} &=\boldsymbol{\Pi}_{\Q_i^\perp}\boldsymbol{\Pi}_{\Q_{i-1}^\perp}\cdots \boldsymbol{\Pi}_{\Q_1^\perp} \B_i \\
        &= \boldsymbol{\Pi}_{(\Q_i\mathbf{M}_i)^\perp}\boldsymbol{\Pi}_{(\Q_{i-1}\mathbf{M}_{i-1})^\perp}\cdots \boldsymbol{\Pi}_{(\Q_1\mathbf{M}_1)^\perp} \B_i.
    \end{align}
    
    Since $[\![\Q]\!]$ is hierarchy-preserving and rotations do not change subspaces, we have $[\Q_1,\Q_2, \dots, \Q_i] = [\Q_1\M_1,\Q_2\M_2,\dots,\Q_i \M_i]$. Thus $[\![\Q \M]\!]$ is hierarchy-preserving.
    
    Define $\bR^{(\mathbf{M})}$ with blocks $\bR^{(\mathbf{M})}_{i,j} = (\Q_i\mathbf{M}_i)^\top\B_j$. Notice 
    \begin{align}
        \B_j &= \sum_{i=1}^j \Q_i \bR_{i,j},\\
             &= \sum_{i=1}^j \Q_i \Q_i^\top  \B_j,\\
             &= \sum_{i=1}^j (\Q_i \M_i) (\M_i \Q_i)^\top \B_j,\\
             &= \sum_{i=1}^j (\Q_i \M_i)\bR^{(\mathbf{M})}_{i,j}.
    \end{align}
    Thus $\D = (\Q\mathbf{M})\bR^{(\mathbf{M})} \bP^\top$ is a hierarchy-preserving flag decomposition.
\end{proof}

\section{Relationship to MLMD~\cite{ong2016beyond}}\label{sec:alg_rev}
The Multiscale Low Rank Matrix Decomposition (MLMD)~\cite{ong2016beyond} models $\D = \sum_i \X_i$ where each block low-rank matrix $\X_i$ models finer-grained features than $\X_{i+1}$. Suppose $\D = [\B_1|\B_2]\in \R^{n \times p}$ is of rank $n_k$ with columns sorted according to the hierarchy $\cA_1 \subset \cA_2$. The FD with flag type $(n_1,n_2;n)$ is $\D = \Q \bR$ where $\Q = [\Q_1 | \Q_2] \in St(n_k,n)$, $\Q_1 \in \R^{n \times n_1}$, and $\bR$ is block upper triangular. FD does not seek block low-rank representations for different scales, rather it extracts a hierarchy-preserving flag $[\![\Q]\!] \in \flag(n_1,n_2;n)$. Moreover, MLMD partitions $\D$ into column blocks requiring the block partition $P_2$ to be an `order of magnitude' larger than $P_1$ ($1$st par. Sec. II). Hence, it can run only when $|\cA_1| = |\cA_2|$. FD is more general being free of this restriction. Suppose $|\cA_1| = |\cA_2|$. MLMD models $\D = \X_1 + \X_2$ with $\X_i = \sum_{b \in P_i} R_b (\U_b \boldsymbol{\Sigma}_b \V^\top_b)$ where $R_b$ is a block reshaper. The output would be $3$ bases (in each $\U_b$), two for the columns of $\B_1$ and $\B_2$, and one for all of $\D$. These are neither mutually orthogonal nor guaranteed to be hierarchy-preserving. FD outputs one basis in the columns of $\Q$ are hierarchy-preserving: $[\Q_1]=[\B_1]$, and $[\Q] = [\D]$.


\section{Algorithms}\label{sec:algs}
Our get\_basis algorithm extracts $\Q_i \in St(m_i,n)$ from $\mathbf{C}_i \in \R^{n \times |\cB_i|}$ so that $[\Q_i] = [\C_i]$ by solving the optimization
\begin{equation}
    \Q_i = \argmin_{\X \in St(m_i,n)} \sum_{j=1}^{|\cB_i|}\| \boldsymbol{\Pi}_{\X^\perp} \mathbf{c}_j^{(i)}\|_2^q
\end{equation}
for $q=1,2$. We use $\mathbf{c}_j^{(i)}$ to denote the $j$th column of $\C_i$. A naive implementation of IRLS-SVD addresses $q=1$ and SVD addresses $q=2$.

\begin{algorithm}[ht!]
\caption{get\_basis}\label{alg:get_basis}
 \textbf{Input}: {$\mathbf{C}_i \in \R^{n \times |\cB_i|}$, $m_i \in \R$ (optional)}\\
 \textbf{Output}: {$\X_i \in \R^{m_i}$} \\[0.25em]
     \If{SVD}{
           $\U \boldsymbol{\Sigma} \V^T \gets \mathrm{SVD}(\C_i)$;\\
           \If{$m_i$ is none}
           {
           $m_i \gets \mathrm{rank}(\C_i)$;\\
           }
           $\Q_i \gets \U(1:\mathrm{end},1:m_i)$;\\
        }
     \If{IRLS-SVD}{
        \While{not converged}{
            \For{$j \gets 1$ \KwTo $|\cB_i|$}{
            $\mathbf{c}_j^{(i)} \gets  \C_i(1:\mathrm{end},j)$;\\
            $w_j \gets \mathrm{max}\left(\|\mathbf{c}_j^{(i)}-  \Q_i \Q_i^\top \mathbf{c}_j^{(i)}\|_2,10^{-8}\right)^{-1/2}$;\\
            }
            $\mathbf{W}_i \gets\mathrm{diag}(w_1,w_2,\dots,w_{|\cB_i|})$;\\
            $\U \boldsymbol{\Sigma} \V^T \gets \mathrm{SVD}(\C_i\mathbf{W}_i)$;\\
           \If{$m_i$ is none}{
           $m_i \gets \mathrm{rank}(\C_i\mathbf{W}_i)$;\\
           }
           $\Q_i \gets \U(1:\mathrm{end},1:m_i)$;\\
        }
        }
\end{algorithm}%\vspace{-2mm}

\algname~is essentially BMGS~\cite{jalby1991stability} with a different get\_basis function. The get\_basis in~\cref{alg:get_basis} is used at each iteration of~\algname~to extract a $\Q_i \in St(m_i,n)$ so that $[\Q_i] = [\boldsymbol{\Pi}_{\Q_{i-1}^\perp} \cdots \boldsymbol{\Pi}_{\Q_1^\perp}\B_i]$. The second nested for loop in~\algname~defines $\bR_{i,j}$ using~\cref{eq:R_and_P_app} and updates $\B_j$ so that $\C_i = \boldsymbol{\Pi}_{\Q_{i-1}^\perp} \cdots \boldsymbol{\Pi}_{\Q_1^\perp}\B_i$ is fed into get\_basis. %$\C_i$ is also computed in the second nested loop and is stored in the variable $\B_j$ in the step where $\B_j$ is re-assigned $\B_j - \Q_i \bR_{i,j} = (\I - \Q_i \Q_i^\top) \B_j = \boldsymbol{\Pi}_{\Q_{i}^\perp}$ for $i < j$.

\begin{algorithm}[ht!]%\label{alg:train}
\caption{\algname}\label{alg:FD}
 \textbf{Input}: {A data matrix $\mathbf{D} \in \R^{n \times p}$, \\
 c. hierarchy $\cA_1\subset \cA_2 \subset \cdots \subset \cA_k = \{1,2,\dots,p\}$, \\
 flag type $(n_1,n_2,\cdots,n_k;n)$ with $n_k \leq p$}\\
 \textbf{Output}: {Hierarchy-preserving flag $[\![\Q]\!] \in \flag(n_1,n_2,\dots,n_k;n)$, \\
 weights $\bR \in \R^{n_k \times p}$, perm. mat. $\bP \in \R^{p \times p}$} \\
 with $\D = \Q \bR \bP^\top$\\[0.25em]
     \For{$i\gets1$ \KwTo $k$}{
         $\cB_i \gets \cA_i \setminus \cA_{i-1}$; \\
         $\mathbf{B}_i  \gets \mathbf{D}(1:\mathrm{end},\cB_i) \in \R^{n \times |\cB_i|}$;\\
         $\bP_i \gets  \left[ \mathbf{e}_{b_{i,1}}| \mathbf{e}_{b_{i,2}}| \cdots| \mathbf{e}_{b_{i,|\cB_i|}} \right]$
        }
     \For{$i\gets1$ \KwTo $k$}{
         $m_i \gets n_i-n_{i-1}$;\\
         $\Q_i\gets \mathrm{get\_basis}(\B_i, m_i)$;\\
         $\bR_{i,i} \gets \Q_i^\top \B_i$;\\
         \For{$j\gets i+1$ \KwTo $k$}{
             $\bR_{i,j} \gets \Q_i^\top \B_j$; \%assign $\bR_{i,j}$\\
             $\B_j \gets \B_j - \Q_i \bR_{i,j}$; \%project: $\B_j$ into nullspace of $\Q_i$\\
         }
        }
     $\Q \gets \begin{bmatrix}\Q_1 | \Q_2 | \cdots | \Q_k\end{bmatrix}$;\\
     $\bR \gets \begin{bmatrix}
            \bR_{11} & \bR_{12} & \cdots & \bR_{1k}\\
            \mathbf{0} & \bR_{22} & \cdots & \bR_{2k}\\
            \vdots & \vdots & \ddots & \vdots\\
            \mathbf{0} & \mathbf{0} & \cdots & \bR_{kk} \end{bmatrix}$;\\
     $\bP \gets \begin{bmatrix} \bP_1 | \bP_2 | \cdots | \bP_k \end{bmatrix}$;
     % $(n_1,n_2,\dots,n_k) =\mathrm{cumsum}(m_1,m_2,\dots,m_k)$;\\ % $n_i = \sum_{j=1}^{i-1}m_i$.
\end{algorithm}%\vspace{-2mm}
\begin{remark}[~\algname~operations count]
    In this remark, we use $O$ to denote big-$O$ notation and not the orthogonal group. We also denote $b_i = |\cB_i|$ so $\B_i \in \R^{n \times b_i}$.
    
     The operations count for the SVD of a matrix $\B_i$ is $O(nb_i\mathrm{min}(n,b_i))$. FD runs $k$ SVDs for each piece of the column hierarchy. Thus its operations count is $O\left(n\sum_{i=1}^kb_i\mathrm{min}(n,b_i)\right)$.

    The IRLS-SVD operations count is $O(c_inb_i\mathrm{min}(n,b_i))$ where $c_i$ is the number of iterations until convergence for IRLS-SVD on $\B_i$. Since IRLS-SVD is run $k$ times in Robust FD, the operations count is $O\left(n\sum_{i=1}^kc_ib_i\mathrm{min}(n,b_i)\right)$.
\end{remark}

We summarize the properties for flag recovery methods in~\cref{tab:alg_table2}.
\begin{table}[ht!]
    \centering
    \caption{A summary of flag recovery methods and their properties.}
    \footnotesize
    \label{tab:alg_table2}
    \begin{tabular}{c|c@{\:\:\:\:\:\:\:\:}c@{\:\:}c@{\:\:}c@{\:\:\:\:\:\:\:\:}c}
        \toprule
        Decomp. & QR & SVD & IRLS-SVD & FD  & RFD\\
        \midrule
        Robust & \xmark & \xmark & \cmark & \xmark & \cmark \\ 
        Order-pres. & \cmark & \xmark & \xmark & \cmark & \cmark \\ 
        Flag-type & \xmark & \xmark & \xmark & \cmark & \cmark \\ 
        Hier.-pres. & \xmark & \xmark & \xmark & \cmark & \cmark \\ 
        \bottomrule
    \end{tabular}
\end{table}

\section{Results}\label{sec:extra_experiments}
% \subsection{Trivial Experiment}
We first describe data generation for each simulation. Then we provide details on the hyperspectral image clustering experiment and confidence intervals for Few-shot learning.



%\subsection{Reconstruction Simulations}
\subsection{Reconstruction Simulations}
We consider either additive noise to the data or data contamination with outliers. For both experiments, we generate a Stiefel matrix $[ \X_1 | \X_2 ] = \X \in St(10,4)$ that represents $[\![ \X ]\!] \in \flag(2,4;10)$. Then we generate the data matrix $\D$ with the feature hierarchy $\mathcal{A}_1 \subset \mathcal{A}_2 = \{ 1,2, \cdots, 20\} \subset \{1,2, \cdots, 40\}$. We attempt to recover $[\![ \X]\!]$ and $\D = [\B_1| \B_2] \in \R^{10 \times 40}$ using FD and Robust FD with a flag type of $(2,4;10)$, and the first $4$ left singular vectors from SVD. We evaluate the estimated $[\![\hat{\X}]\!]$ and $\hat{\D}$ using chordal distance and LRSE.

 


\paragraph{Additive noise}
We consider the following model for $\D$:
\begin{equation}
    \mathbf{d}_i = \begin{cases}
        \X_1 \mathbf{s}_{1i}, & i \in \mathcal{B}_1 = \{1,\cdots, 20\}\\
        \X\mathbf{s}_{2i}, & i \in \mathcal{B}_2 = \{21,\cdots, 40\}\\
    \end{cases}
\end{equation}
where each entry of $\mathbf{s}_{ji}$ from a normal distribution with mean $0$ and variance $1$. We contaminate $\D$ with noise by $\tilde{\D} = \D  + \boldsymbol{\epsilon}$ where $\boldsymbol{\epsilon}$ is sampled from either a normal, exponential, or uniform distribution of increasing variance. The goal is to recover $\D$ and $\X$ from $\tilde{\D}$. FD and Robust FD improve flag recovery over SVD and produce similar reconstruction errors. %The results are similar for exponential and uniform distributions and are provided in the supplementary.







\paragraph{Outliers columns}
We randomly sample a subset of columns of $\tilde{\D}$ to be the outliers, denoted $\mathbf{o}_i$. Each entry of $\mathbf{o}_i$ is sampled from a normal distribution with mean $0$ and variance $1$. Each column of $\tilde{\D}$ is
\begin{equation}
    \tilde{\mathbf{d}}_i = \begin{cases}
        \X_1\mathbf{s}_{1i}, & i \in \mathcal{B}_1\setminus \mathcal{O}\\
        \X\mathbf{s}_{2i}, & i \in \mathcal{B}_2\setminus \mathcal{O}\\
        (\I - \X\X^T)\mathbf{o}_i, & i \in \mathcal{O}.\\
    \end{cases}
\end{equation}
We attempt to recover the inlier columns of $\tilde{\D}$ (forming the matrix $\D$) and $\X$ from $\tilde{\D}$. We measure the chordal distance between our flag estimate $[\![\hat{\X}]\!]$ and $[\![\X]\!]$ and the LRSE between our inlier estimates $\hat{\D}$ and $\D$.
% (see~\cref{fig:synthetic_outliers}. FD and Robust FD outperform SVD and FD for flag recovery. Robust FD and FD produce more robust reconstructions of the inlier data than SVD with Robust FD producing the best reconstructions.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/outliers.pdf}
%     \caption{FD and Robust FD improve flag recovery and reconstruction error over SVD with Robust FD producing the best results. Dist is flag chordal distance in~\cref{tab:chordaldist} and LRSE is defined in~\cref{tab:metrics}.}
%     \label{fig:synthetic_outliers}
% \end{figure}


\subsection{Clustering Simulation}
We generate three Stiefel matrices to serve as centers of our clusters $\left[ \X_1^{(c)} | \X_2^{(c)} \right]= \X^{(c)} \in St(10,4)$ that represent $[\![ \X^{(c)} ]\!] \in \flag(2,4;10)$ for $c = 1,2,3$. We use each of these centers to generate $20$ $\D$-matrices with the feature hierarchy $\mathcal{A}_1 = \{ 1,2, \cdots, 20\}$, $\cA_2 = \{1,2, \cdots, 40\}$ in each cluster. The $i$th column in cluster $c$ of the data matrix $\D_i^{(c)}$ is generated as
\begin{equation}
    \mathbf{d}_i^{(c)} = \begin{cases}
        \X_1^{(c)}\mathbf{s}_{1i} , & i \in \mathcal{B}_1\\
        \X^{(c)}\mathbf{s}_{2i} , & i \in \mathcal{B}_2.\\
    \end{cases}
\end{equation}
Then we generate the detected data matrices as $\tilde{\D}_i^{(c)} = \D_i^{(c)}+\boldsymbol{\epsilon}_{i}^{(c)}$. We sample $\boldsymbol{\epsilon}_{1i}^{(c)}$ and $\boldsymbol{\epsilon}_{2i}^{(c)}$ from a normal distribution with mean $0$ and standard deviation $.95$ and $\mathbf{s}_{1i}$ and $\mathbf{s}_{2i}$ from a normal distribution with mean $0$ and standard deviation $1$. 



\subsection{Hyperspectral image clustering}
A total of 326 patches were extracted, each with a shape of ($3 \times 3$), with the following distribution: 51 patches of class Scrub, 7 of Willow swamp, 12 of Cabbage palm hammock, 10 of Cabbage palm/oak hammock, 11 of Slash pine, 13 of Oak/broad leaf hammock, 7 of Hardwood swamp, 20 of Graminoid marsh, 39 of Spartina marsh, 25 of Cattail marsh, 29 of Salt marsh, 24 of Mudflats, and 78 of Water.

In this experiment, we measure the distance between two flags $[\![\X]\!],[\![\Y]\!]$ as
\begin{equation}
    \frac{1}{\sqrt{2}}\|\X_1\X_1^T - \Y_1 \Y_1^T\|_F + \frac{1}{\sqrt{2}}\|\X_2\X_2^T - \Y_2 \Y_2^T\|_F.
\end{equation}

\subsection{Few-shot learning}
We now expand on the methodological details of the baseline methods for few-shot learning and report further results including standard deviations.

\paragraph{Prototypical networks}
Prototypical networks~\cite{snell2017prototypical} are a classical few-shot architecture that uses averages for class representatives and Euclidean distance for distances between representatives and queries. Specifically, a prototype for class $c$ is
\begin{equation}
    \mathbf{q} = \frac{1}{s} \sum_{i=1}^s f_\Theta(\bm{x}_{c,i})
\end{equation}
and the distance between a query point, $f_{\Theta}(\bm{x})$, is
\begin{equation}
    \|\mathbf{q} - f_{\Theta}(\bm{x})\|_2^2.
\end{equation}
In experiments, we refer to this method as `Euc.'


\paragraph{Subspace classifiers}
Subspace classifiers from adaptive subspace networks~\cite{simon2020adaptive} use subspace representatives and measure distances between subspace representatives and queries via projections of the queries onto the subspace representatives. Although the original work suggests mean subtraction before computing subspace representatives and for classification, we notice that there is no mean-subtraction in the code provided on \href{https://github.com/chrysts/dsn_fewshot/blob/master/Resnet12/models/classification_heads.py}{GitHub}. Therefore, we summarize the model used on GitHub as
\begin{equation}
    \tilde{\bm{X}}_c = \left[ f_\Theta(\bm{x}_{c,1})|f_\Theta(\bm{x}_{c,2})|\cdots| f_\Theta(\bm{x}_{c,s}) ,\right]
\end{equation}
\begin{equation}
    \mathbf{U}_c \boldsymbol{\Sigma}_c \mathbf{V}_c^\top = \tilde{\bm{X}}_c,
\end{equation}
\begin{equation}
    \mathbf{Q}_c = \mathbf{U}_c(1:\mathrm{end},1:s-1).
\end{equation}
We say that the span of the columns of $\Q_c$ serves as the subspace representative for class $c$. This can be seen as a mapping of a set feature space representation of the shots from one class to $Gr(s-1,n)$ via the SVD. The distance between a query $f_{\Theta}(\bm{x})$ and class $c$ is 
\begin{equation}
    \|f_{\Theta}(\bm{x}) - \mathbf{Q}_c \mathbf{Q}_c^\top f_{\Theta}(\bm{x})\|_F^2.
\end{equation}
This is the residual of the projection of a query point onto the subspace representative for class $c$.

\paragraph{Stacking features}
Our application of flag classifiers uses an alexnet backbone $f_\Theta = f^{(2)}_\Theta \circ f^{(1)}_\Theta$. Given a sample $\bm{f}$, flag classifiers leverage both the information extracted by $f_\Theta$ \emph{and} $f^{(1)}_\Theta$. This is already an advantage over the baseline methods because flag classifiers see more features. Therefore, we modify prototypical network and subspace classifiers for a fair baseline to flag nets. Specifically, we replace $f_\Theta(\bm{x})$ with 
\begin{equation}
    \begin{bmatrix}
        f^{(1)}_\Theta(\bm{x})\\
        f_\Theta(\bm{x})
    \end{bmatrix}.
\end{equation}
This doubles the dimension of the extracted feature space and thereby exposes these algorithms to problems like the curse of dimensionality. Additionally, it assumes \emph{no order} on the features extracted by $f_\Theta$ and $f^{(1)}_\Theta$ therein not respecting the natural hierarchy of the alexnet feature extractor.

\paragraph{Further results}
We provide the classification accuracies along with standard deviations over $20$ random trials in~\cref{tab:fewshot_app1,tab:fewshot_app2}. 
%For a fair comparison between flag classifiers and the baselines, the baselines use features extracted by $f_\Theta = f^{(2)}_\Theta \circ f^{(1)}_\Theta$ and $f^{(1)}_\Theta$ in~\cref{tab:fewshot_app1}. For completeness, we also run the baselines only on features from $f_\Theta$ and report the results in~\cref{tab:fewshot_app2}.
\setlength{\tabcolsep}{10pt}
\begin{table}[ht]
    \centering
    \caption{\emph{Classification accuracy ($\uparrow$)} with $s$ shots, $5$ ways, and $100$ evaluation tasks each containing $10$ query images, averaged over $20$ random trials. Flag types for `Flag' are $(s-1,2(s-1))$ and the subspace dimension is $s-1$. Baselines see stacked features from both $f^{(1)}_\Theta$ and $f_\Theta$.}
    \label{tab:fewshot_app1}
    \begin{tabular}{@{\hskip 4pt}l@{\hskip 4pt}l@{\hskip 4pt}c@{\hskip 7pt} c @{\hskip 7pt}c}
    \toprule
    $s$ & Dataset & Flag & Euc. & Subsp. \\
    \midrule
    \multirow[t]{3}{*}{3} & EuroSat & $\bm{77.7} \pm 1.0$ & $76.7 \pm 1.0$ & $77.6 \pm 1.0$ \\
     & CIFAR-10 & $\bm{59.6} \pm 1.0$ & $58.6 \pm 0.9$ & $\bm{59.6} \pm 1.0$ \\
     & Flowers102 & $\bm{90.2} \pm 0.7$ & $88.2 \pm 1.0$ & $\bm{90.2} \pm 0.7$ \\
    \cline{1-5}
    \multirow[t]{3}{*}{5} & EuroSat & $\bm{81.8} \pm 0.7$ & $80.7 \pm 0.8$ & $\bm{81.8} \pm 0.7$ \\
     & CIFAR-10 & $\bm{65.2} \pm 0.9$ & $\bm{65.2} \pm 0.9$ & $\bm{65.2} \pm 0.9$ \\
     & Flowers102 & $\bm{93.2} \pm 0.5$ & $91.4 \pm 0.6$ & $\bm{93.2} \pm 0.5$ \\
    \cline{1-5}
    \multirow[t]{3}{*}{7} & EuroSat & $\bm{83.9} \pm 0.8$ & $82.6 \pm 0.8$ & $83.8 \pm 0.8$ \\
     & CIFAR-10 & $68.0 \pm 0.7$ & $\bm{68.6} \pm 0.8$ & $68.1 \pm 0.7$ \\
     & Flowers102 & $\bm{94.5} \pm 0.5$ & $92.7 \pm 0.5$ & $\bm{94.5} \pm 0.5$ \\
    \bottomrule
    \end{tabular}
\end{table}

\setlength{\tabcolsep}{10pt}
\begin{table}[ht]
    \centering
    \caption{\emph{Classification accuracy ($\uparrow$)} with $s$ shots, $5$ ways, and $100$ evaluation tasks each containing $10$ query images, averaged over $20$ random trials. Flag types for `Flag' are $(s-1,2(s-1))$ and the subspace dimension is $s-1$. Baselines see features only from $f_\Theta$.}
    \label{tab:fewshot_app2}
    \begin{tabular}{@{\hskip 4pt}l@{\hskip 4pt}l@{\hskip 4pt}c@{\hskip 7pt} c @{\hskip 7pt}c}
        \toprule
        $s$ & Dataset & Flag & Euc. & Subsp. \\
        \midrule
        \multirow{3}{*}{3} & EuroSat & $\bm{77.7} \pm 1.0$ & $75.9 \pm 0.9$ & $76.8 \pm 1.1$ \\
         & CIFAR-10 & $\bm{59.6} \pm 1.0$ & $58.4 \pm 0.8$ & $58.5 \pm 0.9$ \\
         & Flowers102 & $\bm{90.2} \pm 0.7$ & $87.9 \pm 0.9$ & $88.8 \pm 0.8$ \\
        \cline{1-5}
        \multirow{3}{*}{5} & EuroSat & $\bm{81.8} \pm 0.7$ & $79.8 \pm 0.8$ & $80.8 \pm 0.8$ \\
         & CIFAR-10 & $\bm{65.2} \pm 0.9$ & $64.5 \pm 1.0$ & $63.8 \pm 0.9$ \\
         & Flowers102 & $\bm{93.2} \pm 0.5$ & $91.1 \pm 0.6$ & $92.0 \pm 0.5$ \\
        \cline{1-5}
        \multirow{3}{*}{7} & EuroSat & $\bm{83.9} \pm 0.8$ & $81.7 \pm 0.8$ & $82.9 \pm 0.8$ \\
         & CIFAR-10 & $\bm{68.0} \pm 0.7$ & $67.9 \pm 0.8$ & $66.7 \pm 0.7$ \\
         & Flowers102 & $\bm{94.5} \pm 0.5$ & $92.3 \pm 0.5$ & $93.4 \pm 0.5$ \\
        \bottomrule
    \end{tabular}\vspace{-3mm}
\end{table}



%We generate a column of the $i$th matrix in cluster $c$ in two steps:
% \begin{enumerate}
%     \item QR decomposition
%     \begin{equation}
%         \mathbf{Q}_i^{(c)} \mathbf{R}_i^{(c)} = \X^{(c)}_i + \mathbf{E}_{i}^{(c)}
%     \end{equation}
%     \item Data generation
%     \begin{equation}
%     \mathbf{d}_i^{(c)} = \begin{cases}
%         \mathbf{Q}_i^{(c)}\mathbf{s}_{1i}, & i \in \mathcal{B}_1 = \{1,\cdots, 20\}\\
%         \mathbf{Q}_i^{(c)}\mathbf{s}_{2i}, & i \in \mathcal{B}_2 = \{21,\cdots, 40\}\\
%     \end{cases}
% \end{equation}
% \end{enumerate}
% We sample $\mathbf{E}_{i}^{(c)}$ from a normal distribution with mean $0$ and standard deviation $.28$ and $\mathbf{S}_{1i}$ and $\mathbf{S}_{2i}$ from a normal distribution with mean $0$ and standard deviation $1$. 


\begin{comment}
\subsection{Additional synthetic experiment}
We generate hierarchical data with $p=4$ features: $\mathcal{A}_1 = \{1\}$, $\mathcal{A}_2 = \{1,2,3\}$, and $\mathcal{A}_3 = \{1,2,3,4\}$. To this end, we sample $\mathbf{d}_1\sim\mathcal{N}(0,1)$, $\mathbf{d}_2\sim\mathcal{N}(0,2)$, and $\mathbf{d}_4\sim\mathcal{N}(0,3)$. We set $\mathbf{d}_3=2\mathbf{d}_2$ to induce correlation. The rank of the final data matrix $\mathbf{D} = \begin{bmatrix} \mathbf{d}_1 & \mathbf{d}_2  & \mathbf{d}_3 & \mathbf{d}_4 \end{bmatrix} \in \R^{10 \times 4}$ is $3$.
% These data are $n=10$ samples from feature $1$ from $\mathcal{N}(0,1)$, feature $2$ from $\mathcal{N}(0,2)$, and feature $4$ from $\mathcal{N}(0,3)$. Then we define feature $3$ to as twice feature $2$. Notice that the rank of the data matrix $\mathbf{D} = \begin{bmatrix} \mathbf{d}_1 & \mathbf{d}_2  & \mathbf{d}_3 & \mathbf{d}_4 \end{bmatrix} \in \R^{10 \times 4}$ is $3$ because $\mathbf{d}_3 = 2\mathbf{d}_2$. 
We then run $3$ different flag representation methods, \emph{FD, QR decomposition}, and \emph{SVD} in Steifel coordinates for $\phi : (\mathcal{D}, \mathcal{A}) \rightarrow \flag(1,2,3;4)$. QR decomposition has been used in the past for flag representations of data~\cite{Mankovich_2023_ICCV}. The mapping $\phi$ is illustrated in~\cref{fig:concept}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{figures/concept_cropped.pdf}\vspace{-3mm}
    \caption{Stiefel coordinates $\phi$ for a flag of type $(1,2,3;4)$ for data $\mathcal{D} \subset \R^{10}$ with a feature hierarchy $\mathcal{A}_1 \subset \mathcal{A}_2 \subset \mathcal{A}_3 $.\vspace{-2mm}}
    \label{fig:concept}
\end{figure}

Each of these flags is represented by a Steifel matrix of the form $\X = [\x_1 | \x_2 |\x_3]  \in \mathbb{R}^{10 \times 3}$. We expect $\x_1$ to be correlated with $\mathbf{d}_1$, $\x_2$ to be correlated with $\mathbf{d}_2$ and $\mathbf{d}_3$, and $\x_3$ to be correlated with $\mathbf{d}_4$. To test this, we compute the average absolute correlation over the data. Geometrically, this is the cosine of the acute angles between the subspaces spanned by the data and corresponding columns of the flag representative $\mathbf{X} \in St(3,10)$.
\cref{fig:synthetic} shows that the mean of these absolute correlations is higher for FD than QR and SVD which indicates that FD produces the best representation of these data as a flag of type $(1,2,3;4)$.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/FD_vs_qr_vs_svd2.pdf}
    \caption{$1000$ random trials on data with nested features. FD, QR, and SVD are used to represent these data as a flag $[\![\X ]\!]$ of type $(1,2,3;10)$ with coordinates $\X = [\x_1 | \x_2 | \x_3]\in St(3,10)$. We plot the mean absolute correlation between the original data and the corresponding columns of flag representation.\vspace{-3mm}}
    \label{fig:synthetic}
\end{figure}
\end{comment}


\flushcolsend
