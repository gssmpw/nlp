\begin{table}[t]
\caption{Metrics for evaluating simulations. LRSE stands for Log Relative Squared Error and $\|\cdot\|_F$ is the Frobenius norm.}
    \label{tab:metrics}
    \centering
    \begin{tabular}{c|c}
       \toprule
       Metric ($\downarrow$) & Formula\\
       \midrule
       \midrule
       Chordal Distance & $\sqrt{\sum_{i=1}^k m_i - \tr(\X_i^T \hat{\X}_i\hat{\X}_i^T\X_i))}$ \\
       LRSE & $10 \log_{10} \left(\|\mathbf{D} - \hat{\mathbf{D}}\|_F^2/\|\mathbf{D}\|_F^2\right)$\\
       \bottomrule
    \end{tabular}
\end{table}
\vspace{-1mm}
\section{Results}\label{sec:results}
%\vspace{-2mm}
We run three simulations in~\cref{sec: reconstruction} to test the capacity of FD and RFD for flag recovery and reconstruction for noisy and outlier-contaminated data. In~\cref{sec:mds clustering} we visualize clusters of flag representations for hierarchically structured $\D$ matrices using FD. We highlight the utility of FD for denoising hyperspectral images in~\cref{sec: denoising_hs}. We cluster FD-extracted flag representations of hyperspectral image patches via a pixel hierarchy in~\cref{sec: clustering}. Finally, in~\cref{sec: fewshot learning}, we apply flag classifiers to three datasets for classification.

\paragraph{Baselines}
While more modern, task-specific algorithms may exist as baselines for each experiment, our primary objective is to demonstrate the effectiveness of FD and RFD (computed using~\algname) compared to the de facto standards, SVD and QR, in the context of hierarchical data. SVD is a standard denoising method. Two common flag extraction algorithms are SVD~\cite{draper2014flag,mankovich2022flag,Mankovich_2023_ICCV,szwagier2024curseisotropyprincipalcomponents} and QR~\cite{Mankovich_2023_ICCV}. In~\cref{sec: fewshot learning} we compare our results to two standard prototypes (e.g., means and subspaces) for classification within the few-shot learning paradigm.

\paragraph{Metrics}
In the additive noise model $\tilde{\D} = \D +\boldsymbol{\epsilon}$, we compute the signal-to-noise ratio (SNR) in decibels (dB) as 
\begin{equation}\label{eq:snr}
    \mathrm{SNR}(\mathbf{D},\bm{\epsilon}) = 10 \log_{10} \left(\|\mathbf{D}\|_F^2/\|\boldsymbol{\epsilon}\|_F^2\right).
\end{equation}
A negative SNR indicates more noise than signal, and a positive SNR indicates more signal than noise. In the rest of our metrics (see~\cref{tab:metrics}), $[\![\X]\!]$ represents true flag, $\D$ the true data, $[\![\hat{\X}]\!]$ the estimated flag, and $\hat{\D}$ the reconstructed data.



\subsection{Reconstruction Under Corruption}\label{sec: reconstruction}
For both experiments, we generate $\X \in St(10,4)$ that represents $[\![ \X ]\!] \in \flag(2,4;10)$. Then we use $\X$ to build a data matrix $\D \in \R^{10 \times 40}$ with the feature hierarchy $\mathcal{A}_1 \subset \mathcal{A}_2 = \{ 1,2,\cdots,20\} \subset \{1,2,\cdots,40\}$. We generate $\tilde{\D}$ as either $\D$ with additive noise or $\D$ with columns replaced by outliers. Our goal is to recover $[\![ \X]\!]$ and $\D=\X\bR\bP^\top$ from $\tilde{\D}$ using FD and RFD with a flag type of $(2,4;10)$, and the first $4$ left singular vectors from SVD. We evaluate the estimated $[\![\hat{\X}]\!]$ and $\hat{\D}$ using~\cref{tab:metrics}.

 


\paragraph{Additive noise}
We contaminate with noise by $\tilde{\D} = \D  + \boldsymbol{\epsilon}$ where $\boldsymbol{\epsilon}$ is sampled from either a mean-zero normal, exponential, or uniform distribution of increasing variance. 
%The goal is to recover $\D$ and $\X$ from $\tilde{\D}$. 
FD and RFD improve flag recovery over SVD and produce similar reconstruction errors (see~\cref{fig:synthetic_noise}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/noise.pdf}
    \caption{FD \& RFD improve flag recovery while maintaining accurate reconstructions. SNR is~\cref{eq:snr}. LSRE \& Dist are in~\cref{tab:metrics} with \emph{Dist} as the chordal distance. Best fit lines are quadratic.}
    \label{fig:synthetic_noise}
\end{figure}





\paragraph{Robustness to outliers}
We construct $\tilde{\D}$ to contain outlier columns. The inlier columns of $\tilde{\D}$ form the flag-decomposable $\D = \X \bR \bP^\top$ with the flag $[\![\X]\!]$. 
FD and RFD outperform SVD and IRLS-SVD, with RFD providing the most accurate flag recovery and inlier reconstructions (see~\cref{fig:synthetic_outliers}).




\subsection{MDS Clustering}\label{sec:mds clustering}


We generate $60$ $\D$ matrices in $3$ clusters, each with $20$ points. Then we add normally-distributed noise to generate $60$ $\tilde{\D}$ matrices (see suppl. material). We compute the SNR for each $\tilde{\D}$ via~\cref{eq:snr} and find the mean SNR for the $60$ matrices is $-4.79$ dB, indicating that significant noise has been added to each $\D$. This experiment aims to find the method that best clusters the $\tilde{\D}$ matrices.



We use SVD (with $4$ left singular vectors) and FD (with flag type $(2,4;10)$) on each of the $60$ $\tilde{\D}$ matrices to recover the flag representations. Then the chordal distance is used to generate distance matrices. Finally, MDS visualizes these data in $2$ dimensions. Our additional baseline run on the Euclidean distance matrix between the flattened $\tilde{\D}$ matrices. We find that FD produces a distance matrix and MDS with more defined clusters in~\cref{fig:synthetic_clustering}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/outliers.pdf}
    \caption{FD and RFD improve flag recovery and reconstruction error over SVD and IRLS-SVD with RFD. LSRE \& Dist are defined in~\cref{tab:metrics} with \emph{Dist} as the chordal distance.}
    \label{fig:synthetic_outliers}
\end{figure}

\begin{figure}[b]
\vspace{-3mm}
    \centering
    \includegraphics[width=\linewidth]{figures/synthetic_clustering_dr.pdf}
    \vspace{-4mm}
    \caption{\textbf{(Top row)} distance matrices using Euclidean distance (Euclidean) and chordal distance between flags (SVD and FD). \textbf{(Bottom row)} 2D representation of the data colored by cluster via MDS applied to the distance matrix.}
    \label{fig:synthetic_clustering}
\end{figure}


\subsection{Hyperspectral Image Denoising}\label{sec: denoising_hs}
We consider denoising images captured by the \href{https://aviris.jpl.nasa.gov/}{AVIRIS} hyperspectral sensor. Two hyperspectral images are used for model evaluation: the KSC and Indian Pines datasets~\cite{Bandos09}. KSC is a $(512\times614)$ image with $176$ bands and Indian Pines is a $(145\times145)$ image with $200$ bands. 
We run two experiments, one on each image, by randomly selecting a $50 \times 50$ square and flattening it to generate $\D \in \R^{2500 \times p}$ (pixels as rows and bands as columns). Then, we add mean-zero Gaussian noise of increasing variance to obtain $\tilde{\D}$, on which we run our FD and SVD to denoise. LRSE is measured between $\D$ and the denoised reconstruction $\hat{\D}$ (see~\ref{tab:metrics}) to determine the quality of the reconstruction.

For our FD, we specify a flag of type $(8,9,10;2500)$, and SVD is run using the first $10$ left singular vectors. The hierarchy used as input to our algorithm mirrors the spectrum hierarchy (see~\cref{ex:spectrum_hierarchy}) by assigning $\mathcal{A}_1$ to the first $40$ bands, $\mathcal{A}_2$ to the first $100$ bands, and $\mathcal{A}_3$ to all the bands. We find in~\cref{fig:hsi denoising} that FD consistently improves HSI denoising over SVD. When testing exponential and uniform noise, FD and SVD produce similar quality denoising. 

\begin{figure}[t]
    \vspace{-.3cm}
    \centering
    \includegraphics[width=\linewidth]{figures/RS_Reconstruction.pdf}
    \caption{FD improves hyperspectral image denoising over SVD (see SNR~\cref{eq:snr}, LRSE~\cref{tab:metrics}). \vspace{-5mm}}
    \label{fig:hsi denoising}
\end{figure}


\subsection{Hyperspectral Image Clustering}\label{sec: clustering}
We now showcase image patch clustering using the KSC hyperspectral image.  
The data was pre-processed to remove low SNR and water absorption bands, then split into $3 \times 3$ patches of pixels from the same class. Each patch is translated into a $\mathbf{D} \in \R^{176 \times 9}$ (bands as rows and pixels as columns) with the hierarchy described in~\cref{ex:pixel_hierarchy} with $\cA_1$ as the center pixel. A flag recovery method is run on each $\D$ to extract a flag of type $(1,8;176)$. Then, we compute a chordal distance matrix between the collection of flags. Finally, we classify these flags using $k$-nearest neighbors. We compare FD to QR and SVD in~\cref{fig:ksc_knn} and find that FD produces the highest classification accuracy for the number of nearest neighbors between $6$ and $24$.

Instead of using chordal distance between flags to measure distance, we use a sum of Grassmannian chordal distances.
We hypothesize that this is a more suited distance for this example because it is more robust to outlier pixels. Given $[\![\X]\!],[\![\Y]\!] \in \flag(1,8;176)$, we use a chordal distance on the product of Grassmannians that takes advantage of the embedding of $\flag(1,8;176)$ in $Gr(1,176) \times Gr(7,176)$. See our suppl. material for details.

\subsection{Few-shot Learning}\label{sec: fewshot learning}
We deploy FD in few-shot learning using an Alexnet~\cite{krizhevsky2012imagenet}, pre-trained on ImageNet, as the feature extractor $f_{\Theta}: \mathcal{X} \rightarrow \R^{4096}$, admitting the representation $f_{\Theta} = f_{\Theta}^{(1)} \circ f_{\Theta}^{(2)}$ where the range of both $f_{\Theta}^{(1)}$ and $f_{\Theta}^{(2)}$ is $\R^{4096}$. We use the feature hierarchy outlined in~\cref{ex:feature_hierarchy} and the procedure in~\cref{sec:fewshot_flags} to map the support of one class to a flag prototype using FD (see~\cref{fig:fewshot_flags}). The distance between a query point and a flag prototype is~\cref{eq: fewshot_dist}. We call this pipeline a flag classifier. Our baselines include Euclidean~\cite{snell2017prototypical} and subspace~\cite{simon2020adaptive} classifiers. No fine-tuning is used to optimize the feature extractor in any experiments.


Our two baseline methods, Euclidean and subspace, use means and subspaces as prototypes. Specifically, prototypical networks~\cite{snell2017prototypical} are a classical few-shot architecture that uses averages for prototypes and Euclidean distance between prototypes and queries. On the other hand, subspace classifiers from adaptive subspace networks~\cite{simon2020adaptive} use subspaces as prototypes and measure distances between prototypes and queries via projections of the queries onto the prototype. Building upon these baseline methods, we propose a flag-based approach (see~\cref{fig:concept,fig:fewshot_flags}). For a fair comparison, baselines use features extracted by $f^{(1)}_\Theta$ and $f_\Theta$. 

We evaluate flag classifiers on EuroSat~\cite{helber2019eurosat}, CIFAR-10~\cite{krizhevsky2009learning}, and Flowers102~\cite{nilsback2008automated} datasets, and report the average classification accuracy in~\cref{tab:fewshot} over $20$ random trials each containing $100$ evaluation tasks with $10$ query images and $5$ ways per task. We find flag classifiers improve classification accuracy for all tested shots ($s=3,5,7$) over the baselines. Further results are in suppl. material.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/KSC.pdf}
    \caption{$k$-nearest neighbors \emph{classification accuracies} ($\uparrow$) using chordal distance matrices derived from flag representations of $3 \times 3$ image patches of the KSC dataset over $20$ random trials with a $70\%-30\%$ training-validation split. }
    \label{fig:ksc_knn}
\end{figure}
\setlength{\tabcolsep}{10pt}
\begin{table}[ht!]
    \centering
    \caption{\emph{Classification accuracy ($\uparrow$)} with $s$ shots, $5$ ways, and $100$ evaluation tasks each containing $10$ query images, averaged over $20$ random trials. Flag types for `Flag' are $(s-1,2(s-1))$ and the subspace dimension is $s-1$.}
    \label{tab:fewshot}
    \begin{tabular}{llccc}
    \toprule
    $s$ & Dataset & Flag & Euc. & Subsp. \\
    \midrule
    \multirow[t]{3}{*}{3} & EuroSat & \textbf{77.7}  & 76.7  & 77.6  \\
     & CIFAR-10 & \textbf{59.6}  & 58.6  & \textbf{59.6}  \\
     & Flowers102 & \textbf{90.2}  & 88.2  & \textbf{90.2}  \\
    \cline{1-5}
    \multirow[t]{3}{*}{5} & EuroSat & \textbf{81.8}  & 80.7  & \textbf{81.8}  \\
     & CIFAR-10 & \textbf{65.2}  & \textbf{65.2}  & \textbf{65.2}  \\
     & Flowers102 & \textbf{93.2}  & 91.4 & \textbf{93.2}  \\
    \cline{1-5}
    \multirow[t]{3}{*}{7} & EuroSat & \textbf{83.9}  & 82.6  & 83.8  \\
     & CIFAR-10 & 68.0  & \textbf{68.6}  & 68.1  \\
     & Flowers102 & \textbf{94.5} & 92.7 & \textbf{94.5}  \\
    \bottomrule
    \end{tabular}\vspace{-3mm}
\end{table}

\begin{comment}
\setlength{\tabcolsep}{10pt}
\begin{table}[t!]
    \centering
    \caption{\emph{Classification accuracy ($\uparrow$)} with $s$ shots, $5$ ways, and $100$ evaluation tasks each containing $10$ query images, averaged over $20$ random trials. Flag types for `Flag' are $(s-1,2(s-1))$ for EuroSat and $(1,2)$ for CIFAR-10 and Flowers102.}
    \label{tab:fewshot}
    \begin{tabular}{llccc}
    \toprule
    $s$ & Dataset & Flag & Euc. & Subsp.\\
    \midrule
    \multirow{3}{*}{$3$} & EuroSat & \textbf{77.4} & 75.9 & 76.8 \\
     & CIFAR-10 & \textbf{59.2} & 58.4 & 58.5 \\
     & Flowers102 & \textbf{89.4} & 87.9 & 88.8 \\
    \cline{1-5}
    \multirow{3}{*}{$5$} & EuroSat & \textbf{81.6} & 79.8 & 80.8 \\
     & CIFAR-10 & \textbf{65.1} & 64.5 & 63.8 \\
     & Flowers102 & \textbf{92.3} & 91.1 & 92.0 \\
    \cline{1-5}
    \multirow{3}{*}{$7$} & EuroSat & \textbf{83.6} & 81.7 & 82.9 \\
     & CIFAR-10 & \textbf{68.4} & 67.9 & 66.7 \\
     & Flowers102 & \textbf{93.5} & 92.3 & 93.4 \\
    \bottomrule
    \end{tabular}\vspace{-3mm}
\end{table}
\end{comment}





