\documentclass{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp,nicefrac}
\usepackage{caption} 
\captionsetup{justification=centering}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX
2024}
{Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for
Review by the \textsc{IEEE Transactions on Nuclear 
Science} \newline (May 2020)}
\begin{document}
\title{Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging} 

% \author{Mohammed Abdul Hafeez Khan, \IEEEmembership{Graduate Student Member, IEEE}, Samuel Morries, and Third C. Author, Jr., \IEEEmembership{Member, IEEE}

\author{Mohammed Abdul Hafeez Khan, \IEEEmembership{Graduate Student Member, IEEE}, Samuel Morries Boddepalli, \IEEEmembership{Graduate Student Member, IEEE}, Siddhartha Bhattacharyya, \IEEEmembership{Senior Member, IEEE}, and Debasis Mitra, \IEEEmembership{Senior Member, IEEE}
\thanks{Authors Mohammed Abdul Hafeez Khan, Samuel Morries Boddepalli, Siddhartha Bhattacharyya and Debasis Mitra are all affiliated with Florida Institute of Technology, Melbourne, FL 32901 USA (emails:mkhan@my.fit.edu, sboddepalli2023@my.fit.edu, sbhattacharyya@fit.edu and dmitra@fit.edu)}}

\maketitle

\begin{abstract}
Accurate classification and anatomical localization are essential for effective medical diagnostics and research, which may be efficiently performed using deep learning techniques. However, availability of limited labeled data poses a significant challenge. To address this, we adapted Prototypical Networks and the Propagation-Reconstruction Network (PRNet) for few-shot classification and localization,  respectively, in Single Photon
Emission Computed Tomography (SPECT) images. For the proof of concept we used a 2D-sliced image cropped around heart. The Prototypical Network, with a pre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver tissues with 96.67\% training and 93.33\% validation accuracy. PRNet, adapted for 2D imaging with an encoder-decoder architecture and skip connections, achieved a training loss of 1.395, accurately reconstructing patches and capturing spatial relationships. These results highlight the potential of Prototypical Networks for tissue classification with limited labeled data and PRNet for anatomical landmark localization, paving the way for improved performance in deep learning frameworks.
\end{abstract}

\begin{IEEEkeywords}
Few-shot Classification, Anatomical Localization, Prototypical Network, PRNet, SPECT Image analyses
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{I}{n} the rapidly advancing domain of medical imaging, accurate classification and anatomical localization of tissues are critical for diagnostic, therapeutic, and research purposes. Recent advances in the application of deep learning techniques have significantly expanded their potential in imaging classification \cite{ker2017deep}\cite{khan2022detection}. However, the scarcity of data in the medical domain often necessitates specialized adaptations to meet the stringent demands for precision and reproducibility. This research draws inspiration from two pivotal studies: The development of Prototypical Networks \cite{snell2017prototypical}, and Propagation Reconstruction Network (PRNet) \cite{lei2023one}. These studies form the foundation for our methodologies, adapted and applied to Single Photon Emission Computed Tomography (SPECT) images, focusing on the tissues of the ventricles, myocardium, and liver.

The first aim of our study was to address the challenges posed by a severely limited dataset of only 12 SPECT images. To address this shortcoming, we implemented a Few-Shot Learning (FSL) approach, leveraging Prototypical Networks \cite{snell2017prototypical}. This approach was tailored to classify segmented tissue mask types—myocardium, ventricles, and liver—by representing each class with a prototype in a learned metric space. The second aim of our study was to accurately determine the anatomical locations of tissues within 2D SPECT images. To achieve this, we adapted the architecture of the PRNet \cite{lei2023one}, originally designed for 3D CT and MRI images, for 2D convolutional processing. As shown in Fig. \ref{fig:prnet}, we accurately reconstruct patches, capturing spatial relationships to predict the relative spatial coordinates between anatomical landmarks. We reviewed advancements in few-shot learning that have shaped image classification. Koch et al. \cite{koch2015siamese} used Siamese Neural Networks for one-shot image recognition, setting a benchmark for learning with minimal data. Building on this, Shaban et al. \cite{shaban2017one} enhanced semantic segmentation by adapting Fully Convolutional Networks (FCNs) to new tasks. Similarly, Snell et al. \cite{snell2017prototypical} introduced Prototypical Networks to streamline classification by creating class-specific prototypes in a learned metric space. Expanding on these ideas, Rakelly et al. \cite{rakelly2018conditional} used support images to improve image classification.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.7\columnwidth]{proto1.drawio.png} % Adjust the width as needed
%     \caption{Support (N=3) and query (N=6) sets of segmented tissues for training Prototypical Network, which needs labeled support set and unlabeled query set}
%     \label{fig:proto}
% \end{figure}

\begin{figure}[b]
    \centering
    \includegraphics[width=0.6\columnwidth]{prnet.drawio.png} % Adjust the width as needed
    \caption{Real vs reconstructed image of myocardium tissue using PRNet}
    \label{fig:prnet}
\end{figure}

% \section{Literature Review}
% \label{sec:litreview}
% In this section, we review advancements in few-shot learning. Koch et al. \cite{koch2015siamese} introduced Siamese Neural Networks for one-shot image recognition. Shaban et al. \cite{shaban2017one} enhanced semantic segmentation by adapting Fully Convolutional Networks (FCNs) to new tasks. Snell et al. \cite{snell2017prototypical} proposed Prototypical Networks for classification using class-specific prototypes. Rakelly et al. \cite{rakelly2018conditional} used support images to improve segmentation and classification.

% while Sung et al. \cite{sung2018learning} developed the Relation Network, focussing on deep distance metrics for image comparison.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.7\columnwidth]{prnet.drawio.png} % Adjust the width as needed
%     \caption{Real Image Vs Reconstructed Image}
%     \label{fig:prnet}
% \end{figure}

\section{Methodology}
\label{sec:methodology}

\subsection{Prototypical Network}
To apply Prototypical Network for few-shot classification of myocardium, ventricles, and liver masks, we designed a convolutional neural network (CNN) to map input images to a metric space via the embedding function \( f_\phi \). We used a pretrained ResNet-18\cite{chen2019closer} as the backbone, fine-tuning it for the specific few-shot classification task. The forward pass of the Prototypical Network computes classification scores using support and query images. The support images create prototype \( c_k \) representations for each class \(k\), which are the mean feature vectors of the support examples. Query images \( x \) are then classified by computing a softmax over the Euclidean distances \(d(z,z')\) to these prototypes, as shown in Eq. \ref{eq5}:
\begin{equation}
\label{eq5}
p_\phi(y = k | x) = \frac{\exp(-d(f_\phi(x), c_k))}{\sum_{k'} \exp(-d(f_\phi(x), c_{k'}))} 
\end{equation}
Subsequently, using the Eq. \ref{eq6}, the classification scores are obtained by minimizing the negative log-probability.  
\begin{equation}
\label{eq6}
J(\phi) = - \log p_\phi(y = k | x)
\end{equation}
The model then selects the class \(k\) with the highest score for each query image. Accuracy is calculated by comparing the predicted class with the true label and dividing the number of correct predictions by the total predictions. 

Therefore, prototypical networks map input data to an embedding space and classify using class prototypes, enabling efficient classification from just a few samples of data.

\subsection{Propagation-Reconstruction Network (PRNet)}

The Propagation-Reconstruction Network (PRNet) was originally designed for 3D CT and MRI images \cite{lei2023one}. We adapted it for 2D slices of SPECT imaging by modifying its configuration from 3D to 2D layers, enabling it to focus on spatial information and accurately identify anatomical landmarks. PRNet includes an encoder, fully connected layers for predicting anatomical positions, and a decoder that reconstructs the original image for validation, as shown in Fig. \ref{fig:prnet}. The model uses self-supervised learning by randomly selecting two points, \(c_i\) and \(c_j\), within an image. The relative offset between them, \(d_{ji}\), guides the learning process. Two fixed-size patches, \(x(c_i)\) and \(x(c_j)\), are cropped around \(c_i\) and \(c_j\), then processed through PRNet to obtain their 2D anatomical coordinates, \(a(c_i)\) and \(a(c_j)\). The predicted offset, \(d'_{ji}\), determines the spatial relationship between the points in the latent vector space. As shown in Eq. \ref{eq1}, PRNet employs a self-supervised loss function \(L_{ssl}\), composed of two main components: the distance loss (\( L_{dis} \)), which quantifies the error in the predicted relative positions, and the reconstruction loss (\( L_{rec} \)), which measures the accuracy of the reconstructed patches x\(_r(c_i)\) and x\(_r(c_j)\), compared to the originally cropped patches.

\begin{equation}
\label{eq1}
L_{ssl} = L_{dis} + L_{rec}
\end{equation}

In summary, PRNet provides a self-supervised framework that leverages inherent anatomical similarities across individuals to facilitate landmark localization in medical imaging with minimal reliance on extensive labeled datasets.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\columnwidth]{prnet.drawio.png} % Adjust the width as needed
%     \caption{Real Image Vs Reconstructed Image}
%     \label{fig:prnet}
% \end{figure}

\section{Results and Discussion}
\label{sec:expresults}

The experimental results highlight the effectiveness of both the Prototypical Network and PRNet in their respective tasks.

For the Prototypical Network, we used episodic batches for training and testing. Each episode included a fixed number of classes, support images, and query images. As illustrated in Fig. \ref{fig:proto}, we used \textbf{3} support images and \textbf{6} query images per class, training the model for 10 episodes with the Adam optimizer (learning rate: \textbf{0.001} and cross-entropy loss). The model achieved \textbf{96.67\%} accuracy and \textbf{0.486} loss on the training set, and \textbf{93.33\%} accuracy and \textbf{0.366} loss on the validation set. It effectively learned to classify tissue types from limited labeled data, demonstrating its proficiency in few-shot learning for classification of SPECT images.

The PRNet model, trained using numerous pairs of cropped patches from a single SPECT image via random double cropping, used an encoder-decoder architecture with skip connections to learn relative positions between patches. Training involved minimizing the self-supervised loss \( L_{ssl} \) using the Adam optimizer (learning rate: \textbf{1e-3)}. Over \textbf{50} epochs, each with 10 iterations, the model achieved a training loss of \textbf{1.395}, demonstrating its ability to capture spatial relationships and accurately reconstruct input patches, as illustrated in Fig. \ref{fig:prnet}.

\section{Conclusion}
\label{sec:conclusion}

In this paper, we addressed the challenges of accurate tissue classification and anatomical localization in SPECT imaging under the constraint of limited data. The Prototypical Network effectively classified myocardium, ventricle, and liver tissues with high training and validation accuracies, demonstrating proficiency in few-shot learning. PRNet, adapted for SPECT imaging, learned spatial relationships between anatomical landmarks and achieved a low training loss, paving the way for integrating and improving segmentation accuracy in deep learning frameworks in the future.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\columnwidth]{proto1.drawio.png} % Adjust the width as needed
    \caption{Support (N=3) and query (N=6) sets of segmented tissues for training Prototypical Network, which needs labeled support set and unlabeled query set}
    \label{fig:proto}
\end{figure}

% \section{Guidelines for Graphics Preparation}
% Tables, graphs, charts, and images should be embedded in the text of your
% paper rather than collected in pages after the text. They should be
% mentioned on the same page or before the appearance of the object itself, as
% \figurename~\ref{fig1} is mentioned here. If there is insufficient
% room in a column for a figure, rather than compress it, the figure is
% allowed to move to the next page and white space is left immediately below.
% Alternatively, you can move text from below the figure to this space, if it
% makes sense to do so. It is good practice to tell the reader what is in the
% figure in the caption and you can include a sentence or two about the
% significance of the figure in the caption. However, lengthy explanations
% should be placed in the body of the paper.

% \begin{figure}[t]
% \centerline{\includegraphics[width=3.5in]{tns1.png}}
% \centerline{\includegraphics[width=3.5in]{tns2.png}}
% \caption{Differential (a) and integral probability of interaction as a function of
% angle of incidence.}
% \label{fig1}
% \end{figure}

% \subsection{Multipart figures}
% Figures comprising more than one sub-figure are presented side-by-side, or
% stacked. Add an alphabetic label enclosed in parentheses inside each
% sub-figure: (a), (b), (c), {\textellipsis}~. Choose a color for the text so
% that it is visible over the image. The text box containing the label should
% have no fill or border; only the letter in parentheses should be visible.

% The figure caption will identify each sub-figure it is describing with these
% labels. Write ``(a) Differential probability of interaction.
% (b) Integral probability of interaction, both as a function of angle of
% incidence,'' or ``Differential (a) and integral (b) probability of
% interaction as a function of angle of incidence.'' Refer to the sub-figure as
% \figurename~\ref{fig1}a in the body of the text.

% If you choose to place figures (multipart or not) side-by-side in a single
% column you might find it convenient to insert them in a single row of a
% tabular. You can then place the captions in next row or insert them in the
% same cell as the figure to keep the captions aligned with the figures.

% \subsection{Using Labels Within Figures}
% When preparing your graphics, if you prefer them to match the text of
% the article, IEEE suggests that you use Times New Roman. The example
% uses Cambria. (This guideline was originally written for \emph{Word}.)

% Figure axis labels are often a source of confusion. Use words rather than
% symbols. As an example, write the quantity ``Angle,'' and the units in
% parentheses. Do not label axes only with units. Thus ``Angle of incidence
% ($^{\circ}$)'' and not just ``$^{\circ}$,''in Fig. 1, above. Do not
% label axes as a ratio of quantities and units. For example, write
% ``Temperature (K),'' not ``Temperature/K.''

% Multipliers can be especially confusing. Write ``Distance (m)'' or
% ``Distance (km).'' Do not write ``Distance (m) $\times$ 1000'' because the
% reader would not know whether the tick mark labels have already been
% multiplied by 1000 or whether the reader needs to multiply them by 1000.
% Similarly, do not put a multiplier at the end of an axis. Figure labels must
% be legible; use at least 8- to 10-point type.

% \subsection{Tables}
% Tables are inserted as {\LaTeX} tables, not as images, and should also be mentioned
% in advance of their appearance. Table~\ref{tab1} shows an
% acceptable layout of a table having both supplementary information below the
% table and an internal footnote. The explanation of the footnote appears
% after the supplementary information.

% \begin{table}
% \caption{Units for Magnetic Properties}
% \label{table}
% \setlength{\tabcolsep}{3pt}
% \begin{tabular}{|p{25pt}|p{75pt}|p{115pt}|}
% \hline
% Symbol&
% Quantity&
% Conversion from Gaussian and \par CGS EMU to SI $^{\mathrm{a}}$ \\
% \hline
% $\Phi $&
% magnetic flux&
% 1 Mx $\to 10^{-8}$ Wb $= 10^{-8}$ V$\cdot $s \\
% $B$&
% magnetic flux density, \par magnetic induction&
% 1 G $\to 10^{-4}$ T $= 10^{-4}$ Wb/m$^{2}$ \\
% $H$&
% magnetic field strength&
% 1 Oe $\to 10^{3}/(4\pi )$ A/m \\
% $m$&
% magnetic moment&
% 1 erg/G $=$ 1 emu \par $\to 10^{-3}$ A$\cdot $m$^{2} = 10^{-3}$ J/T \\
% $M$&
% magnetization&
% 1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 10^{3}$ A/m \\
% 4$\pi M$&
% magnetization&
% 1 G $\to 10^{3}/(4\pi )$ A/m \\
% $\sigma $&
% specific magnetization&
% 1 erg/(G$\cdot $g) $=$ 1 emu/g $\to $ 1 A$\cdot $m$^{2}$/kg \\
% $j$&
% magnetic dipole \par moment&
% 1 erg/G $=$ 1 emu \par $\to 4\pi \times 10^{-10}$ Wb$\cdot $m \\
% $J$&
% magnetic polarization&
% 1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 4\pi \times 10^{-4}$ T \\
% $\chi , \kappa $&
% susceptibility&
% 1 $\to 4\pi $ \\
% $\chi_{\rho }$&
% mass susceptibility&
% 1 cm$^{3}$/g $\to 4\pi \times 10^{-3}$ m$^{3}$/kg \\
% $\mu $&
% permeability&
% 1 $\to 4\pi \times 10^{-7}$ H/m \par $= 4\pi \times 10^{-7}$ Wb/(A$\cdot $m) \\
% $\mu_{r}$&
% relative permeability&
% $\mu \to \mu_{r}$ \\
% $w, W$&
% energy density&
% 1 erg/cm$^{3} \to 10^{-1}$ J/m$^{3}$ \\
% $N, D$&
% demagnetizing factor&
% 1 $\to 1/(4\pi )$ \\
% \hline
% \multicolumn{3}{p{251pt}}{Vertical lines are optional in tables. Statements that serve as captions for
% the entire table do not need footnote letters. }\\
% \multicolumn{3}{p{251pt}}{$^{\mathrm{a}}$Gaussian units are the same as cg emu for magnetostatics; Mx
% $=$ maxwell, G $=$ gauss, Oe $=$ oersted; Wb $=$ weber, V $=$ volt, s $=$
% second, T $=$ tesla, m $=$ meter, A $=$ ampere, J $=$ joule, kg $=$
% kilogram, H $=$ henry.}
% \end{tabular}
% \label{tab1}
% \end{table}

% \subsection{Referring to a Figure or Table Within Your Paper}
% \label{subsec:referring}
% When referring to your figures and tables within your paper, use the
% abbreviation ``Fig.'' even at the beginning of a sentence. Do not abbreviate
% ``Table.''

% It is best if tables do not span pages and columns, but sometimes that
% cannot be helped. When a table must span a page, please repeat the column
% headings on each page.

% \subsection{Sizing of Graphics}
% Most charts, graphs, and tables are one column wide (3.5 inches~/ 88
% millimeters~/ 21 picas) or page wide (7.16 inches~/ 181 millimeters~/ 43
% picas). The maximum height a graphic can be is 8.5 inches (216 millimeters~/
% 54 picas). When choosing the height of a graphic, please allow space for a
% caption. Figures can be sized between column and page widths if the author
% chooses, however it is recommended that figures are not sized less than
% column width unless necessary.

% \subsection{File Formats For Graphics}\label{formats}
% Format and save your graphics using a suitable graphics processing program
% that will allow you to create the images as PostScript (PS), Encapsulated
% PostScript (.EPS), Tagged Image File Format (.TIFF), Portable Document
% Format (.PDF), Portable Network Graphics (.PNG), or Metapost (.MPS), sizes them, and adjusts
% the resolution settings. When
% submitting your final paper, your graphics should all be submitted
% individually in one of these formats along with the manuscript.

% \section{Some Common Misteaks}
% Always check your spelling. The word ``data'' is plural, not singular; the
% singular is ``datum.'' Use the word ``micrometer'' instead of ``micron,'' or
% write ``$\mu $m.'' A graph within a graph is an ``inset,'' not an
% ``insert.'' The word ``alternatively'' is preferred to the word
% ``alternately'' (unless you really mean something that alternates). Use the
% word ``whereas'' instead of ``while'' (unless you are referring to
% simultaneous events). Do not use the word ``essentially'' to mean
% ``approximately'' or ``effectively.'' Do not use the word ``issue'' as a
% euphemism for ``problem.'' When compositions are not specified, separate
% chemical symbols by en-dashes; for example, ``NiMn'' indicates the intermetallic
% compound Ni$_{0.5}$Mn$_{0.5}$ whereas \text{``Ni--Mn''} indicates an alloy of some
% composition Ni$_{x}$Mn$_{1-x}$.

% Be aware of the different meanings of the similar words ``affect'' (usually
% a verb) and ``effect'' (usually a noun), ``complement'' and ``compliment,''
% ``discreet'' and ``discrete,'' ``principal'' (e.g., ``principal
% investigator'') and ``principle'' (e.g., ``principle of measurement''). Do
% not confuse ``imply'' and ``infer.'' Data imply something; people infer
% something from the data.

% Prefixes such as ``non,'' ``sub,'' ``micro,'' ``multi,'' and ``ultra'' are
% not independent words when used as modifiers; they should be joined to the
% words they modify, usually, but not exclusively, without a hyphen:
% nonuniform, subminiature, micrometer, multimeter, ultrasound. However, if
% your paper needs to mention a submarine, TNS prefers you write out the word
% rather than use the word ``sub.'' There is no period after the ``et'' in the
% Latin abbreviation ``\textit{et al.}'' (it is the italicized abbreviation for \textit{et alia,} meaning
% ``and others''). The abbreviation ``i.e.'' abbreviates \textit{id est}, meaning ``that
% is,'' and ``e.g.'' abbreviates the Latin \textit{exempli gratia}, meaning ``for example.'' These
% last two abbreviations are \textit{not} italicized).

% Please do not use math mode for italicized text.

% \section{Conclusion}
% The manuscript submitted to TNS for consideration for publication should be
% formatted according to this document. Doing so will allow an efficient
% review process and minimize the effort required by authors in preparing
% their final files, if the manuscript is accepted for publication.

% Key factors to consider include the following:

% \begin{itemize}
% \item The document must be in this two-column format.
% \item Font sizes for text, section headings, etc. must conform to those in this template.
% \item Proper English usage is necessary. Manuscripts which are vague or unclear because of poor grammar or wording may be rejected without review.
% \item The nominal maximum page length for the submitted manuscript is 8 pages, unless prior arrangements are made with the editor to accept a longer submission.
% \end{itemize}

% \appendices

% \section*{Appendix}

% Appendixes, if needed, appear before the acknowledgment. They do not have
% Roman numeral section numbers. Use
% the \verb|\appendices| command for multiple appendices and
% \verb|\appendix| for a single appendix. Do not use \verb|\section|
% inside a single Appendix.

% If you prefer to number equations separately in each Appendix, please
% use \verb|\numberwithin{equation}{section}| instead of redefining
% equation macros or resetting the equation counter.

% \section*{Acknowledgment}

% Acknowledgments, if any, appear after any appendices and before the list of
% references. The preferred spelling of the word ``acknowledgment'' in US
% English is without an ``e'' after the ``g.'' Use the singular heading even
% if you have many acknowledgments. Avoid expressions such as ``One of us
% (S.B.A.) would like to thank \textellipsis~.'' Instead, write ``F. A. Author thanks
% \textellipsis~.'' In most cases, sponsor and financial support acknowledgments are
% placed in the unnumbered footnote on the first page, not here.


{\em Acknowledgement:} This work was performed as a class project in the Artificial Intelligence course at the Florida Institute of Technology in Spring 2024 and was partially supported by the NIH grant R15EB030807.

\bibliographystyle{IEEEtran}
\bibliography{references.bib}

\end{document}
