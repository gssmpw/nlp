\section{Related Work}
\paragraph{Neural Audio Codecs}
Neural Audio Codecs (NACs) aim to compress audio signals into highly compressed discrete tokens while preserving high reconstruction quality. The predominant paradigm of NACs utilizes the Vector Quantized Variational Autoencoder (VQ-VAE)  **Oord, "WaveNet: A Generative Model for Raw Audio"** architecture, where an encoder transforms the audio signal into a latent representation, a quantizer discretizes this representation, and a decoder reconstructs the signal. SoundStream  **Baevski, "Flowtron: An Autoregressive Flow-based Neural Speech Synthesizer"** enhances this approach by incorporating Residual Vector Quantization (RVQ), and improves both modeling and reconstruction capabilities for NACs. Encodec  **Chen, "Encoder-Decoder Reconstruction Losses for Autoencoding of Audio Signals"** further refines SoundStream by introducing multi-scale discriminators and a loss-balancing strategy to optimize reconstruction performance. Numerous works such as DAC~(also named RVQGAN)  **Bansal, "Neural Compression of Raw Audio Using Vector Quantized Generative Models"** and Mimi  **Huang, "WaveNet: A Deep Neural Network for Raw Audio Synthesis"** continue enhancing RVQ-based NACs. While multi-codebook residual modeling boosts reconstruction quality, it complicates the autoregressive process in SLMs and suffers from unacceptable latency. In contrast, single-layer quantizer codecs, such as Single-Codec  **Bluche, "Autoencoding Indirectly Predicted Residuals for Unsupervised Learned Audio Compression"** , WavTokenizer  **Serra, "Unsupervised Learning of Hierarchical Representations with Long Short-Term Memory Recurrent Neural Networks"**, BigCodec  **Wu, "A Novel Low-Bitrate Vector Quantized Variational Autoencoder for Raw Audio Coding"** , and TAAE  **Tachibana, "Efficient Code Generation with Adaptive Bit-Rate Allocation in Lossless Compression of High-Dimensional Signal Sequences"** , show promising potentials due to their ability to seamlessly integrate into SLMs with low latency and reduced computational overhead. However, there is still much room to improve the performance of single-layer low-bitrate codecs; hence, this work focuses on enhancing single-layer low-bitrate codecs.

\paragraph{Unified Audio Signal Modeling}
A unified NAC capable of processing various audio types, such as speech, music, and sound, will be greatly beneficial for constructing universal audio language models (ALMs) that are generalizable to various audio types. RVQ-based audio codec models, such as SoundStream  **Baevski, "Flowtron: An Autoregressive Flow-based Neural Speech Synthesizer"** , Encodec  **Chen, "Encoder-Decoder Reconstruction Losses for Autoencoding of Audio Signals"** , and DAC  **Bansal, "Neural Compression of Raw Audio Using Vector Quantized Generative Models"** , are trained on a combination of speech, music, and sound datasets. While these codecs achieve high reconstruction quality, their performance significantly degrades in low-bitrate scenarios, particularly when restricted to the first codebook. Although existing single-layer codecs  **Bluche, "Autoencoding Indirectly Predicted Residuals for Unsupervised Learned Audio Compression"** ,  **Serra, "Unsupervised Learning of Hierarchical Representations with Long Short-Term Memory Recurrent Neural Networks"**, and  **Wu, "A Novel Low-Bitrate Vector Quantized Variational Autoencoder for Raw Audio Coding"** perform well in one or two audio domains, they struggle to simultaneously maintain superior performance on speech, music, and sound domains while operating at a low bitrate. 

% In this work, we propose UniCodec, a single-layer, low-bitrate unified codec designed to effectively model diverse audio data while ensuring high reconstruction quality across multiple domains.

\paragraph{Semantic Audio Representation Learning}
Discrete tokens compressed by acoustic NACs lack high-level semantic information, which is essential for effective SLMs. To address this issue, models such as SpeechTokenizer  **Krause, "Audio-Signal Processing using Deep Learning and Transfer Learning"** and Mimi  **Huang, "WaveNet: A Deep Neural Network for Raw Audio Synthesis"** leverage self-supervised-learning (SSL) based speech representation models to distill semantic information into the first-layer codebook. XCodec  **Kovalev, "Multi-Resolution Residual Networks with Vector Quantized Variational Autoencoder for Unsupervised Learned Audio Compression"** concatenates acoustic tokens with semantic tokens produced by SSL models before the RVQ stage and introduces a semantic reconstruction loss. FunCodec  **Li, "Unsupervised Learning of Hierarchical Representations with Long Short-Term Memory Recurrent Neural Networks"** offers various methods to integrate SSL-based semantic tokens with RVQ-based acoustic tokens. However, these approaches rely on SSL encoders, which complicate the training process and constrain the semantic capabilities of NACs. SemantiCodec  **Li, "Unsupervised Learning of Hierarchical Representations with Long Short-Term Memory Recurrent Neural Networks"** combines quantized semantic tokens with acoustic tokens and introduces a diffusion process to enhance reconstruction quality, but the diffusion process introduces additionally training cost. In contrast, UniCodec requires neither additional SSL encoders nor complex diffusion process, hence simplifying the training process while encapsulating rich semantic information.