\section{Related Work}
\paragraph{Neural Audio Codecs}
Neural Audio Codecs (NACs) aim to compress audio signals into highly compressed discrete tokens while preserving high reconstruction quality. The predominant paradigm of NACs utilizes the Vector Quantized Variational Autoencoder (VQ-VAE)~\cite{nips2017-vqvae,icassp2019-vqvaeaudio} architecture, where an encoder transforms the audio signal into a latent representation, a quantizer discretizes this representation, and a decoder reconstructs the signal. SoundStream~\cite{taslp2021-soundstream} enhances this approach by incorporating Residual Vector Quantization (RVQ), and improves both modeling and reconstruction capabilities for NACs. Encodec~\cite{tmlr2023-encodec} further refines SoundStream by introducing multi-scale discriminators and a loss-balancing strategy to optimize reconstruction performance. Numerous works such as DAC~(also named RVQGAN)~\cite{nips2023-dac-rvqgan} and Mimi~\cite{defossez2024moshi} continue enhancing RVQ-based NACs. While multi-codebook residual modeling boosts reconstruction quality, it complicates the autoregressive process in SLMs and suffers from unacceptable latency. In contrast, single-layer quantizer codecs, such as Single-Codec~\cite{interspeech2024-singlecodec}, WavTokenizer~\cite{ji2024wavtokenizer}, BigCodec~\cite{arxiv2024-bigcodec}, and TAAE~\cite{arxiv2024-taae}, show promising potentials due to their ability to seamlessly integrate into SLMs with low latency and reduced computational overhead. However, there is still much room to improve the performance of single-layer low-bitrate codecs; hence, this work focuses on enhancing single-layer low-bitrate codecs.

\paragraph{Unified Audio Signal Modeling}
A unified NAC capable of processing various audio types, such as speech, music, and sound, will be greatly beneficial for constructing universal audio language models (ALMs) that are generalizable to various audio types. RVQ-based audio codec models, such as SoundStream~\cite{taslp2021-soundstream}, Encodec~\cite{tmlr2023-encodec}, and DAC~\cite{nips2023-dac-rvqgan}, are trained on a combination of speech, music, and sound datasets. While these codecs achieve high reconstruction quality, their performance significantly degrades in low-bitrate scenarios, particularly when restricted to the first codebook. Although existing single-layer codecs~\cite{ji2024wavtokenizer} perform well in one or two audio domains, they struggle to simultaneously maintain superior performance on speech, music, and sound domains while operating at a low bitrate. 

% In this work, we propose UniCodec, a single-layer, low-bitrate unified codec designed to effectively model diverse audio data while ensuring high reconstruction quality across multiple domains.

\paragraph{Semantic Audio Representation Learning}
Discrete tokens compressed by acoustic NACs lack high-level semantic information, which is essential for effective SLMs. To address this issue, models such as SpeechTokenizer~\cite{zhang2023speechtokenizer} and Mimi~\cite{defossez2024moshi} leverage self-supervised-learning (SSL) based speech representation models to distill semantic information into the first-layer codebook. XCodec~\cite{xcodec} concatenates acoustic tokens with semantic tokens produced by SSL models before the RVQ stage and introduces a semantic reconstruction loss. FunCodec~\cite{icassp2024-funcodec} offers various methods to integrate SSL-based semantic tokens with RVQ-based acoustic tokens. However, these approaches rely on SSL encoders, which complicate the training process and constrain the semantic capabilities of NACs. SemantiCodec~\cite{liu2024semanticodec} combines quantized semantic tokens with acoustic tokens and introduces a diffusion process to enhance reconstruction quality, but the diffusion process introduces additionally training cost. In contrast, UniCodec requires neither additional SSL encoders nor complex diffusion process, hence simplifying the training process while encapsulating rich semantic information.