


\section{Mixture Density Particle Smoothers} \label{sec:mdps}


    Our novel \emph{Mixture Density Particle Smoother} (MDPS, Fig.~\ref{fig:forward_backward_smoother_flow_diagram}) can be seen as a differentiable TFS, where the forward and backward filters of Eq.~\eqref{eqn:tfs_final_form} are defined as MDPFs (Sec.~\ref{sec:mdpf}).  Using discriminative differentiable particle filters (MDPFs) within the TFS frameworks, and replacing Eq.~\eqref{eq:tfsWeights} with an importance-weighted integration of forward and backward particles, enables an effective and end-to-end differentiable particle smoother. 
    We begin by rewriting Eq.~\eqref{eqn:tfs_final_form} as 
    \begin{equation}
        p(x_t|y_{1:T}) \propto \frac{p(y_t|x_t) p(x_t|y_{1:t-1}) \tilde{p}(x_t| y_{t+1:T})}{\gamma_t(x_t)},
        \label{eqn:mdps}
    \end{equation}
    where the forward and backward filters no longer condition on the current observation.     
    This allows for functionally identical MDPFs to be used for both directions, simplifying implementation. MDPFs parameterize state posteriors as continuous kernel density mixtures:
    \begin{equation}
        p(x_t|y_{1:t-1}) = \sum_{i=1}^{N} \overrightarrow{w}_{t}^{(i)} K(x_t - \overrightarrow{x}_{t}^{(i)}; \overrightarrow{\beta}), \quad\quad
        p(x_t|y_{t+1:T}) = \sum_{i=1}^{N} \overleftarrow{w}_{t}^{(i)} K(x_t - \overleftarrow{x}_{t}^{(i)}; \overleftarrow{\beta}).
        \label{eqn:mdps_mixture_posteriors}
    \end{equation}
    Unlike discrete probability measures, these continuous mixture distributions can be combined via direct multiplication to give a smoothed posterior mixture containing $N^2$ components, one for each pair of forward and backward particles.
    %with parameters $\{ \overleftrightarrow{x}_{t}^{(1:N^2)}, \overleftrightarrow{w}_{t}^{(1:N^2)} ,\overleftrightarrow{\beta}\}$. 
    For this product integration of forward and backward filters, the normalizing constants for all pairs of kernels must be explicitly computed to correctly account for the degree to which hypotheses represented by forward and backward particles are consistent.  These normalization constants are tractable for some simple kernels including Gaussians~\cite{ihler2003efficient}, but more complex for the other cases such as von Mises kernels of orientation angles~\cite{bc1d15dd-524c-38f4-a83a-d55cfe9db0a2, 10.1145/355744.355753}.
    %Importantly we cannot simply express the smoothed posterior as two separate mixture distributions (forward and backward posteriors) which we evaluate when needed, we must multiply them to compute the $N^2$ mixture components.  This is required to form a valid probability distribution for the smoothed posterior $p(x_t|y_{1:T})$ where the weights are normalized such that $\sum_{i=1}^{N^2} \overleftrightarrow{w}_{t}^{(i)} = 1$; i.e. we compute the normalization constant $Z$ of eqn. \ref{eqn:mdps}. Further analytically computing the normalization constant $Z$ may not be possible if $K$ does not admit a closed form CDF function as is the case with the Von Mises distribution \cite{bc1d15dd-524c-38f4-a83a-d55cfe9db0a2, 10.1145/355744.355753}, making weight normalization necessary to compute $Z$.
    
    Direct mixture multiplication eliminates the need to evaluate the dynamics model, as in classic TFS, but introduces significant overhead due to the quadratic scaling of the number of mixture components. To address this issue, our MDPS uses importance sampling where the smoothed posterior is defined by $M \ll N^2$ particles drawn from a mixture of the filter posteriors:
    \begin{equation}
        \overleftrightarrow{x}_{t}^{(i)} \sim q(x_t) = \frac{1}{2}p(x_t|y_{1:t-1}) + \frac{1}{2} p(x_t|y_{t+1:T}), \quad\quad i=1,\ldots,M.
        \label{eqn:mdps_resampling_for_combination}
    \end{equation}
    By construction, this proposal will include regions of the state space that lie within the support of \emph{either} $p(x_t|y_{1:t-1})$ or $p(x_t|y_{t+1:T})$, improving robustness.  
    Our experiments set $M=2N$, drawing $N$ particles from each of the filtered and smoothed posteriors.
    %We set $\overleftrightarrow{x}_{t}^{(i)} = \overrightarrow{x}_{t}^{(i)} \cup \overleftarrow{x}_{t}^{(i)}$ to yield the smoothed particle set of with $2N$ particles as this is a valid sampling from eqn. \ref{eqn:mdps_resampling_for_combination} and simplifies implementation. 
    Given true dynamics and likelihood models, importance sampling may correct for the fact that smoothed particles are drawn from a mixture rather than a product of filtered densities, as well as incorporate the local observation:
    \begin{equation}
        \overleftrightarrow{w}_{t}^{(i)} \propto \frac{ p(y_t|\overleftrightarrow{x}_{t}^{(i)}) p(\overleftrightarrow{x}_{t}^{(i)}|y_{1:t-1}) \tilde{p}(\overleftrightarrow{x}_{t}^{(i)}| y_{t+1:T})  }{\gamma_t(\overleftrightarrow{x}_{t}^{(i)}) q(\overleftrightarrow{x}_{t}^{(i)})},
        \qquad \sum_{i=1}^{M} \overleftrightarrow{w}_{t}^{(i)} = 1.
        \label{eqn:mdps_weights}
    \end{equation} 
    %Finally $\overleftrightarrow{\beta}$ is set as a learnable parameter within MDPS and weights are normalized such that $\sum_{i=1}^{M} \overleftrightarrow{w}_{t}^{(i)} = 1$.  
    To more easily train a discriminative PS, rather than estimating each term in Eq.~\eqref{eqn:mdps_weights} separately, we directly parameterize their product via a feed-forward neural network $l(\cdot)$: 
    \begin{equation}
        \overleftrightarrow{w}_{t}^{(i)} \propto \frac{l(\overleftrightarrow{x}_{t}^{(i)}; y_t, p(\overleftrightarrow{x}_{t}^{(i)}|y_{1:t-1}), \tilde{p}(\overleftrightarrow{x}_{t}^{(i)}| y_{t+1:T}))}{q(\overleftrightarrow{x}_{t}^{(i)})},
        \qquad \sum_{i=1}^{M} \overleftrightarrow{w}_{t}^{(i)} = 1.
        \label{eqn:mdps_weights_nn}
    \end{equation} 
    The posterior weight network $l(\cdot)$ scores particles based on agreement with $y_t$, as well as consistency with the forward and backward filters, and implicitly accounts for the auxiliary distribution $\gamma_t(\cdot)$.  To allow state prediction and compute the training loss, the  smoothed posterior is approximated as:
    \begin{equation}
        p(x_t|y_{1:T}) \approx m(x_t| \overleftrightarrow{x}_{t}^{(:)}, \overleftrightarrow{w}_{t}^{(:)}, \overleftrightarrow{\beta})=\sum_{i=1}^N  \overleftrightarrow{w}_{t}^{(i)} K(x_t - \overleftrightarrow{x}_{t}^{(i)}; \overleftrightarrow{\beta}),
        \label{eqn:smoothed_posterior}
    \end{equation}    
    where $\overleftrightarrow{\beta}$ is a learned, dimension-specific bandwidth parameter.
    
    
    \textbf{Training Loss and Gradient Computation}. We discriminatively train our MDPS by minimizing the negative log-likelihood of the true state sequence: %in the prediction of the true state:
    \begin{equation}
        \mathcal{L} = \frac{1}{T} \sum_{t\in T} -\log(m(x_t | \overleftrightarrow{x}_{t}^{(:)}, \overleftrightarrow{w}_{t}^{(:)}, \overleftrightarrow{\beta})).
    \end{equation}
    During training, the IWSG estimator of Eq.~\eqref{eqn:iwsg_2} provides unbiased estimates of the gradients of the forward and backward resampling steps.  We may similarly estimate gradients of the mixture resampling~\eqref{eqn:mdps_resampling_for_combination} that produces smoothed particles,
    %for the posterior weight model is also possible using similar ideas to IWSG, 
    enabling the first end-to-end differentiable PS:
    \begin{equation}
        \nabla_{\phi }\overleftrightarrow{w}_{t}^{(i)} \propto \frac{\nabla_{\phi }l(\overleftrightarrow{x}_{t}^{(i)}; y_t, p(\overleftrightarrow{x}_{t}^{(i)}|y_{1:t-1}), \tilde{p}(\overleftrightarrow{x}_{t}^{(i)}| y_{t+1:T}))}{q(\overleftrightarrow{x}_{t}^{(i)})}.
        \label{eqn:mdps_weights_nn_diffy}
    \end{equation} 
    
    
    
    \textbf{Training Details.} Because the smoother weights of Eq.~\eqref{eqn:mdps_weights_nn} cannot be effectively learned when filter parameters are random, we train MDPS via a three-stage procedure.  In stage 1, the forward and backward PFs are trained separately (sharing only parameters for the encoders, see Fig.~\ref{fig:forward_backward_smoother_flow_diagram}) to individually predict the state. In stage 2, the PFs are frozen and the particle smoother measurement model $l(\cdot)$ of Eq.~\eqref{eqn:mdps_weights_nn} is trained. In stage 3, all models are unfrozen and trained jointly to minimize the loss in the MDPS output state posterior predictions of the true states. The forward MDPF, backward MDPF, and MDPS posterior each have separate kernel bandwidths ($\overrightarrow{\beta}$, $\overleftarrow{\beta}$, $\overleftrightarrow{\beta}$) %respectively; 
    that are jointly learned with the dynamics and measurement models.  We randomly resample a stochastic subset of the training sequences for each step, and adapt learning rates via the Adam \cite{DBLP:journals/corr/KingmaB14} optimizer. % to optimize all methods.

    \textbf{Computational Requirements.} At training time, to allow unbiased gradient propagation, MDPS computes importance weights for each particle during resampling.  For $N$ particles and $T$ time-steps, this requires $\mathcal{O}(TN^2)$ operations.  At inference time, importance weighting is not needed as the particle weights can simply be set as uniform, and resampling only requires $\mathcal{O}(TN)$ operations.  All phases of our MDPS scale linearly with $N$ at test time, in contrast with other differentiable relaxations such as OT-PF~\cite{pmlr-v139-corenflos21a_optimal_transport}, which requires $\mathcal{O}(TN^2)$ operations for both training and inference.

        \begin{figure}[t]
            \centering
            \includegraphics[width=0.98\textwidth]{resources/main_paper/mapillary_panel_seq_001.png}
            \vskip -0.08in
            \caption{\small{Example trajectories from the MGL dataset with observations shown in the top row. We show the current true state and state history (black arrow and black line), the estimated posterior density of the current state (red cloud, with darker being higher probability) and the top 3 extracted modes (blue arrows) for the MDPS as well as its forward and backward MDPFs. Due to ambiguity at early time-steps, MDPF~\cite{younis2023mdpf} is unable to resolve the correct intersection, and instead places probability mass at multiple intersections. By fusing both forward and backward filters, our MDPS resolves this ambiguity with probability mass focused on the correct intersection.  Furthermore, MDPS provides a tighter posterior density than either MDPF-Forward or MDPF-Backward.}}
            \label{fig:mapillary_panel}
            \vskip -0.1in
        \end{figure}


