

\begin{abstract}
    % Erik GPT
    For challenging state estimation problems arising in domains like vision and robotics, particle-based representations attractively enable temporal reasoning about multiple posterior modes.  Particle smoothers offer the potential for more accurate offline data analysis by propagating information both forward and backward in time, but have classically required human-engineered dynamics and observation models.  Extending recent advances in discriminative training of particle filters, we develop a framework for low-variance propagation of gradients across long time sequences when training particle smoothers.  Our ``two-filter'' smoother integrates particle streams that are propagated forward and backward in time, while incorporating stratification and importance weights in the resampling step to provide low-variance gradient estimates for neural network dynamics and observation models.  The resulting mixture density particle smoother is substantially more accurate than state-of-the-art particle filters, as well as search-based baselines, for city-scale global vehicle localization from real-world videos and maps.
    \vspace*{-4pt}
    % Pre Chatgpt
    % Particle Smoothers approximate complex multi-modal posteriors over time, from a sequence of observations, via a collection of weighted particles. Unlike particle filters, these smoothers use the full sequence of observations when estimating posteriors making them particularly useful for tasks such as offline global localization where the full observation sequence is given and the true posterior is highly multi-modal. While prior work applies particle smoothers to a variety of tasks, none seek to train them end-to-end since they are inherently non-differentiable. The state transition dynamics and observation likelihoods are therefore non-learn functions or are individually trained outside the particle smoothing framework via complex training procedures. Building on prior work of differentiable discriminative particle filters we present a fully differentiable particle smoother. Our mixture density particle smoother has only learned models and is trained end-to-end on data. Our approach shows significant accuracy improvements over existing differentiable particle filters and when applied to the city scale global location task with real-world data.
    % Chat GPT  (with minor edits)
    %Particle smoothers approximate complex, multi-modal posteriors over time using a collection of weighted particles from a sequence of observations and actions. Unlike particle filters, which use only past information when computing state posteriors, particle smoothers utilize the entire sequence, making them ideal for tasks like offline global localization with highly multi-modal posteriors where the full observation sequence is given at inference time. Previous research applied particle smoothers to various tasks but did not train them end-to-end due to their non-differentiability. Thus the state transition dynamics and observation models were either non-learned human-engineered functions or were trained separately via complex training procedures. Building on differentiable discriminative particle filters, we present a fully differentiable particle smoother. Our mixture density particle smoother uses learnable neural networks to parameterize dynamics and observation models and is trained end-to-end on data. Our approach demonstrates significant accuracy improvements over existing differentiable particle filters, particularly when applied to the city-scale global localization task with real-world data.
\end{abstract}

