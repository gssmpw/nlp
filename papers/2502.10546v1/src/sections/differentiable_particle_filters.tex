


\section{Differentiable Particle Filters} \label{sec:diffy_pf}

    Particle filters iteratively estimate the posterior distribution $p(x_t |y_{1:t}, a_{1:t})$ over the state $x_t$ at discrete time $t$ given a sequence of observations $y_{t}$ and, optionally, actions $a_{t}$. PFs use a collection of $N$ samples, or \emph{particles}, $x_t^{(:)} = \{ x_t^{(1)}, ..., x_t^{(N)} \}$ with weights $w_t^{(:)} = \{ w_t^{(1)}, ..., w_t^{(N)} \}$ to flexibly capture multiple posterior modes nonparametrically. Classic PFs are derived from a Markov generative model, %with state transition probabilities $p(x_t| x_{t-1}, a_t)$ and observation likelihoods $p(y_t|x_t)$, 
    leading to an intuitive recursive algorithm that alternates between proposing new particle locations and updating particle weights. End-to-end training requires gradients for each PF step. %Importantly to create an end-to-end learnable PF, each component of the PF must be differentiable.

    \textbf{Particle Proposals.} At each iteration, new particles $x_t^{(:)}$ are proposed by applying a model of the state transition dynamics individually to each particle $x_t^{(i)} \sim p(x_t^{(i)} | x_{t-1}^{(i)}, a_t)$, conditioned on actions $a_t$ if available.
    %\begin{equation}
    %    x_t^{(i)} \sim p(x_t^{(i)} | x_{t-1}^{(i)}, a_t)
    %    \label{eqn:pf_dynamics_model}
    %\end{equation}
    Of note, only simulation of the dynamics model is required; explicit density evaluation is unnecessary.  %Computing the full state transition distribution is not needed but careful attention must be given to create a differentiable stochastic simulation of the dynamics.  
    Using reparameterization \cite{mnih14amortized,kingma14,rezende14vae}, the dynamics model can be defined as a feed-forward neural network $f(\cdot)$ that transforms random (Gaussian) noise to produce new particles:
    \begin{equation}
        x_{t}^{(i)} = f(\eta_t^{(i)}; x_{t-1}^{(i)}, a_t),  \qquad\qquad \eta_t^{(i)} \sim N(0,I).
            \label{eqn:dynamics_model_learned}
    \end{equation}	 

    \textbf{Measurement Updates and Discriminative Training.}  Proposed particles are importance-weighted by the likelihood function, $w_t^{(i)} \propto p(y_t | x_t^{(i)})w_{t-1}^{(i)}$, to incorporate the latest observation $y_t$ into the posterior. 
    %\begin{equation}
    %    w_t^{(i)} = p(y_t | x_t^{(i)})w_{t-1}^{(i)}
    %    \label{eqn:measurement_generative}
    %\end{equation}
    The updated weights are then normalized such that $\sum_{i=1}^{N} w_t^{(i)} = 1$. %to form a valid posterior distribution for $x_t$. 
    However, for complex observations like images or LiDAR, learning accurate generative models $p(y_t|x_t)$ is extremely challenging. Recent work \cite{9635972GausePF, pmlr-v139-corenflos21a_optimal_transport, pmlr-v87-karkus18a_soft_resampling, younis2023mdpf, jonschkowski18_differentiable_particle_filter} has instead learned \emph{discriminative} PFs parameterized by differentiable (typically, deep neural network) \textit{measurement models}:
    \begin{equation}
        w_t^{(i)} \propto l(x_t^{(i)};y_t)w_{t-1}^{(i)}.
    \end{equation}
    Here, $l(x_t;y_t)$ scores particles to minimize a loss, such as negative-log-likelihood or squared-error, in the prediction of true target states $x_t$ that are observed during training.


            \begin{figure}[t]
        \centering
        \includegraphics[width=0.6\textwidth]{resources/main_paper/forward_backward_smoother_flow_diagram.pdf}
        \hspace{0.45in}
        \includegraphics[width=0.3\columnwidth]{resources/main_paper/bev_measurement_model.pdf}
        \vskip -0.03in
         \caption{\small{\emph{Left:} Our MDPS method showing the forward and backward particle filters, which are integrated (via learned neural networks, indicated by trapezoids) to produce a smoothed mixture posterior.
         \emph{Right:} Feature encoders and measurement model used for global localization. First-person camera views are encoded into a Birds-Eye-View (BEV) feature map by extracting features before applying a geometric projection~\citep{sarlin2023orienternet}. Map features are extracted via a feed-forward encoder, and un-normalized particle weights are computed as an inner product between BEV features and features of a local map extracted from the global map at the particle location.}}
         \label{fig:forward_backward_smoother_flow_diagram}
        \vskip -0.1in
    \end{figure}

    \subsection{Particle Resampling}
        The stochastic nature of PF dynamics causes some particles to drift towards states with low posterior probability. These low-weight particles do not usefully track the true system state, wasting computational resources and reducing the expressiveness of the overall approximate posterior. %particles significantly impact the posterior density, its representational power is diminished, and further divergence can negatively affect subsequent posterior densities.
        
        Particle resampling offers a remedy by drawing a new uniformly weighted particle set $\hat{x}_t^{(:)}$ from $x_t^{(:)}$, with each particle duplicated (or not) proportional to its current weight $w_t^{(i)}$. The simplest 
        \emph{multinomial} resampling strategy~\cite{doi:10.1080/10618600.1996.10474692stratified, douc2005comparison, 7079001Resapling} chooses particles independently with replacement:
        \begin{equation}
             \hat{x}_t^{(i)} = x_t^{(j)},  \qquad\qquad j \sim \text{Cat}(w_t^{(1)},\ldots,w_t^{(N)}).
            \label{eqn:multinomial_resampling}
        \end{equation}	
        To maintain an unbiased posterior, resampled particles have weight $\hat{w}_t^{(i)} = \frac{1}{N}$. 
        %maintains an unbiased approximation of the posterior distribution while avoiding degeneracy by redistributing probability mass to all particles. 
        Multinomial resampling may be implemented~\cite{douc2005comparison} by drawing a continuous $\text{Unif}(0,1)$ variable for each particle, and transforming these draws by the inverse \emph{cumulative distribution function} (CDF) of particle weights. %via by sampling from a uniform distribution, $z \sim \text{Uniform}(0, 1)$, and using the inverse cumulative distribution function (CDF) to transform $z$ into a particle index \cite{douc2005comparison}. 

        %Though multinomial resampling is unbiased and trivial to implement, it introduces additional variance caused by random draws. 
        \emph{Stratified} resampling \cite{doi:10.1080/10618600.1996.10474692stratified, douc2005comparison, 7079001Resapling} reduces the variance of conventional multinomial resampling, by first partitioning the interval $(0, 1]$ into $N$ sub-intervals $(0, \frac{1}{N}] \cup ... \cup (1- \frac{1}{N}, 1]$.  One uniform variable is then sampled within each sub-interval, before transforming these draws by the inverse CDF of particle weights. %The same inverse CDF transformation as multinomial resampling is then used to generate the resampled particle index \cite{douc2005comparison}.      
        Our differentiable PS incorporate stratified resampling to reduce variance with negligible computational overhead, making training more robustly avoid local optima (see Fig.~\ref{fig:bearings_only_box_plot_all}).
        
        While other methods like \emph{residual} resampling~\cite{liu1998sequential,douc2005comparison, whitley1994genetic} have been proposed in the PF literature, this partially-deterministic approach is less robust than stratified resampling in our experiments (see Fig.~\ref{fig:bearings_only_box_plot_all}), and also much slower because residual resampling cannot be parallelized across particles.
        %is an alternative variance reduced resampling technique composed of 2 stages   In the first stage, the minimum number of times each particle should be resampled is deterministically computed using the particle weights to produce a resampled particle set that is smaller than the desired resampled particle set size.  In stage two, the remaining particles are sampled via multinomial resampling but with modified weights to account for the particles resampled in stage one \cite{douc2005comparison}. Unfortunately residual resampling is not easily parralelizable onto modern GPU hardware.  
        
        For our mixture density PS, particles are resampled from a continuous Gaussian mixture, in which all components share a common standard deviation $\beta$.  This resampling can equivalently be expressed as $\hat{x}_t^{(i)} = \mu_t^{(i)} + \beta \eta_t^{(i)}$, where $\eta_t^{(i)} \sim N(0, I)$ and $\mu_t^{(i)}$ is generated via discrete sampling of the mixture component means.  We incorporate stratified resampling in this step to boost performance.
        %Variance reducing sampling techniques can be applied to sampling from a mixture model fairly easily. In the case of a Gaussian mixture model with shared standard deviation $\sigma$ for all the mixture components, drawing a sample $z$ from the mixture amounts to discretely sampling a component mean $\mu$ and applying zero mean Gaussian noise $\eta \sim N(0, \sigma)$ to the mean; $z = \mu + \eta$. Selecting the component mean is a discrete sampling procedure allowing for variance reducing sampling methods such as stratified or residual resampling to be use used. 




    \subsection{Differentiable Approximations of Discrete Resampling}
        For discriminative PFs to effectively learn to propagate state estimates over time, gradients are needed for all steps of the PF.  While differentiable dynamics and measurement models are easily constructed via standard neural-network architectures, discrete particle resampling is \emph{not} differentiable.
        %A key requirement of learned particle filters is end-to-end trainability, requiring full end-to-end differentiability.  Though the measurement and state dynamics models can easily be defined to be differentiable, classical discrete particle resampling is not differentiable prompting several works to tackle this issue.

        \textbf{Truncated-Gradient Particle Filters (TG-PF)}~\cite{jonschkowski18_differentiable_particle_filter}, the first so-called ``differentiable'' particle filter, actually treated the resampling step as non-differentiable and simply truncated gradients to zero at resampling, preventing \emph{back-propagation through time} (BPTT)~\cite{werbos1990backpropagation}. Due to this weakness, dynamics models were assumed known rather than learned, and measurement models were learned from biased gradients that fail to propagate information over time, reducing accuracy~\cite{younis2023mdpf}.

        \textbf{Soft Resampling Particle Filters (SR-PF)} \cite{pmlr-v87-karkus18a_soft_resampling} utilize a differentiable resampling procedure that sets particle resampling weights to be a mixture of the true weights and a discrete uniform distribution:
        \begin{equation}
             \hat{x}_t^{(i)} = x_t^{(j)},  \qquad j \sim \text{Cat}(v_t^{(1)},\ldots,v_t^{(N)}), \qquad v_t^{(i)} = (1-\lambda)w_t^{(i)} + \frac{\lambda}{N}.
            \label{eqn:soft_resampling_1}
        \end{equation}	
        Gradients are then propagated via the resampled particle weights defined as:
        \begin{equation}
                 \hat{w}_t^{(i)} = \frac{w_t^{(i)}}{(1-\lambda)w_t^{(i)} + \lambda/N}, 
                 \qquad\qquad \nabla_{\phi}\hat{w}_t^{(i)} = \nabla_{\phi}\Bigg(\frac{w_t^{(i)}}{(1-\lambda)w_t^{(i)} + \lambda/N}\Bigg).
                \label{eqn:sr_weight_2}
        \end{equation}
        %with $\phi$ being parameters which define the particle resampling. 
        This simple approach resamples low-weight particles more frequently, degrading performance.
        The gradients of Eq.~\eqref{eqn:sr_weight_2} also have substantial bias, because they incorrectly assume model perturbations only influence the particle weights in~\eqref{eqn:sr_weight_2}, and not the discrete particle resampling in~\eqref{eqn:soft_resampling_1}.

        \textbf{Relaxations of Discrete Resampling.}
        While discrete particle resampling could potentially be replaced by continuous particle interpolation with samples from a Gumbel-Softmax or Concrete distribution~\cite{gumbel_softmax, maddison2016concrete}, no work has successfully applied such relaxations to PFs, and experiments in \citet{younis2023mdpf} show very poor performance for this baseline.
        %\textbf{Concrete Particle Filter (C-PF)} \todo{I think we get rid of this since we dont experiment on it and it was our own bad baseline} was proposed by \citet{younis2023mdpf} as a baseline whereby discrete particle resampling is relaxed into the continuous domain using the Gumbel-Softmax or Concrete distribution \cite{gumbel_softmax, maddison2016concrete}, allowing for application of reparameterization. A temperature parameter $\lambda$ controls the relaxation whereby large $\lambda$ will interpolate between particles and small $\lambda$ will return samples similar to that of discrete resampling. Selecting $\lambda$ is a variance-bias trade-off with large $\lambda$ having low variance but large bias with bias decreasing and variance increasing as $\lambda \rightarrow 0$. This method degrades as $N$ increases \cite{maddison2016concrete} limiting its usefulness for particle resampling in PFs \cite{younis2023mdpf}.
        Alternatively, \emph{entropy-regularized optimal transport particle filters} (OT-PF)~\cite{pmlr-v139-corenflos21a_optimal_transport} replace discrete resampling with an entropy-regularized optimal transport problem, that minimizes a Wasserstein metric to determine a probabilistic mapping between the weighted pre-resampling particles and uniformly weighted post-resampling particle. 
        %This mapping is computed via optimization to minimizing a Wasserstein metric $\mathcal{W}^2_{2, \lambda}$:
        %\begin{equation}
        %    %\mathcal{W}^2_{2, \lambda} = 
        %    \min_{\tilde{\alpha} \in [0,1]^{N \times N} } \sum_{i,j=1}^{N} %\sum_{j=1}^N 
        %    \tilde{\alpha}_{ij} \Bigg( ||x_t^{(i)} - x_t^{(j)}||^2 + \lambda\log\frac{\tilde{\alpha}_{ij}}{N^{-1} w_t^{(j)}}  \Bigg), 
        %    \text{ s.t. } \sum_{j=1}^N \tilde{\alpha}_{ij} = \frac{1}{N}, \sum_{i=1}^N \tilde{\alpha}_{ij} = w_t^{(j)}.
        %    \label{eqn:ot_problem}
        %\end{equation}
        %where the resampled particles are assigned as interpolations of the pre-resampling particle set according to the computed optimal transport assignment plan $\tilde{\alpha}$; $\hat{x}_t^{(i)} = \sum_{j=1}^N N \tilde{\alpha}_{ij} x_t^{(j)}$. 
        OT-PF performance is sensitive to a non-learned entropy regularization hyperparameter, and the biased gradients induced by this regularization may substantially reduce performance~\cite{younis2023mdpf}.  Furthermore, ``fast'' Sinkhorn algorithms~\cite{NIPS2013_af21d0c9} for entropy-regularized OT still scale quadratically with the number of particles, and in practice are orders of magnitude slower than competing resampling strategies.  This makes OT-PF training prohibitively slow on the challenging city-scale localization tasks considered in this paper, so we do not compare to it.
        
      
        

        \begin{figure}[t]
        \centering
        \includegraphics[width=0.95\textwidth]{resources/main_paper/bearings_only_box_plot_all.pdf}
        \vskip -0.05in
        \caption{\small{Box plots showing median (red line), quartiles (blue box), and range (whiskers) over 11 training runs for Bearings-Only tracking (Sec.~\ref{sec:bearings_only_task}).  We boost the robustness of the top-performing MDPF~\cite{younis2023mdpf}, which previously used multinomial resampling, by incorporating variance-reduced stratified resampling; residual resampling is both slower and less effective.  Stratified resampling provides larger advantages for the less-sophisticated TG-PF~\cite{jonschkowski18_differentiable_particle_filter} and SR-PF~\cite{pmlr-v87-karkus18a_soft_resampling} gradient estimators, but these baselines remain inferior to MDPF.  
        Our MDPS substantially improves on all PFs by incorporating both past and future observations when computing posteriors. 
        Classic FFBS particle smoothers~\cite{doucet2009tutorial, Klaas2006FastPS} have poor performance, even when provided the true likelihoods (rather than a learned approximation), showing the effectiveness of our end-to-end learning of particle proposals and weights.
        %MDPS also substantially improves on FFBS due to being end-to-end learned and assigning the smoothed particle set from both the forward and backward filters. 
        Forward PFs are initialized with noisy samples of the true state, while MDPF-Backward (the backwards-time PF component of MDPS) is initialized by sampling uniformly from the state space.}}
        \label{fig:bearings_only_box_plot_all}
        \vskip -0.1in
    \end{figure}
    \subsection{Mixture Density Particle Filters}
    \label{sec:mdpf}
    \emph{Mixture Density Particle Filters} (MDPF) \cite{younis2023mdpf} are a differentiable variant of regularized PFs~\cite{859873, musso01}. MDPF estimates a continuous \emph{kernel state density}~\cite{Silverman86} by convolving particles with a continuous, and differentiable, kernel function $K$ (such as a Gaussian) with bandwidth hyperparameter $\beta$:
        \begin{equation}    
            m(x_t \mid x_t^{(:)}, w_t^{(:)}, \beta) = \sum_{i=1}^N  w_t^{(i)} K(x_t - x_t^{(i)}; \beta).
            \label{eqn:regularized_pf_density}
        \end{equation}
        Particles are then resampled $\hat{x}^{(i)}_{t}\sim m(x_t \mid x_t^{(:)}, w_t^{(:)}, \beta)$ from this continuous mixture instead of via discrete resampling.  Unbiased and low-variance \emph{Importance Weighted Sample Gradient} (IWSG)~\cite{younis2023mdpf} estimates may then be constructed by viewing the particle proposal $q(z) = m(z \mid \phi_{0})$ to be fixed to the mixture model parameters $\phi_0 = \{ x_t^{(:)}, w_t^{(:)}, \beta\}$ at the current training iteration.  Gradients then account for parameter perturbations \emph{not} by perturbing particle locations as in standard reparameterization~\cite{mnih14amortized,kingma14,rezende14vae}, but by perturbing particle importance weights away from uniform:
        %Instead of sampling from $m(z \mid \phi)$ directly, where $\phi = \{ x_t^{(:)}, w_t^{(:)}, \beta\}$, IWSG samples from some arbitrary continuous distribution $z^{(i)} \sim q(z)$ and applies importance weighting to compute the resampled particle weights and associated gradient:   
        %\begin{equation}
        %    \hat{w}^{(i)} = \frac{m(z^{(i)} \mid \phi)}{q(z^{(i)})}, \qquad\qquad
        %    \nabla_{\phi} \hat{w}^{(i)} = \frac{\nabla_{\phi }m(z^{(i)} \mid \phi)}{q(z^{(i)})}.
        %    \label{eqn:iwsg_1}
        %\end{equation}
        %Setting $q(z) = m(z \mid \phi_{0})$ to be a distinct but identical copy of the continuous mixture, with $\phi = \phi_{0}$, yields uniform importance weights for the resampled particles and associated gradients:
        \begin{equation}
            \hat{w}^{(i)} = \frac{m(z^{(i)} \mid \phi)\big\rvert_{\phi=\phi_0}}{m(z^{(i)} \mid \phi_0)} = 1, \qquad\qquad
            \nabla_{\phi} \hat{w}^{(i)} = \frac{\nabla_{\phi }m(z^{(i)} \mid \phi) \big\rvert_{\phi=\phi_0}}{m(z^{(i)} \mid \phi_0)}.
            \label{eqn:iwsg_2}
        \end{equation}
        %This resampling method produces uniformly weighted samples from the desired mixture distribution with gradients accounting for how importance weights change as the parameters of the mixture are altered. Of note, gradients computed using IWSG do not change the sampling distribution $q(z) = m(z \mid \phi_{0})$ since gradients do not alter resampled particles but rather just their weights. 
        % Importantly for IWSG to be applied, the selected kernel function $K$ must be differentiable. 
        With this approach, the bandwidth parameter $\beta$ may also be tuned for end-to-end prediction of state distributions, avoiding the need for classic bandwidth-selection heuristics~\cite{Silverman86, jones1996brief, bowman1984alternative}.
        %though they often apply narrowly to specific kernel functions and have asymptotic justifications that may not hold for small number of particles.  As such MDPF sets the bandwidth to be a learned parameter trained end-to-end with the rest of the PF.
        An ``adaptive'' variant of MDPF~\cite{younis2023mdpf} incorporates two bandwidths, one for particle resampling (to propagate information over time) and a second for estimation of state posteriors (to compute the loss).  Our MDPS also incorporates separate bandwidths for resampling and state estimation, as detailed below. %experiments always use this adaptive MDPF variant.
        %MDPF further differs from traditional (regularized) particle filters by estimating a continuous distribution, eqn. \ref{eqn:regularized_pf_density}, not only for resampling but also for the posterior state estimate, allowing for more expressive posteriors compared to discrete distributions. An adaptive variant, A-MDPF \cite{younis2023mdpf}, was also proposed where separate mixture densities are used for resampling and the final posterior estimate. These densities share particles but employ distinct weights (computed by separate measurement models) and bandwidths. This adaptation is driven by the differing objectives of resampling, which may prioritize particle exploration, and the final state posterior, which tends to exploit current state knowledge. Throughout the remainder of this paper, we exclusively consider the A-MDPF variant and henceforth refer to it simply as MDPF, omitting the "A-".
        
        
        