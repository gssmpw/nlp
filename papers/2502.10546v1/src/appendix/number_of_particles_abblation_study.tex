\FloatBarrier
\newpage
\section{Number of Particles Ablation Study}

    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.99\textwidth]{resources/appendix/mapillary_recall_curve_plot_varying_number_of_particles.pdf}
        \caption{\small{Recall of position and angle for MDPS using the MGL dataset \cite{sarlin2023orienternet} with varying numbers of particles at inference time. Here we specify the number of particles $N$ used for the forward and backward filters of MDPS. The final MDPS posterior density is defined by $2N$ particles as described in sec. \ref{sec:mdps}. Interestingly, performance plateaus quickly as we increase the number of particles implying MDPS's ability to use particles smartly and efficiently, allowing for fewer particles to be used, lowering the computation and memory requirements neeeded for effective models.}}
        \label{appx_fig:mapillary_recall_vs_num_particles}
    \end{figure}

    A key hyper-parameter for particle filters and smoothers is the number of particles to use at inference time. Using more particles increases performance as shown in fig. \ref{appx_fig:mapillary_recall_vs_num_particles} but performance can quickly plateau. As seen in fig. \ref{fig:mapillary_panel}, effective models tend to concentrate particles densely in likely regions of the state space. By using more particles, the particle density of these likely regions is increased but with diminishing returns.  Each additional particle within the dense regions will vary only slightly from its neighbors, minimally adding to the particle set diversity. Further using more particles increases computation and memory requirements making smarter models which are more particle efficient, such as our MDPS, attractive for real world deployment
    