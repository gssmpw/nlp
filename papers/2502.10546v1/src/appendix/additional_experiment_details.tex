\newpage
\section{Additional Experiment Details}
% - GPU run time

\subsection{Bearings Only Tracking Task}
    The Bearings Only Tracking Task adopted from \citet{younis2023mdpf} aims to track the state of a vehicle as it navigates a simple environment. No actions are provided for this task and the observations are given as noisy bearings to a radar station:
     \begin{equation*}
            y_t \sim \alpha \cdot \text{Uniform}(-\pi, \pi) + (1-\alpha) \cdot \text{VonMises}(\psi(x_t), \kappa),
    \end{equation*}	
    where $\psi(x_t)$ is the true bearing with $\alpha=0.15$ and $\kappa=50$. The velocity of the vehicle varies over time, changing randomly when selecting a target new way point with 1m/s or 2m/s being equally likely.  During training, ground truth states are provided every 4 time-steps however dense true states are given at evaluation time.  All sequences are of length $T=50$ and we use 5000, 1000 and 5000 sequences for training, validation and evaluation respectively. 
    
    For all methods we use 50 particle during training and evaluation. For forward-in-time running PF methods, we initialize the particle set as the true state with small Gaussian noise ($\sigma = 0.01$) on the x-y components of the state. For the angle components of the initial particles we add Von Mises noise (with concentration $\kappa=100$) to the true state. For backward-in-time running PF methods we set the initial particle set as random samples drawn uniformly from the state space. 
        
    Learning rates are varied throughout the training stages ranging from 0.001 to 0.000001 though we find that all methods are robust to learning rate selection when using the Adam \cite{DBLP:journals/corr/KingmaB14} optimizer, with sensible learning rate effecting convergence speed but not performance.

    For SR-PF \cite{pmlr-v87-karkus18a_soft_resampling} we set $\lambda=0.1$.
    
    \subsubsection{Model Architectures}
        All methods (baselines and ours) for the Bearings Only Tracking Task use the same dynamics and measurement model architectures which are described below.

        
        \textbf{Dynamics Models.} We parameterize the dynamics model as a residual neural network as shown in figs. \ref{appx_fig:bearings_only_dynamics} and \ref{appx_fig:bearings_only_dynamics_ffbs}.  To maintain position in-variance, we mask out the position components of particles when input into the dynamics model. We also transform the angle component of the state into a vector representation $T(\theta) = (\sin(\theta), \cos(\theta))$ before applying dynamics.  Afterwards we transform the angle component of the state back into an angle representation $T(u, v) = \text{atan2}(u, v) = \theta$

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.35\textwidth]{resources/appendix/bearings_only_dynamics.pdf}
            \caption{\small{Dynamics model used for the Bearings Only Tracking Task. The output scaling scales the position components of the residual to be within $[-5, 5]$ and $-2, 2$ for the positional and angle (in vector representation) components respectively.}}
            \label{appx_fig:bearings_only_dynamics}
        \end{figure}

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.5\textwidth]{resources/appendix/ffbs_dynamics.pdf}
            \caption{\small{Dynamics model used for FFBS in the Bearings Only Tracking Task. The output scaling scales the position components of the residual to be within $[-5, 5]$ and $-2, 2$ for the positional and angle (in vector representation) components respectively. The dynamics model outputs a mean. Using this mean along with a hand tuned standard deviation allows for simulation from the dynamics model as well as evaluating state transition probabilities as required for FFBS. Of note: the angle dimension of the state is approximated by a Normal distribution with bound variance to avoid issues with angular discontinuities.  The hand-tuned standard deviations values used are $[1.0, 1.0, 1.25]$.}}
            \label{appx_fig:bearings_only_dynamics_ffbs}
        \end{figure}


        \textbf{Measurement Models.} Figure \ref{appx_fig:bearings_only_measurement} shows the feed-forward neural network architecture for the measurement models used for the Bearings Only Tracking Task.

        
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.35\textwidth]{resources/appendix/bearings_only_measurement.pdf}
            \caption{\small{Particle filter measurement model used for the Bearings Only Tracking Task. The output scaling scales the weights to be within $[0.00001, 1]$}}
            \label{appx_fig:bearings_only_measurement}
        \end{figure}


        \textbf{MDPS Forward Backward Combination.} For the Bearings Only Tracking Task, the MDPS smoothed measurement model is very similar to the measurement model using for MDPF but with additional inputs. The MDPS smoothed measurement model network architecture is shown in fig. \ref{appx_fig:bearings_only_smoothed_measurement}.

        
        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.5\textwidth]{resources/appendix/bearings_only_smoothed_measurement.pdf}
            \caption{\small{Measurement model used for the Bearings Only Tracking Task when computing the smoothed particle weights for MDPS. The output scaling scales the weights to be within $[0.00001, 1]$}}
            \label{appx_fig:bearings_only_smoothed_measurement}
        \end{figure}


\subsection{Global Localization Task with Mapillary Geo-Location Dataset and KITTI Datasets}

    In this section we give more information about the experiments conducted with the Mapillary Geo-Location Dataset (MGL)\cite{sarlin2023orienternet} and KITTI \cite{Geiger2013IJRRKitti} datasets.

    For all particle filter methods (including ones internal to MDPS) we use 250 particle during training and evaluation and initialize the filters using 1000 particles. For PF and smoother methods, we initialize the particle set as the true state with Gaussian noise ($\sigma=50$ meters) on the x-y components of the state. For the angle components of the initial particles we add Von Mises noise to the true state.
    
    Initial learning rates are varied throughout the training stages ranging from 0.01 to 0.000001 though we find that all methods are robust to learning rate selection when using the Adam \cite{DBLP:journals/corr/KingmaB14} optimizer, with sensible learning rates effecting convergence speed. For comparison methods, we use the learning rates as specified by the method authors or select them via a brief hyper-parameter search if they are not stated.

    \subsubsection{Mapillary Geo-Location Dataset Additional Details}

        In the MGL dataset, observations are images captured by various types of handheld or vehicle mounted cameras. Some cameras capture $360^{\circ}$ images which require additional processing before being used as observations.  These $360^{\circ}$ images are cropped to a $90^{\circ}$ Field-of-View in random viewing directions, with the same viewing direction being used for the whole observation sequence. All images are then gravity aligned to produce the observation sequence.   A planimetric map of the environment is also provided via the OpenStreetMap platform \cite{OpenStreetMap} at 0.5 meter/pixel resolution, and all observation images are publicly available under a CC-BY-SA license via the Mapillary platform.  All KITTI data is published under the CC-BY-NC-SA licence. 

        Unfortunately the creators of the MGL dataset trained their methods on single observations from the dataset and did not use sequences during training \cite{sarlin2023orienternet}. As such, observations from sequences are scattered amongst the training and testing splits, preventing effecting training of methods that require longer uninterrupted sequence data such as MDPF and MDPS.  We therefore create custom train, validation and evaluation splits of the MGL dataset in order to accommodate longer sequences for training and evaluation.

        Due to data integrity and corruption issues we exclude all sequences from the ``Vilnius" portion of the dataset.


    \subsubsection{MDPF/MDPS Model Architectures}

        \textbf{Dynamics Models.} The network architecture of the dynamics model used for the MGL and KITTI is shown in fig. \ref{appx_fig:mapillary_dynamics} and is similar to that used in the Bearings Only Tracking task, using the same angle to vector particle transformation.  Note for this dynamics model we do not mask out any component of the state.

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.35\textwidth]{resources/appendix/bearings_only_dynamics.pdf}
            \caption{\small{Network architecture for the dynamics model used for the Bearings Only Tracking Task. The output scaling scales the position components of the residual to be within $[-128, 128]$ and $-2, 2$ for the positional and angle (in vector representation) components respectively.}}
            \label{appx_fig:mapillary_dynamics}
        \end{figure}


        

        \textbf{Measurement Models.} The measurement model used for MDPF and MDPS uses the Birds-Eye-View (BEV) feature encoder and the map encoder from the official Dense Search implementation released by \citet{sarlin2023orienternet}. As shown in fig. \ref{fig:forward_backward_smoother_flow_diagram}, a Birds-Eye-View (BEV) feature map is estimated from the observation using a geometric projection \cite{sarlin2023orienternet} where columns of the observation image are considered polar rays with features binned into course depth planes projected away from the camera focal plane. This gives a top-down representation of the local area in polar coordinates (bearing and course distance of an image feature from the camera center). The polar representation of the scene is then sampled into top-down Cartesian coordinates to yield the final BEV feature map.  We refer the reader to \citet{sarlin2023orienternet} and the appendix for more details. To compute particle weights, we compute the alignment, via a dot-product, between the BEV feature map and a local region from the neural map, cropped and rotated at the current particles location. For $l(\cdot)$ from eqn. \ref{eqn:mdps_weights_nn} we use fully-connected layers to produce the final smoothed particle weights from the BEV-map dot-product alignment and the forward and backward filter posterior densities. The map feature encoder is a U-Net based architecture with a VGG-19 \cite{DBLP:journals/corr/SimonyanZ14a_VGG} backbone and the BEV feature encoder is based on a multi-head U-Net with a ResNet-101 \cite{He2015DeepRL} backbone as well as a differentiable but un-learned geometric projection. We refer the reader to \citet{sarlin2023orienternet} for more details about the encoders. To derive the un-normalized particle log-weights, the BEV encoding is compared, via dot-product, to a local map patch extracted at a specific particle to compute an alignment value. This is akin to the dense search described in \citet{sarlin2023orienternet} but  at only a single location determined by the particle.


        \textbf{MDPS Forward Backward Combination.} The MDPS smoother measurement model differs from the measurement models of MDPF as it requires additional inputs as described in eqn \ref{eqn:mdps_weights_nn}.  We implement this model as a 4-layered, 64-wide fully-connected feed-forward neural network with PReLU \cite{He2015DelvingDI_PRELU} activation's shown in fig. \ref{appx_fig:mapillary_smoothed_measurement}. Importantly all computed un-normalized weights are bound to be within $[0.0001, 1]$ using a Sigmoid function with an offset. Input into this network is the BEV-map feature alignment computed in the same way as the MDPF measurement model as well as the posterior probability values from the forward and backward filters for the current smoothed particle.

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.5\textwidth]{resources/appendix/mapillary_kitti_smoothed_measurement_model.pdf}
            \caption{\small{Network architecture for the MDPS smoothed measurement model used for the MGL and KITTI datasets.}}
            \label{appx_fig:mapillary_smoothed_measurement}
        \end{figure}

    
    \subsubsection{MDPS Training Procedure Details}
        As stated in sec. \ref{sec:mdps}, effective training of MDPS requires training in stages. Importantly due to VRAM constraints, we are unable to train large map and observation encoders on long sequences.  Therefore we train on short sequences before freezing the encoders, training the rest of the models on longer sequences.
        
        Training procedure for MDPS on the MGL dataset: 
        \begin{enumerate}   
            \item Train forward and backward MDPFs individually via independent loss functions, sharing map and observation encoders, on short sequences from the dataset. Here we hold the output posterior bandwidths fixed to prevent converging to poor local optima where the bandwidth is widened while the dynamics and measurement models are not informative.
            \item Freeze all MDPF models, unfreeze the MDPF output posterior bandwidths and train on long sequences.
            \item Freeze all MDPF models (including bandwidths) and train the MDPS measurement model and output posterior bandwidth on long sequences.
            \item Unfreeze all models except the map and BEV encoders and train MDPS on long sequences.
        \end{enumerate}

        Due to the small size of the KITTI dataset along with pre-training using the MGL dataset, a special constrained training procedure is required to prevent immediate over-fitting to the training split.  Instead of jointly refining all components simultaneously, we fine-tuning the forward and backward MDPFs before freezing those models and fine-tuning the MDPS smoothed measurement model:
        \begin{enumerate}   
            \item Train the forward and backward MDPFs individually via independent loss functions, sharing map and observation encoders, on short sequences from the dataset. We hold the output posterior bandwidths fixed.
            \item Freeze BEV and map encoders, training the forward and backward MDPFs individually via independent loss functions on long sequences.           
            \item Freeze all MDPF models, unfreeze the MDPF output posterior bandwidths and train on long sequences.
            \item Freeze all MDPF models (including bandwidths) and train the MDPS smoothed measurement model and output posterior bandwidth on long sequences.
            \item Freeze all models except for the MDPS output posterior bandwidth and train on long sequences.
        \end{enumerate}

    \subsubsection{Baseline Implementation Details}
    
        \textbf{Retrieval.} The Retrieval \cite{noe2020eccv} baseline as described by \citet{noe2020eccv} encodes individual patches from the environment global map into the a latent space.  This is inefficient if the map patches are densely sampled and is prohibitive to run for large environments.  Instead a dense feature map can be predicted from the global map in one forward CNN pass to generate dense map encoding for map patches roughly sized according to the CNN receptive field \cite{sarlin2023orienternet}. We adopt this dense encoding approach for Retrieval, implementing the Retrieval method as specified by \citet{sarlin2023orienternet}.
                
        \textbf{Dense Search.} For Dense Search we use the official implementation released by \citet{sarlin2023orienternet} which differs from the text description present in the official paper. In the paper description a location prior is computed from the provided map which estimates regions of the state space that are likely to be occupied (e.g. the prior says that rivers and inside buildings are unlikely to be occupied by a car).  The prior is then multiplied with the observation likelihood (probability volume computed via dense search) to produce the final state posterior. In the official implementation this prior is disabled making the state posterior simply the observation likelihood. Further we use VGG-19 \cite{DBLP:journals/corr/SimonyanZ14a_VGG} as our map encoder backbone.

        \textbf{Retrieval (PF).} \citet{9635972GausePF} embeds standard Retrieval methods within a non-differentiable particle filter where the dynamics are set as Gaussian Noise:
        \begin{equation}
            x_t^{(i)} \sim \mathcal{N}(x_{t-1}^{(i)} + a_t, \gamma)
        \end{equation}
        In our experiments we set $\gamma$ as $2.5m$ for the x-y state position components and $15^{\circ}$ for the angular state components, chosen via a brief hyper-parameter search.  The measurement model is defined as
        \begin{equation}
            w_t^{(i)} = \text{exp}\Bigg( \frac{-d_t^{(i)}}{2 \sigma^2} \Bigg)
        \end{equation}
    where $d_t^{(i)}$ is the alignment of the observation latent encoding with the map patch encoding at the current particles location $x_t^{(i)}$,computed via the Retrieval method.  After a brief hyper-parameter search we set $\sigma = 2$  in our experiments.


        \textbf{Applying baselines to city-scale environments.} Due to memory constraints, dense search over the whole map is not possible (approx. 809 GB is needed for $T=100$ length sequence at 0.5m per pixel resolution). We therefore offer 2 methods for applying this dense search at city scales.  \emph{Ground Truth (GT) Cheat Method:} using the ground truth state, we extract a small region from the map in which we do dense search. This greatly reduces the search space, saving memory but also greatly (and \emph{artificially}) improves performance. \emph{Sliding Window Method:} At $t=1$ a small region extracted around the true state is densely searched. At subsequent time-steps, the best alignment from the dense search of the previous time-step is propagated using $a_t$ and used as the center of a new small region which is then searched. Retrieval methods tend to fail when applied to large environments with \citet{9635972GausePF} even limiting the search to patches on known road networks. To address this, we limit the search space of Retrieval like in Dense Search using the GT Cheat and Sliding Window techniques, considering map patches within small regions.


    \subsubsection{Top 3 Mode Finding via Non-Maximal Suppression}
        Due to multi-modality of the posterior estimate, simply extracting the top mode to evaluate errors is not a good gauge of performance.  Instead we  to extract the top-3 modes from the posterior density for evaluation.  This can be easily achieved via a non-maximal suppression scheme where modes are extracted before particles around those modes are deleted.
        Specifically after extracting the top mode, the distance of all particle to that top mode is calculated. Particle within some threshold of the top mode are deleted from the particle set and the weights of all remaining particles are re-normalized to admit a valid probability density after deletion.  The next top mode is then extracted and the deletion process repeated until a total of 3 modes are extracted. In our implementation, we delete particles that are within $5$m and $30^{\circ}$ of the top mode. 
        
        For methods that admit a discrete probability volume (such as Dense Search and Retrieval), we suppress the values of all probability cells  within $5$m and $30^{\circ}$ of the top mode during the deletion step.  Extracting the top mode can be simply achieved by finding the maximum value within the discrete probability volume.



\subsection{Compute Resources}

    We give an approximation for compute resources needed to run our experiments in tables \ref{appx_tab:bearings_only_compute}, \ref{appx_tab:mapillary_compute} and \ref{appx_tab:kitti_compute}. Since our experiments are bottle-necked by GPU resources and require only minimal CPU and memory needs, we report the GPU needs and GPU runtime for each experiment. In addition to the compute resources stated in this section, we used additional resources over the course of our project when developing our methods, though the amount of resources used is difficult to quantify and thus we do not report here. 
    
    For evaluation, all methods can comfortably run in under 2 hours for the full evaluation split of MGL (using a NVIDIA A6000 GPU) and in under 1 hour for Bearings Only (using a NVIDIA RTX 3090 GPU) and KITTI (using a NVIDIA A6000 GPU).
    

    \begin{table}[ht]
    \centering
    \caption{Computation needs for training Bearing Only Tracking Task. Of note: none of the methods require using the whole GPU and thus we usually train 2-3 methods per GPU simultaneously. The numbers reported assume training each method 11 times sequentially without running in parallel.}
    \label{appx_tab:bearings_only_compute}
    \begin{tabular}{lll}
    \hline
    Experiment          & GPU                      & GPU Runtime  \\ \hline
    TG-PF (Multinomial) & 1x NVIDIA RTX 3090       & $\sim$25 hrs \\
    TG-PF (Stratified)  & 1x NVIDIA RTX 3090       & $\sim$25 hrs \\
    SR-PF (Multinomial) & 1x NVIDIA RTX 3090       & $\sim$21 hrs \\
    SR-PF (Stratified)  & 1x NVIDIA RTX 3090       & $\sim$21 hrs \\
    MDPF (Multinomial)  & 1x NVIDIA RTX 3090       & $\sim$80 hrs \\
    MDPF (Stratified)   & 1x NVIDIA RTX 3090       & $\sim$80 hrs \\
    MDPF-Backward       & 1x NVIDIA RTX 3090       & $\sim$80 hrs \\
    MDPS                & 1x NVIDIA RTX 3090       & $\sim$160 hrs \\ \hline
    \end{tabular}
    \end{table}



    \begin{table}[ht]
    \centering
    \caption{Computation needs for global localization on the MGL dataset. Retrieval (PF) requires no training since all trained models are taken from the Retrieval baseline.  Similarly MDPF requires no training as it is trained within MDPS.}
    \label{appx_tab:mapillary_compute}
    \begin{tabular}{lll}
    \hline
    Experiment          & GPU                          & GPU Runtime  \\ \hline
    Retrieval           & 1x NVIDIA A6000              & $\sim$12 hrs \\
    Retrieval (PF)      & --                           & NA (No Training) \\
    Dense Search        & 4x NVIDIA A6000              & $\sim$48 hrs \\
    MDPF                & --                           & NA (No Training) \\
    MDPS                & 3x NVIDIA A6000              & $\sim$72 hrs \\ \hline
    \end{tabular}
    \end{table}



    
    \begin{table}[ht]
    \centering
    \caption{Computation needs for global localization on the KITTI dataset. Retrieval (PF) requires no training since all trained models are taken from the Retrieval baseline.  For MDPF we report additional resources used during refinement on the KITTI dataset.}
    \label{appx_tab:kitti_compute}
    \begin{tabular}{lll}
    \hline
    Experiment          & GPU                          & GPU Runtime  \\ \hline
    Retrieval           & 1x NVIDIA A6000              & $\sim$1 hrs \\
    Retrieval (PF)      & --                           & NA (No Training) \\
    Dense Search        & 4x NVIDIA A6000              & $\sim$18 hrs \\
    MDPF                & 3x NVIDIA A6000              & $\sim$5 hrs \\
    MDPS                & 3x NVIDIA A6000              & $\sim$10 hrs \\ \hline
    \end{tabular}
    \end{table}