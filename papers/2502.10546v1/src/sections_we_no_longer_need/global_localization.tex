% \section{Global Localization}
%     Algorithms for global localization estimate the posterior distribution of a system state $x_t$, given a sequence of observations $y_t$ and input actions $a_t$ (if available). To localize globally, a map of the environment must be provided (or constructed) in order to anchor algorithms. In this work assume a map is given.

%     Choice of map greatly affects the usefulness of a particular method. Using 3D maps (point clouds, meshes, ext) \todo{cite} gives rich environment features but have prohibitive storage requirements when scaled to cities and are sensitive to small visual appearance changes of the 3D environments (weather, seasons, ext). Implicit maps such as Neural Radiance Fields \cite{mildenhall2020nerf, tancik2022blocknerf, xiangli2022bungeenerf} and Gaussian Splats \cite{kerbl3Dgaussians} offer more compact representations but continue to suffer from the same drawbacks as 3D maps. Recent work has looked at using 2D top-down maps which are widely available from a range of sources \todo{cite works and Google/Apple maps} and include overhead (i.e. satellite) imagery and planimetric 2D maps. Though overhead imagery uses significantly less storage than 3D maps, it is still sensitive to visual appearance changes. In contrast planimetric 2D maps represent the world semantically and are thus robust to small appearance changes.  Planimetric 2D maps also have minimal storage requirements as they are often stored as shape primitives (polygons, lines) with labels and are thus easily expandable to city sized environments. The appeal of using planimetric 2D maps and 2D maps in general has led to a variety of global localization methods.
    
%     \textbf{Retrieval methods} rely on latent vector similarity where a database of latent map vectors is created by extracting and encoding patches from the map into a low-dimentional latent space.  At inference time the observations are encoded into the same low-dimentional space and the database is searched to find the map patch with the closest encoding. Accuracy of retrieval is dependant on the density of patch extraction and how similar nearby patches are. Patches with substantial overlap may encode into very similar latent vectors resulting in errors during database search, hurting performance. 
    
%     \textbf{Refinement methods} refine an initial position estimate via expensive optimization techniques by maximizing the alignment of extracted observation features and features extracted from the map at the current current position. Due to the extreme non-linearity of this alignment problem, refinement methods require a somewhat accurate initial estimate to converge to the correct solution, if at all, limiting their applicability to real problems. Further when applied to city scales, the large optimization landscape often leads to failures refinement with refinement algorithms failing to converge or returning wildly invalid position estimates.

%     \textbf{Dense Search} was proposed by \citet{sarlin2023orienternet} where a birds-eye-view (BEV) of the local area is estimated from the observation via geometric transformation. A dense search for maximum alignment between the BEV and map is then performed using a 2D convolution of the BEV and map used to give a matching score volume $M$ with shape WxHxR where W, H and R are the width, height and rotation search discretizations. Applying a softmax yields a probability volume $P=\text{softmax}(M)$ from which the best position estimate can be extracted. Higher discretization density can lead to better accuracy but memory requirements scaling cubically. Temporal information can be used by warping probability volumes onto the current time-step, like in Markov localization \todo{cite markov localization}, which requires near-exact relative pose information.  \citet{sarlin2023orienternet} computes these near-exact relative poses via a secondary complex un-learned VI-SLAM system \todo{cite vi slam} which significantly adds to the computation and memory needs.