\section{Related Works}
\label{lit_review}
\subsection{Advances in Information Retrieval}
We do not find any papers that leverage commit messages for the task of code search. However, in general natural-language document search we note many advances. Classic lexical methods, such as BM25 ____, have stood the test of time being computationally efficient but suffer from vocabulary mismatch issues, as they rely on exact term matching and fail to capture semantic similarities between queries and documents. To address this limitation, neural ranking models, including BERT-based approaches like BERT-RR ____ and Dense-RR ____, have been proposed. These models leverage the pre-trained contextual representations of BERT ____ to better understand the semantic relevance between queries and documents, leading to significantly improved retrieval precision compared to traditional lexical methods.
% \subsection{Information Retrieval}
% Traditional lexical methods like BM25 ____ are efficient but fail to capture semantics. Neural ranking models ____ based on BERT ____ provide much better semantic relevance leading to significantly improved precision. 
%Multistage retrieval ____ combines traditional high-recall rankers with neural rerankers to significantly improve ranking quality.

\subsection{Neural Code Models}
 % BERT was only pretrained on Natural Language (NL). CodeBERT ____, tries to employ bimodal pre-training with both programming languages (PL) and NL, with Masked Language Modelling (MLM) and Replaced Token Detection (RTD) objectives, leading to significant improvements in tasks like natural language code search and code documentation generation. GraphCodeBERT ____ further advances this by incorporating the inherent structure of code, particularly data flow, into its pre-training, leading to superior performance in code search, clone detection, code translation, and refinement. Finally, with the advent of LLMs trained on Next Token Prediction, systems like CodeT5 ____, StarCoder ____ and CodeLlama specializes in understanding and generating code, leveraging code semantics and developer-assigned identifiers for enhanced performance in tasks like code generation.

Recent advancements in pre-training techniques have significantly improved the performance of language models on code-related tasks. CodeBERT ____ employs bimodal pre-training using Masked Language Modeling (MLM) and Replaced Token Detection (RTD) objectives on a dataset of natural language (NL) and programming language (PL) pairs. This approach enables CodeBERT to learn the relationships between NL and PL, leading to improvements in tasks like natural language code search and code documentation generation.

GraphCodeBERT ____ extends this idea by incorporating the inherent structure of code into its pre-training, constructing a graph representation of code snippets to capture the relationships between variables, functions, and other code elements. More recently, generative LLMs like CodeT5 ____, StarCoder ____, and CodeLlama have emerged, specializing in understanding and generating code by leveraging semantic information present in code, such as developer-assigned identifiers and comments. Since we only focus on retrieval and not generation, we will use CodeBERT as our neural retrieval model.

\subsection{Neural Code Search Benchmarks}
Popular code search benchmarks like CodeSearchNet ____ and CodeXGLUE ____ focus on retrieving relevant code snippets for natural language queries from a large collection of functions across multiple programming languages. Models such as CodeBERT, GraphCodeBERT, and DenseCode ____ tackle this task by encoding both queries and code, computing relevance scores using the \texttt{[CLS]} token or dense representations. However, these benchmarks do not adequately address the more complex task of repository-level code file search, which requires understanding the interconnections and dependencies among various files within a larger codebase, considering the context of changes over time. While benchmarks like SWEBench ____, which consists of 2,294 real-world software engineering problems drawn from GitHub issues and corresponding pull requests across 12 popular Python repositories, propose repository-wide tasks such as editing the codebase to address a given issue, they focus on code completion and only consider bugs involving one file.

\subsection{Repo-Level Code Models and LLM Agents}
Prior works in repository-level code completion have focused on leveraging pre-trained code language models and incorporating project-level context to improve performance. These approaches aim to continue writing unfinished code by considering the broader context of the repository, which is essential in modern modular software development. RepoCoder ____ proposes a framework that combines a similarity-based retriever (over sliding windows of code) with a pre-trained code language model to generate code at line level. CCFinder ____ integrates cross-file context into pre-trained code language models by building a project context graph based on \texttt{import} statements, and does joint learning of in-file and cross-file context. 

More recently, on SWE-Bench ____, the performance of popular LLMs like GPT-4 and Claude Opus remains limited, with only 1-4\% of tasks solved using Retrieval Augmented Generation (RAG) ____. RAG employs BM25 on code files and passes as much context as available within the LLM's context window (max 32K). Stronger LLM-based agents, such as the commercial Devin and open-source agents like Open Devin and SWE-Agent ____, have reportedly increased the success rate to 12-15\% on SWE-Bench tasks. However, the internal workings of these agents have not been publicly disclosed.

Interestingly, SWE-Bench notes that even under oracle settings, where the LLM is provided with the relevant file, GPT-4 could only solve 5\% of issues, compared to 1\% under normal conditions. While this suggests that the retriever may not be the primary bottleneck, we hypothesize that vanilla LLMs do not yet fully exploit the iterative nature and feedback loop available in software development (e.g., bugs, error messages). Consequently, we believe that LLM-based agents could benefit from improved retrievers to quickly and reliably identify a small set of relevant files (5-10) to iterate on for a given user bug, potentially leading to more effective bug resolution. Importantly, none of these works have looked into leveraging past commit histories as a mapping to be learnt to solve current user bugs. This is the area we investigate in this work.

% Recently, it was reported that popular LLMs like GPT-4 and Claude Opus were only able to solve 1-4\% of SWE-Bench tasks with Retrieval Augmented Generation. This is done by using BM25 on code files, and passes as much context as available in the LLM's context that they evaluate (max 32K). With stronger LLMs commercial agents like  Devin and open-source agents like Open Devin and SWE-Agent, they find that they can now solve 12-15\% of SWE-bench tasks reliably, however none of them have publicly revealed how they work yet (Devin is commericial while Open Devin and SWE-Agent are still being worked on). Notably, in SWE-Bench, they note that under oracle settings (meaning the LLM was given the file itself), GPT-4 was only able to solve 5\% of issues (compared to 1\% in normal conditions). While this hints that retriever may not be a bottleneck, we hypothesize that it is because vanilla LLMs do not yet take proper advantage of iteration and feedback loop available in software development (bugs, error messages), and thus LLM agents might be able to take advantage of better retrievers to quickly and reliably pick out 5-10 files to iterate on for a given user bug.


\begin{table*}[ht!]
\footnotesize
\ttfamily
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Column}             & \textbf{Sample} \\ \hline
owner                       & facebook        \\ \hline
repo\_name                  & react        \\ \hline
commit\_date                & 1575406296         \\ \hline
commit\_id                  & f523b2e0d369e3f42938b56784f9ce1990838753        \\ \hline
commit\_message             & Use fewer global variables in Hooks (\#17480)...        \\ \hline
file\_path                  & packages/react-reconciler/src/ReactFiberThrow.js        \\ \hline
previous\_commit\_id        & d75323f65d0f263dd4b0c15cebe987cccf822783        \\ \hline
previous\_file\_content     & @hello ... [content truncated]        \\ \hline
cur\_file\_content          & @world ... [content truncated]        \\ \hline
diff                        & @@ -195,6 +195,18 @@ function throwException(... [content truncated]        \\ \hline
status                      & modified        \\ \hline
is\_merge                   & False          \\ \hline
file\_extension             & js        \\ \hline
\end{tabular}
\caption{Format of data stored in parquet file. \texttt{previous\_*} are set when \texttt{status} is not \texttt{added}, \texttt{is\_merge} is \texttt{True} when \texttt{commit\_id} has $>2$ parents (merge of 2 branches), \texttt{date} is in UNIX format (UTC).}
\label{tab:df_format}
\end{table*}


% Popular code search benchmarks like CodeSearchNet ____ and CodeXGLUE ____ focus on retrieving relevant code snippets for natural language queries from a large collection of functions across multiple programming languages. Models such as CodeBERT, GraphCodeBERT, and DenseCode ____ tackle this task by encoding both queries and code, computing relevance scores using the \texttt{[CLS]} token or dense representations. However, these benchmarks do not adequately address the more complex task of repository-level code file search, which requires understanding the interconnections and dependencies among various files within a larger codebase, considering the context of changes over time. While benchmarks like SWEBench ____ and RepoBench ____ propose repository-wide tasks, they focus on code completion rather than code search specifically.

% The semantic code search task defined by popular code benchmarks like CodeSearchNet ____ or CodeXGLUE ____ involves retrieving relevant code snippets for a natural language query from approximately 6 million functions across six programming languages. Models like CodeBERT/GraphCodeBERT form queries from a function's documentation, and use a BERT reranking by encoding both query and code, and computing relevance scores using \texttt{[CLS]} token. ____ tries doing this by using dense reranking, embedding both query and code in higher-dimensional space, and doing a similarity search.

% This, however, is not quite relevant to the repository-level code file search task that we have which requires analyzing entire repositories. Unlike snippet-level search, repository-level search necessitates a more intricate understanding of a larger and more complex codebase, where the focus extends beyond individual functions to include the interconnections and dependencies among various files, with the context of changes over time. This wider scope aims to capture the essence of a software project as a whole, addressing the needs of comprehensive code understanding and maintenance in real-world development scenarios.

% In our limited search, we were not able to find any papers which tried to do such code file search. The closest system attempting such a task was Github in its recent GitHub Universe 2023 demo ____.

% The closest benchmarks we could find were SWEBench ____ proposing a repository-wide completion task, with 2294 Github Issues across 12 Python repositories, and RepoBench ____ where they develop repository-level retrieval benchmark with 12,000 samples for Python and Java. However, both of them are for single/few line completions, not code search specifically.

% Hence, we find a lack of focus on repository-level code file search in the current literature. This is an important task which can be used for richer context for LLMs downstream for better completion or understanding of tasks.
% \begin{enumerate}
    % \item Lack of focus on repository-level code file search. While existing literature extensively covers code snippet search and repository-level code completion, there is a notable gap in research focusing on repository-level code file search (finding relevant files to be fetched for a given user query). This is an important task which can be used for richer context for LLMs downstream for better completion or understanding of tasks.
    % \item Lack of high quality, multi-language, code file search + snippet completion benchmark or datasets at the repository level. Most of the current benchmarks focus on function-level searching tasks (given a natural language query, which one of the 6M functions is most relevant).
% \end{enumerate}

\begin{table*}[htbp]
\centering
\scriptsize
\ttfamily
\resizebox{\textwidth}{!}{
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% \hline
% \multicolumn{1}{|c|}{\textbf{Repository}}      & \textbf{\begin{tabular}[c]{@{}c@{}}Total \\ Commits\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Total Files \\ Edited\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Mean \\ Edit \\ Freq \\ Per File\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Median Edit \\ Freq \\ Per File\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Mean \\ Files\\  Edited \\ Per Commit\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Median \\ Files\\  Edited \\ Per Commit\end{tabular}} \\ \hline
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Repository} & \textbf{Total Commits} & \textbf{Total Files Edited} & \multicolumn{2}{c|}{\textbf{Edit Freq Per File}} & \multicolumn{2}{c|}{\textbf{Files Edited Per Commit}} \\ \cline{4-7} 
                    &                        &                             & \textbf{\hspace*{1.25mm} Mean \hspace*{1.25mm}} & \textbf{Median} & \textbf{\hspace*{1.5mm} Mean \hspace*{1.5mm}} & \textbf{Median} \\ \hline
                    

\ttfamily{facebook\_react}    & 11609           & 73765                & 10.1         & 4              & 6.4               & 2       \\ \hline
\ttfamily{angular\_angular}                      & 19464           & 151904               & 7.1          & 3              & 7.8               & 2       \\ \hline
\ttfamily{apache\_spark}      & 33679           & 188006               & 13.3         & 4              & 5.6               & 2       \\ \hline
\ttfamily{apache\_kafka}      & 10445           & 75655                & 9.9          & 4              & 7.2               & 2       \\ \hline
\ttfamily{django\_django}     & 21991           & 81252                & 18.3         & 7              & 3.7               & 2       \\ \hline
\ttfamily{julialang\_julia}                      & 46778           & 182112               & 41.7         & 8              & 3.9               & 2       \\ \hline
% \ttfamily{ruby\_ruby}         & 70211           & 180467               & 10.1         & 3              & 2.6               & 1       \\ \hline
% \ttfamily{pytorch\_pytorch}                      & 59554           & 276846               & 14.1         & 6              & 4.6               & 2       \\ \hline
% \ttfamily{\begin{tabular}[c]{@{}l@{}}huggingface\_\\ transformers\end{tabular}} & 11157           & 56363                & 13.3         & 6              & 5.1               & 1       \\ \hline
\ttfamily{redis\_redis}       & 11077           & 29533                & 15.9         & 2              & 2.7               & 1

\\ \hline\multicolumn{5}{|c|}{} \\[-1em]  % This creates an empty row with reduced height
\hline  % Double hline for a thicker line
\textbf{Average}            & 21006.1         & 111746.7             & 16.6        & 4.5            & 5.3              & 1.8         \\ \hline       \end{tabular}}
\caption{Overall Statistics Per Repository}
\label{app_tab:overall_repo_stats}
\end{table*}