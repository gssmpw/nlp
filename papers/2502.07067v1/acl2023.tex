% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[10pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage{listings}

\usepackage{adjustbox}

\usepackage{hyperref}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{algpseudocode}
\usepackage{algorithm}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{longtable}

\usepackage{graphicx}
\usepackage{booktabs}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% \newcommand{\mono}[1]{\texttt{#1}}
\newcommand{\mono}[1]{{\footnotesize\texttt{#1}}}
\newcommand*\NOINDENT{\@@par   % clear parshape parameters
% fool list-awareness code (as supposedly in lstlisting, not checked)
      \@totalleftmargin\z@ \@listdepth\z@ \rightmargin\z@
}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{float}

\title{Repository-level Code Search with Neural Retrieval Methods}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Siddharth Gandhi, Luyu Gao, Jamie Callan \\
  School of Computer Science\\
  Carnegie Mellon University\\
  \texttt{\{ssg2,luyug,callan\}@cs.cmu.edu}}

\begin{document}
\maketitle
\begin{abstract}
% Large Language Models (LLMs) excel at code generation in isolation, but falter when faced with real-world complexities like multi-file dependencies and project context. These gaps limit their practical application in code understanding and maintenance tasks. Current code search models focus majorly on code-snippet level search, but rarely on repository-level search which involves understanding the rationale behind prior changes in the same project to guide the search. Thus, we propose a novel system integrating information retrieval techniques, like BM25 with neural reranking with CodeBERT, to identify relevant files within code repositories for a given user query/bug. Currently, the system produces relevant files for a given user query (like a bug/issue), but the future goal is to use snippets from these highly relevant files to enhance LLM context understanding to achieve better generations for code completion tasks. We start by establishing a BM25 baseline and experiment with various reranking configurations leading to 40\% improvement in MAP/P@10 on a set of LLM-modified test queries to simulate real-world Github issues. We also start development of a dataset of 80+ popular Github repositories, encompassing commit details, file edits, and diffs to encourage further research in this direction. Our code and results obtained are available here\footnote{\texttt{https://github.com/Siddharth-Gandhi/ds}}.
% Large Language Models (LLMs) excel at generating code in isolation but struggle with complex real-world scenarios involving multi-file dependencies and project contexts. This limits their use in code understanding and maintenance. Existing code search models are primarily focused on snippet-level searches, and not on repository-level searches that require understanding the context of past changes in a project. To address this, we propose a new system that combines information retrieval techniques like BM25 with neural reranking using CodeBERT. This system identifies relevant files in code repositories based on user queries or bugs. We hope this tool can help give LLM agents better contextual understanding leading to more accurate code generation by using relevant snippets from retrieved files. (todo) We've achieved a 40\% improvement in MAP/P@10 on test queries resembling real-world Github issues, using various reranking configurations with a BM25 baseline. Our code and results obtained are publicly available\footnote{\texttt{https://github.com/Siddharth-Gandhi/ds}}.

This paper presents a multi-stage reranking system for repository-level code search, which leverages the vastly available commit histories of large open-source repositories to aid in bug fixing. We define the task of repository-level code search as retrieving the set of files from the current state of a code repository that are most relevant to addressing a user's question or bug. The proposed approach combines BM25-based retrieval over commit messages with neural reranking using CodeBERT to identify the most pertinent files. By learning patterns from diverse repositories and their commit histories, the system can surface relevant files for the task at hand. The system leverages both commit messages and source code for relevance matching, and is evaluated in both normal and oracle settings. Experiments on a new dataset created from 7 popular open-source repositories demonstrate substantial improvements of up to 80\% in MAP, MRR and P@1 over the BM25 baseline, across a diverse set of queries, demonstrating the effectiveness this approach. We hope this work aids LLM agents as a tool for better code search and understanding. Our code and results obtained are publicly available\footnote{\texttt{https://github.com/Siddharth-Gandhi/ds}}.

\end{abstract}

\section{Introduction}
\label{intro}
The rise of Large Language Models (LLMs) like GPT-4 \cite{gpt4} has revolutionized software development, enabling powerful code generation and assistance capabilities. Tools like GitHub Copilot, with over 1 million paying users \cite{copilot_demo}, demonstrate the commercial success and developer adoption of these technologies.

However, LLMs still struggle with complex, real-world coding tasks that require understanding code at the repository level, such as bug fixing, which often involves reasoning about function interactions across multiple files and identifying subtle mistakes within large codebases. Even as LLM context windows expand to 100K tokens \cite{anthropic100k} and more \cite{munkhdalai2024leave}, empirical evidence suggests that response quality degrades while the risk of hallucination and cost increases. Research and industry findings \cite{resesarchlongcontext} \cite{pinecone} indicate that a concise, high-quality retrieved context outperforms a longer, lower-quality one. This highlights the potential for well-tuned Information Retrieval (IR) methods like BM25, combined with robust reranking, to greatly narrow the search scope and provide better context to LLMs.

Until recently, the majority of code search and generation research has focused on function or snippet-level queries in isolated contexts, largely ignoring the intricacies of repository-level search. However, the emergence of SWE-Bench \cite{swebench} has provided a standardized benchmark for assessing the capability of LLMs in addressing GitHub issues from prominent open-source repositories. State-of-the-art approaches on SWE-Bench, including SWE-Agent \cite{yang2024sweagent} and Retrieval-Augmented Generation (RAG) methods with Claude 3 Opus \cite{anthropic2024claude3} or GPT-4, have demonstrated promising results by employing basic file search techniques using error messages and stack traces to identify buggy files. Nevertheless, these methods do not fully capitalize on the rich repository-level context available, such as the commit history that reflects code evolution and the underlying rationale behind changes.

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=\linewidth]{arch.png}
    \caption{An overview of our system}
    \label{fig:arch}
\end{figure*}

\paragraph{Task Definition} We define the task of repository-level code search as - given a user query $q$, the goal is to retrieve the set of files $\mathcal{F}$ from the \textit{current} state of a code repository that are most relevant to addressing the question or bug described in $q$.
While the search is performed only over the current repository snapshot, the proposed retrieval system can be trained on a diverse collection of code repositories and their associated commit histories.
This enables the model to learn patterns of how similar issues have been resolved across different codebases over time, potentially enhancing its ability to surface pertinent files for the task at hand.

In this paper, we propose a multi-stage ranking system to address the task of repository-level code search, leveraging both commit messages and source code as relevance indicators. Figure \ref{fig:arch} outlines the architecture of our system.

Given a user query at a specific state of the repository, our system outputs a highly relevant ranked list of files most likely to address the query.
The proposed approach consists of three main components:

\begin{enumerate}
    \item A BM25-based system to search over previous commits with similar messages in the repository and identify the files they modified, narrowing down the search scope from the order of 10,000 to approximately 1,000 files.
    \item A BERT-based CommitReranker that reranks the list obtained from the BM25 system, taking into account the semantic understanding of the commit messages. This step further reduces the number of candidate files from the order of 1,000 to hundreds.
    \item A BERT-based CodeReranker that examines the source code of the candidate files and reranks them to achieve maximum precision, ultimately reducing the list from hundreds to tens of files.
\end{enumerate}


 The ultimate goal is to produce a concise set of highly pertinent files that can serve as enriched file context for a powerful language model, such as GPT-4 or CodeLlama \cite{codellama}. This enhanced context can then be utilized to suggest appropriate code modifications that effectively address the specified issues. We hypothesize that this strategy will result in a substantial improvement in the quality and relevance of the generated code.

% The ideal output of the system would be a handful of very relevant files which can be used as enhanced file context to a capable LLM, such as GPT-4 or CodeLlama \cite{codellama}, to recommend relevant code changes for the given issues. We hypothesize that this approach should lead to significantly improved code generation quality.




\section{Related Works}
\label{lit_review}
\subsection{Advances in Information Retrieval}
We do not find any papers that leverage commit messages for the task of code search. However, in general natural-language document search we note many advances. Classic lexical methods, such as BM25 \cite{bm25}, have stood the test of time being computationally efficient but suffer from vocabulary mismatch issues, as they rely on exact term matching and fail to capture semantic similarities between queries and documents. To address this limitation, neural ranking models, including BERT-based approaches like BERT-RR \cite{bertrr} and Dense-RR \cite{denserr}, have been proposed. These models leverage the pre-trained contextual representations of BERT \cite{devlin2018bert} to better understand the semantic relevance between queries and documents, leading to significantly improved retrieval precision compared to traditional lexical methods.
% \subsection{Information Retrieval}
% Traditional lexical methods like BM25 \cite{bm25} are efficient but fail to capture semantics. Neural ranking models \cite{bertrr,denserr} based on BERT \cite{devlin2018bert} provide much better semantic relevance leading to significantly improved precision. 
%Multistage retrieval \cite{multistage} combines traditional high-recall rankers with neural rerankers to significantly improve ranking quality.

\subsection{Neural Code Models}
 % BERT was only pretrained on Natural Language (NL). CodeBERT \cite{codebert}, tries to employ bimodal pre-training with both programming languages (PL) and NL, with Masked Language Modelling (MLM) and Replaced Token Detection (RTD) objectives, leading to significant improvements in tasks like natural language code search and code documentation generation. GraphCodeBERT \cite{graphcodebert} further advances this by incorporating the inherent structure of code, particularly data flow, into its pre-training, leading to superior performance in code search, clone detection, code translation, and refinement. Finally, with the advent of LLMs trained on Next Token Prediction, systems like CodeT5 \cite{codet5}, StarCoder \cite{starcoder} and CodeLlama specializes in understanding and generating code, leveraging code semantics and developer-assigned identifiers for enhanced performance in tasks like code generation.

Recent advancements in pre-training techniques have significantly improved the performance of language models on code-related tasks. CodeBERT \cite{codebert} employs bimodal pre-training using Masked Language Modeling (MLM) and Replaced Token Detection (RTD) objectives on a dataset of natural language (NL) and programming language (PL) pairs. This approach enables CodeBERT to learn the relationships between NL and PL, leading to improvements in tasks like natural language code search and code documentation generation.

GraphCodeBERT \cite{graphcodebert} extends this idea by incorporating the inherent structure of code into its pre-training, constructing a graph representation of code snippets to capture the relationships between variables, functions, and other code elements. More recently, generative LLMs like CodeT5 \cite{codet5}, StarCoder \cite{starcoder}, and CodeLlama have emerged, specializing in understanding and generating code by leveraging semantic information present in code, such as developer-assigned identifiers and comments. Since we only focus on retrieval and not generation, we will use CodeBERT as our neural retrieval model.

\subsection{Neural Code Search Benchmarks}
Popular code search benchmarks like CodeSearchNet \cite{codesearchnet} and CodeXGLUE \cite{codexglue} focus on retrieving relevant code snippets for natural language queries from a large collection of functions across multiple programming languages. Models such as CodeBERT, GraphCodeBERT, and DenseCode \cite{densecode} tackle this task by encoding both queries and code, computing relevance scores using the \texttt{[CLS]} token or dense representations. However, these benchmarks do not adequately address the more complex task of repository-level code file search, which requires understanding the interconnections and dependencies among various files within a larger codebase, considering the context of changes over time. While benchmarks like SWEBench \cite{swebench}, which consists of 2,294 real-world software engineering problems drawn from GitHub issues and corresponding pull requests across 12 popular Python repositories, propose repository-wide tasks such as editing the codebase to address a given issue, they focus on code completion and only consider bugs involving one file.

\subsection{Repo-Level Code Models and LLM Agents}
Prior works in repository-level code completion have focused on leveraging pre-trained code language models and incorporating project-level context to improve performance. These approaches aim to continue writing unfinished code by considering the broader context of the repository, which is essential in modern modular software development. RepoCoder \cite{repocoder} proposes a framework that combines a similarity-based retriever (over sliding windows of code) with a pre-trained code language model to generate code at line level. CCFinder \cite{ccfinder} integrates cross-file context into pre-trained code language models by building a project context graph based on \texttt{import} statements, and does joint learning of in-file and cross-file context. 

More recently, on SWE-Bench \cite{swebench}, the performance of popular LLMs like GPT-4 and Claude Opus remains limited, with only 1-4\% of tasks solved using Retrieval Augmented Generation (RAG) \cite{rag}. RAG employs BM25 on code files and passes as much context as available within the LLM's context window (max 32K). Stronger LLM-based agents, such as the commercial Devin and open-source agents like Open Devin and SWE-Agent \cite{yang2024sweagent}, have reportedly increased the success rate to 12-15\% on SWE-Bench tasks. However, the internal workings of these agents have not been publicly disclosed.

Interestingly, SWE-Bench notes that even under oracle settings, where the LLM is provided with the relevant file, GPT-4 could only solve 5\% of issues, compared to 1\% under normal conditions. While this suggests that the retriever may not be the primary bottleneck, we hypothesize that vanilla LLMs do not yet fully exploit the iterative nature and feedback loop available in software development (e.g., bugs, error messages). Consequently, we believe that LLM-based agents could benefit from improved retrievers to quickly and reliably identify a small set of relevant files (5-10) to iterate on for a given user bug, potentially leading to more effective bug resolution. Importantly, none of these works have looked into leveraging past commit histories as a mapping to be learnt to solve current user bugs. This is the area we investigate in this work.

% Recently, it was reported that popular LLMs like GPT-4 and Claude Opus were only able to solve 1-4\% of SWE-Bench tasks with Retrieval Augmented Generation. This is done by using BM25 on code files, and passes as much context as available in the LLM's context that they evaluate (max 32K). With stronger LLMs commercial agents like  Devin and open-source agents like Open Devin and SWE-Agent, they find that they can now solve 12-15\% of SWE-bench tasks reliably, however none of them have publicly revealed how they work yet (Devin is commericial while Open Devin and SWE-Agent are still being worked on). Notably, in SWE-Bench, they note that under oracle settings (meaning the LLM was given the file itself), GPT-4 was only able to solve 5\% of issues (compared to 1\% in normal conditions). While this hints that retriever may not be a bottleneck, we hypothesize that it is because vanilla LLMs do not yet take proper advantage of iteration and feedback loop available in software development (bugs, error messages), and thus LLM agents might be able to take advantage of better retrievers to quickly and reliably pick out 5-10 files to iterate on for a given user bug.


\begin{table*}[ht!]
\footnotesize
\ttfamily
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Column}             & \textbf{Sample} \\ \hline
owner                       & facebook        \\ \hline
repo\_name                  & react        \\ \hline
commit\_date                & 1575406296         \\ \hline
commit\_id                  & f523b2e0d369e3f42938b56784f9ce1990838753        \\ \hline
commit\_message             & Use fewer global variables in Hooks (\#17480)...        \\ \hline
file\_path                  & packages/react-reconciler/src/ReactFiberThrow.js        \\ \hline
previous\_commit\_id        & d75323f65d0f263dd4b0c15cebe987cccf822783        \\ \hline
previous\_file\_content     & @hello ... [content truncated]        \\ \hline
cur\_file\_content          & @world ... [content truncated]        \\ \hline
diff                        & @@ -195,6 +195,18 @@ function throwException(... [content truncated]        \\ \hline
status                      & modified        \\ \hline
is\_merge                   & False          \\ \hline
file\_extension             & js        \\ \hline
\end{tabular}
\caption{Format of data stored in parquet file. \texttt{previous\_*} are set when \texttt{status} is not \texttt{added}, \texttt{is\_merge} is \texttt{True} when \texttt{commit\_id} has $>2$ parents (merge of 2 branches), \texttt{date} is in UNIX format (UTC).}
\label{tab:df_format}
\end{table*}


% Popular code search benchmarks like CodeSearchNet \cite{codesearchnet} and CodeXGLUE \cite{codexglue} focus on retrieving relevant code snippets for natural language queries from a large collection of functions across multiple programming languages. Models such as CodeBERT, GraphCodeBERT, and DenseCode \cite{densecode} tackle this task by encoding both queries and code, computing relevance scores using the \texttt{[CLS]} token or dense representations. However, these benchmarks do not adequately address the more complex task of repository-level code file search, which requires understanding the interconnections and dependencies among various files within a larger codebase, considering the context of changes over time. While benchmarks like SWEBench \cite{swebench} and RepoBench \cite{repobench} propose repository-wide tasks, they focus on code completion rather than code search specifically.

% The semantic code search task defined by popular code benchmarks like CodeSearchNet \cite{codesearchnet} or CodeXGLUE \cite{codexglue} involves retrieving relevant code snippets for a natural language query from approximately 6 million functions across six programming languages. Models like CodeBERT/GraphCodeBERT form queries from a function's documentation, and use a BERT reranking by encoding both query and code, and computing relevance scores using \texttt{[CLS]} token. \citet{densecode} tries doing this by using dense reranking, embedding both query and code in higher-dimensional space, and doing a similarity search.

% This, however, is not quite relevant to the repository-level code file search task that we have which requires analyzing entire repositories. Unlike snippet-level search, repository-level search necessitates a more intricate understanding of a larger and more complex codebase, where the focus extends beyond individual functions to include the interconnections and dependencies among various files, with the context of changes over time. This wider scope aims to capture the essence of a software project as a whole, addressing the needs of comprehensive code understanding and maintenance in real-world development scenarios.

% In our limited search, we were not able to find any papers which tried to do such code file search. The closest system attempting such a task was Github in its recent GitHub Universe 2023 demo \cite{copilot_demo}.

% The closest benchmarks we could find were SWEBench \cite{swebench} proposing a repository-wide completion task, with 2294 Github Issues across 12 Python repositories, and RepoBench \cite{repobench} where they develop repository-level retrieval benchmark with 12,000 samples for Python and Java. However, both of them are for single/few line completions, not code search specifically.

% Hence, we find a lack of focus on repository-level code file search in the current literature. This is an important task which can be used for richer context for LLMs downstream for better completion or understanding of tasks.
% \begin{enumerate}
    % \item Lack of focus on repository-level code file search. While existing literature extensively covers code snippet search and repository-level code completion, there is a notable gap in research focusing on repository-level code file search (finding relevant files to be fetched for a given user query). This is an important task which can be used for richer context for LLMs downstream for better completion or understanding of tasks.
    % \item Lack of high quality, multi-language, code file search + snippet completion benchmark or datasets at the repository level. Most of the current benchmarks focus on function-level searching tasks (given a natural language query, which one of the 6M functions is most relevant).
% \end{enumerate}

\begin{table*}[htbp]
\centering
\scriptsize
\ttfamily
\resizebox{\textwidth}{!}{
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% \hline
% \multicolumn{1}{|c|}{\textbf{Repository}}      & \textbf{\begin{tabular}[c]{@{}c@{}}Total \\ Commits\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Total Files \\ Edited\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Mean \\ Edit \\ Freq \\ Per File\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Median Edit \\ Freq \\ Per File\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Mean \\ Files\\  Edited \\ Per Commit\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Median \\ Files\\  Edited \\ Per Commit\end{tabular}} \\ \hline
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Repository} & \textbf{Total Commits} & \textbf{Total Files Edited} & \multicolumn{2}{c|}{\textbf{Edit Freq Per File}} & \multicolumn{2}{c|}{\textbf{Files Edited Per Commit}} \\ \cline{4-7} 
                    &                        &                             & \textbf{\hspace*{1.25mm} Mean \hspace*{1.25mm}} & \textbf{Median} & \textbf{\hspace*{1.5mm} Mean \hspace*{1.5mm}} & \textbf{Median} \\ \hline
                    

\ttfamily{facebook\_react}    & 11609           & 73765                & 10.1         & 4              & 6.4               & 2       \\ \hline
\ttfamily{angular\_angular}                      & 19464           & 151904               & 7.1          & 3              & 7.8               & 2       \\ \hline
\ttfamily{apache\_spark}      & 33679           & 188006               & 13.3         & 4              & 5.6               & 2       \\ \hline
\ttfamily{apache\_kafka}      & 10445           & 75655                & 9.9          & 4              & 7.2               & 2       \\ \hline
\ttfamily{django\_django}     & 21991           & 81252                & 18.3         & 7              & 3.7               & 2       \\ \hline
\ttfamily{julialang\_julia}                      & 46778           & 182112               & 41.7         & 8              & 3.9               & 2       \\ \hline
% \ttfamily{ruby\_ruby}         & 70211           & 180467               & 10.1         & 3              & 2.6               & 1       \\ \hline
% \ttfamily{pytorch\_pytorch}                      & 59554           & 276846               & 14.1         & 6              & 4.6               & 2       \\ \hline
% \ttfamily{\begin{tabular}[c]{@{}l@{}}huggingface\_\\ transformers\end{tabular}} & 11157           & 56363                & 13.3         & 6              & 5.1               & 1       \\ \hline
\ttfamily{redis\_redis}       & 11077           & 29533                & 15.9         & 2              & 2.7               & 1

\\ \hline\multicolumn{5}{|c|}{} \\[-1em]  % This creates an empty row with reduced height
\hline  % Double hline for a thicker line
\textbf{Average}            & 21006.1         & 111746.7             & 16.6        & 4.5            & 5.3              & 1.8         \\ \hline       \end{tabular}}
\caption{Overall Statistics Per Repository}
\label{app_tab:overall_repo_stats}
\end{table*}

\section{Proposed System}
From Figure \ref{fig:arch}, our system aims to take a user bug as input and outputs a list of potential files relevant to the query. At the repository level, we retrieve files from the current state of the repository and ground it in past commit messages and source code. We also incorporate cross-repository training for our rerankers. We discuss specifics below. 



\subsection{Dataset}
At the time of this research, there were no publicly available datasets specifically designed for repository-level code-file search using commit histories. To address this gap, we create a new dataset from scratch, guided by the following objectives:
\begin{itemize}
\item Encompassing a diverse range of programming languages and software projects, ensuring variability in project size, complexity, and domains.
\item Incorporating comprehensive commit histories to facilitate an understanding of the codebase's evolution and the context surrounding changes.
\item Maintaining all versions of each file, along with their corresponding commit IDs, messages, diffs, and status (adhering to the GitHub API convention: \texttt{modified}, \texttt{added}, \texttt{deleted}, or \texttt{renamed}).
\end{itemize}

To efficiently collect this data, we clone repositories locally and employ GitPython\footnote{\scriptsize \url{https://github.com/gitpython-developers/GitPython}}, a library offering optimized and well-tested abstractions for Git commands, to scrape commits effectively. It took approximately 2 full days on 4 SSD-compute nodes to finish the scraping with approximately 150 GB of data (including BM25 index) for 90 popular open source repositories\footnote{\scriptsize List available at \url{https://github.com/Siddharth-Gandhi/ds/blob/boston/misc/repo_info.txt}}. Data for each repository is stored as parquet files, because of its efficient compression, fast reads and cross-platform compatibility. A sample row is shown in Table \ref{tab:df_format}.

Due to compute limitations in subsequent experiments, we currently focus on 7 repositories. Table \ref{app_tab:overall_repo_stats} presents relevant statistics for each repository, including the average number of files edited per commit (to assess commit granularity) and the frequency of edits for individual files throughout the commit history (to identify potentially 'easy' files that are modified across many commits). The median edit frequencies suggests that most edited files are still only in $<2\%$ of commits on average. Thus a Zipfian edit distribution is not likely, and metrics inflation should not be a problem.

% \paragraph{Tokenization}



\begin{table*}[h]
\centering
\begin{tabular}{@{}p{0.45\textwidth}p{0.45\textwidth}@{}}

\hline

\textbf{\small\ttfamily Original Commit Message} & \textbf{\small\ttfamily GPT-4 Transformed Short Query} 
\\ \hline

\tiny\ttfamily
[facebook\_react] Resolve default onRecoverableError at root init (\#23264) Minor follow up to initial onRecoverableError PR. When onRecoverableError is not provided to `createRoot`, the renderer falls back to a default implementation. Originally I implemented this with a host config method, but what we can do instead is pass the default implementation the root constructor as if it were a user provided one. &
\tiny\ttfamily
`createRoot` doesn't provide a default implementation for `onRecoverableError`, causing the renderer to fail when `onRecoverableError` is not specified.
\\
\tiny\ttfamily
[facebook\_react] Allow the user to opt out of seeing "The above error..." addendum (\#13384) * Remove e.suppressReactErrorLogging check before last resort throw It's unnecessary here. It was here because this method called console.error(). But we now rethrow with a clean stack, and that's worth doing regardless of whether the logging is silenced. * Don't print error addendum if 'error' event got preventDefault() * Add fixtures * Use an expando property instead of a WeakSet * Make it a bit less fragile * Clarify comments &
\tiny\ttfamily
When an 'error' event is preventDefault(), unnecessary error addendum prints are still occurring.

\\ \hline
\end{tabular}
\caption{Examples of transformed queries for two sample commit messages from {\small\ttfamily facebook\_react}.}
\label{tab:transformed_query_examples}
\end{table*}

    \subsection{Evaluation}
    \label{sec:eval}
    At the time of starting this research, SWE-bench was not available publicly. Further, SWE-bench only tests the capability to retrieve one file for a given query, however we envision repo-level code search to retrieve multiple files if necessary. Thus we created our own evaluation data from the commit messages we have available.
    
    \paragraph{GPT-Modified (GPT-M) Test Query Set} To create a realistic evaluation setup, we select 100 commits from each repository as test-commits, which are excluded from the training data for all models. For each test-commit, we feed the commit message to GPT-4 and prompt it to generate the most probable problem description in the style of a GitHub issue, using 1-2 sentences. Importantly, we ask GPT-4 to mask the solution presented in the commit message to ensure that the generated issue does not contain information about the fix. Samples are available in Table \ref{tab:transformed_query_examples} and the complete prompt with additional samples is in Appendix \ref{app:gpt4_prompt}. 
    
    The resulting issue descriptions are typically 30-40 tokens, mimicking real-world scenarios where developers often describe issues concisely without extensive knowledge of the underlying problem or its solution. We also experimented with GPT-3.5 Turbo but found that it often leaked solution details into the modified query, undermining the purpose of this masking step.
    
    \paragraph{Metrics} For this task, we use usual IR metrics to evaluate our system: Mean Average Precision (MAP), Precision at $k$ (P@{1,5,10,20}) Mean Reciprocal Rank (MRR), Recall at $k$ (R@{1,10,20,100,500,1000}). 
    
    % \paragraph{Relevance matching} Given a test query $q_t$, which is a transformed version of some test commit $c_t$'s message with a list of actual modified files $F_t$, and our system's retrieved list of files $F_r$, we consider file $F_r[i]$ relevant if it is in $F_t$. Note that by file, we mean file path. A limitation of this approach is not all files in $F_t$ may be relevant to $q_t$, because different Github repos have different rules about how granular each commit is and how much effort the dev put into the commit message (many small commits with precise changes or one huge commit with many different changes, not even listed in commit message).
    
    \paragraph{Relevance matching} For a test query $q_t$, which is a GPT-Modified version of a test commit $c_t$ with a list of actually modified files $F_t$, we consider a file $F_r[i]$ from our system's retrieved list of files $F_r$ to be relevant if it is present in $F_t$. Here, a file refers to its file path. However, this approach has a limitation: not all files in $F_t$ may be relevant to $q_t$, as different GitHub repositories vary in their commit granularity and the level of detail in commit messages, ranging from many small commits with precise changes to a single large commit with multiple changes not fully described in the message. Table \ref{tab:test_set_stats} shows the number of relevant files per test query for each repository.
    
\begin{table}[ht!]
\centering
\scriptsize
\ttfamily
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|l|c|c|}
% \hline
% \textbf{Repository} & \begin{tabular}[c]{@{}c@{}}\textbf{Avg Files} \\ \textbf{Edited / Query}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Median Files} \\ \textbf{Edited / Query}\end{tabular} \\ \hline

\begin{tabular}{|l|c|c|}
\hline
\textbf{Repository} & \multicolumn{2}{c|}{\textbf{Files Edited Per Query}} \\ \cline{2-3} 
                     & \textbf{\hspace*{1.5mm} Mean \hspace*{1.5mm}} & \textbf{Median} \\ \hline
\ttfamily{facebook\_react}            & 5.1                & 4                     \\ \hline
\ttfamily{angular\_angular}           & 3.5                & 2                     \\ \hline
\ttfamily{apache\_spark}              & 3.9                & 3                     \\ \hline
\ttfamily{apache\_kafka}              & 5.4                & 3                     \\ \hline
\ttfamily{django\_django}             & 2                  & 2                     \\ \hline
\ttfamily{julialang\_julia}          & 2.4                & 2                     \\ \hline
% \ttfamily{ruby\_ruby}                  & 1.4                & 1                     \\ \hline
% \ttfamily{pytorch\_pytorch}            & 3.4                & 3                     \\ \hline
% \ttfamily{huggingface\_transformers}    & 5.1                & 4                     \\ \hline
\ttfamily{redis\_redis}               & 2.4                & 2                     \\ \hline
\multicolumn{3}{|c|}{} \\[-1em]  % This creates an empty row with reduced height
\hline  % Double hline for a thicker line
\textbf{Average}           & 3.5               & 2.8                     \\ \hline
\end{tabular}
\caption{Test Query Set Statistics Per Repository}
\label{tab:test_set_stats}
\end{table}





\subsection{BM25 Initial Ranker}
\label{sec:bm25}
To identify relevant files for a given user query, we first employ BM25 to calculate similarity scores between the query and commit messages (as shown in Table \ref{tab:df_format}). We select BM25 \cite{bm25} as a baseline and initial ranker for further rerankers because of its efficiency and high recall over a variety of datasets. We use Pyserini's \cite{Lin_etal_SIGIR2021_Pyserini} implementation of BM25 in Python.

The objective of matching commits is to determine whether a similar issue has been encountered before in the repository, either exactly or approximately. BM25 retrieves relevant commits, but each commit can modify multiple files simultaneously. However, our ultimate goal is to retrieve a set of files, not commits. To achieve this, we aggregate the BM25 scores across the files, considering the scores of all commits that have modified each file. This aggregation process allows us to identify the most relevant files based on their association with commits that are similar to the given query.

\paragraph{File Based Aggregation} Say for a given query $q$, BM25 retrieves commits $[c_1, c_2, c_3]$ where $c_1$ had edited files $[f_1, f_2, f_3]$ and received a BM25 score 5, $c2$ had files $[f_2, f_5, f_1, f_6]$ with score 3, and $c3$ had $[f_4, f_3, f_2, f_7, f_8]$ with score 1. Then we can aggregate scores across files as $[f_2: 9, f_1: 8, f_3: 6...]$ using various aggregation strategies like \mono{sump} (shown), \mono{maxp} or \mono{avgp}. In our preliminary experiments, we found that \mono{maxp} performed the best (in R@1000), so we use that for the remaining experiments.

\paragraph{Time Masking} As our evaluation is based on test commits, which represent past repository states, and BM25 searches over all available commits, it is essential to apply masking to filter out retrieved future commits from the search results. This prevents information leakage from future commits when evaluating historical repository states, maintaining the integrity of the evaluation and eliminating any unfair advantage. It is important to note that in a real-time deployment scenario, masking would not be necessary since there would be no future versions of the files available at the time of the search.


\paragraph{Tokenization} We use TikToken \cite{tiktoken}, a fast Byte Pair Encoding (BPE) tokenizer trained on both code and natural language, to tokenize the commit messages when creating search index for each repository. This approach is more suitable for queries like GitHub issues or user bugs, which often contain programming snippets, compared to Pyserini's built-in lexical tokenizers designed for natural language. Table \ref{app_tab:repo_token_stats} shows token statistics over commit messages and GPT-Modified query set.

\begin{table}
\centering
\scriptsize
\ttfamily

\begin{tabular}{|l|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Repository}}  & \textbf{\begin{tabular}[c]{@{}c@{}}Commit \\  Messages \\ Avg Tokens\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}GPT-M \\  Queries \\ Avg Tokens\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}File Path \\ Avg Tokens\end{tabular}} \\ \hline
\ttfamily{facebook\_react}                   & 242.1   & 36.1    & 20.9        \\ \hline
\ttfamily{angular\_angular}                  & 197.1      & 36.7              & 20.3      \\ \hline
\ttfamily{apache\_spark}                     & 819.9   & 39.6     & 30.3    \\ \hline
\ttfamily{apache\_kafka}                     & 254.9   & 37.1     & 28.1   \\ \hline
\ttfamily{django\_django}                    & 59.9     & 27.3               & 14        \\ \hline
\ttfamily{julialang\_julia}                  & 124.3    & 32.2    & 9     \\ \hline
% \ttfamily{ruby\_ruby}                        & 156.1    & 32      & 6.6   \\ \hline
% \ttfamily{pytorch\_pytorch}                  & 494.5     & 38.4               & 14.5                        \\ \hline
% \ttfamily{\begin{tabular}[c]{@{}l@{}}huggingface\_\\ transformers\end{tabular}} & 388.6  & 33.8      & 15.2  \\ \hline
\ttfamily{redis\_redis}                      & 236.1    & 37.5    & 8.1    \\ \hline
\multicolumn{4}{|c|}{} \\[-1em]  % This creates an empty row with reduced height \\
\hline
% \hline  % Double hline for a thicker line
\textbf{Average}    & 276.3                        & 35.1     & 18.7          \\ \hline
\end{tabular}
\caption{Token Statistics for training sets per repository} %(tokenized by \mono{microsoft/codebert-base})}
\label{app_tab:repo_token_stats}
\end{table}

\paragraph{File ID (FID) Mapping and Filtering for Current Repository State} File paths in a repository can change over time due to deletions, renaming, or branching. To address this issue and ensure that the retrieved files exist in the current repository state, we employ a two-stage approach. First, we parse all commits in the repository and create FID objects, where the FID of a file $f$ represents all possible file paths that the file has ever had. These mappings are stored as caches from FID to file path and vice versa. Figure \ref{fig:fid_hist} shows the distribution of the number of file paths mapped to each File ID (FID). The majority of FIDs map to 1-5 files, but there are a few outliers where a single FID maps to hundreds of files, such as \texttt{init} files or multi-language documentation. Second, after retrieving relevant commits using BM25, we filter out FIDs that do not exist in the current repository state, which has been previously cached. This process guarantees that the output consists of valid files present in the repository's current state and reduces the retrieved BM25 file list by half.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\linewidth]{fid_hist.png}
    \caption{Histogram of Number of files per FID across 7 repositories}
    \label{fig:fid_hist}
\end{figure}

It is important to note that our approach relies on Git's internal tracking of file renames, which is based on heuristics and may not be entirely accurate. Consequently, there are cases where a single FID may map to two different files at the same repository state, which should never occur. In such instances, we currently select one of the file paths. The algorithm for computing file paths is linked in Appendix \ref{app:fid}, along with a discussion on the inaccuracies in Git's tracking.

% \paragraph{FID and Filter Invalid and Caching to get current state} File Paths change over time. BM25 retrieves commits which may have file paths which were otherwise now deleted or have been renamed or were not part of the current branch history. To deal with this, we have a 2 stage approach. Parse through all commits in the repository and create FID objects where FID of a file $f$ is all possible file paths that that file has ever taken. This is stored as caches from FID->file path and file path to FID. Next, after we filter out FIDs which do not exist at current state of repo (which has been cached previously). This ensures that the output are valid files which exist at the current state of the repository. However, one caveat is that we rely on Git's internal tracking of renames which is based on heuristics and not entirely accurate. Thus, FID mappings are not completely correct. There are cases where one FID maps to 2 different files at the same state of repo which should never happen. Currently, we just take one of file paths. The algorithm for computing file paths is available in Appendix \ref{fid} along with an inaccuracy in Git's tracking.

% \begin{table*}[ht!]
% \centering
% \scriptsize
% \ttfamily
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Repository} & \begin{tabular}[c]{@{}c@{}}\textbf{Commit Queries} \\ \textbf{Avg Tokens}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Short Queries} \\ \textbf{Avg Tokens}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Avg Files} \\ \textbf{Edited / Query}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Median Files} \\ \textbf{Edited / Query}\end{tabular} \\ \hline
% \ttfamily{facebook\_react}           & 180.5                   & 36.4                  & 5.1                & 4                     \\ \hline
% \ttfamily{angular\_angular}          & 164.6                   & 37.5                  & 3.5                & 2                     \\ \hline
% \ttfamily{apache\_spark}             & 576.8                   & 38.4                  & 3.9                & 3                     \\ \hline
% \ttfamily{apache\_kafka}             & 198.3                   & 39.7                  & 5.4                & 3                     \\ \hline
% \ttfamily{django\_django}            & 52                      & 25.3                  & 2                  & 2                     \\ \hline
% \ttfamily{julialang\_julia}          & 84.6                    & 31.4                  & 2.4                & 2                     \\ \hline
% % \ttfamily{ruby\_ruby}                & 121.5                   & 33.1                  & 1.4                & 1                     \\ \hline
% % \ttfamily{pytorch\_pytorch}          & 326.6                   & 36.8                  & 3.4                & 3                     \\ \hline
% % \ttfamily{huggingface\_transformers} & 231.2                   & 33                    & 5.1                & 4                     \\ \hline
% \ttfamily{redis\_redis}              & 161                     & 36.3                  & 2.4                & 2                     \\ \hline
% \multicolumn{5}{|c|}{} \\[-1em]  % This creates an empty row with reduced height
% \hline  % Double hline for a thicker line
% \textbf{Average}              & 209.7                     & 34.8             & 3.5               & 2.6                     \\ \hline
% \end{tabular}}
% \caption{Test Query Set Statistics Per Repository (Tokenized using \mono{microsoft/codebert-base} tokenizer)}
% \label{tab:test_set_stats}
% \end{table*}

After this initial retrieval, including retrieving commits, aggregating scores, masking future commits, and filtering invalid file paths, BM25 ultimately provides us with a ranked list of files that it considers highly likely to address the user query $q$ based on the similarity of the query to past commit messages. This reduces the search space from 10,000+ files to about 1000 files.


\subsection{CommitReranker: Better Commit Message Matching}
\label{sec:bertrr}
% BM25 matches commit messages with exact-token matches and does not capture the semantic meaning and context of each message. There could also be vocabulary mismatch issues. Thus to improve the rankings, we use a BERT reranker \cite{bertrr}, on the commit messages to rerank the top $k$ documents (files in this case). We use a BERT model trained on code, such as \mono{microsoft/codebert-base}. We embed the query and commit messages (passage) together (separated by \mono{[SEP]} token) using the BERT model, and get relevance scores by passing the \mono{[CLS]} token representation through a linear layer. This score captures the semantic similarity between the query and the passage. 

While BM25 effectively retrieves relevant commits, it relies on exact token matches and may not capture the semantic meaning and context of each message, leading to vocabulary mismatch issues. To address this and improve the rankings, particularly the Recall@100 (R@100), we employ a BERT-based reranker \cite{bertrr} on the commit messages to rerank the top $k$ files. BERT first embed the query and commit messages (passage) together, separated by the \texttt{[SEP]} token. Relevance scores are obtained by passing the \texttt{[CLS]} token representation through a linear layer, capturing the semantic similarity between the query and the passage.

Our ultimate goal is to incorporate the source code itself into our ranking pipeline. However, computing this for over 1,000 files might be inefficient. By using a BERT reranker to mitigate vocabulary mismatches and improve the predicted scores of (Query, Retrieved Commit Message) pairs, we aim to enhance the R@100 and obtain better rankings of file paths through aggregation.

% However, for each file $f$, it can have many potential commits $C=[c_1, c_2, c_3]$ in which it was edited, so a question arises - which commit messages should we choose? We follow a simple idea - because of the way we do file aggregation in BM25, the commit list $C$ will always be sorted by their initial BM25 scores. So we just pick a fixed number of these commits (hyperparameter) and aggregate their results to get one score per file. This is very similar to the passage count hyperparameter in the usual BERT reranking settings.

\paragraph{One File, Multiple Commit Mapping}
For a given (query, file) pair, the CommitReranker considers all commits $C=[c_1, c_2, c_3]$ in which the file was modified. We compute a relevance score for each (query, commit message) pair and aggregate the individual commit scores to obtain a single score for the file. We experimented with different aggregation methods and found that taking the maximum score (\texttt{maxp}) across all commits yielded the best results, aligning with the intuition that only one of the file modifications throughout the commits might be relevant to the current issue, while others may address different concerns. % This approach is similar to the passage count hyperparameter in standard BERT reranking settings.
% In our approach, each file $f$ can be associated with multiple commits $C=[c_1, c_2, c_3]$ in which it was edited. To determine which commit messages to use for reranking, we leverage the fact that the commit list $C$ is always sorted by their initial BM25 scores due to our file aggregation method. We can select a all (what we do) or a fixed number of these commits (for efficiency), and aggregate their results to obtain a single score per file, similar to the passage count hyperparameter in standard BERT reranking settings. We found \texttt{maxp} aggregation worked best for this, which makes intuitive sense too - only one of the times the file was edited throught the commits might be relevant to the current issue at hand (others may be about different issues)

\paragraph{Supervised Contrastive Finetuning}
To adapt CodeBERT for the specific task of commit message reranking, we finetune it using supervised contrastive training on (query, passage, label) triplets constructed from past training data. The training data is obtained as follows:

\begin{itemize}
\item \textbf{Query:} A query $q_{train}$ from a set of training queries $Q_{train}$, which are GPT-modified versions of a fixed number of train commits (similar to Section \ref{sec:eval}). Each query has a ground truth list of edited files $F_{truth}$. We also found that training on just the commit messages to be equivalent. 

\item \textbf{Passages:} For a given query $q_{train}$, we retrieve pre-file aggregation BM25 results $R_{bm25} = [c_1, c_2,...]$. This step effectively finds commit messages similar to the user query. Note that each commit $c_i$ can have multiple associated files $F_{c_i}$.
\item \textbf{Label:} The task now becomes determining if a given query and retrieved commit message are related to a similar issue. As this is a non-trivial task, we rely on heuristics. One approach is to take the intersection between $F_{truth}$ and $F_{c_i}$ and consider it positive (label = 1) if it is non-zero, otherwise negative (label = 0). Another approach is to compare the diffs $d_{truth}$ and $d_{c_i}$ line by line (expanded in Section \ref{sec:coderr}) to assign labels. However, in our testing, we did not find a significant difference between these approaches.
\end{itemize}

\paragraph{Loss Function} The model is trained using the Mean Squared Error (MSE) loss between the predicted score and the label, with the objective of assigning higher scores to (passage) commits that have edited files similar to those of the (query) commits. This training process aims to capture the model's understanding of the underlying motivation behind the code changes. We also experimented with using cross-entropy loss for finetuning; however, we found that it yielded suboptimal results compared to MSE.

Thus, for a given (query, commit message) pair, CommitReranker predicts a score between 0 and 1, indicating the similarity between the query and the commit message.

% \paragraph{Finetuning BERT} CodeBERT by default is general purpose and not designed for any specific task such as reranking. We experimented without finetuning, and the results were random, significantly worse than just using BM25. Hence, we finetune CodeBERT by using the Mean Squared Error (MSE) loss between the prediction score and label on (query, passage, label) triplets, constructed from past training data. Here, MSE is used because our model with linear layer head predicts a real-valued score between 0-1, not just binary 0/1 and, the triplets are constructed as follows: for a set of training queries $T_q$,

% \begin{itemize}
%     \item \textbf{Query:} A query $q \in T_q$, where $q$\footnote{ \label{note1}Here commits $c$ or queries $q$ are objects similar to Table \ref{tab:df_format}, and we can fetch individual commit message or list of files edited in that particular commit easily from the dataset} is either a commit message directly from training dataset (similar to \mono{commit\_message} from Table \ref{tab:df_format}) or some modification of it, such as using a LLM like GPT4 to transform such commit messages into potential queries or issues which might have resulted in the commit. This is done to learn potentially better models grounded on pseudo-realistic data, however, we also hypothesize that just vanilla commit messages should also be good enough for training.
%     \item \textbf{Passages:} For a given query $q$, we retrieve BM25 results $R_{bm25} = [c_1, c_2,...]$ at the commit level (i.e. not file aggregated), since BERT only cares about query-commit message similarity and does not consider files. Then for each commit $c$\footnotemark[4] in $R_{bm25}$, $c$'s commit message is a potential passage. From Appendix \ref{app:train_stats}, commit messages are short and can generally fit within the 512 sequence length of BERT (worst case, 50 tokens might be truncated), so we do not need to split each passage into subpassages. However, there could be hundreds of retrieved commits for $q$, so we first sort $R_{bm25}$, by the number of common files between $q$ and $c$.
%     \item \textbf{Label: } Next, we pick first $N_{+}$ (hyperparameter) non-0 scored passages ($c$'commit message) from the sorted $R_{bm25}$ and assign them label 1 (implying that $q$ and $c$ are related in some way, since they changed similar files) and next $N_{-}$ (hyperparameter) with label 0 ($q$ and $c$'s commit message are not related since they do not change similar files).
% \end{itemize}
% The goal is for (passage) commits with similar edited files to train (query) commits to get a higher score using our model. This would imply understanding the underlying motivation behind the changes and being able to retrieve those same files, should a similar query occur. 

% \paragraph{Combined BERT} We prepare the triplet data and train individual BERT reranker models for each repository. But an interesting question arises - what happens if we combine all data across 10 repositories and train one big model? We hypothesize that it might not provide very good relevance indicators since different repositories have different commit message contents, conventions or files edited, so BERT might not learn the best relevance indicators from such diverse data.

% \subsection{Code Reranker}
% \label{sec:coderr}
% So far, we only utilize commit messages to do our rankings but not the code files themselves. Here, we develop an initial model for integrating relevance indicators from the code files. We again finetune a \mono{microsoft/codebert-base} model, using the same loss as \ref{sec:bertrr}, and we embed \mono{[SEP]} separated query and \mono{(file\_path + ' ' + code\_snippet)} to get relevance scores. Training triplets are also constructed differently where for a set of training queries $T_q$,
% \begin{itemize}
%     \item \textbf{Query:} A query $q \in T_q$, remains the same from \ref{sec:bertrr}
%     \item \textbf{Passage:} For $q$, we retrieve BM25 results $R_{bm25} = [f_1, f_2,..]$ but file-aggregated this time. Since we want to match query to code, we also need to decide which version of each file to focus on. Currently, we use the most recent version of the file.
%     \item \textbf{Label:} If $f$ was edited in $q$\footnotemark[4], we set label as 1 otherwise 0. We get $N_+$ positive files and $N_-$ negative files.
% \end{itemize}

% \paragraph{Splitting into subpassages} Unlike commit messages, which are a few hundred tokens long, code files are a few thousand tokens long. So we need to split them into subpassages (code snippets) to fit them in BERT's sequence length of 512. Here, subpassage length, count and stride are all hyperparameters. Unlike natural language passage splitting, we cannot split on whitespace, so we split in the tokenized version of the code. Even still we can have potentially 50 or 100 passages per code file, and it would be inefficient to compute scores for all. So which passages to focus on? Currently, we filter subpassages by the matching number of common lines between each subpassage and the \mono{diff} of that file from the same commit $c$\footnotemark[4]. The same labels as the original passage are assigned to all (query, subpassage) pairs.

\subsection{CodeReranker: Integrating Code File Relevance}
\label{sec:coderr}
Next, we develop a model that incorporates relevance indicators from the code files themselves. We finetune another \texttt{microsoft/codebert-base} model using the same loss as in Section \ref{sec:bertrr}, embedding the query and passage separated by the \texttt{[SEP]} token to obtain relevance scores. Training is also supervised contrastive with triplets constructed as follows for a set of training queries $Q_{train}$: % We found that using file paths was not useful since they change frequently over time. 
\begin{itemize}
\item \textbf{Query:} A query $q_{train} \in Q_{train}$, remains the same as in Section \ref{sec:bertrr}.
\item \textbf{Passages (Code Patches):} For $q_{train}$, we retrieve file-aggregated BM25 results $R_{bm25} = [f_1, f_2,..]$, yielding a list of candidate source code files for the query. Since CodeBERT has a sequence length of 512 tokens and files in \texttt{facebook\_react} have an average of 27,000 tokens (~75 patches for a 350-token patch length), we need to split the code files into smaller code patches. We experiment with three splitting strategies:
\begin{enumerate}
\item Random splitting of tokens until a patch length of 350 is reached
\item Function parsing, converting source code into a syntax tree with functions and classes, and considering each function as a code patch
\item Line-wise splitting, using a sliding window over lines and starting the next window once 350 tokens are reached in the current window
\end{enumerate}
We found that line-wise splitting (c) performed best, followed by function parsing (b) and random splitting (a). Function parsing incurs additional overhead and does not guarantee that the resulting code patches fit within 350 tokens.
\item \textbf{Label:} For each query $q_{train}$ with a list of ground truth files $F_{truth}$, we have granular diffs $d_f$ for each file $f \in F_{truth}$. Similarly, for each retrieved file, we have diffs from the candidate commit obtained through previous ranker's \texttt{maxp} aggregation. We perform a line-by-line comparison of the diffs, considering a code patch as positive (label = 1) if there is a non-zero intersection (after removing trivial lines like brackets and newlines), and negative (label = 0) otherwise.
\end{itemize}

Thus, for a given (query, file) pair, the CodeReranker splits the file into code patches and computes a relevance score for each (query, code patch) pair. The final score for the file is obtained by aggregating the individual code patch scores. We experimented with different aggregation methods and found that taking \texttt{maxp} aggregation across all code patches yielded the best results. This aligns with the intuition that only a few specific sections of a file might be relevant for addressing a particular query, rather than the entire file.

\subsection{Experimental Setup}
\label{sec:exp_setup}

\paragraph{Research Questions}
In our experiments, we aim to address the following research questions:
\begin{enumerate}
\item Is leveraging past commit messages a viable approach for repository-level code search? Does this method yield reasonable results?
\item How does the reranking depth influence the performance of the rerankers? We investigate the impact of varying reranking depths, specifically 100, 250, 500, and 1,000.
\item How do the CommitReranker and CodeReranker perform individually under optimal conditions (i.e., in an oracle setting)?
\item How does the complete pipeline, combining BM25, CommitReranker, and CodeReranker, perform on diverse sets of test queries across different repositories?
\end{enumerate}

\paragraph{General} We experiment with 7 different repositories (Table \ref{app_tab:overall_repo_stats}), where each has their own BM25 commit index, 500 GPT-M train queries and 100 GPT-M test queries. 

\paragraph{Cross-Repository Combined Training of Rerankers}
We discovered that training the CommitReranker and CodeReranker on combined data from multiple repositories (the 7 repositories used in our experiments and 3 additional ones) with a total of 5,000 training queries yielded better results compared to training on a single repository, even with a larger number of train commits. This finding suggests that the diversity of data across repositories is more beneficial for the rerankers' performance than a larger amount of training data from a single repository. The cross-repository training approach exposes the models to a wider variety of commit messages, code changes, and query-file relationships, enabling them to generalize better to unseen queries and repositories.


% We experiment with 7 repositories, with individual repository details shown in Appendix \ref{app:repo_stats}. A potential concern was the presence of `easy files', which are files edited so often across commits that it would be trivial for our system to rank them at the top, thus inflating metrics. We looked at the median edit frequencies and found that the most edited files are still only in $<2\%$ of commits on average. Thus a Zipfian edit distribution is not likely, and metrics inflation should not be a problem.

% \paragraph{Query Sets} We have two types of query set: 1. \textbf{Commit}: Commit messages directly from the training data (similar to Table \ref{tab:df_format}), generally being around 200 tokens long, and 2. \textbf{Short:} Modified query set, by passing the commit messages to GPT-4 and asking it to generate the most likely problem in the style of a Github issue for the given commit in 1-2 sentences, while masking the solution presented in the commit message. The full prompt along with example transformations for sample queries is available in Appendix \ref{app:gpt4_prompt}. This is generally $30-40$ tokens long and is done to mimic real-world settings where developers generally describe an issue in a few sentences without knowing too many details about the problem and its solution. We also experimented with GPT-3.5 Turbo, however, we found that there was significant leakage, that is, the modified query would contain the solution, thereby defeating the purpose of this masking.

% Thus, we have 2 query sets Commit and Short, for both training and testing. In each of the 10 repositories, for the commit train set, we have 1000 random training queries from the history, while for (GPT-modified) short train set, we have 500 curated queries (filtering outlier commits with too many files edited or too big commit messages). For testing, there are also 100 curated test queries, which are the same for both commit and test sets with the only difference being if the query is the vanilla commit message or transformed. Important statistics for the test set are available in Table \ref{tab:test_set_stats}.

% \begin{table*}[htbp]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c|l|c|c|c|c|c|c|c|c|c|}
% \hline
% \multicolumn{1}{|c|}{\textbf{}} & \multicolumn{1}{|c|}{\textbf{Model}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Train Query} \\ \textbf{Set}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Test Query} \\ \textbf{Set}\end{tabular}} & \multicolumn{1}{c|}{\textbf{MAP}} & \multicolumn{1}{c|}{\textbf{P@10}} & \multicolumn{1}{c|}{\textbf{P@100}} & \multicolumn{1}{c|}{\textbf{P@1000}} & \multicolumn{1}{c|}{\textbf{MRR}} & \multicolumn{1}{c|}{\textbf{R@100}} & \multicolumn{1}{c|}{\textbf{R@1000}} \\ \hline
% (a) & \textbf{BM25}                        & -                                             & Commit                                       & 0.196                             & 0.073                              & 0.016                               & 0.002                                & 0.258                             & 0.533                                    & 0.687                                     \\ \hline
% (b) & \textbf{BERT Rerank @ 250}           & Commit                                        & Commit                                       & 0.278                             & 0.099                              & 0.018                               & 0.002                                & 0.358                             & 0.596                                    & 0.687                                     \\ \hline
% (c) & \textbf{BERT Rerank @ 250}           & Short                                         & Commit                                       & 0.213                             & 0.085                              & 0.017                               & 0.002                                & 0.280                             & 0.581                                    & 0.687                                     \\ \hline
% (d) & \textbf{Combined BERT @ 250}         & Commit                                        & Commit                                       & 0.308                             & 0.105                              & 0.018                               & 0.002                                & 0.380                             & 0.603                                    & 0.687                                     \\ \hline
% (e) & \textbf{Combined BERT @ 250}         & Short                                         & Commit                                       & 0.240                             & 0.088                              & 0.017                               & 0.002                                & 0.312                             & 0.587                                    & 0.687                                     \\ \hline
% (f) & \begin{tabular}[c]{@{}c@{}}\textbf{Combined BERT @ 250 +} \\ \textbf{CodeReranker @ 100}\end{tabular}        & Short                                        & Commit                                       & 0.149                             & 0.061                              & 0.018                               & 0.002                                & 0.203                             & 0.603                                    & 0.687                                     \\ \hline
% (g) & \textbf{BM25}                        & -                                             & Short                                        & 0.196                             & 0.070                              & 0.015                               & 0.002                                & 0.263                             & 0.521                                    & 0.689                                     \\ \hline
% (h) & \textbf{BERT Rerank @ 250}           & Commit                                        & Short                                        & 0.210                             & 0.078                              & 0.016                               & 0.002                                & 0.274                             & 0.560                                    & 0.689                                     \\ \hline
% (i) & \textbf{BERT Rerank @ 250}           & Short                                         & Short                                        & 0.230                             & 0.086                              & 0.017                               & 0.002                                & 0.295                             & 0.582                                    & 0.689                                     \\ \hline
% (j) & \textbf{Combined BERT @ 250}       & Commit                                        & Short                                        & 0.281                             & 0.099                              & 0.018                               & 0.002                                & 0.352                             & 0.590                                    & 0.689                                     \\ \hline
% (k) & \textbf{Combined BERT @ 250}         & Short                                         & Short                                        & 0.281                             & 0.102                              & 0.017                               & 0.002                                & 0.354                             & 0.589                                    & 0.689                                     \\ \hline
% (l) & \begin{tabular}[c]{@{}c@{}}\textbf{Combined BERT @ 250 +} \\ \textbf{CodeReranker @ 100}\end{tabular} & Short & Short & 0.179 & 0.064 & 0.018 & 0.002 & 0.242 & 0.590 & 0.689 \\
% \hline
% \end{tabular}}
% \caption{Test Set Evaluation for Different Configurations}
% \label{tab:combined_results}
% \end{table*}






\paragraph{BM25 (Baseline)} We use $k1=0.9,b=0.4$ in Pyserini BM25 for initial ranking. For file aggregation, we use \mono{maxp} in all our experiments to maximize Recall@100. We retrieve at most $k=1000$ initial files for initial retrieval. \texttt{p50k\_base} model from tiktoken was the tokenizer for the commit message index.

% \paragraph{For both CommitReranker and CodeReranker} We train these models on Nvidia Quadro RTX6000 (25 GB VRAM) with a batch size 32, for 8 epochs, with a learning rate of $5*10^{-5}$, train depth of 1000 (number of BM25 results retrieved per train query), \mono{maxp} aggregation strategy in both. and $N_+ = N_- = 10$. We perform experiments with reranking depths of $100, 250, 500$ and $1000$. Following the triplet data curation for CommitReranker (Section \ref{sec:bertrr}), we have 38240 each of positive and negatives (query, commit message, label) triplets, while for CodeReranker (Section \ref{sec:coderr}, we have 11202 each of positive and negative (query, code patch, label) triplets.

\paragraph{Training Details for CommitReranker and CodeReranker} Both models are trained on an Nvidia Quadro RTX6000 GPU with 25 GB VRAM. We use a batch size of 32 and train the models for 8 epochs with a learning rate of $5*10^{-5}$. We employ the \texttt{maxp} aggregation strategy for both models. Following the triplet data curation process described in Section \ref{sec:bertrr} for the CommitReranker, we obtain 38,240 positive and 38,240 negative (query, commit message, label) triplets. For the CodeReranker, as outlined in Section \ref{sec:coderr}, we have 11,202 positive and 11,202 negative (query, code patch, label) triplets. Both use \texttt{codebert} tokenizer. For each train query, we retrieve BM25 results and take first 10 positive and first 10 negatives for triplet creation.

\paragraph{Oracle Setting} To assess the individual performance of both CommitReranker and CodeReranker under optimal conditions, we create an oracle setting. Instead of having BM25 as the initial retriever, we randomly distribute the correct files across various reranking depths.

\paragraph{Full Pipeline} This involves BM25 with initial retrieval depth of 1000, CommitReranker at rerank depth 1000 and finally CodeReranker at rerank depth 100.


% Each repository's reranker is trained with either 1000 (Commit) or 500 (Short) train queries depending on the train query set. We use a BERT model trained on code, such as \texttt{microsoft/codebert-base}. Also tokenized with the same (note BM25 uses tiktoken for commit messages, however since the components are independent, this is non-issue).

% We assign a label of 1 to the first $N_{+}$ %znon-zero scored passages (commit messages) from the sorted $R_{bm25}$, indicating that $q$ and $c$ are related, and a label of 0 to the next $N_{-}$ passages, indicating that $q$ and $c$'s commit message are unrelated.

% \paragraph{BERT Reranker} We set (commit) passage count to 5, with rerank depth of 250. For triplet label distribution, in commit train queries (1000 total) it was $(+,-)=(5602,10567)$, while in short train queries (500 total) it was $(+,-)=(2489,4948)$. Label imbalance is presumably because of missing relevant files in top $k$ due to BM25's still mediocre Recall@1000 (see Sec \ref{results}). It takes about 45 minutes on average to train.

% \paragraph{Combined BERT} Here we just merge all training data for 10 repositories into one model. So this will have $500*10=5000$ short train queries and $1000*10=10000$ commit train queries. It takes about 6 hours to train (on commit set).

% \paragraph{CodeReranker} We set subpassage count to 25 (from the code file), with each subpassage length as 350 and a stride of 250. Rerank depth is set to 100 (generally followed after BERT Reranker). For label distribution, we were only able to train on the short train query set and we had $(+,-)=(25367,123362)$. This is very large because we are breaking each file into subpassages and for each positive or negative code file, we add subpassage count (25 in our case) instances. It takes about 3 hours on average to train on the short query set (yet to explore training on the commit set). For experiments, we report results by reranking the top 100 results from the Combined BERT model trained on the commit set (since it had the highest Recall @ 100).




\section{Results \& Discussion}
\label{results}





This section studies results listed in next 3 pages. Table \ref{tab:combined_results} and Figure \ref{fig:res} contain averaged results across 7 repositories. Per repository results are available in Appendix \ref{app:per_repo_results}. Meanwhile, Table \ref{tab:fbr_all} and Figures \ref{fig:fbr_non_oracle}, and \ref{fig:fbr_oracle} look at just one particular repository (\texttt{facebook\_react}) in much more detail in both oracle and normal settings. An important number to keep in mind from Table \ref{tab:test_set_stats} is, that there are approximately 3 files edited per test query. Since the goal is to rank these as high as possible, the most important metrics are P@1 and MRR.

\begin{table}[htbp]
\centering
\ttfamily
\scriptsize
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Type} & \textbf{Depth} & \textbf{Total Time} & \textbf{Per Query Time} \\
\hline
BM25 & - & 7.5 mins & 4.5 s / query \\
\hline
\multirow{4}{*}{Commit Reranker} & 100 & 1.5 hrs & 1 min / query \\
 & 250 & 3.5 hrs & 2 mins / query \\
 & 500 & 5 hrs & 3 mins / query \\
 & 1000 & 5.5 hrs & 3.5 mins / query \\
\hline
\multirow{4}{*}{Code Reranker} & 100 & 1.5 hrs & 1 min / query \\
 & 250 & 3.5 hrs & 2 mins / query \\
 & 500 & 6 hrs & 3.2 mins / query \\
 & 1000 & 7.5 hrs & 4.5 mins / query \\
\hline
Full & - & 7.5 hrs & 4.5 mins / query \\
\hline
\end{tabular}
\caption{Summary of total and per query time for different methods on \texttt{facebook\_react} for 100 test queries}
\label{tab:query_times}
\end{table}

\paragraph{BM25} demonstrates reasonably good performance while being quite efficient, with Recall being the most important metric since it serves as the initial ranker. The Recall@1000 is nearly perfect, around 0.92, indicating that almost all relevant files are retrieved within the top 1,000 results. Surprisingly, we also observe a decent Mean Reciprocal Rank (MRR) of 0.22 and Precision@1 (P@1) of 0.139, suggesting that by simply performing exact matching against past commit messages, we can retrieve some useful files and potentially resolve user queries. We further investigated BM25 results at depths of 2,000, 5,000, and 10,000; however, the recall tapers off, implying that some files are either very challenging to retrieve (due to being committed with an unrelated message) or suffer from the FID issue mentioned in Section \ref{sec:bm25}.

% Perhaps a larger value of $k1$ (term frequency scaling) might be useful to not saturate scores too quickly in case a lot of the commit messages have many overlapping terms due to conventions (here scores will be similar and thus rankings will be sub-optimal). Slightly higher Recall@1000 for Short queries \textit{(g)}, where this might be less likely since due to being modified by an LLM, probably verifies this. It is more difficult to judge the effect of $b$ (document length normalization).  Given that we want to retrieve files, it is hard to see how that will be related to the length of the commit messages. Thus, tuning $b$ is probably best left to trial and error. Finally, the massive drop in MAP in short test queries \textit{(a $\rightarrow$ g)} is probably explained due to the exact-token match of BM25, with LLMs replacing many relevant tokens from commit messages and introducing stochasticity.

\begin{table*}[htbp]
\centering
\ttfamily
\scriptsize
\resizebox{\textwidth}{!}{\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
 \textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\

\hline
\textbf{BM25} & \textbf{} & 0.163 & 0.227 & 0.139 & 0.090 & 0.071 & 0.048 & 0.061 & 0.265 & 0.336 & 0.607 & 0.846 & 0.924 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.252 & 0.361 & 0.250 & 0.143 & 0.103 & 0.067 & 0.114 & 0.390 & 0.476 & 0.607 & 0.846 & 0.924 \\
\textbf{} & \textbf{250} & 0.249 & 0.348 & 0.240 & 0.134 & 0.096 & 0.067 & 0.112 & 0.368 & 0.493 & 0.704 & 0.846 & 0.924 \\
\textbf{} & \textbf{500} & 0.236 & 0.331 & 0.220 & 0.123 & 0.091 & 0.064 & 0.103 & 0.359 & 0.473 & 0.702 & 0.846 & 0.924 \\
\textbf{} & \textbf{1000} & 0.229 & 0.317 & 0.210 & 0.118 & 0.087 & 0.061 & 0.099 & 0.352 & 0.454 & 0.709 & 0.886 & 0.924 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.288 & 0.408 & 0.289 & 0.163 & 0.112 & 0.072 & 0.136 & 0.421 & 0.513 & 0.607 & 0.846 & 0.924 \\
\textbf{} & \textbf{250} & 0.275 & 0.389 & 0.259 & 0.156 & 0.115 & 0.076 & 0.121 & 0.438 & 0.539 & 0.723 & 0.846 & 0.924 \\
\textbf{} & \textbf{500} & 0.258 & 0.365 & 0.233 & 0.142 & 0.105 & 0.074 & 0.111 & 0.399 & 0.524 & 0.752 & 0.846 & 0.924 \\
\textbf{} & \textbf{1000} & 0.246 & 0.352 & 0.224 & 0.136 & 0.096 & 0.069 & 0.105 & 0.373 & 0.503 & 0.753 & 0.895 & 0.924 \\
\cline{1-14}
\textbf{Full Pipeline} & \textbf{} & 0.307 & 0.434 & 0.299 & 0.175 & 0.123 & 0.079 & 0.139 & 0.470 & 0.572 & 0.709 & 0.886 & 0.924 \\
\cline{1-14}
\hline
\end{tabular}
}
\caption{Various test-set metrics at varying depths for BM25 Baseline, CommitReranker (red), CodeReranker (blue), and Full Pipeline, averaged across 7 repositories}
\label{tab:combined_results}
\end{table*}

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=\linewidth]{res.png}
    \caption{Visualization of Table \ref{tab:combined_results}. Bold lines are averages across 7 repositories, light background lines are individual performance of each constituent repository. Performance does not improve with increasing reranking depths. CodeReranker@100 performs almost to Full Pipeline, probably because of minor R@100 increase with intermediate CommitReranker@1000.}
    \label{fig:res}
\end{figure*}

\begin{table*}
    \centering
    \ttfamily
    \scriptsize
    \subsection*{\ttfamily{[a] Repository: facebook\_react Type: Non-Oracle}}
    \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
    \hline
    \textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\
    \hline
    \textbf{BM25} & \textbf{} & 0.119 & 0.180 & 0.080 & 0.094 & 0.077 & 0.056 & 0.022 & 0.189 & 0.260 & 0.557 & 0.797 & 0.951 \\
    \cline{1-14}
    \multirow[t]{9}{*}{\textbf{CommitReranker}} & \textbf{10} & 0.159 & 0.267 & 0.19 & 0.12 & 0.077 & 0.056 & 0.057 & 0.189 & 0.26 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{20} & 0.163 & 0.254 & 0.16 & 0.124 & 0.087 & 0.056 & 0.051 & 0.229 & 0.26 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{30} & 0.168 & 0.285 & 0.2 & 0.134 & 0.09 & 0.059 & 0.06 & 0.218 & 0.281 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{50} & 0.181 & 0.312 & 0.21 & 0.146 & 0.101 & 0.067 & 0.071 & 0.251 & 0.314 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{75} & 0.176 & 0.317 & 0.2 & 0.15 & 0.105 & 0.072 & 0.055 & 0.258 & 0.331 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{100} & 0.187 & 0.330 & 0.200 & 0.168 & 0.114 & 0.076 & 0.053 & 0.280 & 0.373 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{250} & 0.186 & 0.328 & 0.220 & 0.140 & 0.105 & 0.070 & 0.069 & 0.276 & 0.368 & 0.565 & 0.797 & 0.951 \\
    \textbf{} & \textbf{500} & 0.184 & 0.315 & 0.200 & 0.126 & 0.101 & 0.068 & 0.066 & 0.279 & 0.356 & 0.559 & 0.797 & 0.951 \\
    \textbf{} & \textbf{1000} & 0.185 & 0.306 & 0.180 & 0.130 & 0.100 & 0.068 & 0.061 & 0.293 & 0.369 & 0.590 & 0.874 & 0.951 \\
    
    \cline{1-14}
    \multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{10} & 0.187 & 0.304 & 0.24 & 0.128 & 0.077 & 0.056 & 0.083 & 0.189 & 0.26 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{20} & 0.204 & 0.317 & 0.25 & 0.148 & 0.096 & 0.056 & 0.086 & 0.243 & 0.26 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{30} & 0.199 & 0.314 & 0.22 & 0.15 & 0.105 & 0.064 & 0.073 & 0.264 & 0.299 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{50} & 0.208 & 0.355 & 0.26 & 0.158 & 0.115 & 0.075 & 0.081 & 0.301 & 0.355 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{75} & 0.23 & 0.401 & 0.28 & 0.178 & 0.126 & 0.084 & 0.081 & 0.323 & 0.392 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{100} & 0.267 & 0.420 & 0.300 & 0.202 & 0.144 & 0.094 & 0.097 & 0.382 & 0.465 & 0.557 & 0.797 & 0.951 \\
    \textbf{} & \textbf{250} & 0.240 & 0.355 & 0.200 & 0.180 & 0.141 & 0.096 & 0.076 & 0.374 & 0.477 & 0.630 & 0.797 & 0.951 \\
    \textbf{} & \textbf{500} & 0.236 & 0.351 & 0.210 & 0.156 & 0.127 & 0.098 & 0.081 & 0.352 & 0.514 & 0.683 & 0.797 & 0.951 \\
    \textbf{} & \textbf{1000} & 0.225 & 0.334 & 0.200 & 0.146 & 0.117 & 0.089 & 0.076 & 0.332 & 0.470 & 0.740 & 0.901 & 0.951 \\
    \cline{1-14}
    \textbf{Full Pipeline} & \textbf{} & 0.270 & 0.434 & 0.280 & 0.202 & 0.147 & 0.094 & 0.109 & 0.393 & 0.480 & 0.590 & 0.874 & 0.951 \\
    \cline{1-14}
    \hline
    \end{tabular}

    
    \subsection*{\ttfamily{[b] Repository: facebook\_react Type: Oracle}}
    \subsubsection*{\scriptsize \ttfamily{Pre-Reranking Metrics (meaning the rankings to be reranked with with Recall@Depth = 1 )}}
    \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
    \hline
    \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{P@30} \\
    \hline
    % \textbf{BM25} & \textbf{} & 0.119 & 0.180 & 0.080 & 0.094 & 0.077 & 0.056 & 0.022 & 0.189 & 0.260 & 0.557 & 0.797 & 0.951 \\
    \cline{1-8}
    \multirow[t]{4}{*} {\textbf{100}} & 0.009 & 0.134 & 0.04 & 0.04 & 0.05 & 0.05 & 0.05 \\
    \textbf{250} & 0.04 & 0.082 & 0.02 & 0.022 & 0.018 & 0.018 & 0.017 \\
    \textbf{500} & 0.024 & 0.05 & 0.02 & 0.01 & 0.011 & 0.008 & 0.009 \\
    \textbf{1000} & 0.012 & 0.023 & 0.0 & 0.004 & 0.004 & 0.006 & 0.006 \\
    \cline{1-8}
    % \multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.267 & 0.420 & 0.300 & 0.202 & 0.144 & 0.094 & 0.097 \\
    % \textbf{} & \textbf{250} & 0.240 & 0.355 & 0.200 & 0.180 & 0.141 & 0.096 & 0.076 \\
    % \textbf{} & \textbf{500} & 0.236 & 0.351 & 0.210 & 0.156 & 0.127 & 0.098 & 0.081 \\
    % \textbf{} & \textbf{1000} & 0.225 & 0.334 & 0.200 & 0.146 & 0.117 & 0.089 & 0.076 \\
    % \cline{1-9}
    % \textbf{Full Pipeline} & \textbf{} & 0.270 & 0.434 & 0.280 & 0.202 & 0.147 & 0.094 & 0.109 & 0.393 & 0.480 & 0.590 & 0.874 & 0.951 \\
    \cline{1-8}
    \hline
    \end{tabular}

    \subsubsection*{\scriptsize \ttfamily{Post-Reranking Metrics}}
    \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
    \hline
    \textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{P@30} \\
    \hline
    % \textbf{BM25} & \textbf{} & 0.119 & 0.180 & 0.080 & 0.094 & 0.077 & 0.056 & 0.022 & 0.189 & 0.260 & 0.557 & 0.797 & 0.951 \\
    \cline{1-9}
    \multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.299 & 0.427 & 0.28 & 0.198 & 0.156 & 0.109 & 0.096 \\
    \textbf{} & \textbf{250} & 0.23 & 0.353 & 0.22 & 0.156 & 0.123 & 0.085 & 0.077 \\
    \textbf{} & \textbf{500} & 0.196 & 0.316 & 0.19 & 0.136 & 0.107 & 0.073 & 0.066 \\
    \textbf{} & \textbf{1000} & 0.18 & 0.293 & 0.16 & 0.13 & 0.099 & 0.067 & 0.057 \\
    \cline{1-9}
    \multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.456 & 0.61 & 0.45 & 0.312 & 0.229 & 0.151 & 0.114 \\
    \textbf{} & \textbf{250} & 0.329 & 0.456 & 0.28 & 0.234 & 0.183 & 0.125 & 0.096 \\
    \textbf{} & \textbf{500} & 0.263 & 0.387 & 0.23 & 0.166 & 0.138 & 0.109 & 0.083 \\
    \textbf{} & \textbf{1000} & 0.226 & 0.334 & 0.2 & 0.146 & 0.117 & 0.089 & 0.072 \\
    \cline{1-9}
    % \textbf{Full Pipeline} & \textbf{} & 0.270 & 0.434 & 0.280 & 0.202 & 0.147 & 0.094 & 0.109 & 0.393 & 0.480 & 0.590 & 0.874 & 0.951 \\
    \cline{1-9}
    \hline
    \end{tabular}
    \caption{Detailed Results on Facebook React - \textbf{(a) Non-Oracle Results} Normal Rankings at different Rerank Depths Across Different Configurations, \textbf{(b) Oracle Results} Pre-Ranking Metrics (meaning lists that were passed to both CommitReranker and CodeReranker) and Post-Ranking Metrics to have a fair comparison}
    \label{tab:fbr_all}
    \end{table*}

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=\linewidth]{fbr_non_oracle.png}
    \caption{Visualization of Table \ref{tab:fbr_all}a. CodeReranker (blue) is significantly better compared to CommitReranker(red) in all settings, however the Full Pipeline is still surprisingly better.}
    \label{fig:fbr_non_oracle}
\end{figure*}

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=\linewidth]{fbr_oracle.png}
    \caption{Visualization of Table \ref{tab:fbr_all}b. Notice how much the grey curve is lifted by both red and blue lines. CodeReranker (blue) improves the same pre-ranking (grey) significantly more than CommitReranker (red).}
    \label{fig:fbr_oracle}
\end{figure*}

\paragraph{CommitReranker} provides a substantial improvement in all metrics, particularly MRR and P@1, suggesting that there was significant vocabulary mismatch occurring in BM25 matching, which the CommitReranker helps to mitigate. However, as the reranking depth increases, key metrics decline, indicating the presence of noisy files further down the ranking lists. Table \ref{tab:fbr_all} reveals that the performance increases up to a depth of 100 and then starts to decrease, implying that 100 is an optimal reranking depth. We also observe a notable increase in R@100 when the reranking depth is 1,000, suggesting that the CommitReranker could serve as a viable intermediate reranker. However, as noted in Table \ref{tab:query_times}, the CommitReranker is quite inefficient, likely because it considers all commits retrieved by BM25 for a particular file as potential candidates, resulting in a time complexity of $O(\text{depth} \times \text{num\_commits})$ per query.

% \paragraph{Combined BERT} This is the most significant result with about 40\% improvements in MAP and MRR scores, compared to BM25. MRR is close to $1/3$ implying we are putting one relevant file in the third position already on average. This is also perhaps expected since this has a lot more training data (10x) compared to individual rerankers for each repository. More surprising however is that this approach works at all. We hypothesized that due to vastly different training data, BERT would not be able to understand the nuances of each repository. But clearly, it can benefit from the diverse source of data and can discriminate even better when focusing on individual repositories (similar idea to why pre-training is so successful).

\paragraph{CodeReranker} demonstrates significantly better performance compared to the CommitReranker, matching the full pipeline's performance at a reranking depth of 100. This suggests that the intermediate CommitReranker may not be as useful. Figure \ref{fig:fbr_non_oracle} also shows a peak at a reranking depth of 100, indicating a bias in the training data creation process. However, the peak is much higher for the CodeReranker compared to the CommitReranker across all metrics, even when trained with less data. This implies that the line-diff-matching data curation approach was quite effective and that scoring with \texttt{maxp} across all passages aligns with intuitive understanding. As the reranking depth increases, the performance of the CodeReranker plateaus. Table \ref{tab:query_times} reveals that the CodeReranker is also quite slow, with a time complexity of $O(\text{rerank\_depth} \times \text{num\_tokens\_per\_file})$ per query, as it needs to process all tokens in each file up to the specified reranking depth.

% \paragraph{Peaks at Depth=100} From Figure \ref{fig:fbr_all}, we notice increasing metrics both CommitReranker and CodeReranker till depth = 100 and then drop in metrics. This behavior could be attributed to the training data curation (Sections \ref{sec:bertrr} and \ref{sec:coderr}), where we consider at most 10 positive and 10 negative examples. Perhaps finding 10 of each might require exploring around 100 documents in retrieved BM25 documents, potentially biasing the model to only seeing the top 100 documents and neglect those further down the list. Thus the model is not used to documents (both code patches and commit messages) from after depth 100. A possible solution is to include random easy negatives from lower ranks instead of solely relying on hard negatives ordered by BM25 retrieval.

% \paragraph{Full Pipeline} performs well but not much better than CodeReranker @ 100 while taking 7 times more time. This probably means a lot of useless computation is happening and we are looking at documents (commits and code patches) which don't really matter much. Perhaps better techniques to quickly filter out useless documents might be fruitful. 

% \paragraph{Overall Non-Oracle Results} We do manage to get surprisingly good MRR (0.43) and P@1 (0.3) across a diverse set of queries. This shows that using commit messages and cross training of rerankers on other repositories' commit history is indeed a fruitful approach. Perhaps future work could look at better data curation.

% \paragraph{Oracle Results} We really notice big deltas in the pre and post rankings for both Commit and CodeRerankers in Figure \ref{fig:fbr_oracle}, but particularly for CodeReranker @ 100. This validates the effecitveness of our approach and implies that in the current setting, if we are able to improve Recall @ 100 high enough, we can get near perfect MRR and P@1. Perhaps better intermediate rerankers or better data curation is the way forward.

% \paragraph{Why CommitReranker Underperforms} We strongly suspect the heuristic for calculating the label for a given train query and retrieved commit message mentioned in Section \ref{sec:bertrr} is the culprit. However, surprisingly, training on the same data used for CodeReranker (diff-line-matching), does not result in any better performance. Perhaps, this implies that commit messages might still be too noisy for getting more semantic relevance signal from commit messages and perhaps doubling down on source code is the best way forward.

\paragraph{Peaks at Depth=100}
Figure \ref{fig:fbr_non_oracle} reveals that both the CommitReranker and CodeReranker exhibit increasing performance metrics up to a depth of 100, followed by a drop in metrics. This behavior could be attributed to the training data curation process (Sections \ref{sec:bertrr} and \ref{sec:coderr}), where we consider at most 10 positive and 10 negative examples. Finding these examples might require exploring around 100 documents in the retrieved BM25 results, potentially biasing the models to focus on the top 100 documents and neglect those further down the list. Consequently, the models may not be well-equipped to handle documents (both code patches and commit messages) beyond a depth of 100. To address this issue, we suggest including random easy negatives from lower ranks instead of solely relying on hard negatives ordered by BM25 retrieval.
\paragraph{Full Pipeline Performance}
The full pipeline performs well but does not significantly outperform the CodeReranker at a depth of 100, despite taking 7 times longer to execute. This suggests that a considerable amount of computation may be unnecessary, and the pipeline is examining documents (commits and code patches) that do not substantially contribute to the final results. Developing better techniques to quickly filter out irrelevant documents could be a promising direction for future research.
\paragraph{Overall Non-Oracle Results}
Despite the challenges, our approach achieves a surprisingly good MRR of 0.43 and P@1 of 0.3 across a diverse set of queries implying that for 30\% of the test queries, the first-ranked file is relevant, and on average, one out of every three top-ranked files is relevant. R@10 is also half meaning 1 to 2 out of the three files are easily retrieved. These results demonstrate that leveraging commit messages and cross-training rerankers on the commit history of other repositories is indeed a fruitful approach. However, the remaining files remain a harder problem being much further down the rankings (100-500). % Future work could explore better intermediate rerankers and data curation techniques to further improve performance.

\paragraph{Oracle Results}
Figure \ref{fig:fbr_oracle} highlights substantial improvements from the pre to post-reranking metrics for both the CommitReranker and CodeReranker, particularly for the CodeReranker at a depth of 100. These results validate the effectiveness of our approach and suggest that if we can sufficiently improve Recall@100 in the current setting, we can achieve near-perfect MRR and P@1. 

\paragraph{CommitReranker Underperformance}
We strongly suspect that the heuristic used for calculating the label for a given training query and retrieved commit message, as described in Section \ref{sec:bertrr}, is the primary reason for the CommitReranker's underperformance. Surprisingly, training on the same data used for the CodeReranker (diff-line-matching) does not yield better performance. This may imply that commit messages are still too noisy to extract more semantic relevance signals, and focusing on source code could be the most promising way forward.




% However, the counterargument can also be made that it is unlikely how rankings from the previous reranker,  should affect the current reranker given the difference in Recall@100 between the two is quite small \textit{((d vs e) or (j vs k))}. Perhaps the code subpassages being fed to CodeReranker are not good relevance indicators and better selection strategies be explored. We rule out code snippets being cut out due to the max sequence length of BERT, since this is being trained on short queries, and from Appendix \ref{app:train_stats}, we have $35+17=52$ tokens for query and file path combined. This leaves 460 tokens for code, thus no truncation happens.

%Perhaps, just more code subpassages per code file are needed. On average we saw around 60-70 subpassages per code file, however, we only utilize 25 of them currently, meaning we are losing a lot of context from not having enough subpassages. Finally, it takes around 35s/query, so this is fairly inefficient - probably because of computing scores for multiple code subpassages.

% \paragraph{Comparison between Train Query Sets} We notice that Combined BERT trained on commits or short queries \textit{(j \& k)} perform pretty much the same on the short test set (there is a drastic difference on commit test set \textit{(d \& e)}, but short test set is a more realistic scenario). This is fascinating, as it implies that training on LLM-modified queries did not help generalizability significantly. Even just in individual training scenarios \textit{(h \& i)}, the difference is minor. This probably implies that even though the model somewhat overfits to commit data (\textit{d} being an outlier), it still learns very general patterns from the commit messages. This can help us save the cost the generating the GPT modified queries while not sacrificing any performance by training on just more number of past commits.

%\paragraph{Potential Anomaly} We notice that MRR for our results is generally quite high, around 0.25 in the worst case with BM25. This means, on average, a relevant file appears in the 4th position in the ranking. However, that should also mean we should see a P@10 of about 0.1 on average, but currently, it is lower than that (about 0.07). 

% This is possible in scenarios where 1. for some queries, relevant documents are placed very highly (maybe first or second rank), thus inflating the overall MRR, while 2. for other queries, there are no relevant documents in the top 10 at all (maybe in the top 20), leading to 0 P@10 scores but non-0 MRR scores. Such scenarios could inflate the MRR scores (since MRR=1 for one test query can compensate for MRR=0.1 for 10 other queries), while P@10 stays low consistently. Overall, this highlights that the rankings are quite inconsistent, where for some queries our system performs extremely well, while in others it performs quite poorly. 

% The only exceptions are Combined BERT models \textit{(d \& k) }which have MRR $\ge  1/3$ and P@10 = 0.1, implying both metrics agree. This means that the rankings are quite consistent and the system performs well for all test queries on average. This further highlights the extreme generalizability of the combined training approach.






\section{Conclusion}
% In summary, our project combines traditional information retrieval methods with neural code models to develop a multi-stage reranking pipeline for the task of repository-level code search. This involves understanding the rationale behind previous changes to files from past commit history. We used BM25 and (Code)BERT reranking, utilizing a variety of relevance indicators such as commit messages, file paths and code contents, to fetch potential files to be edited in response to a user query (such as a bug or issue). Our tests, which included a diverse set of GitHub repositories and modified queries to represent real GitHub issues, showed that our approach is quite effective. We observe that models trained on combined data across multiple repositories perform best in this task, leading to a 40\% improvement in metrics over the BM25 baseline. In the future, we hope this work helps to develop better LLM tools for understanding and maintaining complex, multi-file projects.

% \section{Future Work}
% There are multiple directions to improve this work, some of which are listed below.
% \begin{itemize}
%     \item The most promising approach is Combined BERT, which seems very generalizable. So scaling this system up with more train queries and more repositories to get an even more diverse source of data is promising.
%     \item The underwhelming utilization of code as a relevance indicator is apparent in the poor performance of CodeReranker. Better techniques could be explored to utilize code files efficiently. Perhaps BERT Rerank is a poor choice for code files due to their large sizes, and thus dense embeddings/reranker for code files is a better approach.
%     \item Multi-relevance training to improve the performance of CodeReranker - instead of just assigning binary scores to each code subpassage based on whether it is part of a diff or not, we instead assign scores by the number of common lines with the diff. This might give better relevance indicators to train a better model, however, efficiency will still be an issue.
%     \item A better sweep of hyperparameters such as BM25 parameters, rerank depths, optimal passage count, length, and strides is always a good idea.
% \end{itemize}

In this work, we introduced a multi-stage reranking system for repository-level code search that leverages the rich context available in commit histories to identify relevant files for a given query. By combining traditional IR techniques like BM25 with neural reranking using CodeBERT, our system learns to prioritize files based on the semantic similarity of commit messages and the relevance of code changes. Extensive experiments on a diverse set of real-world repositories demonstrate the effectiveness of our approach, with significant improvements over the BM25 baseline.

Key findings include the importance of cross-repository training for generalizability, the superior performance of the CodeReranker compared to the CommitReranker, and that leveraging commit histories is a viable approach. Despite challenges in computational efficiency and ranking consistency, our approach shows promise in enhancing the contextual understanding of large language models for complex coding tasks. 

Future research directions include scaling up the system with more diverse training data, exploring dense embeddings for code file retrieval, better labelling heuristics for CommitReranker and multi-relevance training strategies for improving CodeReranker. Developing better intermediate rerankers and increasing R@100 is the most pertinent problem, closely followed by improving computational efficiency (perhaps quickly filtering out irrelevant documents). Finally, just narrowing data to Github issues which are much less noisy than commits can also be a viable option. 

By providing a concise set of highly relevant files as enriched context, our system has the potential to significantly improve the quality and accuracy of code generation and bug fixing by large language models, ultimately streamlining software development and maintenance processes.

\clearpage
\bibliography{acl2023}
\bibliographystyle{acl_natbib}

\appendix
\section{Appendix}
\label{sec:appendix}
\subsection{Explicit definition of evaluation metrics}
Given a test query $q_t$ for some test commit $c_t$ and a list of actual modified files $F_t$, our system's retrieved list of files $F_r$, and a relevance list $R$ where $R[i]=1$ if file $F_r[i]$ is in $F_t$.

\begin{itemize}
    \item $P@k = \frac{1}{k}\sum_{i=1}^{k}R[i]$
    \item $MRR = 1/(j+1)$ where $j$ is the index of first relevant file in $F_p$ (if none, then 0)
    \item $Recall@K = \frac{1}{|F_a|}\sum_{i=1}^{k}R[i]$
    \item Average Precision $(AP) = \frac{\sum_{j \in \mathcal{X}}P@j}{\sum_{i=1}^{k}R[i]}$, $\mathcal{X}$ is list of indices $i$ where $R[i] = 1$ and macro-averaged across $|Q|$ test queries, $MAP = \frac{AP}{|Q|}$.
\end{itemize}

% \subsection{Overall Repository Statistics}
% \label{app:repo_stats}
% Table \ref{app_tab:overall_repo_stats} highlights repository-level statistics like files edited and edit frequency of files.



% \subsection{Train Query Set Statistics for CodeReranker}
% \label{app:train_stats}
% Table \ref{app_tab:repo_token_stats} highlights average tokens for each files, queries and remaining tokens for code. 

% \begin{table*}
% \centering
% \small
% \begin{tabular}{|l|c|c|c|}
% \hline
% \multicolumn{1}{|c|}{\textbf{Repository}}                                     & \textbf{\begin{tabular}[c]{@{}c@{}}Commit  \\ Message \\ Avg Tokens\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}File Path \\ Avg Tokens\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Remaining Tokens \\ For Code\end{tabular}} \\ \hline
% \mono{facebook\_react}                                                      & 242.1                                                                             & 20.9                                                                     & 249                                                                           \\ \hline
% \mono{angular\_angular}                                                     & 197.1                                                                              & 20.3                                                                     & 455                                                                           \\ \hline
% \mono{apache\_spark}                                                        & 819.9                                                                              & 30.3                                                                     & 442.1                                                                         \\ \hline
% \mono{apache\_kafka}                                                        & 254.9                                                                              & 28.1                                                                     & 446.8                                                                         \\ \hline
% \mono{django\_django}                                                       & 59.9                                                                              & 14                                                                       & 470.7                                                                         \\ \hline
% \mono{julialang\_julia}                                                     & 124.3                                                                              & 9                                                                        & 470.8                                                                         \\ \hline
% \mono{ruby\_ruby}                                                           & 156.1                                                                                & 6.6                                                                      & 473.4                                                                         \\ \hline
% \mono{pytorch\_pytorch}                                                     & 494.5                                                                              & 14.5                                                                     & 459.1                                                                         \\ \hline
% \mono{\begin{tabular}[c]{@{}l@{}}huggingface\_\\ transformers\end{tabular}} & 388.6                                                                              & 15.2                                                                     & 463                                                                           \\ \hline
% \mono{redis\_redis}                                                         & 236.1                                                                              & 8.1                                                                      & 466.4                                                                         \\ \hline
% \end{tabular}
% \caption{Overall Token Statistics for Training Set Per Repository (tokenized by \mono{microsoft/codebert-base})}
% \label{app_tab:repo_token_stats}
% \end{table*}


\subsection{Query Modification Prompt}
\label{app:gpt4_prompt}
The following prompt was provided to GPT4 to modify commit messages into short queries: 

% \begin{lstlisting}[breaklines=True, basicstyle=\footnotesize\ttfamily,columns=fullflexible]

{\small\ttfamily
\vspace{5pt}
You are a professional software developer who is given a very specific task. You will be given commit messages solving one or more problems from any big open-source Github repository, and your task is to identify those core problem(s) that this particular commit is trying to solve. With that information, you need to write a short description of problem itself in the style of a Github issue which would have existed *before* this change was committed. In essence you are trying to reconstruct what potential bugs led to certain commits happening.  Do not mention any information about the solution in the commit (as that would be cheating). Just what the potential issue or problem could have been that led to this commit being needed.

Do not mention the names or contact information of any people involved in the commits (like authors or reviewers). Do not start responding with any description or titles, just start the issue. Limit your response to at most 2 lines. Just think of yourself as a developer who encountered a bug in a hurry and has to write 2 lines which captures as much information about the problem as possible. And remember do NOT leak any details of the solution in the issue message.

For example, given the commit message below:
'[Fizz] Fix for failing id overwrites for postpone (\#27684)
When we postpone during a render we inject a new segment synchronously which we postpone. That gets assigned an ID so we can refer to it immediately in the postponed state.

When we do that, the parent segment may complete later even though it's also synchronous. If that ends up not having any content in it, it'll inline into the child and that will override the child's segment id which is not correct since it was already assigned one.

To fix this, we simply opt-out of the optimization in that case which is unfortunate because we'll generate many more unnecessary empty segments. So we should come up with a new strategy for segment id assignment but this fixes the bug.

Co-authored-by: Josh Story <story@hey.com>'

I want you to respond similar to: 'Synchronous render with postponed segments results in incorrect segment ID overrides, causing empty segments to be generated unnecessarily.'

\vspace{5pt}
}
% \end{lstlisting}

The modified queries for all repositories are available at \url{https://github.com/Siddharth-Gandhi/ds/tree/main/gold}. In total, it costs \$120 in OpenAI API credits to modify around 6000 queries ((500 train queries + 100 test queries) * 10 repositories). Table \ref{tab:add_transformed_query_examples} contains additional examples of transformed queries for one sample commit from various repositories.

\begin{table*}[h]
\centering
\begin{tabular}{@{}p{0.45\textwidth}p{0.45\textwidth}@{}}

\hline

\textbf{\small\ttfamily Original Commit Message} & \textbf{\small\ttfamily GPT-4 Transformed Short Query} 
\\ \hline

\tiny\ttfamily
[facebook\_react] Extract queueing logic into shared functions (\#22452) As a follow up to \#22445, this extracts the queueing logic that is shared between dispatchSetState and dispatchReducerAction into separate functions. It likely doesn't save any bytes since these will get inlined, anyway, but it does make the flow a bit easier to follow. &
\tiny\ttfamily
There is repeated queueing logic in 'dispatchSetState' and 'dispatchReducerAction', making the code difficult to follow.

\\
\tiny\ttfamily
[angular\_angular] fix(ivy): classes should not mess up matching for bound dir attributes (\#30002) Previously, we had a bug where directive matching could fail if the directive attribute was bound and followed a certain number of classes. This is because in the matching logic, we were treating classes like normal attributes. We should instead be skipping classes in the attribute matching logic. Otherwise classes will match for directives with attribute selectors, and as we are iterating through them in twos (when they are stored as name-only, not in name-value pairs), it may throw off directive matching for any bound attributes that come after. This commit changes the directive matching logic to skip classes altogether. PR Close \#30002 &
\tiny\ttfamily
Directive matching fails when the directive attribute is bound and follows a certain number of classes, as classes are mistakenly treated as normal attributes in the matching logic.

\\
\tiny\ttfamily
[apache\_kafka] KAFKA-13396: Allow create topic without partition/replicaFactor (\#11429) {[}KIP-464{]}(https://cwiki.apache.org/confluence/display/KAFKA/ KIP-464\%3A+Defaults+for+AdminClient\%23createTopic) (PR: https://github.com/apache/kafka/pull/6728) made it possible to create topics without passing partition count and/or replica factor when using the admin client. We incorrectly disallowed this via https://github.com/apache/kafka/pull/10457 while trying to ensure validation was consistent between ZK and the admin client (in this case the inconsistency was intentional). Fix this regression and add tests for the command lines in quick start (i.e. create topic and describe topic) to make sure it won't be broken in the future. Reviewers: Lee Dongjin \textless{}dongjin@apache.org\textgreater{}, Ismael Juma \textless{}ismael@juma.me.uk\textgreater{} & 
\tiny\ttfamily
Inconsistency between validation in ZK and the admin client after applying KIP-464, resulting in inability to create topics without passing partition count and/or replica factor.

\\
\tiny\ttfamily
[apache\_spark] [SPARK-29612][SQL] ALTER TABLE (RECOVER PARTITIONS) should look up catalog/table like v2 commands \#\#\# What changes were proposed in this pull request? Add AlterTableRecoverPartitionsStatement and make ALTER TABLE ... RECOVER PARTITIONS go through the same catalog/table resolution framework of v2 commands. \#\#\# Why are the changes needed? It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g. ``` USE my\_catalog DESC t // success and describe the table t from my\_catalog ALTER TABLE t RECOVER PARTITIONS // report table not found as there is no table t in the session catalog ``` \#\#\# Does this PR introduce any user-facing change? Yes. When running ALTER TABLE ... RECOVER PARTITIONS Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog. \#\#\# How was this patch tested? Unit tests. Closes \#26269 from huaxingao/spark-29612. Authored-by: Huaxin Gao <huaxing@us.ibm.com> Signed-off-by: Wenchen Fan <wenchen@databricks.com> & 
\tiny\ttfamily
The current implementation of the "ALTER TABLE ... RECOVER PARTITIONS" command in Spark leads to confusion for users when used with catalog/table resolution. When performed under a catalog (for instance, 'my\_catalog'), the table resolution behaviour is inconsistent. Successful DESCRIBE commands for a table in that catalog are followed by failure in the ALTER TABLE command because it does not find the table in the session catalog. Consequently, there is a need for a fix to ensure uniform table resolution behaviour in all commands, avoiding confusion for end users.

\\
\tiny\ttfamily
[django\_django] Fixed \#26940 -- Removed makemessages from no\_settings\_commands whitelist As makemessages uses several settings for proper run (FILE\_CHARSET, LOCALE\_PATHS, MEDIA\_ROOT, and STATIC\_ROOT), we should require settings configuration for this command. &
\tiny\ttfamily
'makemessages' command is running without required settings configuration (FILE\_CHARSET, LOCALE\_PATHS, MEDIA\_ROOT, STATIC\_ROOT), causing incorrect execution results.

\\
\tiny\ttfamily
[julialang\_julia] Fix type of allocated array when broadcasting type unstable function (\#37028) We need to call similar on the `Broadcasted` object rather than on dest array. Otherwise the `BroadcastStyle` isn't taken into account when allocating new array due to function returning elements of different types. &
\tiny\ttfamily
Broadcasted object does not take into account the BroadcastStyle when allocating new arrays, resulting in type mismatches returned by functions.

\\
\tiny\ttfamily
[redis\_redis] Fix active expire division by zero. Likely fix \#6723. This is what happens AFAIK: we enter the main loop where we expire stuff until a given percentage of keys is still found to be logically expired. There are however other potential exit conditions. However the "sampled" variable is not always incremented inside the loop, because we may found no valid slot as we scan the hash table, but just NULLs ad dict entries. So when the do/while loop condition is triggered at the end, we do (expired*100/sampled), dividing by zero if we sampled 0 keys. &
\tiny\ttfamily
Main loop for key expiry can potentially lead to division by zero error when no valid slots are found during a hash table scan, resulting in no keys being sampled.
\\ \hline
\end{tabular}
\caption{Additional examples of transformed queries for one sample commit from various repositories.}
\label{tab:add_transformed_query_examples}
\end{table*}



\begin{table*}
\centering
\ttfamily
\tiny
\subsection*{\ttfamily{Repository: apache\_kafka}}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\
\hline
\textbf{BM25} & \textbf{} & 0.142 & 0.273 & 0.180 & 0.102 & 0.087 & 0.066 & 0.052 & 0.206 & 0.314 & 0.590 & 0.814 & 0.889 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.226 & 0.368 & 0.230 & 0.170 & 0.131 & 0.093 & 0.074 & 0.365 & 0.460 & 0.590 & 0.814 & 0.889 \\
\textbf{} & \textbf{250} & 0.212 & 0.326 & 0.200 & 0.146 & 0.117 & 0.092 & 0.066 & 0.305 & 0.457 & 0.638 & 0.814 & 0.889 \\
\textbf{} & \textbf{500} & 0.198 & 0.295 & 0.170 & 0.130 & 0.105 & 0.084 & 0.062 & 0.291 & 0.410 & 0.664 & 0.814 & 0.889 \\
\textbf{} & \textbf{1000} & 0.180 & 0.268 & 0.160 & 0.112 & 0.090 & 0.074 & 0.060 & 0.263 & 0.367 & 0.647 & 0.824 & 0.889 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.272 & 0.453 & 0.340 & 0.186 & 0.135 & 0.090 & 0.117 & 0.371 & 0.450 & 0.590 & 0.814 & 0.889 \\
\textbf{} & \textbf{250} & 0.247 & 0.452 & 0.340 & 0.184 & 0.140 & 0.097 & 0.092 & 0.368 & 0.459 & 0.672 & 0.814 & 0.889 \\
\textbf{} & \textbf{500} & 0.235 & 0.422 & 0.310 & 0.162 & 0.135 & 0.096 & 0.096 & 0.347 & 0.453 & 0.714 & 0.814 & 0.889 \\
\textbf{} & \textbf{1000} & 0.230 & 0.415 & 0.320 & 0.156 & 0.117 & 0.086 & 0.104 & 0.312 & 0.421 & 0.683 & 0.839 & 0.889 \\
\cline{1-14}
\textbf{Full Pipeline} & \textbf{} & 0.277 & 0.509 & 0.400 & 0.218 & 0.152 & 0.102 & 0.108 & 0.380 & 0.470 & 0.647 & 0.824 & 0.889 \\
\cline{1-14}
\hline
\end{tabular}

\subsection*{\ttfamily{Repository: julia\_julia}}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\
\hline
\textbf{BM25} & \textbf{} & 0.171 & 0.234 & 0.160 & 0.074 & 0.058 & 0.036 & 0.083 & 0.277 & 0.332 & 0.718 & 0.978 & 0.995 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.292 & 0.377 & 0.250 & 0.142 & 0.098 & 0.060 & 0.137 & 0.481 & 0.577 & 0.718 & 0.978 & 0.995 \\
\textbf{} & \textbf{250} & 0.269 & 0.355 & 0.230 & 0.132 & 0.087 & 0.058 & 0.124 & 0.453 & 0.577 & 0.817 & 0.978 & 0.995 \\
\textbf{} & \textbf{500} & 0.251 & 0.335 & 0.210 & 0.118 & 0.083 & 0.057 & 0.109 & 0.433 & 0.558 & 0.819 & 0.978 & 0.995 \\
\textbf{} & \textbf{1000} & 0.248 & 0.332 & 0.210 & 0.116 & 0.082 & 0.056 & 0.109 & 0.432 & 0.555 & 0.816 & 0.984 & 0.995 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.276 & 0.363 & 0.240 & 0.132 & 0.089 & 0.057 & 0.139 & 0.418 & 0.551 & 0.718 & 0.978 & 0.995 \\
\textbf{} & \textbf{250} & 0.238 & 0.319 & 0.210 & 0.112 & 0.085 & 0.056 & 0.123 & 0.412 & 0.523 & 0.812 & 0.978 & 0.995 \\
\textbf{} & \textbf{500} & 0.224 & 0.306 & 0.210 & 0.100 & 0.070 & 0.052 & 0.125 & 0.334 & 0.489 & 0.801 & 0.978 & 0.995 \\
\textbf{} & \textbf{1000} & 0.209 & 0.283 & 0.180 & 0.096 & 0.064 & 0.050 & 0.115 & 0.313 & 0.469 & 0.735 & 0.960 & 0.995 \\
\cline{1-14}
\textbf{Full Pipeline} & \textbf{} & 0.283 & 0.362 & 0.230 & 0.138 & 0.099 & 0.064 & 0.132 & 0.489 & 0.610 & 0.816 & 0.984 & 0.995 \\
\cline{1-14}
\hline
\end{tabular}

\subsection*{\ttfamily{Repository: django\_django}}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\
\hline
\textbf{BM25} & \textbf{} & 0.232 & 0.278 & 0.210 & 0.104 & 0.064 & 0.038 & 0.113 & 0.328 & 0.388 & 0.616 & 0.840 & 0.950 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.285 & 0.374 & 0.290 & 0.118 & 0.083 & 0.049 & 0.158 & 0.438 & 0.513 & 0.616 & 0.840 & 0.950 \\
\textbf{} & \textbf{250} & 0.272 & 0.343 & 0.260 & 0.120 & 0.073 & 0.050 & 0.143 & 0.376 & 0.528 & 0.763 & 0.840 & 0.950 \\
\textbf{} & \textbf{500} & 0.282 & 0.361 & 0.270 & 0.126 & 0.079 & 0.050 & 0.148 & 0.402 & 0.515 & 0.747 & 0.840 & 0.950 \\
\textbf{} & \textbf{1000} & 0.267 & 0.332 & 0.250 & 0.114 & 0.072 & 0.048 & 0.133 & 0.371 & 0.498 & 0.777 & 0.938 & 0.950 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.336 & 0.439 & 0.310 & 0.162 & 0.100 & 0.056 & 0.178 & 0.522 & 0.579 & 0.616 & 0.840 & 0.950 \\
\textbf{} & \textbf{250} & 0.336 & 0.432 & 0.290 & 0.152 & 0.102 & 0.062 & 0.169 & 0.546 & 0.649 & 0.762 & 0.840 & 0.950 \\
\textbf{} & \textbf{500} & 0.313 & 0.400 & 0.250 & 0.142 & 0.090 & 0.058 & 0.149 & 0.491 & 0.628 & 0.803 & 0.840 & 0.950 \\
\textbf{} & \textbf{1000} & 0.307 & 0.397 & 0.240 & 0.146 & 0.087 & 0.058 & 0.128 & 0.477 & 0.633 & 0.852 & 0.950 & 0.950 \\
\cline{1-14}
\textbf{Full Pipeline} & \textbf{} & 0.387 & 0.482 & 0.330 & 0.180 & 0.116 & 0.068 & 0.189 & 0.624 & 0.725 & 0.777 & 0.938 & 0.950 \\
\cline{1-14}
\hline
\end{tabular}

\subsection*{\ttfamily{Repository: redis\_redis}}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\
\hline
\textbf{BM25} & \textbf{} & 0.272 & 0.343 & 0.190 & 0.132 & 0.115 & 0.074 & 0.106 & 0.540 & 0.658 & 0.904 & 0.996 & 0.998 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.409 & 0.533 & 0.400 & 0.190 & 0.137 & 0.086 & 0.219 & 0.635 & 0.781 & 0.904 & 0.996 & 0.998 \\
\textbf{} & \textbf{250} & 0.397 & 0.514 & 0.380 & 0.190 & 0.128 & 0.082 & 0.211 & 0.602 & 0.749 & 0.935 & 0.996 & 0.998 \\
\textbf{} & \textbf{500} & 0.392 & 0.509 & 0.370 & 0.188 & 0.128 & 0.082 & 0.201 & 0.601 & 0.742 & 0.948 & 0.996 & 0.998 \\
\textbf{} & \textbf{1000} & 0.391 & 0.509 & 0.370 & 0.188 & 0.127 & 0.081 & 0.201 & 0.596 & 0.732 & 0.948 & 0.992 & 0.998 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.467 & 0.590 & 0.450 & 0.218 & 0.145 & 0.091 & 0.244 & 0.664 & 0.819 & 0.904 & 0.996 & 0.998 \\
\textbf{} & \textbf{250} & 0.435 & 0.540 & 0.400 & 0.200 & 0.138 & 0.082 & 0.230 & 0.632 & 0.751 & 0.930 & 0.996 & 0.998 \\
\textbf{} & \textbf{500} & 0.411 & 0.513 & 0.370 & 0.196 & 0.129 & 0.078 & 0.218 & 0.599 & 0.694 & 0.953 & 0.996 & 0.998 \\
\textbf{} & \textbf{1000} & 0.392 & 0.490 & 0.340 & 0.188 & 0.123 & 0.077 & 0.201 & 0.577 & 0.681 & 0.931 & 0.992 & 0.998 \\
\cline{1-14}
\textbf{Full Pipeline} & \textbf{} & 0.474 & 0.584 & 0.450 & 0.214 & 0.145 & 0.089 & 0.259 & 0.673 & 0.808 & 0.948 & 0.992 & 0.998 \\
\cline{1-14}
\hline
\end{tabular}

\subsection*{\ttfamily{Repository: apache\_spark}}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\
\hline
\textbf{BM25} & \textbf{} & 0.088 & 0.116 & 0.070 & 0.054 & 0.038 & 0.025 & 0.023 & 0.116 & 0.135 & 0.415 & 0.790 & 0.881 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.158 & 0.253 & 0.170 & 0.104 & 0.077 & 0.051 & 0.054 & 0.232 & 0.280 & 0.415 & 0.790 & 0.881 \\
\textbf{} & \textbf{250} & 0.200 & 0.284 & 0.190 & 0.104 & 0.079 & 0.057 & 0.093 & 0.268 & 0.374 & 0.610 & 0.790 & 0.881 \\
\textbf{} & \textbf{500} & 0.156 & 0.226 & 0.130 & 0.072 & 0.067 & 0.051 & 0.060 & 0.238 & 0.336 & 0.574 & 0.790 & 0.881 \\
\textbf{} & \textbf{1000} & 0.144 & 0.208 & 0.120 & 0.068 & 0.061 & 0.040 & 0.058 & 0.218 & 0.274 & 0.559 & 0.812 & 0.881 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.218 & 0.332 & 0.230 & 0.130 & 0.090 & 0.064 & 0.096 & 0.283 & 0.356 & 0.415 & 0.790 & 0.881 \\
\textbf{} & \textbf{250} & 0.254 & 0.379 & 0.250 & 0.158 & 0.112 & 0.074 & 0.102 & 0.406 & 0.481 & 0.662 & 0.790 & 0.881 \\
\textbf{} & \textbf{500} & 0.216 & 0.322 & 0.180 & 0.132 & 0.097 & 0.068 & 0.071 & 0.361 & 0.461 & 0.687 & 0.790 & 0.881 \\
\textbf{} & \textbf{1000} & 0.207 & 0.317 & 0.190 & 0.118 & 0.090 & 0.066 & 0.078 & 0.326 & 0.463 & 0.682 & 0.849 & 0.881 \\
\cline{1-14}
\textbf{Full Pipeline} & \textbf{} & 0.256 & 0.394 & 0.270 & 0.166 & 0.107 & 0.066 & 0.106 & 0.376 & 0.440 & 0.559 & 0.812 & 0.881 \\
\cline{1-14}
\hline
\end{tabular}

\subsection*{\ttfamily{Repository: angular\_angular}}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\
\hline
\textbf{BM25} & \textbf{} & 0.116 & 0.165 & 0.080 & 0.068 & 0.059 & 0.042 & 0.027 & 0.197 & 0.266 & 0.452 & 0.708 & 0.803 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.206 & 0.294 & 0.210 & 0.106 & 0.078 & 0.053 & 0.104 & 0.297 & 0.346 & 0.452 & 0.708 & 0.803 \\
\textbf{} & \textbf{250} & 0.206 & 0.286 & 0.200 & 0.108 & 0.080 & 0.060 & 0.082 & 0.297 & 0.399 & 0.596 & 0.708 & 0.803 \\
\textbf{} & \textbf{500} & 0.193 & 0.274 & 0.190 & 0.098 & 0.072 & 0.060 & 0.078 & 0.270 & 0.394 & 0.606 & 0.708 & 0.803 \\
\textbf{} & \textbf{1000} & 0.187 & 0.263 & 0.180 & 0.100 & 0.074 & 0.057 & 0.068 & 0.289 & 0.379 & 0.625 & 0.776 & 0.803 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.181 & 0.262 & 0.150 & 0.112 & 0.084 & 0.054 & 0.081 & 0.304 & 0.373 & 0.452 & 0.708 & 0.803 \\
\textbf{} & \textbf{250} & 0.174 & 0.250 & 0.120 & 0.104 & 0.089 & 0.063 & 0.051 & 0.330 & 0.430 & 0.590 & 0.708 & 0.803 \\
\textbf{} & \textbf{500} & 0.167 & 0.240 & 0.100 & 0.108 & 0.085 & 0.063 & 0.035 & 0.311 & 0.427 & 0.622 & 0.708 & 0.803 \\
\textbf{} & \textbf{1000} & 0.154 & 0.228 & 0.100 & 0.100 & 0.073 & 0.056 & 0.034 & 0.276 & 0.386 & 0.647 & 0.774 & 0.803 \\
\cline{1-14}
\textbf{Full Pipeline} & \textbf{} & 0.202 & 0.271 & 0.130 & 0.110 & 0.096 & 0.067 & 0.071 & 0.352 & 0.471 & 0.625 & 0.776 & 0.803 \\
\cline{1-14}
\hline
\end{tabular}

\subsection*{\ttfamily{Repository: facebook\_react}}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\textbf{Type} & \textbf{Depth} & \textbf{MAP} & \textbf{MRR} & \textbf{P@1} & \textbf{P@5} & \textbf{P@10} & \textbf{P@20} & \textbf{R@1} & \textbf{R@10} & \textbf{R@20} & \textbf{R@100} & \textbf{R@500} & \textbf{R@1000} \\
\hline
\textbf{BM25} & \textbf{} & 0.119 & 0.180 & 0.080 & 0.094 & 0.077 & 0.056 & 0.022 & 0.189 & 0.260 & 0.557 & 0.797 & 0.951 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CommitReranker}} & \textbf{100} & 0.187 & 0.330 & 0.200 & 0.168 & 0.114 & 0.076 & 0.053 & 0.280 & 0.373 & 0.557 & 0.797 & 0.951 \\
\textbf{} & \textbf{250} & 0.186 & 0.328 & 0.220 & 0.140 & 0.105 & 0.070 & 0.069 & 0.276 & 0.368 & 0.565 & 0.797 & 0.951 \\
\textbf{} & \textbf{500} & 0.184 & 0.315 & 0.200 & 0.126 & 0.101 & 0.068 & 0.066 & 0.279 & 0.356 & 0.559 & 0.797 & 0.951 \\
\textbf{} & \textbf{1000} & 0.185 & 0.306 & 0.180 & 0.130 & 0.100 & 0.068 & 0.061 & 0.293 & 0.369 & 0.590 & 0.874 & 0.951 \\
\cline{1-14}
\multirow[t]{4}{*}{\textbf{CodeReranker}} & \textbf{100} & 0.267 & 0.420 & 0.300 & 0.202 & 0.144 & 0.094 & 0.097 & 0.382 & 0.465 & 0.557 & 0.797 & 0.951 \\
\textbf{} & \textbf{250} & 0.240 & 0.355 & 0.200 & 0.180 & 0.141 & 0.096 & 0.076 & 0.374 & 0.477 & 0.630 & 0.797 & 0.951 \\
\textbf{} & \textbf{500} & 0.236 & 0.351 & 0.210 & 0.156 & 0.127 & 0.098 & 0.081 & 0.352 & 0.514 & 0.683 & 0.797 & 0.951 \\
\textbf{} & \textbf{1000} & 0.225 & 0.334 & 0.200 & 0.146 & 0.117 & 0.089 & 0.076 & 0.332 & 0.470 & 0.740 & 0.901 & 0.951 \\
\cline{1-14}
\textbf{Full Pipeline} & \textbf{} & 0.270 & 0.434 & 0.280 & 0.202 & 0.147 & 0.094 & 0.109 & 0.393 & 0.480 & 0.590 & 0.874 & 0.951 \\
\cline{1-14}
\hline
\end{tabular}
\caption{Results for various configuartions per repository}
\label{tab:per_repo_results}
\end{table*}



\subsection{Algorithm for getting FIDs}
\label{app:fid}
The algorithm is described at this link \url{https://github.com/Siddharth-Gandhi/ds/blob/boston/notebooks/git_map.ipynb}. The FID mappings for the various repositories tested are available at \url{https://github.com/Siddharth-Gandhi/ds/tree/boston/fids/v4}.


% \begin{figure*}[htpb]
%     \centering
%     \includegraphics[width=\linewidth]{fbr_fid_len_drop.png}
%     \caption{FBR FID Len Drop}
%     \label{fig:fbr_fid_len_drop}
% \end{figure*}

\subsection{Results per Repository}
\label{app:per_repo_results}
Table \ref{tab:per_repo_results} details all of the results for BM25, various rerank depths of CommitReranker and CodeReranker and Full Pipeline (BM25 $\rightarrow$ CommitReranker @ 1000 $\rightarrow$ CodeReranker @ 100) configurations.
\end{document}