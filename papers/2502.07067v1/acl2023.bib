

@inproceedings{sum_and_gen,
    title = "Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages",
    author = "Ahmad, Wasi Uddin  and
      Chakraborty, Saikat  and
      Ray, Baishakhi  and
      Chang, Kai-Wei",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.112",
    doi = "10.18653/v1/2023.eacl-main.112",
    pages = "1528--1542",
    abstract = "Back-translation is widely known for its effectiveness in neural machine translation when there is little to no parallel data. In this approach, a source-to-target model is coupled with a target-to-source model trained in parallel. The target-to-source model generates noisy sources, while the source-to-target model is trained to reconstruct the targets and vice versa. Recent developments of multilingual pre-trained sequence-to-sequence models for programming languages have been very effective for a broad spectrum of downstream software engineering tasks. Hence, training them to build programming language translation systems via back-translation is compelling. However, these models cannot be further trained via back-translation since they learn to output sequences in the same language as the inputs during pre-training. As an alternative, we propose performing back-translation via code summarization and generation. In code summarization, a model learns to generate natural language (NL) summaries given code snippets. In code generation, the model learns to do the opposite. Therefore, target-to-source generation in back-translation can be viewed as a target-to-NL-to-source generation. We show that our proposed approach performs competitively with state-of-the-art methods. We have made the code publicly available.",
}

@article{p2_to_p3,
  title={Using machine translation for converting Python 2 to Python 3 code},
  author={Karan Aggarwal and Mohammad Salameh and Abram Hindle},
  journal={PeerJ Prepr.},
  year={2015},
  volume={3},
  pages={e1459},
  url={https://api.semanticscholar.org/CorpusID:33930905}
}

@inproceedings{lexical,
author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Tien N.},
title = {Lexical Statistical Machine Translation for Language Migration},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2494584},
doi = {10.1145/2491411.2494584},
abstract = {Prior research has shown that source code also exhibits naturalness, i.e. it is written by humans and is likely to be repetitive. The researchers also showed that the n-gram language model is useful in predicting the next token in a source file given a large corpus of existing source code. In this paper, we investigate how well statistical machine translation (SMT) models for natural languages could help in migrating source code from one programming language to another. We treat source code as a sequence of lexical tokens and apply a phrase-based SMT model on the lexemes of those tokens. Our empirical evaluation on migrating two Java projects into C# showed that lexical, phrase-based SMT could achieve high lexical translation accuracy (BLEU from 81.3-82.6\%). Users would have to manually edit only 11.9-15.8\% of the total number of tokens in the resulting code to correct it. However, a high percentage of total translation methods (49.5-58.6\%) is syntactically incorrect. Therefore, our result calls for a more program-oriented SMT model that is capable of better integrating the syntactic and semantic information of a program to support language migration.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {651–654},
numpages = {4},
keywords = {Language Migration, Statistical Machine Translation},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{phrase,
author = {Karaivanov, Svetoslav and Raychev, Veselin and Vechev, Martin},
title = {Phrase-Based Statistical Translation of Programming Languages},
year = {2014},
isbn = {9781450332101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661136.2661148},
doi = {10.1145/2661136.2661148},
abstract = {Phrase-based statistical machine translation approaches have been highly successful in translating between natural languages and are heavily used by commercial systems (e.g. Google Translate).The main objective of this work is to investigate the applicability of these approaches for translating between programming languages. Towards that, we investigated several variants of the phrase-based translation approach: i) a direct application of the approach to programming languages, ii) a novel modification of the approach to incorporate the grammatical structure of the target programming language (so to avoid generating target programs which do not parse), and iii) a combination of ii) with custom rules added to improve the quality of the translation.To experiment with the above systems, we investigated machine translation from C# to Java. For the training, which takes about 60 hours, we used a parallel corpus of 20,499 C#-to-Java method translations. We then evaluated each of the three systems above by translating 1,000 C# methods. Our experimental results indicate that with the most advanced system, about 60\% of the translated methods compile (the top ranked) and out of a random sample of 50 correctly compiled methods, 68\% (34 methods) were semantically equivalent to the reference solution.},
booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \& Software},
pages = {173–184},
numpages = {12},
keywords = {statistical machine translation, programming language translation},
location = {Portland, Oregon, USA},
series = {Onward! 2014}
}

@inproceedings{tree-to-tree,
author = {Chen, Xinyun and Liu, Chang and Song, Dawn},
title = {Tree-to-Tree Neural Networks for Program Translation},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {2552–2562},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{unsupervised,
author = {Roziere, Baptiste and Lachaux, Marie-Anne and Chanussot, Lowik and Lample, Guillaume},
title = {Unsupervised Translation of Programming Languages},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is time-consuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1730},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@inproceedings{
DOBF,
title={{DOBF}: A Deobfuscation Pre-Training Objective for Programming Languages},
author={Marie-anne Lachaux and Baptiste Roziere and Marc Szafraniec and Guillaume Lample},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=3ez9BSHTNT}
}

@inproceedings{BERT,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}

@article{unit-tests,
  title={Leveraging Automated Unit Tests for Unsupervised Code Translation},
  author={Baptiste Rozi{\`e}re and J Zhang and François Charton and Mark Harman and Gabriel Synnaeve and Guillaume Lample},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.06773},
  url={https://api.semanticscholar.org/CorpusID:238744039}
}

@INPROCEEDINGS{graphcodebert_data_1,
  author={Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Tien N.},
  booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)}, 
  year={2015},
  volume={},
  number={},
  pages={585-596},
  doi={10.1109/ASE.2015.74}}

@inproceedings{BLEU,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: A Method for Automatic Evaluation of Machine Translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

@misc{codebleu,
      title={CodeBLEU: a Method for Automatic Evaluation of Code Synthesis}, 
      author={Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and Neel Sundaresan and Ming Zhou and Ambrosio Blanco and Shuai Ma},
      year={2020},
      eprint={2009.10297},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{BLIP2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}







@article{bm25,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{bertrr,
  title={Deeper text understanding for IR with contextual neural language modeling},
  author={Dai, Zhuyun and Callan, Jamie},
  booktitle={Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval},
  pages={985--988},
  year={2019}
}

@article{denserr,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}

@article{multistage,
  title={Multi-stage document ranking with BERT},
  author={Nogueira, Rodrigo and Yang, Wei and Cho, Kyunghyun and Lin, Jimmy},
  journal={arXiv preprint arXiv:1910.14424},
  year={2019}
}

@misc{lostincontext,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{resesarchlongcontext,
      title={Retrieval meets Long Context Large Language Models}, 
      author={Peng Xu and Wei Ping and Xianchao Wu and Lawrence McAfee and Chen Zhu and Zihan Liu and Sandeep Subramanian and Evelina Bakhturina and Mohammad Shoeybi and Bryan Catanzaro},
      year={2023},
      eprint={2310.03025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{codellama,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2023},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{codebert,
      title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages}, 
      author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
      year={2020},
      eprint={2002.08155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
graphcodebert,
title={GraphCode{\{}BERT{\}}: Pre-training Code Representations with Data Flow},
author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=jLoC4ez43PZ}
}

@misc{transformer,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{codet5,
      title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, 
      author={Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
      year={2021},
      eprint={2109.00859},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{starcoder,
      title={StarCoder: may the source be with you!}, 
      author={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and João Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Muñoz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{repocoder,
      title={RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation}, 
      author={Fengji Zhang and Bei Chen and Yue Zhang and Jacky Keung and Jin Liu and Daoguang Zan and Yi Mao and Jian-Guang Lou and Weizhu Chen},
      year={2023},
      eprint={2303.12570},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ccfinder,
      title={CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context}, 
      author={Yangruibo Ding and Zijian Wang and Wasi Uddin Ahmad and Murali Krishna Ramanathan and Ramesh Nallapati and Parminder Bhatia and Dan Roth and Bing Xiang},
      year={2023},
      eprint={2212.10007},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{repoprompt,
      title={Repository-Level Prompt Generation for Large Language Models of Code}, 
      author={Disha Shrivastava and Hugo Larochelle and Daniel Tarlow},
      year={2023},
      eprint={2206.12839},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{swebench,
      title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?}, 
      author={Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},
      year={2023},
      eprint={2310.06770},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{codesearchnet,
      title={CodeSearchNet Challenge: Evaluating the State of Semantic Code Search}, 
      author={Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
      year={2020},
      eprint={1909.09436},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{codexglue,
      title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation}, 
      author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},
      year={2021},
      eprint={2102.04664},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{repobench,
      title={RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems}, 
      author={Tianyang Liu and Canwen Xu and Julian McAuley},
      year={2023},
      eprint={2306.03091},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{densecode,
author = {Gu, Xiaodong and Zhang, Hongyu and Kim, Sunghun},
title = {Deep Code Search},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180167},
doi = {10.1145/3180155.3180167},
pages = {933–944},
numpages = {12},
keywords = {code search, deep learning, joint embedding},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@online{zdnet,
  author = {{ZDNet}},
  title = {Microsoft upgrades Copilot with OpenAI's GPT-4, turbo DALL-E 3, and more},
  year = {2023},
  url = {https://www.zdnet.com/article/microsoft-upgrades-copilot-with-openais-gpt-4-turbo-dall-e-3-and-more/},
  urldate = {2023-12-14}
}

@online{anthropic100k,
  author = {{Anthropic}},
  title = {Introducing 100K Context Windows},
  year = {2023},
  url = {https://www.anthropic.com/index/100k-context-windows},
  urldate = {2023-12-14}
}

@online{pinecone,
  author = {{Pinecone}},
  title = {Less is More: Why Use Retrieval Instead of Larger Context Windows},
  year = {2023},
  url = {https://www.pinecone.io/blog/why-use-retrieval-instead-of-larger-context},
  urldate = {2023-12-14}
}

@online{copilot_demo,
  author = {{GitHub Next}},
  title = {Copilot Workspace},
  year = {2023},
  url = {https://githubnext.com/projects/copilot-workspace/},
  urldate = {2023-12-14}
}

@online{tiktoken,
  author = {{OpenAI}},
  title = {TikToken Github},
  year = {2023},
  url = {https://github.com/openai/tiktoken},
  urldate = {2023-12-14}
}

@INPROCEEDINGS{Lin_etal_SIGIR2021_Pyserini,
   author = "Jimmy Lin and Xueguang Ma and Sheng-Chieh Lin and Jheng-Hong Yang and Ronak Pradeep and Rodrigo Nogueira",
   title = "{Pyserini}: A {Python} Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations",
   booktitle = "Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)",
   year = 2021,
   pages = "2356--2362",
}

@misc{yang2024sweagent,
      title={SWE-agent: Agent Computer Interfaces Enable Software Engineering Language Models},
      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik Narasimhan and Ofir Press},
      year={2024},
}

@inproceedings{wang2023rap,
  title={Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair},
  author={Wang, Weishi and Wang, Yue and Joty, Shafiq and Hoi, Steven CH},
  booktitle={Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={146--158},
  year={2023}
}

@article{rag,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@misc{anthropic2024claude3,
  title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author = {Anthropic},
  year = {2024},
  month = mar,
  url = {https://www.anthropic.com/news/claude-3-family},
  note = {Accessed: 2024-05-02}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{munkhdalai2024leave,
  title={Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  author={Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal={arXiv preprint arXiv:2404.07143},
  year={2024}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}