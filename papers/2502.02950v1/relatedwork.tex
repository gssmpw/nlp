\section{Related Work}
\subsection{Neural codec based Zero-shot TTS}
Inspired by the success of LLMs in NLP, formatting the TTS task as a next-token prediction problem has gained significant popularity in recent years~\citep{borsos2022audiolm,zhang2023speechgpt}. VALL-E~\citep{wang2023valle}, as a pioneering work, introduced the first neural codec based TTS framework, leveraging large-scale speech data to develop in-context learning capabilities. It can synthesize high-quality personalized speech in a zero-shot manner using only a 3-second enrolled recording as an acoustic prompt. Building on this foundation, the typical neural codec-based TTS systems~\citep{kharitonov2023speak,xin2024ralle,peng2024voicecraft} involve tokenizing speech signals into discrete tokens using a neural codec~\citep{defossez2022high,zeghidour2021soundstream}. A decoder-only language model is then trained to predict these discrete tokens alongside phoneme sequences. Finally, the predicted tokens are reconstructed into speech signals using the codec decoder. This approach has demonstrated promising scalability with large datasets and model sizes, enabling high-quality synthesized speech and powerful zero-shot capabilities for unseen speakers~\citep{lajszczak2024base, du2024cosyvoice}.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=13cm]{fig/example.pdf}
  \caption{Typical examples of segmental errors can be categorized into two types: temporal modeling errors and semantic-phonetic alignment errors. The problematic segments in the generated samples are highlighted using red boxes or symbols.}
  \label{fig:example}
  \vspace{-15pt}
\end{figure*}

\subsection{Reinforcement Learning from Human Feedback}
Human feedback has been widely used in language models for NLP tasks~\citep{ouyang2022training}, with RLHF technology playing a crucial role in aligning models with human preferences. Conventionally, preference optimization is performed by maximizing rewards computed from a reward model to align generated content. Recent approaches, such as Direct Preference Optimization (DPO)~\citep{rafailov2024dpo}, leverage closed-form loss functions that directly operate on preference data. This method eliminates the need for explicit reward model training while achieving alignment performance comparable to traditional RLHF.

RLHF in TTS remains in its infancy compared to its advancements in NLP. SpeechAlign first introduced RLHF in TTS~\citep{zhang2024speechalign}, exploring multiple preference alignment methods using paired preference data. UNO~\citep{chen2024uno} and RIO~\citep{hu2024rio} extended this by optimizing unpaired preference data: UNO accounts for annotation uncertainty in subjective evaluations, while RIO introduces a reverse preference data selection method based on Bayesian principles. Recent studies~\citep{tian2024tx_rl} have conducted thorough empirical evaluations of how preference alignment algorithms enhance LM-based TTS, and Seed-TTS~\citep{anastassiou2024seedtts} also have adopted RLHF during the post-training stage of LM-based TTS. However, current approaches are limited to utterance-level preference optimization and overlook the potential of fine-grained alignment.