% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\usepackage{subfigure} %插入多图时用子图显示的宏包
\usepackage{diagbox}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{cite}
\usepackage{color}
\usepackage{url,array}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{booktabs}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Fine-grained Preference Optimization Improves Zero-shot Text-to-Speech}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    Jixun Yao\textsuperscript{1},
    Yuguang Yang\textsuperscript{2},
    Yu Pan\textsuperscript{2},
    Yuan Feng\textsuperscript{2},
    Ziqian Ning\textsuperscript{1}, \\
    \textbf{Jianhao Ye\textsuperscript{2}},
    \textbf{Hongbin Zhou\textsuperscript{2}},
    \textbf{Lei Xie\textsuperscript{1}}\\
    \textsuperscript{1}Northwestern Polytechnical University\\
    \textsuperscript{2}Everest Team, Ximalaya
}
% \author{Jixun Yao \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\
% }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Integrating human feedback to align text-to-speech (TTS) system outputs with human preferences has proven to be an effective approach for enhancing the robustness of language model-based TTS systems. Current approaches primarily focus on using preference data annotated at the utterance level. However, frequent issues that affect the listening experience often only arise in specific segments of audio samples, while other segments are well-generated. 
% These issues call for a fine-grained optimization process that focuses on addressing localized problems rather than optimizing the entire utterance uniformly.
In this study, we propose a fine-grained preference optimization approach (FPO) to enhance the robustness of TTS systems. FPO focuses on addressing localized issues in generated samples rather than uniformly optimizing the entire utterance. Specifically, we first analyze the types of issues in generated samples, categorize them into two groups, and propose a selective training loss strategy to optimize preferences based on fine-grained labels for each issue type.
Experimental results show that FPO enhances the robustness of zero-shot TTS systems by effectively addressing local issues, significantly reducing the bad case ratio, and improving intelligibility. Furthermore, FPO exhibits superior data efficiency compared with baseline systems, achieving similar performance with fewer training samples.



\end{abstract}

\section{Introduction}

% Large language models (LLMs) demonstrate remarkable in-context learning abilities in natural language processing (NLP) tasks. 
% % They rely on the next-token prediction paradigm and scale both model parameters and dataset size, leading to consistent improvements in prediction accuracy. 
% Reinforcement learning from human feedback (RLHF) plays a vital role in calibrating these models, aligning them with human preferences, and reducing the likelihood of generating harmful or toxic content. Reinforcement learning from human feedback (RLHF) is a typical technique that is essential for the success of systems like ChatGPT and has recently been introduced to tasks in other modalities.

Large language models (LLMs) demonstrate remarkable in-context learning abilities in natural language processing (NLP) tasks by scaling both model parameters and dataset size~\citep{brown2020language,touvron2023llama,grattafiori2024llama3}. Reinforcement learning from human feedback (RLHF)~\citep{christiano2017deep} is crucial for calibrating these models, aligning them with human preferences, and minimizing the risk of generating harmful or toxic content. This technique is essential for the success of systems like ChatGPT~\citep{achiam2023gpt} and has recently been extended to tasks in other modalities.

Speech, a key modality of generative artificial intelligence, is experiencing rapid advancements inspired by developments in text-based LLMs. 
Recent text-to-speech (TTS) studies~\citep{wang2023valle,chen2024vall2,du2024cosyvoice} tokenize speech signals into discrete units and employ language model to generate speech tokens in a next-token prediction manner based on phoneme inputs and prompt speech recordings.
By leveraging in-context learning capabilities, these TTS systems can produce high-quality, personalized speech in a zero-shot manner. Similar to the use of RLHF in calibrating text LLMs, there is growing interest in integrating RLHF into TTS systems~\citep{zhang2024speechalign,chen2024uno,hu2024rio,tian2024tx_rl}. These approaches effectively enhance the robustness and zero-shot capabilities of TTS systems by incorporating human evaluation.


Despite significant advancements in integrating RLHF into TTS systems, we believe two primary challenges remain. The first challenge is efficiently calibrating pre-trained TTS systems with limited preference data. RLHF typically requires preference data evaluated by human annotators, which is then used for training to align TTS systems with human preferences. However, obtaining large-scale human-annotated speech preference data is prohibitively expensive. Compared to annotating text data, annotating speech data is more difficult due to its inclusion of both linguistic content and paralinguistic information.

%后面是不是可以参考3.2部分对错误描述，加到前面来 
% We first investigate the issue that affects the listening experience in the generated samples from the evaluation perspective. 这句话可以放到contribution2

%适合当intro
% Our approach begins by identifying issues that affect the listening experience in generated samples from an evaluation perspective. 
% The evaluation of synthesized speech from TTS systems often highlights two distinct types of issues: systemic errors and segmental errors. Systemic errors, such as poor timbre similarity and degraded audio quality, affect the overall auditory characteristics of the generated speech and have been extensively studied. In contrast, segmental errors, including mispronunciations, missing words, or unnatural pauses, are localized to specific parts of the audio. These errors not only impact the intelligibility and naturalness of specific portions of synthesized audio but also result in inefficiencies, as well-synthesized segments of the audio are effectively undermined. This makes addressing segmental errors particularly crucial in TTS systems.

The second challenge is achieving fine-grained alignment of specific segments in generated samples with human feedback.
In recent years, issues such as poor speaker similarity and degraded audio quality, referred to as \textbf{systemic errors}, have been largely mitigated in TTS systems through advanced technology.
However, more frequent issues that impact the listening experience often only occur in specific segments of the audio samples, which we call \textbf{segmental errors}. These include issues like mispronunciation of certain words, abnormal pauses, or repetition, etc. However, current speech RLHF approaches typically annotate preference data at the utterance level, which fails to account for fine-grained issues. Additionally, utterance-level preference optimization calculates the loss for the entire sequence, even though many tokens are already well-trained, leading to inefficiencies~\citep{lin2024rho}. We believe that optimizing problematic segments in a fine-grained manner would be both more effective and more data-efficient.

% To address these challenges, we propose a novel fine-grained preference optimization (FPO) framework, an RLHF-based approach designed to enhance the robustness of zero-shot TTS systems. FPO employs a sampling-annotating-learning pipeline. First, representative training examples are generated using TTS systems, and problematic segments are annotated at the token level, enabling fine-grained alignment and optimization compared to traditional utterance-level annotation. These segments are classified into two categories: temporal modeling errors and semantic-phonetic alignment errors.
% During training, temporal modeling errors are addressed by computing the loss from the beginning of the problematic segment to the end of the sequence, while semantic-phonetic alignment errors are resolved by calculating the loss only within the problematic segment. This selective loss training strategy processes the entire sequence through the TTS models but focuses loss computation solely on the desired and undesired tokens within the identified segments. This approach effectively resolves fine-grained issues while improving training efficiency.
To address the above challenges, we propose a novel fine-grained preference optimization (FPO) framework—an RLHF-based approach aimed at addressing segmental errors in generated samples and improving the robustness of zero-shot TTS systems. FPO employs a fine-grained sampling-annotating pipeline, where representative audio samples are first generated multiple times by TTS systems. Preference data are then annotated from human listening perspectives, labeling each sample as preferred or dispreferred at the utterance level. For each paired preference data, we annotate the problematic segment of the local issue to obtain fine-grained token-level preference labels, enabling more precise optimization than traditional approaches. Furthermore, we analyze the types of issues across different problem segments from an evaluation perspective and propose a selective training loss strategy to optimize each type of issue. During training, FPO processes the entire sequence through the TTS models but computes loss exclusively for the problematic segments, which can effectively resolve fine-grained issues while enhancing training efficiency.
Experimental results demonstrate that FPO significantly enhances the robustness of zero-shot TTS systems and exhibits superior data efficiency compared with baseline systems, improving the quality of generated speech across both subjective and objective metrics. 
% The main contributions of our study can be summarized as follows:
The main contributions can be summarized as follows:

%contribution
% abstract
% conclusion

% 细节放到section 3
% These segments are classified into two categories: temporal modeling errors and semantic-phonetic alignment errors.
% During training, temporal modeling errors are addressed by computing the loss from the beginning of the problematic segment to the end of the sequence, while semantic-phonetic alignment errors are resolved by calculating the loss only within the problematic segment. 



\begin{itemize}
%体现challenge，unseen speaker和style
    \item We propose FPO, an RLHF-based approach designed to align problematic segments in generated samples, thereby enhancing the robustness of TTS systems. The key novelty of FPO lies in its preference optimization strategy, which focuses on optimizing specific problematic segments while avoiding unnecessary computation on well-learned tokens, resulting in a more data-efficient optimization process.
    \item By analyzing issues in generated samples, FPO fine-grained optimizes TTS systems on problematic segments using a selective training strategy. This approach introduces a new perspective for addressing fine-grained fixes in TTS preference optimization, focusing on resolving bad cases efficiently and effectively.
    \item Extensive experiments demonstrate the efficiency and effectiveness of FPO across both subjective and objective metrics. Notably, FPO significantly reduces the occurrence of bad cases compared to powerful baseline systems, effectively addressing the robustness challenges of zero-shot TTS systems. Audio samples can be found at \url{https://yaoxunji.github.io/fpo}.
\end{itemize}





\section{Related Work}
\subsection{Neural codec based Zero-shot TTS}
Inspired by the success of LLMs in NLP, formatting the TTS task as a next-token prediction problem has gained significant popularity in recent years~\citep{borsos2022audiolm,zhang2023speechgpt}. VALL-E~\citep{wang2023valle}, as a pioneering work, introduced the first neural codec based TTS framework, leveraging large-scale speech data to develop in-context learning capabilities. It can synthesize high-quality personalized speech in a zero-shot manner using only a 3-second enrolled recording as an acoustic prompt. Building on this foundation, the typical neural codec-based TTS systems~\citep{kharitonov2023speak,xin2024ralle,peng2024voicecraft} involve tokenizing speech signals into discrete tokens using a neural codec~\citep{defossez2022high,zeghidour2021soundstream}. A decoder-only language model is then trained to predict these discrete tokens alongside phoneme sequences. Finally, the predicted tokens are reconstructed into speech signals using the codec decoder. This approach has demonstrated promising scalability with large datasets and model sizes, enabling high-quality synthesized speech and powerful zero-shot capabilities for unseen speakers~\citep{lajszczak2024base, du2024cosyvoice}.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=13cm]{fig/example.pdf}
  \caption{Typical examples of segmental errors can be categorized into two types: temporal modeling errors and semantic-phonetic alignment errors. The problematic segments in the generated samples are highlighted using red boxes or symbols.}
  \label{fig:example}
  \vspace{-15pt}
\end{figure*}

\subsection{Reinforcement Learning from Human Feedback}
Human feedback has been widely used in language models for NLP tasks~\citep{ouyang2022training}, with RLHF technology playing a crucial role in aligning models with human preferences. Conventionally, preference optimization is performed by maximizing rewards computed from a reward model to align generated content. Recent approaches, such as Direct Preference Optimization (DPO)~\citep{rafailov2024dpo}, leverage closed-form loss functions that directly operate on preference data. This method eliminates the need for explicit reward model training while achieving alignment performance comparable to traditional RLHF.

RLHF in TTS remains in its infancy compared to its advancements in NLP. SpeechAlign first introduced RLHF in TTS~\citep{zhang2024speechalign}, exploring multiple preference alignment methods using paired preference data. UNO~\citep{chen2024uno} and RIO~\citep{hu2024rio} extended this by optimizing unpaired preference data: UNO accounts for annotation uncertainty in subjective evaluations, while RIO introduces a reverse preference data selection method based on Bayesian principles. Recent studies~\citep{tian2024tx_rl} have conducted thorough empirical evaluations of how preference alignment algorithms enhance LM-based TTS, and Seed-TTS~\citep{anastassiou2024seedtts} also have adopted RLHF during the post-training stage of LM-based TTS. However, current approaches are limited to utterance-level preference optimization and overlook the potential of fine-grained alignment.




\section{Methodology}
\subsection{Background}
\textbf{Neural codec based zero-shot TTS} comprises two components: a neural codec model and a language model. The codec model tokenizes the speech signal into discrete acoustic tokens $a \in \mathbb{R}^{n \times l}$, where $n$ is the number of quantizer layers in the codec model and $l$ is the temporal length. The tokenized acoustic tokens can later be reconstructed into speech signals by the codec decoder. Given a text transcript $t$, the language model predicts the acoustic tokens using a next-token prediction approach. A widely used codec model employs a single quantizer, resulting in the extracted acoustic token being a single sequence along the temporal dimension. This setup simplifies the process of directly concat $t$ and $a$ and computing their joint probabilities in an auto-regressive manner:
\begin{equation}
    p(x)=\prod_{i=1}^l p\left(a_l \mid a_1, \ldots, a_{l-1}, t\right),
\end{equation}
it can also be regarded as a speech continuation task, conditioned on both the text and prompt speech during inference.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=12
  cm]{fig/model2.drawio.pdf}
  \caption{The preference optimization process of our proposed FPO. The language model is optimized using a selective training loss strategy, which focuses loss on the problematic segments while avoiding well-learned tokens.}
  \label{fig:model}
  \vspace{-10pt}
\end{figure*}

\textbf{RLHF with Preference Data.} The language model is prompted with prompts $x$ to generate pairs of speech samples ($y_1$,$y_2$). Humans then evaluate these pairs and express preferences for one sample, denoted as $y_w \succ y_l \mid x$, where $y_w$ and $y_l$ represent the preferred and dispreferred samples respectively.
The probability of preference data can be modeled using a reward model, which is trained via maximum likelihood estimation. During the reinforcement learning phase, the reward model provides feedback to the language model, which is then optimized by maximizing the rewards.



% also eliminating the need for explicit reward model learning or policy sampling throughout the training phase.

\subsection{Preference Data Collection}
The selection of preference data plays a critical role in RLHF. We propose a fine-grained sampling-annotation pipeline that begins with annotating pairwise preference data. Specifically, we first sample speech prompts from an unseen speaker pool for each text transcript and use them as input to the TTS system. To promote more diverse zero-shot TTS generation, we adjust sampling hyperparameters during inference and perform multiple inference runs for each sample, constructing a generated sample dataset $\{y_1^b, y_2^b, \ldots, y_k^b\}$, where $b$ is the $b$-th inference process.


After the sampling process, human annotators evaluate the generated samples from each inference process, identifying preferred and dispreferred ones. To mitigate the high human resource demands for annotations, we draw inspiration from previous studies~\citep{lin2024alignslm} and use pre-trained speech perceptual systems to simulate human feedback. We propose a comprehensive scoring method that combines multiple metrics—including intelligibility, quality, similarity, and duration—to determine the final score of each sample. This scoring system enables the identification of preferred samples based on their final scores $s(y)$:

% \tiny
\begin{small}
    \begin{equation}
    s(y) = \lambda_w \cdot w(y)^p + \lambda_m \cdot m(y)^p + \lambda_c \cdot c(y)^p +\lambda_d \cdot \text{Dur}(y)^p,
\end{equation}
\end{small}
where $w(\cdot)$ and $m(\cdot)$ represent the intelligibility evaluation metric and quality evaluation metric, respectively. $c(y)$ measures the speaker similarity and $\text{Dur}(y)$ measures the duration difference between the generated samples and ground truth samples. The weights $\lambda_*$ normalize each metric to ensure they operate at the same amplitude and scale the range of $s(y)$ at [0,1]. Meanwhile, $p$ is used to increase sensitivity to metrics with small changes.
These metrics are computed using pre-trained automatic systems, simulating various aspects of human evaluation. Therefore, the selection of preference data can be described as follows:
\begin{align}
    (y_l, y_w) \in \Big\{
    y_l = \min& \{s(y)\}, \, y_w = \max \{s(y)\}, \notag \\
    &y_w - y_l > \tau
    \Big\},
\end{align}
where $\tau$ is introduced to constrain sufficient differences between the preferred and dispreferred samples. We set $\tau=0.3$ to enhance the reliability of preference data for training. 

% Inspired by the recent success of token-level data filtering in NLP models~\citep{lin2024rho}, we propose a simple pipeline for fine-grained speech preference data selection. 

%细粒度怎么比区分error，两类，不同类不同mask
\subsection{Fine-grained Annotation}
% Fine-grained selective mask?
To achieve fine-grained alignment of TTS systems, we first thoroughly investigate the characteristics of segmental errors and categorize them into two types: temporal modeling errors and semantic-phonetic alignment errors, details can be found in Appendix~\ref{app:appendix_error}. As illustrated in Figure \ref{fig:example}, temporal modeling errors arise from the model's inability to accurately capture the temporal dynamics of speech, resulting in issues such as mispronunciations, abnormal silences, and unnatural prosody. For mispronunciations and abnormal silences, we utilize Whisper-timestamped\footnote{\url{https://github.com/linto-ai/whisper-timestamped}} and Voice Activity Detection systems to annotate the timestamps of these errors. For unnatural prosody, we incorporate the capabilities of text LLMs and professional human annotators to precisely identify affected segments (details provided in the Appendix~\ref{app:appendix_prosody}). This allows us to obtain timestamps for problematic segments and annotate fine-grained preference labels for temporal modeling errors at the token level. 

Semantic-phonetic alignment errors, on the other hand, arise from the mapping between phonetic input and acoustic output. These errors often manifest as repetitions or truncations, where the TTS system either over-relies on its previous outputs or fails to encode essential phonetic information. Annotating these error segments relies on automatic speech recognition systems and ground truth transcripts with forced alignment. 
However, unlike temporal modeling errors, which affect only specific segments, repetition and truncation errors often cause cascading issues after the initial error segment. Therefore, we apply different annotation strategies for each error type. For temporal modeling errors, only the problematic segments are annotated as error segments. In contrast, for semantic-phonetic alignment errors, all segments following the onset of the issue are annotated as error segments. The token-level preference annotation can be described as an indicator function:
\begin{equation}
    I\left(y^i\right)= \begin{cases}1 & \text { if } y^i \text{ in the error segment }  \\ 0 & \text { otherwise }\end{cases},
\end{equation}
where $i$ represents the $i$-th token of an utterance preference data.


% For segmental errors, we further divide them into two categories: temporal modeling errors and semantic-phonetic alignment errors, as shown in Figure 1. 
% Temporal modeling errors stem from the model's limitations in capturing the temporal dynamics of speech, leading to issues such as mispronunciation, where abrupt or unstable pronunciation changes occur due to insufficient modeling of pitch continuity or variation trends. Another common problem is abnormal silence, where the model erroneously inserts silent frames or extends silent segments, disrupting the naturalness of generated speech. Additionally, the generated speech may exhibit unnatural prosody (i.g. unnatural pause), as the model may fail to adequately capture local rhythmic patterns, including stress, duration, and speaking rate, resulting in speech that deviates from natural prosody.

\subsection{Preference Optimization}
The preference optimization process of our proposed FPO is illustrated in Figure~\ref{fig:model}.
A typical preference optimization process of RL with a reward model $r_\phi(x, y)$ can be formulated as
\begin{equation}
    \max _{\pi_\theta} \mathbb{E}\left[r_\phi(x, y)\right]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi_\theta(y \mid x) \| \pi_{\mathrm{ref}}(y \mid x)\right],
\end{equation}
where $\beta$ is a parameter controlling the deviation between the TTS model $\pi_\theta$ and the base reference model $\pi_{\text{ref}}$. 
However, estimating a reward model is computationally expensive and introduces additional complexity during training. To address this, we adopt the DPO~\citep{rafailov2024dpo} approach to align the TTS system, eliminating the need for a reward model. 

Following previous studies~\citep{korbak2022reinforcement,go2023aligning}, the optimal solution to the KL-constrained reward maximization objective in Eq.(5) can be described as follows:
\begin{equation}
    \pi_r(y \mid x)=\frac{1}{Z(x)} \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right).
\end{equation}
where $r(x, y)$ is the reward function widely defined as $r(x,y)=r_\phi(x, y)-\beta(\log \pi_\theta(y|x)-\log \pi_{\text{ref}}(y|x))$, 
while $r(x, y)$ in optimization can be expressed in terms of the corresponding optimal policy, the reference policy, and the unknown partition function $Z(\cdot)$. This reward function is formulated as 
\begin{equation}
    r(x, y)=\beta \log \frac{\pi_r(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}+\beta \log Z(x).
\end{equation}
Since the Bradley-Terry model depends only on the difference in rewards between two completions, the partition function $Z(\cdot)$ can be canceled, allowing the human preference probability to be expressed solely in terms of the optimal TTS model and the corresponding reference model. Finally, to achieve fine-grained optimization, we introduce the token function from Eq. (4) to compute selective training loss for both preferred and dispreferred samples:
\begin{align}
\mathcal{L}_{\text{FPO}}= 
- \mathbb{E} 
\Bigg[ & \sum_{i} I(y^i) \cdot \log \sigma \Big( \beta \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} \nonumber \\
& - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \Big) \Bigg].
\end{align}
This function allows us to fit a token-level implicit reward to optimize the TTS model using preference data while avoiding computational waste on well-trained tokens.


% \begin{equation}
%     \mathcal{L}_{\mathrm{DPO}}=-\mathbb{E}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right]
% \end{equation}

% \begin{small}
% \begin{align}
%     \mathcal{L}_{\mathrm{DPO}}=&\notag\\
%     -\mathbb{E}&\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}
%     -\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right]
% \end{align}
% \end{small}

% \begin{equation}
%     \mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\mathrm{ref}}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\mathrm{ref}}\left(y_l \mid x\right)}\right)\right]
% \end{equation}
% Following previous studies~\citep{korbak2022reinforcement,go2023aligning}, the optimal solution of KL-constrained reward maximization can be reformulated as:
% \begin{equation}
%     \pi_r(y \mid x)=\frac{1}{Z(x)} \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)
% \end{equation}


% \begin{equation}
%     \max _{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y \mid x)}\left[r_\phi(x, y)\right]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi_\theta(y \mid x) \| \pi_{\mathrm{ref}}(y \mid x)\right]
% \end{equation}

% Given a dataset of comparisons $\mathcal{D}=(x, y_w, y_l)$ sampled using the Bradley-Terry (BT) model, which defines the human preference distribution, we can parameterize a reward model $r_\phi$ and estimate its parameters through maximum likelihood. By framing the problem as a binary classification task, the negative log-likelihood loss can be expressed as:

% reward model
% \begin{equation}
%     \mathcal{L}_{r_\phi}=-\mathbb{E}\left[\log \sigma\left(r_\phi\left(x, y_w\right)-r_\phi\left(x, y_l\right)\right)\right]
% \end{equation}











\section{Experimental Setup}
% 人标不一致
% 造数据，xima开会经验


\begin{table*}[ht]
\centering
\caption{Objective results on WER/CER, UTMOS, SECS, and bad case ratio with different languages. For WER/CER and bad case ratio, lower values indicate better performance, while for UTMOS and SECS, higher values are better. The best results in each category are highlighted in bold.}
\label{tab:comp}
\renewcommand\arraystretch{1.2}
\begin{tabular}{lcccccccc}
\hline
            & \multicolumn{2}{c}{CER/WER $\downarrow$} & \multicolumn{2}{c}{SECS$\uparrow$} & \multicolumn{2}{c}{UTMOS$\uparrow$} & \multicolumn{2}{c}{Bad Case Ratio$\downarrow$} \\ \cline{2-9} 
            & CN         & EN         & CN          & EN         & CN          & EN          & CN               & EN              \\ \hline
GT          & 3.06        & 2.17        & \multicolumn{2}{c}{-}    & 3.92        & 4.41        & \multicolumn{2}{c}{-}              \\ \hline
CosyVoice   & 8.26\textcolor{gray}{$_{+0.0\%}$}        & 6.97\textcolor{gray}{$_{+0.0\%}$}        & 0.65        & 0.68       & 3.74        & 3.98        & 31\%             & 24\%            \\
SDPO & 7.76\textcolor{purple}{$_{-6.1\%}$}        & 6.54\textcolor{purple}{$_{-6.2\%}$}        & 0.68        & 0.70       & 3.85        & 4.04        & 17\%             & 13\%            \\
UNO         & 5.36\textcolor{purple}{$_{-35.1\%}$}        & 4.79\textcolor{purple}{$_{-31.3\%}$}        & 0.70        & \textbf{0.73}       & 3.94        & 4.03        & 13\%             & 12\%            \\
RIO         & 4.81\textcolor{purple}{$_{-41.8\%}$}        & 4.45\textcolor{purple}{$_{-36.2\%}$}        & \textbf{0.72}        & 0.71       & \textbf{4.02}        & \textbf{4.11}        & 11\%             & 9\%            \\ \hline
FPO         & \textbf{3.92\textcolor{purple}{$_{-52.5\%}$}}        & \textbf{3.15\textcolor{purple}{$_{-54.8\%}$}}        & 0.66        & 0.68       & 3.83        & 4.01        & \textbf{8\%}             & \textbf{5\%}            \\ \hline
\end{tabular}
\vspace{-10pt}
\end{table*}

%这块得和之前方法内容对上

\subsection{Dataset}
We chose CosyVoice~\citep{du2024cosyvoice} as our backbone model, which was trained on a large-scale dataset comprising approximately 170,000 hours. For RLHF optimization, we use the training set of LibriTTS~\citep{zen2019libritts} and AISHELL-3~\citep{shi2020aishell3} corpora, already included in the base model's training. This setup eliminates the influence of introducing additional high-quality data.
Following prior research~\citep{hu2024rio}, we sample a pool of speech prompts and a separate pool of target transcripts to generate preference samples using pre-trained CosyVoice model for RLHF optimization. Using the proposed fine-grained sampling-annotation pipeline, we select pairs of samples with significant differences as training data for RLHF, resulting in 1,000 preference training utterances for preference optimization.


\subsection{Baseline Systems}
In addition to using CosyVoice as the base model, we reproduce the following powerful preference optimization approaches based on CosyVoice as baseline systems for a comprehensive comparison:

\begin{itemize}
    \item Speech-DPO~\citep{rafailov2024dpo}: 
    We adapt the popular DPO algorithm to align the base model with human feedback, where preference data is optimized at the utterance level, similar to the method described in \citep{zhang2024speechalign}, and denote it as SDPO.
    \item UNO~\citep{chen2024uno}: A recent study proposes a RLHF framework tailored for TTS optimization, specifically considering the uncertainty arising from the inherent variability in subjective human speech perception and evaluation.
    \item RIO~\citep{hu2024rio}: It proposes a reverse inference optimization method based on the Bayesian principle to select exemplars from speech samples generated by the TTS system, enabling preference optimization to enhance the robustness of the TTS system.


\end{itemize}

\subsection{Evaluation Metrics}
\textbf{Subjective Metrics}: We employ the naturalness mean opinion score (NMOS) to evaluate the naturalness of the generated samples. We invited 15 participants to listen to the audio samples and rate the naturalness on a 5-point scale, where 1 indicates very unnatural and 5 indicates completely natural.
Furthermore, we conduct an ABX test where participants listen to two generated speech samples from different models but based on the same input and then choose the sample that sounds more natural. If the samples are too close to distinguish, they are instructed to indicate a tie.

\textbf{Objective Metrics}: For objective evaluation, we use the speaker embedding cosine similarity (SECS), calculated using pre-trained automatic speaker verification models\footnote{\url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}}, to evaluate speaker similarity. Word error rate (WER) and character error rate (CER), transcribed by an automatic speech recognition model\footnote{\url{https://huggingface.co/openai/whisper-medium}}, are employed to evaluate robustness for Mandarin and English datasets, respectively. Additionally, speech quality is measured using a predicted mean opinion score (UTMOS), computed by a neural network model\footnote{\url{https://github.com/sarulab-speech/UTMOS22}}.
Meanwhile, following previous studies~\citep{hu2024rio}, we introduce the bad case ratio to evaluate the robustness of different baseline systems, where samples with UTMOS below 3 or WER exceeding 20\% are classified as bad cases.




% Whisper-medium

\section{Experimental Results}
\subsection{Objective Evaluation}
% 没体现 sdpo对比
We first compare our proposed FPO with baseline systems using objective metrics, results are shown in Table \ref{tab:comp}. For WER and CER results, FPO achieves significant improvements in speech intelligibility that reduces CER by 52.5\% (from 8.26 to 3.92) and WER by 54.8\% (from 6.97 to 3.15) compared to the base model CosyVoice. This outperforms all baselines, including RIO ($-$41.8\% CER and $-$36.2\% WER) and UNO ($-$35.1\% CER and $-$31.3\% WER), demonstrating FPO’s superior ability to mitigate segmental errors such as mispronunciations and repetitions.

On the other hand, FPO demonstrates remarkable robustness in terms of the bad case ratio, reducing it to 8\% (CN) and 5\% (EN), outperforming all baselines by a large margin. This highlights FPO’s ability to effectively minimize segmental errors by optimizing problematic segments. These results demonstrate the efficacy of fine-grained preference optimization in addressing segmental errors and enhancing the overall reliability of TTS systems.



For metrics such as SECS and UTMOS, our proposed FPO achieves only slightly higher results than the base model, while RIO and UNO achieve better performance than FPO. This is because FPO focuses on addressing localized errors rather than global attribute alignment, such as timbre or overall quality. Since the base model already demonstrates strong speaker similarity and speech quality, improvements from baseline systems are marginal, serving as a minor enhancement. We believe this trade-off is justified, as FPO prioritizes resolving segmental errors, emphasizing improvements in intelligibility and robustness over enhancements in global attributes.

\begin{figure}[ht]
  \centering
  \includegraphics[width=7.5cm]{fig/violin.pdf}
  \caption{Violin plots of the subjective NMOS for speech generated between baseline systems and FPO.}
  \label{fig:violin}
  \vspace{-15pt}
\end{figure}

\subsection{Subjective Evaluation}
To further evaluate the effectiveness of FPO, we conducted subjective evaluations on the naturalness of the generated samples. As shown in Figure \ref{fig:violin}, the violin plots illustrate the NMOS distribution, comparing the performance of FPO with baseline systems. The distribution range of CosyVoice is wider than that of other systems, with elongated density tails indicating the presence of notable outliers. This suggests instability in the naturalness performance of the generated samples.
In contrast, the distribution range of FPO is more compact than CosyVoice and SDPO, demonstrating improved robustness after preference optimization. Although UNO and RIO also achieve relatively compact NMOS distributions, FPO outperforms UNO in interquartile range and shows a better overall NMOS distribution compared to RIO. A comparative analysis of the violin plots reveals significant differences between FPO and the baseline systems, particularly in median values and distribution patterns. These results highlight the superior performance of FPO in achieving more natural speech generation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=8cm]{fig/abx.png}
  \caption{Results for ABX preference test, while "Base" denote the original CosyVoice model.}
  \label{fig:abx}
\end{figure}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=15cm]{fig/efficient.pdf}
  \caption{Results of FPO and baseline systems using different sizes of training data.}\vspace{-10pt}
  \label{fig:efficient}
\end{figure*}

Apart from NMOS, we also conduct ABX preference tests to verify the performance gains of our proposed FPO compared to baseline systems. As shown in Figure \ref{fig:abx}, compared to the base model, FPO is preferred in 54.3\% of cases and ties in 24.1\%, significantly outperforming the base model in preference. Notably, FPO also surpasses SDPO by a considerable margin (44.2\% vs. 28.7\%, with 27.1\% ties), demonstrating the effectiveness of fine-grained optimization compared to full-sequence optimization. The ABX test results further highlight FPO's advantages.



On the other hand, FPO achieves a higher preference score than UNO and RIO in the ABX test. While UNO and RIO focus on enhancing overall attributes such as timbre and quality, FPO specifically targets problematic segments, reducing abrupt errors and improving local consistency. The higher preference scores in ABX tests suggest that fine-grained optimization of local segments better aligns with human perceptual priorities.
 

These results indicate that while utterance-level optimization methods effectively align the base model to some extent, FPO directly addresses critical pain points in the human listening experience. This leads to higher practical utility, even when objective metric scores are comparable. The strong alignment between subjective and objective evaluations further validates the effectiveness of the proposed FPO.


\subsection{Analysis on Data Efficient}

To further evaluate the data efficiency of our proposed FPO, we compare its performance with baseline systems using small-scale Mandarin training data, with CER and bad case ratio as evaluation metrics. As shown in Figure \ref{fig:efficient}, FPO achieves a similar CER performance as the RIO system while using only 200 utterances, compared to the 800 utterances required by the baselines. FPO is up to four times faster than the baseline systems. Additionally, FPO outperforms all baselines in bad case ratio across different data sizes and achieves a speedup of approximately 4x compared to UNO. 

% These results demonstrate that the selective training strategy effectively focuses on problematic segments, reducing computational waste and leading to more data-efficient preference optimization.

Furthermore, we scale up the preference data size to compare the data scalability between the SDPO approach and our proposed FPO. As shown in Table \ref{tab:data_comp}, with just 1,000 training utterances, FPO reduces CER by 52.5\% (from 8.26 to 3.92) and WER by 54.8\% (from 6.97 to 3.15) for CN and EN, respectively. This outperforms even SDPO trained on 5,000 utterances (CER: 4.14 vs. 3.11; WER: 3.85 vs. 2.56), highlighting FPO’s ability to maximize efficiency with limited data. Notably, FPO reduces the bad case ratio to 8\% (CN) and 5\% (EN) with only 1,000 samples, whereas SDPO requires 5,000 samples to achieve 9\% (CN) and 8\% (EN). These results further demonstrate FPO’s effectiveness in mitigating critical errors while maintaining high data efficiency.


\begin{table}[ht]
\centering
\renewcommand\arraystretch{1.2}
\caption{Comparison results between FPO and SDPO on several larger datasets. "Num" represents the number of utterances used for training.}
\label{tab:data_comp}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccccc}
\hline
\multirow{2}{*}{}           & \multirow{2}{*}{Num (\#)} & \multicolumn{2}{c}{CER/WER} & \multicolumn{2}{c}{Bad case ratio} \\ \cline{3-6} 
                            &                           & CN           & EN           & CN               & EN              \\ \hline
CosyVoice                   & -                         & 8.26         & 6.97         & 31\%             & 24\%            \\ \hline
\multirow{3}{*}{SDPO} & 1000                      & 7.76         & 6.54         & 17\%             & 13\%            \\
                            & 2000                      & 6.06         & 5.79         & 14\%             & 12\%            \\
                            & 5000                      & 4.14         & 3.85         & 9\%              & 8\%             \\ \hline
\multirow{3}{*}{FPO}        & 1000                      & 3.92         & 3.15         & 8\%              & 5\%             \\ 
                            & 2000                      & 3.79         & 2.84         & 5\%              & 4\%             \\
                            & 5000                      & 3.11         & 2.56         & 3\%              & 2\%             \\ \hline
\end{tabular}}
\end{table}


FPO’s advantage stems from its selective training loss strategy, which focuses optimization on error segments identified through fine-grained annotations. By avoiding unnecessary updates to well-learned tokens, FPO reduces computational redundancy and accelerates convergence. In contrast, utterance-level methods like SDPO apply uniform optimization across entire sequences, diluting the impact of critical segments and requiring larger datasets to achieve comparable improvements.



\section{Conclusion}
In this study, we propose FPO, a novel RLHF framework designed to address localized errors in zero-shot TTS systems. By categorizing generation errors into systemic and segmental types and introducing token-level selective loss optimization, FPO aligns problematic segments using fine-grained preference data while avoiding computational waste on well-learned tokens. Experimental results demonstrate that FPO significantly improves speech intelligibility and robustness, reducing CER by 52.5\% and WER by 54.8\% compared to the base model. Additionally, FPO lowers the bad case ratio to 8\% (CN) and 5\% (EN), outperforming utterance-level preference optimization approaches. Subjective evaluations further confirm FPO’s superiority in naturalness and alignment with human perceptual priorities. Notably, FPO achieves comparable performance with 4× fewer training samples than baseline systems, highlighting its remarkable data efficiency.


\section*{Limitations}
One of the main limitations of this study lies in the construction of the preference training dataset. The proposed framework relies on fine-grained manual annotation to ensure the quality and consistency. This process is both time-consuming and labor-intensive, which may limit the scalability of the approach to larger or more diverse datasets. Although our proposed approach is data efficient and need less annotation data compared with previous works, the fine-grained annotation at each sample is more difficult than utterance level annotation. In future work, we plan to explore strategies to alleviate this limitation. For example, leveraging weakly supervised, semi-supervised, or self-supervised learning techniques may help reduce the dependence on manually annotated data. 

\section*{Ethics Statement}
This work does not raise any ethical concerns. All data and pre-trained models used in this study are publicly available and comply with the following licenses: Creative Commons BY 4.0, Creative Commons CC0, Creative Commons BY-NC-ND 4.0, and Creative Commons BY-SA 4.0.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Appendix}
% In the supplemental material:
% \begin{itemize}
%     \item \ref{app:discuss}. We discuss several common questions for the design of FPO.
%     \item \ref{app:appendix_error}. We describe the error.
% \end{itemize}

% % Question
% % 多个问题在一个音频中

% % 不同annotation 策略，消融。如果都只把问题片段标记

% \subsection{Additional Discussions on the Design of FPO}
% \label{app:discuss}

% \textit{\textbf{Question 1}: Can GenSE employ different SSL models to extract semantic tokens?}

% \textit{\textbf{Question 2}: The difficulty of fine-grained annotation?}

\subsection{Segmental Errors}
\label{app:appendix_error}
Temporal modeling errors stem from the model's limitations in capturing the temporal dynamics of speech, leading to issues such as mispronunciation, where abrupt or unstable pronunciation changes occur due to insufficient modeling of pitch continuity or variation trends. Another common problem is abnormal silence, where the model erroneously inserts silent frames or extends silent segments, disrupting the naturalness of generated speech. Additionally, the generated speech may exhibit unnatural prosody (i.g. unnatural pause), as the model may fail to adequately capture local rhythmic patterns, including stress, duration, and speaking rate, resulting in speech that deviates from natural prosody.

Semantic-phonetic alignment errors arise from mapping between the phonetic input and the acoustic output. These errors often manifest as repetition, where the model over-relies on its previous outputs and redundantly generates the same words or phonemes. Alternatively, truncation errors may occur when the language model fails to encode key phonetic information from the input or neglects certain critical elements, leading to synthesized speech that lacks completeness and accuracy. 

% \subsection{Difficulty on annotation}
\subsection{Prosody Annotation}
\label{app:appendix_prosody}
For prosody issues, we primarily focus on abnormal pauses and annotate fine-grained prosody errors through a three-step process. First, we use an automatic speech recognition system to transcribe the speech into text. Next, we employ text LLMs to determine ideal pauses in the text and insert break tags. Finally, professional annotators evaluate the speech samples based on the break tags, identifying preferred and dispreferred prosody samples. Fine-grained timestamps are then obtained using force-alignment tools based on the text and break tags.
\end{document}
