[
  {
    "index": 0,
    "papers": [
      {
        "key": "borsos2022audiolm",
        "author": "Borsos, Zal{\\'a}n and Marinier, Rapha{\\\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil",
        "title": "Audiolm: a language modeling approach to audio generation.(2022)"
      },
      {
        "key": "zhang2023speechgpt",
        "author": "Zhang, Dong and Li, Shimin and Zhang, Xin and Zhan, Jun and Wang, Pengyu and Zhou, Yaqian and Qiu, Xipeng",
        "title": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wang2023valle",
        "author": "Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others",
        "title": "Neural codec language models are zero-shot text to speech synthesizers"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kharitonov2023speak",
        "author": "Kharitonov, Eugene and Vincent, Damien and Borsos, Zal{\\'a}n and Marinier, Rapha{\\\"e}l and Girgin, Sertan and Pietquin, Olivier and Sharifi, Matt and Tagliasacchi, Marco and Zeghidour, Neil",
        "title": "Speak, read and prompt: High-fidelity text-to-speech with minimal supervision"
      },
      {
        "key": "xin2024ralle",
        "author": "Xin, Detai and Tan, Xu and Shen, Kai and Ju, Zeqian and Yang, Dongchao and Wang, Yuancheng and Takamichi, Shinnosuke and Saruwatari, Hiroshi and Liu, Shujie and Li, Jinyu and others",
        "title": "Rall-e: Robust codec language modeling with chain-of-thought prompting for text-to-speech synthesis"
      },
      {
        "key": "peng2024voicecraft",
        "author": "Peng, Puyuan and Huang, Po-Yao and Li, Shang-Wen and Mohamed, Abdelrahman and Harwath, David",
        "title": "Voicecraft: Zero-shot speech editing and text-to-speech in the wild"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "defossez2022high",
        "author": "D{\\'e}fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi",
        "title": "High fidelity neural audio compression"
      },
      {
        "key": "zeghidour2021soundstream",
        "author": "Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco",
        "title": "Soundstream: An end-to-end neural audio codec"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lajszczak2024base",
        "author": "{\\L}ajszczak, Mateusz and C{\\'a}mbara, Guillermo and Li, Yang and Beyhan, Fatih and van Korlaar, Arent and Yang, Fan and Joly, Arnaud and Mart{\\'\\i}n-Cortinas, {\\'A}lvaro and Abbas, Ammar and Michalski, Adam and others",
        "title": "BASE TTS: Lessons from building a billion-parameter text-to-speech model on 100K hours of data"
      },
      {
        "key": "du2024cosyvoice",
        "author": "Du, Zhihao and Chen, Qian and Zhang, Shiliang and Hu, Kai and Lu, Heng and Yang, Yexin and Hu, Hangrui and Zheng, Siqi and Gu, Yue and Ma, Ziyang and others",
        "title": "{CosyVoice}: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rafailov2024dpo",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2024speechalign",
        "author": "Zhang, Dong and Li, Zhaowei and Li, Shimin and Zhang, Xin and Wang, Pengyu and Zhou, Yaqian and Qiu, Xipeng",
        "title": "SpeechAlign: Aligning Speech Generation to Human Preferences"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2024uno",
        "author": "Chen, Chen and Hu, Yuchen and Wu, Wen and Wang, Helin and Chng, Eng Siong and Zhang, Chao",
        "title": "Enhancing Zero-shot Text-to-Speech Synthesis with Human Feedback"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hu2024rio",
        "author": "Hu, Yuchen and Chen, Chen and Wang, Siyin and Chng, Eng Siong and Zhang, Chao",
        "title": "Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference Optimization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tian2024tx_rl",
        "author": "Tian, Jinchuan and Zhang, Chunlei and Shi, Jiatong and Zhang, Hao and Yu, Jianwei and Watanabe, Shinji and Yu, Dong",
        "title": "Preference Alignment Improves Language Model-Based TTS"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "anastassiou2024seedtts",
        "author": "Anastassiou, Philip and Chen, Jiawei and Chen, Jitong and Chen, Yuanzhe and Chen, Zhuo and Chen, Ziyi and Cong, Jian and Deng, Lelai and Ding, Chuang and Gao, Lu and others",
        "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models"
      }
    ]
  }
]