% ADD THIS HEADER TO ALL NEW CHAPTER FILES FOR SUBFILES SUPPORT

% Allow independent compilation of this section for efficiency
\documentclass[../CLthesis.tex]{subfiles}

% Add the graphics path for subfiles support
\graphicspath{{\subfix{../images/}}}

% END OF SUBFILES HEADER

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START OF DOCUMENT: Every chapter can be compiled separately
\begin{document}

\chapter{Related Work}%
\label{chap:related-work}
Research has shown that juvenile zebra finch birds learn to sing through a complex sensorimotor process \citep{Doupe1999-nm}. The birds progress from simple and highly variable sub-songs to fixed and distinct songs during learning. A direct motor pathway connects HVC to nucleus RA and controls the production of the song \citep{Gale2010-av}. HVC is also connected to RA through a second pathway, the anterior forebrain pathway (AFP). AFP controls the song variability, song learning and plasticity \citep{Doupe2005-iy}. In this pathway, HVC is connected to RA through area X, DLM, and LMAN in sequence. In this pathway, LMAN projects back to the area X in this circuit, thus forming a recursive loop. Research has shown that lesions from the LMAN area in this sensorimotor process stop adult birds from adapting spectral features of their song by operant conditioning \citep{Andalman2009-qt, Bottjer1984-mr}.

\section{Representation Learning Approaches}
Previous work by \cite{Singh_Alvarado2021-vr} investigated the variability of performance in male zebra finch birds in two conditions: solo practice (undirected singing) and performing (directed singing). They developed finchVAE, a Variational Auto-Encoder (VAE), to extract the latent representations of both neural and vocal data, and analyze the relationship between the latent embeddings. 

While only using undirected song data, we extended their work in two ways. First, with annotated syllables from \citep{Lorenz2022-vm}, we could focus on a finer scale of syllables instead of motifs, which is composed of several different syllables. Second, we adapted the original model on another type of neural recordings. The main challenge comes from the fact that the original paper used Calcium imaging as neural recordings, while our neural data was a time series collected from Neuropixels 1.0 \citep{Jun2017-uh} and then spike-sorted with Kilosort \citep{Pachitariu2016-mi}. VAEs were originally designed to deal with image data \citep{Kingma2013-es, Kingma2019-ng}, which is well-suited for the Calcium imaging data. For our time series data, we were particularly inspired by EnCodec from \cite{Defossez2022-lq}, a VAE structure for high-fidelity neural audio compression that utilizes quantization techniques and codebooks. The concept of codebooks originated from Vector Quantized VAE (VQ-VAE) proposed by \cite{van-den-Oord2017-rz} and was later put into work by \cite{Zeghidour2021-yp}. In VQ-VAE, the codebook is used to map continuous representations into discrete codes. We implemented EnCodec and devised different types of VAEs based on finchVAE to deal with spike data. 

Researchers have also explored contrastive learning methods to decode neural data \citep{Schneider2023-xu, Defossez2022-pq}. \cite{Schneider2023-xu} proposed the CEBRA method which processes time-series neural recordings, and generates latent embeddings for each time point. In the paper, the latent embeddings were used to decode neural responses in the visual cortex of mice that were watching a repeating video, and 95\% of the frames were correctly identified in the test dataset. We aim to use this method, and use different syllables to supervise the training. For human speech, \cite{Defossez2022-pq} proposed brainmagick, a contrastive learning based method to decode the speech heard from non-invasive brain recordings, such as Electroencephalogram (EEG) or Magnetoencephalography (MEG). They successfully aligned the neural responses with acoustic stimuli. For neural data, the best result came from the MEG data due to the high spatial accuracy of MEG. They first generated latent embeddings of non-invasive recordings through a Convolutional Neural Network. The speech embeddings were either from Mel Spectrogram, or taken from pre-trained models like wav2vec $2.0$ \citep{Baevski2020-rj}. The embeddings of two modalities were compared using Contrastive Language-Image Pre-Training (CLIP) loss. During our reproduction of brainmagick on the data of \cite{Gwilliams2022-xx}, the model correctly identified the word heard with 40\,\% accuracy from MEG data and had a Top 10 accuracy of 69\,\%.

In conclusion, the above representation learning methods show great potential for neural decoding. VAE excels at data compression and is generative in nature, while contrastive learning methods like CEBRA offer great decoding capability as well as fine temporal resolution. As a result, we combined VAE with contrastive learning in the last task.

There are several representation learning methods for neural data unexplored in this project: GP-VAE \citep{Gondur2023-eg}, MARBLE \citep{Gosztolai2023-by}, pi-VAE \citep{Zhou2020-om}, TiDeVAE \citep{Huang2024-wj}, etc.

\section{Deep Learning Approaches}
With recent advances in Natural Language Processing (NLP) and Machine Learning (ML) architectures, we aimed to use these new tools to deal with the spike train data. Spike train data is composed of spikes, or action potentials, which are discrete electrical signals generated by neurons as a result of underlying cellular and molecular activities. Spike train data is in nature very sparse. In Natural Language Processing (NLP), sparsity can be used to refer to either data sparsity or model sparsity. Data sparsity refers to large embedding sizes where only a few dimensions have significant values \citep{Farina2024-qh}. The most typical case would be One-Hot-Encodings, where each word is represented by a column vector of the length of the vocabulary, and all the positions are zero, except that the position for the current word has a one. Model sparsity refers to a neural network with a lot of empty weights, which could either increase the performance \citep{Child2019-od} or reduce performance by missing out important information \citep{Farina2024-qh}.

The sparsity in spike train data is similar to data sparsity described above. One drawback brought up by sparse spikes is that the activity patterns are encoded in a long `One-Hot-Encodings' where only a small part is meaningful. Transformer is not the best architecture to deal with long sequences, but there has been some work on this direction \citep{Li2023-ge}. In addition to this, transformers are resource-demanding. The computational complexity is quadratic to the sentence length, coming from the dot product calculation of the attention. 

Nevertheless, there are a few promising work done in this direction. \cite{Azabou2023-os} proposed a transformer-based framework Pre-training On manY neurOns (POYO). POYO treats the neural spikes as discrete tokens, and pretrains the model on large-scale recordings to improve the performance on decoding tasks. In this project, we also explored the tokenization method.

Although transformers can handle the sparse data to some extent, they are not the best models to use. With the quadratic bottleneck of transformers, it consumes a lot of time and computational resources to train and inference. \cite{Gu2023-vw} proposed Mamba, a Structured State-Space Model, that only has linear complexity. Mamba is specialized at dealing with long sequential data with minimum inference time and resource consumption. \cite{Dao2024-sp} proposed Mamba-2, a Structured State-Space Duality, which is more efficient than Mamba in training time. With Mamba models, analyzing sparse spike data by addressing data length seems to be more optimistic.

There are a few other methods. Binning is the most commonly used method to resolve this sparsity \citep{Labach2023-ib}. During binning, finer temporal information is lost. \citep{Tipirneni2021-ag} works on sparse and irregularly sampled data, where they used continuous value embeddings (CVE) to deal with such data. The CVE maps continuous values to high-dimensional spaces using a neural network. 

These works show various approaches to process neural data, and each approach offers unique advantage. In our project, we aimed to address the data sparsity issue. Our neural recordings were collected from Neuropixels 1.0 and spike sorted with Kilosort, which produces precise but sparse data. We explored NLP methods and several time series models including Mamba. We also investigated the potential of supervised representation learning methods, and use different VAEs to reconstruct our data.


% There are also potential in the application of aligning the neural  Large Langauge Models (LLM) had been used to align with fMRI data \citep{Caucheteux2023-lu, Doerig2022-ie}.



\subfilebibliography
\end{document}
