% ADD THIS HEADER TO ALL NEW CHAPTER FILES FOR SUBFILES SUPPORT

% Allow independent compilation of this section for efficiency
\documentclass[../CLthesis.tex]{subfiles}

% Add the graphics path for subfiles support
\graphicspath{{\subfix{../images/}}}

% END OF SUBFILES HEADER

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START OF DOCUMENT: Every chapter can be compiled separately
\begin{document}

\chapter{Experiment 5: VAE Latent Representations}%
% \label{chap:experiment_5}
\refstepcounter{experiment}  % This increments the counter
\label{exp:\theexperiment}
Our analysis focused on reconstructing the data using latent representations obtained from Variational Auto-Encoder (VAE). We attempted to capture patterns using both small segments as well as the whole syllable as units. We evaluated how well these generated neural latent embeddings preserve the original information through reconstruction and classification experiments. 

\section{Methods}
The following section describes three experiments conducted. We aimed to reconstruct both neural and vocal data, investigating the correlation between paired neural and vocal data, and investigate the possibility to generate vocalization from neural data.
\subsection{Generation Experiment}
We would like to investigate whether the patterns in the neural data could be captured by VAEs. In this experiment, we examined different methods to extract neural and vocal data separately to avoid encoding temporal information in the data. With the data obtained, we evaluated by visualizing the reconstructed data, investigating the latent embeddings, and training basic ML models on syllable classification tasks using the latent embeddings of either neural of vocal data obtained from the trained models. 

\subsection{Correlation Experiment}
For the second experiment, we applied linear warping to both neural data and spectrograms and mapped them to the maximum sequence length. With warped data, we aimed to investigate the correlation between latent embeddings generated, so that we can investigate the variance between neural and vocal data, as indicated in \cite{Singh_Alvarado2021-vr}. The results of warped data were also included for reconstruction visualization, latent embedding visualization, as well as syllable classifications for completeness.

\subsection{Cross-Modal Transfer: neuro2voc}
We investigated the possibility of generating speech data with neural data. We first created a joint VAE with a speech encoder, a neural encoder, as well as a speech decoder. We trained the VAE first using only speech data to obtain a satisfactory latent representation of speech data. Then, we froze the parameters of the speech encoder, and passed in both speech and neural data together. We used contrastive learning to generate the latent embeddings of neural data to make it align with the neural data as close as possible. The result is evaluated using reconstruction visualization, latent embedding visualization, as well as syllable classification. 

The quality of cross-modal transfer using a mapper function depends on not just the mapper function, but also the quality of reconstructed data in both modalities. The above method was chosen due to the unsatisfactory reconstruction quality of neural data.

\section{Data Preparation}
This following section describes the details of data preparation for binary neural data, binned neural data, and vocal data.
\subsection{Generation Experiment: Binary Neural Data}
% \subsubsection{Extracting Segments with Sliding Window} 
For binary data, we selected neural data during active vocalization. We divided the activity into $30$\,ms segments. A moving window of step size $10$\,ms was used. The choice of segment length was made due to computation constraints.

\subsection{Generation Experiment: Binned Neural Data}
For binned data, we used three approaches to extract data:
% \subsubsection{Extracting Segments with Sliding Window}
The first approach was similar to what we did to the binary neural data. We divided the data into $40$\,ms segments, and we used a moving window of $4$\,ms step size. Since some data were less than $40$\,ms, we padded all the data using real neighboring neural activities to increase the amount of the training data. The neural data extraction began $20$\,ms before each annotated syllable onset and ended earlier by the same amount.

% \paragraph{Trimming to Minimum Length}
The second approach is cutting the all the sequences according to the shortest data sequence. The smallest data sequence is $60$\,ms from syllable 6. However, $40$\,ms was first chosen to compare to the results from sliding windows above, and $80$\,ms were then chosen after visualizing the distribution of data, as $97.5$\,\% data are longer than or equal to $80$\,ms. We would like to investigate the effect of the extra $40$\,ms on the classification accuracy. The neural data extraction began $50$\,ms before each annotated syllable onset. 

% \paragraph{Padding to Maximum Length}
The third approach is padding all sequences according to the longest data with zeros. The longest sequence is $270$\,ms. The neural data extraction also began $50$\,ms before each annotated syllable onset. The padded data is excluded from discussion as it encodes temporal information. Nevertheless, the result is presented together with other types of extracting methods.

\subsection{Generation Experiment: Vocal Data}
For vocal data, spectrograms were provided in $128$ frequency bins. The approaches were the same as the approaches for Binned Neural Data above.

% \paragraph{Extracting Segments with Sliding Window} 
The first approach was the sliding window extraction approach for binned neural data, and we made sure that neural data and vocal data are correctly aligned in content and in length through visualization.

% \paragraph{Trimming to Minimum Length}
The second approach was similar to trimming binned neural data, $40$\,ms was chosen in order to align with the neural data as well as the length of segments from sliding windows.

% \paragraph{Padding to Maximum Length}
At the end, we padded all vocal data to the the maximum sequence, which was $224$\,ms. The neural data had a maximum sequence length of $260$\,ms. It was longer because (1) binning was employed on the original sample time data, (2) $50$\,ms were added to the original neural data before the onset. This had no effect for the generation experiment as only single modalities were used. Similar to padded neural data, the result for padded vocal data is excluded from textual discussion, but the visualizations were presented in the result section as well for completeness. 

\subsection{Correlation Experiment: Neural and Vocal Data}
We first normalized the time to $0$--$1$, and we created interpolation function for individual frequency bin. Then we sample at new time points. The vocal data were warped to a maximum sequence of length $224$\,ms. Neural data were warped similarly to a maximum sequence of length $260$\,ms.

\subsection{Cross-Modal Transfer: Neural and Vocal Data}
For cross-modal transfer, we used segmented neural and vocal data on three levels: $4$\,ms, $40$\,ms, and $80$\,ms. Both data were extracted such that they are aligned in sequence length. The neural data were binned with a window size of $4$\,ms to match the sampling frequency of vocal data ($250$\,Hz).

\section{Models and Evaluation}
The data were passed to the following models:
\begin{enumerate}
    \item 1D VAE: a VAE composed of Conv1D layers for neural data
    \item 2D VAE: a VAE composed Conv2D layers for vocal data, based on \cite{Singh_Alvarado2021-vr}
    \item NeuralEnCodec: a VAE aiming at lossless reconstruction of neural data, based on EnCodec of \cite{Defossez2022-lq}
    \item Joint VAE: a VAE composed of Conv1D for neural data and Conv2D for vocal data
\end{enumerate}

We first briefly describe the training progress through the observation of the reconstruction loss, Mean Squared Error (MSE). Then we included in the results section a simple visualization of the final reconstructed data using random data on the test set (an $8/2$ split was used).

After the models were trained, all data were combined and passed again into the encoder to obtain the latent representations. The latent representations (which were 32-dimensional) of all data were visualized in 2D using Uniform Manifold Approximation and Projection (UMAP) and color-coded with syllable types. The latent presentations were also passed to SVM, RF, and XGBoost for syllable classification tasks. The classification accuracies of neural data were compared to CEBRA under the same conditions.

The warped neural and vocal latent representations were used to perform a Canonical Correlation Analysis. The first few Canonical Components were visualized and we reported the correlation coefficient. 
% \paragraph{Cross-Modal Transfer}
% We evaluate these mappings using test samples and visualized the reconstructed output.

\section{Visualization Results}
% \begin{table}[htbp]
%     \centering
%     \begin{tabular}{|l|cc|c|}
%         \hline
%         {Model Name} & \multicolumn{2}{c|}{Neural} & Speech \\
%         \cline{2-4}
%          & Padded & Warped & Warped \\
%         \hline
%         2D VAE & 0.182 & 0.221 & \textbf{1.691} \\
%         \hline
%         1D VAE & 0.114 & \textbf{0.033} & 21.984 \\
%         \hline
%     \end{tabular}
%     \caption{MSE for Different VAE Architectures and Data Types}
%     \label{tab:vae_losses}
% \end{table}

\subsection{Generation Experiment: Visualization\footnote{All the neurons used in the visualization in this experiment were ranked by the depth to the tip, where N0 is the closest to the probe tip.}}
In all experiments, all the neural models showed convergence early and the loss were high, suggesting more training data would be needed. Since 2D VAE and NeuralEnCodec repeatedly showed insatisfactory reconstruction results on binary and warped neural data, they were not used for further analysis in generation experiments for neural data. Some reconstruction visualizations were included in the results section for completeness, but only 1D VAE was used for neural data for further processing as well as detailed evaluation.

% The original data used for construction could be different, as it was difficult to keep track of data with sliding windows and held-out data sets. This section served only used for visualization purposes. Quantitatively comparing the reconstruction across different data processing techniques was not an objective.

\subsubsection{Binary Neural Data - Segments of 30\,ms}
% \paragraph{1D VAE}
1D VAE showed little amount of learning and produced poor reconstruction quality. Figure~\ref{fig:binary_1d} shows the original and the reconstructed binary neural data. Different scales are used for the reconstructed neural data, because the results are visually imperceptible in the reconstructed data using the original scale.
% 41.0
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/comparison_binary_1dvae_3.pdf}
    \caption{Reconstruction of binary neural data -- 1D VAE}
    \label{fig:binary_1d}
\end{figure}
% \paragraph{NeuralEnCodec}
NeuralEnCodec reached the best result early, and showed limited learning progress. The reconstruction in Figure~\ref{fig:binary_encodec} shows temporal smoothing patterns. Instead of reproducing the precise spikes, the model generates an averaged spiking pattern across time. Similarly, different scaled were used in the figure, because the results were visually imperceptible in the reconstructed data using identical scales.
% 13.4
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/comparison_binary_encodec_3.pdf}
    \caption{Reconstruction of binary neural data -- NeuralEnCodec}
    \label{fig:binary_encodec}
\end{figure}

\subsubsection{Binned Neural Data - Segments of 40\,ms}
% \paragraph{1D VAE} 
Figure~\ref{fig:neural_segmented} shows the reconstruction results of 1D VAE trained on binned neural data. The results were not good enough, and some basic patterns in the original data were not captured. A binning window of $4$\,ms was used.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/1D_VAE_neural_segments.pdf}
    \caption{Reconstruction of binned neural data -- segments of 40\,ms}
    \label{fig:neural_segmented}
\end{figure}

\subsubsection{Binned Neural Data - Trimmed to 40\,ms and 80\,ms}
% \paragraph{1D VAE} 
Figure~\ref{fig:neural_trimmed_40} and Figure~\ref{fig:neural_trimmed} show the reconstruction results using trimmed neural activities at syllable onsets using 1D. The binning window is $10$\,ms.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/1D_VAE_neural_trimmed_40ms.pdf}
    \caption{Reconstruction of binned neural data -- trimmed to 40\,ms}
    \label{fig:neural_trimmed_40}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/1D_VAE_neural_Trimmed.pdf}
    \caption{Reconstruction of binned neural data -- trimmed to 80\,ms}
    \label{fig:neural_trimmed}
\end{figure}

\subsubsection{Binned Neural Data - Padded to 270\,ms}
Figure~\ref{fig:neural_padded} shows the reconstruction results using 1D VAE. Each neural data is padded to the maximum sequence length. Data were binned using a $10$\,ms window.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/1D_VAE_neural_Padded.pdf}
    \caption{Reconstruction of binned neural data -- padded to 270\,ms}
    \label{fig:neural_padded}
\end{figure}

\newpage
\subsubsection{Vocal Data - Segments of 40\,ms}
Figure~\ref{fig:vocal_segmented} shows the reconstruction results for 10 random vocalization segments using 2D VAE. Each segment is extracted from sliding windows and lasts for $40\,\text{ms}$.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/2d_vae_vocal_segments.pdf}
    \caption{Reconstruction of segmented vocal data -- segments of 40\,ms}
    \label{fig:vocal_segmented}
\end{figure}

\newpage
\subsubsection{Vocal Data - Trimmed to 40\,ms}
Figure~\ref{fig:vocal_trimmed} shows the reconstruction results from 2D VAE of random vocalization syllables using 2D VAE, from 2 to 8. Only the first $40\,\text{ms}$ was kept for each annotated syllable.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/2d_vae_vocal_trimmed.pdf}
    \caption{Reconstruction of trimmed vocal data -- trimmed to 40\,ms}
    \label{fig:vocal_trimmed}
\end{figure}

\newpage
\subsubsection{Vocal Data - Padded to 224\,ms}
Figure~\ref{fig:vocal_padded} shows the reconstruction results from 2D VAE of random vocalization syllables. Each syllable was padding to $224\,\text{ms}$.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/2d_vae_vocal_padded.pdf}
    \caption{Reconstruction of padded vocal data -- padded to 224\,ms}
    \label{fig:vocal_padded}
\end{figure}

\newpage
\subsection{Correlation Experiment: Visualization}
\subsubsection{Binned Neural Data - Warped}
The reconstruction loss from 1D VAE was consistently decreasing. Figure~\ref{fig:binned_1d} showed the original neural data on the left, and the reconstructed neural data on the right from 1D VAE. Neural data showed satisfactory reconstruction results upon visual inspection.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/VAE_neuro.pdf}
    \caption{Reconstruction of warped and binned neural data with 1D VAE}
    \label{fig:binned_1d}
\end{figure}
NeuralEnCodec reached the best result as early as Epoch 12. The result is shown below. Similarly to its performance on binary data shown in Figure~\ref{fig:binary_encodec}, NeuralEnCodec captured some characteristics of data, but smoothed out the data over time.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/comparison_binned_encodec_1.pdf}
    \caption{Reconstruction of binary neural data with NeuralEnCodec}
    \label{fig:binned_encodec}
\end{figure}

\newpage
\subsubsection{Vocal Data - Warped}
Figure~\ref{fig:whole_motif_average} shows original and reconstructed data for all different syllables using 2D VAE. The visualization was averaged over all warped samples. \footnote{Individual visualizations of averaged data and random samples are included in Appendix~\ref{appendix:VAE}}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/2d_vae_vocal_warped_average.pdf}
    \caption{Reconstruction of warped vocal data}
    \label{fig:whole_motif_average}
\end{figure}

\newpage
\subsection{Cross-Modal Transfer: neuro2voc}
Figure~\ref{fig:neuro2voc_40ms} shows the generated vocal data from neural data inputs. The neural data were obtained with sliding windows with 40\,ms\footnote{Visualization for 80\,ms is included in Appendix~\ref{appendix:VAE}} sequence length.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/neural2vocal_40ms.pdf}
    \caption{Generate 40\,ms vocalization from 40\,ms neural data}
    \label{fig:neuro2voc_40ms}
\end{figure}

\section{Latent Representation Evaluations}
\subsection{Visualization of Latent Representations}
Figure~\ref{fig:umap_all_1} shows the UMAP projection of latent representations obtained from the generation experiment. Figure~\ref{fig:umap_neuro2voc} shows the result from the cross-modal transfer experiment. The neural representations only showed clustering effect after aligning to pre-trained vocal representations through contrastive learning. 
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/UMAP_segmented_neural_1dvae.pdf}
        \caption{Neural 40\,ms segments}
        \label{fig:umap_segmented_neural}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/UMAP_trimmed_40ms_neural_1dvae.pdf}
        \caption{Neural - trimmed to 40\,ms}
        \label{fig:umap_trimmed_neural_40}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}   
        \includegraphics[width=\linewidth]{images/UMAP_trimmed_neural_1dvae.pdf}
        \caption{Neural - trimmed to 80\,ms}
        \label{fig:umap_trimmed_neural_80}
    \end{subfigure}

    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/UMAP_segmented_vocal_1dvae.jpg}
        \caption{Vocal 40\,ms segments}
        \label{fig:umap_segmented_vocal}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/UMAP_trimmed_vocal_1dvae.pdf}
        \caption{Vocal - trimmed to 40\,ms}
        \label{fig:umap_trimmed_vocal}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \phantom{\includegraphics[width=\linewidth]{images/UMAP_segmented_vocal_1dvae.jpg}}
    \end{subfigure}

    \caption{UMAP visualization of latent representations from generation experiment}
    \label{fig:umap_all_1}
\end{figure}

\begin{figure}[H]
   \centering
   \small
   
   \begin{subfigure}{0.32\linewidth}
       \includegraphics[width=\linewidth]{images/UMAP_4ms_neural_joint.jpg}
       \caption{Neural - 4\,ms segments}
       \label{fig:umap_4ms_neural}
   \end{subfigure}
   \hfill
   \begin{subfigure}{0.32\linewidth}
       \includegraphics[width=\linewidth]{images/UMAP_40ms_neural_joint.jpg}
       \caption{Neural - 40\,ms segments}
       \label{fig:umap_40ms_neural}
   \end{subfigure}
   \hfill
   \begin{subfigure}{0.32\linewidth}
       \includegraphics[width=\linewidth]{images/UMAP_80ms_neural_joint.jpg}
       \caption{Neural - 80\,ms segments}
       \label{fig:umap_80ms_neural}
   \end{subfigure}

   \caption{UMAP visualization of latent representations from neuro2voc experiment}
   \label{fig:umap_neuro2voc}
\end{figure}

\begin{figure}[H]
   \centering
   \small
   \begin{subfigure}{0.32\linewidth}
       \includegraphics[width=\linewidth]{images/UMAP_padded_neural_1dvae.pdf}
       \caption{Neural - padded to 270\,ms}
       \label{fig:umap_padded_neural}
   \end{subfigure}
   \hfill
   \begin{subfigure}{0.32\linewidth}
       \includegraphics[width=\linewidth]{images/UMAP_warped_neural_1dvae.pdf}
       \caption{Neural - warped to 270\,ms}
       \label{fig:umap_warped_neural}
   \end{subfigure}
   \hfill
    \begin{subfigure}{0.32\linewidth}
        \phantom{\includegraphics[width=\linewidth]{images/UMAP_80ms_neural_joint.jpg}}
    \end{subfigure}

   
   \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/UMAP_padded_vocal_1dvae.pdf}
       \caption{Vocal - padded to 224\,ms}
       \label{fig:umap_padded_vocal}
   \end{subfigure}
   \hfill
   \begin{subfigure}{0.32\linewidth}
       \includegraphics[width=\linewidth]{images/UMAP_warped_vocal_1dvae.pdf}
       \caption{Vocal - warped to 224\,ms}
       \label{fig:umap_warped_vocal}
   \end{subfigure}
   \hfill
    \begin{subfigure}{0.32\linewidth}
        \phantom{\includegraphics[width=\linewidth]{images/UMAP_80ms_neural_joint.jpg}}
    \end{subfigure}

    \caption{UMAP visualization of latent representations from correlation experiment}
   \label{fig:umap_all_2}
\end{figure}

Figure~\ref{fig:umap_all_2} shows clear patterns of clustering in both vocal representations and neural representations, due to encoded temporal information during warping.

\subsection{Classification with Latent Representations}
The latent representations of both neural and vocal data were used to perform syllable classification tasks. From Table~\ref{tab:classification_accuracies}, the vocal data achieved high decoding accuracy, and thus conveyed the syllable information better. 

For the neural data, the padded and warped data in light gray have highest performance but encode temporal information. Trimmed neural data of length $40$\,ms, obtained at period $[-50\,\text{ms}, -10\,\text{ms}]$, achieved an accuracy of 49.9\,\% with SVM. This is better than the 27.7\,\% accuracy segmented neural data of the same length, which were obtained using sliding windows. The decoding efficiency of segmented neural data improved to 72.5\,\% after using contrastive learning to align with pre-trained vocal data.

\begin{table}[H]
\centering
\small
\caption{Syllable classification accuracies (\%)}
\begin{tabular}{lllrrccc}
\toprule
Data & Experiment & Method & \shortstack{Window\\(ms)} & \shortstack{Duration\\(ms)} & SVM & RF & XGBoost\\
\midrule
\multirow{8}{*}{\shortstack{Neural\\Data}} 
& Generation & Segmented & 4 & 40 & 27.7 & 25.8 & 26.8\\
& Generation & Trimmed & 10 & 40 & 49.9 & 45.5 & 48.1\\
& Generation & Trimmed & 10 & 80 & 53.5 & 45.2 & 50.1\\
& {\color{lightgray}Generation} & {\color{lightgray}Padded} &  {\color{lightgray}10} & {\color{lightgray}270} & {\color{lightgray}96.4} & {\color{lightgray}93.5} & {\color{lightgray}93.5}  \\
& {\color{lightgray}Correlation} & {\color{lightgray}Warped} &  {\color{lightgray}10} & {\color{lightgray}270} & {\color{lightgray}75.6} & {\color{lightgray}63.6} & {\color{lightgray}67.8}\\
& Contrastive & Segmented & 4 & 4 & 35.1 & 32.4 & 34.1\\
& \textBF{Contrastive} & \textBF{Segmented} & \textBF{4} & \textBF{40} & \textBF{72.5} & \textBF{65.2} & \textBF{69.3}\\
& \textBF{Contrastive} & \textBF{Segmented} & \textBF{4} & \textBF{80} & \textBF{91.0} & \textBF{84.4} & \textBF{89.6}\\
\midrule
\multirow{4}{*}{\shortstack{Vocal\\Data}}
& Generation & Segmented & 4 & 40 & 79.7 & 68.5 & 73.1\\
& Generation & Trimmed & 4 & 40 & 98.8 & 98.0 & 97.8\\
& {\color{lightgray}Correlation}& {\color{lightgray}Padded} & {\color{lightgray}4} &{\color{lightgray}224} & {\color{lightgray}96.4} &{\color{lightgray}93.5} & {\color{lightgray}93.5}\\
& {\color{lightgray}Correlation}& {\color{lightgray}Warped} & {\color{lightgray}4} &{\color{lightgray}224} & {\color{lightgray}99.4} &{\color{lightgray}99.3} & {\color{lightgray}98.8}\\
\bottomrule
\end{tabular}
\label{tab:classification_accuracies}
\end{table}




% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{images/compare-VAE.png}
%     \caption{Comparison of model performance}
%     \label{fig:final}
% \end{figure}

\section{Canonical Component Analysis}
Figure~\ref{fig:CCA} shows that the first two pairs of Canonical Components are strongly correlated ($R > 0.6$). This indicates that the variations among neural data is correlated with the variations among vocal data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/CCA_onlyCorr.png}
    \caption{Result from canonical component analysis}
    \label{fig:CCA}
\end{figure}

% \subsection{Cross Modal Transfer}
% Due to the imperfect of the mapping function (Accuracy: 0.50 for neural to vocal mapping), the reconstructed neural or spectrogram data failed to capture the characteristics of the original data.

% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/neuro2voc7.png}
%         \caption{Neural to Spectrogram Transfer}
%         \label{fig:neural_to_spec}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/voc2neuro7.png}
%         \caption{Spectrogram to Neural Transfer}
%         \label{fig:spec_to_neural}
%     \end{subfigure}
%     \caption{Bidirectional Transfer Results. For each subgraph, the left graph shows the corresponding data expected, the right graph shows the predicted and reconstructed data from the other modality. \color{red}{spectrogram 1 upside down}}
%     \label{fig:transfer_results}
% \end{figure}
\newpage
\section{Discussion}

\subsection{Achievements of Latent Representations from VAE}
We showed different ways to process and reconstruct vocal and neural data, as well as the corresponding qualities of reconstruction. With the models and data from this project, the reconstruction quality was high for all vocal data and warped neural data, and the quality was acceptable for neural data in some modals. 

In the neuro2voc experiment, we combined contrastive learning with generative models, and successfully reconstructed vocal data using neural data. After contrastive learning methods, the decoding accuracy of segmented $40$\,ms data were enhanced from 27.7\,\% to 72.5\,\%. The highest decoding accuracy was $91.0$\,\% from neural data of length $80$\,ms, where the data segments were obtained with a sliding window.

\subsection{Limitations}
During training, the models showed different patterns for vocal and neural data. Training on vocal data exhibited clear learning progress in terms of loss reduction. In contrast, training on neural data models reached plateaux very early. This indicates two things that are essential for better reconstruction of neural data: (1) more training data, and (2) a more sophisticated VAE structure. 

\subsection{Future Directions}
A larger dataset of neural recordings is essential to enhance the model performance. We also observed that the 1D architecture performed better than the 2D model on neural data during training. The 2D VAE successfully captured relationships in spectrograms. However, it failed to detect connections among neuron populations. As EEGNet showed high decoding accuracy in Experiment~\ref{exp:3}, its structure could also be integrated into current VAE structure. These observations point to a clear direction for VAE on binned spike data.

% However, the firing frequency only provides the decoding accuracy up to 0.75,


% self-defined macro: include bibliography even when compiling a single chapter
\subfilebibliography
\end{document}
