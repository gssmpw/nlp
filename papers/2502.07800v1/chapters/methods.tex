% ADD THIS HEADER TO ALL NEW CHAPTER FILES FOR SUBFILES SUPPORT

% Allow independent compilation of this section for efficiency
\documentclass[../CLthesis.tex]{subfiles}

% Add the graphics path for subfiles support
\graphicspath{{\subfix{../images/}}}

% END OF SUBFILES HEADER

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START OF DOCUMENT: Every chapter can be compiled separately
\begin{document}

\chapter{Methodology}%
\label{chap:methods}

\section{Mamba}
\subsection{Selective State Space Model}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/ssm.png}
    \caption{Selective State Space Model}
    \label{fig:ssm}
\end{figure}
\begin{align*}
    h_t &= g(\mathbf{A}_t h_{t-1} + \mathbf{B}_t x_t) \\
    y_t &= \mathbf{C}_t^T h_t
\end{align*}
\begin{align*}
\mathbf{A}_t \in \mathbb{R}^{N,N} &: \text{state to state transition matrices}\\
\mathbf{B}_t \in \mathbb{R}^N &: \text{input to state transition vector}\\
\mathbf{C}_t \in \mathbb{R}^N &: \text{state to output transition vector}\\ 
g &: \text{activation function}
\end{align*}
A selective state space model (SSM) is defined above, where $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ are the three parameters of the model. Mamba \citep{Gu2023-vw} uses a structured SSM, which is a selective SSM with $\mathbf{A}_t$ being a diagonal matrix. 

\subsection{State Space Duality}
Mamba-2 \cite{Dao2024-sp} uses the state space duality (SSD) structure. SSD is built on SSM with one specification:
$$
    \mathbf{A}_t = a_t\mathbf{I}
$$
$$
a_t \in \mathbb{R}: \text{a learnable scalar} 
$$
This linear mode is also called the SSM mode, which is excellent at inference time due to its computational efficiency. This scalar-identity structured model can be expanded to a quadratic mode, which enables matrix multiplication and dramatically improves the training process. The quadratic mode above can be viewed as:
$$
    y = \mathbf{M} x
$$
$\mathbf{M}$ is a semi-separable matrix similar to causal attention, where $\mathbf{M}_{ij} = 0$ for $i\,<\,j$ and otherwise
$$
    \mathbf{M}_{ij} = \prod_{k=j+1}^{i} a_k \cdot \mathbf{C}_i^\top \mathbf{B}_j
$$

\subsection{Mamba and Mamba-2}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/mamba_structure.png}
    \caption{Mamba and Mamba-2 structures}
    \label{fig:mamba_structure}
\end{figure}
Mamba is built upon SSM and uses sequential Mamba block, where $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ are calculated sequentially. Mamba-2 is built upon SSD and uses parallel Mamba block, where $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ are calculated in parallel. Inspired by attention-like architectures, such parallelization accommodates tensor parallelism.

% \subsection{Structured Attention Interpretation}
% The model can be viewed through structured masked attention:
% \begin{align}
%     y_i &= \sum_{j=1}^{i} L_{ij} C_i^\top B_j x_j \\
%     L_{ij} &= \prod_{k=j+1}^{i} a_k
% \end{align}
% where $L_{ij}$ represents cumulative attentional weighting.

% This formulation unifies state space modeling with attention mechanisms while maintaining linear-time computation complexity through its semiseparable matrix structure.


\section{SHapley Additive exPlanations Interaction Values}
SHapley Additive exPlanations (SHAP) \citep{Lundberg2017-oo} are used to evaluate the contribution of each feature to model prediction using game theory concepts. SHAP interaction values measure how much the syllable prediction result changes when two neurons' activities are considered together, on top of their individual contributions. A positive pairwise interaction value indicates two neurons work together, and their joint activation provides more information about the syllable than simply adding up their individual contributions. A negative interaction suggests that such joint effect is less than the sum of individual contributions of two neurons, meaning some redundancy in the encoding of information.


\section{Support Vector Machine}
Support Vector Machine (SVM) is a supervised machine learning algorithm \citep{Boser1992-fa} used extensively in this project to establish a benchmark for classification in order to be compared to by all later more complicated methods.

SVM creates an optimal hyperplane by maximizing the margin between classes. The optimal hyperplane is selected by maximizing the margin between classes. A hyperplane can be defined as a homogeneous non-trivial linear equation. In a space of $R^n$, a hyperplane has dimensionality of $R^{n-1}$. The closest data points to the hyperplane are called the support vectors, and these vectors determine the decision boundary for classification.

Among multiple metrics generated from classification tasks (accuracy, F1, precision, sensitivity, etc.) only overall accuracies are used in this project for simplicity. 

$$
\min_{w, b, \xi} \frac{1}{2}\Vert w\Vert^2 + C\sum_{i=1}^n \xi_i
$$
s.t.:
$$
y_i\left(w^\text{T} x_i + b\right) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i=1,2,\ldots,n
$$
\begin{align*}
w &: \text{the weight vector} \\
b &: \text{the bias} \\
\xi_i &: \text{the slack variables that allow for misclassification} \\
&\quad \text{and encode Hinge Loss}\\
C &: \text{the regularization parameter}
\end{align*}

SVMs are conducted with a linear kernel and a Radio Basis Function (RBF) kernel.
The linear kernel is defined as 
$$
K(x, x') = \langle x, x'\rangle
$$
The RBF kernel in scikit-learn \citep{scikit-learn} is defined as
$$
K(x, x') = \exp(-\gamma \Vert x - x'\Vert_2^2)
$$
\begin{align*}
\gamma &: \text{the influence range of each training example} \\
\Vert x - x'\Vert_2^2 &: \text{the squared 2-Norm (Euclidean Norm)} 
\end{align*}

For this project, we evaluated both linear and RBF kernels using grid search with scikit-learn. The parameters were: (1) $C$ parameter: [$0.1$, $1$, $10$]
(2) gamma parameter (for RBF kernel): $[0.01, 0.1, \text{`scale'}]$

\section{TF-IDF}
Term Frequency-Inverse Document Frequency (TF-IDF) is a common feature in the domain of NLP. This feature combines two features: Term Frequency (TF) and Inverse Document Frequency (IDF). TF refers to how often a term appears in a document. IDF refers to how unique this term is across all document. When a term frequently appears in a document but rarely in other documents, the weight of this term would increase. The formula is written as:
$$w_{t,d} = tf_{t,d} \times \log(\frac{N}{df_t})$$
\begin{align*}
w_{t,d} &: \text{the weight of term $t$ in document $d$}\\
tf_{t,d} &: \text{the occurrence frequency of term $t$ in document $d$}\\
N &: \text{the total number of documents}\\
df_t &: \text{the number of documents containing term $t$}
\end{align*}
In this project, the same concept has been transferred to neural data. The term refers to the neuron, and the document refers to the data sample with a specific labels. Just like in NLP where the documents can be classified into Law, Medicine, News, etc., the data samples used in this project can be classified into different syllables.

\section{Transformers}
Transformers are introduced by \cite{Vaswani2017-ga} and have since established new performance benchmarks in Artificial Intelligence. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{images/transformers.pdf}
    \caption{Transformers architecture}
    \label{fig:transformers}
\end{figure}

The Positional Encoding (PE) are added to the input embeddings. Sinusoidal positional encoding is a basic type of PE introduced in the original paper.
\begin{align*}
PE_{(pos,2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) & pos \in [0, n], i \in [0, d_{model}/2] \\
PE_{(pos,2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) & pos \in [0, n], i \in [0, d_{model}/2-1]
\end{align*}
\begin{align*}
\text{pos} &: \text{the position in the input sequence} \\
d_{\text{model}} &: \text{the dimensionality of the embedding vectors} 
\end{align*}
The word embeddings has a size of $d_{\text{model}}$. The Positional Encoding (PE) matches $d_{\text{model}}$.  In the original transformers \citep{Vaswani2017-ga}, $d_{model} = 512$. In OpenAI Generative Pre-trained Transformer (GPT) 2 \citep{Radford2019-gx} that we used in experiment 2, $d_{model} = 768$.

\begin{align*}
\text{score}(x_i, x_j) &= \frac{q_i \cdot k_j}{\sqrt{d_k}}\\
[\alpha_{ij}] &= \text{softmax}(\text{score}(x_i, x_j)) \quad \forall j \leq i\\
a_i &= \sum_{j\leq i} \alpha_{ij}v_j
\end{align*}
The calculation for masked attention used in GPT is shown above. The attention weights are calculated by doing pair-wise dot product between key and query, and the scores are divided by $\sqrt{d_k}$ to reduce the variance from dot products. Then softmax is applied to the scores, and creates attention weights. The attention weights are multiplied by values and produces the attention output. In Multi-Head Attention (MHA), the outputs of individual heads are stacked. 


\begin{align*}
x &= \text{MHA}(x) + \alpha x, \quad \alpha \in [0,1]\\
\text{LayerNorm}(x) &= \gamma \frac{x - \mu}{\sigma} + \beta
\end{align*}
\begin{align*}
\alpha&: \text{scaling factor for the residual connection}\\
\mu&: \text{mean of the input}\\
\sigma&: \text{standard deviation of the input}\\
\gamma \text{ and } \beta&: \text{learnable parameters}
\end{align*}
The add operation is called the residual connection. It ensures that the transformed output is close to the input shape. The normalization is applied after every layers to control the value ranges. The layer norm is the most popular normalization. In layer normalization, the activations are normalized (demeaned and then divided by the standard deviation) over the feature dimensions ($d_\text{model}$) for each example in the batch.

\begin{align*}
x' &= \text{FFN}(\text{LayerNorm}(x)) + \alpha \text{LayerNorm}(x),  \quad \alpha \in [0,1]\\
\text{LayerNorm}(x') &= \gamma \frac{x' - \mu}{\sigma} + \beta
\end{align*}
Then, a feed-forward network (FFN) is applied. The result is passed to another residual connection and norm layer.

% \section{LSTM}
% \section{State-Space Model: Mamba}
% \section{Convolutional Neural Networks}
% \subsection{EEGNet}
\section{Variational Auto-Encoders}
The Variational Auto-Encoder (VAE) is a generative model introduced by \cite{Kingma2013-es}. VAE transforms the input data into a continuous latent space. It includes an encoder and a decoder. The encoder $q_\varphi(z|x)$ learns to approximate the posterior distribution of latent variables $z$ using input data $x$. The decoder $q_\theta(x|z)$ learns to reconstruct the input data $x$ using samples from this latent distribution. Typically, a standard normal distribution is used as the prior distribution $p(z)$.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/VAE.png}
    \caption{VAE architecture}
    \label{fig:vae}
\end{figure}

\begin{align*}
\mathcal{L}_{\text{VAE}} &= \mathcal{L}_{\text{MSE}}(x, \hat{x}) + \text{KL}(q_\varphi(z|x) \| p(z)) \\[1em] 
q_\varphi(z|x) &= \mathcal{N}(z; \mu_\varphi(x), \sigma^2_\varphi(x)) \\
p(z) &= \mathcal{N}(0, I) \\[0.5em]
\varphi, \theta &: \text{learnable parameters}
\end{align*}

The loss function of VAE combines two terms: (1) the reconstruction loss, and (2) Kullback–Leibler divergence (KL divergence) loss. The first term is the loss function. It uses Mean Squared Error (MSE) to ensure that the output data matches the input data. The second term is a regularization term (KL divergence) that pushes the latent distribution toward a standard normal prior. The encoder produces two learnable parameters for the latent space: mean $\mu_\varphi(x)$ and variance $\sigma_\varphi^2(x)$.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/VAE-2.png}
    \caption{VAE reparameterization}
    \label{fig:vae-bp}
\end{figure}

Back propagation is impossible through the sampling process. To enable back propagation, VAE employs a reparameterization trick. Instead of directly sampling from $q_\phi(z|x)$, VAE samples $\epsilon \sim \mathcal{N}(0, I)$ and then computes $z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon$, where $\odot$ refers to element-wise multiplication. By applying this reparameterization trick, VAE successfully converts the previously random sampling into deterministic and differentiable nodes of encoder parameter $\phi$, input data $x$, as well as random noise $\epsilon$.  As a result, the latent embedding z can be expressed as
\begin{align*}
    z = g(\phi, x, \epsilon)
\end{align*}

% \subsection{EnCodec}
% \section{Contrastive Learning}
% \subsection{Cebra}
% \section{Canonical Correlation Analysis}
% \section{Mamba}
% \section{TF-IDF}

% self-defined macro: include bibliography even when compiling a single chapter
\subfilebibliography
\end{document}
