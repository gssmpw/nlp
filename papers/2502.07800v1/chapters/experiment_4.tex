% ADD THIS HEADER TO ALL NEW CHAPTER FILES FOR SUBFILES SUPPORT

% Allow independent compilation of this section for efficiency
\documentclass[../CLthesis.tex]{subfiles}

% Add the graphics path for subfiles support
\graphicspath{{\subfix{../images/}}}

% END OF SUBFILES HEADER

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START OF DOCUMENT: Every chapter can be compiled separately
\begin{document}

\chapter{Experiment 4: CEBRA Latent Embeddings}%
% \label{chap:experiment_4}
\refstepcounter{experiment}  % This increments the counter
\label{exp:\theexperiment}
Consistent EmBeddings of high-dimensional Recordings using Auxiliary variables (CEBRA) is a contrastive learning method to compress time series data and reveal its hidden structures \citep{Schneider2023-xu}. The model can be self-supervised by temporal adjacency of neural activity, or supervised by behavioral labels, or both. CEBRA consistently generates interpretable latent embeddings in (1) synthesized spikes \citep{Zhou2020-om}, (2) hippocampal place encoding \citep{Grosmark2016-qy}, (3) somatosensory cortical activity during reaching \citep{Chowdhury2019-ev}, and (4) visual cortical responses to videos \citep{de-Vries2020-ld}. We expect CEBRA to reveal the underlying structure of LMAN activity during vocalization as well. With annotated syllables, the neural data can be supervised by a simple categorical system of $7$ syllable types. Thus we investigated the potential of latent embeddings produced by CEBRA for syllable classification.

\section{Methods}
\subsection{Data} 
We extracted the neural activity during the vocalization periods, and binned the data using a $10$\,ms window. Although this method interrupted continuous neural signals, it allowed us to focus on the neural activities of interest. Prior to concatenating, we shuffled the data at the syllable level to break the natural sequence of song. 

\subsection{Models}
CEBRA learns patterns in five ways: 
\begin{enumerate}
    \item over time
    \item over label
    \item over label, shuffled on a label level so that the labels are not in original order
    \item over label, shuffled on a sample level
    \item hybrid, jointly over time and label
\end{enumerate}
We focused on exploring the hyperparameter `offset' which defines the distance (in time) between positive pairs \citep{Schneider2023-xu}. A positive pair is composed of two timepoints separated by the offset value. Time points outside this interval become negative pairs. For example, an offset of $10$ refers to the fact that time point $t\pm10$ form positive pairs, and time point outside of this period will be used as negative pairs. Lower offset values lead to more stable learning, since temporally adjacent neural activities are more correlated. Higher offset values might lead to less stable learning, as the temporal correlation between neural activities decreases with longer time intervals. 

In the experiment, the dimension of latent embedding is set to 3, the learning rate is set to $3e-4$, the number of epochs is dynamically adjusted between 5000 to 50000 by observing the training loss. Train-test split was 8--2. The model was trained on the training set and we obtained the final embeddings by passing all data (including the test set) through the trained CEBRA model.

\subsection{Evaluation}
Our evaluation process consisted of three steps. First, we visualized trained embeddings\footnote{Only the third model is visualized in the experiment. Appendix~\ref{appendix:CEBRA} shows the visualization for four other models, their training loss, and data distribution.} to evaluate how data points from different labels distributed in the embedding space. 

Second, we performed single time point classification. We passed individual CEBRA embeddings to SVM, RF, and XGBoost, and compared the best result to benchmarks from Experiment~\ref{exp:1} using total count of spikes over $10$\,ms. By doing this, we ensured that we were comparing the same temporal resolution of 10\,ms. The performance of all the models, including the ones in the Appendix, were included in the classification accuracy table.

Finally, the latent embeddings were evaluated using sequence classification. For this, we combined the embeddings into sequences, and applied CNN-LSTM to these sequences. Identical window and parameters of data processing from Experiment~\ref{exp:3} were used in the comparison. The CNN-LSTM was picked due to its ease of implementation and its potential for further improvement with enhanced input features.

\section{Results}
\subsection{Visualization of CEBRA latent embeddings}
Figure~\ref{fig:cebra_3d} visualizes the latent embeddings of all binned data, generated by CEBRA model trained on shuffled syllables.

The visualization shows excellent clustering with parameter offset 10 and offset 20, where the latent embeddings form a circular pattern. The spatial arrangement of neural data in the embedding space preserves their original sequence order in songs, despite the fact that the syllables are shuffled. The boundaries between different syllables appear to be thin and discontinuous. In comparison, unshuffled data in Appendix~\ref{appendix:CEBRA} shows more distinct boundaries between syllable regions.

\begin{figure}[H]
    \centering
    \includegraphics[height=\textheight]{images/cebra_embedding_offsets_comparison.pdf}
    \caption{CEBRA embeddings trained with shuffled behavior labels}
    \label{fig:cebra_3d}
\end{figure}


\begin{sidewaystable}[htbp]
\centering
\caption{Model performance across different CEBRA parameters}
\begin{tabular}{lccccccccc}
\toprule
\multirow{2}{*}{\textbf{Accuracy (\%)}} & \multicolumn{3}{c}{\textbf{Offset 1}} & \multicolumn{3}{c}{\textbf{Offset 10}} & \multicolumn{3}{c}{\textbf{Offset 20}} \\
\cmidrule(l){2-4}
\cmidrule(l){5-7}
\cmidrule(l){8-10}
& SVM & RF & XGBoost & SVM & RF & XGBoost & SVM & RF & XGBoost \\
\midrule
Time & 27.4 & 27.2 & 26.2 & 26.5 & 25.8 & 25.2 & 25.3 & 25.5 & 24.3 \\
Behavior & 36.2 & 35.7 & 34.6 & 69.2 & 70.4 & 69.8 & 75.3 & 75.7 & 75.2 \\
\textBF{Behavior shuffled over labels} & 36.3 & 35.6 & 34.5 &\mathBF{ 64.0} & \mathBF{63.7} & \mathBF{64.0} & \mathBF{64.0} & \mathBF{64.2} & \mathBF{64.2} \\
Behavior shuffled over samples & 25.3 & 25.6 & 24.1 & 24.7 & 25.2 & 23.8 & 26.9 & 27.2 & 26.0 \\
Hybrid & 28.0 & 27.7 & 26.2 & 28.9 & 29.3 & 28.2 & 27.0 & 26.9 & 24.6 \\
\bottomrule
\end{tabular}
\label{tab:offset_comparison}
\end{sidewaystable}

\subsection{Single Time Point Classification}
CEBRA assigns each labeled neural data point (obtained by binning the spikes over $10$\,ms) a 3-dimensional latent embedding. We passed these latent embeddings to three classifiers: SVM, RF, and XGBoost. Table~\ref{tab:offset_comparison} shows the results for all models\footnote{The result visualizations were included in Appendix~\ref{appendix:CEBRA}.}. The behavior models achieved the best classification performance, and larger offset parameter increases the model performance. The result visualized previously is the third model: Behavior shuffled over labels. The best result is 64.2\,\% with parameter offset 20.

The result from the behavior models outperformed the baseline method. Using the same $10$\,ms window size, we tested the classification accuracy by counting the total number of spikes during fixed $10$\,ms windows as features to the same SVM. The best accuracy was around $36.7\,\%$ from time window $[-20\,\text{ms}, -10\,\text{ms}]$, which is well below the result obtained using CEBRA latent embeddings.

\subsection{Sequence Classification}
We applied the CNN--LSTM model from Experiment~\ref{exp:3} to sequences of CEBRA embeddings, and obtained an accuracy of $81.8$\,\%. The embeddings were from behavior models with shuffled labels and an offset of $10$.

Table~\ref{tab:cebra_lstm_compare} shows CNN--LSTM with CEBRA embeddings outperforms the same CNN--LSTM using binned spikes. The data had a similar window size and the models have the same structure. The CNN--LSTM with CEBRA embeddings also outperform EGGNet with binned spikes.

\begin{table}[H]
    \centering
    \caption{Model performance comparison on sequences}
    \label{tab:cebra_lstm_compare}
    \begin{tabular}{lrlcc}
        \toprule
        Input & Dimensions & Window (ms) & Model & Accuracy (\%) \\
        \midrule
        \textBF{CEBRA embeddings} & \mathBF{3} & \mathBF{60} & \textBF{CNN--LSTM} & \mathBF{81.8} \\
        Binned spikes & 75 & 62.5 & CNN--LSTM & 24.8 \\
        Binned spikes & 75 & 62.5 & EEGNet & 69.8 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Discussion}
\subsection{Achievements of CEBRA Embeddings}
Classification results from CEBRA embeddings achieve accuracies of 64.0\,\% and 81.8\,\%, outperforming all previous benchmarks. CEBRA effectively squeezes the original $75$ dimensions to $3$, which greatly reduces the sparsity in the data. Offset $20$ models consistently yield the best decoding results, possibly because the timescale of offset $20$ (200\,ms) captures the lengths of syllables.

The circular shape suggests that CEBRA is able to capture the syllable transitions even when trained on shuffled data. Unshuffled data contains information about syllable transitions. Training on unshuffled data achieves higher decoding accuracy of $75.7$\,\%. 

\subsection{Limitations}
CEBRA has been validated in paradigms with clearly defined boundaries in time. Vocal productions are spontaneous and are not triggered by temporally bounded stimuli. Thus, data pre-processing is required to remove the nonactive parts in the dataset, as simply allowing the model to self-supervise over the continuous recording session leads to low efficiency and low decoding accuracy. 

In addition, syllable transitions are continuous during vocalization, and pre-motor neural activity might overlap between adjacent syllables in one motif. Using data from the motor pathway would provide better results.

\subsection{Future Directions}
Future work could focus on contrastive learning frameworks due to their high potential to decode neural data. However, categorical labels like syllables, or redundant and high-dimensional labels like spectrograms might not be the most natural or efficient behavioural labels to use. MFCCs or latent embeddings from pre-trained models (for either human or bird vocalization) should be considered, as they are denser and have shown better performance in decoding tasks \citep{Defossez2022-lq}. 

% self-defined macro: include bibliography even when compiling a single chapter
\subfilebibliography
\end{document}
