% ADD THIS HEADER TO ALL NEW CHAPTER FILES FOR SUBFILES SUPPORT

% Allow independent compilation of this section for efficiency
\documentclass[../CLthesis.tex]{subfiles}

% Add the graphics path for subfiles support
\graphicspath{{\subfix{../images/}}}

% END OF SUBFILES HEADER

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START OF DOCUMENT: Every chapter can be compiled separately
\begin{document}

\chapter{Experiment 3: Temporal Deep Learning Methods}%
% \label{chap:experiment_3}
\refstepcounter{experiment} 
\label{exp:\theexperiment}
This experiment explored deep learning architectures for syllable classification using neural data. We compared the performance of different neural network architectures (CNN-LSTM, EEGNet, LSTM, and Mamba-2) against the benchmark established from previous experiments. Both sampling time binary data and binned data were evaluated. We adopted the newly released Mamba-2 architecture \citep{Dao2024-sp} rather than the original Mamba \citep{Gu2023-vw}, as Mamba-2 is computationally more efficient during training.

\section{Methods}
\subsection{Data} 
Data was selected around the onset using a moving window. Raw data in its original format $[\text{neuron}, \text{sampling time}]$ were used as binary data. We also binned the binary values and tested various bin sizes, ranging from $0.1$\,ms to $20$\,ms. We picked corresponding windows using a parameter sweep, such that different windows around the onset could be picked and compared. The step size was fixed at 1.

For the parameter sweep, we tested bin sizes from $0.05$\,ms to $20$\,ms. For each bin size, we tested different temporal windows relative to syllable onset:
\begin{itemize}
    \item pre-onset: from $-125$\,ms to $0$\,ms, incrementing by $12.5$\,ms
    \item post-onset: three values $37.5$\,ms, $50.0$\,ms, and $62.5$\,ms
\end{itemize}
The sequence length is determined by total time window length (ranging from $37.5$\,ms to $187.5$\,ms, computed as the sum of absolute values of pre-onset and post-onset times) and the bin size (ranging from no bin, which is $0.05$\,ms, to $20$\,ms). As a result, sequence lengths varied by up to three orders of magnitude, primarily driven by the bin size. When using original binary data or small bin sizes, the resulting data was considerably sparser when compared to those data processed with larger bin sizes.


\subsection{Models and Evaluation}
We used a two-layer LSTM model, a parallel CNN-LSTM model, a two-layer Mamba-$2$ model, and CNN-based EEGNet\citep{Lawhern2016-tf}. The models were evaluated by classification accuracies across different pre-onset and post-onset parameters, bin sizes, and model architectures. The results were then compared against results in previous experiments.

\section{Results}
All models but EEGNet successfully completed the parameter sweep analysis. EEGNet encountered memory constraints when processing smaller bin sizes due to long sequence lengths. For the $10$\,ms bin size, we were only able to test pre-onset windows of $-125$\,ms and $-112.5$\,ms due to technical limitations. We therefore focused our bin size analysis on a single parameter combination (pre-onset: $-125$\,ms, post-onset: $62.5$\,ms). The overall trends remained consistent across other parameter combinations, indicating that this partial testing did not affect the reported observations. Results from the parameter sweep are presented below.

\subsection{Top-3 Results for Each Model}

The Top-3 results for each model are included in Table~\ref{tab:model_comparison}. EEGNet achieved a peak decoding accuracy of $89.0\,\%$\,Â±\,$1.7\,\%$, with a bin size of $15$\,ms, a step size of $0.05$\,ms, and data extracted from $-125\,\text{ms to } 62.5\,\text{ms}$. 

\begin{table}[H]
   \centering
   \begin{tabular}{lrccc@{ $\pm$ }l}
       \toprule
       Model & Bin (ms) & Pre-onset (ms) & Post-onset (ms) & \multicolumn{2}{c}{Accuracy (\%)} \\
       \midrule
       \multirow{3}{*}{LSTM} 
           & 2.5 & $-112.5$ & 67.5 & 50.4 & 1.9 \\
           & 3.8 & $-125.0$ & 67.5 & 50.3 & 1.1 \\
           & 5.0 & $-125.0$ & 67.5 & 50.3 & 1.2 \\
       \midrule
       \multirow{3}{*}{Mamba-$2$}
           & 5.0 & $-125.0$ & 67.5 & 57.7 & 1.4 \\
           & 10.0 & $-125.0$ & 67.5 & 57.7 & 1.2 \\
           & 10.0 & $-112.5$ & 67.5 & 57.1 & 1.5 \\
       \midrule
       \multirow{3}{*}{\textBF{EEGNet}}
           & \mathBF{15.0} & $\mathbf{-}$\mathBF{125.0} & \mathBF{67.5} & \mathBF{89.0} & \mathBF{1.7} \\
           & 10.0 & $-125.0$ & 67.5 & 88.6 & 0.5 \\
           & 7.5 & $-125.0$ & 67.5 & 88.3 & 0.8 \\
       \midrule
       \multirow{3}{*}{CNN--LSTM}
           & 7.5 & $-125.0$ & 67.5 & 58.0 & 0.8 \\
           & 5.0 & $-125.0$ & 67.5 & 58.0 & 1.7 \\
           & 5.0 & $-112.5$ & 67.5 & 57.9 & 1.0 \\
       \bottomrule
   \end{tabular}
    \caption{Top-3 parameter combinations for each model on accuracy}
    \label{tab:model_comparison}
\end{table}


\subsection{Model Performance Across Different Windows Periods}
Figure~\ref{fig:window_sweep} shows the performance of models with different pre-onset and post-onset combinations. Each grouped column represents one pre-onset time. Within each group, there are three connected data points showing the classification accuracies using different post-onset times.

The grouped columns of pre-onset times show a pattern from left to right. As the pre-onset time moves closer to the syllable onset, the classification accuracy decreases. Within each group, three connected dots of post-onset times form an ascending line. Larger post-onset time on the right consistently outperforms smaller post-onset time on the left. 

In conclusion, longer data windows in both pre-onset and post-onset directions improve the decoding accuracy. This improvement appears stronger in shorter data, as shown on the right side of the figure. EEGNet has the best performance in all windows, and demonstrates the strongest sensitivity to data duration.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/timeseries-compare-models.pdf}
    \caption{Model performance across different window periods}
    \label{fig:window_sweep}
\end{figure}

\subsection{Model Performance Across Different Bin Sizes} 
Figure~\ref{fig:binning_sweep} shows the performance across different bin sizes. The time window used is [$-125$\,ms, $62.5$\,ms]. The performance improves as bin size increases until the bin size hits 1.5\,ms. Mamba-2 shows a better performance in small bin sizes. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/window_performance.pdf} 
    \caption{Model performance across different bin sizes}
    \label{fig:binning_sweep}
\end{figure}


\section{Discussion}

\subsection{EEGNet Performance on Binned Spike Data}
Our results show that EEGNet's architecture, originally designed to deal with the temporal and spatial characteristics of EEG recordings, successfully transfers its effectiveness to spike data analysis. It achieves a decoding accuracy of $89.0$\,\%, surpassing the $77.3$\,\% benchmark previously established in Experiment~\ref{exp:1}. The results suggest that deep learning models can effectively capture the complex patterns in neural spike data.


\subsection{Effect of Binning}
The performance increase until $1.5\,\text{ms}$ bin size can be attributed to reduction in data sparsity and decrease in input sequence length. The performance plateux for all models beyond the 1.5\,ms threshold indicate that this bin size achieves a balance between data sparsity, input sequence length, and fine-grained neural patterns. This plateau point at $1.5\,\text{ms}$ aligns with the typical duration of an action potential ($1$--$2$\,ms), suggesting that bin sizes beyond the timescale of individual spikes provide no significant benefit for performance. Different bin sizes have been used in literature, such as $1$\,ms, $8.33$\,ms, and $25$\,ms \citep{Schneider2023-xu, Zhou2020-om}, reflecting data-specific choices that balance trade-offs between fine-grained temporal patterns and computational efficiency.

\subsection{Limitations on Model Performance}
Other models we implemented were less effective at capturing information in the neural data. Their best decoding accuracies are lower than the benchmark from Experiment~\ref{exp:1}. With sampling time data, their performances are also lower than the NLP methods, as shown in Table~\ref{tab:compare_time_GPT}.
\begin{table}[H]
   \centering
   \begin{tabular}{lr}
       \toprule
       Model & Accuracy (\%) \\
       \midrule
       \textBF{GPT-2} & \mathBF{47.0} \\
       Mamba-$2$ & $26.2$ \\
       CNN--LSTM & $24.8$ \\
       LSTM & 21.1 \\
       \bottomrule
   \end{tabular}
   \caption{Model performance on sampling time}
   \label{tab:compare_time_GPT}
\end{table}

\subsection{Future Directions}
Mamba-$2$ showed some potential for dealing with the sparsity and long-range dependencies inherent in unbinned spike train data while maintaining computational efficiency. Future work could explore more sophisticated Mamba-$2$ architectures optimized for unbinned spike train data, and Conv2D layers could be incorporated to enhance the spatial pattern recognition capabilities.

% self-defined macro: include bibliography even when compiling a single chapter
\subfilebibliography
\end{document}
