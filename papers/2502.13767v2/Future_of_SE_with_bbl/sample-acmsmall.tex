
\documentclass[acmsmall]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}


\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}




\newcommand{\corina}[1]{\textcolor{green}{Corina: #1}}
\newcommand{\michael}[1]{\textcolor{blue}{Michael: #1}}

\begin{document}


\title{AI Software Engineer: Programming with Trust}

\author{Abhik Roychoudhury}
\authornote{Comments about this opinion piece can be sent to {\tt abhik@comp.nus.edu.sg}}
\email{abhik@comp.nus.edu.sg}
\orcid{0000-0002-7127-1137}
\affiliation{%
  \institution{National University of Singapore}
  %\city{Singapore}
  %\state{Ohio}
  \country{Singapore}
}

\author{Corina P\u{a}s\u{a}reanu}
%\authornote{Both authors contributed equally to this research.}
\email{pcorina@cmu.edu}
\orcid{0000-0002-5579-6961}
\affiliation{%
  \institution{Carnegie Mellon University, KBR Inc., NASA Ames}
  \country{USA}
}
\author{Michael Pradel}
\email{michael@binaervarianz.de}
\orcid{0000-0003-1623-498X}
\affiliation{%
  \institution{University of Stuttgart}
  % \city{Stuttgart}
  \country{Germany}
}
\author{Baishakhi Ray}
\email{rayb@cs.columbia.edu}
\orcid{0000-0003-3406-5235}

\affiliation{%
  \institution{Columbia University}
  \country{USA}
}



\renewcommand{\shortauthors}{Roychoudhury et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to, or even larger than, the trust established by human-driven software engineering practices.
%Increasing trust needs more than an LLM to generate code, but AI software engineering must integrate well-established quality assurance techniques, such as testing, static analysis, and formal verification.
The recent trend toward LLM agents offers a path toward integrating the power LLMs for creating new code with the power of analysis tools to increase trust into the code.
This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future, and whether the focus of programming will shift from programming at scale to programming with trust.
\end{abstract}




\ccsdesc[500]{Software and its Engineering~Automatic Programming}

\keywords{LLM agents, Software Maintenance, Developer Workflows}



\maketitle



\section{Are AI Software Engineers Deployed?}

Software engineering is undergoing a significant phase of greater automation owing to the emergence of Large Language Models (LLMs) for code. It is easy to generate almost correct code by feeding in natural language requirements to LLMs.
Such automated code generation creates public excitement and shows the promise of automatic programming.
Given the recent progress, some are already declaring victory and argue that AI has ``solved'' the software development problem.

%\corina{new text: please check}
While genAI-enabled code generation and code completion are now prevalent in common IDEs, fully automated AI software engineering is not yet widely deployed in industrial practice. 
What is holding people back from adopting generative AI? 
A recent blog post \cite{Kohler25} by the behavioral scientist and future of work advocate Lindsay Kohler points out that the {\em key barrier} to genAI adoption is largely {\em trust}. People are asking if they can trust AI and if they can demonstrate trustworthiness to stakeholders. In the domain of coding, the concern is thus not about the management of the organization not accepting automatic programming, but it is about automatic programming not being accepted by
developers, due to lack of trust.



This brings us to the following question: 
{\em What is the place of automatically generated code in future development workflows? }
If we can figure out how automatically generated code and manually written code can co-exist, this may give us a pathway of greater deployment of LLMs for software engineering!
Starting from early programs of just a few lines written in high-level languages in the 1950s, size of programs has increased greatly to hundreds of millions of lines of code. 
For the past fifty years, there has been clear interest towards {\em programming in the large}, where distributed software development by different teams is carried out across different modules in a software project. For programming in the large, the focus is often on software project management and cleanly partitioning work among developer teams. As a result, for the past fifty years, a lot of attention in software engineering has been on  managing the complexities of scale in large software projects.  With the arrival of AI software engineers, programming is less about code writing and more about code assembling.
Importantly, a lot of the assembled code will be generated automatically or come from other potentially untrusted sources.
This development turns the attention towards the trust that can be placed on such component assemblies. %This will shift the focus towards achieving trust at the component boundaries, with less focus on programming at scale and producing large codebases! 

It is the opinion of these authors that increased automation in programming will make {\em programming with trust} at least as important as, if not more important than, programming at scale. 




\section{Agentic Capability}

As LLMs alone cannot establish sufficient trust by developers to produce correct code, we envision a role for {\em LLM agents} in genAI-based software engineering.
What is an LLM agent? How does the use of LLM agents differ from prompt engineering for generating code? We highlight the following aspects about LLM agents for software.\footnote{See also the following for a discussion \url{ https://www.anthropic.com/research/building-effective-agents}}
%\corina{added some headings to the items please check}
\begin{itemize}
%corina: moved this back here
\item {\em LLMs as back-ends:} An LLM agent is a wrapper around LLMs, using LLMs as back-end computation engines. Agents can interact with multiple LLMs in the back-end.
\item {\em Interaction with software tools:} Agents interact with different tools to achieve a given task. In software engineering, the agentic capability often comes from invoking software testing and analysis tools. Other, more generic tools, can also be used, such as using a shell tool to navigate files in a code base or to invoke arbitrary command-line tools. Appropriate use of testing and analysis tools can enhance trust of the developer in the results of the LLM agent.
\item {\em Autonomy:} An agent invokes tools in an autonomous manner. That is, the agent is not a deterministic algorithm, but rather creates a nondeterministic work-plan with significant autonomy. Deciding on the degree of autonomy is part and parcel of designing an LLM agent.

\end{itemize}

Recently, several software LLM agents have been proposed, starting with the announcement of the Devin AI software engineer from Cognition Labs \cite{Devin24}. Devin can solve natural language tasks (called {\em issues}) involving bug-fixes and feature-additions. The approach combines a back-end LLM with access to standard developer tools, such as a shell, a code editor, and a web browser. Such tools will be employed autonomously to let the AI software engineer mimic human software engineers.
In parallel with the announcement of Devin, several research groups proposed their own LLM agents for software engineering, including RepairAgent~\cite{repairagent25}, AutoCodeRover~\cite{acr24}, SWE-agent~\cite{sweagent24}, and Agentless~\cite{agentless}.


RepairAgent~\cite{repairagent25} focuses on fixing bugs and guides the LLM agent by defining a finite-state machine that outlines the typical steps followed by a developer.
In each state, only a subset of all tools, such as code editing, code search, and testing, is available to the agent. RepairAgent cannot solve natural language issues, and thus cannot behave like a human software engineer. Instead it focuses on conventional program repair where the goal is to make failing tests pass. 

AutoCodeRover, a spinoff acquired by SonarSource, can solve natural language issues requiring bug fixing or feature addition. It was reported to have higher efficacy \cite{acr24} than Devin, owing largely to its attention to extracting the software intent. To this end, the AI software engineer is equipped with program analysis techniques, such as autonomous code search (on an abstract syntax tree) and spectrum-based fault localization.
Such intent extraction for software engineering tasks can also allow the agent to provide explanations of fixes.

SWE-agent~\cite{sweagent24} follows a philosophy similar to Devin, by making file navigation tools and interfaces available to an AI software engineer. Thus, it does not employ any program analysis and does not work on program representations. 

Last but not least, the Agentless approach~\cite{agentless} advocates the use of careful prompt engineering to LLMs for fault localization, repair, and patch validation. This is done without allowing autonomy to an agent operating on top of an LLM. While there is no autonomy, Agentless is nevertheless an AI software engineer conducting software maintenance and evolution. 





\section{Establishing Trust}

There exist numerous examples of LLM-generated code that suffers from bugs and security vulnerabilities.
Obviously, human-written code may also suffer from these problems. What makes us trust human-written, but not LLM-generated code? Part of the reason is the perceived capability of ``{\em passing the blame}''. If a human developer is involved, there is the promise of getting feedback from the developer as needed. Of course, this does not always hold, e.g., if the developer  eventually leaves an organization. Nevertheless, accepting a code commit from a developer often partially depends on the reputation of the developer in the organization.
For AI software engineers, until they become commonplace, there is a greater need to engender trust by applying quality assurance techniques like testing, static analysis, and  formal verification. %While these techniques may not be perfect, systematically applying them has been shown to establish a sufficiently high level of trust for us to rely on software in all aspects of daily life -- from mobile apps to avionics software.
So how can AI software engineers alleviate the issue of trust in automatically generated code?
There can be varying levels of trust that the AI software engineer can establish.



\paragraph*{Testing for Consistency}  At a technical level, some initial degree of trust can be ensured by retrofitting testing into AI software engineering workflows. Thus, in the process of code generation, additional artifacts, such as tests, %or certificates (as in proof-carrying code \cite{10.1145/3691621.3694932}), 
can be generated as well~\cite{ryan2024code}. For the generated tests, not only the test inputs, but also the expected outputs, i.e., the oracles, of the tests will be generated from natural language specifications. These tests can be cross-checked with the bug fixes or feature additions generated by the AI software engineer, thereby yielding greater trust in the program modifications. %Guardrails can be viewed as mini static checkers, and they avoid obviously untrustworthy code. LLM editing approaches where the foundation model itself is minimally edited to generate less vulnerable code is another (extreme) option.






\paragraph*{Specification Inference}
A more conceptual mechanism to establish trust would be to infer the code intent from the initial, possibly buggy program. The system-level intent of what a large software system is supposed to do can often be crisply captured by a detailed natural language prompt. What is missing is the intent of the different code units, such as functions or methods.  An AI software engineer could be geared towards such specification inference, navigating the code-base via code search, and trying to infer intended pre/post-conditions \cite{acr25}. Such explicit specification inference can enable the program modifications made by AI software engineers to be accompanied by well-explained justifications, increasing trust.
The extracted specifications can also be used to cross-check the oracles of any generated tests.
%that were used to check the patches produced.


\paragraph{Formal Proofs}
An enhanced degree of trust can come from formal proofs. A promising paradigm  in this regard is automated, proof-oriented programming ~\cite{POP}, where the LLMs are used to generate code together with a proof that can be automatically checked with an external tool. While historically, proving program correctness has been very difficult due to the high cognitive burden on programmers, LLMs can help by generating the code together with the necessary assertions (pre/post-conditions, loop invariants, and so on) in a verifiable language such as F*, Dafny, or Verus. Such programs can then be  automatically verified, providing greater trust than testing alone.

%\corina{moved the following here; also shortened a bit as otherwise looks like too much emphasis on security? please check}
%\paragraph*{Guardrail based Static Analysis for greater Security} 
\paragraph*{Guardrails for Increased Security}
 Trust can also be ensured 
 %by mitigating security risks in generated code 
 through the use of {\em guardrails}.\footnote{See \url{https://hub.guardrailsai.com/} for example.} These can serve as  %robust 
 sanitization mechanisms, filtering malicious inputs before they reach the LLM and %also by 
 validating the %security of the 
 generated code. 
%To implement guardrails effectively, we must first identify the broad categories of malicious inputs that the guardrails might encounter, much like the specifications used in static analysis. The main challenges can be as follows:
Guardrails can be used to protect against the following threats:
%\begin{itemize}
 %   \item 
    {\em (i) Prompt injections}: Natural language prompts that jailbreak the LLM, convincing it to disregard its safety policies and to perform malicious activities, like generating malicious or vulnerable code. 
    %These prompts need to be filtered.
    %\item 
   {\em (ii) Malicious code}: Even if the natural language part of the prompt is benign, input code snippets may be malicious, forcing the LLM to generate (or modify) malicious code.  
    %\item 
    {\em (iii) Vulnerable code} -- Even without any malicious intent, the input code may still be vulnerable, forcing the LLM to modify vulnerable code.  
%\end{itemize}
%At a high level, some light-weight 
Static checkers can be used for guardrailing such malicious behavior. 



\paragraph*{Open Challenges}
While we see establishing trust as the prime challenge for widely deploying LLM agents in software engineering, there are some related challenges that also need to be addressed:
(i) {\em Communication}:
The explanations given by the AI software engineer must be communicated properly without imposing high cognitive load. Checking the results of AI-generated code currently involves a significant human effort.
(ii) {\em Diverse teams}:
Using AI Software Engineer excessively may reduce diversity of thought\footnote{See \url{https://medium.com/@LuisMizutani/ai-for-software-development-can-we-actually-measure-its-impact-31af45dccdab}} in a development team. Thus, co-operative intelligence across AI-AI and AI-human combinations remains a topic of discourse.
(iii) {\em Costs}: Even though the automation provided by AI promises to reduce costs, an AI software engineer still needs to be cost-sensitive. Program analysis tools invoked by LLM agents can help make only {\em relevant} queries to LLMs \cite{acr24}.
(iv) {\em Datasets:} Last but not the least, the community needs datasets. The latest dataset announced in this regard is the SWE-Lancer \cite{lancer25}. We need datasets for software engineering tasks which support comprehensive patch validation (possibly by supporting test generation), and, ideally, cover more capabilities of the AI software engineer than patching,  as we discuss in the following.
.


\section{Outlook}

With LLM agents performing some of the heavy lifting in software maintenance and software quality control, it may be useful to think about how they will fit into future development workflows. Will there be separate agents for coding, testing, debugging, repair, and various software engineering tasks? The more agents there are, the more the maintainability of these agent may become an issue!  Instead of building hyper-specialized agents for individual software engineering tasks, it could be worthwhile to craft and maintain a {\em unified software engineering agent} (USE-agent) that can be configured to deliver different capabilities.
Such an agent of the future may conduct myriad tasks, including code generation, test generation, bug fixing, and feature addition. Software engineering can then be conducted by clients that rely on the unified agent, with human developers vetting the results of the agent wherever needed.
With newer models like o1 and DeepSeek R1 being developed, some of the current agentic capabilities can be co-opted by LLMs in the future, and hence it is useful to think about more ambitious, unified agents.
 
 



%\section{++++ OLD TEXT BELOW (partially copied into the above text) ++++}


%\section{Is Automatic Programming Deployed?}

%Software engineering is undergoing a significant phase of greater automation owing to the emergence of Large Language Models (LLMs) for code. It is easy to generate almost correct code by feeding in natural language requirements to LLMs. Such automated code generation, create public excitement and show the promise of automatic programming. However such excitement needs to translate to new or modified developer workflows for future programming and software engineering. In a recent blog post \cite{Kohler25}, behavioral scientist and future of work advocate Lindsay Kohler argues that slow adoption of genAI is not owing to organizations being afraid to incorporate LLMs, but rather the value of genAI not being conclusively demonstrated in companies. In the domain of coding, this thus amounts to LLM based programming being accepted not by an organization but rather by developers and clients. Trust by developers, in particular, is a paramount issue. 

%This brings us to the question - how automatically generated code could figure in future developer workflows? If we can figure out how automatically generated code and manually written code can co-exist, this may give us a pathway of greater deployment of LLMs for software engineering!

%\section{LLM agents for Software}

%Since LLMs cannot be trusted enough by developers to produce correct code, there is a role for LLM agents in genAI based software engineering. What is an LLM agent? How does the use of LLM agent differ from prompt engineering for generating code from LLMs?  What is the purpose of LLM agents? In the software engineering context, an LLM agent involves an invocation of autonomous actions, which may involve the use of an LLM in the back-end. Several aspects need to be highlighted for us to discuss LLM agents for software. 
%\begin{itemize}
%\item First, an LLM agent is a wrapper around LLM, using LLM as back-end computation engine.
%\item Since we are discussing LLM agents for software, the agentic capability often comes from invoking software testing and analysis tools. Other more generic tools are also used, such as using a bash tool to navigate files in a given software project. The appropriate use of testing and analysis tools can enhance the trust of the developer in the results of the LLM agent.
%\item The actions of the LLM agent that involve invoking various tools are often autonomous. An LLM agent is thus not a deterministic algorithm, but rather a nondeterministic workplan with significant autonomy. Deciding on the degree of autonomy is part of the design of the LLM agent.
%\end{itemize}

%Recently, LLM agents for software have been proposed since the announcement of the Devin AI software engineer from Cognition Labs \cite{Devin24}. Devin already suggested providing the back-end LLM with access to standard developer tools such as a shell, code editor and browser. Such tools will be employed autonomously to let the AI software engineer behave like a real human software engineer. Within a month of announcement of Devin, AutoCodeRover was proposed and later was published as \cite{acr24}. The main distinct approach in AutoCodeRover is the focus on intent extraction for conducting software maintenance tasks. Thus, instead of making developer tools available to the AI software engineer, the engineer is equipped with analysis techniques like autonomous code search (on an abstract syntax tree) and spectrum-based fault localization. The goal is to discover the intent behind different units of code while fixing bugs or adding features. Such intent extraction for software engineering tasks have been made explicit in subsequent works such as \cite{acr25}. Another approach, called SWE-agent \cite{sweagent24} follows a philosophy similar to Devin, by making file navigation tools / interfaces available to an AI software engineer. Thus it does not employ any program analysis and does not work on program representations.

%Parallel to the above approaches, we would also want to mention two other approaches which offer less autonomy to the AI software engineer. The Repair-Agent \cite{repairagent25} differs by taking in correctness criterion in the form of a test-suite instead of natural language issues. Using test-suites as correctness criterion is common in the area of automated program repair \cite{cacm19}, where a generalization of the test-suites is often carried out to infer software intent. Having test-suites as correctness criterion is more rigorous but less complete than specifying the correctness criterion as natural language issues, as is common in AI software engineers of today. Last but not the least, the Agentless approach \cite{agentless} advocates the use of careful prompt engineering to LLMs for localization/repair/ patch validation. This is done without allowing autonomy to an agent operating on top of a LLM. This the least degree of autonomy but nevertheless should be considered under the umbrella of AI software engineers conducting software maintenance and evolution. 



%\section{Engendering Trust}

%Since the beginning of programming starting with the Hello World program by Brian Kerninghan \cite{hello} in 1972-73, the focus in programming has been for programming in the large. Readers can see \cite{pgm-large} for an article published as early as 1975 fifty years ago, espousing the dream of programming in the large. Today, it is not uncommon to have software projects with hundreds of millions of lines of code. Thus, in the past fifty years, the focus in programming and software engineering has been on how to manage the complexities of scale in large software projects. Most software processes including those focused on software validation are focused on managing scale.

%However, with the arrival of AI software engineers - programming is less of code writing and more of code assembly. Thus automatically generated components from different un-trusted sources may be amalgamated to form a software system. Many of the components of a large software system may become automatically generated in future using LLMs. This puts somehow greater focus on trust at the boundaries of the components, and a bit less focus on programming at scale! It is the opinion of this author that scalability in programming is over-emphasized and increased automation in programming will make programming with trust at least as important as programming at scale. 

%How do LLM agents for software alleviate the issue of trust in automatic programming? The answer to this question can be given either in a purely technical fashion, or in a conceptual fashion. A purely technical answer will center around the cross-checking of artifacts around code generation to ensure consistency. Thus, in the process of code generation, additional artifacts, such as tests, can be generated. For the generated tests, not only the test inputs but also the expected outputs or the oracles of the tests will be generated from natural language specifications. These tests can be cross-checked with the bug fixes / feature additions generated by the AI software engineer, thereby producing greater trust in the program modifications generated by an AI software engineer.

%A more conceptual mechanism for engendering trust would be to infer intent of code, given a buggy program, This would allow the AI software engineer to come up with fixes, while keeping the intended behavior of the software project in mind. We want to stress here that the system level intent of what a large software system is supposed to do, can often be crisply captured by a detailed natural language prompt. What is missing is for the different code units, such as functions / methods- what the intended pre and post conditions are?  An AI software engineer could be geared towards such specification inference, navigating the code-base via code search, and trying to infer intended specifications of the different units of code. If the intent is captured accurately, the program modifications produced by AI software engineers can be accompanied with lucid explanations, which engender greater trust! %Recent works \cite{acr25} suggest such a direction and could provide a guidepost to LLM agents for software. 

%\section{Newer Agents of the future}

%With LLM agents conducting some of the heavy lifting in software maintenance and software quality control, it may be useful to think about how they will fit into developer workflows of the future. Whether there will be separate agents for coding, testing, debugging, repair and various SE tasks remains an open question. However, with a large corpus of agents comes the issue of how usable the development environment remains. The more agents there are, the maintainability of the agents (for conducting software maintenance) may also become an issue !!  The dilemma here almost reminds us of the {\em thin client vs. thick client} debate in computer systems which have continued for decades --- where a thin client relies on a consolidated server for most of the processing.  In this style, it could be beneficial to develop unified software engineering agents of the future which can conduct myriad SE tasks such as code generation, test generation, bug fixing and feature addition. Software engineering can then be conducted by thin clients which rely on the unified software engineering agent (USE-agent) for most of the software processes. 

%Instead of building hyper-specialized agents for individual software engineering tasks - it could be worthwhile to craft and maintain a unified software engineering agent which can be configured as needed to deliver different capabilities. This also creates a smaller trusted code-base (the code for the USE-agent) on which the development environment will be reliant.   Instead if we have many agents - managing and maintaining them in unison may be a challenge and can simply create debugging problems at the level of agent combinations. Instead it would be more fruitful to develop carefully crafted workflows for resiliently maintaining one unified agent. A lot (not all) of the work in conducting automated software engineering of the future can then focus on managing and maintaining this Unified SE agent (USE-agent) in a {\em trustworthy} fashion !!







%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.



\begin{acks}
The authors acknowledge discussions with attendees at  Dagstuhl Seminar 24431 on Automated Programming and Program Repair (Oct 2024), as well as at Shonan Meeting 217 on Trusted Automatic Programming (Jan 2025). Lingming Zhang shared valuable pointers. %This work is partially supported by a Singapore Ministry of Education( MoE) Tier3 grant MOE-MOET32021-0001.
\end{acks}
%\newpage
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.





\end{document}
