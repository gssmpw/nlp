\documentclass[acmsmall]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}


\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}




\newcommand{\corina}[1]{\textcolor{green}{Corina: #1}}
\newcommand{\michael}[1]{\textcolor{blue}{Michael: #1}}

\begin{document}


\title{Agentic AI Software Engineer: Programming with Trust}

\author{Abhik Roychoudhury}
\authornote{Comments about this opinion piece can be sent to {\tt abhik@comp.nus.edu.sg}}
\email{abhik@comp.nus.edu.sg}
\orcid{0000-0002-7127-1137}
\affiliation{%
  \institution{National University of Singapore}
  %\city{Singapore}
  %\state{Ohio}
  \country{Singapore}
}

\author{Corina P\u{a}s\u{a}reanu}
%\authornote{Both authors contributed equally to this research.}
\email{pcorina@cmu.edu}
\orcid{0000-0002-5579-6961}
\affiliation{%
  \institution{Carnegie Mellon University, KBR Inc., NASA Ames}
  \country{USA}
}
\author{Michael Pradel}
\email{michael@binaervarianz.de}
\orcid{0000-0003-1623-498X}
\affiliation{%
  \institution{University of Stuttgart}
  % \city{Stuttgart}
  \country{Germany}
}
\author{Baishakhi Ray}
\email{rayb@cs.columbia.edu}
\orcid{0000-0003-3406-5235}

\affiliation{%
  \institution{Columbia University}
  \country{USA}
}



\renewcommand{\shortauthors}{Roychoudhury et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Large Language Models (LLMs) have shown surprising proficiency in generating code snippets, promising to automate large parts of software engineering via artificial intelligence (AI). We argue that successfully deploying AI software engineers requires a level of trust equal to or even greater than the trust established by human-driven software engineering practices.
The recent trend toward LLM agents offers a path toward integrating the power of LLMs to create new code with the power of analysis tools to increase trust in the code.
This opinion piece comments on whether LLM agents could dominate software engineering workflows in the future and whether the focus of programming will shift from programming at scale to programming with trust.
\end{abstract}




\ccsdesc[500]{Software and its Engineering~Automatic Programming}

\keywords{LLM agents, Software Maintenance, Developer Workflows}



\maketitle



\section{Are AI Software Engineers Deployed?}

Software engineering is undergoing a significant phase of greater automation owing to the emergence of Large Language Models (LLMs) for code. It is easy to generate almost correct code by feeding in natural language requirements to LLMs.
Such automated code generation creates public excitement and shows the promise of automatic programming.
Given the recent progress, some are already declaring victory and argue that AI has ``solved'' the software development problem.

%\corina{new text: please check}
While genAI-enabled code generation and code completion are now prevalent in common IDEs, fully automated AI software engineering is not yet widely deployed in industrial practice. 
What is holding people back from adopting generative AI? 
A recent blog post \cite{Kohler25} by the behavioral scientist and future of work advocate Lindsay Kohler points out that the {\em key barrier} to genAI adoption is largely {\em trust}. People are asking if they can trust AI and if they can demonstrate trustworthiness to stakeholders. In the domain of coding, the concern is thus not about the management of the organization not accepting automatic programming, but it is about automatic programming not being accepted by
developers, due to lack of trust.



This brings us to the following question: 
{\em What is the place of automatically generated code in future development workflows? }
If we can figure out how automatically generated code and manually written code can co-exist, this may give us a pathway of greater deployment of LLMs for software engineering!
Starting from early programs of just a few lines written in high-level languages in the 1950s, size of programs has increased greatly to hundreds of millions of lines of code. 
For the past fifty years, there has been clear interest towards {\em programming in the large}, where distributed software development by different teams is carried out across different modules in a software project. For programming in the large, the focus is often on software project management and cleanly partitioning work among developer teams. As a result, for the past fifty years, a lot of attention in software engineering has been on  managing the complexities of scale in large software projects.  With the arrival of AI software engineers, programming is less about code writing and more about code assembling.
Importantly, a lot of the assembled code will be generated automatically or come from other potentially untrusted sources.
This development turns the attention towards the trust that can be placed on such component assemblies. %This will shift the focus towards achieving trust at the component boundaries, with less focus on programming at scale and producing large codebases! 

It is the opinion of these authors that increased automation in programming will make {\em programming with trust} at least as important as, if not more important than, programming at scale. 




\section{Agentic Capability}

As LLMs alone cannot establish sufficient trust by developers to produce correct code, we envision a role for {\em LLM agents} in genAI-based software engineering.
What is an LLM agent? How does the use of LLM agents differ from prompt engineering for generating code? We highlight the following aspects about LLM agents for software.\footnote{See also the following for a discussion \url{ https://www.anthropic.com/research/building-effective-agents}}
%\corina{added some headings to the items please check}
\begin{itemize}
%corina: moved this back here
\item {\em LLMs as back-ends:} An LLM agent is a wrapper around LLMs, using LLMs as back-end computation engines. Agents can interact with multiple LLMs in the back-end.
\item {\em Interaction with software tools:} Agents interact with different tools to achieve a given task. In software engineering, the agentic capability often comes from invoking software testing and analysis tools. Other, more generic tools, can also be used, such as using a shell tool to navigate files in a code base or to invoke arbitrary command-line tools. Appropriate use of testing and analysis tools can enhance trust of the developer in the results of the LLM agent.
\item {\em Autonomy:} An agent invokes tools in an autonomous manner. That is, the agent is not a deterministic algorithm, but rather creates a nondeterministic work-plan with significant autonomy. Deciding on the degree of autonomy is part and parcel of designing an LLM agent.

\end{itemize}

Recently, several software LLM agents have been proposed, starting with the announcement of the Devin AI software engineer from Cognition Labs \cite{Devin24}. Devin can solve natural language tasks (called {\em issues}) involving bug-fixes and feature-additions. The approach combines a back-end LLM with access to standard developer tools, such as a shell, a code editor, and a web browser. Such tools will be employed autonomously to let the AI software engineer mimic human software engineers.
In parallel with the announcement of Devin, several research groups proposed their own LLM agents for software engineering, including RepairAgent~\cite{repairagent25}, AutoCodeRover~\cite{acr24}, SWE-agent~\cite{sweagent24}, and Agentless~\cite{agentless}.


RepairAgent~\cite{repairagent25} focuses on fixing bugs and guides the LLM agent by defining a finite-state machine that outlines the typical steps followed by a developer.
In each state, only a subset of all tools, such as code editing, code search, and testing, is available to the agent. RepairAgent cannot solve natural language issues, and thus cannot behave like a human software engineer. Instead it focuses on conventional program repair where the goal is to make failing tests pass. 

AutoCodeRover, a spinoff acquired by SonarSource, can solve natural language issues requiring bug fixing or feature addition. It was reported to have higher efficacy \cite{acr24} than Devin, owing largely to its attention to extracting the software intent. To this end, the AI software engineer is equipped with program analysis techniques, such as autonomous code search (on an abstract syntax tree) and spectrum-based fault localization.
Such intent extraction for software engineering tasks can also allow the agent to provide explanations of fixes.

SWE-agent~\cite{sweagent24} follows a philosophy similar to Devin, by making file navigation tools and interfaces available to an AI software engineer. Thus, it does not employ any program analysis and does not work on program representations. 

Last but not least, the Agentless approach~\cite{agentless} advocates the use of careful prompt engineering to LLMs for fault localization, repair, and patch validation. This is done without allowing autonomy to an agent operating on top of an LLM. While there is no autonomy, Agentless is nevertheless an AI software engineer conducting software maintenance and evolution. 





\section{Establishing Trust}

There exist numerous examples of LLM-generated code that suffers from bugs and security vulnerabilities.
Obviously, human-written code may also suffer from these problems. What makes us trust human-written, but not LLM-generated code? Part of the reason is the perceived capability of ``{\em passing the blame}''. If a human developer is involved, there is the promise of getting feedback from the developer as needed. Of course, this does not always hold, e.g., if the developer  eventually leaves an organization. Nevertheless, accepting a code commit from a developer often partially depends on the reputation of the developer in the organization.
For AI software engineers, until they become commonplace, there is a greater need to engender trust by applying quality assurance techniques like testing, static analysis, and  formal verification. %While these techniques may not be perfect, systematically applying them has been shown to establish a sufficiently high level of trust for us to rely on software in all aspects of daily life -- from mobile apps to avionics software.
So how can AI software engineers alleviate the issue of trust in automatically generated code?
There can be varying levels of trust that the AI software engineer can establish.



\paragraph*{Testing for Consistency}  At a technical level, some initial degree of trust can be ensured by retrofitting testing into AI software engineering workflows. Thus, in the process of code generation, additional artifacts, such as tests, %or certificates (as in proof-carrying code \cite{10.1145/3691621.3694932}), 
can be generated as well~\cite{ryan2024code}. For the generated tests, not only the test inputs, but also the expected outputs, i.e., the oracles, of the tests will be generated from natural language specifications. These tests can be cross-checked with the bug fixes or feature additions generated by the AI software engineer, thereby yielding greater trust in the program modifications. %Guardrails can be viewed as mini static checkers, and they avoid obviously untrustworthy code. LLM editing approaches where the foundation model itself is minimally edited to generate less vulnerable code is another (extreme) option.






\paragraph*{Specification Inference}
A more conceptual mechanism to establish trust would be to infer the code intent from the initial, possibly buggy program. The system-level intent of what a large software system is supposed to do can often be crisply captured by a detailed natural language prompt. What is missing is the intent of the different code units, such as functions or methods.  An AI software engineer could be geared towards such specification inference, navigating the code-base via code search, and trying to infer intended pre/post-conditions \cite{acr25}. Such explicit specification inference can enable the program modifications made by AI software engineers to be accompanied by well-explained justifications, increasing trust.
The extracted specifications can also be used to cross-check the oracles of any generated tests.
%that were used to check the patches produced.


\paragraph{Formal Proofs}
An enhanced degree of trust can come from formal proofs. A promising paradigm  in this regard is automated, proof-oriented programming ~\cite{POP}, where the LLMs are used to generate code together with a proof that can be automatically checked with an external tool. While historically, proving program correctness has been very difficult due to the high cognitive burden on programmers, LLMs can help by generating the code together with the necessary assertions (pre/post-conditions, loop invariants, and so on) in a verifiable language such as F*, Dafny, or Verus. Such programs can then be  automatically verified, providing greater trust than testing alone.

%\corina{moved the following here; also shortened a bit as otherwise looks like too much emphasis on security? please check}
%\paragraph*{Guardrail based Static Analysis for greater Security} 
\paragraph*{Guardrails for Increased Security}
 Trust can also be ensured 
 %by mitigating security risks in generated code 
 through the use of {\em guardrails}.\footnote{See \url{https://hub.guardrailsai.com/} for example.} These can serve as  %robust 
 sanitization mechanisms, filtering malicious inputs before they reach the LLM and %also by 
 validating the %security of the 
 generated code. 
%To implement guardrails effectively, we must first identify the broad categories of malicious inputs that the guardrails might encounter, much like the specifications used in static analysis. The main challenges can be as follows:
Guardrails can be used to protect against the following threats:
%\begin{itemize}
 %   \item 
    {\em (i) Prompt injections}: Natural language prompts that jailbreak the LLM, convincing it to disregard its safety policies and to perform malicious activities, like generating malicious or vulnerable code. 
    %These prompts need to be filtered.
    %\item 
   {\em (ii) Malicious code}: Even if the natural language part of the prompt is benign, input code snippets may be malicious, forcing the LLM to generate (or modify) malicious code.  
    %\item 
    {\em (iii) Vulnerable code} -- Even without any malicious intent, the input code may still be vulnerable, forcing the LLM to modify vulnerable code.  
%\end{itemize}
%At a high level, some light-weight 
Static checkers can be used for guardrailing such malicious behavior. 



\paragraph*{Open Challenges}
While we see establishing trust as the prime challenge for widely deploying LLM agents in software engineering, there are some related challenges that also need to be addressed:
(i) {\em Communication}:
The explanations given by the AI software engineer must be communicated properly without imposing high cognitive load. Checking the results of AI-generated code currently involves a significant human effort.
(ii) {\em Diverse teams}:
Using AI Software Engineer excessively may reduce diversity of thought\footnote{See \url{https://medium.com/@LuisMizutani/ai-for-software-development-can-we-actually-measure-its-impact-31af45dccdab}} in a development team. Thus, co-operative intelligence across AI-AI and AI-human combinations remains a topic of discourse.
(iii) {\em Costs}: Even though the automation provided by AI promises to reduce costs, an AI software engineer still needs to be cost-sensitive. Program analysis tools invoked by LLM agents can help make only {\em relevant} queries to LLMs \cite{acr24}.
(iv) {\em Datasets:} Last but not the least, the community needs datasets. The latest dataset announced in this regard is the SWE-Lancer \cite{lancer25}. We need datasets for software engineering tasks which support comprehensive patch validation (possibly by supporting test generation), and, ideally, cover more capabilities of the AI software engineer than patching,  as we discuss in the following.
.


\section{Outlook}

With LLM agents performing some of the heavy lifting in software maintenance and software quality control, it may be useful to think about how they will fit into future development workflows. Will there be separate agents for coding, testing, debugging, repair, and various software engineering tasks? The more agents there are, the more the maintainability of these agent may become an issue!  Instead of building hyper-specialized agents for individual software engineering tasks, it could be worthwhile to craft and maintain a {\em unified software engineering agent} (USE-agent) that can be configured to deliver different capabilities.
Such an agent of the future may conduct myriad tasks, including code generation, test generation, bug fixing, and feature addition. Software engineering can then be conducted by clients that rely on the unified agent, with human developers vetting the results of the agent wherever needed.
With newer models like o1 and DeepSeek R1 being developed, some of the current agentic capabilities can be co-opted by LLMs in the future, and hence it is useful to think about more ambitious, unified agents.
 
 










\begin{acks}
The authors acknowledge discussions with attendees at  Dagstuhl Seminar 24431 on Automated Programming and Program Repair (Oct 2024), as well as at Shonan Meeting 217 on Trusted Automatic Programming (Jan 2025). Lingming Zhang shared valuable pointers. %This work is partially supported by a Singapore Ministry of Education( MoE) Tier3 grant MOE-MOET32021-0001.
\end{acks}
%\newpage
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}


%%
%% If your work has an appendix, this is the place to put it.





\end{document}
