\section{Related work}
\paragraph{System design} 

The composition design problem in PCMAS aligns with contract design \citep{dutting2023multi} and algorithmic mechanism design \citep{nisan2001algorithmic}, focusing on optimizing controllable agents' composition and operation to enhance system performance. Recent works on fleet design highlight fleet size's impact on system efficiency \citep{molina2024bayesian,barrios2014fleet,cabrera2014fleet}. While these works introduced a BO framework for robot fleet design, they often face substantial computational challenges. Similarly, research in PCMAS's mixed-autonomy domain has proposed frameworks for learning policies applicable to both human-driven and autonomous vehicles \citep{xie2023two}. However, these studies overlook system-level optimization from a compositional perspective, equilibrium considerations, and convergence stability concerns. Concurrently, advancements in reward design have explored single-loop approaches integrating reward design with agent learning \citep{yang2022adaptive,li2020end,guresti2023iq}. Despite progress, these methods face limitations such as restrictive theoretical assumptions, stability issues, and the lack of equilibrium-based solutions.

\paragraph{Computational efficiency}

On the other hand, research has increasingly focused on addressing the computational efficiency of MARL. Mean-field reinforcement learning (MFRL) simplifies interactions by approximating other agents' actions as a mean action \citep{yang2018mean}. However, MFRL often relies on historical mean actions to compute current actions, potentially causing delays in agent interactions. To overcome this, recent studies have developed methods to predict mean actions more effectively, which have been incorporated into our work \citep{zhou2020multi,li2024beyond}. Meanwhile, hypernetworks, which are specialized neural networks designed to generate weights for other networks, enable dynamic adaptation, improved generalization, and reduced trainable parameters by leveraging shared structures across tasks \citep{chauhan2023brief}. They have been applied in domains such as AutoML, zero-shot learning, multitasking, and RL. For instance, in computer vision, hypernetworks dynamically adjust feature extraction layers. In RL, they have been used in QMIX \citep{rashid2020monotonic} to mix individual Q-values and explored as alternatives to standard Q-function architectures \citep{sarafian2021recomposing}. Recently, population-size-aware policy optimization (PAPO) was introduced to bridge finite-agent and infinite-agent game theory in mean-field games \citep{li2023population}. However, its reliance on single-agent RL limited equilibrium guarantees in multi-agent systems. While hypernetworks offer strong generalization capabilities, challenges like potential accuracy losses remain only partially addressed. Despite their widespread use across fields, applying hypernetworks to mechanism design in systems with non-cooperative agents remains an open research area.