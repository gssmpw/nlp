%\documentclass{article}
\documentclass[conference] {IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Selecting Optimal Sampling Rate for Stable Super-Resolution}
%\author{Nuha Diab}
\author{
    Nuha Diab \\
    {\small Department of Applied Mathematics, Tel-Aviv University, Tel-Aviv, Israel} \\
    {\small nuhadiab@tauex.tau.ac.il}
}
\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage{courier}
\usepackage{subcaption}
\usepackage{comment}
\usepackage{etex}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage{courier}
\usepackage{comment}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{xcolor}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[normalem]{ulem}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\begin{document}

\maketitle

\begin{abstract}
    We investigate the recovery of nodes and amplitudes from noisy frequency samples in spike train signals, also known as the super-resolution (SR) problem. When the node separation falls below the Rayleigh limit, the problem becomes ill-conditioned. Admissible sampling rates, or decimation parameters, improve the conditioning of the SR problem, enabling more accurate recovery. We propose an efficient preprocessing method to identify the optimal sampling rate, significantly enhancing the performance of SR techniques.
\end{abstract}
\section{Introduction}
%\begin{enumerate}
%    \item SR problem (with complex coefficients)
%    \item conditioning of the problem
%   \item decimation technique (dima's theorem of existence of good $\lambda$)
%    \item used in practice (decimated prony + vexpa)
%    \item NP-hard problem to find good $\lambda$'s 
%    \item find how the singular values of the Toeplitz matrix are related to $\Delta$ and number of clusters (spectrum of T)
%\end{enumerate}
\par Consider the following "spike train" signal:
\begin{equation}\label{spike}
    \mu (x) := \sum_{i=1}^n a_j\delta_{x_j}, \quad a_j \in \mathbb{C}, \quad x_j \in \mathbb{T}:=
    \mathbb{R}\mod{2\pi}.
\end{equation}
where $\delta_x$ is the Dirac $\delta$-distribution. We refer to $\{x_j\}_{j=1}^n$ as the nodes and to $\{a_j\}_{j=1}^n$ as the coefficients.
Given noisy band-limited Fourier measurements $\hat{\mu}_{\epsilon}(\omega) = \hat{\mu}(\omega) + e(\omega) $, where
\begin{equation}\label{eq:samples}
    \hat{\mu}(\omega) = \sum_{j=1}^na_j\exp{(i\omega x_j)} , \quad \omega \in [-\Omega,\Omega]
\end{equation}
where $\Omega > 0$ and $\|e(\omega)\|_{\infty} \leq \epsilon$ for some $\epsilon > 0$, the goal is to recover $\{a_j, x_j\}_{j=1}^n$ in equation \eqref{spike}.
This \textbf{Super-resolution} (SR) problem is an inverse problem of great theoretical and practical interest, with diverse applications in optics, imaging, inverse scattering, signal processing, spectroscopy, and data analysis in general \cite{donoho1992a, bertero1996iii,lindberg2012mathematical, ammari2005music, stoica2005spectral,mulleti2017super,bhandari2016signal}. 
%{A primary challenge involves establishing computational limits of resolution and developing optimal inversion algorithms given \textit{prior} knowledge about the object class and the inherent limitations of the measurement system. }

%\par The \textit{Resolution} (also referred to as the Rayleigh Length) is $1\over N$. Let $\Delta$ be the smallest distance between the nodes, we define the \textit{super-resolution factor} (SRF) as ${{1\over N}/ \Delta} = {1\over N\Delta}$. This quantity measures the difficulty of the problem. We are interested in the case where SRF$>> 1$, equivalently when we have an extrapolation problem for which we lack information about $k \geq N$ [cite donoho?].
\begin{comment}
    A closely related object is the Vandermonde matrix
\begin{equation}\label{eq:Van}
        V(x_1,\dots,x_n;N) := \big[\exp{\imath k x_j }\big]_{k=0,\dots N-1}^{j=1,\dots,n}.
\end{equation}
Rewriting equation \eqref{eq:samples} we get:
\begin{equation}
    \hat{\mu} = Va
\end{equation}
Where $\hat{\mu}:=(\hat{\mu}(0),\dots,\hat{\mu}(N-1))^T$, $V:=V(x_1,\dots,x_n;N)$ and $a=(a_1,\dots,a_n)^T$. Thus the conditioning of the problem is directly related to the condition number of $V$. The Vandermonde matrix \( V \) plays a crucial role in determining the optimal reconstruction error, also known as the min-max error, of the problem. For further insights, please refer to [cite].
\end{comment}
\begin{comment}
    \par When the nodes are spaced apart by at least \( \frac{1}{N} \), the matrix \( V \) is said to be well-conditioned. However, if the nodes are separated by \( \Delta \ll \frac{1}{N} \), where \( \Delta \) represents the smallest distance between the nodes, certain conditions lead to a notable observation: the smallest singular value of \( {1\over \sqrt{N}}V \) scales with an asymptotically tight rate of approximately \( (N\Delta)^{\ell -1} \). Here, \( \ell \leq n \) is the maximal number of nodes forming a "cluster".
\end{comment}

\par Let the SR factor be defined as $SRF:=(\Delta\Omega)^{-1}$, where $\Delta$ is the smallest separation of the nodes. When $SRF \gg 1$, the SR problem becomes very ill-conditioned. In this regime, the nodes form 'clusters'. An important goal is to prove that certain algorithms are optimal, attaining the optimal reconstruction bounds known as the Min-max error bounds \cite{Batenkov2021}. Extensive research has been conducted on this model regarding its stability analysis and algorithms \cite{li2020super,liu2021theory,derevianko2023esprit,batenkov2023super,demanet2015recoverability,morgenshtern2016super,diederichs2018sparse,kunis2018condition,fan2023efficient}.

\par The utilization of the Decimation technique was pioneered in \cite{Batenkov2021} to establish bounds on the Min-max error in the SR regime. The decimation method proceeds as follows: the spectral range $[-\Omega, \Omega]$ is uniformly sampled at a rate $\rho$, and subsequently, a system of equations of the form $\hat{\mu}(\rho k) = \sum_{j=1}^n a_j \exp(i\rho kx_j)$ is obtained. However, not all $\rho$'s are desirable or suitable; some may result in collisions and the formation of new clusters, thus worsening the ill-conditioned nature of the problem. The existence of admissible $\rho$'s within a certain interval was first proved in \cite{Batenkov2021}. An optimal approach proposed in \cite{Batenkov2021} involves employing an oracle to obtain the correct value of the decimation parameter $\rho$.
%We aim to develop a method for automatically determining an optimal decimation parameter $\lambda$.
\par The Decimation method has already been applied in SR algorithms, such as the Decimated Prony Method (DP) \cite{katz2023decimated} and VEXPA \cite{Briani2020}, to improve the conditioning of the problem (see also \cite{plonka2021deterministic}). Using decimation as a preprocessing step in any SR method with a suitable $\rho$ yields a set of solutions $\{e^{\imath \rho x_j}\}_{j=1}^n$ instead of $\{e^{\imath x_j}\}_{j=1}^n$. This new set of solutions forms a well-separated configuration compared to the undecimated one, making recovery easier and more accurate.
\par Furthermore, the optimality of DP is established due to the optimality of Prony's method for $\Omega=2n$ combined with decimation \cite{katz2023accuracy}. \textit{Despite its practical usage, there is currently no mathematical evidence supporting the identification of such admissible decimation parameter}.
\par In this paper, our objective is to develop a method to automatically select an optimal decimation parameter \( \rho \) within a given interval, ensuring effective separation between the nodes.
%The procedure is outlined as follows: Given a certain set of decimation parameters \( \{\lambda_i\}_{i=1,\dots,p} \), we approximate the smallest distance between the decimated nodes \( \Delta_{\lambda_i} \) for each \( \lambda_i \). Subsequently, we select \( \lambda_i \) with the highest value of the approximated \( \Delta_{\lambda_i} \). 
Towards that goal, we find the spectrum of the square Vandermonde matrix of the samples \eqref{eq:samples}, showing its dependence on \( \Delta \) and on the number of clusters the nodes form (similarly to \cite{batenkov2021spectral} for rectangular Vandermonde matrices with number of samples $N \gg 2n$). Consequently, we find the spectrum of the Toeplitz matrix of the samples \eqref{teoplitz}, which is our main tool to select the optimal decimation parameter. Lastly, we present the Enhanced Decimated Prony's method (EDP) and the Decimated Matrix Pencil method (DMP), demonstrating their time-complexity advantage over both DP and the Matrix Pencil Method (MP) \cite{hua1990matrix} and then show the numerical optimality of EDP.
%Additionally, we provide perturbation analysis for the de-aliasing process (which involves recovering $x_j$ from the decimated set $\{e^{\imath \lambda x_j}\}_{j=1,\dots,n}$), as introduced in \cite{cuyt2020get}.

\section{Preliminaries}
We recall some definitions from \cite{batenkov2021spectral}.
\begin{definition}%[Wrap-around distance]
    For $x \in \mathbb{R}$, we denote
    \[ \|x\|_{\mathbb{T}}:=\big|\arg{(e^{\imath x})}\big| = \big|x \mod{(-\pi,\pi]}\big|,\]
    where $\arg(z)$ is the principal value of the argument of $z \in \mathbb{C}\setminus\{0\}$, taking values in $(-\pi,\pi]$.
\end{definition}
For a set of $n$ distinct nodes $X:=\{x_j\}_{j}^n$ with $x_j \in (-{\pi \over 2},{\pi \over 2}]$, we introduce the following definitions.
\begin{definition}\label{def:min-sep}
    Define the \textbf{minimal separation} of the set $X$ as
    \[\Delta = \Delta(X) := \min_{i\neq j}\|x_i-x_j\|_{\mathbb{T}}.\]
    In addition, for any $\rho \in \mathbb{R}$ we define 
     \[\Delta_{\rho} = \Delta(\rho X) := \min_{i\neq j}\|\rho x_i-\rho x_j\|_{\mathbb{T}}.\]
\end{definition}
\begin{definition}[Single cluster configuration]
    The set of nodes $X$ is said to form an $(\Delta, \nu, n)$-cluster if 
    \[
        \forall x,y \in X, x\neq y: \Delta \leq \|y-x\|_{\mathbb{T}} \leq \nu\Delta.
    \]
\end{definition}
\begin{definition}[Multi-cluster configuration]\label{def:multi-cluster}
    The set of nodes $X$ is said to form an $((h^{(j)}, \nu^{(j)}, n^{(j)})_{j=1}^M, \eta)$-clustered configuration if there exist an $M$-partition $X=\bigcup_{j=1}^M\mathcal{C}^{(j)}$, such that for each $j\in \{1,...,M\}$ the following conditions are satisfied:
    \begin{enumerate}
        \item $\mathcal{C}^{(j)}$ is an $(h^{(j)}, \nu^{(j)}, n^{(j)})$-cluster.
        \item $\|x-y\|_{\mathbb{T}} \geq \eta > 0$, $\forall x \in \mathcal{C}^{(j)}, \forall y \in X \setminus \mathcal{C}^{(j)}$.
    \end{enumerate}
\end{definition}
\begin{definition}\label{def:van}
    For a finite set of nodes $X$ and sampling set $S$, we define the corresponding \textbf{Vandermonde} matrix as 
    \[
        V(X;S) = \big[e^{ikx}\big]_{k \in S}^{x\in X}.
    \]
\end{definition}
\begin{comment}
    {\begin{definition}
    The set of nodes $X:=\{x_j\}_{j=1,\dots,n}$, $x_j \in [-{\pi \over 2},{\pi \over 2})$ forms a $(\Delta,\eta,n,\ell,\nu)$-clustered configuration for some $\Delta > 0$, $2 \leq \ell, \leq n$, $\ell -1 \leq \nu < {\pi \over \Delta}$ and $\eta \geq 0$ if for each $x_j$ there exists at most $\ell$ distinct nodes 
    \[
        X^{(j)} = \{x_{j,k}\}_{k=1,\dots,r_j} \subset X, \quad 1\leq r_j \leq \ell, \quad x_{j,1}:=x_j,
    \]
    such that the following conditions are satisfied:
    \begin{enumerate}
        \item For any $y \in X^{(j)}\setminus \{x_j\}$, we have 
        \[ \Delta \leq \|y-x_j\|_{\mathbb{T}} \leq \nu\Delta.\]
        \item For any $y \in X \setminus X^{(j)}$, we have 
        \[ \|y-x_j\|_{\mathbb{T}} \geq \eta.\]
    \end{enumerate}
\end{definition}}
\end{comment}
Let $\sigma_1(L) \geq \sigma_2(L) \geq \dots \geq \sigma_{n}(L)$ denote the singular values of a matrix $L \in \mathbb{C}^{n\times n}$ and let $\lambda_1(L) \geq \lambda_2(L) \geq \dots \geq \lambda_n(L)$ denote it's eigenvalues.

\section{Identification of an Optimal Rate}
In this section, we establish a measure for approximating the minimal separation $\Delta_{\rho}$ between decimated nodes with decimation parameter $\rho$. For $M$ clusters, we prove in proposition \ref{pro:teop-eig} that $\sigma_{M+1}(T_{\rho}) \asymp \Delta_{\rho}^2$, where $\asymp$ denotes asymptotic scaling. 
%By Proposition 5.8 in \cite{Batenkov2021} and Lemma 4.1 in \cite{batenkov2020}, they prove that such admissible decimation parameters exist in some interval with probability of $\geq \frac{1}{2n^2}$.
%\par Now, we highlight the primary insight guiding the selection of an optimal decimation parameter. According to Corollary 2.1 in \cite{batenkov2021spectral}, the quantity of singular values of $V$ scaling in proportion to $\Delta^{(j-1)}$ corresponds to the count of clusters with multiplicity of at least $j$ {\color{red} (for some conditions on the nodes configuration...)}. Consequently, if the count of clusters with multiplicity of at least $1$ is $k$, the $k+1$th singular value of $V$, denoted as $\sigma_{k+1}(V)$, scales similarly to $\Delta$, effectively approximating it. Hence, to evaluate whether a specific decimation parameter $\rho$ is favorable or unfavorable, we can calculate $\sigma_{k+1}(V(\rho X;2n))$, where $\rho X = \{\rho x | x \in X\}$.
\begin{comment}
Because $V$ is not given, we can compute the $k+1$ singular value of the Hankel matrix $V^TAV$, and show that $\sigma_{k+1}(V^TAV) \asymp \sigma_{k+1}(V^*AV) \asymp \sigma_{k+1}(V^*V) \asymp \sigma_{k+1}^2(V) \asymp \Delta^2$. 
\[ V^TAV = (V^*-2Im(V^*)i)AV = (V^* - 2\frac{V-\hat{V}}{2i}i)AV = V^*AV - VAV + \hat{V}AV \]
\[= [(1-i)I + iVA^2V^{-*}]V^*AV\]
Thus we get that
\[ \sigma_j(V^*AV)\sigma_n((1-i)I + iVA^2V^{-*}) \leq \sigma_j(V^TAV) \leq \sigma_j(V^*AV)\sigma_1((1-i)I + iVA^2V^{-*})\]
On the other hand,
\[ \sigma_j^2(V^*AV) = \lambda_j(V^*A^*VV^*AV) = \lambda_j(VV^*A^*VV^*A) = \lambda_j(A^{\frac{1}{2}}VV^*A^{\frac{1}{2}}A^{\frac{1}{2}}VV^*A^{\frac{1}{2}})\]
\[ = \sigma_j^2(A^{\frac{1}{2}}VV^*A^{\frac{1}{2}}) \]
Thus we get that
\[ \sigma_j(V^*V)\sigma_n(A)\sigma_n((1-i)I + iVA^2V^{-*}) \leq \sigma_j(V^TAV) \leq \sigma_j(V^*V)\sigma_1(A)\sigma_1((1-i)I + iVA^2V^{-*})\]
For the following term, we have
\[ \sigma_1((1-i)I + iVA^2V^{-*}) \leq \sigma_1((1-i)I) + \sigma_1(iVA^2V^{-*}) \geq 2 + \sigma_1(A^2)\]
\[ \sigma_n((1-i)I + iVA^2V^{-*}) \geq \sigma_1((1-i)I) - \sigma_n(iVA^2V^{-*}) \geq 2 - \sigma_n(A^2)\]
Finally, we proved that
\[ C_1(A)\sigma_j(V^*V) \leq \sigma_j(V^TAV) \leq C_2(A)\sigma_j(V^*V)\]
where $C_i(A)$ are a function of $A$.
\end{comment}
\subsection{Main result and proofs}
%\par Since the Vandermonde matrix isn't directly available, we aim to identify a structured matrix $T$ that incorporates Vandermonde matrices within its decomposition. \\
Let $\hat{\mu}_k := \hat{\mu}(k) = \sum_{j=1}^na_je^{ikx_j}$ be the Fourier measurement of model \eqref{spike}.
For a set of nodes $X := \{x_1,...,x_n\}$, the Toeplitz matrix of the samples is defined as:
\begin{equation}\label{teoplitz}
    %T = T(X;n) := \begin{bmatrix}
    %        \mu_{n-1} & \mu_{n} & \dots & \mu_{2n-2} \\
    %        \vdots \\
    %        \mu_{0} & \mu_1 & \dots & \mu_{n-1} \\
    %    \end{bmatrix}
        T = T(X;n) := \begin{bmatrix}
            \mu_{n-1} & \mu_{n-2} & \dots & \mu_{0} \\
            \vdots & \vdots &  & \vdots \\
            \mu_{2n-1} & \mu_1 & \dots & \mu_{n-1} \\
        \end{bmatrix}.
\end{equation}
and it can be decomposed into:
\begin{align*}
    T = \tilde{V}_{n}\underbrace{diag(a_1,\dots,a_n)}_{A}V_n^*,
\end{align*}
where 
\begin{align*}\label{eq:van}
    &\tilde{V}_{n} := V(X;\{n-1,\dots,2n-2\}), \\ &V_n := V(X;\{0,\dots,n-1\}).
\end{align*}

\begin{comment}
   \begin{align*}
    T &= \underbrace{\begin{bmatrix}
            e^{inx_1} & \dots & e^{inx_{n}} \\
            \vdots \\
            e^{i2nx_1} & \dots & e^{i2nx_{n}} \\
        \end{bmatrix}}_{V_n}
        \underbrace{\begin{bmatrix}
            a_1 & \dots & 0 \\
            \vdots \\
            0 & \dots & a_n \\
        \end{bmatrix}}_{A}
        \underbrace{\begin{bmatrix}
            e^{i0x_1} & \dots & e^{i0x_{n}} \\
            \vdots \\
            e^{inx_1} & \dots & e^{inx_{n}} \\
        \end{bmatrix}^*}_{V_1} \\
        &= V_nAV_1^*
\end{align*} 
\end{comment}

We have 
\[ 
    \tilde{V}_n := V_ndiag(e^{i(n-1)x_1},...,e^{i(n-1)x_n}) = V_nE,
    \]
then we can write 
\begin{equation}\label{eq:sq-van}
    T = V_nDV_n^*, \quad D=EA.
\end{equation}

\begin{comment}
Let $V$ be the vandermonde matrix of the nodes and $A$ diagonal complex matrix. then we can write:
\[ V^*AV = V^*(A_R + iA_I)V = V^*A_RV+iV^*A_IV\]
where $A_R$ and $A_I$ are the real and imaginary part of $A$.
Let $A=U\Sigma T^*$ be the svd decomposition of V. thus
\[
    V^*AV = V^*A_RV+iV^*A_IV = T\Sigma U^*A_RU\Sigma T^* + iT\Sigma U^*A_IU\Sigma T^*
\]
\[= TP_1A_R^{1\over 2}U^*\Sigma^2U{A^*_R}^{1\over 2}P_1^*T^* + iTP_2A_I^{1\over 2}U^*\Sigma^2U{A^*_I}^{1\over 2}P_2^*T^*
\]
the following lemma proves the last step above:
\[ \Sigma UDU^*\Sigma = \Sigma UD^{1\over 2}D^{1 \over 2}U^*\Sigma^*\]
\[
    \lambda_j(\Sigma UDU^*\Sigma) = \sigma^2(\Sigma UD^{1\over 2}) = \lambda_j(D^{1\over 2}U^*\Sigma^2UD^{1\over 2})
\]
thus we get that we can write:
\[ \Sigma UDU^*\Sigma = P_1D^{1\over 2}U^*\Sigma^2UD^{1\over 2}P_1^*\]
where $P_1$ is an orthogonal matrix.\\
Denote by $T_1:=TP_1A_R^{1\over 2}U^*$ and $T_2:=TP_2A_I^{1\over 2}U^*$. we get:
\[ 
    |\lambda_j(V^*AV)| = |\lambda_j(T_1\Sigma^2T_1^*+iT_2\Sigma^2T_2^*)| = \lambda_j(T_1\Sigma^2T_1^*)+i\lambda_j(T_2\Sigma^2T_2^*)= \theta_j^{(1)}\lambda_j(\Sigma^2)+\theta_j^{(2)}\lambda_j(\Sigma^2)
    \]
\end{comment}
\begin{theorem}\label{thm:main}
    Let $Q \in \mathbb{C}^{n\times n}$ be a matrix of the form $Q = V^*DV$, where $D$ is a diagonal complex matrix and $V \in \mathbb{C}^{n\times n}$. Then we have:
    \begin{equation}
        \sigma_i(Q) = \big|\lambda_i(Q)\big| = \theta_i\lambda_i(VV^*), \quad |D| := (DD^*)^{1\over 2}.
    \end{equation}
    where $\lambda_n((DD^*)^{1\over 2}) \leq \theta_i \leq \lambda_1((DD^*)^{1\over 2})$.
\end{theorem}

\begin{proof}
    \begin{align*}
        \big|\lambda_i(V^*DV)\big|^2 &= \lambda_i(V^*DV) \bar{\lambda}_i(V^*DV)  \\&= \lambda_i(V^*DV)\lambda_i(V^*D^*V).
    \end{align*}
    from properties of eigenvalues:
    \begin{align*}
        &\mu_i := \lambda_i(V^*DV) = \lambda_i(VV^*D) = \lambda_i(D^{1\over 2}VV^*D^{1\over 2}), \\
        &\bar{\mu}_i := \lambda_i(V^*D^*V) = \lambda_i(VV^*D^*) = \lambda_i((D^*)^{1\over 2}VV^*(D^*)^{1\over 2}).
    \end{align*}
    and thus 
    \begin{align*}
        &D^{1\over 2}VV^*D^{1\over 2}u_i = \mu_iu_i, \\
        &u^*_i(D^*)^{1\over 2}VV^*(D^*)^{1\over 2} = \bar{\mu}_i u^*_i.
    \end{align*}
    Note that these matrices are not necessarily Hermitian.
    \begin{align*}
        &u^*_i(D^*)^{1\over 2}VV^*(D^*)^{1\over 2}D^{1\over 2}VV^*D^{1\over 2}u_i = \bar{\mu}_i\mu_i u^*_iu_i, \\
        &u^*_i(D^*)^{1\over 2}VV^*|D|VV^*D^{1\over 2}u_i = \bar{\mu}_i\mu_i u^*_iu_i, \\
        &u^*_i(D^*)^{1\over 2}VV^*D^{1\over 2}(D^*)^{1\over 2}VV^*D^{1\over 2}u_i = \bar{\mu}_i\mu_i u^*_iu_i, \\
        &\frac{\|(D^*)^{1\over 2}VV^*D^{1\over 2}u_i\|_2^2}{\|u_i\|_2^2} = \mu_i\bar{\mu}_i = \sigma_i^2((D^*)^{1\over 2}VV^*D^{1\over 2}), \\
        &\sigma_i((D^*)^{1\over 2}VV^*D^{1\over 2}) = \lambda_i((D^*)^{1\over 2}VV^*D^{1\over 2}).
        %= \lambda_i(VV^*|D|)
    \end{align*}
    Where the last equivalence is true because $(D^*)^{1\over 2}VV^*D^{1\over 2}$ is a positive semi-definite hermitian matrix.
    By Ostrowski's Theorem for Hermitian matrices (Theorem 4.5.9 in \cite{horn2012matrix}), we have
    \[ \lambda_i(D^{1\over 2}VV^*(D^*)^{1\over 2}) = \theta_i\lambda_i(VV^*), \]
    where 
        \[
            \lambda_n((DD^*)^{1\over 2}) \leq \theta_i \leq \lambda_1((DD^*)^{1\over 2}).
        \]
    Lastly we have, $\big|\lambda_i(V^*DV)\big| = \sqrt{\lambda_i(V^*DV)\cdot \bar{\lambda}_i(V^*DV)} = \sqrt{\lambda_i(V^*D^*VV^*DV)} = \sigma_i(V^*DV)$.
\end{proof}

%\begin{theorem}[Corollary 2.1 in \cite{batenkov2021spectral}]\label{thm:eig-van}
%    Let $X$ form an $((h^{(j)}, \nu^{(j)}, n^{(j)})_{j=1}^M, \eta)$-clustered configuration, and furthermore suppose that $h^{(1)} = h^{(2)} = \dots = h^{(M)} = \Delta(X)$. For each $j = 1,2,\dots, \max_k n^{(j)}$, define 
%    \[
%        \ell_j := \#\{1\leq k\leq M: n^{(k)} \geq j\}.
%    \]
%    Then, there exist constants $C_{1}$ and $C_{2}$ such that for all $N\eta \geq C_1$ and $N\Delta \leq C_2$ there are precisely $\ell_j$ singular values of $V(X;N)$ scaling like $\asymp N^{1\over 2}(N\Delta)^{j-1}$. All the constants in the statement depend only on $(n^{(j)}, \nu^{(j)})_{j=1}^M$. 
%\end{theorem}
From now on, we choose $X$ to form a $((h^{(j)}, \nu^{(j)}, n^{(j)})_{j=1}^M, \eta)$-clustered configuration, as defined in Definition~\eqref{def:multi-cluster}, with $h^{(1)} = h^{(2)} = \dots = h^{(M)} = \Delta(X)$.

\begin{theorem}\label{thm:lower-bnd}
    There exist $C_1, C_2$ depending on $s:= \max_j n^{(j)}$ such that for $\eta \geq C_1$ and $\Delta \leq {C_2 \over n^2\nu}, \nu := \max_k \nu^{(k)}$, for each $m=1,...,s$ there are precisely $\ell_m := \# \{1\leq k \leq M : m \leq n^{(k)}\}$ singular values of $V_n:=V_n(X)$ bounded below by
    \[
        C\Delta^{m-1},
    \]
    where $C$ doesn't depend on $\Delta$.
\end{theorem}
\begin{proof}
    Recall the following Theorem:
    \begin{theorem}[Proposition 7.1 in \cite{batenkov2021single}]
        Let $\tilde{\sigma}_1 \geq \tilde{\sigma}_2 \geq \dots \geq \tilde{\sigma}_n$ denote the union of all the singular values of the matrices $V_n^{(j)}:= V(\mathcal{C}^{(j)};\{0,\dots,n-1\})$ in non-increasing order, and ${\sigma}_1 \geq {\sigma}_2 \geq \dots \geq {\sigma}_n$ denote the singular values of $V_n$. Then there exist $C_1(s), C_2(s)$ depending only on $s:= \max_j{n^{(j)}}$ such that for $\eta \geq C_1$ and $\Delta \leq {C_2 \over n^2\nu}, \nu := \max_k \nu^{(k)}$ we have 
        \[
            \sigma_j \geq {1\over 2}\tilde{\sigma}_j, \quad j=1,...,n.
        \]
    \end{theorem}
    We have to find the scaling of the singular values of each $V_n^{(j)}$ to finish the proof.
    For simplicity, assume that $n=2p+1$.
    %and let $\mathcal{C}^{(j)}=\{c_1,...,c_{n^{(j)}}\} \subseteq X$.
    Thus we can write,
    \[
        U_p^{(j)} = \big[e^{ikx}\big]_{k \in \mathcal{G}_p}^{x\in \mathcal{C}^{(j)}}.
    \]
%   \[
%       U_p(\mathcal{C}^{(j)},\mathcal{G}_p) := \begin{bmatrix}
%           e^{-ipc_1} & \dots & e^{-ipc_{n^{(j)}}} \\
%           \vdots \\
%           e^{ipc_1} & \dots & e^{ipc_{n^{(j)}}} \\
%       \end{bmatrix} = V_n^{(j)}D_j,
%   \]
    %$D_j:=diag{(e^{-ipc_1}, \dots , e^{-ipc_{n^{(j)}}})}$ 
    where $D_j:= diag(e^{-ipx})_{x \in \mathcal{C}^{(j)}} \in \mathbb{C}^{n^{(j)}\times n^{(j)}}$ and $\mathcal{G}_p:=\{-p,-p+1,...,0,...,p\}$ is the symmetric one-dimensional grid. Let $G_j := \big(U_p^{(j)}\big)^*U_p^{(j)}$ for $j=1,...,M$. Now we use the following Lemma and write it for the uni-variate case:
    \begin{lemma}[Lemma 3 in \cite{diab2024spectral}]
         Let $V_{\leq m}(\mathcal{G}) := [g^k]_{g\in \mathcal{G}}^{0\leq k \leq m}$. Then the sampling set $\mathcal{G}_p$ satisfies the condition $rank(V_{\leq n^{(j)} - 1}(\mathcal{G}_p)) \geq n^{(j)}$ and the eigenvalues of $G_j$ split into $n^{(j)}$ groups:
         \begin{align*}
             &\lambda_0 = \Delta^0(\tilde{\lambda}_0 + O(\Delta)), \quad \lambda_1 = \Delta^2(\tilde{\lambda}_1 + O(\Delta)), ..., \\ & \lambda_{n^{(j)}} = \Delta^{2(n^{(j)}-1)}(\tilde{\lambda}_{n^{(j)}} + O(\Delta)).
         \end{align*}
         where $\tilde{\lambda}_i$ doesn't depend on $\Delta$.
    \end{lemma}
    Since $\big(V_n^{(j)}\big)^*V_n^{(j)} = (D_j^{-1})^*G_jD_j^{-1}$, we finish the proof of Theorem \ref{thm:lower-bnd}.
\end{proof}
\begin{theorem}\label{thm:gram-eig}
    The singular values of the Vandermonde matrix $V_n$ scale as follows:
    \begin{align*}
        &\{\sigma_{1,i}(V_n)\}_{i=1}^{\ell_1} \asymp {\Delta}^{0}, \dots, \{\sigma_{s,i}(V_n)\}_{i=1}^{\ell_s} \asymp {\Delta}^{s-1},
    \end{align*}
    where $s := \max_k n^{(j)}$. 
\end{theorem}
\begin{proof}
    The determinant of the Vandermonde matrix $V_n$ is: 
    \[
        det(V_n) = \prod_{k\neq j}|e^{ix_j} - e^{ix_k}|.
    \]
    Using Lemma 5.6 in \cite{Batenkov2021}, for $|x - x'| \leq {\pi \over 2}$ we have 
    \[
        {2\over \pi}|x - x'| \leq |e^{ix} - e^{ix'}| \leq |x - x'|.
    \]
    Thus we get that
    \begin{align*}
        det(V_n) &= \prod_{{\scriptscriptstyle j=1}}^{\scriptscriptstyle M}\prod_{{\scriptscriptstyle x, y \in \mathcal{C}^{(j)}}}|e^{ix} - e^{iy}|\prod_{{\scriptscriptstyle  j \neq i}}\prod_{{\scriptscriptstyle x \in \mathcal{C}^{(j)}, y \in \mathcal{C}^{(i)}}}|e^{ix} - e^{ix'}|  \\ &= C(\eta,\nu,n)\prod_{{\scriptscriptstyle j=1}}^{\scriptscriptstyle M}\prod_{{\tiny x, y \in \mathcal{C}^{(j)}}}|e^{ix} - e^{iy}| = C\Delta^{\sum_{j=1}^M {n^{(j)}\choose 2}}.
    \end{align*}
    Now we show that
    \begin{align*}
        \sum_{j=1}^M {n^{(j)}\choose 2} = \sum_{j=1}^M {n^{(j)}(n^{(j)}-1) \over 2} &= \sum_{j=1}^M \sum_{i=1}^{n^{(j)}}(i-1) \\ &= \sum_{i=1}^s \ell_i(i-1).
    \end{align*}
    where the last step is due to the definition of $\ell_j$.
    Thus we have that $det(V_n) = C\cdot \Delta^{\sum_{i=1}^s \ell_i(i-1)}$. It's known that $|det(V_n)| = \prod_{i=1}^n \sigma_i(V_n)$. Thus on the one hand we have $\prod_{i=1}^n \sigma_i(V_n) = C\cdot \Delta^{\sum_{i=1}^s \ell_i(i-1)}$ and on the other hand (by Theorem \ref{thm:lower-bnd}) we have that
    \begin{align*}
        \prod_{i=1}^n \sigma_i(V_n) \geq \prod_{i=1}^s \prod_{k=1}^{\ell_k}C_{i,k}\sigma_{i,k}(V_n) &= \prod_{i=1}^s\prod_{k=1}^{\ell_k}C_{i,k}\Delta^{i-1} \\ &= \tilde{C}\Delta^{\sum_{i=1}^s \ell_i(i-1)}.
    \end{align*}
    Since $C$ doesn't depend on $\Delta$, this forces the singular values to have exact scaling (i.e. $C_{i,k}$ don't depend on $\Delta$).
\end{proof}
Now we state our main result.
\begin{proposition}\label{pro:teop-eig}
     For any $\rho > 0$, the singular values of the Toeplitz matrix $T_{\rho}:=T(\rho X;n)$ scales as follows:
    \begin{align*}
        \{\sigma_{1,i}(T_{\rho})\}_{i=1}^{\ell_1} \asymp {\Delta}_{\rho}^{0}, \dots , \{\sigma_{s,i}(T_{\rho})\}_{i=1}^{\ell_s} \asymp {\Delta}_{\rho}^{2(s-1)},
    \end{align*}
    where $s := \max_j n^{(j)}$ and $\Delta_{\rho} := \Delta(\rho X)$.
\end{proposition}
\begin{proof}
    Combining Theorems \ref{thm:main} and \ref{thm:gram-eig}, we get the desired result.
    %Using Theorem 7.3 in \cite{batenkov2021single} we have a lower bound on the eigenvalues of the gramian matrix $G:=V^*V$. on the other hand, we can compute the determinant of this square vandermonde. thus combining the two result we get exact scaling of the eigenvalues of $G$ from pigon princple.
\end{proof}
\begin{remark}
    The constants in Proposition \ref{pro:teop-eig} also depend on the amplitudes $\{a_j\}_{j=1}^n$.
\end{remark}
\subsection{Numerical Validation}
Let $k \in \mathbb{N}$ be the number of clusters in a configuration with nodes in $X$. To validate Proposition \ref{pro:teop-eig}, we plotted the minimal separation of decimated nodes for all decimation parameters $\rho$ in the interval $\mathcal{I} := \big[{1\over 2}{\Omega \over 2n-1},{\Omega \over 2n-1}\big]$ against the $k+1$-th singular value of the Toeplitz matrix $T_{\rho}$. Figure \ref{fig:teo_sing} confirms that $\sigma_{k+1}(T_{\rho})$ scales proportionally to $\Delta_{\rho}^2$. 
%The right figure is for the single cluster scenario and the left figure is in the multi-cluster case.
\begin{figure}[htb]
  \includegraphics[width=0.49\linewidth]{figures/srf6.jpg} \hfill
  \includegraphics[width=0.49\linewidth]{figures/srf3.jpg}
  \caption{\small  (right) $X$ has one cluster of size $\ell_1$ with $SRF = 6$. (left) $X$ has two clusters of sizes $\ell_1$ and $\ell_2$ with $SRF = 3$. The plotted $\sqrt{\sigma_{k+1}(T_{\rho})}$ values are scaled by $C := \frac{n}{\Omega}$ for better visualization.
}
  \label{fig:teo_sing}
\end{figure}
\begin{remark}
    We expect our estimates in Proposition \ref{pro:teop-eig} to hold for noisy samples, using standard perturbation analysis for singular values. We leave it for future work.
\end{remark}
%\section{Perturbation Analysis}\label{PA}
%In this section, we establish similar bounds to Proposition \ref{pro:teop-eig} with the presence of noise.
%
%\begin{lemma}\label{lem:noise-mat}
%    Let $M \in \mathbb{C}^{n\times n}$ be a matrix of the form $M = V^*DV$, where $D$ is diagonal complex matrix and $V \in \mathbb{C}^{n\times n}$.  Then we have:
%    \[ \theta_i \lambda_i(VV^*)-\|E\|_2 \leq \sigma_i(M + E) \leq \theta_i \lambda_i(VV^*)+\|E\|_2\]
%    where $E \in \mathbb{C}^n$ and $\lambda_n((DD^*)^{1\over 2}) \leq \theta_i \leq \lambda_1((DD^*)^{1\over 2})$.
%\end{lemma}
%\begin{proof}
%    Combining Theorem \eqref{thm:main} and Lemma \eqref{lm:sum} in the Appendix, we get 
%    \[
%        \big| \sigma_i(V^*DV +E) - \sigma_i(V^*DV) \big| \leq \|V^*DV + E - V^*DV\|_2 = \|E\|_2 
%    \]
%\end{proof}
%\begin{theorem}
%    Let $X$ form an $((h^{(j)}, \nu^{(j)}, n^{(j)})_{j=1}^M, \eta)$-clustered configuration, and furthermore suppose that $h^{(1)} = h^{(2)} = \dots = h^{(M)} = \Delta(X)$. We have:
%    \begin{align*}
%        \big|\sigma_j(T+E) - \sigma_j(T)\big| \leq \|E\|_2
%    \end{align*}
%    where $E \in \mathbb{C}^{N\times N}$ and $j=1,...,N$. 
%\end{theorem}
%\begin{proof}
%    Combining Theorem \ref{thm:gram-eig} and Lemma \ref{lem:noise-mat}, we get the desired result.
%\end{proof}
%{\color{red} Where to use the above theorem...}


% Validating Proposition \ref{pro:teop-eig}. In this section we show that, as Proposition \ref{pro:teop-eig} states, 
\begin{comment}
    \begin{figure}[ht]
    \centering
    % First subplot (top left)
    \subfigure[$h_1 = 5$]{
        \includegraphics[width=0.45\textwidth]{figures/sc2.jpg}
        \label{fig:sc1}
    }
    % Second subplot (top right)
    \subfigure[$h_1 = 100$]{
        \includegraphics[width=0.45\textwidth]{figures/sc3.jpg}
        \label{fig:sc2}
    }
    % Third subplot (bottom left)
    \subfigure[$h_1 = h_2 = 3$]{
        \includegraphics[width=0.45\textwidth]{figures/mc4.jpg}
        \label{fig:mc1}
    }
    % Fourth subplot (bottom right)
    \subfigure[$h_1 = 20, h_2 =5$]{
        \includegraphics[width=0.45\textwidth]{figures/mc1.jpg}
        \label{fig:mc2}
    }
    \caption{A 2x2 grid of plots}
    \label{fig:grid}
\end{figure}
\end{comment}

\section{Algorithm}
A decimation parameter $\lambda \in \mathcal{I}$ is said to be admissible if it attains good separation properties, in particular, when the cluster nodes are separated by at least $O(\Omega\Delta)$ and the non-cluster nodes by $\geq {1\over 2n^2}$ (for the full details, see Proposition 5.8 in \cite{Batenkov2021}).
\begin{comment}
    First, let's discuss the probability of choosing a suitable decimation parameter. Recall the following theorem:
\begin{theorem}[Proposition 5.8 in \cite{Batenkov2021}]
    Let $X=(x_1,...,x_n)$ form an $((h^{(j)}, \nu^{(j)}, n^{(j)})_{j=1}^M, \eta)$-clustered configuration, and furthermore suppose that $h^{(1)} = h^{(2)} = \dots = h^{(M)} = \Delta(X)$ and $\tau :=\max_j\{\nu^{(j)}\}$. Let $\Omega \leq {2n-1 \over 2}\cdot {1\over \Delta}$. For each $\rho > 0$ let $z_{x}(\rho) = e^{i\rho x}$.\\
    Then, each interval $I \subset \big[{1\over 2}{\Omega \over 2n-1},{\Omega \over 2n-1}\big]$ of length $|I| = {1\over \eta}$ contains a sub-interval $I' \subset I$ of length $|I'| \geq {(2n^2\eta)}^{-1}$ such that for each $\rho \in I':$
    \begin{enumerate}
        \item For all $x \in (h^{(j)}, \nu^{(j)}, n^{(j)})$ with $1\leq j \leq M$ and $x_{\ell} \in (h^{(\ell)}, \nu^{(\ell)}, n^{(\ell)}), j \neq \ell$, 
        \[
            \arg\Big({z_{x}(\rho) \over z_{x'}(\rho)}\Big) \geq {1\over 2n^2}.
        \]
        \item For all $x,x' \in (h^{(j)}, \nu^{(j)}, n^{(j)}), x \neq x'$ with $1\leq j \leq M$,
        \[
            \arg\Big({z_{x}(\rho) \over z_{x'}(\rho)}\Big) \geq \pi\rho\tau\Delta \geq {\pi\tau \over 4n-2}\Omega\Delta.
        \]
        {\color{red} Check again!}
    \end{enumerate}
\end{theorem}
Thus the probability of choosing a suitable decimation parameter in the interval $\big[{1\over 2}{\Omega \over 2n-1},{\Omega \over 2n-1}\big]$ is $p:={1\over n^2}$.
\end{comment}

\subsection{Selecting an optimal sampling rate}
By Proposition \ref{pro:teop-eig}, for any \(\rho \in \mathbb{N}\), we have \(\sigma_{M+1}(T_{\rho}) \asymp \Delta_{\rho}^2\), where \(M\) is the number of clusters with multiplicity at least 1. For a set \(\Lambda = \{\rho_j,\}_{j=1}^m \subset \mathbb{N}\), we select $\rho = \arg\max_{j}\{\sigma_{M+1}(T_{\rho_j})\}_{j=1}^m$
as the optimal decimation parameter with the best separation properties.

Since decimation alone is insufficient for recovery, any SR method applied to samples at rate \(\rho\) gives nodes \(\{e^{i\rho x_j}\}_{j=1}^n\), with each \(e^{i\rho x_j}\) having \(\rho\) candidates. To resolve this ambiguity, we use the technique from \cite{cuyt2020get} for de-aliasing, which employs a second shifted sample set \(\mathcal{M}_{ds}:=\{\mu(\rho k+ t)\}_{k=0}^{2n-1}\) alongside \(\mathcal{M}_{d}:=\{\mu(\rho k)\}_{k=0}^{2n-1}\), where \(\rho\) and \(t\) are co-prime. 
%In the noiseless case, the two sets allow us to determine $e^{ix_j} $ by intersecting the candidates of \(e^{i\rho_* x_j}\) and \(e^{it x_j}\). See \cite{cuyt2020get, Briani2020} for more details.
Let $\Phi_j := \exp{(ix_j)}$. In the noiseless case, we have $\mu(\rho k) = \sum_{j=1}^n a_j \Phi_j^{\rho k}$ and $\mu(\rho k + t) = \sum_{j=1}^n (a_j \Phi_j^t) \Phi_j^{\rho k}$.
%\[
%\mu(\rho k) = \sum_{j=1}^n a_j \Phi_j^{\rho k}, \quad \text{and} \quad \mu(\rho k + t) = \sum_{j=1}^n (a_j \Phi_j^t) \Phi_j^{\rho k}.
%\]
To recover $\{\Phi_j^t\}_{j=1}^n$, we first recover the amplitudes $\{a_j\}_{j=1}^n$ and $\{a_j\Phi_j^t\}_{j=1}^n$ from the sampling sets $\mathcal{M}_{d}$ and $\mathcal{M}_{ds}$, respectively. Then compute $\{\Phi_j^t\}_{j=1}^n = \left\{\frac{a_j\Phi_j^t}{a_j} \right\}_{j=1}^n$.
%\[
%\{\Phi_j^t\}_{j=1}^n = \left\{\frac{a_j\Phi_j^t}{a_j} \right\}_{j=1}^n.
%\]  
Finally, since $\rho$ and $t$ are co-prime, the intersection of the candidate sets for $e^{i\rho x_j}$ and $e^{it x_j}$ contains exactly one element. For noisy samples, we first match the aliased nodes \( \{\tilde{\Phi}_j^{\rho}\} \) and \( \{\hat{\Phi}_j^{\rho}\} \) before division. A more stable solution can be achieved using additional shifted sample sets; see \cite{cuyt2020get, Briani2020} for the full details.
\par Summarizing the above, Algorithm \ref{alg:decimation} can be applied to any SR method to generate its decimated version.

\begin{comment}
    By Theorem \ref{pro:teop-eig}, for every \(\rho \in \mathbb{N}\), we have \(\sigma_{k+1}(T_{\rho}) \asymp \Delta_{\rho}^2\), where \(k\) is the number of clusters with multiplicity at least 1. Thus, for a set \(\Lambda := \{\rho_1, \ldots, \rho_m\} \subset \mathbb{N}\), we choose \(\rho_* := \arg\max_{j}\{\sigma_{k+1}(T_{\rho_j})\}_{j=1}^m\) as the best sampling rate of the set \(\Lambda\), i.e., the decimation parameter with the best separation properties. 
%The probability that this \(\rho_*\) is suitable is higher than \(1 - (1 - p)^m\) (Monte Carlo). 
Additionally, we select \(t \in \mathbb{N}\) such that \(t\) and \(\rho_*\) are co-prime numbers. 
%This step is crucial for de-aliasing (i.e recovering the decimated nodes). For more details see \cite{cuyt2020get}.
Since decimation only is not enough. Using any recovery method with decimated samples i.e. sampling at rate $\rho \in \mathbb{N} \neq 1$, gives us the set of nodes $\{e^{i\rho x_j}\}_{j=1}^{n}$, which is not enough to recover $\{e^{ix_j}\}_{j=1}^n$ because each of $e^{i\rho x_j}$ have $\rho$ candidates. To recover the real nodes we can use the technique introduced in \cite{cuyt2020get}. The mentioned technique suggests another shifted set of samples $\mathcal{M}_{\rho,t} := \{\mu(\rho k+ t)\}_{k=1}^{N}$ in addition for $\mathcal{M}_{\rho,0}:=\{\mu(\rho k)\}_{k=1}^{N}$, where $\rho$ and $t$ are co-prime numbers and $\Omega \geq N \in \mathbb{N}$.
Assume we are in the noiseless case, the procedure goes as follows: after obtaining $\{a_j, e^{i\rho x_j}\}_{j=1}^n$ from $\mathcal{M}_{\rho, 0}$ using any SR method and $\{a_je^{it x_j}, e^{i\rho x_j}\}_{j=1}^n$ from $\mathcal{M}_{\rho,t}$, we have $e^{it x_j} = {a_je^{it x_j}\over a_j}$ and thus the intersection of the candidates of $\{e^{i\rho x_j}\}$ and $\{e^{it x_j}\}$ is $e^{ix_j}$. For more details see \cite{cuyt2020get}.
\end{comment}


\begin{algorithm}
\caption{Decimated SR method}
\label{alg:decimation}
\begin{algorithmic}[1]
\Require \(M\), \(\{\hat{\mu}(\omega)\}_{\omega \in [-\Omega,\Omega]}\), \(\Omega\), $n$, SRmethod($\cdot$).
%\State For all natural numbers $\rho \in \mathcal{I}$ calculate \(\sigma_{M+1}(T_{\rho})\).
\State Select Admissible Decimation parameter:
\State \hspace{1em} (a) Select $\rho := \arg\max_{\rho' \in \mathcal{I}\cap\mathbb{N}}\{\sigma_{M+1}(T_{\rho'})\}$.
\State \hspace{1em} (b) Find \(t\) such that \(\rho\) and \(t\) are co-prime.
\State Solve for Decimated and Shifted samples:
\State \hspace{1em} (a) $\{\tilde{\Phi}_j^{\rho}, \tilde{a}_j\}_{j=1}^n =$ SRmethod($\mathcal{M}_d$).
\State \hspace{1em} (b) $\{\hat{\Phi}_j^{\rho}, \hat{a}_j\hat{\Phi}_j^{t}\}_{j=1}^n =$ SRmethod($\mathcal{M}_{ds}$).
\State Perform De-aliasing for $\{\tilde{\Phi}_j^{\rho},\hat{\Phi}_j^{t}\}$ (see the text above).
\State Return the estimates $\{\tilde{x}_j, \tilde{a}_j\}$.

\end{algorithmic}
\end{algorithm}

\begin{remark}
    Using Proposition 5.8 from \cite{Batenkov2021}, we can derive a lower bound of \( \frac{1}{n^2} \) for selecting an admissible decimation parameter from \( \mathcal{I} \), enabling randomness in algorithm \ref{alg:decimation}.
\end{remark}



%Next, we introduce the Enhanced Decimated Prony Method (EDP) as a modification of the DP, replacing the step of solving many decimated sub-prony problems and constructing a histogram for all the possible solutions with our suggested method for the identification of admissible decimation parameter (ref alg). The algorithm of EDP goes as follows, we select an admissible decimation parameter $\rho$ and a co-prime $t$ via our technique (ref alg) then we solve a decimated prony problem of the selected $\rho_*$ and perform de-aliasing as in VEXPA [ref]. DP method has time complexity of $O(\Omega) + O(\Delta^{-1})$. There are $O(\Omega)$ natural decimation numbers in the interval $\left[\frac{\Omega}{2(2n-1)}, \frac{\Omega}{2n-1}\right]$, for each one of them we construct Teoplitz matrix of size $n \times n$ of samples and calculate it's $k+1$ singular value. This procedure has time complexity of $O(n^3\Omega)$ since the SVD of the Teoplitz matrix is $O(n^3)$. The worst time complexity of finding a co-prime $t \neq 1$ for the selected decimation parameter $\rho_*$ of order $\Omega$ is $O(\Omega \log(\Omega))$, since the gcd computation is $O(\log(\Omega))$ and the worst case we need to compute it is up to $\Omega - 1$. We can choose $t = \rho_* + 1$ {\color{red} not sure why in VEXPA that wasn't the standard choice of co prime}. Then, since prony's method and the de-aliasing step depend only on $n$ (fixed), the time complexity of EDP is $O(\Omega\log(\Omega))$.
We introduce the Enhanced Decimated Prony Method (EDP) and the Decimated Matrix Pencil method (DMP) as improvements over DP and MP, respectively. EDP applies Algorithm \ref{alg:decimation} with Prony's method as the SR method, eliminating the need to solve multiple sub-Prony problems and constructing a histogram, as done in DP. Similarly, DMP applies Algorithm \ref{alg:decimation} with MP as the SR method.

\par DP and MP have a time complexity of $O(\Omega^2) + O(\Delta^{-1})$ and $O(\Omega^3)$, respectively. For EDP and DMP, there are $O(\Omega)$ natural decimation parameters in the interval $\mathcal{I}$. For each, we construct an $n \times n$ Toeplitz matrix of samples and compute its $M+1$-th singular value, which requires $O(n^3\Omega)$. Finding a co-prime $t \neq 1$ for $\rho$ has a worst-case complexity of $O(\Omega \log(\Omega))$, as gcd computations take $O(\log(\Omega))$, and the search might iterate up to $\Omega - 1$. Since matching the aliased nodes, applying Prony's method, MP (for $2n$ samples) and de-aliasing depend only on $n$ (fixed), the total time complexity of EDP and DMP is $O(\Omega \log(\Omega))$. 
%{\color{red} not sure why in VEXPA choosing $t = \rho_* + 1$ isn't the standard choice of co prime}

\par We implemented the EDP, DP, DMP and MP methods, assigning the number of nodes for both MP and DMP as an input and using $3n$ decimated samples in DMP. The implementation is in MATLAB. We recall that for DP, $N_{\rho}$ is the number of decimation parameters being tested in the interval $\mathcal{I}$ and $N_b$ is the number of bins in the constructed histogram of node candidates. The noise in the samples is generated from a Cauchy distribution. Each method was tested $10$ times across four different SRF values. In Figure \ref{fig:time-com}, we present the mean reconstruction error of the cluster node $x_1$ as a function of SRF. The results show that all methods achieve high accuracy, with EDP and DMP being the fastest, offering a speed improvement of up to two orders of magnitude.
   

\begin{figure}[htb]
    \centering
  \includegraphics[width=0.49\linewidth]{figures/scnl900x3f.jpg} \hfill
  \includegraphics[width=0.49\linewidth]{figures/mcnl900x8f.jpg}
  \caption{\small (right) single cluster configuration. (left) multi-cluster configuration. For both experiments the noise level is $10^{-6}$, $N_{\rho} = 900$ and $N_b = 3\Delta^{-1}$.}
  \label{fig:time-com}
\end{figure}
%computing singular values of Toeplitz matrix is known to be $O(n^3)$. For small $n$ this is enough. There is an algorithm that computes the inverse of teoplitz with complexity $O(nlogn)$ \cite{gohberg1994,kailath1999}.
\begin{remark}
    To obtain higher accuracy for DMP, we can use $N \gg 2n$ decimated samples if possible.
\end{remark}
\subsection{Optimality of EDP}
%Let $F$ be the space of signals of the form \eqref{spike}. Recall the definition of the min-max error bounds:
%\begin{theorem}[\cite{Batenkov2021}]\label{thm:min-max}
%    Let $X$ be a node configuration with one cluster $\mathcal{C}$ of size $1 < \ell \leq n$ and bounded $\{a_j\}$. For SRF $\geq 1$ and $\epsilon \lessapprox (\Omega \Delta)^{2\ell -1}$
%    \begin{align*}
%    \Lambda^{x_j}(\epsilon, F, \Omega) &\asymp 
%    \begin{cases}
%        SRF^{2\ell - 1} \Delta \epsilon &  x_j \in \mathcal{C}, \\
%        \frac{\epsilon}{\Omega} &  x_j \notin \mathcal{C},
%    \end{cases} \\
%    \Lambda^{a_j}(\epsilon, F, \Omega) &\asymp 
%    \begin{cases}
%        SRF^{2\ell - 1} \epsilon &  x_j \in \mathcal{C}, \\
%        \epsilon &  x_j \notin \mathcal{C}.
%    \end{cases}
%\end{align*}
%
%\end{theorem}
%
In Figure \ref{fig:opt}, we numerically show that EDP is optimal, meaning it achieves the min-max error bounds (Theorem 2.8 in \cite{Batenkov2021}) in the multi-cluster geometry. We plot the node/amplitude error amplification factors \eqref{naf} as a function of SRF.
\begin{equation}\label{naf}
    \mathcal{K}_{x_j} := \epsilon^{-1}\Omega |x_j - \tilde{x}_j|, \quad \mathcal{K}_{a_j} := \epsilon^{-1} |a_j - \tilde{a}_j|.
\end{equation}


%\begin{figure}[htb]
%    \centering
%%    \begin{minipage}{0.49\linewidth}
%%        \centering
%%        \includegraphics[width=\linewidth]{figures/sc_k.jpg}
%%        \subcaption{Caption for top-left}
%%        \label{fig:sub1}
%%    \end{minipage}
%%    \hfill
%%    \begin{minipage}{0.49\linewidth}
%%        \centering
%%        \includegraphics[width=\linewidth]{figures/sc_k_a.jpg}
%%        \subcaption{Caption for top-right}
%%        \label{fig:sub2}
%%    \end{minipage}
%%    \vspace{0.2cm}
%    \begin{minipage}{0.49\linewidth}
%        \centering
%        \includegraphics[width=\linewidth]{figures/mc_k_x.jpg}
%        \subcaption{ }
%        \label{fig:sub3}
%    \end{minipage}
%    \hfill
%    \begin{minipage}{0.49\linewidth}
%        \centering
%        \includegraphics[width=\linewidth]{figures/mc_k_a.jpg}
%        \subcaption{ }
%        \label{fig:sub4}
%    \end{minipage}
%    \caption{\small EDP - asymptotic optimality. For cluster node $x_1$, $\mathcal{K}_{x_1}$ (a) scale
%                like $SRF^{2\ell_1 - 2}$, while the $\mathcal{K}_{a_1}$ (b) scale like $SRF^{2\ell_1 - 1}$.
%                For the non-cluster node $x_4$, both $\mathcal{K}_{x_4}$ and $\mathcal{K}_{a_4}$ are lower bounded
%                by a constant.}
%    \label{fig:opt}
%\end{figure}

\begin{figure}[htb]
  \includegraphics[width=0.49\linewidth]{figures/mc_k_x.jpg} \hfill
  \includegraphics[width=0.49\linewidth]{figures/mc_k_a.jpg}
  \caption{\small EDP - asymptotic optimality. For cluster node $x_1$, $\mathcal{K}_{x_1}$ (left) scales
                like $SRF^{2\ell_1 - 2}$, while the $\mathcal{K}_{a_1}$ (right) scales like $SRF^{2\ell_1 - 1}$.
                For the non-cluster node $x_4$, both $\mathcal{K}_{x_4}$ and $\mathcal{K}_{a_4}$ are lower bounded
                by a constant. These scaling rates are optimal.}
  \label{fig:opt}
\end{figure}

%\begin{remark}
 %   Unlike DP, EDP has a clear separation between the search for an admissible decimation parameter and the de-aliasing step. 
    %This distinction simplifies the analysis required to establish the optimality of EDP.
%\end{remark}
\begin{comment}
    \begin{figure}[htbp]
    \centering
    \subfigure[ ]{%
        \includegraphics[width=0.45\textwidth]{figures/nodes3.8177.jpg} % Replace with your image file
        \label{fig:figure1}
    }
    \hfill
    \subfigure[ ]{%
        \includegraphics[width=0.45\textwidth]{figures/amp4.7688.jpg} % Replace with your image file
        \label{fig:figure2}
    }
    \caption{Error amplification factors}
    \label{fig:main}
\end{figure}
\end{comment}




%Let $p:=\frac{1}{2n^2}$ be the lower bound of the probability of choosing a good lambda, thus if we randomly choose $m$ lambdas then we get a good lambda with probability at least $1-{(1-p)}^m$ (Monte carlo).\\
%step 1: Find the number of clusters $c$ (like using DBSCAN) \\
%step 2: randomly choose $m$ decimation numbers $\{q_i\}_{i=1}^m$ between $[{\Omega\over 2(2d-1)},{\Omega\over 2d-1}]$ and return the parameter $t$ with the largest $|\lambda_{c+1}(V_t^*DV_t)|$. \\
%step 3: choose $\rho$ s.t. $\rho$ and $t$ are co-prime. (for the shifts - to beat aliasing). \\
%return: $t,\rho$



%\begin{figure}
%\begin{center}
%\includegraphics[width=0.98\linewidth]{sgn10l3o300.jpg}
%\end{center}
%\caption{Single Cluster}
%\end{figure}

%\begin{figure}
%\begin{center}
%\includegraphics[width=0.98\linewidth]{mcn7l32o200.jpg}
%\end{center}
%\caption{Multi-Cluster}
%\end{figure}

%\begin{figure}
%\begin{center}
%\includegraphics[width=0.98\linewidth]{n12l43o1000.jpg}
%\end{center}
%\caption{Multi-Cluster}
%\end{figure}

%\begin{figure}
%\begin{center}
%\includegraphics[width=0.98\linewidth]{n13l43o100.jpg}
%\end{center}
%\caption{Multi-Cluster, we have a problem with $1\over{n^2}$ - check this!}
%\end{figure}

%\appendix
%\section{Auxiliary Lemmas for Section \ref{PA}}
%    \begin{corollary}\label{cor:eig-inq}
%    Let $A,B \in \mathbb{C}^{n\times n}$ be Hermitian matrices. Then 
%    \begin{equation}
%        \big|\lambda_i(A+B)-\lambda_i(A)\big| \leq \|B\|_2
%    \end{equation}
%\end{corollary}
%\begin{proof}
%    Recall Corollary 4.3.15 in [cite Horn], for $A,B \in \mathbb{C}^{n\times n}$ Hermitian we have 
%    \begin{equation}
%        \lambda_i(A) + \lambda_n(B) \leq \lambda_i(A+B) \leq \lambda_i(A)+\lambda_1(B)
%    \end{equation}
%    we get that  
%    \begin{align*}
%        \big|\lambda_i(A+B)-\lambda_i(A)\big| \leq \max{(|\lambda_n(B)|,|\lambda_1(B)|)} = \|B\|_2
%    \end{align*}
%\end{proof}
%\begin{corollary}\label{cor:eig-sin}
%    For any matrix $A \in \mathbb{C}^{n\times n}$, we have that the eigenvalues of the matrix $\mathcal{A}:=\begin{bmatrix}
%        &0 &A \\
%        &A^* &0 
%    \end{bmatrix}$ are plus and minus the singular values of $A$.
%\end{corollary}
%\begin{proof}
%    Let $\lambda$ be an eigenvalue of the matrix $\mathcal{A}$:
%    \[ \begin{bmatrix}
%        &0 &A \\
%        &A^* &0 
%    \end{bmatrix}\begin{bmatrix}
%        &x \\ &y
%    \end{bmatrix} = \begin{bmatrix}
%        &A^*y \\ &Ax
%    \end{bmatrix} = \lambda\begin{bmatrix}
%        &x \\ &y
%    \end{bmatrix}\]
%    thus we have 
%    \[ Ax = \lambda y, \quad A^*y=\lambda x \longrightarrow A^*Ax=A^*(\lambda y) = \lambda^2x\]
%    We get that $\lambda$ is an eigenvalue of $\mathcal{A}$ iff $\lambda^2$ is an eigenvalue of ${A^*A}$. In addition, we have $\lambda(A^*A) = \sigma(A^*A) = \sigma^2(A)$ and thus $\lambda = \pm \sigma(A)$.
%\end{proof}
%\begin{lemma}\label{lm:sum}
%    For $A, B \in \mathbb{C}^{n\times n}$ we have
%    \[ \big| \sigma_i(A) - \sigma_i(B) \big| \leq \|A-B\|_2\]
%    where $\sigma_i$ is the ith singular value.
%\end{lemma}
%\begin{proof}
%    We apply Corollary \ref{cor:eig-inq} on the matrices $\mathcal{A}:=\begin{bmatrix}
%        &0 &A \\
%        &A^* &0 
%    \end{bmatrix}$ and $\mathcal{B}:=\begin{bmatrix}
%        &0 &B \\
%        &B^* &0 
%    \end{bmatrix}$. we have
%    \[ \big| \lambda_i(\mathcal{A}) - \lambda_i(\mathcal{B}) \big| \leq \|\mathcal{A}-\mathcal{B}\|_2\]
%    Using Corollary \ref{cor:eig-sin} we have
%    \[\big| \lambda_i(\mathcal{A}) - \lambda_i(\mathcal{B}) \big| = \big| \pm \sigma_i(A) - \pm \sigma_i(B) \big|\]
%    Thus we have two options for the last term:
%    \begin{enumerate}
%        \item $\big| \pm \sigma_i(A) - \pm \sigma_i(B) \big| = \big| \sigma_i(A) - \sigma_i(B) \big|$
%        \item $\big| \pm \sigma_i(A) - \pm \sigma_i(B) \big| = \big| \sigma_i(A) + \sigma_i(B) \big| \geq \big| \sigma_i(A) - \sigma_i(B) \big|$
%    \end{enumerate}
%    Again, applying Corollary \ref{cor:eig-sin} for the matrix $\mathcal{A}-\mathcal{B}$ we get
%    \[ \|\mathcal{A}-\mathcal{B}\|_2^2 = \lambda^2_1(\mathcal{A}-\mathcal{B}) = \sigma^2_1(A-B) = \|A-B\|_2^2\]
%    To conclude, we got that 
%    \[
%         \big| \sigma_i(A) - \sigma_i(B) \big| \leq \|A-B\|_2
%    \]
%    
%    
%\end{proof}
%\section*{Acknowledgment}
\textbf{Acknowledgment}. We thank Dr. Dmitry Batenkov and Prof. Ronen Talmon for their insightful comments.
\clearpage
\bibliographystyle{plain}
\bibliography{ref}
\end{document}
