%\documentclass[gray]{jmlr} % test grayscale version
%\documentclass[tablecaption=bottom]{jmlr}% journal article
\documentclass[pmlr,twocolumn,10pt]{jmlr} % W&CP article

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

%\usepackage{rotating}% for sideways figures and tables
%\usepackage{longtable}% for long tables

% The booktabs package is used by this sample document
% (it provides \toprule, \midrule and \bottomrule).
% Remove the next line if you don't require it.

\usepackage{booktabs}
% The siunitx package is used by this sample document
% to align numbers in a column by their decimal point.
% Remove the next line if you don't require it.
%\usepackage[load-configurations=version-1]{siunitx} % newer version 
\usepackage{siunitx}
\usepackage{soul}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\definecolor{customgray}{gray}{0.9}
\usepackage[many]{tcolorbox}    	% for COLORED BOXES (tikz and xcolor included)
\usepackage{setspace}               % for LINE SPACING
\usepackage{multicol}               % for MULTICOLUMNS
\usepackage{algorithm2e}
\usepackage{xcolor, soul}
\newcommand{\ds}[1]{\sethlcolor{yellow}\hl{[Dimitris: #1]}}

\usepackage[hang,flushmargin]{footmisc}
\usepackage{lipsum}
\makeatletter
\newcommand{\algorithmfootnote}[2][\footnotesize]{%
  \let\old@algocf@finish\@algocf@finish% Store algorithm finish macro
  \def\@algocf@finish{\old@algocf@finish% Update finish macro to insert "footnote"
    \leavevmode\rlap{\begin{minipage}{\linewidth}
    #1#2
    \end{minipage}}%
  }%
}

\definecolor{main}{HTML}{000000}    % setting main color to be used

\newtcolorbox{boxB}{
    fontupper = \color{main}, % font color
    boxrule = 1.5pt,
    colframe = main,
    rounded corners,
    arc = 5pt   % corners roundness
}
% The lineno package is required for denoting line
% numbers for paper review.
\usepackage[switch]{lineno}
% \newcommand{\model}{\texttt{Time2Lang}}

% The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}% remove this in your real article

% The following is to recognise equal contribution for authorship
\newcommand{\equal}[1]{{\hypersetup{linkcolor=black}\thanks{#1}}}

% Define an unnumbered theorem just for this sample document for
% illustrative purposes:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

% change the arguments, as appropriate, in the following:
\jmlrvolume{LEAVE UNSET}
\jmlryear{2025}
\jmlrsubmitted{LEAVE UNSET}
\jmlrpublished{LEAVE UNSET}
% \jmlrworkshop{Conference on Health, Inference, and Learning (CHIL) 2025} % W&CP title

% The optional argument of \title is used in the header
% \title[Time2Lang]{Time2Lang: Integrating Behavioral Sensing Data With LLMs for Mental Health Detection}
% \title[Time2Lang]{Time2Lang: Efficiently Integrating Time-Series Behavioral Data from Phones and Wearables with LLMs}
\title[Time2Lang]{Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models and Large Language Models for Health Sensing}
% Anything in the title that should appear in the main title but 
% not in the article's header or the volume's table of
% contents should be placed inside \titletag{}

%\title{Title of the Article\titletag{\thanks{Some footnote}}}


% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% \thanks must come after \Name{...} not inside the argument for
% example \Name{John Smith}\nametag{\thanks{A note}} NOT \Name{John
% Smith\thanks{A note}}

% Anything in the name that should appear in the title but not in the 
% article's header or footer or in the volume's
% table of contents should be placed inside \nametag{}

% Two authors with the same address
% \author{%
%  \Name{Author Name1\nametag{\thanks{A note}}} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address
% }

% Three or more authors with the same address:
% \author{%
%  \Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \Name{Author Name4} \Email{an4@sample.com}\\
%  \Name{Author Name5} \Email{an5@sample.com}\\
%  \Name{Author Name6} \Email{an6@sample.com}\\
%  \Name{Author Name7} \Email{an7@sample.com}\\
%  \Name{Author Name8} \Email{an8@sample.com}\\
%  \Name{Author Name9} \Email{an9@sample.com}\\
%  \Name{Author Name10} \Email{an10@sample.com}\\
%  \Name{Author Name11} \Email{an11@sample.com}\\
%  \Name{Author Name12} \Email{an12@sample.com}\\
%  \Name{Author Name13} \Email{an13@sample.com}\\
%  \Name{Author Name14} \Email{an14@sample.com}\\
%  \addr Address
% }

\author{
\Name{Arvind Pillai} \Email{arvind.pillai.gr@dartmouth.edu} \\
\addr Dartmouth College, USA \AND
\Name{Dimitris Spathis} \Email{dispathis@gmail.com} \\
\addr University of Cambridge, UK \AND
\Name{Subigya Nepal} \Email{sknepal@stanford.edu} \\
\addr Stanford University, USA \AND
\Name{Amanda C Collins} \Email{Amanda.C.Collins@dartmouth.edu} \\
\addr Dartmouth College, USA \AND
\Name{Daniel M Mackin} \Email{Daniel.M.Mackin@dartmouth.edu} \\
\addr Dartmouth College, USA \AND
\Name{Michael V Heinz} \Email{Michael.Vincent.Heinz@dartmouth.edu} \\
\addr Dartmouth College, USA \AND
\Name{Tess Z Griffin} \Email{Tess.Z.Griffin@dartmouth.edu} \\
\addr Dartmouth College, USA \AND
\Name{Nicholas C Jacobson} \Email{Nicholas.C.Jacobson@dartmouth.edu} \\
\addr Dartmouth College, USA \AND
\Name{Andrew Campbell} \Email{Andrew.T.Campbell@dartmouth.edu}\\
\addr Dartmouth College, USA
}


% % Authors with different addresses and equal first authors:
% \author{%
% \Name{Anonymous First Author 1}\equal{These authors contributed equally} \Email{abc@sample.com}\\
% \addr University X, Country 1
% \AND
% % footnotemark[1] is to refer to the \equal footnote
% \Name{Anonymous First Author 2}\footnotemark[1] \Email{def@sample.com}\\
% \addr University Y, Country 2
% \AND
% \Name{Anonymous Last Author} \Email{ghi@sample.com}\\
% \addr University Z, Country 3
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Remove the \linenumbers in the final version %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \linenumbers % Activate line numbering

\begin{document}

\maketitle

\begin{abstract}
% Incorporating behavioral sensing data with large language models (LLMs) has useful implications in mental health classification. Current methods rely on transforming sensor data into text-based prompts for LLMs. However, in long duration sensing data, this leads to increased errors, slower inference times, and effective prompting in mental health requires domain knowledge. To address these challenges, we propose Time2Lang, a framework that learns a mapping between a time series foundation model (FM) and an LLM, removing the need for text conversion. Time2Lang is trained exclusively on synthetic data to predict periodicity as a pretext task. We evaluate the effectiveness of the learned embeddings on two longitudinal mental health datasets: predicting daily depression (N=17,251) using step count data and classifying flourishing through conversation duration (N=46; 10-week duration). Our results indicate that Time2Lang's learned mapping preserves the performance of the time series FM. Furthermore, it is more efficient, maintaining consistent inference time regardless of input length, and after mapping, the LLM outputs align with fundamental time series properties such as auto-correlation. Ultimately, Time2Lang prioritizes enhanced performance, efficiency, and time-series understanding over the open-ended generative capabilities of LLMs.

% Integrating behavioral sensing data with large language models (LLMs) has great potential for mental health classification. Current methods typically transform sensor data into text-based prompts. However, because sensing data consists of long-duration sequences, this approach can introduce errors, increase inference time, and require domain expertise for effective prompting. To address these challenges, we introduce Time2Lang, a framework that maps a time series foundation model (FM) directly to an LLM, eliminating text conversion. We first train Time2Lang exclusively on synthetic data to predict periodicity as a pretext task, and evaluate the embeddings with mental health classification as an application. Specifically, we use two longitudinal mental health datasets for: (1) predicting daily depression (N=17,251) from step count and (2) classifying flourishing via conversation duration (N=46; 10 weeks). Our findings indicate that Time2Lang preserves the performance of the time series FM while improving efficiency. Unlike prompting, its inference time remains consistent regardless of input length. Additionally, the mapped LLM outputs retain key time-series properties, such as auto-correlation. By prioritizing performance, efficiency, and time-series comprehension over open-ended generative capabilities, Time2Lang offers an alternative approach to leverage LLMs for behavioral sensing applications in mental health. 


% \ds{Nice framing! What if we make it a bit more ambitious? RE: title, I'd go with something like: Efficiently Bridging Time-Series Foundation Models and Large Language Models for Health Tasks Beyond Prompting. Essentially this approach eliminates prompting from this workflow and uses TFMs in an elegant way. The name is interesting but maybe we shouldn't focus so much on the sensor part here? What about Time2Lang? }
% Integrating behavioral sensing data with large language models (LLMs) holds promise for mental health classification. Current methods typically transform sensor data into text-based prompts, which can introduce errors, increase inference time, and require domain expertise for effective prompting. These challenges are more pronounced when using LLMs to process long-duration numerical time series. To address these challenges, we introduce Time2Lang, a framework that maps a time series foundation model (TFM) to an LLM, eliminating text conversion. We first train Time2Lang on synthetic data to predict periodicity as a pretext task, and then evaluate the embeddings for mental health classification. Specifically, we use two longitudinal datasets for: (1) predicting daily depression (17,251 days from 256 people) from step count and (2) classifying flourishing via conversation duration (46 people over 10 weeks). Our findings indicate that Time2Lang maintains \ds{I'd rephrase this sentence, start with the positive result first} the TFM’s performance while improving efficiency. Unlike prompting, its inference time remains consistent regardless of input length. Additionally, the mapped LLM outputs retain key time-series properties such as auto-correlation. By emphasizing efficiency and structured time-series comprehension over open-ended generative capabilities, Time2Lang provides a scalable alternative for leveraging LLMs in behavioral sensing. Crucially, Time2Lang demonstrates that time-series and language models can be synergistically integrated for mental health classification, achieving performance comparable to dedicated time-series models, thus paving the way for future translational models leveraging the combined strengths of both.

% \ds{let's bring up the argument that LLMs are sort of established in processing unstructured data but at the same time TFMs have progressed and learn very good representations of arbitrary timeseries. Our goal is to bridge these two seemingly unconnected approaches in the most efficient manner.}

 %Integrating behavioral sensing data with large language models (LLM) holds great potential for health applications. However, current methods typically transform sensor data into text-based prompts, which can introduce errors, increase inference time, and require domain expertise for effective prompting. These challenges become more pronounced when applying LLMs to long-duration numerical time series. To overcome these limitations, recent advancements in time series foundation models (TFMs) have demonstrated their ability to learn meaningful representations from arbitrary time series data. In this paper, we efficiently bridge the gap between two seemingly unconnected approaches—TFMs and LLMs—by introducing Time2Lang, a framework that maps a TFM to an LLM, eliminating the need for text conversion. We first train Time2Lang on synthetic data to predict periodicity as a pretext task, and then evaluate the embeddings for mental health classification. Specifically, we use two longitudinal datasets for: (1) predicting daily depression (17,251 days from 256 people) from step count and (2) classifying flourishing via conversation duration (46 people over 10 weeks). Our results show that Time2Lang is efficient, with inference time remaining consistent regardless of input size, unlike prompting. Moreover, the resulting embeddings exhibit key time-series properties, such as auto-correlation. Crucially, Time2Lang demonstrates that TFMs and LLMs can be synergistically integrated for health sensing, minimizing information loss while enabling positive performance transfer across models trained for two distinct domains. Ultimately, we envision Time2Lang aiding future research in leveraging the strengths of distinct large models for complex generation and reasoning tasks.

Large language models (LLMs) show promise for health applications when combined with behavioral sensing data. Traditional approaches convert sensor data into text prompts, but this process is prone to errors, computationally expensive, and requires domain expertise. These challenges are particularly acute when processing extended time series data. While time series foundation models (TFMs) have recently emerged as powerful tools for learning representations from temporal data, bridging TFMs and LLMs remains challenging. Here, we present Time2Lang, a framework that directly maps TFM outputs to LLM representations without intermediate text conversion. Our approach first trains on synthetic data using periodicity prediction as a pretext task, followed by evaluation on mental health classification tasks. We validate Time2Lang on two longitudinal wearable and mobile sensing datasets: daily depression prediction using step count data (17,251 days from 256 participants) and flourishing classification based on conversation duration (46 participants over 10 weeks). Time2Lang maintains near constant inference times regardless of input length, unlike traditional prompting methods. The generated embeddings preserve essential time-series characteristics such as auto-correlation. Our results demonstrate that TFMs and LLMs can be effectively integrated while minimizing information loss and enabling performance transfer across these distinct modeling paradigms. To our knowledge, we are the first to integrate a TFM and an LLM for health, thus establishing a foundation for future research combining general-purpose large models for complex healthcare tasks.
 
\end{abstract}

\paragraph*{Data and Code Availability} In this work, we use the StudentLife dataset \citep{wang2014studentlife} which is publicly available. Another dataset, the Major Depressive Disorder (MDD) dataset, is currently private, with plans for public release in the future. Code pertaining to public data can be made available upon request. 

\paragraph*{Institutional Review Board (IRB)} The study protocol for the major depressive disorder dataset has received full approval from the respective institutional review boards (IRB).

\section{Introduction}
Personal sensing devices such as mobile phones and wearables collect diverse behavioral data that can be used to assess mental health. Longitudinal data from these devices, including step counts and conversation patterns, effectively detect mental health conditions like depression \citep{nepal2024capturing, price2023predicting}. Meanwhile, large language models (LLMs) have recently demonstrated their ability to process a variety of data modalities beyond text, including audio \citep{ghosh2024gama} and images \citep{radford2021learning}. A notable strength of LLMs is their in-context learning capability, enabling them to generalize from prompt-based examples without additional training \citep{dong2022survey}. Building on these advancements, integrating behavioral sensing data with LLMs offers significant promise for developing personalized health models, with considerable impact for real-world applications \citep{cosentino2024towards}.

% The growing prevalence of personal sensing devices such as mobile phones and wearables has led to 
% Large language models (LLMs) have demonstrated strong predictive capabilities across diverse tasks and modalities. Primarily trained to understand and reason about natural language, these models have exhibited promising potential to extend beyond text to other data modalities such as audio \citep{ghosh2024gama} and images \citep{radford2021learning}. Notably, in-context learning, the ability to learn through prompt-based examples without additional training makes them suitable for various applications \cite{dong2022survey}. With the growing prevalence of personal health sensing devices, there is an increasing availability of multi-modal data. Combining this sensing data with LLMs to develop personalized health models holds significant promise for real-world applications \citep{cosentino2024towards}.

Recent studies have incorporated health sensing data with LLMs to detect depression \citep{kim2024health}, atrial fibrillation \citep{liu2023large}, and sleep quality \citep{cosentino2024towards}. A common approach involves converting raw sensor data into text prompts to enable LLMs to process numeric time series data for classifying these conditions \citep{kim2024health, liu2023large}. However, this method ignores the modality gap between sensing and text data \citep{spathis2024first}. Applying this technique to long-duration time series leads to higher error rates during prompting and requires a large amount of tokens (Section \ref{sec:limitations}). Constructing effective prompts also requires substantial domain expertise, making it a challenging task even for domain experts (Section \ref{sec:baselines}). 
Recent advancements in time series foundation models (TFM), such as Chronos \citep{ansari2024chronos} and Moment \citep{goswami2024moment}, have demonstrated their effectiveness in generating meaningful representations by capturing fundamental time series characteristics. 

Therefore, we explore if incorporating a TFM in an LLM pipeline can address the aforementioned limitations. One way to efficiently incorporate two large models from different domains is through the concept of \textit{model re-programming} \citep{chen2024model}. Previous works have successfully reprogrammed models from distinct domains such as text to biochemical sequence \citep{vinod2023reprogramming} and speech to time series \citep{yang2021voice2series}. Hence, we argue that incorporating a TFM into an LLM model through reprogramming can produce meaningful representations. Specifically, rather than using raw signals in prompts, we leverage TFMs as feature extractors (Figure \ref{fig:prompting_Time2Lang}). A key question is whether an LLM can understand TFM-derived features. To this end, we introduce an adapter that maps TFM features to LLM representations, effectively serving as a translator between the two models (Figure \ref{fig:prompting_Time2Lang}). We demonstrate our approach through behavioral sensing applications in mental health. Since behavioral sensing data is collected longitudinally in real-world settings, its unobtrusive nature results in long-duration sequences with realistic noise. Moreover, mental health is a high-stakes domain where minimizing errors, enhancing time-series comprehension, and improving efficiency are critical. Rather than proposing a new model, our goal is to maximize the potential of existing powerful models through an elegant adapter strategy. Thus, we explore whether two large models from distinct domains can be integrated with minimal information loss.

% Here, we explore the incorporation of a TFM in an LLM pipeline, that can address the aforementioned limitations. One way to efficiently incorporate two large models from different domains is through the concept of \textit{model re-programming} \citep{chen2024model}. Previous works have successfully reprogrammed models from distinct domains such as text to biochemical sequence \citep{vinod2023reprogramming} and speech to time series \citep{yang2021voice2series}. Hence, we argue that incorporating a TFM into an LLM model through reprogramming can produce meaningful representations for mental health classification from behavioral sensing \ds{this sounds a bit abrupt, better say that use behavioural sensing as a case study because of the interesting properties of the data and the fact that the application is high-stakes etc}(Figure \ref{fig:prompting_Time2Lang}). In this study, we evaluate whether LLM outputs can retain the performance of a TFM while providing advantages such as reduced inference time, fixed input sizes, and the elimination of manual prompt design. Rather than proposing another SOTA model, our goal is to maximize the potential of existing powerful models through an elegant adapter strategy, with applications in mental health. Specifically, we explore whether two large models from distinct domains can be integrated with minimal information loss. Our work establishes a crucial foundation for leveraging LLMs in behavioral sensing, potentially enabling more complex and dynamic applications in the future.

% After mapping, the LLM outputs could deliver results comparable to those of a TFM, with advantages such as faster inference, fixed input sizes, and no reliance on prompting. In particular, we seek to present findings from the alternative perspective of inputting time series to an LLM without text conversion, instead of state-of-the-art-results.

Toward the vision of seamless integration of time series data into LLMs, our contributions are as follows: (1) \textbf{Time2Lang Framework.} We propose Time2Lang, a framework that learns a mapping between two frozen foundation models (FMs): Chronos \citep{ansari2024chronos} and LLaMA \citep{dubey2024llama} (Figures \ref{fig:prompting_Time2Lang} and \ref{fig:Time2Lang}). To our knowledge, we are the first to integrate a TFM and an LLM for health tasks. Unlike most prior work, Time2Lang is trained within a self-supervised framework exclusively using intricate synthetic data generated from a Gaussian process (Section \ref{sec:synthetic}). (2) \textbf{Evaluation on Longitudinal Mental Health Data.} We assess Time2Lang’s performance on two real-world longitudinal mental health datasets with long-duration sequences. Specifically, we predict daily depression levels using step count data (17,251 days/labels) and analyze student flourishing from conversation duration (46 participants over 10 weeks) as described in Section \ref{sec:experiments}. (3) \textbf{Efficiency and Temporal Dynamics.} We examine Time2Lang’s inference time, the impact of pre-training data size, the role of periodicity, and its relationship to classical time-series characteristics (Section \ref{sec:results}). Our \textbf{key findings} (Section \ref{sec:findings}) are: (1) Time2Lang’s learned mapping minimizes information loss and enables positive transfer between two distinct domains—time series and text, (2) it maintains near constant inference time regardless of input length, and (3) its mapped LLM outputs align with fundamental time-series metrics, such as the autocorrelation factor. Overall, Time2Lang demonstrates that reprogramming can effectively integrate a TFM with an LLM, enhancing performance, efficiency, and time-series comprehension.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/Time2Lang_vs_prompting.pdf}
    \caption{\textbf{Time2Lang vs traditional prompting.} An example of using sensing data to predict depression, comparing text-based prompting (left) with Time2Lang (right). In text-based prompting, the sensor signals are converted into text for LLM prompting. As an alternative, we introduce Time2Lang—our method learns a mapping ($f$ and $g$) between a TFM and an LLM, enabling health sensing tasks without the need for text conversion, while making the most of powerful models.}
    \label{fig:prompting_Time2Lang}
    \vspace{-0.5cm}
\end{figure}

% To address these challenges, we employ model reprogramming, a resource-efficient strategy for cross-modal learning that re-purposes pre-trained foundation models (FM) across domains \citep{chen2024model}. Previous works have successfully reprogrammed models from distinct domains such as text to biochemical sequence \citep{vinod2023reprogramming} and speech to time series \citep{yang2021voice2series}. In parallel, TFMs, such as Chronos \citep{ansari2024chronos} and Moment \citep{goswami2024moment} have showcased their ability to produce useful representations by capturing fundamental time series characteristics. 

% Given that most tasks involving LLMs and sensor data revolve around classification, we argue that including a TFM into an LLM model through reprogramming can produce meaningful representations. After mapping, we hypothesize that LLM outputs can deliver results comparable to those of a TFM, with advantages such as faster inference, fixed input sizes, and no reliance on prompting. Our underlying assumption is that TFMs possess the capabilities necessary for sensing-related tasks, and by mapping them to LLMs, we can advance LLM and sensing research. Importantly, our work focuses on tackling practical challenges associated with integration of sensor data into LLM workflows, providing scalable solutions.

% Toward this goal, our key contributions are as follows: (1)
\section{Related Work}

\subsection{Model Reprogramming}

Model reprogramming is a training paradigm that efficiently repurposes a pre-trained, frozen large model across domains using an input transformation layer and an output mapping layer \citep{chen2024model}. \citet{yang2021voice2series} reprogrammed a speech model for univariate time series classification, while \citet{neekhara2022cross} proposed an adversarial approach to convert an image model for protein sequence prediction. Similarly, \citet{vinod2023reprogramming} focused on mapping a text model to biochemical sequences. From a reprogramming perspective, our work introduces several key advancements. It demonstrates that a TFM and an LLM can be effectively integrated for health-related tasks. Unlike most prior work, we adopt a self-supervised learning (SSL) framework instead of supervised learning. Additionally, our approach explicitly incorporates periodicity properties, further enhancing time-series understanding.

% \subsection{Mental Health and Behavioral Sensing}

% Recent studies have leveraged diverse sensing data from mobile phones and wearables to detect mental health conditions such as depression and anxiety \citep{xu2023globem, fukazawa2019predicting, nepal2024capturing}. For instance, \citet{xu2023globem} evaluated multiple machine learning algorithms for depression detection across various datasets, utilizing behavioral features such as sleep patterns, location, and physical activity. Beyond mental health conditions, passive sensing has also been applied to understand behavioral traits such as personality and detect rare life events \citep{wang2014studentlife, wang2018sensing, pillai2023rare}. For example, \citet{pillai2023rare} proposed an autoencoder-based approach to identify anomalous life events using multivariate sensing. These studies demonstrate substantial progress in passive sensing for mental health and behavioral analysis.

\subsection{LLMs and Healthcare}
Using LLMs to evaluate health outcomes has seen rapid growth \citep{he2023survey, han2023medalpaca, liu2025generalist, singhal2025toward}. For instance, MedPaLM 2 \citep{singhal2025toward}, an LLM fine-tuned for medical question answering has demonstrated strong performance by passing medical exam benchmarks. Similarly, models such as MedAlpaca \citep{han2023medalpaca} and Me-LLaMA \citep{xie2024me} also fine-tune general-purpose LLMs for comprehensive analysis of medical texts. For mental health tasks, \citet{xu2024mental} comprehensively evaluated different LLMs using online text data. Their work demonstrated that incorporating additional health context information in prompts enhances performance, but specialized domain knowledge is required.

\subsection{LLMs and Sensor Data}

Health sensing data consists of temporal numeric values that change over time. Approaches for time-series forecasting with LLMs involved converting the raw data into text prompts, as demonstrated by methods like LLMTime \citep{gruver2024large} and PromptCast \citep{xue2023promptcast}. For instance, \citet{gruver2024large} highlighted the importance of careful pre-processing, such as handling periods (“.”) and spaces (“ ”), to ensure effective tokenization of floating-point numbers.  Recently, alternative approaches have used multi-modal LLMs that encode sensing data as images \cite{yoon2024my} or use modality-specific encoders \cite{belyaeva2023multimodal}. For example, \citet{yoon2024my} proposed transforming time-series data into visual prompts for use with vision-language models. 

% Health sensing data represents temporal numeric data that indicates change in value over time. Initially, converting time-series to text prompts for forecasting has been proposed in methods such as LLMTime \citep{gruver2024large} and PromptCast \cite{xue2023promptcast}. For example, \citet{gruver2024large} showcase careful pre-processing of periods (``.") and spaces (`` ") is essential for effective tokenization of floating point numbers. Recently, other approaches have used multi-modal LLMs that encode sensing data as images \cite{yoon2024my} or using modality-specific encoders \cite{belyaeva2023multimodal}. For example, \citep{yoon2024my} transforms time series into visual prompts to probe a vision-language model. Nevertheless, the most common approach for health sensing incorporates raw sensor data as text strings into prompts without pre-processing \cite{kim2024health, liu2023large, xu2024penetrative}.

\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.24\textwidth]{images/mean_prediction.pdf}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{images/token_length.pdf}} 
    \caption{\textbf{Effect of Increasing Sequence Length on Prompting Performance.} We evaluate LLaMA 3.2 (1B) on: (a) a mean prediction task, where mean absolute error increases with sequence length, and (b) tokens, where the number of tokens is $\sim 10\times$ the time-series length.} 
    \label{fig:limitations}
\end{figure}

\subsection{Limitations of integrating mobile sensing with LLMs} \label{sec:limitations}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{images/SensorBridge.pdf}
    \caption{\textbf{Time2Lang Framework.} To meaningfully integrate Timeseries Foundation Models  (here: Chronos $C$) and Large Language Models (LLaMA $M$), we train two smaller networks $f$ and $g$ that optimally map TFM features ($\mathbf{z^c}$) to an LLM. The learned embeddings from $f$ and $g$ are $\mathbf{z^i}$ and $\mathbf{z^o}$, respectively. To improve positive knowledge transfer, we use a residual connection between the TFM and LLM features ($\mathbf{z^c} \rightarrow \mathbf{z^m}$) only during training.}
    \label{fig:Time2Lang}
\end{figure*}

Despite advances for integrating sensor data into LLMs, the most common approach remains incorporating the raw sensor data as text strings into prompts without extensive pre-processing \citep{kim2024health, liu2023large, xu2024penetrative}. While this technique has shown promise for aggregated short-duration time series (e.g., $<$ 200 time steps), behavioral sensing data often has long duration sequences. For example, one day of data in one minute intervals consists of 1440 data points. Sampling rates $>1$Hz will have substantially more data points. Therefore, representing sensor data as text has the following limitations. First, an LLM performance decreases with longer duration sequences \citep{yoon2024my}. To empirically demonstrate this using LLaMA 3.2 \citep{dubey2024llama}, we perform a mean prediction task using synthetic data, where LLaMA is asked to predict the mean with increasing sequence length. In Figure \ref{fig:limitations}, we notice that the mean absolute error (MAE) increases with increasing sequence length. Second, long duration sequences require significantly more tokens, which may quickly exhaust the models limits. Specifically, observe that the number of tokens required for LLaMA 3.2 is $\sim 10\times$ the length of the time series (Figure \ref{fig:limitations}). Third, effective prompting is challenging and requires strong domain knowledge (adding specific knowledge improves prompting performance -- Table \ref{tab:overall_performance} LLaMA-ICL vs LLaMA-ICL-DK), remaining an active research area in NLP \citep{zamfirescu2023johnny, bommasani2021opportunities, liu2023pre}.

Our approach offers an alternative perspective for addressing these limitations with mental health as an application. By learning a mapping that seamlessly integrates a TFM into the LLM workflow, \textit{Time2Lang inputs fixed-size embeddings directly into the LLM, eliminating the need for prompting}. We show that the Time2Lang framework is more scalable and efficient compared to generative LLM prompting, while preserving the performance of the TFM.

\section{Methods} \label{sec:methods}

\subsection{Synthetic Data Generation} \label{sec:synthetic}
Human behavioral signals often display periodic patterns, and leveraging self-supervised learning (SSL) to capture these patterns has been shown to produce robust representations \citep{yang2022simper, gu2019home}. To enable Time2Lang to learn periodicity, we generate synthetic data that mimics intricate sensing patterns by sampling from a Gaussian Process (GP) regression. In particular, we modify KernelSynth \citep{ansari2024chronos} to produce time series that exactly repeats itself after specific interval. A GP is defined by a mean function $m(a)$ (set to 0) and a covariance function or kernel $k(a, a')$ which models the joint variability of the pair of points $(a, a')$ \citep{schulz2018tutorial}. 
% \ds{we need some intuition here, why do we think that predicting periodicity would help? maybe cite the SimPer paper as inspiration?}

We perform the following steps to generate the synthetic data. First, we randomly sample up to $j=4$ non-periodic kernels from a bank $\mathcal{K}$ consisting of the following kernels: constant, white noise, linear, radial basis function, and rational quadratic ($\{k_1(a, a'), \cdots, k_j(a, a')\}$). These kernels are useful to model general sensor patterns such as trend, noise, and local variation. Second, we sample a periodic value $p$ from a set of pre-defined values $p \in \{30, 60, 90, 120, 150, 180\}$, and generate the exponential sine squared kernel, $k_{per}(a, a')$. This infuses seasonality into the generated data such that patterns exactly repeat themselves. Third, given a set of non-periodic and periodic kernels $\{k_1(a, a'), \cdots, k_j(a, a'), k_{per}(a, a')\}$, we compose them into a single $k(a, a')$ by applying a random binary operation $+$ or $\times$. Finally, a synthetic time series of length $l=1440$ (one day of data in one-minute intervals) is generated by sampling from the GP with $m(a)=0$ and covariance function $k(a, a')$. 
% \ds{a good counter-experiment here (and good baseline) would be to pre-train with completely random timeseries, would the results still hold?}

\subsection{Time2Lang} \label{sec:Time2Lang}

Time2Lang consists of four sequential components (Figure \ref{fig:Time2Lang}), a frozen TFM ($C$), a learnable input encoder ($f$), a frozen LLM ($M$), and a learnable projection ($g$). It is worth noting that inputs to the input encoder and the projection are the outputs of $C$ and $M$, respectively. 

\subsubsection{Training} 
We train Time2Lang within an SSL framework using synthetic data, where periodicity prediction is the pretext task. Formally, given a time series $\mathbf{x} \in \mathbb{R}^{n}$ of length $n$ and the corresponding periodicity $y \in \{30, 60, 90, 120, 150, 180\}$, the objective is to learn $f$ and $g$ to accurately predict periodicity as a multi-class classification problem.

% Time2Lang is trained following the steps outlined in Algorithm \ref{alg:training}. Given an pair $(\mathbf{x}, y)$, Time2Lang first extracts the features from $\mathbf{x}$ using a frozen TFM - Chronos ($C$) \citep{ansari2024chronos}. The extracted chronos embeddings $\mathbf{z^c} \in \mathbb{R}^{c \times d}$ consist of temporal features that are fixed irrespective of input size, where context length $c=513$ and features $d=768$. To learn the appropriate mapping between Chronos and the frozen LLM -- LLaMA 3.2 ($M$), we train an input encoder ($f$), which is a lightweight 1D ResNet-style CNN consisting of 6 layers. The encoder takes $C$'s output $\mathbf{z^c}^T$ as inputs to produce compressed temporal features $\mathbf{z^i} \in {\mathbb{R}^{d \times c}}$, where features $d$ and length $c$ are dependent on the convolution and pooling parameters ($d=65$ and $c=64$ in our case). Next, we pass $\mathbf{z^i}^T$ as an input embedding to LLaMA ($M$) and take the average across $f$, producing $\mathbf{z^m} \in \mathbb{R}^{2048}$. Subsequently, we project $\mathbf{z^m}$ using two fully-connected (FC) layers with 768 and 256 neurons, and a residual connection between the averaged $\mathbf{z^c}$ and the first FC layer. Finally, $\mathbf{z^o}$ is passed through a output FC layer $l$ to produce the logit vector $\mathbf{\hat{y}}$ for the each periodicity. Intuitively, the input encoder and projection networks $f$ and $g$ learn the optimal way to probe the frozen LLaMA model by using periodicity as a pretext task. The model is trained end-to-end by optimizing the multi-class cross entropy loss as follows:

% \ds{we are missing some intuition here (or earlier in the paper): essentially most people just throw raw signals to LLMs, here we use a feature extractor and TFMs happen to be good extractors. The question is why do we think that these features are understandable by an LLM? Our answer is that we force the adapter in the middle to learn to translate between TS features to LM features. Let's make it more explicit.}

% \ds{do we show any ablation with and without the residual?}

Time2Lang is trained following the steps outlined in Algorithm \ref{alg:training}. Given an pair $(\mathbf{x}, y)$, we first extract features from $\mathbf{x}$ using a frozen TFM - Chronos ($C$) \citep{ansari2024chronos}. The resulting Chronos embeddings $\mathbf{z^c} \in \mathbb{R}^{c \times d}$ are temporal features that are fixed irrespective of input size, where context length $c=513$ and features $d=768$. Previous work has shown that TFMs, such as Chronos, effectively extract meaningful time-series features. Instead of using raw signals as prompts, we leverage these extracted features. However, since there is no guarantee that an LLM can interpret these features, we aim to explicitly establish a meaningful mapping between TFM-derived features and an LLM, specifically LLaMA 3.2 ($M$). To achieve this, we train an input encoder ($f$), a lightweight 1D ResNet-style CNN. This encoder processes the transposed output of $C$, $\mathbf{z^c}^T$, generating compressed temporal features $\mathbf{z^i} \in {\mathbb{R}^{d \times c}}$, where the feature dimension $d$ and sequence length $c$ are determined by the convolution and pooling parameters ($d=65$, $c=64$ in our case). The encoded representation, $\mathbf{z^i}^T$, is passed to input embedding of LLaMA ($M$), instead of token ids. The output is averaged to obtain $\mathbf{z^m} \in \mathbb{R}^{2048}$. Subsequently, we project $\mathbf{z^m}$ using two fully-connected (FC) layers with 768 and 256 neurons. Additionally, a residual connection is introduced between the averaged $\mathbf{z^c}$ and the first FC layer to enhance information retention from Chronos. Finally, $\mathbf{z^o}$ is passed through a output FC layer $l$ to produce a logit vector $\mathbf{\hat{y}}$. Conceptually, the input encoder ($f$) and projection network ($g$) are trained to optimize the probing of the frozen LLaMA model, leveraging periodicity as a pretext task to learn useful embeddings. The model is trained end-to-end by optimizing the multi-class cross entropy loss as follows:

\begin{equation} \label{eqn:cross_entropy}
\mathcal{L} = -\frac{1}{N}\sum_{k=1}^{N} \log \frac{\exp{(\mathbf{\hat{y}}_{k,y_k})}}{\sum_{j=1}^{6}\exp{(\mathbf{\hat{y}}_{k,j})}}
\end{equation}
where $N$ is batch size, $y_i$ is the correct index for sample $i$, and $6$ represents the number of periodic classes.

% \begin{algorithm}
% \caption{Time2Lang Training}
% \begin{algorithmic}
% \Require $N$ synthetic time series $\{\mathbf{x}_k\}_{k=1}^{N}$, $C$, $M$, $f$, $g$
% \For{$k \in \{1, \dots, N\}$}
%     \State $\mathbf{z^c} \gets C(\mathbf{x}_k)$
%     % \State $\mathbf{z^i} \gets f(\mathbf{z^c}^T)$
%     % \State $\mathbf{z^m} \gets M(\mathbf{z^i}^T)$
%     % \State $\mathbf{z^m} \gets \text{mean}(\mathbf{z^m})$
% \EndFor
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm2e}[t]
\caption{Time2Lang Training} \label{alg:training}
\algorithmfootnote{\textsuperscript{\textdagger} mean is computed over the context length dimension}
\KwIn{$N$ synthetic time series with periodicity $\{\mathbf{x}_k, y_k\}_{k=1}^{N}$, TFM: $C$, LLM: $M$, Input Encoder: $f$, Projection: $g$, Output layer: $l$.}
\For{$k \in \{1, \dots, N\}$}{
     $\mathbf{z^c}_k \leftarrow C(\mathbf{x}_k)$ \\
     $\mathbf{z^i}_k \leftarrow f((\mathbf{z^c}_k)^T)$ \\
     $\mathbf{z^m}_k \leftarrow M((\mathbf{z^i}_k)^T)$ \\
     $\mathbf{z^m}_k \leftarrow \text{mean}(\mathbf{z^m}_k)$ \\
     $\mathbf{z^o}_k \leftarrow g(\mathbf{z^m}_k, \text{mean($\mathbf{z^c}_k$))}$ \# residual\\
     $\mathbf{\hat{y}}_k \leftarrow l(\mathbf{z^o}_k)$
} 
Compute $\mathcal{L} = -\frac{1}{N}\sum_{k=1}^{N} \log \frac{\exp{(\mathbf{\hat{y}}_{k,y_k})}}{\sum_{j=1}^{6}\exp{(\mathbf{\hat{y}}_{k,j})}}$ \\
Update networks $f$ and $g$ to minimize $\mathcal{L}$.
\end{algorithm2e}

Our rationale for choosing Chronos and LLaMA is based on several key considerations. First, Chronos draws inspiration from language modeling, specifically, its approach to time series quantization and discretization is analogous to tokenization in language models. Second, it transforms variable-length inputs into fixed-size embeddings, enhancing efficiency as input lengths grow. Lastly, these models are well-established within their respective fields, promoting the development of findings that are more broadly generalizable.

\vspace{-0.3cm}
\subsubsection{Implementation} \label{sec:implementation}
\vspace{-0.1cm}
We generate 200K synthetic samples, as outlined in Section \ref{sec:synthetic}. These samples are further split into training, validation, and testing set in 70/10/20 ratio. Time2Lang is trained \textit{exclusively} on synthetic samples for 25 epochs, using a batch size of 16, on an NVIDIA A4500 GPU. The training process optimizes the model to predict periodicity by minimizing the cross-entropy loss (Equation \ref{eqn:cross_entropy}). Specifically, the Adam optimizer with a learning rate of $5 \times 10^{-4}$ is used to update the parameters of the networks $f$ and $g$. Given the sensitive nature of mental health data, we implement the model locally, utilizing Chronos base and LLaMA 3.2 as frozen foundation models (FMs) with 200M and 1B parameters, respectively. Moreover, if our approach works for these smaller models, it will improve with larger models due to scaling laws \citep{ansari2024chronos, kaplan2020scaling}. The input encoder $f$ consist of 300K parameters ($\approx 0.03\%$ of LLaMA), while the projection $g$ consists of 1.7M parameters ($\approx0.17\%$ of LLaMA).

\vspace{-0.2cm}
\section{Experimental Setup} \label{sec:experiments}
In this section, we describe our rationale for choosing datasets, baselines, and evaluation protocol.

\subsection{Datasets} \label{sec:datasets}
We sought to evaluate Time2Lang on diverse longitudinal datasets in terms of sensing modality, devices, population, and tasks. Therefore, we examine: (1) daily depression detection in a clinical population using step count data from a wearable device, and (2) classification of flourishing levels in a student population (non-clinical) based on their conversation duration from mobile phones over the course of an academic term. We describe dataset information relevant to our analyses below. 

\subsubsection{Major Depressive Disorder (MDD)}
The Major Depressive Disorder (MDD) longitudinal study investigates intra-day fluctuations in depressive symptoms among individuals diagnosed with MDD. In this 90-day study, we recruited participants aged 18 years and older, residing in the United States. To determine eligibility, we administered the structured clinical interview for DSM-5 (SCID), ensuring that only individuals with MDD, but without co-morbid bipolar disorder, active suicidality, or psychosis, were enrolled. Each participant was instrumented with a Garmin vivoactive 3 wearable and installed our Android application. While the Garmin passively collected sensing data, the mobile application prompted to the user to answer the Patient Health Questionnaire-9 (PHQ-9) three times a day \citep{kroenke2001phq}. This self-reported survey consisting of nine questions rated on a 0-3 Likert scale is well-validated tool to evaluate depression. To encourage engagement, participants received \$1 per completed survey, with an additional \$50 bonus for maintaining a survey completion rate greater than 90\%. Now, we describe information pertaining to the analysis in this work.

Physical activity plays a crucial role in depression fluctuations, and step count provides a straightforward measure of activity levels. Notably, a daily step count exceeding 5,000 has been associated with fewer depressive symptoms \citep{bizzozero2024daily}. Hence, we assess daily depression levels using step count data from the Garmin wearable. It is worth noting that daily depression prediction is challenging problem compared to longer time-scales. In our analysis, the input sensing data consists of 1,440 minute-level step count values per day, while the ground truth is derived from the Patient Health Questionnaire-9 (PHQ-9), where a score of 10 or higher indicates depression (Figure \ref{fig:phq9}). To handle missing data, we apply a threshold-based approach: if missingness is below 25\%, we impute missing values using zero-filling; otherwise, we exclude the day from the analysis. The PHQ-9 score is averaged within each day to align with the step count data. Our dataset comprises 256 participants, covering a total of 17,251 person-days (68\% depressed days), with corresponding depression labels.

\subsubsection{StudentLife}
The StudentLife study \citep{wang2014studentlife} tracked the behavior of 48 undergraduate students at Dartmouth College over a 10-week term using mobile sensing. An Android application passively collected conversation duration from ambient sound, while self-reported surveys were administered to examine students' mental health. Beyond academics, psychological well-being is an important part of college life, involving personal growth, social connections, and civic engagement. The Flourishing Scale (FS) \citep{diener2010new} measures this well-being through an 8-item questionnaire rated on a 1-7 scale from strongly disagree to strongly agree (Appendix \ref{apd:emas}). The total FS score provides a single measurement of psychological well-being. As social contact is directly examined in FS, we focus on classifying students into high and low flourishing (median split) groups based on their conversation duration throughout the academic term. Specifically, the input is a variable-length conversation duration time series up to 10 weeks, where each data point represents hourly conversation duration, and the output is a binary indicator of high or low flourishing. We use data from 46 students with and average time series length of $1455 \pm 193$. 

\begin{table*}[] 
\centering
\caption{\textbf{Time2Lang performance comparison against baselines}. Values in \textbf{bold} denote when models outperform the upper bound (Chronos). Higher values indicate better performance.}
\begin{tabular}{lllll}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{l}{Depression (step count)} & \multicolumn{2}{l}{Flourishing (conversation)} \\ \cmidrule{2-5}
    & AUROC                 & AUPRC                & AUROC                  & AUPRC                 \\ \midrule
 Random (Lower) &0.50 &0.68 &0.50 &0.52 \\
 Chronos (Upper) &0.56 (0.01) &0.72 (0.01) &0.70 (0.15) & 0.69 (0.14) \\
 \hdashline
 LLaMA-ICL &0.49 (0.03) &0.67 (0.02) &0.51 (0.11) &0.61 (0.09) \\
 LLaMA-ICL-DK &0.53 (0.03) & 0.68 (0.02) &0.54 (0.10) & 0.63 (0.15) \\
 Chronos + LLaMA  &0.54 (0.01) &0.70 (0.00) &0.56 (0.16) & 0.65 (0.15) \\
 Time2Lang &\textbf{0.57} (0.01) &\textbf{0.73} (0.00) & \textbf{0.71} (0.10) &\textbf{0.75} (0.11) \\
 \bottomrule
 \label{tab:overall_performance}
\end{tabular}
\vspace{-0.25cm}
\end{table*}

\subsection{Baselines} \label{sec:baselines}
We evaluate Time2Lang using several baseline approaches. The lower bound baseline is \textbf{Random} classification where AUROC is 0.5 and AUPRC is the proportion of positive samples. As we transfer information from Chronos to LLaMA, we treat \textbf{Chronos} as the upper bound baseline, assuming some information loss occurs during translation. To determine whether reprogramming through a mapping function is necessary, we directly embed Chronos into the LLaMA embedding without a mapping function (with embeddings padded as needed), referred to as \textbf{Chronos + LLaMA}. To compare against prior prompting methods, we use a 3-shot in-context learning (ICL) approach with LLaMA, referred to as \textbf{LLaMA-ICL}. The prompt design follows established best practices from existing literature. First, from LLMTime \citep{gruver2024large}, we round numerical values to two-digit precision, remove periods without adding spaces, and enclose the time series values in quotes to convert them into strings (e.g., $0.123 \rightarrow ``012"$). Second, building on HealthLLM \citep{kim2024health}, we include health-related context (e.g., ``Some symptoms of depression include a lack of motivation and fatigue, which can be inferred from daily step counts.") and temporal context (e.g., ``\{20, 25, ..., 30"\}). To further support LLaMA-ICL, we incorporate domain-specific knowledge -- \textbf{LLaMA-ICL-DK}, which is crucial for guiding the model effectively. For example, ``Conversation duration is associated with flourishing, with longer durations typically indicating higher levels of flourishing." This addition from the self-reported instrument provides more context to better understand and interpret the data within the specific application domain.

\vspace{-0.25cm}
\subsection{Evaluation} \label{sec:evaluation}
After initial pre-training, Time2Lang can be used for a wide variety of different time-series tasks (here convo and steps) without further fine-tuning. For downstream evaluation, we process the input sensing data from real datasets and extract embeddings from the penultimate linear layer \textit{without} the residual connection to determine whether the information from Chronos has been effectively distilled. The extracted embeddings are then used to train either a logistic regression (LR) or random forest (RF) model for mental health classification. Prior to training, we split the dataset into an 80/20 subject-wise split for training and hold-out testing. Next, we perform grid search cross-validation (3 folds) on the training data to identify the optimal model parameters. Once the best parameters are selected, we evaluate the final model on the hold-out test set using the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). AUROC evaluates the overall discriminative power of the model, while AUPRC places greater emphasis on the performance for the positive class. Moreover, we repeat the experiments across five different shuffled training splits and report the standard deviations of the results for the best model (LR or RF). 

\vspace{-0.25cm}
\section{Results} \label{sec:results}

\subsection{Overall Performance}

Table \ref{tab:overall_performance} presents the performance comparison of Time2Lang with the different baselines for depression detection and flourishing classification. For depression detection using step count, Time2Lang achieves an AUROC of 0.57 and an AUPRC of 0.73, outperforming the baselines. Compared to Chronos, Time2Lang slightly improves the AUROC (0.56 vs. 0.57) and AUPRC (0.72 vs. 0.73) for depression detection. For flourishing classification using conversation duration, Time2Lang surpasses the prompting-based and Chronos + LLaMA baselines, with AUROC and AUPRC improvements ranging from 0.15 to 0.20 and 0.10 to 0.14, respectively. Compared to Chronos, Time2Lang achieves an AUROC of 0.71 (+0.01) and an AUPRC of 0.75 (+0.06). The higher AUPRC suggests that Time2Lang improves positive case detection. Overall, Time2Lang consistently outperforms most baselines, demonstrating marginal gains over Chronos in three cases and a more substantial improvement in one case.

Some other interesting observations from Table \ref{tab:overall_performance} are as follows.  First, incorporating domain knowledge into prompting enhances performance across tasks in both AUROC and AUPRC (LLaMA-ICL vs. LLaMA-ICL-DK). Notably, for depression detection, LLaMA-ICL underperforms the lower-bound baseline, with an AUROC of 0.49 and an AUPRC of 0.67, suggesting that prompting for long-duration sequences poses challenges. However, adding domain knowledge improves AUROC by 0.04 and AUPRC by 0.01, demonstrating its positive impact. Second, integrating Chronos with LLaMA leads to a decline in performance, particularly in the flourishing prediction task, where AUROC and AUPRC drop by 0.14 and 0.04, respectively, compared to Chronos alone. In contrast, Time2Lang enables positive performance transfer, a 0.01 increase in AUROC while achieving a notable 0.06 increase in AUPRC, highlighting the necessity of having a mapping to bridge two distinct domains.

\vspace{-0.25cm}
\subsection{Efficiency Analysis}

\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.24\textwidth]{images/inference_time_length_bar_Time2Lang.pdf}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{images/inference_time_studentlife.pdf}} 
    \caption{\textbf{Efficiency Analysis}. Inference time (seconds) comparisons between Time2Lang and Prompting for (a) different input sequence lengths, (b) variable-length conversation duration data from StudentLife. We repeat measurements 100 times and observed a variance of $<10^{-2}$.}
    \label{fig:efficient}
\end{figure}

\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=\linewidth]{images/effect_of_parameters_flourishing.pdf}} 
    \subfigure[]{\includegraphics[width=0.5\textwidth]{images/effect_of_parameters_depression.pdf}} 
    \caption{\textbf{Pre-training ablations}. The effect of pre-training data size and periodicity classes on downstream performance. Optimal performance is achieved with 200K samples in a 6-class classification problem.}
    \label{fig:effect_of_pre_training}
    \vspace{-0.7cm}
\end{figure}

\begin{figure*}
    \centering
    \subfigure[]{\includegraphics[width=0.34\linewidth]{images/sl_acf_time2lang_corr.pdf}}
    \subfigure[]{\includegraphics[width=0.15\linewidth]{images/sl_kde_corr.pdf}}
    \subfigure[]{\includegraphics[width=0.34\linewidth]{images/mdd_acf_time2lang_corr.pdf}}
    \subfigure[]{\includegraphics[width=0.15\linewidth]{images/mdd_kde_corr.pdf}}
    \caption{\textbf{Comparing embeddings' temporal dynamics.} The Spearman rank correlation between Time2Lang embeddings and the Auto-Correlation Factor (ACF) of the original sensing data. Subfigures (a) and (c) illustrate the correlation between the ACF of conversation and step count data with their respective embeddings, while subfigures (b) and (d) display the distribution of the strongest correlation values across all 256 embedding dimensions.}
    \label{fig:correlation}
\end{figure*}

To evaluate the efficiency of Time2Lang and LLaMA prompting, we assess inference times using two approaches. First, we measure inference time across varying time series lengths: $\{256, 512, 1024, 2048, 4096\}$ (Figure \ref{fig:efficient}(a)). Second, given that the StudentLife conversation data represents a variable-length time series, we evaluate inference time across all data samples (Figure \ref{fig:efficient}(b)). Prior to measurement, we ensure consistent execution by transferring the data and model to the device, synchronizing GPU, and performing a warm-up phase with 100 iterations. As shown in Figure \ref{fig:efficient}, Time2Lang consistently achieves lower inference times compared to prompting across different data lengths in the synthetic experiment and conversation duration data in the StudentLife dataset. Notably, the inference time for Time2Lang remains relatively stable as the data length increases, whereas LLaMA prompting exhibits a proportional increase in inference time. This disparity is primarily due to the increasing relationship between input token length and sensing data length, which affects the computational overhead of prompting-based approaches.

\vspace{-0.35cm}
\subsection{Effect of Parameters}
\vspace{-0.2cm}

We evaluate the impact of synthetic data size and number of periodicity classes on downstream performance. Specifically, we consider dataset sizes of 50K, 100K, and 200K samples and periodicity class counts of 3 (30 to 90), 6 (30 to 180), and 9 (30 to 270), resulting in nine combinations. As shown in Figure \ref{fig:effect_of_pre_training}, pre-training with 200K samples and 6 periodicity classes achieved the best performance across all cases. Among dataset sizes, performance improves as data increases from 50K to 100K, but the difference between 100K and 200K is marginal. Similarly, increasing periodicity classes from 3 to 6 generally enhances performance. However, further increasing the classes to 9 does not yield additional improvements, regardless of dataset size. We attribute this plateau to the model’s parameter capacity, as a 9-class classification problem demands a more complex network, which may exceed the capacity of the trainable parameters.

\vspace{-0.45cm}
\subsection{Case Study: Correlation of Embeddings and ACF} 
\vspace{-0.25cm}
\label{sec:case_study}
We aimed to evaluate whether, after reprogramming through Time2Lang, the LLM outputs effectively capture fundamental time series characteristics. Thus, we computed the Spearman rank correlation between the Auto-Correlation Factor (ACF) of the original time series and the embeddings. First, we calculated the ACF with 10 lags from the original sensing data. Then, we computed the pairwise correlation between each ACF lag and the corresponding Time2Lang embedding across the samples. The heatmaps in Figure \ref{fig:correlation} (a) and (c) show that Time2Lang embeddings capture both positive and negative correlations across conversation and step count data. Notably, we observe stronger correlations at higher time lags (7–9), suggesting that Time2Lang embeddings retain information extending beyond recent past observations. To further analyze the strength of these correlations, we identified the highest correlation value for each embedding dimension at any lag. The distribution of these values, shown in Figure \ref{fig:correlation} (b) and (d), indicates that for conversation data, 174 dimensions ($\sim$68\%) have at least one lag where the correlation exceeds 0.3, signifying a moderate correlation. Similarly, for step count data, 70 dimensions ($\sim$27\%) demonstrate a moderate correlation greater than 0.3.


\section{Discussion}

\subsection{Findings} \label{sec:findings}

Time2Lang achieved an AUROC=0.57 and AUPRC=0.73 for predicting daily depression based on step count and an AUROC=0.71 and AUPRC=0.75 for predicting flourishing. In the context of mental health, its depression prediction performance aligns with findings from previous studies \citep{xu2023globem}. Among prompting approaches, incorporating specific domain knowledge enhances performance, consistent with prior observations by \citet{kim2024health}. However, Time2Lang consistently outperforms prompting across both metrics and tasks. The LLaMA + Chronos approach showed markedly lower performance than Chronos alone, indicating that information is lost when integrating two distinct models. Our goal is to minimize performance loss during translation when leveraging Chronos. In this regard, Time2Lang outperforms LLaMA + Chronos, demonstrating that learning a direct mapping yields better results than simply combining embeddings. Ultimately, Time2Lang achieves both marginal and notable improvements over Chronos, showcasing that a TFM can be integrated with an LLM by minimizing information loss across these two distinct domains.

Through our efficiency analysis, we find that Time2Lang's inference time is steady across increasing length at approximately 0.04 seconds. However, we notice that LLaMA Prompting increases with longer duration sequences primarily because of more tokens. The faster inference time of Time2Lang can be attributed to the TFM that produces a fixed sized embeddings, irrespective of input length. Our correlation analysis reveals that after mapping, the LLM outputs from Time2Lang exhibit moderate to high correlations with ACF, indicating that our method effectively captures key time series properties. We attribute this capability to our self-supervised pre-training, which is designed to predict periodicity. Interestingly, this pre-training on synthetic data, generalizes to real-world datasets, demonstrating the robustness of our approach.

\subsection{Implications}

Our work has significant implications for the development of personalized mental health LLMs and context-aware interventions. It represents an essential first step toward integrating sensing data into LLM-driven mental health systems, enabling them to provide critical, user-specific insights. Prior research has demonstrated that mental health symptoms fluctuate significantly within a single day and across different populations \citep{lim2018prevalence, crowe2019intra}. A personalized system that continuously monitors behavioral sensing data can identify individual-specific patterns, offering a more nuanced understanding of mental health variations. Unlike traditional check-ups, which provide only periodic assessments, continuous monitoring enables dynamic, real-time recommendations tailored to the user’s needs. However, to deliver accurate and effective mental health recommendations, the first step is ensuring that the model can comprehend and process sensing data efficiently. Scalability and computational efficiency are crucial for handling continuous sensing streams. As discussed in Sections \ref{sec:results} and \ref{sec:findings}, the Time2Lang approach minimizes information loss while achieving performance comparable to a TFM, improving efficiency, and establishing strong connections to fundamental time series characteristics. These results show that fusing two FMs from distinct domains can lead to meaningful representations, consequently paving the way for future work that may enable real-time generation and dynamic mental health interventions for users.

% Our work has implications in building personalized mental health LLMs and context-aware interventions. Our work presents an important first step toward the vision of personalized mental health LLMs that integrates sensing data to provide critical insights to the user. Previous work has showcased that mental symptoms vary greatly within a single day and across different populations \citep{lim2018prevalence, crowe2019intra}. Therefore, having a personalized system with access to continuous behavioral sensing data can analyze patterns specific to each individual user. Unlike traditional check-ups, continuous monitoring allows for dynamic recommendations. However, to realize accurate and efficient recommendations for mental health care the first step is to ensure that the model comprehends the characteristics of the sensing data and the processing is scalable efficient for continuous sensing. As discussed in Sections \ref{}, the Time2Lang approach emphasizes better efficiency and relationship to classical time series. 

\subsection{Limitations \& Future Work}

We acknowledge the limitations of our work, particularly its reliance on uni-modal time series data, as Time2Lang (Chronos) processes a single modality. Extending it to multi-modal data is a promising direction that would enhance Time2Lang's ability to handle richer data. While we demonstrate Time2Lang’s effectiveness for mental health, its potential extends to other areas including physiological sensing, industrial sensors, and environmental monitoring. Therefore, we plan to evaluate our method on more diverse datasets to evaluate generalizability in the future.

The selection of Chronos and LLaMA is motivated by their fixed-length embeddings, relation to language modeling, and practical utility, as discussed in Section \ref{sec:Time2Lang}. However, different tasks or modalities may benefit from alternative pre-trained models. Rather than prescribing a one-size-fits-all approach, we aim to provide a new perspective on leveraging LLMs for sensing data analysis. Future work should systematically evaluate modality interactions and model suitability.

Designing computationally efficient reasoning methods for behavioral sensing in mental health remains challenging. Prior research suggests reasoning is strongly tied to scaling \citep{moon2024anymal, girdhar2023imagebind, spathis2024first}. While our approach emphasizes efficiency, its reasoning capabilities remain limited. However, Time2Lang provides a foundation for integrating time series data into LLMs, which we aim to extend for more complex reasoning and generative tasks. Ultimately, our goal is to develop a personalized LLM that understands individual user behavior through daily data, enabling more tailored and insightful interactions.

\vspace{-0.2cm}
\section{Conclusion}
\vspace{-0.1cm}

In this work, we propose Time2Lang, a framework that integrates sensing data with LLMs by learning a direct mapping from a TFM (Chronos) to an LLM (LLaMA).  This approach overcomes the limitations of text-based prompting. Through extensive evaluation on real-world longitudinal datasets for tasks such as daily depression prediction and flourishing classification, we demonstrate that Time2Lang positively integrates a TFM and an LLM while maintaining stable inference times across varying input lengths. Furthermore, our correlation analysis highlights Time2Lang effectively reprograms LLM outputs to capture fundamental time-series properties such as auto-correlation. Ultimately, Time2Lang enhances efficiency, accuracy, and time-series comprehension compared to traditional prompting methods. By bridging the gap between behavioral sensing and LLM-based inference, Time2Lang represents a significant step toward enhancing AI-driven mental health assessment. Future work will explore its extension to reasoning with multi-modal sensing data, further improving its applicability in personalized mental health monitoring and intervention systems.

\bibliography{chil-sample}
\newpage

\appendix

\section{Architecture \& Hyperparameters} \label{apd:architecture}
The input encoder ($f$) comprises 13 convolutional layers (Table \ref{tab:resnet}): an initial layer, followed by six 2-layer blocks (Tables \ref{tab:resnet_block1} and \ref{tab:resnet_block2}). The input channels are 768, and we begin with 32 base filters, using a kernel size of 3 and stride of 2. The projection layer consists of two fully connected layers with 768 and 256 features, respectively. Each layer is followed by batch normalization and ReLU activation to ensure stable training and improved representation learning.

\begin{table}[h]
    \centering
    \caption{ResNet-Style CNN}
    \begin{tabular}{l}
    \toprule
        \textbf{Layer}\\
        \midrule
        Conv1D \\
        BatchNorm \\
        ReLU \\
        (Basic Block Type 1) x 1 \\
        (Basic Block Type 2) x 5 \\
        BatchNorm \\
        ReLU\\
        \bottomrule
    \end{tabular}
    \label{tab:resnet}
\end{table}

\begin{table}[h]
\centering
\begin{minipage}{0.4\linewidth}
    \centering
    \caption{Basic Block Type 1}
    \begin{tabular}{@{}l@{}}
    \toprule
    \textbf{Layer} \\ \midrule
    Conv1D \\
    BatchNorm \\
    ReLU \\
    Dropout \\
    Conv1D \\
    \bottomrule
    \end{tabular}
    \label{tab:resnet_block1}
\end{minipage}
\hfill
\begin{minipage}{0.4\linewidth}
    \centering
    \caption{Basic Block Type 2}
    \begin{tabular}{@{}l@{}}
    \toprule
    \textbf{Layer} \\ \midrule
    BatchNorm \\
    ReLU \\
    Dropout \\
    Conv1D \\
    BatchNorm \\
    ReLU \\
    Dropout \\
    Conv1D \\
    Maxpool \\
    \bottomrule
    \end{tabular}
    \label{tab:resnet_block2}
\end{minipage}
\end{table}

In our downstream evaluation, logistic regression performed the best for depression detection using step count, whereas random forest is used for classifying flourishing using conversation duration. We obtained the best model using the following hyperparameters. Logistic regression: \{'penalty': ['l1', 'l2', 'elasticnet', 'none'], 'C': [0.1, 1, 10, 100], 'solver': ['saga'], 'l1\_ratio': [0.5, 0.7, 0.9]\} and Random Forest: \{'n\_estimators': [50, 100], 'max\_depth': [10, 20, 50], min\_samples\_split': [2, 10], 'min\_samples\_leaf': [1, 4], 'max\_features': ['sqrt', 'log2', None], 'bootstrap': [True, False]\}

\section{Data} \label{apd:data}

\subsection{MDD}

The MDD study recruited 300 participants for a 90-day study. The sample is predominantly middle-aged, with an average age of 40.13 years and an age range of 19 to 79 years, covering early adulthood to late older adulthood. The majority of participants are White (79\%), heterosexual (66\%), and women (84\%), with an average age of 40 years. The racial distribution of minority groups, including Black, Asian, and American Indian or Alaska Native participants, closely reflects the U.S. MDD population, while Hispanic and Latino individuals (12\%) match national demographic proportions. Most participants (93\%) have some college experience, with 68\% holding a college degree, and over half (61\%) are employed. Household incomes are distributed across all income brackets, largely aligning with the general U.S. distribution at the time of sampling. The study also maintained high engagement, with an average compliance rate of 82.07\%, based on daily Ecological Momentary Assessment (EMA) responses per participant over the 90-day period.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{images/phq_9_scores.pdf}
    \caption{\textbf{Label statistics}. PHQ-9 Distribution in the MDD dataset. Scores $\ge$ 10 are considered depressed. 68\% of the days correspond to depression.}
    \label{fig:phq9}
\end{figure}

\section{Self-reported Surveys and Ecological Momentary Assessments}\label{apd:emas}

\subsection{Patient Health Questionnaire - 9 (PHQ-9) \citep{kroenke2001phq}}
In our MDD study, the user is asked over the past four hours if they have experienced the questions/statements in Table \ref{tab:phq9_scale}.

\begin{table*}[]
\centering
\caption{Patient Health Questionnaire - 9 (PHQ-9).}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{No.} &\textbf{Statement} \\
\midrule
1 &Little interest or pleasure in doing things \\
2 &Feeling down, depressed, or hopeless \\
3 &Trouble falling or staying asleep, or sleeping too much \\
4 &Feeling tired or having little energy \\
5 &Poor appetite or overeating \\
6 &Feeling bad about yourself or that you are a failure or
have let yourself or your family down \\
7 &Trouble concentrating on things, such as reading the
newspaper or watching television \\
8 &Moving or speaking so slowly that other people could
have noticed \\
9 &Thoughts that you would be better off dead, or of
hurting yourself \\
\bottomrule
\end{tabular} \label{tab:phq9_scale}
\end{table*}


\subsection{Flourishing Scale \citep{diener2010new}.} The user to indicate their agreement with each item in Table \ref{tab:flourishing_scale} by rating each statement from 1 to 7: (1) strongly disagree, (2) disagree, (3) slightly disagree, (4) mixed or neither agree nor disagree, (5) slightly agree, (6) agree, (7) strongly agree.

\begin{table*}[]
\centering
\caption{Flourishing scale statements.}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{No.} &\textbf{Statement} \\
\midrule
1 &I lead a purposeful and meaningful life. \\
2 &My social relationships are supportive and rewarding. \\
3 &I am engaged and interested in my daily activities \\
4 &I actively contribute to the happiness and well-being of others \\
5 &I am competent and capable in the activities that are important to me \\
6 &I am a good person and live a good life \\
7 &I am optimistic about my future \\
8 &People respect me \\
\bottomrule
\end{tabular} \label{tab:flourishing_scale}
\end{table*}

\section{Prompt Templates} \label{apd:prompt_templates}

\subsection{Depression Prompt}
The prompt template for depression is given below. In this template, we use temporal and health context. Furthermore, we convert the labels from 0 to 1 (as a probability). This approach produced better results than directly predicting 0 or 1. 

\begin{boxB}
    You are an AI assistant that evaluates if a user is depressed based on their step count in a day.\\

    Below is an instruction that describes a task, paired with an input that provides further context. 
    Write a response that appropriately completes the request in the format provided.\\

    Task: Some symptoms of depression are a lack of motivation and fatigue. It is linked to the number of steps taken in a day. Using the step count time series for a day, output a probability between 0 to 1 where 1 indicates high depression and 0 indicates low depression\\

    Input: 
        - Step Count: \{$<$"sensor data"$>$\}
\end{boxB}

\subsection{Depression Prompt with Knowledge}
The prompt template for depression is given below where add domain knowledge (highlighted).
\begin{boxB}
    You are an AI assistant that evaluates if a user is depressed based on their step count in a day.\\

    Below is an instruction that describes a task, paired with an input that provides further context. 
    Write a response that appropriately completes the request in the format provided.\\

    Task: Some symptoms of depression are a lack of motivation and fatigue. It is linked to the number of steps taken in a day \hl{such that higher step count is associated with lower depression symptoms.} Using the step count time series for a day, output a probability between 0 to 1 where 1 indicates high depression and 0 indicates low depression\\

    Input: 
        - Step Count: \{$<$"sensor data"$>$\}
\end{boxB}

\subsection{Flourishing Prompt} 
The prompt template for flourishing is given below. In this template, we use temporal and health context. 
\begin{boxB}
    You are an AI assistant that evaluates if a user is flourishing based on their conversation duration over time.\\

    Below is an instruction that describes a task, paired with an input that provides further context.
    Write a response that appropriately completes the request in the format provided.\\
    
    Task: We know that conversation duration is related to flourishing. Using the conversation duration (seconds) time series given below, output a probability between 0 to 1 where 1 indicates high flourishing and  0 indicates low flourishing. \\

    Input: 
        - Conversation duration: \{$<$"sensor data"$>$\}
\end{boxB}

\subsection{Flourishing Prompt with Domain Knowledge}
The prompt template for flourishing is given below. In this template, we use temporal and health context with domain knowledge.
\begin{boxB}
    You are an AI assistant that evaluates if a user is flourishing based on their conversation duration over time.\\

    Below is an instruction that describes a task, paired with an input that provides further context.
    Write a response that appropriately completes the request in the format provided.\\
    
    Task: We know that conversation duration is related to flourishing \hl{such that higher conversation duration is linked to higher flourishing.} Using the conversation duration (seconds) time series given below, output a probability between 0 to 1 where 1 indicates high flourishing and  0 indicates low flourishing. \\

    Input: 
        - Conversation duration: \{$<$"sensor data"$>$\}
\end{boxB}

\end{document}
