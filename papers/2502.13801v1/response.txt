\section{Related Work}
\label{section:related_work}

\paragraph{\textbf{Safe Reinforcement Learning}}
Constrained optimal control methods allow to synthesize controllers with safety guarantees but they assume a perfect or at least partial knowledge of the system dynamics **Kallenberg, "Approximation Theorems in Mathematical Statistics"**. 
On the contrary, model-free safe reinforcement learning (safe RL) only assumes
the system dynamics to be stochastic and markovian. 
Most safe RL methods aim at solving a Constrained Markov Decision Process (CMDP),
which is built upon an analogy between rewards and constraints
**Sutton et al., "A Generalized Framework for Safe Reinforcement Learning"**,
as they formulate the constraint as a discounted cumulative objective, which is convenient 
from the RL point of view but not sufficient to satisfy persistent safety constraints along trajectories.
To overcome this issue, some methods reformulate the critic target. For example, in **Garrett et al., "Provably Safe and Robust Learning-Based Control with Learned Constraints"**, a safety critic allows to formulate the constraint in terms of failure probability **Tamar et al., "Safe Exploration in Continuous Action Spaces through Gaussian Process Constraints"**. 
Reachability-constrained RL (RCRL) **Chu et al., "Reachability-Constrained Reinforcement Learning"** proposes to use a $\max$ operator in the safety critic target to discover 
the largest set of reachable states by the policy.
%Reachability-constrained RL (RCRL) uses the $\max$ operator to obtain the largest feasible set **Bansal et al., "Safe Exploration for Reinforcement Learning"** {\color{blue} CLARIFIER}. 
In our work, we pre-train a safety policy using ingredients from RCRL and then use the critic in the
safe exploration phase to decide when to switch from one policy to another.
Also, since our goal is to prevent the worst-case scenario,
we use quantile-based distributional reinforcement learning to train our safety policy 
**Dabney et al., "Distributional Reinforcement Learning with Quantile Regression"**.

\paragraph{\textbf{Goal-conditioned RL}}
The standard RL framework addresses a single task, specified by the reward function.
Goal-conditioned reinforcement learning (GCRL) provide 
bases to build a single robotic controller that is able to achieve a wide variety of tasks and
adapt its behavior to its environment **Vezhnevets et al., "Strategic Attentive Recurrence Networks"**.
A goal can be a desired position for a given rigid body in the environment. In addition to being convenient 
for a robot user, it allows to learn from failed attempts. Indeed, an agent that has failed to reach
a given desired goal has actually reached another accidentally.
This feature can be exploited by relabelling methods, like Hindsight Experience Replay
(HER), to learn a goal-conditioned policy from sparse rewards. 
In our experiments, we consider a sparse reward setting and combine  
Soft Actor-Critic (SAC) and HER to learn goal-conditioned policies **Henderson et al., "Distributed Distributional Deep Deterministic Policy Gradients"**. 

\paragraph{\textbf{Learning diverse safe skills}}
**Chang et al., "Safe Locomotion in Three Directions for the Minitaur Robot"** have developed a method to learn safe locomotion in three directions 
for the Minitaur robot. Although policy improvement over time leads to constraint satisfaction, 
reducing the need for humain intervention for resets, the formalism does not explicitly enforce constraint 
satisfaction during exploration **Javdani et al., "Safe Exploration via Reward Reshaping"**. SASD on the other hand combines 
unsupervised skill discovery and safe RL to learn a richer set of skills, but still does not enforce safety
during the exploration phase **Chang et al., "Safe Locomotion in Three Directions for the Minitaur Robot"**.

\paragraph{\textbf{Safe exploration}}
**Javdani et al., "Safe Exploration via Reward Reshaping"** identify two categories of safe exploration methods: those based on
auxiliary reward and those based on human designer knowledge **Turchetta et al., "Learning Heuristics for Safe Exploration"**. 
Auxiliary reward methods penalize the agent when it puts itself in danger. 
By definition, these methods incentivize the agent to avoid repeated mistakes and catastrophic behaviors, 
but do not constrain the exploration policy behavior directly. As a result, they reduce the occurrence of 
mistakes compared to baselines **Turchetta et al., "Learning Heuristics for Safe Exploration"** or even improve learning 
**Javdani et al., "Safe Exploration via Reward Reshaping"** but do not prevent them.
In methods based on human designer knowledge, boundaries safe and unsafe states or behaviors 
are defined by the human designer. Most of these methods make strong assumptions about the environment
or the agent behavior, which allows the use of baseline behaviors, 
human intervention, hand-crafted model or heuristics
**Turchetta et al., "Learning Heuristics for Safe Exploration"**. 
A recent approach proposes a Meta-Algorithm for Safe Exploration (MASE) and provides theoretical guarantees on 
optimality and safety during exploration **Chow et al., "Meta-Algorithm for Safe Exploration in Reinforcement Learning"**. However, this approach assumes the agent has access to an emergency stop action that can reset the environment when no viable actions are available, an assumption that does not hold in general, and particularly not in our environments. In contrast, our focus is on environments where unviable states exist, and the agent must navigate within these constraints.
Closer to us, Dalal et al. **Dalal et al., "Safe Exploration via Constrained Policy Optimization"** use a pre-trained linearized constraint model and solve a quadratic program to project actions onto a feasible set during exploration. Their method, however, assumes access to a linearized model and that constraint violations can be avoided in a single step, which is not our case.
Like us, Srinivasan et al. **Srinivasan et al., "Safe Exploration via Model-Based Reinforcement Learning"** propose two training phases. But they pre-train and then fine-tune a single policy, while we consider two different policies that are trained separately and, above all, do not share the same input. 
In our approach, the constraint function is used as an auxiliary reward signal for the safety policy, unlike other auxiliary reward methods where it influences the task-solving policy. Additionally, by making general assumptions about the environment's structure, such as the uniformity in space of the dynamics, our method straddles both categories.
Also, contrary to previously cited approaches, we explicitly merge GCRL with safe RL tools to build our safe exploration framework.

%\textcolor{blue}{SAC-N____ (ATTENTION Finn utilise autre chose)}

%\textcolor{blue}{Safe exploration in parameters space (GP and QD): ____}