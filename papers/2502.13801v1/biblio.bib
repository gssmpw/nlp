% RL general
@misc{SAC,
      title={Soft Actor-Critic Algorithms and Applications}, 
      author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
      year={2019},
      eprint={1812.05905},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{rockafellarCVaR,
  title={Optimization of conditional value-at-risk},
  author={Rockafellar, R Tyrrell and Uryasev, Stanislav and others},
  journal={Journal of risk},
  volume={2},
  pages={21--42},
  year={2000},
  publisher={Citeseer}
}


@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@misc{SuttonBartho, 
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
} % book

@article{SACN_edac,
  title={Uncertainty-based offline reinforcement learning with diversified q-ensemble},
  author={An, Gaon and Moon, Seungyong and Kim, Jang-Hyun and Song, Hyun Oh},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={7436--7447},
  year={2021}
}

% Distributional RL
% InProceedings
@misc{TQC,
  title = 	 {Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics},
  author =       {Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5556--5566},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/kuznetsov20a/kuznetsov20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/kuznetsov20a.html},
  abstract = 	 {The overestimation bias is one of the major impediments to accurate off-policy learning. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method—Truncated Quantile Critics, TQC,—blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. Distributional representation and truncation allow for arbitrary granular overestimation control, while ensembling provides additional score improvements. TQC outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25% improvement on the most challenging Humanoid environment.}
}

% article
@misc{QR-DQN, title={Distributional Reinforcement Learning With Quantile Regression}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11791}, DOI={10.1609/aaai.v32i1.11791}, abstractNote={ &lt;p&gt; In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Dabney, Will and Rowland, Mark and Bellemare, Marc and Munos, Rémi}, year={2018}, month={Apr.} }

@inproceedings{IQN,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={1096--1105},
  year={2018},
  organization={PMLR}
}

% Environments
%software
@misc{menagerie2022github,
  author = {Zakka, Kevin and Tassa, Yuval and {MuJoCo Menagerie Contributors}},
  title = {{MuJoCo Menagerie: A collection of high-quality simulation models for MuJoCo}},
  url = {http://github.com/google-deepmind/mujoco_menagerie},
  year = {2022},
}

%software
@misc{towers2024gymnasium,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  journal={arXiv preprint arXiv:2407.17032},
  year={2024}
}

% GCRL & Skill learning
%inproceedings
@inproceedings{RIS,
  title={Goal-conditioned reinforcement learning with imagined subgoals},
  author={Chane-Sane, Elliot and Schmid, Cordelia and Laptev, Ivan},
  booktitle={International conference on machine learning},
  pages={1430--1440},
  year={2021},
  organization={PMLR}
}

@misc{skewfit,
  title={Skew-Fit: State-Covering Self-Supervised Reinforcement Learning}, 
  author={Vitchyr H. Pong and Murtaza Dalal and Steven Lin and Ashvin Nair and Shikhar Bahl and Sergey Levine},
  year={2020},
  eprint={1903.03698},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1903.03698}, 
}

@misc{PlanningWithGCpolicies,
      title={Planning with Goal-Conditioned Policies}, 
      author={Soroush Nasiriany and Vitchyr H. Pong and Steven Lin and Sergey Levine},
      year={2019},
      eprint={1911.08453},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1911.08453}, 
}


@misc{Kaelbling1993LearningTA,
  title={Learning to Achieve Goals},
  author={Leslie Pack Kaelbling},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={1993},
  url={https://api.semanticscholar.org/CorpusID:5538688}
}

%INPROCEEDINGS
@misc{SASD,
  author={Kim, Sunin and Kwon, Jaewoon and Lee, Taeyoon and Park, Younghyo and Perez, Julien},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Safety-Aware Unsupervised Skill Discovery}, 
  year={2023},
  volume={},
  number={},
  pages={894-900},
  keywords={Training;Heuristic algorithms;Reinforcement learning;Programming;Safety;Behavioral sciences;Dynamic programming},
  doi={10.1109/ICRA48891.2023.10160985}
}

@inproceedings{sac_n_edac,
 author = {An, Gaon and Moon, Seungyong and Kim, Jang-Hyun and Song, Hyun Oh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7436--7447},
 publisher = {Curran Associates, Inc.},
 title = {Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/3d3d286a8d153a4a58156d0e02d8570c-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{exploration_survey,
  title={Exploration in deep reinforcement learning: A survey},
  volume={85},
  ISSN={1566-2535},
  url={http://dx.doi.org/10.1016/j.inffus.2022.03.003},
  DOI={10.1016/j.inffus.2022.03.003},
  journal={Information Fusion},
  publisher={Elsevier BV},
  author={Ladosz, Pawel and Weng, Lilian and Kim, Minwoo and Oh, Hyondong},
  year={2022},
  month=sep, pages={1–22} 
}

%InProceedings
@misc{VGCRL,
  title = 	 {Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning},
  author =       {Choi, Jongwook and Sharma, Archit and Lee, Honglak and Levine, Sergey and Gu, Shixiang Shane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1953--1963},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/choi21b/choi21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/choi21b.html},
  abstract = 	 {Learning to reach goal states and learning diverse skills through mutual information maximization have been proposed as principled frameworks for unsupervised reinforcement learning, allowing agents to acquire broadly applicable multi-task policies with minimal reward engineering. In this paper, we discuss how these two approaches {—} goal-conditioned RL (GCRL) and MI-based RL {—} can be generalized into a single family of methods, interpreting mutual information maximization and variational empowerment as representation learning methods that acquire function-ally aware state representations for goal reaching.Starting from a simple observation that the standard GCRL is encapsulated by the optimization objective of variational empowerment, we can derive novel variants of GCRL and variational empowerment under a single, unified optimization objective, such as adaptive-variance GCRL and linear-mapping GCRL, and study the characteristics of representation learning each variant provides. Furthermore, through the lens of GCRL, we show that adapting powerful techniques fromGCRL such as goal relabeling into the variationalMI context as well as proper regularization on the variational posterior provides substantial gains in algorithm performance, and propose a novel evaluation metric named latent goal reaching (LGR)as an objective measure for evaluating empowerment algorithms akin to goal-based RL. Through principled mathematical derivations and careful experimental validations, our work lays a novel foundation from which representation learning can be evaluated and analyzed in goal-based RL}
}

%inproceedings
@misc{HER,
 author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hindsight Experience Replay},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{colas2022autotelic,
  title={Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: a short survey},
  author={Colas, C{\'e}dric and Karch, Tristan and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={Journal of Artificial Intelligence Research},
  volume={74},
  pages={1159--1199},
  year={2022}
}

@InProceedings{UVFA,
  title = 	 {Universal Value Function Approximators},
  author = 	 {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1312--1320},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schaul15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schaul15.html},
  abstract = 	 {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}
}

% Safety constraints
@INPROCEEDINGS{viability_wieber,
  author={Wieber, Pierre-Brice},
  booktitle={2008 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={Viability and predictive control for safe locomotion}, 
  year={2008},
  volume={},
  number={},
  pages={1103-1108},
  keywords={Legged locomotion;Robots;Approximation methods;Predictive models;Predictive control;Computational modeling;Equations},
  doi={10.1109/IROS.2008.4651022}
}


%book
@misc{AltmanCMDP,
  added-at = {2007-07-05T16:17:35.000+0200},
  author = {Altman, E.},
  biburl = {https://www.bibsonomy.org/bibtex/2421fb1dafa61f1d028550297084c3cb8/jleny},
  description = {bandit problems},
  interhash = {84fb43430ab46ff2336d7e9926a37b45},
  intrahash = {421fb1dafa61f1d028550297084c3cb8},
  keywords = {imported},
  publisher = {Chapman and Hall},
  timestamp = {2007-07-05T16:17:35.000+0200},
  title = {Constrained Markov Decision Processes},
  year = 1999
}

@misc{learningbasedMPCkoller2019,
  title={Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning}, 
  author={Torsten Koller and Felix Berkenkamp and Matteo Turchetta and Joschka Boedecker and Andreas Krause},
  year={2019},
  eprint={1906.12189},
  archivePrefix={arXiv},
  primaryClass={eess.SY},
  url={https://arxiv.org/abs/1906.12189}, 
}

@article{2015safeRLComprehensive,
  author  = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437--1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}

@misc{2021safeLearningRobotics,
      title={Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning}, 
      author={Lukas Brunke and Melissa Greeff and Adam W. Hall and Zhaocong Yuan and Siqi Zhou and Jacopo Panerati and Angela P. Schoellig},
      year={2021},
      eprint={2108.06266},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

%InProceedings
@misc{RCRL2022,
  title = 	 {Reachability Constrained Reinforcement Learning},
  author =       {Yu, Dongjie and Ma, Haitong and Li, Shengbo and Chen, Jianyu},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25636--25655},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/yu22d/yu22d.pdf},
  url = 	 {https://proceedings.mlr.press/v162/yu22d.html},
  abstract = 	 {Constrained reinforcement learning (CRL) has gained significant interest recently, since safety constraints satisfaction is critical for real-world problems. However, existing CRL methods constraining discounted cumulative costs generally lack rigorous definition and guarantee of safety. In contrast, in the safe control research, safety is defined as persistently satisfying certain state constraints. Such persistent safety is possible only on a subset of the state space, called feasible set, where an optimal largest feasible set exists for a given environment. Recent studies incorporate feasible sets into CRL with energy-based methods such as control barrier function (CBF), safety index (SI), and leverage prior conservative estimations of feasible sets, which harms the performance of the learned policy. To deal with this problem, this paper proposes the reachability CRL (RCRL) method by using reachability analysis to establish the novel self-consistency condition and characterize the feasible sets. The feasible sets are represented by the safety value function, which is used as the constraint in CRL. We use the multi-time scale stochastic approximation theory to prove that the proposed algorithm converges to a local optimum, where the largest feasible set can be guaranteed. Empirical results on different benchmarks validate the learned feasible set, the policy performance, and constraint satisfaction of RCRL, compared to CRL and safe control baselines.}
}

%InProceedings
@misc{ha2020SACLagLevine,
  title = 	 {Learning to Walk in the Real World with Minimal Human Effort},
  author =       {Ha, Sehoon and Xu, Peng and Tan, Zhenyu and Levine, Sergey and Tan, Jie},
  booktitle = 	 {Proceedings of the 2020 Conference on Robot Learning},
  pages = 	 {1110--1120},
  year = 	 {2021},
  editor = 	 {Kober, Jens and Ramos, Fabio and Tomlin, Claire},
  volume = 	 {155},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v155/ha21c/ha21c.pdf},
  url = 	 {https://proceedings.mlr.press/v155/ha21c.html},
  abstract = 	 {Reliable and stable locomotion has been one of the most fundamental challenges for legged robots. Deep reinforcement learning (deep RL) has emerged as a promising method for developing such control policies autonomously. In this paper, we develop a system for learning legged locomotion policies with deep RL in the real world with minimal human effort. The key difficulties for on-robot learning systems are automatic data collection and safety. We overcome these two challenges by developing a multi-task learning procedure and a safety-constrained RL framework. We tested our system on the task of learning to walk on three different terrains: flat ground, a soft mattress, and a doormat with crevices. Our system can automatically and efficiently learn locomotion skills on a Minitaur robot with little human intervention.}
}

@misc{CBF_th_2019,
      title={Control Barrier Functions: Theory and Applications}, 
      author={Aaron D. Ames and Samuel Coogan and Magnus Egerstedt and Gennaro Notomista and Koushil Sreenath and Paulo Tabuada},
      year={2019},
      eprint={1903.11199},
      archivePrefix={arXiv},
      primaryClass={cs.SY}
}

@misc{lyapunovbased_policy_gradient_chow2019,
      title={Lyapunov-based Safe Policy Optimization for Continuous Control}, 
      author={Yinlam Chow and Ofir Nachum and Aleksandra Faust and Edgar Duenez-Guzman and Mohammad Ghavamzadeh},
      year={2019},
      eprint={1901.10031},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{WCSAC2021,
      title={WCSAC: Worst-Case Soft Actor Critic for Safety-Constrained Reinforcement Learning}, 
      volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17272}, DOI={10.1609/aaai.v35i12.17272},
      author={Yang, Qisong and Simão, Thiago D. and Tindemans, Simon H and Spaan, Matthijs T. J.},
      year={2021},
      journal={Proceedings of the AAAI Conference on Artificial Intelligence},
}

@misc{2021safetyCriticExploration,
      title={Conservative Safety Critics for Exploration}, 
      author={Homanga Bharadhwaj and Aviral Kumar and Nicholas Rhinehart and Sergey Levine and Florian Shkurti and Animesh Garg},
      year={2021},
      eprint={2010.14497},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{2020safetyCritic,
      title={Learning to be Safe: Deep RL with a Safety Critic}, 
      author={Krishnan Srinivasan and Benjamin Eysenbach and Sehoon Ha and Jie Tan and Chelsea Finn},
      year={2020},
      eprint={2010.14603},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dalal2018safetyLayer,
      title={Safe Exploration in Continuous Action Spaces}, 
      author={Gal Dalal and Krishnamurthy Dvijotham and Matej Vecerik and Todd Hester and Cosmin Paduraru and Yuval Tassa},
      year={2018},
      eprint={1801.08757},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{safe_control_certificates_dawson2022safe,
  title={Safe control with learned certificates: A survey of neural lyapunov, barrier, and contraction methods},
  author={Dawson, Charles and Gao, Sicun and Fan, Chuchu},
  journal={arXiv preprint arXiv:2202.11762},
  year={2022}
}

@misc{stability_khader2021learning,
      title={Learning Stable Normalizing-Flow Control for Robotic Manipulation}, 
      author={Shahbaz Abdul Khader and Hang Yin and Pietro Falco and Danica Kragic},
      year={2021},
      eprint={2011.00072},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{2022safetyEditorPolicy,
      title={Towards Safe Reinforcement Learning with a Safety Editor Policy}, 
      author={Haonan Yu and Wei Xu and Haichao Zhang},
      year={2022},
      eprint={2201.12427},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{WorstCasePG2019,
      title={Worst Cases Policy Gradients}, 
      author={Yichuan Charlie Tang and Jian Zhang and Ruslan Salakhutdinov},
      year={2019},
      eprint={1911.03618},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Safe exploration
@inproceedings{karimpanal2020learning,
  title={Learning transferable domain priors for safe exploration in reinforcement learning},
  author={Karimpanal, Thommen George and Rana, Santu and Gupta, Sunil and Tran, Truyen and Venkatesh, Svetha},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--10},
  year={2020},
  organization={IEEE}
}

@misc{lipton2018sisyphean,
      title={Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear}, 
      author={Zachary C. Lipton and Kamyar Azizzadenesheli and Abhishek Kumar and Lihong Li and Jianfeng Gao and Li Deng},
      year={2018},
      eprint={1611.01211},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.01211}, 
}

@InProceedings{fatemiDeadEnds,
  title = 	 {Dead-ends and Secure Exploration in Reinforcement Learning},
  author =       {Fatemi, Mehdi and Sharma, Shikhar and Van Seijen, Harm and Kahou, Samira Ebrahimi},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1873--1881},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/fatemi19a/fatemi19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/fatemi19a.html},
  abstract = 	 {Many interesting applications of reinforcement learning (RL) involve MDPs that include numerous “dead-end" states. Upon reaching a dead-end state, the agent continues to interact with the environment in a dead-end trajectory before reaching an undesired terminal state, regardless of whatever actions are chosen. The situation is even worse when existence of many dead-end states is coupled with distant positive rewards from any initial state (we term this as Bridge Effect). Hence, conventional exploration techniques often incur prohibitively many training steps before convergence. To deal with the bridge effect, we propose a condition for exploration, called security. We next establish formal results that translate the security condition into the learning problem of an auxiliary value function. This new value function is used to cap “any" given exploration policy and is guaranteed to make it secure. As a special case, we use this theory and introduce secure random-walk. We next extend our results to the deep RL settings by identifying and addressing two main challenges that arise. Finally, we empirically compare secure random-walk with standard benchmarks in two sets of experiments including the Atari game of Montezuma’s Revenge.}
}


@InProceedings{SafeOpt,
  title = 	 {Safe Exploration for Optimization with Gaussian Processes},
  author = 	 {Sui, Yanan and Gotovos, Alkis and Burdick, Joel and Krause, Andreas},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {997--1005},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sui15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sui15.html},
  abstract = 	 {We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified "safety" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation.}
}

@misc{SafeExpPeters,
  title={Information-Theoretic Safe Exploration with Gaussian Processes}, 
  author={Alessandro G. Bottero and Carlos E. Luis and Julia Vinogradska and Felix Berkenkamp and Jan Peters},
  year={2022},
  eprint={2212.04914},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2212.04914}, 
}

@InProceedings{SafeExpActiveLearningGP,
author="Schreiter, Jens
and Nguyen-Tuong, Duy
and Eberts, Mona
and Bischoff, Bastian
and Markert, Heiner
and Toussaint, Marc",
editor="Bifet, Albert
and May, Michael
and Zadrozny, Bianca
and Gavalda, Ricard
and Pedreschi, Dino
and Bonchi, Francesco
and Cardoso, Jaime
and Spiliopoulou, Myra",
title="Safe Exploration for Active Learning with Gaussian Processes",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="133--149",
isbn="978-3-319-23461-8"
}

@inproceedings{ResetFreeQD, series={GECCO ’22},
  title={Learning to walk autonomously via reset-free quality-diversity},
  url={http://dx.doi.org/10.1145/3512290.3528715},
  DOI={10.1145/3512290.3528715},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  publisher={ACM},
  author={Lim, Bryan and Reichenbach, Alexander and Cully, Antoine},
  year={2022},
  month=jul, pages={86–94},
  collection={GECCO ’22}
}

@article{GarciaSafeExp2012,
  title={Safe Exploration of State and Action Spaces in Reinforcement Learning},
  volume={45},
  ISSN={1076-9757},
  url={http://dx.doi.org/10.1613/jair.3761},
  DOI={10.1613/jair.3761},
  journal={Journal of Artificial Intelligence Research},
  publisher={AI Access Foundation},
  author={Garcia, J. and Fernandez, F.},
  year={2012},
  month=dec, pages={515–564}
}

@misc{VerifSafeExp,
  title={Verifiably Safe Exploration for End-to-End Reinforcement Learning}, 
  author={Nathan Hunt and Nathan Fulton and Sara Magliacane and Nghia Hoang and Subhro Das and Armando Solar-Lezama},
  year={2020},
  eprint={2007.01223},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2007.01223}, 
}

@inproceedings{achiam2017CPO,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@misc{TrialWithoutErrorSafeExp,
  title={Trial without Error: Towards Safe Reinforcement Learning via Human Intervention}, 
  author={William Saunders and Girish Sastry and Andreas Stuhlmueller and Owain Evans},
  year={2017},
  eprint={1707.05173},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/1707.05173}, 
}

@misc{SafeExpGPMDP,
  title={Safe Exploration in Finite Markov Decision Processes with Gaussian Processes}, 
  author={Matteo Turchetta and Felix Berkenkamp and Andreas Krause},
  year={2016},
  eprint={1606.04753},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1606.04753}, 
}

@article{MASESafeExp,
  title={Safe exploration in reinforcement learning: A generalized formulation and algorithms},
  author={Wachi, Akifumi and Hashimoto, Wataru and Shen, Xun and Hashimoto, Kazumune},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
