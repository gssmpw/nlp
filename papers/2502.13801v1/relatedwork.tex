\section{Related Work}
\label{section:related_work}

\paragraph{\textbf{Safe Reinforcement Learning}}
Constrained optimal control methods allow to synthesize controllers with safety guarantees but they assume a perfect or at least partial knowledge of the system dynamics \cite{CBF_th_2019,learningbasedMPCkoller2019,safe_control_certificates_dawson2022safe}. 
On the contrary, model-free safe reinforcement learning (safe RL) only assumes
the system dynamics to be stochastic and markovian. 
Most safe RL methods aim at solving a Constrained Markov Decision Process (CMDP),
which is built upon an analogy between rewards and constraints
\cite{AltmanCMDP,ha2020SACLagLevine,2015safeRLComprehensive,achiam2017CPO,lyapunovbased_policy_gradient_chow2019},
as they formulate the constraint as a discounted cumulative objective, which is convenient 
from the RL point of view but not sufficient to satisfy persistent safety constraints along trajectories.
To overcome this issue, some methods reformulate the critic target. For example, in \cite{2020safetyCritic}, a safety critic allows to formulate the constraint in terms of failure probability \cite{2020safetyCritic}. 
Reachability-constrained RL (RCRL) \cite{RCRL2022} proposes to use a $\max$ operator in the safety critic target to discover 
the largest set of reachable states by the policy.
%Reachability-constrained RL (RCRL) uses the $\max$ operator to obtain the largest feasible set \cite{RCRL2022} {\color{blue} CLARIFIER}. 
In our work, we pre-train a safety policy using ingredients from RCRL and then use the critic in the
safe exploration phase to decide when to switch from one policy to another.
Also, since our goal is to prevent the worst-case scenario,
we use quantile-based distributional reinforcement learning to train our safety policy 
\cite{QR-DQN,WorstCasePG2019,WCSAC2021}.

\paragraph{\textbf{Goal-conditioned RL}}
The standard RL framework addresses a single task, specified by the reward function.
Goal-conditioned reinforcement learning (GCRL) provide 
bases to build a single robotic controller that is able to achieve a wide variety of tasks and
adapt its behavior to its environment \cite{Kaelbling1993LearningTA,UVFA,colas2022autotelic}.
A goal can be a desired position for a given rigid body in the environment. In addition to being convenient 
for a robot user, it allows to learn from failed attempts. Indeed, an agent that has failed to reach
a given desired goal has actually reached another accidentally.
This feature can be exploited by relabelling methods, like Hindsight Experience Replay
(HER), to learn a goal-conditioned policy from sparse rewards. 
In our experiments, we consider a sparse reward setting and combine  
Soft Actor-Critic (SAC) and HER to learn goal-conditioned policies \cite{SAC, HER}. 

\paragraph{\textbf{Learning diverse safe skills}}
\citeauthor{ha2020SACLagLevine} have developed a method to learn safe locomotion in three directions 
for the Minitaur robot. Although policy improvement over time leads to constraint satisfaction, 
reducing the need for humain intervention for resets, the formalism does not explicitly enforce constraint 
satisfaction during exploration \cite{ha2020SACLagLevine}. SASD on the other hand combines 
unsupervised skill discovery and safe RL to learn a richer set of skills, but still does not enforce safety
during the exploration phase \cite{SASD}.

\paragraph{\textbf{Safe exploration}}
\citeauthor{exploration_survey} identify two categories of safe exploration methods: those based on
auxiliary reward and those based on human designer knowledge \cite{exploration_survey}. 
Auxiliary reward methods penalize the agent when it puts itself in danger. 
By definition, these methods incentivize the agent to avoid repeated mistakes and catastrophic behaviors, 
but do not constrain the exploration policy behavior directly. As a result, they reduce the occurrence of 
mistakes compared to baselines \cite{karimpanal2020learning} or even improve learning 
\cite{lipton2018sisyphean, fatemiDeadEnds} but do not prevent them.
In methods based on human designer knowledge, boundaries safe and unsafe states or behaviors 
are defined by the human designer. Most of these methods make strong assumptions about the environment
or the agent behavior, which allows the use of baseline behaviors, 
human intervention, hand-crafted model or heuristics
\cite{GarciaSafeExp2012,VerifSafeExp,TrialWithoutErrorSafeExp,SafeExpGPMDP}. 
A recent approach proposes a Meta-Algorithm for Safe Exploration (MASE) and provides theoretical guarantees on 
optimality and safety during exploration \cite{MASESafeExp}. However, this approach assumes the agent has access to an emergency stop action that can reset the environment when no viable actions are available, an assumption that does not hold in general, and particularly not in our environments. In contrast, our focus is on environments where unviable states exist, and the agent must navigate within these constraints.
Closer to us, Dalal et al. \cite{dalal2018safetyLayer} use a pre-trained linearized constraint model and solve a quadratic program to project actions onto a feasible set during exploration. Their method, however, assumes access to a linearized model and that constraint violations can be avoided in a single step, which is not our case.
Like us, Srinivasan et al. \cite{2020safetyCritic} propose two training phases. But they pre-train and then fine-tune a single policy, while we consider two different policies that are trained separately and, above all, do not share the same input. 
In our approach, the constraint function is used as an auxiliary reward signal for the safety policy, unlike other auxiliary reward methods where it influences the task-solving policy. Additionally, by making general assumptions about the environment's structure, such as the uniformity in space of the dynamics, our method straddles both categories.
Also, contrary to previously cited approaches, we explicitly merge GCRL with safe RL tools to build our safe exploration framework.

%\textcolor{blue}{SAC-N\cite{sac_n_edac} (ATTENTION Finn utilise autre chose)}

%\textcolor{blue}{Safe exploration in parameters space (GP and QD): \cite{SafeOpt,SafeExpPeters,SafeExpActiveLearningGP,ResetFreeQD}}