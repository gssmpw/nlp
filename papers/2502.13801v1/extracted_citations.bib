@article{2015safeRLComprehensive,
  author  = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437--1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}

@misc{2020safetyCritic,
      title={Learning to be Safe: Deep RL with a Safety Critic}, 
      author={Krishnan Srinivasan and Benjamin Eysenbach and Sehoon Ha and Jie Tan and Chelsea Finn},
      year={2020},
      eprint={2010.14603},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{AltmanCMDP,
  added-at = {2007-07-05T16:17:35.000+0200},
  author = {Altman, E.},
  biburl = {https://www.bibsonomy.org/bibtex/2421fb1dafa61f1d028550297084c3cb8/jleny},
  description = {bandit problems},
  interhash = {84fb43430ab46ff2336d7e9926a37b45},
  intrahash = {421fb1dafa61f1d028550297084c3cb8},
  keywords = {imported},
  publisher = {Chapman and Hall},
  timestamp = {2007-07-05T16:17:35.000+0200},
  title = {Constrained Markov Decision Processes},
  year = 1999
}

@misc{CBF_th_2019,
      title={Control Barrier Functions: Theory and Applications}, 
      author={Aaron D. Ames and Samuel Coogan and Magnus Egerstedt and Gennaro Notomista and Koushil Sreenath and Paulo Tabuada},
      year={2019},
      eprint={1903.11199},
      archivePrefix={arXiv},
      primaryClass={cs.SY}
}

@article{GarciaSafeExp2012,
  title={Safe Exploration of State and Action Spaces in Reinforcement Learning},
  volume={45},
  ISSN={1076-9757},
  url={http://dx.doi.org/10.1613/jair.3761},
  DOI={10.1613/jair.3761},
  journal={Journal of Artificial Intelligence Research},
  publisher={AI Access Foundation},
  author={Garcia, J. and Fernandez, F.},
  year={2012},
  month=dec, pages={515–564}
}

@misc{HER,
 author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hindsight Experience Replay},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{Kaelbling1993LearningTA,
  title={Learning to Achieve Goals},
  author={Leslie Pack Kaelbling},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={1993},
  url={https://api.semanticscholar.org/CorpusID:5538688}
}

@article{MASESafeExp,
  title={Safe exploration in reinforcement learning: A generalized formulation and algorithms},
  author={Wachi, Akifumi and Hashimoto, Wataru and Shen, Xun and Hashimoto, Kazumune},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{QR-DQN, title={Distributional Reinforcement Learning With Quantile Regression}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11791}, DOI={10.1609/aaai.v32i1.11791}, abstractNote={ &lt;p&gt; In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Dabney, Will and Rowland, Mark and Bellemare, Marc and Munos, Rémi}, year={2018}, month={Apr.} }

@misc{RCRL2022,
  title = 	 {Reachability Constrained Reinforcement Learning},
  author =       {Yu, Dongjie and Ma, Haitong and Li, Shengbo and Chen, Jianyu},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25636--25655},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/yu22d/yu22d.pdf},
  url = 	 {https://proceedings.mlr.press/v162/yu22d.html},
  abstract = 	 {Constrained reinforcement learning (CRL) has gained significant interest recently, since safety constraints satisfaction is critical for real-world problems. However, existing CRL methods constraining discounted cumulative costs generally lack rigorous definition and guarantee of safety. In contrast, in the safe control research, safety is defined as persistently satisfying certain state constraints. Such persistent safety is possible only on a subset of the state space, called feasible set, where an optimal largest feasible set exists for a given environment. Recent studies incorporate feasible sets into CRL with energy-based methods such as control barrier function (CBF), safety index (SI), and leverage prior conservative estimations of feasible sets, which harms the performance of the learned policy. To deal with this problem, this paper proposes the reachability CRL (RCRL) method by using reachability analysis to establish the novel self-consistency condition and characterize the feasible sets. The feasible sets are represented by the safety value function, which is used as the constraint in CRL. We use the multi-time scale stochastic approximation theory to prove that the proposed algorithm converges to a local optimum, where the largest feasible set can be guaranteed. Empirical results on different benchmarks validate the learned feasible set, the policy performance, and constraint satisfaction of RCRL, compared to CRL and safe control baselines.}
}

@inproceedings{ResetFreeQD, series={GECCO ’22},
  title={Learning to walk autonomously via reset-free quality-diversity},
  url={http://dx.doi.org/10.1145/3512290.3528715},
  DOI={10.1145/3512290.3528715},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  publisher={ACM},
  author={Lim, Bryan and Reichenbach, Alexander and Cully, Antoine},
  year={2022},
  month=jul, pages={86–94},
  collection={GECCO ’22}
}

@misc{SAC,
      title={Soft Actor-Critic Algorithms and Applications}, 
      author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
      year={2019},
      eprint={1812.05905},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{SASD,
  author={Kim, Sunin and Kwon, Jaewoon and Lee, Taeyoon and Park, Younghyo and Perez, Julien},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Safety-Aware Unsupervised Skill Discovery}, 
  year={2023},
  volume={},
  number={},
  pages={894-900},
  keywords={Training;Heuristic algorithms;Reinforcement learning;Programming;Safety;Behavioral sciences;Dynamic programming},
  doi={10.1109/ICRA48891.2023.10160985}
}

@InProceedings{SafeExpActiveLearningGP,
author="Schreiter, Jens
and Nguyen-Tuong, Duy
and Eberts, Mona
and Bischoff, Bastian
and Markert, Heiner
and Toussaint, Marc",
editor="Bifet, Albert
and May, Michael
and Zadrozny, Bianca
and Gavalda, Ricard
and Pedreschi, Dino
and Bonchi, Francesco
and Cardoso, Jaime
and Spiliopoulou, Myra",
title="Safe Exploration for Active Learning with Gaussian Processes",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="133--149",
isbn="978-3-319-23461-8"
}

@misc{SafeExpGPMDP,
  title={Safe Exploration in Finite Markov Decision Processes with Gaussian Processes}, 
  author={Matteo Turchetta and Felix Berkenkamp and Andreas Krause},
  year={2016},
  eprint={1606.04753},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1606.04753}, 
}

@misc{SafeExpPeters,
  title={Information-Theoretic Safe Exploration with Gaussian Processes}, 
  author={Alessandro G. Bottero and Carlos E. Luis and Julia Vinogradska and Felix Berkenkamp and Jan Peters},
  year={2022},
  eprint={2212.04914},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2212.04914}, 
}

@InProceedings{SafeOpt,
  title = 	 {Safe Exploration for Optimization with Gaussian Processes},
  author = 	 {Sui, Yanan and Gotovos, Alkis and Burdick, Joel and Krause, Andreas},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {997--1005},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sui15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sui15.html},
  abstract = 	 {We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified "safety" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation.}
}

@misc{TrialWithoutErrorSafeExp,
  title={Trial without Error: Towards Safe Reinforcement Learning via Human Intervention}, 
  author={William Saunders and Girish Sastry and Andreas Stuhlmueller and Owain Evans},
  year={2017},
  eprint={1707.05173},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/1707.05173}, 
}

@InProceedings{UVFA,
  title = 	 {Universal Value Function Approximators},
  author = 	 {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1312--1320},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schaul15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schaul15.html},
  abstract = 	 {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}
}

@misc{VerifSafeExp,
  title={Verifiably Safe Exploration for End-to-End Reinforcement Learning}, 
  author={Nathan Hunt and Nathan Fulton and Sara Magliacane and Nghia Hoang and Subhro Das and Armando Solar-Lezama},
  year={2020},
  eprint={2007.01223},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2007.01223}, 
}

@misc{WCSAC2021,
      title={WCSAC: Worst-Case Soft Actor Critic for Safety-Constrained Reinforcement Learning}, 
      volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17272}, DOI={10.1609/aaai.v35i12.17272},
      author={Yang, Qisong and Simão, Thiago D. and Tindemans, Simon H and Spaan, Matthijs T. J.},
      year={2021},
      journal={Proceedings of the AAAI Conference on Artificial Intelligence},
}

@misc{WorstCasePG2019,
      title={Worst Cases Policy Gradients}, 
      author={Yichuan Charlie Tang and Jian Zhang and Ruslan Salakhutdinov},
      year={2019},
      eprint={1911.03618},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{achiam2017CPO,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@article{colas2022autotelic,
  title={Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: a short survey},
  author={Colas, C{\'e}dric and Karch, Tristan and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={Journal of Artificial Intelligence Research},
  volume={74},
  pages={1159--1199},
  year={2022}
}

@misc{dalal2018safetyLayer,
      title={Safe Exploration in Continuous Action Spaces}, 
      author={Gal Dalal and Krishnamurthy Dvijotham and Matej Vecerik and Todd Hester and Cosmin Paduraru and Yuval Tassa},
      year={2018},
      eprint={1801.08757},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{exploration_survey,
  title={Exploration in deep reinforcement learning: A survey},
  volume={85},
  ISSN={1566-2535},
  url={http://dx.doi.org/10.1016/j.inffus.2022.03.003},
  DOI={10.1016/j.inffus.2022.03.003},
  journal={Information Fusion},
  publisher={Elsevier BV},
  author={Ladosz, Pawel and Weng, Lilian and Kim, Minwoo and Oh, Hyondong},
  year={2022},
  month=sep, pages={1–22} 
}

@InProceedings{fatemiDeadEnds,
  title = 	 {Dead-ends and Secure Exploration in Reinforcement Learning},
  author =       {Fatemi, Mehdi and Sharma, Shikhar and Van Seijen, Harm and Kahou, Samira Ebrahimi},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1873--1881},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/fatemi19a/fatemi19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/fatemi19a.html},
  abstract = 	 {Many interesting applications of reinforcement learning (RL) involve MDPs that include numerous “dead-end" states. Upon reaching a dead-end state, the agent continues to interact with the environment in a dead-end trajectory before reaching an undesired terminal state, regardless of whatever actions are chosen. The situation is even worse when existence of many dead-end states is coupled with distant positive rewards from any initial state (we term this as Bridge Effect). Hence, conventional exploration techniques often incur prohibitively many training steps before convergence. To deal with the bridge effect, we propose a condition for exploration, called security. We next establish formal results that translate the security condition into the learning problem of an auxiliary value function. This new value function is used to cap “any" given exploration policy and is guaranteed to make it secure. As a special case, we use this theory and introduce secure random-walk. We next extend our results to the deep RL settings by identifying and addressing two main challenges that arise. Finally, we empirically compare secure random-walk with standard benchmarks in two sets of experiments including the Atari game of Montezuma’s Revenge.}
}

@misc{ha2020SACLagLevine,
  title = 	 {Learning to Walk in the Real World with Minimal Human Effort},
  author =       {Ha, Sehoon and Xu, Peng and Tan, Zhenyu and Levine, Sergey and Tan, Jie},
  booktitle = 	 {Proceedings of the 2020 Conference on Robot Learning},
  pages = 	 {1110--1120},
  year = 	 {2021},
  editor = 	 {Kober, Jens and Ramos, Fabio and Tomlin, Claire},
  volume = 	 {155},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v155/ha21c/ha21c.pdf},
  url = 	 {https://proceedings.mlr.press/v155/ha21c.html},
  abstract = 	 {Reliable and stable locomotion has been one of the most fundamental challenges for legged robots. Deep reinforcement learning (deep RL) has emerged as a promising method for developing such control policies autonomously. In this paper, we develop a system for learning legged locomotion policies with deep RL in the real world with minimal human effort. The key difficulties for on-robot learning systems are automatic data collection and safety. We overcome these two challenges by developing a multi-task learning procedure and a safety-constrained RL framework. We tested our system on the task of learning to walk on three different terrains: flat ground, a soft mattress, and a doormat with crevices. Our system can automatically and efficiently learn locomotion skills on a Minitaur robot with little human intervention.}
}

@inproceedings{karimpanal2020learning,
  title={Learning transferable domain priors for safe exploration in reinforcement learning},
  author={Karimpanal, Thommen George and Rana, Santu and Gupta, Sunil and Tran, Truyen and Venkatesh, Svetha},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--10},
  year={2020},
  organization={IEEE}
}

@misc{learningbasedMPCkoller2019,
  title={Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning}, 
  author={Torsten Koller and Felix Berkenkamp and Matteo Turchetta and Joschka Boedecker and Andreas Krause},
  year={2019},
  eprint={1906.12189},
  archivePrefix={arXiv},
  primaryClass={eess.SY},
  url={https://arxiv.org/abs/1906.12189}, 
}

@misc{lipton2018sisyphean,
      title={Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear}, 
      author={Zachary C. Lipton and Kamyar Azizzadenesheli and Abhishek Kumar and Lihong Li and Jianfeng Gao and Li Deng},
      year={2018},
      eprint={1611.01211},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.01211}, 
}

@misc{lyapunovbased_policy_gradient_chow2019,
      title={Lyapunov-based Safe Policy Optimization for Continuous Control}, 
      author={Yinlam Chow and Ofir Nachum and Aleksandra Faust and Edgar Duenez-Guzman and Mohammad Ghavamzadeh},
      year={2019},
      eprint={1901.10031},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{sac_n_edac,
 author = {An, Gaon and Moon, Seungyong and Kim, Jang-Hyun and Song, Hyun Oh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7436--7447},
 publisher = {Curran Associates, Inc.},
 title = {Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/3d3d286a8d153a4a58156d0e02d8570c-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{safe_control_certificates_dawson2022safe,
  title={Safe control with learned certificates: A survey of neural lyapunov, barrier, and contraction methods},
  author={Dawson, Charles and Gao, Sicun and Fan, Chuchu},
  journal={arXiv preprint arXiv:2202.11762},
  year={2022}
}

