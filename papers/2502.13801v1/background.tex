\section{Background}

In this section, we introduce the notations and building blocks needed to develop our safe exploration method: 
RL and safe RL for learning the safety policy, goal-conditioned RL to achieve goals and distributional critics 
for risk-aware action selection during the safe exploration phase.

\subsection{Reinforcement Learning formalism}
\label{subsection:RL}

An RL problem is described by a tuple $(\stateSpace, \actionSpace, p, p_0, r)$, called a Markov decision process (MDP),
where $\stateSpace$ is the state space, $\actionSpace$ the action space,
$p : \stateSpace \times \actionSpace \to \probSpace(\stateSpace)$ the transition function, 
$p_0 \in \probSpace(\stateSpace)$ the initial state probability distribution, 
$r : \stateSpace \times \actionSpace \to \R$ the reward function. 
The objective in RL is to obtain a policy $\pi : \stateSpace \to \probSpace(\actionSpace)$ 
that maximizes the expected sum of discounted rewards $\expect_{\pi} \left[ \sum_{t=0}^{+\infty} \gamma^t r(s_t, a_t) \right]$, where $\gamma \in [0,1]$ is the discount factor \cite{SuttonBartho}.

\subsection{Goal-conditioned RL formalism}
\label{subsection:GCRL}

Goal-conditioned RL (GCRL) extends the RL framework to a multi-goal setting 
$(\stateSpace \times \goalSpace, \actionSpace, p, p_0, \pG, \rGC)$, where $\goalSpace$ is the goal space, 
\textit{eg} positions to reach, $\rGC : \stateSpace \times \actionSpace \times \goalSpace \to \R$ the 
new reward function, and $\pG \in \probSpace(\goalSpace)$ the distribution of goals sampled at the beginning of each episode.
In this setting, we consider a goal-conditioned (GC) policy $\pi : \stateSpace \times \goalSpace \to \probSpace(\actionSpace)$ 
whose objective is to maximize $\expect_{g\sim\pG, \pi(.|.,g)} \left[ \sum_{t=0}^{+\infty} \gamma^t \rGC(s_t, a_t, g) \right]$, 
so that the resulting policy maximizes goal space coverage \cite{UVFA}.
In our framework, we consider a sparse reward setting, where the reward is equal to $1$ when the goal $g$ is reached 
and $0$ otherwise. It avoids the need for complicated reward engineering but requires relabelling to learn from failed attempts.
We use Hindsight Experience Replay (HER) with "future" strategy combined with SAC for
policy learning \cite{HER, SAC}. 

\subsection{Safe Reinforcement Learning}
\label{subsection:SafeRL}

Safe RL aims at solving an RL problem under constraints, summarized in a Constrained MDP (CMDP) 
$(\stateSpace, \actionSpace, p, p_0, \rS, h)$, where $h: \stateSpace \to \R$ is 
the constraint function \cite{AltmanCMDP}. In many safe RL algorithms $h$ plays the 
same role as a reward function and the constraint is based on the expected sum of discounted costs (Cf \refSection{section:related_work}). 
As the sum form allows compensations between terms, it does not guarantee that every state $s$ in an episode verifies $h(s) \le 0$.
Therefore, we rather opted for the RCRL framework and its reachability critic that offer such a guarantee \cite{RCRL2022}. Also, it relates the reachability value to a notion of distance between the current state and the set of unsafe states, which is a crucial aspect of our action selection mechanism. 

\subsection{Distributional Reinforcement Learning}

Safe exploration is inherently related to a notion of risk. To explore the agent has to go through previously unvisited states.
But to do so it must already have some \textit{a priori} knowledge about the safety of the states it is about to visit.
A balancing robot for example may have to evaluate the probability to fall given its current state $s$ and the action $a$ it is
about to execute. 
Distributional RL methods based on quantile regression, like TQC, allow to compute such a probability \cite{QR-DQN,TQC}. Rather than computing the mean of the return distribution, their critic function approximates the entire distribution using a sum of Dirac delta functions $Z_{\psi}(s, a) := \frac{1}{N}\sum_{i=1}^{N} \delta_{\theta_{\psi}^{(i)}(s, a)}$. Each Dirac position $\theta_{\psi}^{(i)}(s, a)$ corresponds to a quantile of the return distribution, and $N$ is the number of quantiles. Parameter $\psi$ is optimized via quantile regression Q-learning \cite{QR-DQN} updates on batches of transitions uniformly sampled from a replay buffer $\buffer$. The mean of all quantiles is the expected return $\mathbb{E} \left[ \left. \sum_{t=0}^{+\infty} \gamma^t \rS(s_t, a_t) \right| s, a \right]$ and we use it for policy learning. 

By definition, each quantile $\theta_{\psi}^{(i)}(s, a)$ is associated to a cumulative probability $\hat{\tau_i} \triangleq \proba\left(Z^{\pi}(s, a) \le \theta_{\psi}^{(i)}(s, a)\right) = \frac{2.i - 1}{2.N}$, with $i\in\{1,...,N\}$ \cite{QR-DQN}. We assume that the sum of rewards must not fall below a given threshold $v$, which is associated to a potential mistake. As the cumulative distribution function is non-decreasing, if $\theta_{\psi}^{(i)}(s, a) \le v$, then $\hat{\tau_i} = \frac{2.i - 1}{2.N} \le \proba\left(Z^{\pi}(s, a) \le v\right)$. As a result, the relative positions of the quantiles with respect to a given threshold allow us to master the risk level during the safe exploration phase of our method. In our implementation, for a given hyperparameter $\tau \in [0, 1]$, we compute the mean of quantiles $\theta_{\psi}^{(i)}(s, a)$ that verify $\hat{\tau_i} \le \tau$, then we compare it to the threshold $v$ to decide whether switching to the safety policy is necessary. For example, $\tau = 0.1$ corresponds to the worst $10 \%$ of possible value outcomes.

%\begin{equation}
%    \theta_{\psi}^{(\lfloor N . \tau \rfloor)}(s, a) \ge \vmin \Rightarrow \proba\left(Z^{\pi}(s, a) \le \vmin\right) \le \tau
%    \label{eq:threshold_min}
%\end{equation}

%\begin{equation}
%    \theta_{\psi}^{(\lceil N . \tau \rceil)}(s, a) \le \vmax \Rightarrow \proba\left(Z^{\pi}(s, a) > \vmax\right) \le 1 - \tau
%    \label{eq:threshold_max}
%\end{equation}

%To this end, we opted for a distributional RL approach to approximate the distribution $Z^{\pi}(s, a) = \Distrib_{\pi} \left[ \left. \sum_{t=0}^{+\infty} \gamma^t \rS(s_t, a_t) \right| s, a \right]$ of the future sum of safety rewards. The tail of the distribution gives to the agent an insight of the worst-case situation, while the mean can be used to optimize a parametrized stochastic policy $\piS : \stateSpace \to \probSpace(\actionSpace)$. The distribution itself is approximated by a quantile distribution $Z_{\psi}$ that maps each state-action pair $(s, a)$ to a uniform probability distribution: 

%\begin{equation}
%    Z_{\psi}(s, a) := \frac{1}{N}\sum_{i=1}^{N} \delta_{\theta_{\psi}^{(i)}(s, a)}
%    \label{eq:quantile_dist}
%\end{equation}
%where $\delta_z$ is the Dirac distribution $z \in \R$, $N$ the number of quantiles, and $\theta_{\psi} = \left(\theta_{\psi}^{(i)}\right)_{i \in   [\![1, N]\!] } : \stateSpace \times \actionSpace \to \R^N$. 

%Parameter $\psi$ is optimized via quantile regression Q-learning \cite{QR-DQN} updates on batches of transitions uniformly sampled from a replay buffer $\buffer$:

%\begin{equation}
%    \mathcal{L}_Z(\psi) = \expect_{\substack{s, a, s' \sim \mathcal{B}\\ a'\sim \piS(.|s')}}
%    \left[ l_{QR}\left( \psi, \overline{\psi}, (s, a, s', a') \right)  \right]
%    \label{eq:Z_loss}
%\end{equation}

%\begin{multline}
%    l_{QR}\left( \psi, \overline{\psi}, (s, a, s', a') \right) = \\
%    \frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N} \rho_{\hat{\tau}_i}^{\kappa} \left(   
%        r(s, a) + \gamma \theta_{\Bar{\psi}}^{(j)}(s', a') - \theta_{\psi}^{(i)}(s, a)
%    \right)
%    \label{eq:QR_loss}
%\end{multline}
%where $\rho_{\hat{\tau}_i}^{\kappa}$ is the quantile Huber loss \cite{QR-DQN} and $\Bar{\psi}$ the  target network parameter that allows to compute the target distribution and follows $\psi$ via exponential moving average \cite{SAC, TQC}.

%By definition, each quantile $\theta_{\psi}^{(i)}(s, a)$ is associated to a cumulative probability $\tau_i \triangleq \proba\left(Z^{\pi}(s, a) \le \theta_{\psi}^{(i)}(s, a)\right) = \frac{i}{N}$ \cite{QR-DQN}. 
%Because the cumulative distribution function is non-decreasing, we can show for given thresholds $\vmin, \vmax \in \R$ and $\tau \in [0, 1]$ the following properties:

%\begin{equation}
%    \theta_{\psi}^{(\lfloor N . \tau \rfloor)}(s, a) \ge \vmin \Rightarrow \proba\left(Z^{\pi}(s, %a) \le \vmin\right) \le \tau
%    \label{eq:threshold_min}
%\end{equation}

%\begin{equation}
%    \theta_{\psi}^{(\lceil N . \tau \rceil)}(s, a) \le \vmax \Rightarrow \proba\left(Z^{\pi}(s, a) > \vmax\right) \le 1 - \tau
%    \label{eq:threshold_max}
%\end{equation}

%These properties enable us to master the risk level during the safe exploration phase of our method. To do so, we simply fix $\tau$ and a threshold, $\vmin$ or $\vmax$, and verify the precondition inequality on the corresponding quantile. In our implementation, we compute the mean over the quantiles verifying the precondition and compare it to a threshold to decide which action should be executed.
%The intuition is the same as the one behind the conditional value-at-risk (CVaR), which is more restrictive in terms of safety
%than a single quantile \cite{rockafellarCVaR,WCSAC2021}.

% Base proof:
% \begin{equation}
%     \theta_{\psi}^{(i)}(s, a) \ge \vmin \Rightarrow \proba\left(Z^{\pi}(s, a) \le \vmin\right) \le \tau_i
%     \label{eq:threshold_min}
% \end{equation}

% \begin{equation}
%     \theta_{\psi}^{(j)}(s, a) \le \vmax \Rightarrow \proba\left(Z^{\pi}(s, a) > \vmax\right) \le 1 - \tau_j
%     \label{eq:threshold_max}
% \end{equation}
