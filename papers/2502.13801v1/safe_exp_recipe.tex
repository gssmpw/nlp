\section{Method}

In this section, we formalize the safe exploration problem and detail the method we propose to solve it.

\subsection{Defining the safe exploration problem}
\label{subsection:safe_exp_problem}

We aim for our agent to learn how to maximize coverage of a goal set, which corresponds to solving a multi-goal MDP (\refSection{subsection:GCRL}). However, the agent must also explore its environment safely, avoiding mistakes. In our framework, a mistake occurs when a terminal state is reached. Therefore, the agent must avoid terminal states during exploration. To do so, we provide our agent with a safety policy that we can activate when the GC policy is about to make a mistake and lead the agent out of danger. 

A way to build this safety policy is under the angle of classic RL. We consider a set $\neighborhood \subset \stateSpace$ of states that are desirable in terms of safety, for instance near some equilibrium point. The safety reward $\rS$ equals $1$ if $s \in \neighborhood$ and 0 otherwise. This reward setting is convenient as it relates the sum of rewards to the number of steps $T^{\piS}(s, a)$ necessary to reach $\neighborhood$ from a given state-action couple $(s, a)$. Then, to decide when to switch from one policy to the other we can compare the estimation given by the safety critic of the number of steps necessary to reach $ \neighborhood$ to a threshold.

However, critics are neural networks, which are continuous functions, while the number of steps is finite for states from which we can reach $\neighborhood$ and infinite for terminal states regarding an optimal safety policy. Thus, it pushes the critic to generalize safety to unsafe states and leads the agent to believe that it can still use the GC policy even though objectively, it is about to make a mistake. Therefore, in addition to the temporal distance, we added a notion of distance in the state space between the current state and the set of terminal states. To do so, we assume the agent receives a cost value $h(s)$ at each environment step, where $s$ is the current state and $h: \stateSpace \to \R$ a continuous constraint function that verifies $h(s) > 0$ if and only if $s$ is a terminal state. In our framework, we identify terminal states as mistakes the agent absolutely has to avoid. Like in RCRL, the function $h$ is related to a kind of distance between the current state $s$ and the set of terminal states \cite{RCRL2022}. Assuming to have access to such a function is not very restrictive as robots have sensors and often state estimation modules. The continuity of function $h$ is crucial as it allows for generalization from known states to unseen states. Indeed, an unvisited state near another visited and safe state which is far enough from terminal states is likely to be safe too. On the contrary, states near unsafe states are likely to be unsafe. As a result, we switch from the GC policy to the safety policy if the number of steps to reach $\neighborhood$ is too high or if $h(s)$ is close to $0$. 

All these considerations lead us to define the safe exploration problem as the combination of a CMDP $(\stateSpace, \actionSpace, p, p_0, \rS, h)$
and a multi-goal MDP $(\stateSpace \times \goalSpace, \actionSpace, p, p_0, \pG, \rGC)$:

\begin{equation}
    \mathcal{T} = (\stateSpace \times \goalSpace, \actionSpace, p, p_0, \pG, \rS, \rGC, h)
    \label{eq:SafeExpProblem}
\end{equation}

In the pretraining phase, we train a parametrized stochastic safety policy $\piS$ that solves the CMDP. 
The CMDP is independent of the goal space as the notion of safety is independent of the goals pursued by the agent.
We assume that we have access to simulation to perform the safety pretraining, allowing us to perform reset anywhere 
and then to train the policy on a wide variety of situations. The critics that have been trained are then used in the 
action selection mechanism.

During the safe exploration phase, the safety policy and its critics are fixed and we train a GC policy. 
The multi-goal MDP part follows the framework developed in \refSection{subsection:GCRL}. 
Transitions are collected and stored in an episodic replay buffer $\buffer$, regardless of the policy that generates them.
Thus transitions generated by both policies can be found in the same episode. The main idea behind is that both policies 
can learn from each other as their respective objectives may be complementary in some situations.

\subsection{Safety policy learning}
\label{subsection:safety_policy_learning}

To train the safety policy, we take inspiration from the distributional algorithm TQC, as it uses an ensemble critics for robustness and truncation in the critic update to prevent overestimation \cite{TQC}.
On the one hand, we train $M$ approximations $Z_{\psi_1}, ... Z_{\psi_M}$ of the safety policy return distribution $Z^{\piS}(s, a) = \Distrib_{\piS} \left[ \left. \sum_{t=0}^{+\infty} \gamma^t \rS(s_t, a_t) \right| s, a \right]$ using the TQC critic loss. The targets $Z_{\overline{\psi_1}}, ... Z_{\overline{\psi_M}}$ are initialized in the same way and follow $Z_{\psi_1}, ... Z_{\psi_M}$ via exponential moving average update. 
Similarly, we train an ensemble of $M$ reachability critics $R_{\xi_1}, ... R_{\xi_M}$ but we replace the usual target based on a sum with the RCRL \cite{RCRL2022} target based on a $\max$ operator, leading to equation \refEq{eq:RCRL_target}. 
Likewise, targets $R_{\overline{\xi_1}}, ... R_{\overline{\xi_M}}$ follow $R_{\xi_1}, ... R_{\xi_M}$ via exponential moving average. Unlike TQC, the reachability critics are updated separately using the quantile regression loss \cite{QR-DQN}.
Critic ensemble are trained on the same batched transitions but initialized with different seeds.

\begin{equation}
    \mathcal{T}\theta^{(j)} = (1-\gamma) h(s') + \gamma \max\left( h(s'), \theta_{\overline{\xi}}^{(j)}(s', a') \right)
    \label{eq:RCRL_target}
\end{equation}

Policy parameters $\phi_S$ are optimized to minimize the following loss, also inspired by TQC \cite{TQC}:

\begin{equation}
    \mathcal{L}_{\pi_S}(\phi_S) = \expect_{\substack{s, a \sim \mathcal{B}}}
    \left[ \alpha \log \piS(a|s) - \overline{Q}(s, a) + \lambda \overline{R}(s, a) \right]
    \label{eq:safety_actor_loss}
\end{equation}
where $\overline{Q}(s, a) = \frac{1}{M N} \sum_{i = 1}^{M} \sum_{j = 1}^{N} \theta_{\psi_i}^{(j)}(s, a)$

$\overline{R}(s, a) = \frac{1}{M N} \sum_{i = 1}^{M} \sum_{j = 1}^{N} \theta_{\xi_i}^{(j)}(s, a)$ and $\lambda \in \R_+$ is
a positive multiplier that we keep fixed. In our experiments, we performed an ablation study to identify the effect of 
reachability critics on safety during exploration. So we tested $\lambda = 0$ and $\lambda = 100$. The value of $100$
has been chosen so that $\overline{Q}(s, a)$ and $\overline{R}(s, a)$ have the same scale. We also tested a varying $\lambda$, 
like \citeauthor{ha2020SACLagLevine}, but it led to too much instability in the safety training \cite{ha2020SACLagLevine}.

\subsection{Action selection mechanism}
\label{subsection:Action selection mechanism}

\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \STATE \textbf{Inputs:} $s, g, \SafetyActivated, \thGCtoS, \thStoGC$ 
    \STATE $\aGC \sim \piGC(.|s, g)$ ; $\aS \sim \piS(.|s)$
    \STATE $\text{lower} \leftarrow (\hat{\sigma}^{\piS}(s, \aGC) > \thStoGC)$
    \STATE $\text{raise} \leftarrow (\hat{\sigma}^{\piS}(s, \aS) > \thGCtoS)$
    \STATE $\SafetyActivated \leftarrow \SafetyActivated \, \text{\textbf{and}} \, \text{lower}$ \hfill // Lower the flag
    \STATE $\SafetyActivated \leftarrow \SafetyActivated \, \text{\textbf{or}} \, \text{raise}$  \hfill // Raise the flag
    \STATE $a \leftarrow \SafetyActivated . \aS + (1 - \SafetyActivated) . \aGC$
    \STATE \textbf{Return} $a$, $\SafetyActivated$
    \end{algorithmic}
    \caption{Action selection for safe exploration}
    \label{alg:Action selection}
\end{algorithm}

The action selection mechanism that must ensure safe exploration has been reproduced in \refAlg{alg:Action selection}.
It takes as input the current state $s$, current desired goal $g$, the current value of a boolean flag, called $\SafetyActivated$, and 
risk thresholds $\thGCtoS$ and $\thStoGC$. The flag indicates which action to choose, and the role of the action selection mechanism is to update this flag. If equal to $1$, action $\aS$ 
sampled by the safety policy is selected, else action $\aGC$ sampled by the GC policy is selected.
The flag is updated according to the level of risk associated  
with state $s$ and possible actions $\aGC$ and $\aS$. If the level of risk $\hat{\sigma}^{\piS}(s, \aS)$ 
associated with state $s$ and action $\aS$ exceeds threshold $\thGCtoS$, the safety flag is set to $1$ (\textit{raised})
so as to avoid a future potentially dangerous situation. On the contrary, if the level of risk $\hat{\sigma}^{\piS}(s, \aGC)$ 
associated with state $s$ and action $\aGC$ goes below threshold $\thStoGC$, the safety flag is set to $0$ (\textit{lowered}).
Note that the risk function $\hat{\sigma}^{\piS}$ is related to the safety policy, as its goal is to evaluate the capacity 
of the safety policy to put the agent out of danger. 
Also, because the safety policy is optimized on its critics, it tends to minimize the risk function undirectly. As a result, 
the thresholds should verify $\thStoGC \le \thGCtoS$. Otherwise, the safety policy would be the only one to act.
Depending on the environment, one could choose either equality or strict inequality between thresholds 
(See section \ref{subsec:ablation_dist}). 

The framework defined in section \refSection{subsection:safe_exp_problem} essentially leads to two 
definitions of the risk function, leading to three different strategies. 
The first is based on the time necessary to reach the set $\neighborhood$ from current state, using the safety policy. 
The second is based on the maximum of constraint function $h$ along future trajectories generated by the safety policy.
The third possibility is to combine both. We will use the labels \textbf{Time}, \textbf{Constraint} and \textbf{Time-constraint}
for the respective three strategies. 

\paragraph{\textbf{Time:}} We choose the set $\neighborhood$ so that, if the safety policy is well trained, 
then $\neighborhood$ is a forward set regarding the safety policy $\piS$. More precisely, we assume that, 
for any sequence of transitions $(s_0, a_0, ..., s_t, ...)$ generated by $\piS$, if 
the starting state $s_0$ lies in $\neighborhood$, then all future states $s_t$ also lie in $\neighborhood$.
Under this assumption the safety reward $\rS(s_t, a_t)$ equals $0$ until $s_t$ lies in $\neighborhood$, resulting in equation 
\refEq{eq:time_and_rewards}:

\begin{equation}
    T^{\piS}(s_0, a_0) = f_{\gamma}\left(\sum_{t = 0}^{+\infty} \gamma_t \rS(s_t, a_t)\right) 
    \label{eq:time_and_rewards}
\end{equation}
where $T^{\piS}(s_0, a_0)$ is the random variable corresponding to the number of steps necessary for $\piS$ to reach 
$\neighborhood$ starting from state-action couple $(s_0, a_0)$, and $f_{\gamma}(x) = \log \left( (1-\gamma)  x\right) / \log \gamma$,
which is a continuous bijection from $]0, 1/(1-\gamma)]$ to $\R^+$. By applying the bijective mapping $f_{\gamma}(x)$ to the atoms $\theta_{\psi_i}^{(j)}(s, a)$ 
of ensemble $Z_{\psi_1}, ... Z_{\psi_M}$, we obtain new atoms that approximate the distribution 
of the random variable $T^{\piS}(s, a)$.
We denote $\hat{T}^{\piS}(s, a ; \tau)$ the mean of the quantiles corresponding to cumulative probabilities 
greater than $\tau$. For example, $\tau = 0.9$ corresponds to the mean of worst $10\%$ of cases. $\tau$
is a hyperparameter of the algorithm. In the Time strategy : $\hat{\sigma}^{\piS}(s, a) = \hat{T}^{\piS}(s, a ; \tau)$.
So the thresholds $\thGCtoS$ and $\thStoGC$ are given in number of environment steps.

\paragraph{\textbf{Constraint:}}
In the same way, we denote $\hat{R}^{\piS}(s, a ; \tau)$ the mean of atoms from reachability critics $R_{\xi_1}, ... R_{\xi_M}$
corresponding to cumulative probabilities greater than $\tau$. 
In the constraint strategy : $\hat{\sigma}^{\piS}(s, a) = \hat{R}^{\piS}(s, a ; \tau)$.
So the thresholds $\thGCtoS$ and $\thStoGC$ are negative real numbers corresponding to safety margins.

\paragraph{\textbf{Time-constraint:}} This strategy combines the two previous ones. Let $\epsilon > 0$ be a positive real number 
representing a safety margin. If $\hat{R}^{\piS}(s, a ; \tau) > -\epsilon$, then: $\hat{\sigma}^{\piS}(s, a) = T_{max}$. 
Where $T_{max}$ is the maximum number of episode steps. 
Otherwise: $\hat{\sigma}^{\piS}(s, a) = \hat{T}^{\piS}(s, a ; \tau)$. 
The thresholds $\thGCtoS$ and $\thStoGC$ are given in number of environment steps.

\subsection{Algorithm}

\paragraph{\textbf{Pretraining phase:}} We use the same training loop as TQC 
\cite{TQC}. The only difference is the gradient step update where, in addition to the TQC update,
parameters $\xi_1, ... \xi_M$ are updated by performing Adam optimizer step on 
loss $\mathcal{L}_{Z}$ with target \refEq{eq:RCRL_target}, and the actor loss is replaced with loss $\mathcal{L}_{\pi_S}$.
\refAlg{alg:safety_update} describes how the update is done.
The safety policy interacts with the environment, generating a
transition that is stored in a replay buffer $\buffer_S$. Then a batch of transtions is sampled uniformly
from the buffer and a gradient step update is performed on all parameters, so that for each stored transition,
one step of gradient is performed. 
Also, we start the training after $5000$ steps for which we sample random actions
to favor exploration and learning at the start.

\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE Sample a batch from the replay buffer $\buffer_S$ uniformly
\STATE Perform TQC critic update on $\psi_1, ... \psi_M$ \cite{TQC}
\STATE Update $\xi_1, ... \xi_M$ using Adam on loss $\mathcal{L}_{Z}$ with target \refEq{eq:RCRL_target}
\STATE Update actor parameters $\phi_S$ using Adam on loss $\mathcal{L}_{\pi_S}$
\STATE Update temperature $\alpha_S$ according to SAC rule \cite{SAC}
\end{algorithmic}
\caption{Safety gradient step update}
\label{alg:safety_update}
\end{algorithm}

\paragraph{\textbf{Safe exploration phase:}} The procedure is summarized in \refAlg{alg:main_algorithm}, 
which is also off-policy. 
We first choose safety thresholds depending on the risk level we want. 
Then the parameters of the previously learned safety policy and its critics are loaded,
while the parameters related to the GC policy are randomly initialized. The episodic replay buffer is 
initially empty. For each environment step during training, the action selected according to 
\refAlg{alg:Action selection} to ensure safety during exploration. Each transition is stored in an 
episodic replay buffer $\buffer$ regardless of the policy that generated it. As a result, in a single stored episode,
there are probably samples generated by both policies. 

Initially, the GC policy performs poorly, as it has not yet been trained.
Thus the first stored episodes contain a large majority of transitions generated by the safety policy, as it had to
make up for the random behavior of the GC policy. Then, as the GC policy improves itself, the proportion of transitions 
it has generated increases in the buffer. The fact that many transitions have not been generated by the GC policy can 
lead off-policy algorithms like SAC to value over-estimation, due to the distributional drift between the dataset 
and the current learned policy \cite{levine2020offline}. This is why we used SAC-N instead of SAC, which is the 
same algorithm as SAC but with $N$ critics instead of $2$ \cite{sac_n_edac}. As the critics are initialized and updated
separately, disagreement between them regarding unvisited states is likely to lead the Bellman target to low values via the 
$\min$ operator, thus preventing over-estimation. 

\begin{algorithm}[H]
\begin{algorithmic}[1]
%\STATE \textbf{Initialize} Safety and GC parameters ;  %$\psi_1... \psi_M$, $\xi_1... \xi_M$, $\phi_S$, $\alpha_S$
%\STATE \textbf{Initialize} GC parameters %$\psi_{GC}... \psi_M$, $\xi_1... \xi_M$, $\phi_S$, $\alpha_S$
\STATE \textbf{Inputs:} $\thGCtoS, \thStoGC$ 
\STATE \textbf{Load} Safety parameters $\psi_i, \overline{\psi}_i$, $\xi_i, \overline{\xi}_i$, $i\in[\![1, M]\!]$, $\phi_S$, $\alpha_S$
\STATE \textbf{Initialize} GC parameters $\psi_{GC}, \overline{\psi}_{GC}$, $\phi_{GC}$, $\alpha_{GC}$
\STATE \textbf{Initialize} Episodic replay buffer $\buffer \leftarrow \emptyset$
\STATE \textbf{Sample} initial state $s_0 \sim p_0$, and goal $g \sim \pG$ 
\FOR{each iteration}
\FOR{each environment step, until done}
\STATE $a_t, \SafetyActivated \leftarrow \text{select\_action}(s, g, \SafetyActivated,$ \\$ \thGCtoS, \thStoGC)$
(\refAlg{alg:Action selection})
\STATE Collect transition $(s_t, a_t, s_{t+1}, r_{S, t}, r_{GC, t}, h_{t+1})$
\STATE $\buffer \leftarrow \buffer \cup \{ (s_t, a_t, s_{t+1}, r_{S, t}, r_{GC, t}, h_{t+1}) \}$
\ENDFOR 
\FOR{each gradient step}

\STATE Sample a batch $b_{GC} = (s_t, a_t, s_{t+1}, r_{GC, t})$ from $\buffer$ \\ using HER \cite{HER}
\STATE Update GC parameters on $b_{GC}$ using SAC-N rule \cite{SACN_edac}
% \IF{safety finetuning activated}
% \STATE Update safety policy using \refAlg{alg:safety_update}
% \ENDIF
\ENDFOR
\ENDFOR
\STATE \textbf{return} GC parameters $\psi_{GC}, \overline{\psi}_{GC}$, $\phi_{GC}$, $\alpha_{GC}$ %\\
%Safety parameters $\psi_i, \overline{\psi}_i$, $\xi_i, \overline{\xi}_i$, $i\in[\![1, M]\!]$, $\phi_S$, $\alpha_S$
%Replay buffer $\buffer$
\end{algorithmic}
\caption{Safe exploration algorithm}
\label{alg:main_algorithm}
\end{algorithm}
