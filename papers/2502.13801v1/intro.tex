\section{Introduction}

Goal-conditioned reinforcement learning (GCRL) provides 
bases to build a single robotic controller that can achieve a wide variety of tasks and
adapt its behavior to its environment \cite{colas2022autotelic, VGCRL, UVFA, HER}.
GCRL methods have shown impressive performance on a wide variety of tasks, in simulation but also in the real world, 
essentially with manipulator robots \cite{RIS,skewfit,PlanningWithGCpolicies}.
In the case of manipulator robots, the action is often the desired position of the end effector. Then the agent strongly depends on primitives, 
facilitating exploration and avoiding safety problems.  

However, moving towards fully autonomous agents, we would like to build robots that can explore their environment 
and discover new skills while staying out of danger. An example of such an ideal case would be 
a humanoid robot that does not fall while learning to move around in a new environment. Though it may seem contradictory at first glance, 
humans constantly explore new environments with the confidence that their balance and walking skills will enable them to do it safely.
Hikers, for instance, can navigate in unknown terrains with minor risks as 
their knowledge on how to avoid falling is general and applicable to diverse environments. 
Transposing these intuitions into formal vocabulary from viability theory \cite{viability_wieber}, hikers know how to remain in viable states, even when they explore.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.45\textwidth]{images/schema_general_safe_exp.pdf}
    \caption{Action selection mechanism to guarantee safe exploration. The agent 
    observes the current state $s$ of the environment and the current goal $g$.
    The safety policy samples an action $a_S \sim \piS(.|s)$ that must prevent future mistakes, while the 
    goal-conditioned policy samples an action $a_{GC} \sim \piGC(.|s,g)$ to go towards $g$. 
    For each possibility, the function $\sigma^{\piS}$ estimates the level of confidence in the safety policy's ability to avoid potential future errors. If it is too low, the safe action $a_S$ is executed to keep the system safe. Otherwise, $a_{GC}$ is executed, allowing the agent to explore. 
    }
    
    \Description{Action selection mechanism to guarantee safe exploration. The agent 
    observes the current state $s$ of the environment and the current goal $g$.
    The safety policy samples an action $a_S \sim \piS(.|s)$ that must prevent future mistakes, while the 
    goal-conditioned policy samples an action $a_{GC} \sim \piGC(.|s,g)$ to go towards $g$. 
    For each possibility, the function $\sigma^{\piS}$ estimates the level of confidence in the safety policy's ability to avoid potential future errors. If it is too low, the safe action $a_S$ is executed to keep the system safe. Otherwise, $a_{GC}$ is executed, allowing the agent to explore. }
    \label{fig:schema_switch}
\end{figure}

We propose a safe exploration method that combines a goal-conditioned (GC) policy aiming to explore, discover and 
learn to cover a goal space of positions, and a safety policy aiming at maintaining the system in viable states. As the system dynamics are usually invariant in space and independent from the agent's goals, the safety policy
is defined independently from goals with the hope that it will be globally reliable. The method involves two distinct phases: a first phase of \textit{pretraining}
and a second phase of \textit{safe exploration}. In the first phase, the safety policy is trained in simulation 
using safe reinforcement learning (safe RL) and distributional critics. In the second phase, the GC policy is
trained. To avoid mistakes during exploration, an action-selection mechanism estimates the risk for future states to be 
unviable thanks to the distributional critics of the safety policy, then chooses which policy to execute at the current step. 
Obtaining theoretical results would depend on strong assumptions about the environment dynamics and near-perfect knowledge from the safety policy. Instead, our work focuses on empirically demonstrating that, in practice, an agent can learn to explore safely without making mistakes.
Contrary to other approaches assuming
to have access to instantaneous emergency actions \cite{MASESafeExp,dalal2018safetyLayer}, 
we test our approach on two environments for which there exist states that 
irreversibly lead to mistakes or failures: a goal-based version of CartPole and an environment based on the Skydio X2 drone from Mujoco Menagerie
\cite{towers2024gymnasium,menagerie2022github}. In these environments, random actions or even zero actions lead to mistakes, which makes the conservation of safety quite challenging. To our knowledge, our approach is the only one that explicitly gathers safe RL and GCRL to perform safe exploration.

Our contributions are threefold:
\begin{itemize}
    \item The design of a distributional safe RL framework to pretrain a safety policy that will prevent mistakes during the exploration phase.
    \item The design of an action selection mechanism to ensure safe exploration.
    \item The study of the key components of the method and failure modes to orient future research.
\end{itemize} 


% Goal-conditioned reinforcement learning (GCRL), and more generally variational empowerment, provide 
% bases to build a single robotic controller that is able to achieve a wide variety of tasks and
% adapt its behavior to its environment \cite{colas2022autotelic, VGCRL, UVFA, HER}. These methods 
% have shown impressive performance on a wide variety of tasks (\textcolor{red}{cite}), mostly in simulation 
% and without taking into account physical constraints that are inherrent to robotics.
% Indeed, a simulated robot that learns how to walk can fall hundreds of times without breaking its components
% while a real robot obviously cannot. In the same way, when people begin learning to roller skate, 
% they don't simply attempt random actions, regardless of the risk of injury. 
% Instead, they first focus on maintaining their balance. Once they have that skill, 
% they practice movements that are likely to propel them forward while minimizing the risk of falling.
% This is the intuition behind the method we propose to learn a goal-conditioned policy while avoiding
% dangerous states during exploration. We first train a safety in simulation and use the learned critics
% to compute a risk measure and prevent the exploratory policy to jump into terminal states. 

% Include Safe Exploration in Markov Decision Processes in biblio

% It is a very difficult problem that covers many different aspects. Safe exploration can be well defined in 
% finite MDPs (\textcolor{red}{cite works on finite MDPs and explain why}). To our knowledge, 
% \textcolor{red}{cite Dalal et al} is the only deep reinforcement learning approach that tackles this
% problem and it does so by learning a forward model of the constraint function that is then used to 
% project the action sampled by the policy onto a feasible set. Another approach is based on shooting 
% (\textcolor{red}{cite Safe exp Peters}).

% \textcolor{blue}{On the other hand, safe RL and distributional methods provide a framework to 
% quantify risk}

% \textcolor{blue}{Previous works have already tried to tackle this problem (Dalal)} 
% \textcolor{red}{About safe exploration and then safe RL}
