%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
%\documentclass[sigconf]{aamas} 

%%%% For camera-ready, use this
\documentclass[sigconf,nonacm]{aamas} 
\usepackage[utf8]{inputenc}

%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page

% BEGIN custom packages
%\usepackage{amssymb,amsfonts} %amsmath
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{wrapfig}

\usepackage[
%ragged
raggedright
%raggedleft
]{sidecap}   
\sidecaptionvpos{figure}{t} 
% END custom packages


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\makeatletter
\gdef\@copyrightpermission{
  \begin{minipage}{0.2\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{by}}
  \end{minipage}\hfill
  \begin{minipage}{0.8\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
  \end{minipage}
  \vspace{5pt}
}
%\makeatother

%\setcopyright{ifaamas}
%\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}{Detroit, Michigan, USA}{Y.~Vorobeychik, S.~Das, A.~Nowé  (eds.)}
\acmConference[]{}{}{}{}
%\copyrightyear{2025}
\copyrightyear{}
%\acmYear{2025}
\acmYear{}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Colors
\definecolor{valid}{rgb}{0., 0.8, 0.}

% Commands to ease readction
% Notations
\newcommand{\institutenfrancais}{\institution{Sorbonne Université, CNRS\\ Institut des Systèmes Intelligents et de Robotique, ISIR}}
\newcommand{\ifonlyf}{\textit{iff}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Distrib}{\mathcal{D}}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\buffer}{\mathcal{B}}
\newcommand{\probSpace}{\mathcal{P}}
\newcommand{\actionSpace}{\mathcal{A}}
\newcommand{\stateSpace}{\mathcal{S}}
\newcommand{\goalSpace}{\mathcal{G}}
\newcommand{\rGC}{r_{\mathcal{G}}}
\newcommand{\pG}{p_{\mathcal{G}}}
\newcommand{\rS}{r_S}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\aS}{a_S}
\newcommand{\aGC}{a_{GC}}
\newcommand{\piS}{\pi_{\phi_S}}
\newcommand{\piGC}{\pi_{\phi_{GC}}}
\newcommand{\thGCtoS}{\text{th}_{GC \rightarrow S}}
\newcommand{\thStoGC}{\text{th}_{S \rightarrow GC}}
\newcommand{\SafetyActivated}{\text{safety}\_\text{flag}}
\newcommand{\neighborhood}{N_0}
\newcommand{\vmin}{v_{min}}
\newcommand{\vmax}{v_{max}}

% Refs
\newcommand{\refAlg}[1]{Algorithm \ref{#1}}
\newcommand{\refFig}[1]{Figure \ref{#1}}
\newcommand{\refEq}[1]{(\ref{#1})}
\newcommand{\refSection}[1]{section \ref{#1}}
% END commands


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{1042}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 Formatting Instructions]{Learning to explore when mistakes are not allowed}

%%% Provide names, affiliations, and email addresses for all authors.



\author{Charly Pecqueux-Guézénec}
\affiliation{
  \institution{Sorbonne Université, CNRS, ISIR}%\institutenfrancais
  \city{F-75005 Paris}
  \country{France}}
\email{pecqueuxguezenec@isir.upmc.fr} % TODO: google or lab ? 
% TODO: ORCID

\author{Stéphane Doncieux}
\affiliation{
  \institution{Sorbonne Université, CNRS, ISIR}%\institutenfrancais
  \city{F-75005 Paris}
  \country{France}}
\email{doncieux@isir.upmc.fr}
% TODO: ORCID

\author{Nicolas Perrin-Gilbert}
\affiliation{
  \institution{Sorbonne Université, CNRS, ISIR}%\institutenfrancais
  \city{F-75005 Paris}
  \country{France}}
\email{perrin@isir.upmc.fr}
% TODO: ORCID

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
Goal-Conditioned Reinforcement Learning (GCRL) provides a versatile framework for developing 
unified controllers capable of handling wide ranges of tasks, exploring environments, 
and adapting behaviors. However, its reliance on trial-and-error poses challenges for real-world 
applications, as errors can result in costly and potentially damaging consequences. To address the 
need for safer learning, we propose a method that enables agents to learn goal-conditioned behaviors 
that explore without the risk of making harmful mistakes. Exploration without risks can seem paradoxical, 
but environment dynamics are often uniform in space, therefore a policy trained for safety without 
exploration purposes can still be exploited globally. Our proposed approach involves two distinct 
phases. First, during a pretraining phase, we employ safe reinforcement learning and distributional
techniques to train a safety policy that actively tries to avoid failures in various situations. 
In the subsequent safe exploration phase, a goal-conditioned (GC) policy is learned while ensuring safety. 
To achieve this, we implement an action-selection mechanism leveraging the previously learned 
distributional safety critics to arbitrate between the safety policy and the GC policy, ensuring 
safe exploration by switching to the safety policy when needed.
We evaluate our method in simulated environments and demonstrate that it not only provides 
substantial coverage of the goal space but also reduces the occurrence of mistakes to a minimum, 
in stark contrast to traditional GCRL approaches. Additionally, we conduct an ablation study and 
analyze failure modes, offering insights for future research directions.

% Goal-Conditioned Reinforcement Learning (GCRL) offers a powerful framework to develop
% a single robotic controller that can achieve a wide variety of tasks, explore its environment,
% and eventually, adapt its behavior.
% Yet its trial-and-error approach often prevents real-world applications as mistakes, intended as action sequences leading to catastrophic consequences,  
% can lead to significant damage and high costs.
% Moving towards agents that learn and explore in the real world, we propose a method that 
% allows robotic agents to learn goal-oriented behaviors when mistakes are not allowed. 
% Our approach involves two phases. In the first phase, called pretraining, 
% we use safe reinforcement learning and distributional tools to train a safety policy
% in simulation that minimizes the risk of mistakes. In the second phase, called safe exploration, 
% a goal-conditioned (GC) policy is safely learned. To prevent mistakes, 
% we build an action selection mechanism, based on the previously learned distributional safety 
% critics, that arbitrates between the safety policy and the GC policy and chooses
% the action to execute at each step to maintain safety during exploration.
% We test our approach on simulated robotic environments and show that it not only achieves 
% satisfying goal space coverage but also results in only a few mistakes during exploration. In contrast, GCRL alone
% leads to hundreds of mistakes at least. We also perform an ablation study and investigate failure modes to 
% guide future research.

% Reinforcement learning (RL) offers a powerful framework to develop adaptable robotic behaviors.
% Yet its trial-and-error approach often prevent real-world applications due to safety concerns. 
% For example, a robot learning to walk may frequently fall during exploration before it performs a stable gait. 
% While safe reinforcement learning aims to design controllers that satisfy safety constraints after the training, 
% it does not prevent safety violations during the learning process.  
% Moving towards agents that learn and explore in the real world, we propose a method that enables robotic agents to 
% learn goal-oriented behaviors while avoiding dangerous states during exploration.
% Our approach involves two phases. The first is the pre-training in simulation of a policy optimizing a safety objective.
% The second is the safe exploration phase, where a goal-conditioned (GC) policy is safely learned. 
% In this phase, an action selection mechanism uses previously learned safety critics to choose between 
% actions proposed by the GC policy or the safety policy, thereby maintaining safety while exploring the environment.
% \textcolor{blue}{results and shows}

% Reinforcement learning (RL) provides a framework to design adaptable and hard-to-engineer behaviors for robots. 
% But to do so RL agents proceed by trial and error, which reduces the scope for real-world applications. 
% For example a robot that learns to walk often falls during the exploration phase before finding a stable gait. 
% Safe reinforcement learning aims to learn controllers that satisfy safety constraints after the training, 
% but do not prevent constraint violations during the learning process, which is why RL agents learning in the real world 
% are rare. 
% Moving towards agents that learn and explore in the real world, we propose a method that enables a robotic agent to learn 
% how to achieve goals without falling into dangerous states during exploration. In a first phase we pre-train a safety 
% policy in simulation. In a second phase, called \textit{safe exploration}, a goal-conditioned policy explores the 
% environment and learns to reach goals. To ensure safety while exploring it hands over to the safety policy when it 
% is about to violate the safety constraint. \textcolor{red}{Affiner et compléter}
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Safe Exploration, Safe Reinforcement Learning, Goal-Conditioned Reinforcement Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{intro}

\input{related_work}

\input{background}

\input{safe_exp_recipe}

\input{experiments}

\input{conclusion}

% \begin{acks}
% If you wish to include any acknowledgments in your paper (e.g., to 
% people or funding agencies), please do so using the `\texttt{acks}' 
% environment. Note that the text of your acknowledgments will be omitted
% if you compile your document with the `\texttt{anonymous}' option.
% \end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{biblio}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\input{supmat}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

