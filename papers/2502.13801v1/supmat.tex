\appendix

%\let\balance\relax
%\begin{multicols}{2}

\onecolumn

\section{Supplementary material}

\subsection{Videos}

\begin{itemize}
  \item \url{failure\_cartpole.mp4}: CartPoleGC failure mode described in the paper ($(\thGCtoS, \thStoGC) = (70, 30)$).
  \item \url{near\_bound\_cartpole.mp4}: CartPoleGC with a "difficult" goal near the environment bounds, thus near unsafe states ($(\thGCtoS, \thStoGC) = (70, 30)$).
  \item \url{simple\_cartpole.mp4}: CartPoleGC with a simple goal with $(\thGCtoS, \thStoGC) = (70, 30)$.
  \item \url{30\_30\_cartpole.mp4}: CartPoleGC with a simple goal with $(\thGCtoS, \thStoGC) = (30, 30)$ to show the impact of low thresholds on coverage.
  \item \url{skydiox2\_example.mp4}: Example of SkydioX2 reaching a goal ($(\thGCtoS, \thStoGC) = (10, 10)$)
\end{itemize}

Note that for CartPoleGC, the cart is blue when the safety policy is activated and green otherwise.
The goal is represented by a red box. 

\subsection{Further analysis of failure modes}

\begin{figure}[ht]
  \includegraphics[width=0.32\textwidth]{appendix_image/921_safety_value_estimation.pdf}
  \includegraphics[width=0.32\textwidth]{appendix_image/921_scritic_critic_disagreement.pdf}
  \includegraphics[width=0.32\textwidth]{appendix_image/921_scritic_reach_disagreement.pdf}
  \caption{From left to right: Value of the risk function $\hat{\sigma}^{\piS}(s, a)$ along the failed episode
  shown as an example in Figure 10, where the thresholds are represented in magenta and purple ; Critic disagreement (Figure 10) ; Reachability critic disagreement.
  Blue dots correspond to the safety policy and green dots to the GC policy. One can see the hysteretic behavior on the left plot. A video of the failure (\url{failure\_cartpole.mp4}) is also attached.}
  \label{fig:failures}
  \Description{Further analysis of failure modes}
\end{figure}

We can see on the left plot (Figure \ref{fig:failures}) that the agent switches from the GC policy to the safety policy before making a 
mistake. 
We also observe that the oscillations of the disagreement, for both ensemble of critics,
are synchronized with the change of policies. Indeed, as the GC policy has a different objective
than the safety policy, it goes towards states that have been less visited by the safety policy during 
its pretraining. This phenomenon motivates safety finetuning for future works. 

  

\newpage

\subsection{Goal-space coverage performance with safe exploration on CartPoleGC}

\begin{figure}[ht]
  \includegraphics[width=0.5\textwidth]{appendix_image/rbest1614_450000_coverage.png}
  \caption{Coverage map obtained with $L\&S$ safe exploration variant and $(\thGCtoS, \thStoGC) = (70, 30)$
  on CartPoleGC. Only for this experience, the cartpole is reset on different $x$ positions. 
  Each cell corresponds to the combination of a starting position and a desired goal and we measure the 
  success rate. We can see that the success rate is lower for starting positions and goals near the 
  environment bounds, than for positions around the center. 
  The safety policy tends to prevent the agent from reaching goals near the bounds.
  In the same way, if the initial state is too close to the bound, the safety policy prevents the 
  GC policy from acting most of the time.}
  \label{fig:cartpole_cov}
  \Description{CartPoleGC coverage}
\end{figure}

% \newpage

\subsection{Constraint strategy}

\begin{figure}[ht]
\includegraphics[width=0.3\textwidth]{appendix_image/constraint_only.pdf}
\caption{Occurrence of mistakes obtained with the time-constraint strategy and the constraint (only) strategy 
on the CartPoleGC environment. Performance of constraint strategy in terms of safety is catastrophic.}
\label{fig:constraint_only}
\Description{Occurrence of mistakes obtained with the time-constraint strategy and the constraint (only) strategy 
on the CartPoleGC environment.}
\end{figure}

\subsection{Agent hyperparameters}

\begin{table}[th]
	\caption{Safety pretraining: TQC's hyperparameters}
	\label{tab:safety_pretraining_TQC}
	\begin{tabular}{rll}
    \toprule
		\textit{Name} & \textit{Value} \\ \midrule
		Actor learning rate & $3\times 10^{-4}$  \\
		Critic learning rate & $3\times 10^{-4}$ \\
		Temperature learning rate & $3\times 10^{-4}$ \\
    Initial temperature & $1.0$ \\
    $\tau$ & $5\times 10^{-3}$ \\
		No entropy backup & -  \\
		Discount factor & 0.99  \\ 
    Hidden layers & (256, 256) \\
    Number of critics & 5 \\
    Number of atoms per critic & 25 \\
    Number of quantiles to drop & 2 for CartPoleGC ; 0 for SkydioX2GC\\
    \bottomrule
	\end{tabular}
\end{table}

\begin{table}[th]
	\caption{Safety pretraining: Reachability critics' hyperparameters}
	\label{tab:safety_pretraining_RCRL}
	\begin{tabular}{rll}
    \toprule
		\textit{Name} & \textit{Value} \\ \midrule
		Critic learning rate & $3\times 10^{-4}$ \\
    $\tau$ & $5\times 10^{-3}$ \\
		Discount factor & 0.99  \\ 
    Hidden layers & (256, 256) \\
    Number of critics & 5 \\
    Number of atoms per critic & 25 \\
    \bottomrule
	\end{tabular}
\end{table}

\begin{table}[th]
	\caption{SAC and SAC-N hyperparameters for safe exploration}
	\label{tab:sac_sac_n}
	\begin{tabular}{rll}
    \toprule
		\textit{Name} & \textit{Value} \\ \midrule
		Actor learning rate & $3\times 10^{-4}$  \\
		Critic learning rate & $3\times 10^{-4}$ \\
		Temperature learning rate & $3\times 10^{-4}$ \\
    Initial temperature & $1.0$ \\
    $\tau$ & $5\times 10^{-3}$ \\
		No entropy backup & -  \\
		Discount factor & 0.99  \\ 
    Hidden layers & (256, 256) \\
    Number of critics (Specific to SAC-N) & 50 for CartPoleGC ; 10 for SkydioX2GC \\
    \bottomrule
	\end{tabular}
\end{table}

As for the buffer we choose to keep all transitions. There is no forgetting. 
Thus, the buffer size is always larger than the number of training steps.

%\end{multicols}