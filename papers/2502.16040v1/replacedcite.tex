\section{Related Work}
% \textbf{LLMs for Rec.} LLMs can be leveraged to enhance recommendation systems by providing contextual understanding and powerful reasoning. Recent studies utilizing LLMs for augmentation can be divided into two paths: one is using LLMs for text embedding. The arrtibutes of users, items can be combined and calculate the embedding____. Another line is to utilize LLMs to generate additional information. For example, KAR____ requires LLMs to generate item descriptions and user reasoning preferences. LLM-ESR____ also summarizes user preferences. RLMRec____ describes user and item, also reason more collaborative information. LLM-CF____ generate a CoT of user decision. However, previous works rely on fast thinking, and it often leads to incomplete coverage and insufficient concreteness____.

\textbf{LLMs for Rec.} LLMs can enhance recommendation systems by providing contextual understanding and reasoning abilities____. Recent work on LLM-based augmentation in recommendation generally follows two main directions. The first approach leverages LLMs for text embedding, integrating user and item attributes into unified representations____. The second approach focuses on generating additional information for recommendations. For instance, KAR____ instructs LLMs to produce item descriptions and user preference rationales, while LLM-ESR____ summarizes user preferences. RLMRec____ describes both user and item attributes, incorporating collaborative information, and LLM-CF____ uses a chain-of-thought to represent user decisions. However, most of these methods rely on relatively fast inference, frequently resulting in incomplete coverage and insufficient specificity____.

% \textbf{Inference Computation Scaling.} Inference scaling is to allocate more computation resources into inference stages____, so that LLMs can reason based on more previous steps, shifting from System-1 fast thinking to System-2 slow thinking model____. ____ first investigates the relationship between performance and inference time. Recent works____ have shown promising results in math and coding problems, but none of them investigate whether this inference scaling techniques can indeed perform well in personalized tasks such as recommendations.
\textbf{Inference Computation Scaling.} Inference scaling involves allocating more computational resources to the inference stage, allowing LLMs to reason based on additional prior steps. This shift from System-1 (fast thinking) to System-2 (slow thinking) reasoning enhances their ability to process complex tasks____. The relationship between performance and inference time was first examined by____, and recent studies____ have demonstrated promising results in math and coding tasks. Moreover,____ investigates inference scaling for Retrieval Augmented Generation, and____ employs test-time scaling to address challenges in complex multi-step reasoning.
However, none of these works have investigated whether inference scaling can help address the incomplete coverage problem in recommendation tasks, and we aim to explore how this technique can improve personalized recommendation systems through feature augmentation.