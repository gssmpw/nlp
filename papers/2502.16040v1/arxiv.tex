% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{epstopdf}
\usepackage{subfigure}
% \usepackage{subcaption}
\usepackage{amsthm}
% \usepackage{enumerate}
\usepackage{enumitem}
\usepackage[normalem]{ulem}
% \usepackage[ruled,linesnumbered]{algorithm2e}  
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage[most]{tcolorbox}
\usepackage{xcolor} % 引入xcolor包
\usepackage{colortbl}
\usepackage{bbm}
\usepackage{bm}
\useunder{\uline}{\ul}{}
\graphicspath{{./fig/}}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\usepackage{balance}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{tabularray}


\usepackage{array}          % 基础表格支持
\usepackage{booktabs}       % 优美的表格线格式
\usepackage{arydshln}       % 支持虚线
\usepackage{longtable}      % 长表格
\usepackage{multirow}       % 支持表格的单元格合并
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Inference Computation Scaling for Feature Augmentation in Recommendation Systems}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Weihao Liu\textsuperscript{1}, 
  Zhaocheng Du\textsuperscript{2},
  Haiyuan Zhao\textsuperscript{1}, 
  Wenbo Zhang\textsuperscript{1}, 
  Xiaoyan Zhao\textsuperscript{3}, \\
  \textbf{Gang Wang\textsuperscript{2},
  Zhenhua Dong\textsuperscript{2},
  Jun Xu\textsuperscript{1}\thanks{Corresponding author.} }\\
  \textsuperscript{1}Renmin University of China \\
  \textsuperscript{2}Huawei Noah's Ark Lab\\
  \textsuperscript{3}The Chinese University of Hong Kong \\
  \texttt{\{weihaoliu, haiyuanzhao, junxu\}@ruc.edu.cn, xzhao@se.cuhk.edu.hk} \\
    \texttt{\{zhaochengdu, wanggang110, dongzhenhua\}@huawei.com} \\
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}



Large language models have become a powerful method for feature augmentation in recommendation systems. However, existing approaches relying on quick inference often suffer from \emph{incomplete feature coverage} and \emph{insufficient specificity} in feature descriptions, limiting their ability to capture fine-grained user preferences and undermining overall performance. 
Motivated by the recent success of \emph{inference scaling} in math and coding tasks, we explore whether scaling inference can address these limitations and enhance feature quality.



Our experiments show that scaling inference leads to significant improvements in recommendation performance, with a 12\% increase in NDCG@10. The gains can be attributed to two key factors: feature quantity and specificity. In particular, models using extended Chain-of-Thought (CoT) reasoning generate a greater number of detailed and precise features, offering deeper insights into user preferences and overcoming the limitations of quick inference. We further investigate the factors influencing feature quantity, revealing that model choice and search strategy play critical roles in generating a richer and more diverse feature set. This is the first work to apply inference scaling to feature augmentation in recommendation systems, bridging advances in reasoning tasks to enhance personalized recommendation.

\end{abstract}

\section{Introduction}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/performance_wrt_feature_count.pdf}
  \caption{The positive correlation between recommendation performance and the number of unique valid features generated by different LLMs. The red dotted line represents the best-fit line of the data.}
  % Recommendation performance of features generated by different LLMs is positively correlated with their effective unique counts. The red dotted line is the straight line they fit. 
  \label{fig:performance_wrt_count}
\end{figure}



Large language models (LLMs) have recently become a prevalent and effective approach to feature augmentation in recommendation systems~\citep{wu2024survey, wei2024llmrec}. By leveraging their generative capabilities, LLMs can produce a range of item~\citep{acharya2023llm, ren2024representation} or user-level~\citep{wang2023zero, wang2023enhancing} features—ranging from high-level product attributes~\citep{xi2024towards} to nuanced decision factors~\citep{sun2024large}—thereby enhancing recommendation performance. However, most existing methods rely on quick inference (also called System-1 thinking)~\citep{kahneman2011thinking} to generate these features, which often leads to issues such as incomplete feature coverage and insufficient specificity~\citep{ji2025test}. As a result, these fast-thinking models lack depth analysis and some critical dimensions of user preferences remain unexplored, so the resulting features may fall short in capturing the subtle decision-making factors that differentiate preferred items from disliked ones.

A concrete example illustrates this limitation: when using a standard LLM, like gpt-4o-mini~\citep{gpt_4o_mini}, to analyze a user's interaction history with musical instruments, the model generates a vague feature such as ``Component Functionality.'' However, this feature lacks the specific details that truly matter to a musician’s decision-making process, like ``Enhanced Durability,'' ``Surface Look,'' and ``Material Type.'' In contrast,  
by employing extended Chain-of-Thought (CoT) reasoning~\citep{wei2022chain} with models like o1-mini~\citep{o1_system_card} or o3-mini~\citep{o3}, these more detailed and personalized features are uncovered, resulting in a richer and more precise feature set.  
This example highlights how deeper reasoning allows LLMs to better capture the nuances of user preferences, emphasizing the need for inference scaling to overcome limitations of quick thinking.

Inspired by the remarkable success of \emph{inference computation scaling}~\citep{snell2024Scaling, o1_blog} in tasks such as math~\citep{wang2024openr} and coding~\citep{o1-coder}—where allocating more computational resources yields better performance~\citep{zeng2024scaling}—this work investigates whether these same principles can be harnessed for feature augmentation in recommendation. Our goal is to determine whether extended reasoning chains can discover more diverse and interpretable user-level features, thereby improving downstream recommendation performance. Concretely, we address the following three research questions (RQs):

\textbf{RQ1: Can inference scaling improve recommendation performance, and if so, to what extent?}
We compare baseline LLMs (e.g., gpt-4o-mini) with models that use extended reasoning (e.g., o1-mini and o3-mini). Our experiments show that models with long-CoT reasoning outperform those without, achieving a 12\% increase in NDCG@10. 
This demonstrates that extended reasoning, which has proven effective in math and coding tasks, can also enhance recommendation models.
Interestingly, while algorithms like Beam Search~\citep{xie2023self} and Monte Carlo Tree Search (MCTS)~\citep{browne2012survey} have achieved significant success in other domains, they do not perform as well as the Best-of-N~\citep{cobbe2021training} strategy for feature augmentation in recommendation tasks. These findings suggest that deeper inference, combined with the appropriate search strategy, can lead to substantial improvements in recommendation performance.

\textbf{RQ2: Which factors drive these performance gains?}
We observe that the improvements in recommendation performance are largely driven by the increased feature quantity and enhanced specificity achieved through inference scaling. In our study, feature quantity serves as a proxy for reasoning depth—more unique valid features indicate a deeper exploration of user preferences, thereby addressing the issue of incomplete feature coverage. To assess this, we plot the number of unique valid features generated by different LLMs against their recommendation performance, as shown in Figure~\ref{fig:performance_wrt_count}. Our results reveal a positive correlation: models like DeepSeek-R1~\citep{deepseek_r1} and o3-mini, which generate the most features, also deliver the best performance. In addition, using an LLM-as-a-judge~\citep{zheng2024judging} approach, we find that inference scaling models consistently produce more detailed and precise features compared to non-scaled models, effectively mitigating the problem of insufficient specificity.



\textbf{RQ3: Given that feature quantity influences performance, what factors affect the number of generated features?}
We investigate how model families, long-CoT reasoning, and model size impact the overall quantity of generated features. Additionally, we explore the effect of search strategies. Our results show that feature generation is not merely the sum of individual steps—pursuing local optimality at the step level does not always lead to overall optimality. Consequently, step-level methods like Beam Search and MCTS underperform compared to the solution-level Best-of-N approach in producing a larger number of useful features. These findings highlight the important interaction between models and search strategies in uncovering a more comprehensive set of features.



In summary, our contributions are as follows:

(1) To the best of our knowledge, we are the first to explore inference scaling to address limitations in feature augmentation for recommendation systems, while also evaluating performance improvements across various LLMs.

(2) We investigate how feature quantity and specificity explain the performance gains achieved through inference scaling.

(3) We provide an in-depth analysis of how search algorithms influence feature generation, highlighting their role in enhancing performance.




\section{Preliminaries}



% In this section, we first formalize our task setting (2.1), followed by a description of our *experimental setup* (§2.2), including details on datasets and how we incorporate inference scaling from a reinforcement learning (RL) perspective.


\subsection{Task Formulation}


We adopt a typical sequential recommendation setting~\citep{fang2020deep}, where each user's interactions are sorted in chronological order. For each user \(u\), the final interacted item is set aside as the test item, while all previously interacted items constitute the training set~\citep{he2017neural}. The goal is to analyze the training interactions—inferring the factors that differentiate items the user likes from those the user does not—and then predict the held-out test item (i.e., the next item they will interact with) from a set of candidate items.




\subsection{Experimental Setup}



\subsubsection{Datasets} 
% We evaluate our approach on two subsets of the Amazon dataset, with key parameters summarized in Table 1. 
We conducted experiments on two public datasets: \textbf{Toys} and \textbf{Instruments}, representing the ``Toys and Games'', and ``Musical Instruments'' categories from the Amazon dataset~\citep{ni2019justifying}, respectively. The characteristics of them are summarized in Appendix Table~\ref{tab:stats_datasets}. We use the dataset in a manner consistent with its original intended use.





% These subsets provide a diverse range of user-item interactions, enabling us to assess the generalizability of the proposed method across different domains.




\subsubsection{Key Components of Inference Scaling} 

We frame our inference scaling procedure from a reinforcement learning (RL) perspective~\citep{zeng2024scaling} with the following elements:

\textbf{Policy Model} is responsible for generating features that capture user preferences. 
   % In this work, we experiment with both open-source and closed-source LLMs, including QwQ-32B-perview and deepseek-r1 (open source), as well as o1-mini and gemini-2.0-flash-thinking-exp (closed source). 
   With the prompt shown in Appendix Figure~\ref{fig:PromptTemplate}, the policy model is prompted to analyze the differences between items the user likes and dislikes and generate potential features for recommendation systems. %For consistency and reproducibility, we set the sampling temperature to 0 (greedy decoding).

% \textbf{Value Model}, serving as the critic, evaluates the features generated by the policy model and determines whether they are effective at distinguishing between items the user likes and those they dislike. 
\textbf{Reward model} evaluates all features generated by the policy model, designating them as \emph{valid} if they successfully distinguish between a user’s liked and disliked items. Only those valid features are used to enhance downstream recommendation performance.
Specifically, we use Qwen2.5-7B-Instruct~\citep{qwen_2.5} as the reward model. While one could, in principle, use downstream recommendation performance as a reward signal, doing so would require an extensive and costly process of item-level feature completion for each candidate feature. As the focus of this paper is on the use of inference computation for feature augmentation, we leave the more complex tasks of designing reward functions and fine-tuning LLMs based on rewards for future research.

\textbf{Search Strategy} guides how the policy model explores and selects the generated features. Various strategies can be employed, such as Best-of-N~\citep{cobbe2021training}, Beam Search~\citep{xie2023self}, or Monte Carlo Tree Search (MCTS)~\citep{browne2012survey}. These strategies strike a balance between exploring new features and exploiting features that have already shown promise in previous steps. By carefully selecting features with high rewards, the model can efficiently navigate the vast space of potential features, ensuring that the most effective features are identified and used for the final recommendation task.



\section{Impact of Inference Scaling on Recommendation Performance (RQ1)}


% \input{tab/result_main}
\begin{table*}
\caption{Performance of features from various policy models on downstream recommendation tasks. The baseline performance, labeled as ``w/o features'', reflects results without the inclusion of additional features. Bold denotes the highest scores across all policy models. For brevity, `\%' is omitted from scores in subsequent tables and figures.}
\label{tab:main_res}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|l|ccccc|ccccc|c}
\hline\hline
\multirow[m]{2}{*}{Dataset}      & \multirow{2}{*}{Policy Model} & \multicolumn{5}{c|}{DR}                        & \multicolumn{5}{c|}{ICL}                        & NIP \\ \cline{3-13} 
    &     & Valid Rate & NDCG@5 & HIT@5 & NDCG@10 & HIT@10 & Valid Rate & NDCG@5 & HIT@5 & NDCG@10 & HIT@10 & HIT     \\ 
\hline
\multirow[m]{15}{*}{Toys}        & w/o features                  & 94.80            & 37.21  & 51.01 & 43.50    & 70.67  & 99.33           & 35.46  & 49.69 & 42.06   & 70.28  & 24.46   \\
    & Qwen2.5-7B-Instruct           & 97.28           & 40.75  & 56.72 & 46.84   & 75.54  & 99.67           & 39.26  & 54.88 & 45.73   & 74.98  & 27.13   \\
    & Qwen2.5-14B-Instruct          & 97.38           & 39.83  & 55.24 & 46.20    & 74.98  & 99.52           & 38.22  & 53.47 & 44.90    & 74.22  & 26.23   \\
    & Qwen2.5-32B-Instruct          & 96.95           & 40.39  & 54.94 & 46.95   & 75.41  & 99.71           & 39.46  & 55.14 & 45.91   & 75.23  & 26.75   \\
    & DeepSeek-R1-7B   & 96.90            & 37.39  & 52.36 & 44.15   & 73.33  & 99.52           & 36.24  & 51.37 & 43.09   & 72.69  & 24.27   \\
    & DeepSeek-R1-14B  & 95.90            & 38.14  & 52.56 & 44.71   & 73.05  & 99.57           & 35.81  & 49.86 & 43.12   & 72.65  & 25.08   \\
    & DeepSeek-R1-32B  & 97.38           & 38.79  & 53.97 & 45.58   & 75.02  & 100.00            & 37.11  & 52.84 & 43.85   & 73.63  & 25.56   \\
    & QwQ-32B-Preview               & 96.09           & 38.16  & 53.10  & 44.37   & 72.41  & 99.71           & 35.32  & 50.65 & 42.22   & 72.12  & 24.75   \\
    & gpt-4o-mini                        & 97.14           & 38.87  & 52.97 & 45.64   & 74.13  & 99.71           & 38.16  & 52.89 & 44.78   & 73.46  & 26.51   \\
    & o1-mini                       & 97.19           & 39.25  & 53.29 & 46.13   & 74.68  & 99.81           & 38.82  & 53.94 & 45.44   & 74.58  & 25.99   \\
    & o3-mini                       & 97.23           & 42.23  & \textbf{56.89} & 48.11   & 75.28  & 99.86           & 40.69  & 55.97 & 47.25   & \textbf{76.41}  & 29.04   \\
    & DeepSeek-V3                   & 97.28           & 41.63  & 56.47 & 48.15   & \textbf{76.72}  & 99.90            & 40.98  & 55.80  & 47.06   & 74.65  & 28.42   \\
    & DeepSeek-R1                   & 97.23           & \textbf{42.56}  & 56.74 & \textbf{48.86}   & 76.31  & 99.62           & \textbf{42.35}  & \textbf{58.83} & \textbf{47.66}   & 75.30   & \textbf{29.33}   \\
    & gemini-2.0-flash              & 97.38           & 40.89  & 55.63 & 47.12   & 75.07  & 99.76           & 40.76  & 56.64 & 46.82   & 75.48  & 27.61   \\
    & gemini-2.0-flash-thinking-exp & 97.42           & 41.13  & 55.07 & 47.35   & 74.40   & 99.76           & 41.24  & 57.31 & 47.20    & 75.72  & 28.85   \\ 
\hline
\multirow[m]{15}{*}{Instruments} & w/o features                  & 95.33           & 40.77  & 56.13 & 46.85   & 75.23  & 99.75           & 36.01  & 53.14 & 43.23   & 75.59  & 23.99   \\
    & Qwen2.5-7B-Instruct           & 97.42           & 44.10   & 58.33 & 50.24   & 77.53  & 99.63           & 39.73  & 54.44 & 46.66   & 75.80   & 27.68   \\
    & Qwen2.5-14B-Instruct          & 98.28           & 39.28  & 53.44 & 46.67   & 76.47  & 99.63           & 35.87  & 52.10  & 44.07   & 77.53  & 27.68   \\
    & Qwen2.5-32B-Instruct          & 97.79           & 43.28  & 58.11 & 49.92   & 78.62  & 99.88           & 39.95  & 57.27 & 47.13   & 79.56  & 27.68   \\
    & DeepSeek-R1-7B   & 96.56           & 41.88  & 57.96 & 48.37   & 77.96  & 99.88           & 36.80   & 53.94 & 44.45   & 77.83  & 25.22   \\
    & DeepSeek-R1-14B  & 96.43           & 40.31  & 55.23 & 47.75   & 78.44  & 99.63           & 35.55  & 52.72 & 43.01   & 76.05  & 24.48   \\
    & DeepSeek-R1-32B  & 97.66           & 40.13  & 55.04 & 46.99   & 76.20   & 99.63           & 38.66  & 55.93 & 45.23   & 76.30   & 25.46   \\
    & QwQ-32B-Preview               & 95.69           & 43.20   & 59.25 & 49.50    & 78.66  & 99.63           & 39.22  & 55.80  & 46.64   & 78.77  & 25.22   \\
    & gpt-4o-mini                        & 97.79           & 41.86  & 57.48 & 48.76   & 78.99  & 100.00            & 40.79  & 58.43 & 47.34   & 78.84  & 29.77   \\
    & o1-mini                       & 98.15           & 46.21  & 62.66 & 51.65   & 79.57  & 99.88           & 43.04  & 61.08 & 48.81   & 79.06  & 30.01   \\
    & o3-mini                       & 97.54           & 45.60   & 59.90  & 52.12   & 80.08  & 99.88           & 43.20   & 59.85 & 49.31   & 78.82  & 30.01   \\
    & DeepSeek-V3                   & 97.91           & 48.10   & 62.69 & 53.84   & 80.65  & 99.63           & 44.59  & 62.10  & 51.04   & 82.10   & 32.10    \\
    & DeepSeek-R1                   & 98.52           & 47.82  & 62.05 & 53.36   & 79.28  & 99.63           & 44.29  & 61.73 & 50.60    & 81.48  & 30.75   \\
    & gemini-2.0-flash              & 97.54           & 48.98  & 64.56 & 55.05   & \textbf{83.23}  & 99.88           & 45.65  & 64.53 & 51.54   & 82.76  & \textbf{31.49}   \\
    & gemini-2.0-flash-thinking-exp & 98.77           & \textbf{49.96}  & \textbf{65.63} & \textbf{55.50}    & 82.57  & 100.00            & \textbf{47.41}  & \textbf{64.82} & \textbf{53.54}   & \textbf{83.76}  & 31.24   \\ 
\hline\hline
\end{tabular}%
}
\end{table*}

In this section, we first investigate whether inference scaling can enhance the performance of recommendation systems. %and investigate the relationship between the number of features generated and the performance metrics.

\subsection{Experimental Setup}

\textbf{Models.} To investigate how features from different models impact performance, we employ various LLMs as policy models. Specifically, we use the \textbf{GPT} series, including gpt-4o-mini~\citep{gpt_4o_mini}, o1-mini~\citep{o1_system_card}, o3-mini~\citep{o3}. The \textbf{Qwen} series consists of Qwen2.5-Instruct at 7B, 14B, and 32B scales~\citep{qwen_2.5}, while the \textbf{Gemini} series includes gemini-2.0-flash~\citep{gemini_2.0_flash} and gemini-2.0-flash-thinking-exp~\citep{gemini_2.0_flash}. For the \textbf{DeepSeek} series, we incorporate DeepSeek-V3~\citep{deepseek_v3} and DeepSeek-R1~\citep{deepseek_r1}. In addition, DeepSeek-R1 provides distilled variants based on Qwen in 7B, 14B, and 32B configurations, which we include in our experiments \footnote{For the sake of simplicity, DeepSeek-R1-Distill-Qwen-7B will be referred to as DeepSeek-R1-7B subsequently, and the same applies to 14B and 32B.}. %As the reward model for verifying the effectiveness of the generated features, we use Qwen2.5-7B-Instruct.

\textbf{Recommenders.} We evaluate the effect of generated features using three different recommendation models: (1) \textbf{Direct Recommendation (DR)}~\citep{liu2023chatgpt}: A listwise ranking method where the model ranks $C$ candidate items based on the user's interaction history. (2) \textbf{In-Context Learning (ICL)}~\citep{dai2023uncovering}: Similar to DR, this approach also ranks $C$ candidate items based on the user's history, but uses a one-shot setting with an example prompt. (3) \textbf{Next Item Prediction (NIP)}~\citep{geng2022recommendation}: The model predicts the next item a user is likely to interact with, based on their history and a set of $C$ candidate items.


\textbf{Metrics.} For both DR and ICL, we report HIT@K and NDCG@K (with $K \in \{5, 10\}$) as the evaluation metrics. NDCG places higher weight on items ranked at the top, while HIT treats all positions equally and essentially measures recall. For NIP, we evaluate HIT to measure prediction accuracy. Additionally, to ensure that the LLM does not produce invalid outputs (such as missing or duplicate items), we compute the Valid Rate, which measures the proportion of compliant rankings.


\textbf{Details.} For each set of candidate items, the user’s most recent interaction is treated as the ground truth (positive sample), while \(C-1\) other items are randomly sampled as negative samples. To ensure fairness, we set \(C=20\) and use gpt-4o-mini to perform the recommendation tasks. The experiment is repeated three times, and the average value is taken.



\subsection{Effect of Different Policy Model Features}
\label{sec: RQ1_policy}


% \input{tab/result_search}


\begin{table*}
\caption{Performance of features generated by various search strategies on recommendation tasks. Bold denotes the highest scores across all strategies.}
\label{tab:search_res}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ll|ccccc|ccccc|c}
\hline\hline
\multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Policy \\ Model\end{tabular}}}         & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Search \\ Strategy\end{tabular}} & \multicolumn{5}{c|}{DR}                                                        & \multicolumn{5}{c|}{ICL}                                                       & NIP            \\ \cline{3-13} 
\multicolumn{1}{l|}{}                    &                & Valid Rate & NDCG@5         & HIT@5          & NDCG@10        & HIT@10         & Valid Rate & NDCG@5         & HIT@5          & NDCG@10        & HIT@10         & HIT            \\ \hline
\multicolumn{2}{c|}{w/o features}                         & 95.33      & 40.77          & 56.13          & 46.85          & 75.23          & 99.75      & 36.01          & 53.14          & 43.23          & 75.59          & 23.99          \\ \hline
\multicolumn{1}{l|}{\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Qwen2.5-7B\\ -Instruct\end{tabular}}}  & CoT            & 97.42      & 44.10           & 58.33          & 50.24          & 77.53          & 99.63      & 39.73          & 54.44          & 46.66          & 75.80           & 27.68          \\
\multicolumn{1}{l|}{}                    & Best-of-N      & 97.54      & \textbf{44.91} & \textbf{60.66} & \textbf{51.22} & \textbf{80.20}  & 99.75      & 39.70           & 56.23          & 46.64          & 77.68          & \textbf{28.04} \\
\multicolumn{1}{l|}{}                    & Beam Search    & 97.42      & 43.70           & 59.09          & 49.85          & 78.41          & 99.63      & \textbf{40.06} & \textbf{56.67} & \textbf{46.92} & \textbf{77.90}  & 26.20           \\
\multicolumn{1}{l|}{}                    & MCTS           & 97.42      & 41.87          & 56.44          & 48.12          & 75.88          & 99.51      & 39.43          & 56.37          & 45.81          & 76.27          & 26.57          \\ \hline
\multicolumn{1}{l|}{\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Qwen2.5-14B\\ -Instruct\end{tabular}}} & CoT            & 98.28      & 39.28          & 53.44          & 46.67          & 76.47          & 99.63      & 35.87          & 52.10           & 44.07          & \textbf{77.53} & \textbf{27.68} \\
\multicolumn{1}{l|}{}                    & Best-of-N      & 97.91      & \textbf{43.11} & \textbf{58.54} & \textbf{49.22} & 77.76          & 99.88      & \textbf{40.06} & \textbf{56.77} & \textbf{46.73} & 77.34          & \textbf{27.68} \\
\multicolumn{1}{l|}{}                    & Beam Search    & 97.79      & 41.09          & 55.97          & 47.79          & 76.73          & 99.75      & 36.62          & 52.90           & 43.93          & 75.59          & 27.31          \\
\multicolumn{1}{l|}{}                    & MCTS           & 97.79      & 40.64          & 54.72          & 48.17          & \textbf{77.99} & 99.51      & 37.99          & 54.39          & 44.95          & 76.14          & 24.72          \\ 
\hline\hline
\end{tabular}%
}
\end{table*}

We compare the effects of features generated by different policy models on recommendation performance across two datasets, as shown in Table~\ref{tab:main_res}. Key findings include:


\textbf{(1) Positive Impact of Features:}
   Overall, adding distinguishable features that capture user preferences improves the recommendation performance of the LLM. For instance,  DeepSeek-R1 achieves an NDCG@10 of 48.86 in DR, which is a 12\% improvement over the baseline (43.50). Furthermore, the Valid Rate improves after feature augmentation, indicating that the features validated by the reward model are helping the LLM complete the recommendation task more effectively.

\textbf{(2) Long-CoT vs. Non-Long-CoT Models:}
   % Among the policy models tested, DeepSeek-R1, gemini-2.0-flash-thinking, and o3-mini exhibit the best performance, with each of the first two excelling on different datasets. These models also rank highly on math and coding topics of Chatbot Arena~\citep{chiang2024chatbot}, suggesting that their advanced inference scaling techniques transfer effectively to recommendation tasks. Additionally, models with long-CoT processes, such as o1-mini and o3-mini, demonstrate a significant performance boost over their non-long-CoT counterparts, such as gpt-4o-mini. For example, on the Toys dataset, the NDCG@5 metric improved from 38.87 (for gpt-4o-mini) to 39.25 (for o1-mini) and 42.23 (for o3-mini). Notably, the distilled versions of DeepSeek-R1 perform considerably worse than the 671B version, suggesting that larger models should be preferred when computational resources permit.
   In our experiments, we observe that models with long-CoT processes, such as o1-mini and o3-mini, outperform their non-long-CoT counterparts (e.g., gpt-4o-mini). For instance, on the Toys dataset, the NDCG@5 metric improves from 38.87 (for gpt-4o-mini) to 39.25 (for o1-mini) and 42.23 (for o3-mini). Similarly, gemini-2.0-flash-thinking-exp shows an improvement over the non-long-CoT gemini-2.0-flash, confirming that long-CoT reasoning provides a significant advantage in feature generation. While DeepSeek-V3 performs slightly worse than DeepSeek-R1 on the Instruments dataset, the latter outperforms DeepSeek-V3 on the larger Toys dataset.
   One notable exception is QwQ, which underperforms relative to the Qwen2.5-Instruct series. We speculate that this is due to QwQ generating fewer features, a point we will further explore in Section~\ref{sec:RQ3}.

\textbf{(3) Transfer of Advanced Reasoning to Recommendation:}
Models like DeepSeek-R1, gemini-2.0-flash-thinking-exp, and o3-mini stand out for their superior performance, with the first two excelling on different datasets. These models also perform strongly in math and coding topics of Chatbot Arena~\citep{chiang2024chatbot}, indicating that \textbf{the advanced inference scaling techniques driving their success in reasoning-intensive tasks also benefit recommendation}. 
% We believe this success stems from the fact that recommendation data—characterized by highly personalized user behavior—is generally not used during the training of most LLMs~\citep{dubey2024llama, deepseek_v3}. According to ~\citep{yao2025unveiling}, training LLMs on long-CoT data can improve generalization, making LLMs more adept at handling OOD scenarios like recommendation tasks. Therefore, inference scaling in LLMs enables them to better handle the intricacies of user preferences, resulting in better performance on recommendation tasks.
We propose that this stems from the out-of-distribution (OOD) nature of recommendation data: due to highly personalized user behavior, recommendation data is generally not used during the pre-training of most LLMs~\citep{dubey2024llama, deepseek_v3}. As suggested by~\citep{yao2025unveiling}, training LLMs on long-CoT data can improve generalization to OOD tasks, enabling them to reason more effectively about nuanced user preferences.


% \textbf{(3) Correlation Between Effective Features and Performance: }
%    We observed a positive correlation between the number of effective features generated by LLM and its performance on downstream tasks. Models like deepseek-R1, gemini-2.0-flash-thinking, and o3-mini that generated a higher number of valid features consistently outperformed other models. This suggests that, in information-scarce recommendation scenarios, the model's ability to generate a wide variety of features increases the likelihood of capturing the user's decision factors. Once valid features are discovered and utilized, the performance boost is substantial.


% These findings highlight that inference scaling methods, which enhance a model's ability to generate and validate features, not only improve the LLM's feature extraction capabilities but also lead to better recommendation performance across various tasks.

% In summary, our findings highlight the importance of feature generation in enhancing the performance of LLM-based recommendation systems. The models that generate a higher number of effective features, especially those using extended chain-of-thought reasoning, perform better in recommending items tailored to user preferences. These results suggest that the application of inference scaling techniques in the recommendation domain is not only feasible but also beneficial for improving personalization and prediction accuracy.
In summary, we demonstrate that augmenting recommendation systems with LLM-generated features significantly improves performance, especially when models use long-CoT reasoning. Moreover, policy models that excel in math and coding tasks tend to generate superior features for capturing user preferences. This convergence underscores the potential of inference scaling as a general strategy for enhancing recommendation systems.


\subsection{Effect of Different Search Strategies}
\label{sec: RQ1_search}

In math and coding tasks, models often cannot arrive at a final solution in a single step. Instead, they often rely on search algorithms—either exploring multiple solution paths in parallel~\citep{brown2024large} or iteratively refining a candidate solution~\citep{madaan2024self}. Naturally, one might ask whether these same algorithms are equally beneficial for feature generation in a recommendation context, where the model's goal is to uncover user preferences. In this study, we evaluate four search strategies for generating features: CoT, Best-of-N, Beam Search, and MCTS. Details of each strategy can be found in Appendix~\ref{app:search_strategy}.


To balance effectiveness and computational efficiency, we evaluate these strategies on the smaller Instruments dataset, using Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct as our policy models. The reward model measures how many valid features—those that effectively distinguish between a user's liked and disliked items—each strategy generates, and we select the output with the highest count of valid features.

The results, presented in Table~\ref{tab:search_res}, show that all search strategies outperform the baseline CoT approach. For example, with Qwen2.5-14B-Instruct as the policy model, CoT alone achieves an NDCG@5 of 39.28. In comparison, advanced search algorithms achieve significantly better results: Best-of-N (43.11), Beam Search (41.09), and MCTS (40.64). When Qwen2.5-7B-Instruct is used in the ICL task, Beam Search slightly outperforms Best-of-N. However, as the policy model size increases (from 7B to 14B), Best-of-N maintains its advantage over step-level methods like Beam Search and MCTS. Overall, Best-of-N shows the largest performance gains especially when the policy model is sufficiently large, likely due to its ability to select the best features from multiple generated outputs.

% These findings suggest that, while step-level strategies such as Beam Search and MCTS are common in domains where each incremental step must be precisely correct (e.g., solving complex mathematical proofs or playing games like Go), Best-of-N appears more effective at identifying more beneficial features for recommendation tasks. We will further explore the reasons for this trend in subsequent Section~\ref{sec: RQ3_search}.
These findings suggest that while step-level strategies like Beam Search and MCTS are effective in domains where precise incremental steps are crucial (such as solving mathematical proofs~\citep{xin2024deepseek} or playing games like Go~\citep{silver2016mastering}), Best-of-N is more effective in identifying the most relevant features for recommendation tasks. We will explore the reasons for this trend in greater detail in Section~\ref{sec: RQ3_search}.




\section{Advantages of Inference Scaling for Feature Augmentation(RQ2)}
\label{sec: RQ2}


In the previous section, we showed that features generated through inference scaling can significantly improve recommendation performance. However, it remains unclear how these gains are achieved—specifically, what advantages long-CoT LLMs offer over non-long-CoT models. 

\subsection{Increased Number of Unique Features}

To investigate this aspect, we compare the features generated by gpt-4o-mini and o1-mini.
% Specifically, we collect the features generated for all users and calculate the total number of features. After clustering and removing duplicates from these features, we calculate the number of unique features. Specifically, We employ the bge-m3~\citep{chen2024m3} model to convert these features into embeddings and then use DBSCAN~\citep{ester1996density} clustering to identify unique features. Since decision factors in recommendation systems vary significantly among different users~\citep{he2024impact}, the unique feature quantity should increase as reasoning goes deeper.  
We first collect all valid features generated across all users, then measure both the total number of features and, after clustering and removing duplicates, the number of unique features. In particular, we use the bge-m3 model~\citep{chen2024m3} to embed the features and DBSCAN~\citep{ester1996density} to group them. Since users can exhibit diverse decision factors~\citep{he2024impact}, deeper and more personalized reasoning allows the LLM to uncover the unique preferences and decision-making factors of each user. As the model performs more in-depth analysis of these varied factors, it is likely to generate a greater number of distinct features, each reflecting different aspects of the user's preferences. Therefore, the number of unique features can serve as a valuable metric for assessing the extent to which the model has captured the complexity and individuality of user decision-making.
% The total number of features and the number of unique features are shown in Appendix Table~\ref{tab:feature_quantity}. The results demonstrate that o1-mini outperforms gpt-4o-mini in both the total number of features and the number of unique features, suggesting that the extended chain-of-thought reasoning process in o1 enables a deeper exploration and understanding of user preferences.
The results, presented in Appendix Table~\ref{tab:feature_quantity}, show that o1-mini generates more features overall and more unique features than gpt-4o-mini, indicating that extended CoT reasoning enables a more comprehensive exploration of user preferences.

% The factors affecting the number of unique features will be further analyzed in the subsequent section. To further validate the relationship between quantity and recommendation performance, we plot the recommendation performance of features generated by different policy models against the number of effective unique features they generate in the Toys dataset. For visual clarity, we take the logarithm (base 10) of the unique valid feature count. The results, shown in Figure~\ref{fig:performance_wrt_count}, reveal a positive correlation: as the number of unique valid features increases, recommendation performance also improves. This finding suggests that unique valid feature quantity is a reasonable indicator of the degree of personalized reasoning and a useful predictor of recommendation performance.
To further analyze the relationship between feature quantity and recommendation performance, we use the larger Toys dataset—chosen for its size, which minimizes noise and yields robust insights. In this dataset, we plot the recommendation performance of features generated by different policy models against the number of unique valid features (using the logarithm, base 10, of the count for clarity). The results, shown in Figure~\ref{fig:performance_wrt_count}, indicate a positive correlation: as the number of unique valid features increases, recommendation performance improves. This finding suggests that the quantity of unique valid features can be considered a reasonable indicator of the depth of personalized reasoning, and can serve as a useful predictor of recommendation performance.


\subsection{More Detailed and Specific Descriptions}

Beyond generating more features, we also observe that o1-mini produces features with more detailed and specific descriptions. To illustrate this more clearly, we provide an example in Appendix~\ref{app: appendix_example}. 

To rigorously evaluate the specificity of the descriptions, we adopt an LLM-as-a-judge~\citep{zheng2024judging} approach. 
% in which we ask gpt-4o-mini, o1-mini, and claude-3-5-sonnet~\citep{ckaude_3_5_sonnet} to determine which description in each pair is more concrete. 
In this method, we present pairs of feature descriptions—one from gpt-4o-mini and one from o1-mini—to three judge models (gpt-4o-mini, o1-mini, and claude-3-5-sonnet~\citep{ckaude_3_5_sonnet}).
To mitigate position bias~\citep{shi2024judging}, each feature pair is evaluated twice, with the positions of the generated features swapped between the two rounds. We consider a feature description superior only when both evaluations are consistent; otherwise, the result is recorded as a tie. 

The results are presented in Figure~\ref{fig:llm_as_judge}, indicating unanimous agreement among all judges that o1-mini generates more specific and better-described features than gpt-4o-mini. Notably, even though LLMs often exhibit self-enhancement bias~\citep{zheng2024judging, brown1986evaluations} — where they tend to favor their own generated content — gpt-4o-mini still rated o1-mini's descriptions more favorably. This further supports the conclusion that o1-mini produces more detailed and precise features.

In summary, the key benefits of inference scaling in feature generation are: (1) it generates more features, and (2) these generated features are more specific and detailed. These advantages suggest that longer reasoning chains enable models to explore a richer space of possible features, providing more meaningful insights into user preferences for recommendation tasks.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/winlose.pdf}
  \caption{Win-Tie-Lose Comparisons on specificity of features from gpt-4o-mini and o1-mini.}
  \label{fig:llm_as_judge}
\end{figure}


\section{Factors Influencing the Number of Generated Features (RQ3)}
\label{sec:RQ3}

Having established that inference scaling can produce both a larger and more concrete set of features, we now turn our attention to examining the factors that affect the quantity of generated features. 

%To this end, we conduct a systematic investigation of different policy models as well as various model sizes and training strategies.

\begin{figure*}[t]  
    \centering    

    \subfigure[]
    {
    \centering
    \includegraphics[width=0.48\linewidth]{figs/feature_growth_toy_closed_source.pdf}
    \label{fig: toy_multi_LLMs_a}
   }
  % \hspace{-0.25in}
  \subfigure[]
    {
    \centering
    \includegraphics[width=0.48\linewidth]{figs/feature_growth_toy_open_source.pdf}
    \label{fig: toy_multi_LLMs_b}
   }
   % \quad
   
    \caption{Number of unique features generated by different LLMs compared against the total number of generated features in the Toys dataset.}

    \label{fig:feature_growth_toy_multi_LLMs}
\end{figure*}

\subsection{Model Selection}



Following the procedure in Section~\ref{sec: RQ2}, we collect all features generated by each policy model across all users. The reward model then identifies which features effectively distinguish users’ liked items from disliked ones. We cluster and remove duplicates from these valid features, ultimately comparing the number of unique features from each policy model.
% we collect all features generated by the policy model across multiple users and filter out those deemed invalid by the reward model. We then cluster and remove duplicates to obtain a final set of unique features for each policy model.

Figure~\ref{fig:feature_growth_toy_multi_LLMs} 
% shows how the number of unique features grows with the total number of features on the Toys dataset. 
illustrates the growth of unique features as a function of the total number of valid features on the Toys dataset.
The horizontal axis represents the total number of valid, distinguishable features generated across all users, while the vertical axis indicates the number of unique features—serving as a proxy for the depth of personalization. Three key observations emerge:

\textbf{(1) Model Families.} The DeepSeek and Gemini families attain the top points on the vertical and horizontal axes, respectively. DeepSeek-R1 generates the highest number of unique valid features, whereas gemini-2.0-flash-thinking-exp produces a large number of features overall but fewer truly unique ones—indicating high quantity but somewhat weaker personalization. Within the GPT family, o3-mini stands out by producing a substantial number of unique features, second only to DeepSeek-R1. By contrast, the Qwen series yields fewer features in total (under 20K) and fewer unique features (under 5K). 

\textbf{(2) Long-CoT.} When comparing models within the same family, those incorporating extended reasoning—via SFT~\citep{o1_journey_part2}, RL~\citep{min2024imitate}, or other advanced training techniques~\citep{deepseek_r1}—produce significantly more unique features than their base counterparts. For example, in Figure~\autoref{fig: toy_multi_LLMs_a}, DeepSeek-R1 yields more than three times the number of unique features compared to DeepSeek-V3, while o3-mini outperforms gpt-4o-mini by over 50\%. Similarly, gemini-2.0-flash-thinking-exp surpasses gemini-2.0-flash, and the distilled versions of DeepSeek-R1 for Qwen exhibit steeper slopes than standard Qwen models of the same scale (Figure~\autoref{fig: toy_multi_LLMs_b}). %, indicating stronger personalization under the same computational budget. 
We propose that explicitly producing a richer chain-of-thought—rather than keeping it implicit in model parameters—helps capture more granular decision factors, facilitating the discovery of a broader range of user-specific features. One exception is QwQ, which generates fewer features than Qwen2.5-7B-Instruct, aligning with the results in Section~\ref{sec: RQ1_policy} showing that QwQ underperforms Qwen in downstream recommendation tasks.

\textbf{(3) Model Size.} Overall, larger models tend to perform better, as exemplified by DeepSeek-R1 (with 671B parameters) generating over 17,500 unique features, whereas its distilled 7B, 14B, and 32B versions produce only a few thousand. Nonetheless, within the 7B–32B range, larger models do not always dominate: across both Qwen and distilled DeepSeek models, the 7B variants often surpass their 14B and 32B counterparts in personalization capacity. Intriguingly, smaller models that benefit from long-CoT training can even outperform larger models without extended reasoning. For instance, DeepSeek-V3 (671B) generates around 5K unique features, whereas DeepSeek-R1-7B produces over 7K—highlighting the substantial role of long chain-of-thought processes.

% In summary, long-CoT training and model family exert the most significant influence on feature generation capacity. Not only have these models demonstrated breakthroughs in mathematical and coding tasks, but they also show promise for personalized preference mining. In highly personalized domains such as recommendation, where users' decision factors can vary dramatically, an in-depth, case-by-case approach that leverages explicit and extended reasoning proves extremely beneficial. %Moreover, even relatively small models—if endowed with long-CoT capabilities—may surpass larger counterparts in discovering a wider range of user-relevant features.

In summary, long-CoT training and model family exert the greatest influence on feature generation capacity. In highly personalized domains such as recommendation, where user decision factors vary widely, a detailed, case-by-case approach that leverages explicit and extended reasoning proves particularly advantageous.


\begin{figure}[t]  
    \centering    

    \subfigure[Qwen2.5-7B-Instruct as policy model]
    {
    \centering
    \includegraphics[width=0.45\linewidth]{figs/feature_growth_search_method_instrument_qwen7b.pdf}
    \label{fig: search_method_qwen7b}
   }
  % \hspace{-0.25in}
  \subfigure[Qwen2.5-14B-Instruct as policy model]
    {
    \centering
    \includegraphics[width=0.45\linewidth]{figs/feature_growth_search_method_instrument_qwen14b.pdf}
    \label{fig: search_method_qwen14b}
   }
   % \quad
   
    \caption{Comparison of different search strategies on the Instruments dataset.}

    \label{fig:search_method}
\end{figure}





\subsection{Search Strategies}
\label{sec: RQ3_search}

We next investigate how different search strategies affect the quantity of generated features. Figure~\ref{fig:search_method} presents results on the Instruments dataset. All strategies exhibit a roughly linear relationship between the total number of generated features and the number of unique features. Notably, Best-of-N consistently produces the highest total feature count, while Beam Search and MCTS yield fewer features overall. This aligns with their recommendation performance in Section~\ref{sec: RQ1_search}, where Best-of-N outperforms the other search strategies.

The underlying reason lies in the difference between step-level and solution-level optimization. In tasks like math~\citep{xin2024deepseek} or game playing (e.g., Go~\citep{silver2016mastering}), each intermediate step must be correct to avoid disrupting the final outcome; local missteps can ruin the entire sequence, making step-level search essential for achieving a globally optimal result. In contrast, user-feature generation in recommendation is less tightly coupled: each newly proposed feature—whether relevant or not—does not necessarily affect subsequent features. Even if some features are ineffective, the model can still generate valid and valuable features afterward. For instance, generating features such as appearance, price, or durability are relatively independent, so discovering useful features after ineffective ones can still improve the overall result. Therefore, step-level search provides limited benefits in recommendation tasks, whereas solution-level strategies like Best-of-N are more effective in identifying the most promising solution without requiring each incremental step to be optimal.


% \begin{figure*}[t]
%   \includegraphics[width=0.5\linewidth]{figs/feature_growth_toy_closed_source.pdf} \hfill
%   \includegraphics[width=0.5\linewidth]{figs/feature_growth_toy_open_source.pdf}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}




% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}











\section{Related Work}

% \textbf{LLMs for Rec.} LLMs can be leveraged to enhance recommendation systems by providing contextual understanding and powerful reasoning. Recent studies utilizing LLMs for augmentation can be divided into two paths: one is using LLMs for text embedding. The arrtibutes of users, items can be combined and calculate the embedding~\citep{hou2024bridging, zhang2024text, sheng2024language, lee2024star, yuan2023go}. Another line is to utilize LLMs to generate additional information. For example, KAR~\citep{xi2024towards} requires LLMs to generate item descriptions and user reasoning preferences. LLM-ESR~\citep{liu2024llm} also summarizes user preferences. RLMRec~\citep{ren2024representation} describes user and item, also reason more collaborative information. LLM-CF~\citep{sun2024large} generate a CoT of user decision. However, previous works rely on fast thinking, and it often leads to incomplete coverage and insufficient concreteness~\citep{ji2025test}.

\textbf{LLMs for Rec.} LLMs can enhance recommendation systems by providing contextual understanding and reasoning abilities~\citep{huang2022towards}. Recent work on LLM-based augmentation in recommendation generally follows two main directions. The first approach leverages LLMs for text embedding, integrating user and item attributes into unified representations~\citep{hou2024bridging, zhang2024text, sheng2024language, lee2024star, yuan2023go}. The second approach focuses on generating additional information for recommendations. For instance, KAR~\citep{xi2024towards} instructs LLMs to produce item descriptions and user preference rationales, while LLM-ESR~\citep{liu2024llm} summarizes user preferences. RLMRec~\citep{ren2024representation} describes both user and item attributes, incorporating collaborative information, and LLM-CF~\citep{sun2024large} uses a chain-of-thought to represent user decisions. However, most of these methods rely on relatively fast inference, frequently resulting in incomplete coverage and insufficient specificity~\citep{ji2025test}.

% \textbf{Inference Computation Scaling.} Inference scaling is to allocate more computation resources into inference stages~\citep{o1_blog}, so that LLMs can reason based on more previous steps, shifting from System-1 fast thinking to System-2 slow thinking model~\citep{ji2025test}. ~\citealp{snell2024Scaling} first investigates the relationship between performance and inference time. Recent works~\citep{qi2024mutual, zhang2024accessing, chen2024alphamath, yang2024qwen2, shao2024deepseekmath} have shown promising results in math and coding problems, but none of them investigate whether this inference scaling techniques can indeed perform well in personalized tasks such as recommendations.
\textbf{Inference Computation Scaling.} Inference scaling involves allocating more computational resources to the inference stage, allowing LLMs to reason based on additional prior steps. This shift from System-1 (fast thinking) to System-2 (slow thinking) reasoning enhances their ability to process complex tasks~\citep{ji2025test}. The relationship between performance and inference time was first examined by~\citet{snell2024Scaling}, and recent studies~\citep{qi2024mutual, zhang2024accessing, chen2024alphamath, yang2024qwen2, shao2024deepseekmath} have demonstrated promising results in math and coding tasks. Moreover,~\citet{yue2024inference} investigates inference scaling for Retrieval Augmented Generation, and~\citet{sun2025rearter} employs test-time scaling to address challenges in complex multi-step reasoning.
However, none of these works have investigated whether inference scaling can help address the incomplete coverage problem in recommendation tasks, and we aim to explore how this technique can improve personalized recommendation systems through feature augmentation.

\section{Conclusion}
In this paper, we demonstrate that inference scaling can significantly enhance feature augmentation for recommendation systems. 
% We showed that extending the reasoning capabilities of LLMs through long-CoT processes results in a greater quantity of more concrete and detailed features, which significantly improve recommendation performance. 
Our experiments show that long-CoT models, compared to traditional fast inference methods, generate more detailed and specific features that better capture user decision factors, resulting in markedly improved recommendation performance.
In contrast to faster inference methods, long-CoT reasoning overcomes challenges like incomplete feature coverage and insufficient specificity. 
Additionally, we observe that the quantity and specificity of features are closely linked to the accuracy and effectiveness of the recommendations.  
Overall, our findings highlight the potential of inference scaling to improve user preference modeling and personalization in recommendation systems, paving the way for future research on its broader application across personalized tasks.

% \clearpage

\section*{Limitations}



In this work, we demonstrate that inference scaling effectively enhances feature generation for recommendation tasks: extended chain-of-thought reasoning yields more diverse, specific features, leading to significant performance improvements. Nevertheless, several limitations warrant attention. First, inference scaling requires additional computational resources—both in terms of model size and inference time~\citep{snell2024Scaling}—which restricts its applicability in low-latency or resource-constrained settings. Second, although our empirical findings highlight the benefits of long-CoT reasoning, we do not provide a theoretical explanation of why inference scaling is so effective in recommendation tasks or why strategies such as beam search and MCTS do not offer comparable improvements. Finally, our experiments are limited to public datasets, so validating inference-scaled models in industrial-scale, heterogeneous environments with more complex user behaviors may introduce further challenges and necessitate refinements. Despite these limitations, our work provides valuable insights into the potential of inference scaling for personalized recommendation systems, underscoring a promising direction for future research.


\section*{Ethics Statement}
\textbf{Data Privacy and Security:}
% The datasets used in this study are publicly available. They are collected and used according to the use agreement of the data providers. All user information is well anonymized to protect the privacy of them. No personally identifiable information was involved in the study.
All datasets used in this study are publicly available and adhere to the usage agreements provided by their respective data sources. The user information in these datasets is fully anonymized to protect individual privacy, and no personally identifiable information is disclosed.

\textbf{Fairness Consideration and Social Impact:}
% The use of LLMs in this study is well controlled to avoid bias or discrimination. It can be guaranteed that there are no negative prompts that can deliberately mislead LLMs to generate specific harmful content. All the prompts are shared and verifiable. To further confirm this, we randomly check the generated samples with a good proportion manually and we also use the LLMs to judge them.
We carefully controlled the use of LLMs to prevent bias or discrimination. Specifically, we avoided prompts that could intentionally mislead LLMs into generating harmful content. The prompts used in our experiments are made publicly available and can be verified. To further ensure safety, we manually checked a representative sample of generated outputs and employed LLM-based evaluation to screen for potentially offensive or biased content.

% One aim of this study is to generate more specific features to improve the outcome of recommendation system. The generated features through our prompts are objective and only describe the attributes of the items without further judging their good or bad to confirm that there is no guidance.
\textbf{Feature Generation Approach:} One of the primary goals of this research is to generate more specific features to improve recommendation quality. The features generated through our prompts are objective, without appraising users or items as ``good'' or ``bad.'' This approach minimizes the risk of introducing undue influence or bias into the recommendation process.

\textbf{Dataset Impact:} The datasets used in this study, which include user interactions with Toys and Instruments, further reduce the likelihood of negative social impact. The only user-related data we utilize are historical interactions and 1–5 ratings, limiting the scope of information available to the LLM. Given these constraints, harmful or sensitive content is highly unlikely to arise.


% \textbf{Licenses}:
% We plan to release our code under an open-source license, enabling the community to replicate and extend our findings while ensuring compliance with any relevant data usage agreements.

% \section*{Acknowledgments}



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{ref}

\appendix



\section{Appendix}





\subsection{Search Strategies}
\label{app:search_strategy}

In this section, we introduce the search algorithms in detail:

\textbf{(a) CoT}. LLM explicitly outputs its intermediate reasoning steps. This has been the default approach in our experiments.

\textbf{(b) Best-of-N~\citep{cobbe2021training}.} The policy model generates $N$ complete outputs %, and the value model selects the one that yields the highest reward (i.e., produces the most effective distinguishing features).
for each user. For each output, the reward model then evaluates each proposed feature inside and marks it as \emph{valid} if it effectively distinguishes between the user's liked and disliked items. Only the valid features are retained in each output, and the final output is selected as the one containing the greatest number of valid features.

% \input{tab/stats_dataset}
\begin{table}
\centering
\caption{Statistics of the two datasets. }
\label{tab:stats_datasets}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Dataset~                       & \#Users & \#Items & \#Interactions~  \\
\midrule
Toys    & 3,962  & 11,119  & 65,099          \\
Instruments & 1,411  & 6,317   & 23,804        \\
\bottomrule
\end{tabular}
}
\end{table}

% \input{tab/feature_quantity}
\begin{table}
\centering
\caption{Total and unique feature counts generated by two policy models on different datasets.}
\label{tab:feature_quantity}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Dataset~        &        Policy Model~                       & \#Total features & \#Unique features   \\
\midrule
\multirow[m]{2}{*}{Toys} & gpt-4o-mini    & 25,536  & 4,037         \\
& o1-mini & 31,822   & 4,482        \\
\midrule
\multirow[m]{2}{*}{Instruments} & gpt-4o-mini    & 8,156  & 2,766          \\
& o1-mini & 11,443   & 4,086        \\
\bottomrule
\end{tabular}
}
\end{table}

% \input{figs/pocily_model_prompt}
\begin{lrbox}{0}
\begin{tcolorbox}[colback=green!5,colframe=green!35!black,boxrule=0.5pt,title={\textbf{LLM Prompt and Response Example}}]
\textbf{Prompt:}
{You are a user behavior analyst. Now a [user ID]  interacted with the following products:


[item 1], [item description 1], [user rating].

[item 2], [item description 2], [user rating].


…, …, …


What do you think is the biggest difference in attributes between these items that this user has rated high and low?


Return the most likely feature you can think of and the definition of this feature.
}\\


\textbf{Response:}
{Based on the historical user behavior provided, I think the reasons why users make these decisions can be attributed to:


\textbf{1. Durability Enhancements}: ….


\textbf{2. Brand Compatibility}: ….


…}
\end{tcolorbox}
\end{lrbox}


\begin{figure}[t]
    \centering
    % \vspace{-10pt}
    \resizebox{0.9\linewidth}{!}{
        \usebox{0}
    }
    % \vspace{-15pt}
    \caption{An example of a prompt for the policy model and its corresponding response.}
    \label{fig:PromptTemplate}
    \vspace{-15pt}
\end{figure}




\textbf{(c) Beam Search~\citep{xie2023self}.} The policy model first produces $N$ partial outputs (beams). The reward model then filters out the beams with lower rewards, retaining only $N/M$ of them. Each of these retained beams is then expanded by $M$ steps, maintaining a total of $N$ partial outputs at each stage. This process repeats until the final output is formed.

\textbf{(d) MCTS~\citep{browne2012survey}.} This lookahead method constructs a search tree from the root (initial prompt) to a terminal state (complete feature set). At each step, four phases occur: 
\begin{itemize}
    \item \textbf{Selection}: From the root, the algorithm traverses down to a leaf node that has not yet been explored; if all are explored, it chooses the leaf with the highest UCT (Upper Confidence Bound for Trees) score.
    \item \textbf{Expansion}: It expands the leaf node by generating one step forward.
    \item \textbf{Evaluation} (Rollout): From the newly added node, it simulates a path to a terminal state to estimate the reward via the reward model.
    \item \textbf{Backpropagation}: The reward is propagated back to update the intermediate nodes, guiding future selections.
\end{itemize} 

CoT and Best-of-N operate on the \textbf{solution level}, selecting among multiple fully generated outputs. In contrast, beam search and MCTS function at the \textbf{step level}, intervening on the policy model's partial outputs as they unfold~\citep{zeng2024scaling}.



\subsection{Example of specificity Comparison}
\label{app: appendix_example}
Here we provide a comparison of features generated by gpt-4o-mini and o1-mini in response to the same user request, as shown in Figure~\ref{fig:4o_o1_comparison_example}. As highlighted in red, gpt-4o-mini produces a relatively vague feature—``Component Functionality''—while o1-mini yields more specific attributes, such as ``Component Type'', ``surface'', ``material'', and ``performance or usability''. This example illustrates how extended CoT reasoning in o1-mini can lead to more detailed feature descriptions.

% \subsection{Parameters for Packages}
% In this study, we used several established packages for evaluation, including Transformers (version 4.44.2) and PyTorch (version 2.4.0), among others. The complete list of packages and their settings can be found in the code repository.



% \input{figs/4o_o1_generated}
\onecolumn

\begin{lrbox}{1}
\begin{tcolorbox}[colback=green!5,colframe=green!35!black,boxrule=0.5pt,title={\textbf{Features generated by gpt-4o-mini}}]
{
"Instrument Quality": "Reflects the perceived quality and material of the instrument components based on user feedback. Possible values: `High', `Medium', `Low'",


"Usability": "Indicates how easy or difficult the instrument components are to use or install based on the user's experience. Possible values: `Easy', `Moderate', `Difficult'",

        
"Sound Characteristics": "Describes the tonal qualities and output levels of the instrument components, specifically for pickups. Possible values: `Bright', `Mellow', `Aggressive', `Balanced'",


"Brand Reputation": "The perceived trust and quality associated with specific brands based on user history. Possible values: `Highly Regarded', `Moderately Regarded', `Low Reputation'",


"User Preferences": "Denotes specific features that align with the user's musical style or performance preferences, like compatibility with certain styles. Possible values: `Suits Rock', `Suits Jazz', `Suits Metal', `Suits Blues'",


"Component Functionality": "Indicates the \textcolor{red}{functional aspects of components}, such as reliability and effectiveness in enhancing the overall performance of an instrument. Possible values: `Highly Functional', `Moderately Functional', `Low Functionality'"
}
\end{tcolorbox}
\end{lrbox}

\begin{lrbox}{2}
\begin{tcolorbox}[colback=green!5,colframe=green!35!black,boxrule=0.5pt,title={\textbf{Features generated by o1-mini}}]
{
"Component Type": "\textcolor{red}{The category of the instrument part}, indicating its primary function. Possible values include: Switch, Screws, Washer, Knobs, Pickups, Bridge Assembly, Strap Hardware, etc.",


"Aesthetic Finish": "The visual appearance and \textcolor{red}{surface treatment of the component}. Possible values include: Chrome, White, Black, Silver, Matte, Glossy, etc.",


"Material Composition": "\textcolor{red}{The materials} used in the construction of the component. Possible values include: Stainless Steel, Plastic, Felt, Metal Alloy, Aluminum, Brass, etc.",


"Functional Features": "Special features that enhance the \textcolor{red}{component's performance or usability}. Possible values include: Wax Potted, Heavy-Duty, Locking Mechanism, Adjustable, etc.",


"Sound Characteristics": "For components related to sound production, describes the tonal qualities. Possible values include: Aggressive, Clear, Loud, Obnoxious, Warm, Bright, etc.",


"Installation Complexity": "The ease or difficulty of installing the component. Possible values include: Easy, Moderate, Difficult, Requires Professional Installation, Tool-Free, etc.",

"Durability Enhancements": "Features that improve the longevity and resilience of the component. Possible values include: Corrosion-Resistant, Reinforced, Shock-Absorbing, Wear-Resistant, etc.",


"Brand Compatibility": "The compatibility of the component with specific brands or models. Possible values include: Fender Stratocaster, Gibson Les Paul, Universal Fit, Artist Signature Models, etc.",


"Customization Options": "Available options for personalizing the component. Possible values include: Number of Positions (e.g., 3-way, 5-way), Color Variants, Finish Types, etc.",


"Usage Context": "The intended use or suitable musical genres for the component. Possible values include: Rock, Metal, Jazz, Blues, Punk, Nu-Metal, etc."}
\end{tcolorbox}
\end{lrbox}


% \begin{figure*}[bp]
%     \centering
%     % \vspace{-10pt}
    
%     \resizebox{0.9\textwidth}{!}{
%     \usebox{1}
%     }
%     % \vspace{-15pt}
%     \caption{Features generated by gpt-4o-mini.}
%     \label{fig:feature4o}
%     \vspace{-15pt}
% \end{figure*}

% \begin{figure*}[bp]
%     \centering
%     % \vspace{-10pt}
%     \resizebox{0.9\textwidth}{!}{
%     \usebox{2}
%     }
%     % \vspace{-15pt}
%     \caption{Features generated by o1-mini.}
%     \label{fig:featuremini}
%     \vspace{-15pt}
% \end{figure*}

\begin{figure*}[htbp]
    \centering
    % \vspace{-10pt}
    
    \subfigure[]{
        \centering
        \resizebox{0.9\textwidth}{!}{
            \usebox{1}
        }
        \label{fig:feature_4o_example}
    }
    \hfill
    \subfigure[]{
        \centering
        \resizebox{0.9\textwidth}{!}{
            \usebox{2}
        }
        \label{fig:feature_o1_example}
    }

    \caption{Comparison of features generated by gpt-4o-mini and o1-mini.}
    \label{fig:4o_o1_comparison_example}
    % \vspace{-15pt}
\end{figure*}


\twocolumn




\end{document}
