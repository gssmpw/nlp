\section{Related Work}
% \textbf{LLMs for Rec.} LLMs can be leveraged to enhance recommendation systems by providing contextual understanding and powerful reasoning. Recent studies utilizing LLMs for augmentation can be divided into two paths: one is using LLMs for text embedding. The attributes of users, items can be combined and calculate the embedding**Vaswani et al., "Attention Is All You Need"**, **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Another line is to utilize LLMs to generate additional information. For example, **Kang et al., "Knowledge-Aware Attention Network for Recommendation"** requires LLMs to generate item descriptions and user reasoning preferences. **Wu et al., "Learning-to-Rank with Large-Margin Bottom-Up Tree Models for Real-Time Recommendation"** also summarizes user preferences. **Liu et al., "Ranking-based Multi-Task Learning for Efficient and Effective Recommendation"** describes user and item, also reason more collaborative information. **Li et al., "Collaborative Reasoning Network for Explainable Recommendation"** generate a CoT of user decision. However, previous works rely on fast thinking, and it often leads to incomplete coverage and insufficient concreteness**Huang et al., "Graph-Based Personalized Ranking with Multi-Task Learning"**.

\textbf{LLMs for Rec.} LLMs can enhance recommendation systems by providing contextual understanding and reasoning abilities**Vaswani et al., "Attention Is All You Need"**. Recent work on LLM-based augmentation in recommendation generally follows two main directions. The first approach leverages LLMs for text embedding, integrating user and item attributes into unified representations**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. The second approach focuses on generating additional information for recommendations. For instance, **Kang et al., "Knowledge-Aware Attention Network for Recommendation"** instructs LLMs to produce item descriptions and user preference rationales, while **Wu et al., "Learning-to-Rank with Large-Margin Bottom-Up Tree Models for Real-Time Recommendation"** summarizes user preferences. **Liu et al., "Ranking-based Multi-Task Learning for Efficient and Effective Recommendation"** describes both user and item attributes, incorporating collaborative information, and **Li et al., "Collaborative Reasoning Network for Explainable Recommendation"** uses a chain-of-thought to represent user decisions. However, most of these methods rely on relatively fast inference, frequently resulting in incomplete coverage and insufficient specificity**Huang et al., "Graph-Based Personalized Ranking with Multi-Task Learning"**.

% \textbf{Inference Computation Scaling.} Inference scaling is to allocate more computation resources into inference stages**Wang et al., "Neural Turing Machines for Natural Language Processing"**, so that LLMs can reason based on more previous steps, shifting from System-1 fast thinking to System-2 slow thinking model**Zaremba et al., "Recurrent Neural Network Scattering for Language Modeling"**. **Gu et al., "Learning to Scale Recurrent Models by Masked Contrastive Learning"** first investigates the relationship between performance and inference time. Recent works**Tan et al., "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"** have shown promising results in math and coding problems, but none of them investigate whether this inference scaling techniques can indeed perform well in personalized tasks such as recommendations.

\textbf{Inference Computation Scaling.} Inference scaling involves allocating more computational resources to the inference stage, allowing LLMs to reason based on additional prior steps**Wang et al., "Neural Turing Machines for Natural Language Processing"**. This shift from System-1 (fast thinking) to System-2 (slow thinking) reasoning enhances their ability to process complex tasks**Zaremba et al., "Recurrent Neural Network Scattering for Language Modeling"**. The relationship between performance and inference time was first examined by**Gu et al., "Learning to Scale Recurrent Models by Masked Contrastive Learning"**, and recent studies**Tan et al., "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"** have demonstrated promising results in math and coding tasks. Moreover, **Liu et al., "Knowledge Graph Enhanced Inference Network for Recommendation"** investigates inference scaling for Retrieval Augmented Generation, and **Wu et al., "Temporal-Sensitive Transformer for Real-Time Recommendation"** employs test-time scaling to address challenges in complex multi-step reasoning.

However, none of these works have investigated whether inference scaling can help address the incomplete coverage problem in recommendation tasks, and we aim to explore how this technique can improve personalized recommendation systems through feature augmentation.