[
  {
    "index": 0,
    "papers": [
      {
        "key": "Vaswani2017AttentionIA",
        "author": "Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin",
        "title": "Attention is All you Need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhou2021informer",
        "author": "Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai",
        "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting"
      },
      {
        "key": "wu2021autoformer",
        "author": "Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng",
        "title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2023itransformer",
        "author": "Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng",
        "title": "itransformer: Inverted transformers are effective for time series forecasting"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "nie2022time",
        "author": "Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant",
        "title": "A time series is worth 64 words: Long-term forecasting with transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "das2023decoder",
        "author": "Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen",
        "title": "A decoder-only foundation model for time-series forecasting"
      },
      {
        "key": "goswami2024moment",
        "author": "Goswami, Mononito and Szafer, Konrad and Choudhry, Arjun and Cai, Yifu and Li, Shuo and Dubrawski, Artur",
        "title": "Moment: A family of open time-series foundation models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "Vaswani2017AttentionIA",
        "author": "Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin",
        "title": "Attention is All you Need"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lu2022frozen",
        "author": "Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor",
        "title": "Frozen pretrained transformers as universal computation engines"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "shen2023cross",
        "author": "Shen, Junhong and Li, Liam and Dery, Lucio M and Staten, Corey and Khodak, Mikhail and Neubig, Graham and Talwalkar, Ameet",
        "title": "Cross-modal fine-tuning: Align then refine"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhou2023one",
        "author": "Zhou, Tian and Niu, Peisong and Sun, Liang and Jin, Rong and others",
        "title": "One fits all: Power general time series analysis by pretrained lm"
      },
      {
        "key": "jin2023time",
        "author": "Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others",
        "title": "Time-llm: Time series forecasting by reprogramming large language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Ding2022DeltaTA",
        "author": "Ning Ding and Yujia Qin and Guang Yang and Fu Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Haitao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juan Li and Maosong Sun",
        "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"
      },
      {
        "key": "Zhang2023LearnedAA",
        "author": "Yuming Zhang and Peng Wang and Ming Tan and Wei-Guo Zhu",
        "title": "Learned Adapters Are Better Than Manually Designed Adapters"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "aghajanyan-etal-2021-intrinsic",
        "author": "Aghajanyan, Armen  and\nGupta, Sonal  and\nZettlemoyer, Luke",
        "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "2018arXiv180408838L",
        "author": "{Li}, Chunyuan and {Farkhoor}, Heerad and {Liu}, Rosanne and {Yosinski}, Jason",
        "title": "{Measuring the Intrinsic Dimension of Objective Landscapes}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "2023arXiv230514314D",
        "author": "{Dettmers}, Tim and {Pagnoni}, Artidoro and {Holtzman}, Ari and {Zettlemoyer}, Luke",
        "title": "{QLoRA: Efficient Finetuning of Quantized LLMs}"
      },
      {
        "key": "PromptCBLUE",
        "author": "{Zhu}, Wei and {Wang}, Xiaoling and {Zheng}, Huanran and {Chen}, Mosha and {Tang}, Buzhou",
        "title": "{PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain}"
      }
    ]
  }
]