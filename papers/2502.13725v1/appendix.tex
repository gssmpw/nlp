%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}


% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Appendix for Time-LlaMA} 


% Single author syntax
\author{
    Author Name
    \affiliations
    Affiliation
    \emails
    email@example.com
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$\\
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation\\
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

\begin{document}

\maketitle



\section{Experimental settings}

Now we provide more details for the experiments presented in the main contents. 

\subsection{Implementation}

We mainly follow the experimental configurations in \cite{jin2023time} across all baselines within a unified evaluation pipeline in the Time-Series-Library\footnote{https://github.com/thuml/Time-Series-Library} for fair comparisons. We use Llama-2 7B \cite{Touvron2023Llama2O} as the default backbone model, unless stated otherwise. All our experiments are repeated three times and we report the averaged results. Our method is implemented on PyTorch \cite{paszke2019pytorch} with all experiments conducted on NVIDIA L20 GPUs (48 GB RAM). Our detailed model configurations will be 
, and our code will made available upon acceptance. 


% We provide additional technical details of TS-LlaMA on the output projection. After we feed the patch embeddings $\mathbf{O}^{(i)} \in \mathbb{R}^{P \times D}$ through the LLM backbone, we obtain the output representations, denoted as $\tilde{\mathbf{O}}^i \in \mathbb{R}^{P \times D}$:
% \begin{equation}
% \mathbf{H}^{i, 1} = \text{LLM}( \mathbf{O}^{(i)} ).
% \end{equation}
% Subsequently, in order to effectively reduce the dimension of the output layer, we project $\tilde{\mathbf{O}}^i$ into a lower dimension $D_1 < D$:
% \begin{equation}
% \mathbf{H}^{i, 2} = \text{Proj}_{1}( \mathbf{H}^{i, 1} ).
% \end{equation}
% The we follow PatchTST \cite{xxxx} and flatten $\mathbf{H}^{i, 2}$ into a 1D tensor with the length $P \times D_1$, which is then linear projected as $\hat{\mathbf{Y}}^i \in \mathbb{R}^H$:
% \begin{equation}
% \hat{\mathbf{Y}}^i = \text{Proj}_{2}( \mathbf{H}^{i, 2} ).
% \end{equation}



\subsection{Datasets}

We evaluate the long-term forecasting (ltf) performance on the well-established eight different benchmarks, including four ETT datasets  (including ETTh1, ETTh2, ETTm1, and ETTm2) from \cite{zhou2021informer}, Weather, Electricity, Traffic, and ILI from \cite{wu2021autoformer}. For short-term time series forecasting (STF), we employ the M4 benchmark \cite{makridakis2018m4}.


\noindent\textbf{ETT} The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment. This dataset consists of 2 years data from two separated counties in China. To explore the granularity on the Long sequence time-series forecasting (LSTF) problem, different subsets are created, {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point consists of the target value ”oil temperature” and 6 power load features. The train/val/test is 12/4/4 months.


\noindent\textbf{ECL} Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.This archive contains 2075259 measurements gathered in a house located in Sceaux (7km of Paris, France) between December 2006 and November 2010 (47 months).

\noindent\textbf{Traffic} Traffic is a collection of hourly data from California Department of Transportation, which describes the road occupancy rates measured by different sensors on San Francisco Bay area freeways.

\noindent\textbf{Weather} Weather is recorded every 10 minutes for the 2020 whole year, which contains 21 meteorological indicators, such as air temperature, humidity, etc. 

\noindent\textbf{ILI} The influenza-like illness (ILI) dataset
contains records of patients experiencing severe influenza with complications. 

\noindent\textbf{M4} The M4 benchmark comprises 100K time series, amassed from various domains commonly present in business, financial, and economic forecasting. These time series have been partitioned into six distinctive datasets, each with varying sampling frequencies that range from yearly to hourly. These series are categorized into five different domains: demographic, micro, macro, industry, and finance.


The datasets' statistics are presented in Table \ref{tab:dataset_stats}. 


\begin{table*}
\centering
\resizebox{0.92\textwidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{@{}ll|c|c|c|c|c}
\hline
\textbf{Tasks} & \textbf{Dataset} & \textbf{Dim.} & \textbf{Series Length} & \textbf{Dataset Size} & \textbf{Frequency} & \textbf{Domain} \\ 
\hline
\multirow{7}{*}{Long-term Forecasting} & ETTm1 & 7 & \{96, 192, 336, 720\} & (34465, 11521, 11521) & 15 min & Temperature \\
& ETTm2 & 7 & \{96, 192, 336, 720\} & (34465, 11521, 11521) & 15 min & Temperature \\
& ETTh1 & 7 & \{96, 192, 336, 720\} & (8545, 2881, 2881) & 1 hour & Temperature \\
& ETTh2 & 7 & \{96, 192, 336, 720\} & (8545, 2881, 2881) & 1 hour & Temperature \\
& Electricity & 321 & \{96, 192, 336, 720\} & (18317, 2633, 5261) & 1 hour & Electricity \\
& Traffic & 862 & \{96, 192, 336, 720\} & (12185, 1757, 3509) & 1 hour & Transportation \\
& Weather & 21 & \{96, 192, 336, 720\} & (36792, 5271, 10540) & 10 min & Weather \\
& ILI & 7 & \{24, 36, 48, 60\} & (617, 74, 170) & 1 week & Illness \\ \hline

\multirow{6}{*}{Short-term Forecasting} & M4-Yearly & 1 & 6 & (23000, 0, 23000) & Yearly & Demographic \\
& M4-Quarterly & 1 & 8 & (24000, 0, 24000) & Quarterly & Finance \\
& M4-Monthly & 1 & 18 & (48000, 0, 48000) & Monthly & Industry \\
& M4-Weakly & 1 & 13 & (359, 0, 359) & Weakly & Macro \\
& M4-Daily & 1 & 14 & (4227, 0, 4227) & Daily & Micro \\
& M4-Hourly & 1 & 48 & (414, 0, 414) & Hourly & Other \\ 
\hline

\end{tabular}}
\caption{Dataset statistics. The dimension indicates the number of time series (i.e., channels), and the dataset size is organized in (training, validation, testing).}
\label{tab:dataset_stats}
\end{table*}


\subsection{Evaulation metrics}

We now specify the evaluation metrics we used for comparing different models. We utilize the mean square error (MSE) and mean absolute error (MAE) for long-term forecasting. For the short-term forecasting task on M4 benchmark, we adopt the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA), following \cite{oreshkin2019n}. The calculations of these metrics are as follows:
\begin{align}
\text{MSE} &= \frac{1}{H} \sum_{h=1}^{T} (\mathbf{Y}_h - \hat{\mathbf{Y}}_h)^2,  \\
\text{MAE} &= \frac{1}{H} \sum_{h=1}^{H} |\mathbf{Y}_h - \hat{\mathbf{Y}}_h|,  \\
\text{SMAPE} &= \frac{200}{H} \sum_{h=1}^{H} \frac{|\mathbf{Y}_h - \hat{\mathbf{Y}}_h|}{|\mathbf{Y}_h| + |\hat{\mathbf{Y}}_h|},  \\
\text{MAPE} &= \frac{100}{H} \sum_{h=1}^{H} \frac{|\mathbf{Y}_h - \hat{\mathbf{Y}}_h|}{|\mathbf{Y}_h|},  \\
\text{MASE} &= \frac{1}{H} \sum_{h=1}^{H} \frac{|\mathbf{Y}_h - \hat{\mathbf{Y}}_h|}{\frac{1}{H-s} \sum_{j=s+1}^{H} |\mathbf{Y}_j - \mathbf{Y}_{j-s}|},  \\
\text{OWA} &= \frac{1}{2} \left[ \frac{\text{SMAPE}}{\text{SMAPE}_{\text{Naive}}} + \frac{\text{MASE}}{\text{MASE}_{\text{Naive}}} \right],  \\
\end{align}
where $ s $ is the periodicity of the time series data. $ H $ denotes the number of data points (i.e., prediction horizon in our cases). $ \mathbf{Y}_h $ and $ \hat{\mathbf{Y}}_h $ are the $ h $-th ground truth and prediction where $ h \in \{1, \cdots, H\} $.


\subsection{Configurations for training}

The Adam optimizer \cite{losh

chilov2017decoupled} is employed throughout all experiments. The loss objective is MSE for the long-term forecasting tasks, and SMAPE for the short-term forecasting tasks. The learning rate is denoted as $\text{LR}$. We utilize the LlaMA-2 7B \cite{Touvron2023Llama2O} model, maintaining the backbone model layers at 8 across all tasks. Denote the lookback window's length as $ T_L $, the prediction horizon as $T_P$. And the heads $ K $ correlate to the multi-head cross-attention utilized for time-series data reprogramming. For the LoRA modules, the number of ranks $r$ is set to $8$. Each Transformer block's LoRA router activates $n = 4$ LoRA modules. We detail the configurations for each task in Table \ref{tab:configurations}.  



\begin{table*}
\centering
\resizebox{0.98\textwidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
\toprule
\multirow{2}{*}{\textbf{Task-Dataset }} & \multicolumn{6}{c|}{\textbf{Model Hyperparameter}} & \multicolumn{4}{c}{\textbf{Training Process}} \\
\cmidrule(lr){2-7} \cmidrule(lr){8-11}
& \textbf{ Layers} & \textbf{ $ T_{L} $} & \textbf{ $T_P$} &   \textbf{ $K$ }      &     \textbf{ $r$ }    &  \textbf{ $n$ }    &     \textbf{LR*} & \textbf{Loss} & \textbf{Batch Size} & \textbf{Epochs} \\
\midrule
LTF - ETTh1 &  8 & 512 & \{96, 192, 336, 720\} & 8 &  8  &   4  &  $ 10^{-3} $ & MSE &  16  & 20 \\
% LTF - ETTh2 & 1000 & 16 & 512 & {96, 192, 336, 720} & 8 &  2 & $ 10^{-3} $ & MSE & 16 & 20 \\
LTF - ETTm1 &  8 & 512 & \{96, 192, 336, 720\} & 8 &  8  &    4 & $ 10^{-3} $ & MSE & 16 & 20 \\
% LTF - ETTm2 &  16 & 512 & {96, 192, 336, 720} & 8 &  2 & $ 10^{-3} $ & MSE & 16 & 20 \\
LTF - Weather &  8 & 512 & \{96, 192, 336, 720\} & 8 & 8  &    4 & $ 10^{-3} $ & MSE & 16 & 20 \\
LTF - Electricity &  8 & 512 & \{96, 192, 336, 720\} & 8 &  8  &   4 & $ 10^{-2} $ & MSE & 16 & 20 \\
LTF - Traffic &  8 & 512 & \{96, 192, 336, 720\} & 8 & 8  &    4 & $ 10^{-2} $ & MSE & 12 & 20  \\
LTF - ILI &  8 & 96 & \{24, 36, 48, 60\} & 8 & 8  &   4 & $ 10^{-2} $ & MSE & 16 & 20 \\
STF - M4 &  8 & $ 2 \times T_{P} $ &  \{6, 48\}  & 8   & 8  &    4  & $ 10^{-3} $ & SMAPE & 32 & 30 \\


\bottomrule

\end{tabular}}
\caption{An overview of the experimental configurations for TIME-LlaMA. LTF and STF denote long-term and short-term forecasting, respectively. }
\label{tab:configurations}
\end{table*}




%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

