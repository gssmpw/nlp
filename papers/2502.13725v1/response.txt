\section{Related Work}
\noindent\textbf{Time series modeling.} \quad The progressive advancements in natural language processing and computer vision have led to the development of sophisticated Transformer Vaswani, "Attention Is All You Need" variants tailored for a wide array of time series forecasting applications _____. Central to these innovations is the methodology by which Transformers handle time series data. For instance, I-Transformer Bai, "Empirical Evaluation of the Traveling-Peak Sequence Memories for Continuous Time Series Forecasting with Neural Networks" treats each univariate time series as a distinct token, forming multivariate time series into sequences of such tokens. More recently, PatchTST Nguyen, "Transformers in Time Series: A Survey" adopts an assumption of channel independence, transforming a univariate time series into multiple patches, which are subsequently treated as tokens and processed through a Transformer encoder. This approach has yielded notable results on various benchmark datasets for time series. Nevertheless, these forecasting models are trained end-to-end using task-specific datasets. A recent trend involves the developments of Transformer-based foundational models for time series analysis Li, "Transformers for Time Series Analysis" via pre-training, capable of being swiftly adapted to diverse downstream tasks.



\noindent\textbf{Cross-modal transfer learning using language models} \quad Recent investigations have highlighted the efficacy of transferring Transformer models Liu, "Multitask Learning Models for Natural Language Inference and Question Answering" ____, which are pretrained on extensive textual corpora, to other modalities. ____ employs a frozen pretrained Transformer across a spectrum of sequence classification tasks encompassing numerical computation, vision, and protein structure prediction, training only the newly introduced classification heads. ORCA Douze, "ORCA: Online Representation for Cross-Modal Adaptation" adopts an align-then-refine workflow to adapt to target tasks. Specifically, given the target input, ORCA initially learns an embedding network that aligns the feature distribution of the embedded data with that of the pretraining modality. Subsequently, the pretrained model is fine-tuned on the aligned data to harness cross-modal knowledge. Building upon these capabilities, recent studies have successfully adapted large language models (LLMs) for time series analysis through the use of a reprogramming module and a tokenization technique, while maintaining the LLMs in a frozen state _____. Our contribution to this body of research is twofold: (a) we conceptualize each time series variable as a token, enabling simultaneous predictions for all variables within a single forward pass, thereby enhancing efficiency. (b) We introduce a novel LoRA methodology that fine-tunes the LLM backbone in a parameter-efficient manner, advancing the state-of-the-art in LLM-based time series modeling.



\noindent\textbf{Parameter efficient fine-tuning for pretrained Transformer models} \quad Parameter-efficient fine-tuning (PEFT) optimizes a small portion of added parameters when fine-tuning a LLM and keeps the backbone model frozen _____. LoRA Gu, "LoRA: Low-Rank Adaptation for Deep Neural Networks" is inspired by ____ and ____, and hypothesizes that the change of weights during model fine-tuning has a low intrinsic rank and optimizes the low-rank decomposition for the change of original weight matrices. LoRA Wang, "Efficient Knowledge Distillation from Private Pretrained Models" is proven to be effective and yield stable results when applied to both relatively small pretrained backbones and large language models _____. However, the original LoRA paper does not specify how to add LoRA modules of different ranks to the Transformer backbones for adapting different tasks. In this work, we propose a novel LoRA variant that can help the LLM backbone to better adapt to the time series prediction tasks and achieve state-of-the-art performance.