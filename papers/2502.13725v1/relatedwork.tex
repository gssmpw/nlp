\section{Related Work}
\noindent\textbf{Time series modeling.} \quad The progressive advancements in natural language processing and computer vision have led to the development of sophisticated Transformer \cite{Vaswani2017AttentionIA} variants tailored for a wide array of time series forecasting applications \cite{zhou2021informer,wu2021autoformer}. Central to these innovations is the methodology by which Transformers handle time series data. For instance, I-Transformer \cite{liu2023itransformer} treats each univariate time series as a distinct token, forming multivariate time series into sequences of such tokens. More recently, PatchTST \cite{nie2022time} adopts an assumption of channel independence, transforming a univariate time series into multiple patches, which are subsequently treated as tokens and processed through a Transformer encoder. This approach has yielded notable results on various benchmark datasets for time series. Nevertheless, these forecasting models are trained end-to-end using task-specific datasets. A recent trend involves the developments of Transformer-based foundational models for time series analysis \cite{das2023decoder,goswami2024moment} via pre-training, capable of being swiftly adapted to diverse downstream tasks.



\noindent\textbf{Cross-modal transfer learning using language models} \quad Recent investigations have highlighted the efficacy of transferring Transformer models \cite{Vaswani2017AttentionIA}, which are pretrained on extensive textual corpora, to other modalities. \cite{lu2022frozen} employs a frozen pretrained Transformer across a spectrum of sequence classification tasks encompassing numerical computation, vision, and protein structure prediction, training only the newly introduced classification heads. ORCA \cite{shen2023cross} adopts an align-then-refine workflow to adapt to target tasks. Specifically, given the target input, ORCA initially learns an embedding network that aligns the feature distribution of the embedded data with that of the pretraining modality. Subsequently, the pretrained model is fine-tuned on the aligned data to harness cross-modal knowledge. Building upon these capabilities, recent studies have successfully adapted large language models (LLMs) for time series analysis through the use of a reprogramming module and a tokenization technique, while maintaining the LLMs in a frozen state \cite{zhou2023one,jin2023time}. Our contribution to this body of research is twofold: (a) we conceptualize each time series variable as a token, enabling simultaneous predictions for all variables within a single forward pass, thereby enhancing efficiency. (b) We introduce a novel LoRA methodology that fine-tunes the LLM backbone in a parameter-efficient manner, advancing the state-of-the-art in LLM-based time series modeling.




\noindent\textbf{Parameter efficient fine-tuning for pretrained Transformer models} \quad Parameter-efficient fine-tuning (PEFT) optimizes a small portion of added parameters when fine-tuning a LLM and keeps the backbone model frozen \cite{Ding2022DeltaTA,Zhang2023LearnedAA}. LoRA \cite{hu2021lora} is inspired by \cite{aghajanyan-etal-2021-intrinsic} and \cite{2018arXiv180408838L}, and hypothesizes that the change of weights during model fine-tuning has a low intrinsic rank and optimizes the low-rank decomposition for the change of original weight matrices. LoRA \cite{hu2021lora} is proven to be effective and yield stable results when applied to both relatively small pretrained backbones and large language models \cite{2023arXiv230514314D,PromptCBLUE}. However, the original LoRA paper does not specify how to add LoRA modules of different ranks to the Transformer backbones for adapting different tasks. In this work, we propose a novel LoRA variant that can help the LLM backbone to better adapt to the time series prediction tasks and achieve state-of-the-art performance.