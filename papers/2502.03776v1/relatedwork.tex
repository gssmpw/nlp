\section{Related Work}
\label{sec:related_works}
\subsection{Spectral-Based Methods}
\label{subsec:spectral_methods}
Spectral methods are grounded in the spectral theory and derive the low-dimensional embedding through eigenvalue decomposition. PCA~\cite{hotelling1933analysis} decomposes the covariance matrix, and its embedding maximizes the variance within the subspace, which is considered to be the largest shadow of the data. Multidimensional scaling (MDS)~\cite{torgerson1958theory} provides a general framework that utilizes a distance matrix to derive the low-dimensional embedding, and Isomap~\cite{balasubramanian2002isomap} is a variant of MDS that utilizes the geodesic distance. However, these methods do not consider the local structure, which results in crowded visualizations. Laplacian eigenmaps~\cite{belkin2003laplacian} serve as an intermediate method by decomposing the Laplacian of the neighborhood graph where the neighborhood graph is estimated based on the point-wise distance. The relationship between the Laplacian Eigenmaps and the neighbor embedding methods is discussed in the literature~\cite{bohm2022attraction}.

\subsection{Neighbor Embedding Methods}
\label{subsec:ne_methods}
Neighbor embedding methods attempt to minimize the loss of neighbor relations. t-SNE~\cite{van2008visualizing} is considered a gold standard that employs KL divergence as the loss function. The t-SNE embedding is much more discriminative than previous spectral-based methods; thus it is particularly effective for data visualization. Numerous variants of t-SNE have been developed, e.g., parametric~\citep{van2009learning}, triplet~\citep{van2012stochastic}, and computationally efficient extensions~\cite{van2014accelerating, fu2019atsne}. However, KL divergence requires normalization of the neighborhood graph, which complicates scalable optimization. UMAP~\cite{mcinnes2018umap} enhances the scalability of neighbor embedding algorithms by leveraging a fuzzy set cross-entropy loss that does not require normalization, similar to that of LargeVis~\cite{tang2016visualizing}, thereby reducing the computational complexity. Both t-SNE and UMAP have become standard baselines for DR-based data visualization. 
\par
Recent studies have investigated the relationship between neighbor embedding methods and their improvement. The attraction-repulsion framework~\cite{bohm2022attraction, jenssen2024map} and contrastive learning-based analysis~\cite{artemenkov2020ncvis, damrich2022t} are valuable for uncovering their relations. The major problem with neighbor embedding methods is their inability to preserve the global structure, which has been considered in efforts for improvement. For example, PaCMAP~\cite{wang2021understanding} introduces the middle neighbor similarity to preserve the global structure effectively. In addition, initialization-based improvements, particularly using PCA initialization, also play a crucial role~\cite{kobak2019art, kobak2021initialization}. However, while these improvements show promise, they remain limited. We incorporate the PCA embedding into the neighbor embedding to address this limitation.