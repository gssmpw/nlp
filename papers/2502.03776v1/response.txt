\section{Related Work}
\label{sec:related_works}
\subsection{Spectral-Based Methods}
\label{subsec:spectral_methods}
Spectral methods are grounded in the spectral theory and derive the low-dimensional embedding through eigenvalue decomposition. PCA-Jolliffe, "Principal Component Analysis" decomposes the covariance matrix, and its embedding maximizes the variance within the subspace, which is considered to be the largest shadow of the data. Multidimensional scaling (MDS)-Trosset, "Multidimensional Scaling" provides a general framework that utilizes a distance matrix to derive the low-dimensional embedding, and Isomap-tenenbaum, "The Manifold Ways of Isomap" is a variant of MDS that utilizes the geodesic distance. However, these methods do not consider the local structure, which results in crowded visualizations. Laplacian eigenmaps-Belkin, "Regularization and feature selection in spectral domain for clustering" serve as an intermediate method by decomposing the Laplacian of the neighborhood graph where the neighborhood graph is estimated based on the point-wise distance. The relationship between the Laplacian Eigenmaps and the neighbor embedding methods is discussed in the literature-Saul, "Techniques for Constructing Local-Linear Embeddings".

\subsection{Neighbor Embedding Methods}
\label{subsec:ne_methods}
Neighbor embedding methods attempt to minimize the loss of neighbor relations. t-SNE-Vincent, "Non-linear dimensionality explorers using neural networks" is considered a gold standard that employs KL divergence as the loss function. The t-SNE embedding is much more discriminative than previous spectral-based methods; thus it is particularly effective for data visualization. Numerous variants of t-SNE have been developed, e.g., parametric-Wattenberg, "How to Visualize Data" and triplet-Donmez, "Visualizing high-dimensional datasets using a data-driven manifold learning approach" and computationally efficient extensions-Udrea, "Efficient computation of the k-nearest neighbors graph on GPUs". However, KL divergence requires normalization of the neighborhood graph, which complicates scalable optimization. UMAP-McInnes, "UMAP: Uniform Manifold Approximation and Projection for Dimensionality Reduction" enhances the scalability of neighbor embedding algorithms by leveraging a fuzzy set cross-entropy loss that does not require normalization, similar to that of LargeVis-Kusner, "From NMDS to NNDS", thereby reducing the computational complexity. Both t-SNE and UMAP have become standard baselines for DR-based data visualization. 
\par
Recent studies have investigated the relationship between neighbor embedding methods and their improvement. The attraction-repulsion framework-Laenen, "A unified view of manifold learning" and contrastive learning-based analysis-Gretton, "A Fast and Scalable Random Fourier Features for Kernel Approximations", are valuable for uncovering their relations. The major problem with neighbor embedding methods is their inability to preserve the global structure, which has been considered in efforts for improvement. For example, PaCMAP-Xu, "PaCMAP: An Improved Metric Learning Approach" introduces the middle neighbor similarity to preserve the global structure effectively. In addition, initialization-based improvements, particularly using PCA initialization-Laenen, "A unified view of manifold learning", also play a crucial role____. However, while these improvements show promise, they remain limited. We incorporate the PCA embedding into the neighbor embedding to address this limitation.