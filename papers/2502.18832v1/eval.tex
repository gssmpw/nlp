\vspace{-8pt}
\section{Evaluation}
\label{sec:eval}
\vspace{-5pt}

We evaluate Rax in terms of its usability and performance (with
    both macro and micro benchmarking).

\vspace{-4pt}
\subsection{Usability}
\vspace{-3pt}

Measuring usability is challenging.
We evaluate Rax in two ways: (1) heuristic evaluation on whether it saves workarounds to
    the \gap{}, % (\S\ref{sec:motivation}),
    and (2) our dogfooding experience of using Rax to implement a large, complex extension (BMC~\cite{BMC}).
%    and (3) classroom experience.
Overall, we find that Rax enables developers to write simpler and cleaner code.

% We found the use of Rust in \projname{} eliminates the need to depend on a separate
%    in-kernel verifier as in eBPF and, therefore, it is able to address the
%    usability issues directly caused by the verifier.
% At the same time, the rich builtin functionality from a high-level language
%    also allows developers to write simpler and cleaner code in real world use
%    cases.
%We also take BMC~\cite{BMC} and implement it in \projname{} as a case study.

\para{Eliminating workarounds.}
Since Rax introduces no \gap{}, none of the workarounds in \S\ref{sec:motivation} is
    needed in writing Rax extensions.
% We now revisit the verifier issues identified in our motivational study
%    (\S\ref{sec:motivation}) and demonstrate how \projname{} may solve them.

\begin{packed_itemize}
\item Rax extensions have no limit on program size and complexity.
    There is no need to artificially refactor extension programs into
    smaller or simpler ones (\S\ref{motivation:restructure}).
\item There is no need to artificially make Rax extensions verifier-friendly (\S\ref{motivation:llvm-codegen}).
In fact, by decoupling static analysis from the kernel,
    Rax can enable new analysis
%    (e.g., by allowing compilers to optimize
%    for verification instead of performance~\cite{wagner:hotos:13}).
    (e.g., by allowing compilers to optimize for extra analysis/verification~\cite{wagner:hotos:13}).
% \jinghao{The point here is confusing, we don't ever need to optimize for verification.}
\item For the same reason, developers no longer need to tweak code to assist verification (\S\ref{motivation:add-code}).
\item Developers no longer need to manage different verifier bugs across
    kernel versions (\S\ref{motivation:kernel-version}). \new{The Rust compiler
    can have bugs and break safety guarantees, but it is arguably easier to
    upgrade than the kernel for fixes.}
    % but compiler bugs arguably
    % do not affect usability.
\item Rax enables developers to use rich builtin intrinsics defined by the Rax toolchain
    without reinventing wheels (\S\ref{motivation:wheel}).
\end{packed_itemize}

% \jinghao{Add some sentence saying this is a prevalent pattern in BMC?}
% \jinghao{Can we make the claim that this complexity is for passing the
%     verifier?}
% During implementation of function \texttt{bmc\_invalidate\_cache},
%     we assume that each pcket

% This discovery highlighted the obscurity and susceptibility to errors inherent
%     in the original code structure, primarily due to the complex judgment
%     conditions buried within nested if statements.

% Rax-BMC
% 1. eliminates verifier-related checks
% 2. simplifies the code bc of Rust core library

\para{Case study: \projname{}-BMC}
\label{eval:bmc-case-study}
We rewrite BMC~\cite{BMC} as a Rax kernel extension (\projname{}-BMC), which
    was originally written in eBPF extensions (eBPF-BMC).
Rax-BMC is not a line-by-line translation of eBPF-BMC, % the original BMC, referred to as eBPF-BMC
% \ayushb{"Rewrite" - not a line-by-line translation of the eBPF-BMC, but closer to the intended BMC},
    because Rax provides more friendly
    programming experience (e.g., no need to split programs due to the verifier limit; see \S\ref{motivation:restructure}).
In this section, we discuss Rax-BMC from the usability perspective
    and measure its performance in \S\ref{eval:macro}.

Our experience shows that Rax enables cleaner and simpler extension code,
    compared to eBPF.
Essentially, Rax enables us to focus on key program logic without the overhead
    of passing the verifier.
For example, we no longer need to divide code into in parts,
    add auxiliary code to help the verifier,
    dealing with tail calls and state transfer, etc.
In addition, we can directly use Rust's builtin language features
    and libraries (e.g., iterators and closures).
As one metric,
    Rax-BMC is written in 326 lines of Rust code.
    In comparison, eBPF-BMC is written in 513 lines of C code (splitting into seven programs).

% \jinghao{Original BMC paper reported 513 LoC -- but they completely have no
%    formatting. We formatted both eBPF BMC and Rust BMC with the 80-column rule.
%    After we format their original code it has 598 LoC, and our porting process
%    only added 2 actual LoC.}
% We re-implement BMC~\cite{BMC} in \projname{} (\projname{}-BMC) to demonstrate
%    the enhanced usability, and later, its performance (\S\ref{eval:macro}).
% The resulting program is not a direct translation from the original eBPF
%    version in C, rather, we implement the same high-level logic but with
%    slight deviations from BMC where the enhanced usability and expressiveness
%    of Rust allows a simpler implementation.
% \projname{}-BMC is able to avoid the complexities caused by the verifier.

Figure~\ref{fig:rust-code} compares the code snippets of eBPF-BMC and Rax-BMC that implement
    cache invalidation, respectively, as a qualitative example.
% The comparison between the cache invalidation code in eBPF
%    (\S\ref{motivation:restructure}) and \projname{} is shown in
The checks in eBPF-BMC code, required by the eBPF verifier, including these for offset and
    \texttt{\small data\_end} limits, are now being enforced via the inherent language
    features of Rust, such as slices with bound checks in Rax (L2 and L10).
The check on \texttt{\small BMC\_MAX\_PACKET\_LENGTH}, which serves as a constraint to
    minimize the number of jump instructions to circumvent the eBPF verifier,
    is no longer needed.
% At the same time, specific checks such as the ones for identified SET commands
%     and the \texttt{key\_found} state can implemented with built-in
%     functions and closures in an easy and clean way (L4-L6 and L11).
Other checks for identified SET commands and loops states can be implemented
    with built-in functions and closures in an easy and clean way
    (L4--L6 and L11).
% \jinghao{ We should also talk about \texttt{BMC\_MAX\_PACKET\_LENGTH}.}
% Meanwhile, other checks, including those for offset and \texttt{data\_end} limits, are being enforced via
%     the inherent language features of Rust, such as the \texttt{slice} that implements bound checks.

% \jinghao{Need a few sentences to explain why a bunch of checks are no
%     longer needed and why we are safe even without these checks.}
% \jinghao{The four levels of nesting in the original code is significantly...}
% \juowen{break as two paragraphs}

% For the loops in the original implementation, \projname{}-BMC utilizes the
%     lazily evaluated \texttt{windows}, \texttt{enumerate} and
%     \texttt{filter\_map} from slice iterators.
% % Refer back to BMC code, the similar logic is implemented in \projname{}-BMC as shown in Figure~\ref{fig:rust-code}.
% These methods split the whole payload into chunks
%     of 4 bytes, and collects the results for which the chunk equals the keyword
%     of SET commands into iterators.
% % \jinghao{What are these challenges -- are we still in the context of cache
% %     invalidation? By the way, can we abstract this out to all ``loops'' in BMC?
% % At the same time, we probably need to explain what \texttt{filter\_map} does.}
% % This approach facilitates the creation of an iterator, which is instrumental in identifying
% %     the quantity of SET commands present in the current packet payload.
% With the creation of the iterators, the number of SET commands present in the
%     current packet payload can be easily identified.
% Further enhancing the ease of programming and the readability, Rust's syntactic sugar is employed for
%     iterating over the identified memcached SET commands, streamlining the process.
% % \jinghao{The four levels of nesting in the original code is significantly...}
% The four levels of nesting in the original code is significantly reduced by
% converting a \texttt{for}-loop with intricate conditions into a clean chain
%     of higher-order functions with closures.
% % \jinghao{lambda function -> a clean chain of higher-order functions with
% %     lambda function?}
% This conversion is achieved through the utilization of \texttt{take\_while}.
%     With the iterator from \texttt{filter\_map}, \texttt{take\_while} will
%     filter the memcached SET key from the payload, thus dividing the code into three distinct
%     sequential parts and markedly improving its expressiveness.

% \juowen{ How to connect what we mentioned before ralated to limit of verifier? }
% \jinghao{I think the current writing is good, just need some more polishing}

% In the \projname{}-BMC, constraints associated with the verifier are nonexistento.
Moreover, with the elimination of program size and complexity limits in \projname{}-BMC,
% Therefore, program doesn't have to put the metadata in a seperated map,
%     which significantly obscure the logic of program and may undermine the performance.
    developers no longer have to save the computation state in a map
    across tail calls, which leads
    to clearer and more efficient implementation.

% \jinghao{@Ruowen pls take a look.}
% \juowen{It feels weird leaving this thing here, feels like not convincing}

% \jinghao{Can we also write maybe one paragraph on the need to store the parsing
%     context across tail calls?}
% \jinghao{Also should explain what \texttt{take\_while} does.}
% \jinghao{Now it feels like we should make part larger, i.e., include code
%     examples and show how Rust helps with that.
% We can then move this part into the new qualitative evaluation.}

% FA22: 19 tried, 12 full score, 19 partial credit
% FA23: 48 tried, 20 full score, 46 partial credit

% Employed Rax to the OS class for two semesters
%   first semester they ported the eBPF tracex5 sample to Rax
%   second semester they implemented a packet filter with Rax, which drops
%     incoming network traffic based on predefined rules for port numbers and
%     protocol types
% For one semester the students are asked to implement a packet filter with
%    \projname{}, which incoming network traffic based on predefined rules; for
%    the other semester the students ported an existing eBPF kprobe program
%    (tracex5 from kernel samples) to \projname{}.


% \ayushb{Having data and commenting on similar experience for current eBPF would be a great result as well, is it possible for us to get that from some other case studies (maybe from other schools)?}
% (tianyin) brilliant idea but I don't think we have it now, and that's our future work.
    %

Note that the usability benefit does not
    come from the expressiveness difference between Rust and C, but from the
    closing of \gap{} via
    \projname{}.
Evidently, the cleaner code of \projname{}-BMC would fail the
    verifier if it were to be compiled into eBPF (e.g., via Aya~\cite{aya-rs}):
    the compiler is unable to generate verifier-friendly code for convenient
    language features such as slices, and the verifier
    complexity limits will always be an issue.
\projname{} allows us to fully leverage Rust's expressiveness
    without being constrained by verification issues.
% \new{
% % \label{eval:aya-case-study}
% Aya is an eBPF library for the Rust programming language.
% However, there remains a gap between the language and the verifier.
% Compared to C-based eBPF development, Rust's advanced language features often
%     fail to comply with the verifier's stringent requirements.

% In aya, while it is possible to convert packets into slices to use Rust's
%     helper functions for improved code readability and reduced bound checks, the
%     verifier rejects this approach.
% Attempts to convert packet data into slices can result in errors such as
%     "invalid access to packet," thereby preventing the program from loading.
% Moreover, in aya, use code that generates panic checks will fail to pass verifier,
%     displaying the error: "last insn is not an exit or jmp".
% A common instance is indexing a slice.

% Based on the experience with aya, the benefits of \projname{} do not barely
%     come from the programming language itself but rather from its safety design.
% \projname{} allows us to fully leverage Rust's expressiveness without being
%     constrained by verifier limitations.
% }

% Within \projname{}-BMC, packet data was converted into Rust slices, enabling
% the use of iterators and closures. These features significantly enhanced
% program readability and eliminated redundant bound checks. However, such
% transformations are not feasible in aya due to verifier limitations.
\vspace{-4pt}
\subsection{Macro benchmark}
\label{eval:macro}
\vspace{-3pt}

Rax's usability benefits do not come with a performance cost.
We show that Rax extensions deliver comparable performance as eBPF extensions.
% We now demonstrate that enhanced usability of \projname{} does not come at a
%    cost of performance.
% Through our \projname{}-BMC implementation, we show that \projname{} can handle
%    such complicated and performance-sensitive use cases from the real world.
\projname{}-BMC achieves a throughput of 1.98M requests per
    second (RPS) on 8 cores, which is slightly higher than eBPF-BMC (1.92M).


% \subsubsection{\projname{}-based BMC}
% \jinghao{TODO: Preamable}
% BMC implements in-kernel memcached cache -- how it works
% Compilcated program, original paper splits into 7 programs and uses tail calls
%

%\para{Experiment setup}
% Our evaluation setup consists two machines, with one
%     acting as the server and the other one acting as the client.
Our setup consists of a server machine and a client machine.
The server machine runs the \projname{} custom kernel based on Linux v6.11.0 on
    an AMD EPYC 7551P 32-Core processor with 112 GB memory without SMT and
    Turbo.
% SMT and Turbo are turned off for the experiments.
The client machine runs a vanilla v6.11.0 Linux kernel on an AMD Ryzen 9 9950X
    processor with 96 GB memory.
Both machines are equipped with Mellanox ConnectX-3 Pro 40GbE NICs and are
    connected back-to-back using a single port.

% Key: 16 bytes, value: 32 bytes
% 50 million with Zipf 0.99
% 10 GB memcached, 2.5 GB BMC
% Preload all keys
% GET:SET 30:1

% We use the following workload to evaluate our \projname{}-BMC together with the
%     original eBPF-BMC.
% Our dictionary contains 50 million Memcached keys following a Zipf
%     distribution with 0.99 skewness.
% All keys are 16 bytes in size and are paired with 32-byte values in the
%     experiments.
% The storage sizes of the Memcached and BMC are set to 10 GB and 2.5 GB,
%     respectively.
% According to the calculation in the original work~\cite{BMC}, all items
%     can be stored in Memcached itself but only 6.3 million of them can fit in
%     BMC.
% Before experiments, the Memcached server is pre-loaded with all keys by
%     sending TCP SET requests for each key in the dictionary from the client.
% The client then sends requests to the server with a 30:1 ratio betweren UDP GET
%     and TCP SET requests and measures the throughput.

We evaluate the throughput of (1) Memcached which binds
    multiple UDP sockets to the same port~\cite{BMC},
    (2) Memcached with eBPF-BMC,
    and (3) Memcached with \projname{}-BMC.
% The original eBPF-BMC code targets Linux kernel version 5.3.0 and we
%     port it to the v5.15.0 \projname{} kernel for this evaluation.
% For each setup, we vary the number of processor cores and the number of threads
%     used by the Memcached server
% and pin each thread onto each core.
% We also adjust the CPU affinity of IRQs associated with the NIC such that the
%     network interrupts are processed on the same set of cores the Memcached
%     server executes on.
For each setup, we vary the number of CPU cores for Memcached server and
    NIC IRQs and pin one Memcached thread onto each available core.
We use the same workloads as in BMC~\cite{BMC}, albeit with a smaller
    number of Memcached keys.
% \jinghao{I cannot remember exactly, probably because we had some OOM issues with
% 100 million keys, therefore we reduced it to 50 million (which takes ~30GB on
% client and at that time we only have 64GB). But now we have 96GB so I think we
% can try this again. Update: Ruowen optimized the benchmark a little bit and we
% can actually fit 100 million keys (and with 100 million reqs each round) in 96GB.}

\begin{figure}
    \includegraphics[width=0.9\linewidth]{figs/bmc.pdf}
    \centering
    \vspace{-2pt}
    \caption{Throughput of Memcached, with eBPF-BMC, and with \projname{}-BMC
        under different number of cores.
    }
    \label{fig:eval-bmc}
    \vspace{-5pt}
\end{figure}

Figure~\ref{fig:eval-bmc} shows the throughput of the three setups under
    different numbers of CPU cores.
Memcached processes all requests in userspace with the overhead of the kernel network stack,
    achieving only 37K RPS on a single core and 365K RPS on 8 cores.
Both eBPF-BMC and \projname{}-BMC
    achieve a much higher throughput as they process a large
    fraction of requests at NIC driver level without going through
    the kernel network stack.
With 8 cores, eBPF-BMC and \projname{}-BMC achieve a throughput
    of 1.92M and 1.98M, and a performance benefit of 5.26x and 5.43x, respectively.
% It is clear that \projname{} is able achieve a better usability while
%    keeping the same level of performance comparing to
%    eBPF, as the throughput of \projname{}-BMC is comparable to that of
%    eBPF-BMC under all CPU/thread setups.
% \tianyin{If we claim a higher performance, we must explain why.}
The slight performance improvement over eBPF is attributable to the
    elimination of overheads of tail calls and associated state-passing
    via maps, %between the refactored parts of the program,
    along with optimizations in the rustc frontend and
    x86 backend, % which collectively optimize execution efficiency,
    despite the overhead of additional runtime checks.
%The slight higher performance from \projname{} is likely because of the better
%    optimization job performed by the rustc frontend and the LLVM code
%    generator.
%\ayushb{Reasoning doesn't seem convincing. Could it be because there is no overhead of state transfers between tail calls as well ?}

\vspace{-4pt}
\subsection{Micro benchmark}
\vspace{-3pt}

%Though not visible in \projname{}-BMC,
Several of Rax's designs could introduce overheads,
    despite invisible in the \projname{}-BMC evaluation.
We use microbenchmarks specifically designed to stress our design and measure
    overheads.
We show that overheads exist
    in some pessimistic cases, but have negligible impact in real-world scenarios.
All experiments are performed on the same machine that acts as the server in
    the \projname{}-BMC experiments (\S\ref{eval:macro}).
% \jinghao{TODO: need to update certain discussions for the new results}

% \subsubsection{Static memory footprint}
% \label{eval:mem-footprint}
% \begin{figure}[t]
%     \includegraphics[width=1.0\linewidth]{figs/mem.pdf}
%     \centering
%     \vspace{-25pt}
%     \caption{Amount of pages allocated and actual memory occupied by various
%         eBPF and \projname{} programs}
%     \label{fig:eval-mem-footprint}
%     \vspace{-10pt}
% \end{figure}
% % definition of memory footprint
% % eBPF only have JIT code
% % Rust have code sections and other sections, e.g. data and GOT sections
% We evaluate how the in-kernel memory footprint of \projname{} programs
%     compares to that of eBPF programs.
% We found that \projname{} incurs a fair amount of overhead on the memory
%     footprint when there are only few programs defined and compiled together,
%     due to how program code and data are organized in the executable.
% However, with more programs compiled in the same executable, the overhead
%     becomes amortized and the overall memory footprint becomes more efficient.
%
% We define the memory footprint as the number of static memory pages required
%     for program execution.
% For eBPF, this only includes the post-JIT native code, since eBPF program does
%     not support static data sections.
% % For \projname{} this includes all load segments from the compiled ELF
% %     executable, which consists not only the text sections, but also the data
% %     sections for static variables (e.g., program objects and maps objects) as
% %     well as the sections that implement support for position-independent code
% %     (e.g., GOT sections).
% For \projname{}, this includes all \texttt{LOAD} segments from the compiled ELF
%     executable.
% A compiled \projname{} program generally has four \texttt{LOAD} segments that
%     map to relocations, text, read-only data, and the GOT.
% %     map to different sections (e.g. text, rodata, etc).
%
% In this experiment, we compare the memory required for both
%     \projname{} programs and their equivalent eBPF programs.
% We select 3 simple eBPF programs from the sample eBPF programs shipped with the
%     kernel -- the trace point program \texttt{syscall\_tp}, the kprobe program
%     \texttt{tracex5} and the perf event program \texttt{trace\_event}.
% We also include the BMC program (\S\ref{eval:bmc-case-study}) in the
%     experiment because it represents an example of a more complicated use case.
% We implement a \projname{} version for each of the programs.
%
% The number of pages allocated for and the actual amount of memory used by the
%     eBPF programs and their \projname{} counterparts are shown in
%     Figure~\ref{fig:eval-mem-footprint}.
% % In general, the memory footprint becomes more efficient for \projname{} when
% %     more programs are defined in the same source file. \jinghao{or should we
% %     say ``project''}
% Before Linux 5.18, each eBPF program occupied dedicated pages.
% eBPF then introduced the ``packed allocation''
%     feature~\cite{bpf-packed-alloc}, which
%     tries to pack different programs into the same page to use memory more
%     efficiently.
% % and is labeled in Figure~\ref{fig:eval-mem-footprint} as
% %     ``BPF-Packed''.
% For \projname{}, multiple programs in the same project are
%     loaded together in the same executable.
% However, different segments of the executable have to reside in their own
%     pages since they require different memory permissions.
%
% Because each segment of a \projname{} program needs its own page, \projname{}
%     exhibits a higher static memory footprint for all programs when compared to
%     eBPF with packed allocation.
% When comparing to eBPF without packed allocation, \projname{} is more efficient
%     when more programs are defined in the same project.
% \texttt{trace\_event} represents a case where only a single extension program
%     is defined.
% For such cases, \projname{} would exhibit a larger memory footprint, because of
%     its extra segments in addition to code.
% %     its executable contains other sections (e.g., data and relocations) in
% %     addition to code.
% For \texttt{tracex5} and \texttt{syscall\_tp}, eBPF and \projname{} uses the
%     same number of memory pages
%     since both of them consist of 4 programs, and one page per program is
%     allocated without packed allocation.
% For BMC, \projname{} achieves a more efficient memory footprint.
% The original eBPF-BMC has to be split into 7 programs in order to keep it
%     within the limit of the verifier.
% Doing so without packed allocation makes each program occupy its own pages.
% The \projname{}-BMC has all the code packed in the text section, and therefore
%     utilizes the memory more efficiently.
%
% % \jinghao{TODO: Might want to discuss the larger ocupied memory in \projname{} than eBPF}
%
% % \begin{itemize}
% %     \item need a figure: bar graph showing the memory footprint of bpf and
% %         \projname{} programs on our sample programs
% %     \item sample programs: \texttt{syscall\_tp}, \texttt{tracex5},
% %         \texttt{trace\_event}, and BMC.
% %     \item y: total number of pages allocated for the loaded object (JIT-ed code
% %         for BPF, loaded and mapped pages for Rust)
% % \end{itemize}

\para{Setup and teardown.}
% The use of a dedicated stack in \projname{} does
%    not incur significant overhead on program execution.
%startup and exit, with the
%    runtime of an empty \projname{} staying within 1ns of the runtime of an
%    empty eBPF program.
% \projname{}'s usage of a dedicated stack for execution and the implementation
%     of exception handling support may incur additional overhead during program
%     startup and shutdown.
Entering and exiting a \projname{} program requires \projname{}-specific
    operations %in the dispatcher
    (Figure~\ref{fig:eh-overview}).
%\projname{} uses a dedicated stack. It needs to save the stack
%    and frame pointer and set them to the dedicated stack before executing
%    the extension,
\projname{}'s use of a dedicated stack requires saving the stack
    pointer and setting the new stack and frame pointer to the dedicated stack
%replacing them
%    with the top address of the dedicated stack before starting the program,
%     and restoring the saved stack and frame pointer after the program exits.
    (and restoring to the saved values after the extension exits).
\projname{} also needs to set up the per-CPU state used by its
    termination mechanism (\S\ref{principle:termination}).
%, which includes the information of the current program and its start time.
%Rax adds six more memory instructions to
%    the dispatcher function (Figure~\ref{fig:eh-overview}).
In total, these operations add eight instructions on the
    execution path in \projname{}.
%     besides the instructions
%    needed to conform to the x86 calling convention.
% (tianyin) It's really a bad writing practice to add smoke; if you want to say there are more, make it clear; if it is irrelevant, then don't mention it.
% In this experiment, we measure the overhead of these stack pointer operations.
To measure the overhead, we implement an empty extension program in both
    eBPF and
    \projname{} and record their execution time (including the program dispatcher).
As shown in Table~\ref{tab:startup-cleanup}, the measured execution time
    of the empty \projname{} and eBPF
    programs only differ in around a nanosecond on average.
% \new{
% We attribute the small difference to the optimizations within the processor
%    (e.g., superscalar, out-of-order execution, etc).
% }
%\jinghao{10 more memory instructions is actually a lot, I honestly don't know
%why we are not seeing much overhead here. Maybe things will change once we run
%all eBPF experiment on vanilla kernel.}

% (tianyin) this should be a TODO to prepare for the rebuttal.

\para{Exception handling}
\label{eval:termsupport}
Rax's safe cleanup for exception handling requires recording allocated
    resource at runtime (\S\ref{principle:eh}), which, compared to eBPF, adds overhead.
We measure the overhead using a program that acquires and then
    immediately releases an eBPF spinlock.
Since the acquired spinlock needs to be released upon Rust
    panics, \projname{}'s cleanup mechanism records it in its per-CPU
    buffer.
Additionally, \projname{} sets up a per-CPU state flag to indicate
    execution of a helper function (\S\ref{principle:termination}).
The program is implemented in both eBPF and \projname{} and the time used to
    acquire and release the spinlocks are measured.
% \new{
Table~\ref{tab:startup-cleanup} shows that the runtime difference between
    eBPF and \projname{} is roughly 50 nanoseconds.
% The cleanup mechanism for exception handling and runtime stack instrumentation
%     could incur additional performance overheads.
% Here, we evaluate the performance impact from these runtime instrumentations.
% We evaluate the overhead of Rax runtime for
%    exception handling and stack checking.

% \new{
% Our result shows that these runtime mechanisms on non-exceptional execution adds
%    $\sim$50 nanoseconds overhead, and that recursions with runtime
%    stack checks is more efficient than the same logic
%    implemented in eBPF using tail calls.
% }

% \para{Cleanup overhead on normal execution path:}
% \begin{table}[t]
%     \small
%     \centering
%     \begin{tabular}{cc}%{|p{6cm}|p{1cm}|}
%         \toprule
%         \textbf{Extension} & \textbf{Runtime of acquiring \& release a lock (ns)} \\
%         \midrule
%         eBPF & 256.8 $\pm$ 142.4 \\
%         \projname{} & 254.5 $\pm$ 192.3 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Runtime of spinlock acquire and release for eBPF and \projname{}}
%     \vspace{-10pt}
%     \label{tab:cleanup-overhead}
% \end{table}

\begin{table}[t]
    \small
    \centering
    \vspace{-10pt}
    \caption{Time to execute an empty extension program and
        to acquire and release a spinlock in eBPF and \projname{} (nanosecond)}
    \begin{tabular}{ccc}%{|p{6cm}|p{1cm}|}
        \toprule
        \textbf{Extension} & \textbf{Empty prog runtime} & \textbf{Spinlock runtime} \\
        \midrule
        eBPF & 42.1 $\pm$ 4.1 ns & 130.4 $\pm$ 20.3 ns\\
        \projname{} & 42.6 $\pm$ 5.8 ns & 183.1 $\pm$ 27.5 ns\\
        \bottomrule
    \end{tabular}
    \label{tab:startup-cleanup}
\end{table}


% We do note that, however, the spinlock is never contended in the experiment.
% Lock contentions in practice should effectively amortize the additional overhead
%    from \projname{}'s runtime mechanism.
% \jinghao{Probably a bit weak here, since contention is specific to locks}
% }
% with \projname{} being the faster one, \jinghao{We have a large stdev}
%    implying the overhead of \projname{}'s cleanup mechanism is
%    negligible.

\para{Stack check.}
% \para{Stack-check overhead:}
%\begin{figure}[t]
%    \includegraphics[width=1.0\linewidth]{figs/recursive.pdf}
%    \centering
%    \vspace{-25pt}
%    \caption{Runtime of eBPF tail calls and \projname{} recursive calls with
%    stack instrumentation}
%    \label{fig:eval-recursion}
%    \vspace{-10pt}
%\end{figure}
% We then evaluate how much overhead the runtime stack check have on the
%     performance of \projname{} programs.
Stack checks are added before
    function calls in \projname{} extensions that contain indirect
    or recursive calls (\S\ref{principle:stack}).
% that prevent the compiler from calculating the stack usage statically.
We implement recursive extension programs in both
    eBPF and \projname{} to measure the overhead.
The recursive function calls itself for a controlled number of times.
% with no other code.
In \projname{}, we pass the call depth as the argument to the recursive
    function;
since eBPF does not support recursive functions, we use eBPF tail
    calls to implement the logic---since it is inconvenient to pass arguments to
    tail-called programs (\S\ref{motivation:restructure}), we use a static
    variable to set the call depth.
% In \projname{}, we use the \texttt{black\_box} hint from the Rust core library
%     to prevent the compiler from optimizing away the recursive call.
Figure~\ref{fig:eval-recursion} plots execution time of the recursive programs with
    call depths from 1 to 33 (eBPF cannot do more than 33 tail calls).
% The kernel imposes a hard limit of 32 on the number of tail calls an eBPF
%     program can perform.
\projname{} is roughly 3x faster than eBPF.
The overhead of eBPF is due to runtime check on tail-call
    count limit and accessing the static variable,
    which is a map in eBPF % and requires a memory access
    (not a register in normal calls).


% The raw data and script used to make the graph (map_bench.py)
% is in the map_bench directory under scripts
%\begin{figure}[t]
%    \includegraphics[width=1.0\linewidth]{figs/map_bench.pdf}
%    \centering
%    \vspace{-25pt}
%    \caption{Runtime of map lookups on various setups}
%    \label{fig:map-bench}
%    \vspace{-12pt}
%\end{figure}


\begin{figure}[t]
    \vspace{10pt}
    \centering
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/recursive.pdf}
        \caption{eBPF tail call and \projname{} recursive call time}
        \vspace{-10pt}
        \label{fig:eval-recursion}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/map_bench.pdf}
        \caption{Map lookup time under various setups}
        \vspace{-10pt}
        \label{fig:map-bench}
    \end{minipage}
\end{figure}

% \begin{itemize}
%     \item inline v.s. out-of-line map helpers
%         \begin{itemize}
%             \item table showing runtime of a map lookup operation in a eBPF and
%                 a Rust program
%             \item the kernel will automatically inline lookup in BPF
%             \item Rust version will not get inlined
%             \item measurement can be in ns for now, but better if it could be
%                 in cycles
%             \item maps to test: array, hash (htab-lru, xsk-map?)
%         \end{itemize}
% \end{itemize}
% We evaluate the impact of missed optimization opportunities due to the elimination of
%     the eBPF verifier in \projname{} by looking into the eBPF map lookup
%     helpers.

\para{Map access}
\label{eval:map}
Map access in Rax is expected to have more overhead than in eBPF.
First, \projname{} implements wrapping code to enforce safety
    of helper function calls (\S\ref{sec:impl}).
% extensions call helpers safely.
Moreover, the eBPF JIT compiler inlines the helper function for map lookup
    at load time
    as a performance % by replacing the call with equivalent eBPF instructions to
    optimization;
    % avoid additional function call and memory-accessing overheads.
% This optimization can avoid two function calls and two pointer dereferences for
%     map types that support lookup inlining.
however, inlining is not available in \projname{} (no JIT in Rax).
    % as well as kernel internal information (e.g. map addresses).
% We evaluate the overhead.
%  Specifically, we look into the optimization of eBPF map lookups.
% The map lookup helpers represent a pessimistic case for \projname{} because
%     \projname{} not only misses the load time optimization but also needs more
%     wrapping code than other helpers due to its map implementation.
% In general, our experiments shows acceptable helper performance for this
%     pessimistic case.
% We found that the missed optimization slows down the helper call by around 30ns
%     and the safe wrapping code adds another 45ns, which is acceptable for a
%     pessimistic case.
%Specifically, we
We measure map lookup time of Rax, compared with
    eBPF with and without inlining, including
% Our test program for both eBPF and \projname{} is a tracepoint program that
%     uses the \texttt{bpf\_ktime\_get\_ns} helper to measure the runtime of
%     a \texttt{bpf\_map\_lookup\_elem} call.
% The \texttt{bpf\_map\_lookup\_elem} invocation is by default inlined by the
%     eBPF verifier when possible.
% The setup of eBPF without inlining is achieved by removing the inlining logic
%    from the eBPF verifier.
    array map, hash map, and static variable.
In eBPF, static variables are
    converted into maps;
%    (implicitly converted to a map in eBPF).
we use a static Rust atomic variable in Rax, as the counterpart of a
    static variable map in eBPF.
% We select two commonly-used eBPF map types for this experiment: array maps
%     and hash maps, and both of them support lookup inlining.
% Figure~\ref{fig:eval-inline}
% shows the performance of inlining and non-inlining on a vanilla
% v5.15 kernel as well as the \projname{} implementation.
Figure~\ref{fig:map-bench} shows the lookup time of different
    maps in eBPF and \projname{}, respectively.
% The vanilla non-inlined
% kernel is achieved by commenting out the lines in the kernel verifier.c file that
% automatically inlines the bpf\_map\_lookup\_elem method.
% For both array maps and hash maps, the
%     runtime of map lookup can be reduced by 20-30 ns compared to eBPF without
%     inlining.
%For both array maps and hash maps, inlining reduces map lookup time by
%    20--30 nanosecond.
We find that
    inlining map lookups in eBPF are $\sim$0.5$ns$
    faster on array maps and $\sim$1.2$ns$ faster on hash maps.
% In the graph,
% the performance impact of inlining the method is 20-30 nanoseconds.
% The additional slowdown of 30-40 nanoseconds seen in the rust implementation
% can be explained by the wrapping performed around the bpf\_map\_lookup\_elem method.
An additional slowdown of 2$ns$--4$ns$ is present in
    \projname{} over non-inlined eBPF, due to the wrapping
    code.
Static variables in eBPF are always accessed via direct load without
    invoking a helper.
Hence, their access latency is almost the same to accessing Rust atomic variables.

% This is because of the wrapping code around the kernel
%     map lookup helper that allows \projname{} programs to invoke
%     it using safe Rust objects.
%We shall note that most of the other helpers neither have verifier
%    optimizations like inlining in eBPF nor
%    require the amount of wrapping code as in map helpers.
%Hence, their performance in Rax and eBPF is similar.
%\tianyin{very week; can we have data for this? or we should just remove}
% \jinghao{We do not have data}

% Static variables are transformed into array maps in eBPF and its lookup is
%     always inlined,

%     while in \projname{}, we implement the equivalent mechanism
%     through static atomic variables.
% As shown in Figure~\ref{fig:eval-inline}, the performance of static variables
%     in eBPF and \projname{} is roughly the same.

% Egor: Not sure if I should go into more technical details about whats going on
% in the wrapping

% Using libbpf (and libiu in the case of \projname{}) the sample \projname{} or c bpf
% program is loaded and attached to a tracepoint which is then called by the trigger.
% In the instance of the inlined and non-inlined tests this tracepoint is getcwd, and
% in the instance of the rust tests that tracepoint used is sys\_enter\_dup. The
% triggers consist of a one-line c program that calls the appropriate function such as getcwd().
% The bpf program creates the corresponding map and measures the time
% it takes to execute a single map\_lookup\_elem. This time is measured with
% bpf\_ktime\_get\_ns() and then printed with bpf\_printk() and recorded.
% \milo{Maybe we should talk about why we chose these specific maps. i.e. that array and hash maps are the main ones that support the inlining?}


% \subsubsection{Safety robustness in \projname{}}
% \sloppypar
% An unintended bug occurred during our implementation of Rax-BMC in the
% \texttt{bmc\_invalidate\_cache} function
% which resulted in an out-of-bounds access error.
% %Afterwards, examination of the kernel logs showed an shocking number of kernel error messages.
% %With further investigation of these log along with the code, an out-of-bounds access error
% %emerged from the \texttt{bmc\_invalidate\_cache} function.
% This mistake caused kernel panics during experimentation.
% Remarkably, even with the numerous kernel panics, the overall system stability
%     and performance were not conspicuously impacted.
% The kernel continued to function well.
% %We have run several benchmarks in this kernel which already has a lot of Rax panic
% %    and this kernel continued to function well.
% This resilience highlights a robustness in the stack unwind and panic handling mechanisms of Rax.
