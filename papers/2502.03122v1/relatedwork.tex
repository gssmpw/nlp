\section{RELATED WORK}
\label{related work}
\textbf{Motion Tracking.} Traditional RL methods for humanoid robots \cite{radosavovic2024real, siekmann2021sim} are highly dependent on hand-crafted reward signals to promote natural gaits, which require significant human insight and domain expertise. However, recent work has proposed the incorporation of RL with reference motion data to reduce human effort. For example, DeepMimic \cite{peng2018deepmimic} and the perpetual humanoid controller (PHC) \cite{luo2023perpetual} have shown impressive results in generating physically plausible character animations by precisely tracking reference motion data. In addition, Peng et al. \cite{peng2021amp, peng2022ase} have incorporated RL with adversarial imitation learning to produce lifelike behaviors. Despite these advancements in physics-based character animation, there is limited evidence supporting the feasibility and effectiveness of motion tracking in humanoid robots. Notable exceptions include \cite{cheng2024expressive}, where RL policies are trained to track the reference motion of the upper body. He et al. \cite{he2024hover} proposed integrating various control modes into a single policy to track user-specified reference motion. Another study \cite{zhang2024whole} integrates AMP rewards with traditional gait rewards to promote natural full-body motions. In our work, we remove the need for separate gait generation and utilize reference motion as both phase information and reward signals to train a motion tracking controller that achieves human-like whole-body control.

\textbf{Domain Randomization.} Recent advances in RL-based humanoid locomotion \cite{radosavovic2024real,radosavovic2024learning} have been substantially propelled by domain randomization, which is crucial to enhancing the robustness and generalization of RL policies. Traditional domain randomization \cite{andrychowicz2020learning,radosavovic2024real} involves varying terrain types, robot dynamics, and actuator dynamics. This variability helps RL agents learn policies that are more adaptable to real-world conditions, thereby reducing the sim-to-real gap. However, these approaches need an exhaustive identification of the system and the identification of the dynamics distribution. As an alternative to complex domain randomization methods, \cite{valassakis2020crossing} demonstrated impressive sim-to-real performance in manipulation tasks using a simple randomized force injection (RFI) strategy, which emulates dynamics randomization by perturbing the dynamics of the system with randomized forces. ERFI \cite{campanaro2024learning} extended RFI by introducing an episodic actuation offset to perturb the dynamics of quadrupedal robots during training. Although simple RFI and ERFI have proven effective for manipulators and quadrupedal robots, their effectiveness has not been clearly demonstrated in humanoid robots. The substantial mass of a humanoid's upper body forces the actuation motors located in the lower body to operate under a loaded condition. This results in a noticeable delay in response to control commands, which hinders the effectiveness of ERFI in humanoid applications. To address this issue, we integrate ERFI with action delay to effectively perturb the dynamics of humanoid robots. Our experiments show that the RL-based controller trained within this simple domain randomization can be applied to real humanoid robots with zero-shot transfer.