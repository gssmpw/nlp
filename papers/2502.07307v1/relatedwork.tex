\section{Related Work}
% 分为两类， short-term and long-term
\textbf{Evaluation of RS}. How to evaluate RS is a complex and essential task, which can be divided into short-term and long-term evaluation based on their objectives~\cite{zangerle2022evaluating_RS_survey}. Most current studies focus on the short-term objective using offline metrics~\cite{jannach2020escaping, jannach2012recommender_landscape}, relying on pre-collected log data containing users' explicit (e.g., ratings) or implicit (e.g., click) feedback to compute metrics like prediction accuracy~\cite{willmott2005MAE} and ranking metrics~\cite{jarvelin2002ndcg}. 
Existing fairness-aware~\cite{ye2024bankfair, xu2024taxation, naghiaei2022cpfair}, debiasing~\cite{chen2023bias}, and diversity-aware~\cite{carbonell1998mmr} recommendation research often evaluates using domain-specific indicators based on these short-term metrics (e.g., fairness metric~\cite{gastwirth1972giniindex}, ), which fail to adequately reflect their long-term benefits and how they influence RS in the long run, particularly in multi-stakeholder platform environments~\cite{surer2018multistakeholder}.
Due to the inefficiency and high cost of long-term online A/B test~\cite{kohavi2015onlineabtest, saito2024long_term_off_policy_saito}, offline long-term evaluation has gained significant attention in recent years, which can be divided into two main categories: (1) use short-term metric~\cite{hohnhold2015focusing} or data~\cite{saito2024long_term_off_policy_saito} to predict long-term performance, (2) create an RS simulator to replicate the real-world environment~\cite{ie2019recsim, zhang2024agent4rec, wang2023recagent} for evaluation.
In this paper, we focus on the second type. 
% Existing RS simulators use Reinforcement Learning (RL)~\cite{ie2019recsim, shi2019virtualtaobao, rohde2018recogym} or LLM~\cite{wang2023recagent, zhang2024agent4rec, zhang2024agentcf} to simulate human behavior. 

\textbf{RS simulator for long-term evaluation.} 
Most existing recommendation simulators (e.g., LLM-based simulator~\cite{wang2023recagent, zhang2024agent4rec}, reinforcement learning (RL)-based simulator~\cite{ shi2019virtualtaobao,ie2019recsim}) focus on user simulation while overlooking creators, making it difficult to capture the long-term dynamic of content platforms.
Some data-driven methods are proposed to conduct creator simulation.  SimuLine~\cite{zhang2023simuLine} applied heuristic methods to determine creators' next creation in news recommendation. 
Some works~\cite{mladenov2021recsimng, mladenov2020optimizing_long-term} assumed that creators will leave the platform if their user engagement falls below a certain threshold.
Other modeling methods used user positive feedback (e.g., click) as the gradient to update the creation state~\cite{lin2024cfd, yao2024uwo, zhan2021toward_content_provider_rec}.
% EcoAgent\cite{zhan2021toward_content_provider_rec} and RecSim NG~\cite{mladenov2021recsimng} utilized RL-based techniques (e.g., Markov Stochastic Process\cite{zhan2021toward_content_provider_rec} and Decision Process \cite{mladenov2021recsimng}) to model the transition behavior of creators. 
However, these approaches failed to align with real creation behavior because: (1) they are unable to produce authentic content (e.g., text), instead relying on embeddings to represent the content they create~\cite{zhang2023simuLine}; (2) they cannot capture the personalization of real-world creators; (3) they ignored human behavior patterns under information asymmetry, such as risk aversion in prospect theory~\cite{kahneman2013prospecttheory} and bounded rationality~\cite{selten1990bounded_rationality}.
% However, RL-based simulators for RS evaluation may be limited in flexibility and generality, as they often rely on relatively simple rules to model behaviors that could diverge from actual human~\cite{shi2019virtualtaobao，ie2019recsim, zhan2021toward_content_provider_rec}. 
% Recently, LLM-empowered agents have shown great promise in enhancing recommendation simulators~\cite{wang2023recagent, wang2024macrec,zhang2024agent4rec, zhang2024agentcf}.
% For example,
% RecAgent~\cite{wang2023recagent} and Agent4Rec~\cite{zhang2024agent4rec} incorporated human cognitive mechanisms (e.g., memory mechanism) to simulate various user behaviors on the platform.
% AgentCF~\cite{zhang2024agentcf}
% modeled user and item agents simultaneously to explore the interaction patterns between users and items.

% For example, LLM-based simulator RecAgent~\cite{wang2023recagent} and Agent4Rec~\cite{zhang2024agent4rec}, Reinforcement Learning (RL)-based simulator Virtual-Taobao~\cite{shi2019virtualtaobao}, RecSim~\cite{ie2019recsim} solely focus on simulating user's watch, click behaviors.
% % EconAgent\cite{zhan2021toward_content_provider_rec} utilized Markov decision process to model both user and creator behavior to help build a multi-stakeholder recommender. 

% previous research~\cite{zhang2024agent4rec} has indicated that

% For example, RecoGym~\cite{rohde2018recogym} provides a configurable stylized RS simulation environment for studying sequential user interaction.
% Virtual Taobao~\cite{shi2019virtualtaobao} and RecSimu~\cite{zhao2019toward_simulating} utilized generative adversarial networks (GAN) to generate virtual users to support learning policies that can be transferred to real systems
% RecSim~\cite{ie2019recsim} provides a comprehensive toolkit for effectively simulating user behaviors across various stylized RS.





\textbf{Behaviors under information asymmetry.} Creator behaviors in information asymmetry conditions
have been actively studied and emphasized in the game theory literature~\cite{yao2024uwo, lin2024cfd, xu2024ppa}. 
They typically assume that creators are totally rational, i.e., aiming to maximize their utility, which often lacks personalization and differs from real-world human behavior under risk (i.e., bounded rational~\cite{selten1990bounded_rationality}). Rule-based~\cite{xu2024ppa} and heuristic method~\cite{yao2024uwo, lin2024cfd} are applied to model the strategic behavior. These studies mainly focus on the competition among creators~\cite{yao2024uwo,lin2024cfd} and the design of better platform mechanisms\cite{mladenov2020optimizing_long-term,prasad2023contentprompting} to maximize user welfare.

% These works have greatly inspired our study, e.g., \citet{prasad2023contentprompting} assumed that creator create content based on their beliefs about their skills and audience.
% These works are unsuitable as RS evaluators for three reasons: (1) absence of explicit RS modeling~\cite{zhan2021toward_content_provider_rec, mladenov2020optimizing_long-term}; (2) use of artificial feedback like relevance scores~\cite{yao2023howbad, yao2024userwelfare_opt} instead of real user responses (e.g., clicks and views); and (3) oversimplified modeling of creator behavior~\cite{lin2024cfd, yao2024uwo}, deviating from real-world human patterns~\cite{mullainathan2000behavioral_ecnomics}.


% \citet{prasad2023contentprompting}  used a mixed-integer-programming-based prompting policy to minimize the impact of information asymmetry on the platform's social welfare.
% \citet{mladenov2020optimizing_long-term} formulated the recommendation problem as a constrained matching problem and assumed creators might leave the platform due to insufficient user engagement.