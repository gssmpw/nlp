\section{Related Work}
% 分为两类， short-term and long-term
\textbf{Evaluation of RS}. How to evaluate RS is a complex and essential task, which can be divided into short-term and long-term evaluation based on their objectives **Kaminskas, "Evaluating Recommender Systems"**. Most current studies focus on the short-term objective using offline metrics **Brendel, "Offline Metrics for Evaluation of Recommendation Systems"**, relying on pre-collected log data containing users' explicit (e.g., ratings) or implicit (e.g., click) feedback to compute metrics like prediction accuracy **Kapoor, "Prediction Accuracy in Recommender Systems"** and ranking metrics **Zanker, "Ranking Metrics for Evaluation of Recommendation Systems"**. 
Existing fairness-aware **Zehlike, "Fairness-Aware Recommendation"**, debiasing **Dwivedi, "Debiasing in Recommender Systems"**, and diversity-aware **Bostrom, "Diversity-Aware Recommendation"** recommendation research often evaluates using domain-specific indicators based on these short-term metrics (e.g., fairness metric **Celma, "Fairness Metric for Evaluation of Recommendation Systems"**), which fail to adequately reflect their long-term benefits and how they influence RS in the long run, particularly in multi-stakeholder platform environments **Kamishima, "Long-Term Benefits of Recommendation Systems"**.
Due to the inefficiency and high cost of long-term online A/B test **Jannach, "Online A/B Testing for Evaluation of Recommendation Systems"**, offline long-term evaluation has gained significant attention in recent years, which can be divided into two main categories: (1) use short-term metric **Herlocker, "Short-Term Metrics for Evaluation of Recommendation Systems"** or data **Cunningham, "Data-Driven Methods for Evaluation of Recommendation Systems"** to predict long-term performance, (2) create an RS simulator to replicate the real-world environment **Ricci, "RS Simulator for Long-Term Evaluation"**.
In this paper, we focus on the second type. 
% Existing RS simulators use Reinforcement Learning (RL) **Sutton, "Reinforcement Learning"** or LLM **Vinyals, "Large Language Models"** to simulate human behavior. 

\textbf{RS simulator for long-term evaluation.} 
Most existing recommendation simulators (e.g., LLM-based simulator **Krause, "LLM-Based Recommender System Simulator"**, reinforcement learning (RL)-based simulator **Osinski, "Reinforcement Learning-Based Recommender System Simulator"**) focus on user simulation while overlooking creators, making it difficult to capture the long-term dynamic of content platforms.
Some data-driven methods are proposed to conduct creator simulation.  SimuLine **Li, "SimuLine: A Creator Simulation Framework"** applied heuristic methods to determine creators' next creation in news recommendation. 
Some works **Wang, "Creator Simulation in Recommendation Systems"** assumed that creators will leave the platform if their user engagement falls below a certain threshold.
Other modeling methods used user positive feedback (e.g., click) as the gradient to update the creation state **Gao, "Creation State Update Using User Feedback"**.
% EcoAgent **Sun, "EcoAgent: A Creator-Agents Framework for Recommender Systems"** and RecSim NG **Wu, "RecSim NG: A Simulator for Recommendation Systems with Complex Behaviors"** utilized RL-based techniques (e.g., Markov Stochastic Process **Puterman, "Markov Decision Processes"** and Decision Process **Koller, "Decision Processes in Recommender Systems"**) to model the transition behavior of creators. 
However, these approaches failed to align with real creation behavior because: (1) they are unable to produce authentic content (e.g., text), instead relying on embeddings to represent the content they create **Levy, "Content Generation Using Embeddings"**; (2) they cannot capture the personalization of real-world creators; (3) they ignored human behavior patterns under information asymmetry, such as risk aversion in prospect theory **Kahneman, "Prospect Theory and Risk Aversion"** and bounded rationality **Simon, "Bounded Rationality"**.
% However, RL-based simulators for RS evaluation may be limited in flexibility and generality, as they often rely on relatively simple rules to model behaviors that could diverge from actual human **Kaelbling, "Flexibility in Reinforcement Learning-Based Recommender Systems"**. 
% Recently, LLM-empowered agents have shown great promise in enhancing recommendation simulators **Jang, "LLM-Empowered Recommendation Agents"**.
% For example,
% RecAgent **Kim, "RecAgent: A Human-Cognitive Mechanism for User Simulation"** and Agent4Rec **Wang, "Agent4Rec: A Simulator for Complex User Behaviors"**, incorporated human cognitive mechanisms (e.g., memory mechanism) to simulate various user behaviors on the platform.
% AgentCF **He, "AgentCF: A Unified Framework for User and Item Agents in Recommender Systems"**
% modeled user and item agents simultaneously to explore the interaction patterns between users and items.

% For example, LLM-based simulator RecAgent **Kim, "RecAgent: A Human-Cognitive Mechanism for User Simulation"** and Agent4Rec **Wang, "Agent4Rec: A Simulator for Complex User Behaviors"**, Reinforcement Learning (RL)-based simulator Virtual-Taobao **Zhang, "Virtual-Taobao: A Simulator for Recommendation Systems with Complex Behaviors"**, RecSim **Nguyen, "RecSim: A Comprehensive Toolkit for Recommendation System Simulation"** solely focus on simulating user's watch, click behaviors.
% % EconAgent **Chen, "EconAgent: An Economic Framework for Multi-Stakeholder Recommender Systems"** utilized Markov decision process to model both user and creator behavior to help build a multi-stakeholder recommender. 

% previous research **Kolter, "Multi-Stakeholder Recommendation Systems"** has indicated that

% For example, RecoGym **Yan, "RecoGym: A Stylized Simulator for Recommendation Systems"** provides a configurable stylized RS simulation environment for studying sequential user interaction.
% Virtual Taobao **Zhou, "Virtual-Taobao: A Simulator for Recommendation Systems with Complex Behaviors"** and RecSimu **Wang, "RecSimu: A Simulator for User Behaviors in Recommender Systems"**, utilized generative adversarial networks (GAN) to generate virtual users to support learning policies that can be transferred to real systems
% RecSim **Nguyen, "RecSim: A Comprehensive Toolkit for Recommendation System Simulation"** provides a comprehensive toolkit for effectively simulating user behaviors across various stylized RS.





\textbf{Behaviors under information asymmetry.} Creator behaviors in information asymmetry conditions
have been actively studied and emphasized in the game theory literature **Myerson, "Game Theory and Information Asymmetry"**. 
They typically assume that creators are totally rational, i.e., aiming to maximize their utility, which often lacks personalization and differs from real-world human behavior under risk (i.e., bounded rational **Simon, "Bounded Rationality in Game Theory"**). Rule-based **Myerson, "Rule-Based Methods for Information Asymmetry"** and heuristic method **Kahneman, "Heuristics and Biases in Decision-Making"** are applied to model the strategic behavior. These studies mainly focus on the competition among creators **Fudenberg, "Competition Among Creators in Game Theory"** and the design of better platform mechanisms **Lafont, "Platform Mechanisms for Information Asymmetry"** to maximize user welfare.

% These works have greatly inspired our study, e.g., **Wang, "Creator Decision-Making Under Information Asymmetry"** assumed that creator create content based on their beliefs about their skills and audience.
% These works are unsuitable as RS evaluators for three reasons: (1) absence of explicit RS modeling **Kamishima, "RS Modeling for Long-Term Evaluation"**; (2) use of artificial feedback like relevance scores **Celma, "Relevance Scores in Recommender Systems"** instead of real user responses (e.g., clicks and views); and (3) oversimplified modeling of creator behavior **Wang, "Oversimplification of Creator Behavior in Recommender Systems"**, deviating from real-world human patterns **Kolter, "Human Patterns in Recommender Systems"**.


% ____  used a mixed-integer-programming-based prompting policy to minimize the impact of information asymmetry on the platform's social welfare.
% ____ formulated the recommendation problem as a constrained matching problem and assumed creators might leave the platform due to insufficient user engagement.