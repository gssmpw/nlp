\documentclass[10pt, journal, compsoc]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{url}
\usepackage{xcolor}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{gensymb}
\usepackage{mathtools}
\usepackage{adjustbox}
\usepackage{framed}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{float}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{array}
\usepackage{arydshln}
\usepackage{stfloats}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\usepackage{arydshln}
\setlength{\dashlinedash}{1.5pt}
\setlength{\dashlinegap}{2.5pt}
\setlength{\arrayrulewidth}{0.3pt}

\usepackage{xcolor}
\definecolor{DarkGreen}{rgb}{0.2,0.5,0.2} % to color links in references
\definecolor{mediumgray}{gray}{0.4} % Adjust the value (0.5 for medium gray)

\makeatletter
\AtBeginDocument{\def\@citecolor{DarkGreen}}
\makeatother

\usepackage{arydshln}
\setlength{\dashlinedash}{1.5pt}
\setlength{\dashlinegap}{2.5pt}
\setlength{\arrayrulewidth}{0.3pt}

\newcommand{\etal}{et al.}

% Include other packages here, before hyperref.
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{color, colortbl}
\usepackage{tikz}
\usepackage{ragged2e}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
\hyphenation{op-tical net-works semi-conduc-tor}


\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false, urlcolor=DarkGreen, citecolor=DarkGreen]{hyperref}

\begin{document}
\title{Faces of Fairness: Examining Bias in Facial Expression Recognition Datasets and Models}


% author names and affiliations
% use a multiple-column layout for up to three different
% affiliations

\author
{
Mohammad~Mehdi~Hosseini, \IEEEmembership{Student Member, IEEE}, Ali~Pourramezan~Fard, and Mohammad~H.~Mahoor, \IEEEmembership{Senior Member, IEEE}

\IEEEcompsocitemizethanks
    {
    \IEEEcompsocthanksitem Mohammad~Mehdi~Hosseini, Ali~Pourramezan~Fard, and Mohammad~H.~Mahoor are with the Ritchie School of Engineering and Computer Science, University of Denver, Denver,
    CO, 80208 .\protect\\
    E-mails: \{MohammadMehdi.Hosseini, Ali.Pourramezanfard, Mohammad.Mahoor\}@du.edu
    }  
}

% \markboth{IEEE Transactions on Affective Computing,~Vol.~00, No.~00, Month~Year}%
% {Hosseini\MakeLowercase{\textit{et al.}}: Bias and Fairness Study in FER}


% =================================================================================
% ==================================== Abstract ===================================
% =================================================================================
\IEEEtitleabstractindextext{%
\begin{abstract}
\label{sec:abstract}
\justifying
Building AI systems, including Facial Expression Recognition (FER), involves two critical aspects: data and model design. Both components significantly influence bias and fairness in FER tasks. Issues related to bias and fairness in FER datasets and models remain underexplored. This study investigates bias sources in FER datasets and models. Four common FER datasets—AffectNet, ExpW, Fer2013, and RAF-DB—are analyzed. The findings demonstrate that AffectNet and ExpW exhibit high generalizability despite data imbalances. Additionally, this research evaluates the bias and fairness of six deep models, including three state-of-the-art convolutional neural network (CNN) models: MobileNet, ResNet, XceptionNet, as well as three transformer-based models: ViT, CLIP, and GPT-4o-mini. Experimental results reveal that while GPT-4o-mini and ViT achieve the highest accuracy scores, they also display the highest levels of bias. These findings underscore the urgent need for developing new methodologies to mitigate bias and ensure fairness in datasets and models, particularly in affective computing applications. See our implementation details at https://github.com/MMHosseini/bias\_in\_FER.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
\justifying
Bias, Fairness, Facial Expression Recognition, Facial Affect Analysis, Bias in FER Datasets, Bias in FER Algorithms.
\end{IEEEkeywords}}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract

% no keywords


% For peerreview papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


% =================================================================================
% ================================== Introduction =================================
% =================================================================================
\section{Introduction}
\label{sec:introduction}
Facial expressions serve as a key non-verbal communication channel for humans~\cite{schirmer2024non}, allowing them to express emotions and shape behavior, social interactions, and lifestyle~\cite{dolan2002emotion}. Automated facial expression recognition (FER) has a borad range of applications, including mental health monitoring~\cite{jiang2020facial}, human-computer interaction (HCI)\cite{pantic2003toward}, and surveillance\cite{zhang2018facial}. Deep learning methods have recently achieved significant progress in FER, particularly through the use of convolutional neural networks (CNNs)~\cite{borgalli2025hybrid}, transformers~\cite{fatima2025enhanced}, and large vision-language models ~\cite{saadi2024leveraging}.

Despite advancements in deep learning, bias and fairness issues in FER systems remain unexplored. Bias refers to prejudices or favoritism toward groups, while fairness ensures impartiality and avoids disparities~\cite{mehrabi2021survey, barocas2023fairness}. Bias can be analyzed in two dimensions: datasets and models~\cite{mehrabi2021survey, suresh2019framework, ferrara2023fairness}. Datasets mainly suffer from missing data, noisy data, labeling problems, imbalanced distribution, scalability, and bias. However, some classification approaches have been proposed to tackle these problems~\cite{estiri2022low, shams2024evolutionary, ganj2023lr}. Dataset bias often arises from demographic imbalances, such as disparities in age, gender, or race~\cite{mehrabi2021survey}. Model bias stems from architecture, training process, and unrepresentative datasets~\cite{liang2024linking, caro2020local, dablain2024towards, veeramachaneni2025large}. Bias impacts model performance for underrepresented groups, reduces generalizability, and compromises fairness. Addressing bias in datasets and models is crucial for fairness in FER.

Bias in machine learning can be classified in various ways. Pagano~\etal~\cite{pagano2023bias} identified three types: \textbf{(1)} pre-existing bias from inequalities in data, \textbf{(2)} technical bias introduced during model design, and \textbf{(3)} emergent bias causing system-level disparities. Pagano's model argues bias mainly originates from data~\cite{dooley2024rethinking}: underrepresented demographic groups in training data lead to poorer performance. Expanding this theory, Cheong~\etal~\cite{cheong2023causal} identified three sources: \textbf{i)} dataset bias which is related to imbalanced data, \textbf{ii)} labeling bias that describes incorrect labels, and \textbf{iii)} contextual bias originating from image capture conditions.

Various bias mitigation strategies have been proposed to address these issues: \textbf{a)} pre-processing methods, which focus on data manipulation before training~\cite{jiang2024joint, hosseini2017facial}, \textbf{b)} in-processing methods, which aim to reduce bias during the training phase~\cite{yang2024robust, yu2024exploring}, and
\textbf{c)} post-processing methods, which tend to mitigate bias after the model is trained~\cite{pagano2023bias, dooley2024rethinking, liang2023benchmarking}. Bias mitigation endeavors can target either the data or the algorithms themselves, depending on the source of bias.

Despite the above-mentioned efforts to address bias, there is a notable research gap in automatic FER, particularly in in-the-wild datasets and novel deep models. In-the-wild datasets introduce biases from variations in illumination conditions, camera resolution, background noise, head pose, and gesture, as well as demographic attributes of age, gender, and race~\cite{xu2020investigating}. Moreover, many studies have focused on limited sources of bias, primarily examining label distributions or one or two demographic attributes~\cite{deuschel2020uncovering, li2020deeper, chen2021understanding}. This approach overlooks the multifaceted nature of bias in FER. Additionally, recent advancements in deep architectures, such as vision transformers~\cite{dosovitskiy2020image} and large vision-language models~\cite{radford2021learning, achiam2023gpt}, have significantly impacted this field. However, their implications for bias and fairness in FER tasks remain largely unexplored. Fig.~\ref{fig:proposed_approach} shows the approach we utilize different metrics to evaluate various aspects of bias and fairness in the datasets and models.

This research aims to address gaps by examining overlooked aspects of bias and fairness in FER. We provide a comprehensive analysis of bias and fairness in both datasets and deep models. Specifically, we argue that both datasets and models can introduce bias into FER systems. To assess the impact of datasets, we analyze four real-world FER datasets: AffectNet~\cite{mollahosseini2017affectnet}, ExpW~\cite{zhang2018facial}, RAF-DB~\cite{li2017reliable}, and Fer2013~\cite{goodfellow2013challenges}. The bias in these datasets is evaluated using three metrics: Richness~\cite{jost2006entropy}, Evenness~\cite{hill1973diversity}, and Dominance~\cite{simpon1949measurement}. Additionally, we investigate inherent biases in the datasets through experiments, as discussed in Sec.~\ref{sec:methodology:data_analysis}.

\begin{figure*}[t]
\centering
\caption{Our approach focuses on examining equity, including bias and fairness, in FER datasets and models. We leveraged various metrics to assess both datasets and models, where each metric quantifies specific aspects and enables numerical evaluation and comparison.}
\includegraphics[width=0.98\textwidth]{figs/proposed_approach.pdf}
\label{fig:proposed_approach}
\end{figure*}

In this study, we evaluate six deep models, commonly used for FER tasks. To this aim, we train three CNN models, i.e., MobileNetV1~\cite{howard2017mobilenets}, ResNet-50~\cite{he2016deep}, XceptionNet~\cite{chollet2017xception}, and the vision-transformer-based ViT~\cite{dosovitskiy2020image} model, using all the aforementioned datasets. From the two large language-vision models, CLIP~\cite{radford2021learning} is fine-tuned using the datasets, while GPT-4o-mini~\cite{achiam2023gpt} is evaluated without training or fine-tuning. We refer to MobileNetV1 as MobileNet and ResNet-50 as ResNet in the paper. To assess the bias of these models, we leverage four key bias metrics: Equalized Odds~\cite{hardt2016equality}, Equal Opportunity~\cite{hardt2016equality}, Demographic Parity~\cite{dwork2012fairness, kusner2017counterfactual}, and Treatment Equality~\cite{berk2021fairness}. These metrics capture aspects of bias and fairness, allowing for a comprehensive evaluation of the six models across multiple attributes. This methodology facilitates a fair and thorough comparison of their respective bias scores.

The experiments conducted in this study show that AffectNet is the most diverse dataset, while RAF-DB ranks the lowest in diversity. Regarding model performance, despite the high accuracy of the GPT-4o-mini and ViT models, they exhibited the highest bias scores when evaluated using the bias metrics. In contrast, ResNet demonstrated the lowest bias score, making it the fairest model among those tested in this study. Our implementation code and more detail about our experiments is reachable at \href{https://github.com/MMHosseini/bias_in_FER}{GitHub}.

In the remainder of this paper, Sec.~\ref{sec:literature_review} provides a review of the literature on bias and fairness in both datasets and algorithms. In continue, Sec.~\ref{sec:methodology} outlines the metrics used for evaluating bias, while Sec.~\ref{sec:experimental_results} discusses bias scores for the datasets and models. Finally, Sec.~\ref{sec:discussion_and_future_works} concludes this bias study in FER.
% =================================================================================
% ============================== Related Work (Edited by GPT) =====================
% =================================================================================

\section{Literature Review}
\label{sec:literature_review}
FER has many applications, such as sensitive biometrics~\cite{igene2024face}, where biased data or algorithms can cause irreparable damages. Understanding the sources of bias is essential for effectively addressing its challenges. Bias can originate from both datasets and models. Factors such as data collection and annotation processes, model architecture, objective functions, and evaluation methodologies are among the primary sources of bias. Figure~\ref{fig:bias_sources_in_fer} presents a general classification of bias sources. Addressing bias in deep machine learning methods can significantly enhance the reliability and accuracy of FER models. More importantly, developing fair and unbiased deep methods for FER methods potentially ensures equitable treatment across diverse demographic groups~\cite{amigo2023mitigating}. Thus, dealing with bias in FER has recently raised attention from researchers. Examining data distribution in existing FER datasets~\cite{dominguez2024metrics, dominguez2022gender, dominguez2022assessing, li2020deeper, huber2024bias, cheong2023causal}, as well as analyzing the algorithms used in the FER 
 models~\cite{xu2020investigating, yu2023exploring, dooley2024rethinking, kolahdouzi2023toward, liang2023benchmarking, abdullah2024effects} are the key approaches for studying bias in FER. In the following section, we review recent studies that address bias in FER models and datasets.

\begin{figure}[b!]
\centering
\caption{Bias can originate from datasets and models. In datasets, prominent sources of bias include issues in data collection, such as demographic disparities, variations in illumination and lighting conditions, gestures, head poses, and cultural differences in emotional interpretation. In models, key bias sources include architecture design, training parameters, overfitting to specific demographic groups, and the selection of evaluation metrics.}
\includegraphics[width=0.98\columnwidth]{figs/bias_sources_in_fer.pdf}
\label{fig:bias_sources_in_fer}
\end{figure}

\subsection{Bias in FER Datasets} 
\label{sec:literature_review:bias_in_fer_datasets}
Bias in datasets is broadly categorized into representational bias and stereotypical bias~\cite{dominguez2024metrics, dominguez2022gender, dominguez2022assessing}. Representational bias focuses mainly on the demographic diversity of data, including age, gender, and race attributes. This bias is evaluated using metrics like Richness, Evenness, and Dominance (for definitions see Sec.~\ref{sec:methodology:data_analysis}). On the other hand, stereotypical bias investigates the correlation between specific subgroups and their corresponding labels, which can lead to unfair or biased outcomes. 

Another categorization of bias in dataset divided them into intrinsic and extrinsic biases~\cite{mavadati2013disfa, srinivas2019face, singh2022anatomizing}. Intrinsic bias inherent to the process of data collection, i.e., the way data are collected, sampled, and labeled. Demographic attributes, such as age, gender, and race, as well as head-pose, gesture, eye-gaze, and expression intensity are studied as intrinsic biases. On the contrary, extrinsic bias refers to the disparities that could affect the model's performance. For example, lab-controlled datasets can lead to high biases in models, because they are not collected based on the real-world scenarios. Extrinsic bias mainly stems from environmental conditions, such as camera resolution, background noise, illumination, and annotator bias. There are other definitions of bias in dataset. While Cheong~\etal~\cite{cheong2023causal} grouped bias sources as observed and non-observed sources, Terhorst~\etal~\cite{terhorst2021comprehensive} classified bias sources into demographic and non-demographic groups. Despite all these different bias categorizations, a strong consensus is that demographic attributes, including age, gender, and race, are the most effective sources of bias in dataset that need to be studied.

In the following we review the most notable works on this topic. Dominguez~\etal~\cite{dominguez2024metrics} proposed seventeen representational metrics, where they ranked AffectNet~\cite{mollahosseini2017affectnet}, ExpW~\cite{zhang2018facial}, RAF-DB~\cite{li2017reliable}, and Fer2013~\cite{goodfellow2013challenges} among the most diverse FER datasets. It is notable that their stereotypical metrics showed different results for this four datasets where they did not achieve good scores and rankings. Another research by Li~\etal~\cite{li2020deeper} studied the intrinsic bias among seven different FER datasets and reported that the main reasons for bias in datasets are cultural differences and data collection conditions. Huber~\etal~\cite{huber2024bias} studied bias in synthetic facial datasets where they revealed that synthetic datasets follow the same bias patterns as the authentic datasets used for training their corresponding models. Finally, Cheong~\etal~\cite{cheong2023causal} used a directed acyclic graph (DAG) to explore the relation between sources of bias. They introduced gender and age as the strongest sources of bias in emotion labeling. These experiments highlight the importance of dataset analysis in FER tasks, especially demographic attributes and their correlation with the labels for each subgroup. In Sec.~\ref{sec:methodology:data_analysis} we review some common metrics for data analysis, and in Sec.~\ref{sec:experimental_results:data_experiments} demonstrate their results.

\subsection{Bias in FER Algorithms} 
\label{sec:literature_review:bias_in_fer_algorithms}
The analysis of bias in  FER algorithms involves examining how deep models and algorithms impact various demographic groups, aiming to enhance fairness and improve emotion recognition accuracy. Below, we review recent studies addressing algorithmic bias. While some studies tried to reduce the bias of the model's using data compensation, some experiments revealed that there are some inherent biases in the models irrelevant to the data.

To mitigate the effects of imbalanced data distribution, Yu~\etal~\cite{yu2023exploring} utilized unlabeled data in a semi-supervised learning framework. This framework leveraged unlabeled facial data by assigning pseudo-labels to underrepresented samples in the datasets. As a result, their method achieved a more balanced data distribution and improved the overall accuracy of the FER system. In a related study, Kolahdouzi~\etal~\cite{kolahdouzi2023toward} introduced a Hilbert-space kernel to compute the mean distribution of specific demographic groups. To reduce bias, they subtracted a facial image's embedding from the extracted mean, obscuring sensitive attributes such as age, gender, and race. Furthermore, Liang~\etal~\cite{liang2023benchmarking} investigated the influence of facial attributes on model performance. They categorized these attributes into protected attributes (e.g., gender and race) and unprotected attributes (e.g., pose and lighting). Their findings demonstrated that protected attributes affect model performance, contributing to bias in predictions.

In contrast, some researchers argue that biases originate from the models. Dooley~\etal~\cite{dooley2024rethinking} hypothesized that bias stems from the architecture of deep models. They employed a joint CNN architecture with one component emphasizing fairness and another focusing on extracting hyperparameters to mitigate bias. Their results indicated that bias remains inherently tied to model architecture. Abdullah~\etal~\cite{abdullah2024effects} conducted experiments focusing on the attention areas allocated to faces by the models. They observed that the attention area assigned to dark-skinned faces was smaller compared to lighter skin tones, revealing an inherent bias in the models. Xu~\etal~\cite{xu2020investigating} compared a baseline model, ResNet-18~\cite{he2016deep}, with two disentangled and attribute-aware models. Their experiments demonstrated that data augmentation did not significantly enhance the accuracy of the baseline model. Similarly, Amigo~\etal~\cite{amigo2023mitigating} proposed a debiasing variational autoencoder to address bias in models. Their findings showed that, despite training on different datasets, models could perpetuate biases. Furthermore, Dominguez~\etal~\cite{dominguez2024less} revealed that stereotypical biases are intrinsic to models, regardless of whether the training data is balanced or imbalanced. These findings underscore the importance of studying bias and fairness in both data and algorithms.

\subsection{Perspectives}
Bias and fairness in facial expression recognition are critical to preventing discrimination, ensuring equitable systems, improving performance across diverse populations, and upholding ethical standards. Real-world FER applications rely on in-the-wild datasets, making it imperative to thoroughly examine commonly used datasets to identify and address bias issues. While some studies have partially investigated these datasets, a more comprehensive exploration of popular FER datasets is needed.

At the same time, recent advancements in model architectures, such as deep convolutional neural networks, vision transformers, and large vision-language models, have significantly improved FER accuracy~\cite{rodrigo2024comprehensive, aldahoul2024exploring, zhao2024enhancing}. However, despite their broad adoption, these models must be rigorously evaluated for fairness and bias mitigation. This study provides an in-depth analysis of bias in four prominent FER datasets—AffectNet, ExpW, Fer2013, and RAF-DB—and assesses six state-of-the-art models, including MobileNet, ResNet, XceptionNet, ViT, CLIP, and GPT-4o-mini.

% =================================================================================
% ================================== Methodology ==================================
% =================================================================================
\section{Methodology} 
\label{sec:methodology}
Bias and fairness are entangled and multifaceted concepts, with various definitions proposed in the literature (see~\cite{verma2018fairness}). Fairness is often described as the absence of favoritism or unjust privilege toward any group~\cite{mehrabi2021survey}, while bias refers to prejudice or preferential treatment toward certain cases~\cite{dominguez2024metrics}. In essence, bias pertains to the data, whereas fairness focuses on the algorithms that process it. Numerous metrics have been introduced to measure both bias and fairness in datasets and algorithms (see~\cite{dominguez2024metrics, mittal2023bias}). In the following sections, we delve into the selected metrics for evaluating datasets and algorithms, aiming to provide a comprehensive understanding of their impact on facial expression recognition.
\subsection{Data Analysis} 
\label{sec:methodology:data_analysis}
In Sec.~\ref{sec:literature_review:bias_in_fer_datasets}, we explored various classifications of bias, including intrinsic vs. extrinsic, observed vs. non-observed, demographic vs. non-demographic, and representational vs. stereotypical categories. Bias can manifest in many ways, such as through camera inconsistencies, background variations, body posture differences, and, most notably, demographic imbalances. Among these, demographic attributes—age, gender, and race—emerge as the most significant contributors to bias in FER datasets. As a result, these attributes, along with label distribution, warrant the most attention in this study to address bias and promote fairness.

Building on the taxonomy proposed by Dominguez \etal~\cite{dominguez2024metrics}, this study analyzes bias in dataset using two major categories of metrics. Representational metrics examine the Richness, Evenness, and Dominance of the dataset, while stereotypical metrics reveal the local and global relationships between attributes.

\textbf{Richness} evaluates the total number of samples within a dataset. More samples lead to a higher Richness score. However, this metric solely considers the quantity of samples, without addressing their distribution across various attributes~\cite{jost2006entropy}.

We consider $R_i=N_i$ as the basic Richness equation where $R_i$ is Richness of dataset $i$ and $N_i$ stands for the number of samples correspond to dataset $i$. To have a better understanding and scoring, we normalized the Richness score of each dataset, in the range $[0, 1]$, through Eq.~\ref{eq:dataset_richness}, where $max(N)$ is maximum Richness score of all the datasets.

\begin{align}\label{eq:dataset_richness}
R_i=\frac{N_i}{max(N)},
\end{align}

\textbf{Evenness} measures the equity of data distribution across different classes of attributes. It is commonly assessed using the Shannon's Evenness Index~\cite{shannon1948mathematical}, which quantifies how uniformly the samples are distributed in the datasets.
\begin{align}\label{eq:dataset_evenness}
E = -\frac{\sum_{i=1}^C p_i \log(p_i)}{\log(C)},~~~~~~~~~~p_i = \frac{n_i}{N}.
\end{align}
Eq.~\ref{eq:dataset_evenness} shows the Shannon’s Evenness Index, where $0 \leq E \leq 1$ represents the Evenness, $C$ refers to the total number of classes, $n_i$ denotes the number of samples in class i, and $N$ is total number of samples. $E=1$ indicates a perfectly even distribution, whereas $E<1$ reflects an uneven data distribution. 

\textbf{Dominance} measures the extent to which one group holds an advantage over other groups within a dataset.  Since this definition gives more score to non-uniform data distribution, we subtracted it from 1, to convert it to the same range and the concept that we are using for the Richness and Evenness metrics. The proposed metric for assessing Dominance is the Simpson's Dominance Index~\cite{simpon1949measurement},
\begin{align}\label{eq:dataset_dominance}
D = 1 - \sum_{i=1}^C p_i^2,~~~~~~~~~~p_i = \frac{n_i}{N},
\end{align}
\\
which provides a quantitative evaluation of this phenomenon. In this equation, $0 \leq D \leq 1$ represents Dominance, $C$ denotes the total number of classes, $p_i$ is the proportion of samples in each class $i$, $n_i$ and $N$ indicates the number of samples in a class, and the total number of samples, respectively. In this metric, a value of $D=0$ signifies that one class dominates entirely, while $D>0$ reflects a more evenly distribution across classes.

\textbf{Global-Stereotypical} and \textbf{Local-Stereotypical} metrics examine the correlation between an attribute and a class label. Global-Stereotypical metrics focus on the overall relationship between an attribute and data distribution, such as the connection between age groups and expression labels. Local-Stereotypical metrics delve into more specific relationships, analyzing the connection between a particular attribute value and a class label. For example, the relationship between the attribute value 'age-group = young' and the Happy expression.

To investigate the local and global stereotypical metrics across different datasets, we extracted the data distribution for each dataset based on expression labels, as well as demographic attributes, age, gender, and race. The mathematical definition of the data distribution is provided in Eq.~\ref{eq:data_distribution} as follows: 
\begin{align}\label{eq:data_distribution}
P(A=a)=\frac{f_a}{N},
\end{align}
where $A$ represents an attribute selected from expression, age, gender, or race, $a$ is any possible value for $A$, $f_a$ is the frequency of $a$, and $N$ is the total number of samples. In this study, utilizing DeepFace~\cite{serengil2021lightface} definitions, age is classified into four ranges: [0$\sim$15], [16$\sim$32], [33$\sim$53], and [Over 54], gender is categorized as Woman or Man, and ultimately, six racial groups of White, Black, Asian, Indian, Latinx, and Middle Eastern, are considered.  

In addition, we assessed the conditional data distribution of each dataset. Given that each expression class is associated with three attributes—age, gender, and race—we explored the data distribution under both single and joint conditions. This was done using the general distribution equation $P(A \mid B) = \frac{P(A \cap B)}{P(B)}$, where the denominator remains constant across all conditions, allowing us to simplify it to $P(A \mid B) = P(A \cap B)$. Equations~\ref{eq:single_condition_data_distribution}, ~\ref{eq:couple_condition_data_distribution}, and~\ref{eq:triple_condition_data_distribution} present the mathematical formulation of this analysis.

\begin{align}
\label{eq:single_condition_data_distribution}
\begin{adjustbox}{width=0.6\columnwidth}
$P(Y=y \mid A=a)=P(Y=y \cap A=a)$,
\end{adjustbox} 
\end{align}

\begin{align}
\label{eq:couple_condition_data_distribution}
\begin{adjustbox}{width=0.85\columnwidth}
$P(Y=y \mid A=a, B=b)=P(Y=y \cap A=a \cap B=b)$,
\end{adjustbox} 
\end{align}

\begin{equation}
\label{eq:triple_condition_data_distribution}
\begin{adjustbox}{width=0.55\columnwidth}
$\begin{aligned}
P(Y=y \mid A=a, B=b, C=c) = \\ P(Y=y \cap A=a \cap B=b \cap C=c).
\end{aligned}$
\end{adjustbox} 
\end{equation}
In Eqs.~\ref{eq:single_condition_data_distribution},~\ref{eq:couple_condition_data_distribution},~\ref{eq:triple_condition_data_distribution}, $Y$ shows the expression, while $A$, $B$, $C$ correspond to the facial attributes age, gender, and race, with $y$, $a$, $b$, $c$ representing their respective values. 

To evaluate the intrinsic biases of each dataset, we designed an experiment where a model is trained to recognize the datasets. In this experiment, each sample was labeled with its corresponding dataset name, for both the training and validation sets. The model was then trained to predict the dataset of each image. High accuracy in this experiment indicates higher intrinsic biases, such as those related to illumination, background noise, camera resolution, gesture, eye gaze, head pose, hair color, and any other hidden bias in the datasets. In other words, a stronger correlation between any pair of datasets reflects a lower intrinsic bias in those datasets.

We conducted an additional experiment to evaluate intrinsic biases by implementing a leave-one-dataset-out approach. In this experiment, we trained a deep model for FER using all except one dataset, which was held out for test. By assessing the model's performance on the excluded dataset, we aimed to identify potential biases within the dataset itself while also measuring the model’s ability to generalize across different data sources. In this experiment, a lower accuracy suggests bias in the datasets. To ensure a fair comparison, we trained the model on the training sets of all datasets except one, validated its accuracy using their respective validation sets, and tested the bias using the validation set of the excluded (left-out) dataset. If the results of the validation and test experiments show similar accuracies, the model is considered unbiased. If there is a discrepancy, it indicates that the dataset suffers from bias.

To sum up, we extracted statistical data from each dataset and designed two model-based experiments—the dataset recognition model and the leave-one-dataset-out FER model—to assess both intrinsic and extrinsic biases within the datasets.

\subsection{Model Analysis} 
\label{sec:methodology:model_analysis}
A variety of general metrics have been proposed to evaluate fairness in machine learning models. Verma~\etal~\cite{verma2018fairness} introduced three categories of fairness parameters: statistical measures, similarity-based measures, and causal reasoning. Additionally, Mittal~\etal~\cite{mittal2023bias} presented metrics, such as parity-based metrics, score-based metrics, and facial-analysis-specific metrics.

Recent studies have explored various bias and fairness metrics~\cite{pagano2023bias, mehrabi2021survey, chouldechova2017fair}. While some researchers introduced their own metrics, most studies have concentrated on widely recognized metrics such as Equalized Odds~\cite{hardt2016equality}, Equal Opportunity~\cite{hardt2016equality}, Demographic Parity~\cite{dwork2012fairness, kusner2017counterfactual}, Treatment Equality~\cite{berk2021fairness}. In this study, we focus on these four metrics and evaluate the bias and fairness of the models. Before delving deeper into these metrics, it is essential to explain the concepts of protected and unprotected groups. Protected groups are privileged against discrimination, while unprotected groups may face exclusion or bias.

\textbf{Equalized Odds} evaluates fairness by examining the positive predicted samples. Specifically, for both protected and unprotected groups, the model should exhibit similar ratios of True Positives (TP) and False Positives (FP). In other words, the likelihood of correctly identifying a positive case or making a False Positive prediction should not depend on the group membership.
\begin{align}\label{eq:equalized_odds}
\begin{adjustbox}{width=205pt}
$P({\hat{Y}}=1|A=0, Y=y) = P({\hat{Y}}=1|A=1, Y=y)$,
\end{adjustbox} 
\end{align}
where $y\in\{0, 1\}$. Equation~\ref{eq:equalized_odds} states that for an attribute $A$, a fair model will produce equal ratios of True Positives (TPs) and False Positives (FPs) for both the protected group ($A=0$) and unprotected group ($A=1$). For example, consider gender as the attribute, where $A=female$. A fair model should yield identical TP and FP ratios for females ($A=0$) and males ($A=1$). This ensures the model's predictions are not disproportionately influenced by gender, satisfying Equalized Odds.

\textbf{Equal Opportunity} focuses exclusively on the True Positive samples. Specifically, for a protected and an unprotected group, the probability of correctly predicting a positive outcome ($Y=1$) should be the same. The key difference between this metric and Equalized Odds is that, while Equalized Odds consider both True Positives ($Y=1$) and False Positives ($Y=0$), Equal Opportunity is solely concerned with the True Positive ($Y=1$).
\begin{align}\label{eq:equal_opportunity}
\begin{adjustbox}{width=200pt}
$P({\hat{Y}}=1|A=0, Y=1) = P({\hat{Y}}=1|A=1, Y=1)$.
\end{adjustbox} 
\end{align}
Eq.~\ref{eq:equal_opportunity} evaluates the equity in the ratio of TP between the protected ($A=0$) and unprotected ($A=1$) groups.

\textbf{Demographic Parity}, also referred to as statistical parity, examines the independence of predictions from protected attributes. In essence, it evaluates the distribution of positive predictions, irrespective of whether they are TP or FP. This measure expects a uniform distribution of predictions across different groups. Eq.~\ref{eq:demographic_parity} provides a formal definition of Demographic Parity.
\begin{align}\label{eq:demographic_parity}
\begin{adjustbox}{width=147pt}
$P({\hat{Y}=1}|A=0) = P({\hat{Y}=1}|A=1)$.
\end{adjustbox} 
\end{align}
For instance, when considering the attribute race, the number of samples predicted as ${\hat{Y}}=1$ should be balanced across all racial groups, such as Black, White, and Asian.

\textbf{Treatment Equality} examines the distribution of errors across different demographic groups. Specifically, it ensures that the ratio of False Negatives (FNs) to False Positives (FPs) remains consistent between protected and unprotected groups (see Eq.~\ref{eq:treatment_equality}).
\begin{align}\label{eq:treatment_equality}
\begin{adjustbox}{width=200pt}
$\frac{P({\hat{Y}}=0|A=0, Y=1)}{P({\hat{Y}}=1|A=0, Y=0)} = \frac{P({\hat{Y}}=0|A=1, Y=1)}{P({\hat{Y}}=1|A=1, Y=0)}$.
\end{adjustbox} 
\end{align}

For example, the ratio of FN to FP should be consistent across all age groups, such as children, young adults, mature adults, and older individuals. Interestingly, some studies choose to evaluate this relationship by calculating the ratio of FPs to FNs instead.

To obtain an evaluation score for fairness (bias score) in each model, we applied the four bias metrics to each expression label. Next, we selected the maximum bias across all expressions as the bias score for that metric (see Eq.\ref{eq:bias_score_metric}). To assess the bias of each attribute, we averaged the bias scores across the different metrics using Eq.\ref{eq:bias_score_attribute}. Finally, the overall bias of each model was calculated by averaging the bias scores of the attributes (Eq.~\ref{eq:bias_score}). 
\begin{equation}
\begin{aligned}
\label{eq:bias_score_metric}
\adjustbox{width=0.88\columnwidth}
{
\text{$Bias_{att}(met) = Max(Bias_{met, att}(exp))~~\forall exp \in EXP$,}
}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\label{eq:bias_score_attribute}
\text{$Bias(att) = \frac{1}{K}\sum_{k=1}^{K}Bias_{att}(met)~~\forall met \in MET$,}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\label{eq:bias_score}
\text{$Bias = \frac{1}{N}\sum_{n=1}^{N}Bias(att)~~\forall att \in ATT$.}
\end{aligned}
\end{equation}
In Eq.~\ref{eq:bias_score_metric}, $EXP$ represents all seven basic expressions (Neutral, Happy, Sad, Surprise, Fear, Disgust, and Anger)~\cite{ekman1971constants}. In Eq.~\ref{eq:bias_score_attribute}, $K$ denotes the number of bias metrics, while $MET$ refers to the individual metrics (Equalized Odds, Equal Opportunity, Demographic Parity, and Treatment Equality). In Eq.~\ref{eq:bias_score}, $N$ represents the number of attributes, and $ATT$ refers to all attributes.

To sum up, we employed four well-established bias metrics to evaluate key attributes in facial expression analysis, namely age, gender, and race. Our experiments were conducted on six popular models to assess their fairness. The detailed discussion of the models' bias will follow in Section~\ref{sec:experimental_results}.

% =================================================================================
% ============================== Experimental Results =============================
% =================================================================================

\section{Experimental Results} 
\label{sec:experimental_results}
This section explores bias across four well-known FER datasets: AffectNet~\cite{mollahosseini2017affectnet}, Fer2013~\cite{goodfellow2013challenges}, RAF-DB~\cite{li2017reliable}, and ExpW~\cite{zhang2018facial}. Additionally, it examines the fairness in six distinct generations of deep models, including three CNN models: MobileNet~\cite{howard2017mobilenets}, ResNet~\cite{he2016deep}, XceptionNet~\cite{chollet2017xception}, as well as three transformer-based models: ViT~\cite{dosovitskiy2020image},  CLIP~\cite{radford2021learning}, and GPT-4o-mini~\cite{achiam2023gpt}.

\subsection{Bias in Datasets} 
\label{sec:experimental_results:data_experiments}
Many studies suggest that the primary sources of bias in machine learning models lie in the data itself~\cite{barocas2023fairness, mehrabi2021survey, suresh2019framework}. Consequently, to investigate bias in FER, it is essential to first examine the datasets. While numerous facial attributes can contribute to bias (see~\cite{terhorst2021comprehensive}), and a variety of biases can be present within the datasets (see~\cite{cheong2023causal, mehrabi2021survey}), our focus is on the three most prominent facial attributes. In the following sections, we provide an overview of the datasets used in our study and detail the preprocessing steps we applied. Additionally, we explore the data distribution, attribute correlations, and model-based analysis.

\subsubsection{Datasets} 
\label{sec:experimental_results:data_experiments:datasets}
Although numerous FER datasets exist, only a few feature in-the-wild facial images. By considering factors such as popularity, number of expression labels, and annotation methods, we selected four datasets for our study: AffectNet~\cite{mollahosseini2017affectnet}, Fer2013~\cite{goodfellow2013challenges}, RAF-DB~\cite{li2017reliable}, and ExpW~\cite{zhang2018facial}. All these datasets include at least six Ekman~\cite{ekman1971constants} basic expression labels, including Happy, Sad, Surprise, Fear, Disgust, Anger, plus Neutral, manually annotated by annotators.

\input{tbls/dataset_diversity_score}



AffectNet is the largest in-the-wild FER dataset, comprising nearly 1 million images, with half of them manually annotated. It includes eight expressions (seven basic expressions plus Contempt), continuous labels (valence-arousal), and additional metadata. Fer2013 contains approximately 36K images, divided into 29K training samples and 3.5K validation samples. However, its small image size (40$\times$40 pixels) and grayscale color representation pose limitations. RAF-DB provides both single-label and compound-label subsets, with around 30K facial images and facial landmark annotations. The compound-labeling approach in RAF-DB results in 19 expressions, including the seven basic expressions and 12 compound expressions such as Happily-Surprise and Sadly-Angry. Finally, ExpW is a large-scale FER dataset, featuring over 90K manually annotated facial images. We utilize these datasets in our study to train and evaluate our models.

\begin{figure*}[t]
\centering
\caption{The data distribution across different datasets shows several trends: a) Happy and Neutral dominate the datasets, while Fear and Disgust are underrepresented. Among all datasets, Fer2013 exhibits the most balanced expression distribution, b) A noticeable bias is observed in the age groups, where [16$\sim$32] and [33$\sim$53] being more frequent, while [0$\sim$15] and [Over 54] have significantly fewer samples, c) Across all the datasets, there are more Man samples than Womans. This gender imbalance is most pronounced in the ExpW dataset and least evident in AffectNet, d) Regarding race, White samples are the most represented group, while Indians are the least represented. Data distribution for Black, Latinx, and Middle-Eastern races is more even.}
\includegraphics[width=0.98\textwidth]{figs/1_attribute_distribution_in_percent_all_in_one.pdf}
\label{fig:data_distribution}
\end{figure*}

\subsubsection{Data Preprocessing} \label{sec:experimental_results:data_experiments:data_preprocessing}
We used four in-the-wild datasets, each with different image sizes. In the first step, we aligned the images by ensuring the same cropping area, image size, and color domain across all the datasets. To achieve a consistent cropping area, we extended the original crop region by 25 to 35 percent in each dimension for all images. We then used the DeepFace model~\cite{serengil2024lightface, serengil2020lightface} to process the extended crop and align the facial images across all datasets. Next, we resized all cropped faces to $224\times224$. While three datasets are in the RGB color domain, Fer2013 is grayscale. To address this, we replicated the grayscale layer three times to match the RGB format, ensuring uniformity across the data. Additionally, since the ExpW dataset lacks a validation set, we randomly selected 10 percent of its images to serve as the validation set.

\subsubsection{Data Diversity and Distribution}
\label{sec:experimental_results:data_experiments:data_distribution}
After data processing and achieving a unified format, we used the DeepFace model~\cite{serengil2021lightface} to extract facial attributes, including age, gender, and race. Ages were classified into four ranges: [0$\sim$15], [16$\sim$32], [33$\sim$53], and [Over 54]. DeepFace predicts six racial categories: White, Black, Asian, Indian, Latinx, and Middle Eastern. For gender classification, the model outputs either Woman or Man. Throughout the remainder of this paper, we will use these terms to refer to the attribute values.

In our initial experiment, we assessed the diversity scores of the datasets using the metrics described in Sec.~\ref{sec:methodology:data_analysis}, namely Richness, Evenness, and Dominance. By applying Eqs.~\ref{eq:dataset_richness},~\ref{eq:dataset_evenness}, and~\ref{eq:dataset_dominance}, we derived the diversity scores summarized in Table~\ref{tbl:dataset_diversity_score}. This table presents the diversity scores for each dataset, where the average score is a weighted combination of the attributes. Notably, Richness was assigned a weight of 0.25 because it is a dataset-level metric rather than an attribute-specific one, resulting in the same score across all attributes. The overall diversity score was then calculated as the mean of these weighted averages. Our findings indicate that AffectNet is the most diverse dataset, whereas the other datasets scored below 50 on average out of 100.

\begin{figure*}[t]
\centering
\caption{The 2D correlation matrix illustrates the relationships between different attributes across all the datasets (in percent). The diagram visualizes the data distribution for each attribute value and highlights the contribution of each attribute to others. This heat map reveals biases toward Neutral and Happy. The age groups [16$\sim$32] and [33$\sim$53] are the most prominent, while Man appears the most frequent gender. In terms of race, White is overrepresented, whereas Indian is underrepresented.}
\includegraphics[width=0.95\textwidth]{figs/2_correlation_matrix_2D_All.pdf}
\label{fig:correlation_matrix_2D}
\end{figure*}

In a subsequent experiment, we applied global-and-local-stereotypical metrics (Eq.~\ref{eq:data_distribution}) to evaluate data bias. Fig.~\ref{fig:data_distribution}-a depicts the expression distribution of datasets, highlighting that Neutral and Happy are the most prevalent, whereas Fear and Disgust are the least represented. Notably, Fer2013 and RAF-DB demonstrate a more balanced distribution of expressions compared to the pronounced imbalances observed in AffectNet and ExpW. 
Fig.~\ref{fig:data_distribution}-b reveals a pronounced bias in dataset across age groups, with the [16$\sim$32] and [33$\sim$53] ranges being significantly overrepresented. In contrast, samples from the [0$\sim$15] and [Over 54] age groups account for less than 5\% of the total data in all datasets.

As shown in Fig.~\ref{fig:data_distribution}-c, the number of Man samples is approximately 50\% more than Woman, though this gender bias is less pronounced compared to other attributes. Fig.~\ref{fig:data_distribution}-d, on the other hand, highlights a notable racial imbalance, with White samples constituting over 50\% of the total across all datasets. The distribution of Asian, Black, Latinx, and Middle Eastern samples is more balanced, while Indian samples remain consistently underrepresented. In conclusion, an examination of the data distribution across four essential attributes—age, gender, race, and expression—uncovers notable imbalances that may introduce bias into the model. Addressing these disparities is vital to ensuring fairness in FER systems.

\subsubsection{Attributes Correlation}
\label{sec:experimental_results:data_experiments:attribures_correlation}
Building upon the global-and-local-stereotypical analysis, we applied Eq.~\ref{eq:single_condition_data_distribution} to explore bias across the data samples. Additionally, examining the correlation between different attributes offers another meaningful approach for data analysis. For this purpose, we focused on expressions as the primary attribute and analyzed their correlation with other facial attributes of age, gender, and race.

Figure~\ref{fig:correlation_matrix_2D} illustrates that 27.7\% of the data is labeled Neutral, where 18.1\% belong to the [16$\sim$32] age group, and 9.3\% fall within [33$\sim$53]. In contrast, less than 0.3\% of Neutral label corresponds to the two remaining age groups. Regarding gender distribution, Neutral reveals a significant disparity between Man and Woman samples, with the number of Man being more than double that of Woman. For racial distribution within this expression, 15.5\% of the samples are associated with White, while the total representation of the other five racial groups is below 13\%.

The distribution of Happy exhibits a similar bias toward the age groups [16$\sim$32] and [33$\sim$53]. However, no significant gender disparity is observed between Man and Woman. In contrast, bias exists toward White group for this expression. Compared to Happy and Neutral, the bias across attributes is less pronounced for the other expressions, particularly across age groups and racial categories. Among the datasets, ExpW demonstrates the highest bias, while Fer2013 emerges as the least biased.

\begin{figure*}[t]
\centering
\caption{The 4D correlation matrix between different attributes of all the datasets is presented, with expressions represented in the rows and the columns divided first by age groups, followed by gender, and finally by race groups. This heat map highlights an uneven data distribution, where a great portion of data are underrepresented. This diagram reveals limited and imbalance diversity in the datasets.}
\includegraphics[width=1\textwidth]{figs/4_correlation_matrix_4D_All_age_gender_race.pdf}
\label{fig:correlation_matrix_4D}
\end{figure*}

The joint attribute correlation, calculated using Eqs.~\ref{eq:couple_condition_data_distribution} and~\ref{eq:triple_condition_data_distribution}, offers deeper insights into bias in dataset. Fig.~\ref{fig:correlation_matrix_4D} reveals that the Happy and Neutral expressions, [16$\sim$32] and [33$\sim$53] age groups, Man, and White are the most correlated values across attributes. Specifically, Happy and Neutral account for 60.7\% of the data, 98.6\% of the samples fall within the [16$\sim$53] age range, 62.6\% of the samples are labeled as Man, and White dominates with 59.3\% of the racial distribution.

\begin{figure}[b]
\centering
\caption{The confusion matrix for FER datasets (in percent) reveals that the high diagonal values for Fer2013, ExpW, and AffectNet point to inherent biases within these datasets. Notably, RAF-DB stands out as the only dataset that shows a significant correlation with the others.}
\includegraphics[width=0.5\columnwidth]{figs/confusion_matrix_FER_datasets.pdf}
\label{fig:confusion_matrix_FER_datasets}
\end{figure}

\subsubsection{Dataset Generality}
\label{sec:experimental_results:data_experiments:dataset_generality}
Based on the data distribution and correlation analysis, we examined demographic biases within the datasets. While demographic disparity is a primary source of bias, other factors such as illumination, head pose, image quality, background settings, lighting effects, and facial accessories also contribute to bias in dataset~\cite{srinivas2019face, singh2022anatomizing, udefi2023analysis}. These sources of bias vary significantly across different datasets.

To investigate general bias in FER datasets, we conducted an experiment where a model was trained to identify the dataset to which any given image belongs. Each image was labeled with its respective dataset name, and the XceptionNet model~\cite{chollet2017xception} was trained to predict the dataset label. The results, as shown in Fig.~\ref{fig:confusion_matrix_FER_datasets}, reveal biases inherent to FER datasets. While some correlation exists between RAF-DB and two other datsets (ExpW and AffectNet), Fer2013, ExpW, and AffectNet remain uncorrelated. This experiment underscores that differences in data collection methodologies—including image acquisition, processing, cropping, and storage—significantly affect FER datasets. Such biases, stemming from dataset-specific protocols, remain a relatively underexplored drawback in FER research.

\input{tbls/leave_one_dataset_out_accuracy}

An additional experiment was conducted to further investigate the generality and biases inherent in the datasets. In this experiment, one dataset was excluded, and a facial expression recognition model (XceptionNet) was trained using the remaining datasets. To ensure fairness, the model was first evaluated on the unseen validation sets of the datasets included in training. Subsequently, the generalizability of the excluded dataset was assessed by testing the model on its images. Since the excluded dataset was not part of the training process, the results reflect the dataset's bias. Table~\ref{tbl:leave_one_dataset_out_accuracy} compares the validation accuracies achieved on the included datasets with the test accuracy for the excluded dataset. Both Validation and Test accuracies in the table represent average per-class accuracies. Interestingly, the Test accuracies for RAF-DB and Fer2013 were higher than their Validation accuracies, while the opposite trend was observed for the ExpW and AffectNet datasets. For instance, when AffectNet was excluded, the Validation accuracy across the validation sets of RAF-DB, Fer2013, and ExpW was 57.0\%. However, the Test accuracy on the excluded AffectNet dataset dropped to 45.9\%. This result suggests that the challenges presented in the AffectNet dataset are not sufficiently captured by RAF-DB, Fer2013, and ExpW. A similar pattern was observed when the ExpW dataset was excluded. Conversely, for RAF-DB and Fer2013, the Validation accuracies were lower than their respective Test accuracies, demonstrating an inverse trend. Based on these findings, we conclude that the generalizability of the AffectNet and ExpW datasets is higher than that of RAF-DB and Fer2013. This indicates that RAF-DB and Fer2013 exhibit more inherent biases compared to AffectNet and ExpW.

\subsection{Fairness of Models} 
\label{sec:experimental_results:model_experiments}
After analyzing bias in the FER data, we further investigate bias and fairness within the models. As mentioned, four bias metrics are used to evaluate six deep models with different architectures. In the following sections, we describe our training methodology and then discuss the accuracy and bias of each model.

\begin{figure*}[t]
\centering
\caption{Model accuracy for each expression (in percent). In this experiment, models were trained on individual datasets and evaluated on their respective evaluation sets. $\mu$ and $\sigma$ denote the average accuracy and its standard deviation for each dataset, while MEAN and STD represent the overall average accuracy and standard deviation across each model. RAF-DB emerged as the least challenging dataset, whereas ExpW posed the greatest difficulty. Among the models, GPT-4o-mini delivered the highest accuracy, contrasting with the CLIP model, which recorded the lowest performance.}
\includegraphics[width=1.0\textwidth]{figs/accuracy_matrix_of_models_over_datasets.pdf}
\label{fig:accuracy_matrix_of_models_over_datasets}
\end{figure*}

\subsubsection{Training and Implementation Details}
In this experiment, we selected six deep learning architectures, including both CNN- and transformer-based models: MobileNet, ResNet, XceptionNet, ViT, CLIP, and GPT-4o-mini. MobileNet was used as a baseline CNN model due to its streamlined feed-forward structure without residual connections, making it computationally efficient for mobile and embedded applications. ResNet and XceptionNet, incorporating residual connections, were chosen for their ability to mitigate the vanishing gradient problem and improve model accuracy. Among the transformer-based models, ViT represented a state-of-the-art approach for computer vision tasks, leveraging self-attention mechanisms instead of traditional convolutional operations. CLIP, a large vision-language model designed for multi-modal learning, enabled joint processing of images and text. Lastly, GPT-4o-mini, a lightweight variant of GPT-4, was included for its capability to efficiently handle both NLP and vision tasks.

The experiment was conducted using Python 3.8.10, TensorFlow 2.9.2~\cite{abadi2016tensorflow}, and the Hugging Face~\cite{wolf2020transformers} libraries. Four Nvidia 1080 GPUs with 8 GB and 12 GB of memory were utilized. As a preprocessing step, image dimensions were fixed at $224\times224$. We conducted experiments on each dataset and model separately and also merged the training and validation sets of all datasets for subsequent experiments. Notably, after merging the datasets, we obtained 404,755 training samples and 22,858 validation samples.

In the first experiment, we fully trained four models, including MobileNet, ResNet, XceptionNet, and ViT, using each dataset and then evaluated their accuracy over their corresponding validation set. In the next experiment, we concatenated training sets, trained all the models, and evaluated them using the concatenated validation set. All four models were trained for 30 epochs with a fixed batch size of 35. The Adam optimizer was used with a learning rate of $10^{-3}$ for MobileNet, ResNet, and XceptionNet, and $10^{-5}$ for the ViT model. The two other Adam parameters were set to beta-1 = $0.9$ and beta-2 = $0.999$ for all models. Notably, we selected the categorical cross-entropy loss function for our training process.

Aside from the ViT model, we also leveraged two other transformer-based models, CLIP and GPT-4o-mini. These two models are large vision-language models capable of handling multiple tasks. Since CLIP is trained on multi-modal datasets and is open-source, we fine-tuned its decision-making head using each training set of different datasets, and their combined version. Then we evaluated its accuracy and fairness on each dataset and the concatenated version. We used the Adam optimizer with a learning rate of $10^{-3}$, beta-1 of $0.9$, and beta-2 of $0.999$. Additionally, we selected the categorical cross-entropy loss function for fine-tuning this model over 30 epochs with a batch size of 35.

GPT-4o-mini is another transformer-based vision-language model included in this research. However, since it is not open-source, we did not train or fine-tune it for this specific task. To evaluate this model and compare its accuracy and fairness with the other models, we only evaluated it using the validation set of each dataset and their combined version. We prompted all the facial images of the validation set into this model and asked it to assign one of the seven expressions used in this study. The prompt we used was:
"\textit{What is the expression of this person among happy, sad, surprise, fear, disgust, anger, and neutral, in only one word? Try to find one of these expressions, but if your prediction is out of these expressions, select the closest expression to it (among the 7 aforementioned expressions). If you cannot determine any expression, randomly select one of the mentioned expressions. If the predicted expression is another word in the family of the aforementioned expressions (happy, sad, surprise, fear, disgust, anger, neutral) change it to the corresponding word. For example, fearful is equal to fear, disgusted is equal to disgust, angry is equal to anger, surprised is equal to surprise, scared is equal to fear, and so on. If you randomly select an expression, do not need to explain, only generate the expression name.} We can leverage this data for comparing their accuracy, and more importantly, models' bias over each group of data, on over different demographic attributes, age, gender, and race.

\subsubsection{Facial Expression Recognition}
\label{sec:experimental_results:model_experiments:facial_expression_recognition}
Figure~\ref{fig:accuracy_matrix_of_models_over_datasets} shows the accuracy of each model across the datasets, categorized by expression. The results highlight a noticeable bias toward Neutral and Happy expressions, where they achieved significantly higher accuracy scores compared to the other expressions, in all the models. When comparing the expression distribution in Figure~\ref{fig:correlation_matrix_2D} with the accuracy results in Figure~\ref{fig:accuracy_matrix_of_models_over_datasets}, it becomes evident that data imbalance contributes to this bias. Nonetheless, the models displayed an ability to partially compensate for this imbalance. For example, although the ratio of Disgust to Happy data samples was highly skewed ($\frac{2.2}{41.8}=0.05$), the accuracy gap between these two expressions was considerably narrower ($\frac{29.1}{85.9}=0.33$), suggesting that the models made efforts to mitigate this bias during training.

\begin{figure*}[t]
\centering
\caption{Accuracy of each model across all datasets (in percent). In this experiment, all datasets were used for both training and validation. Notably, no model was able to achieve high scores for the two challenging expressions, Fear and Disgust. A comparison of the results reveals that GPT-4o-mini and ViT models demonstrated the most consistent accuracy scores, while CLIP struggled to achieve satisfactory performance.}
\includegraphics[width=0.80\textwidth]{figs/confusion_matrix_FER_models.pdf}
\label{fig:confusion_matrix_FER_models}
\end{figure*}

\input{tbls/general_accuracy_fer_models}

The average accuracy for each dataset, represented by $\mu$ in Fig.~\ref{fig:accuracy_matrix_of_models_over_datasets}, demonstrates a correlation between dataset Richness and complexity. Richer datasets tend to be more challenging, leading to lower average accuracies. The last two columns of this figure, labeled Mean and STD, summarize the mean accuracy and standard deviation for each model. These results show GPT-4o-mini achieved the highest accuracy while maintaining the lowest standard deviation. This performance can be attributed to GPT-4o-mini's extensive pretraining across diverse tasks and modalities, enabling it to extract features particularly well-suited for vision tasks. While this suggests GPT-4o-mini may be the fairest model, subsequent experiments reveal it exhibits significant biases.

In contrast to GPT-4o-mini, the CLIP model showed the worst performance, with an average accuracy below 50\% and a high standard deviation exceeding 25\%. These results indicate that the CLIP model is not well-optimized for FER tasks in its current form. Training the entire model, rather than just fine-tuning its decision-making head, could potentially improve its performance. Lastly, among the fully trained models (MobileNet, ResNet, XceptionNet, and ViT), ViT achieved the highest accuracy score, while ResNet exhibited the lowest standard deviation. Based solely on high accuracy and low standard deviation metrics, the models can be ranked from least to most accurate models as follows: GPT-4o-mini, ViT, ResNet, MobileNet, XceptionNet, and CLIP. We will discuss this ranking in the next sections, when we extract the metric-based bias scores.

Another experiment for FER models involved training each model using the training samples from all datasets and evaluating their performance on all validation sets. Fig.~\ref{fig:confusion_matrix_FER_models} displays the confusion matrix for these general models across all validation sets. At first glance, it is clear that Fear and Disgust are the most challenging expressions for all models, with none achieving more than 50\% accuracy in predicting them.

A closer analysis reveals significant confusion among certain expressions. For instance, pairs like Sad-Neutral, Disgust-Neutral, and Disgust-Anger pose considerable challenges, with erroneous predictions in the range of [18.8-35.3] percent, while zero values would be expected. While Fig.~\ref{fig:accuracy_matrix_of_models_over_datasets} showed GPT-4o-mini as the top performer across various datasets, the general model results, summarized in Table~\ref{tbl:general_accuracy_fer_models}, indicate that the ViT model achieved the highest average accuracy in this experiment. Additionally, MobileNet had the lowest standard deviation, indicating more consistency in its predictions. For further details on the experiments, please visit our~\href{https://github.com/MMHosseini/bias_in_FER}{GitHub}. 

So far, we have assessed the models' accuracy to evaluate their robustness against bias stemming from imbalanced data distribution. Our findings indicate that, overall, GPT-4o-mini and ViT are the most accurate models, while CLIP performed poorly. Although these results suggest that GPT-4o-mini and ViT might be the least biased models, subsequent experiments reveal different outcomes. In the next step, we will apply various bias metrics to further analyze the models' biases across different attributes.

\subsubsection{Bias Analysis}\label{sec:experimental_results:model_experiments:bias_analysis}
To investigate bias, we combined the training sets of all four datasets into a unified training set, applying the same approach to the validation sets. Four models—MobileNet, ResNet, XceptionNet, and ViT—were trained on the unified training set, and their bias scores were evaluated using the unified validation set. Additionally, the CLIP model was fine-tuned on the training set, and its bias scores were calculated using the validation set. For the GPT-4o-mini model, bias scores were directly quantified using the validation set without any additional training or fine-tuning. This section first explores bias across different attributes and expressions for each model, followed by a comparison of the models' overall bias. It is notable that all the four bias metrics used Eq.~\ref{eq:bias_score_metric} for their calculations.

\input{tbls/bias_equalized_odds}
\input{tbls/bias_equal_opportunity}

The first metric, Equalized Odds, examines the fairness of True Positive (TP) and False Positive (FP) rates across different groups. Table~\ref{tbl:bias_equalized_odds} highlights that the Neutral and Happy expressions were the most biased, with nearly all maximum biases for age, gender, and race originating from these two expressions. For the Age attribute, Surprise emerged as the least biased expression, whereas for Gender and Race attributes, Fear showed the lowest bias. The Max column of this table reveals that the Age attribute consistently exhibited the lowest bias across all models. In contrast, Gender displayed the highest bias in four out of six models. However, for the CLIP and GPT-4o-mini models, Race was the most biased attribute. The Mean and STD columns further emphasize that, on average, Race was the most biased attribute across all models, while Age demonstrated the least bias. When comparing the Mean and STD values among the models, the Equalized Odds metric indicates that the ViT model was the most biased overall, while ResNet demonstrated the lowest bias score.

\input{tbls/bias_demographic_parity}
\input{tbls/bias_treatment_equality}

Another metric we introduced in Sec.\ref{sec:methodology:model_analysis} is Equal Opportunity, which focuses on the ratio of TP predictions across different groups. Information about this metric is provided in Table~\ref{tbl:bias_equal_opportunity}. Unlike Equalized Odds, this table shows that the Happy expression was not the most biased in terms of Equal Opportunity. For the Age attribute, Neutral ranked highest in bias, while for Gender and Race attributes, Happy and Surprise expressions exhibited the highest bias scores, respectively. Examining the Max, Mean, and STD columns of this table reveals that Race is the most bias-affected attribute across all models. Based on this metric, GPT-4o-mini exhibited the highest bias, while ResNet and MobileNet achieved the lowest bias scores. Comparing the Mean values from Tables~\ref{tbl:bias_equalized_odds} and~\ref{tbl:bias_equal_opportunity}, it becomes evident that Equal Opportunity demonstrates more bias than Equalized Odds. For instance, the bias range for Age in Equalized Odds was [1.7-2.3], while Equal Opportunity showed a higher bias range of [3-4.6].

Demographic Parity analyzes the likelihood of any group receiving a particular outcome. The data in Table~\ref{tbl:bias_demographic_parity} shows that, compared to other bias metrics, Demographic Parity results in lower bias scores. For the Age attribute, the highest bias was observed in the Neutral expression, while Happy, Sad, and Surprise had minimal effects on the Demographic Parity of Age. In contrast, Happy was the most biased expression for the Gender attribute, while Fear and Sad showed the least bias. For the Race attribute, Neutral and Happy were the primary sources of bias, while Fear exhibited the lowest bias across all models. Although the Max column highlights higher biases for the Gender attribute, the Mean column underscores Race as the most biased attribute overall. As with the other bias metrics, Age remained the least biased attribute. The Mean and STD columns illustrate that for the Age attribute, the CLIP model was the most biased model, while for Gender and Race, the ViT model was the most biased model. Overall, considering the Mean bias scores, the ViT model emerged as the most biased, while ResNet demonstrated the least bias.

The final bias measure we examined is Treatment Equality, which assesses the distribution of misclassified samples across different groups. Table~\ref{tbl:bias_treatment_equality} demonstrates that Treatment Equality resulted in higher bias scores. A detailed expression-level analysis shows that Sad, Fear, and Disgust were the main sources of bias for the Age attribute. While Happy and Anger exhibited the highest biases for the Gender attribute, Happy significantly influenced the bias score for Race in three out of the six models. Among the attributes, Race had the highest bias scores, while Age exhibited the lowest. This trend was consistent with our findings in the Equalized Odds and Equal Opportunity metrics. Comparing the Max column in Table~\ref{tbl:bias_treatment_equality} with the Max columns in Tables~\ref{tbl:bias_equalized_odds},~\ref{tbl:bias_equal_opportunity}, and~\ref{tbl:bias_demographic_parity} shows that Treatment Equality exhibited higher bias scores across all attributes. In a model-wise comparison of Treatment Equality, CLIP showed the lowest bias score, while the highest bias was observed in GPT-4o-mini.

\input{tbls/bias_score_for_all_models}

To sum up, we analyzed the bias of the models for each attribute and expression, followed by a model-based bias comparison. Based on our experiments, the ViT model showed the highest bias in the Equalized Odds and Demographic Parity metrics, while GPT-4o-mini exhibited significant bias in the Equal Opportunity and Treatment Equality metrics. On the other hand, ResNet was the most robust model against bias in three out of four bias metrics.

To generate a unique mathematical bias score, we utilized Eqs.~\ref{eq:bias_score_attribute} and~\ref{eq:bias_score} to calculate the final bias score for each model. Table~\ref{tbl:bias_score_for_all_models} presents the bias scores of each model across different attributes and bias metrics. As outlined in Eq.~\ref{eq:bias_score_metric}, we used the maximum bias score of each model (i.e., the Max column from Tables~\ref{tbl:bias_equalized_odds},~\ref{tbl:bias_equal_opportunity},~\ref{tbl:bias_demographic_parity}, and~\ref{tbl:bias_treatment_equality}) as the bias score for each model. In the next step, we calculated the average bias score for each model across all attributes using Eq.~\ref{eq:bias_score_attribute} (the average of each row in Table~\ref{tbl:bias_score_for_all_models}). Finally, the overall bias score for each model was obtained by averaging the bias scores for each attribute by Eq.~\ref{eq:bias_score}. In conclusion, the results showed that GPT-4o-mini was the most biased model, followed by ViT in second place. Additionally, this table highlights that residual-based models, including ResNet and XceptionNet, were the least biased models. Ultimately, our experiments rank the models from the most biased to the least biased as follows: GPT-4o-mini, ViT, CLIP, MobileNet, XceptionNet, and ResNet. Notably, this research focused solely on the FER task, and the observed bias in the models could be evaluated on different tasks, potentially yielding varying bias scores.

% =================================================================================
% ======================== Conclusion ============================
% =================================================================================
\section{Discussion and Future Works} 
\label{sec:discussion_and_future_works}
This research presented a comprehensive analysis of bias and fairness in FER. We examined four in-the-wild datasets (AffectNet, ExpW, Fer2013, and RAF-DB) and six popular deep models (MobileNet, ResNet, XceptionNet, ViT, CLIP, and GPT-4o-mini). We mainly focused on studying bias in demographic attributes, including age, gender, and race, as well as label distribution. 

Based on the experiments conducted in this research, bias was examined in both datasets and models. Initially, bias within the datasets was analyzed using the Richness, Evenness, and Dominance metrics. The results indicated that ExpW is the most biased dataset, whereas AffectNet was identified as the least biased. Additionally, six state-of-the-art models were evaluated using four bias metrics: Equalized Odds, Equal Opportunity, Demographic Parity, and Treatment Equality. The findings revealed that, despite achieving high accuracy, the GPT-4o-mini and ViT models exhibited the highest levels of bias.

The contributions of this research are pivotal for advancing fairness in FER tasks. Below, we outline several open research directions in FER that can build on this study to address bias and promote fair decision-making:

\begin{itemize}
\item Investigating non-demographic sources of bias, such as illumination, background noise, head pose, gestures, eye gaze, and hair color, within datasets and their impact on fairness of models.
\item Reporting the bias score of trained models for FER tasks alongside their accuracy, which enables a comprehensive evaluation, ensuring both the effectiveness and fairness of the models.
\item Comparing the biases present in lab-controlled datasets versus in-the-wild datasets, and analyzing their influence on model training and fairness outcomes.
\item Exploring face generator models as potential sources of bias in future datasets and developing fair data generation methods to address these issues.
\item Developing more robust FER models by incorporating fairness constraints into the loss function during training. Future work could examine methods for implementing such constraints and their effects on model accuracy.
\item Analyzing the evolution of bias throughout the different stages of model development, including pretraining, training, fine-tuning, and domain adaptation.
\item Exploring the role of self-supervised learning methods in reducing bias within datasets and evaluating how they can contribute to fair model development.
\item Evaluating video-based datasets and models, where temporal dynamics between sequences are critical, to mitigate bias in video-based FER tasks.
\item Investigating bias in multi-modal models that leverage data from diverse sources such as images, text, video, audio, and physiological signals.
\end{itemize}

This research highlighted the significance of considering bias and fairness in FER tasks. Establishing a foundation for enhancing the equity of FER models is essential. These improvements have the potential to benefit critical applications, including human-computer interaction (HCI), mental health monitoring, and surveillance. Future studies will focus on proposing methods to address bias in FER tasks while maintaining high accuracy.
% =================================================================================
% =================================================================================
% =================================================================================
% \clearpage
\bibliographystyle{IEEEtran}
\bibliography{ref.bib}
% \input{bio}

% That's all folks
\end{document}