\section{Literature Review}
\label{sec:literature_review}
FER has many applications, such as sensitive biometrics~\cite{igene2024face}, where biased data or algorithms can cause irreparable damages. Understanding the sources of bias is essential for effectively addressing its challenges. Bias can originate from both datasets and models. Factors such as data collection and annotation processes, model architecture, objective functions, and evaluation methodologies are among the primary sources of bias. Figure~\ref{fig:bias_sources_in_fer} presents a general classification of bias sources. Addressing bias in deep machine learning methods can significantly enhance the reliability and accuracy of FER models. More importantly, developing fair and unbiased deep methods for FER methods potentially ensures equitable treatment across diverse demographic groups~\cite{amigo2023mitigating}. Thus, dealing with bias in FER has recently raised attention from researchers. Examining data distribution in existing FER datasets~\cite{dominguez2024metrics, dominguez2022gender, dominguez2022assessing, li2020deeper, huber2024bias, cheong2023causal}, as well as analyzing the algorithms used in the FER 
 models~\cite{xu2020investigating, yu2023exploring, dooley2024rethinking, kolahdouzi2023toward, liang2023benchmarking, abdullah2024effects} are the key approaches for studying bias in FER. In the following section, we review recent studies that address bias in FER models and datasets.

\begin{figure}[b!]
\centering
\caption{Bias can originate from datasets and models. In datasets, prominent sources of bias include issues in data collection, such as demographic disparities, variations in illumination and lighting conditions, gestures, head poses, and cultural differences in emotional interpretation. In models, key bias sources include architecture design, training parameters, overfitting to specific demographic groups, and the selection of evaluation metrics.}
\includegraphics[width=0.98\columnwidth]{figs/bias_sources_in_fer.pdf}
\label{fig:bias_sources_in_fer}
\end{figure}

\subsection{Bias in FER Datasets} 
\label{sec:literature_review:bias_in_fer_datasets}
Bias in datasets is broadly categorized into representational bias and stereotypical bias~\cite{dominguez2024metrics, dominguez2022gender, dominguez2022assessing}. Representational bias focuses mainly on the demographic diversity of data, including age, gender, and race attributes. This bias is evaluated using metrics like Richness, Evenness, and Dominance (for definitions see Sec.~\ref{sec:methodology:data_analysis}). On the other hand, stereotypical bias investigates the correlation between specific subgroups and their corresponding labels, which can lead to unfair or biased outcomes. 

Another categorization of bias in dataset divided them into intrinsic and extrinsic biases~\cite{mavadati2013disfa, srinivas2019face, singh2022anatomizing}. Intrinsic bias inherent to the process of data collection, i.e., the way data are collected, sampled, and labeled. Demographic attributes, such as age, gender, and race, as well as head-pose, gesture, eye-gaze, and expression intensity are studied as intrinsic biases. On the contrary, extrinsic bias refers to the disparities that could affect the model's performance. For example, lab-controlled datasets can lead to high biases in models, because they are not collected based on the real-world scenarios. Extrinsic bias mainly stems from environmental conditions, such as camera resolution, background noise, illumination, and annotator bias. There are other definitions of bias in dataset. While Cheong~\etal~\cite{cheong2023causal} grouped bias sources as observed and non-observed sources, Terhorst~\etal~\cite{terhorst2021comprehensive} classified bias sources into demographic and non-demographic groups. Despite all these different bias categorizations, a strong consensus is that demographic attributes, including age, gender, and race, are the most effective sources of bias in dataset that need to be studied.

In the following we review the most notable works on this topic. Dominguez~\etal~\cite{dominguez2024metrics} proposed seventeen representational metrics, where they ranked AffectNet~\cite{mollahosseini2017affectnet}, ExpW~\cite{zhang2018facial}, RAF-DB~\cite{li2017reliable}, and Fer2013~\cite{goodfellow2013challenges} among the most diverse FER datasets. It is notable that their stereotypical metrics showed different results for this four datasets where they did not achieve good scores and rankings. Another research by Li~\etal~\cite{li2020deeper} studied the intrinsic bias among seven different FER datasets and reported that the main reasons for bias in datasets are cultural differences and data collection conditions. Huber~\etal~\cite{huber2024bias} studied bias in synthetic facial datasets where they revealed that synthetic datasets follow the same bias patterns as the authentic datasets used for training their corresponding models. Finally, Cheong~\etal~\cite{cheong2023causal} used a directed acyclic graph (DAG) to explore the relation between sources of bias. They introduced gender and age as the strongest sources of bias in emotion labeling. These experiments highlight the importance of dataset analysis in FER tasks, especially demographic attributes and their correlation with the labels for each subgroup. In Sec.~\ref{sec:methodology:data_analysis} we review some common metrics for data analysis, and in Sec.~\ref{sec:experimental_results:data_experiments} demonstrate their results.

\subsection{Bias in FER Algorithms} 
\label{sec:literature_review:bias_in_fer_algorithms}
The analysis of bias in  FER algorithms involves examining how deep models and algorithms impact various demographic groups, aiming to enhance fairness and improve emotion recognition accuracy. Below, we review recent studies addressing algorithmic bias. While some studies tried to reduce the bias of the model's using data compensation, some experiments revealed that there are some inherent biases in the models irrelevant to the data.

To mitigate the effects of imbalanced data distribution, Yu~\etal~\cite{yu2023exploring} utilized unlabeled data in a semi-supervised learning framework. This framework leveraged unlabeled facial data by assigning pseudo-labels to underrepresented samples in the datasets. As a result, their method achieved a more balanced data distribution and improved the overall accuracy of the FER system. In a related study, Kolahdouzi~\etal~\cite{kolahdouzi2023toward} introduced a Hilbert-space kernel to compute the mean distribution of specific demographic groups. To reduce bias, they subtracted a facial image's embedding from the extracted mean, obscuring sensitive attributes such as age, gender, and race. Furthermore, Liang~\etal~\cite{liang2023benchmarking} investigated the influence of facial attributes on model performance. They categorized these attributes into protected attributes (e.g., gender and race) and unprotected attributes (e.g., pose and lighting). Their findings demonstrated that protected attributes affect model performance, contributing to bias in predictions.

In contrast, some researchers argue that biases originate from the models. Dooley~\etal~\cite{dooley2024rethinking} hypothesized that bias stems from the architecture of deep models. They employed a joint CNN architecture with one component emphasizing fairness and another focusing on extracting hyperparameters to mitigate bias. Their results indicated that bias remains inherently tied to model architecture. Abdullah~\etal~\cite{abdullah2024effects} conducted experiments focusing on the attention areas allocated to faces by the models. They observed that the attention area assigned to dark-skinned faces was smaller compared to lighter skin tones, revealing an inherent bias in the models. Xu~\etal~\cite{xu2020investigating} compared a baseline model, ResNet-18~\cite{he2016deep}, with two disentangled and attribute-aware models. Their experiments demonstrated that data augmentation did not significantly enhance the accuracy of the baseline model. Similarly, Amigo~\etal~\cite{amigo2023mitigating} proposed a debiasing variational autoencoder to address bias in models. Their findings showed that, despite training on different datasets, models could perpetuate biases. Furthermore, Dominguez~\etal~\cite{dominguez2024less} revealed that stereotypical biases are intrinsic to models, regardless of whether the training data is balanced or imbalanced. These findings underscore the importance of studying bias and fairness in both data and algorithms.

\subsection{Perspectives}
Bias and fairness in facial expression recognition are critical to preventing discrimination, ensuring equitable systems, improving performance across diverse populations, and upholding ethical standards. Real-world FER applications rely on in-the-wild datasets, making it imperative to thoroughly examine commonly used datasets to identify and address bias issues. While some studies have partially investigated these datasets, a more comprehensive exploration of popular FER datasets is needed.

At the same time, recent advancements in model architectures, such as deep convolutional neural networks, vision transformers, and large vision-language models, have significantly improved FER accuracy~\cite{rodrigo2024comprehensive, aldahoul2024exploring, zhao2024enhancing}. However, despite their broad adoption, these models must be rigorously evaluated for fairness and bias mitigation. This study provides an in-depth analysis of bias in four prominent FER datasets—AffectNet, ExpW, Fer2013, and RAF-DB—and assesses six state-of-the-art models, including MobileNet, ResNet, XceptionNet, ViT, CLIP, and GPT-4o-mini.

% =================================================================================
% ================================== Methodology ==================================
% =================================================================================