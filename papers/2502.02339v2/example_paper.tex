%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}
\usepackage[accepted]{icml_to_arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{graphicx}
% \usepackage{array}
\usepackage{colortbl}
% \usepackage{arydshln}
\usepackage{enumitem}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{xcolor} % 使用xcolor宏包

\definecolor{backblue}{RGB}{210, 230, 250}
\newcommand{\high}{\cellcolor{backblue}}
\definecolor{backred}{RGB}{255, 223, 223}
\newcommand{\highr}{\cellcolor{backred}}
\definecolor{backgreen}{RGB}{220,244,229}
\newcommand{\highg}{\cellcolor{backgreen}}
\definecolor{back_deepblue}{RGB}{180, 210, 240}
\newcommand{\highdeep}{\cellcolor{back_deepblue}}

\definecolor{back_deepred}{RGB}{255, 200, 200}
\newcommand{\highrdeep}{\cellcolor{back_deepred}}

\definecolor{back_deepgreen}{RGB}{190, 230, 210}
\newcommand{\highgdeep}{\cellcolor{back_deepgreen}}
% \definecolor{mygray}{gray}{.92}
\definecolor{mygray}{gray}{0.95}
% \definecolor{mygray}{rgb}{242,242,242}

\definecolor{greentable3}{rgb}{0,0.5,0}
% \definecolor{greentable3}{rgb}{0,174,239}
% \newcommand{\cmark}{\textcolor{greentable3}{\ding{\Checkmark}}}
% \newcommand{\xmark}{\textcolor{red}{\ding{\XSolidBrush}}}
\newcommand{\cmark}{\textcolor{greentable3}{\ding{51}}} % \ding{51} 是checkmark
\newcommand{\xmark}{\textcolor{red}{\ding{55}}} % \ding{55} 是xmark

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{AStar: Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking}

\begin{document}

\twocolumn[
\icmltitle{Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jinyang Wu}{yyy}
\icmlauthor{Mingkuan Feng}{yyy}
\icmlauthor{Shuai Zhang}{yyy}
% \icmlauthor{Pengpeng Shao}{yyy}
\icmlauthor{Ruihan Jin}{yyy}
\icmlauthor{Feihu Che}{comp}
\icmlauthor{Zengqi Wen}{comp}
\icmlauthor{Jianhua Tao}{yyy,comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Automation, Tsinghua University, Beijing, China}
\icmlaffiliation{comp}{Beijing National Research Center for Information Science and Technology, Beijing, China}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Shuai Zhang}{zhang$\_$shuai@mail.tsinghua.edu.cn}
\icmlcorrespondingauthor{Jianhua Tao}{jhtaoo@tsinghua.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an \textbf{A}utomated \textbf{S}tructured \textbf{t}hinking paradigm for multimod\textbf{a}l \textbf{r}easoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\%$) while maintaining substantial data and computational efficiency.


\end{abstract}

\section{Introduction}\label{sec1}

\begin{figure}[ht!]
\centerline{\includegraphics[width=0.96\linewidth]{Figure1.pdf}}
\caption{Performance comparison on the MathVerse benchmark. Our AStar framework achieves competitive results against most open-sourced MLLMs and closed-source ones, showing outstanding structured thinking and reasoning abilities.}
\vskip 0.1in
\label{Figure1}
\end{figure}

Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across diverse tasks and domains \cite{yin2023survey,openai2024gpt4o,chen2024expanding}, such as autonomous driving \cite{cui2024survey}, and visual question answering \cite{hartsock2024vision}. Their proficiency in complex multimodal reasoning, particularly in mathematical tasks involving visual content, has emerged as a critical benchmark for evaluating fundamental cognitive abilities towards strong artificial intelligence \cite{searle1980minds,goertzel2007artificial,wang2024exploring,qiao2024we}. Mastering multi-step visual reasoning requires the integration of multimodal information, along with rigorous adherence to complex rules and sophisticated problem-solving strategies, presenting significant challenges for existing MLLMs \cite{zhang2024multimodal,wang2024measuring}.  

\begin{figure*}[ht!]
\vskip 0.05in
\begin{center}
\centerline{\includegraphics[width=0.96\textwidth]{Figure2.pdf}}
\caption{Schematic comparison between AStar and two mainstream structured reasoning methods. $\mathcal{L}_{(\cdot)}$ denotes the training optimization objective. (a) Search-based methods suffer from computational inefficiency due to extensive solution space iterations. (b) Teacher-guided training optimizes models using distilled rationales from powerful models like GPT-4o, but requires substantial data and computational resources, resulting in low-efficiency pattern extraction and poor data utilization. (c) Our approach effectively combines MLLMs' internal implicit reasoning capabilities with explicitly extracted insights, achieving a compelling balance between performance and efficiency.}
\label{Figure2}
\end{center}
\vskip -0.1in
\end{figure*}


Inspired by recent advances in System 2 slow-thinking reasoning systems like OpenAI o1 \cite{o1} and QVQ \cite{qvq}, there is growing interest in incorporating structured thinking into MLLMs \cite{xu2024llava,dong2024insight,du2025virgo}. This direction aims to address the limitations of conventional MLLMs that often rely on simple `direct prediction' modes due to the scarcity of high-quality long-chain reasoning data \cite{xu2024llava,luo2025ursa}. According to existing literature, two primary approaches have emerged for implementing slow-thinking reasoning systems: explicit search and teacher supervision-guided training. The first approach leverages explicit search structures (e.g., Monte Carlo tree search, MCTS) with specialized reward models to guide the exploration of solution paths \cite{dong2024progressive,yao2024mulberry}. The second approach focuses on distilling structured reasoning patterns through long-form Chain-of-Thought (CoT) \cite{wei2022chain,zhang2024multimodal,luo2025ursa} instruction data, typically requiring supervision from closed-source models like GPT-4o for data synthesis.


Despite these advances, current reasoning paradigms face three critical limitations (Figure \ref{Figure2}). First, search-based methods \cite{dong2024progressive} suffer from computational inefficiency due to extensive solution space iterations. Second, teacher-guided training methods \cite{hu2024visual,yao2024mulberry,luo2025ursa} typically require substantial training data ($\ge$100K) and computational resources to implicitly extract reasoning patterns, resulting in low efficiency and poor data utilization. They also heavily depend on proprietary models like GPT-4o for data synthesis, making them impractical for researchers outside major enterprises. Third, static and predefined reasoning processes \cite{xu2024llava,thawakar2025llamav} constrain flexibility, leaving the reasoning potential of MLLMs underexplored.


\vskip 0.05in
To address these challenges, we propose AStar, an \textbf{A}utomated \textbf{S}tructured \textbf{T}hinking paradigm for multimod\textbf{A}l \textbf{R}easoning via MCTS. Our approach introduces a novel mechanism for automatically deriving high-level cognitive reasoning patterns from limited data (500 samples) using MCTS-powered hierarchical structures (aiming to address the second limitation). Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates internal and external advantageous attributes, enabling efficient adaptive inference with minimal tree iterations, thereby tackling the first and third limitations. \textit{This novel reasoning paradigm effectively combines MLLMs' internal implicit reasoning capabilities with external explicit reasoning guidelines, achieving a compelling balance between performance and efficiency.}
% This novel paradigm achieves a compelling balance between performance and efficiency.

Specifically, our method comprises three steps: (1) visual reasoning action definition, (2) MCTS-powered thought card construction, and (3) adaptive reasoning and verification. \underline{\textit{First}}, we define six atomic reasoning actions as building blocks of chain-structured reasoning patterns (termed ``thought cards'' that serve as reference insights during inference). These actions simulate human-like cognitive behaviors, including problem decomposition and reasoning step reflection. \underline{\textit{Second}}, using a small seed dataset (500 samples), we apply MCTS to derive reference reasoning patterns to construct multiple thought cards. \underline{\textit{Finally}}, in the reasoning stage, we select the five optimal thought cards most aligned with the target problem's cognitive complexity. With these reasoning guidelines, we perform visual reasoning and validate the final solutions via self-consistency checks or outcome reward models. Experiments demonstrate that AStar exhibits impressive reasoning performance with enhanced efficiency, comparable to powerful closed-source models like GPT-4o (Figure \ref{Figure1}). Our main contributions are:
% With these reasoning guidelines, we perform visual reasoning and validate the final solution via self-consistency checks or outcome reward models.

\begin{figure*}[ht!]
% \vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{Figure3.pdf}}
\caption{Flowchart of our proposed method AStar. This framework consists of three main parts: (1) Visual Atomic Reasoning Action Definition; (2) MCTS-Powered Thought Card Construction; (3) Adaptive Reasoning and Verification.}
\label{Figure3}
\end{center}
\vskip -0.1in
\end{figure*}


\vskip -0.6in
\begin{itemize}[leftmargin=1.18em]
% \setlength{\itemsep}{-0.01in}
    % \vskip -0.2in
    \item \textbf{Automated Reasoning Paradigm}: proposing an MCTS-based automated approach for generating and selecting the optimal reasoning patterns.
    % \vskip -0.2in
    \item \textbf{Efficient Action-Chain Guided Reasoning}: providing explicit guidance for each step of the visual reasoning process, enhancing structured thinking capabilities.
    % \item \textbf{Superior Performance}: achieving 32.3$\%$ score on the challenging MathVision benchmark with a 7B backbone, surpassing GPT-4o (30.4$\%$).
    \item \textbf{Superior Performance}: achieving 54.0$\%$ score on the challenging MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\%$).
    \item \textbf{Improved Efficiency}: achieving comparable performance to recent tree-based methods while reducing inference overhead by $6.4\times$, and matching training-based methods while requiring $520\times$ less prior data.
\end{itemize}


\section{Related Work}\label{sec2}
\textbf{Multimodal Reasoning}
$\,$ Recent advancements in MLLMs have demonstrated robust capabilities across diverse domains, including visual understanding \cite{10445007}, mathematics \cite{zhang2024mavis,du2025virgo}, and scientific inquiries \cite{zong-qiu-2024-gaokao}. Despite these achievements, complex multimodal reasoning remains challenging due to its demands on both visual perception and high-level cognition. Inspired by OpenAI o1's impressive performance, recent approaches \cite{zhang2024improve,xu2024llava,thawakar2025llamav} attempt structured reasoning with pre-defined stages, enhancing MLLMs' CoT capabilities \cite{zhang2024multimodal}. However, their rigid structure limits flexibility across different tasks, overlooking the importance of adaptive reasoning in unleashing multimodal reasoning potential \cite{wang2024enhancing}. Our approach addresses this by introducing a hierarchical tree structure that enables task-specific reasoning path generation and selection.

\vskip 0.0827in
\textbf{Tree-based Search}
$\,$ Tree structures have demonstrated significant potential in language models \cite{zhang2024accessing,qi2024mutual,wu2024beyond}. Recent efforts explore applying these tree search methods to search effective reasoning paths for MLLMs. While AR-MCTS \cite{dong2024progressive} enhances multimodal reasoning by integrating MCTS with active retrieval, its extensive iteration requirements and computational overhead limit practical applications. Similarly, Mulberry \cite{yao2024mulberry} leverages tree structures to distill 260K long-chain reasoning data from powerful models like GPT-4o, but requires substantial computational resources and high-capacity teacher models. These methods struggle to achieve an optimal balance between performance and efficiency. To address these limitations, we propose incorporating high-level reasoning abstractions into MCTS, achieving competitive performance with higher efficiency.




\section{Methodology}\label{sec4}
\textbf{Overview of AStar} This section introduces AStar in detail. As shown in Figure \ref{Figure3} and \cref{algo:AStar}, our approach consists of three steps:
% \vspace{-1.0em}
\begin{itemize}[leftmargin=1.0em]
    \item \textit{Visual Reasoning Action Definition}: Establish six human-like reasoning actions as building blocks for chain-structured thought cards.
    \item \textit{MCTS-Powered Thought Card Construction}: Leverage MCTS to systematically construct thought cards, which serve as reference insights during inference.
    \item \textit{Adaptive Reasoning and Verification}: Dynamically select and execute optimal reasoning patterns based on problem complexity, followed by solution verification.
\end{itemize}


\subsection{Visual Reasoning Action Definition}\label{sec4.1}
Understanding human complex reasoning is crucial for modeling cognitive processes \cite{Jaffe23}. Existing studies distinguish between two cognitive systems: System 1 and System 2 \cite{Kahneman2011,da2023system}. While ``System 1'' represents fast, intuitive, yet error-prone thinking, ``System 2'' involves slow, deliberative thinking with superior performance. With the emergence of advanced models like OpenAI o1 \cite{o1} and QVQ \cite{qvq}, developing efficient ``System 2'' approaches to emulate human cognitive processes has gained significant research attention \cite{xu2024llava,yao2024mulberry,thawakar2025llamav}. Inspired by this, we introduce six vision-language reasoning actions to bridge the gap between model reasoning and human cognition: \textit{Visual Parsing (VP, $a_{1}$)}, \textit{System Analysis (SA, $a_{2}$)}, \textit{One-Step Thought (OST, $a_{3}$)}, \textit{Chain-of-Thought (CoT, $a_{4}$)}, \textit{Divide and Conquer (DC, $a_{5}$)}, \textit{Self-Reflection (SR, $a_{6}$)}. Details are provided in Appendix \ref{B.1}.

\subsection{MCTS-Powered Thought Card Construction}\label{sec4.2}
Following the action definition, we introduce ``\textit{thought cards}'' as structured reasoning templates to guide inference in \textit{Sec.} \ref{sec4.3}. Using a small seed dataset, we first derive reasoning paths (Phase 1) and then distill them into high-level thought cards (Phase 2). These cards serve as prior insights during inference, providing structured guidance for efficient problem-adaptive reasoning.


% \vspace{0.7em} \noindent 
\textbf{Phase 1: Acquire reasoning paths for seed data} 
$\,$ As shown in \cref{Figure3}, we employ MCTS to iteratively optimize the solution search process, generating high-quality reasoning paths for the seed dataset. This design leverages MCTS's systematic exploration \cite{NEURIPS2021_d5eca8dc} and MLLMs' inherent reasoning capabilities \cite{yin2023survey,wang2024exploring}. We formulate each multimodal reasoning problem $x$ (consisting of input question and images) as a tree search problem, where $x$ represents the root node and subsequent nodes denote reasoning steps (actions and corresponding outcomes) generated by a policy MLLM $\pi_{\theta}$. We define the state $S_{t-1}$ as the trajectory $x, s_{1},...,s_{t-1}$, where $S_{0}=x$. The next step is sampled as $s_{t}\sim \pi_{\theta}(S_{t-1})$. To guide tree expansion, we define $Q(s)$ as the reward value for node $s$. Initially, all unexplored nodes are assigned $Q(s_{i})=0$. They are updated using a weighted average between the parent's current value and its child node's value:

\begin{equation}
    Q(p) \leftarrow (1-\alpha)Q(p)+\alpha Q(s) \label{eq1}
\end{equation} 
where $\alpha$ is a discount factor for future rewards. For terminal nodes, following \citet{zhou2024language,wu2024beyond}, we adopt the likelihood of self-consistency majority voting as the reward value, enabling supervision-free generalization.

\begin{algorithm}
\caption{Multimodal Reasoning with AStar}
\label{algo:AStar}
\begin{algorithmic}
    \STATE {\bfseries Input:} a policy model $\pi_{\theta}$; a multimodal test question $x_t$; a set of seed data $D_{s}$
    \STATE \textcolor{gray}{// 3.1. Visual Reasoning Action Definition}
    \STATE Initialize action space $A = \left \{ a_1, a_2, a_3, a_4, a_5, a_6 \right \} $ \\

    \STATE \textcolor{gray}{// 3.2. MCTS-Powered Thought Card Construction}
    \STATE $D \leftarrow \left [ \; \right ] $; $Cards \leftarrow \left \{  \right \} $
    \FOR{\textit{$(x,y) \in$ $D_{s}$}}
    \STATE $\{x, y, P\} \leftarrow \text{MCTS}(\pi_{\theta};x)$ 
    \IF{\textit{found an valid reasoning path}}{
     \STATE Find $p_{\text{best}}$ from $P$
     \STATE Add $\{x, p_{\text{best}}\}$ into $D$
     \STATE Update $Cards$ from $D$}
    \ENDIF
    \ENDFOR

    \STATE \textcolor{gray}{// 3.3. Adaptive Reasoning and Verification}
    \STATE $c \leftarrow \text{CardMatch}(Cards;x_t)$ 
    \STATE $y_{t} \leftarrow \text{ReasonAndVerify}(\pi_{\theta};x_t;c)$ 
  \STATE {\bfseries Output:} {the optimal reasoning trajectory $y_{t}$}
\end{algorithmic}
\end{algorithm}

Specifically, this phase comprises four MCTS operations:

\textit{(1) Selection}. This operation identifies promising nodes for expansion. Starting from the root node, we iteratively select child nodes using the Upper Confidence Bounds applied to Trees (UCT) \cite{11871842_29} until reaching a leaf node:
\begin{equation}
    UCT(s) = Q(s) + w\sqrt{\frac{lnN(p) }{N(s)} }  \label{eq2}
\end{equation}
where $Q(s)$ is the reward value for node $s$, $N(s)$ is the visit count, $p$ is the parent node, and $w$ is the exploration weight. The node with the highest UCT value is selected for subsequent phases, balancing exploration and exploitation.

% \vspace{0.5em}
\textit{(2) Expansion}. The selected node $s$ is expanded by sampling $n$ actions from $\pi_{\theta}$ and generating corresponding reasoning outcomes. These $n$ child nodes are added to the tree and stored in an external memory structure.

\textit{(3) Simulation}. Starting from the selected node, we iteratively sample and expand nodes until reaching a terminal state (maximum depth or answer node). 


% \vspace{0.5em}
\textit{(4) Backpropagation}. Upon simulation completion, node information is updated along the simulation path $s_{0},...s_{d}$. Visit counts are incremented ($N(s) \leftarrow N(s) + 1$), and node value $Q(s)$ is propagated backward to its parent node $p$ using Equation \ref{eq1}. These updated values are used to guide subsequent UCT-based node selection.

\textbf{Phase 2: Distill paths into thought cards}
$\,$ After executing MCTS, we obtain a tree structure for each seed dataset question, yielding multiple valid reasoning paths that constitute the path set $P$. Inspired by the concept of Value of Computation (VOC) \cite{RUSSELL1991361}, which optimizes the trade-off between computational benefits and costs, we propose a VOC-inspired selection metric to identify the optimal reasoning trajectory from candidate solutions:
\begin{equation}
    Score(x,p_{x}) = k\cdot R(p_{x}|x) - (1-k)\cdot C(p_{x}) \label{eq3}
\end{equation}
where $x$ is the task input, $p_{x}$ represents a candidate reasoning path from $P$, and $k$ balances computational benefits against costs. Here, for simplicity, $R(p_{x}|x)$ denotes the path's final reward (defined as the leaf node's Q-value), while $C(p_{x})$ measures the reasoning cost (defined as the number of actions in the sequence). 


Then, for each question in seed dataset, we select path $p_{best}$ with highest $Score(x,p_{x})$ to build a \textit{Question-path repository} $D$ with one-to-one mappings. Inspired by metareasoning principles \cite{RUSSELL1991361,roberts2024smart}, which advocate for adaptive reasoning strategies, we distill these question-path pairs into abstract thought cards $\mathbb{C}$ using Problem Condition Complexity (PCC) \cite{lee2000problem,embretson2008understanding}, a cognitive complexity metric designed for complex reasoning. Each card represents a high-level reasoning pattern abstracted from multiple prior problems, enabling efficient transfer of reasoning insights during inference in the next step.


\subsection{Adaptive Reasoning and Verification}\label{sec4.3}
During inference, given a multimodal test query $x_t$, we compute its PCC and perform nearest neighbor matching \cite{6809191} against pre-constructed thought cards $\mathbb{C}$ to identify the most relevant five cards that best align with its complexity level. The selection process is formalized as: 
\begin{equation}
    NN_5(x_t, \mathbb{C}) = \mathop{\arg\min}_{\mathbb{C}_{x_t} \subseteq \mathbb{C}, |S| = 5}  {\textstyle \sum_{c \in \mathbb{C}_{x_t}}}d(x_t, c) \label{eq33}
\end{equation}
where $NN_5(x_t, \mathbb{C})\subseteq \mathbb{C}$ is the closest subset to $x_t$, determined by the distance $d: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$. By leveraging these reasoning templates, our method maintains the benefits of tree-structured reasoning without extensive node expansion, enabling efficient generation of candidate solutions.


Identifying the optimal reasoning trajectory among these candidates represents a critical challenge \cite{luo2025ursa}. Given the scarcity of verification models in the visual domain, we employ both self-consistency checks and text-domain outcome reward models to select the final solution. 
% Detailed descriptions are provided in Appendix \ref{A.4}.

In summary, our approach can be viewed as an optimized variant of tree search algorithms. Unlike traditional MCTS methods that require extensive exploration, we strategically reduce computational complexity by incorporating prior insights through thought cards. As shown in \cref{Figure3}, this enables efficient tree traversal along promising trajectories while maintaining high performance, offering valuable insights for developing efficient reasoning strategies.



\section{Experiments}\label{sec5}
The section presents \hyperlink{sec5.1}{4.1 experimental setup} and assesses AStar's effectiveness from four aspects: \hyperlink{sec5.2}{4.2 performance}, \hyperlink{sec5.4}{4.3 data and computational efficiency}, \hyperlink{sec5.3}{4.4 out-of-distribution generalization}, and \hyperlink{sec5.5}{4.5 ablation study and analysis}.


\hypertarget{sec5.1}{\subsection{Experimental Setup}}\label{sec5.1}

\textbf{Datasets}
$\,$ We perform extensive experiments across three reasoning tasks and six datasets: (1) general visual question answering: ChartQA \cite{masry-etal-2022-chartqa} and MMStar \cite{chen2024we}; (2) mathematical reasoning: MathVista \cite{lu2023mathvista}, MathVerse \cite{zhang2025mathverse}, and MathVision \cite{wang2024measuring}; (3) commonsense and scientific reasoning: GAOKAO-MM \cite{zong-qiu-2024-gaokao}. 


\textbf{Models}
$\,$ To demonstrate the versatility of AStar, we evaluate its effectiveness on both LLM and MLLM, including Qwen2.5-7B \cite{qwen25}, Qwen2-VL-2B \cite{wang2024qwen2}, and Qwen2-VL-7B. This design aims to validate that AStar can seamlessly leverage pre-trained LLM and MLLM as its inference backbone without modifications.

\textbf{Baselines}
$\,$ We evaluate AStar against four strong baseline categories: (1) open-source general MLLMs, including the powerful Qwen2-VL \cite{wang2024qwen2} and InternVL2 series \cite{chen2024internvl}; (2) open-source MLLMs specifically optimized for mathematical reasoning, such as recent work URSA \cite{luo2025ursa} and Math-LLaVA \cite{shi2024math}; (3) advanced closed-source MLLMs, including GPT-4V \cite{openai2023gpt4v}, and GPT-4o \cite{openai2024gpt4o}; and (4) tree-based methods, such as AR-MCTS \cite{dong2024progressive} and Mulberry \cite{yao2024mulberry}.




\begin{table*}[!t]
\centering
\caption{Evaluation of AStar's reasoning capabilities on MathVista and MathVerse \textit{testmini}. The best results are highlighted in \textbf{bold}. For MathVista, we pick five categories: ALL (overall accuracy), ARI (arithmetic reasoning), LOG (logical reasoning), STA (statistical reasoning), and VQA (visual question answering). For MathVerse, we present seven categories: ALL (overall accuracy), VI (vision intensive), VD (vision dominant), VO (vision only), TD (text dominant), TL (text lite), and TO (text only).}
\label{table1}
\vskip 0.1in
% \tiny
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|c|ccccc|ccccccc} 
\toprule
Model & \#Params  &\multicolumn{5}{c}{MathVista} & \multicolumn{7}{c}{MathVerse} \\ \midrule
 & &\textbf{ALL} & ARI & LOG & STA & VQA & \textbf{ALL}&VI& VD& VO & TD & TL& TO \\
\midrule
Random & -  & 17.9 & 13.8 & 13.4 & 14.3 & 26.3 & 12.4 & 12.4 & 12.4 & 12.4 & 12.4 & 12.4 & 12.4 \\
Human & - & 60.3  & 59.2 & 40.7 & 63.9 & 55.9 & 64.9 & 61.4& 68.3 & 66.7 & 71.2 & 70.9 & 41.7  \\

\midrule
\multicolumn{14}{c}{ \textit{Open-Source General MLLMs} } \\ \midrule
mPLUG-Owl2-7B~\cite{ye2023mplug} &7B  & 22.2 & 19.2 & 13.5 & 21.4 & 27.9 & 10.3 & 11.1 & 9.4 & 8.0 & 11.6 & 11.4 & 13.8 \\
MiniGPT4-7B~\cite{zhu2023minigpt} & 7B  & 23.1 & 32.0 & 10.8 & 17.9 & 30.2  & 12.2 & 12.5 & 14.8 & 8.7 & 12.3 & 12.9 & 13.4 \\
LLaVA-1.5-13B~\cite{liu2024improved} & 13B  & 27.7 & 28.6 & 10.8 & 22.9 & 30.2 & 12.7 & 12.6 & 12.7 & 9.0 & 17.1 & 12.0 & 22.6  \\
SPHINX-V2-13B~\cite{lin2023sphinx} & 13B &  36.7 & 33.4 & 24.3 &51.5 & 43.0 & 16.1 & 16.4 & 15.6 & 16.2 & 20.8 & 14.1 & 14.0\\
SPHINX-MoE~\cite{lin2023sphinx} & 8$\times$7B  &  42.6 & 43.0 & 14.4 & 50.8 & 43.3 & 22.8 & 21.1 & 19.6 & 18.3 & 33.3 & 21.9 & 23.1\\
LLaVA-NeXT-34B~\cite{liu2024llava} & 34B  & 46.5 & - & - & - & - & 34.6 & 35.2 & 28.9 & 22.4 & 49.0 & 37.6 & 30.1 \\
InternLM-XComposer2-VL~\cite{dong2024internlm} & 7B  & 57.6 & 51.6 & 13.5 & 62.8 & 39.7 & 25.9 & 20.1 & 24.4 & 19.8 & 36.9 & 28.3 & 42.5 \\
Deepseek-VL~\cite{lu2024deepseek} & 7B  & 34.9 & 38.8 & 18.9 & 33.2 & 34.6 & 19.3 & 20.2 & 18.4 & 11.8 & 23.0 & 23.2 & 23.1 \\
InternVL2-8B~\cite{chen2024internvl} & 8B  & 58.3 & 56.4 & 10.8 & \textbf{68.8} & 49.7 & 35.9 & 32.2 & 30.9 & 27.7 & 39.0 & 33.8 & 36.0 \\
Qwen2-VL~\cite{wang2024qwen2} & 7B  & 58.9 & 57.5 & 24.3 & 43.1 & 58.1 & 33.6 & 31.3 & 30.3 & 28.1 & 37.4 & 33.5 & 35.0  \\
\midrule

\multicolumn{14}{c}{ \textit{Open-Source Math MLLMs (Large-Scale Training)} } \\ 
\midrule
G-LLaVA-7B~\cite{gao2023g}& 7B  & 25.1 & 19.4 & 15.2 & 15.1 & 28.7 & 16.6 & 17.2 & 14.6 & 9.4 & 20.9 & 20.7 & 21.1 \\
Math-LLaVA-13B~\cite{shi2024math} &13B  & 46.6 &  40.7 & 23.3 & 42.3 & 33.5 & 22.9 & 24.5 & 21.7 & 16.1 & 27.3 & 24.9 & 27.0 \\
Math-PUMA-Qwen2-7B~\cite{zhuang2024math} & 7B  & 47.9 & 46.2 & 21.6 & 55.8 & 30.2 & 33.6 &  33.4 & 31.6 & 26.0 & 42.1 & 35.0 & 39.8 \\
Math-PUMA-DeepSeek-Math~\cite{zhuang2024math} & 7B  & 44.7 & 41.9 & 8.1 & 50.8 & 31.3 & 31.8 & 33.6 & 31.6 & 14.7 & 43.4 & 35.4 & 47.5\\
MAVIS-7B~\cite{zhang2024mavis} & 7B & - & - & - & - & - & 35.2 &  34.1 & 29.7 & 31.8 & 43.2 & 37.2 &-  \\
InfiMM-Math~\cite{han24infimm} & 7B  & - & - & - & - & - & 34.5 & 38.1 & 32.4 & 15.8 & 46.7 & 32.4 & - \\
MultiMath-7B~\cite{peng2024multimath} & 7B  & 50.0 & 42.2 & 23.3 & 64.9 & 49.2 & 27.7 & 28.1 & 25.9 & 15.0 & 34.8 & 30.8 & 35.3 \\
URSA-7B~\cite{luo2025ursa} &7B  & 59.8 & 53.5 & 21.6 & 57.1 & 40.2 & 45.7 & 46.4 & 43.9 & 28.6 & \textbf{55.3} & \textbf{48.3} & 51.8\\
\midrule
\rowcolor{mygray}
AStar (Ours, Training-free Reasoning) & 7B & \textbf{63.5} & \textbf{63.1} & \textbf{61.3} & \textbf{68.8} & \textbf{60.1} & \textbf{54.0} & \textbf{46.8} & \textbf{61.8} & \textbf{46.4} & 53.9 & 44.3 & \textbf{53.9}\\
\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table*}



\begin{table}[!t]
\centering
\caption{Comparison with powerful baselines on general VQA datasets: MMStar and ChartQA.}
\label{table2}
\vskip 0.1in
% \tiny
\resizebox{0.98\columnwidth}{!}{
% \begin{tabular}{l|c|cc} 
\begin{tabular}{lccc}
\toprule
Model & \#Params & MMStar & ChartQA  \\
\midrule
\multicolumn{4}{c}{ \textit{2B-Scale Baselines} } \\ [0.3em]
% \midrule
Qwen2-VL \cite{wang2024qwen2}   & 2B   & 48.0    & 73.5  \\ 
Mulberry \cite{yao2024mulberry}    & 2B   & 51.3    & 77.7  \\
\rowcolor{mygray}
AStar (Ours) &2B   & \textbf{51.7}    & \textbf{78.3}  \\
\midrule
\multicolumn{4}{c}{ \textit{$\ge$ 7B-Scale Baselines} } \\ [0.3em]
% \midrule
% InternLM-XComposer2-VL  & 7B & 55.4 & 71.8  \\
Deepseek-VL \cite{lu2024deepseek} & 7B   & 36.1    & 59.1 \\
Qwen2-VL \cite{wang2024qwen2}   & 7B   & 60.7    & 83.0  \\
InternVL2 \cite{chen2024internvl}   & 8B   & 61.5    & 83.3  \\
Insight-V \cite{dong2024insight}  & 7B   & 61.5    & 82.3 \\
Mulberry \cite{yao2024mulberry}   & 7B   & 61.3    & \textbf{83.9} \\
LLaVA-CoT \cite{xu2024llava}  & 11B  & 58.1    & - \\
LlamaV-o1 \cite{thawakar2025llamav}  & 11B  & 59.5    & - \\
\rowcolor{mygray}
AStar (Ours) &7B & \textbf{61.7} & \textbf{83.9}\\
\bottomrule
\end{tabular}
}
\vskip -0.12in
\end{table}




% \subsection{Main results}\label{sec5.2}
\hypertarget{sec5.2}{\subsection{Performance Comparison with Baselines}}\label{sec5.2}
\textbf{Results on Diverse Reasoning Benchmarks}
$\,$ Table \ref{table1} and \ref{table2} presents the performance of AStar across four mainstream reasoning benchmarks. We have four key findings:

\vskip 0.023in
$\diamondsuit$ AStar consistently outperforms both powerful open-source general and math-specialized MLLMs across general visual question answering and mathematical reasoning tasks. Specifically, using Qwen2.5-7B as the reasoning backbone, our method achieves $54.0\%$ accuracy on MathVerse, surpassing InternVL2-8B by $18.1\%$ and the recent high-quality CoT-enhanced URSA-7B model by $8.3\%$.

\vskip 0.023in
$\diamondsuit$ AStar demonstrates strong performance across multiple reasoning capabilities. Notably, it achieves $63.1\%$ accuracy on logical reasoning (LOG), outperforming InternVL2-8B by $50.5\%$ and URSA-7B by $39.7\%$. Similar improvements are observed in statistical reasoning (STA: $68.8\%$) and visual question answering (VQA: $60.1\%$).
% , validating the method's versatility.

\vskip 0.023in
$\diamondsuit$ The benefits of AStar's adaptive reasoning are universal and maintain consistent performance improvements across varying degrees of multimodal information content. Notably, our method achieves consistent gains across different information distributions, from vision-intensive (VI: 46.8$\%$) and vision-dominant (VD: 61.8$\%$) to text-dominant (TD: 53.9$\%$) and text-only (TO: 53.9$\%$) scenarios, demonstrating robust performance regardless of the modality balance.



$\diamondsuit$ Beyond specialized reasoning tasks, AStar performs superior on general VQA benchmarks (MMStar and ChartQA), suggesting that enhanced reasoning capabilities also benefit general visual understanding.


\begin{table*}[t]
    \centering
    % \scalebox{0.8}{
    % \setlength{\tabcolsep}{5.0pt}
    \caption{Comparison with leading multimodal tree search methods. AStar achieves competitive performance with only 0.5K prior data and 5 search iterations per sample, demonstrating superior efficiency and effectiveness.}
    \vskip 0.1in
    \label{table3}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{@{}c|ccccccc}
    \toprule  
    % No GPT-4o
    Methods & Open-Source Only & Training-Free & Data Volume & Search Iter.~\textbf{$\downarrow$} & MathVista Acc.~\textbf{$\uparrow$} & MMStar Acc.~\textbf{$\uparrow$} & GAOKAO Acc.~\textbf{$\uparrow$}\\
    \midrule
    AR-MCTS \cite{dong2024progressive} & \cmark & \xmark & 34.5K & 32.0 & \textbf{64.1} & - & 37.4\\
    Mulberry \cite{yao2024mulberry} & \xmark & \xmark & 260K & - & 63.1 & 61.3 & -\\
    % \rowcolor{mygray}
    Ours & \cmark & \cmark & 0.5K & \textbf{5.0} & 63.5 & \textbf{61.7}$^{\color{teal}{\textbf{\scriptsize0.4}\uparrow}}$ & \textbf{49.7}$^{\color{teal}{\textbf{\scriptsize12.3}\uparrow}}$\\
    \bottomrule
  \end{tabular}
  }
\end{table*}


\vskip 0.1in
\textbf{Results on More Challenging Datasets}
$\,$ As shown in Figure \ref{Figure4}, we evaluate AStar against leading models on the challenging MathVision benchmark \cite{wang2024measuring} across multiple reasoning dimensions. AStar achieves 32.3$\%$ average accuracy across all dimensions, surpassing GPT-4o (30.4$\%$). Notably, in logical reasoning area, AStar attains 45.8$\%$ accuracy, outperforming both GPT-4o (29.4$\%$) and Math-LLaVA-13B (16.0$\%$). AStar's effectiveness can be attributed to its adaptive tree-structured decomposition framework, which simplifies complex multimodal reasoning tasks into manageable sub-problems, enabling substantial improvements across diverse reasoning capabilities.


\begin{figure}[ht!]
\centerline{\includegraphics[width=0.92\linewidth]{Figure4.pdf}}
\caption{Comparison with leading MLLMs across various capabilities on the challenging MathVision dataset. Our 7B-parameter model achieves comparable performance to GPT-4o.}
% \vskip -0.1in
\label{Figure4}
\end{figure}


% \vskip 0.06in
\textbf{Comparison with Leading MLLMs}
$\,$ We further compare AStar against leading closed-source and larger open-source models. As illustrated in Figure \ref{Figure5}, AStar-empowered models deliver competitive performance, matching or surpassing larger models. Notably, AStar-7B achieves an average score of 50.0$\%$ across three challenging benchmarks, outperforming GPT-4o (48.1$\%$) and Qwen2-VL-72B (46.0$\%$). Additionally, our reasoning paradigm enables Qwen2-VL-2B to surpass the larger Qwen2-VL-7B model. 



% \vspace{0.2em}
% \vskip -0.1in
\hypertarget{sec5.4}{\subsection{Data and Computational Efficiency}}\label{sec5.4}
\vskip 0.05in
We further compare our approach AStar with two recent powerful multimodal tree search methods: AR-MCTS \cite{dong2024progressive} and Mulberry \cite{yao2024mulberry}. As shown in Table \ref{table3}, we compare multiple dimensions, including training requirements, training data volume, computational efficiency (measured by average search iterations per sample), and accuracy on three representative benchmarks.

Results demonstrate that AStar achieves competitive performance while significantly improving both data and computational efficiency. In terms of data efficiency, AStar requires only 0.5K prior samples, which represents a 69-fold reduction compared to AR-MCTS (34.5K) and a 520-fold reduction compared to Mulberry (260K), respectively. This efficiency stems from explicit reasoning pattern extraction, enabling performance comparable to implicit learning methods with far less training data. Such efficiency is particularly advantageous in limited-data scenarios. Regarding computational efficiency, AStar reduces the average iterations per sample by $6.4\times$ compared to AR-MCTS, while maintaining comparable or superior accuracy across all benchmarks.


\begin{figure}[ht!]
\centerline{\includegraphics[width=0.92\linewidth]{Figure5.pdf}}
\caption{Comparison between AStar and powerful MLLMs across 3 challenging benchmarks: MathVista, MathVerse, and MathVision. `OS' and `CS' denote open-source and closed-source models. AStar with 7B model outperforms larger OS and CS models.}
% \vspace{-0.1em}
\vskip -0.1in
\label{Figure5}
\end{figure}

\begin{figure*}[ht!]
\centerline{\includegraphics[width=1.0\linewidth]{Figure6.pdf}}
\caption{Cross-distribution performance comparison on GAOKAO-MM. We test on Qwen2-VL-2B, Qwen2-VL-7B, and GPT-4o. Results highlight that AStar exhibits superior performance when testing on out-of-distribution data.}
\vskip -0.1in
\label{Figure6}
\end{figure*}




\hypertarget{sec5.3}{\subsection{Out-of-Distribution Generalization}}\label{sec5.3}
Recent work has highlighted that distributional shifts severely affect the reliability of MLLMs \cite{yin2023survey,zhang2024out}. While these models excel in in-distribution (ID) tasks, their performance often degrades in out-of-distribution (OOD) scenarios \cite{dong2024multiood,miyai2024generalized}, a challenge further compounded by the difficulty of acquiring sufficient high-quality training data.


In this section, we evaluate AStar's performance in OOD scenarios. Since our reasoning guidance during inference is derived from the mathematical domain, we further test on GAOKAO-MM \cite{zong-qiu-2024-gaokao}, a Chinese human-level multimodal reasoning benchmark that includes historical, geographical, and scientific reasoning tasks. This design naturally constitutes an OOD evaluation setting. As shown in Figure \ref{Figure6}, all models demonstrate significant improvements over baseline methods when enhanced with the AStar framework. Notably, for Qwen2-VL-7B, our method improves Geography task accuracy from 26.9$\%$ to 51.6$\%$ and achieves an average improvement of 19.5$\%$ across all subjects. These results validate AStar's strong generalization across different languages, task distributions, and reasoning domains, establishing it as a robust and versatile approach for both ID and OOD reasoning tasks. 



\hypertarget{sec5.5}{\subsection{Ablation Study and Analysis}}\label{sec5.5}
$\,$ To investigate the contribution of each component in AStar, we conduct ablation study as shown in Table \ref{table4}. Here, ``w/o'' denotes variants with specific components removed. Our analysis reveals three key findings:

\begin{table}[htp!]
\centering
\caption{Ablation results on AStar-7B. `RA', `RC', `RS', `SC' denotes `random actions', `random card', `random selection', and `self-consistency', respectively. We observe that every component is important for optimal performance.}
\vskip 0.1in
\label{table4}
\resizebox{1.0\linewidth}{!}{
% \begin{tabular}{@{}lccc@{}}
\begin{tabular}{lccl}
\toprule
Model Setting & MathVista & MathVerse & Average\\
\midrule
AStar & \textbf{63.5} & \textbf{54.0} & \textbf{58.8}\\   
\quad $-$ w/o thought cards (RA)  & 58.9 & 48.5 & 53.7$^{\color{red}{\textbf{\scriptsize-5.1}\downarrow}}$ \\
\quad $-$ w/o card match (RC)  & 61.3 & 50.3 & 55.8$^{\color{red}{\textbf{\scriptsize-3.0}\downarrow}}$ \\
\quad $-$ w/o verification (RS)    & 62.0 & 51.6 & 56.8$^{\color{red}{\textbf{\scriptsize-2.0}\downarrow}}$ \\
\quad $-$ w/o verification (SC)    & 63.0 & 51.8 & 57.4$^{\color{red}{\textbf{\scriptsize-1.4}\downarrow}}$ \\
\bottomrule
\end{tabular}
}
\end{table}

First, removing any component leads to performance degradation, validating the necessity of all component designs. Specifically, excluding the card construction module results in the most substantial drops (4.6$\%$ on MathVista and 5.5$\%$ on MathVerse), highlighting the critical role of thought cards in providing high-level insights for test-time inference.

Second, the precision of card matching also impacts performance, particularly on complex tasks like MathVerse where degradation is more pronounced than on MathVista. While this paper employs PCC-based nearest-neighbor matching, future work could explore more advanced strategies.

Third, while the verification module improves performance, simpler alternatives like self-consistency (SC) or random selection (RS) show only modest degradation (2.0$\%$ and 1.4$\%$). This resilience suggests that thought card guidance enables robust candidate solution generation even with basic selection methods.


\textbf{Inference Time Scaling}
$\,$ We gradually increase the number of selected thought cards during inference to investigate whether our method follows test-time scaling laws. Figure \ref{Figure7} demonstrates that incorporating additional reasoning guidance via thought cards leads to consistent performance improvements in our AStar framework. 

\begin{figure}[htp!]
\centerline{\includegraphics[width=1.0\linewidth]{Figure7.pdf}}
\caption{Inference Time Scaling. We examine the variation in AStar performance with the number of selected reasoning guidance-providing thought cards in \textit{Sec.} \ref{sec4.3}.}
\vskip -0.1in
\label{Figure7}
\end{figure}


\textbf{Visualization of Sampling Space}
$\;$ To analyze the diversity of solutions generated by our AStar-powered reasoning framework across varying numbers of thought cards, we conduct a visualization study here. We sample 250 problems from MathVision and generate 2 candidate solutions for each problem using AStar (Qwen2-VL-2B), resulting in 500 total samples. Following \citet{dong2024progressive}, we employ a three-step analysis process using BGE-M3~\citep{bge_m3} for semantic embedding, followed by dimensionality reduction (PCA) and clustering techniques, DBSCAN~\cite{dbscan}.

Figure \ref{FigureS3} and \ref{FigureS4} illustrate the visualization results across varying numbers of thought cards ($k = 2$ and $k = 5$) on MathVista and MathVision, respectively. The empirical analysis reveals consistent conclusions across both standard mathematical reasoning tasks (MathVista) and challenging scenarios (MathVision). When fewer thought cards are provided ($k = 2$), the model generates clusters with fewer centroids for the same problem set, indicating a more constrained sampling space with limited potential. Conversely, increasing the number of thought cards to $k = 5$ leads to clusters with more centroids, suggesting a richer sampling space with higher performance potential. The modest difference in cluster counts between $k = 2$ and $k = 5$ indicates that our framework effectively adapts to problem complexity by dynamically selecting suitable reasoning patterns from high-quality thought cards. Even with fewer thought cards, it maintains robust sampling diversity and performance through this adaptive matching mechanism.

\begin{figure*}[ht!]
\vskip 0.05in
\centerline{\includegraphics[width=0.85\linewidth]{FigureS3.pdf}}
\caption{Visualization of candidate reasoning paths on MathVista with $k = 2$ (left) and $k = 5$ (right) thought cards. Dots represent individual reasoning steps, while colors indicate distinct solution clusters. The increasing density and diversity of clusters from left to right demonstrate the model's enhanced exploration capability with more thought cards, leading to more comprehensive coverage of the solution space.}
\vskip 0.05in
\label{FigureS3}
\end{figure*}


\begin{figure*}[ht!]
\vskip 0.05in
\centerline{\includegraphics[width=0.85\linewidth]{FigureS4.pdf}}
\caption{Visualization of candidate reasoning paths on MathVision with $k = 2$ (left) and $k = 5$ (right) thought cards. Dots represent individual reasoning steps, while colors indicate distinct solution clusters. The increasing density and diversity of clusters from left to right demonstrate the model's enhanced exploration capability with more thought cards, leading to more comprehensive coverage of the solution space.} 
\vskip 0.05in
\label{FigureS4}
\end{figure*}

Notably, the observed lower cluster count in MathVista compared to MathVision offers valuable insights into task-specific reasoning requirements. On MathVista, which contains relatively simpler problems, our AStar framework exhibits strong reasoning path convergence: multiple high-quality thought cards typically lead to consistent solutions. This convergence allows for effective performance even with basic self-consistency verification. However, on the more challenging MathVision dataset, we observe greater path divergence, emphasizing the importance of careful validation and selection of optimal reasoning paths. This behavior closely resembles human problem-solving patterns. When addressing elementary problems, different approaches tend to naturally converge to the same answer. In contrast, for complex challenges like Olympic problems, diverse approaches often yield different conclusions, making thorough solution validation necessary.

In summary, our visualization results empirically validate that AStar effectively addresses the challenge of limited diversity in candidate solutions, enabling comprehensive coverage of the problem-solving space while providing robust prior guidance for efficient inference.

\section{Conclusion}\label{sec6}
\vskip 0.1in
In this paper, we propose AStar, a novel automated structured thinking paradigm for multimodal reasoning that effectively tackles both performance and efficiency challenges in visual reasoning tasks. By leveraging the hierarchical structure of MCTS and extracting high-level cognitive reasoning patterns to guide inference, our approach achieves notable advantages: (1) 32.3$\%$ accuracy on the challenging MathVision benchmark with a 7B backbone, surpassing GPT-4o's 30.4$\%$; (2) $6.4\times$ reduction in inference overhead compared to existing tree-based methods; and (3) $520\times$ less prior data than training-based approaches while maintaining comparable performance. These results demonstrate that AStar's integration of structured decomposition with efficient prior guidance offers a promising pathway for advancing multimodal reasoning, making it both more powerful and accessible to the broader research community.


% \section*{Impact Statement}
% Multimodal reasoning plays a crucial role in real-world applications. This paper proposes an efficient MCTS-based automated reasoning paradigm that achieves strong performance while significantly reducing computational and energy costs. This research advances our understanding of MLLM reasoning capabilities, paving the way for more responsible and scalable AI development.



\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section*{Appendix of AStar}
This comprehensive supplementary material provides in-depth insights into our AStar method, covering additional descriptions, experimental details, and results. The appendix is organized as follows:

\vspace{2mm}

\section*{\textit{Table of Contents}}  % Title centered above the line
\vspace{3mm}

\hrule
\vspace{3mm}

\begin{center}
    \renewcommand{\labelitemi}{}
    \begin{itemize}
        \item \textbf{A. Preliminaries}
        \begin{itemize}
            \item \ref{A.1}. Overall Notations  
            \item \ref{A.2}. MLLM Reasoning
            \item \ref{A.3}. Monte Carlo Tree Search
            \item \ref{A.4}. Verification Methods
        \end{itemize}
        \item \textbf{B. Algorithm Details}
        \begin{itemize}
            \item \ref{B.1}. Action Space
            \item \ref{B.2}. Reward Value in MCTS
        \end{itemize}
        \item \textbf{C. More Details about Experimental Setup}
        \begin{itemize}
            \item \ref{C.1}. Benchmarks and Datasets
            \item \ref{C.2}. Baselines
            \item \ref{C.3}. Implementation Details
        \end{itemize}
        \item \textbf{D. Supplementary Results}
        \begin{itemize}
            \item \ref{D.1}. Detailed Results on Multimodal Reasoning Benchmarks
            \item \ref{D.2}. Comparison with Strong Baselines
            \item \ref{D.4}. Integration with SFT
            \item \ref{D.5}. Weak-to-Strong Generalization
        \end{itemize}
        \item \textbf{E. Case Study}
    \end{itemize}
\end{center}
\vspace{5mm}
\hrule
\vspace{3mm}

\section{Preliminaries}\label{A}
This section describes overall notations (\ref{A.1}), MLLM reasoning (\ref{A.2}), monte carlo tree search (\ref{A.3}), and verification methods (\ref{A.4}).

\subsection{Overall Notations}\label{A.1}
The definitions for notations are in Table \ref{table s1}.


\begin{table*}[ht!]
\caption{Notation Table}
\label{table s1}
\vskip 0.15in
\centering
% \begin{tabular}{@{\extracolsep{\fill}}cc}
\begin{tabular}{cc}
\toprule
\textbf{Character} & \textbf{Meaning} \\
\midrule
$\pi_{\theta}$ & policy MLLM \\
$I$           & task instruction\\
$\mathcal{D}_{I}$          & demonstration examples of $I$, which is $\phi$ in zero-shot settings\\
$D_{s}$        & seed data\\
$D_{t}$        & test data\\
$x_{t}$        & multimodal test problem, consisting of input question $q$ and images $i$\\      
% $x_{t}$        & multimodal test problem, consisting of an input question paired with one or more corresponding images. \\   
$y_{d}$        & decoded answer \\     
$y_{g}$        & gold standard answer \\   
$y_{t}$        & reasoning trajectory / solution\\
$T$            & number of reasoning steps\\
$T_d$          & number of tokens in decoded answer $y_{p}$\\
$a_{t}$        & t-th decoded answer token of $y_{d}$\\
$s_{t}$        & t-th reasoning step of trajectory $y_{t}$\\
$S_{t}$        & t-th state, which consists of input x and preceding reasoning steps $(s_{1},s_{2}, ..., s_{t-1})$ \\
$a_{t}$        & t-th action based on the previous state $S_{t-1}$\\
$s$            & node $s$ in the tree structure\\
$p$            & parent node of $s$\\
$Q(s)$         & reward value of node $s$\\
$p_{\varphi } $& process reward model\\
$o_{\psi }  $  & outcome reward model\\
\bottomrule
\end{tabular}
\end{table*}


\subsection{MLLM Reasoning}\label{A.2}
With the advancement of computational resources and expanded datasets, MLLMs have demonstrated remarkable capabilities across various multimodal tasks \cite{yin2023survey,10445007,wang2024exploring}. These range from visual reasoning tasks like chart understanding and visual question-answering \cite{masry-etal-2022-chartqa,hartsock2024vision} to more complex perception-action tasks in autonomous driving and robotics \cite{driess2023palm,cui2024survey,yang2025octopus}, where models must integrate visual inputs with decision-making processes. Central to these achievements is the development of effective reasoning methods, which can substantially enhance MLLM problem-solving capabilities, enabling even smaller models to achieve sophisticated reasoning abilities \cite{wang2024qwen2,dong2024progressive,thawakar2025llamav}. To formalize this reasoning process, we consider an autoregressive pre-trained MLLM serving as a policy model $\pi_{\theta}$. Given an input problem $x_{t}$, the model generates a sequence $output=(s_{0}, s_{1}, s_{2}, ..., s_{T})$ through iterative token prediction, where $s_{0}:=x_t$ represents the initial state and $s_{T}$ corresponds to the solution $y_{p}$. We define this generated sequence $(s_{0}, s_{1}, s_{2}, ..., s_{T})$ as a reasoning trajectory $y_{t}$. The conditional probability distribution of generating the complete reasoning trajectory is:
% \begin{equation}
% \pi_{\theta}(y_t,y_p \mid I, x_t)=\prod_{t=1}^{T} \pi_{\theta}\left(s_{t} \mid x_t, s_{<t}\right)
% \end{equation}

\begin{equation}
    \pi_{\theta}(y_t,y_d \mid I, x_t, \mathcal{D}_{I})=\underbrace{\prod_{t=1}^{T} \pi_{\theta}\left(s_t \mid s_{<t}, I, x_t, \mathcal{D}_{I}\right)}_{\text {Intermidiate Reasoning Process }} \cdot \underbrace{\prod_{t=1}^{T_{d}} \pi_{\theta}\left(a_{t}^{'} \mid a_{<t}^{'}, y_t, I, x_t\right)}_{\text {Answer Decoding }},
\end{equation}

Following \cite{hao-etal-2023-reasoning,ge2024worldgpt}, we can conceptualize MLLMs as world models, with the complex reasoning process formulated as a Markov decision process. Specifically, when addressing complex reasoning challenges in real-world scenarios, at each time step $t$, the model receives a state $S_{t-1}$, comprising the original input problem $x$ and preceding reasoning steps $(s_{0},s_{1},s_{2}, ..., s_{t-1})$. The policy model $\pi_{\theta}$ then generates the current action $a_t=\pi_{\theta}(\Phi(S_{t-1}))$, which prompts the MLLM to produce the next reasoning step $s_t$. The entire process, from the initial step $s_{0}$ to the final output $s_{T}$, naturally forms a complete trajectory or chain of thought.

Inspired by recent advances in language model reasoning capabilities, researchers have explored OpenAI o1-like structured reasoning approaches to enhance long-chain reasoning in MLLMs. These approaches aim to develop systematic thinking patterns that enable models to perform complex multi-step reasoning. The process can be formalized under the following theoretical framework:

\begin{equation} 
P_{\pi_{\theta}}\left(y_d=y_g \mid x_t\right)=\mathbb{E}_{\left(s{0}, s_{1}, \cdots, s_{T}\right) \sim P_{\pi_{\theta}}(y_t \mid x_t)}\left[P\left(y_d=y_g \mid s_{0}, s_{1}, \cdots, s_{T}, x_t\right)\right] 
\end{equation}


\begin{table*}[ht!]
\caption{Method Comparison: Recent Multimodal Structured Reasoning Approaches.}
\label{table s2}
\vskip 0.1in
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{cccccc}
\toprule
Method & Open-Source Only & Data Volume & Training-Free & Training Cost $\downarrow$ & Inference Cost $\downarrow$ \\
\midrule
AR-MCTS \cite{dong2024progressive}   & \cmark & 34.5K & \cmark & -     & High \\
Mulberry \cite{yao2024mulberry}      & \xmark & 260K  & \xmark & High  & Low \\
LLaVA-CoT \cite{xu2024llava}         & \xmark & 100K  & \xmark & High  & Low \\
URSA \cite{luo2025ursa}              & \xmark & 1100K & \xmark & High  & Low \\
LlamaV-o1 \cite{thawakar2025llamav}  & \xmark & 118K  & \xmark & High  & Low \\
\rowcolor{mygray}
AStar (Ours)                         & \cmark & 0.5K  & \cmark & Low   & Low \\
\bottomrule
\end{tabular}}
\end{table*}
\vskip -0.1in

where $P\left(y_d=y_g \mid s_{0}, s_{1}, \cdots, s_{T}, x\right)$ represents the probability of obtaining an accurate final answer given the test problem $x_t$ and reasoning trajectory $y_t$.

Early work explored multimodal chain-of-thought (CoT) \cite{zhang2024multimodal}, addressing the limitations of conventional MLLMs that typically defaulted to simple 'direct prediction' mode due to the scarcity of high-quality long-chain reasoning data. However, these approaches often exhibit instability in multimodal reasoning tasks due to the distribution shift between training and inference \cite{wang2024enhancing}. Two primary methods have emerged to address this challenge: \textit{explicit search mechanisms} and \textit{teacher-guided distillation}. Recent advances in these approaches are summarized in Table \ref{table s2}.

\textit{(1) Explicit search mechanisms.} \; These approaches leverage explicit search structures (e.g., Monte Carlo tree search, MCTS) with specialized reward models to guide the exploration of solution paths. AR-MCTS \cite{dong2024progressive} combines MCTS with active retrieval to expand the solution space and enhance performance, though its extensive iterations demand significant computational resources. Moreover, the quality and relevance of retrieved examples at each exploration step substantially impact model performance. Similarly, Mulberry \cite{yao2024mulberry} employs collective knowledge to enable collaborative conjecture, search, and identification of effective reasoning paths via MCTS. However, it requires substantial training data (260K samples) generated with expensive proprietary models, making it computationally intensive.

\textit{(2) Teacher-guided distillation.} \; This approach focuses on distilling structured reasoning patterns through long-form CoT \cite{wei2022chain,zhang2024multimodal,luo2025ursa} instruction data, typically requiring supervision from proprietary models for data synthesis. For instance, LLaVA-CoT \cite{xu2024llava} explicitly structures visual reasoning into four stages (Summary, Caption, Reasoning, and Conclusion), utilizing 100K synthetic data samples for downstream task fine-tuning. Similarly, URSA \cite{luo2025ursa} proposes a three-module data synthesis strategy integrating CoT distillation, trajectory rewriting, and format standardization, which relies on Gemini-Flash-002.

\subsection{Monte Carlo Tree Search}\label{A.3}
As a heuristic search algorithm, MCTS has demonstrated remarkable success in complex reasoning and decision-making environments \cite{chaslot2008monte,NEURIPS2021_d5eca8dc,zhou2024language}. The algorithm conceptualizes search spaces as tree structures and has achieved significant breakthroughs across various domains, most notably in game-playing AI such as AlphaGo and AlphaZero \cite{dong2020deep}. The basic MCTS algorithm involves an iterative search process with four key steps: selection, expansion, simulation, and backpropagation. As an example in multimodal mathematical reasoning, Figure \ref{FigureS1} illustrates the four phases in an iteration, expanding the tree and then updating reward values.

\begin{figure*}[ht!]
\centerline{\includegraphics[width=1.0\linewidth]{FigureS1.pdf}}
\caption{An illustration of the four phases in an iteration of MCTS for complex reasoning tasks (\ref*{A.3}).}
% \vskip -0.1in
\label{FigureS1}
\end{figure*}

Leveraging MCTS, recent approaches like AR-MCTS \cite{dong2024progressive} exploit MLLMs' intrinsic capabilities and active retrieval for iterative exploration to enhance visual complex reasoning. However, these methods demand substantial computational resources due to extensive iterations. While approaches like Mulberry \cite{yao2024mulberry} attempt to integrate tree structures through model training, they require large-scale datasets and significant computational overhead. Moreover, current methods typically employ static reasoning processes, lacking the ability to adapt reasoning strategies based on problem complexity. In contrast, our approach employs MCTS only during the generation of prior reasoning patterns (referred to as "thought cards" in \textit{Sec.} \ref{sec4.2}) and references these thought cards during inference to achieve efficient reasoning. This design enables AStar to adaptively match reasoning strategies to problem complexity, significantly reducing time complexity compared to traditional tree search methods while maintaining comprehensive search coverage and performance, thus achieving an optimal efficiency-effectiveness trade-off.

\subsection{Verification Methods}\label{A.4}
After obtaining multiple valid candidate solutions for the test multimodal query $x_t=[q;i]$, selecting the most accurate reasoning trajectory among candidates presents a critical challenge. In multimodal reasoning, self-consistency \cite{wang2023selfconsistency} represents a simple yet effective approach, where the final answer is determined through majority voting among sampled paths:
\begin{equation}
    y^* = \arg\max_{y \in Y} \sum_{i=1}^{m} \mathbb{I}(y_i = y)
\end{equation}
where $Y$ denotes the set of all possible decoded answers, and $\mathbb{I}$ is the indicator function. This strategy exploits the observation that a complex reasoning problem typically admits multiple valid reasoning paths leading to its unique correct answer. Thus, repeatedly sampled actions at the same state likely indicate successful task completion. However, for complex reasoning tasks where models may only achieve correct reasoning with low probability, simple self-consistency might not provide stable performance. 

Recent advances in LLM reasoning verification have demonstrated significant improvements in reasoning capabilities through verification modules \cite{wang2024math,zhang2024rest}. This has motivated the integration of outcome and process supervision in multimodal domains \cite{dong2024progressive,luo2025ursa} to enhance reasoning performance. However, the scarcity of multimodal supervision data and challenges in ensuring supervision signal quality often compromise verification effectiveness, with few high-performance open-source verification models currently available. Therefore, for simplicity, we leverage mature text-domain outcome reward models (ORM). 

Specifically, in the text domain, given a text problem $q$ and its reasoning trajectory $y_t$, ORM ($\mathcal{D} \times \mathcal{S} \rightarrow \mathbb{R}$) assigns a single real-value to indicate whether $y_t$ is correct. ORM is usually trained with a cross-entropy loss \cite{cobbe2021training,li-etal-2023-making}:
\begin{equation}
    \mathcal{L}_\text{ORM} = y_g \log r_{y_t} + (1 - y_g) \log(1 - r_{y_t})
\end{equation}
where $y_g$ is the golden answer ($y_g=1$ if $traj$ is correct else $y_g=0$), and $r_{traj}$ is the sigmoid score of $traj$ assigned by ORM. The effectiveness of ORM heavily depends on training data quality. For reasoning problems with definitive answers, researchers can construct the ORM training set automatically by: (1) sampling candidate solutions, and (2) labeling solutions based on answer correctness. While this approach may introduce false positives (incorrect reasoning leading to correct answers), prior work has demonstrated its effectiveness in training robust ORM models \cite{lightman2024lets,wang-etal-2024-math}. This enables the development of high-performing open-source verification models in the text domain.

To leverage these powerful text-domain verification models for visual reasoning while maintaining our principle of balancing performance and efficiency, we directly employ an MLLM to translate the original visual problems into pure text format.
\begin{equation}
    q = \pi_{\theta} (x_t, \text{prompt}_{translate})
\end{equation}
where $\text{prompt}_{translate}$ denotes a translation prompt (e.g., ``please translate the question and visual image into a pure text format''). The resulting text question $q$ and the derived language-based reasoning steps derived from prior visual reasoning are then jointly processed by the ORM model for path selection and verification.


\begin{table*}[htbp!]
\caption{Comparison with other search-based LLM and MLLM methods. Note that, most method contain limited space. In contrast, we define a rich set of reasoning actions, thus enhancing the upper bound of model reasoning capabilities.}
\label{table s3}
\vskip 0.1in
\begin{center}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{clc}
% \begin{tabular*}{\linewidth}{@{}lccc@{}}
\toprule
Type & Method & Action Space\\
\midrule
\multirow{4}{*}{LLM} & Tree-of-Thought \cite{NEURIPS2023_271db992}     & $a_{3}$: one-step thought \\
& RAP \cite{hao-etal-2023-reasoning}     & $a_{5}$: divide and conquer        \\
& ReST-MCTS* \cite{zhang2024rest}        &  $a_{3}$: one-step thought\\
& MCTSr \cite{zhang2024accessing}              & $a_{4}$: chain-of-thought, $a_{6}$: self-reflection \\
% & LLaMA-Berry \cite{zhang2024llama} & \\
\midrule
\multirow{4}{*}{MLLM} & AR-MCTS \cite{dong2024progressive}     & $a_{3}$: one-step thought \\
& LLaVA-CoT \cite{xu2024llava}     & $a_{4}$: chain-of-thought \\
& URSA \cite{luo2025ursa}     & $a_{4}$: chain-of-thought \\
\rowcolor{mygray}
& Ours & $a_{1}$, $a_{2}$, $a_{3}$, $a_{4}$, $a_{5}, a_{6}$ \\
\bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table*}


Our experiments indicate that this approach achieves improvements over simple self-consistency with minimal computational overhead. Future work could explore integration with high-quality verification models for potentially better results. In this paper, we utilize off-the-shelf ORM model: Llama3.1-8B-ORM-Mistral-Data\footnote{\href{https://huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data}{https://huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data}}.

\section{Algorithm Details}\label{B}
\subsection{Action Space}\label{B.1}
Reasoning capabilities are fundamental for handling diverse tasks, encompassing multiple cognitive dimensions. Prior work has categorized reasoning into deductive, inductive, abductive, and analogical approaches \cite{wang2024exploring}. While open-source MLLMs demonstrate basic task competency, advanced models necessitate more sophisticated human-like reasoning abilities to achieve their full potential \cite{zeng2024scaling}. A key insight from recent studies suggests that model reasoning performance is bounded by their available reasoning actions \cite{wu2024comparative}. Current frameworks, however, operate within a constrained action space (Table \ref{table s3}), which may limit the reasoning capabilities of MLLMs. To expand models' reasoning capabilities beyond these constraints, we propose a comprehensive framework with six fundamental reasoning actions:

\begin{itemize}
    \item \textit{Visual Parsing ($a_{1}$)}: Extracting and analyzing visual information from input images to support multimodal reasoning tasks.
    \item \textit{System Analysis ($a_{2}$)}: Analyzing the overall structure of the problem and identifying the constraints and conditions before addressing it, thereby clarifying task requirements effectively.
    \item \textit{One-Step Thought ($a_{3}$)}: Generating the next one-step thought based on the given question and the preceding reasoning steps.
    \item \textit{Chain-of-Thought ($a_{4}$)}: Facilitating step-by-step reasoning by constructing a logical sequence of intermediate thoughts, where each step incrementally builds on the previous ones.
    \item \textit{Divide and Conquer ($a_{5}$)}: Breaking down a complex reasoning problem into several smaller subproblems and progressively solving them to achieve the overall solution.
    \item \textit{Self-Reflection ($a_{6}$)}: Engaging in timely reflection of prior solutions and implementing necessary refinement during the reasoning process to ensure accuracy.
\end{itemize}


\begin{table*}[htbp!]
\caption{Comparison with other search-based LLM and MLLM methods. Note that, most method contain limited space. In contrast, we define a rich set of reasoning actions, thus enhancing the upper bound of model reasoning capabilities.}
\label{table s4}
\vskip 0.1in
\begin{center}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{cll}
% \begin{tabular*}{\linewidth}{@{}lccc@{}}
\toprule
Type & Dataset & Evaluation Dimensions\\
\midrule
\multirow{2}{*}{Visual Question Answering} & ChartQA \cite{masry-etal-2022-chartqa}     & chart understanding and reasoning \\
& MMStar~\cite{chen2024we}     & 6 core capabilities, like scientific reasoning      \\
\midrule
\multirow{3}{*}{Mathematics} & MathVista~\cite{lu2023mathvista}     & 12 core capabilities, like arithmetic reasoning \\
& MathVerse~\cite{zhang2025mathverse}     & 6 distinct versions-text-dominant \\
& MathVision \cite{wang2024measuring}     & 16 mathematical domains \\
% \rowcolor{mygray}
\midrule
Commonsense and science & GAOKAO-MM \cite{zong-qiu-2024-gaokao}     & 8 subjects, like history \\
\bottomrule
\end{tabular}
\end{center}
\vskip -0.1in
\end{table*}



\subsection{Reward Value in MCTS}\label{B.2}
To simplify the construction of thought cards via the MCTS iteration, we avoid introducing an external reward model to score each step. Given the current skepticism regarding the self-rewarding capabilities of language models, alternative methods are necessary. Inspired by the principle that actions leading to correct answers should be rewarded more frequently, we aim to increase their likelihood of selection in future MCTS tree expansions. Following \cite{zhou2024language,wu2024beyond}, we define the reward value as the likelihood (or confidence) of self-consistency via majority voting. Note that this principle applies only to leaf nodes. The Q-values of intermediate nodes are initially set to 0 and are dynamically updated during the backpropagation process, as described in Equation \ref{eq1}.


\section{More Details about Experimental Setup}\label{C}

\subsection{Benchmarks and Datasets}\label{C.1}
Here are the details of the benchmarks used in our experiments. The statistics of the datasets are recorded in Table \ref{table s4}. Note that the evaluation metrics for all datasets are consistent with their official standards, primarily focusing on accuracy.

\begin{itemize}

\item \textbf{ChartQA}~\cite{masry-etal-2022-chartqa} is a comprehensive benchmark for chart-based question answering that emphasizes visual and logical reasoning capabilities. The dataset comprises 9,608 human-authored questions and 23,111 machine-generated questions derived from human-written chart summaries. It incorporates 20,882 real-world charts from diverse sources including Statista, Pew Research, Our World in Data, and OECD, spanning multiple domains and visualization styles. ChartQA addresses prior dataset limitations through its focus on human-authored questions, real-world charts, and open-vocabulary responses. We utilize this dataset to evaluate our method's performance on visual chart question-answering tasks commonly encountered in real-world scenarios.

\item \textbf{MMStar}~\cite{chen2024we} serves as a vision-critical multimodal benchmark for evaluating MLLM capabilities. It addresses two fundamental limitations in existing evaluations: the redundancy of visual inputs and potential training data contamination. Comprising 1,500 meticulously curated challenge samples across six core competencies and 18 evaluation axes, the benchmark undergoes rigorous validation to ensure visual necessity and minimal data leakage. We employ this comprehensive dataset to assess our method's general reasoning capabilities.

\item \textbf{MathVista}~\cite{lu2023mathvista} is a mathematical visual benchmark consisting of 6,141 examples. These examples are divided into two subsets: \textit{testmini} with 1,000 annotated examples and \textit{test} comprising 5,141 problems without public answers. We conduct our evaluation on the \textit{testmini} subset to assess visual comprehension and compositional reasoning capabilities.

\item \textbf{MathVerse}~\cite{zhang2025mathverse} is a comprehensive and specialized visual mathematics benchmark for assessing multimodal mathematical reasoning capabilities of MLLMs. The benchmark contains 2,612 visual math problems, combining 1,236 newly collected problems from public repositories with 1,376 problems from existing benchmarks. Human annotators have transformed each problem into six distinct versions-text-dominant, text-lite, text-only, vision-intensive, vision-dominant, and vision-only—each offering different levels of multimodal information. We utilize this dataset to evaluate AStar's performance across varying degrees of multimodal integration.

\item \textbf{MathVision}~\cite{wang2024measuring} constitutes a meticulously compiled collection of 3,040 mathematics problems, each incorporating visual elements from authentic mathematics competitions. The dataset spans 16 mathematical domains and features a five-tier difficulty classification system. We employ this dataset to evaluate our model's performance on challenging reasoning tasks.

\item \textbf{GAOKAO-MM} \cite{zong-qiu-2024-gaokao} represents a multimodal evaluation framework derived from the Chinese National College Entrance Examination (Gaokao). It encompasses eight academic subjects and includes twelve categories of images, such as diagrams, function graphs, maps, and photographs. The benchmark aims to evaluate models' abilities to understand and reason over diverse multimodal content, reflecting the complexity and breadth of knowledge. We leverage this dataset to assess AStar's cross-domain generalization capabilities.

\end{itemize}


\subsection{Baselines}\label{C.2}
We evaluate AStar against four strong baseline categories: 

\textbf{Open-source general MLLMs}: Recent advances include Qwen2-VL \cite{wang2024qwen2} and InternVL2 \cite{chen2024internvl}, which demonstrate exceptional capabilities in complex reasoning and mathematical problem-solving.

\textbf{Open-source math MLLMs}: Several models have been specifically optimized for mathematical reasoning in MLLMs. These models can be grouped based on their core methodologies:
\begin{enumerate}[label=\textit{\arabic*.}, leftmargin=*, itemsep=1em]
  \item \textit{Dataset Construction and Fine-Tuning:}
  \begin{enumerate}[label=\textit{\alph*.}, leftmargin=*, itemsep=0.5em]
    \item G-LLaVA \cite{gao2023g}: Introduces the Geo170K dataset, comprising over 170K geometric image-caption and question-answer pairs, to enhance MLLMs' geometric problem-solving capabilities.
    \item Math-LLaVA \cite{shi2024math}: Develops the MathV360K dataset by collecting 40K high-quality images with question-answer pairs and synthesizing an additional 320K pairs, then fine-tunes the LLaVA-1.5 model to improve multimodal mathematical reasoning.
    \item MultiMath \cite{peng2024multimath}: Constructs the MultiMath-300K dataset, encompassing K-12 level mathematical problems with image captions and step-wise solutions, and trains the MultiMath-7B model to bridge visual and mathematical reasoning.
  \end{enumerate}

  \item \textit{Progressive Alignment and Curriculum Learning:}
  \begin{enumerate}[label=\textit{\alph*.}, leftmargin=*, itemsep=0.5em]
    \item Math-PUMA \cite{zhuang2024math}: Proposes a three-stage Progressive Upward Multimodal Alignment methodology to enhance MLLMs' mathematical reasoning skills, focusing on aligning visual and textual modalities through a structured training process.
    \item LlamaV-o1 \cite{thawakar2025llamav}: Introduces a multimodal visual reasoning model trained using a multi-step curriculum learning approach, organizing tasks progressively to facilitate incremental skill acquisition and problem-solving.
  \end{enumerate}

  \item \textit{CoT Reasoning Integration:}
  \begin{enumerate}[label=\textit{\alph*.}, leftmargin=*, itemsep=0.5em]
    \item LLaVA-CoT \cite{xu2024llava}: Develops a vision-language model designed to conduct autonomous multistage reasoning, employing a structured approach that includes summarization, visual interpretation, logical reasoning, and conclusion generation.
    \item URSA \cite{luo2025ursa}: Proposes a three-module synthesis strategy integrating CoT distillation, trajectory-format rewriting, and format unification, resulting in the MMathCoT-1M dataset and the URSA-7B model, which demonstrates state-of-the-art performance on multiple multimodal mathematical benchmarks.
  \end{enumerate}

  \item \textit{Large-Scale Multimodal Pre-Training:}
  \begin{enumerate}[label=\textit{\alph*.}, leftmargin=*, itemsep=0.5em]
    \item InfiMM-Math \cite{han24infimm}: Introduces InfiMM-WebMath-40B, a high-quality dataset comprising 24 million web pages, 85 million associated image URLs, and 40 billion text tokens, aiming to improve MLLMs' mathematical reasoning through large-scale multimodal pre-training.
  \end{enumerate}

  \item \textit{Slow-Thinking System Implementation:}
  \begin{enumerate}[label=\textit{\alph*.}, leftmargin=*, itemsep=0.5em]
    \item Virgo \cite{du2025virgo}: Explores a straightforward approach to implementing multimodal slow-thinking systems by fine-tuning a capable MLLM with a small amount of textual long-form thought data, demonstrating that such reasoning processes can be effectively transferred to MLLMs.
  \end{enumerate}
\end{enumerate}

\textbf{Advanced closed-source MLLMs}: Leading proprietary models including GPT-4V \cite{openai2023gpt4v}, GPT-4o \cite{openai2024gpt4o}, and Gemini-1.5-Pro \cite{team2023gemini} demonstrate exceptional capabilities in multimodal understanding and task-solving, setting benchmarks for open-source alternatives.

\textbf{Multimodal tree-based methods}: Recent works incorporate explicit search mechanisms into multimodal reasoning. AR-MCTS \cite{dong2024progressive} enhances reasoning by combining Monte Carlo Tree Search (MCTS) with active retrieval, improving reasoning space diversity and reliability. Mulberry \cite{yao2024mulberry} leverages multi-model collaboration through MCTS's four iterative operations (selection, expansion, simulation, backpropagation) to identify optimal reasoning paths.


\subsection{Implementation Details}\label{C.3}
We utilize the vLLM framework\footnote{\url{https://github.com/vllm-project/vllm}} with the following parameters: temperature set to 0.8, top p set to 0.9, and max tokens set to 1024. All experiments were conducted on a machine running Ubuntu 22.04, equipped with NVIDIA A100-80GB GPUs. We list all hyperparameters in Table \ref{table s5}.


\begin{table*}[htbp!]
\caption{All hyperparameters utilized in this paper.}
\label{table s5}
\centering
\vskip 0.05in
\begin{tabular}{@{\extracolsep{\fill}}ccc}
\toprule
Hyperparameter & Value & Description\\
\midrule
temperature  & 0.8 & \multirow{5}{*}{vllm inference settings}\\
% \midrule
top-p     & 0.9 \\
top-k     & 40  \\
repetition penalty & 1.0 \\
max tokens & 1024 \\
\midrule
maximum tree depth $d_{max}$ & 5 & \multirow{3}{*}{MCTS}\\
exploration weight $w$     & 2.0 \\
% discount factor $\alpha$   & 0.5 \\ 
predefined terminal threshold $c$   & 0.90 \\
\midrule
balance factor $k$ & 0.95 & VOC-based optimal path selection in \textit{Sec. \ref{sec4.2}} Phase 2\\
\bottomrule
\end{tabular}
\vskip 0.05in
\end{table*}

\textit{In the MCTS-powered prior thought card construction stage}, we implement an early termination strategy based on self-consistency \cite{wang2023selfconsistency} for enhanced efficiency. Building on the observation that repeated action sampling at the same state often indicates successful task completion, we terminate simulation early when the model's consistency score exceeds a threshold $c$ (i.e., $SC(s) > c$).

\textit{In the inference stage}, we evaluate AStar's effectiveness across both LLM and MLLM architectures. For MLLMs, we can directly utilize their native visual understanding and reasoning capabilities for visual parsing. For traditional LLMs like Qwen2.5-7B, we employ Qwen2-72B-VL solely for visual information extraction, deliberately avoiding its visual reasoning capabilities. This design choice ensures that the inference backbone remains the primary reasoning component while maintaining computational efficiency.

\section{Supplementary Results}\label{D}
This section presents supplementary results and analysis, including: \hyperlink{D.1}{D.1} Detailed Results on Multimodal Reasoning Benchmarks, \hyperlink{D.2}{D.2} Comparison with Strong Baselines, \hyperlink{D.4}{D.3} Integration with SFT, and \hyperlink{D.5}{D.4} Weak-to-Strong Generalization.

\hypertarget{D.1}{\subsection{Detailed Results on Multimodal Reasoning Benchmarks}}\label{D.1}
We provide detailed test results based on various mathematical abilities using the MathVista benchmark. As shown in Table \ref{table s6}, AStar demonstrates notable strengths in statistics and challenging logic, whereas other models exhibit superior performance in algebraic and geometric problem-solving. Notably, with Qwen2-VL-2B as our inference backbone, our 2B model even surpasses larger models such as InternLM-XComposer2-VL-7B and Math-LLaVA, achieving performance comparable to GPT-4V. This indicates that, regardless of model size, our AStar reasoning framework effectively enhances multimodal reasoning capabilities.


\begin{table*}[htbp]
\centering
\caption{Results on MathVista \textit{testmini} detailed mathematics capabilities. The best results of closed-source MLLMs are highlighted in \textcolor{back_deepgreen}{green}. The best and second-best results of open-source MLLMs are highlighted in \textcolor{back_deepred}{red} and \textcolor{back_deepblue}{blue} respectively.}
\label{table s6}
\vskip 0.1in
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c} 
\toprule
Model & \#Params & ALL & ALG & ARI & GEO & LOG & NUM & SCI & STA \\

\midrule 
\multicolumn{10}{c}{\textit{Baselines}}\\ \midrule
Random Choice & - & 17.9 & 25.8 & 13.8 & 22.7 & 13.4 & 8.8 & 15.8 & 14.3 \\
Human Performance & - & 60.3 & 50.9 & 59.2 & \highg{51.4} & 40.7 & 53.8 & 64.9 & 63.9 \\
\midrule

\multicolumn{10}{c}{\textit{Closed-source MLLMs}}\\ \midrule
Qwen-VL-Plus~\cite{bai2023qwen} & - & 43.3 & 39.1 & 32.0 & 39.3 & 18.9 & \highg{26.4} & 59.0 & \highg{56.1} \\
GPT-4V~\cite{openai2023gpt4v} & - & \highg{49.9} & \highg{53.0} & \highg{49.0} & 51.0 & \highg{21.6} & 20.1 & \highg{63.1} & 55.8 \\
\midrule 
% \midrule
\multicolumn{10}{c}{\textit{Open-source Genreral MLLMs}}\\ \midrule
mPLUG-Owl2-7B~\cite{ye2023mplug} & 7B & 22.2 & 23.6 & 19.2 & 23.9 & 13.5 & 12.7 & 26.3 & 21.4 \\
LLaVA-1.5-13B~\cite{liu2024llava} & 13B & 25.7 & 19.6 & 28.6 & 17.6 & 10.8 & 27.8 & 33.6 & 22.9 \\
MiniGPT-v2~\cite{chen2023minigpt} & 7B & 23.1 & 28.1 & 21.0 & 24.7 & 16.2 & 16.7 & 25.4 & 17.9 \\
InternLM-XComposer2-VL-7B~\cite{dong2024internlm} & 7B & 47.8 & 32.0 & 51.6 & 30.5 & 13.5 & 43.8 & 37.7 & \high{62.8}  \\
SPHINX-MoE~\cite{lin2023sphinx} & 8$\times$ 7B & 42.3 & 31.7 & 41.6 & 30.5 & 16.2 & 27.1 & 50.8 & 50.8 \\
DeepSeek-VL~\cite{lu2024deepseek} & 7B & 34.9 & 29.2 & 38.8 & 27.2 & 18.9 & \high{43.1} & 35.3 & 33.2 \\
InternVL2-8B~\cite{chen2024internvl} & 8B & 58.3 & 59.8 & \high{56.4} & 60.3 & 10.8 & 30.6 & \high{59.0} & \highr{68.8} \\
Qwen2-VL~\cite{wang2024qwen2} & 7B & 58.9 & 44.1 & 57.5 & 43.1 & 24.3 & 41.7 & \highr{66.4} & 75.1 \\ \midrule
\multicolumn{10}{c}{\textit{Open-source Math MLLMs (Large-Scale Training)}}\\ \midrule
G-LLaVA~\cite{gao2024llm} & 7B & 25.1 & 36.0 & 19.4 & 37.6 & 15.2 & 17.7 & 21.0 & 15.1  \\
Math-LLaVA~\cite{shi2024math} & 13B & 46.6 & 51.5 & 40.7 & 56.2 & 23.3 & 34.7 & 47.7 & 42.3 \\
Multimath-7B~\cite{peng2024multimath} &7B & 50.0 & 61.9 & 42.2 & 64.9 & 23.3 & 32.6 & 42.6 & 49.2 \\
Math-PUMA-Qwen2-7B~\cite{zhuang2024math} & 7B & 47.9 &47.7 & 46.2 & 47.3 & 21.6 & 32.6 & 42.6 & 55.8 \\
URSA-7B \cite{luo2025ursa} & 7B & \high{59.8} & \highr{74.0} & 53.5 & \highr{77.4} & 21.6 & 35.4 & 58.2 & 57.1 \\
\midrule
\multirow{2}{*}{AStar (Ours, Training-free Reasoning)} & 7B & \highr{63.5} & \high{69.0} & \highr{63.1} & \high{71.7} & \highr{61.3} & \highr{60.0} & 48.2 & \highr{68.8} \\
 & 2B & 49.6 & 51.0 & 49.9 & 53.1 & \high{34.7} & 46.8 & 41.3 & 54.3 \\
% $\Delta$ over SOTA \textit{Open-Source Math MLLMs} & - & +3.7 & +3.7 & - & - & - & - & - & -\\
\bottomrule
\end{tabular}
}
\vskip 0.05in
\end{table*}


\hypertarget{D.2}{\subsection{Comparison with Strong Baselines}}\label{D.2}
Table \ref{table s7} provides a performance comparison of our method against leading open-source and closed-source models. AStar with a 7B inference backbone achieves competitive performance with larger MLLMs. Notably, on the challenging MathVision dataset, our approach exhibits substantial improvements. This suggests that while simpler visual tasks may not significantly benefit from structured reasoning strategies—with performance primarily determined by model capacity—our method shows increasing performance advantages on more complex datasets like MathVerse and MathVision, surpassing both InternVL2.5-26B and InternVL2-Llama3-76B models despite their larger parameter counts of 26B and 72B respectively.

We also benchmark our approach against recent powerful multimodal reasoning methods, including LLaVA-CoT \cite{xu2024llava} and Virgo \cite{du2025virgo}. As shown in Table \ref{table s8}, comparative experiments across different model scales demonstrate that our AStar reasoning framework, which effectively integrates MLLMs' inherent reasoning capabilities with external reasoning guidelines, achieves superior performance with minimal prior data. On challenging datasets like MathVerse, our method exhibits substantial improvements, with the 2B model achieving comparable performance to the extensively trained Virgo 7B model. Further analysis reveals two key insights: First, methods relying on vast solution spaces often struggle to identify appropriate reasoning paths, analogous to finding a needle in a haystack. Second, approaches dependent on large-scale training data typically face difficulties in fully capturing complex long-chain reasoning patterns through implicit learning. In contrast, our method adaptively identifies suitable reasoning strategies based on problem complexity, enabling efficient inference across diverse scenarios.


\begin{table*}[htbp]
\caption{Comparison with leading LLMs. The best results are highlighted in \textbf{bold}. Results for off-the-shelf models are sourced from corresponding official websites.}
\label{table s7}
\vskip 0.1in
\begin{center}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccc}
\toprule
Model & MathVista & MathVerse & MathVision & Average\\
\midrule
% \rowcolor[HTML]{EBF5F9}
\multicolumn{5}{c}{\textit{Closed-Source Models}}\\
GPT-4o-0513~\cite{openai2024gpt4o}       & 63.8 & 50.2 & 30.4 & 48.2 \\
GPT-4V~\cite{openai2023gpt4v}                & 49.9 & 54.4 & 24.8 & 43.1 \\
% Gemini-1.5-002-Flash~\cite{team2023gemini} & 58.4 & - & - \\
Gemini-1.5-Pro~\cite{team2023gemini}         & 63.9 & 35.3 & 19.2 & 39.5 \\
Claude-3.5-Sonnet~\cite{claude3}             & 67.7 & - & - & - \\
Qwen-VL-Plus~\cite{bai2023qwen}              & 43.3 & 21.3 & 10.8 & 25.2 \\
\midrule
\multicolumn{5}{c}{\textit{Open-Source Models}}\\
Qwen2-VL-72B \cite{wang2024qwen2}            & \textbf{70.5} & - & 25.9 & - \\
LLaVA-OneVision-72B \cite{li2024llava}       & 67.5 & 39.1 & - & - \\
InternVL2.5-26B \cite{chen2024internvl}      & 67.7 & 40.1 & 28.0 & 45.3 \\
InternVL2-Llama3-76B \cite{chen2024internvl} & 65.5 & 42.8 & 23.7 & 44.0 \\
% InternVL2.5-78B                            & 72.3 & 51.7 & 34.9 \\
\midrule
\rowcolor{mygray}
Astar (Ours) & 63.5  & \textbf{54.0} & \textbf{32.4} & \textbf{50.0} \\
\bottomrule
\end{tabular}
\end{center}
% \vskip -0.1in
\end{table*}


\begin{table*}[htbp!]
\caption{Comparison with recent works targeting enhanced multimodal reasoning through structured thinking. We list 2B and $\ge$ 7B-scale baselines. The best results in each box are highlighted in \textbf{bold}. Our method demonstrates significant performance improvements.}
\label{table s8}
\vskip 0.1in
\begin{center}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccccc}
\toprule
Model & Size & MMStar & ChartQA & MathVista & MathVerse\\
\midrule
% \multicolumn{7}{c}{\textit{Closed-Source Models}}\\
\multicolumn{6}{c}{\textit{2B-Scale Baselines}}\\
Mulberry~\cite{yao2024mulberry}              & 2B    & 51.3 & 77.7 & \textbf{51.7}  & -    \\
\rowcolor{mygray}
Astar (Ours) & 2B & \textbf{51.7} & \textbf{78.3} & 49.6 & \textbf{33.7} \\
\midrule
\multicolumn{6}{c}{\textit{$\ge$ 7B-Scale Baselines}}\\
Insight-V~\cite{dong2024insight}             & 7B    & 61.5    & 81.5    & 59.9     & -    \\
AR-MCTS~\cite{dong2024progressive}           & 7B    & -    & -    & \textbf{64.1}  & -     \\
Mulberry~\cite{yao2024mulberry}              & 7B    & 61.3 & \textbf{83.9} & 63.1  & -    \\
LLaVA-CoT~\cite{openai2024gpt4o}             & 11B    & 57.6 & -    & 54.8  & -    \\
LlamaV-o1~\cite{thawakar2025llamav}          & 11B    & 59.6 & -    & 54.4  & -    \\
URSA~\cite{luo2025ursa}                      & 7B    & -    & -    & 59.8  & 45.7 \\
Virgo~\cite{luo2025ursa}                     & 7B    & -    & -    & -  & 37.5 \\
\rowcolor{mygray}
Astar (Ours) & 7B & \textbf{61.7} & \textbf{83.9} & 63.5 & \textbf{54.0} \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}




\hypertarget{D.4}{\subsection{Integration with SFT}}\label{D.4}
To evaluate the versatility of the AStar framework, we conducted experiments on both pretrained base models and their supervised fine-tuning (SFT) counterparts. Table \ref{table s10} presents a comprehensive comparison of model performance across different configurations. Our results demonstrate that the AStar framework consistently yields substantial improvements over the zero-shot performance reported in the original papers, regardless of whether it is applied to pretrained or SFT models.

\begin{table*}[htbp!]
\caption{Integration With SFT on MathVision. We evaluate the AStar framework using Qwen2-VL-2B and Qwen2-VL-7B as backbone models, comparing both pre-trained base models and SFT variants across different difficulty levels.}
\label{table s10}
\vskip 0.1in
\begin{center}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccccc}
\toprule
Backbone & 1 & 2 & 3 & 4 & 5 & Average\\
\midrule
Qwen2-VL-2B-Base           & 20.8 & 20.7 & 21.4 & 17.8 & 10.2 & 18.2\\
Qwen2-VL-2B  & \textbf{25.0} & 20.0 & 15.6 & 16.7 & \textbf{17.6} & \textbf{22.3}\\
\midrule
Qwen2-VL-7B-Base           & 26.8 & 20.6 & 23.7 & 26.7 & 22.4 &23.4\\
Qwen2-VL-7B           & 26.5 & \textbf{21.3} & \textbf{26.2} & \textbf{29.8} & \textbf{23.8} & \textbf{25.2}\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}

Notably, the combination of SFT and AStar achieves superior performance, suggesting a synergistic effect between supervised fine-tuning and our framework. For instance, with Qwen2-VL-2B as the backbone, the SFT variant (Qwen2-VL-2B-Instruct) achieves better average performance (22.3$\%$ vs 18.2$\%$) and shows particular strength in handling problems at difficulty levels 1 and 5, where it achieves 25.0$\%$ and 17.6$\%$ accuracy, respectively. Similarly, the Qwen2-VL-7B model with AStar demonstrates robust performance across all difficulty levels, achieving an average accuracy of 25.2$\%$.
These findings suggest promising future research directions for further integrating SFT techniques into the AStar framework to enhance overall system performance.


\hypertarget{D.5}{\subsection{Weak-to-Strong Generalization}}\label{D.5}
As described in \cite{bansal2024smaller,yang-etal-2024-weak}, interactions between weak and strong models can be categorized into three primary paradigms: 1) weak-to-strong improvement, where models with limited capabilities can effectively guide the development of more advanced models, 2) self-improvement, wherein the weak and strong models are identical, focusing on designing methods to enhance the model's own performance, and 3) knowledge distillation, which involves transferring the capabilities or knowledge from strong models to weak models. Through our main results, where thought cards are constructed based on Qwen2-VL-7B, we have already demonstrated the significant potential of our AStar method in knowledge distillation (Qwen2-VL-7B$\longrightarrow$Qwen2-VL-2B) and self-improvement (Qwen2-VL-7B$\longrightarrow$Qwen2-VL-7B).

Therefore, in this section, we empirically evaluate AStar's effectiveness in weak-to-strong generalization. We hypothesize that a weaker model, when integrated with AStar, can effectively guide a more powerful policy model. To test this hypothesis, we leverage the reasoning guidance (thought cards) generated by Qwen2-VL-7B to assist GPT-4o in solving challenging problems from the MathVision dataset.

As illustrated in Figure \ref{FigureS2}, we present a comprehensive comparison between GPT-4o's zero-shot performance and its performance when enhanced with AStar (using Qwen2-VL-7B's reasoning guidance) across multiple dimensions of the MathVision benchmark. Our results demonstrate that incorporating our reasoning paradigm enables models with limited capabilities to provide valuable supervision to stronger policy models. Notably, the Qwen2-VL-7B model, despite its relatively limited capabilities, successfully guides the more powerful, closed-source GPT-4o. This finding validates the potential of learning from prior reasoning patterns and offers promising insights for developing more scalable and sophisticated strategies to enhance AI reasoning capabilities.


\begin{figure*}[ht!]
% \vskip 0.1in
\centerline{\includegraphics[width=1.0\linewidth]{FigureS2.pdf}}
\caption{Weak-to-strong generalization results: Performance comparison of GPT-4o in zero-shot and AStar-enhanced (Qwen2-VL-7B guidance) settings across various dimensions of MathVision. The results demonstrate that a weaker model (Qwen2-VL-7B) can effectively guide a stronger model (GPT-4o) through the AStar framework, validating the framework's potential for weak-to-strong generalization in mathematical reasoning tasks.}
\vskip -0.1in
\label{FigureS2}
\end{figure*}


\section{Case Study}\label{F}
Taking a geometry problem in MathVerse as an example, we provide a qualitative comparison of LLaVA-NeXT-8B \cite{liu2024llava}, Qwen2-VL-7B \cite{wang2024qwen2}, and AStar-7B in Figure \ref{FigureS15}. The results demonstrate that LLaVA-NeXT-8B and Qwen2-VL-7B struggle to accurately parse complex geometric relationships, leading to potential errors in each step of their reasoning processes. In contrast, our AStar framework effectively combines MLLMs' internal implicit reasoning capabilities with external explicit reasoning guidelines. This integration exhibits explicit and well-defined reasoning steps with comprehensive understanding, ultimately arriving at the correct solution.

\begin{figure*}[htp!]
\centering
\vskip 0.1in
\includegraphics[width=0.95\textwidth]{FigureS15.pdf}
\caption{Qualitative comparison of reasoning processes across different models. AStar demonstrates superior understanding through its explicit, well-structured reasoning steps, leading to accurate solution derivation. The comparison highlights AStar's ability to systematically decompose complex geometric relationships while maintaining reasoning clarity throughout the problem-solving process.}
\label{FigureS15}
\vskip 0.05in
\end{figure*}


% \section{Limitations and Future Work}\label{F}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%













\end{document}

