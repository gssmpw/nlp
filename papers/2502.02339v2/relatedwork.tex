\section{Related Work}
\label{sec2}
\textbf{Multimodal Reasoning}
$\,$ Recent advancements in MLLMs have demonstrated robust capabilities across diverse domains, including visual understanding \cite{10445007}, mathematics \cite{zhang2024mavis,du2025virgo}, and scientific inquiries \cite{zong-qiu-2024-gaokao}. Despite these achievements, complex multimodal reasoning remains challenging due to its demands on both visual perception and high-level cognition. Inspired by OpenAI o1's impressive performance, recent approaches \cite{zhang2024improve,xu2024llava,thawakar2025llamav} attempt structured reasoning with pre-defined stages, enhancing MLLMs' CoT capabilities \cite{zhang2024multimodal}. However, their rigid structure limits flexibility across different tasks, overlooking the importance of adaptive reasoning in unleashing multimodal reasoning potential \cite{wang2024enhancing}. Our approach addresses this by introducing a hierarchical tree structure that enables task-specific reasoning path generation and selection.

\vskip 0.0827in
\textbf{Tree-based Search}
$\,$ Tree structures have demonstrated significant potential in language models \cite{zhang2024accessing,qi2024mutual,wu2024beyond}. Recent efforts explore applying these tree search methods to search effective reasoning paths for MLLMs. While AR-MCTS \cite{dong2024progressive} enhances multimodal reasoning by integrating MCTS with active retrieval, its extensive iteration requirements and computational overhead limit practical applications. Similarly, Mulberry \cite{yao2024mulberry} leverages tree structures to distill 260K long-chain reasoning data from powerful models like GPT-4o, but requires substantial computational resources and high-capacity teacher models. These methods struggle to achieve an optimal balance between performance and efficiency. To address these limitations, we propose incorporating high-level reasoning abstractions into MCTS, achieving competitive performance with higher efficiency.