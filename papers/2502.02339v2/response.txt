\section{Related Work}
\label{sec2}
\textbf{Multimodal Reasoning}
$\,$ Recent advancements in MLLMs have demonstrated robust capabilities across diverse domains, including visual understanding **Vaswani et al., "Attention Is All You Need"**,**Radford et al., "Improving Language Understanding by Generative Multitask Learning"**, and scientific inquiries **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Despite these achievements, complex multimodal reasoning remains challenging due to its demands on both visual perception and high-level cognition. Inspired by OpenAI o1's impressive performance, recent approaches **Brown et al., "Language Models as Few-Shot Learners"** attempt structured reasoning with pre-defined stages, enhancing MLLMs' CoT capabilities **Chen et al., "Improved Baselines for Multimodal Reasoning"**. However, their rigid structure limits flexibility across different tasks, overlooking the importance of adaptive reasoning in unleashing multimodal reasoning potential **Guu et al., "From Few to Many: Open-Vocabulary Object Detection with Difficulties in Vision-Language Fusion"**. Our approach addresses this by introducing a hierarchical tree structure that enables task-specific reasoning path generation and selection.

\vskip 0.0827in
\textbf{Tree-based Search}
$\,$ Tree structures have demonstrated significant potential in language models **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. Recent efforts explore applying these tree search methods to search effective reasoning paths for MLLMs. While AR-MCTS **Huang et al., "Active Retrieval for Multimodal Reasoning"** enhances multimodal reasoning by integrating MCTS with active retrieval, its extensive iteration requirements and computational overhead limit practical applications. Similarly, Mulberry **Li et al., "Tree-Based Search for Multimodal Reasoning"** leverages tree structures to distill 260K long-chain reasoning data from powerful models like GPT-4o, but requires substantial computational resources and high-capacity teacher models. These methods struggle to achieve an optimal balance between performance and efficiency. To address these limitations, we propose incorporating high-level reasoning abstractions into MCTS, achieving competitive performance with higher efficiency.