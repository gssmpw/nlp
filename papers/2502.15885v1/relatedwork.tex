\section{Related Work}
\begin{figure*}[t] 
    \centering
    \includegraphics[width=\textwidth]{figures/IJCAI-fig3.pdf}
    \caption{An overview of the proposed method. The RGB image undergoes transformation into class tokens and patch tokens via the embedding layer. The dot-product attention mechanism is then employed to compute the attention score matrix and generate output tokens. To further refine the attention score matrix, the cosine similarity between the RGB values of the original image and the tokens is used to adjust the distribution of attention weights. Furthermore, the CPDO and PPDO methods are customized to amplify high-confidence information and suppress the influence of low-confidence information. Finally, the optimized tokens are incorporated into the original tokens as residuals, producing refined output tokens for subsequent computations in the encoder.
    }
    \label{Figure 3} 
    \end{figure*}

\subsection{Weakly Supervised Semantic Segmentation}

    In WSSS, full pixel-level segmentation relies on limited supervisory signals. The development of WSSS has significantly alleviated the dependency on large amounts of pixel-level labels in traditional semantic segmentation models. Prevailing approaches mainly use the CAM technique introduced by Zhou et al.~[\citeyear{zhou2016learning}] as an initial step in identifying target object locations. The CAM combines the global average pooling layer with the classification layer to efficiently consolidate feature information for each pixel, generating an activation map that is aligned with the semantic representation of a specific category and serving as a key component of WSSS. However, CAMs typically display only the salient regions of the target object, hindering the capture of complete location information. Therefore, directly adopting CAM as full pixel-level segmentation labels is constrained by these shortcomings. Most of the research focuses on generating more precise and comprehensive CAMs and refining pseudo-labels to augment the fidelity and precision of segmentation results. By optimizing the CAM generation process and incorporating richer semantic information, researchers aim to mitigate issues of insufficient or overly concentrated activation areas, paving the way for more accurate and versatile WSSS methodologies.

    
    
\subsection{Generating High-quality CAMs}

    Convolutional Neural Networks (CNNs) \cite{qi2025medconv} and Vision Transformers (ViTs) \cite{wu2024xlip,ji2024sine,zhang2024jointvit} are two popular approaches for generating CAMs. CNN-based methods often face challenges with localized activation due to the limited receptive field and reliance on local features. To address this, strategies such as random occlusion~\cite{kumar2017hide} (e.g., "hide-and-seek") and adversarial training~\cite{kweon2023weakly} have been employed to expand activation areas. Techniques like dilated convolutions~\cite{huang2018weakly} and multi-layer feature integration~\cite{li2022weakly} further elevate segmentation accuracy by capturing more contextual information. Additionally, specialized loss functions, such as contrastive loss~\cite{zhu2024weakclip} and SEC loss~\cite{wu2022adaptive}, and supplementary data, including saliency maps and videos~\cite{wang2018weakly}, have been used to improve object localization.An alternative method for generating CAMs is to utilize the self-attention weight matrix in ViT~\cite{dosovitskiy2021image}. For example, Gao et al.~[\citeyear{gao2021ts}] combine semantic-aware annotations with semantically unrelated attention maps, providing a feasible approach for object localization by utilizing the semantic and localization information extracted by ViT. Ru et al.~[\citeyear{ru2022learning}] refine the initial pseudo-labels for segmentation by learning robust semantic affinities with the aid of a multi-head attention mechanism. Xu et al.~[\citeyear{xu2023learning}] transform simple class labels into high-dimensional semantic information through Contrastive Language-Image Pretraining (CLIP) to guide the ViT, forming a multimodal representation of text and images, thus generating more precise object localization maps.
    
\subsection{Refinement of CAMs}

     At present, existing refinement methods primarily focus on the second stage of multi-stage models. For instance, Ahn et al.~[\citeyear{ahn2018learning}] train AffinityNet by leveraging reliable foreground and background activation maps to predict affinities between neighboring pixels, which are then used as metrics for the random walk algorithm, thereby expanding the CAM. The IRN method~\cite{ahn2019weakly} further refines CAM by estimating object boundary information through the semantic affinities between the original pixels in the image. Wang et al.~[\citeyear{wang2020weakly}] propose a method that refines CAM by leveraging high-confidence pixels from segmentation results as inputs to a pairwise affinity network. Xu et al.~[\citeyear{xu2021leveraging}] highlight that affinities in saliency maps and segmented representations more effectively reflect the diversity in CAM representations. For WSSS tasks, such refinement is crucial for improving the final segmentation performance and helps generate more accurate and reliable pixel-level pseudo-labels.