﻿
@proceedings{.2016,
 year = {2016},
 title = {2016 IEEE Symposium on 3D User Interfaces (3DUI)},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1}
}


@article{Reason.1978,
 author = {Reason, J. T.},
 year = {1978},
 title = {Motion sickness adaptation: a neural mismatch model},
 urldate = {08.11.2022},
 pages = {819--829},
 volume = {71},
 number = {11},
 issn = {0141-0768},
 journal = {Journal of the Royal Society of Medicine},
 doi = {10.1177/014107687807101109}
}


@misc{Petersen.26.06.2021,
 author = {Petersen, Klaus},
 year = {26.06.2021},
 title = {HMD Body Adjustment with Optitrack},
 url = {https://lp-research.atlassian.net/wiki/spaces/LKB/pages/1778876417/HMD+Body+Adjustment+with+Optitrack},
 urldate = {16.11.2022}
}


@misc{Petersen.2020,
 author = {Petersen, Klaus},
 year = {2020},
 title = {Collaboration with Varjo},
 url = {https://lp-research.com/collaboration-with-varjo/},
 urldate = {01.11.2022}
}


@misc{Petersen.2019,
 abstract = {Building on our sensor fusion technology, LP-RESEARCH Inc. has created LPVR middleware as a full solution for VR and AR applications.},
 author = {Petersen, Klaus},
 year = {2019},
 title = {LPVR Middleware a Full Solution for AR / VR},
 url = {https://lp-research.com/middleware-full-solution-ar-vr/},
 urldate = {01.11.2022},
 note = {Accessed: 01.11.2022}
}


@misc{Petersen.2019b,
 abstract = {Building on our sensor fusion technology, LP-RESEARCH Inc. has created LPVR middleware as a full solution for VR and AR applications.},
 author = {Petersen, Klaus},
 year = {2019},
 title = {LPVR Middleware a Full Solution for AR / VR: Introducing LPVR Middleware},
 url = {https://lp-research.com/middleware-full-solution-ar-vr/},
 urldate = {01.11.2022}
}


@inproceedings{Pavanatto.2021,
  author={Pavanatto, Leonardo and North, Chris and Bowman, Doug A. and Badea, Carmen and Stoakley, Richard},
  booktitle={2021 IEEE Virtual Reality and 3D User Interfaces (VR)}, 
  title={Do we still need physical monitors? An evaluation of the usability of AR virtual monitors for productivity work}, 
  year={2021},
  pages={759-767},
  doi={10.1109/VR50410.2021.00103}
}


@article{Owens.2012,
author = {Justin W. Owens and Jennifer Teves and Bobby Nguyen and Amanda Smith and Mandy C. Phelps and Barbara S. Chaparro},
title ={Examination of Dual vs. Single Monitor Use during Common Office Tasks},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {56},
number = {1},
pages = {1506-1510},
year = {2012},
doi = {10.1177/1071181312561299},
URL = {https://doi.org/10.1177/1071181312561299},
eprint = {https://doi.org/10.1177/1071181312561299}
}


@inproceedings{Ng.2021,
 author = {Ng, Alexander and Medeiros, Daniel and Mcgill, Mark and Williamson, Julie and Brewster, Stephen},
 title = {The Passenger Experience of Mixed Reality Virtual Display Layouts in Airplane Environments},
 urldate = {19.05.2022},
 pages = {265--274},
 publisher = {IEEE},
 isbn = {978-1-6654-0158-6},
 booktitle = {2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
 year = {2021},
 doi = {10.1109/ISMAR52148.2021.00042}
}


@misc{NaturalPoint.o.D.,
 abstract = {Offering the power of multiple-camera, 6 DoF object tracking for head tracking, VR, and desktop motion capture.},
 year = {o. D.},
 title = {V120:Duo - Tech Specs},
 url = {https://optitrack.com/cameras/v120-duo/specs.html},
 urldate = {28.11.2022}
}


@misc{NaturalPoint.o.D.b,
 abstract = {{\textquotedbl} id=},
 year = {o. D.},
 title = {Rigid Body Tracking},
 url = {https://docs.optitrack.com/v/v2.3/motive/rigid-body-tracking},
 urldate = {28.11.2022}
}


@misc{NaturalPoint.o.D.c,
 abstract = {Motive + OptiTrack cameras deliver the best performing real time human and object tracking available today.},
 year = {o. D.},
 title = {Motive - In Depth: Optical motion capture software.},
 url = {https://optitrack.com/software/motive/},
 urldate = {31.10.2022}
}


@misc{Schluter.2022,
 author = {Schl{\"u}ter, Tobias and Petersen, Klaus},
 year = {2022},
 title = {LPVR-DUO-Varjo Setup, Configuration and Calibration},
 url = {https://lp-research.atlassian.net/wiki/spaces/LKB/pages/1335492638/LPVR-DUO-Varjo+Setup+Configuration+and+Calibration},
 urldate = {01.11.2022}
}


@misc{NaturalPoint.o.D.d,
 abstract = {{\textquotedbl} id=},
 year = {o. D.},
 title = {Markers},
 url = {https://docs.optitrack.com/v/v2.3/motive/markers},
 urldate = {28.11.2022}
}


@inproceedings{Milgram.1995,
 author = {Milgram, Paul and Takemura, Haruo and Utsumi, Akira and Kishino, Fumio},
 title = {Augmented reality: a class of displays on the reality-virtuality continuum},
 urldate = {23.11.2022},
 pages = {282--292},
 publisher = {SPIE},
 series = {SPIE Proceedings},
 editor = {Das, Hari},
 booktitle = {Telemanipulator and Telepresence Technologies},
 year = {1995},
 doi = {10.1117/12.197321}
}


@incollection{Meschtscherjakov.2019,
 author = {Meschtscherjakov, Alexander and Strumegger, Sebastian and Tr{\"o}sterer, Sandra},
 title = {Bubble Margin: Motion Sickness Prevention While Reading on Smartphones in Vehicles},
 urldate = {15.11.2022},
 pages = {660--677},
 volume = {11747},
 publisher = {Springer International Publishing},
 isbn = {978-3-030-29384-0},
 series = {Lecture Notes in Computer Science},
 editor = {Lamas, David and Loizides, Fernando and Nacke, Lennart and Petrie, Helen and Winckler, Marco and Zaphiris, Panayiotis},
 booktitle = {Human-Computer Interaction -- INTERACT 2019},
 year = {2019},
 address = {Cham},
 doi = {10.1007/978-3-030-29384-0_39}
}

@misc{MercedesBenzGroup.2022,
 year = {2022},
 title = {The front runner in automated driving and safety technologies.},
 url = {https://group.mercedes-benz.com/innovation/case/autonomous/drive-pilot-2.html},
 urldate = {06.04.2022},
 note = {Accessed: 12.04.2023}
}

@misc{Melakari.21.08.2019,
 abstract = {In order to work in virtual reality, you need to be able to see clearly. Read how Varjo has achieved 20/20 resolution virtual reality.},
 author = {Melakari, Klaus},
 year = {21.08.2019},
 title = {How Varjo Delivers Human-Eye Resolution VR/XR with Bionic Display{\texttrademark}},
 url = {https://varjo.com/blog/introducing-bionic-display-how-varjo-delivers-human-eye-resolution/},
 urldate = {16.10.2022}
}


@inproceedings{Mcgill.2022,
author = {McGill, Mark and Wilson, Graham and Medeiros, Daniel and Brewster, Stephen Anthony},
title = {PassengXR: A Low Cost Platform for Any-Car, Multi-User, Motion-Based Passenger XR Experiences},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545657},
doi = {10.1145/3526113.3545657},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {15},
keywords = {Toolkit, Passenger, Extended Reality, Mixed Reality, Vehicle, In-Car},
location = {Bend, OR, USA},
series = {UIST '22}
}


@article{Mcgill.2020,
 author = {Mcgill, Mark and Williamson, Julie and Ng, Alexander and Pollick, Frank and Brewster, Stephen},
 year = {2020},
 title = {Challenges in passenger use of mixed reality headsets in cars and other transportation},
 urldate = {23.05.2022},
 pages = {583--603},
 volume = {24},
 number = {4},
 issn = {1359-4338},
 journal = {Virtual Reality},
 doi = {10.1007/s10055-019-00420-x}
}


@inproceedings{Mcgill.2017,
author = {McGill, Mark and Ng, Alexander and Brewster, Stephen},
title = {I Am The Passenger: How Visual Motion Cues Can Influence Sickness For In-Car VR},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3026046},
doi = {10.1145/3025453.3026046},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {5655–5668},
numpages = {14},
keywords = {passenger, virtual reality, automobile, autonomous car, in-motion, hmd, in-car, motion sickness, mixed reality},
location = {Denver, Colorado, USA},
series = {CHI '17}
}


@article{Mcgill.2020b,
 author = {Mcgill, Mark and Kehoe, Aidan and Freeman, Euan and Brewster, Stephen},
 year = {2020},
 title = {Expanding the Bounds of Seated Virtual Workspaces},
 urldate = {08.11.2022},
 pages = {1--40},
 volume = {27},
 number = {3},
 issn = {1073-0516},
 journal = {ACM Transactions on Computer-Human Interaction},
 doi = {10.1145/3380959}
}


@book{Maurer.2016,
 author = {Maurer, Markus and Gerdes, J. Christian and Lenz, Barbara and Winner, Hermann},
 year = {2016},
 title = {Autonomous Driving},
 address = {Berlin, Heidelberg},
 publisher = {Springer Berlin Heidelberg},
 isbn = {978-3-662-48845-4},
 doi = {10.1007/978-3-662-48847-8}
}


@book{Maurer.2015,
 author = {Maurer, Markus and Gerdes, J. Christian and Lenz, Barbara and Winner, Hermann},
 year = {2015},
 title = {Autonomes Fahren},
 address = {Berlin, Heidelberg},
 publisher = {Springer Berlin Heidelberg},
 isbn = {978-3-662-45853-2},
 doi = {10.1007/978-3-662-45854-9}
}


@incollection{Mathis.2021,
 author = {Mathis, Lesley-Ann and Widlroither, Harald and Traub, Nico},
 title={Towards Future Interior Concepts: User Perception and Requirements for the Use Case Working in the Autonomou Car},
booktitle={Advances in Human Aspects of Transportation},
year={2021},
 urldate = {09.11.2022},
 pages = {315--322},
 volume = {270},
 publisher = {Springer International Publishing},
 isbn = {978-3-030-80012-3},
 series = {Lecture Notes in Networks and Systems},
 editor = {Stanton, Neville},
 address = {Cham},
 doi = {10.1007/978-3-030-80012-3_37}
}


@article{Muhlbacher.2020,
AUTHOR = {Mühlbacher, Dominik and Tomzig, Markus and Reinmüller, Katharina and Rittger, Lena},
TITLE = {Methodological Considerations Concerning Motion Sickness Investigations during Automated Driving},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {265},
URL = {https://www.mdpi.com/2078-2489/11/5/265},
ISSN = {2078-2489},
ABSTRACT = {Automated driving vehicles will allow all occupants to spend their time with various non-driving related tasks like relaxing, working, or reading during the journey. However, a significant percentage of people is susceptible to motion sickness, which limits the comfort of engaging in those tasks during automated driving. Therefore, it is necessary to investigate the phenomenon of motion sickness during automated driving and to develop countermeasures. As most existing studies concerning motion sickness are fundamental research studies, a methodology for driving studies is yet missing. This paper discusses methodological aspects for investigating motion sickness in the context of driving including measurement tools, test environments, sample, and ethical restrictions. Additionally, methodological considerations guided by different underlying research questions and hypotheses are provided. Selected results from own studies concerning motion sickness during automated driving which were conducted in a motion-based driving simulation and a real vehicle are used to support the discussion.},
DOI = {10.3390/info11050265}
}


@inproceedings{Smyth.2018,
  author={Smyth, Joseph and Jennings, Paul and Mouzakitis, Alex and Birrell, Stewart},
  booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Too Sick to Drive: How Motion Sickness Severity Impacts Human Performance}, 
  year={2018},
  volume={},
  number={},
  pages={1787-1793},
  doi={10.1109/ITSC.2018.8569572}
}


@proceedings{Spencer.2018,
 year = {2018},
 title = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450360869},
 editor = {Spencer, Stephen N. and Morishima, Shigeo and Itoh, Yuichi and Shiratori, Takaaki and Yue, Yonghao and Lindeman, Rob},
 doi = {10.1145/3281505}
}


@book{Stanton.2021,
 year = {2021},
 title = {Advances in Human Aspects of Transportation},
 address = {Cham},
 publisher = {Springer International Publishing},
 isbn = {978-3-030-80011-6},
 series = {Lecture Notes in Networks and Systems},
 editor = {Stanton, Neville},
 doi = {10.1007/978-3-030-80012-3}
}


@misc{VarjoTechnologies.31.10.2022,
 abstract = {Developing a tracking plugin for Varjo Tracking Plugin API {\&}nbsp;{\&}nbsp;{\&}nbsp;{\&}nbsp; Tracking Plugin API for user configuration System definitions {\&}nbsp;{\&}nbsp;{\&}nbsp;{\&}nbsp; Coordinate system {\&}nbsp;{\&}nbsp;{\&}nbsp;{\&}nbsp; HMD tracking point Important information {\&}nbsp;{\&}nbsp;{\&}nbsp;{\&}nbsp; Critical HMD components Using a Tracking Plugin {\&}nbsp;{\&}nbsp;{\&}nbsp;{\&}nbsp; Tracking plugin discovery {\&}nbsp;{\&}nbsp;{\&}nbsp;{\&}nbsp; Tracking plugin installation {\&}nbsp;{\&}nbsp;{\&}nbsp;{\&}nbsp; Tracking plugin registration {\&}nbsp;{\&}nbsp;{\&}nbsp;{\&}nbsp; Tracking plugin selection UI},
 year = {31.10.2022},
 title = {Third-party tracking},
 url = {https://developer.varjo.com/docs/get-started/third-party-tracking},
 urldate = {16.10.2022}
}


@misc{VarjoTechnologies.31.10.2022b,
 abstract = {The Varjo XR-3, VR-3, and VR-2 Pro headsets feature integrated Ultraleap's hand tracking. Hand tracking lets you reach into the virtual world with your hands and without using a physical controller. Gestures such as pinching, grabbing, and interacting with objects allow for a new level of immersion in your applications.},
 year = {31.10.2022},
 title = {Hand Tracking},
 url = {https://developer.varjo.com/docs/get-started/hand-tracking},
 urldate = {16.10.2022}
}



@misc{VarjoTechnologies.31.10.2022c,
 abstract = {Varjo headsets feature the 20/20 Eye Tracker, our integrated eye tracking functionality. You can use eye tracking in your application and log gaze information for analytics. Eye tracking can also be used to interact with content; you can use it to select an object or prompt for additional information simply by looking at it.},
 year = {31.10.2022},
 title = {Eye tracking},
 url = {https://developer.varjo.com/docs/get-started/eye-tracking-with-varjo-headset},
 urldate = {16.10.2022}
}


@misc{VarjoTechnologies.31.10.2022d,
 abstract = {Defining camera render position},
 year = {2022},
 title = {Camera render position},
 url = {https://developer.varjo.com/docs/get-started/camera-render-position},
 urldate = {06.04.2023},
 note = {Accessed: 06.04.2023}
}


@misc{VarjoTechnologies.16.10.2022,
 author = {Varjo Technologies},
abstract = {True-to-life professional mixed reality headset with the highest resolution across widest field of view. Built-in LiDAR, eye {\&} hand tracking.},
 year = {2022},
 title = {Varjo XR-3 - The industry's highest resolution XR headset},
 url = {https://varjo.com/products/xr-3/},
 urldate = {12.10.2022},
 note = {Accessed: 12.10.2022}
}


@misc{VarjoTechnologies.27.09.2022,
 year = {27.09.2022},
 title = {System requirements for XR-3 and VR-3},
 url = {https://varjo.com/use-center/get-started/varjo-headsets/system-requirements/xr-3-vr-3/},
 urldate = {12.10.2022}
}


@misc{VarjoTechnologies.11.05.2022,
 year = {11.05.2022},
 title = {Setting up tracking},
 url = {https://varjo.com/use-center/get-started/varjo-headsets/setting-up-tracking/},
 urldate = {16.10.2022}
}


@misc{VarjoTechnologies.04.03.2021,
 abstract = {Varjo XR-3 and VR-3 headsets are now shipping worldwide, making photorealistic immersive applications accessible for all professionals.},
 year = {04.03.2021},
 title = {Varjo's Next Generation XR-3 and VR-3 Headsets Start Shipping Worldwide},
 url = {https://varjo.com/press-release/varjos-next-generation-xr-3-and-vr-3-headsets-now-shipping-worldwide/},
 urldate = {12.10.2022}
}


@misc{VarjoTechnologies.01.12.2020,
 abstract = {Varjo launches XR-3 {\&} VR-3 headsets with human-eye resolution (70 ppd), 115° FOV, integrated eye {\&} hand tracking, and depth awareness.},
 year = {01.12.2020},
 title = {Varjo Launches XR-3 and VR-3 Headsets to Bring the Next Generation of Immersive Technology to Every Workplace},
 url = {https://varjo.com/press-release/varjo-launches-xr-3-and-vr-3-headsets-to-bring-the-next-generation-of-immersive-technology-to-every-workplace/},
 urldate = {12.10.2022}
}


@misc{VarjoTechnologies.o.D.,
 year = {o. D.},
 title = {Using Varjo Base},
 url = {https://varjo.com/use-center/get-to-know-your-headset/using-varjo-base/},
 urldate = {16.11.2022}
}


@misc{VarjoTechnologies.o.D.b,
 abstract = {Unity version compatibility},
 year = {o. D.},
 title = {Unity XR SDK Compatibility},
 url = {https://developer.varjo.com/docs/unity-xr-sdk/compatibility},
 urldate = {01.11.2022}
}


@misc{VarjoTechnologies.o.D.c,
 abstract = {Developing mixed reality applications for the Varjo XR-3 and XR-1 Developer Edition headsets is quick and easy with the Varjo XR plugin. Follow these instructions to get started.},
 year = {o. D.},
 title = {Mixed Reality with Varjo XR plugin},
 url = {https://developer.varjo.com/docs/unity-xr-sdk/mixed-reality-with-varjo-xr-plugin},
 urldate = {01.11.2022}
}


@misc{VarjoTechnologies.o.D.d,
 abstract = {Bionic Display{\texttrademark}},
 year = {o. D.},
 title = {Human eye resolution},
 url = {https://developer.varjo.com/docs/get-started/human-eye-resolution},
 urldate = {16.11.2022}
}


@misc{VarjoTechnologies.o.D.e,
 year = {o. D.},
 title = {Getting the perfect image quality},
 url = {https://varjo.com/use-center/get-to-know-your-headset/getting-the-perfect-image-quality/},
 urldate = {16.11.2022}
}


@misc{VarjoTechnologies.o.D.f,
 abstract = {Adding Varjo XR support to a Unity project},
 year = {o. D.},
 title = {Getting Started with Varjo XR Plugin for Unity},
 url = {https://developer.varjo.com/docs/unity-xr-sdk/getting-started-with-varjo-xr-plugin-for-unity},
 urldate = {01.11.2022}
}


@misc{VarjoTechnologies.o.D.g,
 abstract = {By default, Varjo software adjusts cameras for optimal brightness and color tone. However, you can override those values. Through the API, you can change exposure, ISO/gain, white balance, flickering compensation and sharpness.},
 year = {o. D.},
 title = {Camera settings},
 url = {https://developer.varjo.com/docs/unity-xr-sdk/camera-settings-in-varjo-xr-plugin},
 urldate = {01.11.2022}
}


@misc{UniversitatZurich.2022,
 abstract = {{\&}nbsp;},
 year = {2022},
 title = {t-Test f{\"u}r unabh{\"a}ngige Stichproben},
 url = {https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/unterschiede/zentral/ttestunabh.html#1.2._Voraussetzungen_des_t-Tests_f%C3%BCr_unabh%C3%A4ngige_Stichproben},
 urldate = {28.11.2022}
}


@misc{UniversitatZurich.2022b,
 abstract = {{\&}nbsp;},
 year = {2022},
 title = {Rangkorrelation nach Spearman},
 url = {https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/zusammenhaenge/rangkorrelation.html},
 urldate = {28.11.2022}
}


@misc{UniversitatZurich.2022c,
 abstract = {{\&}nbsp;},
 year = {2022},
 title = {Mann-Whitney-U-Test},
 url = {https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/unterschiede/zentral/mann.html},
 urldate = {28.11.2022}
}


@misc{UnityTechnologies.2022,
 year = {2022},
 title = {Unity - Manual: XR Plug-in Framework},
 url = {https://docs.unity3d.com/2020.3/Documentation/Manual/XRPluginArchitecture.html},
 urldate = {28.11.2022}
}


@misc{UnityTechnologies.2022b,
 year = {2022},
 title = {Unity - Manual: Prefabs},
 urldate = {28.11.2022}
}


@misc{UnityTechnologies.2022c,
 year = {2022},
 title = {Unity - Manual: GameObjects},
 url = {https://docs.unity3d.com/2020.3/Documentation/Manual/GameObjects.html},
 urldate = {28.11.2022}
}


@misc{UnityTechnologies.o.D.,
 abstract = {Mit der Echtzeit-3D-Entwicklungsplattform von Unity k{\"o}nnen Grafiker, Designer und Entwickler zusammenarbeiten, um immersive {\&} interaktive Spiele zu erschaffen. Probieren Sie Unity noch heute aus!},
 year = {o. D.},
 title = {Unity-Plattform},
 url = {https://unity.com/de/products/unity-platform},
 urldate = {01.11.2022}
}


@misc{UnityTechnologies.o.D.b,
 abstract = {Finden Sie heraus, wie der Unity Editor und Tools professionellen Programmierern helfen, qualitativ hochwertige und leistungsf{\"a}hige Spiele in C{\#} erstellen.},
 year = {o. D.},
 title = {Multiplattform},
 url = {https://unity.com/de/solutions/multiplatform},
 urldate = {01.11.2022}
}


@article{Talsma.2023,
title = {Validation of a moving base driving simulator for motion sickness research},
journal = {Applied Ergonomics},
volume = {106},
pages = {103897},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103897},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022002204},
author = {Tessa M.W. Talsma and Omar Hassanain and Riender Happee and Ksander N. {de Winkel}},
keywords = {Motion, Sickness, Driving, Simulator, Comfort, Validation}
}


@proceedings{Mark.2017,
 year = {2017},
 title = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450346559},
 editor = {Mark, Gloria and Fussell, Susan and Lampe, Cliff and schraefel, m.c. and Hourcade, Juan Pablo and Appert, Caroline and Wigdor, Daniel},
 doi = {10.1145/3025453}
}


@article{Woo.2016,
author = {E. H. C. Woo and P. White and C. W. K. Lai},
title = {Ergonomics standards and guidelines for computer workstation design and the impact on users’ health – a review},
journal = {Ergonomics},
volume = {59},
number = {3},
pages = {464-475},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/00140139.2015.1076528},
note ={PMID: 26224145},
URL = {https://doi.org/10.1080/00140139.2015.1076528},
eprint = {https://doi.org/10.1080/00140139.2015.1076528}
}


@proceedings{Mandryk.2018,
 year = {2018},
 title = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450356206},
 editor = {Mandryk, Regan and Hancock, Mark and Perry, Mark and Cox, Anna},
 doi = {10.1145/3173574}
}


@inproceedings{Li.2021,
author = {Li, Jingyi and Reda, Agnes and Butz, Andreas},
title = {Queasy Rider: How Head Movements Influence Motion Sickness in Passenger Use of Head-Mounted Displays},
year = {2021},
isbn = {9781450380638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409118.3475137},
doi = {10.1145/3409118.3475137},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {28–38},
numpages = {11},
keywords = {rear-seat passenger, motion sickness, head movement, head-mounted display, engagement},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21}
}


@article{Diels.2016,
 abstract = {This paper discusses the predicted increase in the occurrence and severity of motion sickness in self-driving cars. Self-driving cars have the potential to lead to significant benefits. From the driver's perspective, the direct benefits of this technology are considered increased comfort and productivity. However, we here show that the envisaged scenarios all lead to an increased risk of motion sickness. As such, the benefits this technology is assumed to bring may not be capitalised on, in particular by those already susceptible to motion sickness. This can negatively affect user acceptance and uptake and, in turn, limit the potential socioeconomic benefits that this emerging technology may provide. Following a discussion on the causes of motion sickness in the context of self-driving cars, we present guidelines to steer the design and development of automated vehicle technologies. The aim is to limit or avoid the impact of motion sickness and ultimately promote the uptake of self-driving cars. Attention is also given to less well known consequences of motion sickness, in particular negative aftereffects such as postural instability, and detrimental effects on task performance and how this may impact the use and design of self-driving cars. We conclude that basic perceptual mechanisms need to be considered in the design process whereby self-driving cars cannot simply be thought of as living rooms, offices, or entertainment venues on wheels.},
 author = {Diels, Cyriel and Bos, Jelte E.},
 title = {Self-driving carsickness},
journal = {Applied Ergonomics},
volume = {53},
pages = {374-382},
year = {2016},
note = {Transport in the 21st Century: The Application of Human Factors to Future User Needs},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2015.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0003687015300818},
keywords = {Vehicle automation, Design, Displays, Motion sickness, Carsickness, Sensory conflict, Anticipation}
}


@proceedings{Das.1995,
 year = {1995},
 title = {Telemanipulator and Telepresence Technologies},
 publisher = {SPIE},
 series = {SPIE Proceedings},
 editor = {Das, Hari}
}


@inproceedings{Dam.2021,
author = {Dam, Abhraneil and Jeon, Myounghoon},
title = {A Review of Motion Sickness in Automated Vehicles},
year = {2021},
isbn = {9781450380638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409118.3475146},
doi = {10.1145/3409118.3475146},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {39–48},
numpages = {10},
keywords = {measures of motion sickness, review, induction methods, motion sickness, vehicle automation, mitigation methods, sensory conflict, anticipation},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21}
}


@book{Cohen.1988,
 author = {Cohen, Jacob},
 year = {1988},
 title = {Statistical power analysis for the behavioral sciences},
 address = {Hillsdale, NJ},
 edition = {2. ed.},
 publisher = {Erlbaum},
 isbn = {0-8058-0283-5}
}


@article{Cho.2022,
 author={Cho, Hyung-Jun and Kim, Gerard J.},
  journal={IEEE Access}, 
  title={RideVR: Reducing Sickness for In-Car Virtual Reality by Mixed-in Presentation of Motion Flow Information}, 
  year={2022},
  volume={10},
  number={},
  pages={34003-34011},
  doi={10.1109/ACCESS.2022.3162221}
}


@article{Bos.2005,
 abstract = {INTRODUCTION

Vehicle motion characteristics differ between air, road, and sea environments, both vestibularly and visually. Effects of vision on motion sickness have been studied before, though less systematically in a naval setting. It is hypothesized that appropriate visual information on self-motion is beneficial in a naval setting and that task performance is likely reduced as sickness increases.

METHODS

Using a within-subjects design, 24 subjects were exposed to 30 min of motion in a ship's bridge motion simulator with 3 visual conditions: an Earth-fixed outside view; an inside view that moved with the subjects; and a blindfolded condition. Subjective sickness symptoms and severity were rated repeatedly before, during, and after motion exposure. During the motion, subjects performed a mental task.

RESULTS

Though not excessive, sickness was highest in the inside viewing condition, intermediate in the outside viewing condition, and least in the blindfolded condition. The blindfolded condition was equally as bad as the inside viewing condition during the first 5-10 min of motion exposure. The overall temporal increase of sickness during motion was about equal to the decrease during recovery. No effect of sickness on task performance was observed.

DISCUSSION

Most sickness in a naval setting is observed when the visual environment moves with the subjects, as has been reported in other environments, such as cars. Only mild sickness, caused by moderate motions, was provoked in this study and was alleviated by the performance task. A non-linear brain mechanism integrating visual and vestibular information may explain why the least sickness was observed when subjects were blindfolded.},
 author = {Bos, Jelte E. and MacKinnon, Scott N. and Patterson, Anthony},
 year = {2005},
 month = {dec},
language = {English},
 title = {Motion sickness symptoms in a ship motion simulator: effects of inside, outside, and no view},
 urldate = {27.05.2022},
 pages = {1111--1118},
 volume = {76},
 number = {12},
 issn = {0095-6562},
 journal = {Aviation, Space, and Environmental Medicine},
 publisher = {Aerospace Medical Association}
}


@inproceedings{Borojeni.2016,
 author = {Borojeni, Shadan Sadeghian and Chuang, Lewis and Heuten, Wilko and Boll, Susanne},
 title = {Assisting Drivers with Ambient Take-Over Requests in Highly Automated Driving},
 urldate = {05.11.2022},
 pages = {237--244},
 publisher = {ACM},
 isbn = {9781450345330},
 editor = {Green, Paul and Boll, Susanne and Gabbard, Joe and Osswald, Sebastian and Burnett, Gary and Borojeni, Shadan Sadeghian and L{\"o}cken, Andreas and Pradhan, Anuj},
 booktitle = {Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/3003715.3005409}
},

@inproceedings{b.adhanom2020GazeMetricsOpenSourceTool,
  title = {{{GazeMetrics}}: {{An Open-Source Tool}} for {{Measuring}} the {{Data Quality}} of {{HMD-based Eye Trackers}}},
  shorttitle = {{{GazeMetrics}}},
  booktitle = {{{ACM Symposium}} on {{Eye Tracking Research}} and {{Applications}}},
  author = {B. Adhanom, Isayas and Lee, Samantha C. and Folmer, Eelke and MacNeilage, Paul},
  year = {2020},
  month = jun,
  pages = {1--5},
  publisher = {ACM},
  address = {Stuttgart Germany},
  doi = {10.1145/3379156.3391374},
  urldate = {2024-02-19},
  isbn = {978-1-4503-7134-6},
  langid = {english},
  keywords = {Accuracy,EyeCalibrationTool,EyeTracking,OpenSource,Precision},
  file = {C:\Users\msasalo\Zotero\storage\BHS34A3S\B. Adhanom et al. - 2020 - GazeMetrics An Open-Source Tool for Measuring the.pdf}
}

@inproceedings{hertel2021TaxonomyInteractionTechniques,
  title = {A {{Taxonomy}} of {{Interaction Techniques}} for {{Immersive Augmented Reality}} Based on an {{Iterative Literature Review}}},
  booktitle = {2021 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Hertel, Julia and Karaosmanoglu, Sukran and Schmidt, Susanne and Braker, Julia and Semmann, Martin and Steinicke, Frank},
  year = {2021},
  month = oct,
  pages = {431--440},
  publisher = {IEEE},
  address = {Bari, Italy},
  doi = {10.1109/ISMAR52148.2021.00060},
  urldate = {2024-12-05},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-66540-158-6},
  file = {C:\Users\msasalo\Zotero\storage\AU8XXZZV\Hertel et al. - 2021 - A Taxonomy of Interaction Techniques for Immersive.pdf}
}


@misc{Ultraleap.2024,
author = {Ultraleap},
 year = {2024},
 title = {Camera Placement},
 url = {https://docs.ultraleap.com/TouchFree/touchfree-user-manual/camera-placement.html},
 urldate = {05.12.2024},
 note = {Accessed: 05.12.2024}
}

@misc{BMWMGmbH.2022,
author = {BMW M GmbH},
 year = {2022},
 title = {M MIXED REALITY. Drive the change -- change the drive.},
 url = {https://www.bmw-m.com/en/topics/magazine-article-pool/m-mixed-reality.html},
 urldate = {29.03.2023},
 note = {Accessed: 09.11.2023}
}

@inproceedings{hincapie-ramos2014ConsumedEnduranceMetric,
  title = {Consumed Endurance: A Metric to Quantify Arm Fatigue of Mid-Air Interactions},
  shorttitle = {Consumed Endurance},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {{Hincapi{\'e}-Ramos}, Juan David and Guo, Xiang and Moghadasian, Paymahn and Irani, Pourang},
  year = {2014},
  month = apr,
  pages = {1063--1072},
  publisher = {ACM},
  address = {Toronto Ontario Canada},
  doi = {10.1145/2556288.2557130},
  urldate = {2024-09-11},
  isbn = {978-1-4503-2473-1},
  langid = {english}
}

@misc{colley2024rcode, 
    author = {Mark Colley}, 
    title = {rCode: Enhanced R Functions for Statistical Analysis and Reporting}, 
    year = {2024}, 
    howpublished = {\url{https://github.com/M-Colley/rCode}}, 
    note = {A collection of custom R functions for streamlining statistical analysis, visualizations, and APA-compliant reporting.} }

@inproceedings{wobbrock2011AlignedRankTransform,
  title = {The Aligned Rank Transform for Nonparametric Factorial Analyses Using Only Anova Procedures},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wobbrock, Jacob O. and Findlater, Leah and Gergle, Darren and Higgins, James J.},
  year = {2011},
  month = may,
  pages = {143--146},
  publisher = {ACM},
  address = {Vancouver BC Canada},
  doi = {10.1145/1978942.1978963},
  urldate = {2024-09-11},
  isbn = {978-1-4503-0228-9},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\XT7KBF5C\Wobbrock et al. - 2011 - The aligned rank transform for nonparametric facto.pdf}
}


@misc{Microsoft.2021,
author = {Microsoft},
 year = {2021},
 title = {Comfort},
 url = {https://learn.microsoft.com/en-us/windows/mixed-reality/design/comfort},
 urldate = {19.10.2021},
 note = {Accessed: 22.06.2024}
}


@article{Bertolini.2016,
 abstract = {Motion sickness is a common disturbance occurring in healthy people as a physiological response to exposure to motion stimuli that are unexpected on the basis of previous experience. The motion can be either real, and therefore perceived by the vestibular system, or illusory, as in the case of visual illusion. A multitude of studies has been performed in the last decades, substantiating different nauseogenic stimuli, studying their specific characteristics, proposing unifying theories, and testing possible countermeasures. Several reviews focused on one of these aspects; however, the link between specific nauseogenic stimuli and the unifying theories and models is often not clearly detailed. Readers unfamiliar with the topic, but studying a condition that may involve motion sickness, can therefore have difficulties to understand why a specific stimulus will induce motion sickness. So far, this general audience struggles to take advantage of the solid basis provided by existing theories and models. This review focuses on vestibular-only motion sickness, listing the relevant motion stimuli, clarifying the sensory signals involved, and framing them in the context of the current theories.},
 author = {Bertolini, Giovanni and Straumann, Dominik},
 year = {2016},
 title = {Moving in a Moving World: A Review on Vestibular Motion Sickness},
 urldate = {10.11.2022},
 pages = {14},
 volume = {7},
 URL={https://www.frontiersin.org/articles/10.3389/fneur.2016.00014},
 issn = {1664-2295},
 journal = {Frontiers in Neurology},
 doi = {10.3389/fneur.2016.00014}
}

@article{bangor2009sus,
  author     = {Bangor, Aaron and Kortum, Philip and Miller, James},
  title      = {Determining What Individual SUS Scores Mean: Adding an Adjective Rating Scale},
  year       = {2009},
  issue_date = {May 2009},
  publisher  = {Usability Professionals' Association},
  address    = {Bloomingdale, IL},
  volume     = {4},
  number     = {3},
  abstract   = {The System Usability Scale (SUS) is an inexpensive, yet effective tool for assessing the usability of a product, including Web sites, cell phones, interactive voice response systems, TV applications, and more. It provides an easy-to-understand score from 0 (negative) to 100 (positive). While a 100-point scale is intuitive in many respects and allows for relative judgments, information describing how the numeric score translates into an absolute judgment of usability is not known. To help answer that question, a seven-point adjective-anchored Likert scale was added as an eleventh question to nearly 1,000 SUS surveys. Results show that the Likert scale scores correlate extremely well with the SUS scores (r=0.822). The addition of the adjective rating scale to the SUS may help practitioners interpret individual SUS scores and aid in explaining the results to non-human factors professionals.},
  journal    = {J. Usability Studies},
  month      = {may},
  pages      = {114–123},
  numpages   = {10},
  keywords   = {system usability scale, SUS, usability, surveys, user satisfaction}
}

@article{Bell.2022,
 author = {Bell, Stephen W. and Kong, Joseph C. H. and Clark, David A. and Carne, Peter and Skinner, Stewart and Pillinger, Stephen and Burton, Paul and Brown, Wendy},
 year = {2022},
 title = {The National Aeronautics and Space Administration-task load index: NASA-TLX: evaluation of its use in surgery},
 urldate = {19.11.2022},
 pages = {3022--3028},
 volume = {92},
 number = {11},
 journal = {ANZ journal of surgery},
 doi = {10.1111/ans.17830}
}


@proceedings{Barbosa.2022,
 year = {2022},
 title = {CHI Conference on Human Factors in Computing Systems},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450391573},
 editor = {Barbosa, Simone and Lampe, Cliff and Appert, Caroline and Shamma, David A. and Drucker, Steven and Williamson, Julie and Yatani, Koji},
 doi = {10.1145/3491102}
}


@proceedings{Dunne.2014,
 year = {2014},
 title = {Proceedings of the 2014 ACM International Symposium on Wearable Computers},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450329699},
 editor = {Dunne, Lucy and Martin, Tom and Beigl, Michael},
 doi = {10.1145/2634317}
}


@misc{Bach.2022,
 author = {Bach, Deborah},
 year = {2022},
 title = {With their HoloLens 2 project, Microsoft and Volkswagen collaborate to put augmented reality glasses in motion},
 url = {https://news.microsoft.com/source/features/digital-transformation/with-their-hololens-2-project-microsoft-and-volkswagen-collaborate-to-put-augmented-reality-glasses-in-motion/},
 urldate = {29.03.2023},
 note = {Accessed: 09.11.2023}
}


@misc{AUDIAG.2022,
 author = {AUDI AG},
year = {2022},
 title = {holoride: Virtual Reality meets the real world},
 url = {https://www.audi.com/en/innovation/development/holoride-virtual-reality-meets-the-real-world.html},
 urldate = {29.03.2023},
 note = {Accessed: 09.11.2023}
}

@misc{AppleInc.2023,
 author = {Apple Inc.},
year = {2023},
 title = {Introducing Apple Vision Pro: Apple’s first spatial computer},
 url = {https://www.apple.com/newsroom/2023/06/introducing-apple-vision-pro/},
 urldate = {05.06.2023},
 note = {Accessed: 09.11.2023}
}

@misc{Meta.2023,
 author = {Meta},
year = {2023},
 title = {Meet Meta Quest 3, Our Mixed Reality Headset Starting at \$499.99},
 url = {https://about.fb.com/news/2023/09/meet-meta-quest-3-mixed-reality-headset/},
 urldate = {27.09.2023},
 note = {Accessed: 09.11.2023}
}

@article{brookeSUSQuickDirty1995,
  title = {{{SUS}}: {{A}} Quick and Dirty Usability Scale},
  shorttitle = {{{SUS}}},
  author = {Brooke, John},
  year = {1995},
  month = nov,
  journal = {Usability Eval. Ind.},
  volume = {189},
  abstract = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for "quick and dirty" methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.}
}

@incollection{korber2019TheoreticalConsiderationsDevelopment,
  author="K{\"o}rber, Moritz",
editor="Bagnara, Sebastiano
and Tartaglia, Riccardo
and Albolino, Sara
and Alexander, Thomas
and Fujita, Yushi",
title="Theoretical Considerations and Development of a Questionnaire to Measure Trust in Automation",
booktitle="Proceedings of the 20th Congress of the International Ergonomics Association (IEA 2018)",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="13--30",
abstract="The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation because it predicts not only whether but also how an operator interacts with an automation. In this work, a theoretical model of trust in automation is established and the development and evaluation of a corresponding questionnaire (Trust in Automation, TiA) are described.",
isbn="978-3-319-96074-6"
}

@inproceedings{faas2020LongitudinalVideoStudy,
  title = {A {{Longitudinal Video Study}} on {{Communicating Status}} and {{Intent}} for {{Self-Driving Vehicle}}  {{Pedestrian Interaction}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Faas, Stefanie M. and Kao, Andrea C. and Baumann, Martin},
  year = {2020},
  date = {2020-04-21},
  pages = {1--14},
publisher={ACM},
address={New York, NY, USA},
  location = {{Honolulu HI USA}},
  doi = {10.1145/3313831.3376484},
  url = {https://dl.acm.org/doi/10.1145/3313831.3376484},
  urldate = {2023-11-09},
  isbn = {978-1-4503-6708-0},
  langid = {english}
}

@proceedings{AssociationforComputingMachinery.2021,
 year = {2021},
 title = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450380638},
 editor = {Association for Computing Machinery},
 doi = {10.1145/3409118}
}


@proceedings{Agrawala.2022,
 year = {2022},
 title = {The 35th Annual ACM Symposium on User Interface Software and Technology},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450393201},
 editor = {Agrawala, Maneesh and Wobbrock, Jacob O. and Adar, Eytan and Setlur, Vidya},
 doi = {10.1145/3526113}
}


@proceedings{.2021,
 year = {2021},
 title = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
 publisher = {IEEE},
 isbn = {978-1-6654-1838-6}
}


@proceedings{.2021b,
 year = {2021},
 title = {2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
 publisher = {IEEE},
 isbn = {978-1-6654-0158-6}
}


@proceedings{.2020,
 year = {2020},
 title = {12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450380669},
 doi = {10.1145/3409251}
}


@proceedings{.2019,
 year = {2019},
 title = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
 publisher = {IEEE},
 isbn = {978-1-7281-4765-9},
 doi = {10.1109/ISMAR-Adjunct48749.2019}
}


@proceedings{.2018,
 year = {2018},
 title = {SAE Technical Paper Series},
 publisher = {SAE International400 Commonwealth Drive, Warrendale, PA, United States},
 series = {SAE Technical Paper Series}
}


@proceedings{.2018b,
 year = {2018},
 title = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
 publisher = {IEEE},
 isbn = {978-1-5386-3365-6}
}


@proceedings{.2018c,
 year = {2018},
 title = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
 publisher = {IEEE},
 isbn = {978-1-7281-0321-1}
}


@proceedings{.2017,
 year = {2017},
 title = {2017 IEEE Symposium on 3D User Interfaces (3DUI)},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9}
}


@article{Azuma.1997,
 author = {Azuma, Ronald T.},
 year = {1997},
 title = {A Survey of Augmented Reality},
 urldate = {23.11.2022},
 pages = {355--385},
 volume = {6},
 number = {4},
 issn = {1054-7460},
 journal = {Presence: Teleoperators and Virtual Environments},
 doi = {10.1162/pres.1997.6.4.355}
}


@inproceedings{Fernandes.2016,
  author={Fernandes, Ajoy S and Feiner, Steven K.},
  booktitle={2016 IEEE Symposium on 3D User Interfaces (3DUI)}, 
  title={Combating VR sickness through subtle dynamic field-of-view modification}, 
  year={2016},
  volume={},
  number={},
  pages={201-210},
  doi={10.1109/3DUI.2016.7460053}
}


@article{Fos.2000,
 abstract = {The Paced Auditory Serial Addition Test (PASAT; Gronwall, 1977; Gronwall {\&} Sampson, 1974) is a measure of attention and information processing speed sensitive to mild traumatic brain injury (MTBI), but it is aversive and inappropriate for many other neurologically impaired patients. This study examines a simpler, less aversive visual analog of the PASAT (the Paced Visual Serial Addition Test; PVSAT) in a sample of 74 college students (26 with a history of TBI). Results indicated that the PVSAT is moderately correlated with and less difficult than the PASAT. Both tests had identical relations to other measures of attention. Neither the PVSAT, PASAT, nor the other attentional measures differentiated participants with MTBI from normal controls in a college population. This preliminary study thus demonstrates the comparability of the two tests and presents the PVSAT as a viable alternative to the PASAT. Directions for future research and applications of these findings are discussed.},
author = { Lori A.   Fos  and  Kevin W.   Greve  and  Marne B.   South  and  Charles   Mathias  and  Hope   Benefield },
title = {Paced Visual Serial Addition Test: An Alternative Measure of Information Processing Speed},
journal = {Applied Neuropsychology},
volume = {7},
number = {3},
pages = {140-146},
year  = {2000},
publisher = {Routledge},
 doi = {10.1207/S15324826AN0703_4}
}


@inproceedings{Freiwald.2018,
author = {Freiwald, Jann Philipp and Katzakis, Nicholas and Steinicke, Frank},
title = {Camera Time Warp: Compensating Latency in Video See-through Head-Mounted-Displays for Reduced Cybersickness Effects},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281521},
doi = {10.1145/3281505.3281521},
abstract = {We introduce Camera Time Warp (CamWarp), a novel reprojection technique for video-see-through augmented reality, which reduces the registration error between captured real-world videos and rendered virtual images. Instead of rendering the image plane locked to the virtual camera, CamWarp renders the image plane at the real-world position it was captured at, and compensates for potential artifacts. We conducted two experiments to evaluate the effectiveness of CamWarp. In the first experiment participants were asked to report subjective discomfort while moving their head in a pattern inspired by the ISO 9241-9 Fitts' Law task at different speeds while the video feed was rendered at varying frame rates. The results show that the technique can significantly reduce subjective levels of discomfort and cybersickness symptoms for all tested configurations. In the second experiment participants were asked to move physical objects on a projected path as quickly and precisely as possible. Results show a positive effect of CamWarp on speed and accuracy.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {9},
numpages = {7},
keywords = {cybersickness, latency compensation, augmented reality},
location = {Tokyo, Japan},
series = {VRST '18}
}


@article{Li.2021b,
 author = {Li, Jingyi and George, Ceenu and Ngao, Andrea and Holl{\"a}nder, Kai and Mayer, Stefan and Butz, Andreas},
 year = {2021},
 title = {Rear-Seat Productivity in Virtual Reality: Investigating VR Interaction in the Confined Space of a Car},
 urldate = {08.11.2022},
 volume = {5},
 number = {4},
 journal = {Multimodal Technologies and Interaction},
 doi = {10.3390/mti5040015}
}


@inproceedings{Li.2020,
author = {Li, Jingyi and George, Ceenu and Ngao, Andrea and Holl\"{a}nder, Kai and Mayer, Stefan and Butz, Andreas},
title = {An Exploration of Users’ Thoughts on Rear-Seat Productivity in Virtual Reality},
year = {2020},
isbn = {9781450380669},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409251.3411732},
doi = {10.1145/3409251.3411732},
pages = {92–95},
numpages = {4},
booktitle = {12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
keywords = {Virtual Reality, HMD, Rear-Seat Productivity, Commute.},
location = {Virtual Event, DC, USA},
series = {AutomotiveUI '20}
}


@article{LaViola.2000,
author = {LaViola, Joseph J.},
title = {A Discussion of Cybersickness in Virtual Environments},
year = {2000},
issue_date = {Jan. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {0736-6906},
url = {https://doi.org/10.1145/333329.333344},
doi = {10.1145/333329.333344},
journal = {SIGCHI Bull.},
month = {jan},
pages = {47–56},
numpages = {10}
}


@book{Lamas.2019,
 year = {2019},
 title = {Human-Computer Interaction -- INTERACT 2019},
 address = {Cham},
 publisher = {Springer International Publishing},
 isbn = {978-3-030-29383-3},
 series = {Lecture Notes in Computer Science},
 editor = {Lamas, David and Loizides, Fernando and Nacke, Lennart and Petrie, Helen and Winckler, Marco and Zaphiris, Panayiotis},
 doi = {10.1007/978-3-030-29384-0}
}


@article{Kuiper.2018,
title = {Looking forward: In-vehicle auxiliary display positioning affects carsickness},
journal = {Applied Ergonomics},
volume = {68},
pages = {169-175},
year = {2018},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S000368701730251X},
author = {Ouren X. Kuiper and Jelte E. Bos and Cyriel Diels},
keywords = {Motion sickness, Displays, Autonomous vehicles}
}


@inproceedings{Kodama.2017,
 author = {Kodama, Ryo and Koge, Masahiro and Taguchi, Shun and Kajimoto, Hiroyuki},
 title = {COMS-VR: Mobile virtual reality entertainment system using electric car and head-mounted display},
 urldate = {04.11.2022},
 pages = {130--133},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 booktitle = {2017 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2017},
 doi = {10.1109/3DUI.2017.7893329}
}


@article{Koch.2018,
 abstract = {BACKGROUND

Seasickness and travel sickness are classic types of motion illness. Modern simulation systems and virtual reality representations can also induce comparable symptoms. Such manifestations can be alleviated or prevented by various measures.

METHODS

This review is based on pertinent publications retrieved by a PubMed search, with special attention to clinical trials and review articles.

RESULTS

Individuals vary in their susceptibility to autonomic symptoms, ranging from fatigue to massive vomiting, induced by passive movement at relatively low frequencies (0.2 to 0.4 Hz) in situations without any visual reference to the horizontal plane. Younger persons and women are considered more susceptible, and twin studies have revealed a genetic component as well. The various types of motion sickness are adequately explained by the intersensory conflict model, incorporating the vestibular, visual, and proprioceptive systems and extended to include consideration of postural instability and asymmetry of the otolith organs. Scopolamine and H1-antihistamines, such as dimenhydrinate and cinnarizine, can be used as pharmacotherapy. The symptoms can also be alleviated by habituation through long exposure or by the diminution of vestibular stimuli.

CONCLUSION

The various types of motion sickness can be treated with general measures to lessen the intersensory conflict, behavioral changes, and drugs.},
 author = {Koch, Andreas and Cascorbi, Ingolf and Westhofen, Martin and Dafotakis, Manuel and Klapa, Sebastian and Kuhtz-Buschbeck, Johann Peter},
 year = {2018},
 title = {The Neurophysiology and Treatment of Motion Sickness},
 urldate = {29.04.2022},
 pages = {687--696},
 volume = {115},
 number = {41},
 journal = {Deutsches Arzteblatt international},
 doi = {10.3238/arztebl.2018.0687}
}


@inproceedings{Kim.2014,
 author = {Kim, Sei-Young and Lee, Joong Ho and Park, Ji Hyung},
 title = {The effects of visual displacement on simulator sickness in video see-through head-mounted displays},
 urldate = {22.03.2022},
 pages = {79--82},
 publisher = {ACM},
 isbn = {9781450329699},
 editor = {Dunne, Lucy and Martin, Tom and Beigl, Michael},
 booktitle = {Proceedings of the 2014 ACM International Symposium on Wearable Computers},
 year = {2014},
 address = {New York, NY, USA},
 doi = {10.1145/2634317.2634339}
}


@inproceedings{Jones.2018,
author={{Jones, Monica Lynn Haumann} and {Sienko, Kathleen} and {Ebert-Hamilton, Sheila} and {Kinnaird, Catherine} and {Miller, Carl} and {Lin, Brian} and {Park, Byoung-Keon} and {Sullivan, John} and {Reed, Matthew} and {Sayer, James}},
title={Development of a Vehicle-Based Experimental Platform for Quantifying Passenger Motion Sickness during Test Track Operations},
booktitle={WCX World Congress Experience},
publisher={SAE International},
month={apr},
year={2018},
doi={https://doi.org/10.4271/2018-01-0028},
url={https://doi.org/10.4271/2018-01-0028},
abstract={Motion sickness in road vehicles may become an increasingly important problem as automation transforms drivers into passengers. Motion sickness could be mitigated through control of the vehicle motion dynamics, design of the interior environment, and other interventions. However, a lack of a definitive etiology of motion sickness challenges the design of automated vehicles (AVs) to address motion sickness susceptibility effectively. Few motion sickness studies have been conducted in naturalistic road-vehicle environments; instead, most research has been performed in driving simulators or on motion platforms that produce prescribed motion profiles. To address this gap, a vehicle-based experimental platform using a midsize sedan was developed to quantify motion sickness in road vehicles. A scripted, continuous drive consisting of a series of frequent 90-degree turns, braking, and lane changes were conducted on a closed track. The route was selected to be representative of naturalistic urban driving conditions and parameterized in terms of lateral and longitudinal acceleration intensities likely to produce motion sickness. Vehicle instrumentation included simultaneous measure of vehicle acceleration, passenger head kinematics, self-reported motion sickness ratings and associated sensations, and physiological responses. A no-task condition involved normative passenger behavior and unconstrained gaze. During the task condition, passengers read a handheld mini iPad tablet. The resulting vehicle-based experimental platform provided a reliable methodology designed to quantify motion sickness. Knowledge generated from studies with this platform will inform the design of AVs and the development and evaluation of countermeasures.}
}


@misc{Jaaskelainen.2022,
 abstract = {Varjo Lab Tools 1.2 introduces a new hand masking method with skeletal hand masking, improved U.I. and performance, and chroma key controls.},
 author = {J{\"a}{\"a}skel{\"a}inen, Samuli},
 year = {2022},
 title = {Varjo Lab Tools 1.2: Introducing skeletal hand masks, chroma key and more},
 url = {https://varjo.com/vr-lab/varjo-lab-tools-1-2-introducing-skeletal-hand-masks-chroma-key-and-more/},
 urldate = {16.10.2022}
}


@proceedings{Iqbal.2020,
 year = {2020},
 title = {Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450375153},
 editor = {Iqbal, Shamsi and MacLean, Karon and Chevalier, Fanny and Mueller, Stefanie},
 doi = {10.1145/3379350}
}


@inproceedings{Hock.2017,
author = {Hock, Philipp and Benedikter, Sebastian and Gugenheimer, Jan and Rukzio, Enrico},
title = {CarVR: Enabling In-Car Virtual Reality Entertainment},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025665},
doi = {10.1145/3025453.3025665},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {4034–4044},
numpages = {11},
keywords = {motion platform, force-feedback, immersion, entertainment, gaming, automotive, virtual reality},
location = {Denver, Colorado, USA},
series = {CHI '17}
}


@misc{Hecomi.2021,
 abstract = {Desktop Duplication API implementation for Unity (only for Windows 8/10) - GitHub - hecomi/uDesktopDuplication: Desktop Duplication API implementation for Unity (only for Windows 8/10)},
 author = {Hecomi},
 year = {2021},
 title = {uDesktopDuplication: Desktop Duplication API implementation for Unity},
 url = {https://github.com/hecomi/uDesktopDuplication},
 urldate = {29.11.2022}
}


@article{Hart.2006,
 author = {Hart, Sandra G.},
 year = {2006},
 title = {Nasa-Task Load Index (NASA-TLX); 20 Years Later},
 urldate = {05.11.2022},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {50},
number = {9},
pages = {904-908},
doi = {10.1177/154193120605000909}
}


@book{Hancock.1988,
 year = {1988},
 title = {Advances in psychology},
 address = {Amsterdam u.a.},
 edition = {52},
 publisher = {North Holland Publ. Co},
 isbn = {0166-4115},
 editor = {Hancock, Peter A. and Meshkati, Najmedin}
}


@inproceedings{Haeling.2018,
  author={Haeling, Jonas and Winkler, Christian and Leenders, Stephan and Keßelheim, Daniel and Hildebrand, Axel and Necker, Marc},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={In-Car 6-DoF Mixed Reality for Rear-Seat and Co-Driver Entertainment}, 
  year={2018},
  volume={},
  number={},
  pages={757-758},
  doi={10.1109/VR.2018.8446461},
publisher={IEEE},
address={New York, NY, USA}
}


@misc{H.P.R.Group.,
 title = {NASA Task Load Index (TLX): Paper and Pencil Package},
 url = {https://humansystems.arc.nasa.gov/groups/TLX/downloads/TLX_pappen_manual.pdf},
 urldate = {05.11.2022},
 file = {TLX{\_}pappen{\_}manual:Attachments/TLX{\_}pappen{\_}manual.pdf:application/pdf}
}


@proceedings{Green.2016,
 year = {2016},
 title = {Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450345330},
 editor = {Green, Paul and Boll, Susanne and Gabbard, Joe and Osswald, Sebastian and Burnett, Gary and Borojeni, Shadan Sadeghian and L{\"o}cken, Andreas and Pradhan, Anuj},
 doi = {10.1145/3003715}
}


@article{Golding.2006,
 author = {Golding, John F.},
 year = {2006},
 title = {Predicting individual differences in motion sickness susceptibility by questionnaire},
 urldate = {02.06.2022},
 pages = {237--248},
 volume = {41},
 number = {2},
 issn = {01918869},
 journal = {Personality and Individual Differences},
 doi = {10.1016/j.paid.2006.01.012}
}


@article{Golding.1998,
 author = {Golding, John F.},
 year = {1998},
 title = {Motion sickness susceptibility questionnaire revised and its relationship to other forms of sickness},
 urldate = {02.06.2022},
 pages = {507--516},
 volume = {47},
 number = {5},
 issn = {03619230},
 journal = {Brain Research Bulletin},
 doi = {10.1016/S0361-9230(98)00091-4}
}


@inproceedings{Goedicke.2018,
author = {Goedicke, David and Li, Jamy and Evers, Vanessa and Ju, Wendy},
title = {VR-OOM: Virtual Reality On-ROad Driving SiMulation},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173739},
doi = {10.1145/3173574.3173739},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {autonomous vehicles, design evaluation, prototyping, virtual reality},
location = {Montreal QC, Canada},
series = {CHI '18}
}


@inproceedings{Goedicke.2022,
author = {Goedicke, David and Bremers, Alexandra W.D. and Lee, Sam and Bu, Fanjun and Yasuda, Hiroshi and Ju, Wendy},
title = {XR-OOM: MiXed Reality Driving Simulation with Real Cars for Research and Design},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517704},
doi = {10.1145/3491102.3517704},
articleno = {107},
numpages = {13},
keywords = {driving simulation, XR, automotive, mixed reality, user studies, design, safety},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{batmaz2022EffectiveThroughputAnalysis,
  title = {Effective {{Throughput Analysis}} of {{Different Task Execution Strategies}} for {{Mid-Air Fitts}}' {{Tasks}} in {{Virtual Reality}}},
  author = {Batmaz, Anil Ufuk and Stuerzlinger, Wolfgang},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {11},
  pages = {3939--3947},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2022.3203105},
  urldate = {2024-09-04},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/Crown.html},
  file = {C:\Users\msasalo\Zotero\storage\VRVXYACA\Batmaz und Stuerzlinger - 2022 - Effective Throughput Analysis of Different Task Ex.pdf}
}



@article{pfeuffer2024DesignPrinciplesChallenges,
  title = {Design {{Principles}} and {{Challenges}} for {{Gaze}} + {{Pinch Interaction}} in {{XR}}},
  author = {Pfeuffer, Ken and Gellersen, Hans and {Gonzalez-Franco}, Mar},
  year = {2024},
  month = may,
  journal = {IEEE Computer Graphics and Applications},
  volume = {44},
  number = {3},
  pages = {74--81},
  issn = {0272-1716, 1558-1756},
  doi = {10.1109/MCG.2024.3382961},
  urldate = {2024-07-10},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {C:\Users\msasalo\Zotero\storage\PXF7RRPS\Pfeuffer et al. - 2024 - Design Principles and Challenges for Gaze + Pinch .pdf}
}

@inproceedings{pfeuffer2017GazePinchInteraction,
  title = {Gaze + Pinch Interaction in Virtual Reality},
  booktitle = {Proceedings of the 5th {{Symposium}} on {{Spatial User Interaction}}},
  author = {Pfeuffer, Ken and Mayer, Benedikt and Mardanbegi, Diako and Gellersen, Hans},
  year = {2017},
  month = oct,
  pages = {99--108},
  publisher = {ACM},
  address = {Brighton United Kingdom},
  doi = {10.1145/3131277.3132180},
  urldate = {2024-07-10},
  isbn = {978-1-4503-5486-8},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\Z4XNGN63\Pfeuffer et al. - 2017 - Gaze + pinch interaction in virtual reality.pdf}
}


@inproceedings{sidenmark2019EyeHeadSynergetic,
  title = {Eye\&{{Head}}: {{Synergetic Eye}} and {{Head Movement}} for {{Gaze Pointing}} and {{Selection}}},
  shorttitle = {Eye\&{{Head}}},
  booktitle = {Proceedings of the 32nd {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Sidenmark, Ludwig and Gellersen, Hans},
  year = {2019},
  month = oct,
  pages = {1161--1174},
  publisher = {ACM},
  address = {New Orleans LA USA},
  doi = {10.1145/3332165.3347921},
  urldate = {2024-07-12},
  isbn = {978-1-4503-6816-2},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\W24BTGGK\Sidenmark und Gellersen - 2019 - Eye&Head Synergetic Eye and Head Movement for Gaz.pdf}
}

@book{maxwell2017DesigningExperimentsAnalyzing,
  title = {Designing {{Experiments}} and {{Analyzing Data}}: {{A Model Comparison Perspective}}},
  shorttitle = {Designing {{Experiments}} and {{Analyzing Data}}},
  author = {Maxwell, Scott E. and Delaney, Harold D. and Kelley, Ken},
  year = {2017},
  month = sep,
  edition = {3},
  publisher = {Routledge},
  address = {Third edition / Scott E. Maxwell, Harold D. Delaney, and Ken Kelley. {\textbar} New York,},
  doi = {10.4324/9781315642956},
  urldate = {2024-12-10},
  isbn = {978-1-315-64295-6},
  langid = {english}
}


@article{Gianaros.2001,
 abstract = {Background: A limited number of attempts have been made to develop a questionnaire that assesses the experience of motion sickness. Further, many available questionnaires quantify motion sickness as a unidimensional construct Method: Exploratory and confirmatory factor analyses of motion sickness descriptors were used to derive and verify four dimensions of motion sickness, which were defined as gastrointestinal, central, peripheral, and sopite-related. These dimensions of motion sickness were then used to construct a motion sickness assessment questionnaire (MSAQ) that was administered to individuals who were exposed to a rotating optokinetic drum. Results: Total scores from the MSAQ correlated strongly with overall scores from the Pensacola Diagnostic index (r = 0.81, p {\textless} 0.001) and the Nausea Profile (r = 0.92, p {\textless} 0.001). Conclusions: The MSAQ is a valid instrument for the assessment of motion sickness. In addition, the MSAQ may be used to assess motion sickness as a multidimensional rather than unidimensional construct},
 author = {Gianaros, Peter J. and Muth, Eric R. and Mordkoff, J. Toby and Levine, Max E. and Stern, Robert M.},
 year = {2001},
 title = {A questionnaire for the assessment of the multiple dimensions of motion sickness},
 url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2910410/},
 pages = {115-119},
 volume = {72},
 number = {2},
 issn = {0095-6562},
 journal = {Aviation, space, and environmental medicine}
}

@inproceedings{speicher2019PseudohapticControlsMidair,
  title = {Pseudo-Haptic {{Controls}} for {{Mid-air Finger-based Menu Interaction}}},
  booktitle = {Extended {{Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Speicher, Marco and Ehrlich, Jan and Gentile, Vito and Degraen, Donald and Sorce, Salvatore and Kr{\"u}ger, Antonio},
  year = {2019},
  month = may,
  pages = {1--6},
  publisher = {ACM},
  address = {Glasgow Scotland Uk},
  doi = {10.1145/3290607.3312927},
  urldate = {2024-12-04},
  isbn = {978-1-4503-5971-9},
  langid = {english}
}

@article{kim2022PseudohapticButtonImproving,
  title = {Pseudo-Haptic Button for Improving User Experience of Mid-Air Interaction in {{VR}}},
  author = {Kim, Woojoo and Xiong, Shuping},
  year = {2022},
  month = dec,
  journal = {International Journal of Human-Computer Studies},
  volume = {168},
  pages = {102907},
  issn = {10715819},
  doi = {10.1016/j.ijhcs.2022.102907},
  urldate = {2024-12-04},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\4WND3GJV\Kim und Xiong - 2022 - Pseudo-haptic button for improving user experience.pdf}
}


@inproceedings{mutasim2021PinchClickDwell,
  title = {Pinch, {{Click}}, or {{Dwell}}: {{Comparing Different Selection Techniques}} for {{Eye-Gaze-Based Pointing}} in {{Virtual Reality}}},
  shorttitle = {Pinch, {{Click}}, or {{Dwell}}},
  booktitle = {{{ACM Symposium}} on {{Eye Tracking Research}} and {{Applications}}},
  author = {Mutasim, Aunnoy K and Batmaz, Anil Ufuk and Stuerzlinger, Wolfgang},
  year = {2021},
  month = may,
  pages = {1--7},
  publisher = {ACM},
  address = {Virtual Event Germany},
  doi = {10.1145/3448018.3457998},
  urldate = {2024-07-10},
  isbn = {978-1-4503-8345-5},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\WCNSKLUP\Mutasim et al. - 2021 - Pinch, Click, or Dwell Comparing Different Select.pdf}
}


@inproceedings{Ghiurau.2020,
author = {Ghiur\~{a}u, Florin-Timotei and Bayta\c{s}, Mehmet Ayd\i{}n and Wickman, Casper},
title = {ARCAR: On-Road Driving in Mixed Reality by Volvo Cars},
year = {2020},
isbn = {9781450375153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379350.3416186},
doi = {10.1145/3379350.3416186},
booktitle = {Adjunct Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {62–64},
numpages = {3},
keywords = {mixed reality, virtual reality, volvo cars, augmented reality, automotive, driving, on-road},
location = {Virtual Event, USA},
series = {UIST '20 Adjunct}
}


@book{Galler.2021,
 author = {Galler, B. and Steinh{\"a}user, J. and Psathakis, D.},
 year = {2021},
 title = {NASA-Task Load Index: ein Instrument, um sich der Komplexit{\"a}t von Beratungsanl{\"a}ssen in der Allgemeinmedizin zu n{\"a}hern},
 url = {https://www.zhb.uni-luebeck.de/epubs/ediss2445.pdf},
 urldate = {07.09.2022},
 publisher = {Zentrale Hochschulbibliothek L{\"u}beck}
}


@misc{LPResearch.2020,
 abstract = {LPVR-DUO Manual Version 2.4      Contents Version History	4 Overview	5 System Versions	6 LPVR Middleware DETAILS	6 Application of LPVR Middleware to In-Car VR / AR (LPVR-DUO)	7 Driver Implementation	8 SteamVR Driver Wrapper	8 Tracking And Sensor Fusion	9 System Setup	10 Hardware Installat...},
 year = {2020},
 title = {LPVR-DUO Manual},
 url = {https://docs.google.com/document/d/19mvxoMKKY6bTzv456TzUiCpaVLIfUiOWnRU6dAqoWR4/edit},
 urldate = {16.11.2022}
}


@inproceedings{Yeo.2019,
  author={Yeo, Dohyeon and Kim, Gwangbin and Kim, SeungJun},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={MAXIM: Mixed-reality Automotive Driving XIMulation}, 
  year={2019},
  pages={460-464},
  doi={10.1109/ISMAR-Adjunct.2019.00124}
}

@inproceedings{sasalovici2023InCarOfficeCan,
  author = {Sasalovici, Markus and Leenders, Stephan and Schramm, Robin Connor and Freiwald, Jann Philipp and Botzet, Hannes Frederic and Ke\ss{}elheim, Daniel and Krach, Thomas and Winkler, Christian},
title = {In-Car Office: Can HMD-Based AR Alleviate Passenger Motion Sickness?},
year = {2023},
isbn = {9798400701122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581961.3609869},
doi = {10.1145/3581961.3609869},
abstract = {Performing non-driving-related tasks as car passenger reduces visual perception of surroundings, which may cause a conflict with the human vestibular system and thus lead to motion sickness. Augmented reality head-mounted displays offer a possible solution to this phenomenon by presenting digital content at head level as opposed to common displays placed on one’s lap, keeping the peripheral vision intact. However, technical limitations such as end-to-end latency of video see-through devices may counteract this advantage. Therefore, we investigated a mobile office scenario by comparing video see-through augmented reality to a traditional laptop setup with regard to motion sickness and task performance in a moving car. Our results suggest similar responses to motion sickness between conditions, with limited effects on task performance and improved ergonomics when using augmented reality.},
booktitle = {Adjunct Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {133–138},
numpages = {6},
keywords = {augmented reality, automotive, human factors, in-car, mixed reality, motion sickness, office, work environment},
location = {Ingolstadt, Germany},
series = {AutomotiveUI '23 Adjunct}
}

@inproceedings{ahmadInteractiveDisplaysVehicles2014,
  title = {Interactive {{Displays}} in {{Vehicles}}: {{Improving Usability}} with a {{Pointing Gesture Tracker}} and {{Bayesian Intent Predictors}}},
  shorttitle = {Interactive {{Displays}} in {{Vehicles}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Ahmad, Bashar I. and Langdon, Patrick M. and Godsill, Simon J. and Hardy, Robert and Dias, Eduardo and Skrypchuk, Lee},
  year = {2014},
  date = {2014-09-17},
  pages = {1--8},
publisher={ACM},
address={New York, NY, USA},
  location = {{Seattle WA USA}},
  doi = {10.1145/2667317.2667413},
  url = {https://dl.acm.org/doi/10.1145/2667317.2667413},
  urldate = {2023-06-05},
  eventtitle = {{{AutomotiveUI}} '14: 6th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-3212-5},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\F9S34ABS\Ahmad et al. - 2014 - Interactive Displays in Vehicles Improving Usabil.pdf}
}

@inproceedings{ahmadTouchscreenUsabilityInput2015,
  title = {Touchscreen Usability and Input Performance in Vehicles under Different Road Conditions: An Evaluative Study},
  shorttitle = {Touchscreen Usability and Input Performance in Vehicles under Different Road Conditions},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Ahmad, Bashar I. and Langdon, Patrick M. and Godsill, Simon J. and Hardy, Robert and Skrypchuk, Lee and Donkor, Richard},
  year = {2015},
  date = {2015-09},
  pages = {47--54},
  publisher = {{ACM}},
  address = {New York, NY, USA},
  location = {{Nottingham United Kingdom}},
  doi = {10.1145/2799250.2799284},
  url = {https://dl.acm.org/doi/10.1145/2799250.2799284},
  urldate = {2023-06-05},
  eventtitle = {{{AutomotiveUI}} '15: {{The}} 7th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-3736-6},
  langid = {english},
  keywords = {Reading Done,ToDo},
  file = {C:\Users\msasalo\Zotero\storage\H8YVIK7N\Ahmad et al. - 2015 - Touchscreen usability and input performance in veh.pdf}
}

@inproceedings{arizaProximity2018,
  author={Ariza, Oscar and Bruder, Gerd and Katzakis, Nicholas and Steinicke, Frank},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Analysis of Proximity-Based Multimodal Feedback for 3D Selection in Immersive Virtual Environments}, 
  year={2018},
  volume={},
  number={},
  pages={327-334},
  doi={10.1109/VR.2018.8446317}}


@inproceedings{aslanLeapTouchProximity2015,
author = {Aslan, Ilhan and Krischkowsky, Alina and Meschtscherjakov, Alexander and Wuchse, Martin and Tscheligi, Manfred},
title = {A leap for touch: proximity sensitive touch targets in cars},
year = {2015},
isbn = {9781450337366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2799250.2799273},
doi = {10.1145/2799250.2799273},
abstract = {Combining touch screen technology with mid-air gestures into a unified input modality has potential to improve interaction with touch interfaces in cars. Moreover, target objects on a touch screen can be adapted to the proximity of a users' finger in mid-air. In this paper, we present an exploration of this design space based on two studies and various prototypical systems. First, results of a driving simulator study are reported, indicating that a driver's performance in acquiring a target object on a touch screen in a central console position increases with expanding targets, while altering the position of a target object on the screen leads to a decrease of performance. In a follow-up workshop with automotive experts, prototypes with multiple expanding targets were utilized to foster in-depth discussions on potential challenges and benefits with expanding targets in cars. Results of both studies indicate a high potential of expanding targets for in-car interaction scenarios.},
booktitle = {Proceedings of the 7th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {39–46},
numpages = {8},
keywords = {touch, leap motion, automotive, Fitts's law},
location = {Nottingham, United Kingdom},
series = {AutomotiveUI '15},
  file = {C:\Users\msasalo\Zotero\storage\UQAIGGMW\Aslan et al. - 2015 - A leap for touch proximity sensitive touch target.pdf}
}

@article{benglerInteractionPrinciplesCooperative2012,
  title = {Interaction {{Principles}} for {{Cooperative Human-Machine Systems}}},
  author = {Bengler, Klaus and Zimmermann, Markus and Bortot, Dino and Kienle, Martin and Damböck, Daniel},
  date = {2012-08},
  journaltitle = {itit},
  volume = {54},
  number = {4},
  pages = {157--164},
  issn = {2196-7032, 1611-2776},
  doi = {10.1524/itit.2012.0680},
  url = {https://www.degruyter.com/document/doi/10.1524/itit.2012.0680/html},
  urldate = {2023-06-25},
  abstract = {Abstract             Human-machine systems with shared authority can be observed in different domains of assistance systems. This article creates a taxonomy of the most important aspects of human-machine cooperation in five layers: intention, modes of cooperation, allocation, interfaces and contact. This is investigated with help of driver assistance and Human-Robot Interaction. Furthermore, a perspective for possibilities of cross-domain generalization is given.           ,              Zusammenfassung             Mensch-Maschine-Systeme mit geteilter Autorität entwickeln sich in vielen Domänen der Assistenzsysteme. Dieser Beitrag stellt die wichtigsten Aspekte der Mensch-Maschine-Kooperation in fünf Ebenen der Intention, Kooperationsmodi, Allokation, Schnittstellen und Kontakt dar. Dies wird anhand der Beispiele Fahrerassistenz und Mensch-Roboter-Interaktion beleuchtet. Weiterhin wird ein Ausblick auf Möglichkeiten zur domänenübergreifenden Generalisierung gegeben.},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\542875GK\Bengler et al. - 2012 - Interaction Principles for Cooperative Human-Machi.pdf}
}

@incollection{bergerDesigningConvenientInCar2021,
  title = {Designing for a {{Convenient In-Car Passenger Experience}}: {{A Repertory Grid Study}}},
  shorttitle = {Designing for a {{Convenient In-Car Passenger Experience}}},
  booktitle = {Human-{{Computer Interaction}} – {{INTERACT}} 2021},
  author = {Berger, Melanie and Pfleging, Bastian and Bernhaupt, Regina},
  editor = {Ardito, Carmelo and Lanzilotti, Rosa and Malizia, Alessio and Petrie, Helen and Piccinno, Antonio and Desolda, Giuseppe and Inkpen, Kori},
  date = {2021},
  volume = {12933},
  pages = {117--139},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-85616-8_9},
  url = {https://link.springer.com/10.1007/978-3-030-85616-8_9},
  urldate = {2023-09-02},
  isbn = {978-3-030-85615-1 978-3-030-85616-8},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\5HJC8W3Y\Berger et al. - 2021 - Designing for a Convenient In-Car Passenger Experi.pdf}
}

@inproceedings{blattgersteAdvantagesEyegazeHeadgazebased2018,
  author = {Blattgerste, Jonas and Renner, Patrick and Pfeiffer, Thies},
title = {Advantages of eye-gaze over head-gaze-based selection in virtual and augmented reality under varying field of views},
year = {2018},
isbn = {9781450357906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206343.3206349},
doi = {10.1145/3206343.3206349},
abstract = {The current best practice for hands-free selection using Virtual and Augmented Reality (VR/AR) head-mounted displays is to use head-gaze for aiming and dwell-time or clicking for triggering the selection. There is an observable trend for new VR and AR devices to come with integrated eye-tracking units to improve rendering, to provide means for attention analysis or for social interactions. Eye-gaze has been successfully used for human-computer interaction in other domains, primarily on desktop computers. In VR/AR systems, aiming via eye-gaze could be significantly faster and less exhausting than via head-gaze.To evaluate benefits of eye-gaze-based interaction methods in VR and AR, we compared aiming via head-gaze and aiming via eye-gaze. We show that eye-gaze outperforms head-gaze in terms of speed, task load, required head movement and user preference. We furthermore show that the advantages of eye-gaze further increase with larger FOV sizes.},
booktitle = {Proceedings of the Workshop on Communication by Gaze Interaction},
articleno = {1},
numpages = {9},
keywords = {virtual reality, human computer interaction, head-mounted displays, field of view, eye-tracking, augmented reality, assistance systems},
location = {Warsaw, Poland},
series = {COGAIN '18}
}

@inproceedings{chatterjeeGazeGestureExpressive2015,
  title = {Gaze+{{Gesture}}: {{Expressive}}, {{Precise}} and {{Targeted Free-Space Interactions}}},
  shorttitle = {Gaze+{{Gesture}}},
  booktitle = {Proceedings of the 2015 {{ACM}} on {{International Conference}} on {{Multimodal Interaction}}},
  author = {Chatterjee, Ishan and Xiao, Robert and Harrison, Chris},
  date = {2015-11-09},
  pages = {131--138},
  publisher = {{ACM}},
  location = {{Seattle Washington USA}},
  doi = {10.1145/2818346.2820752},
  url = {https://dl.acm.org/doi/10.1145/2818346.2820752},
  urldate = {2023-06-05},
  eventtitle = {{{ICMI}} '15: {{INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION}}},
  isbn = {978-1-4503-3912-4},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\A7GDF3ZZ\Chatterjee et al. - 2015 - Gaze+Gesture Expressive, Precise and Targeted Fre.pdf}
}

@inproceedings{colleyDesignSpaceExternal2020,
  title = {A {{Design Space}} for {{External Communication}} of {{Autonomous Vehicles}}},
  booktitle = {12th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Colley, Mark and Rukzio, Enrico},
  date = {2020-09-21},
  pages = {212--222},
  publisher = {{ACM}},
  location = {{Virtual Event DC USA}},
  doi = {10.1145/3409120.3410646},
  url = {https://dl.acm.org/doi/10.1145/3409120.3410646},
  urldate = {2023-05-31},
  eventtitle = {{{AutomotiveUI}} '20: 12th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-8065-2},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\PLQ3X2Y6\Colley und Rukzio - 2020 - A Design Space for External Communication of Auton.pdf}
}

@article{colleySwiVRCarSeatExploringVehicle2021,
  author = {Colley, Mark and Jansen, Pascal and Rukzio, Enrico and Gugenheimer, Jan},
title = {SwiVR-Car-Seat: Exploring Vehicle Motion Effects on Interaction Quality in Virtual Reality Automated Driving Using a Motorized Swivel Seat},
year = {2022},
issue_date = {Dec 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3494968},
doi = {10.1145/3494968},
abstract = {Autonomous vehicles provide new input modalities to improve interaction with in-vehicle information systems. However, due to the road and driving conditions, the user input can be perturbed, resulting in reduced interaction quality. One challenge is assessing the vehicle motion effects on the interaction without an expensive high-fidelity simulator or a real vehicle. This work presents SwiVR-Car-Seat, a low-cost swivel seat to simulate vehicle motion using rotation. In an exploratory user study (N=18), participants sat in a virtual autonomous vehicle and performed interaction tasks using the input modalities touch, gesture, gaze, or speech. Results show that the simulation increased the perceived realism of vehicle motion in virtual reality and the feeling of presence. Task performance was not influenced uniformly across modalities; gesture and gaze were negatively affected while there was little impact on touch and speech. The findings can advise automotive user interface design to mitigate the adverse effects of vehicle motion on the interaction.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {150},
numpages = {26},
keywords = {vehicle motion simulation, interface design, interaction quality, Autonomous vehicles}
}

@inproceedings{cuiGlanceWriterWritingText2023,
  title = {{{GlanceWriter}}: {{Writing Text}} by {{Glancing Over Letters}} with {{Gaze}}},
  shorttitle = {{{GlanceWriter}}},
  author = {Cui, Wenzhe and Liu, Rui and Li, Zhi and Wang, Yifan and Wang, Andrew and Zhao, Xia and Rashidian, Sina and Baig, Furqan and Ramakrishnan, Iv and Wang, Fusheng and Bi, Xiaojun},
  date = {2023-04-19},
  pages = {1--13},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3581269},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581269},
  urldate = {2023-05-07},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\UW43C6TI\3544548.3581269}
}

@inproceedings{curranoLittleRoadDriving2021,
  title = {Little {{Road Driving HUD}}: {{Heads-Up Display Complexity Influences Drivers}}’ {{Perceptions}} of {{Automated Vehicles}}},
  shorttitle = {Little {{Road Driving HUD}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Currano, Rebecca and Park, So Yeon and Moore, Dylan James and Lyons, Kent and Sirkin, David},
  date = {2021-05-06},
  pages = {1--15},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445575},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445575},
  urldate = {2023-07-12},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\YDIW3CM3\Currano et al. - 2021 - Little Road Driving HUD Heads-Up Display Complexi.pdf}
}

@article{ellisGroupwareIssuesExperiences1991,
  title = {Groupware: Some Issues and Experiences},
  shorttitle = {Groupware},
  author = {Ellis, Clarence A. and Gibbs, Simon J. and Rein, Gail},
  date = {1991-01-03},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {34},
  number = {1},
  pages = {39--58},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/99977.99987},
  url = {https://dl.acm.org/doi/10.1145/99977.99987},
  urldate = {2023-05-31},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\S838KZ3B\Ellis et al. - 1991 - Groupware some issues and experiences.pdf}
}

@inproceedings{evangelistabeloXRgonomicsFacilitatingCreation2021,
  title = {{{XRgonomics}}: {{Facilitating}} the {{Creation}} of {{Ergonomic 3D Interfaces}}},
  shorttitle = {{{XRgonomics}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Evangelista Belo, João Marcelo and Feit, Anna Maria and Feuchtner, Tiare and Grønbæk, Kaj},
  date = {2021-05-06},
  pages = {1--11},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445349},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445349},
  urldate = {2023-08-21},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  keywords = {Reading Done,ToDo},
  file = {C:\Users\msasalo\Zotero\storage\ATQ8KCNC\Evangelista Belo et al. - 2021 - XRgonomics Facilitating the Creation of Ergonomic.pdf}
}

@article{fittsInformationCapacityHuman1992,
  title = {The Information Capacity of the Human Motor System in Controlling the Amplitude of Movement.},
  author = {Fitts, Paul M. and {Paul M. Fitts}},
  date = {1992-09-01},
  journaltitle = {Journal of Experimental Psychology},
  volume = {47},
  number = {6},
  pages = {381--391},
  doi = {10.1037/h0055392},
  abstract = {Information theory has recently been employed to specify more precisely than has hitherto been possible man's capacity in certain sensory, perceptual, and perceptual-motor functions (5, 10, 13, 15, 17, 18). The experiments reported in the present paper extend the theory to the human motor system. The applicability of only the basic concepts, amount of information, noise, channel capacity, and rate of information transmission, will be examined at this time. General familiarity with these concepts as formulated by recent writers (4, 11,20, 22) is assumed. Strictly speaking, we cannot study man's motor system at the behavioral level in isolation from its associated sensory mechanisms. We can only analyze the behavior of the entire receptor-neural-effector system. However, by asking 51 to make rapid and uniform responses that have been highly overlearned, and by holding all relevant stimulus conditions constant with the exception of those resulting from 5"s own movements, we can create an experimental situation in which it is reasonable to assume that performance is limited primarily by the capacity of the motor system. The motor system in the present case is defined as including the visual and proprioceptive feedback loops that permit S to monitor his own activity. The information capacity of the motor system is specified by its ability to produce consistently one class of movement from among several alternative movement classes. The greater the number of alternative classes, the greater is the information capacity of a particular type of response. Since measurable aspects of motor responses, such as their force, direction, and amplitude, are continuous variables, their information capacity is limited only by the amount of statistical variability, or noise, that is characteristic of repeated efforts to produce the same response. The information capacity of the motor Editor's Note. This article is a reprint of an original work published in 1954 in the Journal of Experimental Psychology, 47, 381391.},
  keywords = {ToDo},
  annotation = {MAG ID: 2179427518},
  file = {C:\Users\msasalo\Zotero\storage\BJGU68PC\Fittslaw.pdf}
}

@incollection{fuestTaxonomyTrafficSituations2018,
  title = {Taxonomy of {{Traffic Situations}} for the {{Interaction}} between {{Automated Vehicles}} and {{Human Road Users}}},
  booktitle = {Advances in {{Human Aspects}} of {{Transportation}}},
  author = {Fuest, Tanja and Sorokin, Lenja and Bellem, Hanna and Bengler, Klaus},
  editor = {Stanton, Neville A},
  date = {2018},
  volume = {597},
  pages = {708--719},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-60441-1_68},
  url = {http://link.springer.com/10.1007/978-3-319-60441-1_68},
  urldate = {2023-06-23},
  isbn = {978-3-319-60440-4 978-3-319-60441-1},
  keywords = {ToDo}
}

@inproceedings{goelWalkTypeUsingAccelerometer2012,
  title = {{{WalkType}}: Using Accelerometer Data to Accomodate Situational Impairments in Mobile Touch Screen Text Entry},
  shorttitle = {{{WalkType}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Goel, Mayank and Findlater, Leah and Wobbrock, Jacob},
  date = {2012-05-05},
  pages = {2687--2696},
  publisher = {{ACM}},
  location = {{Austin Texas USA}},
  doi = {10.1145/2207676.2208662},
  url = {https://dl.acm.org/doi/10.1145/2207676.2208662},
  urldate = {2023-06-05},
  eventtitle = {{{CHI}} '12: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-1015-4},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\RRF2VQFZ\Goel et al. - 2012 - WalkType using accelerometer data to accomodate s.pdf}
}

@article{goodeImpactOnroadMotion2012,
  title = {The Impact of On-Road Motion on {{BMS}} Touch Screen Device Operation.},
  author = {Goode, Natassia and Lenné, Michael G. and Salmon, Paul M.},
  date = {2012-08-14},
  journaltitle = {Ergonomics},
  volume = {55},
  number = {9},
  eprint = {22676650},
  eprinttype = {pmid},
  pages = {986--996},
  doi = {10.1080/00140139.2012.685496},
  abstract = {This study investigates the effect of vehicle motion on performance, usability and workload for a touch screen in-vehicle Battle Management System (BMS). Participants performed a series of battle management tasks while a vehicle was driven over sealed (characteristic of ‘normal’ vehicle motion) and unsealed (characteristic of ‘high’ vehicle motion) roads. The results indicate that unsealed road conditions impair the performance of information input tasks (tasks that require the user to enter information, e.g. text entry) but not information extraction tasks (tasks that require the user to retrieve information from the system, e.g. reading coordinates). Participants rated workload as higher and the system as less usable on the unsealed road. In closing, the implications for in-vehicle touch screen design and use in both military and civilian driving contexts are discussed. Practitioner Summary: The effect of motion on interacting with in-vehicle touch screen devices remains largely unexplored. This study e...},
  keywords = {ToDo},
  annotation = {MAG ID: 2118086586}
}

@article{henriksonHeadCoupledKinematicTemplate2020,
  title = {Head-{{Coupled Kinematic Template Matching}}: {{A Prediction Model}} for {{Ray Pointing}} in {{VR}}},
  author = {Henrikson, Rorik and Grossman, Tovi and Trowbridge, Sean and Wigdor, Daniel and {Hrvoje Benko} and {Hrvoje Benko} and Benko, Hrvoje},
  date = {2020-04-21},
  pages = {1--14},
  doi = {10.1145/3313831.3376489},
  abstract = {This paper presents a new technique to predict the ray pointer landing position for selection movements in virtual reality (VR) environments. The technique adapts and extends a prior 2D kinematic template matching method to VR environments where ray pointers are used for selection. It builds on the insight that the kinematics of a controller and Head-Mounted Display (HMD) can be used to predict the ray's final landing position and angle. An initial study provides evidence that the motion of the head is a key input channel for improving prediction models. A second study validates this technique across a continuous range of distances, angles, and target sizes. On average, the technique's predictions were within 7.3° of the true landing position when 50\% of the way through the movement and within 3.4° when 90\%. Furthermore, compared to a direct extension of Kinematic Template Matching, which only uses controller movement, this head-coupled approach increases prediction accuracy by a factor of 1.8x when 40\% of the way through the movement.},
  annotation = {MAG ID: 3031287269}
}

@inproceedings{hespanholVoteYouGo2015,
  title = {Vote as You Go: Blending Interfaces for Community Engagement into the Urban Space},
  shorttitle = {Vote as You Go},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Communities}} and {{Technologies}}},
  author = {Hespanhol, Luke and Tomitsch, Martin and McArthur, Ian and Fredericks, Joel and Schroeter, Ronald and Foth, Marcus},
  date = {2015-06-27},
  pages = {29--37},
  publisher = {{ACM}},
  location = {{Limerick Ireland}},
  doi = {10.1145/2768545.2768553},
  url = {https://dl.acm.org/doi/10.1145/2768545.2768553},
  urldate = {2023-06-25},
  eventtitle = {C\&{{T}} '15: {{Communities}} and {{Technologies}} 2015},
  isbn = {978-1-4503-3460-0},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\7NID66XD\Hespanhol et al. - 2015 - Vote as you go blending interfaces for community .pdf}
}

@inproceedings{hirzleWatchVRExploringUsage2018,
  title = {{{WatchVR}}: {{Exploring}} the {{Usage}} of a {{Smartwatch}} for {{Interaction}} in {{Mobile Virtual Reality}}},
  shorttitle = {{{WatchVR}}},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hirzle, Teresa and Rixen, Jan and Gugenheimer, Jan and Rukzio, Enrico},
  date = {2018-04-20},
  pages = {1--6},
  publisher = {{ACM}},
  location = {{Montreal QC Canada}},
  doi = {10.1145/3170427.3188629},
  url = {https://dl.acm.org/doi/10.1145/3170427.3188629},
  urldate = {2023-06-05},
  eventtitle = {{{CHI}} '18: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-5621-3},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\LQKIA7NT\Hirzle et al. - 2018 - WatchVR Exploring the Usage of a Smartwatch for I.pdf}
}

@inproceedings{hollanderTaxonomyVulnerableRoad2021,
  title = {A {{Taxonomy}} of {{Vulnerable Road Users}} for {{HCI Based On A Systematic Literature Review}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Holländer, Kai and Colley, Mark and Rukzio, Enrico and Butz, Andreas},
  date = {2021-05-06},
  pages = {1--13},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445480},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445480},
  urldate = {2023-06-23},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\CN3LXIDN\Holländer et al. - 2021 - A Taxonomy of Vulnerable Road Users for HCI Based .pdf}
}

@inproceedings{houClassifyingHeadMovements2023,
  title = {Classifying {{Head Movements}} to {{Separate Head-Gaze}} and {{Head Gestures}} as {{Distinct Modes}} of {{Input}}},
  author = {Hou, Baosheng James and Newn, Joshua and Sidenmark, Ludwig and Ahmad Khan, Anam and Bækgaard, Per and Gellersen, Hans},
  date = {2023-04-19},
  pages = {1--14},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3581201},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581201},
  urldate = {2023-05-07},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\GS78NPA3\3544548.3581201}
}

@inproceedings{jansenAutoVisEnablingMixedImmersive2023,
  title = {{{AutoVis}}: {{Enabling Mixed-Immersive Analysis}} of {{Automotive User Interface Interaction Studies}}},
  shorttitle = {{{AutoVis}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Jansen, Pascal and Britten, Julian and Häusele, Alexander and Segschneider, Thilo and Colley, Mark and Rukzio, Enrico},
  date = {2023-04-19},
  pages = {1--23},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3580760},
  url = {https://dl.acm.org/doi/10.1145/3544548.3580760},
  urldate = {2023-06-01},
  abstract = {Automotive user interface (AUI) evaluation becomes increasingly complex due to novel interaction modalities, driving automation, heterogeneous data, and dynamic environmental contexts. Immersive analytics may enable efcient explorations of the resulting multilayered interplay between humans, vehicles, and the environment. However, no such tool exists for the automotive domain. With AutoVis, we address this gap by combining a non-immersive desktop with a virtual reality view enabling mixed-immersive analysis of AUIs. We identify design requirements based on an analysis of AUI research and domain expert interviews (N=5). AutoVis supports analyzing passenger behavior, physiology, spatial interaction, and events in a replicated study environment using avatars, trajectories, and heatmaps. We apply context portals and driving-path events as automotive-specifc visualizations. To validate AutoVis against real-world analysis tasks, we implemented a prototype, conducted heuristic walkthroughs using authentic data from a case study and public datasets, and leveraged a real vehicle in the analysis process.},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\AZ4USTSL\Jansen et al. - 2023 - AutoVis Enabling Mixed-Immersive Analysis of Auto.pdf}
}

@article{jansenDesignSpaceHuman2022,
  author = {Jansen, Pascal and Colley, Mark and Rukzio, Enrico},
title = {A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3534617},
doi = {10.1145/3534617},
abstract = {Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jul,
articleno = {56},
numpages = {51},
keywords = {systematic literature review, in-vehicle interaction, human sensors and actuators, design space}
}

@article{jensenSpeakingSystemSocial2017,
  title = {Speaking into the System: {{Social}} Media and Many-to-One Communication},
  shorttitle = {Speaking into the System},
  author = {Jensen, Klaus Bruhn and Helles, Rasmus},
  date = {2017-02},
  journaltitle = {European Journal of Communication},
  shortjournal = {European Journal of Communication},
  volume = {32},
  number = {1},
  pages = {16--25},
  issn = {0267-3231, 1460-3705},
  doi = {10.1177/0267323116682805},
  url = {http://journals.sagepub.com/doi/10.1177/0267323116682805},
  urldate = {2023-06-23},
  abstract = {Social media have been associated with the coming of many-to-many forms of communication, but they also depend on many-to-one communication: bit trails or metadata that the users of digital media leave behind and which serve to structure future communications. Departing from a communicative rather than a technical understanding of metadata, this article discusses the place of many-to-one communication in the modus operandi of social media. Speaking into the system, users engage with media that are social in distinctive ways and, thus, participate in the structuration of particular forms of society, with or without their knowledge and consent. The rights and responsibilities of the users of social media can be addressed with reference to a principle of habeas data, which complements both habeas corpus and the classic freedoms of expression and access to information.},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\S38XXZ5H\Jensen und Helles - 2017 - Speaking into the system Social media and many-to.pdf}
}

@inproceedings{jokelaDiaryStudyCombining2015,
  title = {A {{Diary Study}} on {{Combining Multiple Information Devices}} in {{Everyday Activities}} and {{Tasks}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Jokela, Tero and Ojala, Jarno and Olsson, Thomas},
  date = {2015-04-18},
  pages = {3903--3912},
  publisher = {{ACM}},
  location = {{Seoul Republic of Korea}},
  doi = {10.1145/2702123.2702211},
  url = {https://dl.acm.org/doi/10.1145/2702123.2702211},
  urldate = {2023-06-05},
  eventtitle = {{{CHI}} '15: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-3145-6},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\R63GYKQ2\Jokela et al. - 2015 - A Diary Study on Combining Multiple Information De.pdf}
}

@inproceedings{kariHandyCastPhonebasedBimanual2023,
 author = {Kari, Mohamed and Holz, Christian},
title = {HandyCast: Phone-based Bimanual Input for Virtual Reality in Mobile and Space-Constrained Settings via Pose-and-Touch Transfer},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580677},
doi = {10.1145/3544548.3580677},
abstract = {Despite the potential of Virtual Reality as the next computing platform for general purposes, current systems are tailored to stationary settings to support expansive interaction in mid-air. However, in mobile scenarios, the physical constraints of the space surrounding the user may be prohibitively small for spatial interaction in VR with classical controllers. In this paper, we present HandyCast, a smartphone-based input technique that enables full-range 3D input with two virtual hands in VR while requiring little physical space, allowing users to operate large virtual environments in mobile settings. HandyCast defines a pose-and-touch transfer function that fuses the phone’s position and orientation with touch input to derive two individual 3D hand positions. Holding their phone like a gamepad, users can thus move and turn it to independently control their virtual hands. Touch input using the thumbs fine-tunes the respective virtual hand position and controls object selection. We evaluated HandyCast in three studies, comparing its performance with that of Go-Go, a classic bimanual controller technique. In our open-space study, participants required significantly less physical motion using HandyCast with no decrease in completion time or body ownership. In our space-constrained study, participants achieved significantly faster completion times, smaller interaction volumes, and shorter path lengths with HandyCast compared to Go-Go. In our technical evaluation, HandyCast’s fully standalone inside-out 6D tracking performance again incurred no decrease in completion time compared to an outside-in tracking baseline.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {528},
numpages = {15},
keywords = {3D controller, VR input, Virtual reality, bimanual interaction., interaction techniques},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{lakierCrossCarMultiplayerGames2019,
  title = {Cross-{{Car}}, {{Multiplayer Games}} for {{Semi-Autonomous Driving}}},
  author = {Lakier, Matthew and Nacke, Lennart E. and Igarashi, Takeo and Vogel, Daniel},
  date = {2019-10-17},
  pages = {467--480},
  publisher = {{ACM}},
  location = {{Barcelona Spain}},
  doi = {10.1145/3311350.3347166},
  url = {https://dl.acm.org/doi/10.1145/3311350.3347166},
  urldate = {2023-05-06},
  eventtitle = {{{CHI PLAY}} '19: {{The Annual Symposium}} on {{Computer-Human Interaction}} in {{Play}}},
  isbn = {978-1-4503-6688-5},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\GV6NV3EP\3311350.3347166.pdf}
}

@article{lankEndpointPredictionUsing2007,
  title = {Endpoint Prediction Using Motion Kinematics},
  author = {Lank, Edward and Cheng, Yi-Chun Nikko and Ruiz, Jaime},
  date = {2007-04-29},
  pages = {637--646},
  doi = {10.1145/1240624.1240724},
  abstract = {Recently proposed novel interaction techniques such as cursor jumping [1] and target expansion for tiled arrangements [13] are predicated on an ability to effectively estimate the endpoint of an input gesture prior to its completion. However, current endpoint estimation techniques lack the precision to make these interaction techniques possible. To address a recognized lack of effective endpoint prediction mechanisms, we propose a new technique for endpoint prediction that applies established laws of motion kinematics in a novel way to the identification of motion endpoint. The technique derives a model of speed over distance that permits extrapolation. We verify our model experimentally using stylus targeting tasks, and demonstrate that our endpoint prediction is almost twice as accurate as the previously tested technique [13] at points more than twice as distant from motion endpoint.},
  annotation = {MAG ID: 2030286026}
}

@article{liWebPageDesign1998,
  title = {Web {{Page Design}} and {{Graphic Use}} of Three {{U}}.{{S}}. {{Newspapers}}},
  author = {Li, Xigen},
  date = {1998-06},
  journaltitle = {Journalism \& Mass Communication Quarterly},
  shortjournal = {Journalism \& Mass Communication Quarterly},
  volume = {75},
  number = {2},
  pages = {353--365},
  issn = {1077-6990, 2161-430X},
  doi = {10.1177/107769909807500210},
  url = {http://journals.sagepub.com/doi/10.1177/107769909807500210},
  urldate = {2023-06-23},
  abstract = {A content analysis of three U.S. Internet newspapers has found that Internet newspapers gave more priority to providing textual information than graphic information, and large graphics were more likely to appear on homepages than on frontpages and news article pages. The news links and the multiple communication channels adopted by Internet newspapers in web page design created a new environment of communication, involving more than host newspaper and initial audience. With interconnected links, the traditional one-to-many newspaper publishing process turned into many-to-many communication centered with and facilitated by the host Internet newspapers. The interconnected news links brought in audience participation in producing newspaper content and providing information beyond the original newspaper content, which demonstrates a shift of balance of communicative power from sender to receiver.},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\WMQDBHRH\Li - 1998 - Web Page Design and Graphic Use of three U.S. News.pdf}
}

@inproceedings{luroComparativeStudyEye2019,
  title = {A Comparative Study of Eye Tracking and Hand Controller for Aiming Tasks in Virtual Reality},
  booktitle = {Proceedings of the 11th {{ACM Symposium}} on {{Eye Tracking Research}} \& {{Applications}}},
  author = {Luro, Francisco Lopez and Sundstedt, Veronica},
  date = {2019-06-25},
  pages = {1--9},
  publisher = {{ACM}},
  location = {{Denver Colorado}},
  doi = {10.1145/3317956.3318153},
  url = {https://dl.acm.org/doi/10.1145/3317956.3318153},
  urldate = {2023-06-05},
  eventtitle = {{{ETRA}} '19: 2019 {{Symposium}} on {{Eye Tracking Research}} and {{Applications}}},
  isbn = {978-1-4503-6709-7},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\YHG5SL9M\Luro und Sundstedt - 2019 - A comparative study of eye tracking and hand contr.pdf}
}

@article{matsumuraActivePassengeringSupporting2018,
  title = {On {{Active Passengering}}: {{Supporting In-Car Experiences}}},
  shorttitle = {On {{Active Passengering}}},
  author = {Matsumura, Kohei and Kirk, David S.},
  date = {2018-01-08},
  journaltitle = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  shortjournal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {1},
  number = {4},
  pages = {1--23},
  issn = {2474-9567},
  doi = {10.1145/3161176},
  url = {https://dl.acm.org/doi/10.1145/3161176},
  urldate = {2023-05-31},
  abstract = {We describe the development of an interactive car window system designed to support passengers in engaging with the external environment during a journey. Through advances in embedded digital technologies, cars increasingly have a potential to become interactive spaces, in which passengers will find it possible to interact with the external environment through in-car interfaces. However the utility and benefit of such interactive systems for passengers has not been well studied. There is a need therefore, to study the design and use of these technologies, as they are emerging. We thus investigated how digital technology might support passengers' interactions with the external environment. Through a focus group (n=6) and interviews (n=5) we investigated passengers' attitudes towards, and practices during, ordinary car journeys. From this scoping study we formulated five design considerations for designing/implementing a prototype interactive car-window system. This system was then evaluated through an in-lab user study (n=8). Qualitative thematic analysis of interviews during the user study suggested a variety of orientations towards ‘passengering’, the act of being a passenger, on a journey. Herein we critically examine the role of our interactive technology in supporting desired experiences of ‘active passengering’.},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\6NEQVR6A\Matsumura und Kirk - 2018 - On Active Passengering Supporting In-Car Experien.pdf}
}

@inproceedings{mayerEffectRoadBumps2018,
  title = {The {{Effect}} of {{Road Bumps}} on {{Touch Interaction}} in {{Cars}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Mayer, Sven and Le, Huy Viet and Nesti, Alessandro and Henze, Niels and Bülthoff, Heinrich H. and Chuang, Lewis L.},
  year = {2018},
  date = {2018-09-23},
  pages = {85--93},
publisher={ACM},
address={New York, NY, USA},
  doi = {10.1145/3239060.3239071},
  url = {https://dl.acm.org/doi/10.1145/3239060.3239071},
  urldate = {2023-06-05},
  eventtitle = {{{AutomotiveUI}} '18: 10th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-5946-7},
  langid = {english},
  keywords = {Notes Done,Reading Done},
  file = {C:\Users\msasalo\Zotero\storage\78TF3ZRD\Mayer et al. - 2018 - The Effect of Road Bumps on Touch Interaction in C.pdf}
}

@article{mcguffinFittsLawExpanding2005,
  title = {Fitts' Law and Expanding Targets: {{Experimental}} Studies and Designs for User Interfaces},
  author = {McGuffin, Michael J. and Balakrishnan, Ravin},
  date = {2005-12-01},
  journaltitle = {ACM Transactions on Computer-Human Interaction},
  volume = {12},
  number = {4},
  pages = {388--422},
  doi = {10.1145/1121112.1121115},
  abstract = {Recently, there has been renewed interest in techniques for facilitating the selection of user interface widgets or other on-screen targets with a pointing device. We report research into using target expansion for facilitating selection. Widgets that expand or grow in response to the user's focus of attention allow for a reduced initial size which can help optimize screen space use and may be easier to select than targets that do not expand. However, selection performance could plausibly suffer from a decreased initial widget size. We describe an experiment in which users select a single, isolated target button that expands just before it is selected. Our results show that users benefit from target expansion even if the target only begins expanding after 90p of the distance to the target has been travelled. Furthermore, our results suggest that, for sufficiently large ID values, users are able to take approximately full advantage of the expanded target size. For interfaces with multiple expanding widgets, however, subtle problems arise due to the collisions or overlap that may occur between adjacent expanding widgets. We give a detailed examination of the issues involved in both untiled and tiled multiple expanding targets and present various design strategies for improving their performance.},
  keywords = {currentlyReading,ToDo},
  annotation = {MAG ID: 2096316847},
  file = {C:\Users\msasalo\Zotero\storage\8HYCPGV7\McGuffin und Balakrishnan - 2005 - Fitts' law and expanding targets Experimental stu.pdf}
}

@inproceedings{medeirosBenefitsPassiveHaptics2023,
  title = {The {{Benefits}} of {{Passive Haptics}} and {{Perceptual Manipulation}} for {{Extended Reality Interactions}} in {{Constrained Passenger Spaces}}},
  author = {Medeiros, Daniel and Wilson, Graham and Mcgill, Mark and Brewster, Stephen Anthony},
  date = {2023-04-19},
  pages = {1--19},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3581079},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581079},
  urldate = {2023-05-07},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\XVEC4ZYP\3544548.3581079}
}

@article{meyerOptimalityHumanMotor1988,
  title = {Optimality in Human Motor Performance: {{Ideal}} Control of Rapid Aimed Movements.},
  author = {Meyer, David E. and Abrams, Richard A. and Kornblum, Sylvan and {Charles E. Wright} and Wright, Charles E. and Smith, Jason E and Smith, J. E. K. and Smith, J. E. Keith},
  date = {1988-01-01},
  journaltitle = {Psychological Review},
  volume = {95},
  number = {3},
  eprint = {3406245},
  eprinttype = {pmid},
  pages = {340--370},
  doi = {10.1037/0033-295x.95.3.340},
  abstract = {A stochastic optimized-submovement model is proposed for Pitts' law, the classic logarithmic tradeoff between the duration and spatial precision of rapid aimed movements. According to the model, an aimed movement toward a specified target region involves a primary submovement and an optional secondary corrective submovement. The submovements are assumed to be programmed such that they minimize average total movement time while maintaining a high frequency of target hits. The programming process achieves this minimization by optimally adjusting the average magnitudes and durations of noisy neuromotor force pulses used to generate the submovements. Numerous results from the literature on human motor performance may be explained in these terms. Two new experiments on rapid wrist rotations yield additional support for the stochastic optimizedsubmovement model. Experiment 1 revealed that the mean durations of primary submovements and of secondary submovements, not just average total movement times, conform to a square-root approximation of Pitts' law derived from the model. Also, the spatial endpoints of primary submovements have standard deviations that increase linearly with average primary-submovement velocity, and the average primary-submovement velocity influences the relative frequencies of secondary submovements, as predicted by the model. During Experiment 2, these results were replicated and extended under conditions in which subjects made movements without concurrent visual feedback. This replication suggests that submovement optimization may be a pervasive property of movement production. The present conceptual framework provides insights into principles of motor performance, and it links the study of physical action to research on sensation, perception, and cognition, where psychologists have been concerned for some time about the degree to which mental processes incorporate rational and normative rules. An enduring issue in the study of the human mind concerns of mathematical probability theory and statistical decision thethe rationality and optimality of the mental processes that guide ory (e.g., see Edwards, 1961; Edwards, Lindman, \& Savage,},
  annotation = {MAG ID: 2135101375}
}

@article{monteiroEvaluationHandsFreeVR2023,
  title = {Evaluation of {{Hands-Free VR Interaction Methods During}} a {{Fitts}}’ {{Task}}: {{Efficiency}} and {{Effectiveness}}},
  shorttitle = {Evaluation of {{Hands-Free VR Interaction Methods During}} a {{Fitts}}’ {{Task}}},
  author = {Monteiro, Pedro and Gonçalves, Guilherme and Peixoto, Bruno and Melo, Miguel and Bessa, Maximino},
  date = {2023},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {11},
  pages = {70898--70911},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3293057},
  url = {https://ieeexplore.ieee.org/document/10176123/},
  urldate = {2023-10-18},
  keywords = {Notes Done,Reading Done},
  file = {C:\Users\msasalo\Zotero\storage\WBRX4FCY\Monteiro et al. - 2023 - Evaluation of Hands-Free VR Interaction Methods Du.pdf}
}

@article{murataImprovementPointingTime1998,
  title = {Improvement of {{Pointing Time}} by {{Predicting Targets}} in {{Pointing With}} a {{PC Mouse}}},
  author = {Murata, Atsuo},
  date = {1998-03-01},
  journaltitle = {International Journal of Human-computer Interaction},
  volume = {10},
  number = {1},
  pages = {23--32},
  doi = {10.1207/s15327590ijhc1001_2},
  abstract = {A method was proposed for the prediction of a target to which a user is to point with a mouse on the basis of the trajectory of the mouse cursor. An empirical study was carried out in order to evaluate the validity of the proposed prediction algorithm to reduce the pointing time with the prediction accuracy remaining high. The effects of the distance between the edges of two adjacent targets and the position of the indicated target on the prediction accuracy were investigated. Pointing with no prediction mode was also conducted. In the prediction method, the angle between the cursor movement vector and the vector that connected the current cursor position and the center of each target was calculated respectively at various sampling frequencies and intervals, and the minimum cumulative value was determined as the prediction target. The pointing time of the prediction method was shown to be less than that of the control condition for various combinations of the sampling interval and frequency. The trade-off...},
  annotation = {MAG ID: 2122531973}
}

@inproceedings{niyazovUserDrivenConstraintsLayout2023,
  title = {User-{{Driven Constraints}} for {{Layout Optimisation}} in {{Augmented Reality}}},
  author = {Niyazov, Aziz and Ens, Barrett and Satriadi, Kadek Ananta and Mellado, Nicolas and Barthe, Loic and Dwyer, Tim and Serrano, Marcos},
  date = {2023-04-19},
  pages = {1--16},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3580873},
  url = {https://dl.acm.org/doi/10.1145/3544548.3580873},
  urldate = {2023-05-07},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\V8YCVKZF\3544548.3580873}
}

@inproceedings{pampelFittsGoesAutobahn2019,
author = {Pampel, Sanna M. and Burnett, Gary and Hare, Chrisminder and Singh, Harpreet and Shabani, Arber and Skrypchuk, Lee and Mouzakitis, Alex},
title = {Fitts Goes Autobahn: Assessing the Visual Demand of Finger-Touch Pointing Tasks in an On-Road Study},
year = {2019},
isbn = {9781450368841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342197.3344538},
doi = {10.1145/3342197.3344538},
abstract = {The visual demand of finger-touch based interactions with touch screens has been increasingly modelled using Fitts' Law. With respect to driving, these models facilitate the prediction of mean glance duration and total glance time with an index of difficulty based on target size and location. Strong relationships between measures have been found in the controlled conditions of driving simulators. The present study aimed to validate such models in naturalistic conditions. Nineteen experienced drivers carried out a range of touchscreen button-press tasks in an instrumented car on a UK motorway. In contrast with previous simulator-based work, our on-road data produced much weaker relationships between the index of difficulty and glance times. The model improved by focusing on tasks that required one glance only. Limitations of Fitts' Law in the more complex and dynamic real-world driving environment are discussed, as are the potential drawbacks of driving simulators for conducting visual demand research.},
booktitle = {Proceedings of the 11th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {254–261},
numpages = {8},
keywords = {Fitts' Law, driving, human-computer interaction, pointing, touchscreen, visual demand},
location = {Utrecht, Netherlands},
series = {AutomotiveUI '19}
}

@article{plaumann2018AccurateCursorlessPointing,
  title = {Towards Accurate Cursorless Pointing: The Effects of Ocular Dominance and Handedness},
  shorttitle = {Towards Accurate Cursorless Pointing},
  author = {Plaumann, Katrin and Weing, Matthias and Winkler, Christian and Müller, Michael and Rukzio, Enrico},
  date = {2018-08},
  journaltitle = {Personal and Ubiquitous Computing},
  shortjournal = {Pers Ubiquit Comput},
  volume = {22},
  number = {4},
  pages = {633--646},
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-017-1100-7},
  url = {http://link.springer.com/10.1007/s00779-017-1100-7},
  urldate = {2023-10-26},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\Q46YSPRA\Plaumann et al. - 2018 - Towards accurate cursorless pointing the effects .pdf}
}

@article{plaumannImprovingInputAccuracy2018,
  title = {Improving {{Input Accuracy}} on {{Smartphones}} for {{Persons}} Who Are {{Affected}} by {{Tremor}} Using {{Motion Sensors}}},
  author = {Plaumann, Katrin and Babic, Milos and Drey, Tobias and Hepting, Witali and Stooss, Daniel and Rukzio, Enrico},
  date = {2018-01-08},
  journaltitle = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  shortjournal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {1},
  number = {4},
  pages = {1--30},
  issn = {2474-9567},
  doi = {10.1145/3161169},
  url = {https://dl.acm.org/doi/10.1145/3161169},
  urldate = {2023-06-05},
  abstract = {Having a hand tremor often complicates interactions with touchscreens on mobile devices. Due to the uncontrollable oscillations of both hands, hitting targets can be hard, and interaction can be slow. Correcting input needs additional time and mental effort. We propose a method for automatically correcting such inputs based on motion data, gathered both with the devices' sensors and a small wearable sensor on the finger used for tapping. The development was informed by interviews with persons with tremor. Two empirical studies showed that our method, involving both smartphone and finger motion sensors without changing the user interface, allows users with tremor to select objects with up to 40 \% fewer misses.},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\VI4HXYMA\Plaumann et al. - 2018 - Improving Input Accuracy on Smartphones for Person.pdf}
}

@inproceedings{plaumannImprovingTouchscreenInput2016,
  title = {Towards Improving Touchscreen Input Speed and Accuracy on Smartphones for Tremor Affected Persons},
  booktitle = {Proceedings of the 2016 {{ACM International Joint Conference}} on {{Pervasive}} and {{Ubiquitous Computing}}: {{Adjunct}}},
  author = {Plaumann, Katrin and Babic, Milos and Drey, Tobias and Hepting, Witali and Stooß, Daniel and Rukzio, Enrico},
  date = {2016-09-12},
  pages = {357--360},
  publisher = {{ACM}},
  location = {{Heidelberg Germany}},
  doi = {10.1145/2968219.2971396},
  url = {https://dl.acm.org/doi/10.1145/2968219.2971396},
  urldate = {2023-08-31},
  eventtitle = {{{UbiComp}} '16: {{The}} 2016 {{ACM International Joint Conference}} on {{Pervasive}} and {{Ubiquitous Computing}}},
  isbn = {978-1-4503-4462-3},
  langid = {english},
  keywords = {ToDo}
}

@inproceedings{pollmannHowWorkCar2019,
  title = {How to {{Work}} in the {{Car}} of the {{Future}}?: {{A Neuroergonomical Study Assessing Concentration}}, {{Performance}} and {{Workload Based}} on {{Subjective}}, {{Behavioral}} and {{Neurophysiological Insights}}},
  shorttitle = {How to {{Work}} in the {{Car}} of the {{Future}}?},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Pollmann, Kathrin and Stefani, Oilver and Bengsch, Amelie and Peissner, Matthias and Vukelić, Mathias},
  date = {2019-05-02},
  pages = {1--14},
  publisher = {{ACM}},
  location = {{Glasgow Scotland Uk}},
  doi = {10.1145/3290605.3300284},
  url = {https://dl.acm.org/doi/10.1145/3290605.3300284},
  urldate = {2023-09-02},
  eventtitle = {{{CHI}} '19: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-5970-2},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\TAP9UDHA\Pollmann et al. - 2019 - How to Work in the Car of the Future A Neuroergo.pdf}
}

@inproceedings{qianEyesDonHave2017,
  title = {The Eyes Don't Have It: An Empirical Comparison of Head-Based and Eye-Based Selection in Virtual Reality},
  shorttitle = {The Eyes Don't Have It},
  booktitle = {Proceedings of the 5th {{Symposium}} on {{Spatial User Interaction}}},
  author = {Qian, Yuan Yuan and Teather, Robert J.},
  date = {2017-10-16},
  pages = {91--98},
  publisher = {{ACM}},
  location = {{Brighton United Kingdom}},
  doi = {10.1145/3131277.3132182},
  url = {https://dl.acm.org/doi/10.1145/3131277.3132182},
  urldate = {2023-06-05},
  eventtitle = {{{SUI}} '17: {{Symposium}} on {{Spatial User Interaction}}},
  isbn = {978-1-4503-5486-8},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\MDMGKVI2\Qian und Teather - 2017 - The eyes don't have it an empirical comparison of.pdf}
}

@inproceedings{rahmatiNoShakeContentStabilization2009,
  title = {{{NoShake}}: {{Content}} Stabilization for Shaking Screens of Mobile Devices},
  shorttitle = {{{NoShake}}},
  booktitle = {2009 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications}}},
  author = {Rahmati, Ahmad and Shepard, Clayton and Zhong, Lin},
  date = {2009-03},
  pages = {1--6},
  publisher = {{IEEE}},
  location = {{Galveston, TX, USA}},
  doi = {10.1109/PERCOM.2009.4912750},
  url = {http://ieeexplore.ieee.org/document/4912750/},
  urldate = {2023-06-05},
  eventtitle = {2009 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications}} ({{PerCom}})},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\8NNRPTTS\Rahmati et al. - 2009 - NoShake Content stabilization for shaking screens.pdf}
}

@inproceedings{rienerStandardizationIncarGesture2013,
  title = {Standardization of the In-Car Gesture Interaction Space},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Riener, A. and Ferscha, A. and Bachmair, F. and Hagmüller, P. and Lemme, A. and Muttenthaler, D. and Pühringer, D. and Rogner, H. and Tappe, A. and Weger, F.},
  date = {2013-10-28},
  pages = {14--21},
  publisher = {{ACM}},
  location = {{Eindhoven Netherlands}},
  doi = {10.1145/2516540.2516544},
  url = {https://dl.acm.org/doi/10.1145/2516540.2516544},
  urldate = {2023-09-02},
  eventtitle = {{{AutomotiveUI}} '13: {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-2478-6},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\VVUZT62X\Riener et al. - 2013 - Standardization of the in-car gesture interaction .pdf}
}

@inproceedings{rietzlerBreakingTrackingEnabling2018,
  title = {Breaking the {{Tracking}}: {{Enabling Weight Perception}} Using {{Perceivable Tracking Offsets}}},
  shorttitle = {Breaking the {{Tracking}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rietzler, Michael and Geiselhart, Florian and Gugenheimer, Jan and Rukzio, Enrico},
  date = {2018-04-19},
  pages = {1--12},
  publisher = {{ACM}},
  location = {{Montreal QC Canada}},
  doi = {10.1145/3173574.3173702},
  url = {https://dl.acm.org/doi/10.1145/3173574.3173702},
  urldate = {2023-06-06},
  eventtitle = {{{CHI}} '18: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-5620-6},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\3DTS52R4\Rietzler et al. - 2018 - Breaking the Tracking Enabling Weight Perception .pdf}
}

@article{riexingerKooperativeFabrikplanungMit2023,
  title = {Kooperative {{Fabrikplanung}} Mit {{Mixed Reality}}/{{Cooperative}} Factory Planning Based on Mixed Reality},
  author = {Riexinger, Günther and Kaucher, Christian and Sasalovici, Markus and Bauernhansl, Thomas},
  date = {2023},
  journaltitle = {wt Werkstattstechnik online},
  shortjournal = {wt},
  volume = {113},
  number = {04},
  pages = {135--139},
  issn = {1436-4980},
  doi = {10.37544/1436-4980-2023-04-35},
  url = {https://elibrary.vdi-verlag.de/index.php?doi=10.37544/1436-4980-2023-04-35},
  urldate = {2023-05-08},
  abstract = {Anwendungen aus dem Bereich „Mixed Reality“ (MR) bieten große Potenziale bei der Lösung komplexer Planungsaufgaben. In diesem Beitrag wird die MR-Anwendung „HoloLayouts“ zur Fabrikplanung mit der Microsoft „HoloLens 2“ vorgestellt. Diese erlaubt die direkte Zusammenarbeit bei der Entwicklung und Validierung von 3D-Produktionslayouts im Team. Im Rahmen der digitalen Transformation der Produktion bietet die Integration von 3D-Visualisierungswerkzeugen eine effiziente Unterstützung bei gemeinsamen Planungsentscheidungen.             ~             Mixed reality (MR) tools offer great potential for solving complex planning tasks in production. This article presents the MR application „HoloLayouts“ for factory planning with Microsoft „HoloLens 2“. The application enables the collaborative development and validation of production layouts in 3D. Within the context of the digital transformation of manufacturing, the integration of 3D visualization tools offers efficient support for collaborative planning decisions.},
  file = {C:\Users\msasalo\Zotero\storage\DNDUFENB\Riexinger et al. - 2023 - Kooperative Fabrikplanung mit Mixed RealityCooper.pdf}
}

@inproceedings{rixenExploringAugmentedVisual2021,
  title = {Exploring {{Augmented Visual Alterations}} in {{Interpersonal Communication}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rixen, Jan Ole and Hirzle, Teresa and Colley, Mark and Etzel, Yannick and Rukzio, Enrico and Gugenheimer, Jan},
  date = {2021-05-06},
  pages = {1--11},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445597},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445597},
  urldate = {2023-05-31},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\7KE37RHL\Rixen et al. - 2021 - Exploring Augmented Visual Alterations in Interper.pdf}
}

@inproceedings{roiderSeeYourPoint2018,
  title = {I {{See Your Point}}: {{Integrating Gaze}} to {{Enhance Pointing Gesture Accuracy While Driving}}},
  shorttitle = {I {{See Your Point}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Roider, Florian and Gross, Tom},
  year = {2018},
  date = {2018-09-23},
  pages = {351--358},
publisher={ACM},
address={New York, NY, USA},
  location = {{Toronto ON Canada}},
  doi = {10.1145/3239060.3239084},
  url = {https://dl.acm.org/doi/10.1145/3239060.3239084},
  urldate = {2023-06-05},
  eventtitle = {{{AutomotiveUI}} '18: 10th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-5946-7},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\65Y3LXGV\Roider und Gross - 2018 - I See Your Point Integrating Gaze to Enhance Poin.pdf}
}

@inproceedings{sadeghianborojeniFeelMovementReal2018,
  title = {Feel the {{Movement}}: {{Real Motion Influences Responses}} to {{Take-over Requests}} in {{Highly Automated Vehicles}}},
  shorttitle = {Feel the {{Movement}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sadeghian Borojeni, Shadan and Boll, Susanne C.J. and Heuten, Wilko and Bülthoff, Heinrich H. and Chuang, Lewis},
  date = {2018-04-21},
  pages = {1--13},
  publisher = {{ACM}},
  location = {{Montreal QC Canada}},
  doi = {10.1145/3173574.3173820},
  url = {https://dl.acm.org/doi/10.1145/3173574.3173820},
  urldate = {2023-08-21},
  eventtitle = {{{CHI}} '18: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-5620-6},
  langid = {english},
  keywords = {Notes Done,Reading Done},
  file = {C:\Users\msasalo\Zotero\storage\JREXVWSE\Sadeghian Borojeni et al. - 2018 - Feel the Movement Real Motion Influences Response.pdf}
}

@article{salmon2011EffectsMotionInvehicle,
  title = {The Effects of Motion on In-Vehicle Touch Screen System Operation: {{A}} Battle Management System Case Study},
  author = {Salmon, Paul M. and Lenné, Michael G. and Triggs, Tom and Goode, Natassia and Cornelissen, Miranda and Demczuk, Victor},
  date = {2011-11-01},
  journaltitle = {Transportation Research Part F-traffic Psychology and Behaviour},
  volume = {14},
  number = {6},
  pages = {494--503},
  doi = {10.1016/j.trf.2011.08.002},
  abstract = {The use of in-vehicle touch screen devices is currently common in both military and civilian vehicles; despite this, the effects of motion on touch screen device operation within vehicles remains largely unexplored. This article describes a study that examined, using driving simulation, the influences of motion on performance, workload and usability when using a touch screen in-vehicle battle management system. Acting in the role of battle management system operator, 20 participants undertook four simulated drives, two under high motion (representative of an unsealed road) and two under normal motion (representative of a sealed road), whilst performing various battle management tasks. In the high motion condition, lower accuracy and longer task completion times were found, along with greater levels of subjective and physiological workload and lower levels of perceived device usability, when compared to the normal motion condition. The findings indicate that, compared to normal motion, the high motion condition impaired key aspects of battle management system operation. In closing, the importance of considering motion and its effects during touch screen system design is discussed.},
  annotation = {MAG ID: 1974396900},
  file = {C:\Users\msasalo\Zotero\storage\463HDPBG\Salmon et al. - 2011 - The effects of motion on in-vehicle touch screen s.pdf}
}

@incollection{sasaloviciAlgorithmischAutomatisierteArtwork2022,
  title = {Algorithmisch automatisierte Artwork Generation im Netflix Empfehlungssystem},
  booktitle = {KI in der digitalisierten Medienwirtschaft},
  author = {Sasalovici, Markus},
  editor = {Zydorek, Christoph},
  date = {2022},
  pages = {57--85},
  publisher = {{Springer Fachmedien Wiesbaden}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-658-37404-4_3},
  url = {https://link.springer.com/10.1007/978-3-658-37404-4_3},
  urldate = {2023-05-09},
  isbn = {978-3-658-37403-7 978-3-658-37404-4},
  langid = {ngerman}
}

@inproceedings{schildbachInvestigatingSelectionReading2010,
  title = {Investigating Selection and Reading Performance on a Mobile Phone While Walking},
  booktitle = {Proceedings of the 12th International Conference on {{Human}} Computer Interaction with Mobile Devices and Services},
  author = {Schildbach, Bastian and Rukzio, Enrico},
  date = {2010-09-07},
  pages = {93--102},
  publisher = {{ACM}},
  location = {{Lisbon Portugal}},
  doi = {10.1145/1851600.1851619},
  url = {https://dl.acm.org/doi/10.1145/1851600.1851619},
  urldate = {2023-06-05},
  eventtitle = {{{MobileHCI}} '10: 12th {{International Conference}} on {{Human Computer Interaction}} with {{Mobile Devices}} and {{Services}}},
  isbn = {978-1-60558-835-3},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\V6YW5BTB\Schildbach und Rukzio - 2010 - Investigating selection and reading performance on.pdf}
}

@inproceedings{schonTailorTwistAssessing2023,
  title = {Tailor {{Twist}}: {{Assessing Rotational Mid-Air Interactions}} for {{Augmented Reality}}},
  shorttitle = {Tailor {{Twist}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Schön, Dominik and Kosch, Thomas and Müller, Florian and Schmitz, Martin and Günther, Sebastian and Bommhardt, Lukas and Mühlhäuser, Max},
  date = {2023-04-19},
  pages = {1--14},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3581461},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581461},
  urldate = {2023-10-11},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  keywords = {Notes Done,Reading Done},
  file = {C:\Users\msasalo\Zotero\storage\UVANRVT4\Schön et al. - 2023 - Tailor Twist Assessing Rotational Mid-Air Interac.pdf}
}

@inproceedings{schramm2023AssessingAugmentedReality,
  title = {Assessing {{Augmented Reality Selection Techniques}} for {{Passengers}} in {{Moving Vehicles}}: {{A Real-World User Study}}},
  shorttitle = {Assessing {{Augmented Reality Selection Techniques}} for {{Passengers}} in {{Moving Vehicles}}},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Schramm, Robin Connor and Sasalovici, Markus and Hildebrand, Axel and Schwanecke, Ulrich},
  year = {2023},
  date = {2023-09-18},
  pages = {22--31},
publisher={ACM},
address={New York, NY, USA},
  location = {{Ingolstadt Germany}},
  doi = {10.1145/3580585.3607152},
  url = {https://dl.acm.org/doi/10.1145/3580585.3607152},
  urldate = {2023-11-07},
  eventtitle = {{{AutomotiveUI}} '23: 15th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {9798400701054},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\W9RNJXBN\3580585.3607152.pdf}
}

@inproceedings{schroeterSocialCarNew2012,
  title = {The Social Car: New Interactive Vehicular Applications Derived from Social Media and Urban Informatics},
  shorttitle = {The Social Car},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Schroeter, Ronald and Rakotonirainy, Andry and Foth, Marcus},
  date = {2012-10-17},
  pages = {107--110},
  publisher = {{ACM}},
  location = {{Portsmouth New Hampshire}},
  doi = {10.1145/2390256.2390273},
  url = {https://dl.acm.org/doi/10.1145/2390256.2390273},
  urldate = {2023-06-25},
  eventtitle = {{{AutomotiveUI}} '12: {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-1751-1},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\2EIZDZC5\Schroeter et al. - 2012 - The social car new interactive vehicular applicat.pdf}
}

@inproceedings{serimExplicatingImplicitInteraction2019,
  title = {Explicating "{{Implicit Interaction}}": {{An Examination}} of the {{Concept}} and {{Challenges}} for {{Research}}},
  shorttitle = {Explicating "{{Implicit Interaction}}"},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Serim, Barış and Jacucci, Giulio},
  date = {2019-05-02},
  pages = {1--16},
  publisher = {{ACM}},
  location = {{Glasgow Scotland Uk}},
  doi = {10.1145/3290605.3300647},
  url = {https://dl.acm.org/doi/10.1145/3290605.3300647},
  urldate = {2023-05-31},
  eventtitle = {{{CHI}} '19: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-5970-2},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\4LP4838A\Serim und Jacucci - 2019 - Explicating Implicit Interaction An Examination.pdf}
}

@inproceedings{sidenmarkVergenceMatchingInferring2023,
  title = {Vergence {{Matching}}: {{Inferring Attention}} to {{Objects}} in {{3D Environments}} for {{Gaze-Assisted Selection}}},
  shorttitle = {Vergence {{Matching}}},
  author = {Sidenmark, Ludwig and Clarke, Christopher and Newn, Joshua and Lystbæk, Mathias N. and Pfeuffer, Ken and Gellersen, Hans},
  date = {2023-04-19},
  pages = {1--15},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3580685},
  url = {https://dl.acm.org/doi/10.1145/3544548.3580685},
  urldate = {2023-05-06},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\I9CDCA2N\VergenceMatching.pdf}
}

@article{sorokowskaPreferredInterpersonalDistances2017,
  title = {Preferred {{Interpersonal Distances}}: {{A Global Comparison}}},
  shorttitle = {Preferred {{Interpersonal Distances}}},
  author = {Sorokowska, Agnieszka and Sorokowski, Piotr and Hilpert, Peter and Cantarero, Katarzyna and Frackowiak, Tomasz and Ahmadi, Khodabakhsh and Alghraibeh, Ahmad M. and Aryeetey, Richmond and Bertoni, Anna and Bettache, Karim and Blumen, Sheyla and Błażejewska, Marta and Bortolini, Tiago and Butovskaya, Marina and Castro, Felipe Nalon and Cetinkaya, Hakan and Cunha, Diana and David, Daniel and David, Oana A. and Dileym, Fahd A. and Domínguez Espinosa, Alejandra Del Carmen and Donato, Silvia and Dronova, Daria and Dural, Seda and Fialová, Jitka and Fisher, Maryanne and Gulbetekin, Evrim and Hamamcıoğlu Akkaya, Aslıhan and Hromatko, Ivana and Iafrate, Raffaella and Iesyp, Mariana and James, Bawo and Jaranovic, Jelena and Jiang, Feng and Kimamo, Charles Obadiah and Kjelvik, Grete and Koç, Fırat and Laar, Amos and De Araújo Lopes, Fívia and Macbeth, Guillermo and Marcano, Nicole M. and Martinez, Rocio and Mesko, Norbert and Molodovskaya, Natalya and Moradi, Khadijeh and Motahari, Zahrasadat and Mühlhauser, Alexandra and Natividade, Jean Carlos and Ntayi, Joseph and Oberzaucher, Elisabeth and Ojedokun, Oluyinka and Omar-Fauzee, Mohd Sofian Bin and Onyishi, Ike E. and Paluszak, Anna and Portugal, Alda and Razumiejczyk, Eugenia and Realo, Anu and Relvas, Ana Paula and Rivas, Maria and Rizwan, Muhammad and Salkičević, Svjetlana and Sarmány-Schuller, Ivan and Schmehl, Susanne and Senyk, Oksana and Sinding, Charlotte and Stamkou, Eftychia and Stoyanova, Stanislava and Šukolová, Denisa and Sutresna, Nina and Tadinac, Meri and Teras, Andero and Tinoco Ponciano, Edna Lúcia and Tripathi, Ritu and Tripathi, Nachiketa and Tripathi, Mamta and Uhryn, Olja and Yamamoto, Maria Emília and Yoo, Gyesook and Pierce, John D.},
  date = {2017-05},
  journaltitle = {Journal of Cross-Cultural Psychology},
  shortjournal = {Journal of Cross-Cultural Psychology},
  volume = {48},
  number = {4},
  pages = {577--592},
  issn = {0022-0221, 1552-5422},
  doi = {10.1177/0022022117698039},
  url = {http://journals.sagepub.com/doi/10.1177/0022022117698039},
  urldate = {2023-05-31},
  abstract = {Human spatial behavior has been the focus of hundreds of previous research studies. However, the conclusions and generalizability of previous studies on interpersonal distance preferences were limited by some important methodological and sampling issues. The objective of the present study was to compare preferred interpersonal distances across the world and to overcome the problems observed in previous studies. We present an extensive analysis of interpersonal distances over a large data set ( N = 8,943 participants from 42 countries). We attempted to relate the preferred social, personal, and intimate distances observed in each country to a set of individual characteristics of the participants, and some attributes of their cultures. Our study indicates that individual characteristics (age and gender) influence interpersonal space preferences and that some variation in results can be explained by temperature in a given region. We also present objective values of preferred interpersonal distances in different regions, which might be used as a reference data point in future studies.},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\PLWHPD58\Sorokowska et al. - 2017 - Preferred Interpersonal Distances A Global Compar.pdf}
}

@inproceedings{sportilloImmersiveVirtualReality2017,
  title = {An {{Immersive Virtual Reality System}} for {{Semi-autonomous Driving Simulation}}: {{A Comparison}} between {{Realistic}} and 6-{{DoF Controller-based Interaction}}},
  shorttitle = {An {{Immersive Virtual Reality System}} for {{Semi-autonomous Driving Simulation}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Computer}} and {{Automation Engineering}}},
  author = {Sportillo, Daniele and Paljic, Alexis and Boukhris, Mehdi and Fuchs, Philippe and Ojeda, Luciano and Roussarie, Vincent},
  date = {2017-02-18},
  pages = {6--10},
  publisher = {{ACM}},
  location = {{Sydney Australia}},
  doi = {10.1145/3057039.3057079},
  url = {https://dl.acm.org/doi/10.1145/3057039.3057079},
  urldate = {2023-06-05},
  eventtitle = {{{ICCAE}} '17: 9th {{International Conference}} on {{Computer}} and {{Automation Engineering}}},
  isbn = {978-1-4503-4809-6},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\E8P7EGF8\3057039.3057079.pdf}
}

@article{stampfMoveConnectInteract2017,
  title = {Move, {{Connect}}, {{Interact}}: {{Introducing}} a {{Design Space}} for {{Cross-Traffic Interaction}}},
  author = {Stampf, Annika and Meinhardt, Luca-Maxim and Colley, Mark},
  date = {2017},
  abstract = {Ever-increasing connectivity has opened up a plethora of interaction opportunities in traffic. Despite various design spaces existing for subareas such as in-vehicle interaction or eHMI, a comprehensive view of interaction possibilities across traffic participants remains absent. To address this gap, we developed a design space for Cross-Traffic Interaction, based on a focus group with HCI experts, encompassing the three dimensions: (1) interaction partners, (2) their traffic situations, and (3) the relationship of their interaction. Through the classification of 80 publications in this field, we identify trends and research gaps, both within current research fields and for new research areas. Illustrating the practical application of our design space, we further present three prototypical applications. A study evaluation (N=12) shows [TODO]. The proposed design space for Cross-Traffic Interaction provides a foundation for understanding and exploring the diverse possibilities of interactions in traffic, offering valuable insights to researchers, designers, and developers for creating innovative and user-friendly systems that enhance safety and efficiency on the roads.},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\RN9UMUDV\Stampf et al. - 2017 - Move, Connect, Interact Introducing a Design Spac.pdf}
}

@inproceedings{steinbergerZombiesRoadHolistic2015,
  title = {Zombies on the Road: A Holistic Design Approach to Balancing Gamification and Safe Driving},
  shorttitle = {Zombies on the Road},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Steinberger, Fabius and Schroeter, Ronald and Lindner, Verena and Fitz-Walter, Zachary and Hall, Joshua and Johnson, Daniel},
  date = {2015-09},
  pages = {320--327},
  publisher = {{ACM}},
  location = {{Nottingham United Kingdom}},
  doi = {10.1145/2799250.2799260},
  url = {https://dl.acm.org/doi/10.1145/2799250.2799260},
  urldate = {2023-06-25},
  eventtitle = {{{AutomotiveUI}} '15: {{The}} 7th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-3736-6},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\PXDCATX6\Steinberger et al. - 2015 - Zombies on the road a holistic design approach to.pdf}
}

@inproceedings{stevensUsingTimeSpace2019,
  title = {Using {{Time}} and {{Space Efficiently}} in {{Driverless Cars}}: {{Findings}} of a {{Co-Design Study}}},
  shorttitle = {Using {{Time}} and {{Space Efficiently}} in {{Driverless Cars}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Stevens, Gunnar and Bossauer, Paul and Vonholdt, Stephanie and Pakusch, Christina},
  date = {2019-05-02},
  pages = {1--14},
  publisher = {{ACM}},
  location = {{Glasgow Scotland Uk}},
  doi = {10.1145/3290605.3300635},
  url = {https://dl.acm.org/doi/10.1145/3290605.3300635},
  urldate = {2023-05-31},
  eventtitle = {{{CHI}} '19: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-5970-2},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\5BH6MVBB\Stevens et al. - 2019 - Using Time and Space Efficiently in Driverless Car.pdf}
}

@inproceedings{streliHOOVHandOutOfView2023,
  title = {{{HOOV}}: {{Hand Out-Of-View Tracking}} for {{Proprioceptive Interaction}} Using {{Inertial Sensing}}},
  shorttitle = {{{HOOV}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Streli, Paul and Armani, Rayan and Cheng, Yi Fei and Holz, Christian},
  date = {2023-04-19},
  pages = {1--16},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3581468},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581468},
  urldate = {2023-06-05},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\WRLNT5RD\3544548.3581468.pdf}
}

@inproceedings{togwellIncARGamingExploring2022,
  title = {In-{{cAR Gaming}}: {{Exploring}} the Use of {{AR}} Headsets to {{Leverage Passenger Travel Environments}} for {{Mixed Reality Gameplay}}},
  shorttitle = {In-{{cAR Gaming}}},
  author = {Togwell, Henry and McGill, Mark and Wilson, Graham and Medeiros, Daniel and Brewster, Stephen Anthony},
  date = {2022-04-27},
  pages = {1--7},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491101.3519741},
  url = {https://dl.acm.org/doi/10.1145/3491101.3519741},
  urldate = {2023-05-07},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9156-6},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\PAVSZBE2\3491101.3519741.pdf}
}

@inproceedings{triantafyllidisChallengesModelingHuman2021,
  title = {The {{Challenges}} in {{Modeling Human Performance}} in {{3D Space}} with {{Fitts}}’ {{Law}}},
  booktitle = {Extended {{Abstracts}} of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Triantafyllidis, Eleftherios and Li, Zhibin},
  date = {2021-05-08},
  pages = {1--9},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411763.3443442},
  url = {https://dl.acm.org/doi/10.1145/3411763.3443442},
  urldate = {2023-10-18},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8095-9},
  langid = {english},
  keywords = {Notes Done,Reading Done},
  file = {C:\Users\msasalo\Zotero\storage\ELWL3NTS\Triantafyllidis und Li - 2021 - The Challenges in Modeling Human Performance in 3D.pdf}
}

@inproceedings{tsengFingerMapperMappingFinger2023,
 author = {Tseng, Wen-Jie and Huron, Samuel and Lecolinet, Eric and Gugenheimer, Jan},
title = {FingerMapper: Mapping Finger Motions onto Virtual Arms to Enable Safe Virtual Reality Interaction in Confined Spaces},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580736},
doi = {10.1145/3544548.3580736},
abstract = {Whole-body movements enhance the presence and enjoyment of Virtual Reality (VR) experiences. However, using large gestures is often uncomfortable and impossible in confined spaces (e.g., public transport). We introduce FingerMapper, mapping small-scale finger motions onto virtual arms and hands to enable whole-body virtual movements in VR. In a first target selection study (n=13) comparing FingerMapper to hand tracking and ray-casting, we found that FingerMapper can significantly reduce physical motions and fatigue while having a similar degree of precision. In a consecutive study (n=13), we compared FingerMapper to hand tracking inside a confined space (the front passenger seat of a car). The results showed participants had significantly higher perceived safety and fewer collisions with FingerMapper while preserving a similar degree of presence and enjoyment as hand tracking. Finally, we present three example applications demonstrating how FingerMapper could be applied for locomotion and interaction for VR in confined spaces.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {874},
numpages = {14},
keywords = {Body Re-Association in VR, Confined Spaces, FingerMapper},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{wagnerFittsLawStudy2023,
  author = {Wagner, Uta and Lystb\ae{}k, Mathias N. and Manakhov, Pavel and Gr\o{}nb\ae{}k, Jens Emil Sloth and Pfeuffer, Ken and Gellersen, Hans},
title = {A Fitts’ Law Study of Gaze-Hand Alignment for Selection in 3D User Interfaces},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581423},
doi = {10.1145/3544548.3581423},
abstract = {Gaze-Hand Alignment has recently been proposed for multimodal selection in 3D. The technique takes advantage of gaze for target pre-selection, as it naturally precedes manual input. Selection is then completed when manual input aligns with gaze on the target, without need for an additional click method. In this work we evaluate two alignment techniques, Gaze&Finger and Gaze&Handray, combining gaze with image plane pointing versus raycasting, in comparison with hands-only baselines and Gaze&Pinch as established multimodal technique. We used Fitts’ Law study design with targets presented at different depths in the visual scene, to assess effect of parallax on performance. The alignment techniques outperformed their respective hands-only baselines. Gaze&Finger is efficient when targets are close to the image plane but less performant with increasing target depth due to parallax.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {252},
numpages = {15},
keywords = {augmented reality, eye-tracking, gaze interaction, menu selection, mid-air gestures, pointing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{weiPredictingGazebasedTarget2023,
  title = {Predicting {{Gaze-based Target Selection}} in {{Augmented Reality Headsets}} Based on {{Eye}} and {{Head Endpoint Distributions}}},
  author = {Wei, Yushi and Shi, Rongkai and Yu, Difeng and Wang, Yihong and Li, Yue and Yu, Lingyun and Liang, Hai-Ning},
  date = {2023-04-19},
  pages = {1--14},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3581042},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581042},
  urldate = {2023-05-07},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\FYWJH2SQ\3544548.3581042}
}

@article{wilsonLackRestraintComparing2023,
  title = {A {{Lack}} of {{Restraint}}: {{Comparing Virtual Reality Interaction Techniques}} for {{Constrained Transport Seating}}},
  shorttitle = {A {{Lack}} of {{Restraint}}},
  author = {Wilson, Graham and McGill, Mark and Medeiros, Daniel and Brewster, Stephen},
  date = {2023-05},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  volume = {29},
  number = {5},
  pages = {2390--2400},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2023.3247084},
  url = {https://ieeexplore.ieee.org/document/10058530/},
  urldate = {2023-05-31},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\HD9XGEMX\Wilson et al. - 2023 - A Lack of Restraint Comparing Virtual Reality Int.pdf}
}

@inproceedings{winklerInvestigatingMidairPointing2012,
  title = {Investigating Mid-Air Pointing Interaction for Projector Phones},
  booktitle = {Proceedings of the 2012 {{ACM}} International Conference on {{Interactive}} Tabletops and Surfaces},
  author = {Winkler, Christian and Pfeuffer, Ken and Rukzio, Enrico},
  date = {2012-11-11},
  pages = {85--94},
  publisher = {{ACM}},
  location = {{Cambridge Massachusetts USA}},
  doi = {10.1145/2396636.2396650},
  url = {https://dl.acm.org/doi/10.1145/2396636.2396650},
  urldate = {2023-10-06},
  eventtitle = {{{ITS}}'12: {{Interactive Tabletops}} and {{Surfaces}}},
  isbn = {978-1-4503-1209-7},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\M5PMX7DV\Winkler et al. - 2012 - Investigating mid-air pointing interaction for pro.pdf}
}

@inproceedings{wolfUnderstandingHeisenbergEffect2020,
  title = {Understanding the {{Heisenberg Effect}} of {{Spatial Interaction}}: {{A Selection Induced Error}} for {{Spatially Tracked Input Devices}}},
  shorttitle = {Understanding the {{Heisenberg Effect}} of {{Spatial Interaction}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wolf, Dennis and Gugenheimer, Jan and Combosch, Marco and Rukzio, Enrico},
  date = {2020-04-21},
  pages = {1--10},
  publisher = {{ACM}},
  location = {{Honolulu HI USA}},
  doi = {10.1145/3313831.3376876},
  url = {https://dl.acm.org/doi/10.1145/3313831.3376876},
  urldate = {2023-06-05},
  eventtitle = {{{CHI}} '20: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-6708-0},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\2K4AGKC8\Wolf et al. - 2020 - Understanding the Heisenberg Effect of Spatial Int.pdf}
}

@inproceedings{xieInfluenceDisplayLocation2023,
  title = {The {{Influence}} of {{Display Location}} on {{Motion Sickness}}: {{An Analysis Based}} on 6-{{DoF SVC Model}}},
  shorttitle = {The {{Influence}} of {{Display Location}} on {{Motion Sickness}}},
  author = {Xie, Weiyin and Yan, Song and Huang, Chunxi and He, Dengbo},
  date = {2023-09-18},
  pages = {186--190},
  publisher = {{ACM}},
  location = {{Ingolstadt Germany}},
  doi = {10.1145/3581961.3609896},
  url = {https://dl.acm.org/doi/10.1145/3581961.3609896},
  urldate = {2023-09-22},
  eventtitle = {{{AutomotiveUI}} '23: 15th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {9798400701122},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\CCT2BTJK\3581961.3609896}
}

@article{yamanakaRethinkingDualGaussian2020,
  title = {Rethinking the {{Dual Gaussian Distribution Model}} for {{Predicting Touch Accuracy}} in {{On-screen-start Pointing Tasks}}},
  author = {Yamanaka, Shota and Usuba, Hiroki},
  date = {2020},
  volume = {4},
  pages = {1--20},
  abstract = {The dual Gaussian distribution hypothesis has been used to predict the success rate of target pointing on touchscreens. Bi and Zhai evaluated their success-rate prediction model in off-screen-start pointing tasks. However, we found that their prediction model could also be used for on-screen-start pointing tasks. We discuss the reasons why and empirically validate our hypothesis in a series of four experiments with various target sizes and distances. The prediction accuracy of Bi and Zhai's model was high in all of the experiments, with a 10-point absolute (or 14.9\% relative) prediction error at worst. Also, we show that there is no clear benefit to integrating the target distance when predicting the endpoint variability and success rate.},
  annotation = {MAG ID: 3093509095}
}

@inproceedings{zennerBlinkSuppressedHandRedirection2021,
  title = {Blink-{{Suppressed Hand Redirection}}},
  booktitle = {2021 {{IEEE Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  author = {Zenner, Andre and Regitz, Kora Persephone and Kruger, Antonio},
  date = {2021-03},
  pages = {75--84},
  publisher = {{IEEE}},
  location = {{Lisboa, Portugal}},
  doi = {10.1109/VR50410.2021.00028},
  url = {https://ieeexplore.ieee.org/document/9417723/},
  urldate = {2023-06-06},
  eventtitle = {2021 {{IEEE Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  isbn = {978-1-66541-838-6},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\I2Z62ALV\Zenner et al. - 2021 - Blink-Suppressed Hand Redirection.pdf}
}

@inproceedings{zhangVRGitVersionControl2023,
  title = {{{VRGit}}: {{A Version Control System}} for {{Collaborative Content Creation}} in {{Virtual Reality}}},
  shorttitle = {{{VRGit}}},
  author = {Zhang, Lei and Agrawal, Ashutosh and Oney, Steve and Guo, Anhong},
  date = {2023-04-19},
  pages = {1--14},
  publisher = {{ACM}},
  location = {{Hamburg Germany}},
  doi = {10.1145/3544548.3581136},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581136},
  urldate = {2023-06-02},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\TSXA9G2I\3544548.3581136.pdf}
}

@inproceedings{zieglerMobilityExperienceTypes2018,
  title = {Mobility Experience Types: Towards Designing a Positive Personal Commuting Experience},
  shorttitle = {Mobility Experience Types},
  booktitle = {Proceedings of the 10th {{Nordic Conference}} on {{Human-Computer Interaction}}},
  author = {Ziegler, Daniel and Pollmann, Kathrin and Schüle, Mareike and Kuhn, Max and Fronemann, Nora},
  date = {2018-09-29},
  pages = {910--915},
  publisher = {{ACM}},
  location = {{Oslo Norway}},
  doi = {10.1145/3240167.3240234},
  url = {https://dl.acm.org/doi/10.1145/3240167.3240234},
  urldate = {2023-09-02},
  eventtitle = {{{NordiCHI}}'18: {{Nordic Conference}} on {{Human-Computer Interaction}}},
  isbn = {978-1-4503-6437-9},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\YUS8FMR3\Ziegler et al. - 2018 - Mobility experience types towards designing a pos.pdf}
}


@inproceedings{walch_towards_2016,
	address = {New York, NY, USA},
	series = {Automotive'{UI} 16},
	title = {Towards {Cooperative} {Driving}: {Involving} the {Driver} in an {Autonomous} {Vehicle}'s {Decision} {Making}},
	isbn = {978-1-4503-4533-0},
	shorttitle = {Towards {Cooperative} {Driving}},
	url = {https://doi.org/10.1145/3003715.3005458},
	doi = {10.1145/3003715.3005458},
	abstract = {Although there are already fully autonomous vehicles on the roads for testing purposes, a rollout is far away. Autonomous vehicles are still not able to handle everyday driving and remain reliant on the driver when they reach their system limitations. One suggested approach to this problem is handing over the control entirely to the driver, which might become annoying when such situations occur frequently. In contrast, we suggest the usage of cooperative interfaces to avoid full handovers in situations in which the system needs the driver, for instance to approve or monitor a specific maneuver. A driving simulator study with 32 participants revealed that they felt comfortable choosing how the system should handle a situation. They reportedly assessed the situations first instead of relying blindly on the system and were able to handle every situation safely. We report lessons learned regarding cooperative interaction and interfaces, and their in-lab evaluation.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Walch, Marcel and Sieber, Tobias and Hock, Philipp and Baumann, Martin and Weber, Michael},
	month = oct,
	year = {2016},
	keywords = {Automated driving, human factors, human-machine cooperation, multimodal interface, user study},
	pages = {261--268},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\C6NUDUST\\Walch et al. - 2016 - Towards Cooperative Driving Involving the Driver .pdf:application/pdf}
}

@inproceedings{kauer_how_2010,
	title = {How to conduct a car? {A} design example for maneuver based driver-vehicle interaction},
	shorttitle = {How to conduct a car?},
	doi = {10.1109/IVS.2010.5548099},
	abstract = {Conduct-by-Wire is a vehicle guidance paradigm, which investigates the possibility of controlling an automobile by maneuver commands. The focus of this paper is to show one possible interaction strategy for maneuver-based vehicle guidance between driver and vehicle by means of discrete maneuvers. Therefore, the paper starts with a short introduction to the advantages of maneuver-based vehicle guidance and proceeds to the current design option for maneuver-based driver-vehicle interaction chosen by the TU Darmstadt. This includes the presentation of the user interface as well as the basic assumptions for maneuver-based interaction. This paper is finished by a specific example for the maneuver “Lane Change right”, which includes system behavior, as well as, information displaying and some restricting facts which are presented together with possible countermeasures.},
	booktitle = {2010 {IEEE} {Intelligent} {Vehicles} {Symposium}},
	author = {Kauer, M. and Schreiber, M. and Bruder, R.},
	month = jun,
	year = {2010},
	note = {ISSN: 1931-0587},
	keywords = {Cameras, Communication system traffic control, Frequency, Image processing, Light emitting diodes, Lighting, Proposals, Radio transmitters, Vehicles, Wireless communication},
	pages = {1214--1221},
		publisher={IEEE},
	address = {New York, NY, USA},
}

@inproceedings{walch_touch_2017,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '17},
	title = {Touch {Screen} {Maneuver} {Approval} {Mechanisms} for {Highly} {Automated} {Vehicles}: {A} {First} {Evaluation}},
	isbn = {978-1-4503-5151-5},
	shorttitle = {Touch {Screen} {Maneuver} {Approval} {Mechanisms} for {Highly} {Automated} {Vehicles}},
	url = {https://doi.org/10.1145/3131726.3131756},
	doi = {10.1145/3131726.3131756},
	abstract = {Prototypes of highly automated vehicles are already able to drive on public roads, however fully automated rides where humans in the vehicle have only the role of a passenger regardless in which environment they travel are far away. Major issues are limited sensor range, mixed traffic, and an insufficient capability of classifying situations. We propose that vehicles can cooperate with the human inside to overcome such system boundaries. A possible input modality for driver-vehicle interaction in such scenarios are touch screens. We investigated three implementations regarding different confirmation processes to avoid erroneous inputs. Our evaluation in a driving simulator with 18 participants indicates that drivers prefer a one-tap selection, however they accept error prevention mechanisms like a confirmation dialog to approve maneuvers in more dynamic and complex scenarios. Moreover, these mechanisms did not have a negative effect on usability and workload.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications} {Adjunct}},
	publisher = {Association for Computing Machinery},
	author = {Walch, Marcel and Jaksche, Lorenz and Hock, Philipp and Baumann, Martin and Weber, Michael},
	month = sep,
	year = {2017},
	keywords = {Automated driving, human-machine cooperation, study},
	pages = {206--211},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\SHQBME79\\Walch et al. - 2017 - Touch Screen Maneuver Approval Mechanisms for High.pdf:application/pdf}
}

@inproceedings{detjen_user-defined_2019,
	address = {New York, NY, USA},
	series = {{MuC}'19},
	title = {User-{Defined} {Voice} and {Mid}-{Air} {Gesture} {Commands} for {Maneuver}-based {Interventions} in {Automated} {Vehicles}},
	isbn = {978-1-4503-7198-8},
	url = {https://doi.org/10.1145/3340764.3340798},
	doi = {10.1145/3340764.3340798},
	abstract = {For highly automated vehicles (AVs), new interaction concepts need to be developed. Even in AVs, the driver might want to intervene and override the automation from time to time. To create the possibility of control, we explore vehicle control through maneuver-based interventions (MBI). Thereby, we focus on explicit, contact-less interaction, which could be beneficial in future AV designs, where the driver is not necessarily bound to classical controls. We propose a set of freehand gestures and keywords for voice control derived in a user-centered design process. Further, we discuss properties, applicability and user impressions of both interaction modalities. Voice control seems to be an efficient way to select a maneuver and free-hand gestures could be used, if voice channel is blocked, e.g., through conversation with passengers.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of {Mensch} und {Computer} 2019},
	publisher = {Association for Computing Machinery},
	author = {Detjen, Henrik and Faltaous, Sarah and Geisler, Stefan and Schneegass, Stefan},
	month = sep,
	year = {2019},
	keywords = {Automotive HMI, Mid-Air Gestures, UCD, Voice Control},
	pages = {341--348},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\FR4V5TRX\\Detjen et al. - 2019 - User-Defined Voice and Mid-Air Gesture Commands fo.pdf:application/pdf}
}

@inproceedings{manawadu_hand_2016,
	title = {A hand gesture based driver-vehicle interface to control lateral and longitudinal motions of an autonomous vehicle},
	doi = {10.1109/SMC.2016.7844497},
	abstract = {Autonomous vehicles would make the future roads safer by keeping the human driver out of the loop. However, reduced degree of human-control could result in loss of the feeling of driving for some drivers. Therefore, in this study we proposed a method of interaction between the driver and autonomous vehicle by allowing the driver to control the vehicle's lateral and longitudinal motions. We adopted hand gestures as input modality because it can reduce driver's visual and cognitive demands. We first derived seven fundamental vehicle maneuvers to improve driver experience, and related them to seven independent hand gestures. We then created a hand gesture interface to control an autonomous vehicle, using Leap Motion as the gesture recognition platform. We conducted driving experiments involving twenty drivers in a virtual reality driving simulator to investigate the effectiveness of this interface for vehicle control. We evaluated the driving experience and drivers' opinions regarding the gestural interface. The results proved that semi-autonomous controlling using the hand gesture interface significantly reduced drivers' perceived workload.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Manawadu, Udara E. and Kamezaki, Mitsuhiro and Ishikawa, Masaaki and Kawano, Takahiro and Sugano, Shigeki},
	month = oct,
	year = {2016},
	keywords = {Aerospace electronics, Autonomous vehicles, Conferences, Cybernetics, Tracking, Visualization},
	pages = {001785--001790},
		publisher={IEEE},
	address = {New York, NY, USA},
}

@article{kim_cascaded_2020,
	title = {A {Cascaded} {Multimodal} {Natural} {User} {Interface} to {Reduce} {Driver} {Distraction}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3002775},
	abstract = {Natural user interfaces (NUI) have been used to reduce driver distraction while using in-vehicle infotainment systems (IVIS), and multimodal interfaces have been applied to compensate for the shortcomings of a single modality in NUIs. These multimodal NUIs have variable effects on different types of driver distraction and on different stages of drivers' secondary tasks. However, current studies provide a limited understanding of NUIs. The design of multimodal NUIs is typically based on evaluation of the strengths of a single modality. Furthermore, studies of multimodal NUIs are not based on equivalent comparison conditions. To address this gap, we compared five single modalities commonly used for NUIs (touch, mid-air gesture, speech, gaze, and physical buttons located in a steering wheel) during a lane change task (LCT) to provide a more holistic view of driver distraction. Our findings suggest that the best approach is a combined cascaded multimodal interface that accounts for the characteristics of a single modality. We compared several combinations of cascaded multimodalities by considering the characteristics of each modality in the sequential phase of the command input process. Our results show that the combinations speech + button, speech + touch, and gaze + button represent the best cascaded multimodal interfaces to reduce driver distraction for IVIS.},
	journal = {IEEE Access},
	author = {Kim, Myeongseop and Seong, Eunjin and Jwa, Younkyung and Lee, Jieun and Kim, Seungjun},
	publisher={IEEE},
	address = {New York, NY, USA},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Cascaded multimodal interface, driver distraction, head-up display (HUD), human-computer interaction (HCI), in-vehicle infotainment system (IVIS), learning effect, natural user interface (NUI), Navigation, Task analysis, Usability, User interfaces, Vehicles, Visualization, Wheels},
	pages = {112969--112984},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\MEB9LHPC\\Kim et al. - 2020 - A Cascaded Multimodal Natural User Interface to Re.pdf:application/pdf}
}

@inproceedings{roider_i_2018,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '18},
	title = {I {See} {Your} {Point}: {Integrating} {Gaze} to {Enhance} {Pointing} {Gesture} {Accuracy} {While} {Driving}},
	isbn = {978-1-4503-5946-7},
	shorttitle = {I {See} {Your} {Point}},
	url = {https://doi.org/10.1145/3239060.3239084},
	doi = {10.1145/3239060.3239084},
	abstract = {Mid-air pointing gestures enable drivers to interact with a wide range of vehicle functions, without requiring drivers to learn a specific set of gestures. A sufficient pointing accuracy is needed, so that targeted elements can be correctly identified. However, people make relatively large pointing errors, especially in demanding situations such as driving a car. Eye-gaze provides additional information about the drivers' focus of attention that can be used to compensate imprecise pointing. We present a practical implementation of an algorithm that integrates gaze data, in order to increase the accuracy of pointing gestures. A user experiment with 91 participants showed that our approach led to an overall increase of pointing accuracy. However, the benefits depended on the participants' initial gesture performance and on the position of the target elements. The results indicate a great potential to support gesture accuracy, but also the need for a more sophisticated fusion algorithm.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Roider, Florian and Gross, Tom},
	month = sep,
	year = {2018},
	keywords = {Attentive interfaces, automotive interface, gaze-added interface, mid-air gestures, multimodal fusion, pointing gestures},
	pages = {351--358},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\T89DTF8E\\Roider and Gross - 2018 - I See Your Point Integrating Gaze to Enhance Poin.pdf:application/pdf}
}


@inproceedings{goedicke2022xroom,
author = {Goedicke, David and Bremers, Alexandra W.D. and Lee, Sam and Bu, Fanjun and Yasuda, Hiroshi and Ju, Wendy},
title = {XR-OOM: MiXed Reality driving simulation with real cars for research and design},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517704},
doi = {10.1145/3491102.3517704},
abstract = {High-fidelity driving simulators can act as testbeds for designing in-vehicle interfaces or validating the safety of novel driver assistance features. In this system paper, we develop and validate the safety of a mixed reality driving simulator system that enables us to superimpose virtual objects and events into the view of participants engaging in real-world driving in unmodified vehicles. To this end, we have validated the mixed reality system for basic driver cockpit and low-speed driving tasks, comparing the use of the system with non-headset and with the headset driving conditions, to ensure that participants behave and perform similarly using this system as they would otherwise. This paper outlines the operational procedures and protocols for using such systems for cockpit tasks (like using the parking brake, reading the instrument panel, and turn signaling) as well as basic low-speed driving exercises (such as steering around corners, weaving around obstacles, and stopping at a fixed line) in ways that are safe, effective, and lead to accurate, repeatable data collection about behavioral responses in real-world driving tasks.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {107},
numpages = {13},
keywords = {XR, automotive, design, driving simulation, mixed reality, safety, user studies},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{bu2024portobello,
author = {Bu, Fanjun and Li, Stacey and Goedicke, David and Colley, Mark and Sharma, Gyanendra and Ju, Wendy},
title = {Portobello: Extending Driving Simulation from the Lab to the Road},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642341},
doi = {10.1145/3613904.3642341},
abstract = {In automotive user interface design, testing often starts with lab-based driving simulators and migrates toward on-road studies to mitigate risks. Mixed reality (XR) helps translate virtual study designs to the real road to increase ecological validity. However, researchers rarely run the same study in both in-lab and on-road simulators due to the challenges of replicating studies in both physical and virtual worlds. To provide a common infrastructure to port in-lab study designs on-road, we built a platform-portable infrastructure, Portobello, to enable us to run twinned physical-virtual studies. As a proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32) both in-lab and on-road to investigate study design portability and platform-driven influences on study outcomes. To our knowledge, this is the first system that enables the twinning of studies originally designed for in-lab simulators to be carried out in an on-road platform.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {256},
numpages = {13},
keywords = {Driving Simulations, Human-Autonomous Vehicle Interaction},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{mcgill20222passengxr,
author = {McGill, Mark and Wilson, Graham and Medeiros, Daniel and Brewster, Stephen Anthony},
title = {PassengXR: A Low Cost Platform for Any-Car, Multi-User, Motion-Based Passenger XR Experiences},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545657},
doi = {10.1145/3526113.3545657},
abstract = {We present PassengXR, an open-source toolkit for creating passenger eXtended Reality (XR) experiences in Unity. XR allows travellers to move beyond the physical limitations of in-vehicle displays, rendering immersive virtual content based on - or ignoring - vehicle motion. There are considerable technical challenges to using headsets in moving environments: maintaining the forward bearing of IMU-based headsets; conflicts between optical and inertial tracking of inside-out headsets; obtaining vehicle telemetry; and the high cost of design given the necessity of testing in-car. As a consequence, existing vehicular XR research typically relies on controlled, simple routes to compensate. PassengXR&nbsp;is a cost-effective open-source in-car passenger XR solution. We provide a reference set of COTS hardware that enables the broadcasting of vehicle telemetry to multiple headsets. Our software toolkit then provides support to correct vehicle-headset alignment, and then create a variety of passenger XR experiences, including: vehicle-locked content; motion- and location-based content; and co-located multi-passenger applications. PassengXR&nbsp;also supports the recording and playback of vehicle telemetry, assisting offline design without resorting to costly in-car testing. Through an evaluation-by-demonstration, we show how our platform can assist practitioners in producing novel, multi-user passenger XR experiences.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {15},
keywords = {Extended Reality, In-Car, Mixed Reality, Passenger, Toolkit, Vehicle},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{walch2019cooperation,
author = {Walch, Marcel and Colley, Mark and Weber, Michael},
title = {CooperationCaptcha: On-The-Fly Object Labeling for Highly Automated Vehicles},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3313022},
doi = {10.1145/3290607.3313022},
abstract = {In the emerging field of automated vehicles (AVs), the many recent advancements coincide with different areas of system limitations. The recognition of objects like traffic signs or traffic lights is still challenging, especially under bad weather conditions or when traffic signs are partially occluded. A common approach to deal with system boundaries of AVs is to shift to manual driving, accepting human factor issues like post-automation effects. We present CooperationCaptcha, a system that asks drivers to label unrecognized objects on the fly, and consequently maintain automated driving mode. We implemented two different interaction variants to work with object recognition algorithms of varying sophistication. Our findings suggest that this concept of driver-vehicle cooperation is feasible, provides good usability, and causes little cognitive load. We present insights and considerations for future research and implementations.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {study, human-machine cooperation, automated driving},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{colley2021orias,
author = {Colley, Mark and Askari, Ali and Walch, Marcel and Woide, Marcel and Rukzio, Enrico},
title = {ORIAS: On-The-Fly Object Identification and Action Selection for Highly Automated Vehicles},
year = {2021},
isbn = {9781450380638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409118.3475134},
doi = {10.1145/3409118.3475134},
abstract = {Automated vehicles are about to enter the mass market. However, such systems regularly meet limitations of varying criticality. Even basic tasks such as Object Identification can be challenging, for example, under bad weather or lighting conditions or for (partially) occluded objects. One common approach is to shift control to manual driving in such circumstances, however, post-automation effects can occur in these control transitions. Therefore, we present ORIAS, a system capable of asking the driver to (1) identify/label unrecognized objects or to (2) select an appropriate action to be automatically executed. ORIAS extends the automation capabilities, prevents unnecessary takeovers, and thus reduces post-automation effects. This work defines the capabilities and limitations of ORIAS and presents the results of a study in a driving simulator (N=20). Results indicate high usability and input correctness.},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {79–89},
numpages = {11},
keywords = {Automated driving, human-machine cooperation, interface design.},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21}
}

@article{bengler_hmi_2020,
	title = {From {HMI} to {HMIs}: {Towards} an {HMI} {Framework} for {Automated} {Driving}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {From {HMI} to {HMIs}},
	url = {https://www.mdpi.com/2078-2489/11/2/61},
	doi = {10.3390/info11020061},
	abstract = {During automated driving, there is a need for interaction between the automated vehicle (AV) and the passengers inside the vehicle and between the AV and the surrounding road users outside of the car. For this purpose, different types of human machine interfaces (HMIs) are implemented. This paper introduces an HMI framework and describes the different HMI types and the factors influencing their selection and content. The relationship between these HMI types and their influencing factors is also presented in the framework. Moreover, the interrelations of the HMI types are analyzed. Furthermore, we describe how the framework can be used in academia and industry to coordinate research and development activities. With the help of the HMI framework, we identify research gaps in the field of HMI for automated driving to be explored in the future.},
	language = {en},
	number = {2},
	urldate = {2021-04-20},
	journal = {Information},
	author = {Bengler, Klaus and Rettenmaier, Michael and Fritz, Nicole and Feierle, Alexander},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {automated vehicles, communication, framework, human machine interface},
	pages = {61},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\N89G94Y7\\Bengler et al. - 2020 - From HMI to HMIs Towards an HMI Framework for Aut.pdf:application/pdf}
}

@inproceedings{rumelin_free-hand_2013,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '13},
	title = {Free-hand pointing for identification and interaction with distant objects},
	isbn = {978-1-4503-2478-6},
	url = {https://doi.org/10.1145/2516540.2516556},
	doi = {10.1145/2516540.2516556},
	abstract = {In this paper, we investigate pointing as a lightweight form of gestural interaction in cars. In a pre-study, we show the technical feasibility of reliable pointing detection with a depth camera by achieving a recognition rate of 96\% in the lab. In a subsequent in-situ study, we let drivers point to objects inside and outside of the car while driving through a city. In three usage scenarios, we studied how this influenced their driving objectively, as well as subjectively. Distraction from the driving task was compensated by a regulation of driving speed and did not have a negative influence on driving behaviour. Our participants considered pointing a desirable interaction technique in comparison to current controller-based interaction and identified a number of additional promising use cases for pointing in the car.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Rümelin, Sonja and Marouane, Chadly and Butz, Andreas},
	month = oct,
	year = {2013},
	keywords = {camera-based tracking, gesture interaction, pointing},
	pages = {40--47},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\5TEMZFXI\\Rümelin et al. - 2013 - Free-hand pointing for identification and interact.pdf:application/pdf}
}

@inproceedings{fujimura_driver_2013,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '13},
	title = {Driver queries using wheel-constrained finger pointing and 3-{D} head-up display visual feedback},
	isbn = {978-1-4503-2478-6},
	url = {https://doi.org/10.1145/2516540.2516551},
	doi = {10.1145/2516540.2516551},
	abstract = {With the capability of fast, wireless communication, combined with cloud and location-based services, modern drivers can potentially access a wide variety of information about their automobile's environment. This paper presents a system for information query by the driver by using a simple pointing mechanism, combined with visual feedback in the form of a 3-D Head-up Display (3D-HUD). Because of its 3-D properties, the HUD can also be used for Augmented Reality (AR) as it allows physical elements in the driver's field of view to be annotated with computer graphics. The combination of simple natural user input tailored for the constraints of the driver with a see-thru 3D-HUD allows drivers to query information while minimizing visual and manual distraction.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Fujimura, Kikuo and Xu, Lijie and Tran, Cuong and Bhandari, Rishabh and Ng-Thow-Hing, Victor},
	month = oct,
	year = {2013},
	pages = {56--62},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\AMLTZFZ9\\Fujimura et al. - 2013 - Driver queries using wheel-constrained finger poin.pdf:application/pdf}
}

@inproceedings{poitschke_gaze-based_2011,
	title = {Gaze-based interaction on multiple displays in an automotive environment},
	doi = {10.1109/ICSMC.2011.6083740},
	abstract = {This paper presents a multimodal interaction system for automotive environments that uses the driver's eyes as main input device. Therefore, an unobtrusive and contactless sensor analyzes the driver's eye gaze, which enables the development of gaze driven interaction concepts for operating driver assistance and infotainment systems. The following sections present the developed interaction concepts, the used gaze tracking system, and the test setup consisting of multiple monitors and a large touchscreen as central interaction screen. Finally the comparison results of the gaze-based interaction with a more conventional touch interaction are being discussed. Therefore, well-defined tasks were completed by participants and task completion times, distraction and cognitive load were recorded and analyzed. The tests show promising results for gaze driven interaction.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics}},
	author = {Poitschke, Tony and Laquai, Florian and Stamboliev, Stilyan and Rigoll, Gerhard},
	month = oct,
	year = {2011},
	note = {ISSN: 1062-922X},
	keywords = {automotive, Automotive engineering, gaze tracking, Instruments, Meteorology, multi display interaction, Multimodal interaction, Roads, touch interaction, Vehicles, Visualization, Wheels},
	pages = {543--548},
	file = {Full Text:C\:\\Users\\SunyyPC\\Zotero\\storage\\2MI5DLWD\\Poitschke et al. - 2011 - Gaze-based interaction on multiple displays in an .pdf:application/pdf},
		publisher={IEEE},
	address = {New York, NY, USA},
}

@inproceedings{neselrath_combining_2016,
	title = {Combining Speech, Gaze, and Micro-gestures for the Multimodal Control of In-Car Functions},
	doi = {10.1109/IE.2016.42},
	abstract = {Modern cars are already incredibly smart environments today due to the sheer number of sensors and processors packed into a small space. Likewise, new technologies in human-computer interaction increasingly find their way inside, e.g. eye tracking, speech interaction and gesture recognition. The support of new modalities is promising a reduction of driver distraction and a better handling of an increasing number of functions offered by in-vehicle systems. With multiple modalities to choose from, which can be combined arbitrarily via multimodal fusion, drivers can make a free choice depending on the demands of the situation and their preferences. Our paper presents a prototype in-car system that allows car features (like turning lights and windows) to be controlled by combinations of speech, gaze, and micro-gestures. We propose an interaction concept, sketch our architecture based on a domain-independent multimodal dialogue platform, and draw some first conclusions on the outcome.},
	booktitle = {2016 12th {International} {Conference} on {Intelligent} {Environments} ({IE})},
	author = {Neßelrath, Robert and Moniri, Mohammad Mehdi and Feld, Michael},
	month = sep,
	year = {2016},
	note = {ISSN: 2472-7571},
	keywords = {Actuators, ADAS, Automobiles, dialogue, eye tracking, EyeVIUS, micro gestures, Mirrors, multimodality, speech, Speech, Speech recognition, Wheels},
	pages = {190--193},
	publisher={IEEE},
	address={New York, NY, USA}
}

@inproceedings{sezgin_multimodal_2009,
	address = {New York, NY, USA},
	series = {{ICMI}-{MLMI} '09},
	title = {Multimodal inference for driver-vehicle interaction},
	isbn = {978-1-60558-772-1},
	url = {https://doi.org/10.1145/1647314.1647348},
	doi = {10.1145/1647314.1647348},
	abstract = {In this paper we present a novel system for driver-vehicle interaction which combines speech recognition with facial-expression recognition to increase intention recognition accuracy in the presence of engine- and road-noise. Our system would allow drivers to interact with in-car devices such as satellite navigation and other telematic or control systems. We describe a pilot study and experiment in which we tested the system, and show that multimodal fusion of speech and facial expression recognition provides higher accuracy than either would do alone.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 2009 international conference on {Multimodal} interfaces},
	publisher = {Association for Computing Machinery},
	author = {Sezgin, Tevfik Metin and Davies, Ian and Robinson, Peter},
	month = nov,
	year = {2009},
	keywords = {driver monitoring, facial-expression recognition, multimodal inference, speech recognition},
	pages = {193--198},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\8AKJJEIW\\Sezgin et al. - 2009 - Multimodal inference for driver-vehicle interactio.pdf:application/pdf}
}

@inproceedings{aiorduachioae2019design,
author = {Aiord\u{a}chioae, Adrian and Vatavu, Radu-Daniel and Popovici, Dorin-Mircea},
title = {A Design Space for Vehicular Lifelogging to Support Creation of Digital Content in Connected Cars},
year = {2019},
isbn = {9781450367455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319499.3328234},
doi = {10.1145/3319499.3328234},
abstract = {Connected cars can create, store, and share a wide variety of data reported by in-vehicle sensors and systems, but also by mobile and wearable devices, such as smartphones, smart-watches, and smartglasses, operated by the vehicle occupants. This wide variety of driving- and journey-related data creates ideal premises for vehicular logs with applications ranging from driving assistance to monitoring driving performance and to generating content for lifelogging enthusiasts. In this paper, we introduce a design space for vehicular lifelogging consisting of five dimensions: (1) nature and (2) source of the data, (3) actors, (4) locality, and (5) representation. We use our design space to characterize existing vehicular lifelogging systems, but also to inform the features of a new prototype for the creation of digital content in connected cars using a smartphone and a pair of smartglasses.},
booktitle = {Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
articleno = {9},
numpages = {6},
keywords = {design space, vehicular lifelogging, connected cars},
location = {Valencia, Spain},
series = {EICS '19}
}

@inproceedings{alpern2003developing,
author = {Alpern, Micah and Minardo, Katie},
title = {Developing a Car Gesture Interface for Use as a Secondary Task},
year = {2003},
isbn = {1581136374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/765891.766078},
doi = {10.1145/765891.766078},
abstract = {Existing gesture-interface research has centered on controlling the user's primary task. This paper explores the use of gestures to control secondary tasks while the user is focused on driving. Through contextual inquiry, ten iterative prototypes, and a Wizard of Oz experiment, we show that a gesture interface is a viable alternative for completing secondary tasks in the car.},
booktitle = {CHI '03 Extended Abstracts on Human Factors in Computing Systems},
pages = {932–933},
numpages = {2},
keywords = {divided attention, gesture interfaces, in-car telematics, glance, secondary task},
location = {Ft. Lauderdale, Florida, USA},
series = {CHI EA '03}
}

@inproceedings{gheran2020controls,
author = {Gheran, Bogdan-Florin and Vatavu, Radu-Daniel},
title = {From Controls on the Steering Wheel to Controls on the Finger: Using Smart Rings for In-Vehicle Interactions},
year = {2020},
isbn = {9781450379878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393914.3395851},
doi = {10.1145/3393914.3395851},
abstract = {We explore the opportunity of shifting car controls from the steering wheel to the driver's fingers by means of smart rings that enable tap, touch, and mid-air gesture input. We also discuss the opportunity of using smart rings together with other input modalities toward more efficient and safer in-vehicle interactions. By accounting for the driver's location, activity, and distance from the car, we identify unique characteristics of smart rings to control the connected car compared to steering wheel controls, smartphones and in-vehicle touchscreens, mid-air gestures, and voice commands. We also present application opportunities for smart ring input for both in-vehicle and outside-the-vehicle interaction.},
booktitle = {Companion Publication of the 2020 ACM Designing Interactive Systems Conference},
pages = {299–304},
numpages = {6},
keywords = {in-vehicle interaction, outside-the-vehicle interaction, steering wheel controls, smart rings},
location = {Eindhoven, Netherlands},
series = {DIS' 20 Companion}
}

@inproceedings{ng2016investigating,
author = {Ng, Alexander and Brewster, Stephen A.},
title = {Investigating Pressure Input and Haptic Feedback for In-Car Touchscreens and Touch Surfaces},
year = {2016},
isbn = {9781450345330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003715.3005420},
doi = {10.1145/3003715.3005420},
abstract = {The way drivers interact with in-car centre console controls is undergoing radical change as physical switchgear are replaced by virtual counterparts with the use of touchscreens. This provides the opportunity to design new input techniques to improve the way on-screen widgets are operated in driving situations. This paper investigates the effectiveness of pressure-based input with haptic feedback as an alternative touch modality for in-car interactions. Two user studies were conducted: one using a driving simulator and the other inside a vehicle driven on public roads, to evaluate two main pressure-based input techniques: positional and rate-based control. The results from a list-based targeting task showed that rate-based control performed well and was comparable to standard touch input and the physical dial while users had difficulties with positional pressure input. These findings from our studies will help engineers make more appropriate design decisions when developing in-car interactions with touchscreens and touch surfaces.},
booktitle = {Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {121–128},
numpages = {8},
keywords = {In-car interactions, Vibrotactile feedback, Touchscreen input, Pressure-based input},
location = {Ann Arbor, MI, USA},
series = {Automotive'UI 16}
}

@inproceedings{hock2017carvr,
author = {Hock, Philipp and Benedikter, Sebastian and Gugenheimer, Jan and Rukzio, Enrico},
title = {CarVR: Enabling In-Car Virtual Reality Entertainment},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025665},
doi = {10.1145/3025453.3025665},
abstract = {Mobile virtual reality (VR) head-mounted displays (HMDs) allow users to experience highly immersive entertainment whilst being in a mobile scenario. Long commute times make casual gaming in public transports and cars a common occupation. However, VR HMDs can currently not be used in moving vehicles since the car's rotation affects the HMD's sensors and simulator sickness occurs when the visual and vestibular system are stimulated with incongruent information. We present CarVR, a solution to enable VR in moving vehicles by subtracting the car's rotation and mapping vehicular movements with the visual information. This allows the user to actually feel correct kinesthetic forces during the VR experience. In a user study (n = 21), we compared CarVR inside a moving vehicle with the baseline of using VR without vehicle movements. We show that the perceived kinesthetic forces caused by CarVR increase enjoyment and immersion significantly while simulator sickness is reduced compared to a stationary VR experience. Finally, we explore the design space of in-car VR entertainment applications using real kinesthetic forces and derive design considerations for practitioners.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {4034–4044},
numpages = {11},
keywords = {entertainment, immersion, motion platform, automotive, force-feedback, gaming, virtual reality},
location = {Denver, Colorado, USA},
series = {CHI '17}
}


@inproceedings{koyama2014multi,
author = {Koyama, Shunsuke and Sugiura, Yuta and Ogata, Masa and Withana, Anusha and Uema, Yuji and Honda, Makoto and Yoshizu, Sayaka and Sannomiya, Chihiro and Nawa, Kazunari and Inami, Masahiko},
title = {Multi-Touch Steering Wheel for in-Car Tertiary Applications Using Infrared Sensors},
year = {2014},
isbn = {9781450327619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2582051.2582056},
doi = {10.1145/2582051.2582056},
abstract = {This paper proposes a multi-touch steering wheel for in-car tertiary applications. Existing interfaces for in-car applications such as buttons and touch displays have several operating problems. For example, drivers have to consciously move their hands to the interfaces as the interfaces are fixed on specific positions. Therefore, we developed a steering wheel where touch positions can correspond to different operating positions. This system can recognize hand gestures at any position on the steering wheel by utilizing 120 infrared (IR) sensors embedded in it. The sensors are lined up in an array surrounding the whole wheel. An Support Vector Machine (SVM) algorithm is used to learn and recognize the different gestures through the data obtained from the sensors. The gestures recognized are flick, click, tap, stroke and twist. Additionally, we implemented a navigation application and an audio application that utilizes the torus shape of the steering wheel. We conducted an experiment to observe the possibility of our proposed system to recognize flick gestures at three positions. Results show that an average of 92% of flick could be recognized.},
booktitle = {Proceedings of the 5th Augmented Human International Conference},
articleno = {5},
numpages = {4},
keywords = {gesture recognition, infrared sensor, interaction design, multi-touch, automobile, torus interface},
location = {Kobe, Japan},
series = {AH '14}
}

@incollection{pfeiffer2010multi,
author = {Pfeiffer, Max and Kern, Dagmar and Sch\"{o}ning, Johannes and D\"{o}ring, Tanja and Kr\"{u}ger, Antonio and Schmidt, Albrecht},
title = {A Multi-Touch Enabled Steering Wheel: Exploring the Design Space},
year = {2010},
isbn = {9781605589305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753846.1753984},
doi = {10.1145/1753846.1753984},
abstract = {Cars offer an increasing number of infotainment systems as well as comfort functions that can be controlled by the driver. With our research we investigate new interaction techniques that aim to make it easier to interact with these systems while driving. In contrast to the standard approach of combining all functions into hierarchical menus controlled by a multifunctional controller or a touch screen we suggest to utilize the space on the steering wheel as additional interaction surface. In this paper we show the design challenges that arise for multi-touch interaction on a steering wheel. In particular we investigate how to deal with input and output while driving and hence rotating the wheel. We describe the details of a functional prototype of a multi-touch steering wheel that is based on FTIR and a projector, which was built to explore experimentally the user experience created. In an initial study with 12 participants we show that the approach has a general utility and that people can use gestures for controlling applications intuitively but have difficulties to imagine gestures to select applications.},
booktitle = {CHI '10 Extended Abstracts on Human Factors in Computing Systems},
pages = {3355–3360},
numpages = {6},
keywords = {multi-touch interaction, automotive interfaces, gesture input},
location = {Atlanta, Georgia, USA},
series = {CHI EA '10}
}

@inproceedings{doring2011gestural,
author = {D\"{o}ring, Tanja and Kern, Dagmar and Marshall, Paul and Pfeiffer, Max and Sch\"{o}ning, Johannes and Gruhn, Volker and Schmidt, Albrecht},
title = {Gestural Interaction on the Steering Wheel: Reducing the Visual Demand},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979010},
doi = {10.1145/1978942.1979010},
abstract = {Cars offer an increasing number of infotainment systems as well as comfort functions that can be controlled by the driver. In our research, we investigate new interaction techniques that aim to make it easier to interact with these systems while driving. We suggest utilizing the steering wheel as an additional interaction surface. In this paper, we present two user studies conducted with a working prototype of a multi-touch steering wheel. In the first, we developed a user-defined steering wheel gesture set, and in the second, we applied the identified gestures and compared their application to conventional user interaction with infotainment systems in terms of driver distraction. The main outcome was that driver's visual demand is reduced significantly by using gestural interaction on the multi-touch steering wheel.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {483–492},
numpages = {10},
keywords = {automotive user interfaces, multi-touch, driver distraction, user-defined gestures, gestural input, visual demand},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{christiansen2011don,
  title={Don’t look at me, i’m talking to you: investigating input and output modalities for in-vehicle systems},
  author={Christiansen, Lars Holm and Frederiksen, Nikolaj Yde and Jensen, Brit Susan and Ranch, Alex and Skov, Mikael B and Thiruravichandran, Nissanthen},
  booktitle={IFIP Conference on Human-Computer Interaction},
  pages={675--691},
  year={2011},
  publisher={Springer}
}

@inproceedings{riegler2020gaze,
author = {Riegler, Andreas and Aksoy, Bilal and Riener, Andreas and Holzmann, Clemens},
title = {Gaze-Based Interaction with Windshield Displays for Automated Driving: Impact of Dwell Time and Feedback Design on Task Performance and Subjective Workload},
year = {2020},
isbn = {9781450380652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409120.3410654},
doi = {10.1145/3409120.3410654},
abstract = { With increasing automation, vehicles could soon become mobile work- and living spaces, but traditional user interfaces (UIs) are not designed for this domain. We argue that high levels of productivity and user experience will only be achieved in SAE L3 automated vehicles if UIs are modified for non-driving related tasks. As controls might be far away (up to 2 meters), we suggest to use gaze-based interaction with windshield displays. In this work, we investigate the effect of different dwell times and feedback designs (circular and linear progress indicators) on user preference, task performance and error rates. Results from a user study conducted in a virtual reality driving simulator (N = 24) highlight that circular feedback animations around the viewpoint are preferred for gaze input. We conclude this work by pointing out the potential of gaze-based interactions with windshield displays for future SAE L3 vehicles.},
booktitle = {12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {151–160},
numpages = {10},
keywords = {windshield display, virtual reality, user study, feedback design, gaze-based interaction, dwell time, automated driving},
location = {Virtual Event, DC, USA},
series = {AutomotiveUI '20}
}

@inproceedings{cao2010evaluating,
author = {Cao, Yujia and van der Sluis, Frans and Theune, Mari\"{e}t and op den Akker, Rieks and Nijholt, Anton},
title = {Evaluating Informative Auditory and Tactile Cues for In-Vehicle Information Systems},
year = {2010},
isbn = {9781450304375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1969773.1969791},
doi = {10.1145/1969773.1969791},
abstract = {As in-vehicle information systems are increasingly able to obtain and deliver information, driver distraction becomes a larger concern. In this paper we propose that informative interruption cues (IIC) can be an effective means to support drivers' attention management. As a first step, we investigated the design and presentation modality of IIC that conveyed not only the arrival but also the priority level of a message. Both sound and vibration cues were created for four different priority levels and tested in 5 task conditions that simulated possible perceptional and cognitive load in real driving situations. Results showed that the cues were quickly learned, reliably detected, and quickly and accurately identified. Vibration was found to be a promising alternative for sound to deliver IIC, as vibration cues were identified more accurately and interfered less with driving. Sound cues also had advantages in terms of shorter response time and more (reported) physical comfort.},
booktitle = {Proceedings of the 2nd International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {102–109},
numpages = {8},
keywords = {interruption management, in-vehicle information systems, multimodal interfaces},
location = {Pittsburgh, Pennsylvania},
series = {AutomotiveUI '10}
}

@article{weng2016conversational,
  title={Conversational in-vehicle dialog systems: The past, present, and future},
  author={Weng, Fuliang and Angkititrakul, Pongtep and Shriberg, Elizabeth E and Heck, Larry and Peters, Stanley and Hansen, John HL},
  journal={IEEE Signal Processing Magazine},
  volume={33},
  number={6},
  pages={49--60},
  year={2016},
  publisher={IEEE},
	address = {New York, NY, USA},
}

@inproceedings{telpaz2015haptic,
  title={Haptic seat for automated driving: preparing the driver to take control effectively},
  author={Telpaz, Ariel and Rhindress, Brian and Zelman, Ido and Tsimhoni, Omer},
  booktitle={Proceedings of the 7th international conference on automotive user interfaces and interactive vehicular applications},
  pages={23--30},
  year={2015}
}

@inproceedings{richter2010haptouch,
author = {Richter, Hendrik and Ecker, Ronald and Deisler, Christopher and Butz, Andreas},
title = {HapTouch and the 2+1 State Model: Potentials of Haptic Feedback on Touch Based in-Vehicle Information Systems},
year = {2010},
isbn = {9781450304375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1969773.1969787},
doi = {10.1145/1969773.1969787},
abstract = {Haptic feedback on touch-sensitive displays provides significant benefits in terms of reducing error rates, increasing interaction speed and minimizing visual distraction. This particularly holds true for multitasking situations such as the interaction with mobile devices or touch-based in-vehicle systems. In this paper, we explore how the interaction with tactile touchscreens can be modeled and enriched using a 2+1 state transition model. The model expands an approach presented by Buxton. We present HapTouch -- a force-sensitive touchscreen device with haptic feedback that allows the user to explore and manipulate interactive elements using the sense of touch. We describe the results of a preliminary quantitative study to investigate the effects of tactile feedback on the driver's visual attention, driving performance and operating error rate. In particular, we focus on how active tactile feedback allows the accurate interaction with small on-screen elements during driving. Our results show significantly reduced error rates and input time when haptic feedback is given.},
booktitle = {Proceedings of the 2nd International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {72–79},
numpages = {8},
keywords = {in-vehicle information systems, touchscreen, haptics, exploration, tactile feedback, multitasking},
location = {Pittsburgh, Pennsylvania},
series = {AutomotiveUI '10}
}

@article{beruscha2011haptic,
  title={Haptic warning signals at the steering wheel: A literature survey regarding lane departure warning systems (short paper)},
  author={Beruscha, Frank and Augsburg, Klaus and Manstetten, Dietrich},
  year={2011},
  journal = {Haptics-e, The electronic journal of haptics research}
}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Effects of Involuntary Movement on Interaction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{goode_impact_nodate,
	title = {The impact of on-road motion on {BMS} touch screen device operation},
	language = {en},
	author = {Goode, Natassia and Lenné, Michael G and Salmon, Paul},
	pages = {12},
	file = {Goode et al. - The impact of on-road motion on BMS touch screen d.pdf:C\:\\Users\\SunyyPC\\Zotero\\storage\\G3UWRT6P\\Goode et al. - The impact of on-road motion on BMS touch screen d.pdf:application/pdf},
	journal = {Ergonomics},
	year = {2012},
	volume = {55},
    number = {9},
	doi={10.1080/00140139.2012.685496},
	publisher={Taylor \& Francis}
}

@article{kim_evaluation_2014,
title = {Evaluation of the safety and usability of touch gestures in operating in-vehicle information systems with visual occlusion},
journal = {Applied Ergonomics},
volume = {45},
number = {3},
pages = {789-798},
year = {2014},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0003687013002238},
author = {Huhn Kim and Haewon Song},
keywords = {In-vehicle information systems, Touch gestures, Occlusion technique, Visual distraction},
abstract = {Nowadays, many automobile manufacturers are interested in applying the touch gestures that are used in smart phones to operate their in-vehicle information systems (IVISs). In this study, an experiment was performed to verify the applicability of touch gestures in the operation of IVISs from the viewpoints of both driving safety and usability. In the experiment, two devices were used: one was the Apple iPad, with which various touch gestures such as flicking, panning, and pinching were enabled; the other was the SK EnNavi, which only allowed tapping touch gestures. The participants performed the touch operations using the two devices under visually occluded situations, which is a well-known technique for estimating load of visual attention while driving. In scrolling through a list, the flicking gestures required more time than the tapping gestures. Interestingly, both the flicking and simple tapping gestures required slightly higher visual attention. In moving a map, the average time taken per operation and the visual attention load required for the panning gestures did not differ from those of the simple tapping gestures that are used in existing car navigation systems. In zooming in/out of a map, the average time taken per pinching gesture was similar to that of the tapping gesture but required higher visual attention. Moreover, pinching gestures at a display angle of 75° required that the participants severely bend their wrists. Because the display angles of many car navigation systems tends to be more than 75°, pinching gestures can cause severe fatigue on users' wrists. Furthermore, contrary to participants' evaluation of other gestures, several participants answered that the pinching gesture was not necessary when operating IVISs. It was found that the panning gesture is the only touch gesture that can be used without negative consequences when operating IVISs while driving. The flicking gesture is likely to be used if the screen moving speed is slower or if the car is in heavy traffic. However, the pinching gesture is not an appropriate method of operating IVISs while driving in the various scenarios examined in this study.},
publisher={Elsevier},
    address={Amsterdam, The Netherlands}
}

@article{salmon_effects_2011,
	series = {Special {Issue}: {Driving} {Simulation} in {Traffic} {Psychology}},
	title = {The effects of motion on in-vehicle touch screen system operation: {A} battle management system case study},
	volume = {14},
	issn = {1369-8478},
	shorttitle = {The effects of motion on in-vehicle touch screen system operation},
	url = {https://www.sciencedirect.com/science/article/pii/S1369847811000738},
	doi = {10.1016/j.trf.2011.08.002},
	abstract = {The use of in-vehicle touch screen devices is currently common in both military and civilian vehicles; despite this, the effects of motion on touch screen device operation within vehicles remains largely unexplored. This article describes a study that examined, using driving simulation, the influences of motion on performance, workload and usability when using a touch screen in-vehicle battle management system. Acting in the role of battle management system operator, 20 participants undertook four simulated drives, two under high motion (representative of an unsealed road) and two under normal motion (representative of a sealed road), whilst performing various battle management tasks. In the high motion condition, lower accuracy and longer task completion times were found, along with greater levels of subjective and physiological workload and lower levels of perceived device usability, when compared to the normal motion condition. The findings indicate that, compared to normal motion, the high motion condition impaired key aspects of battle management system operation. In closing, the importance of considering motion and its effects during touch screen system design is discussed.},
	language = {en},
	number = {6},
	urldate = {2021-04-20},
	journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
	author = {Salmon, Paul M. and Lenné, Michael G. and Triggs, Tom and Goode, Natassia and Cornelissen, Miranda and Demczuk, Victor},
	month = nov,
	year = {2011},
	keywords = {Battle management system, Motion, Simulation, Touch screen, Usability, Workload},
	pages = {494--503},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\MU26788X\\Salmon et al. - 2011 - The effects of motion on in-vehicle touch screen s.pdf:application/pdf},
	publisher={Elsevier},
	    address={Amsterdam, The Netherlands}
}

@inproceedings{baltodano_rrads_2015,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '15},
	title = {The {RRADS} platform: a real road autonomous driving simulator},
	isbn = {978-1-4503-3736-6},
	shorttitle = {The {RRADS} platform},
	url = {https://doi.org/10.1145/2799250.2799288},
	doi = {10.1145/2799250.2799288},
	abstract = {This platform paper introduces a methodology for simulating an autonomous vehicle on open public roads. The paper outlines the technology and protocol needed for running these simulations, and describes an instance where the Real Road Autonomous Driving Simulator (RRADS) was used to evaluate 3 prototypes in a between-participant study design. 35 participants were interviewed at length before and after entering the RRADS. Although our study did not use overt deception---the consent form clearly states that a licensed driver is operating the vehicle---the protocol was designed to support suspension of disbelief. Several participants who did not read the consent form clearly strongly believed that they were interacting with a fully autonomous vehicle. The RRADS platform provides a lens onto the attitudes and concerns that people in real-world autonomous vehicles might have, and also points to ways that a protocol deliberately using misdirection can gain ecologically valid reactions from study participants.},
	urldate = {2021-04-21},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Baltodano, Sonia and Sibi, Srinath and Martelaro, Nikolas and Gowda, Nikhil and Ju, Wendy},
	month = sep,
	year = {2015},
	keywords = {experimentation, on-the-road simulation, research protocols, wizard-of-oz},
	pages = {281--288},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\FCGFVFPP\\Baltodano et al. - 2015 - The RRADS platform a real road autonomous driving.pdf:application/pdf}
}

@inproceedings{goedicke_vr-oom_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {{VR}-{OOM}: {Virtual} {Reality} {On}-{rOad} driving {siMulation}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {{VR}-{OOM}},
	url = {https://doi.org/10.1145/3173574.3173739},
	doi = {10.1145/3173574.3173739},
	abstract = {Researchers and designers of in-vehicle interactions and interfaces currently have to choose between performing evaluation and human factors experiments in laboratory driving simulators or on-road experiments. To enjoy the benefit of customizable course design in controlled experiments with the immediacy and rich sensations of on-road driving, we have developed a new method and tools to enable VR driving simulation in a vehicle as it travels on a road. In this paper, we describe how the cost-effective and flexible implementation of this platform allows for rapid prototyping. A preliminary pilot test (N = 6), centered on an autonomous driving scenario, yields promising results, illustrating proof of concept and indicating that a basic implementation of the system can invoke genuine responses from test participants.},
	urldate = {2021-04-21},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Goedicke, David and Li, Jamy and Evers, Vanessa and Ju, Wendy},
	month = apr,
	year = {2018},
	keywords = {autonomous vehicles, design evaluation, prototyping, virtual reality},
	pages = {1--11},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\9PJF9PAW\\Goedicke et al. - 2018 - VR-OOM Virtual Reality On-rOad driving siMulation.pdf:application/pdf}
}


@inproceedings{mohan_dualgaze_2018,
    title={DualGaze: Addressing the Midas Touch Problem in Gaze Mediated VR Interaction}, 
	shorttitle = {{DualGaze}},
	doi = {10.1109/ISMAR-Adjunct.2018.00039},
	abstract = {With the increasing acceptance of eye tracking as a viable interaction method for Virtual Reality (VR) headsets, thoughtful gaze interaction methods need to be carefully designed to meet common challenges such as the Midas Touch problem, where users unintentionally select onscreen objects by gazing upon them. This paper presents DualGaze, a novel interaction method in which users perform a distinctive two-step gaze gesture for object selection. Once users gaze upon an object that they wish to select, a confirmation flag pops up next to the object at a location where the users' gaze just passed through. This trajectory-adaptive flag placement strategy reduces the chance of unintentional confirmation by requiring a returning gaze back to the flag. We conducted a user study to compare the accuracy and selection speed of DualGaze and the popular gaze fixation method on a simple gaze-typing task. Our results show that DualGaze is significantly more accurate while maintaining a comparable selection speed that was observed to improve with familiarity of use.},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} {Adjunct} ({ISMAR}-{Adjunct})},
	author = {Mohan, P. and Goh, W. B. and Fu, C. and Yeung, S.},
	month = oct,
	year = {2018},
	keywords = {Back, eye tracking, gaze interaction, Gaze tracking, H.5.2 [Information Interfaces and Presentation]: User Interfaces—Interaction styles, Headphones, interaction methods, Midas touch, Reliability engineering, Task analysis, Trajectory, virtual reality, Virtual reality},
	pages = {79--84},
	publisher={IEEE},
	address = {New York, NY, USA},
}

@inproceedings{ahmad_touchscreen_2015,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '15},
	title = {Touchscreen usability and input performance in vehicles under different road conditions: an evaluative study},
	isbn = {978-1-4503-3736-6},
	shorttitle = {Touchscreen usability and input performance in vehicles under different road conditions},
	url = {https://doi.org/10.1145/2799250.2799284},
	doi = {10.1145/2799250.2799284},
	abstract = {With the proliferation of the touchscreen technology, interactive displays are becoming an integrated part of the modern vehicle environment. However, due to road and driving conditions, the user input on such displays can be perturbed resulting in erroneous selections. This paper describes an evaluative study of the usability and input performance of in-vehicle touchscreens. The analysis is based on data collected in instrumented cars driven under various road/driving conditions. We assess the frequency of failed selection attempts, distances by which users miss the intended on-screen target and the durations of undertaken free hand pointing gestures to accomplish the selection tasks. It is shown that the road/driving conditions can notably undermine the usability of an interactive display when the user input is perturbed, e.g. due to the experienced vibrations and lateral accelerations in the vehicle. The distance between the location of an erroneous on-screen selection and the intended endpoint on the display, is closely related to the level of present in-vehicle noise. The conducted study can advise graphical user interfaces design for the vehicle environment where the user free hand pointing gestures can be subject to varying levels of perturbations.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Ahmad, Bashar I. and Langdon, Patrick M. and Godsill, Simon J. and Hardy, Robert and Skrypchuk, Lee and Donkor, Richard},
	month = sep,
	year = {2015},
	keywords = {distractions, human computer interactions, interactive displays, perturbations, user interface},
	pages = {47--54}
}


@inproceedings{kern_design_2009,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '09},
	title = {Design space for driver-based automotive user interfaces},
	isbn = {978-1-60558-571-0},
	url = {https://doi.org/10.1145/1620509.1620511},
	doi = {10.1145/1620509.1620511},
	abstract = {Over the last 100 years it has become much easier to operate a car. However in recent years the number of functions a user can control while driving has greatly increased. Infotainment, entertainment and comfort systems as well as driver assistance contribute to this trend. Interaction with these systems plays an important role, as on one hand this can improve the user experience while driving but on the other hand it may distract from the primary task of driving. User interfaces in cars differ regarding the number of input and output devices and their placement in the car to a great extent. In this paper, we introduce a first design space for driver-based automotive user interfaces that allows a comprehensive description of input and output devices in a car with regard to placement and modality. This design space is intended to provide a basis for analyzing and discussing different user interface arrangements in cars, to compare alternative user interface setups, and to identify new opportunities for interaction and placement of controls. We present a graphical representation of the design space and discuss its usage in detail based on several examples. To assess the completeness of the proposed design space we used it to classify and compare user interfaces from more than 100 cars shown at IAA2007, cars from the BMW museum, and from the A2Mac1 image database.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 1st {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Kern, Dagmar and Schmidt, Albrecht},
	month = sep,
	year = {2009},
	keywords = {automotive user interfaces, car user interfaces, design space},
	pages = {3--10},
	file = {Submitted Version:C\:\\Users\\SunyyPC\\Zotero\\storage\\42JJTLTA\\Kern and Schmidt - 2009 - Design space for driver-based automotive user inte.pdf:application/pdf}
}

@article{ataya_how_2021,
	title = {How to {Interact} with a {Fully} {Autonomous} {Vehicle}: {Naturalistic} {Ways} for {Drivers} to {Intervene} in the {Vehicle} {System} {While} {Performing} {Non}-{Driving} {Related} {Tasks}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {How to {Interact} with a {Fully} {Autonomous} {Vehicle}},
	url = {https://www.mdpi.com/1424-8220/21/6/2206},
	doi = {10.3390/s21062206},
	abstract = {Autonomous vehicle technology increasingly allows drivers to turn their primary attention to secondary tasks (e.g., eating or working). This dramatic behavior change thus requires new input modalities to support driver–vehicle interaction, which must match the driver’s in-vehicle activities and the interaction situation. Prior studies that addressed this question did not consider how acceptance for inputs was affected by the physical and cognitive levels experienced by drivers engaged in Non-driving Related Tasks (NDRTs) or how their acceptance varies according to the interaction situation. This study investigates naturalistic interactions with a fully autonomous vehicle system in different intervention scenarios while drivers perform NDRTs. We presented an online methodology to 360 participants showing four NDRTs with different physical and cognitive engagement levels, and tested the six most common intervention scenarios (24 cases). Participants evaluated our proposed seven natural input interactions for each case: touch, voice, hand gesture, and their combinations. Results show that NDRTs influence the driver’s input interaction more than intervention scenario categories. In contrast, variation of physical load has more influence on input selection than variation of cognitive load. We also present a decision-making model of driver preferences to determine the most natural inputs and help User Experience designers better meet drivers’ needs.},
	language = {en},
	number = {6},
	urldate = {2021-04-20},
	journal = {Sensors},
	author = {Ataya, Aya and Kim, Won and Elsharkawy, Ahmed and Kim, SeungJun},
	month = jan,
	year = {2021},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {fully autonomous vehicle (FAV), input interactions, intervene vehicle system, non-driving related task (NDRT)},
	pages = {2206},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\KWWRLV2N\\Ataya et al. - 2021 - How to Interact with a Fully Autonomous Vehicle N.pdf:application/pdf}
}

@inproceedings{gomaa_studying_2020,
	address = {Virtual Event Netherlands},
	title = {Studying {Person}-{Specific} {Pointing} and {Gaze} {Behavior} for {Multimodal} {Referencing} of {Outside} {Objects} from a {Moving} {Vehicle}},
	isbn = {978-1-4503-7581-8},
	url = {https://dl.acm.org/doi/10.1145/3382507.3418817},
	doi = {10.1145/3382507.3418817},
	abstract = {Hand pointing and eye gaze have been extensively investigated in automotive applications for object selection and referencing. Despite significant advances, existing outside-the-vehicle referencing methods consider these modalities separately. Moreover, existing multimodal referencing methods focus on a static situation, whereas the situation in a moving vehicle is highly dynamic and subject to safety-critical constraints. In this paper, we investigate the specific characteristics of each modality and the interaction between them when used in the task of referencing outside objects (e.g. buildings) from the vehicle.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 2020 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Gomaa, Amr and Reyes, Guillermo and Alles, Alexandra and Rupp, Lydia and Feld, Michael},
	month = oct,
	year = {2020},
	pages = {501--509},
	file = {Gomaa et al. - 2020 - Studying Person-Specific Pointing and Gaze Behavio.pdf:C\:\\Users\\SunyyPC\\Zotero\\storage\\J463WY8S\\Gomaa et al. - 2020 - Studying Person-Specific Pointing and Gaze Behavio.pdf:application/pdf}
}

@inproceedings{roider_effects_2017,
	address = {Oldenburg Germany},
	title = {The {Effects} of {Situational} {Demands} on {Gaze}, {Speech} and {Gesture} {Input} in the {Vehicle}},
	isbn = {978-1-4503-5150-8},
	url = {https://dl.acm.org/doi/10.1145/3122986.3122999},
	doi = {10.1145/3122986.3122999},
	abstract = {Various on-the-road situations can make additional demands on the driver that go beyond the basic demands of driving. Thereby, they inﬂuence the appropriateness of in-vehicle input modalities to operate secondary tasks in the car. In this work, we assess the speciﬁc impacts of situational demands on gaze, gesture and speech input regarding driving performance, interaction efﬁciency and subjective ratings. An experiment with 29 participants in a driving simulator revealed signiﬁcant interactions between situational demands and the input modality on secondary task completion times, perceived suitability and cognitive workload. Impairments were greatest when the situational demand addressed the same sensory channel as the used input modality. This was reﬂected differently in objective and subjective data depending on the used input modality. With this work, we explore the performance of natural input modalities across different situations and thereby support interaction designers that plan to integrate these modalities in automotive interaction concepts.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {ACM},
	author = {Roider, Florian and Rümelin, Sonja and Pfleging, Bastian and Gross, Tom},
	month = sep,
	year = {2017},
	pages = {94--102},
	file = {Roider et al. - 2017 - The Effects of Situational Demands on Gaze, Speech.pdf:C\:\\Users\\SunyyPC\\Zotero\\storage\\ZMNZ8T6W\\Roider et al. - 2017 - The Effects of Situational Demands on Gaze, Speech.pdf:application/pdf}
}

@article{bilius_synopsis_2020,
	title = {A {Synopsis} of {Input} {Modalities} for {In}-{Vehicle} {Infotainment} and {Consumption} of {Interactive} {Media}},
	abstract = {Connected vehicles collect and share data by communicating with road infrastructure, with each other, the web, IoT systems, and with their occupants’ personal devices. Part of this data is presented to drivers via a multitude of interactive devices and systems. Thus, one challenge that arises in such a complex environment is effective and safe operation of the various interactive systems, e.g., the invehicle infotainment (IVI). In this paper, we present a synopsis of input modalities from the literature of automotive user interfaces (AutoUIs) for media consumption inside connected vehicles.},
	language = {en},
	author = {Bilius, Laura-Bianca and Vatavu, Radu-Daniel},
	year = {2020},
	pages = {5},
	file = {Bilius and Vatavu - 2020 - A Synopsis of Input Modalities for In-Vehicle Info.pdf:C\:\\Users\\SunyyPC\\Zotero\\storage\\T6GW8UP4\\Bilius and Vatavu - 2020 - A Synopsis of Input Modalities for In-Vehicle Info.pdf:application/pdf},
	journal = {IMX '20: ACM International Conference on Interactive Media Experiences}
}

@article{fridman_walk_2017,
	title = {To {Walk} or {Not} to {Walk}: {Crowdsourced} {Assessment} of {External} {Vehicle}-to-{Pedestrian} {Displays}},
	shorttitle = {To {Walk} or {Not} to {Walk}},
	url = {http://arxiv.org/abs/1707.02698},
	abstract = {Researchers, technology reviewers, and governmental agencies have expressed concern that automation may necessitate the introduction of added displays to indicate vehicle intent in vehicle-to-pedestrian interactions. An automated online methodology for obtaining communication intent perceptions for 30 external vehicle-to-pedestrian display concepts was implemented and tested using Amazon Mechanic Turk. Data from 200 qualified participants was quickly obtained and processed. In addition to producing a useful early-stage evaluation of these specific design concepts, the test demonstrated that the methodology is scalable so that a large number of design elements or minor variations can be assessed through a series of runs even on much larger samples in a matter of hours. Using this approach, designers should be able to refine concepts both more quickly and in more depth than available development resources typically allow. Some concerns and questions about common assumptions related to the implementation of vehicle-to-pedestrian displays are posed.},
	urldate = {2021-04-20},
	journal = {arXiv:1707.02698 [cs]},
	author = {Fridman, Lex and Mehler, Bruce and Xia, Lei and Yang, Yangyang and Facusse, Laura Yvonne and Reimer, Bryan},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.02698},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\PY5ZTCCR\\Fridman et al. - 2017 - To Walk or Not to Walk Crowdsourced Assessment of.pdf:application/pdf}
}

@inproceedings{wolf2020understanding,
  title={Understanding the heisenberg effect of spatial interaction: A selection induced error for spatially tracked input devices},
  author={Wolf, Dennis and Gugenheimer, Jan and Combosch, Marco and Rukzio, Enrico},
  booktitle={Proceedings of the 2020 CHI conference on human factors in computing systems},
  pages={1--10},
  year={2020},
publisher={ACM},
address={New York, NY, USA}
}

@misc{scikit,
  title = {scikit-learn Machine Learning in Python},
  author={scikit},
      year=2024,
  note  = {\url{https://scikit-learn.org/stable/}, last accessed on 15 AUGUST 2024},
}

@article{marcot2021optimal,
  title={What is an optimal value of k in k-fold cross-validation in discrete Bayesian network analysis?},
  author={Marcot, Bruce G and Hanea, Anca M},
  journal={Computational Statistics},
  volume={36},
  number={3},
  pages={2009--2031},
  year={2021},
  publisher={Springer}
}

@inproceedings{Chen:2016:XST:2939672.2939785,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {{XGBoost}: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}

@inproceedings{detjen2020wizard,
author = {Detjen, Henrik and Pfleging, Bastian and Schneegass, Stefan},
title = {A Wizard of Oz Field Study to Understand Non-Driving-Related Activities, Trust, and Acceptance of Automated Vehicles},
year = {2020},
isbn = {9781450380652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409120.3410662},
doi = {10.1145/3409120.3410662},
abstract = { Understanding user needs and behavior in automated vehicles (AVs) while traveling
is essential for future in-vehicle interface and service design. Since AVs are not
yet market-ready, current knowledge about AV use and perception is based on observations
in other transportation modes, interviews, or surveys about the hypothetical situation.
In this paper, we close this gap by presenting real-world insights into the attitude
towards highly automated driving and non-driving-related activities (NDRAs). Using
a Wizard of Oz AV, we conducted a real-world driving study (N = 12) with six rides
per participant during multiple days. We provide insights into the users’ perceptions
and behavior. We found that (1) the users’ trust a human driver more than a system,
(2) safety is the main acceptance factor, and (3) the most popular NDRAs were being
idle and the use of the smartphone.},
booktitle = {12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {19–29},
numpages = {11},
keywords = {Highly Automated Driving, Field Study, Non-Driving-Related Activities, Real-World Driving, Robo-Taxis, Travel-Based Multitasking, Travel Time Use, User Acceptance, Wizard of Oz Experiment, Automation Trust., Autonomous Vehicles},
location = {Virtual Event, DC, USA},
series = {AutomotiveUI '20}
}


@article{colley2023effectsurgency,
author = {Colley, Mark and Evangelista, Cristina and Rubiano, Tito Daza and Rukzio, Enrico},
title = {Effects of Urgency and Cognitive Load on Interaction in Highly Automated Vehicles},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {MHCI},
url = {https://doi.org/10.1145/3604254},
doi = {10.1145/3604254},
abstract = {In highly automated vehicles, passengers can engage in non-driving-related activities. Additionally, the technical advancement allows for novel interaction possibilities such as voice, gesture, gaze, touch, or multimodal interaction, both to refer to in-vehicle and outside objects (e.g., thermostat or restaurant). This interaction can be characterized by levels of urgency (e.g., based on late detection of objects) and cognitive load (e.g., because of watching a movie or working). Therefore, we implemented a Virtual Reality simulation and conducted a within-subjects study with N=11 participants evaluating the effects of urgency and cognitive load on modality usage in automated vehicles. We found that while all modalities were possible to use, participants relied on touch the most. This was followed by gaze, especially for external referencing. This work helps to further understand multimodal interaction and the requirements this poses on natural interaction in (automated) vehicles.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {sep},
articleno = {207},
numpages = {20},
keywords = {automated vehicles, interaction design, multimodal}
}

@inproceedings{colley2021resync,
  title={Resync: Towards Transferring Somnolent Passengers to Consciousness},
  author={Colley, Mark and Wolf, Dennis and B{\"o}hm, Sabrina and Lahmann, Tobias and Porta, Luca and Rukzio, Enrico},
  booktitle={Adjunct Publication of the 23rd International Conference on Mobile Human-Computer Interaction},
  pages={1--6},
  year={2021},
publisher={ACM},
address={New York, NY, USA}
}

@inproceedings{bergstrom2021HowEvaluateObject,
  title = {How to {{Evaluate Object Selection}} and {{Manipulation}} in {{VR}}? {{Guidelines}} from 20 {{Years}} of {{Studies}}},
  shorttitle = {How to {{Evaluate Object Selection}} and {{Manipulation}} in {{VR}}?},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Bergstr{\"o}m, Joanna and Dalsgaard, Tor-Salve and Alexander, Jason and Hornb{\ae}k, Kasper},
  year = {2021},
  month = may,
  pages = {1--20},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3411764.3445193},
  urldate = {2024-05-24},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\Z9FVYJCG\Bergström et al. - 2021 - How to Evaluate Object Selection and Manipulation .pdf}
}


@book{raffelhuschen2018deutsche,
  title={{Deutsche Post Gl{\"u}cksatlas 2018}},
  author={Raffelh{\"u}schen, B. and Schlinkert, R.},
  isbn={9783641238902},
  url={https://books.google.de/books?id=x3ByDwAAQBAJ},
  year={2018},
  publisher={Penguin Verlag},
  address={Munich, Germany}
}

@inproceedings{colley2023much,
  title={How Much Home Office is Ideal? A Multi-Perspective Algorithm},
  author={Colley, Mark and Jansen, Pascal and Matthiesen, Jennifer and Hoberg, Hanne and Reger, Carmen and Thiermann, Isabel},
  booktitle={Proceedings of the 2nd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
  pages={1--1},
  year={2023},
publisher={ACM},
address={New York, NY, USA}
}

@misc{uscensus_commutetime_2022,
  author       = {{U.S. Census Bureau}},
  title        = {Commute Time in the United States},
  year         = {2022},
  note         = {2006 to 2022 American Community Survey, 1-year estimates, Table S0801},
  url          = {https://www.census.gov/programs-surveys/acs},
  howpublished = {\url{https://www.census.gov/programs-surveys/acs}},
}


@article{loschiavo_big-city_2021,
	title = {Big-city life (dis)satisfaction? {The} effect of urban living on subjective well-being},
	volume = {192},
	issn = {01672681},
	shorttitle = {Big-city life (dis)satisfaction?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167268121004583},
	doi = {10.1016/j.jebo.2021.10.028},
	language = {en},
	urldate = {2024-08-01},
	journal = {Journal of Economic Behavior \& Organization},
	author = {Loschiavo, David},
	month = dec,
	year = {2021},
	pages = {740--764},
}


@article{colley2022systematic100m,
author = {Colley, Mark and Wankm\"{u}ller, Bastian and Rukzio, Enrico},
title = {A Systematic Evaluation of Solutions for the Final 100m Challenge of Highly Automated Vehicles},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {MHCI},
url = {https://doi.org/10.1145/3546713},
doi = {10.1145/3546713},
abstract = {Automated vehicles will change the interaction with the user drastically. While freeing the user of the driving task for most of the journey, the "final 100 meters problem'', directing the vehicle to the final parking spot, could require human intervention. Therefore, we present a classification of interaction concepts for automated vehicles based on modality and interaction mode. In a subsequent Virtual Reality study (N=16), we evaluated sixteen interaction concepts. We found that the medially abstracted interaction mode was consistently rated most usable over all modalities (joystick, speech, gaze, gesture, and tablet). While the steering wheel was still preferred, our findings indicate that other interaction concepts are usable if the steering wheel were unavailable.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {sep},
articleno = {178},
numpages = {19},
keywords = {automated vehicles., interaction modalities, systematic comparison}
}


@inproceedings{hock2022introducing,
author = {Hock, Philipp and Colley, Mark and Askari, Ali and Wagner, Tobias and Baumann, Martin and Rukzio, Enrico},
title = {Introducing VAMPIRE – Using Kinaesthetic Feedback in Virtual Reality for Automated Driving Experiments},
year = {2022},
isbn = {9781450394154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543174.3545252},
doi = {10.1145/3543174.3545252},
abstract = {Investigating trust, acceptance, and attitudes towards automated driving is often investigated in simulator experiments. Therefore, behavioral validity is a crucial aspect of automated driving studies. However, static simulators have reduced behavioral validity because of their inherent safe environment. We propose VAMPIRE (VR automated movement platform for immersive realistic experiences), a movement platform designed to increase the sensation of realism in automated driving simulator studies using an automated wheelchair. In this work, we provide a detailed description to build the prototype (including software components and assembly instructions), a proposal for safety precautions, an analysis of possible movement patterns for overtaking scenarios, and practical implications for designers and practitioners. We provide all project-related files as auxiliary materials.},
booktitle = {Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {204–214},
numpages = {11},
keywords = {Automated vehicles, Immersive technology, driving simulator, on-road simulation., user studies},
location = {Seoul, Republic of Korea},
series = {AutomotiveUI '22}
}



@article{colley2021swivr,
author = {Colley, Mark and Jansen, Pascal and Rukzio, Enrico and Gugenheimer, Jan},
title = {SwiVR-Car-Seat: Exploring Vehicle Motion Effects on Interaction Quality in Virtual Reality Automated Driving Using a Motorized Swivel Seat},
year = {2022},
issue_date = {Dec 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3494968},
doi = {10.1145/3494968},
abstract = {Autonomous vehicles provide new input modalities to improve interaction with in-vehicle information systems. However, due to the road and driving conditions, the user input can be perturbed, resulting in reduced interaction quality. One challenge is assessing the vehicle motion effects on the interaction without an expensive high-fidelity simulator or a real vehicle. This work presents SwiVR-Car-Seat, a low-cost swivel seat to simulate vehicle motion using rotation. In an exploratory user study (N=18), participants sat in a virtual autonomous vehicle and performed interaction tasks using the input modalities touch, gesture, gaze, or speech. Results show that the simulation increased the perceived realism of vehicle motion in virtual reality and the feeling of presence. Task performance was not influenced uniformly across modalities; gesture and gaze were negatively affected while there was little impact on touch and speech. The findings can advise automotive user interface design to mitigate the adverse effects of vehicle motion on the interaction.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {dec},
articleno = {150},
numpages = {26},
keywords = {vehicle motion simulation, interface design, interaction quality, Autonomous vehicles}
}

@article{crossval,
  title={Cross-validation: A review},
  author={Stone, Mervyn},
  journal={Statistics: A Journal of Theoretical and Applied Statistics},
  volume={9},
  number={1},
  pages={127--139},
  year={1978},
  publisher={Taylor \& Francis}
}

@article{xu2018splitting,
  title={On splitting training and validation set: a comparative study of cross-validation, bootstrap and systematic sampling for estimating the generalization performance of supervised learning},
  author={Xu, Yun and Goodacre, Royston},
  journal={Journal of analysis and testing},
  volume={2},
  number={3},
  pages={249--262},
  year={2018},
  publisher={Springer}
}