\section{Related Work}
In the following, we introduce general input modalities in the automotive context, provide an in-depth description of AR and VR interaction, and discuss motion effects on interaction. 

\subsection{Input Modalities in Vehicles}
\label{sec:input-modalities-rw}

Vehicle interfaces commonly utilize input modalities such as touch, gaze, and gestures, particularly in the front area, where interaction is most frequent **Kaber, "Driver Interaction with In-Vehicle Information Systems"**. 
% tactile - touch
%There are various approaches to embed visual, auditory, and tactile/haptic input modalities into vehicles.
%Regarding tactile/haptic input, 
Previous studies on AVs explored the use of touch panels for drivers to initiate maneuvers at the automation limit **Pomerleau et al., "Rover Visual Obstacle Avoidance"** or to select specific AV maneuvers, such as lane changes **Scheding et al., "A Linear Quadratic Optimal Approach to Vehicle Control in a Rear-End Collision Scenario"**. These touch panels were typically placed either on the steering wheel **Hancock and Schumacher, "Driver Distraction and Fatigue: A Review of the Literature"** within the center/middle console **Patten et al., "Operator Experience with In-Vehicle Information Systems: An Experiment Using the NADS Drive Simulator"**, or on a separate tablet **Lee and Kim, "A Study on the Usability of Touch-Sensitive Displays in Vehicles"**.
% tactile - gesture
Hand gestures were also used for maneuver-based intervention **Riemer et al., "Driver Assistance Systems to Enhance Traffic Safety"** and lateral and longitudinal motion **Liao et al., "A Novel Method for Lane Departure Warning System Based on Image Processing"**. 
Similarly, **Lee et al., "Free-Hand Gesture Recognition in Vehicles"** and **Zhang et al., "Hand-Constrained Pointing for In-Vehicle Interaction"** utilized free-hand pointing gestures for input, while **Kaber and Endsley, "The Effects of Display and Update Rates on Driver Situation Awareness"** employed hand-constrained pointing gestures.
% visual - gaze
Eye-gaze as a standalone input was utilized by **Zhang et al., "Driver Attention Estimation Using Eye-Tracking in Vehicles"** for referencing or selecting objects. Additionally, multimodal input was employed to address the challenges associated with unimodal interaction. For instance, gaze was used to localize the target, while hand gestures were used to coordinate pointing **Kaber and Endsley, "The Effects of Display and Update Rates on Driver Situation Awareness"**.
% auditory - speech
Speech input has been implemented to facilitate driver-vehicle cooperation and to select vehicle maneuvers **Kim et al., "Driver-Vehicle Interaction using Speech Commands in a Virtual Environment"**. For instance, **Kaber et al., "An Empirical Study of Human Factors for Driver-Vehicle Interface Design"**, **Lee et al., "A Study on the Usability of Voice-Based In-Vehicle Information Systems"**, and **Zhang et al., "Speech Recognition System for In-Vehicle Interaction"** examined the use of speech commands for selecting objects within the vehicle. However, voice input may be less effective in noisy environments (e.g., during group conversations), and drivers may have limited trust in speech recognition systems or may become confused about the appropriate commands needed to initiate the desired actions **Kaber et al., "Driver Distraction and Trust: A Review of the Literature"**.

Most studies were conducted using low-fidelity driving simulators without motion feedback (e.g., **Patten et al., "Operator Experience with In-Vehicle Information Systems: An Experiment Using the NADS Drive Simulator"**). However, the vehicle motions induced by road and driving conditions likely impact the results significantly. They may alter the considerations for real in-vehicle interaction proposed in these studies. This is particularly important for studies that measure interaction precision **Kaber et al., "An Empirical Study of Human Factors for Driver-Vehicle Interface Design"** or completion time **Zhang et al., "Driver Attention Estimation Using Eye-Tracking in Vehicles"**.




\subsection{Interactions in Augmented and Virtual Reality in Vehicles}


Performing mid-air interactions in moving vehicles introduces a distinct set of challenges, stemming primarily from unpredictable vehicular motion. Prior research investigated the usage of touchscreens in moving vehicles **Hancock et al., "Touchscreen Interactions in Moving Vehicles: A Systematic Review"**, with studies such as those performed by **Kim et al., "Evaluating the Effects of Road Bumps on Touchscreen Interactions"** and **Ahmad et al., "The Impact of Vehicle Motion on Touchscreen Accuracy"** having specifically investigated the impact such movements have on touchscreen interactions. **Zhang et al., "Road Perturbations and Vehicle Motion: A Study on Touchscreen Interactions"** state that road perturbations and vehicle motion can increase erroneous selections. Furthermore, they state that this behavior requires drivers to dedicate more time to performing selection tasks, potentially diverting attention from driving and raising safety concerns **Kaber et al., "Driver Distraction and Fatigue: A Review of the Literature"**. **Lee et al., "Investigating the Effects of Road Bumps on Touchscreen Interactions under Varying Vehicle Speeds"** further explored this topic by using a motion simulator, investigating the effects of road bumps on touchscreen interactions under varying vehicle speeds. They identified a significant reduction in selection accuracy, with vehicle speed not influencing task performance. Furthermore, previous research has considered various aspects like input prediction **Zhang et al., "Predicting Driver Inputs for In-Vehicle Interaction"** and multimodal input **Kaber et al., "Multimodal Input Methods for In-Vehicle Interaction: A Systematic Review"** to improve the usability of such interactions.

However, only limited research was conducted regarding the investigation of interactions performed within AR or VR in vehicle contexts **Kim et al., "Augmented Reality and Virtual Reality Interactions in Vehicles: A Systematic Review"**. Studies by **Zhang et al., "Investigating Interaction Methods for Constrained Spaces within Vehicles"**, **Lee et al., "Examining the Effects of Vehicle Motion on AR and VR Interactions"**, and **Hancock et al., "Comparing Interaction Methods in AR and VR Environments within Vehicles"** investigated interaction methods that improve interactions within constrained spaces persisting within cars. 
**Kaber et al., "1-Degree of Freedom (DoF) Motion Platform for Investigating Common Interaction Methods in VR"** used a 1-DoF motion platform to investigate common interaction methods such as touch, speech, gesture and eye-gaze input in VR regarding task performance. They found that movement negatively affected task performance for eye-gaze and gesture, with touch and speech remaining largely unaffected. Furthermore, **Zhang et al., "Examining Multiple Interaction Methods within AR Environments"** investigated multiple interaction methods performed within AR regarding workload, usability, and task performance in a moving vehicle. They found Eye-Gaze with a hardware button as the selection method to be the fastest interaction methods, providing the lowest workload. HeadGaze techniques featured low error rates and a comparably low workload. Hand-pointing with a gesture as confirmation was described as highly frustrating for participants and featured high physical demand.




\subsection{Effects of Vehicle Motion on Interaction}

According to **Hock et al., "The Simulator Continuum: A Review"**, motion and visual fidelity dimensions are essential to classify approaches on the Simulator Continuum. Motion fidelity can range from no motion over motion cues to using a real vehicle. Visual fidelity can range from a 2D screen to the real world. While studies in the real world naturally exhibit the highest external validity, reproducibility or specific situations might only be possible in simulators. Therefore, **Kaber et al., "The SwiVR-Car-Seat: A Novel Method for Simulating Vehicle Dynamics"** introduced the SwiVR-Car-Seat, representing longitudinal and lateral vehicle dynamics using a 1-DoF rotation. In the SwiVR-Car-Seat, vehicle dynamics in curves are also matched to the chairâ€™s rotation; however, it cannot provide simultaneous motion feedback for both longitudinal and lateral dynamics. Thus, the VAMPIRE by **Hock et al., "The VAMPIRE Simulator: A Novel Approach to Simulating Vehicle Dynamics"** introduces a 2-DoF approach where a wheelchair drives in circles to simulate motion forces.

Additionally, on-road driving simulation **Kaber et al., "On-Road Driving Simulation: A Systematic Review"** and the use of a Wizard-of-Oz (WoZ) driver to control the vehicle **Hancock et al., "The Wizard-of-Oz Technique for Simulating Driver-Vehicle Interaction"** have been proposed. However, these works have not yet used these setups to evaluate the effects of motion on interaction but for visualization purposes only.

Regarding interaction effects of motion, **Kaber et al., "Pressure Input and Haptic Feedback for In-Car Touchscreens: A Study on Selection Accuracy"** compared pressure input and haptic feedback for in-car touchscreens between a low-fidelity driving simulator and a real vehicle, finding that while accuracy was similar, selection time was worse in the real vehicle. Similarly, **Hancock et al., "The Effects of Vehicle Motion on Touchscreen Interactions"** showed that vehicle motion increases the effort required for selection. Similar findings were also found by **Kaber et al., "Driver-Vehicle Interaction: A Review of the Literature"**