@inproceedings{ahmadInteractiveDisplaysVehicles2014,
  title = {Interactive {{Displays}} in {{Vehicles}}: {{Improving Usability}} with a {{Pointing Gesture Tracker}} and {{Bayesian Intent Predictors}}},
  shorttitle = {Interactive {{Displays}} in {{Vehicles}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Ahmad, Bashar I. and Langdon, Patrick M. and Godsill, Simon J. and Hardy, Robert and Dias, Eduardo and Skrypchuk, Lee},
  year = {2014},
  date = {2014-09-17},
  pages = {1--8},
publisher={ACM},
address={New York, NY, USA},
  location = {{Seattle WA USA}},
  doi = {10.1145/2667317.2667413},
  url = {https://dl.acm.org/doi/10.1145/2667317.2667413},
  urldate = {2023-06-05},
  eventtitle = {{{AutomotiveUI}} '14: 6th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-3212-5},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\F9S34ABS\Ahmad et al. - 2014 - Interactive Displays in Vehicles Improving Usabil.pdf}
}

@inproceedings{ahmadTouchscreenUsabilityInput2015,
  title = {Touchscreen Usability and Input Performance in Vehicles under Different Road Conditions: An Evaluative Study},
  shorttitle = {Touchscreen Usability and Input Performance in Vehicles under Different Road Conditions},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Ahmad, Bashar I. and Langdon, Patrick M. and Godsill, Simon J. and Hardy, Robert and Skrypchuk, Lee and Donkor, Richard},
  year = {2015},
  date = {2015-09},
  pages = {47--54},
  publisher = {{ACM}},
  address = {New York, NY, USA},
  location = {{Nottingham United Kingdom}},
  doi = {10.1145/2799250.2799284},
  url = {https://dl.acm.org/doi/10.1145/2799250.2799284},
  urldate = {2023-06-05},
  eventtitle = {{{AutomotiveUI}} '15: {{The}} 7th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-3736-6},
  langid = {english},
  keywords = {Reading Done,ToDo},
  file = {C:\Users\msasalo\Zotero\storage\H8YVIK7N\Ahmad et al. - 2015 - Touchscreen usability and input performance in veh.pdf}
}

@inproceedings{aslanLeapTouchProximity2015,
author = {Aslan, Ilhan and Krischkowsky, Alina and Meschtscherjakov, Alexander and Wuchse, Martin and Tscheligi, Manfred},
title = {A leap for touch: proximity sensitive touch targets in cars},
year = {2015},
isbn = {9781450337366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2799250.2799273},
doi = {10.1145/2799250.2799273},
abstract = {Combining touch screen technology with mid-air gestures into a unified input modality has potential to improve interaction with touch interfaces in cars. Moreover, target objects on a touch screen can be adapted to the proximity of a users' finger in mid-air. In this paper, we present an exploration of this design space based on two studies and various prototypical systems. First, results of a driving simulator study are reported, indicating that a driver's performance in acquiring a target object on a touch screen in a central console position increases with expanding targets, while altering the position of a target object on the screen leads to a decrease of performance. In a follow-up workshop with automotive experts, prototypes with multiple expanding targets were utilized to foster in-depth discussions on potential challenges and benefits with expanding targets in cars. Results of both studies indicate a high potential of expanding targets for in-car interaction scenarios.},
booktitle = {Proceedings of the 7th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {39–46},
numpages = {8},
keywords = {touch, leap motion, automotive, Fitts's law},
location = {Nottingham, United Kingdom},
series = {AutomotiveUI '15},
  file = {C:\Users\msasalo\Zotero\storage\UQAIGGMW\Aslan et al. - 2015 - A leap for touch proximity sensitive touch target.pdf}
}

@article{ataya_how_2021,
	title = {How to {Interact} with a {Fully} {Autonomous} {Vehicle}: {Naturalistic} {Ways} for {Drivers} to {Intervene} in the {Vehicle} {System} {While} {Performing} {Non}-{Driving} {Related} {Tasks}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {How to {Interact} with a {Fully} {Autonomous} {Vehicle}},
	url = {https://www.mdpi.com/1424-8220/21/6/2206},
	doi = {10.3390/s21062206},
	abstract = {Autonomous vehicle technology increasingly allows drivers to turn their primary attention to secondary tasks (e.g., eating or working). This dramatic behavior change thus requires new input modalities to support driver–vehicle interaction, which must match the driver’s in-vehicle activities and the interaction situation. Prior studies that addressed this question did not consider how acceptance for inputs was affected by the physical and cognitive levels experienced by drivers engaged in Non-driving Related Tasks (NDRTs) or how their acceptance varies according to the interaction situation. This study investigates naturalistic interactions with a fully autonomous vehicle system in different intervention scenarios while drivers perform NDRTs. We presented an online methodology to 360 participants showing four NDRTs with different physical and cognitive engagement levels, and tested the six most common intervention scenarios (24 cases). Participants evaluated our proposed seven natural input interactions for each case: touch, voice, hand gesture, and their combinations. Results show that NDRTs influence the driver’s input interaction more than intervention scenario categories. In contrast, variation of physical load has more influence on input selection than variation of cognitive load. We also present a decision-making model of driver preferences to determine the most natural inputs and help User Experience designers better meet drivers’ needs.},
	language = {en},
	number = {6},
	urldate = {2021-04-20},
	journal = {Sensors},
	author = {Ataya, Aya and Kim, Won and Elsharkawy, Ahmed and Kim, SeungJun},
	month = jan,
	year = {2021},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {fully autonomous vehicle (FAV), input interactions, intervene vehicle system, non-driving related task (NDRT)},
	pages = {2206},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\KWWRLV2N\\Ataya et al. - 2021 - How to Interact with a Fully Autonomous Vehicle N.pdf:application/pdf}
}

@inproceedings{baltodano_rrads_2015,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '15},
	title = {The {RRADS} platform: a real road autonomous driving simulator},
	isbn = {978-1-4503-3736-6},
	shorttitle = {The {RRADS} platform},
	url = {https://doi.org/10.1145/2799250.2799288},
	doi = {10.1145/2799250.2799288},
	abstract = {This platform paper introduces a methodology for simulating an autonomous vehicle on open public roads. The paper outlines the technology and protocol needed for running these simulations, and describes an instance where the Real Road Autonomous Driving Simulator (RRADS) was used to evaluate 3 prototypes in a between-participant study design. 35 participants were interviewed at length before and after entering the RRADS. Although our study did not use overt deception---the consent form clearly states that a licensed driver is operating the vehicle---the protocol was designed to support suspension of disbelief. Several participants who did not read the consent form clearly strongly believed that they were interacting with a fully autonomous vehicle. The RRADS platform provides a lens onto the attitudes and concerns that people in real-world autonomous vehicles might have, and also points to ways that a protocol deliberately using misdirection can gain ecologically valid reactions from study participants.},
	urldate = {2021-04-21},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Baltodano, Sonia and Sibi, Srinath and Martelaro, Nikolas and Gowda, Nikhil and Ju, Wendy},
	month = sep,
	year = {2015},
	keywords = {experimentation, on-the-road simulation, research protocols, wizard-of-oz},
	pages = {281--288},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\FCGFVFPP\\Baltodano et al. - 2015 - The RRADS platform a real road autonomous driving.pdf:application/pdf}
}

@article{bengler_hmi_2020,
	title = {From {HMI} to {HMIs}: {Towards} an {HMI} {Framework} for {Automated} {Driving}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {From {HMI} to {HMIs}},
	url = {https://www.mdpi.com/2078-2489/11/2/61},
	doi = {10.3390/info11020061},
	abstract = {During automated driving, there is a need for interaction between the automated vehicle (AV) and the passengers inside the vehicle and between the AV and the surrounding road users outside of the car. For this purpose, different types of human machine interfaces (HMIs) are implemented. This paper introduces an HMI framework and describes the different HMI types and the factors influencing their selection and content. The relationship between these HMI types and their influencing factors is also presented in the framework. Moreover, the interrelations of the HMI types are analyzed. Furthermore, we describe how the framework can be used in academia and industry to coordinate research and development activities. With the help of the HMI framework, we identify research gaps in the field of HMI for automated driving to be explored in the future.},
	language = {en},
	number = {2},
	urldate = {2021-04-20},
	journal = {Information},
	author = {Bengler, Klaus and Rettenmaier, Michael and Fritz, Nicole and Feierle, Alexander},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {automated vehicles, communication, framework, human machine interface},
	pages = {61},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\N89G94Y7\\Bengler et al. - 2020 - From HMI to HMIs Towards an HMI Framework for Aut.pdf:application/pdf}
}

@inproceedings{bu2024portobello,
author = {Bu, Fanjun and Li, Stacey and Goedicke, David and Colley, Mark and Sharma, Gyanendra and Ju, Wendy},
title = {Portobello: Extending Driving Simulation from the Lab to the Road},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642341},
doi = {10.1145/3613904.3642341},
abstract = {In automotive user interface design, testing often starts with lab-based driving simulators and migrates toward on-road studies to mitigate risks. Mixed reality (XR) helps translate virtual study designs to the real road to increase ecological validity. However, researchers rarely run the same study in both in-lab and on-road simulators due to the challenges of replicating studies in both physical and virtual worlds. To provide a common infrastructure to port in-lab study designs on-road, we built a platform-portable infrastructure, Portobello, to enable us to run twinned physical-virtual studies. As a proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32) both in-lab and on-road to investigate study design portability and platform-driven influences on study outcomes. To our knowledge, this is the first system that enables the twinning of studies originally designed for in-lab simulators to be carried out in an on-road platform.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {256},
numpages = {13},
keywords = {Driving Simulations, Human-Autonomous Vehicle Interaction},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{colley2021orias,
author = {Colley, Mark and Askari, Ali and Walch, Marcel and Woide, Marcel and Rukzio, Enrico},
title = {ORIAS: On-The-Fly Object Identification and Action Selection for Highly Automated Vehicles},
year = {2021},
isbn = {9781450380638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409118.3475134},
doi = {10.1145/3409118.3475134},
abstract = {Automated vehicles are about to enter the mass market. However, such systems regularly meet limitations of varying criticality. Even basic tasks such as Object Identification can be challenging, for example, under bad weather or lighting conditions or for (partially) occluded objects. One common approach is to shift control to manual driving in such circumstances, however, post-automation effects can occur in these control transitions. Therefore, we present ORIAS, a system capable of asking the driver to (1) identify/label unrecognized objects or to (2) select an appropriate action to be automatically executed. ORIAS extends the automation capabilities, prevents unnecessary takeovers, and thus reduces post-automation effects. This work defines the capabilities and limitations of ORIAS and presents the results of a study in a driving simulator (N=20). Results indicate high usability and input correctness.},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {79–89},
numpages = {11},
keywords = {Automated driving, human-machine cooperation, interface design.},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21}
}

@article{colley2022systematic100m,
author = {Colley, Mark and Wankm\"{u}ller, Bastian and Rukzio, Enrico},
title = {A Systematic Evaluation of Solutions for the Final 100m Challenge of Highly Automated Vehicles},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {MHCI},
url = {https://doi.org/10.1145/3546713},
doi = {10.1145/3546713},
abstract = {Automated vehicles will change the interaction with the user drastically. While freeing the user of the driving task for most of the journey, the "final 100 meters problem'', directing the vehicle to the final parking spot, could require human intervention. Therefore, we present a classification of interaction concepts for automated vehicles based on modality and interaction mode. In a subsequent Virtual Reality study (N=16), we evaluated sixteen interaction concepts. We found that the medially abstracted interaction mode was consistently rated most usable over all modalities (joystick, speech, gaze, gesture, and tablet). While the steering wheel was still preferred, our findings indicate that other interaction concepts are usable if the steering wheel were unavailable.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {sep},
articleno = {178},
numpages = {19},
keywords = {automated vehicles., interaction modalities, systematic comparison}
}

@article{colley2023effectsurgency,
author = {Colley, Mark and Evangelista, Cristina and Rubiano, Tito Daza and Rukzio, Enrico},
title = {Effects of Urgency and Cognitive Load on Interaction in Highly Automated Vehicles},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {MHCI},
url = {https://doi.org/10.1145/3604254},
doi = {10.1145/3604254},
abstract = {In highly automated vehicles, passengers can engage in non-driving-related activities. Additionally, the technical advancement allows for novel interaction possibilities such as voice, gesture, gaze, touch, or multimodal interaction, both to refer to in-vehicle and outside objects (e.g., thermostat or restaurant). This interaction can be characterized by levels of urgency (e.g., based on late detection of objects) and cognitive load (e.g., because of watching a movie or working). Therefore, we implemented a Virtual Reality simulation and conducted a within-subjects study with N=11 participants evaluating the effects of urgency and cognitive load on modality usage in automated vehicles. We found that while all modalities were possible to use, participants relied on touch the most. This was followed by gaze, especially for external referencing. This work helps to further understand multimodal interaction and the requirements this poses on natural interaction in (automated) vehicles.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {sep},
articleno = {207},
numpages = {20},
keywords = {automated vehicles, interaction design, multimodal}
}

@article{colleySwiVRCarSeatExploringVehicle2021,
  author = {Colley, Mark and Jansen, Pascal and Rukzio, Enrico and Gugenheimer, Jan},
title = {SwiVR-Car-Seat: Exploring Vehicle Motion Effects on Interaction Quality in Virtual Reality Automated Driving Using a Motorized Swivel Seat},
year = {2022},
issue_date = {Dec 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3494968},
doi = {10.1145/3494968},
abstract = {Autonomous vehicles provide new input modalities to improve interaction with in-vehicle information systems. However, due to the road and driving conditions, the user input can be perturbed, resulting in reduced interaction quality. One challenge is assessing the vehicle motion effects on the interaction without an expensive high-fidelity simulator or a real vehicle. This work presents SwiVR-Car-Seat, a low-cost swivel seat to simulate vehicle motion using rotation. In an exploratory user study (N=18), participants sat in a virtual autonomous vehicle and performed interaction tasks using the input modalities touch, gesture, gaze, or speech. Results show that the simulation increased the perceived realism of vehicle motion in virtual reality and the feeling of presence. Task performance was not influenced uniformly across modalities; gesture and gaze were negatively affected while there was little impact on touch and speech. The findings can advise automotive user interface design to mitigate the adverse effects of vehicle motion on the interaction.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {150},
numpages = {26},
keywords = {vehicle motion simulation, interface design, interaction quality, Autonomous vehicles}
}

@inproceedings{detjen2020wizard,
author = {Detjen, Henrik and Pfleging, Bastian and Schneegass, Stefan},
title = {A Wizard of Oz Field Study to Understand Non-Driving-Related Activities, Trust, and Acceptance of Automated Vehicles},
year = {2020},
isbn = {9781450380652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409120.3410662},
doi = {10.1145/3409120.3410662},
abstract = { Understanding user needs and behavior in automated vehicles (AVs) while traveling
is essential for future in-vehicle interface and service design. Since AVs are not
yet market-ready, current knowledge about AV use and perception is based on observations
in other transportation modes, interviews, or surveys about the hypothetical situation.
In this paper, we close this gap by presenting real-world insights into the attitude
towards highly automated driving and non-driving-related activities (NDRAs). Using
a Wizard of Oz AV, we conducted a real-world driving study (N = 12) with six rides
per participant during multiple days. We provide insights into the users’ perceptions
and behavior. We found that (1) the users’ trust a human driver more than a system,
(2) safety is the main acceptance factor, and (3) the most popular NDRAs were being
idle and the use of the smartphone.},
booktitle = {12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {19–29},
numpages = {11},
keywords = {Highly Automated Driving, Field Study, Non-Driving-Related Activities, Real-World Driving, Robo-Taxis, Travel-Based Multitasking, Travel Time Use, User Acceptance, Wizard of Oz Experiment, Automation Trust., Autonomous Vehicles},
location = {Virtual Event, DC, USA},
series = {AutomotiveUI '20}
}

@inproceedings{detjen_user-defined_2019,
	address = {New York, NY, USA},
	series = {{MuC}'19},
	title = {User-{Defined} {Voice} and {Mid}-{Air} {Gesture} {Commands} for {Maneuver}-based {Interventions} in {Automated} {Vehicles}},
	isbn = {978-1-4503-7198-8},
	url = {https://doi.org/10.1145/3340764.3340798},
	doi = {10.1145/3340764.3340798},
	abstract = {For highly automated vehicles (AVs), new interaction concepts need to be developed. Even in AVs, the driver might want to intervene and override the automation from time to time. To create the possibility of control, we explore vehicle control through maneuver-based interventions (MBI). Thereby, we focus on explicit, contact-less interaction, which could be beneficial in future AV designs, where the driver is not necessarily bound to classical controls. We propose a set of freehand gestures and keywords for voice control derived in a user-centered design process. Further, we discuss properties, applicability and user impressions of both interaction modalities. Voice control seems to be an efficient way to select a maneuver and free-hand gestures could be used, if voice channel is blocked, e.g., through conversation with passengers.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of {Mensch} und {Computer} 2019},
	publisher = {Association for Computing Machinery},
	author = {Detjen, Henrik and Faltaous, Sarah and Geisler, Stefan and Schneegass, Stefan},
	month = sep,
	year = {2019},
	keywords = {Automotive HMI, Mid-Air Gestures, UCD, Voice Control},
	pages = {341--348},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\FR4V5TRX\\Detjen et al. - 2019 - User-Defined Voice and Mid-Air Gesture Commands fo.pdf:application/pdf}
}

@inproceedings{doring2011gestural,
author = {D\"{o}ring, Tanja and Kern, Dagmar and Marshall, Paul and Pfeiffer, Max and Sch\"{o}ning, Johannes and Gruhn, Volker and Schmidt, Albrecht},
title = {Gestural Interaction on the Steering Wheel: Reducing the Visual Demand},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979010},
doi = {10.1145/1978942.1979010},
abstract = {Cars offer an increasing number of infotainment systems as well as comfort functions that can be controlled by the driver. In our research, we investigate new interaction techniques that aim to make it easier to interact with these systems while driving. We suggest utilizing the steering wheel as an additional interaction surface. In this paper, we present two user studies conducted with a working prototype of a multi-touch steering wheel. In the first, we developed a user-defined steering wheel gesture set, and in the second, we applied the identified gestures and compared their application to conventional user interaction with infotainment systems in terms of driver distraction. The main outcome was that driver's visual demand is reduced significantly by using gestural interaction on the multi-touch steering wheel.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {483–492},
numpages = {10},
keywords = {automotive user interfaces, multi-touch, driver distraction, user-defined gestures, gestural input, visual demand},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{fujimura_driver_2013,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '13},
	title = {Driver queries using wheel-constrained finger pointing and 3-{D} head-up display visual feedback},
	isbn = {978-1-4503-2478-6},
	url = {https://doi.org/10.1145/2516540.2516551},
	doi = {10.1145/2516540.2516551},
	abstract = {With the capability of fast, wireless communication, combined with cloud and location-based services, modern drivers can potentially access a wide variety of information about their automobile's environment. This paper presents a system for information query by the driver by using a simple pointing mechanism, combined with visual feedback in the form of a 3-D Head-up Display (3D-HUD). Because of its 3-D properties, the HUD can also be used for Augmented Reality (AR) as it allows physical elements in the driver's field of view to be annotated with computer graphics. The combination of simple natural user input tailored for the constraints of the driver with a see-thru 3D-HUD allows drivers to query information while minimizing visual and manual distraction.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Fujimura, Kikuo and Xu, Lijie and Tran, Cuong and Bhandari, Rishabh and Ng-Thow-Hing, Victor},
	month = oct,
	year = {2013},
	pages = {56--62},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\AMLTZFZ9\\Fujimura et al. - 2013 - Driver queries using wheel-constrained finger poin.pdf:application/pdf}
}

@inproceedings{goedicke2022xroom,
author = {Goedicke, David and Bremers, Alexandra W.D. and Lee, Sam and Bu, Fanjun and Yasuda, Hiroshi and Ju, Wendy},
title = {XR-OOM: MiXed Reality driving simulation with real cars for research and design},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517704},
doi = {10.1145/3491102.3517704},
abstract = {High-fidelity driving simulators can act as testbeds for designing in-vehicle interfaces or validating the safety of novel driver assistance features. In this system paper, we develop and validate the safety of a mixed reality driving simulator system that enables us to superimpose virtual objects and events into the view of participants engaging in real-world driving in unmodified vehicles. To this end, we have validated the mixed reality system for basic driver cockpit and low-speed driving tasks, comparing the use of the system with non-headset and with the headset driving conditions, to ensure that participants behave and perform similarly using this system as they would otherwise. This paper outlines the operational procedures and protocols for using such systems for cockpit tasks (like using the parking brake, reading the instrument panel, and turn signaling) as well as basic low-speed driving exercises (such as steering around corners, weaving around obstacles, and stopping at a fixed line) in ways that are safe, effective, and lead to accurate, repeatable data collection about behavioral responses in real-world driving tasks.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {107},
numpages = {13},
keywords = {XR, automotive, design, driving simulation, mixed reality, safety, user studies},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{goedicke_vr-oom_2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {{VR}-{OOM}: {Virtual} {Reality} {On}-{rOad} driving {siMulation}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {{VR}-{OOM}},
	url = {https://doi.org/10.1145/3173574.3173739},
	doi = {10.1145/3173574.3173739},
	abstract = {Researchers and designers of in-vehicle interactions and interfaces currently have to choose between performing evaluation and human factors experiments in laboratory driving simulators or on-road experiments. To enjoy the benefit of customizable course design in controlled experiments with the immediacy and rich sensations of on-road driving, we have developed a new method and tools to enable VR driving simulation in a vehicle as it travels on a road. In this paper, we describe how the cost-effective and flexible implementation of this platform allows for rapid prototyping. A preliminary pilot test (N = 6), centered on an autonomous driving scenario, yields promising results, illustrating proof of concept and indicating that a basic implementation of the system can invoke genuine responses from test participants.},
	urldate = {2021-04-21},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Goedicke, David and Li, Jamy and Evers, Vanessa and Ju, Wendy},
	month = apr,
	year = {2018},
	keywords = {autonomous vehicles, design evaluation, prototyping, virtual reality},
	pages = {1--11},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\9PJF9PAW\\Goedicke et al. - 2018 - VR-OOM Virtual Reality On-rOad driving siMulation.pdf:application/pdf}
}

@inproceedings{gomaa_studying_2020,
	address = {Virtual Event Netherlands},
	title = {Studying {Person}-{Specific} {Pointing} and {Gaze} {Behavior} for {Multimodal} {Referencing} of {Outside} {Objects} from a {Moving} {Vehicle}},
	isbn = {978-1-4503-7581-8},
	url = {https://dl.acm.org/doi/10.1145/3382507.3418817},
	doi = {10.1145/3382507.3418817},
	abstract = {Hand pointing and eye gaze have been extensively investigated in automotive applications for object selection and referencing. Despite significant advances, existing outside-the-vehicle referencing methods consider these modalities separately. Moreover, existing multimodal referencing methods focus on a static situation, whereas the situation in a moving vehicle is highly dynamic and subject to safety-critical constraints. In this paper, we investigate the specific characteristics of each modality and the interaction between them when used in the task of referencing outside objects (e.g. buildings) from the vehicle.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 2020 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Gomaa, Amr and Reyes, Guillermo and Alles, Alexandra and Rupp, Lydia and Feld, Michael},
	month = oct,
	year = {2020},
	pages = {501--509},
	file = {Gomaa et al. - 2020 - Studying Person-Specific Pointing and Gaze Behavio.pdf:C\:\\Users\\SunyyPC\\Zotero\\storage\\J463WY8S\\Gomaa et al. - 2020 - Studying Person-Specific Pointing and Gaze Behavio.pdf:application/pdf}
}

@article{goode_impact_nodate,
	title = {The impact of on-road motion on {BMS} touch screen device operation},
	language = {en},
	author = {Goode, Natassia and Lenné, Michael G and Salmon, Paul},
	pages = {12},
	file = {Goode et al. - The impact of on-road motion on BMS touch screen d.pdf:C\:\\Users\\SunyyPC\\Zotero\\storage\\G3UWRT6P\\Goode et al. - The impact of on-road motion on BMS touch screen d.pdf:application/pdf},
	journal = {Ergonomics},
	year = {2012},
	volume = {55},
    number = {9},
	doi={10.1080/00140139.2012.685496},
	publisher={Taylor \& Francis}
}

@inproceedings{hock2022introducing,
author = {Hock, Philipp and Colley, Mark and Askari, Ali and Wagner, Tobias and Baumann, Martin and Rukzio, Enrico},
title = {Introducing VAMPIRE – Using Kinaesthetic Feedback in Virtual Reality for Automated Driving Experiments},
year = {2022},
isbn = {9781450394154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543174.3545252},
doi = {10.1145/3543174.3545252},
abstract = {Investigating trust, acceptance, and attitudes towards automated driving is often investigated in simulator experiments. Therefore, behavioral validity is a crucial aspect of automated driving studies. However, static simulators have reduced behavioral validity because of their inherent safe environment. We propose VAMPIRE (VR automated movement platform for immersive realistic experiences), a movement platform designed to increase the sensation of realism in automated driving simulator studies using an automated wheelchair. In this work, we provide a detailed description to build the prototype (including software components and assembly instructions), a proposal for safety precautions, an analysis of possible movement patterns for overtaking scenarios, and practical implications for designers and practitioners. We provide all project-related files as auxiliary materials.},
booktitle = {Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {204–214},
numpages = {11},
keywords = {Automated vehicles, Immersive technology, driving simulator, on-road simulation., user studies},
location = {Seoul, Republic of Korea},
series = {AutomotiveUI '22}
}

@article{jansenDesignSpaceHuman2022,
  author = {Jansen, Pascal and Colley, Mark and Rukzio, Enrico},
title = {A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3534617},
doi = {10.1145/3534617},
abstract = {Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jul,
articleno = {56},
numpages = {51},
keywords = {systematic literature review, in-vehicle interaction, human sensors and actuators, design space}
}

@inproceedings{kariHandyCastPhonebasedBimanual2023,
 author = {Kari, Mohamed and Holz, Christian},
title = {HandyCast: Phone-based Bimanual Input for Virtual Reality in Mobile and Space-Constrained Settings via Pose-and-Touch Transfer},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580677},
doi = {10.1145/3544548.3580677},
abstract = {Despite the potential of Virtual Reality as the next computing platform for general purposes, current systems are tailored to stationary settings to support expansive interaction in mid-air. However, in mobile scenarios, the physical constraints of the space surrounding the user may be prohibitively small for spatial interaction in VR with classical controllers. In this paper, we present HandyCast, a smartphone-based input technique that enables full-range 3D input with two virtual hands in VR while requiring little physical space, allowing users to operate large virtual environments in mobile settings. HandyCast defines a pose-and-touch transfer function that fuses the phone’s position and orientation with touch input to derive two individual 3D hand positions. Holding their phone like a gamepad, users can thus move and turn it to independently control their virtual hands. Touch input using the thumbs fine-tunes the respective virtual hand position and controls object selection. We evaluated HandyCast in three studies, comparing its performance with that of Go-Go, a classic bimanual controller technique. In our open-space study, participants required significantly less physical motion using HandyCast with no decrease in completion time or body ownership. In our space-constrained study, participants achieved significantly faster completion times, smaller interaction volumes, and shorter path lengths with HandyCast compared to Go-Go. In our technical evaluation, HandyCast’s fully standalone inside-out 6D tracking performance again incurred no decrease in completion time compared to an outside-in tracking baseline.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {528},
numpages = {15},
keywords = {3D controller, VR input, Virtual reality, bimanual interaction., interaction techniques},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{kauer_how_2010,
	title = {How to conduct a car? {A} design example for maneuver based driver-vehicle interaction},
	shorttitle = {How to conduct a car?},
	doi = {10.1109/IVS.2010.5548099},
	abstract = {Conduct-by-Wire is a vehicle guidance paradigm, which investigates the possibility of controlling an automobile by maneuver commands. The focus of this paper is to show one possible interaction strategy for maneuver-based vehicle guidance between driver and vehicle by means of discrete maneuvers. Therefore, the paper starts with a short introduction to the advantages of maneuver-based vehicle guidance and proceeds to the current design option for maneuver-based driver-vehicle interaction chosen by the TU Darmstadt. This includes the presentation of the user interface as well as the basic assumptions for maneuver-based interaction. This paper is finished by a specific example for the maneuver “Lane Change right”, which includes system behavior, as well as, information displaying and some restricting facts which are presented together with possible countermeasures.},
	booktitle = {2010 {IEEE} {Intelligent} {Vehicles} {Symposium}},
	author = {Kauer, M. and Schreiber, M. and Bruder, R.},
	month = jun,
	year = {2010},
	note = {ISSN: 1931-0587},
	keywords = {Cameras, Communication system traffic control, Frequency, Image processing, Light emitting diodes, Lighting, Proposals, Radio transmitters, Vehicles, Wireless communication},
	pages = {1214--1221},
		publisher={IEEE},
	address = {New York, NY, USA},
}

@article{kim_cascaded_2020,
	title = {A {Cascaded} {Multimodal} {Natural} {User} {Interface} to {Reduce} {Driver} {Distraction}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3002775},
	abstract = {Natural user interfaces (NUI) have been used to reduce driver distraction while using in-vehicle infotainment systems (IVIS), and multimodal interfaces have been applied to compensate for the shortcomings of a single modality in NUIs. These multimodal NUIs have variable effects on different types of driver distraction and on different stages of drivers' secondary tasks. However, current studies provide a limited understanding of NUIs. The design of multimodal NUIs is typically based on evaluation of the strengths of a single modality. Furthermore, studies of multimodal NUIs are not based on equivalent comparison conditions. To address this gap, we compared five single modalities commonly used for NUIs (touch, mid-air gesture, speech, gaze, and physical buttons located in a steering wheel) during a lane change task (LCT) to provide a more holistic view of driver distraction. Our findings suggest that the best approach is a combined cascaded multimodal interface that accounts for the characteristics of a single modality. We compared several combinations of cascaded multimodalities by considering the characteristics of each modality in the sequential phase of the command input process. Our results show that the combinations speech + button, speech + touch, and gaze + button represent the best cascaded multimodal interfaces to reduce driver distraction for IVIS.},
	journal = {IEEE Access},
	author = {Kim, Myeongseop and Seong, Eunjin and Jwa, Younkyung and Lee, Jieun and Kim, Seungjun},
	publisher={IEEE},
	address = {New York, NY, USA},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Cascaded multimodal interface, driver distraction, head-up display (HUD), human-computer interaction (HCI), in-vehicle infotainment system (IVIS), learning effect, natural user interface (NUI), Navigation, Task analysis, Usability, User interfaces, Vehicles, Visualization, Wheels},
	pages = {112969--112984},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\MEB9LHPC\\Kim et al. - 2020 - A Cascaded Multimodal Natural User Interface to Re.pdf:application/pdf}
}

@article{kim_evaluation_2014,
title = {Evaluation of the safety and usability of touch gestures in operating in-vehicle information systems with visual occlusion},
journal = {Applied Ergonomics},
volume = {45},
number = {3},
pages = {789-798},
year = {2014},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0003687013002238},
author = {Huhn Kim and Haewon Song},
keywords = {In-vehicle information systems, Touch gestures, Occlusion technique, Visual distraction},
abstract = {Nowadays, many automobile manufacturers are interested in applying the touch gestures that are used in smart phones to operate their in-vehicle information systems (IVISs). In this study, an experiment was performed to verify the applicability of touch gestures in the operation of IVISs from the viewpoints of both driving safety and usability. In the experiment, two devices were used: one was the Apple iPad, with which various touch gestures such as flicking, panning, and pinching were enabled; the other was the SK EnNavi, which only allowed tapping touch gestures. The participants performed the touch operations using the two devices under visually occluded situations, which is a well-known technique for estimating load of visual attention while driving. In scrolling through a list, the flicking gestures required more time than the tapping gestures. Interestingly, both the flicking and simple tapping gestures required slightly higher visual attention. In moving a map, the average time taken per operation and the visual attention load required for the panning gestures did not differ from those of the simple tapping gestures that are used in existing car navigation systems. In zooming in/out of a map, the average time taken per pinching gesture was similar to that of the tapping gesture but required higher visual attention. Moreover, pinching gestures at a display angle of 75° required that the participants severely bend their wrists. Because the display angles of many car navigation systems tends to be more than 75°, pinching gestures can cause severe fatigue on users' wrists. Furthermore, contrary to participants' evaluation of other gestures, several participants answered that the pinching gesture was not necessary when operating IVISs. It was found that the panning gesture is the only touch gesture that can be used without negative consequences when operating IVISs while driving. The flicking gesture is likely to be used if the screen moving speed is slower or if the car is in heavy traffic. However, the pinching gesture is not an appropriate method of operating IVISs while driving in the various scenarios examined in this study.},
publisher={Elsevier},
    address={Amsterdam, The Netherlands}
}

@inproceedings{koyama2014multi,
author = {Koyama, Shunsuke and Sugiura, Yuta and Ogata, Masa and Withana, Anusha and Uema, Yuji and Honda, Makoto and Yoshizu, Sayaka and Sannomiya, Chihiro and Nawa, Kazunari and Inami, Masahiko},
title = {Multi-Touch Steering Wheel for in-Car Tertiary Applications Using Infrared Sensors},
year = {2014},
isbn = {9781450327619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2582051.2582056},
doi = {10.1145/2582051.2582056},
abstract = {This paper proposes a multi-touch steering wheel for in-car tertiary applications. Existing interfaces for in-car applications such as buttons and touch displays have several operating problems. For example, drivers have to consciously move their hands to the interfaces as the interfaces are fixed on specific positions. Therefore, we developed a steering wheel where touch positions can correspond to different operating positions. This system can recognize hand gestures at any position on the steering wheel by utilizing 120 infrared (IR) sensors embedded in it. The sensors are lined up in an array surrounding the whole wheel. An Support Vector Machine (SVM) algorithm is used to learn and recognize the different gestures through the data obtained from the sensors. The gestures recognized are flick, click, tap, stroke and twist. Additionally, we implemented a navigation application and an audio application that utilizes the torus shape of the steering wheel. We conducted an experiment to observe the possibility of our proposed system to recognize flick gestures at three positions. Results show that an average of 92% of flick could be recognized.},
booktitle = {Proceedings of the 5th Augmented Human International Conference},
articleno = {5},
numpages = {4},
keywords = {gesture recognition, infrared sensor, interaction design, multi-touch, automobile, torus interface},
location = {Kobe, Japan},
series = {AH '14}
}

@inproceedings{manawadu_hand_2016,
	title = {A hand gesture based driver-vehicle interface to control lateral and longitudinal motions of an autonomous vehicle},
	doi = {10.1109/SMC.2016.7844497},
	abstract = {Autonomous vehicles would make the future roads safer by keeping the human driver out of the loop. However, reduced degree of human-control could result in loss of the feeling of driving for some drivers. Therefore, in this study we proposed a method of interaction between the driver and autonomous vehicle by allowing the driver to control the vehicle's lateral and longitudinal motions. We adopted hand gestures as input modality because it can reduce driver's visual and cognitive demands. We first derived seven fundamental vehicle maneuvers to improve driver experience, and related them to seven independent hand gestures. We then created a hand gesture interface to control an autonomous vehicle, using Leap Motion as the gesture recognition platform. We conducted driving experiments involving twenty drivers in a virtual reality driving simulator to investigate the effectiveness of this interface for vehicle control. We evaluated the driving experience and drivers' opinions regarding the gestural interface. The results proved that semi-autonomous controlling using the hand gesture interface significantly reduced drivers' perceived workload.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Manawadu, Udara E. and Kamezaki, Mitsuhiro and Ishikawa, Masaaki and Kawano, Takahiro and Sugano, Shigeki},
	month = oct,
	year = {2016},
	keywords = {Aerospace electronics, Autonomous vehicles, Conferences, Cybernetics, Tracking, Visualization},
	pages = {001785--001790},
		publisher={IEEE},
	address = {New York, NY, USA},
}

@inproceedings{mayerEffectRoadBumps2018,
  title = {The {{Effect}} of {{Road Bumps}} on {{Touch Interaction}} in {{Cars}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Mayer, Sven and Le, Huy Viet and Nesti, Alessandro and Henze, Niels and Bülthoff, Heinrich H. and Chuang, Lewis L.},
  year = {2018},
  date = {2018-09-23},
  pages = {85--93},
publisher={ACM},
address={New York, NY, USA},
  doi = {10.1145/3239060.3239071},
  url = {https://dl.acm.org/doi/10.1145/3239060.3239071},
  urldate = {2023-06-05},
  eventtitle = {{{AutomotiveUI}} '18: 10th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-5946-7},
  langid = {english},
  keywords = {Notes Done,Reading Done},
  file = {C:\Users\msasalo\Zotero\storage\78TF3ZRD\Mayer et al. - 2018 - The Effect of Road Bumps on Touch Interaction in C.pdf}
}

@inproceedings{mcgill20222passengxr,
author = {McGill, Mark and Wilson, Graham and Medeiros, Daniel and Brewster, Stephen Anthony},
title = {PassengXR: A Low Cost Platform for Any-Car, Multi-User, Motion-Based Passenger XR Experiences},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545657},
doi = {10.1145/3526113.3545657},
abstract = {We present PassengXR, an open-source toolkit for creating passenger eXtended Reality (XR) experiences in Unity. XR allows travellers to move beyond the physical limitations of in-vehicle displays, rendering immersive virtual content based on - or ignoring - vehicle motion. There are considerable technical challenges to using headsets in moving environments: maintaining the forward bearing of IMU-based headsets; conflicts between optical and inertial tracking of inside-out headsets; obtaining vehicle telemetry; and the high cost of design given the necessity of testing in-car. As a consequence, existing vehicular XR research typically relies on controlled, simple routes to compensate. PassengXR&nbsp;is a cost-effective open-source in-car passenger XR solution. We provide a reference set of COTS hardware that enables the broadcasting of vehicle telemetry to multiple headsets. Our software toolkit then provides support to correct vehicle-headset alignment, and then create a variety of passenger XR experiences, including: vehicle-locked content; motion- and location-based content; and co-located multi-passenger applications. PassengXR&nbsp;also supports the recording and playback of vehicle telemetry, assisting offline design without resorting to costly in-car testing. Through an evaluation-by-demonstration, we show how our platform can assist practitioners in producing novel, multi-user passenger XR experiences.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {15},
keywords = {Extended Reality, In-Car, Mixed Reality, Passenger, Toolkit, Vehicle},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{neselrath_combining_2016,
	title = {Combining Speech, Gaze, and Micro-gestures for the Multimodal Control of In-Car Functions},
	doi = {10.1109/IE.2016.42},
	abstract = {Modern cars are already incredibly smart environments today due to the sheer number of sensors and processors packed into a small space. Likewise, new technologies in human-computer interaction increasingly find their way inside, e.g. eye tracking, speech interaction and gesture recognition. The support of new modalities is promising a reduction of driver distraction and a better handling of an increasing number of functions offered by in-vehicle systems. With multiple modalities to choose from, which can be combined arbitrarily via multimodal fusion, drivers can make a free choice depending on the demands of the situation and their preferences. Our paper presents a prototype in-car system that allows car features (like turning lights and windows) to be controlled by combinations of speech, gaze, and micro-gestures. We propose an interaction concept, sketch our architecture based on a domain-independent multimodal dialogue platform, and draw some first conclusions on the outcome.},
	booktitle = {2016 12th {International} {Conference} on {Intelligent} {Environments} ({IE})},
	author = {Neßelrath, Robert and Moniri, Mohammad Mehdi and Feld, Michael},
	month = sep,
	year = {2016},
	note = {ISSN: 2472-7571},
	keywords = {Actuators, ADAS, Automobiles, dialogue, eye tracking, EyeVIUS, micro gestures, Mirrors, multimodality, speech, Speech, Speech recognition, Wheels},
	pages = {190--193},
	publisher={IEEE},
	address={New York, NY, USA}
}

@inproceedings{ng2016investigating,
author = {Ng, Alexander and Brewster, Stephen A.},
title = {Investigating Pressure Input and Haptic Feedback for In-Car Touchscreens and Touch Surfaces},
year = {2016},
isbn = {9781450345330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003715.3005420},
doi = {10.1145/3003715.3005420},
abstract = {The way drivers interact with in-car centre console controls is undergoing radical change as physical switchgear are replaced by virtual counterparts with the use of touchscreens. This provides the opportunity to design new input techniques to improve the way on-screen widgets are operated in driving situations. This paper investigates the effectiveness of pressure-based input with haptic feedback as an alternative touch modality for in-car interactions. Two user studies were conducted: one using a driving simulator and the other inside a vehicle driven on public roads, to evaluate two main pressure-based input techniques: positional and rate-based control. The results from a list-based targeting task showed that rate-based control performed well and was comparable to standard touch input and the physical dial while users had difficulties with positional pressure input. These findings from our studies will help engineers make more appropriate design decisions when developing in-car interactions with touchscreens and touch surfaces.},
booktitle = {Proceedings of the 8th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {121–128},
numpages = {8},
keywords = {In-car interactions, Vibrotactile feedback, Touchscreen input, Pressure-based input},
location = {Ann Arbor, MI, USA},
series = {Automotive'UI 16}
}

@inproceedings{pampelFittsGoesAutobahn2019,
author = {Pampel, Sanna M. and Burnett, Gary and Hare, Chrisminder and Singh, Harpreet and Shabani, Arber and Skrypchuk, Lee and Mouzakitis, Alex},
title = {Fitts Goes Autobahn: Assessing the Visual Demand of Finger-Touch Pointing Tasks in an On-Road Study},
year = {2019},
isbn = {9781450368841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342197.3344538},
doi = {10.1145/3342197.3344538},
abstract = {The visual demand of finger-touch based interactions with touch screens has been increasingly modelled using Fitts' Law. With respect to driving, these models facilitate the prediction of mean glance duration and total glance time with an index of difficulty based on target size and location. Strong relationships between measures have been found in the controlled conditions of driving simulators. The present study aimed to validate such models in naturalistic conditions. Nineteen experienced drivers carried out a range of touchscreen button-press tasks in an instrumented car on a UK motorway. In contrast with previous simulator-based work, our on-road data produced much weaker relationships between the index of difficulty and glance times. The model improved by focusing on tasks that required one glance only. Limitations of Fitts' Law in the more complex and dynamic real-world driving environment are discussed, as are the potential drawbacks of driving simulators for conducting visual demand research.},
booktitle = {Proceedings of the 11th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {254–261},
numpages = {8},
keywords = {Fitts' Law, driving, human-computer interaction, pointing, touchscreen, visual demand},
location = {Utrecht, Netherlands},
series = {AutomotiveUI '19}
}

@incollection{pfeiffer2010multi,
author = {Pfeiffer, Max and Kern, Dagmar and Sch\"{o}ning, Johannes and D\"{o}ring, Tanja and Kr\"{u}ger, Antonio and Schmidt, Albrecht},
title = {A Multi-Touch Enabled Steering Wheel: Exploring the Design Space},
year = {2010},
isbn = {9781605589305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753846.1753984},
doi = {10.1145/1753846.1753984},
abstract = {Cars offer an increasing number of infotainment systems as well as comfort functions that can be controlled by the driver. With our research we investigate new interaction techniques that aim to make it easier to interact with these systems while driving. In contrast to the standard approach of combining all functions into hierarchical menus controlled by a multifunctional controller or a touch screen we suggest to utilize the space on the steering wheel as additional interaction surface. In this paper we show the design challenges that arise for multi-touch interaction on a steering wheel. In particular we investigate how to deal with input and output while driving and hence rotating the wheel. We describe the details of a functional prototype of a multi-touch steering wheel that is based on FTIR and a projector, which was built to explore experimentally the user experience created. In an initial study with 12 participants we show that the approach has a general utility and that people can use gestures for controlling applications intuitively but have difficulties to imagine gestures to select applications.},
booktitle = {CHI '10 Extended Abstracts on Human Factors in Computing Systems},
pages = {3355–3360},
numpages = {6},
keywords = {multi-touch interaction, automotive interfaces, gesture input},
location = {Atlanta, Georgia, USA},
series = {CHI EA '10}
}

@inproceedings{poitschke_gaze-based_2011,
	title = {Gaze-based interaction on multiple displays in an automotive environment},
	doi = {10.1109/ICSMC.2011.6083740},
	abstract = {This paper presents a multimodal interaction system for automotive environments that uses the driver's eyes as main input device. Therefore, an unobtrusive and contactless sensor analyzes the driver's eye gaze, which enables the development of gaze driven interaction concepts for operating driver assistance and infotainment systems. The following sections present the developed interaction concepts, the used gaze tracking system, and the test setup consisting of multiple monitors and a large touchscreen as central interaction screen. Finally the comparison results of the gaze-based interaction with a more conventional touch interaction are being discussed. Therefore, well-defined tasks were completed by participants and task completion times, distraction and cognitive load were recorded and analyzed. The tests show promising results for gaze driven interaction.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics}},
	author = {Poitschke, Tony and Laquai, Florian and Stamboliev, Stilyan and Rigoll, Gerhard},
	month = oct,
	year = {2011},
	note = {ISSN: 1062-922X},
	keywords = {automotive, Automotive engineering, gaze tracking, Instruments, Meteorology, multi display interaction, Multimodal interaction, Roads, touch interaction, Vehicles, Visualization, Wheels},
	pages = {543--548},
	file = {Full Text:C\:\\Users\\SunyyPC\\Zotero\\storage\\2MI5DLWD\\Poitschke et al. - 2011 - Gaze-based interaction on multiple displays in an .pdf:application/pdf},
		publisher={IEEE},
	address = {New York, NY, USA},
}

@inproceedings{riegler2020gaze,
author = {Riegler, Andreas and Aksoy, Bilal and Riener, Andreas and Holzmann, Clemens},
title = {Gaze-Based Interaction with Windshield Displays for Automated Driving: Impact of Dwell Time and Feedback Design on Task Performance and Subjective Workload},
year = {2020},
isbn = {9781450380652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409120.3410654},
doi = {10.1145/3409120.3410654},
abstract = { With increasing automation, vehicles could soon become mobile work- and living spaces, but traditional user interfaces (UIs) are not designed for this domain. We argue that high levels of productivity and user experience will only be achieved in SAE L3 automated vehicles if UIs are modified for non-driving related tasks. As controls might be far away (up to 2 meters), we suggest to use gaze-based interaction with windshield displays. In this work, we investigate the effect of different dwell times and feedback designs (circular and linear progress indicators) on user preference, task performance and error rates. Results from a user study conducted in a virtual reality driving simulator (N = 24) highlight that circular feedback animations around the viewpoint are preferred for gaze input. We conclude this work by pointing out the potential of gaze-based interactions with windshield displays for future SAE L3 vehicles.},
booktitle = {12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {151–160},
numpages = {10},
keywords = {windshield display, virtual reality, user study, feedback design, gaze-based interaction, dwell time, automated driving},
location = {Virtual Event, DC, USA},
series = {AutomotiveUI '20}
}

@inproceedings{roiderSeeYourPoint2018,
  title = {I {{See Your Point}}: {{Integrating Gaze}} to {{Enhance Pointing Gesture Accuracy While Driving}}},
  shorttitle = {I {{See Your Point}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Roider, Florian and Gross, Tom},
  year = {2018},
  date = {2018-09-23},
  pages = {351--358},
publisher={ACM},
address={New York, NY, USA},
  location = {{Toronto ON Canada}},
  doi = {10.1145/3239060.3239084},
  url = {https://dl.acm.org/doi/10.1145/3239060.3239084},
  urldate = {2023-06-05},
  eventtitle = {{{AutomotiveUI}} '18: 10th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {978-1-4503-5946-7},
  langid = {english},
  keywords = {ToDo},
  file = {C:\Users\msasalo\Zotero\storage\65Y3LXGV\Roider und Gross - 2018 - I See Your Point Integrating Gaze to Enhance Poin.pdf}
}

@inproceedings{roider_effects_2017,
	address = {Oldenburg Germany},
	title = {The {Effects} of {Situational} {Demands} on {Gaze}, {Speech} and {Gesture} {Input} in the {Vehicle}},
	isbn = {978-1-4503-5150-8},
	url = {https://dl.acm.org/doi/10.1145/3122986.3122999},
	doi = {10.1145/3122986.3122999},
	abstract = {Various on-the-road situations can make additional demands on the driver that go beyond the basic demands of driving. Thereby, they inﬂuence the appropriateness of in-vehicle input modalities to operate secondary tasks in the car. In this work, we assess the speciﬁc impacts of situational demands on gaze, gesture and speech input regarding driving performance, interaction efﬁciency and subjective ratings. An experiment with 29 participants in a driving simulator revealed signiﬁcant interactions between situational demands and the input modality on secondary task completion times, perceived suitability and cognitive workload. Impairments were greatest when the situational demand addressed the same sensory channel as the used input modality. This was reﬂected differently in objective and subjective data depending on the used input modality. With this work, we explore the performance of natural input modalities across different situations and thereby support interaction designers that plan to integrate these modalities in automotive interaction concepts.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {ACM},
	author = {Roider, Florian and Rümelin, Sonja and Pfleging, Bastian and Gross, Tom},
	month = sep,
	year = {2017},
	pages = {94--102},
	file = {Roider et al. - 2017 - The Effects of Situational Demands on Gaze, Speech.pdf:C\:\\Users\\SunyyPC\\Zotero\\storage\\ZMNZ8T6W\\Roider et al. - 2017 - The Effects of Situational Demands on Gaze, Speech.pdf:application/pdf}
}

@inproceedings{rumelin_free-hand_2013,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '13},
	title = {Free-hand pointing for identification and interaction with distant objects},
	isbn = {978-1-4503-2478-6},
	url = {https://doi.org/10.1145/2516540.2516556},
	doi = {10.1145/2516540.2516556},
	abstract = {In this paper, we investigate pointing as a lightweight form of gestural interaction in cars. In a pre-study, we show the technical feasibility of reliable pointing detection with a depth camera by achieving a recognition rate of 96\% in the lab. In a subsequent in-situ study, we let drivers point to objects inside and outside of the car while driving through a city. In three usage scenarios, we studied how this influenced their driving objectively, as well as subjectively. Distraction from the driving task was compensated by a regulation of driving speed and did not have a negative influence on driving behaviour. Our participants considered pointing a desirable interaction technique in comparison to current controller-based interaction and identified a number of additional promising use cases for pointing in the car.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Rümelin, Sonja and Marouane, Chadly and Butz, Andreas},
	month = oct,
	year = {2013},
	keywords = {camera-based tracking, gesture interaction, pointing},
	pages = {40--47},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\5TEMZFXI\\Rümelin et al. - 2013 - Free-hand pointing for identification and interact.pdf:application/pdf}
}

@article{salmon_effects_2011,
	series = {Special {Issue}: {Driving} {Simulation} in {Traffic} {Psychology}},
	title = {The effects of motion on in-vehicle touch screen system operation: {A} battle management system case study},
	volume = {14},
	issn = {1369-8478},
	shorttitle = {The effects of motion on in-vehicle touch screen system operation},
	url = {https://www.sciencedirect.com/science/article/pii/S1369847811000738},
	doi = {10.1016/j.trf.2011.08.002},
	abstract = {The use of in-vehicle touch screen devices is currently common in both military and civilian vehicles; despite this, the effects of motion on touch screen device operation within vehicles remains largely unexplored. This article describes a study that examined, using driving simulation, the influences of motion on performance, workload and usability when using a touch screen in-vehicle battle management system. Acting in the role of battle management system operator, 20 participants undertook four simulated drives, two under high motion (representative of an unsealed road) and two under normal motion (representative of a sealed road), whilst performing various battle management tasks. In the high motion condition, lower accuracy and longer task completion times were found, along with greater levels of subjective and physiological workload and lower levels of perceived device usability, when compared to the normal motion condition. The findings indicate that, compared to normal motion, the high motion condition impaired key aspects of battle management system operation. In closing, the importance of considering motion and its effects during touch screen system design is discussed.},
	language = {en},
	number = {6},
	urldate = {2021-04-20},
	journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
	author = {Salmon, Paul M. and Lenné, Michael G. and Triggs, Tom and Goode, Natassia and Cornelissen, Miranda and Demczuk, Victor},
	month = nov,
	year = {2011},
	keywords = {Battle management system, Motion, Simulation, Touch screen, Usability, Workload},
	pages = {494--503},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\MU26788X\\Salmon et al. - 2011 - The effects of motion on in-vehicle touch screen s.pdf:application/pdf},
	publisher={Elsevier},
	    address={Amsterdam, The Netherlands}
}

@inproceedings{schramm2023AssessingAugmentedReality,
  title = {Assessing {{Augmented Reality Selection Techniques}} for {{Passengers}} in {{Moving Vehicles}}: {{A Real-World User Study}}},
  shorttitle = {Assessing {{Augmented Reality Selection Techniques}} for {{Passengers}} in {{Moving Vehicles}}},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  author = {Schramm, Robin Connor and Sasalovici, Markus and Hildebrand, Axel and Schwanecke, Ulrich},
  year = {2023},
  date = {2023-09-18},
  pages = {22--31},
publisher={ACM},
address={New York, NY, USA},
  location = {{Ingolstadt Germany}},
  doi = {10.1145/3580585.3607152},
  url = {https://dl.acm.org/doi/10.1145/3580585.3607152},
  urldate = {2023-11-07},
  eventtitle = {{{AutomotiveUI}} '23: 15th {{International Conference}} on {{Automotive User Interfaces}} and {{Interactive Vehicular Applications}}},
  isbn = {9798400701054},
  langid = {english},
  file = {C:\Users\msasalo\Zotero\storage\W9RNJXBN\3580585.3607152.pdf}
}

@inproceedings{sezgin_multimodal_2009,
	address = {New York, NY, USA},
	series = {{ICMI}-{MLMI} '09},
	title = {Multimodal inference for driver-vehicle interaction},
	isbn = {978-1-60558-772-1},
	url = {https://doi.org/10.1145/1647314.1647348},
	doi = {10.1145/1647314.1647348},
	abstract = {In this paper we present a novel system for driver-vehicle interaction which combines speech recognition with facial-expression recognition to increase intention recognition accuracy in the presence of engine- and road-noise. Our system would allow drivers to interact with in-car devices such as satellite navigation and other telematic or control systems. We describe a pilot study and experiment in which we tested the system, and show that multimodal fusion of speech and facial expression recognition provides higher accuracy than either would do alone.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 2009 international conference on {Multimodal} interfaces},
	publisher = {Association for Computing Machinery},
	author = {Sezgin, Tevfik Metin and Davies, Ian and Robinson, Peter},
	month = nov,
	year = {2009},
	keywords = {driver monitoring, facial-expression recognition, multimodal inference, speech recognition},
	pages = {193--198},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\8AKJJEIW\\Sezgin et al. - 2009 - Multimodal inference for driver-vehicle interactio.pdf:application/pdf}
}

@inproceedings{tsengFingerMapperMappingFinger2023,
 author = {Tseng, Wen-Jie and Huron, Samuel and Lecolinet, Eric and Gugenheimer, Jan},
title = {FingerMapper: Mapping Finger Motions onto Virtual Arms to Enable Safe Virtual Reality Interaction in Confined Spaces},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580736},
doi = {10.1145/3544548.3580736},
abstract = {Whole-body movements enhance the presence and enjoyment of Virtual Reality (VR) experiences. However, using large gestures is often uncomfortable and impossible in confined spaces (e.g., public transport). We introduce FingerMapper, mapping small-scale finger motions onto virtual arms and hands to enable whole-body virtual movements in VR. In a first target selection study (n=13) comparing FingerMapper to hand tracking and ray-casting, we found that FingerMapper can significantly reduce physical motions and fatigue while having a similar degree of precision. In a consecutive study (n=13), we compared FingerMapper to hand tracking inside a confined space (the front passenger seat of a car). The results showed participants had significantly higher perceived safety and fewer collisions with FingerMapper while preserving a similar degree of presence and enjoyment as hand tracking. Finally, we present three example applications demonstrating how FingerMapper could be applied for locomotion and interaction for VR in confined spaces.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {874},
numpages = {14},
keywords = {Body Re-Association in VR, Confined Spaces, FingerMapper},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{walch2019cooperation,
author = {Walch, Marcel and Colley, Mark and Weber, Michael},
title = {CooperationCaptcha: On-The-Fly Object Labeling for Highly Automated Vehicles},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3313022},
doi = {10.1145/3290607.3313022},
abstract = {In the emerging field of automated vehicles (AVs), the many recent advancements coincide with different areas of system limitations. The recognition of objects like traffic signs or traffic lights is still challenging, especially under bad weather conditions or when traffic signs are partially occluded. A common approach to deal with system boundaries of AVs is to shift to manual driving, accepting human factor issues like post-automation effects. We present CooperationCaptcha, a system that asks drivers to label unrecognized objects on the fly, and consequently maintain automated driving mode. We implemented two different interaction variants to work with object recognition algorithms of varying sophistication. Our findings suggest that this concept of driver-vehicle cooperation is feasible, provides good usability, and causes little cognitive load. We present insights and considerations for future research and implementations.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {study, human-machine cooperation, automated driving},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{walch_touch_2017,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '17},
	title = {Touch {Screen} {Maneuver} {Approval} {Mechanisms} for {Highly} {Automated} {Vehicles}: {A} {First} {Evaluation}},
	isbn = {978-1-4503-5151-5},
	shorttitle = {Touch {Screen} {Maneuver} {Approval} {Mechanisms} for {Highly} {Automated} {Vehicles}},
	url = {https://doi.org/10.1145/3131726.3131756},
	doi = {10.1145/3131726.3131756},
	abstract = {Prototypes of highly automated vehicles are already able to drive on public roads, however fully automated rides where humans in the vehicle have only the role of a passenger regardless in which environment they travel are far away. Major issues are limited sensor range, mixed traffic, and an insufficient capability of classifying situations. We propose that vehicles can cooperate with the human inside to overcome such system boundaries. A possible input modality for driver-vehicle interaction in such scenarios are touch screens. We investigated three implementations regarding different confirmation processes to avoid erroneous inputs. Our evaluation in a driving simulator with 18 participants indicates that drivers prefer a one-tap selection, however they accept error prevention mechanisms like a confirmation dialog to approve maneuvers in more dynamic and complex scenarios. Moreover, these mechanisms did not have a negative effect on usability and workload.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications} {Adjunct}},
	publisher = {Association for Computing Machinery},
	author = {Walch, Marcel and Jaksche, Lorenz and Hock, Philipp and Baumann, Martin and Weber, Michael},
	month = sep,
	year = {2017},
	keywords = {Automated driving, human-machine cooperation, study},
	pages = {206--211},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\SHQBME79\\Walch et al. - 2017 - Touch Screen Maneuver Approval Mechanisms for High.pdf:application/pdf}
}

@inproceedings{walch_towards_2016,
	address = {New York, NY, USA},
	series = {Automotive'{UI} 16},
	title = {Towards {Cooperative} {Driving}: {Involving} the {Driver} in an {Autonomous} {Vehicle}'s {Decision} {Making}},
	isbn = {978-1-4503-4533-0},
	shorttitle = {Towards {Cooperative} {Driving}},
	url = {https://doi.org/10.1145/3003715.3005458},
	doi = {10.1145/3003715.3005458},
	abstract = {Although there are already fully autonomous vehicles on the roads for testing purposes, a rollout is far away. Autonomous vehicles are still not able to handle everyday driving and remain reliant on the driver when they reach their system limitations. One suggested approach to this problem is handing over the control entirely to the driver, which might become annoying when such situations occur frequently. In contrast, we suggest the usage of cooperative interfaces to avoid full handovers in situations in which the system needs the driver, for instance to approve or monitor a specific maneuver. A driving simulator study with 32 participants revealed that they felt comfortable choosing how the system should handle a situation. They reportedly assessed the situations first instead of relying blindly on the system and were able to handle every situation safely. We report lessons learned regarding cooperative interaction and interfaces, and their in-lab evaluation.},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Walch, Marcel and Sieber, Tobias and Hock, Philipp and Baumann, Martin and Weber, Michael},
	month = oct,
	year = {2016},
	keywords = {Automated driving, human factors, human-machine cooperation, multimodal interface, user study},
	pages = {261--268},
	file = {Full Text PDF:C\:\\Users\\SunyyPC\\Zotero\\storage\\C6NUDUST\\Walch et al. - 2016 - Towards Cooperative Driving Involving the Driver .pdf:application/pdf}
}

