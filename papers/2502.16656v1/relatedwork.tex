\section{Related Work}
In the following, we introduce general input modalities in the automotive context, provide an in-depth description of AR and VR interaction, and discuss motion effects on interaction. 

\subsection{Input Modalities in Vehicles}
\label{sec:input-modalities-rw}

Vehicle interfaces commonly utilize input modalities such as touch, gaze, and gestures, particularly in the front area, where interaction is most frequent~\cite{jansenDesignSpaceHuman2022}. 
% tactile - touch
%There are various approaches to embed visual, auditory, and tactile/haptic input modalities into vehicles.
%Regarding tactile/haptic input, 
Previous studies on AVs explored the use of touch panels for drivers to initiate maneuvers at the automation limit~\cite{walch_towards_2016, walch_touch_2017} or to select specific AV maneuvers, such as lane changes~\cite{kauer_how_2010}. These touch panels were typically placed either on the steering wheel~\cite{koyama2014multi, pfeiffer2010multi, doring2011gestural} within the center/middle console~\cite{rumelin_free-hand_2013, ng2016investigating, ahmadTouchscreenUsabilityInput2015, walch2019cooperation, colley2021orias}, or on a separate tablet~\cite{colley2022systematic100m}.
% tactile - gesture
Hand gestures were also used for maneuver-based intervention~\cite{detjen_user-defined_2019, colley2022systematic100m} and lateral and longitudinal motion~\cite{manawadu_hand_2016}. 
Similarly, \citet{rumelin_free-hand_2013} and \citet{colley2022systematic100m} utilized free-hand pointing gestures for input, while \citet{fujimura_driver_2013} employed hand-constrained pointing gestures.
% visual - gaze
Eye-gaze as a standalone input was utilized by \citet{poitschke_gaze-based_2011} for referencing or selecting objects~\cite{roider_effects_2017, neselrath_combining_2016}. Additionally, multimodal input was employed to address the challenges associated with unimodal interaction. For instance, gaze was used to localize the target, while hand gestures were used to coordinate pointing~\cite{kim_cascaded_2020, roiderSeeYourPoint2018, colley2023effectsurgency}.
% auditory - speech
Speech input has been implemented to facilitate driver-vehicle cooperation and to select vehicle maneuvers~\cite{ataya_how_2021}. For instance, \citet{roider_effects_2017}, \citet{neselrath_combining_2016}, and \citet{sezgin_multimodal_2009} examined the use of speech commands for selecting objects within the vehicle. However, voice input may be less effective in noisy environments (e.g., during group conversations), and drivers may have limited trust in speech recognition systems or may become confused about the appropriate commands needed to initiate the desired actions~\cite{bengler_hmi_2020, detjen_user-defined_2019}.

Most studies were conducted using low-fidelity driving simulators without motion feedback (e.g.,\cite{gomaa_studying_2020, roider_effects_2017, rumelin_free-hand_2013, roiderSeeYourPoint2018, riegler2020gaze}). However, the vehicle motions induced by road and driving conditions likely impact the results significantly. They may alter the considerations for real in-vehicle interaction proposed in these studies. This is particularly important for studies that measure interaction precision~\cite{gomaa_studying_2020} or completion time~\cite{ng2016investigating}.




\subsection{Interactions in Augmented and Virtual Reality in Vehicles}


Performing mid-air interactions in moving vehicles introduces a distinct set of challenges, stemming primarily from unpredictable vehicular motion. Prior research investigated the usage of touchscreens in moving vehicles~\cite{ahmadInteractiveDisplaysVehicles2014, ahmadTouchscreenUsabilityInput2015, mayerEffectRoadBumps2018, aslanLeapTouchProximity2015, pampelFittsGoesAutobahn2019}, with studies such as those performed by \citet{mayerEffectRoadBumps2018} and Ahmad et al.~\cite{ahmadInteractiveDisplaysVehicles2014, ahmadTouchscreenUsabilityInput2015} having specifically investigated the impact such movements have on touchscreen interactions. \citet{ahmadInteractiveDisplaysVehicles2014} state that road perturbations and vehicle motion can increase erroneous selections. Furthermore, they state that this behavior requires drivers to dedicate more time to performing selection tasks, potentially diverting attention from driving and raising safety concerns. \citet{mayerEffectRoadBumps2018} further explored this topic by using a motion simulator, investigating the effects of road bumps on touchscreen interactions under varying vehicle speeds. They identified a significant reduction in selection accuracy, with vehicle speed not influencing task performance. Furthermore, previous research has considered various aspects like input prediction~\cite{ahmadInteractiveDisplaysVehicles2014, mayerEffectRoadBumps2018} and multimodal input~\cite{roiderSeeYourPoint2018} to improve the usability of such interactions.

However, only limited research was conducted regarding the investigation of interactions performed within AR or VR in vehicle contexts~\cite{schramm2023AssessingAugmentedReality, colleySwiVRCarSeatExploringVehicle2021, kariHandyCastPhonebasedBimanual2023, tsengFingerMapperMappingFinger2023}. Studies by \citet{tsengFingerMapperMappingFinger2023}, and \citet{kariHandyCastPhonebasedBimanual2023} investigated interaction methods that improve interactions within constrained spaces persisting within cars. 
\citet{colleySwiVRCarSeatExploringVehicle2021} used a 1-Degree of Freedom (DoF) motion platform to investigate common interaction methods such as touch, speech, gesture and eye-gaze input in VR regarding task performance. They found that movement negatively affected task performance for eye-gaze and gesture, with touch and speech remaining largely unaffected. Furthermore, \citet{schramm2023AssessingAugmentedReality} investigated multiple interaction methods performed within AR regarding workload, usability, and task performance in a moving vehicle. They found Eye-Gaze with a hardware button as the selection method to be the fastest interaction methods, providing the lowest workload. HeadGaze techniques featured low error rates and a comparably low workload. Hand-pointing with a gesture as confirmation was described as highly frustrating for participants and featured high physical demand.




\subsection{Effects of Vehicle Motion on Interaction}

According to Hock, Colley et al.~\cite{hock2022introducing}, motion and visual fidelity dimensions are essential to classify approaches on the Simulator Continuum. Motion fidelity can range from no motion over motion cues to using a real vehicle. Visual fidelity can range from a 2D screen to the real world. While studies in the real world naturally exhibit the highest external validity, reproducibility or specific situations might only be possible in simulators. Therefore, \citet{colleySwiVRCarSeatExploringVehicle2021} introduced the SwiVR-Car-Seat, representing longitudinal and lateral vehicle dynamics using a 1-DoF rotation. In the SwiVR-Car-Seat, vehicle dynamics in curves are also matched to the chairâ€™s rotation; however, it cannot provide simultaneous motion feedback for both longitudinal and lateral dynamics. Thus, the VAMPIRE by Hock, Colley et al.~\cite{hock2022introducing} introduces a 2-DoF approach where a wheelchair drives in circles to simulate motion forces.

Additionally, on-road driving simulation~\cite{goedicke_vr-oom_2018, bu2024portobello, mcgill20222passengxr, goedicke2022xroom} and the use of a Wizard-of-Oz (WoZ) driver to control the vehicle~\cite{baltodano_rrads_2015, detjen2020wizard} have been proposed. However, these works have not yet used these setups to evaluate the effects of motion on interaction but for visualization purposes only.

Regarding interaction effects of motion, \citet{ng2016investigating} compared pressure input and haptic feedback for in-car touchscreens between a low-fidelity driving simulator and a real vehicle, finding that while accuracy was similar, selection time was worse in the real vehicle. Similarly, \citet{ahmadTouchscreenUsabilityInput2015} showed that vehicle motion increases the effort required for selection. Similar findings were also found by \citet{goode_impact_nodate, salmon_effects_2011, kim_evaluation_2014}.