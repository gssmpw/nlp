\section{Conclusion}


In this work, we introduce a novel framework \textbf{\textit{AnyCharV}} with fine-to-coarse guidance for controllable character video generation under a two-stage training strategy.
% We design a two-stage training strategy to preserve the reference character identity, target scene, and complex human-object interactions.
The self-supervised composition strategy with fine mask guidance in the first stage basically learn to drive a reference image with a target driving video to guarantee the motion correctness and target scene maintenance.
Self-boosting training is then carried out via building interactions between the reference and target characters with coarse mask guidance, where the detailed identity of the reference character can be better preserved.
AnyCharV clearly beats state-of-the-art open-source models and performs just as well as leading closed-source industrial products.
Most importantly, AnyCharV can be used for images and videos created by T2I and T2V models, showing its strong ability to generalize.
