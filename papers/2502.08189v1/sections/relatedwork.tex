\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/framework.pdf}
    \caption{The overview of our proposed AnyCharV. We design a two-stage training strategy with fine-to-coarse guidance for controllable character video generation. In the first stage, we utilize a self-supervised manner to train a base model for integrating a reference character with the target scene $\boldsymbol{s}_d$, guided by fine segmentation mask $\boldsymbol{m}_d$ and 2D pose sequence $\boldsymbol{p}_d$. In the second stage, we propose a self-boosting training strategy by interacting between the reference and target character using coarse bounding box mask guidance. Such a mechanism can better preserve the identity of the reference character and eliminate the bad influence caused by the mask shape. The CLIP encoder and VAE are always frozen. We train denoising UNet, ReferenceNet, and pose guider during the first stage, while only finetuning denoising UNet in the second stage.}
    \label{fig:framework}
\end{figure*}



\section{Related Work}

\subsection{Controllable Video Generation}
Controlling the process of video generation is crucial and has been extensively studied. 
Recent research efforts in this direction often rely on introducing additional control signals, such as depth maps \cite{chai2023stablevideo, zhang2023controlvideo, wang2024videocomposer}, canny edges \cite{zhang2023controlvideo}, text descriptions \cite{zhang2023i2vgen}, sketch maps \cite{wang2024videocomposer}, and motions \cite{wang2024videocomposer}. 
These works achieve control by incorporating these signals into the video generation model, resulting in the desired outcomes.
Meanwhile, other approaches focus on learning high-level feature representations related to appearance \cite{he2024id, wang2024customvideo, wei2024dreamvideo} and motion \cite{wu2024motionbooth, yang2024direct, zhao2025motiondirector} within video diffusion models. 
These methods aim to customize the video generation by learning the identity of subjects and motions.
However, the controls provided by these approaches are often too coarse to meet the stringent requirements for controllable character video generation, where fine-grained control over both character and scene is essential.


% controlnet series, ip-adapter







\subsection{Character Video Generation}

Character video generation has achieved remarkable results recently with the development of diffusion models \cite{ho2020denoising,song2020denoising,lu2022dpm}.
Generating a character video with high-fidelity is challenging due to the complex character motion and various appearance details.
To achieve high-quality generation, some approaches try to train a large model upon vast amounts of data \cite{liu2024sora,yang2024cogvideox,li2024openhumanvid,guo2023animatediff,blattmann2023stable,bao2024vidu}.
In this case, the character video can be directly synthesized by the input text prompt or reference image.
Nevertheless, these direct methods introduce significant randomness when generating the characters, resulting in many artifacts and unsmooth motions.
To tackle these issues, recent researches try to guide the character video generation with 2D pose sequence \cite{hu2024animate,xu2024magicanimate,peng2024controlnext,chang2023magicdance,chang2024magicpose,wang2024holistic,zhai2024idol,wang2024disco}, depth maps \cite{zhu2025champ}, and 3D normal \cite{zhu2025champ}.
However, these approaches are limited to animating the given image within its original background scene, which restricts their applicability in diverse scenarios.




Recent works have proposed more flexible settings by combining arbitrary characters with arbitrary background scenes. An early work, Make-A-Protagonist \cite{zhao2023make}, modifies the foreground character using image prompts and mask supervision based on the Stable Diffusion model \cite{rombach2022high}. 
However, this approach suffers from poor temporal consistency in the generated videos, and the background scene can change unexpectedly, which is undesirable for users. 
MIMO \cite{men2024mimo} addresses this by decoupling the foreground character and background scene for further composition during video generation. Although MIMO's approach is based on 3D representations, it can result in unrealistic video generation. Additionally, inaccuracies in conditioned 3D motion, occlusion, and structure modeling can degrade performance. 
Viggle \cite{viggle2024}, which is also based on 3D representations, encounters similar challenges.
MovieCharacter \cite{qiu2024moviecharacter} introduces a tuning-free framework for video composition using pose-guided character animation and video harmonization \cite{guerreiro2023pct}. 
However, MovieCharacter struggles with complex motions and interactions. 
In contrast to these methods, we address the composition problem using a self-supervised training framework with fine-to-coarse guidance driven by 2D pose guidance, enhancing composition capabilities and improving generation quality.
% Building on this basic model, we propose a self-boosting technique to introduce fine-to-coarse mask guidance in training, enhancing composition capabilities and improving generation quality.


% MIMO, Make