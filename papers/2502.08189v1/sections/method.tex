




\section{Method}

We aim to generate a video featuring our desired character, guided by a target driving video that provides character motion, background scene, and human-object interaction information. This task presents several significant challenges.
Firstly, the appearance of the given character must be well-preserved to ensure that the generated character is vivid and high-fidelity. Secondly, the model should accurately animate the character to follow the motion provided by the target driving video. Thirdly, to ensure the seamless integration of the generated character into the target scene within the corresponding region, the generation model is required to learn the complex human-object interactions and vividly represent these connections.
To address these challenges, we design a two-stage training framework \textbf{\textit{AnyCharV}} with fine-to-coarse guidance.
In the following, we successively describe the preliminary of video diffusion model in Section \ref{sec:preliminary}, the first stage of our framework about training a base model for controllable character video generation with fine mask guidance in Section \ref{sec:composition}, and the second stage of our framework on how to improve the basic generator via our proposed self-boosting training strategy with coarse mask guidance in Section \ref{sec:self_boost}.
An overview of our proposed framework is illustrated in Figure \ref{fig:framework}.


\subsection{Preliminary: Video Diffusion Model}
\label{sec:preliminary}


Video diffusion models (VDMs)~\cite{chen2024videocrafter2,blattmann2023stable,yang2024cogvideox} synthesize videos by successively refining randomly sampled Gaussian noise $\epsilon$. 
This procedure parallels the reversal of a fixed-length Markov chain. 
Through iterative denoising, VDMs capture the temporal relationships embedded in video data. 
Concretely, at each timestep $t \in \{1, 2, \dots, T\}$, a video diffusion model $\Theta$ estimates the noise given an image condition $\boldsymbol{c}_{img}$. 
The training process optimizes a reconstruction objective:
\begin{equation}
\mathcal{L}=\mathbb{E}_{\epsilon, \boldsymbol{z}, \boldsymbol{c}_{img}, t}\left[\left\|\epsilon-\epsilon_{\Theta}\left(\boldsymbol{z}_t, \boldsymbol{c}_{img}, t\right)\right\|_2^2\right],
\label{equ:vdm}
\end{equation}
where $\boldsymbol{z} \in \mathbb{R}^{B \times L \times H \times W \times D}$ denotes the latent representation of the video, with $B$ as the batch size, $L$ the sequence length, $H$ and $W$ the spatial dimensions, and $D$ the latent dimensionality. 
Here, $\epsilon_{\Theta}$ is the model-estimated noise.
% , and $\mathcal{I}$ is a pre-trained image encoder. 
The noisy intermediate state $\boldsymbol{z}_{t}$ is obtained by combining ground-truth features $\boldsymbol{z}_{0}$ with noise $\epsilon$ as $\boldsymbol{z}_{t} = \alpha_{t} \boldsymbol{z}_{0} + \sqrt{1 - \alpha_{t}^{2}} \, \epsilon$, where $\alpha_{t}$ is a diffusion parameter.


\subsection{Self-Supervised Composition with Fine Guidance}
\label{sec:composition}


\paragraph{Naive Self-Supervised Strategy.}
To train our model, it is impractical to collect a large number of video pairs that feature the same character motion and scene but with different characters. 
Therefore, we propose a naive self-supervised strategy to make our training feasible.
Supposing we have a training video $\boldsymbol{X}$, we can easily sample a reference image $\boldsymbol{x}_r$ and a target driving video $\boldsymbol{x}_d$ from $\boldsymbol{X}$.
Given $\boldsymbol{x}_d$, we extract the corresponding character segmentation mask $\boldsymbol{m}_d$ by a segmentation model, such as SAM \cite{kirillov2023segment}.
To this end, we can train the model to generate $\boldsymbol{x}_d$ with the reference character $\boldsymbol{x}_r$ and the target scene $\boldsymbol{s}_d = \overline{\boldsymbol{m}}_d \odot \boldsymbol{x}_d$, where $\overline{\boldsymbol{m}}_d=\boldsymbol{J}-\boldsymbol{m}_d$ is the complementary part of the mask $\boldsymbol{m}_d$, and $\boldsymbol{J}$ is a all-ones matrix.
During training, the target scene $\boldsymbol{s}_d$ is concatenated with the noisy target video and fed into the denoising UNet, as shown in Figure \ref{fig:framework}.
% To this end, we can train the model to generate $\boldsymbol{x}_t$ with the reference character $\boldsymbol{x}_r$, the target character mask $\boldsymbol{m}_t$, the target character pose $\boldsymbol{p}_t$, and the target scene $\overline{\boldsymbol{m}}_t \odot \boldsymbol{x}_t$, where $\overline{\boldsymbol{m}}_t=\boldsymbol{J}-\boldsymbol{m}_t$ is the complementary part of the mask $\boldsymbol{m}_t$, and $\boldsymbol{J}$ is a all-ones matrix.


\paragraph{Reference Prior Preservation.}

As mentioned before, preserving the appearance and identity of the reference character is vital in our studied problem.
Thus, in our design, the identity of the reference character is introduced via two pathways.
Firstly, the reference image $\boldsymbol{x}_r$ is encoded by a pre-trained CLIP \cite{radford2021learning} image encoder and conditioned on the cross-attention layers.
However, such CLIP-style feature injection leads to information loss, due to low resolution ($224 \times 224$) and coarse feature representation.
Secondly, with the success of previous character animation works \cite{hu2024animate,xu2024magicanimate}, we utilize a ReferenceNet to better encode the character appearance.
This ReferenceNet only focus on extracting the spatial features, without temporal layers.
The spatial features from ReferenceNet are concatenated with spatial features from denoising UNet in the spatial attention layers for self-attention modeling.
% Following, a self-attention operation is performed on the concatenated spatial features and the first half of the operated features is extracted for further attention processes.
We remove the background scene in the reference image to avoid introducing redundant context information.




% \paragraph{Mask .}

\paragraph{Fine Mask Supervision.}
With the character appearance preserved, the character should be composed with the target scene in the correct region.
However, naively taking the reference character $\boldsymbol{x}_r$ and the target scene $\boldsymbol{s}_d$ will lead to high randomness of the composition process.
To this end, we further control the generation by introducing the target character segmentation mask $\boldsymbol{m}_d$ to guide the generation region.
This fine segmentation mask is directly concatenated on the noisy target video together with the target scene as a whole.
However, we find that such fine mask will introduce some harmful shape information, especially when there is large difference between the reference and target characters regarding the appearance shape.
In this case, to reduce this redundant information input, we apply strong augmentation onto the input fine character segmentation mask $\boldsymbol{m}_d$ to make it have irregular mask border.
Following, the scene is obtained by masking $\boldsymbol{x}_d$ with the augmented mask $\boldsymbol{m}_d$.



\paragraph{Pose Guidance.}
Smoothly animating the reference character in the target scene is required for compositional generation.
Recent works \cite{hu2024animate,xu2024magicanimate} has achieved great success on animating arbitrary character with arbitrary pose sequence.
Similar with them, we build a lightweight pose guider to embed the target character pose $\boldsymbol{p}_d$, consisting of 4 convolution layers, and the embedded pose images are added to the noisy target video latents.
The pose $\boldsymbol{p}_d$ can be extracted from the target video $\boldsymbol{x}_d$ via a pose detector, such as DWPose \cite{yang2023effective}.
% The pose guider consists of 4 convolution layers, and the embedded pose images are added to the noisy target video latents.




% \paragraph{Classifier-free Guidance.}







\subsection{Self-Boosting Training with Coarse Guidance}
\label{sec:self_boost}

With the self-supervised composition strategy introduced in Section \ref{sec:composition}, we develop a base model for integrating a reference character with a target driving video. 
However, the guidance from the fine segmentation mask can distort the shape of the reference character, resulting in diminished reference identity preservation and overall video quality. 
The proposed mask augmentation mechanism still falls short of achieving optimal results. 
To address this, we introduce a self-boosting strategy to mitigate the negative impact of the fine mask shape. 
Instead of using a precise segmentation mask of the target character for training, we rely on a coarse bounding box mask to indicate the approximate region. 
Additionally, there's no need to extract the target scene separately, as the target video itself can provide the necessary context of the background scene, avoiding excessive shape borders. 
Then we show how to better learn this compositional generation with coarse bounding box mask and original target video, assisted by self-boosting training with the generated videos in the first stage.

% With self-supervised composition strategy proposed in Section \ref{sec:composition}, we obtain a basic model for composing a reference character with a target driving video.
% Nevertheless, the guidance from the fine segmentation mask can alter the shape of the reference character, leading to a decline in reference identity preservation and overall video quality.
% The proposed mask augmentation mechanism can still not achieve an optimal result.
% Thus, we propose a self-boosting strategy to eliminate the bad influence of fine mask shape.
% To achieve this goal, we choose not to provide the precise segmentation mask of the target character for training.
% Instead, we only need the coarse bounding box mask to indicate the approximate region.
% Meanwhile, there is no need to separately extract the target scene out of the original target video, as the target video can be directly used to provide the context information of the background scene to avoid the abundant shape border.
% Then we show how to better learn this compositional generation with coarse bounding box mask and original target video, assisted by self-boosting training with the generated videos in the first stage.


\paragraph{Pair Construction.}

As mentioned in the beginning of Section \ref{sec:composition}, we lack video pairs with the same character motion and scene but different characters to train our model.
Given a base model developed using the self-supervised composition strategy in Section \ref{sec:composition}, it becomes possible to create such pairs through generation.
% After training the model under our proposed self-supervised strategy, we obtain a base model that generates such video pairs, even if the identity of the generated character is not well preserved.
We create the data pairs via drawing samples in the original dataset, where we choose one video for reference character and another for driving video, inspired by the generative enhancement technique \cite{zhu2024generative}. 
Specifically, we randomly sample a frame from the first video to serve as the reference character image $\boldsymbol{x}_r'$ and regard another video as the target driving video $\boldsymbol{x}_d'$.
Given $\boldsymbol{x}_r'$ and $\boldsymbol{x}_d'$, we can generate a result video, where we name it as reference driving video $\boldsymbol{x}_d^r$.
To this end, we obtain a video pair $(\boldsymbol{x}_d^r, \boldsymbol{x}_d')$, in which the character motion, scene, and human-object interaction are all the same while they features different characters.
We randomly sample videos from our training data and build 64,000 video pairs following the above process.

% Supposing a generated video pair $(\boldsymbol{x}_t)$

% These video pairs can be used for recover the original 



% rewrite this para.


\paragraph{Self-Boosting Training.}

To better preserve the identity of the reference character and eliminate the harmful impact of the mask shape, we construct a number of video pairs showing different characters with the same motion and scene information for model training.
During training, instead of separately extracting the scene from the target driving video $\boldsymbol{x}_d'$, we take the generated reference driving video $\boldsymbol{x}_d^r$ as the input to provide the scene information.
To help the model localize the corresponding area of the target character, we provide a coarse bounding box mask $\boldsymbol{m}_d^{c\prime}$ as the coarse guidance.
Given the reference driving video $\boldsymbol{x}_d^r$ and coarse bounding box mask $\boldsymbol{m}_d^{c\prime}$, the model learns to build interactions between the reference and target characters, as well as integrate the reference character with the background scene.
By loosening the mask guidance from fine to coarse, we mitigate the negative influence of the fine mask shape incurred in Section \ref{sec:composition}, resulting in more natural generated videos that better preserve the details of the reference character.
% This approach eliminates the negative influence of the fine mask shape in Section \ref{sec:composition}, resulting in more natural generated videos that better preserve the details of the reference character.





% Specifically, given a constructed video pair $(\boldsymbol{x}_d^r, \boldsymbol{x}_d')$, we want to reversely generate $\boldsymbol{x}_d'$ driven by $\boldsymbol{x}_d^r$ with a reference image $\boldsymbol{x}_r^d$ from $\boldsymbol{x}_d^r$.
% As $\boldsymbol{x}_d^r$ is generated driven by $\boldsymbol{x}_d'$ with another reference character, in which the driving video and the result video are both $\boldsymbol{x}_d'$, we name it a \textit{self-boosting} process. 
% Within this self-boosting strategy, the target driving video $\boldsymbol{x}_d^r$ is directly fed into the denoising UNet without extracting the background scene.
% Also, the concise target character mask is replaced by the much more coarse bounding box mask $\boldsymbol{m}_d^{c\prime}$ to inform the model to pay attention to.
% In this case, the bad influence of mask shape is eliminated and the generated video will be more natural and preserve more details of the reference character.










\subsection{Training and Inference}

\paragraph{Training Strategy.}
The training of our model is in a two-stage manner.
In the first stage, we train the denoising UNet, ReferenceNet, and pose guider and fix VAE \cite{kingma2013auto} and CLIP image encoder \cite{radford2021learning}.
The training objective is as following:
\begin{equation}
\scriptsize
\mathcal{L}_{\text{1}}=\mathbb{E}_{\epsilon, \boldsymbol{z}, \boldsymbol{x}_r, \boldsymbol{s}_d, \boldsymbol{m}_d, \boldsymbol{p}_d, t}\left[\left\|\epsilon\!-\!\epsilon_{\Theta}\left(\boldsymbol{z}_t, \boldsymbol{x}_r, \boldsymbol{s}_d, \boldsymbol{m}_d, \boldsymbol{p}_d, t\right)\right\|_2^2\right].
\label{equ:first_stage}
\end{equation}
In the second stage, we load the trained weights of the denoising UNet, ReferenceNet, and pose guider in the stage1 and only finetune the denoising UNet. 
The training loss is
\begin{equation}
\scriptsize
\mathcal{L}_{\text{2}}=\mathbb{E}_{\epsilon, \boldsymbol{z}', \boldsymbol{x}_r^d, \boldsymbol{x}_d^r, \boldsymbol{m}_d^{c\prime}, \boldsymbol{p}_d', t}\left[\left\|\epsilon\!-\!\epsilon_{\Theta}\left(\boldsymbol{z}_{t}', \boldsymbol{x}_r^d, \boldsymbol{x}_d^r, \boldsymbol{m}_d^{c\prime}, \boldsymbol{p}_d', t\right)\right\|_2^2\right],
\label{equ:second_stage}
\end{equation}
where $\epsilon$ is the random noise, $\epsilon_\Theta$ is the noise prediction from $\Theta$, $\boldsymbol{z}'$ is the latent of target driving video $\boldsymbol{x}_d'$, $\boldsymbol{p}_d'$ is the 2D pose sequence extracted from reference driving video $\boldsymbol{x}_d^r$, and $t$ is the sampling timestep.

\paragraph{Inference.}
During inference, we only need a reference image and a target driving video to generate the desired output. 
The target character's pose and bounding box mask are extracted in advance.
With our proposed two-stage training mechanism, our model can produce high-fidelity videos that accurately preserve the identity of the reference character and the target background scene. 
The generated videos exhibit smooth motions and natural human-object interactions.


% \subsection{}