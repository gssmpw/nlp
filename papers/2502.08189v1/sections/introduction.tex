\section{Introduction}

% {\bf Refine the idea as change the character in the video.}

Recent advancements in diffusion modeling have significantly advanced character video generation for various applications \cite{blattmann2023stable, ho2022video, hong2022cogvideo, wu2023tune, zhang2023i2vgen, wang2024customvideo, wu2024motionbooth}. While these approaches achieve remarkable success with guidance from text and image priors, they often fail to preserve the intrinsic characteristics of the character, such as nuanced appearance and complex motion. Additionally, these methods lack the capability for users to exert individual control during the character video generation process, making it difficult to modify the character's motion and the surrounding scene. These limitations hinder the practical application and development of character video generation technologies.

Several recent works aim to improve the controllability of character video generation by incorporating additional conditions, such as pose \cite{hu2024animate, xu2024magicanimate, zhang2024mimicmotion, zhu2025champ}, depth \cite{xing2025make, zhu2025champ}, and normal maps \cite{zhu2025champ}. 
However, these methods primarily focus on animating characters within fixed background scenes or customizing characters against random backgrounds, lacking the capability for precise background control and often introducing artifacts. 
These limitations reduce the flexibility of character video generation and imposes significant constraints on practical applications. 
Directly replacing a character within a given scene is crucial for applications such as art creation and movie production. 
Some related works attempt to address this issue using 3D priors \cite{men2024mimo, viggle2024} and naive 2D representations \cite{zhao2023make, qiu2024moviecharacter}. However, characters constructed from 3D information often lack realistic interaction appearances, while 2D prior-driven characters suffer from temporal inconsistencies and noticeable jittering.


% Thus, developing a flexible and high-fidelity character video generation framework is crucial.

% change people application

% MIMO, MovieCharacter, Viggle, 3d info as unrealistic, large puzzle (extract 3d information inaccurate) 


% why to do fine-to-coarse, how to do, the results


% In this work, we aim to develop a flexible and high-fidelity controllable character video generation framework. 
% Given the complexity and challenges of our problem, we propose a novel two-stage framework with fine-to-coarse guidance, consisting of a self-supervised composition stage and a self-boosting training stage.
% For the first stage, we build a base model to compose the reference character and the target scene with fine guidance from target character segmentation mask, together with the pose as the conditional signal.
% During this stage, the model learns to compose the source character and target scene with accurate spatial and temporal control, akin to existing literature that focuses on guiding the model with controlled elements. 
% However, due to shape differences between the source and target characters, the generated video often becomes blurry, contains artifacts, and appears less realistic.
% To address this critical issue, which is also a major drawback of existing research, we propose a self-boosting training stage. 
% In this stage, we generate a number of source-target video pairs using the model obtained in the first stage and use the synthesized video for bootstrap training. 
% These video pairs correspond to the same background scene, with the source and target characters in the same pose. 
% Instead of using a fine target character segmentation mask, we employ a bounding box mask to provide coarse guidance for the character area. 
% By focusing solely on the source character and not separately feeding the model the target video scene, we enable character control and detail recovery during self-boosting training. 
% This approach ensures that the details of the source character are better preserved during further inference.
In this work, we aim to develop a flexible and high-fidelity framework for controllable character video generation. 
Given the complexity and challenges of this task, we propose a novel two-stage approach with fine-to-coarse guidance, \textbf{\textit{AnyCharV}}, consisting of a self-supervised composition stage and a self-boosting training stage.
In the first stage, we construct a base model to integrate the reference character and the target scene, using fine guidance from the target character's segmentation mask and the pose as a conditional signal. 
During this phase, the model learns to accurately compose the source character and target scene with precise spatial and temporal control, akin to existing methods that guide the model with controlled elements. 
However, due to shape differences between the source and target characters, the generated video often appears blurry, contains artifacts, and lacks realism.
To address this critical issue, which is a significant drawback of current research, we introduce a self-boosting training stage. 
In this stage, we generate multiple source-target video pairs using the model from the first stage and employ these synthesized videos for bootstrap training. 
These video pairs share the same background scene, with the source and target characters in identical poses. 
Instead of using a fine target character segmentation mask, we utilize a bounding box mask to provide coarse guidance for the character area. 
By focusing solely on the source character and not separately feeding the model the target video scene, we enable better character control and detail recovery during self-boosting training. 
This approach ensures that the details of the source character are better preserved during subsequent inference, as shown in Figure \ref{fig:teaser}.
In summary, our contributions are as follows:
\begin{itemize}
    % \item We propose a novel framework for controllable character video generation with fine-to-coarse guidance that allows for the integration of an arbitrary source character into a target video scene, achieving flexible and high-fidelity results.
    \item We propose a novel framework AnyCharV for controllable character video generation that employs fine-to-coarse guidance. This approach enables the seamless integration of any source character into a target video scene, delivering flexible and high-fidelity results.
    \item We achieve the composition of the source character and target video scene in a self-supervised manner under the guidance of a fine segmentation mask, introducing a simple yet effective method for controllable generation.
    \item We develop a self-boosting strategy that leverages the interaction between the target and source characters with guidance from a coarse bounding box mask, enhancing the preservation of the source character's details during inference.
    \item Our method demonstrates superior generation results, outperforming previous open-sourced state-of-the-art approach both qualitatively and quantitatively.
\end{itemize}


% \begin{itemize}
%     \item current human video generation status
%     \item controllable video generation 
%     \item our method
% \end{itemize}
