\appendix



% {\bf Roadmap.}
% The appendix is organized as the following: 
% Section \ref{sec:appendix_exp_setting} describes the detailed experimental settings;
% Section \ref{sec:appendix_exp_results} shows additional experimental results.
% Section \ref{sec:appendix_limitations} discuss some possible limitations and our future works.




\section{Detailed Experimental Setting}
\label{sec:appendix_exp_setting}


\subsection{Dataset}
We build a character video dataset called CharVG to train our proposed model, which contains 5,482 videos from the internet with various characters.
These videos range from 7 to 47 seconds in length.
We utilize DWPose \cite{yang2023effective} to extract the whole-body poses from these videos, and the estimated poses are rendered as pose skeleton image sequences.
Meanwhile, we use YOLOv8 \cite{reis2023real} to obtain the segmentation and bounding box masks for the characters presented in these videos.

\subsection{Implementation Details}
The ReferenceNet and denoising UNet are both initialized from Stable Diffusion 1.5 \cite{rombach2022high}, where the motion module in the denoising UNet is initialized from AnimateDiff \cite{guo2024animatediff}.
The pose guider is initialized from ControlNet \cite{zhang2023adding}.
In our training, firstly, we train the ReferenceNet, denoising UNet without motion module, and pose guider with individual frames from videos for 50,000 steps to learn the spatial information.
Secondly, we perform self-boosted training on only denoising UNet without motion module with individual video frames for 3,000 steps to help the model eliminate the bad influence of the mask shape.
Above two training processes are both conducted with resolution $768 \times 768$ and batch size 64.
Thirdly, we only train the motion module in the denoising UNet with 24-frame video clips with resolution $768 \times 768$ for 10,000 steps to improve the temporal consistency.
Last, the self-boosted training strategy is performed only on the denoising UNet with resolution $704 \times 704$ for 10,000 steps to better preserve the identity of the reference character.
Above two training processes are both conducted with batch size 8.
All above are trained using learning rate 1e-5.
During inference, the long video is generated sequentially with several short clips and temporally aggregated following \cite{tseng2023edge}.
We use a DDIM \cite{song2020denoising} scheduler for 30 denoising steps with classifier-free guidance \cite{ho2021classifierfree} as 3.0.
All of our experiments are finished on 8 NVIDIA H800 GPUs using PyTorch \cite{paszke2019pytorch}.
The overall training process takes about 3 days.
Our model takes 5 minutes to generate a 5 seconds 24FPS video with resolution $576 \times 1024$ on a single NVIDIA H800 GPU.

\subsection{Evaluation Metrics}
We adopt several metrics to evaluate the generation quality.
1) FVD \cite{unterthiner2018towards} score is a widely used metric for assessing the generated videos. We compute FVD score between our generated videos and 1,000 real character videos from our dataset.
2) Dover++ \cite{wu2023exploring} score is a video quality assessment metric from both aesthetic and technical perspective, demonstrating the overall quality of the generated videos.
3) CLIP Image Score \cite{radford2021learning} is used to evaluate the similarity between the generated video and the reference character, validating the model capability for preserving the identity.


\subsection{Comparison Methods}

We compare our proposed AnyCharV with three state-of-the-art methods. We describe these methods as follows:
\begin{itemize}
    \item Make-A-protagonist \cite{zhao2023make} memorizes the visual and motion information of the target video by fine-tuning the video generation model and guiding the generation of the desired output through masks and reference images during the inference stage.
    \item Viggle \cite{viggle2024} performs rapid 3D modeling of the reference image and manipulates it according to the pose sequence of the input video, thereby accomplishing tasks such as character generation.
    \item MIMO \cite{men2024mimo} decomposes the character's identity, pose, and scene to facilitate video synthesis.
\end{itemize}


\section{Additional Experimental Results}
\label{sec:appendix_exp_results}









\subsection{Qualitative Results}
We show more visualization results with diverse characters, scenes, motions, and human-object interactions in Figure \ref{fig:visualization1} and Figure \ref{fig:visualization2}, further demonstrating the robustness and effectiveness of our proposed method.
% We design a user interface to help human raters easily evaluate the generated videos. 
% Our user interface is illustrated in Figure \ref{fig:user_interface}.



\subsection{User Interface}

We design a user interface to help human raters easily evaluate the generated videos. 
Our designed user interface is illustrated in Figure \ref{fig:user_interface}.


% \subsection{Demo Video}

% We attach a demo video in the supplementary material, including three parts: 1) comparison with state-of-the-art methods; 2) generated videos with our AnyCharV; 3) application: combining with 
% Text-to-Image and Text-to-Video models.
% To be note that MIMO automatically intercepts the first half in the first comparison video.




\section{Limitations and Future Works}

\label{sec:appendix_limitations}

While our AnyCharV model demonstrates impressive results in controllable character video generation, it may struggle when tasked with inferring the back view of a character from a front-facing reference image. 
% Additionally, AnyCharV currently cannot handle the reflection of a character in a mirror, which presents a significant challenge in this context.
Looking ahead, we plan to enhance the generalization capabilities of AnyCharV by integrating a more robust video generator and introducing more robust controls for open-world scenarios \cite{wang2024efficient,che2024gamegen,wang2024open}.





\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/visualization1.pdf}
    \caption{Qualitative visualization results of our method given a reference image (left) and a target video (top).}
    \label{fig:visualization1}
\end{figure*}


\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/visualization2.pdf}
    \caption{Qualitative visualization results of our method given a reference image (left) and a target video (top).}
    \label{fig:visualization2}
\end{figure*}


\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/user_interface.pdf}
    \caption{Our user interface for user study.}
    \label{fig:user_interface}
\end{figure*}