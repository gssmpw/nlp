
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_sota.pdf}
    \caption{Qualitative results of our method compared with previous state-pf-the-art methods. The reference character and target video are shown in the top. Each following line indicates the generated video from a method.}
    \label{fig:comparison_sota}
\end{figure*}







\section{Experiment}





% \subsection{Dataset}
% We build a character video dataset called CharVG to train our proposed model, which contains 5,482 videos from the internet with various characters.
% These videos range from 5 to 15 seconds in length.
% We utilize DWPose \cite{yang2023effective} to extract the whole-body poses from these videos, and the estimated poses are rendered as pose skeleton image sequences.
% Meanwhile, we use YOLOv8 \cite{reis2023real} to obtain the segmentation and bounding box masks for the characters presented in these videos.












% \subsection{Implementation and Evaluation}
\subsection{Experimental Setting}

We build a character video dataset called CharVG to train our proposed model, which contains 5,482 videos from the Internet with various characters.
These videos range from 7 to 47 seconds in length.
The ReferenceNet and denoising UNet are both initialized from Stable Diffusion 1.5 \cite{rombach2022high}, where the motion module in the denoising UNet is initialized from AnimateDiff \cite{guo2024animatediff}.
The pose guider is initialized from ControlNet \cite{zhang2023adding}.
We conduct both qualitative and quantitative evaluation for our method.
For quantitative evaluation, we collect 10 character images and 10 target driving videos from the internet, then generate videos with every image-video pair, which results in 100 evaluation videos.
We adopt FVD \cite{unterthiner2018towards}, Dover++ \cite{wu2023exploring}, and CLIP Image Score \cite{radford2021learning} to evaluate the generation quality.
Please refer to Section \ref{sec:appendix_exp_setting} for detailed experimental settings.


% In our training, firstly, we train the ReferenceNet, denoision UNet without motion module, and pose guider with individual frames from videos for 50,000 steps to learn the spatial information.
% Secondly, we perform self-boosted training on only denoising UNet without motion module with individual video frames for 3,000 steps to help the model eliminate the bad influence of the mask shape.
% Above two training processes are both conducted with resolution $768 \times 768$ and batch size 64.
% Thirdly, we only train the motion module in the denoising UNet with 24-frame video clips with resolution $768 \times 768$ for 10,000 steps to improve the temporal consistency.
% Last, the self-boosted training strategy is performed only on the denoising UNet with resolution $704 \times 704$ for 10,000 steps to better preserve the identity of the reference character.
% Above two training processes are both conducted with batch size 8.
% All above are trained using learning rate 1e-5.
% During inference, the long video is generated sequentially with several short clips and temporally aggregated following \cite{tseng2023edge}.
% We use a DDIM \cite{song2020denoising} scheduler for 30 denoising steps with classifier-free guidance \cite{ho2021classifierfree} 3.0.
% All of our experiments are finished on 8 NVIDIA H800 GPUs using PyTorch.
% The overall training process takes about 3 days.
% Our model takes 5 minutes to generate a 5 seconds 24FPS video with resolution $576 \times 1024$ on a single NVIDIA H800 GPU.




\begin{table}[t]

  \centering
  \tabcolsep=1mm
  \caption{Quantitative results of our method compared with previous state-of-the-art methods. MAP denotes Make-A-Protagonist. The best results are in bold, and second-best results are underlined.}
  \scalebox{0.95}{

  
% Table generated by Excel2LaTeX from sheet '___1'
\begin{tabular}{lccc}
\toprule
Method & FVD$\downarrow$ & DOVER++$\uparrow$ & CLIP-I$\uparrow$ \\
\midrule
MAP \cite{zhao2023make} &   2626.60    &   33.08    &  71.37  \\
Viggle \cite{viggle2024} & \textbf{2321.28} & 50.96 & \uline{73.59} \\
MIMO \cite{men2024mimo} & 2791.39 & \textbf{55.59} & \textbf{73.70} \\
\midrule AnyCharV (ours) & \uline{2582.59} & \uline{52.20} & 73.15 \\
\bottomrule
\end{tabular}%





} 
  \label{tab:comparison_sota}%
\end{table}%


\begin{table}[t]

  \centering
  \tabcolsep=1mm
  \caption{User study results of our method compared with previous state-of-the-art methods. 
  % MAP denotes Make-A-Protagonist.
  % The best results are in bold, and second-best results are underlined.
  }
  \scalebox{0.95}{

  
% Table generated by Excel2LaTeX from sheet '___1'
\begin{tabular}{lccc}
\toprule
Method & Identity$\downarrow$ & Motion$\downarrow$ & Scene$\downarrow$ \\
\midrule
MAP \cite{zhao2023make} &   3.98    &   3.95    &  3.98  \\
Viggle \cite{viggle2024} &   {\bf 1.93}    &    {\bf 1.89}   &  \uline{2.04} \\
MIMO \cite{men2024mimo} &   \uline{2.03}    &    2.18   & 2.05 \\
\midrule
AnyCharV (ours) &   2.07    &    \uline{1.98}   &  {\bf 1.91} \\
\bottomrule
\end{tabular}%



} 
  \label{tab:user_study}%
\end{table}%



\begin{table}[t]
  \centering
  \tabcolsep=1mm
  \caption{The effect of mask augmentation and self-boosting. 
  % The best results are in bold, and second-best results are underlined.
  }
    \scalebox{0.95}{

    
% Table generated by Excel2LaTeX from sheet '___1'
\begin{tabular}{lccc}
\toprule
Method & FVD$\downarrow$ & DOVER++$\uparrow$ & CLIP-I$\uparrow$ \\
\midrule
w/o mask aug. & \uline{2719.46} & 51.09 & \uline{72.68} \\
w/o self-boosting & 2784.62 & \uline{51.24} & 72.41 \\
AnyCharV (ours) & \textbf{2582.59} & \textbf{52.20} & \textbf{73.15} \\
\bottomrule
\end{tabular}%




    }
  \label{tab:boost_aug}%
\end{table}%



\begin{table}[!htb]
  \centering
  \tabcolsep=1mm
  \caption{The effect of different mask types. `Box \& Seg.' indicates that the  bounding box mask is used in the first stage and the segmentation mask is used in the second stage. 
  % The best results are in bold, and second-best results are underlined.
  }
  \scalebox{0.95}{
  
  


% Table generated by Excel2LaTeX from sheet '___1'
\begin{tabular}{lccc}
\toprule
Mask Type & FVD$\downarrow$ & DOVER++$\uparrow$ & CLIP-I$\uparrow$ \\
\midrule
Box \& Seg. & 2971.43 & 50.91 & 72.49 \\
Box \& Box & 2806.74 & 51.28 & \uline{72.71} \\
Seg. \& Seg. & \uline{2759.62} & \uline{51.72} & 72.60 \\
Seg. \& Box & \textbf{2582.59} & \textbf{52.20} & \textbf{73.15} \\
\bottomrule
\end{tabular}%





    }
  \label{tab:mask_type}%
\end{table}%






\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/boost_aug.pdf}
    \caption{Visualization for the effect of mask augmentation and self-boost strategy. The reference character and target video are shown in the top. Each following line indicates the generated video from a variant.}
    \label{fig:boost_aug}
\end{figure}













\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{figures/mask_type.pdf}
    \caption{Visualization for the effect of different mask types. The reference character and target video are shown in the top. Each following line indicates the generated video from a variant. `Box \& Seg.' indicates that the  bounding box mask is used in the first stage and the segmentation mask is used in the second stage.}
    \label{fig:mask_type}
\end{figure}



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/application.pdf}
    \caption{Qualitative results of combining AnyCharV with FLUX \cite{flux2023} and HunyuanVideo \cite{kong2024hunyuanvideo}. The text prompts used for generating reference image and target video are given above them, respectively.}
    \label{fig:application}
\end{figure}







\subsection{Comparison}


We compare our approach with three recent state-of-the-art methods, including Make-A-Protagonist \cite{zhao2023make}, Viggle \cite{viggle2024}, and MIMO \cite{men2024mimo}.

% \begin{itemize}
%     \item Compare: Make-A-Protagonist \cite{zhao2023make}, Viggle, MIMO \cite{men2024mimo}
%     \item multiple people, edit one person
% \end{itemize}


\paragraph{Qualitative Results.}


As shown in Figure \ref{fig:comparison_sota}, Make-a-Protagonist performs worst as for lacking effective spatial and temporal modeling.
Notably, our AnyCharV can preserve more detailed appearance and avoid lots of artifacts than Viggle and MIMO, especially looking at the generated arms and hands in Figure \ref{fig:comparison_sota}.
Moreover, our approach can handle the complex human-object interactions very well, {\it e.g.}, playing basketball, which can not be done with Make-a-Protagonist and Viggle.
These results strongly affirm the effectiveness and robustness of our proposed AnyCharV.

% Here mark



\paragraph{Quantitative Results.}
We show the results in Table \ref{tab:comparison_sota}. 
Our AnyCharV surpasses open-sourced Make-A-Protagonist by a significant margin, with 1.7\% FVD, 51.2\% DOVER++, and 2.49\% CLIP Image Score improvements.
% Similar with qualitative results, Make-A-Protagonist shows worst performance among four methods in terms of all three metrics.
Notably, AnyCharV performs better than MIMO by 8.08\% regarding FVD, indicting the high reality and diversity of our method.
Meanwhile, our proposed AnyCharV outperforms Viggle by 2.43\% in terms of DOVER++ score, demonstrating the high aesthetic and technical quality of our generated videos.
Considering Viggle and MIMO are both closed-source industrial products, our AnyCharV shows great effectiveness and robustness comparing with them.
For inference cost, Make-A-Protagonist, Viggle, MIMO, and AnyCharV take 140, 2, 8, and 5 minutes to generate a 5 seconds 24 FPS video with resolution $576 \times 1024$, respectively, further denoting the high efficiency of our approach.

% Make-A-Protagonist performs worst among four methods.





\paragraph{User Study.}
We conduct human evaluations to further evaluate our approach, comparing with three state-of-the-art methods.
We gather 750 answers from 25 independent human raters, evaluating the identity preservation of the reference character, the motion consistency between the generated video and the target driving video, and the scene similarity and interaction.
The average ranking is computed and shown in Table \ref{tab:user_study}.
It shows that our AnyCharV significantly outperforms the open-source Make-A-Protagonist and performs on par with the closed-source models Viggle and MIMO, demonstrating the effectiveness of our model.









\subsection{Ablation Studies}


We further do some ablation studies for our proposed framework, with deep analysis on self-boosting strategy, mask augmentation, and the choices of mask types.







\paragraph{Self-Boosting Strategy.}

We introduce a self-boosting training mechanism to enhance the identity preservation.
% We firstly study the effect of our proposed self-boosting strategy.
From Figure \ref{fig:boost_aug}, we can learn that without self-boosting training, the identity and appearance of the reference character can not be well preserved, especially for the dressing cloth and face. 
Such performance drop can also be observed in Table \ref{tab:boost_aug}, indicating the effectiveness of our self-boosting training.


\paragraph{Mask Augmentation.}
AnyCharV augments the segmentation mask during the first stage to reduce the negative effect caused by the fine mask shape.
The qualitative and quantitative results shown in Table \ref{tab:boost_aug} and Figure \ref{fig:boost_aug} demonstrate the necessity of our design.
Without mask augmentation, the appearance of the reference character is disturbed greatly.


\paragraph{Mask Type.}
% We find that the mask types used in different training stages matter a lot.
We conduct ablation on mask types where we use different masks during stage 1 and stage 2.
In the first training stage, the model composes the reference character and the target video scene within the accurate region, where the fine segmentation mask is preferred, indicated by the great spatial information loss for both character and background scene, produced by lines `Box \& Seg.' and `Box \& Box' in Figure \ref{fig:mask_type} and Table \ref{tab:mask_type}.
% Thus, using coarse bounding box mask (lines `Box \& Seg.' and `Box \& Box' in Figure \ref{fig:mask_type} and Table \ref{tab:mask_type}) leads to .
In the second training stage, the model is expected to better preserve the details of the reference character.
In this case, we use more loose mask constrain, {\it i.e.}, coarse bounding box mask, to guide the character generation, eliminating the adverse effect caused by the fine segmentation mask shape.
Such performance improvement can be validated in Table \ref{tab:mask_type} and Figure \ref{fig:mask_type}.

















\subsection{Application}


Our AnyCharV can also be used to generate a video with a reference image generated by text-to-image (T2I) models, {\it e.g.}, FLUX \cite{flux2023}, driving by a target video generated by text-to-video (T2V) models, {\it e.g.}, HunyuanVideo \cite{kong2024hunyuanvideo}.
As shown in Figure \ref{fig:application}, we first utilize FLUX to generate a reference image and HunyuanVideo to synthesize a target video, then the generated target video is used to drive the synthesized reference character.
These results clearly the strength and flexibility of our AnyCharV, proving its versatility.


