\section{Background and Related Work}
\label{sec:background}
\subsection{Deep Assertion Generation}
\label{sec:background_AG}



With the success of deep learning (DL), researchers have increasingly been utilizing advanced DL techniques to automate a variety of software engineering tasks____.
For example, Watson~\etal____ propose \atla{}, the first DL-based assertion generation approach that utilizes deep neural networks to learn correct assertions from existing test-assert pairs.
Yu~\etal____ propose two retrieval-based approaches to generate assertions by searching for similar assertions given a focal-test, namely IR-based assertion retrieval (\irar{}) and retrieved-assertion adaptation ($RA_{adapt}$ ).
\irar{} retrieves the assertion with the highest similarity to the given focal-test using measures like Jaccard similarity.
As \irar{} may not always retrieve completely accurate assertions, $RA_{adapt}$ attempts to revise the retrieved assertions by replacing tokens within them.
Two strategies are proposed for determining replacement values, \ie a heuristic-based approach \rah{} and a neural network-based approach \rann{}.
Furthermore, Yu~\etal____ propose an integration-based approach called \inte{} by integrating IR and DL techniques.
Building on \inte{}, Sun~\etal____ introduce \edit{} by searching for a similar focal-test, which is then modified by a neural model.
Unlike prior AG studies retrieving relevant assertions____ with token matching or training generator from scratch____ with historical test-assert pairs, our work attempts to utilize PLMs as an assertion generator with the help of a hybrid assertion retriever for more effective retrieval and generation.

The literature has also witnessed several studies____, exploring the use of PLMs like T5____ in the field of assertion generation.
For instance, Mastropaolo~\etal____ pre-train a T5 model and fine-tune it on four code-related tasks: bug-fixing, mutant generation, assertion generation, and code summarization.
These studies typically pre-train language models from scratch and fine-tune them on multiple downstream tasks.
In contrast, our work attempts to propose a specific AG approach by leveraging off-the-shelf PLMs. 
Nashid~\etal____ propose CEDAR, a large language model (LLM)-based approach to apply retrieval-based demonstration selection strategy for program repair and assertion generation.
\toolname{} and CEDAR are fundamentally distinct regarding their learning paradigms, retrievers, and generators.
First, CEDAR utilizes few-shot learning with prompt engineering, while \toolname{} leverages fine-tuning with augmented inputs.
Second, CEDAR utilizes a single retriever, while \toolname{} builds a hybrid retriever to identify relevant assertions jointly, indicating superiority over a single retriever. 
Third, CEDAR queries APIs from a black-box LLM Codex with 12 billion parameters, whereas RetriGen fine-tunes an open-source PLM CodeT5 with only 220 million parameters. 
\revise{Recently, Zhang~\etal____ conduct an empirical study to explore the potential of several PLMs for generating assertions in a fine-tuning scenario.}


\subsection{Pre-trained Language Model}
\label{sec:background_LLM}


Recently, researchers have explored the capabilities of PLMs to revolutionize various code-related tasks____, such as code review____, test generation____ and program repair____.

There exist three main categories based on model architectures.
(1) Encoder-only PLMs, \eg CodeBERT____, train the encoder part of the Transformer with masked language modeling, thus suitable for code understanding.
(2) Decoder-only PLMs, \eg CodeGPT____, train the decoder part of the Transformer with unidirectional language modeling, thus suitable for auto-regressive generation.
(3) Encoder-decoder PLMs, \eg CodeT5____, train both encoder and decoder parts of the Transformer with denoising objectives, thus suitable for sequence-to-sequence generation.

In our work, the foundation model selection of \toolname{} is limited to encoder-decoder PLMs, as assertions are generated in a sequence-to-sequence learning manner.
Following prior work____, we consider CodeT5, a generic code-aware language model that is pre-trained on a large code corpus, achieving state-of-the-art performance in both code understanding and generation tasks.
CodeT5 is the most popular encoder-decoder PLM that is adopted by previous fine-tuning-based sequence-to-sequence code generation tasks____.
\revise{Besides, CodeT5 is trained with CodeSearchNet____ without test code snippets, thus avoiding the data leakage issue.}



\subsection{Information Retrieval for SE}
\label{sec:background_IR}

Information Retrieval (IR) refers to the process of identifying relevant information from a large collection of data.
IR has been extensively applied to a range of code-related tasks, such as fault localization____ and test case prioritization____.
Such approaches search for the object that best matches the given query from the database based on different similarity coefficients, such as Jaccard similarity.
Besides, the literature has seen an increasing number of studies performing generation tasks with the retrieval-augmented paradigm, such as code repair____ and code summarization____.
This paradigm improves the quality of generated results by grounding the model with external knowledge sources to supplement the PLM's internal representation of information____.
In our work, inspired by the intuition of retrieval-augmented generation in the PLM domain, we focus on the assertion generation problem and propose \toolname{} to fine-tune a PLM-based assertion generator with a hybrid assertion retriever.