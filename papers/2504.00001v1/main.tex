\documentclass[conference]{IEEEtran}
\usepackage{epsfig,amsmath,amsfonts}
\usepackage{colortbl}
\usepackage{amsthm}
\graphicspath{{.}{./figures/}}
\newcommand{\TODO}[1]{{\color{red}{\bf TODO:} #1}}
\newcommand{\Question}[1]{{\color{red}{\bf Question:} #1}}
\newcommand{\Answer}[1]{{\color{cyan}{\bf Answer:} #1}}
\newcommand{\comment}[1]{}
\newcommand{\Fmin}{F_{h-}}
\newcommand{\Fmax}{F_{h+}}
\newcommand{\shortcaption}{} % redefined below
\newtheorem{theorem}{Theorem}
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{1.5\baselineskip}{1\baselineskip}
\titlespacing*{\subsection}{0pt}{1.2\baselineskip}{1\baselineskip}
\titlespacing*{\subsubsection}{0pt}{1.2\baselineskip}{1\baselineskip}


\begin{document}

\title{HistogramTools for Efficient Data Analysis and Distribution Representation in Large Data Sets}
\author{
    \IEEEauthorblockN{Shubham Malhotra}
    \IEEEauthorblockA{
        Alumnus, Rochester Institute of Technology, Rochester, NY, USA \\
        Email: shubham.malhotra28@gmail.com
    }
}


\maketitle

\begin{abstract}
Histograms provide a powerful means of summarizing large data sets by representing their distribution in a compact, binned form. The HistogramTools R package enhances R's built-in histogram functionality, offering advanced methods for manipulating and analyzing histograms, especially in large-scale data environments. Key features include the ability to serialize histograms using Protocol Buffers for distributed computing tasks, tools for merging and modifying histograms, and techniques for measuring and visualizing information loss in histogram representations. The package is particularly suited for environments utilizing MapReduce, where efficient storage and data sharing are critical. This paper presents various methods of histogram bin manipulation, distance measures, quantile approximation, and error estimation in cumulative distribution functions (CDFs) derived from histograms. Visualization techniques and efficient storage representations are also discussed alongside applications for large data processing and distributed computing tasks.
\end{abstract}

\section{Introduction}

In many cloud-scale systems, monitoring, measuring, and logging
performance metrics is extremely difficult because there are literally
billions of possibly interesting metrics. For example, in a large
distributed file system, it is possible to monitor the resource usage,
throughput, and latency per active user so that the
sources of performance anomalies~\cite{tailscale} can be tracked down. To detect
interactions with other applications and further narrow down the
source of performance issues, monitoring such metrics for
combinations of users and resources can be done.  However, the memory requirements
for comprehensive logging at this scale are usually exorbitant. An
alternative is to maintain aggregate statistics, such as the empirical
distributions of the metrics, as histograms. Even so, the memory
requirements for maintaining histograms for a large number of metrics
can be burdensome; as such, it is critical to make the histograms as
efficient as possible to minimize information loss while limiting the
memory used.


In cloud data centers, it is common to collect histograms per user,
per server for a variety of metrics - IO delays, network latencies,
CPU throttling, etc. - so that Service Level Agreements can be
monitored and resources allocated appropriately.  For example,
\textbf{Blinded System} collects histograms for the ages of files read and also the age of files stored per workload group, which can be individual users, subsets of
files from each user or even groups of columns from a user's
tables. Since there can be tens of thousands of servers, thousands of
users, and tens of metrics being monitored per user/server, there can
be billions of histograms maintained in a data center.

Memory requirements for histograms can be reduced in several ways.
Coarser bins could be used, but this reduces the fidelity of the histogram. We
could dynamically adjust bin boundaries to improve accuracy, but
this requires additional processing on sensitive nodes, and may introduce
non-deterministic overhead, and makes aggregation difficult or impossible across
computers.


The effectiveness of using {\em polynomial
  histograms,} is explored, where the number of bins is reduced, but the
distribution of samples within each bin is maintained using a
low-order polynomial. For a fixed amount of memory, when is it
preferable to store polynomial annotations in coarser bins?

The contributions are as follows: (1) The information loss
due to normal (fixed bin) histograms is compared to those with a moment annotation;
(2)The errors made empirically is compared to that of some empirical distributions of system metrics
in a cloud environment; and (3)Rules of thumb is given for when
using polynomial histograms is effective.


\section{Background}

A variety of techniques have been developed to make synopses of
massive data sets.  Streaming quantile algorithms
\cite{chambers2006monitoring} keep an approximation of a given
quantile of the observed values in a stream.  These algorithms are
most useful when there are a small number of quantiles of interest,
but they do not offer a density estimate across the full distribution
for cases where a variety of downstream data analysis will be done
based on the synopsis.

There has been a lot of work in the database community on histograms
that dynamically adjust bin breakpoints as new data are seen to
minimize error, but these methods are less useful for distributions
with a small number of samples, and the resulting irregular bins are
harder to combine as part of a distributed computation.

Information loss metrics for fixed-boundary histograms of file system parameters are
explored in \cite{douceur1999large}. A similar information
loss metrics is utilized but also considered the space versus information loss
trade-off of adding additional moments to each bin of the histogram to
build low-order ``Polynomial'' \cite{sagae1997bin} or ``Spline''
\cite{Poosala:1997:HET:269157} histograms.



As with the work of K{\"o}nig and Weikum \cite{konig1999combining} we
do not require continuity across bucket boundaries and
find that this attribute is essential in order to accurately capture
large jumps in bucket frequencies.  Unlike that work, however, it is
not focused on optimal dynamic partitioning of bucket boundaries and
the information loss metric is focused on making definitive statements.

Instead, the
focus is on constrained resource environments where the computational
and memory requirements of those techniques would be excessive.

\subsection{Information Loss Due to Binning}

Binning of an empirical
distribution into a histogram representation introduces a form of
preprocessing that constrains all later analyses based on that data
\cite{blocker2013potential}.  Bin breakpoints are often fixed in
advance for specific system quantities to reduce the computational
over head of keeping track of many different histograms.
However, bin breakpoints that are
poorly chosen relative to the underlying data set may introduce
considerable error when one tries to compute means or percentiles
based on the histogram representations.  This is especially true for
the exponentially bucketed (e.g. buckets that double in size)
representations of distributions, such as
latencies or arrival times that have a large dynamic range.


In evaluating representations of system distributions, the
\emph{Earth Mover's Distance of the Cumulative Curves} (EMDCC) is defined as the information
loss metric.  In particular, if $X$ is the (unknown) underlying data set with
distribution $F$, and $h$ is the data representation, r is the range of the representation,
then the upper and lower bounds $F_{h+}(x) \quad F_{h-}(x)$ are defined as the highest and lowest possible
values for the true distribution given the observed representation and the
EMDCC as the normalized L1 distance between them:

$$ EMDCC(X, h) = \frac{1}{r} \int_{\mathbb{R}} |F_{h+}(x) - F_{h-}(x)|dx $$

\noindent noting that in the case that $h$ is a histogram bucketing scheme,
$F_{h-}$ always puts its mass on the left endpoints and $F_{h+}$ always puts
near the right endpoints.  The Earth Mover's Distance
\cite{rubner2000earth} is also known as the
Mallows distance, or Wasserstein distance with $p=1$ in
different communities.

Figure~\ref{fig:emdcc} shows a histogram (left) along with the CDF representation and the
associated area of uncertainty (in yellow).  Any underlying dataset
having the given histogram representation must have a true ecdf lying
entirely within the yellow area.  A histogram with more granular
buckets would reduce the information loss at the expense of additional
storage space to store the buckets.

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{exhists-crop.pdf}
\caption{An example histogram (left) with its CDF representation and a
  yellow area of uncertainty showing where the true empirical cdf of
  the unbinned data must lie (right).}
\label{fig:emdcc}
\end{figure}


Adding more buckets, as in this example, usually reduces the EMDCC, but
are there more efficient ways to reduce the EMDCC for a given amount
of storage space?

\subsubsection{First Moment}

Suppose that $\mu = \mu_1$ is known, but $\mu_2$ is not.
The goal in this section is to determine $\Fmin$, $\Fmax$, and
EMDCC.


For any value of $x$, distributions that minimize
$F_{-}(x)$ and maximize $F(x)$ can be found, to determine $\Fmin$ and $\Fmax$.
Surprisingly, a single distribution does both---the distribution that
places maximum mass at $x$, subject to $\mu$.

For $x \le \mu$, let $F_1$ be the distribution
with mass at $x$ and $1$,
with $P(X = x) = p_1 = (1-\mu)/(1-x)$ and $P(X = 1) = 1-p_1$.
($F_1$ and $p_1$ are for notational convenience, and depend on $x$.)
This distribution has $F_1(x) = 0$ and $F_{1+}(x) = p_1$, the mimimum
and maximum possible given $E(X) = \mu$ (proof below).
Then $\Fmin(x) = 0$ and $\Fmax(x) = p_1$.
There are other distributions that achieve the same $\Fmin$,
e.g. the distribution with $P(X=\mu) = 1$.

Similarly, for $x > \mu$, let $F_2$ have mass at $0$ and $x$,
with $P(X = x) = p_2 = \mu/x$ and $P(X=0) = 1-p_2$,
$\Fmin(x) = 1-p_2 = 1 - \mu/x$ and $\Fmax(x) = 1$.

These distributions are summarized in the first two cases in
Table~\ref{table:optimalDistributions}
and shown in the left column of
Figure~\ref{fig:densities}.
The lower and upper bounds $\Fmin$ and $\Fmax$ are shown
in Figure~\ref{fig:bounds}.



\begin{table*}[htbp]
\centering
\caption{Distributions that minimize $\Fmin$ and maximize $\Fmax$. $f(\cdot)$ is the probability that the corresponding distribution ($F_1$, $F_2$, $F_3$, $F_4$) places on $\cdot$.}
\label{table:optimalDistributions}
\scriptsize % Reduce font size
\renewcommand{\arraystretch}{1.3} % Increase row spacing
\begin{tabular}{|p{2.5cm}|p{4.5cm}|p{4.5cm}|}
\hline
\textbf{Case} & \textbf{Distribution} & \textbf{Used Below} \\ \hline
% Case 1
$\begin{array}{l}
  x \leq \mu \\
  \mu_2 \text{ unknown}
\end{array}$ &
$\begin{array}{l}
  F_{1}\\
  f(x) = p_1\\
  f(1) = 1 - p_1\\
  p_1 = \frac{1-\mu}{1-x}
\end{array}$ &
$\begin{array}{l}
  c_1 = \mu - \frac{\sigma^2}{1-\mu}\\
  \sigma_{F_1}^2 = (1-\mu)(\mu-x)
\end{array}$ \\ \hline

% Case 2
$\begin{array}{l}
  x > \mu \\
  \mu_2 \text{ unknown}
\end{array}$ &
$\begin{array}{l}
  F_{2}\\
  f(0) = 1 - p_2\\
  f(x) = p_2\\
  p_2 = \frac{\mu}{x}
\end{array}$ &
$\begin{array}{l}
  c_2 = \mu + \frac{\sigma^2}{\mu}\\
  \sigma_{F_2}^2 = (x-\mu)(x-0)
\end{array}$ \\ \hline

% Case 3
$\begin{array}{l}
  x < c_1 \, | \, x > c_2 \\
  \sigma^2 < \sigma^2_*
\end{array}$ &
$\begin{array}{l}
  F_{3}\\
  f(x) = p_3\\
  f(a) = 1 - p_3\\
  p_3 = \frac{\sigma^2}{\sigma^2 + (x-\mu)^2}\\
  a = \mu + \frac{\sigma^2}{\mu-x}
\end{array}$ & \\ \hline

% Case 4
$\begin{array}{l}
  c_1 \leq x \leq c_2 \\
  \sigma^2 \geq \sigma^2_*
\end{array}$ &
$\begin{array}{l}
  F_{4}\\
  f(0) = 1 - p_4 - f(1)\\
  f(x) = p_4\\
  f(1) = \mu - x p_4\\
  p_4 = \frac{\mu-\mu_2}{x-x^2}
\end{array}$ & \\ \hline
\end{tabular}
\end{table*}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{densities.pdf}
\caption{Distributions that minimize $\Fmin$ and maximize $\Fmax$.
The left column has $\mu_2$ unknown ($F_2$ and $F_1$).
The middle column has variance increasing from 0 to $\sigma_*^2$ ($F_3$).
The right column has variance increasing from $\sigma_*^2$ to the
maximum ($F_4$).
\label{fig:densities}
}
\end{figure}


The EMDCC is the integral of the difference between upper and lower
bounds, given by $p_1$ or $p_2$:
\begin{eqnarray}
  \mbox{EMDCC}
  &=& \int_0^1 \Fmax(x) - \Fmin(x) du \nonumber \\
  &=& \int_0^\mu p_1 dx + \int_\mu^1 p_2 dx \nonumber \\
  &=& \int_0^\mu (1-\mu)/(1-x)dx + \int_\mu^1 1 - (1 - \mu/x) dx \nonumber \\
  &=& -(1-\mu) \log(1-\mu) - \mu \log(\mu)
\end{eqnarray}


The EMDCC approaches 0 for $\mu$ near 0 or 1, and has a maximum value
of $\log(2) = .69$ when $\mu = 1/2$. This EMDCC is smaller than for
a non-polynomial histogram with twice as many bins
for $0 \le \mu < 0.1997$ or $0.8003 < \mu \le 1$.

\begin{theorem}
$F_1$ and $F_2$ minimize $F_{-}(x)$ and maximize $F(x)$,
for $x \le \mu$ and $x \ge \mu$, respectively.
\end{theorem}

\begin{proof}
The case $x = \mu$ is trivial; both $F_1$ and $F_2$ reduce to
$F_x$, the distribution with a point mass at $x$, which optimizes
both objectives.

Consider the case with $x < \mu$; it is claimed that $F_1$ given above is optimal,
with density (point mass) $f^*(x) = p_1$
and $f(1) = 1-p_1$, and zero elsewhere.
This has the minimum possible lower bound $F_{1-}(x) = 0$,
so consider the upper bound.

Suppose some other distribution $F$ has mass elsewhere. Then:

\begin{align}
    \mu &= \int_0^1 u \, dF(u) \\
    &= \int_0^{x-} u \, dF(u) + x P(X = x) + \int_{x+}^{1-} u \, dF(u) + P(X = 1) \\
    &< \int_0^{x-} x \, dF(u) + x P(X = x) + \int_{x+}^{1-} 1 \, dF(u) + P(X = 1) \\
    &= x F(x) + 1 - F(x).
\end{align}

Solving for \( F(x) \) gives:
\begin{equation}
    F(x) < \frac{1 - \mu}{1 - x} = p_1,
\end{equation}
which is inferior to \( F_1(x) \).

\subsection{Case Analysis}

The case with \( x > \mu \) is similar; \( F_2 \) is optimal for both \( \Fmin \) and \( \Fmax \). The solutions for this case can be obtained from the previous using:
\begin{align*}
    X' &= 1 - X, \\
    x' &= 1 - x, \\
    \mu' &= 1 - \mu.
\end{align*}
\end{proof}

\subsubsection{Second Moment}

Now suppose that the first two moments are known.
For any value of $x$, distributions that minimize
$F_{-}(x)$ and maximize $F(x)$ are sought, to determine $\Fmin$ and $\Fmax$.
As before, surprisingly, a single distribution
does both---the distribution that places maximum mass at $x$, subject
to the moments.

As before, there are two cases. It is easiest to
think of these as the ``small variance'' and ``large variance'' cases,
though they also correspond to values of $x$.
Let $\sigma^2 = \mu_2 - \mu^2$.
Let $\mu_2^*$ be the second moment for $F_1$ or $F_2$, for
$x \le \mu$ or $x > \mu$ respectively, and
$\sigma_*^2$ be the corresponding variance.
it can also be written as $\sigma^{*2}$ as
$\sigma_{F_1}^2 = (1-\mu)(\mu-x)$
or
$\sigma_{F_2}^2 = (x-\mu)(x - 0)$, respectively.

% May eliminate some of that notation if I don't use it.

There are three sub-cases to consider:
$\mu_2 < \mu_2^*$,
$\mu_2 > \mu_2^*$, and
$\mu_2 = \mu_2^*$.
The third case is trivial; the solution is the same as for $\mu_2$ unknown,
either $F_1$ or $F_2$.

Consider the small variance case, with $\mu_2 < \mu_2^*$.
Recall that $F_1$ places mass at $x$ and $1$,
and $F_2$ at $0$ and $x$.
With smaller $\mu_2$ (and smaller variance), the optimal solution
is again a two-point distribution,
with support at $x$ and $a$.
Solving the moment equations gives
$a = \mu + \sigma^2/(\mu-x)$,
with $P(X = x) = p_3 = \sigma^2 / (\sigma^2 + (\mu-x)^2)$ and
$P(X = a) = 1-p_3$.
this distribution $F_3$ below is called.
It reduces to either $F_1$ or $F_2$
(i.e.\ $a=1$ or $a=0$) when $\mu_2 = \mu_2^*$.
As $\sigma^2$ shrinks, $a$ moves from $0$ or $1$ toward $\mu$,
and mass moves from $x$ to $a$.

\begin{theorem}
For $\mu_2 < \mu_2^*$,
$F_3$ maximizes $F(x)$ and minimizes $F_{-}(x)$, subject to $\mu$
and $\mu_2$.
\end{theorem}

In particular,
for $x < \mu$, $F_3$
has $F_{-}(x) = 0$ (the smallest possible) and
$F(x) = p_3$, the largest possible given the constraints.
For $x > \mu$, $F_3$
has $F(x) = 1$ (the largest possible) and
$F_{-}(x) = f_x$, the smallest possible given the constraints.
This case ($\mu_2 < \mu_2^*$) does not occur when $x = \mu$.

\begin{proof}
Consider the case with $x < \mu$.
For any $F$, the first two moments can be decomposed as
\begin{eqnarray}
  \mu &=& \int_0^x u dF(u) + \int_{x+}^1 u dF(u) \nonumber \\
      &\le& x F(x) + \int_{x+}^1 u dF(u) \nonumber \\
  \sigma^2 &=& \int_0^x (u-\mu)^2 dF(u) + \int_{x+}^1 (u-\mu)^2 dF(u) \nonumber\\
       &\le& (x-\mu)^2 F(x) + \int_{x+}^1 (u-\mu)^2 dF(u) \nonumber
\end{eqnarray}
with equality if $P(X < x) = 0$.
From the first inequality, the following is obtained
$\int_{x+}^1 u dF(u) \ge \mu - x F(x)$, so
the conditional mean satisfies $E(X | X > x) \ge (\mu - x F(x))/(1-F(x)$.
Second,
$$\int_{x+}^1 (u-\mu)^2 dF(u) \le \sigma^2 - (x-\mu^2) F(x).$$
But from the conditional mean and Jensen's inequality have
\begin{eqnarray}
  \int_{x+}^1 (u-\mu)^2 dF(u)
  &\ge& (E(X | X > x) - \mu)^2P(X > x) \nonumber \\
  &\ge& \left( \frac{\mu-x F(x)}{1-F(x)} - \mu\right)^2 (1-F(x)) \nonumber \\
  &=& F(x)^2 (\mu-x)^2 / (1-F(x)) \nonumber
\end{eqnarray}
with equality
if $F$ has conditional variance zero for $X > x$ and $P(X < x) = 0$.

Combining inequalities, it is given 
$F(x)^2 (x-\mu)^2 / (1-F(x)) \le \sigma^2 - (x-\mu^2) F(x)$
with equality if $F$ has two point masses, one at $x$.
This simplifies to
$$ F(x) \le \sigma^2 / (\sigma^2 + (x - \mu)^2) = F_{1A}(x).$$
Hence no other distribution can have larger $F(x)$.

The case with $x > \mu$ is similar.
\end{proof}

For the large variance case,
with $\mu_2 > \mu_2^*$,
the variance is larger than $\sigma_*^2$,
and as the variance increase
the optimal solution moves mass
from $x$ to $0$ and $1$.
Let $F_4$ be the distribution with mass at $0$, $x$, and $1$ that
satisfies the two moment constraints; this gives
$F_4(x) = p_4 = (\mu-\mu_2)/(x-x^2)$,
$F_4(1) = \mu - x p_4$, and
$F_4(0) = 1 - p_4 - F_4(1)$.

\begin{theorem}
For $\mu_2 \ge \mu_2^*$,
$F_4$ maximizes $F(x)$ and minimizes $F_{-}(x)$, subject to $\mu$
and $\mu_2$.
\end{theorem}

\begin{proof}
For any $F$,
suppose there is mass between $0$ and $x$.
Then move that mass to 0 and 1, while keeping the same mean
(If $b$ is the conditional mean given $0 < X < x$, then move
fraction $b/x$ of the mass to $x$ and the rest to $0$).
Similarly, if there is mass between $x$ and $1$, move that
mass to $x$ and $1$ while keeping the same mean.
Call the resulting distribution $F'$; it has the same mean and
larger variance than $F$,
and objective functions that are at least as good:
$F_{-}'(x) \le F_{-}(x)$ (with equality if $F$ has no mass in $(0,x)$)
and
$F'(x) \ge F(x)$ (with equality if $F$ has no mass in $(x,1)$).

The objective functions can be further improved by reducing
the variance to the desired value, using a linear combination of $F'$
and $F_x$.
Let $F'' = \lambda F' + (1-\lambda) F_x$,
where
% $F_x$ is the distribution with a point mass at $x$ and
$\lambda = \sigma^2 / \sigma^2_{F'}$.
$F''$ has moments $\mu$ and $\mu_2$
and objective functions
$F_{-}''(x) = \lambda F_{-}'(x) < F_{-}(x)$
and
$F''(x) = \lambda F'(x) + (1-\lambda) > F(x)$.
In other words, if a distribution has mass anywhere other than $0$, $x$,
and $1$, both objective functions can be improved.
Hence $F_4$, the only distribution with mass at only those three points
that satisfies both moment conditions, is optimal.
\end{proof}

It is earlier expressed the boundary between small and large variance
cases according to whether $\sigma^2 < \sigma_*^2$.
For $x < \mu$ this simplifies to $x < c_1 = \mu - \sigma^2 / (1-\mu)$
and for $x > \mu$ it simpifies to $x > c_2 = \mu + \sigma^2 / \mu$.
In other words, the large variance case occurs when
$c_1 \le x \le c_2$,
and the small variance case when $x$ is outside these bounds.

The distributions are summarized in Table~\ref{table:optimalDistributions}.
These distributions are summarized in the first two cases in
Table~\ref{table:optimalDistributions}
and shown in the left column of
Figure~\ref{fig:densities}.
The lower and upper bounds $\Fmin$ and $\Fmax$ are shown
in Figure~\ref{fig:bounds}.


The EMDCC is the integral of the difference between upper and lower
bounds; the difference is given by $p_3$ or $p_4$:
\begin{eqnarray}
  \mbox{EMDCC}
  &=& \int_0^1 \Fmax(x) - \Fmin(x) du \nonumber \\
  &=& \int_0^{c_1} p_3 dx + \int_{c_1}^{c+2} p_4 + \int_{c_2}^1 p_3 dx\nonumber \\
  &=& |_0^{c_1} P_3 + |_{c_1}^{c+2} P_4 + |_{c_2}^1 P_3 \nonumber \\
\end{eqnarray}
where $P_3 = \sigma \tan^{-1}((x-\mu)/\sigma)$ and
$P_4 = (\mu - \mu_2)\log(x/(1-x))$ are antiderivatives of $p_3$ and $p_4$.


\begin{figure}
\centering
\includegraphics[width=\linewidth]{bounds.pdf}
\caption{Lower and upper bounds $\Fmin$ and $\Fmax$, for $\mu$
unknown and small, middle and large variances.
\label{fig:bounds}
}
\end{figure}


\subsection{Polynomial Histograms}

Given a fixed amount of storage space, the granularity of
histogram buckets can be traded for additional statistics within each bucket.  For example,
in addition to storing the counts between histogram boundaries $(a, b)$, we
could also store the mean and higher moments.  Histograms with
annotations of moments per bin are known as \emph{Polynomial
Histograms} \cite{sagae1997bin}.  Storing the moments is appealing in
a distributed systems context because merging histograms with the same
bucket boundaries remains trivial.  The notation $H(b,p)$ can be used to
denote a histogram with $b$ bins annotated with the $p$-moments of
each bin.

Knowing the first moment can help a lot when it is near the boundary; the
EMDCC associated with bucket $(a, b)$ will be zero if the mean is $a$.

In general, with many points in a bucket, a continuous approximation says that a mean of $\mu = \alpha * a + (1-\alpha) * b$ gives an EMDCC of

$$
\lambda(\alpha) = \alpha * ln(\frac{1}{\alpha}) + (1 - \alpha) * ln(\frac{1}{1-\alpha})
$$

The function $\lambda(\alpha)$ is symmetric around $0.5$, is increasing up to it's max of $\lambda(0.5) \approx 0.7$, integrates to $0.5$, and $\lambda(0.2) = \lambda(0.8) \approx 0.5$.  Since bisection always halves the EMDCC, this gives rules of thumb about the merits of bisection vs. storing the mean; if $\alpha < 0.2$ or $\alpha > 0.8$, then storing the mean is better, storing the mean can be worse than bisection by 40\% but it can also be infinitely better if $\alpha's$ are uniformly distributed and independent of the counts per bucket then bisection and storing the mean should give the same reduction on average.  If the true density is smooth enough relative to the bucketing scheme, then $\alpha$ will tend to be closer to $\frac{1}{2}$, which implies inferiority of keeping the mean with respect to the EMDCC metric.


\noindent
\textbf{Proof:} \\

The attention is restricted to $(a,b)$ where it is also known as the $p^{th}$ moment$\big(\mu_p^p = \frac{1}{n} \sum_{i=1}^n x_i^p \big)$.  Construct $F_{h+}(x), F_{h-}(x)$ pointwise as the upper and lower bound curves, then integrate to find the reduction in EMDCC.

For $x \in (a, \mu_p)$, the lower bound is $F(a)$ and the upper bound has support $\{x, b\}$.  This implies that $\mu_p^p$ equals

$$
\frac{F_{h+}(x)-F(a)}{F(b)-F(a)} * x^p + \frac{F(b)-F_{h+}(x)}{F(b)-F(a)} * b^p
$$

Therefore,

$$
F_{h+}(x) = F(a) + (F(b)-F(a)) \frac{b^p-\mu^p}{b^p-x^p}
$$

For $x \in (\mu_p, b)$, the upper bound is $F(b)$ and the lower bound has support $\{a+\epsilon, x+\epsilon\}$ where $$
\frac{F_{h-}(a+\epsilon)-F(a)}{F(b)-F(a)} * (a+\epsilon)^p + \frac{F_{h-}(x+\epsilon)-F_{h-}(a + \epsilon)}{F(b)-F(a)} (x + \epsilon)^p
$$

Therefore, re-arranging, noting that $F_{h-}(a+\epsilon)=F_{h-}(x)$, $F_{h-}(x+\epsilon)=F(b)$, and letting $\epsilon \rightarrow 0$ gives

$$
F_{h-}(x) = F(b) - (F(b)-F(a)) \frac{\mu_p^p - a^p}{x^p-a^p}
$$

Next, the area reduction from knowing the moment comes from the integral between upper and lower bounds:

$$
\frac{1}{(F(b)-F(a))(b-a)}\int_{a}^{b} |F_{h+}(x) - F_{h-}(x)|dx =
$$
$$
\frac{1}{b-a} \bigg(
\int_{a}^{\mu_p} \frac{b^p-\mu_p^p}{b^p-x^p}dx + \int_{\mu_p}^{b} \frac{\mu_p^p-a^p}{x^p-a^p}dx
\bigg)
$$

\noindent
and this gives the stated result when $p=1$.  More complex formulas exist when multiple moments are known simultaneously.

Figure~\ref{fig:polyemdcc} uses an example bin with points taken from
Beta(0.5, 0.05) and a mean value of 0.9 to illustrate visually the
tradeoff in information loss between $H(2,0)$ and $H(1,1)$ histograms.
Knowing the mean value in this case
constrains the area where an ecdf of the underlying distribution with
that binned representation lies more than if just adding twice
as many bins at the same storage cost.

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{polyhistemdcc-crop.pdf}
\caption{Yellow areas of uncertainty for where the ecdf of the
  unbinned data must lie given a histogram bin bisected in two (left)
  or a histogram annotated with the mean of values in that bin (right).}
\label{fig:polyemdcc}
\end{figure}



\section{Empirical Validation}

The efficacy of polynomial histogramsis tested in tracking read sizes
for 315 storage users in one of
% TODO(unredact)
\textbf{Blinded's}
% Google's
data centers.  For this
system, the interest is in log read sizes, and the range is restricted from $log(0) = 1 \text{byte}$ to $log(24) = 16\text{MB}$ and find that storing mean and counts in each of the 24 buckets is far more effective than bisecting into 48 buckets.

If the mean is not stored, then K equally sized bins will give an EMDCC of 1/K.  When the mean is tsored in K equally sized bins and get an EMDCC of X, then it is defined as 
$$ \text{information gain} \quad = \frac{1}{2*K*X} $$
A value of 5 implies that it would need 5 times as much storage space from equally spaced buckets to achieve the same EMDCC.  This is bounded below by 1/1.4=0.73, but can get arbitrarily large.

Figure ~\ref{fig:validation} shows the information gain associated with 24 integer buckets of log file sizes.  While $\approx20\%$ see a minor loss, approximately the same number see gains over 10, and 40\% see gains over 2.5.  This shows that 24 buckets with means are superior to 48 regular buckets, and it is also checked that 12 buckets with means are superior to 24 regular buckets, although the relative gain is slightly smaller.  On the other hand, 6 buckets with means are slightly worse than 12 regular buckets because the biggest discontinuities are less likely to sit on the endpoints at this scale.

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{inforatio.pdf}
\caption{The Information gained from storing the mean in 24 integer buckets of log file sizes across 315 storage users.}
\label{fig:validation}
\end{figure}




\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{pkg/vignettes/histogram-mapreduce-diag1.pdf}
\caption{Effect of the \texttt{TrimHistogram} function.}
\label{fig:trimhist}
\end{figure*}





\section{Quantiles and Cumulative Distribution Functions}

\section*{Approximating Quantiles and CDF from Histograms}

Histograms are a common tool for reducing data storage costs by binning data, but this process results in a loss of detailed information about the underlying distribution. Despite this limitation, quantiles and the cumulative distribution function (CDF) can still be approximated using histogram data.

\subsection*{Key Functions}
\begin{itemize}
    \item \texttt{Count}: Calculates the total number of observations in the histogram.
    \item \texttt{ApproxMean}: Approximates the mean of the underlying distribution.
    \item \texttt{ApproxQuantile}: Estimates specific quantiles. Note that these approximations are only accurate for histograms with finely granular buckets, which may not be the case with the default R settings.
\end{itemize}

\subsection*{Example Code}
\begin{verbatim}
# Create a histogram
hist <- hist(c(1, 2, 3), breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), plot = FALSE)

# Approximation functions
Count(hist)  
ApproxMean(hist)  
ApproxQuantile(hist, 0.5)  
ApproxQuantile(hist, c(0.05, 0.95))  
\end{verbatim}

\subsection*{Converting a Histogram to an ECDF}
The \texttt{HistToEcdf} function converts histogram data into an empirical cumulative distribution function (ECDF), similar to the output of the \texttt{ecdf} function.

\subsection*{Saving Plots to File}
\begin{verbatim}
# Save histogram plot
png("plot1.png", width = 800, height = 600)
plot(hist)
dev.off()

# Save ECDF plot
png("plot2.png", width = 800, height = 600)
plot(HistToEcdf(hist))
dev.off()
\end{verbatim}




\section{Applications}



The DTrace framework provides a robust and scalable mechanism for dynamically gathering and aggregating system performance metrics on Unix-based operating systems. The function \texttt{ReadHistogramsFromDtraceOutputFile} is specifically designed to parse the text-based output generated by the DTrace command. It converts the ASCII representation of aggregated distributions into R histogram objects, enabling further manipulation and analysis within the R environment.

\subsection{Efficiently Binning Large Data Sets Using MapReduce}

In domains such as particle physics and information processing, managing vast datasets often necessitates techniques to optimize storage and computational efficiency. A common solution involves storing these datasets in binned or histogram formats, which significantly reduces the storage requirements while retaining key distributional information \cite{scott2009multivariate}.

\subsubsection{Approaches to Histogram Generation with MapReduce}

MapReduce, a widely adopted framework for distributed data processing, supports two primary methods for generating histograms from large datasets:

\begin{enumerate}
    \item \textbf{Independent Histogram Generation by Mappers}  
    In this approach, each mapper processes a specific subset of the dataset assigned to it and independently generates a histogram for that subset. These individual histograms are then passed to one or more reducer tasks, which aggregate and merge them into a unified histogram representation for storage or further analysis.

    \item \textbf{Bucket-Based Key-Value Mapping}  
    This method takes a different approach. Each mapper rounds data points to the nearest predefined bucket and emits key-value pairs, where the bucket identifier serves as the key, and the value is always \texttt{1}, representing a single count. Reducer tasks then aggregate these key-value pairs by summing up the values associated with each key to produce the final histogram.
\end{enumerate}

Both methods require strict synchronization of bucket boundaries across all mapper tasks, even though the mappers process disjoint parts of the dataset, which may span different data ranges. When data ranges vary significantly, a multi-phase process is necessary to ensure consistent bucket alignment.

\subsubsection{Application in R and Other Languages}

This package is especially useful when either the Map or Reduce tasks are implemented in R or when components are written in other programming languages, but the resulting histograms need to be processed and analyzed in R. It simplifies the workflow by facilitating the integration of distributed systems’ output with R-based analytical tools.

\subsubsection{Visual Illustration}

Figure 6 illustrates the second method for histogram generation using MapReduce. The diagram highlights how bucket-based key-value mapping can efficiently process and bin large-scale data within a distributed computing environment.

By enabling seamless integration between distributed data processing frameworks like MapReduce and the R programming environment, this package provides a powerful solution for managing binned data, particularly in applications that rely on large-scale data analysis and storage optimization.


\section{Conclusion}
\label{sec:Conclusions}

Cloud data centers monitor a very large number of metric distributions,
particularly for latency metrics, such as compact histograms. Memory for
these histograms are limited, so it is important to use a
representation that minimizes information loss without increasing the
memory footprint. An information loss metrics for
histograms is described, and shown that by using histograms with fewer bins but
adding information about the moments of the samples in the bin,
information loss can be reduced for certain classes of distributions,
and that such distributions occur commonly in practice.
%\section{Availability}
An open-source R package for analyzing the information loss due to
% TODO(unredact)
binning of histogram representations is available at \textbf{Blinded}.
%\cite{histogramtoolsredacted}.
The package includes an example code to
generate all figures included in this paper, and the data set
used in the empirical validation section.


\bibliographystyle{IEEEtran}
\bibliography{refs}


\end{document}
