% TODO: section name
\section{Schedule Design for Faster Convergence}
\label{sec:instance}

% TODO: highlight the importance, how it affects convergence rate (done)
% prove the the time schedule design for the second \gamma, add another theorem (done?)

%longbo{first give a high level description about what we are doing here. something like the previous theorem provides a general framework, so we want to design optimized time schedule for faster convergence blabla}\yuhaoDone

In \Cref{thm:main}, we provide an upper bound on the KL divergence from the target distribution to the estimated distribution for a general class of SDE-based generative models. Since the bound depends on the choice of latent scale $\gamma(t)$ and schedule $\{t_k\}_{k=0}^N$, we are able to carefully design a time schedule for a given latent scale, thereby achieving a provably bounded error within a minimum number of steps.

% TODO: add more references
Specifically, we consider the common choice of latent scale in stochastic interpolants, $\gamma(t)=\sqrt{at(1-t)}$, which is first introduced in \citet{interpolation}. 
% In fact, the process $x_t=(1-t)x_0+tx_1+\sqrt{2t(1-t)}z$ is a variance-preserving process. 
This choice is equivalent to changing the definition $$x_t=I(t,x_0,x_1)+\gamma(t)z$$
to $$x_t=I(t,x_0,x_1)+\sqrt{a}\dd B_t,$$
where $B_t$ is a standard Brownian bridge process independent of $(x_0,x_1)$. %(We can just write $B_t=W_t-tW_1$ to obtain a Brownian bridge process.) 
For this $\gamma(t)$, we present the following time schedule to optimize the sample complexity.
% Now, for the given choice of $\gamma(t)$, we give the following time schedule to optimize the sample complexity, which is the number of steps required to achieve a specific error bound.

% TODO: intuition behind the schedule design, explain why
\paragraph{Exponentially Decaying Time Schedule} 
% Consider the bound given by \Cref{thm:main}, we can see that for those steps with smaller $\bar{\gamma}_k$, the error terms in the bound are larger. When $\gamma(t)=\sqrt{at(1-t)}$, for $t_k$ close to $0$ or $1$, the error term in the bound is larger. Therefore, to reduce the error bound for a fixed number of steps $N$, we want to take shorter steps when $t$ is close to $0$ or $1$, and longer steps when $t$ is around $0.5$. 

As suggested by \Cref{thm:main}, smaller steps need to be taken in order to balance the error terms. Moreover, to exactly cancel the $\gamma$-terms, we need $h_k=O(\bar{\gamma}_k^2)$ where $\bar{\gamma}$ is defined in \Cref{thm:main}. Hence, we propose an exponentially decaying time schedule inspired by the approach of \citet{dlinear}. %\longbo{is it the same form? "adapted to our setting" make our result look very weak. if it is not the same, you can just say "inspired".}
Specifically, we first select a midpoint $t_M=\frac{1}{2}$. Let $h\in(0,1)$ be a parameter that controls the step size. We then define the time steps as follows:
$$t_{k+1}-t_k=\begin{cases}\frac{1}{2}h(1-h)^{M-k-1},&k<M\\\frac{1}{2}h(1-h)^{k-M},&k\ge M.\end{cases}$$
This leads to $$t_k=\begin{cases}
    \frac{1}{2}(1-h)^{M-k},&k<M\\
    1-\frac{1}{2}(1-h)^{k-M},&k\ge M.
\end{cases}$$
The parameter $h$ determines the overall scale of the step sizes. A smaller $h$ results in a finer discretization of the time interval. 

Let $h_k=t_{k+1}-t_k$ denote the step size at the $k$-th step. We observe that $$h_k=O(h\min\{t_k,1-t_{k+1}\})=O(h\bar{\gamma}_k^2),$$
which satisfies the condition of canceling the $\gamma$-terms.
% where $\bar{\gamma}_k$ is defined in \Cref{thm:main}. 
% This design ensures that smaller step sizes are taken when $\bar{\gamma}_k$ is smaller, which is crucial for mitigating the error contributions from regions with smaller latent scales, as suggested by \Cref{thm:main}. 
Moreover, the total number of steps is given by
$$\begin{aligned}
    N&=O\left(\frac{\log(1/t_0)+\log(1/(1-t_N))}{\log(1/(1-h))}\right)\\
    &=O\left(h^{-1}\log\left(\frac{1}{t_0(1-t_N)}\right)\right).
\end{aligned}$$
%\longbo{can you state the schedule design using a different order: 1. from theorem 4.3, we know that in order to cancel the gamma terms, we need $h_k=O(sth)$. thus, we use the following step size inspired by xxx. 3. we then see that by choosing this step fulfill our purpose}

Now we can provide the following bound:

\begin{proposition}
    Consider the same settings as in \Cref{thm:main}. Suppose $h_k=t_{t+1}-t_k=O(h\bar{\gamma}^2)$, $\epsilon=\Theta(1)$ and $h=O(\frac{1}{d})$. %\longbo{explain what is $\lesssim$ } 
    Then, we have
    $$\begin{aligned}
        \textnormal{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))&\lesssim\varepsilon_{b_F}^2+\textnormal{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))\\
        &+hd\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+Nh^2d^2.
    \end{aligned}$$
    \label{cor:schedule}
\end{proposition}

\Cref{cor:schedule} provides the KL error bound when the step sizes is chosen so that the $\gamma$-terms are canceled.

\begin{corollary}
    Using $\gamma=\sqrt{at(1-t)}$ and the time schedule defined above, suppose that $\textnormal{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))\le\varepsilon^2$ and $\varepsilon^2_{b_F}\le\varepsilon^2$. Furthermore, assume that $\epsilon=\Theta(1)$ and $h=O(\frac{1}{d})$. Then, under the same settings as in \Cref{thm:main}, the total number of steps required to achieve $\textnormal{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))=O(\varepsilon^2)$ is:
    $$\begin{aligned}
        N=O\left\{\frac{1}{\varepsilon^2}\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d\log\left(\frac{1}{t_0(1-t_N)}\right)\right.\right.\\
        \left.\left.+d^2\log^2\left(\frac{1}{t_0(1-t_N)}\right)\right]\right\}.
    \end{aligned}$$
    \label{cor:instant}
\end{corollary}


Corollary \ref{cor:instant} provides the computational complexity of sampling data using the forward SDE. For a fixed error bound $\varepsilon$, the complexity scales proportionally to $\varepsilon^{-2}$. We can further decompose the complexity into distance-related complexity and Gaussian diffusion complexity. 
% \yu{Do not present a single expression here, try to make a name for them, for example, the distance-related error, gaussian diffusion error or something like that.} (done)
Here $O\left(\frac{1}{\varepsilon^2}\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d\log\left(\frac{1}{t_0(1-t_N)}\right)\right)$ is the distance-related complexity representing the number of steps required to achieve a sufficiently small discretization error associated with the velocity function $v(t, x)$. $O\left(\frac{1}{\varepsilon^2}d^2\log^2\left(\frac{1}{t_0(1-t_N)}\right)\right)$ is the Gaussian diffusion complexity  representing the number of steps required to achieve a sufficiently small discretization error associated with the score function $s(t, x)$.

We briefly explain how to obtain this complexity. First, given a desired number of steps $N$, we select $$h=\Theta\left(N^{-1}\log\left(\frac{1}{t_0(1-t_N)}\right)\right)$$
to achieve the specified number of steps. Since $h_k=O(\bar{\gamma}_k^2h)$, we have: 
%\longbo{dont use phrases like this: "by direct calculation,"}
$$\begin{aligned}
    \sum_{k=0}^{N-1}h_k^3\left[M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\\le Nh^3d^3+h^2\left(M_2+d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^8}\right),
\end{aligned}$$
and 
$$\begin{aligned}
    \sum_{k=0}^{N-1}\left(h^2d^2+h_khd\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}\right)\\
    \le Nh^2d^2+hd\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}.
\end{aligned}$$
By substituting the chosen value of $h$ for the given $N$ into \Cref{thm:main}, we can derive the stated complexity bound.

\paragraph{Comparison to a Uniform Schedule.} 
%Here we want to show the advantages of our schedule. We first compare our schedule to a natural uniform schedule, which satisfies $h_k=\frac{t_N-t_0}{N}\approx\frac{1}{N}$. 
To highlight the benefits of our proposed exponentially decaying time schedule, we compare it with a natural uniform schedule that satisfies $h_k = \frac{t_N - t_0}{N} \approx \frac{1}{N}$. 
We further assume the ideal case where  $\varepsilon_{b_F}^2=0$ and $\rho(t_0)=\hat{\rho}(t_0)$ in our analysis. 

According to \Cref{thm:main}, the error bound for the uniform schedule is given by
$$\begin{aligned}
    &\quad\sum_{k=0}^{N-1}h_k^3(M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^8})\\
    &+\sum_{k=0}^{N-1}h_k^2(\bar{\gamma}_k^{-4}d^2+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}).
\end{aligned}$$
Since $\bar{\gamma}_k^2=\Theta(\min\{t_k,1-t_{k+1}\})$, and noting that $$\int_{\delta}^{0.5}t^{-p}\dd t=
\begin{cases}
    \Theta(\log(1/\delta)),&p=1\\
    \Theta(\delta^{-(p-1)}),&p>1
\end{cases}$$
for a uniform schedule, the overall error bound becomes:
$$\begin{aligned}
    &\qquad\textnormal{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))\\&=O\left(\frac{1}{N}\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d\log\left(\frac{1}{t_0(1-t_N)}\right)\right.\right.\\
    &\qquad\qquad\qquad\left.\left.+\frac{1}{t_0(1-t_N)}d^2\right]\right).
\end{aligned}$$
Consequently, the complexity of using a uniform schedule is given by 
\begin{eqnarray*}
N&=&O\bigg(\varepsilon^{-2}\bigg[\log\left(\frac{1}{t_0(1-t_N)}\right)d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}\\&& \qquad\qquad\qquad+\frac{1}{t_0(1-t_N)}d^2\bigg]\bigg),
\end{eqnarray*}
which exhibits a higher computational complexity compared to the proposed exponentially decaying schedule.

\paragraph{Comparison to Diffusion Models Results.} By setting $I(t,x_0,x_1)=(1-t)x_0+tx_1$, $\gamma(t)=\sqrt{2t(1-t)}$, $x_0\sim\rho_0=N(0,I_d)$, and assuming that $x_0$ and $x_1$ are independent, the stochastic interpolant reduces to $x_t=\sqrt{1-t^2}\bar{z}+tx_1$ for some $\bar{z}\sim N(0,I_d)$, which fits the diffusion model setting \cite{song2021scorebased}. %\longbo{give a diffusion ref}. 
Assuming that the fourth moment of $\rho_1$ is bounded by a constant (see \Cref{appendix:reduce-to-gaussian} for details), the complexity of our approach simplifies to $$N=O\left(\varepsilon^{-2}d^2\log^2\left(\frac{1}{1-t_N}\right)\right).$$
% Furthermore, $\text{KL}(\rho(t_0)\Vert\rho_0)\lesssim dt_0^2$ (see, e.g., Proposition 4 in \citealt{dlinear}). By selecting $t_0\lesssim\sqrt{\varepsilon^2/d}$ and setting $\hat{\rho}(t_0)=\rho_0=N(0,I_d)$, the assumption $\text{KL}(\rho(t_0)\Vert\rho(t_0))\le\varepsilon^2$ is satisfied.\longbo{I am confused by this two sentences. what are you trying to say?}

For diffusion models with an early stopping time $\delta$, \citet{chen2023improved} established a complexity bound of $\tilde{O}\left(\varepsilon^{-2}d^2\log^2\left(\frac{1}{\delta}\right)\right)$.
By setting $\delta=1-t_N$ in our analysis, we recover the same complexity bound as that obtained for diffusion models. % \longbo{give a ref}. {\color{orange}[the first ref is added now; I have already referenced Chen et al. so idk if there are any other refs to give.]}
% 
While \citet{dlinear} further improves the complexity bound for diffusion models to $\tilde{O}\left(\varepsilon^{-2}d\log^2\left(\frac{1}{\delta}\right)\right)$ by leveraging techniques from stochastic localization, these techniques heavily rely on the Gaussian structure of diffusion models and cannot be directly applied to the more general stochastic interpolant framework.

% TODO: our framework also applies in analyzing other \gamma
% make it a formal corollary
\paragraph{Other Choices of $\gamma(t)$.} In addition to the commonly used $\gamma(t)=\sqrt{at(1-t)}$, our framework can readily be extended to analyze other choices of $\gamma(t)$. In Appendix \ref{appendix:another}, we present an analysis for $\gamma^2(t)=(1-s)^2s$, which is equivalent to the definition in \citet{chen2024forcasting}. We show that the proposed time schedule in Appendix \ref{appendix:another} also outperforms the uniform schedule in terms of computational complexiting the effectiveness of our schedule design, demonstrating the effectiveness of our schedule design. %\longbo{explain why this is an important feature?} 

% [generated by Gemini] This ability to analyze and optimize for different choices of $\gamma(t)$$ is a significant advantage of our framework, as it provides greater flexibility in designing and optimizing the generative process.
%applies for the analyses of other $\gamma$. In 
%For example, Appendix \ref{appendix:another} studies the choice $\gamma^2(t)=(1-s)^2s$, which is equivalent to the definition in \cite{chen2024forcasting}.

% \longbo{experiments? }