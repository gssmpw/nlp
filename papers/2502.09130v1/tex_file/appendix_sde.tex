
\section{Supplementary Details for Section \ref{sec:preliminaries}}
\label{appendix:preliminaries}

This part summarizes some of the results from \cite{interpolation} that are not introduced in Section \ref{sec:preliminaries}.

\begin{proposition}
    (\cite{interpolation}, Theorem 2.6, Corollaries 2.10 and 2.18, and their proofs)
    Suppose that the joint measure $\nu$ and the function $I$ satisfies 
    \begin{equation}
        \underset{(x_0,x_1)\sim\nu}{\mathbb{E}}\Vert\partial_tI(t,x_0,x_1)\Vert^4\le M_1<\infty,\quad\underset{(x_0,x_1)\sim\nu}{\mathbb{E}}\Vert\partial_t^2I(t,x_0,x_1)\Vert^2\le M_2<\infty,\quad \forall t\in[0,1].
        \label{eq:assumption1}
    \end{equation}
    Then, $\rho\in C^1((0,1),C^p(\mathbb{R}^d))$, $s\in C^1((0,1),(C^p(\mathbb{R}^d))^d)$ and $b\in C^0((0,1),(C^p(\mathbb{R}^d))^d)$, and both the solution of the probability flow ODE
    $$\frac{\dd}{\dd t}X_t=b(t,X_t), \qquad X_0\sim\rho_0$$
    and the solution of the forward SDE
    $$\dd X_t^F=b_F(t,X_t^F)\dd t+\sqrt{2\epsilon(t)}\dd W_t, \qquad X_0^F\sim\rho_0$$
    have the same marginal densities as $(x_t)_{t\in[0,1]}$. Here $\epsilon\in C[0,1]$ with $\epsilon(t)\ge0$ for all $t\in[0,1]$ and $b_F$ is defined as \begin{equation}
        b_F(t,x)=b(t,x)+\epsilon(t)s(t,x).
        \label{eq:defbf}
    \end{equation}
    
    Moreover, suppose that the densities $\rho_0,\rho_1$ are strictly positive elements of $C^2(\mathbb{R}^d)$, and are such that
    $$\int_{\mathbb{R}^d}\Vert\nabla\log\rho_0(x)\Vert^2\rho_0(x)dx<\infty,\qquad\int_{\mathbb{R}^d}\Vert\nabla\log\rho_0(x)\Vert^2\rho_1(x)dx<\infty.$$
    Then $\rho\in C^1([0,1],C^p(\mathbb{R}^d))$, $s\in C^1([0,1],(C^p(\mathbb{R}^d))^d)$ and $b\in C^0([0,1],(C^p(\mathbb{R}^d))^d)$. The notation is adapted from \cite{interpolation} where $f\in C^1([0,1],C^p(\mathbb{R}^d))$ means that the function $f$ is $C^1$ in $t\in[0,1]$ and $C^p$ in $x\in\mathbb{R}^d$.
    \label{prop:generative-modeling}
\end{proposition}

% Also, by the proof of Theorem 2.6 in \cite{interpolation} and study how the assumptions are used, we can derive the following result when we do not have the boundary assumptions.

% \begin{proposition}
%     (\cite{interpolation}, Appendix B.1)
%     With only the condition (\ref{eq:assumption1}) in Proposition \ref{prop:generative-modeling}, we still have the same result for any subintervals of $(0,1)$ as Proposition \ref{prop:generative-modeling}.
%     \label{prop:generative-modeling2}
% \end{proposition}

The above proposition provides a generative modeling in the form of 
$$\frac{\dd}{\dd t}X_t=b(t,X_t)$$
and
$$\dd X_t^F=b_F(t,X_t^F)\dd t+\sqrt{2\epsilon(t)}\dd W_t.$$

In practice, we need to train an estimator to estimate velocity functions. By the following proposition, we can use the optimization objectives to train the estimators. 

\begin{proposition}
    (\cite{interpolation}, Theorems 2.7 and 2.8)
    $b$ is the unique minimizer of
    $$\mathcal{L}_b[\hat{b}]=\int_0^1\mathbb{E}\left[\frac{1}{2}\Vert\hat{b}(t,x_t)\Vert^2-(\partial_tI(t,x_0,x_1)+\dot{\gamma}(t)z)\cdot\hat{b}(t,x_t)\right]\dd t,$$
    and $s$ is the unique minimizer of
    $$\mathcal{L}_s[\hat{s}]=\int_0^1\mathbb{E}\left[\frac{1}{2}\Vert\hat{s}(t,x_t)\Vert^2+\gamma^{-1}(t)z\cdot\hat{s}(t,x_t)\right]\dd t.$$
    Here the notation ``$\cdot$" represents the inner product of two vectors.
    \label{prop:objectives}
\end{proposition}
