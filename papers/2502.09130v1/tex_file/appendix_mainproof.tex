
\section{Omitted Proofs in Sections \ref{sec:results} and \ref{sec:instance}}
\label{appendix:overall}

\subsection{Bounds along the forward Path}

Recall the forward ODE $$\dd X_t=b(t,X_t)\dd t$$
and the forward SDE $$\dd X_t^F=b_F(t,X_t^F)\dd t+\sqrt{2\epsilon}dW_t.$$
Their solutions are denoted by $X_t$ and $X_t^F$, respectively. Using the chain rule or It\^o's formula, for a function $f(t,x)$ that is twice continuously differentiable, we have 
$$\dd f(t,X_t)=[\partial_tf(t,X_t)+\nabla f(t,X_t)\cdot b(t,X_t)]\dd t,$$
and $$\dd f(t,X_t^F)=[\partial_tf(t,X_t^F)+\nabla f(t,X_t^F)\cdot b_F(t,X_t^F)+\epsilon\Delta f]\dd t+\sqrt{2\epsilon}\nabla f(t,X_t^F)\cdot \dd W_t.$$

With the above formula, we can now provide the following bound on the discretization error.

\begin{lemma}
    % Suppose that we take $\epsilon=\Theta(1)$. Then 
    For $0<t_0\le t_1<1$, suppose $\epsilon=O(1)$, then,
    $$\begin{aligned}
        \mathbb{E}\Vert v(t_0,X_{t_0}^F)-v(t_1,X_{t_1}^F)\Vert^2&\lesssim(t_1-t_0)^2\left[M_2+\gamma_{\min}^{-6}d^3+\gamma_{\min}^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\
        &\qquad+\epsilon(t_1-t_0)\gamma_{\min}^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{4}},
    \end{aligned}$$
    and $$\mathbb{E}\Vert s(t_0,X_{t_0}^F)-s(t_1,X_{t_1}^F)\Vert^2\lesssim(t_1-t_0)^2\left[\gamma_{\min}^{-4}d^2\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\gamma_{\min}^{-6}d^3\right]+\epsilon(t_1-t_0)\gamma_{\min}^{-4}d^2.$$
    Here we denote $\gamma_{\min}=\min_{u\in[t_0,t_1]}\gamma$.
    \label{lem:discretize}
\end{lemma}

\begin{proof}
    According to the formula 
    $$\dd v(t,X_t^F)=[\partial_tv(t,X_t^F)+\nabla v(t,X_t^F)\cdot b_F(t,X_t^F)+\epsilon\Delta v(t,X_t^F)]\dd t+\sqrt{2\epsilon}\nabla v(t,X_t^F)\cdot \dd W_t,$$
    we know that 
    $$\begin{aligned}
        \Vert v(t_1,X_{t_1}^F)-v(t_0,X_{t_0}^F)\Vert^2
        &\le4\left\Vert\int_{t_0}^{t_1}\partial_uv(u,X_u^F)\dd u\right\Vert^2\\
        &\qquad+4\left\Vert\int_{t_0}^{t_1}\nabla v(u,X_u^F)\cdot b(u,X_u^F)\dd u\right\Vert^2\\
        &\qquad+4\left\Vert\int_{t_0}^{t_1}\epsilon\Delta v(u,X_u^F)\dd u\right\Vert^2\\
        &\qquad+4\left\Vert\int_{t_0}^{t_1}\sqrt{2\epsilon}\nabla v(u,X_u^F)\cdot \dd W_u\right\Vert^2.
    \end{aligned}$$
    For the first three terms, by Jensen's inequality we know that for any function $Y$, we have $$\left\Vert\int_{t_0}^{t_1}Y(u)du\right\Vert^2\le(t_1-t_0)\int_{t_0}^{t_1}\Vert Y(u)\Vert^2du.$$
    For the last term, use It\^o's isometry (\citealt{le2016brownian}, Equation 5.8), we can get $$\mathbb{E}\left[\left\Vert\int_{t_0}^{t_1}\sqrt{2\epsilon}\nabla v(u,X_u^F)\cdot dW_u\right\Vert^2\right]=\int_{t_0}^{t_1}\mathbb{E}\Vert\sqrt{2\epsilon}\nabla v(u,X_u^F)\Vert_F^2du.$$
    Therefore, we can use Fubini's theorem to change the order of expectation and integral, and combine the results of Lemma \ref{lem:v-time}, \ref{lem:v-space}, \ref{lem:v-laplace} and \ref{lem:vsb-bound} and Assumption \ref{a:regularity} to get
    $$\begin{aligned}
        \Vert v(t_1,X_{t_1}^F)-v(t_0,X_{t_0}^F)\Vert^2&\lesssim(t_1-t_0)\int_{t_0}^{t_1}\mathbb{E}\Vert\partial_uv(u,X_u)\Vert^2\dd u\\
        &\qquad+(t_1-t_0)\int_{t_0}^{t_1}\left[\gamma_{\min}^2d^{-1}\mathbb{E}\Vert\nabla v(u,X_u)\Vert^4+\gamma_{\min}^{-2}d\mathbb{E}\Vert b_F(u,X_u)\Vert^4\right]\dd u\\
        &\qquad+\epsilon^2(t_1-t_0)\int_{t_0}^{t_1}\mathbb{E}\Vert\Delta v(u,X_u)\Vert^2\dd u\\
        &\qquad+2\epsilon\int_{t_0}^{t_1}\mathbb{E}\Vert\nabla v(u,X_u^F)\Vert_F^2\dd u\\
        &\lesssim(t_1-t_0)^2\left[M_2+\gamma_{\min}^{-6}d^3+\gamma_{\min}^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\
        &\qquad+\epsilon(t_1-t_0)\gamma^{-2}d^{1}\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{4}}
    \end{aligned}$$
    Note that we have already used the condition $\gamma^2\in C^2[0,1]$ and $\gamma\dot{\gamma}=O(1)$.

    Similarly, we can use Lemma \ref{lem:s-time}, \ref{lem:s-space}, \ref{lem:s-laplace} and \ref{lem:vsb-bound}, and the formula
    $$\dd s(t,X_t^F)=[\partial_ts(t,X_t^F)+\nabla s(t,X_t^F)\cdot b_F(t,X_t^F)+\epsilon\Delta s(t,X_t^F)]\dd t+\sqrt{2\epsilon}\nabla s(t,X_t^F)\cdot \dd W_t$$
    to bound
    $$\begin{aligned}
        \mathbb{E}\Vert s(t_1,X_{t_1}^F)-s(t_0,X_{t_0}^F)\Vert^2\lesssim(t_1-t_0)^2\left[\gamma_{\min}^{-4}d^2\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\gamma_{\min}^{-6}d^3\right]+\epsilon(t_1-t_0)\gamma_{\min}^{-4}d^2,
    \end{aligned}$$
    where $\gamma_{\min}=\min_{u\in[s,t]}\gamma$.
\end{proof}

\subsection{Proof of Theorem \ref{thm:main}}
\label{appendix:proofofmain}

We first give the following proposition, which is a result from \cite{chen2023ddpm}.

\begin{proposition}
    (Section 5.2 of \cite{chen2023ddpm}) Let $P$, $Q$ be the path measures of solutions of SDE (\ref{eq:forward-sde}) and (\ref{eq:estimated-sde}), where they both start from the same distribution $\rho(t_0)$ at time $t=t_0$ and end at time $t=t_N$. Then, if
    $$\mathbb{E}[\Vert b_F(t,X_t^F)-\hat{b}_F(t_k,X_{t_k}^F)\Vert^2]\le C$$
    for any $t\in[t_0,t_N]$ and some constant $C$, we have 
    $$\text{KL}(P\Vert Q)=\frac{1}{4\epsilon}\sum_{k=0}^{N-1}\int_{t_k}^{t_{k+1}}\mathbb{E}[\Vert b_F(t,X_t^F)-\hat{b}_F(t_k,X_{t_k}^F)\Vert^2]\dd t.$$
    Here the expectations are taken over the ground-truth forward process $(X_t^F)_{t\in[t_0,t_N]}\sim P$.
    \label{prop:girsanov}
\end{proposition}

Now, using the above proposition, we are ready to prove \Cref{thm:main}.

\begin{proof}
    Let $P$, $Q$ be the path measures of the solutions to the SDE (\ref{eq:forward-sde}) and (\ref{eq:estimated-sde}), where the solutions start from the same distribution $\rho(t_0)$ at time $t=t_0$, as in Proposition \ref{prop:girsanov}. We first want to check the condition of Proposition \ref{prop:girsanov}. Note that 
    $$\begin{aligned}
        \mathbb{E}\Vert\hat{b}_F(t_k,X_{t_k}^F)-b_F(t,X_t^F)\Vert^2&\overset{(a)}{\le}2\mathbb{E}\Vert\hat{b}_F(t_k,X_{t_k}^F)-b_F(t_k,X_{t_k}^F)\Vert^2+2\mathbb{E}\Vert b_F(t_k,X_{t_k}^F)-b_F(t,X_t^F)\Vert^2\\
        &\overset{(b)}{\le}2\mathbb{E}\Vert\hat{b}_F(t_k,X_{t_k}^F)-b_F(t_k,X_{t_k}^F)\Vert^2+4\mathbb{E}\Vert v(t_k,X_{t_k}^F)-v(t,X_t^F)\Vert^2\\
        &\qquad+4\mathbb{E}\Vert(-\gamma(t_k)\dot{\gamma}(t_k)+\epsilon)s(t_k,X_{t_k}^F)-(-\gamma(t_k)\dot{\gamma}(t_k)+\epsilon)s(t,X_t^F)\Vert^2\\
        &\overset{(c)}{\le}2\mathbb{E}\Vert\hat{b}_F(t_k,X_{t_k}^F)-b_F(t_k,X_{t_k}^F)\Vert^2+4\mathbb{E}\Vert v(t_k,X_{t_k}^F)-v(t,X_t^F)\Vert^2\\
        &\qquad+8(-\gamma(t_k)\dot{\gamma}(t_k)+\epsilon)^2\mathbb{E}\Vert s(t_k,X_{t_k}^F)-s(t,X_t^F)\Vert^2\\
        &\qquad+8(\gamma(t)\dot{\gamma}(t)-\gamma(t_k)\dot{\gamma}(t_k))^2\mathbb{E}\Vert s(t,X_t^F)\Vert^2.
    \end{aligned}$$
    Here (a), (b) and (c) use the triangle inequality and the fact $(a+b)^2\le a^2+b^2$. By Lemmas \ref{lem:vsb-bound} and \ref{lem:discretize}, this term is uniformly bounded in the closed interval $[t_0,t_N]$. In fact, we can apply these lemmas to obtain that
    $$\begin{aligned}
        \mathbb{E}\Vert\hat{b}_F(t_k,X_{t_k}^F)-b_F(t,X_t^F)\Vert^2&\overset{\text{(a)}}{\lesssim}\mathbb{E}\Vert\hat{b}_F(t_k,X_{t_k}^F)-b_F(t_k,X_{t_k}^F)\Vert^2\\
        &\qquad+(t-t_k)^2\left[M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\
        &\qquad+\epsilon(t-t_k)\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{4}}\\
        &\qquad+(t-t_k)^2\left[\bar{\gamma}_k^{-4}d^2\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\bar{\gamma}_k^{-6}d^3\right]+\epsilon(t-t_k)\bar{\gamma}_k^{-4}d^2\\
        &\qquad+(t-t_k)^2\bar{\gamma}_k^{-2}d\\
        &\overset{\text{(b)}}{\lesssim}\mathbb{E}\Vert\hat{b}_F(t_k,X_{t_k}^F)-b_F(t_k,X_{t_k}^F)\Vert^2\\
        &\qquad+(t-t_k)^2\left[M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\
        &\qquad+\epsilon(t-t_k)\bar{\gamma}_k^{-2}d\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\bar{\gamma}_k^{-2}d\right].
    \end{aligned}$$
    Here step (a) directly expands the discretization error using Lemmas \ref{lem:vsb-bound} and \ref{lem:discretize}; step (b) simplifies the terms by applying Young's inequality and that $1+\epsilon^2=O(1)$. Then, by Proposition \ref{prop:girsanov},
    $$\begin{aligned}
        \text{KL}(P\Vert Q)&=\frac{1}{4\epsilon}\sum_{k=0}^{N-1}\int_{t_k}^{t_{k+1}}\mathbb{E}[\Vert b_F(t,X_t^F)-\hat{b}_F(t_k,X_{t_k}^F)\Vert^2]\dd t\\
        &\overset{\text{(a)}}{\lesssim}\varepsilon_{b_F}^2+\epsilon^{-1}\sum_{k=0}^{N-1}(t_{k+1}-t_k)^3\left[M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\
        &\qquad+\sum_{k=0}^{N-1}(t_{k+1}-t_k)^2\bar{\gamma}_k^{-2}d\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\bar{\gamma}_k^{-2}d\right].
    \end{aligned}$$
    Here step (a) just integrates over the upper bound of the disretization error. Now, consider $\text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))$. Let $\hat{Q}$ be the path measure of solutions of (\ref{eq:estimated-sde}) starting from $\hat{\rho}(t_0)$ instead of $\rho(t_0)$. Then,
    $$\begin{aligned}
        \text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))\le\text{KL}(P\Vert\hat{Q})&=\mathbb{E}_P\left[\log\frac{\dd P}{\dd\hat{Q}}(X)\right]\\
        &=\mathbb{E}_P\left[\log\left(\frac{\dd P}{\dd Q}(X)\cdot\frac{\dd Q}{\dd\hat{Q}}(X)\right)\right]\\
        &=\mathbb{E}_P\left[\log\frac{\dd P}{\dd Q}(X)\right]+\mathbb{E}_P\left[\log\frac{\dd\rho(t_0)}{\dd\hat{\rho}(t_0)}(X_{t_0})\right]\\
        &=\text{KL}(P\Vert Q)+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0)).
    \end{aligned}$$
    The proof is then completed.
\end{proof}

\subsection{Proof of Proposition \ref{cor:schedule}}
\label{appendix:proofofcor}

\begin{proof}
    Using the results of Theorem \ref{thm:main}, 
    $$\begin{aligned}
        \text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))&\overset{\text{(a)}}{\lesssim}\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+\epsilon^{-1}\sum_{k=0}^{N-1}(t_{k+1}-t_k)^3\left[M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\
        &\qquad+\sum_{k=0}^{N-1}(t_{k+1}-t_k)^2\bar{\gamma}_k^{-2}d\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\bar{\gamma}_k^{-2}d\right]\\
        &\overset{\text{(b)}}{\lesssim}\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+\epsilon^{-1}\sum_{k=0}^{N-1}\left[M_2h_k^3+h^3d^3+hh_k^2d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^8}\right]\\
        &\qquad+\sum_{k=0}^{N-1}\left[hh_kd\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+h^2d^2\right]\\
        &\overset{\text{(c)}}{\lesssim}\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+\epsilon^{-1}h^2\left(M_2+d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^8}\right)+\epsilon^{-1}Nh^3d^3\\
        &\qquad+hd\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+Nh^2d^2\\
        &\overset{\text{(d)}}{\lesssim}\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+hd\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+Nh^2d^2.
    \end{aligned}$$
    Here step (a) is the result of Theorem \ref{thm:main}; step (b) uses the fact $h_k=t_{k+1}-t_k=O(h\bar{\gamma}_k^2)$; step (c) uses the fact $\sum_{k=0}^{N-1}h_k=t_N-t_0\le 1$; step (d) omits the higher-order terms.
\end{proof}

\subsection{Proof of Corollary \ref{cor:instant}}
\label{appendix:proofofschedule}

\begin{proof}
    When the number of steps is $N$, we have 
    $$h=\Theta\left(N^{-1}\log\left(\frac{1}{t_0(1-t_N)}\right)\right).$$ 
    Then, by Corollary \ref{cor:schedule} and the assumptions,
    $$\text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))\lesssim\varepsilon^2+N^{-1}\left[d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}\log\left(\frac{1}{t_0(1-t_N)}\right)+d^2\log^2\left(\frac{1}{t_0(1-t_N)}\right)\right].$$
    This gives the complexity to to make $\text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))\lesssim\varepsilon^2$.
\end{proof}
%     $$\begin{aligned}
%         \text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))&\lesssim\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+\sum_{k=0}^{N-1}(t_{k+1}-t_k)^3\left[M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\
%         &\qquad+\sum_{k=0}^{N-1}(t_{k+1}-t_k)^2\bar{\gamma}_k^{-2}d\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\bar{\gamma}_k^{-2}d\right]\\
%         &\lesssim\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+\sum_{k=0}^{N-1}\left[h_k^3M_2+h^3d^3+hh_k^2d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^8}\right]\\
%         &\qquad+\sum_{k=0}^{N-1}\left[hh_kd\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+h^2d^2\right]\\
%         &\lesssim\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+N(h^3d^3+h^2d^2)\\
%         &\qquad+M_2h^3\sum_{k=0}^{N-1}\left(\min\{t_k,1-t_{k+1}\}\right)^3\\
%         &\qquad+\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^8}h^3d\sum_{k=0}^{N-1}\left(\min\{t_k,1-t_{k+1}\}\right)^2\\
%         &\qquad+\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}h^2d\sum_{k=0}^{N-1}\min\{t_k,1-t_{k+1}\}.
%     \end{aligned}$$
%     It is easy to see that $$\sum_{k=0}^{N-1}\min\{t_k,1-t_{k+1}\}^p\lesssim\sum_{k=0}^\infty(1-h)^{kp}\le\frac{1}{ph},$$
%     so $$\begin{aligned}
%         \text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))&\lesssim\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+(h^2d^3+hd^2)(\log(1/t_0)+\log(1/\delta))\\
%         &\qquad+h^2\left(M_2+\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^8}d\right)\\
%         &\qquad+h\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d.
%     \end{aligned}$$
% When $h\lesssim 1/d$, view $\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^8}$ and $M_2$ as constants (Assumption \ref{a:regularity}), then we have
% $$\begin{aligned}
%     \text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))&\lesssim\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))+h\left[d^2\log\left(\frac{1}{t_0(1-t_0)}\right)+d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}\right].
% \end{aligned}$$

% $$\begin{aligned}
%     \text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))&\lesssim\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))\\
%     &+N^{-1}\left[d^2\log^2\left(\frac{1}{t_0(1-t_0)}\right)+d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}\log\left(\frac{1}{t_0(1-t_0)}\right)\right].
% \end{aligned}$$
% This gives the result.


\subsection{Reducing to Diffusion Models}
\label{appendix:reduce-to-gaussian}

By modifying the definition of stochastic interpolant to $$x_t=I(t,x_1)+\gamma(t)z$$
and change the condition on $I$ to $\Vert\partial_tI(t,x_1)\Vert\le C\Vert x_1\Vert$, we can repeat the previous analysis while replacing $\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^p}$ by $\sqrt{\mathbb{E}\Vert x_1\Vert^p}$. For the case of diffusion models, we can choose $I(t,x_1)=tx_1$ and $\gamma(t)=\sqrt{1-t^2}$ to obtain a process with the same marginal distributions. Moreover, under this definition of interpolants, we can choose $t_0=0$ and $h_k=t_{k+1}-t_k\propto(1-t_k)$ as the time schedule to recover the sample complexity of diffusion models.

\subsection{Omitted Proofs for $\gamma^2(t)=(1-t)^2t$}
\label{appendix:another}

In this section, we will design a schedule for $\gamma^2(t)=(1-t)^2t$, and provide the corresponding complexity deduced using \Cref{thm:main}. Moreover, we also derived the complexity of using a uniform schedule for comparison.

\begin{corollary}
    For $\gamma^2(t)=(1-t)^2t$, there exists a schedule so that under the same assumptions as \Cref{cor:instant}, the complexity is given by
    $$N=O\left(\frac{1}{\varepsilon^2}\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d\left(\frac{1}{\sqrt{1-t_N}}+\log\left(\frac{1}{t_0}\right)\right)+d^2\left(\frac{1}{(1-t_N)^2}+\log^2\left(\frac{1}{t_0}\right)\right)\right]\right).$$
    In addition, the complexity for using a uniform schedule is $$N=O\left(\frac{1}{\varepsilon^2}\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d\left(\frac{1}{1-t_N}+\log\left(\frac{1}{t_0}\right)\right)+d^2\left(\frac{1}{(1-t_N)^3}+\frac{1}{t_0}\right)\right]\right).$$
\end{corollary}

\begin{proof}
    Here we also take $h_M=0.5$ for some $M>0$. Then, we define
    $$h_k=\begin{cases}
        h_A\cdot t_{k+1},&k<M\\
        h_B\cdot (1-t_k)^{1.5},&k\ge M.
    \end{cases}$$
    for some $h_A\in[0,0.5),h_B\in[0,1)$.
    For the part $k<M$ and $t_k\in[0,0.5)$, $\gamma^2(t_k)=\Theta(t_k)$, so it is the same as what we have discussed for the case, and we need $$M=N_1=O\left(\frac{1}{\varepsilon^2}\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d\log\left(\frac{1}{t_0}\right)+d^2\log^2\left(\frac{1}{t_0}\right)\right]\right)$$
    steps to make the discretization error $$\begin{aligned}
        &\varepsilon_{b_F}^2+\sum_{k=0}^{M-1}(t_{k+1}-t_k)^3\left[M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]\\
        &\qquad+\sum_{k=0}^{M-1}(t_{k+1}-t_k)^2\bar{\gamma}_k^{-2}d\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\bar{\gamma}_k^{-2}d\right]\lesssim\varepsilon^2.
    \end{aligned}$$
    For the part $k\ge M$, 
    $$\sum_{k=M}^{N-1}(t_{k+1}-t_k)^3\left[M_2+\bar{\gamma}_k^{-6}d^3+\bar{\gamma}_k^{-2}d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^{8}}\right]=O(h_B^2),$$
    and by that $h_k=\Theta(h_B\bar{\gamma}_k^{1.5})=\Theta(h_B(1-t_k)^{1.5})$ (use in step (a) below),
    $$\begin{aligned}
        &\qquad\sum_{k=M}^{N-1}h_k^2\bar{\gamma}_k^{-2}d\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\bar{\gamma}_k^{-2}d\right]\\
        &\overset{\text{(a)}}{\lesssim} h_B\sum_{k=M}^{N-1}h_k\left[d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}\bar{\gamma}_k^{-0.5}+\bar{\gamma}_k^{-2.5}d^2\right]\\
        &\overset{\text{(b)}}{\lesssim} h_B\left[d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}\int_{0.5}^{t_N}(1-s)^{-0.5}\dd s+d^2\int_{0.5}^{t_N}(1-s)^{-2.5}\dd s\right]\\
        &\lesssim h_B\left[d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+d^2\frac{1}{(1-t_N)^{1.5}}\right].
    \end{aligned}$$
    Here the inequality (b) is by that $\bar{\gamma}_k=\Theta(1-t)$ for $t\in[t_k,t_{k+1}]$. Now, we want to compute the number of steps $N_2=N-M$ for the part $k\ge M$. Note that if $t_k=1-2^{-p}$, it takes $O(2^{p/2}h_B^{-1})$ more steps to reach $1-2^{-p-1}$. Hence $N_2=O\left(h_B^{-1}(1-t_N)^{-0.5}\right)$, so we need to take $h_B=\Theta\left(N^{-1}(1-t_N)^{-0.5}\right)$.
    Therefore,
    $$\begin{aligned}
        &\qquad\sum_{k=M}^{N-1}h_k^2\bar{\gamma}_k^{-2}d\left[\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}+\bar{\gamma}_k^{-2}d\right]\\
        &\lesssim N^{-1}\left[\frac{d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}}{\sqrt{1-t_N}}+\frac{d^2}{(1-t_N)^{2}}\right].
    \end{aligned}$$
    Thus, for the part $k>M$, we need $$N-M=N_2=O\left(\frac{1}{\varepsilon^2}\left(\frac{d\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}}{\sqrt{1-t_N}}+\frac{d^2}{(1-t_N)^{2}}\right)\right)$$
    steps to make the discretization error bounded by $O(\varepsilon^2)$. Hence, the overall complexity is given by $N=N_1+N_2$, which is our result.

    If we use a uniform schedule, by \Cref{thm:main} and that $\gamma^2(t)=\Theta(\min\{t,(1-t)^2\})$, we can bound
    $$\begin{aligned}
        \text{KL}(\rho(t_N)\Vert\hat{\rho}(t_N))&\overset{\text{(a)}}{\lesssim}\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))\\
        &\qquad+\frac{1}{N}\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d\left(\int_{t_0}^{0.5}s^{-1}\dd s+\int_{0.5}^{t_N}(1-s)^{-2}\dd s\right)\\
        &\qquad+\frac{1}{N}d^2\left(\int_{t_0}^{0.5}s^{-2}\dd s+\int_{0.5}^{t_N}(1-s)^{-4}\dd s\right)\\
        &\lesssim\varepsilon_{b_F}^2+\text{KL}(\rho(t_0)\Vert\hat{\rho}(t_0))\\
        &\qquad+\frac{1}{N}\sqrt{\mathbb{E}\Vert x_0-x_1\Vert^4}d\left(\frac{1}{1-t_N}+\log\left(\frac{1}{t_0}\right)\right)\\
        &\qquad+\frac{1}{N}d^2\left(\frac{1}{(1-t_N)^3}+\frac{1}{t_0}\right),
    \end{aligned}$$
    which further gives the complexity bound for uniform schedule. Here the inequality (a) is by applying \Cref{thm:main} and replacing $\bar{\gamma}_k$ with the term of the same order. This bound is worse than using the schedule satisfying that $h_k\lesssim h\bar{\gamma}_k$.
\end{proof}