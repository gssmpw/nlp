\section{Conclusion and Future Directions}

This paper provides the first discrete-time analysis for the SDE-based generative models within the stochastic interpolant framework. We formulate a discrete-time sampler using the Eulerâ€“Maruyama scheme to estimate the target distribution by leveraging learned velocity estimators. We then provide an upper bound on the KL divergence from the target distribution to the estimated distribution. 
Our result provides a novel quantification on how different factors, including 
the distance between source and target distributions $\sqrt{\mathbb{E}_{\nu}\Vert x_0-x_1\Vert^p}$ and the desired estimation accuracy $\varepsilon^2$, affect the convergence rate and also offers a new principled way to design efficient schedule for convergence acceleration. 
Finally, we also conduct numerical experiments with the discrete-time sampler, which validates our theoretical findings. 

% An interesting direction for future research is to improve the convergence bound, especially the dependence on the dimension $d$, given that diffusion models have already been proved to have almost $d$-linear convergence rate. Another topic involves optimizing the sampling algorithm to yield convergence rates better than $O(\varepsilon^{-2})$. Moreover, the finite-time convergence of ODE-based generative model within the stochastic interpolant framework requires more exploration.
Future research avenues can fruitfully explore the enhancement of convergence bounds, with a particular focus on addressing the dependency on the dimension $d$. Notably, diffusion models are demonstrated by previous works to exhibit near $d$-linear convergence rates, indicating the potential for improvement. Another direction is to investigate strategies for refining the sampling algorithm to attain convergence rates superior to the currently observed $O(\varepsilon^{-2})$. Furthermore, the finite-time convergence phenomenon of ODE-based generative models within the context of the stochastic interpolant framework warrants a more comprehensive investigation.
%This result offers novel theoretical insights into how the distance between the source and destination distributions affects the convergence rate.

%Furthermore, we devise a schedule for step sizes that enables a rapid convergence rate. Our proposed schedule surpasses the commonly used natural uniform step sizes. The overall complexity of our approach is comparable to previous results for diffusion models. We also conduct numerical experiments with the discrete-time sampler, which validates our theoretical findings.

% should I mension future works? idk


% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.