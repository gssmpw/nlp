% 再加个gamma
% 开始就讲做实验的目的，打算怎么比，比什么
% 
\section{Numerical Experiments}
\label{sec:experiments}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{images/transform_v2.png}
    \caption{An illustration of the interpolants. We choose $I(t,x_0,x_1)=(1-t)x_0+tx_1$ and $\gamma(t)=\sqrt{2t(1-t)}$. The first row of graphs shows the densities given by (\ref{eq:stochastic-interpolant}). The second row of graphs shows the estimated densities using SDE (\ref{eq:estimated-sde}), where the estimator $\hat{b}_F$ is learned by a neural network.}
    \label{fig:1}
\end{figure}

% 
\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{images/convergence.png}
        \caption{An illustration of generation results using different schedules.}
        \label{fig:2}
    \end{subfigure}\quad
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{images/different_nu_v3.png}
        \caption{A view of different $\rho_0$ and their effects on the convergence rate.}
        \label{fig:4}
    \end{subfigure}
    \caption{A visualization of experiments.}
    \label{fig:2-3}
\end{figure}

% fix this (done)
\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{images/tvdist_re.png}
        \caption{Different schedules. }
        \label{fig:3}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{images/tvdist2_re.png}
        \caption{Different $\rho_0$ and $\nu$.}
        \label{fig:5}
    \end{subfigure}
    \caption{Estimated TV distances. The $x$-axis refers to the number of steps, while the $y$-axis refers to the empirical TV distance between estimated distribution $\hat{\rho}(t_N)$ and target distribution $\rho(t_N)$. \Cref{fig:3} compares the exponentially decaying schedule (red line) with the uniform schedule (green line). \Cref{fig:5} compares different settings of $\rho_0$ and $\nu$ (setting A: red line; setting B: green line).}
    \label{fig:4-5}
\end{figure}

% 把2个图a放一起。然后另起一个图，把两个图b压缩放一起(一个column)
% TODO: add references, try TV distance
% add the new experiment, 
% One possible implementation of the discretized sampler is given by Equation (\ref{eq:sampler}). In addition, there exists a simple quadratic objective for the estimator (see Appendix \ref{appendix:preliminaries}), and we can easily compute an empirical loss if the estimator, a batch of data and a batch of uniformly sampled time is given. Then, by parameterizing the estimator $\hat{b}_F$ using neural networks, we can use optimizers such as Adam \cite{adam} to train the network.
In this section, we conduct experiments to validate our theoretical results. We implement the discretized sampler as defined in Equation (\ref{eq:sampler}), and evaluate its performance on on two-dimensional datasets (primarily from \citealt{grathwohl2018scalable}). The experiments focus on the following two aspeccts: (i) the convergence rate using the exponentially decaying schedule, and (ii) the effect of choosing different $\rho_0$ as base densities, which is reflected by the distribution distance terms in our bound.

%\longbo{i dont understand. do we use a quandratic function or a neural network?}{\color{orange}[modified a bit]}
% \rui{This paragraph can be moved to the appendix} We implement the discretized sampler as defined in Equation (\ref{eq:sampler}). To train the estimator $\hat{b}_F(t,x)$, we leverage a simple quadratic objective (detailed in Appendix \ref{appendix:preliminaries}) whose optimizer is the real drift $b_F(t,x)$. Given the estimator, batches of data, and uniformly sampled time points, we can readily compute an empirical loss. After parameterizing the estimator $\hat{b}_F$ using neural networks, we employ the Adam optimizer \cite{adam} to train the network. %\longbo{which optimizer exactly?} {\color{orange}[I thought Adam was here]}

% TODO: do not use ``toy" (removed)
% In order to intuitively visualize the process of transforming from one distribution to another, we test the sampler on 2-dimensional datasets (most of them are from \cite{grathwohl2018scalable}) to show the convergence ability. The datasets we choose are of complex shape and even discontinuous densities, which make it difficult for the samplers to converge when $t\to1$.
% \paragraph{Experiments on 2D Datasets}

% To visually illustrate the process of transforming one distribution to another, we evaluate the sampler on two-dimensional datasets (primarily from \citealt{grathwohl2018scalable}). These datasets exhibit complex shapes and even discontinuous densities, presenting challenging convergence scenarios as $t$ approaches $1$. 
We employ 
$I(t,x_0,x_1)=tx_1+(1-t)x_0$,
$\gamma(t)=\sqrt{2t(1-t)}$
% $$\begin{aligned}
%     I(t,x_0,x_1)&=tx_1+(1-t)x_0,\\
%     \gamma(t)&=\sqrt{2t(1-t)},
% \end{aligned}$$
and $\epsilon=1$ in our experiments.
Figure \ref{fig:1} presents a visualization of the interpolants and the estimated densities generated using the forward SDE. As we can see in Figure \ref{fig:1}, the density defined by the stochastic interpolant progressively changes from the checkerboard density to the spiral density, and the estimated density given by SDE (\ref{eq:estimated-sde}) well tracks the change of the real density. %\longbo{add 1-2 sentences to explain the figure. e.g., say that our results are correct/accurate.}

% We conduct two experiments on the datasets to verify our theoretical results, where the details can be found in Appendix \ref{appendix:2dim}.

% The first experiment compares the convergence rate using different schedules on the same task. Figure \ref{fig:2} directly visualizes the generation quality of different schedules. Moreover, we estimate the total-variation (TV) distances between the target densities and the generated densities in order to quantitatively compare the convergence, as shown in Figure \ref{fig:3}. Both Figure \ref{fig:2} and Figure \ref{fig:3} corroborate the superior convergence performance of exponentially decaying step sizes.

% The second experiment investigate the impact of different source densities ($\rho_0$) and couplings ($\nu$) on the complexity of the generation process while keeping the target density ($\rho_1$) fixed. The densities are shown in Figure \ref{fig:4}, where each ``block" in $\rho_0$ is coupled with the corresponding ``block" in $\rho_1$ in the same vertical position. We also compared the convergence using estimated TV distances in Figure \ref{fig:5}. Both Figures \ref{fig:4} and \ref{fig:5} demonstrate that the generation process converges faster when the source density ($\rho_0$) is closer to the target density ($\rho_1$) under the specified coupling ($\nu$). In contrast, the generation process converges slower when the source density is farther away from the target density. This observation highlights the influence of the initial distribution and the coupling structure on the overall convergence behavior.

%For this part of our experiments, we employ $I(t,x_0,x_1)=tx_1+(1-t)x_0$, $\gamma(t)=\sqrt{2t(1-t)}$ and $\epsilon=1$, enabling the use of the step sizes proposed in Section \ref{sec:instance}.
% \rui{Move this paragraph to appendix}To parameterize the estimator for two-dimensional data, we utilize a simple multilayer perceptron (MLP) network. The input of the network comprises a three-dimensional vector $(x,t)$, and its output is a two-dimensional vector $\hat{b}_F(t,x)$. The MLP architecture consists of three hidden layers, each with $256$ neurons, followed by ReLU activation functions \cite{nair2010rectified}. We set $t_0=0.001$ and $t_N=0.999$ to ensure that the initial density $\rho(t_0)$ is close to $\rho_0$ and the estimated density $\rho(t_N)$ closely approximates $\rho_1$. We utilize over $10,000$ data samples to empirically visualize the densities for Figure \ref{fig:1}, \ref{fig:2} and \ref{fig:4}. 

% Figure \ref{fig:1} presents a visualization of the interpolants and the estimated densities generated using the forward SDE. As we can see in Figure \ref{fig:1}, the density defined by the stochastic interpolant progressively changes from the checkerboard density to the spiral density, and the estimated density given by SDE (\ref{eq:estimated-sde}) well tracks the change of the real density.

\paragraph{Comparison of Different Time Schedules} We now compare the performance of different schedules, namely, those proposed in Section \ref{sec:instance} and the uniform step sizes. The task involves transforming from a "checkerboard" density to a "spiral" density. As illustrated in Figure \ref{fig:2}, employing exponentially decaying step sizes results in about $10\times$ faster convergence of the target density compared to using uniform step sizes.  %\longbo{comment on how much faster? e.g., "about $10\times$"}. {\color{orange}[modified, it is about 10x]}

To quantitatively assess this, we estimate the total-variation (TV) distances between the target density and the generated densities using sampled data points, as direct computation of logarithmic densities is infeasible. We choose the TV distance due to Pinsker's inequality, which bounds the squared TV distance by the KL divergence. The lower bound observed in the estimated distances is attributed to the estimation error $\varepsilon_{b_F}^2$ and the inherent randomness in the TV distance estimation process. Both Figure \ref{fig:2} and Figure \ref{fig:3} corroborate the superior convergence performance of exponentially decaying step sizes.

% Second, for a fixed $\rho_1$, we have studied how a different $\rho_0$, as well as a coupling $\nu$, will affect the complexity. The target density is a ``checkerboard" density, and we choose two different $\rho_0$, as shown in Figure \ref{fig:4}. The first (labeled as A) is another ``checkerboard" density, and is coupled with the target density so that each coupling $(x_0,x_1)\sim\nu$ comes from two adjacent cells. This setting can be viewed as a case when ``paired" data are available. The second (labeled as B) is a simple density with only one single line. Figure \ref{fig:5} shows the estimated TV distances from the estimated data distribution to the target distribution. We use the same number of samples and cells as in Figure \ref{fig:3} to estimate the TV distance. We use the exponentially decaying step size schedule for both choices of $\rho_0$. This implies that with a source density close to $\rho_1$ under coupling $\nu$, the generation process converges faster. And with a source density that is far away to $\rho_1$ under coupling $\nu$, the generation process converges slower.
\paragraph{Effect of Different Distribution Distances} Second, we investigate the impact of different source densities ($\rho_0$) and couplings ($\nu$) on the complexity of the generation process while keeping the target density ($\rho_1$) fixed. The densities are shown in \ref{fig:4}, where we have two source densities, namely ``A'' and ``B''. Each ``block" in $\rho_0$ is coupled with the corresponding ``block" in $\rho_1$ in the same vertical position.

Figure \ref{fig:5} shows the estimated TV distances between the generated data distribution and the target distribution for both scenarios. For both choices of $\rho_0$, we utilized the exponentially decaying step size schedule. The results demonstrate that the generation process converges faster when the source density ($\rho_0$) is closer to the target density ($\rho_1$) under the specified coupling ($\nu$). In contrast, the generation process converges slower when the source density is farther away from the target density. This observation highlights the influence of the initial distribution and the coupling structure on the overall convergence behavior.