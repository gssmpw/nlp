
\section{Preliminaries: Stochastic Interpolants}
\label{sec:preliminaries}

% TODO: follow Yu
% \longbo{can include diffusion here too. also, if the technical lemmas from previous work are not immediately useful, we could cite them later in the analysis. otherwise they look distractive here.}

% \longbo{why consider this one? or is it a common one? if so, state it}
%Following the stochastic interpolant framework \citep{interpolation, flows, albergo2024coupling}, we consider \longbo{is this the same as the one in previous works? if so, maybe change this to "consider"?} a class of continuous-time stochastic processes that bridges any two arbitrary densities in finite time. \longbo{i edited it in the next paragraph}

In this paper, we consider continuous-time stochastic processes that bridge any two arbitrary probability distributions in finite time. 
Formally, given two probability distributions $\rho_0$ and $\rho_1$ in $\mathbb{R}^d$, a stochastic interpolant \cite{interpolation, flows, albergo2024coupling} is a stochastic process defined as: 
\begin{equation}
    x_t=I(t,x_0,x_1)+\gamma(t)z,\qquad t\in[0,1]
    \label{eq:stochastic-interpolant}
\end{equation}
where $I$ is a twice-continuously differentiable interpolation function satisfying the boundary conditions $I(0,x_0,x_1)=x_0$ and $I(1,x_0,x_1)=x_1$, and there exists a constant $C>0$ such that for all $t\in[0,1]$ and $x_0,x_1\in\mathbb{R}^d$, 
    \begin{equation}
        \begin{aligned}
       % \hspace{-.45in} \partial_tI(t,x_0,x_1)\le C\Vert x_0-x_1\Vert,\forall t\in[0,1],\ x_0,x_1\in\mathbb{R}^d
%       \forall &t\in[0,1],\ x_0,x_1\in\mathbb{R}^d:\\
      &\partial_tI(t,x_0,x_1)\le C\Vert x_0-x_1\Vert.
        \end{aligned}
        \label{eq:I-lip}
    \end{equation}
Here $\gamma(t)$ is a time-dependent scale function with $\gamma(t)^2\in C^2[0,1]$, $\gamma(0)=\gamma(1)=0$, and $\gamma(t)>0$ for $t\in[0,1]$. This definition indicates that $\left|\frac{d}{dt}(\gamma^2(t))\right|$ is bounded by a constant.
$(x_0,x_1)$ is drawn from a joint measure $\nu$ with marginals $\rho_0$ and $\rho_1$, i.e., $\rho_0(dx_0)=\nu(dx_0,\mathbb{R}^d)$ and $\rho_1(dx_1)=\nu(\mathbb{R}^d,dx_1).$
$z\sim N(0,I_d)$ is a standard Gaussian variable independent of $(x_0,x_1)$.

In the definition (\ref{eq:stochastic-interpolant}), $I(t,x_0,x_1)$ represents the interpolation component, while $\gamma(t)z$ introduces a Gaussian latent term crucial for subsequent analysis. We denote the density of $x_t$ by $\rho(t,\cdot)$ or simply $\rho(t)$. According to the construction, the stochastic interpolant satisfies $\rho(0)=\rho_0$ and $\rho(1)=\rho_1$.
% \yu{No need to mention this citation here.} (removed)
%For instance, A simple choice is to take $I(t,x_0,x_1)=tx_1+(1-t)x_0$ and $\gamma(t)=\sqrt{2t(1-t)}$ \cite{chen2024recurrent,albergo2024coupling}. \longbo{give ref for this}
This framework allows for a wide range of interpolation functions $I(t,x_0,x_1)$ and scale functions $\gamma(t)$, offering significant flexibility in design.
%{\color{green}  [\text{Modified}]}

%\longbo{don't understand this paragraph. it does not seem to movitate Def 3.1?}
%{\color{green}  [\text{The whole paragraph is modified}]}
Stochastic interpolants provide a framework for generative modeling through stochastic differential equations.
% , for which marginal densities of the solutions coincide with those of stochastic interpolants. 
As shown by \citet{interpolation}, when $\mathbb{E}_{(x_0,x_1)\sim\nu}\Vert\partial_tI(t,x_0,x_1)\Vert^4$ and $\mathbb{E}_{(x_0,x_1)\sim\nu}\Vert\partial^2_tI\Vert^2$ are bounded, the solution to the following forward SDE
\begin{equation}
    \dd X_t^F=b_F(t,X_t^F)\dd t+\sqrt{2\epsilon(t)}\dd W_t,X_0\sim\rho_0
    \label{eq:forward-sde}
\end{equation}
satisfies $X_t^F\sim\rho(t)$ for all $t\in[0,1]$. Here $\epsilon\in C[0,1]$ is a non-negative function, and the drift term $b_F$ is defined as
\begin{equation}
    \begin{aligned}
        b_F(t,x)&=b(t,x)+\epsilon(t)s(t,x),\\
        b(t,x)&=\mathbb{E}[\dot{x}_t|x_t=x]\\
            &=\mathbb{E}[\partial_tI(t,x_0,x_1)+\dot{\gamma}(t)z|x_t=x],\\
        s(t,x)&=\nabla\log\rho(t,x)=-\gamma^{-1}(t)\mathbb{E}[z|x_t=x].
    \end{aligned}
    \label{def:bf}
\end{equation}
In the definition (\ref{def:bf}), $s(t,x)$ is the well-known score function, and $b(t,x)$ represents the mean velocity field of (\ref{eq:stochastic-interpolant}) (following the notations of \citealt{interpolation}). %\longbo{give a ref}. 
This implies that a process starting from $\rho_0$ and evolving according to the forward SDE (\ref{eq:forward-sde}) will have density $\rho(t)$ at time $t$. Consequently, at time $t=1$, the process will have the desired target density $\rho_1$. This establishes a stochastic mapping from $\rho_0$ to $\rho_1$, providing a foundation for generative modeling. 

Following \citet{interpolation}, we further introduce the velocity function $v(t,x)$ as: %\longbo{is this introduced by us or from previous work?}
\begin{equation}
    \begin{aligned}
        v(t,x)&=\mathbb{E}[\partial_tI(t,x_0,x_1)|x_t=x]\\
        &=b(t,x)+\dot{\gamma}\gamma s(t,x).
    \end{aligned}
    \label{def:v}
\end{equation}
Both $b(t,x)$ and $b_F(t,x)$ can be expressed as linear combinations of $v(t,x)$ and $s(t,x)$. The function $\epsilon(t)$ controls the level of randomness in the mapping from $\rho_0$ to $\rho_1$. When $\epsilon(t)\equiv0$  the SDE reduces to an ODE. We assume $\epsilon(t)=\epsilon$ is a constant without loss of generality, similar to \citet{interpolation} and \citet{costa2024equijumpproteindynamicssimulation}. % \longbo{give a ref}.

%\longbo{this def is from albergo? if so, put the reference here}

% \yu{We do not need to detailedly introduce previous results. Maybe we can sketch them and put the details into appendix. Notice that the template of ICML is double-column which do not support long expressions.}
%\longbo{give intuition/explanation about what velocity and score mean}\longbo{give some background for the SDE, i.e., explain what it is for. otherwise we are jumping from (1) directly to here, which is too abrupt}

% TODO: add references, explain connections (done)
\paragraph{Connection with Diffusion Models.} 

Consider the special case where $x_0$ and $x_1$ are independent, with $x_0\sim N(0,I_d)$. Let $I(t,x_0,x_1)=(1-t)x_0+tx_1$ and $\gamma(t)=\sqrt{2t(1-t)}$. Then, the stochastic interpolant can be expressed as 
$$x_t=tx_1+\sqrt{1-t^2}\bar{z}$$
where $\bar{z}\sim N(0,I_d)$ is another standard Gaussian variable independent of $x_1$. Diffusion models \cite{song2021scorebased} employ the Ornsteinâ€“Uhlenbeck (OU) SDE:
$$\dd Y_s=-Y_s\dd s+\sqrt{2}\dd W_s,\quad Y_0\sim p_{\text{data}}$$
which gradually adds noise to the data distribution $p_{\text{data}}$. Given $Y_0$, $Y_s$ can be written as $$Y_s=e^{-s}Y_0+\sqrt{1-e^{-2s}}Z,\quad Z\sim N(0,I_d).$$
Therefore, if we choose $x_1\sim p_{\text{data}}$ in the stochastic interpolant, $Y_s$ and $x_{e^{-s}}$ have the same distribution.

% TODO: discuss current limitations, technical challenge. 
% (intro and here) previous results, their limitations (done here)
\paragraph{Previous Theoretical Results}

%\longbo{add other references?}
For SDE-based generative models, \citet{interpolation} provide the following KL error bound when an estimator $\hat{b}_F(t,X_t^F)$ is used instead of the true drift term $b_F(t,X_t^F)$ in the SDE. This bound, which can also be derived using Girsanov's Theorem \cite{chen2023ddpm,pmlr-v202-oko23a}, is given by:
$$\begin{aligned}
    \textnormal{KL}(\rho(1)\Vert\hat{\rho}(1))&\le\frac{1}{4\epsilon}\int_0^1\int_{\mathbb{R}^d}\Vert\hat{b}_F(t,x)\\
    &\qquad\quad-b_F(t,x)\Vert^2\rho(t,x)\dd x\dd t.
\end{aligned}$$
This inequality establishes an upper bound on the distribution estimation error in terms of the error in estimating the drift function $b_F(t,X_t^F)$. This estimation error is evaluated with respect to the true underlying density $\rho(t,x)$. 

%One important limitation of the existing results is that they assume the SDEs can be solved exactly, e.g., \cite{chen2024forcasting,interpolation}. However, this is highly challenging in practice, and one often can only use a discrete-time sampler to estimate the solution. 
%When one discretizes the SDE, discretization error ultimately leads to distribution estimation error, which invalidates previous results. Moreover, different  discretization heavily affects convergence of the dynamics, making design for optimal discretization largely open. 
The primary limitation of the existing results is the assumption that the SDEs can be solved exactly (e.g.,  \citealt{chen2024forcasting, interpolation}). However, obtaining exact solutions is often challenging in practice, leading to the use of discrete-time samplers for estimating these solutions.
Yet, when an SDE is discretized, discretization error causes distribution estimation error, which ultimately invalidates  previous results. Moreover, the choice of discretizations can have a significant impact on the convergence of the dynamics, %\longbo{any ref? michael jordan has some results on this for optimization. but not sure it is a good idea to cite it here}
(see, e.g., \citealt{andre2016variational}), leaving the design of optimal discretization methods largely open.