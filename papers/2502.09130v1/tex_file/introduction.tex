
\section{Introduction}
% TODO: Write a new introduction
% Introduce more general generative modeling, instead of focusing on diffusion models?
% Introduce the idea of bridging arbitrary distributions, and say that there are no finite-time analysis
% Ask the question
% Difficulties
% Contributions, include the main theorem, the time schedule and bound that applied for a specific setting, and finally show that our result are comparable with results for diffusion models.

%\longbo{new story: stochastic interplant is important, existing results focus on continuous time, we propose a novel discrete time stochastic interplant algorithm with optimized step size, we establish rigorous performance bound, we show with experiments that it outperforms existing baselines}

% Stochastic interpolants \cite{flows,interpolation} is a framework that gives general constructions of continuous mappings between arbitrary distributions. This framework adapts the ideas from flow-based and diffusion-based models, which generate samples by continuously transporting data points from a base distribution $\rho_0$ to a target distribution $\rho_1$ through learned ordinary differential equations (ODEs) or stochastic differential equations (SDEs). Using the framework of stochastic interpolants, we can obtain ODEs or SDEs that transport data by defining an interpolation between data points from different distributions, and then the velocities in the obtained equation will be given in the form of conditional expectations. The framework of stochastic interpolants  introduces many design flexibilities and then shows its latent capacity in many applications, such as image generation \cite{ma2024sitexploringflowdiffusionbased, albergo2024coupling}, probabilistic forecasting \cite{chen2024forcasting} and sequential modeling \cite{chen2024recurrent}.
Stochastic interpolants \cite{flows, interpolation} provide a general framework for constructing continuous mappings between arbitrary distributions. This framework draws inspiration from flow-based and diffusion-based models, which generate samples by continuously transforming data points from a base distribution to a target distribution via learned ordinary differential equations (ODEs) or stochastic differential equations (SDEs).

Within the stochastic interpolant framework, one obtains learnable ODEs or SDEs that transport data by defining an interpolation between data points sampled from different distributions. %\longbo{this modification is still not helpful. what are you trying to say here?}{\color{green}[I think what I want to say is actually ``learnable"]}
This framework offers significant design flexibility and has demonstrated promising results in various applications, including probabilistic forecasting \cite{chen2024forcasting}, image generation \cite{ma2024sitexploringflowdiffusionbased, albergo2024coupling}, and sequential modeling \cite{chen2024recurrent}.

% Previous works have already analyzed the generative model developed using the stochastic interpolant framework, and upper bounds on the generation error is given for both ODE-based and SDE-based generators. However, previous analyses only focus on continuous-time generation where the equations are assumed to be solved perfectly. However, when this framework is used to develop generative models in the real world, usually we can only access the learned estimators for a finite number of times. That is, we can only use a method that is discretized in time to approximate the true continuous generation process. Therefore, some natural questions arise:

%\rui{I've rewritten the following paragraph.}
Despite its potential in real-world applications, there remains a gap between the theoretical analyses and practical implementations of stochastic interpolants. In practical scenarios, instead of perfectly solving the underlying equations, one can only access a learned estimator for a finite number of time steps, which necessitates the use of discrete-time samplers to simulate the true continuous generation process. However, previous analyses have largely focused on continuous-time generation, assuming perfect solutions to the underlying equations. This leads to a crucial question for bridging the gap:

%\longbo{should we change this to sth like: How to Algorithmically Optimize the Stochastic Interpolant Method for Fast and Accurate Estimation? We are looking for the actual discrete evolution}

\begin{center}
\textbf{What is the convergence rate of discrete-time stochastic interpolant, and how to enhance its performance algorithmically?}
%\textbf{What is the convergence rate of the discrete-time stochastic interpolant, and how to enhance its performance what algorithmic optimizations can be employed to enhance its performance?}
%    \textbf{How Fast Does the Time-Discretized Method Converge? How Should We Algorithmically Optimize the Method?}
\end{center}

% In this work, we focus on the case of SDEs, and answer the above question by proposing a theorem on the KL error bound of the estimated target distribution when the SDE is discretized in time. We followed some settings and ideas in the convergence analyses of score-based diffusion models, including early stoppings and the use of Girsanov's theorem in the proof. However, when bounding the discretization error, previous ideas cannot be directly applied here because 1) the structure in this framework is different so we need to develop another proof on the discretization errors of score functions, and 2) we need to bound the discretization errors for the velocity function as well, which is introduced by the general interpolation function.

% 已有的工作 A用了blabla，B用了blabla，但是都不能用在这里，由于…… (4-5 refs)
% 把别人的方法起个名字，high level说一下他们用了什么性质
% However, addressing these obstacles is non-trivial. In particular, when bounding the discretization error, one cannot directly apply the previous results in the theories of diffusion models. For example, \cite{chen2023ddpm} develops  %\longbo{are these the key results? why mention them? or give them names, such as xxx technique, xxx method, and explain that they are the key techniques} 
%\longbo{need revision. you say "however" twice. }
Similar problems have been studied in the theories of diffusion models, and most results were derived based on Girsanov-based methods in SDE analyses, which reduce the problem to providing upper bounds on the discretization errors. 
%\longbo{you can change the following to sth like: existing analysis can be categorized into xx types, (i) use xxx technique, e.g., ref1, ref2, (ii) use xxx technique, e.g., ref3, ref4. then followed by the second "however"} 
% However, previous results in bounding the discretization error cannot be applied directly to stochastic interpolants. 
Specifically, existing analyses on the discretization errors can be mainly categorized into two types. The first type partitions the error into space-discretization and time-discretization. Among them, \citet{lee2022polynomial} and \citet{chen2023ddpm} assume a uniform Lipschitz constant on the score function, while \citet{chen2023improved} do not, but they all utilize the Markovian property and the Gaussian form of the diffusion process to obtain their results. 
The second type uses It\^o's calculus to obtain upper bounds, such as \citet{dlinear}, who adapt existing results from stochastic localization to produce tight bounds by finding the equivalence between two methods.

%\rui{Not very clear. It seems more like two challenges caused by one differences instead of two differences. }{\color{orange}[change to one difference, but split into 2 challenges]}
However, the aforementioned ideas do not apply to stochastic interpolants due to the following key difference: the stochastic interpolant framework in consideration has a distinct structure introduced by a random interpolation between two distributions instead of a linear combination of one distribution with Gaussian, destroying the Markovian property. %\longbo{what structure? be specific}
This difference not only necessitates novel analysis for the discretization errors of score functions but also requires additional analysis to bound the discretization errors for the velocity function, which arises from the general interpolation function introduced in this context. 

% 我们的方法创新点在哪(done?)
To tackle the above challenges, in this work, we offer the first finite-time convergence bound in Kullback-Leibler (KL) error for the SDE-based generative model within the stochastic interpolant framework. 
%first discrete-time bound on the Kullback-Leibler (KL) error for the estimated target distribution under the SDE-base model. 
%when the SDE is discretized over time. 
Our result presents a novel analysis building on existing Girsanov-based techniques.
% Our result presents a novel analysis building on existing techniques for convergence analysis of score-based diffusion models, including early stopping \cite{song2021scorebased} and the application of Girsanov's theorem \cite{chen2023ddpm,pmlr-v202-oko23a}, 
%\longbo{seems redundant? or are you trying to emphasize other things?}
% which are commonly used in convergence theories of diffusion models \cite{chen2023improved,chen2023score,pfprobably,dlinear}. 
% In the analysis we further develop a method that heavily{\color{green}[should I add this ``heavily"?]}\longbo{not catchy at all. need improvement. also, only one new technical contribution?} utilizes the Gaussian latent introduced in the stochastic interpolant framework to derive upper bounds for the derivatives of conditional expectations. \longbo{any other technical novelty? state the extensions/new techniques. here we only have two. are these two very common/important?}
% % in the proof. 
In the analysis of discretization errors, one key highlight of our approach is  modeling the evolution of discretized terms via stochastic calculus. This allows us to decompose the discretization error into components linked to derivatives of conditional expectation. Notably, we leverage the Gaussian latent variables embedded within our stochastic interpolants, enabling the explicit representation of these derivatives as conditional expectations, and hence providing a key solution to the challenges.
%focus on the case of SDEs and address the above question by proposing a theorem that 


\paragraph{Our contributions.} The main contributions of our paper are as follows. 

% We give the first finite-time convergence bound for the SDE-based generative model developed with stochastic interpolants. We propose a general error bound on the generative process, which requires no Lipschitz assumptions on the score functions or velocity functions. This setting is similar to the analyses for score-based diffusion models without Lipschitz score asssumptions, such as the analyses in \cite{chen2023improved} and \cite{dlinear}.

% We further give a carefully designed schedule of step sizes, then we bound the number of steps we need to achieve $\varepsilon^2$ KL error bound. When we reduce the problem to the diffusion model case by changing the base distribution $\rho_0$ to Gaussian, we obtain a bound that is of the same order as in \cite{chen2023improved}. Moreover, we implement the sampler, together with our proposed schedule, and produce an intuitive comparison against using uniform step sizes to show that using our schedule is better if no additional regularity condition is assumed.
(i) This work presents the first finite-time convergence bound for the SDE-based generative model within the stochastic interpolant framework. Specifically, we formulate the discrete-time sampler using the Euler–Maruyama scheme and derive a general error bound on the generative process, which notably does not require Lipschitz assumptions on the score functions or velocity functions. This setting aligns with recent analyses of score-based diffusion models that relax Lipschitz assumptions on the score functions, such as those by \citet{chen2023improved} and \citet{dlinear}.

(ii) We propose a novel  schedule of step sizes and rigorously  bound  the number of steps required to achieve an $\varepsilon^2$ KL-error. In the specific case where the base distribution $\rho_0$ is Gaussian, where our setting reduces to the standard diffusion model, our bound achieves the same order of dependence as that by \citet{chen2023improved}. %\longbo{state here that we design a sampler. otherwise it seems strange in bullet 3}

(iii) We implement the sampler with our proposed schedule and conduct a comparison against using uniform step sizes. Our results validate the theoretical findings and demonstrate the superior performance of our schedule when no additional regularity conditions are assumed. 
% (i) This work presents the first finite-time convergence bound for the SDE-based generative model developed within the stochastic interpolant framework. We derive a general error bound on the generative process, which notably does not require Lipschitz assumptions on the score functions or velocity functions. This setting aligns with recent analyses of score-based diffusion models that relax Lipschitz assumptions on the score functions, such as those presented by \citet{chen2023improved} and \citet{dlinear}.

% (ii) Furthermore, we propose a carefully designed schedule of step sizes and provide a bound on the number of steps required to achieve an $\varepsilon^2$ KL-error. In the specific case where the base distribution $\rho_0$ is Gaussian, which reduces to the standard diffusion model setting, our bound achieves the same order of dependence as that by \citet{chen2023improved}. 

% (iii) Finally, we implement the sampler with our proposed schedule and conduct an comparison against using uniform step sizes. Our results validate the theoretical findings and demonstrate the superior performance of our schedule when no additional regularity conditions are assumed. 

\section{Related Works}

% TODO: diffusion models analysis, stochastic interpolants
% TODO: (new) flow based methods?
%\longbo{all refs are before 2023. add 5-10 more recent works, include some from 2024}

%\longbo{seems a bit long. also, all refs are about Flow-based methods. so why use the bold subtitle? separate them into two?}
% \paragraph{Flow-Based Methods and Stochastic Interpolants} 
\paragraph{Stochastic Interpolants}
% Flow-matching \cite{flowmatching} is a class of methods that learns a velocity field that defines a flow that transports between two distributions by solving an ODE. Rectified flow \citep{liu2022rectifiedflow,liu2022flowstraightfastlearning} is one of the variants of flow matching methods that considers a linear interpolation and focuses on straight paths to transport the data. This is achieved by performing an iterative process that rectifies the path.
% Flow-matching methods \cite{flowmatching} learn a velocity field that defines a flow transporting between two distributions by solving an ODE. Rectified flow \citep{liu2022rectifiedflow,liu2022flowstraightfastlearning} is a prominent variant of flow matching that prioritizes straight paths for data transport by considering a linear interpolation and employing an iterative rectification process.

% \cite{flows} proposes a class of generative models based on continuous-time normalizing flows using stochastic interpolants. \cite{flows} gives a velocity field transporting two distributions using a time-differentiable interpolant. This method was further extended by \cite{interpolation}. By adding Gaussian noise to the interpolant, \cite{interpolation} unifies the flow-based methods and the diffusion-based methods using the framework. Both works analyze the effect of using estimators instead of the true velocities in the equations, and bound the distribution estimation error using the velocity estimation error.
The concept of stochastic interpolants is introduced by \citet{flows}, establishing a framework for constructing generative models based on continuous-time normalizing flows. %This work demonstrates how to derive a velocity field that transports between two distributions using a time-differentiable interpolant. 
Building upon this foundation, \citet{interpolation} extend the framework by incorporating Gaussian noise into the interpolant, effectively unifying flow-based and diffusion-based methods. Both \citet{flows} and \citet{interpolation} investigate the impact of using estimators instead of the true velocities in the equations. %, providing bounds on the distribution estimation error in terms of the velocity estimation error.

% Following the framework of stochastic interpolants, there are works that focus on applications. \cite{albergo2024coupling} use the framework to develop data coupling methods, solving image generation tasks such as in-painting and super-resolution. \cite{chen2024forcasting} and \cite{chen2024recurrent} adapts the conditional generation framework with stochastic interpolants, and studies the problem of future state predictions and sequential modelings, respectively.
Following the stochastic interpolants framework, several works have focused on specific applications. \citet{albergo2024coupling} utilize the framework to develop novel data coupling methods, addressing image generation tasks such as in-painting and super-resolution.  \citet{chen2024forcasting} and \citet{chen2024recurrent} adapt the conditional generation framework with stochastic interpolants to tackle future state prediction and sequential modeling problems, respectively.

% \paragraph{Convergence Analysis of Diffusion Models} Many results on the convergence rate of diffusion models were proposed under different data assumptions. \cite{manifold} studies the convergence rate when the initial distribution lies in a manifold. \cite{chen2023ddpm} assume that all the score functions are $L$-Lipschitz continuous, and then give a polynomial TV error bound. Specifically, \cite{chen2023ddpm} uses an approximation argument to apply Girsanov's theorem when the Novikov's condition does not hold. \cite{chen2023improved} further improves the bound under the same condition. \cite{chen2023improved} also develops another KL error bound without Lipschitzness assumptions which uses early stopping. This bound is then improved by \cite{dlinear} so that the number of steps required to achieve a $\varepsilon^2$ KL error is $\tilde{O}\left(\frac{d\log^2(1/\delta)}{\varepsilon^2}\right)$, which is almost linear in $d$.
\paragraph{Convergence Analysis of Diffusion Models} %\longbo{there are more diffusion work than SI. compress it}\yuhaoDone
Numerous results have been established on the convergence rates of diffusion models under various data assumptions \cite{manifold, lee2022polynomial}. 
% \cite{chen2023ddpm} assumes Lipschitz continuity of all score functions and derives a polynomial total variation (TV) error bound. 
Notably, \citet{chen2023ddpm} employ an approximation argument to apply Girsanov's theorem in scenarios where the Novikov condition does not hold. Based on this analysis, \citet{chen2023ddpm} and \citet{chen2023improved} provide error bounds assuming Lipschitz score functions. \citet{chen2023improved} also develop a KL error bound without requiring Lipschitzness assumptions, leveraging early stopping. This bound is subsequently improved by \citet{dlinear}, achieving a $\varepsilon^2$ KL error in $\tilde{O}\left(\frac{d\log^2(1/\delta)}{\varepsilon^2}\right)$ steps, which exhibits near-linear dependence on the dimension $d$. 
In addition to convergence rate analysis, several works have focused on problems such as score approximation \cite{chen2023score}, improved DDPM samplers \cite{liang2024broadeningtargetdistributionsaccelerated,li2024fasternonasymptoticconvergencediffusionbased,li2024acceleratingconvergencescorebaseddiffusion} and ODE-based methods \cite{pfprobably}. 

% In addition, there are also works that consider the score estimation problem. \cite{chen2023score} studies the score approximation, sampling complexity and the convergence rate when the initial distribution is in a low-dimensional subspace. In theories of improving the DDPM samplers, \cite{liang2024broadeningtargetdistributionsaccelerated, li2024fasternonasymptoticconvergencediffusionbased} improves the DDPM sampler so that the dependence on the error term is $\varepsilon^{-1}$, and \cite{li2024acceleratingconvergencescorebaseddiffusion} gives the same dependence on $\varepsilon$ assuming a Hessian estimator. Other works consider ODE-based methods. For example, \cite{pfprobably} introduces the Langevin corrector steps to the discretized probability flow ODE, and gives TV error bounds for the overall sampler.
