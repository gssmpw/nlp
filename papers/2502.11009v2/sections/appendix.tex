\appendix

\section{Theorems and Proofs}
\subsection{Proof for \cref{prop:sensitivity}}
\label{app:proof_sensitivity}
Recall the proposition states that given a database $D$ and a set of DCs $\constraintset$, where $|D|=n$, the following holds:
\begin{enumerate}
    \item The global sensitivity of \drastic\ is 1. 
    \item The global sensitivity of 
    \mininconsistency\ is $n$.       
    \item The global sensitivity of \problematic\ is $n$.
    \item The global sensitivity of \maxconsistency\ is exponential in $n$.
    \item The global sensitivity of \repair\ is 1.
\end{enumerate}
\begin{proof}
Consider two neighbouring datasets, $D$ and $D'$.
\paragraph{\drastic}
Adding or removing one tuple from the dataset will affect the addition or removal of all conflicts related to it in the dataset. In the worst case, this tuple could remove all conflicts in the dataset, and the $\drastic$ could go from 1 to 0 or vice versa.
\paragraph{\mininconsistency and \problematic}
The $\mininconsistency$ and $\problematic$ are concerned with the set of minimally inconsistent subsets $MI_\constraintset(D)$. The $\mininconsistency$ measure computes the total number of inconsistent subsets $|MI_\constraintset(D)|$ and $\problematic$ computes the total number of unique rows participating in $MI_\constraintset(D)$, $|\cup MI_\constraintset(D)|$. Now, without loss in generality, let's assume $D'$ has an additional tuple compared to $D$. In the worst case, the extra row could violate all other rows in the dataset, adding $|D|$ inconsistent subsets to $MI_\constraintset(D)$. Therefore, in the worst case, the $\mininconsistency$ measure and $\problematic$ could change by $|D|$.
\paragraph{\maxconsistency}
The \maxconsistency\ measure is \#P-complete and can be computed for a conflict graph $\graph$ if the dataset has only FDs in the constraint set $\constraintset$ and $\graph$ is $P_4$-free~\cite{KimelfeldLP20}. The maximum number of maximal independent sets~\cite{moon1965cliques, GriggsGG88} $f(n)$ for a graph with $n$ vertices is given by :  If $n \geqq 2$, then $f(n)= \begin{cases}3^{n / 3}, & \text { if } n \equiv 0(\bmod 3) \text {; } \\ 4.3^{[n / 3]-1}, & \text { if } n \equiv 1(\bmod 3) \text {; } \\ 2.3^{[n / 3]}, & \text { if } n \equiv 2(\bmod 3) .\end{cases}$

Using the above result, we can see that adding or removing a node in the graph can affect the total number of maximal independent sets in the order of $3^{n}$.
\paragraph{\repair}
Without loss in generality, let's assume $D'$ has an additional tuple compared to $D$. This extra row may or may not add extra violations. However, repairing this extra row in $D'$ will always remove these added violations. Therefore, \repair\ for $D'$ can only be one extra than $D$.
\end{proof} 

\subsection{DP Analysis for  $\mininconsistency$ and $\problematic ($\cref{algo:graph_general})}\label{app:graph_general}
\subsubsection{Proof for \cref{thm:privacy_proof_dc_oblivious}}\label{app:privacy_proof_dc_oblivious}
The theorem states that \cref{algo:graph_general} satisfies $(\epsilon_1 + \epsilon_2)$-node DP for $\graph$ and $(\epsilon_1 + \epsilon_2)$-DP for the input database $D$.
\begin{proof}
The proof is straightforward using the composition property of DP~\cref{prop:DP-comp-post}.
\end{proof}

\subsubsection{Proof for \cref{lemma:sens_quality}}\label{app:sens_quality}
\cref{lemma:sens_quality} states that the sensitivity of the quality function $q_{\epsilon_2}(\mathcal{G}, \theta_i)$ in Algorithm~\ref{algo:expo_mech_basic} defined in Equation~\eqref{eq:quality_function} is $\theta_{\max}=\max(\Theta)$. 

\begin{proof}
% \xh{double check the proof, there are a few issues.
% \begin{itemize}
% %    \item Check if Eqn 2 or Eqn 3. I think it should be Eqn 3.
%     \item The 2nd last inequality is incorrect; $ \theta_i$ is an upper bound, and subtracting an upper bound will not preserve $\leq$. Simply directly drop the 2nd term if it is non-negative.
%     \item mention the analysis applies to $\problematic$.
%  %   \item One important point is missing from the current proof is the reasoning for the 2nd last inequality $|\mathcal{G}'_{\theta_{\max}}.E| - |\mathcal{G}_{\theta_{\max}}.E| \leq \theta_{\max}$. This is actually related to the sensitivity of the measure over the projected graph (Lemma~\ref{lemma:sens_lap}. It should be highlighted.
% \end{itemize}}
We prove the lemma for the $\mininconsistency$ measure and show that it is similar for $\problematic$. Let us assume that $\mathcal{G}$ and $\mathcal{G}'$ are two neighbouring graphs and $\mathcal{G}'$ has one extra node $v^*$. 
    \begin{equation*}
        \begin{split}
            &\|q_{\epsilon_2}(\mathcal{G}, \theta) - q_{\epsilon_2}(\mathcal{G}^\prime, \theta)\| \leq -|\mathcal{G}_{\theta_{\max}}.E| + |\mathcal{G}_{\theta}.E| - \sqrt{2}\frac{\theta}{\epsilon_1} \\ &+ |\mathcal{G}'_{\theta_{\max}}.E| - |\mathcal{G}'_{\theta}.E| + \sqrt{2}\frac{\theta}{\epsilon_1} \\
            &\leq \left(|\mathcal{G}'_{\theta_{\max}}.E| - |\mathcal{G}_{\theta_{\max}}.E|\right) - \left(|\mathcal{G}'_\theta.E| - |\mathcal{G}_\theta.E|\right)\\
            &\leq \theta_{\max} - \left(|\mathcal{G}'_\theta.E| - |\mathcal{G}_\theta.E|\right) 
            \leq \theta_{\max}    
        \end{split}
    \end{equation*}
    The second last inequality is due to Lemma~\ref{lemma:sens_lap} that states that $|\mathcal{G}'_{\theta_{max}}.E| - |\mathcal{G}_{\theta_{max}}.E| \leq \theta_{max}$. The last inequality is because $|\mathcal{G}'_\theta.E| \geq |\mathcal{G}_\theta.E|$. Note that the neighboring graph $\mathcal{G}'$ contains all edges of $\mathcal{G}$ plus extra edges of the added node $v^*$. Due to the stable ordering of edges in the edge addition algorithm, each extra edge of $v^*$ either substitutes an existing edge or is added as an extra edge in $\mathcal{G}_\theta$. Therefore, the total edges $|\mathcal{G}'_\theta.E|$ is equal or larger than $|\mathcal{G}_\theta.E|$. We elaborate this detail further in the proof for Lemma~\ref{lemma:sens_lap}. For the $\problematic$ measure, the term in the last inequality changes to $|\mathcal{G}'_\theta.V_{>0}| - |\mathcal{G}_\theta.V_{>0}|$ and is also non-negative because $\mathcal{G}'$ contains an extra node that can only add and not subtract from the total number of nodes with positive degree.
\end{proof}

\subsubsection{Proof for ~\cref{lemma:sens_lap}}
This lemma states that the sensitivity of $f\circ\pi_\theta(\cdot)$ in Algorithm~\ref{algo:graph_general} is $\theta$, where $\pi_\theta$ is the edge addition algorithm with the user input $\theta$, and $f(\cdot)$ return returns edge count for $\mininconsistency$ and the number of nodes with positive degrees for $\problematic$.

\begin{proof}
Let's assume without loss of generality that
$\mathcal{G}^{\prime}=\left(V^{\prime}, E^{\prime}\right)$ has an additional node $v^{+}$compared to $\mathcal{G}=$ $(V, E)$, i.e., $V^{\prime}=V \cup\left\{v^{+}\right\}, E^{\prime}=E \cup E^{+}$, and $E^{+}$is the set of all edges incident to $v^{+}$in $\mathcal{G}^{\prime}$. We prove this lemma separately for $\mininconsistency$ and $\problematic$.

\paragraph{For \mininconsistency}
Let $\Lambda^{\prime}$ be the stable orderings for constructing $\pi_\theta\left(\mathcal{G}^{\prime}\right)$, and $t$ be the number of edges added to $\pi_\theta\left(\mathcal{G}^{\prime}\right)$ that are incident to $v^{+}$. Clearly, $t \leq \theta$ because of the $\theta$-bounded algorithm. Let $e_{\ell_1}^{\prime}, \ldots, e_{\ell_t}^{\prime}$ denote these $t$ edges in their order in $\Lambda^{\prime}$. Let $\Lambda_0$ be the sequence obtained by removing from $\Lambda^{\prime}$ all edges incident to $v^{+}$, and $\Lambda_k$, for $1 \leq k \leq t$, be the sequence obtained by removing from $\Lambda^{\prime}$ all edges that both are incident to $v^{+}$and come after $e_{\ell_k}^{\prime}$ in $\Lambda^{\prime}$. Let $\pi_\theta^{\Lambda_k}\left(\mathcal{G}^{\prime}\right)$, for $0 \leq k \leq t$, be the graph reconstructed by trying to add edges in $\Lambda_k$ one by one on nodes in $\mathcal{G}^{\prime}$, and $\lambda_k$ be the sequence of edges from $\Lambda_k$ that are actually added in the process. Thus $\lambda_k$ uniquely determines $\pi_\theta^{\Lambda_k}\left(\mathcal{G}^{\prime}\right)$; we abuse the notation and use $\lambda_k$ to also denote $\pi_\theta^{\Lambda_k}\left(\mathcal{G}^{\prime}\right)$. We have $\lambda_0=\pi_\theta(\mathcal{G})$, and $\lambda_t=\pi_\theta\left(\mathcal{G}^{\prime}\right)$.

In the rest of the proof, we show that $\forall k$ such that $1 \leq k \leq t$, at most 1 edge will differ between $\lambda_k$ and $\lambda_{k-1}$. This will prove the lemma because there are at most $t$ (upper bounded by $\theta$) edges that are different between $\lambda_t$ and $\lambda_0$.

To prove that any two consecutive sequences differ by at most 1 edge, let's first consider how the sequence $\lambda_k$ differs from $\lambda_{k-1}$. Recall that by construction, $\Lambda_k$ contains one extra edge in addition to $\Lambda_{k-1}$ and that this edge is also incident to $v^*$. Let that additional differing edge be $e_{\ell_k}^\prime = (u_j, v^+)$. In the process of creating the graph $\pi_\theta^{\Lambda_k}(\mathcal{G}^{\prime})$, each edge will need a decision of either getting added or not. The decisions for all edges coming before $e_{\ell_k}^{\prime}$ in $\Lambda^{\prime}$ must be the same in both $\lambda_k$ and $\lambda_{k-1}$. Similarly, after $e_{\ell_k}^{\prime}$, the edges in $\Lambda_k$ and $\Lambda_{k-1}$ are exactly the same. However, the decisions for including the edges after $e_{\ell_k}^{\prime}$ may or may not be the same. Assuming that there are a total of $s \geq 1$ different decisions, we will observe how the additional edge $e_{\ell_k}^{\prime}$ makes a difference in decisions. 


When $s=1$, the only different decision must be regarding differing edge $e_{\ell_k}^\prime = (u_j, v^+)$ and that must be including that edge in the total number of edges for $\lambda_k$. Also note that due to this addition, the degree of $u_j$ gets added by 1 which did not happen for $\lambda_{k-1}$. When $s>1$, the second different decision must be regarding an edge incident to $u_j$ and that is because degree of $u_j$ has reached $\theta$, and the last one of these, denoted by $(u_j, u_{i \theta})$ which was added in $\lambda_{k-1}$, cannot be added in $\lambda_k$. In this scenario, $u_j$ has the same degree (i.e., $\theta$ ) in both $\lambda_k$ and $\lambda_{k-1}$. Now if $s$ is exactly equal to 2, then the second different decision must be not adding the edge $(u_j, u_{i \theta})$ to $\lambda_k$. Again, note here that as $(u_j, u_{i \theta})$ was not added in $\lambda_k$ but was added in $\lambda_{k-1}$, there is still space for one another edge of $u_{i \theta}$. If $s>2$, then the third difference must be the addition of an edge incident to $u_{i \theta}$ in $\lambda_k$. This process goes on for each different decision in $\lambda_k$ and $\lambda_{k-1}$. Since the total number of different decisions $s$ is finite, this sequence of reasoning will stop with a difference of at most 1 in the total number of the edges between $\lambda_{k-1}$ and $\lambda_k$.

\paragraph{For \problematic}
Assume, in the worst case, the graph $\mathcal{G}$ is a star graph with $n$ nodes such that there exists an internal node that is connected to all other $n-1$ nodes. In this scenario, there are no nodes that have 0 degrees, and the $\problematic$ measure $= n-0 = 0$. If the neighbouring graph $\mathcal{G}^\prime$ differs on the internal node, all edges of the graph are removed are the $\problematic = n$. The edge addition algorithm $\pi_\theta$ would play a minimal role here as $\theta$ could be equal to $n$.
\end{proof}

\subsubsection{Proof for \cref{thm:privacy_proof_dc_aware}}
\label{app:privacy_proof_dc_aware}

    Algorithm~\ref{algo:graph_general} with the optimized EM in  Algorithm~\ref{algo:em_opt} satisfies $(\epsilon_1 + \epsilon_2)$-DP.

\begin{proof}
    The total privacy budget is split in two ways for optimization and measure computation. These budgets can be composed using \cref{prop:DP-comp-post}. 
\end{proof}


\subsubsection{Proof for \cref{lemma:sens_quality_2stepEM}}
\label{app:sens_quality_2stepEM}
This lemma states that the sensitivity of $q_{\epsilon_2}(\mathcal{G}, \theta_i)$ in the 2-step EM (Algorithm~\ref{algo:em_opt})
defined in Equation~\eqref{eq:quality_function} is $\theta_{\max}= \min(\noisydegreebound,|V|)$ for 1st EM step and $\theta_{\max}=\theta^*$ for the 2nd EM step.

\begin{proof}
Using Lemma~\ref{lemma:sens_quality}, we know that the sensitivity of quality function $q_{\epsilon_2}(\mathcal{G}, \theta_i)$ is given by the max over all candidates in $\max(\Theta)$. For the first step of the 2-step EM, the maximum candidate apart from $|V|$ is given by $\min(\noisydegreebound,|V|)$. For the candidate $|V|$, the quality function $q$ differs. It only depends on the Laplace error $\frac{\sqrt{2}|V|}{\epsilon_2}$ and has no error from $e_{\text{bias}}$ as no edges are truncated due to $|V|$. For the 2nd step of EM, we truncate all values in the set greater than the output of the first step, i.e., $\theta^*$. Therefore, the sensitivity becomes $\theta^*$.
\end{proof}



\subsubsection{Utility Analysis for \cref{algo:graph_general}}\label{app:graph_general_utility}
\cref{thm:graph_general_utility} states that on any database instance $D$ and its respective conflict graph $\graph$, let $o$ be the output of Algorithm~\ref{algo:graph_general} with Algorithm~\ref{algo:expo_mech_basic} over $D$.  
Then,  with a probability of at least $1-\beta$, we have 
\begin{equation}
|o-a| \leq -\tilde{q}_{\opt}(D,\epsilon_2) + \frac{2 \theta_{\max}}{\epsilon_1} (\ln \frac{2|\Theta|}{|\Theta_{\opt}|\cdot \beta}) 
\end{equation}
where $a$ is the true inconsistency measure over $D$ and $\beta\leq \frac{1}{e^{\sqrt{2}}}$.

\begin{proof}
By the utility property of the exponential mechanism~\cite{mcsherry2007mechanism}, with at most probability $\beta/2$, Algorithm~\ref{algo:expo_mech_basic} will sample a bad $\theta^*$ with a  quality value as below
\begin{equation}
    q_{\epsilon_2}(\graph,\theta^*) \leq q_{\opt}(D,\epsilon_2) - \frac{2 \theta_{\max}}{\epsilon_1} (\ln \frac{2|\Theta|}{|\Theta_{\opt}|\beta})
\end{equation}
which is equivalent to 
\begin{equation}\label{eq:goodtheta}
    e_{\text{bias}}(\mathcal{G},\theta^*)  \geq -q_{\opt}(D,\epsilon_2) + \frac{2 \theta_{\max}}{\epsilon_1} (\ln \frac{2|\Theta|}{|\Theta_{\opt}|\beta}) -  \frac{\sqrt{2}\theta^*}{\epsilon_2}.
\end{equation}

With probability $\beta/2$, where $\beta\leq \frac{1}{e^{\sqrt{2}}}$,
we have 
\begin{equation}    
\text{Lap}(\frac{\theta^*}{\epsilon_2}) \geq
      \frac{\ln(1/\beta)\theta^{*}}{\epsilon_2} \geq \frac{\sqrt{2}\theta^*}{\epsilon_2}
      \end{equation}
Then, by union bound, with at most probability $\beta$, we have 
\begin{eqnarray}
   && |o-a| \nonumber\\
       &=& 
|f(\mathcal{G}_{\theta^*})+\text{Lap}(\frac{\theta^*}{\epsilon_2})-a|
            \nonumber  \\
    &\geq& a- f(\mathcal{G}_{\theta^*})+ \frac{\sqrt{2}\theta^*}{\epsilon_2}
 \nonumber \\
    &=& f(\mathcal{G})-f(\mathcal{G}_{\theta^*})+
     \frac{\sqrt{2}\theta^*}{\epsilon_2} \nonumber \\
    &=& f(\mathcal{G})-f(\mathcal{G}_{\theta_{\max}}) +
f(\mathcal{G}_{\theta_{\max}}) - f(\mathcal{G}_{\theta^*})
 +    \frac{\sqrt{2}\theta^*}{\epsilon_2} \nonumber\\    
    &=& f(\mathcal{G})-f(\mathcal{G}_{\theta_{\max}}) +
        e_{\text{bias}}(\mathcal{G},\theta^*)  + \frac{\sqrt{2}\theta^*}{\epsilon_2} \nonumber\\
     &\geq& -q_{\opt}(D,\epsilon_2) + f(\mathcal{G})-f(\mathcal{G}_{\theta_{\max}}) + \frac{2 \theta_{\max}}{\epsilon_1} (\ln \frac{2|\Theta|}{|\Theta_{\opt}|\beta})  \nonumber \\
      &=& -\tilde{q}_{\opt}(D,\epsilon_2) + \frac{2 \theta_{\max}}{\epsilon_1} (\ln \frac{2|\Theta|}{|\Theta_{\opt}|\beta})
\end{eqnarray}

\end{proof}


\subsection{DP Analysis for $\repair$ (\cref{algo:dp_vertexcover})} \label{app:repair}

\subsubsection{Proof for \cref{thm:vertex_cover_priv_util_analysis}}
Algorithm~\ref{algo:dp_vertexcover} satisfies $\epsilon$-node DP and always outputs the size of a 2-approximate vertex cover of graph $\mathcal{G}$.
\begin{proof}
    The privacy analysis of Algorithm~\ref{algo:dp_vertexcover} is straightforward as we calculate the private vertex cover using the Laplace mechanism with sensitivity $2$ according to Proposition~\ref{prop:vertexcover_sens}. The utility analysis can be derived from the original 2-approximation algorithm. The stable ordering $\Lambda$ in Algorithm~\ref{algo:dp_vertexcover} can be perceived as one particular random order of the edges and hence has the same utility as the original 2-approximation algorithm.
\end{proof}


\subsubsection{Proof for \cref{prop:vertexcover_sens}}\label{app:vertext_cover_sensitivity}
Algorithm~\ref{algo:dp_vertexcover} obtains a vertex cover, and its size has a sensitivity of 2.

\begin{proof}
Let's assume without loss of generality that
$\mathcal{G}^{\prime}=\left(V^{\prime}, E^{\prime}\right)$ has an additional node $v^{+}$compared to $\mathcal{G}=$ $(V, E)$, i.e., $V^{\prime}=V \cup\left\{v^{+}\right\}, E^{\prime}=E \cup E^{+}$, and $E^{+}$ is the set of all edges incident to $v^{+}$ in $\mathcal{G}^{\prime}$. We prove the theorem using a mathematical induction on $i$ that iterates over all edges of the global stable ordering $\Lambda$.

\underline{Base}: At step 0, the value of $c$ and $c'$ are both 0.

\underline{Hypothesis}: As the algorithm progresses at each step $i$ when the edge $e_i$ is chosen, either the edges of graph $\mathcal{G}'$ which is denoted by $E'_i$ has an extra vertex or the edge of graph $\mathcal{G}$ has an extra vertex. Thus, we can have two cases depending on some node $v^*$ and its edges $\{v^*\}$. Note that at the beginning of the algorithm, $v^*$ is the differing node $v^+$ and $\mathcal{G}'$ has the extra edges of $v^*/v^+$, but $v^*$ may change as the algorithm progresses. The cases are as illustrated below:
\begin{itemize}
    \item Case 1: $E_i$ does not contain any edges incident to $v^*$, $E'_i = E_i + \{ v^* \}$ and the vertex cover sizes at step $i$ could be $c_i = c'_i$ or $c_i = c'_i + 2$.
    \item Case 2: $E'_i$ does not contain any edges incident to $v^*$, $E_i = E'_i + \{ v^* \}$ and the vertex cover sizes at step $i$ could be $c_i = c'_i$ or $c'_i = c_i + 2$.
    \item Case 3: $E_i=E'_i$ and the vertex cover sizes at step $i$ is $c_i = c'_i$. This case occurs only when the additional node $v^+$ has no edges. 
\end{itemize}

\underline{Induction}: At step $i+1$, lets assume an edge $e_{i+1} = \{u, v\}$ is chosen. Depending on the $i^{th}$ step, we can have 2 cases as stated in the hypothesis.

\begin{itemize}
    \item Case 1 (When $E'_i$ has the extra edges of $v^*$): We can have the following subcases at step $i+1$ depending on $e_{i+1}$.
        \begin{enumerate}[label=\alph*),ref=\alph*]
            \item If the edge is part of $E'_{i}$ but not of $E_i$ ($e_{i+1} \in E'_{i} \setminus E_{i}$): Then $e_{i+1} = \{u, v\}$ should not exist in $E_i$ (according to the hypothesis at the $i$ step) and one of $u$ or $v$ must be $v^*$. Let's assume without loss of generality that $v$ is $v^*$. The algorithm will add $(u, v)$ to $C'$ and update $c'_{i+1} = c_i + 2$. Hence, we have either $c'_{i+1} = c_{i+1}$ or $c'_{i+1} = c + 2$.

            In addition, all edges of $u$ and $v/v^*$ will be removed from $E'_{i+1}$. Thus, we have $E_{i+1} = E'_{i+1} + \{u\}$, where $\{u\}$ represent edges of $u$. Now $u$ becomes the new $v^*$ and moves to Case 2 for the $i+1$ step.  
            
            \item If the edge is part of both $E'_i$ and $E_i$($e_{i+1} \in E'_i$ and $e_{i+1} \in E_i$): In this case $(u,v)$ will be added to both $C$ and $C'$ and the vertex sizes with be updated as $c_{i+1} = c_i + 2$ and $c'_{i+1} = c' + 2$. 

            Also, the edges adjacent to u and v will be removed from $E_i$ and $E'_i$. We still have $E'_i = E + {v^*}$ (the extra edges of $v^*$ and remain in case 1 for step i+1. 

            \item If the edge is part of neither $E'_i$ nor $E_i$ (If $e_{i+1} \in E'_i$ and $e_{i+1} \in E_i$): the algorithm makes no change. The previous state keep constant: $E'_{i+1} = E'_i, E_{i+1} = E_i$ and $c'_{i+1} = c'_i, c_{i+1} = c_i$. The extra edges of $v^*$ are still in $E'_{i+1}$.
        \end{enumerate}
        
    \item Case 2 (When $E_i$ has the extra edges of $v^*$) : This case is symmetrical to Case 1. There will be three subcases similar to Case 1 -- a) in which after the update the state of the algorithm switches to Case 1, b) in which the state remains in Case 2, and c) where no update takes place.  

    \item Case 3 (When $E_i = E'_i$): In this case, the algorithm progresses similarly for both the graphs, and remains in case 3 with equal vertex covers, $c_{i+1} = c'_{i+1}$.
\end{itemize}

Our induction proves that our hypothesis is true and the algorithm starts with Case 1 and either remains in the same case or oscillates between Case 1 and Case 2. Hence as per our hypothesis statement, the difference between the vertex cover sizes are upper bounded by 2.
\end{proof}

\subsection{Deferred experiment}
This experiment is similar to our runtime analysis experiment. In Figure~\ref{fig:time_datasets}, we fix the total number of nodes to 10k and noise to RNoise at $\alpha=0.001$ and vary the dataset. The x-axis is ordered according to the density of the dataset. We observe that the runtime is proportional to the density of the dataset and increases exponentially with the total number of edges in the graph. 
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/runtime/time_datasets.png}
    \caption{Runtime analysis for all measures by varying datasets.}
    \label{fig:time_datasets}
\end{figure}

\subsection{Utility vs $\beta$ for Theorem~\ref{thm:graph_general_utility}}
In Figure~\ref{fig:utility_vs_beta}, we show the trend of the utility analysis according to Theorem~\ref{thm:graph_general_utility} by varying the $\beta$ parameter. We follow Example~\ref{example:running_example} and set $\epsilon_1$ and $\epsilon_2$ to 1. The figure confirms that as the value of $\beta$ increases, there is a decrease in the distance between the true answer and the output, i.e., the utility increases. However, with higher values of $\beta$, the utility analysis holds with lesser probability, i.e., $1-\beta$. 

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/utility_vs_beta.png}
    \caption{Trend of the utility analysis vs $\beta$}
    \label{fig:utility_vs_beta}
\end{figure}