


\section{Experiments}\label{sec:experiments}



This section presents our experiment results on computing the three measures outlined in Section~\ref{sec:graph-algorithms-graphproj} and Section~\ref{sec:vertex_cover}. The questions that we ask through our experiments are as follows:
\begin{enumerate}
    \item How far are the private measures from the true measures?
    \item How do the different strategies for the degree truncation bound compare against each other? 
    \item How do our methods perform at different privacy budgets?
\end{enumerate}


\begin{table}[b]
\small
    \centering
    %\small
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
%    \multirow{3}{*}{Dataset} & \multirow{3}{*}{\#Tuple} & \multirow{3}{*}{\#Attrs} & \multirow{3}{*}{\#DCs (\#FDs)} & \multirow{3}{*}{Max Deg} \\
%        &    &    &     & \\
%        &    &    &     &  with 1\% RNoise \\
Dataset & \#Tuple & \#Attrs & \#DCs(\#FDs) & 
\begin{tabular}[c]{@{}c@{}}Max Deg \\ 1\% RNoise\end{tabular}
  \\
    \hline
    Adult~\cite{misc_adult_2} & 32561 & 15 & 3 (2) & 9635\\
    Flight~\cite{flight} & 500000 & 20 & 13 (13) & 1520\\
    Hospital~\cite{hospital} & 114919 & 15 & 7 (7) & 793\\
    Stock \cite{oleh_onyshchak_2020} & 122498 & 7 & 1 (1) & 1\\
    Tax \cite{chu2013discovering} & 1000000 & 15 & 9 (7) & 373\\
    \hline
    \end{tabular}
    \caption{Description of datasets. The max deg column shows the maximum degree of any node in a $10k$ rows subset of the conflict graph of the dataset with 1\% RNoise. }
    \label{tab:datasets}
\end{table}



\begin{figure*}
    \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Stock_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Tax_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Hospital_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Flight_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Adult_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \includegraphics[width=0.3\textwidth]{images/legend_2.png}
    \caption{$\problematic$ (Positive degree nodes)}
    \label{fig:tp_rnoise_pdedges}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Stock_no_of_edges_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Tax_no_of_edges_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Hospital_no_of_edges_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Flight_no_of_edges_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Adult_no_of_edges_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
    \includegraphics[width=0.4\textwidth]{images/legend2_r2t.png}
    \caption{$\mininconsistency$ (Number of edges)}
    \label{fig:tp_rnoise_nedges}
    \end{subfigure}
      \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Stock_vertex_cover_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Tax_vertex_cover_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Hospital_vertex_cover_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Flight_vertex_cover_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Adult_vertex_cover_samegraph_10000_rnoise_eps_1.0_r2t.jpg}
         \includegraphics[width=0.3\textwidth]{images/legend_2.png}
         \caption{$\repair$ (Size of vertex cover)}
         \label{fig:tp_rnoise_vcover}
     \end{subfigure}
     \caption{True vs Private estimates for all dataset with RNoise $\alpha = 0.01$ at $\epsilon=1$. The $\problematic$ measure (figure a) and $\mininconsistency$ measure (figure b) are computed using our graph projection approach, and $\repair$ measure using our private vertex cover size approach. }
     \label{fig:tp_RNoise}
\end{figure*}




\subsection{Experimental Setup}

All our experiments are performed on a server with Intel Xeon Platinum 8358 CPUs (2.60GHz) and 1 TB RAM. Our code is in Python 3.11 and can be found in the artifact submission.
All experiments are repeated for $10$ runs, and the mean error value is reported. 

\paratitle{Datasets and violation generation}
We replicate the exact setup as \citet{LivshitsBKS20} for experimentation. We conduct experiments on five real-life datasets and their corresponding DCs as described in Table~\ref{tab:datasets}.
\ifpaper
\else
\begin{itemize}
    \item Adult~\cite{misc_adult_2}: Annual income results from various factors.
    \item Flight~\cite{flight}: Flight information across the US.
    \item Hospital~\cite{hospital}: Information about different hospitals across the US and their services.
    \item Stock~\cite{oleh_onyshchak_2020}: Trading stock information  on various dates
    \item Tax~\cite{chu2013discovering}: Personal tax infomation.
\end{itemize}
\fi
%These datasets exhibit varying numbers of denial constraints (DCs) most of which are FDs as described in Table~\ref{tab:datasets}. 
%All DCs are of the form   $ \sigma : \forall t_i, t_j \in D, \neg(\phi_1 \land \ldots \land \phi_k)$, where $t_i$ and $t_j$ are database tuples. Each $\phi_i$ is of the form $t_i[a_1] \circ t_j[a_2]$ where $a_1, a_2$ are attributes of the schema and $\circ$ is a comparison operator $\circ \in \{<, >, \leq, \geq, =, \neq\}$. \xh{may drop this DC definition here.}
%These datasets vary immensely in the density of their conflict graphs as described in the max degree column of Table~\ref{tab:datasets}. For example, the Adult $10k$ nodes subset has a maximum degree of 9635, whereas the Stock dataset has a maximum of 1 with the same amount of conflict addition. \xh{May move "These datasets vary ...addition." to after describing "violation generation"}
%\paratitle{Violation generation} 
These datasets are initially consistent with the constraints.  All experiments are done on a subset of $10k$ rows, and violations are added similarly using both their proposed algorithms, namely CONoise (for Constraint-Oriented Noise) and RNoise (for Random Noise). 
\ifpaper
CONoise, in every iteration, randomly selects a constraint and two tuples and changes the values of the attributes in those tuples participating in the constraint. In our experiments, we run 200 iterations of the CONoise violations. Similarly, RNoise parameterized by $\alpha$ selects $\alpha$ values in the dataset and changes its value to another value from the active domain of the corresponding attribute (with probability 0.5) or to a typo. 
\else
CONoise introduces random violations of the constraints by running 200 iterations of the following procedure:
\begin{enumerate}
    \item Randomly select a constraint $\sigma$ from the constraint set $\constraintset$.
    \item Randomly select two tuples $t_i$ and $t_j$ from the database.
    \item For every predicate $\phi = (t_i[a_1] \circ t_j[a_2])$ of $\sigma$:
    \begin{itemize}
        \item If $t_i$ and $t_j$ jointly satisfy $\phi$, continue to the next predicate.
        \item If $\circ \in \{=, \leq, \geq\}$, change either $t_i[a_1]$ or $t_j[a_2]$ or vice versa (the choice is random).
        \item If $\circ \in \{<, >, \neq\}$, change either $t_i[a_1]$ or $t_2[a_2]$ (the choice is again random) to another value from the active domain of the attribute such that $\phi$ is satisfied, if such a value exists, or a random value in the appropriate range otherwise.
    \end{itemize}
\end{enumerate}
The second algorithm, RNoise, is parameterized by the parameter $\alpha$ that controls the noise level by modifying $\alpha$ of the values in the dataset. At each iteration of RNoise, we randomly select a database cell corresponding to an attribute that occurs in at least one constraint. Then, we change its value to another value from the active domain of the corresponding attribute (with probability 0.5) or a typo. 
\fi
The datasets vary immensely in the density of their conflict graphs as described in the max degree column of Table~\ref{tab:datasets}. For example, the Adult $10k$ nodes subset has a maximum degree of 9635, whereas the Stock dataset has a maximum of 1 with the same amount of conflict addition. 

\paratitle{Metrics} Following Livshits et al.~\cite{LivshitsBKS20}, we randomly select a subset of 10k rows of each dataset, add violations to the subset, and compute the inconsistency measures on the dataset with violations.   To measure performance, we utilize the normalized $\ell_1$ distance error~\cite{dwork2006calibrating}, $|\mathcal{I}(D,\Sigma)-\mathcal{M}(D,\Sigma,\epsilon)| /\mathcal{I}(D,\Sigma)$, where $\mathcal{M}(D,\Sigma,\epsilon)$ represents the estimated private value of the measure and $\mathcal{I}(D,\Sigma)$ denotes the true value. For $\repair$, we use the linear approximation algorithm from Livshits et al.~\cite{LivshitsBKS20} to estimate the non-private value.  


\begin{figure*}
    \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Stock_positive_degree_nodes_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Tax_positive_degree_nodes_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Hospital_positive_degree_nodes_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Flight_positive_degree_nodes_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Adult_positive_degree_nodes_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \includegraphics[width=0.3\textwidth]{images/legend_2.png}
         \caption{$\problematic$ (Positive degree nodes)}
         \label{fig:tp_conoise_pdedges}
     \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Stock_no_of_edges_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Tax_no_of_edges_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Hospital_no_of_edges_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Flight_no_of_edges_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Adult_no_of_edges_samegraph_10000_conoise_eps_1.0_r2t.jpg}
         \includegraphics[width=0.4\textwidth]{images/legend2_r2t.png}
         \caption{$\mininconsistency$ (Number of edges)}
         \label{fig:tp_conoise_nedges}
     \end{subfigure}
     \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Stock_vertex_cover_samegraph_10000_conoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Tax_vertex_cover_samegraph_10000_conoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Hospital_vertex_cover_samegraph_10000_conoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Flight_vertex_cover_samegraph_10000_conoise_eps_1.0_r2t.jpg}
    \hfill
    \includegraphics[width=0.19\textwidth]{images/true_vs_private/truevsprivate_Adult_vertex_cover_samegraph_10000_conoise_eps_1.0_r2t.jpg}
    \includegraphics[width=0.3\textwidth]{images/legend_3.png}
    \caption{$\repair$ (Size of vertex cover)}
    \label{fig:tp_conoise_vcover}
\end{subfigure}
     \caption{True vs.~private estimates for all dataset with CONoise at $\epsilon = 1$ for 200 iterations. The $\problematic$ measure (a) and $\mininconsistency$ measure (b) are computed using our graph projection approach, and the $\repair$ measure (c) using our private vertex cover size approach.}
     \label{fig:tp_conoise}
\end{figure*}




\paratitle{Algorithm variations}  We experiment with multiple different variations of Algorithm~\ref{algo:graph_general} for $\mininconsistency$ and $\problematic$. The initial candidate set for the degree bound is $\Theta = [1, 5, 10, 100, 500, 1000, 2000, 3000, \dots, 10000]$ with multiples of $1000$ along with some small candidates.
\squishlist
    \item \textit{Baseline 1}: naively sets the bound $\theta^*$ to the maximum possible degree $|V|$ in Algorithm~\ref{algo:graph_general} by skipping line 2 and the unused privacy budget $\epsilon_1$ is used for the final noise addition step. 
    %naively sets the bound as the maximum possible value, i.e., the maximum possible degree $\theta = |V|$ for the graph projection algorithm. This can be achieved by skipping line 2 of Algorithm~\ref{algo:expo_mech_basic} and setting $\theta^*$ to $|V|$ in line 3. 
    \item \textit{Baseline 2}: sets the bound $\theta^*$ to the actual maximum degree of the conflict graph  $\degree_{\max}(\graph)$. Note that this is a non-private baseline that only acts as an upper bound and is one of the best values that can be achieved without privacy constraints. 
    \item \textit{Exponential mechanism}: choose $\theta^*$ over the complete candidate set $\Theta$ using the basic EM in  Algorithm~\ref{algo:expo_mech_basic}.
    \item \textit{Hierarchical exponential mechanism}: chooses $\theta^*$ using a two-step EM with an equal budget for each step in Algorithm~\ref{algo:em_opt}, but skipping Lines 1-5 of the upper bound computation step.  
    \item \textit{Upper bound + hierarchical exponential mechanism (our approach)}: encompasses both the optimization strategies, including 
    the upper bound computation and the hierarchical exponential mechanism
    discussed in Section~\ref{sec:dc_aware} (the full Algorithm~\ref{algo:em_opt}). 
\squishend
By default, we experiment with a total privacy budget of $\epsilon=1$ unless specified otherwise. 
% The variations include two baseline algorithms in which we naively set the degree bound value $\theta$. First baseline naively sets the bound as the maximum possible value, i.e, the maximum possible degree $\theta = |V|$, and the second baseline sets the bound to the actual maximum degree over all vertices in the graph $\theta = \max_{v \in V} deg(v)$. The second baseline is a non-private baseline that only acts as an excellent upper bound as one of the best values that can be achieved. Other variations of our algorithm include three more complex parameter strategies with the initial candidate set as $\Theta = [1, 5, 10, 100, 1000, 2000, \dots, 10000]$. First, the exponential mechanism over the whole candidate set $\Theta$. Second, the two-step hierarchical exponential mechanism that chooses $\theta^*$ using a two-step exponential mechanism with an equal budget for each step as explained in section~\ref{sec:dc_aware_hier_expo_mech} and lastly, the upper bound + hierarchical exponential mechanism that encompasses both the strategies as discussed in Section~\ref{sec:dc_aware} for the upper bound computation and for the hierarchical exponential mechanism.
\subsection{Results}\label{sec:results}



\begin{figure*}
    \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Stock_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Tax_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Hospital_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Flight_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0.jpg}
        \hfill
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Adult_positive_degree_nodes_samegraph_10000_rnoise_eps_1.0.jpg}
        \includegraphics[width=\textwidth]{images/legend_1.png}
         \caption{$\problematic$ (Positive degree nodes)}
         \label{fig:comparing_strategies_pdnodes}
     \end{subfigure}
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Stock_no_of_edges_samegraph_10000_rnoise_eps_1.0.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Tax_no_of_edges_samegraph_10000_rnoise_eps_1.0.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Hospital_no_of_edges_samegraph_10000_rnoise_eps_1.0.jpg}
         \hfill
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Flight_no_of_edges_samegraph_10000_rnoise_eps_1.0.jpg}
        \hfill
         \includegraphics[width=0.19\textwidth]{images/comparing_strategies/comparingtheta_Adult_no_of_edges_samegraph_10000_rnoise_eps_1.0.jpg}
        \includegraphics[width=\textwidth]{images/legend_1.png}
         \caption{$\mininconsistency$ (Number of edges)}
         \label{fig:comparing_strategies_nedges}
     \end{subfigure}
     \caption{Computing different strategies for choosing $\theta$ for all datasets with RNoise at $\alpha=0.01$ and $\epsilon=1$. The datasets are arranged according to their densities from sparsest (left) to densest (right). }
     \label{fig:comparing_strategies}
\end{figure*}


% \subsubsection{True vs private estimation}
\paratitle{True vs private estimation}
In \Cref{fig:tp_RNoise,fig:tp_conoise}, we plot the true vs. private estimates at $\epsilon=1$ for all datasets with RNoise ($\alpha=0.01$) and CONoise (200 iterations) respectively. The datasets are ordered according to their densities from left to right. Each figure contains the measured value ($\mininconsistency$, $\problematic$, or $\repair$) on the Y-axis and the number of iterations on the X-axis. For the CONoise, the number of iterations is set to 200 for every dataset, and for RNoise, the iterations correspond to the number of iterations required to reach 1\% ($\alpha = 0.01$) number of random violations. The orange line corresponds to the true value of the measure, and the blue line corresponds to the private measure using our approach. For $\mininconsistency$ and $\problematic$ measures, the blue line represents the upper bound + hierarchical exponential mechanism strategy described in Section~\ref{sec:dc_aware} along with its standard deviation in shaded blue. For the $\repair$ measure, it represents the private minimum vertex cover size algorithm. We also add a baseline approach using a state-of-the-art private SQL approach called R2T~\cite{dong2022r2t}. We add this baseline only for the $\mininconsistency$ measure as $\repair$ cannot be written with SQL, and $\problematic$ requires the DISTINCT/GROUP BY clause that R2T does not support. Based on the experiments, we draw three significant observations. 

First, compared to the SQL baseline (R2T), our approach has a better relative error on average across all datasets. However, R2T is slightly behind for moderate to high dense datasets such as Tax ($0.207$ vs. $0.334$), Hospital ($0.209$ vs. $0.386$), and Flight($0.202$ vs. $0.205$) and Adult ($0.187$ vs. $0.269$) but \reva{falls short} for sparse datasets such as Stock ($0.492$ vs. $137.05$). This is because the true value of the measure is small, and R2T adds large amounts of noise. 

Second, our approach for the $\mininconsistency$ and $\problematic$ fluctuates more and has a higher standard deviation compared to the $\repair$ measure. This is because of the privacy noise due to the relatively high sensitivity of our upper bound + hierarchical exponential mechanism approach.
On the other hand, the vertex cover size approach for $\repair$ has a sensitivity equal to $2$ and, therefore, does not show much fluctuation when the true measure value is large enough. 

Third, we observe that our approach generally performs well across all five datasets and all inconsistency measures. The $\mininconsistency$ and $\problematic$ measures have average errors of $0.25$ and $0.46$, respectively, across all datasets where Stock is the worst performing dataset for $\mininconsistency$ and Adult is the worst performing for $\problematic$. The $\repair$ performs the best with an average error of $0.08$, with Stock as the worst-performing dataset. We investigate the performance of each dataset in detail in our next experiment and find out that the density of the graphs plays a significant role in the performance of our algorithms. 




% \subsubsection{Comparing different strategies for choosing $\theta$.}
\paratitle{Comparing different strategies for choosing $\theta$}
In \Cref{fig:comparing_strategies}, we present the performance of different algorithm variations in computing $\problematic$ and $\mininconsistency$ for all datasets using RNnoise at $\alpha=0.01$ and $\epsilon=1$. The y-axis in each figure shows the logarithmic scaled error, while the x-axis displays the actual measure value, with different colors representing the strategies. The graphs are ordered from most sparse (Stock) to least sparse (Adult) to compare methods for choosing the $\theta$ value at $\epsilon = 1$ ($\epsilon_1 = 0.4$, $\epsilon_2=0.6$). The methods include all variations described in the algorithm variation section.
\eat{
: \xh{we can drop this part}
\begin{itemize}
 \item Baseline 1, $\theta = |V|$  (blue)
    \item Baseline 2, $\theta = \degree_{\max}(\graph)$ (orange dashed)
    \item Exponential mechanism (EM) with $\epsilon_1 = 0.4$ (green)
    \item Hierarchical EM with $\epsilon_1 = 0.4$ (red)
    \item Upper bound + hierarchical EM with $\epsilon_0 = 0.1$ and $\epsilon_1= 0.3$ (purple)
\end{itemize}}
% Complex parameter selection strategies include the exponential mechanism over the whole candidate set $\Theta$ (green) with $\epsilon_1 = 0.4$, the two-step hierarchical exponential mechanism (red) with $\epsilon_1 = 0.4$ with $0.2$ budget for each step each as explained in section~\ref{sec:dc_aware_hier_expo_mech} and lastly, the upper bound + hierarchical exponential mechanism (purple) as discussed in Section~\ref{sec:dc_aware} with $\epsilon_0 = 0.1$ for upper bound estimation and $\epsilon_1= 0.3$ for the hierarchical exponential mechanism. 
We note all the strategies are private except baseline 2 (orange dash line) that sets $\theta$ as the true maximum degree of the conflict graph. 
% \ag{This would be easier to follow as an itemized list. Also, please refer to the algorithm and explain the exact line/lines that are different for each of the approaches.}.  

Our experimental results, based on error trends and graph density, reveal several key observations.

First, we consistently observed that the initial error was higher at smaller iterations across all five datasets and inconsistency measures. This is because, at smaller iterations, the true value of the measures is small due to fewer violations, and the privacy noise dominates the signal of true value.

Second, for the sparsest dataset (Stock), all strategies have errors of magnitude 3-4 larger, except the non-private baseline (orange) and our approach using both upper bound and hierarchical exponential mechanism (purple). This is because the candidate set contains many large candidates, and it is crucial to prune it using the upper bound strategy to get meaningful results. 

Third, for the moderately sparse graphs (Tax and Hospital), our approach consistently (purple) consistently outperformed other private methods.  However, the two-step hierarchical exponential mechanism (red), which had a $3$ magnitude higher error for Stock, demonstrated comparable performance within a 1-magnitude error difference for Tax and Hospital.  This suggests that when the true max degree is not excessively low, estimating it without the upper bound strategy can be effective.

Finally, for the densest graphs (Flight and Adult), we observe that the optimized exponential mechanisms (red and purple) outperform the private baselines (blue and green) for the $\problematic$ measure (nodes with positive degree) plots (above). However, they fail to beat even the naive baseline (blue) for the $\mininconsistency$ (number of edges) measure (below). This is because the optimal degree bound value $\mininconsistency$ over the dense graphs is close to the largest possible value $|V|$. For such a case, our optimized EM is not able to prune too many candidates and lower the sensitivity, and hence, it wastes some of the privacy budget in the pruning process. However, the relative errors of all the algorithms are reasonably small for dense graphs, and the noisy answers do preserve the order of the true measures (shown in previous experiments in Figures~\ref{fig:tp_RNoise} and \ref{fig:tp_conoise}). 

%we argue that refining the candidate set $\Theta$ without prior knowledge is difficult. Moreover, for these datasets, the absolute measure values are significantly large (in the order of $10^5$ for Flight and $10^6$ for Adult) as shown in the previous experiment in Figures~\ref{fig:tp_RNoise} and \ref{fig:tp_conoise} and error due to privacy results in a difference of $10^2$ for Flight and $10^3$ for Adult, i.e., two magnitudes smaller. 

%We make multiple observations from this experiment based on the error trends and density of the graphs.  Firstly, based on error trends we observe that in our experiments, the error starts at a higher value for all measures across the five datasets but goes down significantly with more violations added per iteration. This is because, at smaller iterations, the true value of the measures is small due to fewer violations and the privacy noise dominates the signal of true value.


%Secondly, based on the density of the conflict graphs, we first observe for the sparsest dataset (Stock) that all strategies, except the max deg (orange) and the upper bound + hierarchical exponential mechanism, all strategies have errors in the magnitude 3-4 larger. This is because the candidate set contains many large candidates, and it is crucial to truncate using the upper bound strategy to get meaningful results. Second, for the moderately sparse graphs (Tax and Hospital), we observe that, again, the upper bound + hierarchical exponential mechanism (purple) performs better than all the other private strategies. However, the two-step hierarchical exponential mechanism (red), which had a $3$ magnitude higher error for Stock, is within $1$ magnitude error difference range for Tax and Hospital. This is because the true max degree is not too low and can be estimated well without the upper bound strategy. 

%Finally, we observe slightly opposing trends for the densest graphs (Flight and Adult). We observe for these two datasets that the constraint-aware algorithms (red and purple) beat the naive baseline (blue) for the $\problematic$ measure (nodes with positive degree) plots (above) but fail for the $\mininconsistency$ (number of edges) measure (below). For the $\mininconsistency$ (number of edges), the constraint-aware algorithms have worse errors than the baseline approaches because the best values of $\theta$ are large ($\approx |V|)$. For example, for the $\mininconsistency$, we can notice that the naive baseline (blue), $\theta = |V|$, has an error $0.5$x more than the non-private baseline (orange) for Adult and $6.7$x for Flight and any truncation algorithm (red or purple) degrades the error of the measures for up to $9$x with Adult and $14.5$x for Flight to the non-private (orange) baseline. For such datasets, we argue that refining the candidate set $\Theta$ without prior knowledge is difficult. Moreover, for these datasets, the absolute measure values are significantly large (in the order of $10^5$ for Flight and $10^6$ for Adult) as shown in the previous experiment in Figures~\ref{fig:tp_RNoise} and \ref{fig:tp_conoise} and error due to privacy results in a difference of $10^2$ for Flight and $10^3$ for Adult, i.e., two magnitudes smaller. 

% \subsubsection{Varying privacy budget}
\paratitle{Varying privacy budget}
Figure~\ref{fig:varying_eps} illustrates how our algorithms perform at $\epsilon \in [0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 5.0]$ with varying privacy budgets. The rightmost figure for the repair measure $\repair$ has a log scale on the y-axis for better readability. We experiment with three datasets of different density properties (sparsest Stock and densest Flight) and show that our algorithm gracefully scales with the $\epsilon$ privacy budget for all three inconsistency measures. We also observe that the algorithm has a more significant error variation at smaller epsilons ($<1$) except for Stock, which has a larger variation across all epsilons. This happens when the true value for this measure is small, and adding noise at a smaller budget ruins the estimate drastically. For the $\repair$ measure, the private value reaches with $0.05$ error at $\epsilon=3$ for Stock and as early as $\epsilon=0.1$ for others. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/epsilon/combined_plot.jpg}
    % \includegraphics[width=.32\linewidth]{images/epsilon/no_of_edges.jpg}
    % \includegraphics[width=.32\linewidth]{images/epsilon/positive_degree_nodes.jpg}
    % \includegraphics[width=.32\linewidth]{images/epsilon/vertex_cover.jpg}
    \includegraphics[width=0.5\linewidth]{images/epsilon/legend_4.png}
    \caption{Computing inconsistency measures for different datasets with RNoise at $\alpha = 0.005$ and varying privacy budgets. $\repair$ plot has a y-axis in the log scale.}
    % Comparing different baselines at varying privacy budget.
    \label{fig:varying_eps}
\end{figure}


\revc{
% \subsubsection{Runtime and scalability analysis}
\paratitle{Runtime and scalability analysis}
Figure~\ref{fig:runtime} presents the runtime of our methods for each measure. We fix the privacy budget $\epsilon=1$ and run three experiments by varying the graph size, numbers of DCs, and dataset. For the first experiment, we use our largest dataset, Tax, and vary the number of nodes from $10^2$ to $10^6$. RNoise uses  $\alpha=0.005$ in the left plot and $\alpha=0.01$ in the center plot. We observe that the number of edges scales exponentially when we increase the number of nodes, and the time taken by our algorithm is proportional to the graph size. With a graph of $10^2$ nodes and $\leq 10$ edges, our algorithm takes $10^{-3}$ seconds and goes up to $4500$ seconds with $10^6$ nodes and $322$ million edges. We omit the experiment with $10^6$ nodes and $\alpha=0.01$ as the graph size for this experiment went over 30GB and was not supported by the pickle library we use to save our graph. This is not an artifact of our algorithm and can be scaled in the future using other graph libraries. 

For our second experiment, we choose a subset of $10k$ rows of the Flight dataset and vary the number of DCs to $13$ with $\alpha=0.005$. With one DC and $\alpha=0.005$, our algorithm takes approximately 5 seconds for $\mininconsistency$ and $\problematic$ and $\leq 1$ second for $\repair$, and goes up to $25$ seconds and $5$ seconds, respectively, for $\alpha=0.065$ and 13 DCs. We also notice some dips in the trend line (e.g., at 10 and 13 constraints) because the exponential mechanism chooses larger thresholds at those points, and the edge addition algorithm takes slightly longer with chosen thresholds. Our third experiment on varying datasets behaves similarly and is deferred to Appendix A.4 in the full version~\cite{full_paper} for lack of space.  
}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/runtime/combined_plot.png}
    % \includegraphics[width=.32\linewidth]{images/runtime/time_0.5.png}
    % \includegraphics[width=.32\linewidth]{images/runtime/time_1.0.png}
    % \includegraphics[width=.32\linewidth]{images/runtime/time_flight.png}
    \includegraphics[width=0.4\linewidth]{images/runtime/legend_time.png}
    \caption{\revc{Runtime analysis for all measures. Varying graph size on Tax dataset with RNoise $\alpha=0.005$ (left) and $\alpha=0.01$ (center) and varying \#DCs for Flight dataset (right). $\mininconsistency$ and $\problematic$ plots have y-axis in the log scale.}}
    % Comparing different baselines at varying privacy budget.
    \label{fig:runtime}
\end{figure}
