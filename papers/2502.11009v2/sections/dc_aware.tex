\subsection{Optimized Parameter Selection}\label{sec:dc_aware}

Our developed strategy to improve the parameter selection includes two optimization techniques. The overarching idea behind these optimizations is to gradually truncate large candidates from the candidate set $\Theta$ based on the density of the graph. For example, we observe that the Stock dataset~\cite{oleh_onyshchak_2020} has a sparse conflict graph, and its optimum degree for graph projection is in the range of $10^0-10^1$. In contrast, the graph for the Adult dataset sample~\cite{misc_adult_2} is extraordinarily dense and has an optimum degree $\theta$ greater than $10^3$, close to the sampled data size. 
Removing unneeded large candidates, especially those greater than the true maximum degree of the graph, can help the high sensitivity issue of the quality function and improve our chances of choosing a better bound. 

Our first optimization estimates an upper bound for the true maximum degree of the conflict graph and removes candidates larger than this upper bound from the initial candidate set. The second optimization is a hierarchical exponential mechanism that utilizes two steps of exponential mechanisms. The first output, $\theta^1$, is used to truncate $\Theta$ further by removing candidates larger than $\theta^1$ from the set, and the second output is chosen as the final candidate $\theta^*$. In the rest of this section, we dive deeper into the details of these optimizations and discuss their privacy analysis. 

% In Algorithm~\ref{algo:dc_aware}, we show the improved version of our algorithm. We truncate values in the candidate set $\Theta$ in a two-step process. In lines 1-2, we first use the constraints in $\constraintset$ that are FDs to find an upper bound of the maximum degree of vertices in the graph denoted by $\theta_{bound}$ and remove candidates larger than this threshold. Second, in line 3, we use a two-step exponential mechanism to choose a value of $\theta^*$. The output of the first step, $\theta^1$, is used to truncate $\Theta$ further by removing candidates larger than $\theta^1$. The output of the second step is chosen as the final candidate $\theta^*$. The rest of the algorithm lines 4-5 is similar to the DC oblivious counterpart of the algorithm, where we project the graph and compute the inconsistency measures. In the rest of the section, we dive deeper into the details of this algorithm and discuss the privacy analysis. 


\paratitle{Estimating the degree upper bound using FDs} \label{sec:dc_aware_ub}
%The maximum degree of a conflict graph $\graph$ is governed by the total number of conflicts and its density properties. As discussed earlier at the start of the section, the densities of different real-world conflict graphs vary immensely. In our use case, as the datasets that give rise to these conflict graphs are private, the density properties of the conflict graphs are unknown. 
Given a conflict graph $\graphsimple(V,E)$, we use $\degree(\graphsimple, v)$ to denote the degree of the node $v\in V$ in $\graphsimple$ and $\degree_{\max}(\graphsimple) = \max_{v\in V} \degree(\graphsimple, v)$ to denote the maximum degree in $\graphsimple$. We estimate $\degree_{\max}$ by leveraging how conflicts were formed for its corresponding dataset $D$ under $\constraintset$. 

The degree for each vertex in $\graphsimple$ can be found by going through each tuple $t$ in the database $D$ and counting the tuples that violate the $\constraintset$ jointly with $t$.
However, computing this value for each tuple is computationally expensive and highly sensitive, making it impossible to learn directly with differential privacy.  
We observe that the conflicts that arise due to functionality dependencies (FDs)  depend on the values of the left attributes in the FD. 
\begin{example}
    Consider the same setup as  Example~\ref{example:running_example}  and an FD 
$\sigma: \text{Capital}\rightarrow \text{Country}$. 
%    $\sigma : \forall t_i, t_j \dots \in D, \neg(t_i[Occupation] = t_j[Occupation] \land t_i[Income] \neq t_j[Income])$. 
We can see that the number of violations added due to the erroneous grey row is 3. This number is also one smaller than the maximum frequency of values occurring in the Capital attribute, and the most frequent value is ``Ottawa''.
\end{example}

Based on this observation, we can derive an upper bound for the maximum degree of a conflict graph if it involves only FDs, and this upper bound has a lower sensitivity. We show the upper bound in Lemma~\ref{lemma:fd_theta} for one FD first and later extend for multiple FDs.  

\begin{lemma}\label{lemma:fd_theta}
Given a database $D$ and 
a FD $\sigma: X\rightarrow Y$ as the single constraint,
where $X = \{A_1,\dots,A_k\}$ and $Y$ is a single attribute. For its respective conflict graph $\graphsimple^D_{\Sigma = \{\sigma\}}$, simplified as $\graphsimple^D_{\sigma}$, we have the maximum degree of the graph $\degree_{\max}(\graphsimple^D_{\sigma})$ upper bounded by
    \begin{equation}
     \degreebound(D,X)
    =
    \max_{\vec{a_X} \in \dom(A_1) \times \ldots \times \dom(A_k) }  \normalfont{\text{freq}}(D, \vec{a_X})-1, 
    \end{equation}    
where $\normalfont{\text{freq}}(D, \vec{a_X})$ is the frequency of values $\vec{a_X}$ occurring for the attributes $X$ in the database $D$. 
The sensitivity for $\degreebound(D,X)$ is 1. %all $\normalfont{\text{freq}}(D, a_x)$ together is 1. 
\end{lemma}

\begin{proof}
An FD violation can only happen to a tuple $t$ with other tuples $t'$ that share the same values for the attributes $X$. 
Let $\vec{a_X}^*$ be the most frequent value for $X$ in $D$, i.e., 
$$\vec{a_X}^*=\text{argmax}_{\vec{a_X} \in \dom(A_1) \times \ldots \times \dom(A_k) } \normalfont{\text{freq}}(D, \vec{a_X}).$$
In the worst case, a tuple $t$ has the most frequent value $\vec{a_X}^*$ for $X$ but has a different value in $Y$ with all the other tuples with $X=\vec{a_X}^*$. Then the number of violations involved by $t$ is $\text{freq}(D,\vec{a_X}^*)-1$.

Adding a tuple or removing a tuple to a database will change, at most, one of the frequency values by 1. Hence, the sensitivity of  the maximum frequency values is 1.
\end{proof}

Now, we will extend the analysis to multiple FDs. 
\begin{theorem}\label{theorem:fd_theta}
Given a database $D$ and a set of FDs 
$\constraintset=\{\sigma_1,\ldots,\sigma_l \}$, for its respective conflict graph $\graph$, we have the maximum degree of the graph $\degree_{\max}(\graph)$ upper bounded by
\begin{eqnarray}
    \degreebound(D,\Sigma) 
    %&\leq& 
    %\sum_{(\sigma: X\rightarrow Y)\in \Sigma} 
     %    \degree_{\max}(\graphsimple^D_{\sigma}) \nonumber \\
     =      \sum_{(\sigma: X\rightarrow Y)\in \Sigma} 
         \degreebound(D,X)
\end{eqnarray}
\end{theorem}


\eat{
\begin{lemma}\label{lemma:fd_theta}
    For any conflict graph $\mathcal{G} (V,E)$ and a functional dependency of the form $X \rightarrow Y$ where $X,Y\subseteq\{A_1,\dots,A_m\}$ and $X$ may have multiple attributes $X = \{A_1,\dots,A_k\}$ and $Y$ is a single attribute,
    \begin{equation}
       \theta_{bound}^\sigma = \max_{v \in V}(\text{deg}(v)) 
    \leq  \max_{a_x \in \dom(A_1) \times \ldots \times \dom(A_k) } |\text{freq}(a_x)|, 
    \end{equation}    
     where $\theta_{bound}^\sigma$ denotes the max theta due to the FD and $freq(a_x)$ calculates the frequency of any value $a_x$ occurring in the attributes of $X$ in the dataset. 
\end{lemma}


\proof
Consider an FD constraint $X \rightarrow Y$ with $X = \{A_1,\dots,A_k\}$, a single attribute $Y$ and a corresponding error $e(A_1, \dots, A_k, Y)$ that changes the value a cell $a_x = t[A_x]$ in any tuple $t$ of the dataset such that attribute $A_x$ is in the FD constraint $A_x \in \{A_1, \dots, A_k, Y\}$. This error could be viewed as a typo. There could be two cases based on the attribute $A_x$. First, if $A_x$ belongs to an attribute $X$, i.e., $A_x \in \{A_1, \dots, A_k\}$, we observe that the error $e$ will add violations to the dataset based on the frequency of the value $a_x$ occurring in the attribute $A_x$. In the worst case, $a_x$ could be the most occurring value, and the number of violations that could be added for the FD is the most frequent value in the domain of all $a_x \in \dom(A_{\phi_1}) \times \dom(A_{\phi_k})$. Suppose the error $e$ is in the attribute $Y$ in the second case. In this case, the number of violations added will also be equal to the frequency of attributes in $X$ but equal to the joint occurrence of those values that participated in the tuple $t$. However, such a joint frequency is upper-bounded by the single frequency of attributes in the former case.   \xh{is the last line part of the proof or it is a statement?} \qed
}

\eat{
Lemma~\ref{lemma:fd_theta} shows that if there is one FD, the maximum number of violations that could be added due to this FD is controlled by the most occurring value that appears in the equality formulas of that FD. The example below demonstrates this lemma.
% We illustrate this using the example below and show its sensitivity using Lemma~\ref{lemma:sens_fd_theta}. 

\begin{example}
    Consider the same setup as  Example~\ref{example:running_example} and assume an FD $\sigma : \forall t_i, t_j \dots \in D, \neg(t_i[Occupation] = t_j[Occupation] \land t_i[Income] \neq t_j[Income])$. We can see that the number of violations added due to the erroneous grey row is 3 which is also the max frequency of values occurring in the Occupation attribute (Doctor). 
\end{example}
}
% Lemma~\ref{lemma:sens_fd_theta} gives the sensitivity of this $\theta_{bound}^\sigma$ calculation for one FD. 

\eat{

The sensitivity of calculating this bound is fortunately also low as it deals with the frequency of values in the datasets. 

\begin{lemma}\label{lemma:sens_fd_theta}
    The sensitivity of $\theta_{bound}^\sigma(D)$ equals 1. 
     % $$\|\theta_{max}^\sigma(D) - \theta_{max}^\sigma(D^\prime)\| \leq 1$$
\end{lemma}
\proof
$\theta_{bound}^\sigma$ calculates the most occurring value in the domain of the equality attributes. When a row is added or removed from the dataset, the frequency of any value in the domain can only change by 1. \qed
}


\eat{
The bound $\theta_{bound}^\sigma$ can be extended to more than one FD $\sigma_1, \dots, \sigma_k$ by summing over all the max frequencies over the equality attributes in the FDs as shown in Theorem~\ref{theorem:fd_theta}.

\begin{theorem}\label{theorem:fd_theta}
    For any conflict graph $\mathcal{G} (V,E)$ and a constraint set $\constraintset = [\sigma_1, \dots, \sigma_k]$ of all functional dependencies of the form $X \rightarrow Y$, 
    \begin{equation}
       \theta_{bound} = \sum_{\sigma_i} \theta_{bound}^{\sigma_i} 
    \end{equation}    
     where $\theta_{max}^{\sigma_i}$ denotes the max theta due to the FD $\sigma_i$ as in Lemma~\ref{lemma:fd_theta}.
\end{theorem}
}

\proof
By Lemma~\ref{lemma:fd_theta}, 
for each FD $\sigma: X\rightarrow Y$, a tuple may violate at most $\degreebound(D,X)$ number of tuples. In the worst case, the same tuple may violate all FDs. \qed

% The sum upper bound assumes that all FDs may violate a tuple and works better for denser datasets. In contrast, the max upper bound assumes that a tuple is violated by only one FD and is superior for sparser datasets. For example in our experiments, we observe that the Adult dataset has better utility with sum bound and the Flight dataset with max bound. One may choose to estimate the exact density of the dataset by computing all $2^n$ combinations of the $n$ FD's equality attributes marginals\footnote{The sum of all possible combinations of $n$ distinct things is $2^n$ as $\Comb{n}{0} + \Comb{n}{1} + \dots + \Comb{n}{n} = 2^n.$}. However, this would be too ineffective with a limited privacy budget and would still not be exact as there could be other types of denial constraints other than FDs in the constraint set $\constraintset$.Therefore based on this observation, we take a middle-ground strategy that involves truncating all $\theta$ candidates in the total candidate set $\Theta$ that are greater than the max upper bound and add an extra candidate to represent the sum bound, i.e, $\theta = |D|$ to account for datasets that have greater density. 

We will spend some privacy budget $\epsilon_0$ to perturb the upper bound $\degreebound(D,X)$ for all FDs with LM and add them together. Each FD is assigned with  $\epsilon_0/|\Sigma_{\text{FD}}|$, where
$\Sigma_{\text{FD}}$ is the set of FDs in $\Sigma$. We denote this perturbed upper bound as $\noisydegreebound$ and add it to the candidate set $\Theta$ if absent. 


%Algorithm~\ref{algo:max_theta} delineates the process for computing this upper bound $\theta_{bound}$.  In Lines 1-2, we initialize a constraint list for FDs, $\constraintset_{FD}$, and store all constraints in $\constraintset$ that are FDs in the list. In line 3, we initialize a variable to store $\theta_{max}$ and the privacy budget to compute the $\theta_{max}$ for FD. In Lines 4-6, we go over each FD $\sigma$ in $\constraintset_{FD}$ and find their equality attributes in $A_{eq}$. The frequency of all domain values in $A_{eq}$ is computed, and then the max is stored in $\theta_{max}^\sigma$ in lines 8-9. In lines 10-11, we add noise to the $\theta_{max}$ and sum all the $\theta_{max}^\sigma$ to find the final value of $\theta_{max}$. This algorithm has a computation complexity of $\mathcal{O}(n|\constraintset|)$.
%We illustrate it in Example~\ref{example:max_bound}.

\eat{
\begin{algorithm}[t]
\caption{Maximum bound $\theta_{bound}$ calculation \xh{update notation}}\label{algo:max_theta}
    \KwData{Dataset $D$, Constraint set $\constraintset$, Privacy budget $\epsilon$}
    \KwResult{Max bound $\theta_{bound}$}
    Initialize FD constraints $\constraintset_{FD} = []$\;
    For each $\sigma \in \constraintset$, add $\sigma$ to $\constraintset_{FD}$ if $\sigma$ is FD\;
    Initialize $\theta_{bound} = 0$ and $\epsilon_{FD} = \epsilon/|\constraintset_{FD}|$\;
    \For{$\sigma$ in $\constraintset_{FD}$}{
        Initialize  $\theta_{bound}^\sigma = 0$\;
        Get equality attributes of $\sigma$ in $A_{eq}$\;
        \For{$a$ in $\dom(A_{eq)}$}{
            \If{Frequency of $a \geq \theta_{bound}^\sigma$ }{
                $\theta_{bound}^\sigma = freq(a)$\;
            }
        }
        Add noise $\theta_{bound}^\sigma = \theta_{bound}^\sigma + Lap(1/\epsilon_{FD})$ \;
        $\theta_{bound} = \theta_{bound} + \theta_{bound}^\sigma$\;
    }
   return $\theta_{bound}$\;
\end{algorithm}
}

\eat{
\begin{example}~\label{example:max_bound}
    Consider the same setup as Example~\ref{example:running_example}. The $\theta_{bound}$ for the example dataset can be computed using its only FD constraint, $\sigma: \neg (t_i[country]=t_j[country] \land t_i[capital] \neq t_j[capital])$. We go through every domain value of the equality attribute $country$ and compute $\theta_{max} = 3$. 
\end{example}}

% \begin{algorithm}[t]
% \caption{Optimized EM for parameter selection}
% \label{algo:em_opt}
%     \KwData{Graph $\mathcal{G} (V,E)$, candidate set $\Theta=\{1,\ldots,|V|\}$, quality function $q$, privacy budget $\epsilon_1, \epsilon_2$ }
%     \KwResult{Candidate $\theta^*$}
    
%     \If{$\Sigma$ mainly consists of FDs} {$\epsilon_0\gets\epsilon_1/4$, $\epsilon_1 \gets \epsilon_1-\epsilon_0$\\
%     Compute noisy upper bound $\noisydegreebound\gets \sum_{\sigma:X\rightarrow Y} (\degreebound(D,X)+\text{Lap}(|\Sigma_{\text{FD}}|/\epsilon_0))$\\
%      }{}
%     Prune candidates $\Theta \gets \{\theta\in \Theta~|~\theta \leq \noisydegreebound\} \cup \{|V|\}$ \\
%     Set $\theta_{\max}\gets \min(\noisydegreebound,|V|)$ \\ 
%     \For{$s\in \{1,2\}$ }{
%    For each $\theta_i \in \Theta$, compute $q_{\epsilon_2}(\mathcal{G}, \theta_i)$     \commenttext{// See Equation~\eqref{eq:quality_function}}
%     \\
%     Sample $\theta^*$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q_{\epsilon_2}(\mathcal{G}, \theta_i)}{2\theta_{\max}})$ \\
%     Prune candidates $\Theta \gets \{\theta\in \Theta~|~\theta \leq \theta^*\}$ \\
%     Set $\theta_{\max}\gets \theta^*$  
%     }
    
%     %For each $\theta_i \in \Theta$, compute $q_{\epsilon_2}(\mathcal{G}, \theta_i)$     \commenttext{// See Equation~\eqref{eq:quality_function}}\\
%     %Sample $\theta_1$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q_{\epsilon_2}(\mathcal{G}, \theta_i)}{2\theta_{\max}})$ \\
%     %Prune candidates $\Theta \gets \{\theta\in \Theta~|~\theta \leq \theta_1\}$ \\
%     %Set $\theta_{\max}\gets \theta_1$ \\ 
%     %For each $\theta_i \in \Theta$, compute $q_{\epsilon_2}(\mathcal{G},\theta_i)$  \commenttext{// See Equation~\eqref{eq:quality_function}}\\
%     %Sample $\theta^*$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q_{\epsilon_2}(\mathcal{G}, \theta_i)}{2 \theta_{\max}})$\\
%     {\bf Return} $\theta^*$\\
% \end{algorithm}


\paratitle{Extension to general DCs}
The upper bound derived in Theorem~\ref{theorem:fd_theta}
only works for FDs but fails for general DCs. General DCs have more complex operators, such as ``greater/smaller than,'' in their formulas. Such inequalities require the computation of tuple-specific information, which is hard with DP. For example, consider the DC $\sigma: \neg(t_i[gain] > t_j[gain] \land t_i[loss] < t_j[loss])$ saying that if the gain for tuple $t_i$ is greater than the gain for tuple $t_j$, then the loss for $t_i$ should also be greater than $t_j$. We can observe that similar analyses for FDs do not work here as the frequency of a particular domain value in $D$ does not bound the number of conflicts related to a tuple. 
Instead, we have to iterate each tuple $t$'s gain value and find how many other tuples $t'$s violate this gain value. In the worst case, such a computation may have a sensitivity equal to the data size. Therefore, estimation using DCs may result in much noise, especially when the dataset has fewer conflicts, and the noise is added to correspond to the large sensitivity.

Our experimental study (\Cref{sec:experiments}) shows that datasets with general DCs have dense conflict graphs, which favors larger $\theta$s for graph projection. Hence, if we learn a small noisy upper bound  $\noisydegreebound$ based on the FDs with LM, we will first prune all degree candidates smaller than $\noisydegreebound$, but then include $|V|$, which corresponds to the case when no edges are truncated, and Laplace mechanism is applied with the largest possible sensitivity $|V|$, i.e., 
\begin{equation}\label{eq:fd-bound}
    \Theta' = \{\theta\in \Theta~|~\theta \leq \noisydegreebound\} \cup \{|V|\}.
\end{equation}
Though the maximum value in $\Theta'$ is $|V|$, the sensitivity of the quality function over the candidate set $\Theta'$ remains $\noisydegreebound$. For the $|V|$ candidate, the quality function only depends on the Laplace error $\frac{\sqrt{2}|V|}{\epsilon_2}$ and has no error from $e_{\text{bias}}$ as no edges will be truncated. 
% \revc{Although noisy and not as robust as our bound for FDs, our approach is cheap and performs well in practice. As we will see in the experiment section, this approach works well for the dense Adult and Flight datasets. However, there is scope for improvement in developing a strategy for general DCs. Our developed strategy justifies the separation between a conflict graph approach and more general approaches in future work.}
% \revc{Our approach performs well in practice despite being less robust than our bound for FDs. In \Cref{sec:experiments}, we show that this approach works well for the dense Adult~\cite{adult} and Flight~\cite{flight} datasets. However, there is merit in developing a more competent strategy for general DCs in future work.}
\revc{Despite being tailored for FDs, we show that, in practice, our approach is cheap and performs well for DCs. In \Cref{sec:experiments}, we show that this approach works well for the dense Adult~\cite{adult} dataset where we compute the $\problematic$ using this strategy in Figure~\ref{fig:comparing_strategies}. Developing a specific strategy for DCs is an important direction of future work.}

%To compensate for the analysis of general DCs, we add an extra candidate to the candidate set $\Theta$ equal to $|V|$. This candidate corresponds to the no truncation of the graph and hence has no error from the first term $e_{measure}$ of the quality function $q$. 
In practice, one may skip this upper bound calculation process and skip directly to the two-step exponential mechanism if it is known that the graph is too dense or contains few FDs and more general DCs. We discuss this in detail in the experiments section. 


\paratitle{Hierarchical EM}\label{sec:dc_aware_hier_expo_mech}
The upper bound $\degreebound$ may not be tight as it estimates the maximum degree in the worst case. The graph would be sparse with low degree values, and there is still room for pruning. 
To further prune candidate values in the set $\Theta$, we use a hierarchical EM that first samples a degree value $\theta^*$ to prune values in $\Theta$ and then sample again another value $\theta^*$ from the remaining candidates as the final degree the graph projection. 
Our work uses a two-step hierarchical EM by splitting the privacy budget equally into halves. One may extend this EM to more steps at the cost of breaking their privacy budget more times, but in practice, we notice that a two-step is enough for a reasonable estimate. 

\begin{example}\label{example:parameter_selection} 
    Consider the same setup as Example~\ref{example:running_example}. For this dataset, we start with $\Theta = [1, 2, 3]$ and %as we saw in Example~\ref{example:max_bound}, 
    the $\theta_{\max}$ for this setup is 3. Assume no values are pruned in the first optimization phase. 
    We compare a single versus a two-step hierarchical EM for the second optimization step. From Table~\ref{tab:example_expo_prob} in Example~\ref{example:quality_function}, we know that the $\theta_1$ has the best quality. However, as the quality values are close, the probability of choosing the best candidate is similar, as shown in Table~\ref{tab:example_expo_prob} with $\epsilon=1$.
    \begin{table}[]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
             \hline
             $\theta$ & $q$ & EM & 2-EM ($\theta^*_1 = \theta_3$) & 2-EM ($\theta^*_1 = \theta_2$)\\
             \hline
             1 & $-3.41$ & $0.35$ & $0.51$ & $1$\\
             2 & $-3.82$ & $0.33$ & $0.49$ & -\\
             3 & $-4.24$ & $0.31$ & - & -\\
             \hline
        \end{tabular}
        \caption{Probabilities of candidates with the exponential mechanism (EM) vs.~the two-step hierarchical exponential mechanism (2-EM). $\theta^*_1$ refers to the first-step output of 2-EM.}
        \label{tab:example_expo_prob}
    \end{table}
    The exponential mechanism will likely choose a suboptimal candidate in such a scenario as the probabilities are close. But if a two-step exponential mechanism is used even with half budget $\epsilon = 0.5$, the likelihood of choosing the best candidate $\theta_1$ goes up to $0.51$ if the first step chose $\theta_3$ or $1$ if the first step chosen $\theta_2$.
\end{example}


\begin{algorithm}[t]
\caption{Optimized EM for parameter selection}
\label{algo:em_opt}
    \KwData{Graph $\mathcal{G} (V,E)$, candidate set $\Theta=\{1,\ldots,|V|\}$, quality function $q$, privacy budget $\epsilon_1, \epsilon_2$ }
    \KwResult{Candidate $\theta^*$}
    
    \If{$\Sigma$ mainly consists of FDs} {$\epsilon_0\gets\epsilon_1/4$, $\epsilon_1 \gets \epsilon_1-\epsilon_0$\\
    Compute noisy upper bound $\noisydegreebound\gets \sum_{\sigma:X\rightarrow Y} (\degreebound(D,X)+\text{Lap}(|\Sigma_{\text{FD}}|/\epsilon_0))$\\
     }{}
    Prune candidates $\Theta \gets \{\theta\in \Theta~|~\theta \leq \noisydegreebound\} \cup \{\noisydegreebound, |V|\}$ \\
    Set $\theta_{\max}\gets \min(\noisydegreebound,|V|)$ \\ 
    \For{$s\in \{1,2\}$ }{
   For each $\theta_i \in \Theta$, compute $q_{\epsilon_2}(\mathcal{G}, \theta_i)$     \commenttext{// See Equation~\eqref{eq:quality_function}}
    \\
    Sample $\theta^*$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q_{\epsilon_2}(\mathcal{G}, \theta_i)}{2\theta_{\max}})$ \\
    Prune candidates $\Theta \gets \{\theta\in \Theta~|~\theta \leq \theta^*\}$ \\
    Set $\theta_{\max}\gets \theta^*$  
    }
    
    %For each $\theta_i \in \Theta$, compute $q_{\epsilon_2}(\mathcal{G}, \theta_i)$     \commenttext{// See Equation~\eqref{eq:quality_function}}\\
    %Sample $\theta_1$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q_{\epsilon_2}(\mathcal{G}, \theta_i)}{2\theta_{\max}})$ \\
    %Prune candidates $\Theta \gets \{\theta\in \Theta~|~\theta \leq \theta_1\}$ \\
    %Set $\theta_{\max}\gets \theta_1$ \\ 
    %For each $\theta_i \in \Theta$, compute $q_{\epsilon_2}(\mathcal{G},\theta_i)$  \commenttext{// See Equation~\eqref{eq:quality_function}}\\
    %Sample $\theta^*$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q_{\epsilon_2}(\mathcal{G}, \theta_i)}{2 \theta_{\max}})$\\
    {\bf Return} $\theta^*$\\
\end{algorithm}


% \paratitle{Putting together}
\paratitle{Incorporating the optimizations into the algorithm}
Algorithm~\ref{algo:em_opt} outlines the two optimization techniques. First, we decide when to use the estimated upper bound for the maximum degrees, for example, when the constraint set $\Sigma$ mainly consists of FDs. We will spend part of the budget $\epsilon_0$ from $\epsilon_1$ to perturb the upper bounds $\degreebound(D,X)$ for all FDs with Laplace mechanism and add them together (lines 1-3). The noisy upper bound $\noisydegreebound$ prunes the candidate set (line 4). We also add $|V|$ to the candidate set if there are general DCs in $\Sigma$, and then set the sensitivity of the quality function $\theta_{\max}$ to be the minimum of the noisy upper bound or $|V|$ (line 5).
Then, we conduct the two-step hierarchical exponential mechanism for parameter selection (lines 6-10). Lines 7-8 work similarly to the previous exponential mechanism algorithm with half of the remaining $\epsilon_1$, where we choose a $\theta^*$ based on the quality function. However, instead of using it as the final candidate, we use it to prune values in $\Theta$ and improve the sensitivity $\theta_{\max}$ for the second exponential mechanism (lines 9-10). Then, we repeat the exponential mechanism and output the sampled $\theta^*$ (line 11).
Algorithm~\ref{algo:em_opt} has a similar complexity of $O(|\Theta|m)$ as Algorithm~\ref{algo:expo_mech_basic}, where $|E|$ is the edge size of the graph. The overall Algorithm~\ref{algo:graph_general} has a complexity of $O(|\Sigma|n^2+|\Theta|m)$.

%\xh{i dropped some of the complexity analysis of the original text, may add it here.}

%In lines 4-5, the second exponential mechanism finds another value $\theta^*$ using the same quality function and returns it for the graph projection.  This algorithm has a computation complexity of $\mathcal{O}(|\Theta|)$. We illustrate the two optimization steps together for our running example in Example~\ref{example:parameter_selection}.

\eat{
\begin{algorithm}
\caption{Two-step parameter selection algorithm}
\label{algo:two_step_expo_mech}
    \KwData{Graph $\mathcal{G} (V,E)$, candidate set $\Theta=\{1,\ldots,n\}$, quality function $q$, privacy budget $\epsilon_1, \epsilon_2$ }
    \KwResult{Candidate $\theta^*$}
    Find max value in $\Theta$ as $\theta_{max}$\\
    For each candidate $\theta_i \in \Theta$, compute $q(\mathcal{G}, \theta_i, \theta_{max},  \epsilon_2)$ \;
    Pick $\theta_1$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q(\mathcal{G}, \theta_i, \theta_{max}, \epsilon_2)}{2\Delta_q})$\;
    Truncate all values in $\Theta$ that are greater than $\theta_1$ and pick new $\theta_{max}$\;
    For each candidate $\theta_i \in \Theta$, compute $q(\mathcal{G}, \theta_i, \theta_{max},  \epsilon_2)$ \;
    Pick $\theta^*$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q(\mathcal{G}, \theta_i, \theta_{max}, \epsilon_2)}{2\Delta_q})$\;
    return $\theta^*$\;
\end{algorithm}
}

\paratitle{Privacy and utility analysis}
The privacy analysis of the optimizations depends on the analysis of three major steps: $\degreebound$ computation with the Laplace mechanism, the two-step exponential mechanism, and the final measure calculation with the Laplace mechanism. By sequential composition, we have Theorem~\ref{thm:privacy_proof_dc_aware}. 


\begin{theorem}\label{thm:privacy_proof_dc_aware}
    Algorithm~\ref{algo:graph_general} with the optimized EM in  Algorithm~\ref{algo:em_opt} satisfies $(\epsilon_1 + \epsilon_2)$-DP.
\end{theorem}
\reva{
\begin{proof}[Proof sketch]
The proof is similar to Theorem~\ref{thm:privacy_proof_dc_oblivious} and is due to the composition property of DP as stated in Proposition~\ref{prop:DP-comp-post}.
\end{proof}
}

We show a tighter sensitivity analysis for the quality function in EM over the pruned candidate set. The sensitivity analysis is given by Lemma~\ref{lemma:sens_quality_2stepEM} and is used for $\theta_{\text{max}}$ in line 8 of Algorithm~\ref{algo:em_opt}.



\begin{lemma} \label{lemma:sens_quality_2stepEM}
The sensitivity of $q_{\epsilon_2}(\mathcal{G}, \theta_i)$ in the 2-step EM (Algorithm~\ref{algo:em_opt})
defined in Equation~\eqref{eq:quality_function} is $\theta_{\max}= \min(\noisydegreebound,|V|)$ for 1st EM step and $\theta_{\max}=\theta^*$ for the 2nd EM step.
\end{lemma}

\reva{
\begin{proof}[Proof sketch]
The proof follows from \Cref{lemma:sens_quality}, substituting the $\theta_{max}$ with the appropriate threshold values for each EM step.
\end{proof}
}

\ifpaper
\else
The proofs for the theorem and lemma are at \cref{app:privacy_proof_dc_aware} and \cref{app:sens_quality_2stepEM}.
\fi 

%We defer the proofs to the full paper. 
The utility analysis in Theorem~\ref{thm:graph_general_utility} for Algorithm~\ref{algo:graph_general} with the basic EM (Algorithm~\ref{algo:expo_mech_basic}) still applies to the optimized EM (Algorithm~\ref{algo:em_opt}). The basic EM usually has $\theta_{\max}=|D|=|V|$ and the full budget $\epsilon_1$, while the optimized EM has a much smaller $\theta_{\max}$ and slightly lower privacy budget when the graph is sparse. 
% Hence, we should see 
In practice (\Cref{sec:experiments}), we see significant utility improvements by the optimized EM for sparse graphs. When the graph is dense, we see the utility degrade slightly due to a smaller budget for each EM. However, the degradation is negligible with respect to the true inconsistency measure.  


%\begin{proof}    The privacy budget for our algorithm with optimizations is split three ways. The upper bound $\theta_{max}$ estimation uses $\epsilon_0$. The two-step hierarchical exponential mechanism with $\epsilon_1$ budget split into two halves to calculate $\theta^*$. This theta value is then used to compute the bounded graph, and finally, the inconsistency measure value is released with Laplace noise of $\epsilon_2$. Therefore, using composition properties of DP, \cref{algo:dc_oblivious} with optimizations satisfies $\epsilon_0 + \epsilon_1 + \epsilon_2$-node DP.\end{proof}


% However, this method poses a risk of privacy leakage, necessitating the privatization of $\theta$ with some privacy budget. Luckily, its sensitivity remains low ($=1$) as adding or removing one row changes the maximum frequency by 1. We call this privatization of maximum frequency method \emph{high-frequency strategy}. We demonstrate the performance of this strategy and baseline strategies experimentally for choosing $\theta$ in Section~\ref{sec:experiments}. 
