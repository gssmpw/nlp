
\section{Introduction}\label{sec:intro}
% \ag{Two main comments: (1) we need a compelling running example in the intro and the rest of the paper to showcase the motivation. This should be used in the following sections to demostrate the different concepts (2) Right now, the content contains a lot of proofs and even that is not enough to fill 12 pages. We need to see if we have enough content so it will not be too short.}

%%Outline
%Data quality matters, and greatly relies on violations to the integrity constraitns and operations that require to fix them. Prior work study "how to measure database inconsistency" and how these measures can be helpful for estimating the potential usefulness and cost of incorporating databases for downstream analytics.
%Data is private, we cannot query these database inconsistency measures directly. Hence we consider the state-of-the-art measure on "how to measure database inconsistency with differential privacy guarantees"
%Why is it hard? Naive approach is to query each integrity constraint over the database?? or ?? So we focus on the 5 measures from recent work?
%Why hasn't it been solved before? Limited work, Talk about R2T, and why it is not good
%Our attempt and contributions 

Data, often hailed as the new currency, is indispensable for data-driven applications.  Assessing its quality is important before investing money and effort into a specific dataset. Data quality is often impacted by inconsistency arising from diverse sources (encyclopedias, historical data, social networks, sensors, etc.), procedures (surveys, ML models, signal processing, etc.), or consolidation of multiple such sources and methods. Prior research~\cite{thimm2017compliance, parisi2019inconsistency, LivshitsKTIKR21} has explored how to measure database inconsistency concerning \emph{integrity constraints} (e.g., functional dependencies) and its implications for downstream analytics. However, when dealing with sensitive data like medical, financial, or social media records, directly querying inconsistency information raises privacy concerns.

Differential privacy~\cite{dwork2006calibrating} has become the de facto standard for querying sensitive databases and has been adopted by various industry and government bodies~\cite{abowd2018us,erlingsson2014rappor,ding2017collecting}. 
Informally, differential privacy guarantees that the output distribution of a mechanism remains minimally affected by the addition or removal of an individual data point. This work aims to explore the question of how to measure database inconsistency with differential privacy guarantees. In particular, we consider the setup shown in Figure~\ref{fig:problemsetup}. Consider a scenario where a vendor offers a private database (with pay) for research purposes.
A researcher interested in using the data desires first to verify its consistency with integrity constraints. The researcher can send queries about these constraints to the vendor, who responds with noisy answers generated using differentially private mechanisms.


\begin{figure}[t]
\includegraphics[width=3.4in]{problem.pdf}
\caption{Problem setup}\label{fig:problemsetup}
\end{figure}    

A standard differentially private mechanism involves adding noise to the query output, constrained by a privacy budget that limits privacy loss. Once the privacy budget is exhausted, no more queries can be answered directly from the database. This poses a challenge when dealing with multiple constraints, as iteratively checking each constraint might deplete the budget.
 Additionally, noisy answers to the question of whether the data violates a constraint (a binary question) may render the answer meaningless if the response deviates from 0 or 1.

An elegant solution is to use \emph{inconsistency measures} \cite{thimm2017compliance, parisi2019inconsistency, LivshitsKTIKR21} that quantify data quality with a single metric for all constraints. This approach aligns well with DP, as such measures give a single numerical value representing data quality. Prior work describes various measures, such as the \emph{problematic measure} (counting constraint violations), the \emph{minimal inconsistency measure} (minimal deletions for consistency), and the \emph{maximal consistent subset}. These measures often exhibit a broader range, making them potentially more suitable for estimating inconsistency under DP. In this paper, we delve into these three representative inconsistency measures, along with two additional measures identified in prior work~\cite{LivshitsKTIKR21}, to assess their suitability for private datasets. While previous research has not directly examined these measures for private databases, we identify that a primary challenge in the DP setting 
is the high sensitivity of these measures, 
where adding or removing a single row can significantly impact the final result.
Directly perturbing these measures requires substantial noise to ensure DP.
 
%Prior work describes various inconsistency measures, including the drastic measure, which is unsuitable due to its binary nature. Other, more suitable measures include the problematic measure (counting violations to constraints), the minimal inconsistency measure (minimal number of deletions needed to make the database consistent), and the maximal consistent subset. These measures have a wider range, making them potentially more suitable for estimating inconsistency under differential privacy. For example, the minimal inconsistency measure has a range of $[1,n]$, where $n$ is the dataset size.  

%In this paper, we investigate five representative inconsistency measures from prior work~\cite{LivshitsKTIKR21} that assess data quality. While previous research has not directly examined these measures for private databases, we identify that a primary challenge in the DP setting  is the high sensitivity of these measures, where adding or removing a single row can significantly impact the final result. Directly perturbing these measures requires substantial noise to ensure DP.

One approach is to treat each inconsistency measure as an SQL query and answer these queries using DP SQL mechanisms~\cite{tao2020computing,dong2022r2t,kotsogiannis2019privatesql,johnson2018towards}. However, all queries for these measures require self-joins if the integrity constraints involve more than one tuple, and the state-of-the-art DP mechanism for self-join queries, R2T~\cite{dong2022r2t}, is limited to measures that can be expressed as the SPJA query type. Measures that are computationally hard or require distinct or group-by operations cannot be handled by R2T.  Even for measures compatible with R2T, such as the minimal inconsistency measure, we found that its performance varies among datasets. As shown in Table~\ref{tab:intro_comparison}, we evaluated R2T on three datasets (detailed in~\cref{tab:datasets}) at privacy budget $\epsilon=1$ for the minimal inconsistency measure. Though R2T performed well for the Adult and Flight datasets, it reports more than 120\% relative errors for the Stock dataset that had very few violations. 

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{DP Algorithms} & \textbf{Adult}                  & \textbf{Flight}                 & \textbf{Stock}                  \\ \hline
\textbf{R2T~\cite{dong2022r2t}}           & $0.17\pm 0.01$ & $0.12\pm0.03$ & $123.19\pm 276.73$    \\ \hline
\textbf{This work}  & $0.10 \pm 0.05$ & $0.10 \pm 0.20$  & $0.07\pm 0.08$ \\ \hline
\end{tabular}
  \caption{Relative errors for a SQL approach vs our approach to compute the minimal inconsistency measure at $\epsilon=1$} \label{tab:intro_comparison}
\end{table}


To address this problem, we propose modeling the violations to the integrity constraints in the dataset as a \emph{conflict graph}. We formulate the problem of computing inconsistency measures as learning graph statistics with differential privacy~\cite{hay2009accurate,KasiviswanathanNRS13,day2016publishing}. We present algorithms that effectively estimate these measures for practical settings. For instance, our approach demonstrates strong performance across all three datasets for the minimal inconsistency measure as shown in Table~\ref{tab:intro_comparison}. This approach leverages the graph projection techniques from the state-of-the-art DP algorithms~\cite{day2016publishing}. While these algorithms have proven effective in prior studies on social network graphs, they may encounter challenges with conflict graphs arising from integrity constraints. 
We provide a utility analysis highlighting these challenges for the general approach adapted from previous work and offer optimization techniques that prove effective for various conflict graph sizes and sparsity levels. In addition, for the optimal repair measure, an NP-hard problem,  we introduce a novel 2-approximation algorithm with differential privacy guarantees. We also identified two of the five measures incompatible with the DP setting. Table~\ref{tab:summary} summarizes these results.
The main contributions of this paper are as follows: 
\squishlist
    \item We are the first to formulate the problem of inconsistency measurements with differential privacy for private datasets and discuss the associated challenges.
    \item We demonstrate that two of five such measures are incompatible in the DP setting, while the other three can be computed by formulating the problem using conflict graphs.
    \item We identify high sensitivity as a challenge in computing some of these measures for practical settings and propose optimizations using the integrity constraints to mitigate this challenge.
    \item We present experiments on five real-world datasets with varying sizes and densities to show that the proposed DP algorithms are efficient in practice.
\squishend

%Modern-day applications require collecting data or purchasing relevant datasets to generate knowledge and wealth. The performance of these applications often boils down to the quality of the data the entities hold. Despite trying multiple methods to control data quality, it is unavoidable that real-world data consists of errors/violations. There are plenty of reasons for such errors. Data may be collected from diverse sources (encyclopedias, historical data, social networks, sensors, etc.), from diverse procedures (surveys, ML models, signal processing, etc.), or due to the consolidation of multiple such sources and methods. Before putting money and effort into a particular dataset, getting an estimate of the dataset's quality is hugely invaluable. However, these datasets often contain private information such as our medical, financial, or social media data, and improper use of such data may result in significant privacy leakages, leading to hefty fines~\cite{GDPR}. Therefore, in this work, we study the problem of computing the quality of a private database. 


\eat{
Violations in datasets are often studied using data structure concepts such as denial constraints. Denial constraints can either be defined by domain experts or can be automatically discovered from a given database instance~\cite{bleifuss2017efficient, chu2013discovering}.  


\begin{example}
    Consider the Adult dataset~\cite{misc_adult_2} consisting of 32k tuples and 15 attributes with denial constraints, such as ‘two tuples with the same education category cannot have different education numbers’ and ‘tuples with higher capital gain cannot have a lower capital loss’. Randomly changing one value in the education or gain/loss attributes adds up to 9.6k violations in the dataset. 
\end{example}

When the data is private, these violations may also reveal information about the dataset. For example, an adversary participating in the dataset collection may intentionally add an error in their education attribute, and the total number of violations will reveal precisely how many other individuals have violations with the changed tuple \xh{I don't understand this line, who is the adversary, what sensitive info does the adversary learn?}. Many concepts of privacy have been proposed to protect against such leakages, but industry and government bodies have widely adopted the notion of differential privacy (DP)~\cite{dwork2006calibrating}. Informally, DP guarantees that the output of a mechanism will not change much if one individual is added or removed from the dataset. A standard method to release a differentially private estimate is by adding noise to the output of an algorithm. However, when applying DP, a privacy budget is set that helps limit the total privacy loss of each output release. Once the privacy budget is used up, no more queries can be answered directly. This is a significant challenge for our use case as a dataset may contain multiple constraints, and there might not be enough privacy budget to naively iterate over all constraints.  \xh{who are the users, and what constraint information do they want to learn with DP budget? why do they want to learn this? do we want to mention "the user" when you mention "putting money..." in the first paragraph}. A user may also be interested in different statistics of these violations, such as the total number of violations or tuples in the dataset that participate in these violations. 



One solution is to compute aggregate statistics about the violations of these constraints to get a concise estimate of the quality of a dataset using {\em inconsistency measures}~\cite{thimm2017compliance, parisi2019inconsistency, LivshitsKTIKR21}. 
% These measures can estimate the potential usefulness and cost of incorporating the dataset for downstream analytics with.
This approach aligns well with DP, as such measures give a single number that aims to quantify the quality of the data. 



\begin{example}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\linewidth]{images/r2t_vs_graphproj.jpg}
        \caption{Comparing naive SQL approach vs our graphical approach to compute total number of violations \xh{use a smaller fig or a table}}
        \label{fig:r2tvsgraph}
    \end{figure}
    \xh{Does the example appear in the wrong place?} \sm{we also talk about this in the problem setup section.}
    Consider three datasets: Adult, Stock, and Flight. They vary immensely concerning the number of DCs and densities, as shown in~\cref{tab:datasets}. In Figure~\ref{fig:r2tvsgraph}, we compare two strategies for computing the total number of violations in the datasets with errors randomly added to $1\%$ rows at a privacy budget of $\epsilon=1$. The blue bar represents summing up the violations for each DC naively using the state-of-the-art DP SQL approach called R2T~\cite{dong2022r2t}, and the orange bar represents our approach. Our approach beats the SQL approach for all datasets. 
\end{example}


Inconsistency measures have been studied vastly in the past~\cite{hunter2008measuring, thimm2017compliance, LivshitsBKS20, LivshitsKTIKR21}. In this paper, we study five such inconsistency measures that measure different aspects of the data quality for private datasets. We note that there has been no prior work that studies these inconsistency measures for private datasets. Therefore, we formally define the research problem in the DP setting and study the challenges that stem from this problem. 
A significant challenge in calculating some of these measures in the DP setting is that the sensitivity is too high, i.e., adding or removing one row may affect the final result significantly. Moreover, we pinpoint that the high sensitivity is an artifact of the interaction of the tuples and that it can be controlled if we can bound this interaction. To tackle this problem, the natural extension that comes to mind is to model the constraints in the dataset as a conflict graph and control the sensitivity using graph projection algorithms. Therefore, in this paper, we formulate the problem of computing inconsistency measures as quantifying graph statistics and show algorithms that can estimate them effectively for practical settings. Despite our efforts, we note that two of the five measures are intractable in the DP setting and we can efficiently calculate three inconsistency measures. The main contributions of this paper are as follows: 
\ag{Add figure with the framework and a component for each step, e.g., `compute conflict graph'} \sm{Benny is working on this. }
\begin{itemize}
    \item We are the first to formulate the problem of inconsistency measurement for private datasets and discuss the challenges that originate from this problem. 
    \item We show that two of five such measures are intractable in the DP setting whereas the other three can be computed by formulating the problem using conflict graphs.
    \item We pinpoint that high sensitivity is a significant challenge in computing some of these measures for practical settings and graph projection techniques can be leveraged to alleviate this challenge.
    \item We present preliminary experiments on five real-world datasets with varying densities to show that the proposed techniques for computing these measures on conflict graphs are efficient in practice.
\end{itemize}
}
% Suppose a vendor has a private database available (with pay) for research purposes. Researchers who want to use the data may want to first verify if the data can be trusted to be consistent and have some desirable properties. 
% For non-private databases, the researcher could simply test a set of integrity constraints and see if they hold in the data. However, if the data is private, a noisy answer to the question of whether the data violates a constraint (a binary question) may render the answer meaningless, since responding with an answer other than 0 or 1 does not give much utility. Furthermore, checking whether the database satisfies a large set of constraints entails running each constraint individually, which requires a large privacy budget.

% \paratitle{Inconsistency measures as a measure of database quality}
% An elegant solution is to use inconsistency measures \cite{LivshitsKTIKR21} to privately quantify the quality of the database with a single measure. There are several measures described in the paper. The drastic measure for example is still unsuitable for use with DP due to it being again a binary function. Other, more suitable measures include the problematic measure (count how many violations are in the database), minimal number of deletions/updates needed to make the database consistent, and the maximal consistent subset. Such measures have a larger range and can possibly be more suitable for estimating the inconsistency in the database. For example, the range of the measure for the minimal number of needed deletions to make the database consistent is [1,n], and its sensitivity is low. 
