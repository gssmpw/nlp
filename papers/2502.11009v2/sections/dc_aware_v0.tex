\subsection{Parameter Selection Optimization}\label{sec:dc_aware}
As we learned from Theorem~\ref{thm:graph_general_utility}, the utility of our algorithm to estimate the two measures $\problematic$ and $\mininconsistency$ depend on the size of the candidate set $\Theta$ and the sensitivity of the quality function that in the worst case is equivalent to $n$.
We observe that these terms depend primarily on the density of the graph and the number of conflicts. Moreover, these issues are intrinsically coupled as the sensitivity of the quality function depends on the max value in $\Theta$, and without any prior knowledge about the graph, the set $\Theta$ needs to include values that range the whole space of possible values, $\theta \in [1, \dots, |V|]$, as different datasets have different optimum values of $\theta$.  For example, we observe that the Stock dataset~\cite{oleh_onyshchak_2020} is sparse and has optimum $\theta$ in the range of $10^0-10^1$ and the Adult dataset~\cite{misc_adult_2} is extraordinarily dense and has $\theta$ equal to the size of the dataset $~\approx 10^3$. To tackle this issue, we devise strategies to remove unnecessary candidates from the candidate set $\Theta$. Specifically, we do so using the constraint set $\constraintset$ that gives rise to the conflicts in the dataset. 

% We develop two optimizations that significantly improve our algorithm. First, we estimate an upper bound for the maximum degree and remove unnecessarily large candidates from the initial candidate set. Second, we use a two-step hierarchical exponential mechanism to truncate further values in $\Theta$ and improve the sensitivity of the quality function. 

% \begin{algorithm}
% \caption{Constraint aware algorithm}\label{algo:dc_aware}
%     \KwData{Conflict graph $\mathcal{G}$, Candidate set $\Theta$, Privacy Budgets $\epsilon_0, \epsilon_1$, $\epsilon_2$}
%     \KwResult{Inconsistency measure $\mininconsistency$ or $\problematic$}
%     Find upper bound $\theta_{bound}$ using FD constraints $\epsilon_0$ \redtext{(Algorithm~\ref{algo:max_theta})}\;
%     Truncate values in $\Theta$ that are larger than $\theta_{bound}$\;
%     Find $\theta^*$ from $\Theta$ using two-step Exponential mechanism using half budget $\epsilon_1/2$ for each step \redtext{(Algorithm~\ref{algo:two_step_expo_mech})}\;
%     Compute $\theta^*$-bounded graph $\mathcal{G}_{\theta^*}$\;
%     Compute measure on $\mathcal{G}_{\theta^*}$ by adding Laplace noise using budget $\epsilon_2$
% \end{algorithm}

Our developed strategy to improve the algorithm includes two optimization techniques. The overarching idea behind these optimizations is to gradually truncate large candidates from the candidate set $\Theta$. The optimizations suggest that removing unneeded large candidates from the pool solves the high sensitivity of the quality function and improves our chances of choosing a better bound. The first optimization is based on estimating an upper bound for the maximum degree and removing candidates larger than this upper bound from the initial candidate set. The second optimization is a hierarchical exponential mechanism that utilizes two steps of exponential mechanisms. The first output, $\theta^1$, is used to truncate $\Theta$ further by removing candidates larger than $\theta^1$ from the set and the second output is chosen as the final candidate $\theta^*$. In the rest of this section, we dive deeper into the details of these optimizations and discuss their privacy analysis. 

% In Algorithm~\ref{algo:dc_aware}, we show the improved version of our algorithm. We truncate values in the candidate set $\Theta$ in a two-step process. In lines 1-2, we first use the constraints in $\constraintset$ that are FDs to find an upper bound of the maximum degree of vertices in the graph denoted by $\theta_{bound}$ and remove candidates larger than this threshold. Second, in line 3, we use a two-step exponential mechanism to choose a value of $\theta^*$. The output of the first step, $\theta^1$, is used to truncate $\Theta$ further by removing candidates larger than $\theta^1$. The output of the second step is chosen as the final candidate $\theta^*$. The rest of the algorithm lines 4-5 is similar to the DC oblivious counterpart of the algorithm, where we project the graph and compute the inconsistency measures. In the rest of the section, we dive deeper into the details of this algorithm and discuss the privacy analysis. 


\subsubsection{Calculating upper bound for maximum degree $\theta_{bound}$}\label{sec:dc_aware_ub}
The maximum degree of a conflict graph $\graph$ is governed by the total number of conflicts and its density properties. As discussed earlier at the start of the section, the densities of different real-world conflict graphs vary immensely. In our use case, as the datasets that give rise to these conflict graphs are private, the density properties of the conflict graphs are unknown. Therefore, to estimate the maximum degree of $\graph$, we study the conflict graph's density by leveraging how conflicts were added to its corresponding dataset under $\constraintset$. Precisely, the theoretical upper bound for any vertex of the graph can be found by going through each tuple $t_i$ in the underlying dataset $D$ and checking for how many tuples it violates $\max_{t_i \in D} vio(t_i, D, \constraintset)$, where $vio(t_i, D)$ counts the number of tuples in $D$ that violate $t_i$ for the constraint set $\constraintset$. However, computing this value for each tuple is computationally expensive and has high sensitivity, as adding or removing a tuple $t_i$ may violate all other connected tuples in the dataset.  Therefore, we rely on an estimation method based on functional dependencies (FD) constraints in the $\constraintset$ to calculate the maximum degree. We observe that the conflicts that arise due to FDs depend on the equality attributes of the FD. For any random violation in a dataset row, an FD adds violations equal to the frequency of other rows occurring in the dataset with the same equality attribute. We illustrate this idea Lemma~\ref{lemma:fd_theta} for one FD and later extend for multiple FDs.  

% of the vertex using the frequency of values occurring in the attributes participating in the constraint set $\constraintset$. We can find an upper bound on the max $\theta$ for all constraints in $\constraintset$ that are functional dependencies (FDs). We denote this bound by $\theta_{bound}$. We observe that the conflicts that arise due to FDs depend on the equality attributes of the FD. For any random violation in a dataset row, an FD adds violations equal to the other rows with the same equality attribute. We illustrate this idea Lemma~\ref{lemma:fd_theta} for one FD and later extend for multiple FDs.  

\begin{lemma}\label{lemma:fd_theta}
    For any conflict graph $\mathcal{G} (V,E)$ and a functional dependency of the form $X \rightarrow Y$ where $X,Y\subseteq\{A_1,\dots,A_m\}$ and $X$ may have multiple attributes $X = \{A_1,\dots,A_k\}$ and $Y$ is a single attribute,
    
    $$ \theta_{bound}^\sigma = \max_{v \in V}(deg(v)) 
    \leq  \max_{a_x \in \dom(A_1) \times \ldots \times \dom(A_k) } |freq(a_x)|$$,
    
     where $\theta_{bound}^\sigma$ denotes the max theta due to the FD and $freq(a_x)$ calculates the frequency of any value $a_x$ occurring in the attributes of $X$ in the dataset. 
\end{lemma}

\proof
Consider an FD constraint $X \rightarrow Y$ with $X = \{A_1,\dots,A_k\}$, a single attribute $Y$ and a corresponding error $e(A_1, \dots, A_k, Y)$ that changes the value a cell $a_x = t[A_x]$ in any tuple $t$ of the dataset such that attribute $A_x$ is in the FD constraint $A_x \in \{A_1, \dots, A_k, Y\}$. This error could be viewed as a typo. There could be two cases based on the attribute $A_x$. First, if $A_x$ belongs to an attribute $X$, i.e., $A_x \in \{A_1, \dots, A_k\}$, we observe that the error $e$ will add violations to the dataset based on the frequency of the value $a_x$ occurring in the attribute $A_x$. In the worst case, $a_x$ could be the most occurring value, and the number of violations that could be added for the FD is the most frequent value in the domain of all $a_x \in \dom(A_{\phi_1}) \times \dom(A_{\phi_k})$. Suppose the error $e$ is in the attribute $Y$ in the second case. In this case, the number of violations added will also be equal to the frequency of attributes in $X$ but equal to the joint occurrence of those values that participated in the tuple $t$. However, such a joint frequency is upper-bounded by the single frequency of attributes in the former case.   \qed

Lemma~\ref{lemma:fd_theta} shows that if there is one FD, the maximum number of violations that could be added due to this FD is controlled by the most occurring value that appears in the equality formulas of that FD. The example below demonstrates this lemma.
% We illustrate this using the example below and show its sensitivity using Lemma~\ref{lemma:sens_fd_theta}. 

\begin{example}
    Consider the same setup as  Example~\ref{example:running_example} and assume an FD $\sigma : \forall t_i, t_j \dots \in D, \neg(t_i[Occupation] = t_j[Occupation] \land t_i[Income] \neq t_j[Income])$. We can see that the number of violations added due to the erroneous grey row is 3 which is also the max frequency of values occurring in the Occupation attribute (Doctor). 
\end{example}

% Lemma~\ref{lemma:sens_fd_theta} gives the sensitivity of this $\theta_{bound}^\sigma$ calculation for one FD. 
The sensitivity of calculating this bound is fortunately also low as it deals with the frequency of values in the datasets. 

\begin{lemma}\label{lemma:sens_fd_theta}
    The sensitivity of $\theta_{bound}^\sigma(D)$ equals 1. 
     % $$\|\theta_{max}^\sigma(D) - \theta_{max}^\sigma(D^\prime)\| \leq 1$$
\end{lemma}

\proof
$\theta_{bound}^\sigma$ calculates the most occurring value in the domain of the equality attributes. When a row is added or removed from the dataset, the frequency of any value in the domain can only change by 1. \qed

The bound $\theta_{bound}^\sigma$ can be extended to more than one FD $\sigma_1, \dots, \sigma_k$ by summing over all the max frequencies over the equality attributes in the FDs as shown in Theorem~\ref{theorem:fd_theta}.

\begin{theorem}\label{theorem:fd_theta}
    For any conflict graph $\mathcal{G} (V,E)$ and a constraint set $\constraintset = [\sigma_1, \dots, \sigma_k]$ of all functional dependencies of the form $X \rightarrow Y$, 
    
    $$ \theta_{bound} = \sum_{\sigma_i} \theta_{bound}^{\sigma_i}$$,
    
     where $\theta_{max}^{\sigma_i}$ denotes the max theta due to the FD $\sigma_i$ as in Lemma~\ref{lemma:fd_theta}.
\end{theorem}

\proof
According to Lemma~\ref{lemma:fd_theta}, a tuple $t_i$ for each FD $\sigma$ may violate at max $\theta_{bound}^\sigma$ number of tuples. In the worst case, the same tuple may violate all FDs. \qed

% The sum upper bound assumes that all FDs may violate a tuple and works better for denser datasets. In contrast, the max upper bound assumes that a tuple is violated by only one FD and is superior for sparser datasets. For example in our experiments, we observe that the Adult dataset has better utility with sum bound and the Flight dataset with max bound. One may choose to estimate the exact density of the dataset by computing all $2^n$ combinations of the $n$ FD's equality attributes marginals\footnote{The sum of all possible combinations of $n$ distinct things is $2^n$ as $\Comb{n}{0} + \Comb{n}{1} + \dots + \Comb{n}{n} = 2^n.$}. However, this would be too ineffective with a limited privacy budget and would still not be exact as there could be other types of denial constraints other than FDs in the constraint set $\constraintset$.Therefore based on this observation, we take a middle-ground strategy that involves truncating all $\theta$ candidates in the total candidate set $\Theta$ that are greater than the max upper bound and add an extra candidate to represent the sum bound, i.e, $\theta = |D|$ to account for datasets that have greater density. 

Algorithm~\ref{algo:max_theta} delineates the process for computing this upper bound $\theta_{bound}$.  In Lines 1-2, we initialize a constraint list for FDs, $\constraintset_{FD}$, and store all constraints in $\constraintset$ that are FDs in the list. In line 3, we initialize a variable to store $\theta_{max}$ and the privacy budget to compute the $\theta_{max}$ for FD. In Lines 4-6, we go over each FD $\sigma$ in $\constraintset_{FD}$ and find their equality attributes in $A_{eq}$. The frequency of all domain values in $A_{eq}$ is computed, and then the max is stored in $\theta_{max}^\sigma$ in lines 8-9. In lines 10-11, we add noise to the $\theta_{max}$ and sum all the $\theta_{max}^\sigma$ to find the final value of $\theta_{max}$. This algorithm has a computation complexity of $\mathcal{O}(n|\constraintset|)$. We illustrate it in Example~\ref{example:max_bound}.

\begin{algorithm}
\caption{Maximum bound $\theta_{bound}$ calculation}\label{algo:max_theta}
    \KwData{Dataset $D$, Constraint set $\constraintset$, Privacy budget $\epsilon$}
    \KwResult{Max bound $\theta_{bound}$}
    Initialize FD constraints $\constraintset_{FD} = []$\;
    For each $\sigma \in \constraintset$, add $\sigma$ to $\constraintset_{FD}$ if $\sigma$ is FD\;
    Initialize $\theta_{bound} = 0$ and $\epsilon_{FD} = \epsilon/|\constraintset_{FD}|$\;
    \For{$\sigma$ in $\constraintset_{FD}$}{
        Initialize  $\theta_{bound}^\sigma = 0$\;
        Get equality attributes of $\sigma$ in $A_{eq}$\;
        \For{$a$ in $\dom(A_{eq)}$}{
            \If{Frequency of $a \geq \theta_{bound}^\sigma$ }{
                $\theta_{bound}^\sigma = freq(a)$\;
            }
        }
        Add noise $\theta_{bound}^\sigma = \theta_{bound}^\sigma + Lap(1/\epsilon_{FD})$ \;
        $\theta_{bound} = \theta_{bound} + \theta_{bound}^\sigma$\;
    }
   return $\theta_{bound}$\;
\end{algorithm}

\begin{example}~\label{example:max_bound}
    Consider the same setup as Example~\ref{example:running_example}. The $\theta_{bound}$ for the example dataset can be computed using its only FD constraint, $\sigma: \neg (t_i[country]=t_j[country] \land t_i[capital] \neq t_j[capital])$. We go through every domain value of the equality attribute $country$ and compute $\theta_{max} = 3$. 
\end{example}

\paratitle{Bound for general DCs}
The upper bound $\theta_{bound}$ in Algorithm~\ref{algo:max_theta} only works for FDs but fails for general DCs. General DCs have more complex operators, such as inequalities, in their formulas. Such inequalities require the computation of tuple-specific information, which is hard with DP. For example, consider a general DC, $\sigma: \neg(t_i[gain] > t_j[gain] \land t_i[loss] < t_j[loss])$, that says that if the gain for tuple $t_i$ is greater than the gain for tuple $t_j$, the loss for $t_i$ should also be greater than $t_j$. First, we can observe that similar analyses like FDs do not work here as the frequency of domain values occurring here does not directly affect the conflicts. Instead, we have to iterate each tuple $t_i$'s gain value and find how many other tuples $t_j$s violate this gain value. In the worst case, such a computation may have a sensitivity equal to the number of nodes $|V|$. Therefore, estimation using DCs may result in much noise, especially when the dataset has fewer conflicts and the noise is added to correspond to the large sensitivity. However, our experiments showed that the constraint set may still have some DCs contributing to the max theta for datasets. To compensate for the analysis of general DCs, we add an extra candidate to the candidate set $\Theta$ equal to $|V|$. This candidate corresponds to the no truncation of the graph and hence has no error from the first term $e_{measure}$ of the quality function $q$. In practice, one may skip this upper bound calculation process and skip directly to the two-step exponential mechanism if it is known that the graph is too dense or contains few FDs and more general DCs. We discuss this in detail in the experiments section. 


\subsubsection{Hierarchical exponential mechanism}\label{sec:dc_aware_hier_expo_mech}

The upper bound $\theta_{bound}$ is still loose as it estimates the maximum degree in the worst case. The maximum degree would not be as high in practice, and there is still room for truncation. Therefore, to further truncate candidate values in the set $\Theta$, we use a hierarchical exponential mechanism that truncates values from $\Theta$ and then chooses a final candidate $\theta^*$ for the graph projection. Our work uses a two-step hierarchical exponential mechanism by splitting the privacy budget equally into halves. One may extend this exponential mechanism to more steps at the cost of breaking their privacy budget more times, but in practice, we notice that a two-step is enough for a reasonable estimate. Algorithm~\ref{algo:two_step_expo_mech} illustrates the two-step hierarchical exponential mechanism for parameter selection. Lines 1-2 work similarly to the previous exponential mechanism algorithm, where we choose a $\theta_1$ based on the quality function. However, in line 3, instead of using it as the final candidate, we use it to truncate values in $\Theta$ and improve the sensitivity $\theta_{max}$ for the second exponential mechanism. In lines 4-5, the second exponential mechanism finds another value $\theta^*$ using the same quality function and returns it for the graph projection.  This algorithm has a computation complexity of $\mathcal{O}(|\Theta|)$. We illustrate the two optimization steps together for our running example in Example~\ref{example:parameter_selection}.


\begin{algorithm}
\caption{Two-step parameter selection algorithm}
\label{algo:two_step_expo_mech}
    \KwData{Graph $\mathcal{G} (V,E)$, Candidate set $\Theta$, Quality function $q$, Privacy budget $\epsilon_1, \epsilon_2$ }
    \KwResult{Candidate $\theta^*$}
    Find max value in $\Theta$ as $\theta_{max}$
    For each candidate $\theta_i \in \Theta$, compute $q(\mathcal{G}, \theta_i, \theta_{max},  \epsilon_2)$ \;
    Pick $\theta_1$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q(\mathcal{G}, \theta_i, \theta_{max}, \epsilon_2)}{2\Delta_q})$\;
    Truncate all values in $\Theta$ that are greater than $\theta_1$ and pick new $\theta_{max}$\;
    For each candidate $\theta_i \in \Theta$, compute $q(\mathcal{G}, \theta_i, \theta_{max},  \epsilon_2)$ \;
    Pick $\theta^*$ with prob $\propto \exp( \frac{\frac{\epsilon_1}{2} q(\mathcal{G}, \theta_i, \theta_{max}, \epsilon_2)}{2\Delta_q})$\;
    return $\theta^*$\;
\end{algorithm}

\begin{example}\label{example:parameter_selection}
    Consider the same setup as Example~\ref{example:running_example}. For this dataset, we start with $\Theta = [1, 2, 3]$ and as we saw in Example~\ref{example:max_bound}, the $\theta_{max}$ for this setup is 3, and thus no values are truncated in the first optimization phase. We compare a single versus a two-step hierarchical exponential mechanism for the second optimization step. From Table~\ref{tab:example_expo_prob} in Example~\ref{example:quality_function}, we know that the $\theta_1$ has the best quality. However, as the quality values are close, the probability of choosing the best candidate is similar, as shown in Table~\ref{tab:example_expo_prob} with $\epsilon=1$.
    \begin{table}[]
        \centering
        \begin{tabular}{|c|c|c|c|}
             \hline
             $\theta$ & $q$ & EM probability   \\
             \hline
             1 & $-3.41$ & $0.35$\\
             2 & $-3.82$ & $0.33$\\
             3 & $-4.24$ & $0.31$\\
             \hline
        \end{tabular}
        \caption{Exponential mechanism probabilities}
        \label{tab:example_expo_prob}
    \end{table}
    The exponential mechanism will likely choose a suboptimal candidate in such a scenario as the probabilities are close. But if a two-step exponential mechanism is used even with half budget $\epsilon = 0.5$, the likelihood of choosing the best candidate $\theta_1$ goes up to $0.51$ if the first step chose $\theta_3$ or $1$ if the first step chosen $\theta_2$.
\end{example}

\subsubsection{Privacy analysis}

The privacy analysis of the optimizations with our algorithm depends on the analysis of three major steps -- $\theta_{max}$ computation, two-step exponential mechanism, and the final measure calculation as shown in Theorem~\ref{thm:privacy_proof_dc_aware}. 

\begin{theorem}\label{thm:privacy_proof_dc_aware}
    Algorithm~\ref{algo:dc_oblivious} with optimizations satisfies $\epsilon_0 + \epsilon_1 + \epsilon_2$-node DP.
\end{theorem}
\begin{proof}
    The privacy budget for our algorithm with optimizations is split three ways. The upper bound $\theta_{max}$ estimation uses $\epsilon_0$. The two-step hierarchical exponential mechanism with $\epsilon_1$ budget split into two halves to calculate $\theta^*$. This theta value is then used to compute the bounded graph, and finally, the inconsistency measure value is released with Laplace noise of $\epsilon_2$. Therefore, using composition properties of DP, \cref{algo:dc_oblivious} with optimizations satisfies $\epsilon_0 + \epsilon_1 + \epsilon_2$-node DP.
\end{proof}

% However, this method poses a risk of privacy leakage, necessitating the privatization of $\theta$ with some privacy budget. Luckily, its sensitivity remains low ($=1$) as adding or removing one row changes the maximum frequency by 1. We call this privatization of maximum frequency method \emph{high-frequency strategy}. We demonstrate the performance of this strategy and baseline strategies experimentally for choosing $\theta$ in Section~\ref{sec:experiments}. 
