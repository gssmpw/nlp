
\section{Introduction}\label{sec:intro}
% \ag{Two main comments: (1) we need a compelling running example in the intro and the rest of the paper to showcase the motivation. This should be used in the following sections to demostrate the different concepts (2) Right now, the content contains a lot of proofs and even that is not enough to fill 12 pages. We need to see if we have enough content so it will not be too short.}

Many may consider data to be the new currency of the world. Modern-day applications require collecting data or purchasing relevant datasets to generate knowledge and wealth. The performance of these applications often boils down to the quality of the data the entities hold. Despite trying multiple methods to control data quality, it is unavoidable that real-world data consists of errors/violations. There are plenty of reasons for such errors. Data may be collected from diverse sources (encyclopedias, historical data, social networks, sensors, etc.), from diverse procedures (surveys, ML models, signal processing, etc.), or due to the consolidation of multiple such sources and methods. Before putting money and effort into a particular dataset, getting an estimate of the dataset's quality is hugely invaluable. However, these datasets often contain private information such as our medical, financial, or social media data, and improper use of such data may result in significant privacy leakages, leading to hefty fines~\cite{GDPR}. Therefore, in this work, we study the problem of computing the quality of a private database. 

Violations in datasets are often studied using data structure concepts such as denial constraints. Denial constraints can either be defined by domain experts or can be automatically discovered from a given database instance~\cite{bleifuss2017efficient, chu2013discovering}.  

\begin{figure}[t]
\includegraphics[width=3.4in]{problem.pdf}
\caption{Problem setup}
\end{figure}    

\begin{example}
    Consider the Adult dataset~\cite{misc_adult_2} consisting of 32k tuples and 15 attributes with denial constraints, such as ‘two tuples with the same education category cannot have different education numbers’ and ‘tuples with higher capital gain cannot have a lower capital loss’. Randomly changing one value in the education or gain/loss attributes adds up to 9.6k violations in the dataset. 
\end{example}

When the data is private, these violations may also reveal information about the dataset. For example, an adversary participating in the dataset collection may intentionally add an error in their education attribute, and the total number of violations will reveal precisely how many other individuals have violations with the changed tuple \xh{I don't understand this line, who is the adversary, what sensitive info does the adversary learn?}. Many concepts of privacy have been proposed to protect against such leakages, but industry and government bodies have widely adopted the notion of differential privacy (DP)~\cite{dwork2006calibrating}. Informally, DP guarantees that the output of a mechanism will not change much if one individual is added or removed from the dataset. A standard method to release a differentially private estimate is by adding noise to the output of an algorithm. However, when applying DP, a privacy budget is set that helps limit the total privacy loss of each output release. Once the privacy budget is used up, no more queries can be answered directly. This is a significant challenge for our use case as a dataset may contain multiple constraints, and there might not be enough privacy budget to naively iterate over all constraints.  \xh{who are the users, and what constraint information do they want to learn with DP budget? why do they want to learn this? do we want to mention "the user" when you mention "putting money..." in the first paragraph}. A user may also be interested in different statistics of these violations, such as the total number of violations or tuples in the dataset that participate in these violations. 

One solution is to compute aggregate statistics about the violations of these constraints to get a concise estimate of the quality of a dataset using {\em inconsistency measures}~\cite{thimm2017compliance, parisi2019inconsistency, LivshitsKTIKR21}. 
% These measures can estimate the potential usefulness and cost of incorporating the dataset for downstream analytics with.
This approach aligns well with DP, as such measures give a single number that aims to quantify the quality of the data. 

\begin{example}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\linewidth]{images/r2t_vs_graphproj.jpg}
        \caption{Comparing naive SQL approach vs our graphical approach to compute total number of violations \xh{use a smaller fig or a table}}
        \label{fig:r2tvsgraph}
    \end{figure}
    \xh{Does the example appear in the wrong place?} \sm{we also talk about this in the problem setup section.}
    Consider three datasets: Adult, Stock, and Flight. They vary immensely concerning the number of DCs and densities, as shown in~\cref{tab:datasets}. In Figure~\ref{fig:r2tvsgraph}, we compare two strategies for computing the total number of violations in the datasets with errors randomly added to $1\%$ rows at a privacy budget of $\epsilon=1$. The blue bar represents summing up the violations for each DC naively using the state-of-the-art DP SQL approach called R2T~\cite{dong2022r2t}, and the orange bar represents our approach. Our approach beats the SQL approach for all datasets. 
\end{example}

Inconsistency measures have been studied vastly in the past~\cite{hunter2008measuring, thimm2017compliance, LivshitsBKS20, LivshitsKTIKR21}. In this paper, we study five such inconsistency measures that measure different aspects of the data quality for private datasets. We note that there has been no prior work that studies these inconsistency measures for private datasets. Therefore, we formally define the research problem in the DP setting and study the challenges that stem from this problem. 
A significant challenge in calculating some of these measures in the DP setting is that the sensitivity is too high, i.e., adding or removing one row may affect the final result significantly. Moreover, we pinpoint that the high sensitivity is an artifact of the interaction of the tuples and that it can be controlled if we can bound this interaction. To tackle this problem, the natural extension that comes to mind is to model the constraints in the dataset as a conflict graph and control the sensitivity using graph projection algorithms. Therefore, in this paper, we formulate the problem of computing inconsistency measures as quantifying graph statistics and show algorithms that can estimate them effectively for practical settings. Despite our efforts, we note that two of the five measures are intractable in the DP setting and we can efficiently calculate three inconsistency measures. The main contributions of this paper are as follows: 
\ag{Add figure with the framework and a component for each step, e.g., `compute conflict graph'} \sm{Benny is working on this. }
\begin{itemize}
    \item We are the first to formulate the problem of inconsistency measurement for private datasets and discuss the challenges that originate from this problem. 
    \item We show that two of five such measures are intractable in the DP setting whereas the other three can be computed by formulating the problem using conflict graphs.
    \item We pinpoint that high sensitivity is a significant challenge in computing some of these measures for practical settings and graph projection techniques can be leveraged to alleviate this challenge.
    \item We present preliminary experiments on five real-world datasets with varying densities to show that the proposed techniques for computing these measures on conflict graphs are efficient in practice.
\end{itemize}

% Suppose a vendor has a private database available (with pay) for research purposes. Researchers who want to use the data may want to first verify if the data can be trusted to be consistent and have some desirable properties. 
% For non-private databases, the researcher could simply test a set of integrity constraints and see if they hold in the data. However, if the data is private, a noisy answer to the question of whether the data violates a constraint (a binary question) may render the answer meaningless, since responding with an answer other than 0 or 1 does not give much utility. Furthermore, checking whether the database satisfies a large set of constraints entails running each constraint individually, which requires a large privacy budget.

% \paratitle{Inconsistency measures as a measure of database quality}
% An elegant solution is to use inconsistency measures \cite{LivshitsKTIKR21} to privately quantify the quality of the database with a single measure. There are several measures described in the paper. The drastic measure for example is still unsuitable for use with DP due to it being again a binary function. Other, more suitable measures include the problematic measure (count how many violations are in the database), minimal number of deletions/updates needed to make the database consistent, and the maximal consistent subset. Such measures have a larger range and can possibly be more suitable for estimating the inconsistency in the database. For example, the range of the measure for the minimal number of needed deletions to make the database consistent is [1,n], and its sensitivity is low. 
