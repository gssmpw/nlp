\section{Preliminaries: Diffusion-based PGPIS and Limitations}
\label{sec:preliminary}

% In this section, we briefly introduce the state-of-the-art (SOTA) Diffusion-based PGPIS methods \cite{shen2024advancing,lu2024coarse}. Then, we discuss the limitations of these methods that could be well-addressed by our method introduced in \secref{sec:method}. 

\subsection{Diffusion-based PGPIS }
\label{subsec:diff_pgpis}

Given a source image $\mathbf{I}_\text{s}$, source pose $\mathbf{P}_\text{s}$, and target pose $\mathbf{P}_\tau$, PGPIS aims to generate a target image $\mathbf{I}_\tau$ in which the generated person and their clothes match those in the source image, while the generated person's pose aligns with the target pose.
%
The SOTA diffusion-based PGPIS methods are designed based on the stable diffusion (SD) \cite{rombach2022high} that involves a forward diffusion process and a backward denoising process of $T$ steps.
%
% during the training process, we first use a variational autoencoder (VAE) to extract the latent of ground truth target image $\mathbf{I}_\tau$ (\ie, $\mathbf{z}_\tau$). 
%
The forward diffusion process progressively add random Gaussian noise $\epsilon\in \mathcal{N}(0,\mathbf{I})$ to the initial latent $\mathbf{z}_0$. At the $t$th timestep, we can formulate it as
%
\begin{align} \label{eq:forward_diffusion}
    \mathbf{z}_t = \sqrt{\bar{\alpha}_t}\mathbf{z}_0+ \sqrt{1-\bar{\alpha}_t}\epsilon, t\in[1,T]
\end{align}
%
where $\bar{\alpha}_1,\bar{\alpha}_2,\ldots,\bar{\alpha}_T$ are calculated from a fixed variance schedule. The denoising process uses an UNet to predict the noise, \ie, $\epsilon_\theta(\mathbf{z}_t, t, \mathcal{C},\mathcal{O}_\beta)$ and remove the noise from the latent.
%
The set $\mathcal{C}$ involves related conditions and the function $\mathcal{O}_\beta(\cdot)$ defines the way of embedding condition features into the UNet. 
%
For PGPIS task, we can set  $\mathcal{C} =\{\mathcal{X}_\text{s},\mathcal{X}_\text{tp}, \mathcal{X}_\text{sp}\}$ where $\mathcal{X}_\text{s}$ denotes the features of source image and $\mathcal{X}_\text{tp}$ and $\mathcal{X}_\text{sp}$ contains the features of target pose and source pose.
%
To train the UNet $\epsilon_\theta(\cdot)$, the predicted noise should be the same as the sampled noise during the forward process
%
\begin{align} \label{eq:loss_diffusion}
    \mathcal{L}_\text{mse} = \mathds{E}_{\mathbf{z}_0, \mathcal{C}, \epsilon, t}(\|\epsilon-\epsilon_\theta(\mathbf{z}_t, t, \mathcal{C},\mathcal{O}_\beta)\|^2_2).
\end{align}
%
During the inference with trained $\epsilon_\theta$ and $\mathcal{O}_\beta$, we use image encoder and pose encoder to extract features of the inputs (\ie, $\mathbf{I}_\text{s}$, $\mathbf{P}_\text{s}$, $\mathbf{P}_\text{t}$) and get $\mathcal{C} =\{\mathcal{X}_\text{s},\mathcal{X}_\text{tp},\mathcal{X}_\text{sp}\}$.
%
Then, given the latent $\mathbf{z}_T$, we perform the backward denoising process iteratively to generate the predicted target image. 

\textbf{Coarse-to-fine latent Diffusion (CFLD) \cite{lu2024coarse}.} 
%
The SOTA method CFLD \cite{lu2024coarse} utilizes the pre-trained Swin-B \cite{liu2021swin} to extract multi-layer features (\ie, $\mathcal{X}_\text{s}$) of the source image and the Adapter to extract multi-layer features \cite{mou2023t2i} (\ie, $\mathcal{X}_\text{tp}$) of the target pose. Moreover, CFLD proposes two new modules as the $\text{F}_\beta$ to embed the condition features effectively.

\textbf{Progressive conditional Diffusion models (PCDM) \cite{shen2024advancing}.} Shen \etal proposed three-stage diffusion models to progressively generate high-quality synthesized images. The first stage is to predict the global embedding of the target image; the second stage is to get a coarse target estimation; the final stage is to refine the coarse result. Each stage relies on a diffusion model that could be approximately formulated with \reqref{eq:forward_diffusion} and \reqref{eq:loss_diffusion} with different condition setups. In particular, all three stages use the pre-trained image encoders, \ie, CLIP and DINOv2.

\subsection{Empirical Study}
\label{sec:limitation}

%
\begin{figure}{}{}
% \vspace{-10pt}
    \includegraphics[width=1\linewidth]{figures/fig_anaylsis.pdf}
    % \vspace{-23pt}
    \caption{{Statistical results of attention maps of our method and two SOTA methods.}
    \label{fig:limitation}
    }
% \vspace{-5pt}
\end{figure}
%
As shown in \figref{fig:fig_intro}, the state-of-the-art methods CFLD \cite{lu2024coarse} and PCDM \cite{shen2024advancing} can hardly preserve the clothing patterns and textures.
%
The feature attention maps in \figref{fig:fig_intro} demonstrate a superficial reason, that is, the SOTA methods' features did not focus on the clothing. 
%
To further validate this observation, we perform a statistical analysis.
%
Specifically, we randomly collect 50 examples from the in-shop clothes retrieval benchmark \cite{liuLQWTcvpr16DeepFashion} and use the two SOTA methods to handle these examples.
%
For each sample, we calculate the feature attention map and get its parsing map.
%
Then, we can count the attention values at different parsing regions.
%
After that, we can obtain the average attention values within different regions among all examples.
%
For CFLD, we obtain the feature attention map by calculating the final layer's average value of the softmax layer in the Swin-B. 
%
For PCDM, we follow the steps outlined in the open-source code of PCDM to calculate the attention map.
%
As the results are shown in \figref{fig:limitation}, PCDM using pre-trained CLIP and DINOv2 feature pay less attention to the clothes when we compare the attention values on clothes with the ones on the other two regions.
%
CFLD uses pre-trained Swin-B features, paying less attention to both cloth and head regions.
%
The observation inspires us to design a novel framework, which should encode the source image with both face and cloth key information preserved.
%
Our intuition is to leverage person-parsing maps to guide the image encoding process and inject the parsing-guided embeddings into diffusion generation.