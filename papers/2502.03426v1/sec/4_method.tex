\section{Methodology: Person-Parsing-guided Siamese Network Diffusion Model}
\label{sec:method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig_pipeline.pdf}
    \caption{Pipeline of the proposed person-parsing-guided Siamese network diffusion model.}
    \label{fig:pipeline}
\end{figure*}

% In this section, we first introduce the whole framework in \secref{subsec: overview} with the main architecture (\ie, Siamese network diffusion model). Then, we detail the pose-guided parsing projection in \secref{subsec: parsing projection} to estimate the person parsing map of the target image based on the source and target poses and the source image, and the module to extract person-parsing-aligned CLIP embeddings in \secref{subsec: parseclip}. We provide all implementation details in \secref{subsec:implementation}. Please refer to the whole pipeline in \figref{fig:pipeline}.

\subsection{Overview and Siamese Network Diffusion Model}
\label{subsec: overview}

%
Following the task definition of PGPIS, we have inputs including source image $\mathbf{I}_\text{s}$, source pose image $\mathbf{P}_\text{s}$, and target pose image $\mathbf{P}_\tau$.
%
We first employ a person-parsing method \cite{li2020self} to extract the parsing map of $\mathbf{I}_\text{s}$ and get $\mathbf{H}_\text{s}$.
%
Then, we propose to estimate the person-parsing map $\mathbf{H}_\tau$ of the target image by projecting the $\mathbf{H}_\text{s}$ based on the $\mathbf{p}_\tau$ and $\mathbf{p}_\text{s}$, that is,
%
\begin{align}\label{eq:parsing-projection}
    \mathbf{H}_\tau =  \text{ParseProj}(\mathbf{H}_\text{s}, \mathbf{P}_\text{s}, \mathbf{P}_\tau).
\end{align}
%
We will detail the function `\text{ParseProj}' in \secref{subsec: parsing projection} and display an example in the \figref{fig:pipeline}.
%
Then, we employ two encoders (\ie, $\phi_\text{p}$ and $\phi_\text{h}$) to extract embeddings from poses and parsing maps, respectively.
%
Specifically, we have $\mathbf{x}_\text{s} = \phi_\text{p}(\mathbf{P}_\text{s})$, $\mathbf{x}_\tau =\phi_\text{p}(\mathbf{P}_\tau)$, $\mathbf{h}_\text{s} = \phi_\text{h}(\mathbf{H}_\text{s})$, and $\mathbf{h}_\tau =\phi_\text{h}(\mathbf{H}_\tau)$. 
%

Our model is based on the pre-trained stable diffusion (SD) and we can also follow \secref{subsec:diff_pgpis} to generate the target image.
%
The key problem is how to extract the features of the source image effectively, which should turn attention to both face and clothing regions.
%
To this end, we propose to employ pre-trained CLIP and a Siamese network trained by ourselves for image encoding. As discussed in \secref{sec:limitation}, the pre-trained CLIP can capture the human face features well. To preserve this advantage, we propose to extract visual and semantic CLIP embeddings of the target-aligned person region in the source image,
%
\begin{align} \label{eq:pareclip}
    \mathbf{F}_\text{clip} = [\mathbf{F}_\text{clip}^\text{img}, \mathbf{F}_\text{clip}^\text{txt}] =  \text{ParseCLIP}(\mathbf{I}_\text{s},\mathbf{H}_\tau,\mathbf{H}_\text{s}),
\end{align}
%
where $\mathbf{F}_\text{clip}^\text{img}$ and $\mathbf{F}_\text{clip}^\text{txt}$ are CLIP embeddings of the person region and parsing categories, respectively. 
%
We will detail the `\text{ParseCLIP}' in \secref{subsec: parseclip} and present its functionality in the whole pipeline in
\figref{fig:pipeline}.

\textbf{Siamese network diffusion model.} We aim to learn a Siamese network to replace the original UNet for the backward denoising process. 
%
Specifically, the Siamese network contains two UNet branches with the same architectures, which are denoted as SourceNet (\ie, $\epsilon_\theta'(\cdot)$) and TargetNet (\ie, $\epsilon_\theta(\cdot)$). The SourceNet is to extract the multi-level features of the source image and can be formulated as
%
\begin{align} \label{eq:sourcenet}
    \mathcal{F}' = \{\mathbf{F}_l'\}_{l=1}^L = \epsilon_{\theta'}(\mathbf{z}_\text{s},\mathcal{C}', 
    \mathcal{O}_{\beta'}), \text{subject to}~\mathcal{C}'=\{\mathbf{x}_\text{s},\mathbf{h}_\text{s},\mathbf{F}_\text{clip}\}
\end{align}
%
% where $\mathbf{F}_l'$ is the self-attention output of the $l$th layer of $ \epsilon_{\theta'}$. 
where $\mathbf{F}_l'$ is the $l$th-layer attention features of $ \epsilon_{\theta'}$ at $t=0$. 
%
The feature $\mathbf{F}_\text{clip}$ is fused with $\mathcal{F}' = \{\mathbf{F}_l'\}_{l=1}^L$ of $\epsilon_{\theta'}$ via decoupled attention denoted as $\mathcal{O}'_\beta$. 
%
Inspired by \cite{ye2023ip}, we design a new decoupled attention to incorporate image and text simultaneously. 
%
We propose two independent cross-attentions to handle different modalities, and the computation can be formulated as follows:
%
\begin{align} \label{eq:decoupled_attention}
\mathcal{O}'_\beta (\mathbf{F}_\text{clip},\mathbf{F}_l') = g_\text{txt} \cdot \text{Att}(\textbf{Q}_{\mathbf{F}_l'},\textbf{K}_{\mathbf{F}_\text{clip}^\text{txt}},\textbf{V}_{\mathbf{F}_\text{clip}^\text{txt}}) + g_\text{img} \cdot \text{Att}(\textbf{Q}_{\mathbf{F}_l'}, \textbf{K}_{\mathbf{F}_\text{clip}^\text{img}},\textbf{V}_{\mathbf{F}_\text{clip}^\text{img}}),
    % &\text{Att}(\textbf{Q},\textbf{K},\textbf{V})=\text{Softmax}\left(\frac{\textbf{Q} \textbf{K}^{T}}{\sqrt{d}} \textbf{V}\right)
\end{align}
%
where $\text{Att}(\textbf{Q}_*,\textbf{K}_*,\textbf{V}_*)=\text{Softmax}\left(({\textbf{Q}_* \textbf{K}_*^{T}}/{\sqrt{d}})\textbf{V}_*\right)$, and $\{\mathbf{Q}_*,\mathbf{K}_*,\mathbf{V}_* | * \in \{ \mathbf{F}_l', \mathbf{F}_\text{clip}^\text{txt},  \mathbf{F}_\text{clip}^\text{img}\} \}$ are trainable matrices of features in the cross-attention layer. The two variables (\ie, $g_\text{txt}$ and $g_\text{img}$) are weights to combine the two attentions.


The TargetNet (\ie, $\epsilon_\theta$) is to predict noise at the $t$th timestep based on the pose and parsing embeddings, CLIP embeddings, and features extracted from the SourceNet.
%
\begin{align}\label{eq:targetnet}
    \hat{\epsilon} = \epsilon_\theta(\mathbf{z}_t, t, \mathcal{C},\mathcal{O}_\beta), 
    \text{subject to}~\mathcal{C}=\{\mathbf{x}_\tau,\mathbf{h}_\tau,\mathbf{F}_\text{clip},\mathcal{F}'\},
\end{align}
%
where $\mathcal{O}_\beta$ denotes the operations to fuse $\mathcal{F}'$ with the features of $\epsilon_{\theta}$, and the $\mathbf{F}_\text{clip}$ are incorporated in $\epsilon_{\theta}$ via original cross-attentions layer.
%
Specifically, regarding the $lth$ attention feature of $\epsilon_{\theta}$ denoted as $\mathbf{F}_l$, we propose the alignment attention based on \cite{xu2023magicanimate} to fuse $\mathbf{F}_l$ and $\mathbf{F}'_l$, which involves two sub-attentions, \ie, fusion attention and matching attention. The fusion attention facilitates the fusion while matching attention is responsible for extracting and enhancing the features from corresponding regions between $\mathbf{F}_l$ and $\mathbf{F}'_l$. We have the whole formulation as:
%
\begin{align} \label{eq:alignment_attention}
      \mathcal{O}_{\beta}(\mathbf{F}_l, \mathbf{F}'_l) = 
      \text{Att}(\mathbf{W}_q \mathbf{F}_l, \mathbf{W}_k\left[\mathbf{F}_l,\mathbf{F}_l'\right],
      \mathbf{W}_v\left[\mathbf{F}_l,\mathbf{F}_l'\right])+
      %
      \lambda\text{Att}(\mathbf{W}'_{q} \mathbf{F}_l, \mathbf{W}'_{k}\mathbf{F}'_l,\mathbf{W}'_{v} \mathbf{F}'_l), 
\end{align}
%
where $\{\mathbf{W}_*, \mathbf{W}'_* | *\in \{q,k,l\}\}$ are trainable weights, $[ \cdot ]$ concatenates features along the sequence dimension, and $\lambda$ is the control coefficient for corresponding enhancement. The two terms are fusion attention and matching attention, respectively.
For training and inference, we adopt the original SD training objective and sampling strategy, which will be further elaborated in \secref{subsec:implementation}.
% %
% The TargetNet (\ie, $\epsilon_\theta(\cdot)$) is to predict the noise at the $t$th timestep.
% %
% Specifically, we have
% %
% \begin{align} \label{eq:targetnet}
%       \hat{\epsilon} = \epsilon_{\theta} (\mathbf{z}_t,t,\mathcal{C}=\{\mathbf{x}_\text{s},\mathbf{h}_\text{s},\mathbf{F}_\text{clip},\mathcal{F}'\},\mathcal{O}_{\beta}),
% \end{align}
% %
% where 
% %
% where $\mathcal{O}_{\beta}$ contains the alignment attention and cross attention, defining the feature fusion operations.
% %
% We denote the $L$ features of $\epsilon_{\theta}$ as $\mathbf{F}$.
% %
% At the $l$th layer, $\mathbf{F}_\text{clip}$ and $\mathbf{F}_l'\in \mathcal{F}'$ are combined by the alignment attetion while $\mathbf{F}_l$ and $\mathbf{F}_l'$ are fused through the cross attention.


% Then, we can remove the noise at the $t$th timestep via the predicted noise. 
% %
% During the training process, we use the following loss function



\subsection{Pose-guided Parsing Projection}
\label{subsec: parsing projection}

As detailed in \secref{subsec: overview}, the parsing map of the target image is critical for different modules. 
%
However, in the practical, we do not have the target image and cannot use the person-parsing method to predict the parsing map directly.
%
Here, we propose to leverage the source parsing map and the poses of source and target images to estimate the target parsing map (\ie, \reqref{eq:parsing-projection}).
%

To achieve this goal, we follow a similar design to the Siamese network diffusion model. Specifically, we extract the embeddings of the source and target poses via an encoder and get $\mathbf{x}_\tau = \phi_\text{p}(\mathbf{P}_\tau)$ and $\mathbf{x}_\text{s} = \phi_\text{p}(\mathbf{P}_\text{s})$. Then, we calculate the latent of the source parsing map (\ie, $\mathbf{H}_\text{s}$) via VAE and get $\mathbf{z}_\text{hs}$.
%
Similar to \reqref{eq:sourcenet}, we obtain multi-level features of source parsing map through the SourceNet
%
\begin{align} \label{eq:sourcenet_ppp}
    \mathcal{F}' = \{\mathbf{F}'_l\}_{l=1}^L = \epsilon_{\theta'}(\mathbf{z}_\text{hs},\mathcal{C}'), \text{subject to}~\mathcal{C}'=\{\mathbf{x}_\text{s},\mathbf{x}_\tau\},
\end{align}
%
where $\mathbf{F}_l'$ is the $l$th-layer features of $ \epsilon_{\theta'}$ at $t=0$. Then, we use the TargetNet to predict the noise at the $t$th timestep during the backward diffusion process.
%
\begin{align} \label{eq:targetnet_ppp}
      \hat{\epsilon} = \epsilon_{\theta} (\mathbf{z}_t,\mathcal{C},
      \mathcal{O}_\beta), \text{subject to}~\mathcal{C}=\{\mathbf{x}_\text{s},\mathbf{x}_\tau, \mathcal{F}'\}
\end{align}
%
where $\mathbf{F}_l\in \mathcal{F}$ is fused with the $l$th layer of $\epsilon_{\theta}$ through the aligntment attention.
%
Then, we remove the noise at the $t$th timestep via the predicted noise. 
%
Note that, we use the same architectures for the pose encoder, SourceNet, and TargetNet as the ones in \secref{subsec: overview}. However, their weights are not shared.


\subsection{Person-Parsing-aligned CLIP Embedding Extraction}
\label{subsec: parseclip}

As introduced in \secref{subsec: overview}, we aim to extract CLIP embeddings of the target-aligned person region in the source image.
%
Given the source parsing map $\mathbf{H}_\text{s}$ and target parsing map $\mathbf{H}_\tau$, we first perform category matching. 
%
We have the category sets $\mathcal{V}_\text{s}$ and $\mathcal{V}_\tau$ of the source and target parsing maps, which involves the categories included in the two maps.
%
For example, we could have $\mathcal{V}_\text{s}=\{\text{`head',`face',`shirt',`pant',\ldots}\}$ and
%
each category in the set corresponds to a region in the parsing map.
%
Then, we remove the categories that are within $\mathcal{V}_\text{s}$ but not within $\mathcal{V}_\text{t}$ from $\mathcal{V}_\text{s}$ and get $\hat{\mathcal{V}}_\text{s}$.
%
As a result, the category-related regions in the parsing map $\mathbf{H}_\text{s}$ are set as zero and we obtain a new source parsing map denoted as $\hat{\mathbf{H}}_\text{s}$.
%
After that, we can get a tight bounding box tightly wrapped in the person region in $\hat{\mathbf{H}}_\text{s}$.
%
We use this bounding box to crop the source image and get $\hat{\mathbf{I}}_\text{s}$. 
%
We leverage the CLIP visual and text encoders to extract the embeddings of $\hat{\mathbf{I}}_\text{s}$ and $\hat{\mathcal{V}}_\text{s}$, respectively.

\subsection{Implementation Details}
\label{subsec:implementation}

\textbf{Optimization and sampling.}
%
During the training process, we also adopt classifier-free guidance \cite{ho2022classifier}, which is a strategy widely used in diffusion models to enhance the quality and control of the generated images. To achieve that we set conditions $\mathcal{C}$ to 0 with a random probability of $\eta$\%.
The final training loss function is rewritten as
%
\begin{align} 
    \label{eq:saimese_loss}
      \mathcal{L} = \mathds{E}_{\mathbf{z}_0, \mathcal{C}, \epsilon, t}(\|\epsilon-\epsilon_\theta(\mathbf{z}_t, t, \mathcal{C},\mathcal{O}_\beta)\|^2_2).
\end{align}
%
For ParseProj training, we estimate the parsing map and skeleton map of DeepFashion dataset using \cite{li2020self} and \cite{yang2023effective}, respectively. It is worth noting that during the training process of ParseProj, only the Alignment attention and PoseEncoder are trained.
For Saimese network, we employ the projected $\mathbf{H}_\tau$ and other estimated mapping results for training. All modules in Saimese network are involved in the training process except for VAE and CLIP remain fixed. 
%

In inference phase, given a randomly initialized Gaussian noise, we utilize the TargetNet (\ie, $\epsilon_\theta(\cdot)$) to predict the noise at the $t$th timestep and employ classifier scale $\omega$ to regulate the strength of guidance. The sampling formula is as follows:
\begin{align} 
    \label{eq:inference}
    \hat{\epsilon}_{t}=\epsilon_{\theta}(z_t,t,\varnothing,\varnothing) + \omega \cdot(\epsilon_{\theta} (\mathbf{z}_t,t,\mathcal{C},\mathcal{O}_{\beta})-\epsilon_{\theta}(z_t,t,\varnothing,\varnothing)).
\end{align}
Target latents can be obtained through multi-step denoising process, and mapping them back to the original image space we can acquire the target images. 


\textbf{Model details.}
Our Siamese network is derived by modifying the structure and weights of Stable Diffusion v2.1, based on the Hugging Face Diffusion library. Both PoseEncoder and ParseEncoder are a lightweight network with four convolutional layers. 
For training, we run on 4 NVIDIA A800 GPUs, with a batch size of 12 for $512\times 352$ and batch size of 60 for $256\times 176$. The training process consists of 100 epochs and uses the AdamW optimizer with a fixed learning rate of $1e^{-5}$. Classifer-free dropout probability $\eta$ is 30\%. For inference, We employ the DDIM\cite{ho2020denoising} scheduler with a sampling step of 35 and set classifier scale $\omega$ to 2.5. More details are explained in \secref{subsec:appendix_detail}.
%
