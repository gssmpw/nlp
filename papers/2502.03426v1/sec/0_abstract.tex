\begin{abstract}
%
Pose-Guided Person Image Synthesis (PGPIS) aims to generate a target image having the same pose as the given pose condition (\eg, skeleton) and the same person as the given source image.
%
Diffusion-based PGPIS methods can effectively preserve face IDs from source images following the specified pose, but they struggle to maintain high-quality clothing IDs.
%
Specifically, the generated clothing patterns and textures often differ significantly from those in the source images. 
%
We identified a critical challenge: SOTA methods use pre-trained models (\eg, CLIP) for image encoding, which well captures face features but overlooks clothing details.
%
% (2) There is a significant discrepancy between the person's pose in the source and target poses, making synthesis particularly challenging.
%
In this work, we propose a novel PGPIS framework denoted as the person-parsing-guided Siamese network diffusion model that can generate high-quality and ID-preserved faces and clothing.
%
The intuitive idea is to use an UNet having the same architecture in the diffusion model for image encoding and leverage person-parsing maps to guide the diffusion denoising process. 
%
To this end, our method involves three modules: \textit{First}, we design a Siamese network containing two UNet branches (\ie, SourceNet and TargetNet) with the same architectures. 
%
% akes the reference image, parsing-aligned pose embeddings, and parsing-aligned CLIP embeddings as the inputs and
SourceNet outputs multi-layer features considering the embeddings of the source image and parsing maps. These features are fed to each layer of TargetNet for noise prediction in the diffusion model. 
% The learnable SourceNet helps preserve both face ID and clothing ID. 
%
\textit{Second}, we propose the pose-guided parsing projection that can estimate the target image's person-parsing map by transferring the source image's parsing map based on the target and source poses. 
% The generated parsing map is employed to extract the parsing-aligned pose embeddings and CLIP embeddings used in SourceNet.
%
\textit{Third}, we propose the person-parsing-aligned CLIP embedding extraction. 
%
We use the parsing maps of the source and target images to crop the person in the source image and extract its CLIP-text \& -visual embeddings to highlight the face and clothing features for both SourceNet and TargetNet.
%
\end{abstract}