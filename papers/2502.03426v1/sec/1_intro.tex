\section{Introduction}
\label{sec:intro}

Images of well-dressed individuals are widely used in shopping stores for advertising. To achieve a high-quality image, a model must wear specific clothing and adopt a designated pose. Once these images are captured and deployed, it becomes difficult for stores to change the model's pose.
%
Otherwise, they must invite the model to wear the same clothes and undergo the photography process again, which is time-consuming and costly.
%
The well-defined task, \ie, pose-guided person image synthesis (PGPIS), could easily meet the above requirement efficiently. 
%
PGPIS aims to generate an image that matches the specified pose condition (\eg, skeleton) while retaining the same appearance (person and clothing) as the source image (See the inputs in \figref{fig:fig_intro}) \cite{ma2017pose,liu2019liquid,zhu2019progressive}.
%
Such a task is challenging due to the potential significant discrepancy between the source and target poses.

Previous works formulate the task as the deep generation problem and employ generative adversarial network (GAN) \cite{zhou2022cross,zhang2022exploring,zhang2021pise,sarkar2021style,tang2020xinggan,zhu2019progressive,siarohin2018deformable} and variational autoencoder (VAE) \cite{esser2018variational} to achieve the goal. 
%
However, GAN-based methods struggle with unstable training and generating high-quality images. VAE-based methods usually suffer from some artifacts like blur. 

Recently, diffusion models have demonstrated powerful capabilities in generating high-quality images \cite{ho2020denoising,song2020score} with flexible control conditions \cite{rombach2022high}.
%
Researchers have developed diffusion-based PGPIS methods, achieving impressive results \cite{bhunia2023person,lu2024coarse,shen2024advancing}.
%
\begin{figure}{r}{}
    \centering
    % \vspace{-10pt}
    \includegraphics[width=1.0\linewidth]{figures/fig_intro.pdf}
    \caption{Comparison with CFLD  \cite{lu2024coarse} and PCDM (ICLR'24) \cite{shen2024advancing}. They fail to preserve the clothing patterns and textures. The main reason is that the image encoder overlooks the clothing details (See the ``Feature Attention Map (Feat. Att. Map)").  In contrast, our method can generate high-quality images with preserved face and clothing IDs.}
    % \vspace{-10pt}
    \label{fig:fig_intro}
\end{figure}
%
In particular, the state-of-the-art (SOTA) methods, \eg, coarse-to-fine latent diffusion (CFLD) \cite{lu2024coarse} and progressive conditional diffusion (PCDM) \cite{shen2024advancing}, can generate realistic images with preserved human pose and face ID (See in \figref{fig:fig_intro}).
%
However, they struggle to transfer clothing patterns and textures (\ie, clothing ID) from the source image to the target image.
%
% For example, in the $1^\text{st}$ case in \figref{fig:fig1}, the t-shirt in the source image contains a sentence. The generated image by CFLD only captures the style of the t-shirt but loses all of the words, while PCDM can transfer some words but fails to recover the intermediate ones.
%
We display four cases in \figref{fig:fig_intro} and observe that: \ding{182} CFLD and PCDM can hardly preserve clothing patterns and textures. The two methods fail to transfer the texts ($1^\text{st}$ and $4^\text{th}$ cases), regular texture ($2^\text{nd}$ case), and irregular textures ($3^\text{rd}$ case).
%
% PCDM presents relatively better clothing ID-preserving capability than CFLD. 
%
\ding{183} As the discrepancy between the source and target poses increases, it becomes more challenging to preserve the clothing IDs. For example, with similar source images ($1^\text{st}$ case vs. $4^\text{th}$ case), PCDM can reproduce some words in the $1^\text{st}$ case but loses all words in the $4^\text{th}$ case.
%
\ding{184} We calculate the feature attention map of baseline methods for each case, which shows that both methods pay less attention to the clothing regions. We will conduct a statistical analysis in \secref{sec:limitation} for further discussion.


These observations inspire us to design a novel PGPIS framework that preserves both facial and clothing identities. Our approach leverages person-parsing maps of both source and target images to guide the encoding process, ensuring the features focus on both face and clothing regions.
%
We propose a person-parsing-guided Siamese network diffusion model with three main contributions: \textit{First}, we design a Siamese network comprising two UNet branches (\ie, SourceNet and TargetNet) with identical architectures. SourceNet generates multi-layer features by incorporating features from the source image and parsing maps. These features are subsequently fed into each layer of TargetNet for noise prediction in the diffusion model.
%
\textit{Second}, we introduce the pose-guided parsing projection, which estimates the person-parsing map of the target image by transferring the source imageâ€™s parsing map based on the poses of both the source and target images.
%
\textit{Third}, we propose person-parsing-aligned CLIP embedding extraction. This technique uses the parsing maps of the source and target images to crop the person in the source image, extracting CLIP-text and visual embeddings to emphasize facial and clothing features for both SourceNet and TargetNet.
%
We validate our method on the in-shop clothes retrieval benchmark with two resolutions ($512\times352$, $256\times176$) and compare with 11 SOTA baselines.
%
As shown in \figref{fig:fig_intro}, our method can generate high-quality images with well-preserved facial appearance and clothing patterns.
\subsection{Language}
%
