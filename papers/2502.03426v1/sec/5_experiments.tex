\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig_qualitative_comparison.pdf}
    \caption{Qualitative comparisons \protect\\with SPGNet \cite{lv2021learning}, CASD \cite{zhou2022cross}, DPTN \cite{zhang2022exploring}, PIDM \cite{bhunia2023person}, NTED \cite{ren2022neural}, PCDM \cite{shen2024advancing} and CFLD \cite{lu2024coarse}.}
    \label{qualitative_figure}
    % \vspace{-10pt}
\end{figure*}
%
\section{Experiments}
\subsection{Setup}
\textbf{Dataset.}
% 这一段主要介绍数据集的构建
We follow \cite{shen2024advancing,lu2024coarse} to conduct extensive experiments on In-Shop Clothes Retrieval benchmark from DeepFashion dataset and evaluated on 512$\times$352 and 256$\times$176 resolutions respectively. 
The dataset is comprised of 52,712 high-resolution images featuring clean backgrounds and diverse fashion models. 
To be consistent with the configuration in PATN~\cite{zhu2019progressive}, we split this dataset into non-overlapping training and testing set with 101,966 and 8,570 pairs, respectively, with each pair including a source image and its corresponding target image.

% It is noteworthy that the person identities in both the training and testing sets for both datasets do not overlap.
% \par
\textbf{Objective metrics.}
% 这一段介绍客观的指标，比如ssim psnr fid
We use four widely used metrics, namely \textit{Structure Similarity Index Measure} (SSIM) \cite{wang2004image}, \textit{Peak Signal to Noise Ratio} (PSNR), \textit{Learned Perceptual Image Patch Similarity} (LPIPS) \cite{zhang2018unreasonable}, and \textit{Fr$\acute{e}$chet Inception Distance} (FID) \cite{heusel2017gans}, to assess quality and fidelity of generated images. 
% The four metrics differ in the way they measure the similarity (distance) between two images or datasets.
% LPIPS uses a model trained on human judgments to measure perceptual similarity, while SSIM considers aspects such as brightness, contrast, and structure. 
% PSNR quantifies the degree of distortion. FID calculates the Wasserstein-2 distance~\cite{vaserstein1969markov} on the distribution of the two data sets.
% \par

\textbf{Subjective metrics.}
% 主观指标，调研问卷等
% Considering that objective metrics cannot fully capture human perception of how similar two images are, which is usually subjective,
We employ three subjective metrics as referenced in the literature \cite{bhunia2023person}: R2G \cite{ma2017pose}, G2R \cite{ma2017pose}, and J2b \cite{siarohin2018deformable,bhunia2023person}.
%
R2G and G2R represent the ratios of real images classified as generated images and generated images classified as real images, respectively. Higher values for R2G and G2R indicate a greater similarity to real images. Meanwhile, J2b reflects user preferences by comparing the proportion of users selecting the best image from a group of images generated by different methods.



\subsection{Quantitative and qualitative comparison}
\label{subsec:quantitative and qualitative comparison}
%
\begin{table*}[t!]
    \centering
    % \renewcommand\arraystretch{1}
    % \setlength{\tabcolsep}{20pt}
    \caption{Quantitative comparisons with the state of the arts in terms of image quality. $^\dagger$We reproduce these results based on the checkpoints instead of generated images provided by the authors. $^\ast$Results are cited from PoCoLD without publicly available checkpoints or generated images. Others are cited from NTED.}
    \label{tab:main_experiment}
    \resizebox{1\linewidth}{!}{
    \input{tab/tab_quantitative}
    }
\end{table*}
%
% \todo{First briefly summarize what this subsection serves for.}
We conducted a comprehensive comparison with SOTA approaches including GAN-based, flow-based, attention-based and diffusion-based ones, they are PATN~\cite{zhu2019progressive}, ADGAN~\cite{men2020controllable}, GFLA~\cite{ren2020deep}, 
SPGNet\cite{lv2021learning},
PISE~\cite{zhang2021pise}, CASD~\cite{zhou2022cross}, CocosNet2~\cite{zhou2021cocosnet}, NTED~\cite{ren2022neural}, DPTN~\cite{zhang2022exploring}, PIDM~\cite{bhunia2023person}, PoCoLD~\cite{han2023controllable}, PCDM~\cite{shen2024advancing}, and CFLD~\cite{lu2024coarse}. 
Additionally, we perform a qualitative evaluation of the latest seven SOTA methods.
% among which PIDM, CFLD, and PCDM are diffusion-based approaches.

\textbf{Quantitative comparison.}
% 主要是表格的分析
% 我们与11种SOTA方法在四个指标上进行了quantitative comparison，结果如表1所示。在第一组分辨率对比中，我们在3/4的指标中超过其他方法，其中远超flow-based和gan-based的方法。值得注意的是我们在由human judgements上训练得到的指标LPIPS中远超其他方法，表示我们的方法在图像的重建上更符合人类审美。尽管我们在FID指标上略高于PIDM，但这可能是PIDM过度拟合了数据集的分布所导致。此外先前的工作也表明FID并不能完全代表图像生成的好坏。我们在第二组实验中相比于其他方法取得了更大的优势，远超其他三类LDM-based的方法。值得注意的是，由于PoCoLD没有提供任何公开的结果导致无法对其进行相同环境下的定量评价，故不纳入最终比较
We perform a quantitative comparison with 11 SOTA methods across four metrics (See \tableref{tab:main_experiment}). In 256$\times$176 resolution comparison, our method outperforms others in 3 out of 4 metrics, particularly showing a substantial lead over flow-based and GAN-based methods. Notably, in the metric LPIPS, which is trained using human judgments, our method significantly outperforms other approaches. This indicates that our method better aligns with human aesthetics in terms of image reconstruction. While our FID metric slightly exceeds that of PIDM, this can potentially be attributed to PIDM overfitting the dataset distribution as also discussed in prior research\cite{shen2024advancing,lu2024coarse,han2023controllable}, and they has also suggested that FID does not fully reflect the quality of image generation.
%
\begin{figure}{}{}
% \vspace{-12pt}
    \includegraphics[width=\linewidth]{figures/fig_user_study.pdf}
    % \vspace{-23pt}
    \caption{{\color{black}User study results in terms of R2G,G2R and Jab metrics. Higher values in metrics means better quality of generated results.}
    \label{user study}
    }
% \vspace{-12pt}
    \label{fig:problem}
\end{figure}
%
In the second set of experiments, our method demonstrates a significant advantage over other approaches, surpassing all three categories of LDM-based methods. It is worth noting that PoCoLD\cite{han2023controllable} does not provide any publicly available results, which hinders us from conducting a quantitative evaluation under the same conditions.
Therefore, it is excluded from the final comparison. 
% Additionally, we also provide the metrics results of VAE reconstructed and ground truth as references for comparison.

\textbf{Qualitative comparison.}
% 主要是图片主观层面的分析
In \figref{qualitative_figure}, we present the results where our framework is compared to the various types of SOTA methods in DeepFashion dataset and the four objective metrics. 
% From left to right, it consists of source images, target poses, generated images of different approaches in comparison, and target images ('Ground Truth').
%
We observer that: 
\ding{182} When confronted with extreme pose variations or extensive occlusions as demonstrated in the images in \textit{rows 1-3}, only diffusion-based methods (\textit{columns 7-10}) are capable of reconstructing the approximate pattern outlines of the clothing and faces, and our method significantly outperforms others in terms of face and cloth ID preservation and consistency. This superiority stems from our ability to explore deeper spatial and semantic matching information, followed by enhanced feature fusion achieved through Siamese networks. 
\ding{183} Addressing the challenge of reconstructing intricate patterns such as designs and text, which previous methods struggle with (as observed in all rows), our approach leverages the Siamese diffusion framework to better retain fine-grained details under significant scale variations. Compared to others, our method preserves finer details, resulting in less disparity from real images.
\ding{184} The last two rows indicate that irrespective of minor or giant pose differences, our method maximizes the retention of structural clothing features from the source image. As evidenced in the fifth line, only our approach accurately preserves features such as suspender designs.
More examples and ParseProj qualitative results are listed in \secref{subsec:appendix_qualitative_compare}.

\textbf{User study.}
% 以上定量和定性的结果比较展示了我们相较于其他方法的显著优势，然而定量的结果仍然不能完全反映图像细节上的差异（比如花纹，文字等）。为了更好的反映我们方法在细节一致性保持的优势，我们完全参照PIDM的实验设置对30名有着计算机背景的志愿者开展了一项用户研究。(1)针对R2G和G2R两项指标，我们从每种生成方法和真实数据集中各随机挑选30张图片组成一组测试数据，志愿者需要判别每张图片是否为生成图片。可以看出我们在G2R上显著优于其他方法，有一半以上的图片被认为是真实的，这反映了我们生成的图片难以被辨别，更趋近真实图片。此外相比之下R2G指标在各类方法中没有显著差异。(2)针对jab指标，为了更好的凸显我们方法的优越性，我们分别进行了两组实验，一组为挑选后的30张具有复杂花纹和文字的图片，另一组为随机挑选的30张图片来进行公平的对比。可以发现两组数据集上我们均显著优于其他方法(70.2和53.6)，表明我们的方法在复杂图像一致性保持上远超其他方法。
% 用户主观分析,这里需要放一个类似这种表的对比图，包含了三个指标
% The quantitative and qualitative comparisons presented above demonstrate the significant advantages of our approach over others.
% However, quantitative results still fail to fully exhibit the differences in image details (such as textures, text, etc.). 
%
To better illustrate the advantage of our method, we further conduct a user study involving 30 volunteers with a background in computer science, following the experimental setup of PIDM \cite{bhunia2023person} completely.
%
As shown in \figref{user study}, we can make the following observations:
\ding{182} For the R2G and G2R metrics, we randomly select 30 images from each generation method and the real dataset to form a test dataset. Volunteers were required to discern whether each image was generated or real. Our method significantly outperforms other methods in terms of the G2R metric, with over half of the images being perceived as real. Furthermore, there were no significant differences in the R2G metric among the various methods.
\ding{183}  We also conduct two Jab experiments. In one set, we select 30 images with complex patterns and text generated by each method. In the other set, we randomly select 30 images generated by each method for fair comparison. Our method significantly outperformed others on both sets of datasets, achieving scores of 70.2 and 53.6 respectively. These results clearly indicate that our approach surpasses other methods by a wide margin in terms of preserving consistency in both complex and normal images, while also exhibiting a stronger alignment with human aesthetics. More detailed information are explained in \secref{subsec:user_study}.

\textbf{Ablation study.}
%
\begin{table}
    \centering
    % \renewcommand\arraystretch{1}
    % \setlength{\tabcolsep}{20pt}
    \caption{Ablation study on different contributions.}
    \label{tab:ablation}
    \resizebox{1\linewidth}{!}{
    \input{tab/tab_ablation}
    }
\end{table}
%
To demonstrate the effectiveness of our contributions, we conduct ablation experiments on DeepFashion as shown in \tableref{tab:ablation_3}. 
% 
\textbf{B1} represents our adoption of pre-trained encoders (\ie CLIP) to extract information from the source images, and then we introduce the extracted information into TargetNet via vanilla cross-attentions. 
%
\textbf{B2} is a variant without Siamese network. We design a stacked multi-layer perceptron (MLP) to map the extracted features from CLIP to the shape of the $lth$ layer in TargetNet, enabling feature incorporation within the same fusion setup.
%
In both B1 and B2, regardless of the fusion approach, it has been demonstrated that CLIP encoders cannot fully extract detailed information, resulting in poor performance across all three metrics. 
%
To better capture fine-grained information, we introduced the Siamese network into \textbf{B4} and \textbf{B3}. All three metrics showed significant improvements, particularly LPIPS and SSIM, which are indicators reflecting image authenticity and human preferences. 
% It is worth noting that there is still a certain gap compared to the SOTA methods. 
We further proposed ParseProj and ParseCLIP to provide semantic-level matching information and region enhancement. Here, B4 represents the absence of ParseProj, and B3 represents the absence of ParseCLIP. The results demonstrate the effectiveness of both modules. Qualitative results of the ablation study can be confirmed in \secref{subsec:appendix_ablation}.
