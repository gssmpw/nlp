% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{makecell}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
% \usepackage{wrapfig}
% \usepackage{amssymb}
% \usepackage{multirow}
% \usepackage{cite}

% Import additional packages in the preamble file, before hyperref
\input{preamble}
\input{gqwritepackage}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).

\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{9807} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\textsc{TruePose:} Human-Parsing-guided Attention Diffusion for \\ Full-ID Preserving Pose Transfer}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{
\textbf{Zhihong Xu}\textsuperscript{1}
\ 
\textbf{Dongxia Wang}\textsuperscript{1,*}
\ 
\textbf{Peng Du}\textsuperscript{2}
\ 
\textbf{Yang Cao}\textsuperscript{2}
\ 
\textbf{Qing Guo}\textsuperscript{3,4,*}
\\
\textsuperscript{1} Zhejiang University
\quad
\textsuperscript{2} Alibaba Group
\\
\quad
\textsuperscript{3} 
Institute of High Performance Computing (IHPC), A*STAR, Singapore
\\
\quad
\textsuperscript{4} 
Centre for Frontier AI Research (CFAR), A*STAR, Singapore
}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both

\begin{document}
\maketitle 

\if TT\insert\footins{\noindent\footnotesize{
*Corresponding authors}}\fi
% \twocolumn[{
% \renewcommand\twocolumn[1][]{#1}
% \maketitle
% \begin{center}
%     \captionsetup{type=figure}
%     \includegraphics[width=1.0\linewidth]{figures/fig_intro.pdf}
%     \caption{Pose-guided person image synthesis (PGPIS) task and comparison among CFLD  \cite{lu2024coarse}, PCDM (ICLR'24) \cite{shen2024advancing}, and our methods. The two SOTA methods fail to preserve the clothing patterns and textures. The main reason is that the image encoder overlooks the clothing details (See the ``Feature Attention Map (Feat. Att. Map)").  In contrast, our method can generate high-quality images with preserved face and clothing patterns.}
% \label{fig:fig_intro}
% \end{center}
% }]

\begin{abstract}
%
Pose-Guided Person Image Synthesis (PGPIS) generates images that maintain a subject's identity from a source image while adopting a specified target pose (e.g., skeleton). While diffusion-based PGPIS methods effectively preserve facial features during pose transformation, they often struggle to accurately maintain clothing details from the source image throughout the diffusion process. 
%
This limitation becomes particularly problematic when there is a substantial difference between the source and target poses, significantly impacting PGPIS applications in the fashion industry where clothing style preservation is crucial for copyright protection.
Our analysis reveals that this limitation primarily stems from the conditional diffusion model's attention modules failing to adequately capture and preserve clothing patterns.
%
To address this limitation, we propose human-parsing-guided attention diffusion, a novel approach that effectively preserves both facial and clothing appearance while generating high-quality results.
%
We propose a human-parsing-aware Siamese network that consists of three key components: dual identical UNets (TargetNet for diffusion denoising and SourceNet for source image embedding extraction), a human-parsing-guided fusion attention (HPFA), and a CLIP-guided attention alignment (CAA). The HPFA and CAA modules can embed the face and clothes patterns into the target image generation adaptively and effectively.
%
Extensive experiments on both the in-shop clothes retrieval benchmark and the latest in-the-wild human editing dataset demonstrate our method's significant advantages over 13 baseline approaches for preserving both facial and clothes appearance in the source image.
%
\end{abstract}


\section{Introduction}
\label{sec:intro}

%
\begin{figure*}[t]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=1.0\linewidth]{figures/fig_intro.pdf}
    \caption{Pose-guided person image synthesis (PGPIS) task and comparison among CFLD  \cite{lu2024coarse}, PCDM (ICLR'24) \cite{shen2024advancing}, and our methods. The two SOTA methods fail to preserve the clothing patterns and textures. The main reason is that the image encoder overlooks the clothing details (See the ``Feature Attention Map (Feat. Att. Map)").  In contrast, our method can generate high-quality images with preserved face and clothing patterns. 
    % Top right: comparing average attention scores of different regions within source images. Bottom right: visualization results of attention maps of our method and two baseline methods.
    }
    % \vspace{-10pt}
    \label{fig:fig_intro}
\end{figure*}
%

Images of well-dressed individuals are widely used in shopping stores for advertising. To achieve a high-quality image, a model must wear specific clothing and adopt a designated pose. Once these images are captured and deployed, it becomes difficult for stores to change the model's pose.
%
Otherwise, they must invite the model to wear the same clothes and undergo the photography process again, which is time-consuming and costly.
%
The well-defined task, \ie, pose-guided person image synthesis (PGPIS), could easily meet the above requirement efficiently. 
%
PGPIS aims to generate an image that matches the specified pose condition (\eg, skeleton) while retaining the same appearance (person and clothing) as the source image (See the inputs in \figref{fig:fig_intro}) \cite{ma2017pose,liu2019liquid,zhu2019progressive}.
%
Such a task is challenging due to the potential significant discrepancy between the source and target poses.

Previous works formulate the task as the deep generation problem and employ generative adversarial network (GAN) \cite{zhou2022cross,zhang2022exploring,zhang2021pise,sarkar2021style,tang2020xinggan,zhu2019progressive,siarohin2018deformable} and variational autoencoder (VAE) \cite{esser2018variational} to achieve the goal. 
%
However, GAN-based methods struggle with unstable training and generating high-quality images. VAE-based methods usually suffer from some artifacts like blur. 

Recently, diffusion models have demonstrated powerful capabilities in generating high-quality images \cite{ho2020denoising,song2020score} with flexible control conditions \cite{rombach2022high}.
%
Researchers have developed diffusion-based PGPIS methods, achieving impressive results \cite{bhunia2023person,lu2024coarse,shen2024advancing}.
%
In particular, the state-of-the-art (SOTA) methods, \eg, coarse-to-fine latent diffusion (CFLD) \cite{lu2024coarse} and progressive conditional diffusion (PCDM) \cite{shen2024advancing}, can generate realistic images with preserved human pose and face ID (See in \figref{fig:fig_intro}).
%
However, they struggle to transfer clothing patterns and textures (\ie, clothing ID) from the source image to the target image.
%
% For example, in the $1^\text{st}$ case in \figref{fig:fig1}, the t-shirt in the source image contains a sentence. The generated image by CFLD only captures the style of the t-shirt but loses all of the words, while PCDM can transfer some words but fails to recover the intermediate ones.
%
We display four cases in \figref{fig:fig_intro} and observe that: \ding{182} CFLD and PCDM can hardly preserve clothing patterns and textures. The two methods fail to transfer the texts ($1^\text{st}$ and $4^\text{th}$ cases), regular texture ($2^\text{nd}$ case), and irregular textures ($3^\text{rd}$ case).
%
% PCDM presents relatively better clothing ID-preserving capability than CFLD. 
%
\ding{183} As the discrepancy between the source and target poses increases, it becomes more challenging to preserve the clothing IDs. For example, with similar source images ($1^\text{st}$ case vs. $4^\text{th}$ case), PCDM can reproduce some words in the $1^\text{st}$ case but loses all words in the $4^\text{th}$ case.
%
\ding{184} We calculate the feature attention map of baseline methods for each case, which shows that both methods pay less attention to the clothing regions. We will conduct a statistical analysis in \secref{sec:limitation} for further discussion.


These observations inspire us to design a novel PGPIS framework that preserves both facial and clothing patterns. Our approach leverages the person-parsing map of the source image to guide the encoding process, ensuring the features focus on both face and clothing regions.
%
We propose a human-parsing-aware Siamese network with three main modules: \textit{First}, we design a dual idenitical UNets (\ie, SourceNet and TargetNet). TargetNet is used for diffusion denoising and SourceNet is designed for extracting source image embedding.
%
\textit{Second}, we introduce the human-parsing-guided fusion attention, which corrects the attention of TargetNet's embedding on the facial and clothes regions according to the guidance of the source parsing map and source image embeddings.
%
\textit{Third}, we propose CLIP-guided attention alignment to further refine the corrected TargetNet embedding based on the consistent constraints across different semantic regions.
%
The proposed modules could be inserted into different layers of the TargetNet.
%
We validate our method on the in-shop clothes retrieval benchmark with two resolutions ($512\times352$, $256\times176$) and compare with 13 SOTA baselines.
%
As shown in \figref{fig:fig_intro}, our method can generate high-quality images with well-preserved facial appearance and clothing patterns.
%

\section{Related Work}
\label{sec:related_work}

\textbf{Pose-guided person image synthesis.}
% 这一部分介绍这个任务的方法发展
% Pose-guided person image synthesis任务聚焦于如何在复杂空间中实现pose的转换并尽可能的保留source image的细节特征一致性。这个任务首次由Ma等人提出，早期的工作聚焦于利用conditional 生成对抗网络将source image作为条件信息来引导target图像的生成，如VUNet利用VAE对source image apperance提取并作为条件引入到UNet来生成图片，Def-GAN通过a set of loacl affine transformations和损失约束来对齐不同pose等。这类GAN-based 方法因为模型自身缺乏学习pose空间变换关系的能力导致效果不佳。之后的一些工作有采用flow-based deformation来帮助解构source image的信息以实现pose的对齐，比如GFLA通过计算图片在flow fields的相关性来提取局部注意力特征实现不同区域的对应；也有引入更为详细稠密的补充信息（如UV[Style and Pose Control for Image Synthesis of Humans from a Single Monocular View]，densepose[Controlllable person image synthesis with attribute-decomposed gan]和parsing map来帮助弥补pose之间的inconsistency）。
% 最近的工作采用diffusion作为生成底模，相比于先前gan或其他生成方法在生成高分辨率图像时细节保留差和不稳定的问题，diffusion通过多步forward-backward denoising 过程不仅训练更为稳定，同时也能保留图片的细节特征生成高分辨的图像。PIDM通过设计一个条件texture diffusion模型和disentangle classifier-free guidance机制来实现pose引导下的图像生成，PoCoLD和PCDM分别通过pose constrained attention和progressively 多stage refine来实现pose的对齐。CFLD通过挖掘source image的corse prompt information 然后不断细化来实现细粒度的生成。以上的diffusion-based方法均依赖于预训练的encoder来实现对source image的apeerance信息进行提取，我们认为这类模型因其本身的缺陷（比如数据集的长尾效应）使其关注于常见区域如人脸/手等区域而无法捕捉细粒度的纹理花纹等细节特征，所以导致先前的diffusion-based方法无法生成具有一致性的复杂图案。
PGPIS achieves pose transformation in a complex spatial context while maintaining consistency with the detailed features of the source image, which was proposed by Ma \etal  \cite{ma2017pose}. 
Early approaches \cite{esser2018variational,ma2017pose} employ conditional generative adversarial networks (CGANs)  \cite{goodfellow2014generative, mirza2014conditional} to guide the generation of target images using the source image as conditional information \cite{ma2017pose,men2020controllable}. However, due to the significant spatial information differences between the source image and the target image, as well as the sparse guidance from the skeleton map, directly achieving pose transfer is highly challenging. To address the challenges, some works decouple and separately optimize pose and appearance information\cite{ma2018disentangled}, or use attention mechanisms to better establish the mapping relationship between pose and appearance\cite{ren2022neural,zhang2022exploring,zhou2021cocosnet}. 
On the other hands, some approaches introduce more detailed and dense supplementary information, such as UV \cite{sarkar2021style}, dense-pose \cite{han2023controllable}, and parsing maps \cite{men2020controllable,zhang2021pise,zhou2022cross,lv2021learning}, to alleviate inconsistencies between poses. 
Recent approaches adopt diffusion as the generative model\cite{bhunia2023person,shen2024advancing,lu2024coarse,han2023controllable}. 
%
By incorporating conditional information into iterative forward-backward denoising procedures \cite{ho2020denoising}, diffusion-based methods outperform GAN-based approaches in terms of both image generation quality and pose control effectiveness.
PIDM is the pioneering endeavor to integrate the diffusion model into PGPIS, which designs a conditional texture diffusion model and a disentangle classifier-free guidance mechanism \cite{ho2022classifier}. 
% 加入latent diffusion描述，ldm方法依赖于预训练
PoCoLD and PCDM aim to achieve pose alignment through a pose-constrained attention mechanism and progressive multi-stage refinement, respectively \cite{han2023controllable,shen2024advancing}.
CFLD \cite{lu2024coarse} endeavors to attain coarse-to-fine generation by extracting coarse prompt information from the source image, followed by refinement through a learnable Perception-Refined Decoder and Hybrid-Granularity Attention to achieve fine-grained results. 
%

\textbf{Conditional diffusion models.}
% 这一部分介绍条件生成模型的在图片生成发展
The significant potential of diffusion models in traditional unconditional generative tasks has recently been evidenced\cite{ho2020denoising,song2020denoising,song2020score}. 
In contrast to conventional single-step generative models such as Generative Adversarial Networks (GANs)\cite{mirza2014conditional} and Variational Autoencoders (VAEs)\cite{kingma2013auto}, diffusion relies on multi-step denoising sampling of initial noise to generate high-quality data, thereby enabling the production of detailed and diverse outputs. The advancement of various conditional diffusion models further enhances its practicality and versatility. Classifier-free methodologies devise implicit classifiers to guide the weights of controllable and uncontrollable processes, thus facilitating conditional generation\cite{ho2022classifier}. Furthermore, Latent Diffusion Model(LDM)\cite{rombach2022high} utilize denoising process by encoding original images into a low-dimensional latent space and integrate cross-attention mechanisms to introduce multi-modal conditions, thereby significantly expanding the diversity of control information. 
Prior LDM-based approaches\cite{han2023controllable,shen2024advancing,lu2024coarse} rely on pre-trained encoders to extract appearance information from the source image. 
We argue that such encoders are ineffective in capturing fine-grained details, which hinders the generation of complex images. In contrast, our proposed framework utilizes guided attention to capture complex detail features, improving the preservation of full identities and ensuring person consistency.
% In contrast, our proposes Siamese conditional diffusion framework excels in capturing complex details and establishing semantic matching, thereby improving the preservation of Full-IDs and ensuring person consistency. 
Our extensive experimental analysis confirms the limitations of traditional approaches. (\secref{sec:limitation}).
% Recent studies\cite{} have incorporated diffusion models into fashion generation tasks. 
% PCDM introduces a progressive three-stage network consisting of prior-inpainting-refining stages. 
% This model integrates pose alignment relations and utilizes the CLIP model to extract image features, gradually refining image details throughout the three stages. 
% CFLD proposed a coarse-to-fine training paradigm, fully exploiting multiscale information by mining and refining coarse-grained prompts obtained from pre-trained feature extractors (e.g., SwinB) and combining them with a mixed-granularity mechanism. In contrast to previous diffusion-based methods that rely on pre-trained models for extracting semantic or pixel-level appearance details from the source image, this paper introduces a siamese diffusion framework built upon LDM. This framework make full use of the feature extraction and matching abilities of pre-trained diffusion models to blend fine-grained features and facilitate pose transfer within the latent space, achieving higher consistency in pose-guided person image synthesis.

\section{Preliminaries and Limitations}
\label{sec:preliminary}

% In this section, we briefly introduce the state-of-the-art (SOTA) Diffusion-based PGPIS methods \cite{shen2024advancing,lu2024coarse}. Then, we discuss the limitations of these methods that could be well-addressed by our method introduced in \secref{sec:method}. 

\subsection{Diffusion-based PGPIS }
\label{subsec:diff_pgpis}

Given a source image $\mathbf{I}_s$ containing a person with pose $\mathbf{p}_s$ (represented as a skeleton map) and a target pose $\mathbf{p}_\tau$, PGPIS aims to generate a target image $\mathbf{I}_\tau$ that preserves the person's appearance (should contain both facial and clothing patterns) while adopting the target pose.
%
The SOTA diffusion-based PGPIS methods are designed based on the stable diffusion (SD) \cite{rombach2022high} that involves a forward diffusion process and a backward denoising process of $T$ steps.
%
% during the training process, we first use a variational autoencoder (VAE) to extract the latent of ground truth target image $\mathbf{I}_\tau$ (\ie, $\mathbf{z}_\tau$). 
%
The forward diffusion process progressively add random Gaussian noise $\epsilon\in \mathcal{N}(0,\mathbf{I})$ to the initial latent $\mathbf{z}_0$. At the $t$th timestep, we can formulate it as
%
\begin{align} \label{eq:forward_diffusion}
    \mathbf{z}_t = \sqrt{\bar{\alpha}_t}\mathbf{z}_0+ \sqrt{1-\bar{\alpha}_t}\epsilon, t\in[1,T]
\end{align}
%
where $\bar{\alpha}_1,\bar{\alpha}_2,\ldots,\bar{\alpha}_T$ are calculated from a fixed variance schedule. The denoising process uses an UNet to predict the noise, \ie, $\epsilon_\theta(\mathbf{z}_t, t, \mathcal{C},\mathcal{O}_\beta)$ and remove the noise from the latent.
%
The set $\mathcal{C}$ involves related conditions and the function $\mathcal{O}_\beta(\cdot)$ defines the way of embedding condition features into the UNet. 
%
For PGPIS task, we can set  $\mathcal{C} =\{\mathcal{X}_\text{s},\mathcal{X}_\text{tp}, \mathcal{X}_\text{sp}\}$ where $\mathcal{X}_\text{s}$ denotes the features of source image and $\mathcal{X}_\text{tp}$ and $\mathcal{X}_\text{sp}$ contains the features of target pose and source pose.
%
To train the UNet $\epsilon_\theta(\cdot)$, the predicted noise should be the same as the sampled noise during the forward process
%
\begin{align} \label{eq:loss_diffusion}
    \mathcal{L}_\text{mse} = \mathds{E}_{\mathbf{z}_0, \mathcal{C}, \epsilon, t}(\|\epsilon-\epsilon_\theta(\mathbf{z}_t, t, \mathcal{C},\mathcal{O}_\beta)\|^2_2).
\end{align}
%
During the inference with trained $\epsilon_\theta$ and $\mathcal{O}_\beta$, we use image encoder and pose encoder to extract features of the inputs (\ie, $\mathbf{I}_\text{s}$, $\mathbf{p}_\text{s}$, $\mathbf{p}_\text{t}$) and get $\mathcal{C} =\{\mathcal{X}_\text{s},\mathcal{X}_\text{tp},\mathcal{X}_\text{sp}\}$.
%
Then, given the latent $\mathbf{z}_T$, we perform the backward denoising process iteratively to generate the predicted target image. 

\textbf{Coarse-to-fine latent Diffusion (CFLD) \cite{lu2024coarse}.} 
%
The SOTA method CFLD utilizes the pre-trained Swin-B \cite{liu2021swin} to extract multi-layer features (\ie, $\mathcal{X}_\text{s}$) of the source image and the Adapter to extract multi-layer features \cite{mou2023t2i} (\ie, $\mathcal{X}_\text{tp}$) of the target pose. Moreover, CFLD proposes two new modules as the $\mathcal{O}_\beta$ to embed the condition features effectively.

\textbf{Progressive conditional Diffusion models (PCDM) \cite{shen2024advancing}.} Shen \etal proposed three-stage diffusion models to progressively generate high-quality synthesized images. The first stage is to predict the global embedding of the target image; the second stage is to get a coarse target estimation; the final stage is to refine the coarse result. Each stage relies on a diffusion model that could be approximately formulated with \reqref{eq:forward_diffusion} and \reqref{eq:loss_diffusion} with different condition setups. In particular, all three stages use the pre-trained image encoders, \ie, CLIP and DINOv2.

\subsection{Empirical Study}
\label{sec:limitation}

%
\begin{SCfigure}{}{}
% \vspace{-10pt}
    \includegraphics[width=0.75\linewidth]{figures/fig_analysis.pdf}
    % \vspace{-23pt}
    \caption{\textbf{Top:} comparing average attention scores of different regions within source images. \textbf{Bottom:} visualization results of attention maps of our method and two baseline methods.
    }
    \vspace{-15pt}
\label{fig:limitation}
\end{SCfigure}

As shown in \figref{fig:fig_intro}, the state-of-the-art methods CFLD \cite{lu2024coarse} and PCDM \cite{shen2024advancing} can hardly preserve the clothing patterns and textures.
%
The feature attention maps in \figref{fig:fig_intro} demonstrate a superficial reason, that is, the SOTA methods' features did not focus on the clothing. 
%
To further validate this observation, we perform a statistical analysis.
%
Specifically, we randomly collect 50 examples from the in-shop clothes retrieval benchmark \cite{liuLQWTcvpr16DeepFashion} and use the two SOTA methods to handle these examples.
%
For each sample, we calculate the feature attention map and get its parsing map.
%
Then, we can count the attention values at different parsing regions.
%
After that, we can obtain the average attention values within different regions among all examples.
%
For CFLD, we obtain the feature attention map by calculating the final layer's average value of the softmax layer in the Swin-B. 
%
For PCDM, we follow the steps outlined in the open-source code of PCDM to calculate the attention map.
%
As the results are shown in \figref{fig:limitation}, PCDM using pre-trained CLIP and DINOv2 feature pay less attention to the clothes when we compare the attention values on clothes with the ones on the other two regions.
%
CFLD uses pre-trained Swin-B features, paying less attention to both cloth and head regions.
%
The observation inspires us to design a novel framework, which should encode the source image with both face and cloth key information preserved.
%
Our intuition is to leverage person-parsing maps to guide the image encoding process and inject the parsing-guided embeddings into diffusion generation.


\section{Human-Parsing-aware Siamese Network}
\label{sec:method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/fig_pipeline3.pdf}
    \caption{Pipeline of the proposed human-parsing-guided attention diffusion model.}
    \label{fig:pipeline}
\end{figure*}

% In this section, we first introduce the whole framework in \secref{subsec: overview} with the main architecture (\ie, Siamese network diffusion model). Then, we detail the pose-guided parsing projection in \secref{subsec: parsing projection} to estimate the person parsing map of the target image based on the source and target poses and the source image, and the module to extract person-parsing-aligned CLIP embeddings in \secref{subsec: parseclip}. We provide all implementation details in \secref{subsec:implementation}. Please refer to the whole pipeline in \figref{fig:pipeline}.

% \subsection{Human-Parsing-aware Siamese Network}
\subsection{Overview}
\label{subsec: overview}

%
Following the task definition of PGPIS, we have similar inputs including source image $\mathbf{I}_\text{s}$ and target pose $\mathbf{p}_\tau$.
%
We begin by acquiring person-parsing maps. For the source image $\mathbf{I}_\text{s}$, we utilize the person-parsing approach of \cite{li2020self} to generate parsing map $\mathbf{H}_\text{s}$, which semantically segments the human body into categories such as arm, leg, dress, and skirt.

With the parsing map and inputs, we also use the diffusion model to generate the target image and the key problem is how to set the condition $\mathcal{C}$ and the fusion function $\mathcal{O}_\beta$ in \reqref{eq:loss_diffusion}. Different from previous works using pre-trained encoders to extract embeddings of source, pose, and parsing images, we design a human-parsing-aware Siamese network. 

\textit{First}, we set an encoder denoted as $\phi(\cdot)$ to extract the latent of source images and the noisy target image during the diffusion process, which can be formulated as $\mathbf{z}_\text{s} = \phi(\mathbf{I}_\text{s})$.

\textit{Second}, we build a Siamese network containing two UNets denoted as TargetNet (\ie, $\epsilon_\theta$) and SourceNet (\ie, $\epsilon_{\theta'}$).
%
TargetNet is the UNet in the stable diffusion, taking the noisy latent as input $\mathbf{z}_t$, predicting the noise amount at $t$th timestamp for inverse denoising. 
%
SourceNet has the same architecture as the TargetNet and takes $\mathbf{z}_\text{s}=\phi(\mathbf{I}_\text{s})$ as the input and output $L$ embeddings from $L$ layers of $\epsilon_{\theta'}$,
%
\begin{align} \label{eq:sourcenet}
    \mathcal{F}' = \{\mathbf{F}_l'\}_{l=1}^L = \epsilon_{\theta'}(\mathbf{z}_\text{s}).
\end{align}
%
Meanwhile, we leverage the parsing map to split the source image into $M$ regions corresponding to their $M$ categories and get $\{\mathbf{R}_i\}_{i=1}^M$. Each region $\mathbf{R}_i$ is a rectangle region wrapping the $i$th category. Then, we use the pre-trained CLIP encoder to extract the embeddings of all regions
%
\begin{align} \label{eq:clip}
    \mathcal{F}_\text{clip}= \{\mathbf{F}_i^\text{clip}\}_{i=1}^M = \{\text{CLIP}(\mathbf{R}_i)\}_{i=1}^M.
\end{align}

We leverage SourceNet and CLIP encoder embeddings of the source image as conditions in our diffusion. The design is motivated by two key advantages: \ding{182} The SourceNet, sharing identical architecture and initial weights with the TargetNet, naturally produces embeddings that are well-aligned with TargetNet's feature representations across all layers. This alignment facilitates effective transfer of source image information during the diffusion process. \ding{183} The CLIP encoder provides rich semantic embeddings that effectively capture region-specific features, enhancing semantic consistency across different areas of the generated image. 
    
We formulate the noise prediction for diffusion by 
%
% (\cdot|\mathbf{H}_\text{s},\mathbf{p}_\tau)
\begin{align}\label{eq:targetnet}
    \hat{\epsilon}_t = \epsilon_\theta(\mathbf{z}_t, t, \mathcal{C},\mathcal{O}_\beta), 
    \text{s.t.}, \mathcal{C}=\{\mathcal{F}', \mathcal{F}_\text{clip}\},
\end{align}
%
where $\hat{\epsilon}_t$ is the predicted noise at $t$th timestamp. The function $\mathcal{O}_{\beta}$ is the way to fuse conditions $\mathcal{C}$ into the TargetNet, and the key problem is how to design the function $\mathcal{O}_{\beta}$. 

We propose a two-stage fusion strategy with the above two conditions, respectively.
%
In \secref{subsec:fusion}, we introduce human-parsing-guided fusion attention that enhances the embedding's attention on the body, clothes, and facial regions in the target image according to the source parsing map and target pose. 
%
However, the output embedding still presents low scores in some regions and different source images have different lower-score regions. To fix this problem, we further propose a CLIP-guided attention alignment in \secref{subsec:alignment} that can enhance the regions with lower attention scores automatically.


\subsection{Human-Parsing-guided Fusion Attention}
\label{subsec:fusion}

At the $l$th layer of the Siamese network, we obtain embeddings from both the TargetNet and SourceNet, denoted as $\mathbf{F}_l$ and $\mathbf{F}_l'$ respectively.
%
Our method generates a refined embedding $\tilde{\mathbf{F}}_l$ by fusing $\mathbf{F}_l$ with $\mathbf{F}_l'$.
%
For notational simplicity, we will omit the layer index $l$ in subsequent discussions.

\textbf{Target-oriented source parsing selection.} The given target pose $\mathbf{p}_\tau$ contains the coordinates of the skeleton points and the categories of all points. We denote the category set of the target pose as $\mathcal{A}_{\tau}=\{a_j^\tau\}$. 
%
Meanwhile, we have the category set of the parsing map $\mathbf{H}_\text{s}$ and denote it as $\mathcal{A}_{\text{s}}=\{a_j^\text{s}\}$. Each category $a_j^\text{s} \in \mathcal{A}_{\text{s}}$ corresponds to a mask indicating the category's region in the source image and is denoted as $\mathbf{M}_{a_j^\text{s}}$.
%
Then, we can get a mask map $\mathbf{M}$ that is the combination of all categories contained in the target pose, that is, we have $\mathbf{M}=\bigcup_{a} \{\mathbf{M}_a|a\in \mathcal{A}_{\tau}\}$.

%
\textbf{Selected-parsing-reweighed attention.} 
% We first get the query, key, and value tensors of the source embedding $\mathbf{F}'_l$ via three matrices
% %
% \begin{align} \label{eq:qkv}
%     \mathbf{Q}_l' = \mathbf{W_{srcnet}}^Q \mathbf{F}'_l, 
%     \mathbf{K}_l' = \mathbf{W_{srcnet}}^K \mathbf{F}'_l, 
%     \mathbf{V}_l' = \mathbf{W_{srcnet}}^V \mathbf{F}'_l.
% \end{align}
% %
% Instead of using the naive self-attention, we propose to leverage $\mathbf{M}$ to reweight the attention score
% %
% \begin{align}\label{eq:reweightedatt}
%      \mathbf{H}_l' & = \text{RWAtt}(\mathbf{Q}_l',\mathbf{K}_l',\mathbf{V}_l'),  \\  
%      & = \text{Softmax}(({\textbf{Q}_l' {\textbf{K}_l'}^{\text{T}}})\odot \mathbf{M} /{\sqrt{d}}) \textbf{V}_l', \nonumber
% \end{align}
% %
% where $d$ denotes the dimension of the embedding. 
% In the naive self-attention of Stable Diffusion, each self-attention layer receives the $\mathbf{F}$ and linearly projects $\mathbf{F}$ to the query, key and value matrices
% $\mathbf{Q}_l=\mathbf{W}^{Q_l} (\mathbf{F}_l)$, 
% $\mathbf{K}_l=\mathbf{W}^{K_l} (\mathbf{F}_l)$ 
% and $\mathbf{V}_l=\mathbf{W}^{V_l} (\mathbf{F}_l)$,
% where $\mathbf{W}^{Q_l}$, 
% $\mathbf{W}^{K_l}$ 
% and $\mathbf{W}^{V_l}$ are pretrained linear networks for featrue projection. The output of self-attention layers is given by $ SoftMax(\frac{\mathbf{Q K}^T}{\sqrt{d}})\mathbf{V} $. 
Previous works in pose-guided generation methods \cite{hu2024animate,xu2023magicanimate} have demonstrated that modifying self-attention can help regularize the identity of images.
%
They normally concatenate $\mathbf{F}$ and $\mathbf{F}'$ and input them into the self-attention layers of TargetNet for fusion and then decompose. We argue that such a fusion method is not suitable for tasks involving significant pose transformations, as it can lead to distortion of image generation(more details in \secref{subsec: ablation study}).
This limitation arises from the inability to effectively guide the model's focus toward the interest region. In response, we propose human-parsing-guided fusion attention, which leverages binary mask $\mathbf{M}$ to reweight the embeddings of $\mathbf{F}'$.

Specifically, we resize the $\mathbf{M}$ using interpolation to match the corresponding size of $\mathbf{F}'\in \mathds{R}^{h\times w \times c}$. 
%
Then, we calculate the query, key, and value of $\mathbf{F}'$ through $({\mathbf{Q}', \mathbf{K}', \mathbf{V}'}) = ({\mathbf{W}^{Q'} \mathbf{F}'},{\mathbf{W}^{K'} \mathbf{F}'},{\mathbf{W}^{V'} \mathbf{F}'})$, and get the attention map $\mathbf{A}' ={ (\textbf{Q}_l' \textbf{K}_l'^\top)} / {\sqrt{d}}$ that indicates the focusing regions within $\mathbf{F}'$.
%
Then, we derive the mask weight matrix $\textbf{M}'$ based on the attention map $\mathbf{A}'$: 
%
\begin{align} \label{eq:attreweight}
    \mathbf{M}'_{i, j} = \begin{cases}
     1 + \delta & \text{if } \mathbf{A}_{i,j}'>0 \text{ and } \mathbf{M}_{i,j}=1 \\
     \delta & \text{if } \mathbf{A}_{i,j}'<0 \text{ and } \mathbf{M}_{i,j}=1 \\
     \sigma & \text{if } \mathbf{A}_{i,j}'>0 \text{ and } \mathbf{M}_{i,j}=0 \\
     1 + \sigma & \text{if } \mathbf{A}_{i,j}'<0 \text{ and } \mathbf{M}_{i,j}=0 \\
 \end{cases}  \\ \nonumber
     \text{s.t.}, i \in \{0,\ldots, h \}, j \in \{0, \dots, w \},
\end{align}
%
where $\sigma>0$ and $\delta>0$ are hyperparameters. Intuitively, the \reqref{eq:attreweight} assigns higher weights to the embeddings with higher attention through $1+\delta$ and  $1+\sigma$ for the masked region and unmasked region, respectively. In our experiments, we empirically set $\sigma$ and $\delta$ as 0.3 and 0.6 to avoid overfitting and distortion of the background.
%
Then, we compute the hidden states via reweighted attention :
%
\begin{align}\label{eq:reweightedatt}
     \mathbf{H}' & = \text{RwSelfAtt}(\mathbf{Q}',\mathbf{K}',\mathbf{V}',\mathbf{M}'),  \\  
     & = \text{SoftMax}(({\textbf{Q}' {\textbf{K}'}^{\top}}/{\sqrt{d}}) \odot \mathbf{M}' ) \textbf{V}'. \nonumber
\end{align}
%
We further fuse $\mathbf{H}'$ and $\mathbf{F}$ via cross attention:
%
\begin{align} \label{eq:fuseatt}
    \tilde{\mathbf{F}} & = \text{CrossAtt}(\mathbf{Q},\mathbf{K}_h',\mathbf{V}_h'),  \\  
     & = \text{SoftMax}(\mathbf{Q} {\mathbf{K}_h'}^{\top}/{\sqrt{d}}) \mathbf{V}_h', \nonumber
\end{align}
%
where $({\textbf{Q}, \textbf{K}_h', \textbf{V}_h'}) = ({\mathbf{W}^{Q} \mathbf{F}},{\mathbf{W}^{K_h'} \mathbf{H}'}, {\mathbf{W}^{V_h'} \mathbf{H}'})$.
% attn1表示第一个attention 的qkv matrix
Then, we can compare the attention map of the original embedding $\mathbf{F}_l$ and the one of the new embedding $\tilde{\mathbf{F}}$. As shown in \figref{fig:pipeline}, we find that the model focuses more on the areas indicated by the mask during the diffusion process, allowing for better feature extraction and effective pose transfer.
% \qing{As shown in the figure, we find that..}
%
We can insert the proposed attention module into different layers.


\subsection{CLIP-guided Attention Alignment}
\label{subsec:alignment}

Human-parsing-guided fusion attention facilitates the correction of the embedding $\mathbf{F}$ to highlight regions that would appear in the target image, but some regions still have low attention scores.
%
To address this limitation, we propose leveraging CLIP embeddings of local image regions to further refine the attention in $\tilde{\mathbf{F}}$.
%

We first split the attention map $\mathbf{A}'$ into $M$ regions according to the parsing map $\mathbf{H}_\text{s}$ and calculate the average attention scores of each parsing region. Then, we select $K$ regions with the lowest average attention scores. We have the $K$ selected CLIP embeddings $\{\mathbf{F}^\text{clip}_i\}_{i=1}^K$ via \reqref{eq:clip}, concatenate them, and get the embedding $\mathbf{F}^\text{clip}$.
%
We perform the cross-attention with $\tilde{\mathbf{F}}$ and output the final refined embedding
%
\begin{align}
    \hat{\mathbf{F}} & = \text{CrossAtt}(\tilde{\mathbf{Q}},\mathbf{K}^\text{clip},\mathbf{V}^\text{clip}), \\ \nonumber
    & = \text{SoftMax}(\tilde{\mathbf{Q}} {\mathbf{K}^\text{clip}}^{\top}/{\sqrt{d}}) \mathbf{V}^\text{clip},
\end{align} 
%
where $\tilde{\mathbf{Q}}, \mathbf{K}^\text{clip}, \mathbf{V}^\text{clip}= 
(\mathbf{W}^{\tilde{Q}}\tilde{\mathbf{F}}, \mathbf{W}^{K^\text{clip}}\mathbf{F}^\text{clip}, \mathbf{W}^{V^\text{clip}}\mathbf{F}^\text{clip})$. The emebdding $\tilde{\mathbf{F}}$ is calculated by \secref{subsec:fusion} with \reqref{eq:fuseatt}.
% attn2表示第二个attention的qkv matrix
% \qing{Adding analysis of an example.}
It can be observed that while $\tilde{\mathbf{F}}$ has extracted the majority of fine-grained features, some areas, particularly the face, remain underrepresented. A comparison between $\tilde{\mathbf{F}}$ and $\hat{\mathbf{F}}$ reveals that our use of CLIP embeddings effectively enhances features in these local regions.

We can insert the proposed attention module and the alignment module into different layers of the TargetNet and enhance the embeddings across multiple layers.

\subsection{Implementation Details}
\label{subsec:implementation}

\textbf{Optimization and sampling.}
%
During the training process, we also adopt classifier-free guidance \cite{ho2022classifier}, which is a strategy widely used in diffusion models to enhance the quality and control of the generated images. To achieve that we set conditions $\mathcal{C}$ to 0 with a random probability of $\eta$\%.
The final training loss function is rewritten as
%
\begin{align} 
    \label{eq:saimese_loss}
      \mathcal{L} = \mathds{E}_{\mathbf{z}_0, \mathcal{C}, \epsilon, t}(\|\epsilon-\epsilon_\theta(\mathbf{z}_t, t, \mathcal{C},\mathcal{O}_\beta)\|^2_2).
\end{align}
%
% For ParseProj training, we estimate the parsing map and skeleton map of DeepFashion dataset using \cite{li2020self} and \cite{yang2023effective}, respectively. It is worth noting that during the training process of ParseProj, only the Alignment attention and PoseEncoder are trained.
% For Saimese network, we employ the projected $\mathbf{H}_\tau$ and other estimated mapping results for training. All modules in Saimese network are involved in the training process except for VAE and CLIP remain fixed. 
%
During inference, given a randomly initialized Gaussian noise, we utilize TargetNet (\ie, $\epsilon_\theta(\cdot)$) to predict the noise at the $t$th timestep and employ classifier scale $\omega$ to regulate the strength of guidance. The sampling formula is as follows:
% \begin{align} 
%     \label{eq:inference}
%     \hat{\epsilon}_{t}=\epsilon_{\theta}(z_t,t,\varnothing,,\mathcal{O}_{\beta}) + \omega \cdot(\epsilon_{\theta} (\mathbf{z}_t,t,\mathcal{C},\mathcal{O}_{\beta})-\epsilon_{\theta}(z_t,t,\varnothing,,\mathcal{O}_{\beta})),
% \end{align}
\begin{equation}
    \begin{split}
    \hat{\epsilon}_{t}=&\epsilon_{\theta}(z_t,t,\varnothing,\mathcal{O}_{\beta}) + \\
    & \omega \cdot(\epsilon_{\theta} (\mathbf{z}_t,t,\mathcal{C},\mathcal{O}_{\beta})-\epsilon_{\theta}(z_t,t,\varnothing,\mathcal{O}_{\beta})),
    \end{split}
\end{equation}
%
Target latents can be obtained through multi-step denoising process, and mapping them back to the original image space we can acquire the target images. 
%

\noindent \textbf{Model details.}
Our framework is derived by modifying the structure and weights of Stable Diffusion v1.5, based on the Hugging Face Diffusion library. PoseEncoder is a lightweight network with four convolutional layers. 
For training, we run on 4 NVIDIA A800 GPUs, with a batch size of 12 for $512\times 352$ and batch size of 60 for $256\times 176$. The training process consists of 50 epochs and uses the AdamW optimizer with a fixed learning rate of $1e^{-5}$. Classifer-free dropout probability $\eta$ is 30\% and $k$ is empirically set to 2. For inference, We employ the DDIM\cite{ho2020denoising} scheduler with a sampling step of 35 and set classifier scale $\omega$ to 3.5. 

%
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig_qualitative_comparison_3.pdf}
    \caption{Qualitative comparisons with PISE \cite{zhang2021pise}, ADGAN\cite{men2020controllable}, SPGNet \cite{lv2021learning}, CASD \cite{zhou2022cross}, DPTN \cite{zhang2022exploring}, PIDM \cite{bhunia2023person}, NTED \cite{ren2022neural}, PCDM \cite{shen2024advancing} and CFLD \cite{lu2024coarse}.}
    \label{qualitative_figure}
    \vspace{-10pt}
\end{figure*}
%

\section{Experiments}
\subsection{Setup}
\noindent \textbf{Dataset.}
% 这一段主要介绍数据集的构建
We follow \cite{shen2024advancing,lu2024coarse} to conduct extensive experiments on In-Shop Clothes Retrieval benchmark from DeepFashion dataset and evaluated on 512$\times$352 and 256$\times$176 resolutions respectively. 
The dataset is comprised of 52,712 high-resolution images featuring clean backgrounds and diverse fashion models. 
% To be consistent with the configuration in PATN \cite{zhu2019progressive}, we split this dataset into non-overlapping training and testing set with 101,966 and 8,570 pairs, respectively, with each pair including a source image and its corresponding target image. 
Furthermore, we evaluate the model trained on the DeepFashion dataset using the high-resolution in-the-wild dataset WPose\cite{li2024unihuman}, which consists of 2,304 pairs. This dataset features a more diverse and complex range of backgrounds and human poses.
% It is noteworthy that the person identities in both the training and testing sets for both datasets do not overlap.
% \par

\noindent \textbf{Objective metrics.}
% 这一段介绍客观的指标，比如ssim psnr fid
We use four commonly employed metrics, namely \textit{Structure Similarity Index Measure} (SSIM) \cite{wang2004image}, \textit{Peak Signal to Noise Ratio} (PSNR), \textit{Learned Perceptual Image Patch Similarity} (LPIPS) \cite{zhang2018unreasonable}, and \textit{Fr$\acute{e}$chet Inception Distance} (FID) \cite{heusel2017gans}, to assess quality \& fidelity of generated images. To better assess the authenticity and quality of the generations, we introduce the widely adopted LLM-based metric, Q-Align \cite{wu2023q}, which contains both Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA).
% The four metrics differ in the way they measure the similarity (distance) between two images or datasets.
% LPIPS uses a model trained on human judgments to measure perceptual similarity, while SSIM considers aspects such as brightness, contrast, and structure. 
% PSNR quantifies the degree of distortion. FID calculates the Wasserstein-2 distance \cite{vaserstein1969markov} on the distribution of the two data sets.
% \par

\noindent \textbf{Subjective metrics.}
% 主观指标，调研问卷等
% Considering that objective metrics cannot fully capture human perception of how similar two images are, which is usually subjective,
We employ three subjective metrics in \cite{bhunia2023person}: R2G \cite{ma2017pose}, G2R \cite{ma2017pose}, and J2b \cite{siarohin2018deformable,bhunia2023person}.
%
R2G and G2R represent the ratios of real images classified as generated images and generated images classified as real images, respectively. Higher values for R2G and G2R indicate a greater similarity to real images. Meanwhile, J2b reflects user preferences by comparing the proportion of users selecting the best image from a group of images generated by different methods.
%
\subsection{Quantitative and Qualitative Comparison}
\label{subsec:quantitative and qualitative comparison}
%
\begin{table}[t!]
    \centering
    \Large
    % \setlength{\tabcolsep}{20pt}
    \caption{Quantitative comparisons with SOTAs on image quality. $^\dagger$We reproduce these results based on the checkpoints instead of generated images provided by the authors. $^\ast$Results are cited from PoCoLD without publicly available checkpoints or generated images. Others are cited from NTED.}
    \label{tab:main_experiment}
    \resizebox{1\linewidth}{!}{
    \input{tab/tab_quantitative_small}
    }
\end{table}
% \todo{First briefly summarize what this subsection serves for.}
We conduct a comprehensive comparison with SOTA approaches including 13 methods encompass GAN-based, flow-based, attention-based and diffusion-based,
% they are PATN \cite{zhu2019progressive}, ADGAN \cite{men2020controllable}, GFLA \cite{ren2020deep}, 
% SPGNet\cite{lv2021learning},
% PISE \cite{zhang2021pise}, CASD \cite{zhou2022cross}, CocosNet2 \cite{zhou2021cocosnet}, NTED \cite{ren2022neural}, DPTN \cite{zhang2022exploring}, PIDM \cite{bhunia2023person}, PoCoLD \cite{han2023controllable}, PCDM \cite{shen2024advancing}, and CFLD \cite{lu2024coarse}. 
along with a qualitative evaluation of the latest nine SOTA methods.
% among which PIDM, CFLD, and PCDM are diffusion-based approaches.

\noindent \textbf{Quantitative comparison.}
% 主要是表格的分析
% 我们与11种SOTA方法在四个指标上进行了quantitative comparison，结果如表1所示。在第一组分辨率对比中，我们在3/4的指标中超过其他方法，其中远超flow-based和gan-based的方法。值得注意的是我们在由human judgements上训练得到的指标LPIPS中远超其他方法，表示我们的方法在图像的重建上更符合人类审美。尽管我们在FID指标上略高于PIDM，但这可能是PIDM过度拟合了数据集的分布所导致。此外先前的工作也表明FID并不能完全代表图像生成的好坏。我们在第二组实验中相比于其他方法取得了更大的优势，远超其他三类LDM-based的方法。值得注意的是，由于PoCoLD没有提供任何公开的结果导致无法对其进行相同环境下的定量评价，故不纳入最终比较
% We perform a quantitative comparison with 12 SOTA methods across six metrics (See \tableref{tab:main_experiment}). In 256$\times$176 resolution comparison, our method outperforms others in 5 out of 6 metrics, particularly showing a substantial lead over flow-based and GAN-based methods. Notably, in the metric LPIPS, which is trained using human judgments, our method significantly outperforms other approaches. This indicates that our method better aligns with human aesthetics in terms of image reconstruction. While our FID metric slightly exceeds that of PIDM, this can potentially be attributed to PIDM overfitting the dataset distribution as also discussed in prior research\cite{shen2024advancing,lu2024coarse,han2023controllable}, and they has also suggested that FID does not fully reflect the quality of image generation.
% %
% In the second set of experiments, our method demonstrates a significant advantage over other approaches, surpassing all three categories of LDM-based methods. It is worth noting that PoCoLD\cite{han2023controllable} does not provide any publicly available results, which hinders us from conducting a quantitative evaluation under the same conditions.
% Therefore, it is excluded from the final comparison. 
% % Additionally, we also provide the metrics results of VAE reconstructed and ground truth as references for comparison.
% %
% On the more challenging WPose dataset, our approach significantly outperformed the two current state-of-the-art methods across all metrics, particularly in LPIPS, which reflects human perception. This indicates that our method demonstrates greater robustness and generation performance.
% %
We conducted a comprehensive quantitative comparison of our method with 13 SOTAs across two datasets, with the results in Table \ref{tab:main_experiment}. Upon analyzing these results, it is clear that our method significantly outperforms existing techniques across nearly all metrics, with the exception of the FID score at the resolution of 256$\times$176. Notably, we achieved substantial improvements in metrics reflecting human preferences, such as LPIPS and LLM-based evaluations, compared to previous GAN-based and diffusion-based methods. This enhancement can be attributed to our proposed model's ability to better capture fine-grained clothing details, resulting in more realistic and higher-quality images.
%
Furthermore, in the WPose dataset, which features complex backgrounds and poses, our method surpasses the two latest diffusion-based approaches(CFLD,PCDM) across all objective metrics. This finding underscores the superior generalization and robustness of our model in handling diverse and intricate scenarios.
%
It is noteworthy that although our method has a slightly lower FID than PIDM, this may be attributed to PIDM's overfitting to the dataset distribution, as mentioned in previous studies \cite{lu2024coarse,han2023controllable,shen2024advancing}. 

\noindent \textbf{Qualitative comparison.}
% 主要是图片主观层面的分析
% In \figref{qualitative_figure}, we present the results where our framework is compared to the various types of SOTA methods in DeepFashion dataset. 
% % From left to right, it consists of source images, target poses, generated images of different approaches in comparison, and target images ('Ground Truth').
% %
% We observer that: 
% \ding{182} When confronted with extreme pose variations or extensive occlusions as demonstrated in the images in \textit{rows 1-3}, only diffusion-based methods (\textit{columns 7-10}) are capable of reconstructing the approximate pattern outlines of the clothing and faces, and our method significantly outperforms others in terms of face and cloth ID preservation and consistency. This superiority stems from our ability to explore deeper spatial and semantic matching information, followed by enhanced feature fusion achieved through Siamese networks. 
% \ding{183} Addressing the challenge of reconstructing intricate patterns such as designs and text, which previous methods struggle with (as observed in all rows), our approach leverages the Siamese diffusion framework to better retain fine-grained details under significant scale variations. Compared to others, our method preserves finer details, resulting in less disparity from real images.
% \ding{184} The last two rows indicate that irrespective of minor or giant pose differences, our method maximizes the retention of structural clothing features from the source image. As evidenced in the fifth line, only our approach accurately preserves features such as suspender designs.
%
In Fig \ref{qualitative_figure}, we compare the visualization our method with nine SOTAs. We observe that:
\ding{182} Previous methods, whether GAN-based or diffusion-based, struggle to retain complex clothing patterns featuring designs or text, even in simple pose transformation scenarios, as demonstrated in \textit{rows 1-2}. In contrast, our method significantly outperforms existing approaches in terms of clothing consistency by incorporating a mask-guided attention fusion mechanism, which effectively captures fine-grained details of garments.
\ding{183} In observing the more extreme pose transformation scenarios in \textit{rows 3}, we attribute our ability to consistently generate images while effectively preserving the clothing details of the source image to the designed local region enhancement and mask-guided overlapping area attention mechanisms. Although the latest diffusion-based methods, such as CFLD and PCDM can generate a rough skeletal representation of the person, they either result in deformations or fail to retain fine-grained patterns.
\ding{184} The last three rows illustrate scenarios where the target pose necessitates the visualization of areas that are not visible in the source image. The results indicate that our method does not overfit and is capable of reasonably generating images of the unseen regions, while also demonstrating superior visual consistency compared to other approaches.

\noindent \textbf{User study.}
% 以上定量和定性的结果比较展示了我们相较于其他方法的显著优势，然而定量的结果仍然不能完全反映图像细节上的差异（比如花纹，文字等）。为了更好的反映我们方法在细节一致性保持的优势，我们完全参照PIDM的实验设置对30名有着计算机背景的志愿者开展了一项用户研究。(1)针对R2G和G2R两项指标，我们从每种生成方法和真实数据集中各随机挑选30张图片组成一组测试数据，志愿者需要判别每张图片是否为生成图片。可以看出我们在G2R上显著优于其他方法，有一半以上的图片被认为是真实的，这反映了我们生成的图片难以被辨别，更趋近真实图片。此外相比之下R2G指标在各类方法中没有显著差异。(2)针对jab指标，为了更好的凸显我们方法的优越性，我们分别进行了两组实验，一组为挑选后的30张具有复杂花纹和文字的图片，另一组为随机挑选的30张图片来进行公平的对比。可以发现两组数据集上我们均显著优于其他方法(70.2和53.6)，表明我们的方法在复杂图像一致性保持上远超其他方法。
% 用户主观分析,这里需要放一个类似这种表的对比图，包含了三个指标
% The quantitative and qualitative comparisons presented above demonstrate the significant advantages of our approach over others.
% However, quantitative results still fail to fully exhibit the differences in image details (such as textures, text, etc.). 
%
To better illustrate the advantage of our method, we further conduct a user study with 30 computer science volunteers, following the experimental setup of PIDM \cite{bhunia2023person}.
%
As shown in \figref{user study}, we have the following observations:
\ding{182} For the R2G and G2R metrics, we randomly select 30 images from each generation method and the real dataset to form a test dataset. Volunteers discern whether each image is generated or real. Our method significantly outperforms other methods in G2R metric, with over half of the images perceived as real, while no significant differences are found in R2G metric.
\ding{183}  We also conduct two Jab experiments. In one set, we select 30 images with complex patterns from each method. In the other set, we randomly select 30 images for fair comparison. Our method significantly outperformed others on both sets, achieving scores of 70.2 and 53.6 respectively. These results demonstrate that our method excels in preserving consistency in both complex and normal images while aligning more closely with human aesthetics.
%
\begin{table}[t]
    \centering
    \caption{Ablation study on different contributions.}
    \label{tab:ablation}
    \resizebox{1\linewidth}{!}{
    \input{tab/tab_ablation}
    }
\end{table}
%
%
\begin{SCfigure}{}{}
% \vspace{-12pt}
    \includegraphics[width=0.75\linewidth]{figures/fig_user_study.pdf}
    % \vspace{-23pt}
    \caption{{\color{black}User study results in terms of R2G,G2R and Jab metrics. Higher values in metrics means better quality of generated results.}
    \label{user study}
    }
% \vspace{-10pt}
\end{SCfigure}
%
%
\begin{SCfigure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig_ablation.pdf}
    \caption{Visualization results of ablation study}
    \label{fig:fig_ablation}
\end{SCfigure}
%
\subsection{Ablation Study}
\label{subsec: ablation study}

To demonstrate the effectiveness of our contributions, we conduct ablation study on DeepFashion with \tableref{tab:ablation} and \figref{fig:fig_ablation}. 
% 
\textbf{B1} represents our adoption of pre-trained encoders (\ie CLIP) to extract features from the source images like before methods. We design a stacket multi-layer perceptron to map the features to the shape of the $l$-th layer in TargetNet, thereby implementing the fusion mechanism proposed in our method.
%
\textbf{B2} utilizes the original architecture of \cite{hu2024animate}, and fine-tuning it for 10 epochs on the DeepFashion dataset for a fair comparison.
%
\textbf{B3} and \textbf{B4} stands for the removal of two proposed attention mechanism.
%
The results of \textbf{B1} in \secref{sec:limitation} indicate that directly utilizing CLIP fails to preserve clothing details. Furthermore, the naive concatenate fusion like \textbf{B2} results in significant distortion of the characters during large-scale pose transformations. The proposed HPFA module incorporates parsing masks as constraints on attention, enhancing the capability to capture relevant features while minimizing distortions caused by irrelevant regions. Consequently, \textbf{B3} achieves a higher SSIM score compared to \textbf{B2}, indicating improved structural consistency. Due to the presence of certain parsing regions that occupy a small proportion of the overall image or have low attention scores, resulting in poor detail generation, our proposed CAA module adaptively enhances features in local regions to achieve finer-grained generation, as illustrated in \figref{fig:fig_ablation}. In summary, the proposed modules demonstrate complementary effects compared to the baseline, achieving significant improvements in both quantitative and qualitative aspects.

% 
\subsection{Application}
Our method inherits the flexibility of diffusion models, allowing for fashion-related tasks without additional training.
%

\noindent \textbf{Style Transfer.}
%
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fig_app.pdf}
    \caption{Style transfer results of our method. We can edit the reference image to incorporate pattern features from the source image while ensuring consistency in appearance and pose.}
    \label{fig:fig_styletransfer}
    \vspace{-10pt}
\end{figure}
%
We select the region of interest from the reference image $\boldsymbol{y}^{r e f}$ to obtain a binary mask $\boldsymbol{m}$. During sampling, the relation $\boldsymbol{y}_t=\boldsymbol{m} \odot \boldsymbol{y}_t+(1-\boldsymbol{m}) \odot \boldsymbol{y}_t^{r e f}$ is applied to obtain the noise at each step $\boldsymbol{t}$, where $\boldsymbol{y}_t$ and $\boldsymbol{y}_t^{r e f}$ represent the predicted noise for the source image and reference image at step $\boldsymbol{t}$ based on the reference image's pose. As shown in Fig \ref{fig:fig_styletransfer}, our method preserves the region of interest in $\boldsymbol{y}^{r e f}$ while generating image details that are visually consistent with the source image.

% \noindent \textbf{Fashion Retrieval.}
% %
% % \begin{table}[]
% %     \small
% %     \centering
% %     \resizebox{1\linewidth}{!}{
% %     \input{tab/tab_application}
% %     }
% %     \caption{Comparison with SOTA on fashion image retrieval. Standard denotes not using synthesized images.}
% %     \label{tab:tab_retrieval}
% % \end{table}
% %
% Our method, capable of reconstructing complex garments, can be applied to downstream tasks such as fashion image retrieval. We conducted experiments on the In-Shop dataset. Following \cite{liuLQWTcvpr16DeepFashion}, we used FashionNet but retained only its global feature branches as the base network. We then employed PCDM, CFLD, and our method to augment images of each ID with 20\%, 60\%, and 100\% variations in pose, ensuring that the generated images of each methods are driven by the same poses. We evaluated the top-1 retrieval accuracy, and the results are presented in Tab\ref{tab:tab_retrieval}. Our findings indicate that our method significantly enhances fashion image retrieval performance compared to other diffusion-based approaches.
% %
\section{Conclusion}

In this paper, we introduced a novel human-parsing-aware Siamese network framework for pose-guided person image synthesis (PGPIS) that effectively preserves both facial features and clothing patterns. Our approach comprises three key innovations: the introduction of a dual identical UNet architecture (SourceNet and TargetNet), a human-parsing-guided fusion attention module, and a CLIP-guided attention alignment module. 
%
Through extensive experiments on the in-shop clothes retrieval benchmark at multiple resolutions, our method significantly outperformed 13 SOTA baselines, successfully generating high-quality images that maintain both facial identity and intricate clothing details - including text patterns, regular textures, and irregular patterns - even under challenging pose transformations. This advancement addresses critical limitations of existing approaches and offers practical value for the retail industry by enabling the efficient generation of realistic product images in various poses while maintaining precise clothing details, thereby reducing the need for additional photo shoots.

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}





\end{document}


