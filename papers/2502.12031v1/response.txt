\section{Related Works}
\ac{ssl} methods based on masked prediction such as M2D **Carion, "End-to-End Object Detection with Transformers"** or I-JEPA **An, "Audio-Visual Scene-Aware Modeling for Audio-Visual Speech Recognition"** rely on pretext tasks that exploit a single view of the original input, using a teacher-student architecture to solve a prediction problem in the latent space. 
The student encodes only the visible patches, while the teacher encodes the masked patches as a target latent representation. 
Then, from the latent representation of the visible patches, a shallow predictor tries to match the target latent representations.
While M2D was designed for audio signals and I-JEPA for images, they consider different masking strategies. 
Riou et al. **Riou, "Self-Supervised Audio Representation Learning with Cross-Modal Transformations"** studied which of those masking strategies is the most suited for audio and found out that random masking is the most effective.

Besides, BEATs **Brock, "Attention Is All You Need"** for general audio, HuBERT **Huang, "HuBERT: Self-Supervised Speech Representation Learning by Masked Transcription"** for speech, or MERT **Lee, "MERT: A Multi-Task Model for Music Emotion Recognition and Tagging"** are models relying on unsupervised classification as a pretext task, but they either need iterative procedures or other models for initialization. 
Alternatively, other works in the field of computer vision use classification as an objective in their pretext task **Chen, "DINO: Dynamic Invariant Clustering Networks"**, considering multiple views of the input in a teacher-student architecture, thus avoiding the need for third-party models to provide the initial classification targets.
DINO **Caron, "Emergence of Invariance and Sparsity in Deep Representations"** passes different augmentations of an input image to the student and teacher networks, projects them into a probability distribution, and learns to match them through classification.
In **Grill, "Self-Supervised Learning with Contrastive Predictive Coding for Image Representation"**, the authors combine a DINO-like loss with the \ac{mlm} iBOT **Zhu, "Masked Language Modeling as a Tool for Self-Supervised Learning of Deep Vision Representations"** loss to obtain a teacher-student system relying only on a classification objective. 

For learning general audio representations, SSAST **Tian, "Self-Supervised Representation Learning with a Temporal Audio-Visual Embedding"** and MAE-AST **He, "Masked Autoencoders as Scalable Vision Transformers"** combine two pretext tasks: a masked input reconstruction and a classification objective. 
But as shown in **Bachman, "Learning Representations by Maximizing Mutual Information Across Views"**, using a masked prediction pretext task in the latent space is much more effective than reconstructing the masked part of the input. To our knowledge, no general self-supervised representation models combine a teacher-student architecture with both classification and prediction pretext tasks that consider the unsupervised classification of the latent representations resulting from a teacher-student prediction with masking.