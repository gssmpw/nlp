\section{Related Works}
\ac{ssl} methods based on masked prediction such as M2D \cite{niizumi23m2d} or I-JEPA \cite{assran23ijepa} rely on pretext tasks that exploit a single view of the original input, using a teacher-student architecture to solve a prediction problem in the latent space. 
The student encodes only the visible patches, while the teacher encodes the masked patches as a target latent representation. 
Then, from the latent representation of the visible patches, a shallow predictor tries to match the target latent representations.
While M2D was designed for audio signals and I-JEPA for images, they consider different masking strategies. 
Riou et al. \cite{riou24jepa} studied which of those masking strategies is the most suited for audio and found out that random masking is the most effective.

Besides, BEATs \cite{chen23beats} for general audio, HuBERT \cite{Hsu21HUBERT} for speech, or MERT \cite{Li24mert} for music, are models relying on unsupervised classification as a pretext task, but they either need iterative procedures or other models for initialization. 
Alternatively, other works in the field of computer vision use classification as an objective in their pretext task \cite{Caron21dino, Zhou22ibot, OquabA24dinov2}, considering multiple views of the input in a teacher-student architecture, thus avoiding the need for third-party models to provide the initial classification targets.
DINO \cite{Caron21dino} passes different augmentations of an input image to the student and teacher networks, projects them into a probability distribution, and learns to match them through classification.
In \cite{OquabA24dinov2}, the authors combine a DINO-like loss with the \ac{mlm} iBOT \cite{Zhou22ibot} loss to obtain a teacher-student system relying only on a classification objective. 

For learning general audio representations, SSAST \cite{Gong22ssast} and MAE-AST \cite{Baade22maeast} combine two pretext tasks: a masked input reconstruction and a classification objective. 
But as shown in \cite{niizumi23m2d}, using a masked prediction pretext task in the latent space is much more effective than reconstructing the masked part of the input. To our knowledge, no general self-supervised representation models combine a teacher-student architecture with both classification and prediction pretext tasks that consider the unsupervised classification of the latent representations resulting from a teacher-student prediction with masking.

% -------------------- -------------------- -------------------- -------------------- --------------------