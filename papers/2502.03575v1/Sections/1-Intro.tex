\vspace{-0.5mm}
\section{Introduction}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{Images/teaser-scanpath.png}
  \caption{We present \name, a computational model that can predict task-driven human scanpaths on charts. The figure demonstrates three analytical tasks involved in the study: retrieve value, filter, and find extreme. The visualization illustrates how models' predictions vary across tasks and match the pattern of human scanpaths, with fixation density maps overlaid.}
  \label{fig:teaser}
\end{figure*}

Visual attention plays a pivotal role in the field of information visualization~\cite{healey2011attention, borkin2015beyond}.
By understanding the visual attention on charts, designers can iteratively refine visualizations using visual saliency as feedback~\cite{shin2023perceptual}; engineers can enhance their AI models by incorporating human-like attention ~\cite{sood2023multimodal}; and researchers can better understand the link between comprehension and gaze behavior when people read charts~\cite{wang2024visrecall++}.
Eye tracking has long been used to understand human visual attention on charts~\cite{goldberg2011eye}.
Beyond visual saliency~\cite{shin2022scanner}, analyzing human scanpaths provides details of the sequence of fixations, helping researchers understand strategies for reasoning~\cite{wang2023scanpath}.
Previous literature has demonstrated that users observe completely different visual elements when performing different analytical tasks~\cite{polatsek2018exploring}.
However, collecting human scanpaths by using eye trackers is expensive both time-wise and monetarily.
\rv{   
Simulations are effective in developing theories by rigorously testing user interactions with visual elements in controlled settings. By simulating eye movements, researchers can uncover the mechanisms behind behaviors of users interpreting data visualizations. This enhances understanding of chart reading~\cite{murray2022simulation}.
}

Human visual attention is guided by two processes: bottom-up and top-down processes~\cite{itti2000saliency}. These processes apply to chart reading as well~\cite{yang2024unifying}. Bottom-up attention is driven by salient visual stimuli (e.g., high-contrast colors), whereas top-down attention is task-driven, with specific goals or intentions shaping where and how users focus their attention.
However, most visual attention models applied for information visualizations capture only bottom-up (free-viewing) attention~\cite{matzen2017data, shin2022scanner, wang2023scanpath}, thus leaving a gap in understanding how tasks influence human visual attention~\cite{amar2005low}.
Compared to exploratory free viewing, scanpaths for the same analysis task are more coherent; also, they vary greatly between tasks~\cite{polatsek2018exploring}.

While recent research has been able to predict task-driven saliency on charts~\cite{wang2024salchartqa}, it has remained one step away from addressing how people read charts. 
Temporal information and individual-specific behaviors are still missing from task-driven-saliency maps.
In other words, what would the scanpath look like when a person carries out a particular task on a given chart?
In this paper, we present \name, the first computational model for predicting task-driven scanpaths on charts~\footnote{\href{https://chart-reading.github.io}{https://chart-reading.github.io}}.
When given both an image of a chart and a sentence as the analytical task, \name can simulate a sequence of human-like fixation positions related to the task (see Figure~\ref{fig:teaser}).
The model has two key distinctions from preexisting models for scanpath prediction: 
1) Our model focuses on predicting fixations made during analytical tasks, including both fixation positions and their order, in contrast against the current state-of-the-art models, which concentrate on free-viewing conditions. Task factors' influence makes it challenging to predict task-driven scanpaths via prior models.
2) Unlike goal-driven scanpath predictions, which are limited to visual search tasks with natural images~\cite{mondal2023gazeformer}, analytical tasks require high-level reasoning and also tackling the limitations of human cognition in chart question answering.

\begin{figure*}[!t]
\centering
  \includegraphics[width=0.9\textwidth]{Images/scanpath-concept.png}
  \caption{The figure illustrates the concept of the model for task-driven eye movement control. When given a task, the agent makes decisions about the next subtask, based on information gathered from observing the chart stored in its memory. Each subtask controls eye movements at pixel level and retrieves information from the foveal vision area of the gaze.}
  \Description{Concept of the model for task-driven eye movement control}
  \label{fig:concept}
\end{figure*}

To enable such capabilities, we propose a hierarchical control architecture for modeling (see Figure~\ref{fig:concept}).
We adopted this architecture for two key reasons:
First, it manifests the critical benefit of mirroring how humans break complex tasks into subtasks, which are easier to solve. Studies in cognitive science suggest that humans use hierarchical frameworks in decision-making~\cite{botvinick2012hierarchical, frank2012mechanisms}.
Second, the machine-learning community's promising advances in implementing hierarchical architectures in various motor-control domains~\cite{huang2023inner, brohan2023can} points to its potential for oculomotor control.
This particular hierarchical architecture is composed of a high-level cognitive controller for deciding subtasks and a low-level oculomotor controller for moving the gaze.
The cognitive controller is powered by large language models (LLMs)~\cite{achiam2023gpt} for understanding the task, analyzing the information obtained, and selecting operations for collecting information from the chart.
In the oculomotor control, the approach employs deep reinforcement learning (RL)~\cite{schulman2017proximal}, training RL agents for each operation to perform detail-level gaze movements.

We evaluated the model's performance through experiments using human data. 
We compared scanpath-level similarity with baseline models' output and human scanpaths, where the baselines were the latest general scanpath prediction model~\cite{kummerer2022deepgaze}, scanpath prediction in visual question answering~\cite{chen2021predicting}, and free-viewing scanpath prediction on charts~\cite{wang2023scanpath}. The results suggest that the hierarchical gaze control model demonstrates closest similarity to human data across tasks.
We also analyze the summary statistic of the gaze behavior from model predictions, which effectively reproduce human-like gaze movement behavior.
The evaluation results highlight the potential for the model to exhibit human-like behavior in task-driven scanpaths across different tasks.

In summary, the main contribution of this work is the first computational model \name, to the best of our knowledge, for predicting task-driven scanpaths on charts. The key technical contribution of \name is its hierarchical gaze control with a cognitive controller and oculomotor controllers. This architecture enables training the model without relying on human eye movement data.
We analyzed human scanpath data from charts, to support modeling for three common analysis tasks: ``retrieve value,'' ``filter,'' and ``find extreme''.
We conducted comprehensive experiments to evaluate scanpath prediction across analytical tasks, comparing scanpath similarity and providing statistical summaries. Our model performs similarly to humans and better than the baselines in predicting task-driven scanpaths.
\rv{This study focused on analytical tasks involving statistical charts, where the scanpaths align more with the problem-solving reasoning process and are less influenced by the complexity of the visual representation. At the end of the paper, we discuss the generalizability of the modeling approach.}

\rv{
The paper is structured such that 
Section~\ref{sec:related} reviews prior research into eye tracking connected with information visualizations, analytical tasks, and scanpath prediction models 
with Section~\ref{sec:model} laying further groundwork by introducing the problem formulation and the design of the computational model for predicting task-driven scanpaths, including the hierarchical control architecture and training workflow.
We then present our evaluation of the modelâ€™s performance relative to baseline methods and human data in Section~\ref{sec:evaluation}.
Finally, Section~\ref{sec:discussion} discusses potential applications of the approach and its generalizability.
}