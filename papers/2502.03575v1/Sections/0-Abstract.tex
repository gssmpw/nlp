\begin{abstract}
To design data visualizations that are easy to comprehend, we need to understand how people with different interests read them. Computational models of predicting scanpaths on charts could complement empirical studies by offering estimates of user performance inexpensively; however, previous models have been limited to gaze patterns and overlooked the effects of tasks. Here, we contribute \name, a computational model that simulates how users move their eyes to extract information from the chart in order to perform analysis tasks, including value retrieval, filtering, and finding extremes. 
The novel contribution lies in a two-level hierarchical control architecture. At the high level, the model uses LLMs to comprehend the information gained so far and applies this representation to select a goal for the lower-level controllers, which, in turn, move the eyes in accordance with a sampling policy learned via reinforcement learning.
\rv{
The model is capable of predicting human-like task-driven scanpaths across various tasks. It can be applied in fields such as explainable AI,  visualization design evaluation, and optimization. 
While it displays limitations in terms of generalizability and accuracy, it takes modeling in a promising direction, toward understanding human behaviors in interacting with charts.
}
\end{abstract} 

% \begin{abstract}
% Understanding how people read charts to perceive and process graphic information can be achieved through analyzing human scanpaths. However, manual collection of scanpaths using eye tracking is expensive, which leads to scanpath modeling approaches.
% Previous studies on modeling scanpaths in charts, focusing on free-viewing for memorization, cannot be applied to task-driven scenarios.
% This paper contributes the computational model of simulating task-driven scanpaths on charts. 
% This modeling approach is based on a comprehensive analysis of human data, which identified gaze behaviors during three common visualization tasks: retrieve value, filter, and find extreme.
% The goal of the model is to predict a sequence of spatial positions as fixations that can help with addressing the task by considering the human limitations in both cognition and perception.
% To achieve the goal, the design of the model adopts a hierarchical gaze control architecture with a high-level gaze controller powered by LLMs for action planning and a low-level oculomotor controller driven by reinforcement learning for detailed gaze movement. 
% The decision-making process relies on the vision model limited by the visual field and a memory model constrained by the gaze visits within the memory capability.
% The evaluation includes exploring model behaviors to account for differences in the task, chart, and individual difference; comparing the similarity of scan paths between human participants and model predictions; and reproducing the statistical summary of gaze behaviors across tasks. Our research indicates that the model is able to predict scanpaths that closely resemble human data during specific tasks compared to other gaze prediction models.
% Currently, the model is limited to specific charts and tasks, we discuss the applicability of the model at the end.
% \end{abstract} 

% The high-level controller is powered by a large language model, helping it understand the task, analyze gathered information, and choose operations to collect information from the chart. It includes key operations based on eye-tracking literature for visualization evaluation. Each operation is trained by reinforcement learning to perform the low-level gaze movement. The decision-making process relies on a memory model that stores information based on gaze visits and within the memory limit. 


%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the exa1mple below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121.10003122.10003332</concept_id>
<concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003145.10003147.10010923</concept_id>
<concept_desc>Human-centered computing~Information visualization</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~HCI theory, concepts and models}
\ccsdesc[500]{Human-centered computing~Information visualization}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{User model; Simulation; Scanpath; Reinforcement learning; LLMs}