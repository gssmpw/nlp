\section{Experiments}
\label{sec:evaluation}

This section presents the experiments conducted for evaluating and comparing scanpath prediction models. We evaluated \name specifically in terms of scanpath similarity and statistical summaries of eye movement behavior. The results from evaluation of our model in comparison to the baselines are summarized in Table~\ref{tab:benchmark}.

\subsection{Data and Metrics}

We evaluated \name by using 12 distinct analytical tasks with horizontal bar charts from a task-driven scanpath dataset~\cite{polatsek2018exploring}. 
Each task has at least 14 human scanpaths (\textit{M} = 15.25, \textit{SD} = 0.60), for a total of 183 human scanpaths. 
\rv{
This set of ground-truth human data was collected by means of Tobii X2-60 eye trackers at 60~Hz while participants were engaged in these analytical tasks on 24.1-inch monitors at a resolution of 1920 Ã— 1080 pixels.
}
To evaluate model performance, we compared the generated scanpaths against human ground truth, with the aim of ascertaining whether the models can produce human-like scanpaths and replicate natural gaze movement patterns. 
We should stress that, to ensure unbiased evaluation, none of the human eye movement data informed our training of the models, only their assessment.
Furthermore, except for scanpath prediction that relies on human-generated data as the training set, no such eye movement data were involved in training for our approach.

Following recent practice in scanpath prediction~\cite{wang2023scanpath, sui2023scandmm}, we evaluated \name by using three established scanpath-based metrics: dynamic time warping (DTW), Levenshtein distance (LEV), and Sequence Score.
DTW computes the optimal alignment between two scanpaths, where lower values indicate better correspondence. For this paper, DTW was calculated in two-dimensional position coordinates.
Both LEV and Sequence Score represent the semantic order of scanpaths as sequences of letters by mapping each fixation to a unique letter, then measuring the string-editing distance between the sequences~\cite{needleman1970general}. 
In LEV, letters are defined by the grid regions on which fixations land. 
For Sequence Score~\cite{yang2020predicting, wang2023scanpath}, letters are based on areas of interest (AOIs) such as the title and legend.
Sequence Score values are normalized between 0 and 1, with higher scores reflecting better alignment.
For DTW, LEV, and Sequence Score, we report the \textit{mean} and \textit{best} evaluation scores (see Table 2). For each method, the number of scanpaths equaled that of human scanpaths. The \textit{mean} scores are the averages across all human--predicted scanpath pairs, while the \textit{best} ones represent the maximum of all pairs for each prediction~\cite{chen2021predicting, wang2023scanpath}.

\input{Tables/benchmark}

\rv{
We looked beyond scanpath metrics, introducing more detailed measurements inspired by \citet{goldberg2010comparing} to show a statistical summary of task-driven scanpaths over charts. % The analysis of human data can be summarized thus:
\begin{itemize}
    \item \textit{Number of fixations}: The length of a scanpath can be measured as the count of gaze fixations (between motions, or saccades). According to the human data, the number of fixations in task-driven scanpaths over charts (89.8 on average) is much larger than the number in free-viewing tasks (37.4 on average). This reflects the difficulty of analytical tasks relative to free viewing of charts.
    \item \textit{Fixation on task-dependent AOI ratio}: Task-dependent AOIs are regions that are relevant to the task, such as value labels, text labels, and data points~\cite{polatsek2018exploring}. People's focus on these areas indicates how they are processing the task. Inspired by the Hit Any AOI Rate metric~\cite{wang22_etvis}, this measurement provides a summary of the overall visual attention to task-related regions. Although the scanpath is task-driven, we ascertained that most of the eye movement does not occur in task-dependent regions: fewer than 20\% of fixations fell in task-dependent AOIs. This suggests that people might devote more time to gathering information or confirming it.
    \item \textit{Percentage of fixations within each area}: We considered the percentage of time devoted to looking at distinct parts of a chart -- namely, the key areas of charts: the title, the marks (such as bars or data points), and the axes. This assists in summarizing where people are focusing their visual attention. The percentages are calculated by dividing the number of fixations in a specific area by the total number of fixations. Humans direct most of their fixations to the axes, then the region of graphical marks. This might be because the three analysis tasks probed are strongly related to values, not other visual features.
    \item \textit{Fixation transitions}: We also used a metric capturing the average number of times the eyes move from one area to another during a task. It helps us understand how often the eyes' fixations shift between distinct areas. Frequent fixation transitions may point to room for improvement in the design of the chart, such as bringing related elements closer together. From human data, we identified a high number of fixation transitions (about 20 per task). We found that, on average, about four consecutive fixations follow each fixation transition.
    \item \textit{Revisit frequencies}: The average number of fixations returning to a previously visited area during a task proved similarly revealing. Human data exhibited high revisit rates. Spatially, users revisit marks and also axes eight times, on average. This frequent double-checking of data information in the chart for the answer may be due to forgetting the information.
\end{itemize}
These metrics help us evaluate whether the model's predictions can accurately capture general human patterns followed with charts for particular tasks. We strove for a system in which the predicted scanpath closely matches human ground-truth performance, ideally being within one standard deviation of the mean value.
}

\subsection{Comparison Methods}

Given the lack of existing methods for predicting task-driven scanpaths on information visualizations, we compare \name against human ground truth with three closely related baselines:

\begin{itemize}
    \item \textit{Human}~\cite{polatsek2018exploring}. With the scanpath metrics, we conducted leave-one-out cross-validation among the human scanpaths. For each viewing condition, every human scanpath was compared with all other human scanpaths for similarity. Human scanpaths were compared with themselves for the \textit{mean} scores but not for contributions to the \textit{best} scores. In applying the statistical metrics, we treated all the human data as the ground truth and gauged all modeling methods by their closeness to this ground truth.
    \item \textit{VQA scanpaths}~\cite{chen2021predicting}. VQA is a deep reinforcement learning model that predicts human visual scanpaths in the context of images with visual question answering. The paper reporting on it demonstrates its strong generalizability across various tasks and datasets, indicating optionality as an approach for predicting task-driven scanpaths over charts. % It allows for comparing stimuli effects on scanpaths.
    \item \textit{UMSS}~\cite{wang2023scanpath}. UMSS represents the state-of-the-art scanpath prediction model for visualizations, making it the most relevant work in this area. However, it is designed to predict scanpaths in a free-viewing context for information visualizations, rather than consider specific tasks. Its inclusion allows for comparison between scanpath prediction with and without task-linked factors.
    \item \textit{DeepGaze iii}~\cite{kummerer2022deepgaze}. DeepGaze iii is a deep-learning-based model that integrates image data with information about previous fixations to forecast free-viewing scanpaths over static images. Trained on large sets of eye tracking data from natural images, it serves as a baseline for evaluating the effect both of stimuli and of tasks on scanpaths.
\end{itemize}


\subsection{Results}

\begin{figure*}[!t]
\centering
  \includegraphics[width=0.95\textwidth]{Images/examples.png}
  \caption{Qualitative comparison: for three tasks, an illustration of \name's predictions relative to three baselines -- VQA scanpaths~\cite{chen2021predicting}, UMSS~\cite{wang2023scanpath}, and DeepGaze iii~\cite{kummerer2022deepgaze}. \name is able to capture human scanpath patterns displayed during analytical tasks.}
  \label{fig:examples}
\end{figure*}

% \paragraph{Scanpath similarity}
\paragraph{\name demonstrates high similarity in scanpaths}

The first six rows for each task type in \autoref{tab:benchmark} present the results from our three scanpath similarity metrics.
\name achieved the highest performance by the Sequence Score and LEV metrics, and it ranked second for DTW, with scores closely approaching the maximum and also close to human ground truth. 
Specifically, \name closely approximates the latter Sequence Score in terms of mean performance, achieving a score of 0.413, relative to 0.486.
The UMSS method, while securing first place for DTW, ranked second for LEV and third for Sequence Score.
\name outperforms human ground truth in LEV \textit{mean} (151.8 vs. 154.7). This means that the predictions from \name deviate less from the ``average human scanpath.''

The results from the scanpath metrics show that \name performed better than the baselines by the Sequence Score and LEV metrics, which are based on regions, but not the DTW metric, which is based on pixel-wise distances. This suggests that \name is more similar to human data when one factors in the semantic order of fixation positions in meaningful portions of charts. However, it may not fully match human data for pixel-level similarity.
This result is consistent with the discussion in the literature~\cite{wang2023scanpath}, which has concluded that metrics based on pixel-wise distances between scanpaths might not wholly capture the quality of human scanpaths. Therefore, we must conduct further analysis of the statistical summary of eye movement behaviors.

% \paragraph{Statistical summary of eye movement behaviors}
\paragraph{\name aligns more closely with human statistical patterns}

The last nine rows for each task type in \autoref{tab:benchmark} provide the mean and standard deviation for each eye movement behavior. 
Because UMSS and DeepGaze iii are not task-driven, our analysis used the same predicted scanpaths across all tasks.
\name achieves strong alignment with human data, with all 27 of its values for the eye movement behavior metrics falling within one standard deviation of the human mean and with 18 of them being the closest to the human data's mean. In comparison, 18 of UMSS's 27 values lie within one standard deviation, and five of them are the closest to the mean. The corresponding figures for DeepGaze iii are 13 out of 27 and 4, respectively, while VQA yielded only six values within the range and only one of the 27 was the closest to the human mean.

Examining the detailed metrics across tasks reveals that humans show significantly variable task-dependent AOI ratios. They devote the majority of their fixations to task AOIs when performing the \textit{F} task (19.1\%). That is followed by the \textit{RV} task (10.4\%), with the \textit{FE} task having the lowest percentage (1.7\%). This distribution makes sense: the first two tasks require individuals to focus on a specific data label, while \textit{FE} can be completed by directly observing the general shape of the graph. \name is the only model that successfully replicates this phenomenon by reproducing the human order of task-dependent AOI ratios: {FE} (10.6\%), then {RV} (4.1\%), and finally {FE} (0.2\%). 
As for per-region fixation ratios, humans direct the most fixations to axes, followed by marks, across all three tasks. \name successfully reproduces this phenomenon in the case of the \textit{RV} task. For the \textit{F} and \textit{FE} tasks, \name shows similar distributions. In contrast, the VQA and UMSS baselines consistently allocate over 50\% of fixations to the marks, and DeepGaze iii allocates most fixations to the title. 
In the realm of revisits, \name and DeepGaze iii align with human data, revisiting the axes most frequently, while the other two models revisit the marks most often. 
In summary, our analysis demonstrates that \name exhibits the pattern most similar to human data.

\paragraph{Qualitative analysis}

Figure~\ref{fig:examples} showcases predicted scanpaths from \name and the three baseline models across six tasks, with fixation density maps overlaid. In all cases, our model's predictions are closer to the human data than the baseline models'. \name and VQA scanpaths both are task-driven, unlike UMSS and DeepGaze iii's, which cannot predict scanpaths solely from images. Here are the main observations from Figure~\ref{fig:examples}:
\begin{itemize}
    \item DeepGaze iii predicts scanpaths from the bottom up on the basis of the saliency of a natural image. That results in a high number of fixations on the title, consistent with producing the highest fixation-on-title ratios. 
    \item Although UMSS is not task-driven, its predicted scanpath shape closely mirrors human data. This indicates that, even in task-driven scenarios, bottom-up mechanisms exert a significant influence. 
    \item Nevertheless, \name, as does VQA, captures human gaze patterns more effectively across tasks than these free-viewing models. Importantly, \name outperforms the VQA scanpath model, for VQA often predicts fixating on irrelevant areas, in the absence of specific knowledge of visualization structures.
\end{itemize}

The examples in Figure~\ref{fig:teaser} demonstrate how \name's predictions compared to human data for the three tasks. The chart read displays a list of top-ranked theme parks worldwide with their corresponding attendance numbers for 2011. 
When given the \textit{RV} task of answering ``what is the attendance level of Universal Studios Hollywood?'' both the human user and the model focus on the text labels to find the theme park in the chart and the relevant positions for the mark and on the value axis.
For the \textit{F} task, the human user and the model both frequently look at the value axis.
Regarding the \textit{FE} task, both human and model focus on the top of the mark and also fixate on the text label associated with that mark. 
We noted that human eye movements are also drawn to other text labels, such as annotations and textual descriptions, while \name remains task-focused without getting distracted by unrelated information.