\section{\name: Modeling Task-driven Eye Movement on Charts}
\label{sec:model}

This section introduces the problem formulation and presents the computational model of eye movement control on charts in settings of analytical tasks.

\subsection{Problem Formulation}

Given a chart image $C$ and an associated analytical task $x$ stated as text, the model is expected to generate a sequence of fixation positions $\{ p_1, p_2, \dots , p_t\}$.
The objective of the output sequence is to closely match the scanpath from humans reading the chart. 
Specifically, the sequence of fixations represents the visual reasoning process, and the information in the patches of pixels fixated upon should be able to support $x$.
We consider general analytical tasks in information visualization~\cite{amar2005low}, and
select three of them used in a human eye-tracking data collection~\cite{polatsek2018exploring}: % \textit{RV}, \textit{F}, and \textit{FE} tasks
\rv{
\begin{itemize}
    \item[1)] \textit{Retrieve value (\textit{RV})}: Given a specific target, find the data value of the target (e.g., what is the value for a certain category?)
    \item[2)] \textit{Filter (\textit{F})}: Given a concrete condition, find which data point satisfies it (e.g., which category has the specific value stated?)
    \item[3)] \textit{Find extreme (\textit{FE})}: Find the data point showing an extreme value for a given attribute within the set of data (e.g., which category shows the highest/lowest value?)
\end{itemize}
}

\subsection{Modeling Overview}

Our goal was to develop the model \name to handle tasks articulated as free-form text and be able to perform gaze movement at a detailed pixel level.
We conceptualize the design of the hierarchical gaze control model in Figure~\ref{fig:model}, where the high-level (cognitive) controller is responsible for reasoning while the low-level (oculomotor) controller determines details of gaze movement. 
The idea behind this is hierarchical supervisory control~\cite{eppe2022intelligent}, which refers to a tiered control system in which the superior controller set goals for its subordinates. The actions from subordinates are integrated into an overall pattern for high-level control~\cite{pew1966acquisition}.
The concept also follows the modeling principle of computational rationality, where we assume that the controllers optimize their policy to maximize expected utility within relevant cognitive bounds~\cite{oulasvirta2022computational,chandramouli2024workflow}.
Specifically, the high-level controller handles abstract information processing, comprehension, and memory storage. 
It sets subtasks to the low-level controller, which then moves the gaze to gather information for task completion. Subsequently, the high-level controller utilizes the amassed information to answer the question.

\begin{figure*}[!t]
\centering
  \includegraphics[width=\textwidth]{Images/h-gaze-control.png}
  \caption{\textbf{An overview of the hierarchical eye-movement control architecture.} When presented with a chart and a task, a cognitive controller, powered by large language models, makes decisions on what to look at next and judges whether it is confident enough to provide an answer to the task's question. It relies on internal memory, which summarizes the information gathered from the chart through eye movements. Once cognitive control has determined the next action, the oculomotor controller is responsible for moving the gaze and observing the chart through a limited vision field. The model's objective is to accurately address the task as quickly as possible within set cognitive and physical constraints.}
  \Description{An overview of the hierarchical eye movement control architecture.}
  \label{fig:model}
\end{figure*}

\subsection{Cognitive Control}

The high-level controller provides cognitive control over the mental processes for a chart, control that performs reasoning in working memory~\cite{liu2010mental}. When performing vision tasks, one observes and analyzes visual information interactively~\cite{chen2020air}. Throughout this process, people analyze the information in their memory and try to gather more useful information to reduce uncertainty in solving the task.
To represent this decision problem accurately, we formulate it as a bounded optimality problem in a partially observable Markov decision process (POMDP). Instead of having access to a full state ($\mathcal{S}$) with pixels of the chart associated with the given task, the POMDP expresses a subset of ($\mathcal{S}$) as the observation of the model:
\begin{itemize}
    \item Observation $O$ refers to the information in memory that is captured from eye movements over the chart.
    \item Action $A$ includes subtasks that the model gives to oculomotor control for performing eye movements.
    \item Reward $R$ is the correctness of the answer for the task from the chart question answering.
\end{itemize}
To solve this POMDP, our model uses LLMs for the policy. The rationale behind this choice is that LLMs are well suited to processing higher-level information, as they have been pre-trained on human text data encompassing a wealth of logic related to planning, reasoning, and interaction~\cite{huang2022language, vemprala2024chatgpt, li2023interactive}.
Although LLMs are limited in their ability to control low-level motor functions in a precise manner~\cite{dalal2024psl}, they are proficient at planning and reasoning, with LLaMA~\cite{touvron2023llama} and GPT~\cite{achiam2023gpt} showing impressive language interpretation and reasoning capabilities.
Also, recent work has shown that utilizing LLMs in the high-level controllers in hierarchical architecture can produce promising results~\cite{huang2022language, brohan2023can, liang2023code}.
For our setting, we used GPT-4o~\cite{achiam2023gpt} for the policy, which takes the information accumulated in the memory as the observation and sets subtasks to guide eye movements in order to obtain information needed for solving the task efficiently.

We consider two human limitations when constructing the model's observation: a limited field of vision~\cite{duchowski2018gaze} and memory capacity~\cite{loftus2019human}. 
The model gets information from the gaze position purely by mimicking the human vision system. 
An optical character recognition technique~\cite{singh2010optical} is used to extract text from the pixels of the chart, and the text in the gaze area, with the position, is passed to the memory.
As a result, the observation consists of image patches (in a limited number) from the full set of chart pixels. The reliability of items in memory is determined by their visit history~\cite{li2023modeling}, with overall memory capacity being restricted too. When new information is added to the memory, a previously added item is removed on the basis of a forgetting probability. The probability of forgetting an item in the memory is calculated by means of the formula $\text{Softmax}(\rho \cdot (t-t_i)) $, where $t$ is the current fixation index, $t_i$ is the index of the $i$th item in the memory, and $\rho$ is the weight parameter (set to 0.1 here). The observation is designed as a prompt that summarizes the memory in line with the memory model and explains the model's goal. 

Given the summary of the memory information, the LLM policy selects predefined operations for task solving~\cite{brohan2023can, liang2023code}. The operations here are based on a sequence of cognitive stages for charts~\cite{goldberg2011eye} -- 1) \textit{search for text label}: visually searching for a text label or value label related to the task, 2) \textit{find associated mark}: visually searching for a graphical mark of the data point when given a reference label, 3) \textit{read associated value}: visually searching to read the given mark's associated value or textual label.
All these actions are allowed to be reused in the process, which enables the model to revisit previous positions for confirmation of the information.
Ultimately, if the information in the memory is sufficient to address the task, the gaze movement can stop and an answer can be given. Operations other than answering the question will be performed by the oculomotor controller for detailed gaze movement. 

The examples in Figure~\ref{fig:memory} demonstrate how utilizing memory information and predefined operations aids in scanpath prediction. Model memory uses the summarization capability of LLMs to convert the text and positions gathered to a paragraph as the observation (as shown in the green boxes). The LLM policy then makes decisions and issues subtasks as actions (in red boxes) for the oculomotor control, which performs pixel-level gaze movements.

\begin{figure*}[!t]
\centering
  \includegraphics[width=\textwidth]{Images/scanpath-memory.png}
  \caption{The figure gives examples of how the internal memory helps the cognitive controller to remember what has been read and then select actions for detailed gaze movement. A green box indicates the information held in memory, a red box represents the action selected by cognitive control, and the blue lines in the images reflect the eye movement scanpaths.}
  \label{fig:memory}
\end{figure*}

\subsection{Oculomotor Control}

The oculomotor controller acts as the interface between the cognitive controller and the actual chart-pixel images. Its main function is to control the movement of the gaze over the pixels in order to gather information related to the task at hand.
Generating oculomotor behavior at pixel level is another sequential decision-making problem that can be formulated as a POMDP:
\begin{itemize}
    \item Observation $o$ comprises vision information obtained from the external environment, which is jointly represented by the human vision system and visual short-term memory (VSTM).
    \item Action $a$  involves specifying the coordinates $(x, y)$ of a particular position to move to.
    \item Reward $r$ is designed to encourage the gaze to reach the target with less cost. It takes into account the number of target hits as well as the cost associated with the distance of the gaze movement.
\end{itemize}

Our modeling of a chart reader's observation follows an idea similar to that in visual search~\cite{yang2020predicting}. Utilizing a representation for accumulating information through fixations, this employs four components: 
1) The foveal and peripheral view come from the human vision system, which receives high-resolution visual input only from the region of the image around the fixation location. It includes two pixel-based modules to read the chart: foveal and peripheral vision~\cite{duchowski2018gaze}). 
2) Visual saliency provides a bottom-up signal to a chart reader for the given task. The saliency of the chart affects gaze behavior. We use a task-driven saliency model to represent this feature ~\cite{wang2024salchartqa}.
3)  Visit history represents VSTM, which stores visual information for a few seconds, thereby allowing its use in ongoing cognitive tasks~\cite{alvarez2004capacity}. We represent this history through a matrix where each point is marked as visited or not.
4) A goal-related reference position serves as the initial starting point of gaze movement. For example, the reader might begin at the position of a text label for locating the associated graphical mark, where the position of the text label serves as the reference for the sub-goal. 
\rv{We use a one-hot matrix to represent the reference, in which all cell values are 0 apart from the single 1 that identifies the target.}
All these components are encoded together via the deep convolutional neural network, followed by a fully connected network.

We train reinforcement learning policies to solve the POMDP for the oculomotor control, because it has been proven to effectively address decision-making challenges in prediction of details of gaze movement~\cite{yang2020predicting, jiang2024eyeformer, shi2024crtypist, bai2024heads}.
In our detail-level implementation, we resize the input chart images to be $320 \times 320$ and discretize the fixation position into a $20 \times 20$ map. Consequently, each fixation becomes a $16 \times 16$ image patch, and the gaze position is randomly sampled from within that patch. In this setup, the maximum approximation error resulting from this discretization process is less than one degree of the visual angle~\cite{yang2020predicting}.
\rv{
Ultimately, both the scanpath and the image will be converted back to the original chart size from $320 \times 320$ pixels.
}

\subsection{\rv{Workflow}}
\label{sec:workflow}

\rv{
Our implementation of \name is trained and tested on a collection of tasks and charts. There are four steps, illustrated in Figure~\ref{fig:pipeline}.
In Step 1, real-world charts are manually collected and labeled for areas of interest (AOIs), while synthetic charts are automatically generated and labeled in a manner powered by Vega-Lite~\cite{satyanarayan2016vega}. The inclusion of synthetic charts helps increase the diversity of the chart collection and addresses the challenge of obtaining numerous annotated charts.
In Step 2, tasks are automatically generated in line with specific rules for the \textit{RV}, \textit{F}, and \textit{FE} tasks. These tasks and labeled charts constitute a data collection for the training environment.t
With Step 3, the policies for oculomotor control are trained through reinforcement learning (using proximal policy pptimization, PPO~\cite{schulman2017proximal}) to optimize gaze movements, enabling the system to reach task-relevant positions as quickly as possible while adhering to vision constraints. Importantly, no eye tracking data are required for PPO training.
In the last phase, prediction, the hierarchical architecture combines pre-trained LLMs (GPT-4o) for cognitive control with RL policies for oculomotor control to generate the scanpath prediction.
}

\begin{figure*}[!h]
\centering
  \includegraphics[width=\textwidth]{Images/pipeline.png}
  \caption{\rv{An overview of the training workflow: 1) chart collection and labeling, wherein diverse real-world and synthetic charts are gathered, involving manual and automatic annotation of AOIs; 2) task generation, utilizing a rule-based approach to create tasks based on labeled charts to construct a data collection for training; 3) policy training, in which policy models are trained via RL from chart images with tasks; and 4) scanpath prediction, wherein pre-trained LLMs and RL policies are coordinated hierarchically to predict task-driven gaze movements over charts.}}
  \label{fig:pipeline}
  % \vspace{-10mm}
\end{figure*}