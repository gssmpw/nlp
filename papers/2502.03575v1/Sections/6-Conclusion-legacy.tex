\section{Discussion}

% We discuss the main findings and implications from our study, followed by the limitations and future work of our work.

In this work, we present a computational model that simulates users eye movements on charts when solving visual analytical tasks. 
The results in Table~\ref{tab:benchmark} demonstrate that the model is able to accurately simulate human-like eye movements when performing analytical tasks. The predicted scanpaths closely match the spatial positions and temporal orders of human visual scanpaths. 
In addition, the model also shows similar behaviors observed in human data, such as the sequence of task-related focal points, the distribution of fixations on different areas of the chart, and the frequency of transitions and revisits.

The model benefits from the hierarchical control architecture~\cite{eppe2022intelligent}. This architecture involves two controllers with different abilities. The high-level controller sets goals based on memory, while the low-level controller is responsible for gaze control based on pixel-level observation. Large language models are well-suited for this hierarchical architecture due to their strong reasoning abilities at the high-level control.
By following the principle of modeling computationally rational behavior models~\cite{chandramouli2024workflow, oulasvirta2022computational}, we gain the advantage of training the model in a controlled environment instead of relying on human eye movement data. This approach saves us from the costly and time-consuming process of collecting eye tracking data.
In our current setup, the reward function is fairly straightforward and objective, as the basic analytical tasks are all based on value lookup. However, there may be a need for additional design considerations for subjective tasks, such as clustering the data or identifying the outliers. This may require extra effort to incorporate visualization knowledge into the training environment, especially in the context of the reward function.


\subsection{Limitations and Future Work}

\paragraph{Generalizability}
Currently, the model was primarily developed and tested using bar charts, the most frequently used chart type in analysis.
The training stage was purely in our synthetic chart environment, where human eye movement data were not involved.
Still, the quantitative and qualitative performance of our model showcases the closet to human data.
%To clarify, our approach and all these baselines never use human eye movement data on the testing charts in the dataset. Furthermore, other than other baseline approaches relying on prior human eye movement data in other datasets as the training set, no such eye movement data is included in the training of our approach.
However, the model's architecture is designed with the flexibility to support other types of charts. By expanding the diversity of synthetic data or incorporating labeled charts, we can extend the model's chart-wise capabilities.
To prove that, we tested the model's performance on a line chart by manually labeling all potential AOIs and training it with random tasks (e.g., asking the value for a randomly selected category). The trained model shows good performance when compared to the human scanpath on the task (Fig.~\ref{fig:case}). By expanding the synthetic environment to cover a wider range of charts through visualization engineering, we can develop models that can be tailored for specific uses on other types of charts in the future.

\begin{figure}[!h]
\centering
  \includegraphics[width=0.75\textwidth]{Images/case.png}
  \caption{A case illustrates the extension of our approach to a line chart.}
  \label{fig:case}
\end{figure}

\paragraph{Chart question answering}
Beyond scanpath prediction, our model, on the other hand, can answer questions using reasoning abilities from LLMs. However, its ability to answer questions is limited to the textual information present in the chart. For example, if a bar is located between two labeled values (10 and 15), the model may provide these numbers as an answer, but not the exact answer. 
While the model's main focus is not on answering questions based on chart data, it possesses the potential to enhance question-answering accuracy by improving spatial perception capabilities.
Some existing methods specifically designed for chart question answering perform well, as mentioned in~\cite{masry2022chartqa}. These methods utilize the full visual information from the chart as input, and the most recent work has achieved an accuracy of 81.3\% accuracy~\cite{cuarbune2024chart}, demonstrating the power of integration of multi-modal large language models.

\paragraph{Application}
One potential usage application of the model is to help with the evaluation and optimization of visualization design~\cite{shin2023perceptual}. Like other design disciplines, visualization design requires user feedback for continual iterations. When visualization designers are creating charts for specific tasks, they may wonder whether the design is good enough for delivery. With the predicted scanpaths from the model, they can easily access quick and affordable feedback before a design is evaluated in an expensive real user study.

% Discussion: It should return to talk about hierarchical control (that's the main contribution)




% In this work, we present a computational model for controlling eye movements when analyzing charts. 


% Usage case: The evaluation and optimization of task-driven visualization design.

%Theoratical assumptions
% Alternative way to build them
% Applicability

% \subsection{Optimal Viewing Sequence}

% \begin{itemize}
%     \item \textbf{Retrieve value}: ``A target item label has to be searched. Then, the value label of the target data point.''
%     \item \textbf{Filter}: ``Read one or multiple item labels of targets whose attribute values fulfill the given criteria.''
%     \item \textbf{Find Extreme}: ``Users may directly search for the data point without reading value labels if data entities are sorted according to the attributeâ€™s value in question.''
% \end{itemize}