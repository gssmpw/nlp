\section{Related Work}
\label{sec:related}

This section reviews the literature on eye tracking for information visualization, tasks in that domain, and preexisting techniques for scanpath prediction.


\subsection{Eye Tracking for Information Visualizations}

Eye tracking often serves as a proxy for visual perception in human analysis of information visualizations~\cite{shin2022scanner}.
There is a long history of applying eye tracking techniques to investigate how people perceive visualizations~\cite{huang2007using, borkin2015beyond, polatsek2018exploring, lalle2020gaze}.
For instance, \citet{huang2007using} examined the relations between eye movement events and visual components in node--link diagrams.
\citet{lalle2020gaze} analyzed the connection between gaze behavior and narrative visualizations, and
\citet{borkin2015beyond} found links between gaze behaviors over visual elements and the memorability of visualizations. For memorable visualizations, a quick look can already effectively convey the visualization's message.
Work by \citet{polatsek2018exploring}, demonstrating significant differences in gaze behaviors under three visual analysis tasks' conditions, attests well that people read completely different regions of charts when handling different tasks.
Other scholars have proposed eye-tracking-based approaches for improvements in visual analytics work such as word-sized visualizations~\cite{beck2015word} and interactive visualizations~\cite{nguyen2015interactive}.
However, while eye trackers have become cheaper and more readily available, the scale of eye tracking datasets is still limited because of the amounts of time and money needed~\cite{shin2022scanner}.
To address this limitation, researchers turned to crowdsourcing platforms as a faster, inexpensive alternative to eye tracking:
web cameras~\cite{shin2022scanner} and mouse clicks~\cite{wang2024salchartqa} have become popular ways to collect human attention data online.
These online alternatives sacrifice the quality of eye tracking data to gain quantity, leaving an open question of how to acquire both high quality and good scale of eye tracking data from information visualizations.

\subsection{Analytical Tasks in Visualizations}
Evidence exists that the task strongly influences how people design and explore visualizations~\cite{polatsek2018exploring, wu2020visact, shi2019task}.
\citet{amar2005low} identified 10 low-level analytical tasks (e.g., retrieving values and finding extremes) while another study~\cite{hibino1999task} highlighted abstract tasks such as background understanding, planning of analysis, and data exploration. 
Considering specific tasks is critical since visualizations can be created for handling any of the tasks in light of the input data, and also they can be evaluated in terms of how well certain tasks can be accomplished~\cite{schulz2013design}. 
To facilitate tasks related to visualizations, some researchers have designed visualizations for explicit displaying of data facts~\cite{srinivasan2018augmenting, shi2020calliope} -- such as showing trends or comparisons directly~\cite{shi2021autoclips, shi2024understanding}. 
Talk2data~\cite{guo2024talk2data} and Datamator~\cite{guo2023datamator} organize data facts related to specific tasks to facilitate question answering.
For this study, we adopted three analytical tasks from previous research~\cite{polatsek2018exploring} and conducted further analysis to understand how humans read charts for given tasks. 
Our analysis inspired us to develop the computational model for predicting task-driven scanpaths over charts.


\subsection{Scanpath Prediction}
In aims of predicting people's spatial and temporal viewing patterns upon exposure to certain stimuli, represented by a sequence of fixations, scholars have studied scanpaths with numerous stimulus types. Among these are natural scenes~\cite{coutrot2018scanpath, yang2020predicting}, web pages~\cite{drusch2014analysing}, graphical user interfaces~\cite{jokinen2020adaptive}, and information visualizations~\cite{wang2023scanpath}.
There is literature on scanpath prediction dedicated to sampling fixations from saliency maps\cite{brockmann2000ecology, boccignone2010gaze, kummerer2022deepgaze}. 
SaltiNet~\cite{assens2017saltinet} extended these maps to ``saliency volumes,'' from which sample scanpaths were created, while the models have drawn inspiration from cognitively plausible mechanisms, such as inhibition of return~\cite{itti1998model, sun2019visual} or foveal--peripheral saliency~\cite{wang2017scanpath, bao2020human}.
In HMM-based methods, in turn, the prediction either splits an image into several grids and regards each grid as a single state of observation~\cite{verma2019hmm} or classifies the fixations into several states~\cite{coutrot2018scanpath}.

The advent of deep learning brought new architectures into play for predicting scanpaths:
PathGAN's developers~\cite{assens2018pathgan} proposed using a generative adversarial network (GAN) for scanpath prediction, while 
Gazeformer~\cite{mondal2023gazeformer} encoded stimuli by using a natural-language model, then applied transformer-based modeling to predict visual scanpaths in a zero-shot setting.
For task-driven scanpath prediction, \citet{yang2020predicting} put inverse reinforcement learning to use to model human scanpaths during visual search, with 
\citet{chen2021predicting} likewise proposing an RL model to predict the scanpath during visual question answering.

Modeling scanpaths on information visualizations is challenging, given that viewing behaviors vary greatly across viewers~\cite{polatsek2018exploring}.
\citet{wang2023scanpath} highlighted the poor performance of prior scanpath prediction models with information visualizations, and they tackled this issue by fine-tuning a multi-duration saliency model~\cite{fosco2020predicting} for the information's graphical presentation and probabilistically sampling fixations from saliency maps.
However, their pioneering model for scanpath prediction in the visualization domain still cannot cope with task-specific scanpaths.
To fill the gap, we sought a task-driven model specifically designed to predict a human scanpath over information visualizations.