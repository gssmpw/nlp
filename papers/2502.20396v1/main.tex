\documentclass[conference]{IEEEtran}
\usepackage[T1]{fontenc} % Add this line
\usepackage[utf8]{inputenc} % Ensure UTF-8 input encoding

\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{capt-of}


\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{balance}
\usepackage{times}
\usepackage{dsfont}
\usepackage{siunitx}
\usepackage{cleveref}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{algorithmicx}
\usepackage{xspace}
\usepackage{xurl}
\usepackage[usenames,dvipsnames,table,xcdraw]{xcolor}
\definecolor{lightgray}{gray}{0.9}
\usepackage{booktabs}  % For better-looking horizontal lines
\usepackage{float} % Include this in the preamble
% \usepackage[noadjust]{cite}
\usepackage[flushleft]{threeparttable}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{arydshln} % Add this to the preamble

\usepackage{stfloats}

\definecolor{mygreen}{RGB}{0 205 0}
\definecolor{mybrown}{RGB}{139 69 19}

\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	urlcolor=mybrown,
	citecolor=mygreen,
}

\newcommand{\paragraphc}[1]{\vspace{0.2em}\noindent\textbf{#1}}
\newcommand{\paragrapht}[1]{\vspace{0.2em}\noindent\textit{#1}}


\pdfinfo{
   /Author (Toru Lin)
   /Title  (Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Reinforcement Learning; Dexterous Manipulation; Humanoids)
}

\begin{document}

\let\oldtwocolumn\twocolumn
\renewcommand\twocolumn[1][]{%
\oldtwocolumn[{#1}{
\vspace{-1em}
\begin{center}
    \includegraphics[width=\linewidth]{figures/fig1.pdf}
    \captionof{figure}{\textbf{Overview.} We train a humanoid robot with two multi-fingered hands to perform a range of contact-rich dexterous manipulation tasks on various objects. Observations are obtained from a third-view camera, an egocentric camera, and robot proprioception. The deployed reinforcement learning policies can adapt to a variety of unseen real-world objects that have varying physical properties (e.g., shape, size, color, material, mass) and remain robust against force disturbances.
    }
    \label{fig:teaser}
\end{center}
}]
}

\title{{\Huge Sim-to-Real Reinforcement Learning for \\ Vision-Based\hspace{-3pt} Dexterous\hspace{-3pt} Manipulation\hspace{-3pt} on\hspace{-3pt} Humanoids}}

\author{
\authorblockN{Toru Lin${}^{1,2}$, Kartik Sachdev${}^{2}$, Linxi ``Jim'' Fan${}^{2}$, Jitendra Malik${}^{1}$, Yuke Zhu${}^{2,3}$}
\vspace{0.1em}
\authorblockA{UC Berkeley${}^{1}$ \quad NVIDIA${}^{2}$ \quad UT Austin${}^{3}$\\
}
\vspace{0.1em}
{\texttt{\url{https://toruowo.github.io/recipe}}}
\vspace{-1em}
}

\maketitle

\begin{abstract}
Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:intro}
Deep reinforcement learning (RL) has delivered a number of impressive results during recent years, covering a diverse range of application domains: classical board games \cite{silver2017mastering}, competitive multiplayer online games \cite{berner2019dota,vinyals2019grandmaster}, large language models \cite{openai2024,deepseekai2025deepseekr1incentivizingreasoningcapability}, real-world robotic locomotion \cite{hwangbo2019learning,lee2020learning}, autonomous drone racing \cite{kaufmann2023champion} --- to name a few. These accomplishments have not only showcased RL's potential to achieve or surpass human-level performance across various tasks but also highlighted its distinctive ability to scale and generalize via autonomous exploration. Such inherent characteristics position RL as a performant and long-term approach to tackling tasks that are difficult to solve with human priors or demonstrations.

Motivated by its potential, we explore RL to address challenging dexterous manipulation tasks from vision. The successes that deep RL has produced in this problem domain remain limited so far. Previous works have demonstrated highly dexterous manipulation capabilities that could not be simply programmed or teleoperated by humans \cite{akkaya2019solving, handa2023dextreme, lin2024twisting}. However, these approaches are often tailored to a single manipulation skill, limiting their broad applicability.

What prevents RL from being more generally applicable to vision-based dexterous manipulation? We first investigate this by identifying the inherent properties of dexterous manipulation that differentiate this application domain from others. Then, we examine how these properties contribute to challenges in applying RL algorithms and develop a collection of novel techniques to address the challenges. Putting together our experiences and techniques, we outline a recipe for applying sim-to-real RL to vision-based humanoid manipulation tasks and show promising results. Below, we articulate the key challenges and our strategies to tackle them.

\paragraphc{Challenge in environment modeling.} The first challenge in applying RL to dexterous manipulation lies in the difficulty (or impossibility) of matching a simulated environment with the real environment. While real-world RL circumvents this problem, training with physical hardware is highly demanding regarding hardware quality, maintenance support, controller robustness, and safety. With a system as high-dimensional as a humanoid with multi-fingered hands, real-world exploration becomes even less tractable. In contrast, simulations offer unlimited chances of trial and error in a virtual sandbox, motivating the development of sim-to-real RL approaches. While previous successes in RL-based locomotion~\cite{haarnoja2024learning,hwangbo2019learning,lee2020learning,RealHumanoid2023} are encouraging, we observe that previous successes in dexterous manipulation involve much more laborious real-to-sim engineering efforts that are task-specific or hardware-specific ~\cite{akkaya2019solving,handa2023dextreme,lin2024twisting}. To better align simulation with the real world, we propose an automated real-to-sim tuning module that substantially reduces the engineering effort required for the environment modeling gap.

\paragraphc{Challenge in reward design.} While the reward function is commonly used as a general interface for specifying a task to train RL policies, it is notoriously hard to design generalizable rewards for manipulation tasks, especially for those that are contact-rich or long-horizon. Prior work often resorts to hand engineering based on the knowledge of human experts \cite{petrenko2023dexpbt,zakka2023robopianist}, which has limited scalability in the long run. This challenge differentiates manipulation from locomotion, where many tasks of interest can be specified with variations of the reward for a single ``walking'' task. We propose a general principle to design rewards for dexterous manipulation tasks: disentangle a full task into intermediate ``contact goals'' and ``object goals''. We use a novel keypoint-based state representation to specify contact goals. Following our reward design techniques, a task as long-horizon and contact-rich as bimanual handover can be learned with RL \textit{tabula rasa}.

\paragraphc{Challenge in policy learning.} A well-defined reward function does not guarantee the successful learning of RL policies due to the sample complexity and reward sparsity of exploring in a high-dimensional space. The variety and complexity of contact patterns in dexterous manipulation with multi-fingered hands further exacerbate the problem. Although unsupervised methods \cite{burda2018exploration,lin2024mimex,pathak2017curiosity} have been proposed to encourage exploration by favoring novel state visitations, they do not fundamentally reduce the difficulty of hard-exploration problems. We tackle this challenge by introducing two practical techniques: (1) initializing tasks with task-aware hand poses; (2) breaking down hard exploration problems into sub-tasks with much-reduced dimensionality, training expert policies on the sub-tasks, then distilling them into a generalist policy for the full task. We experimentally verify that these techniques improve sample efficiency of learning and study how different divide-and-conquer schemes vary in effectiveness.

\paragraphc{Challenge in object perception.} Compared to other robotic tasks, object perception is particularly important for manipulation because the task is inevitably coupled with interaction with objects. Object perception is a long-standing challenge because the variety of objects is uncountable in shapes, sizes, masses, colors, textures, and many other properties. Research in applying sim-to-real RL to dexterous manipulation is bottlenecked by this dilemma --- while object representations that are more expressive and information-dense can improve dexterity and capability of the learned policy, they also present a larger sim-to-real gap. To overcome this challenge, we propose to use a mixture of low-dimensional and high-dimensional object representations, with modality-specific data augmentation on the high-dimensional features to reduce the sim-to-real perceptual gap. We systematically study how this combination could help achieve a good balance between learning dexterous manipulation policy and reliably transferring the policy onto real robot hardware.

The strategies we outline above form a complete recipe of sim-to-real RL for vision-based dexterous manipulation. We show successful results of learning a collection of three dexterous manipulation tasks on humanoids and conduct systematic ablation studies.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\textwidth]{figures/fig2.pdf}
\end{center}
\caption{\textbf{A sim-to-real RL recipe for vision-based dexterous manipulation.} We close the environment modeling gap between simulation and the real world through an automated real-to-sim tuning module, design generalizable task rewards by disentangling each manipulation task into contact states and object states, improve sample efficiency of dexterous manipulation policy training by using task-aware hand poses and divide-and-conquer distillation, and transfer vision-based policies to the real world with a mixture of sparse and dense object representations.}
\label{fig:overview}
\vspace{-1em}
\end{figure*}


\section{Background}

\subsection{Deep Reinforcement Learning Applications to Robotics}

The successes of deep RL across a wide range of applications~\cite{openai2024,berner2019dota,deepseekai2025deepseekr1incentivizingreasoningcapability,hwangbo2019learning,kaufmann2023champion,lee2020learning,silver2017mastering,vinyals2019grandmaster} have sparked lots of excitement in recent years. However, works over the years have identified brittleness with this paradigm, most notably the sensitivity to hyperparameters~\cite{henderson2018deep} and questionable reproducbility~\cite{islam2017reproducibility} due to the high variance intrinsic to RL algorithms.

Among the open problems in RL, the most important and long-standing is exploration. In supervised learning, it is often assumed that data is given. In RL, however, agents need to collect their own data and update their policy based on the collected data. The problem of \textit{how} data is collected is known as the exploration problem.
Real-world robotics, with high-dimensional observations and dynamics and often sparse rewards, present a particularly challenging set of hard exploration problems for RL. While there have been works that algorithmically scale exploration to high-dimensional inputs by encouraging visitation to novel states~\cite{bellemare2016unifying,burda2018exploration,lin2024mimex,ostrovski2017count,pathak2017curiosity,stadie2015incentivizing, tang2017exploration}, they do not fundamentally resolve the exploration challenge.
Additionally, applying RL to solve real-world robotics also reveals important challenges that standard benchmarks in RL~\cite{bellemare2013arcade,tassa2018deepmind} fail to capture: (1) the lack of fully or accurately modeled environments; (2) the lack of well-defined reward functions for tasks of interest.

Past works in the intersection of robotics and RL have proposed various practical techniques to alleviate these problems, such as learning from human motion data or teleoperated demonstrations~\cite{chen2024object,rajeswaran2017learning,yin2025dexteritygen,zhu2018reinforcement}, real-to-sim techniques to model object and visual environments~\cite{akkaya2019solving,haarnoja2024learning,handa2023dextreme,lin2024twisting,torne2024rialto} and more principled ways to design rewards~\cite{memmel2024asid,zhang2024wococo}. While some of them overfit to specific tasks and settings, they point to promising directions upon which this work builds.

\subsection{Vision-Based Dexterous Manipulation on Humanoids}

\paragraphc{Imitation learning and classical approaches.}
Innovations in teleoperation~\cite{cheng2024open,lin2024learning,wu2023gello,zhao2023learning} and learning from demonstrations~\cite{chi2023diffusion,li2024planning} have brought about many recent advances in vision-based dexterous manipulation~\cite{cheng2024open,li2024planning,lin2024learning,zhao2024aloha}. However, in practice, onboarding teleoperators to collect high-quality dexterous manipulation data remains costly, and the performance scaling with data purely collected from real-world teleoperation~\cite{levine2018learning,lin2024data,zhao2024aloha} suggests that the cost to reach human-level performance could be prohibitively large.

\paragraphc{Reinforcement learning approaches.} A number of existing works have successfully applied RL to solve dexterous manipulation problems with multi-fingered hands, but either assume a single-hand setup~\cite{akkaya2019solving,chen2023sequential,handa2023dextreme,lum2024dextrah,qi2023general,singh2024dextrah,wang2024lessons} or do not use pixel inputs as object representation~\cite{chen2024object,huang2023dynamic,lin2024twisting}. Moreover, most of the existing works focus on a single manipulation skill, including in-hand reorientation~\cite{akkaya2019solving,handa2023dextreme,qi2023general,wang2024lessons}, grasping~\cite{lum2024dextrah,singh2024dextrah}, twisting~\cite{lin2024twisting}, and dynamic handover~\cite{huang2023dynamic}. The closest to our work is Chen et al.~\cite{chen2024object}, but their method relies on human hand motion capture data to learn a wrist controller rather than learning the full hand-arm joint control from scratch.
In addition, existing works often focus on hardware whose models in physics simulation have been more extensively tested. Our work is the first to show successful sim-to-real RL transfer of vision-based dexterous manipulation policies to a novel humanoid hardware with multi-fingered hands.

\section{Challenges and Approaches}

In Section~\ref{sec:intro}, we identify four areas of challenges in applying sim-to-real RL to dexterous manipulation and briefly describe our strategies to tackle the challenge in each area. Below, we describe our specific approaches in detail. Figure~\ref{fig:overview} shows an overview of the challenges and approaches.


\subsection{Real-to-Sim Modeling}
\label{sec:realsim}

Simulators offer unlimited trial-and-error chances to perform the exploration necessary for RL. However, whether policies learned in simulation can be reliably transferred to the real world heavily depends on the faithfulness of modeling --- both the robot itself and the environment. When applying sim-to-real RL to solve dexterous manipulation, this real-to-sim modeling problem is further exacerbated by the necessity to model objects, which have great variability and whose full physical properties cannot be easily quantified. Even when one assumes that the ground-truth physical parameters are known, quantitatively matching the simulation with the real world is hard: due to the limitations of physics engines, the same values for physical constants in simulation and the real world do not necessarily correspond to identical kinematic and dynamic relationships.

\paragraphc{Autotuned robot modeling.} While robot manufacturers are often able to provide proprietary model files for their robot hardwares, the models mostly serve a starting reference for robot real-to-sim effort rather than ground truth models that can be used without modifications. Empirical solutions to increase modeling accuracy range from hand-tuning the robot model constants and simulatable physical parameters~\cite{akkaya2019solving} to re-formulating specific kinematic structures (e.g., four-bar linkage) in the simulator of choice~\cite{radosavovic2024real}. This is a laborious process as there is no ``ground truth'' pairing between the real world and the simulated world.
We propose a practical technique to speed up this real-to-sim modeling process via an ``autotune'' module. The autotune module enables rapid calibration of simulator parameters to match real robot behavior by automatically searching the parameter space to identify optimal values for both simulator physics and robot model constants in under four minutes (or 2000 simulated steps in \SI{10}{\Hz}).
We illustrate the module in Figure~\ref{fig:overview}A and Algorithm~\ref{alg:autotune}.
The module operates on two parameter types: simulator physics parameters affecting kinematics and dynamics, as well as robot model constants from the URDF file (including link inertia values, joint limits, and joint/link poses). The calibration process begins by initializing multiple simulated environments using randomly sampled parameter combinations from the parameter space, bootstrapped from the manufacturer's robot model file. It then executes $N$ calibration sequences consisting of joint position targets on both the real robot hardware (single run) and all simulated environments in parallel. By comparing the tracking error between each simulated environment and the real robot when following the same joint targets, the module selects the parameter set that minimizes the mean squared error in tracking performance. This approach eliminates iterative manual tuning by requiring only one set of calibration runs on the real robot, automatically optimizing traditionally hard-to-tune URDF parameters, and supporting parallel evaluation of multiple parameter combinations. The method's generality allows it to tune any exposed simulator or robot model parameter that affects kinematic behavior.

\paragraphc{Approximate object modeling.} As demonstrated in previous works~\cite{lin2024twisting,qi2023hand}, modeling objects as primitive shapes like cylinders with randomized parameters is sufficient for sim-to-real transferrable dexterous manipulation policies to be learned. Our recipe adopts this approach and finds it effective.

\input{algo/autotune}


\subsection{Generalizable Reward Design}
\label{sec:reward}

In the standard formulation of RL~\cite{sutton1998introduction}, the reward function is a crucial element within the paradigm because it is solely responsible for defining the agentâ€™s behavior. Nevertheless, the mainstream of RL research has been preoccupied with the development and analysis of learning algorithms, treating reward signals as given and not subject to change~\cite{eschmann2021}. As tasks of interest become more general, designing reward mechanisms to elicit desired behaviors becomes more important and more difficult~\cite{dewey2014reinforcement} --- as is the case of applications to robotics. When it comes to dexterous manipulation with multi-fingered hands, reward design becomes even more difficult due to the variety of contact patterns and object geometries.

\paragraphc{Manipulation as contact and object goals.} From a wide variety of human manipulation activities~\cite{grauman2024ego}, we observe a general pattern in dexterous manipulation: each motion sequence to execute a task can be defined as a combination of hand-object contact and object states. Building on this intuition, we propose a general reward design scheme for even long-horizon contact-rich manipulation tasks. For each task of interest, we first break it down into an interleaving sequence of contact states and object states. For example, the handover task can be broken down into the following steps: (1) one hand contacting the object; (2) the object being lifted to a position near the other hand; (3) the other hand contacting the object; (4) object being transferred to the final goal position. The reward can then be defined based on solely the ``contact goals'' and ``object goals'': each contact goal can be specified by penalizing the distance from fingers to desirable contact points or simply the object's center-of-mass position; each object goal can be specified by penalizing the distance from its current state (e.g., current \textit{xyz} position) to its target state (e.g., target \textit{xyz} position). To reduce the difficulty of specifying contact goals, we propose a novel keypoint-based technique as follows: for each simulated asset, we procedurally generate a set of ``contact stickers'' attaching to the surface of the object, where each sticker represents a potentially desirable contact point. The contact goal, in terms of reward, can then be specified as 

\begin{equation}
    r_{\mathrm{contact}} = \sum_{i}\left[\frac{1}{1+\alpha d(\mathbf{X}^L, \mathbf{F}^L_i)} + \frac{1}{1+\beta d(\mathbf{X}^R, \mathbf{F}^R_i)}\right],
\end{equation}
where $\mathbf{X}^L\in\mathbb{R}^{n\times 3}$ and $\mathbf{X}^R\in\mathbb{R}^{m\times 3}$ are the positions of contact markers specified for left and right hands, $\mathbf{F}^L\in\mathbb{R}^{4\times 3}$ and $\mathbf{F}^R\in\mathbb{R}^{4\times 3}$ are the position of left and right fingertips, $\alpha$ and $\beta$ are scaling hyperparameters, and $d$ is a distance function defined as
\begin{equation}
    d(\mathbf{A}, \mathbf{x}) = \min_i \Vert\mathbf{A}_i - \mathbf{x}\Vert_2.
\end{equation}
We show a visualization of contact markers in Figure~\ref{fig:overview}B, and experimental results on their effectiveness in Section~\ref{sec:exp}.

\subsection{Sample Efficient Policy Learning}
\label{sec:policy}

Due to the sample complexity and reward sparsity in exploring a high-dimensional space --- especially on a humanoid embodiment with multi-fingered hands --- policy learning can take a prohibitively long time, even with a well-defined reward function. We propose two techniques that more effectively improve the sample efficiency of policy learning: (1) initializing tasks with task-aware hand poses; (2) dividing a challenging task into easier sub-tasks, then distilling the sub-task specialists into a generalist policy.

\paragraphc{Task-aware hand poses for initialization.} We reduce the exploration challenge by collecting task-aware hand pose data from humans. This can be done by connecting any teleoperation system for bimanual multi-fingered hands to the simulator of choice. The collected states, including object poses and robot joint positions, are then randomly sampled as task initialization states in simulation. Distinct from prior works that require full demonstration trajectories \cite{bauza2024demostart}, we find that teleoperators need not accomplish the task and only need to ``play around'' with the task goal in mind while environmental states are collected. The approach massively reduces the time needed for teleoperation since human operators do not need to spend time ``ramping up'' to collect high-quality data. In our experiments, each task requires less than 30 seconds for sufficient amount of hand pose data to be collected.

\paragraphc{Divide-and-conquer distillation.} Previous methods to improve sample efficiency of policy learning mostly focus on exploring the state space more efficiently \cite{burda2018exploration,lin2024mimex,pathak2017curiosity,taiga2019benchmarking}. However, these methods do not reduce the difficulty of the exploration problem fundamentally: the probability of receiving learning signals from exploring the ``right'' states remains the same. Following this observation, we reason that an easier way to overcome the exploration problem in sparse reward settings is to break down the explorable state space itself. For example, a multi-object manipulation task can be divided into multiple single-object manipulation tasks. After dividing a complex task into easier sub-tasks, we can train specialized policies for each sub-task and distill them into a generalist policy. Another benefit of this approach is that we can flexibly filter out trajectory data from the sub-task policies based on their optimality and only retain high-quality samples for training. This effectively brings RL closer to learning from demonstrations, where the sub-task policies act as teleoperators for task data collection in the simulation environment, and the generalist policy acts as a centralized model trained from curated data.

\subsection{Vision-Based Sim-to-Real Transfer}
\label{sec:vis}

Transferring a policy learned in simulation to the real world is challenging because of the sim-to-real gap. In the case of vision-based dexterous manipulation, the sim-to-real gap stems from both dynamics and visual perception --- both are challenging open research problems to solve. We outline two key techniques we employ to reduce the gap.

\paragraphc{Mixing object representations.} Object perception is crucial for dexterous manipulation because the task is inevitably coupled with object interaction. Prior works that show successful sim-to-real transfer of manipulation policies have explored a wide range of object representations, including (in order of increasing dimensionality and complexity) 3D object position \cite{lin2024twisting}, 6D object pose \cite{akkaya2019solving}, depth \cite{lum2024dextrah,qi2023general}, point cloud \cite{liu2024visual}, and RGB images \cite{handa2023dextreme}. There is a delicate trade-off between using these different object representations: while higher-dimensional representations encode richer information about the object, the larger sim-to-real gap in those data modalities makes the learned policy harder to be transferred; on the other hand, it is harder to learn optimal policies with lower-dimensional object representations because of the limited amount of information. We, therefore, propose a combination of both types of object representations to balance the trade-offs: a low-dimensional 3D object position and a high-dimensional depth image. Importantly, the 3D object position is obtained from a third-view camera to ensure the object is also in camera view and its noisy position can be consistently tracked. The depth image complements with information on object geometry. We provide more empirical validation on the effectiveness of this approach in Section~\ref{sec:exp}.

\paragraphc{Domain randomization for dynamics and perception.} We apply a wide range of domain randomizations to ensure reliable sim-to-real transfer. We list the details in Appendix~\ref{sec:dr}.

\section{Experiments}
\label{sec:exp}

Our proposed approaches form a general recipe that allows for the practical application of RL to solve dexterous manipulation with humanoids. In this section, we show experimental results of task capabilities and ablation studies of each proposed technique. Videos can be found on project website.

\subsection{Real-World and Simulator Setup}
We use a Fourier GR1 humanoid robot with two arms and two multi-fingered hands. Each arm has 7 degrees of freedom (DoF). For most experiments, we use the Fourier hands, each of which has six actuated DoFs and five underactuated DoFs. To show cross-embodiment generalization, we include results on the Inspire hands, each with 6 actuated DoFs and six underactuated DoFs. The hardware has substantially different mass properties, surface frictions, finger and palm morphologies, and thumb actuations. Figure~\ref{fig:teaser} shows a visualization of both robot hands.
We use the NVIDIA Isaac Gym simulator~\cite{makoviychuk2021isaac}.

\paragraphc{Perception.} As outlined in Section~\ref{sec:vis}, we use a combination of dense and sparse object features for policy learning in both simulation and real-world transfer. In the real world, we set up an egocentric-view camera by attaching a RealSense D435 depth camera onto the head of a humanoid robot and a third-view camera by putting another RealSense D435 depth camera on a tripod in front of the robot (illustrated in Figure~\ref{fig:teaser}). In simulation, we similarly set up an egocentric-view camera and a third-view camera by calibrating the camera poses against the real camera poses. The \textit{dense} object feature is obtained by directly reading depth observations from the egocentric-view camera. The \textit{sparse} feature is obtained by approximating the object's center-of-mass from the third-view camera, using a similar technique as in Lin et al.~\cite{lin2024twisting}. As illustrated in Figure~\ref{fig:overview}, we utilize the Segment Anything Model 2 (SAM2)~\cite{ravi2024sam} to generate a segmentation mask for the object of interest at the first frame of each trajectory sequence and leverage the tracking capabilities of SAM2 to track the mask throughout all remaining frames. To approximate the 3D center-of-mass coordinates of the object, we calculate the center position of their masks in the image plane, then obtain noisy depth readings from a depth camera to recover a corresponding 3D position. The perception pipeline runs at \SI{5}{\Hz} to match the neural network policy's control frequency.

\subsection{Task Definition}
\label{sec:task}

\paragraphc{Grasp-and-reach.} In this task, the robot needs to use one hand to grasp a tabletop object, lift it up, and move it to a goal position. At task initialization, a scripted visual module determines which hand is closer to the object and instructs the robot to use that hand. The test objects have varying geometric shapes, masses, volumes, surface frictions, colors, and textures; a visualization of all objects can be found in Figure~\ref{fig:teaser}. For each trial, we vary the initial position and orientation of objects, as well as the goal position.

\paragraphc{Box lift.} In this task, the robot needs to lift a box that is too large to be grasped with a single hand. Box colors, sizes, and masses are varied. For each trial, we randomize the boxes' initial position and orientation about the vertical axis.

\paragraphc{Bimanual handover.} In this task, the robot needs to grasp a block object from one side of the table with one hand --- that is too far to reach for the other hand --- and hand the object over to the other hand. Objects include blocks of varying colors, dimensions, and masses. We vary the initial position and orientation of blocks in each trial.


\begin{figure}[!t]
    \centering
    \vspace{-1em}
    \includegraphics[width=\linewidth]{figures/simtraj.pdf}
    \caption{\textbf{Policies learned in simulation.} Left: grasp; middle: box lift; right: bimanual handover (right-to-left, left-to-right).}
    \label{fig:sim}
\end{figure}

\subsection{Evaluation of Real-to-Sim Modeling}

\paragraphc{Effectiveness of autotuned robot modeling.}
We apply the autotune module outline in Section~\ref{sec:realsim} to find the optimal parameter set for robot modeling. To evaluate its effectiveness, we compare the sim-to-real transfer success rates of three sets of policy checkpoints. All policies are trained with identical task and RL settings, but each set of policies uses robot modeling parameters that achieve different MSE from autotune, ranging from lowest (i.e., smallest real-to-sim gap) to highest (i.e., highest real-to-sim gap). The quantitative results in Table~\ref{table:autotune} indicate that autotuned robot modeling improves sim-to-real transfer. Additionally, in our video, we show qualitative results of successful sim-to-real transfer of \texttt{grasp-and-reach} policies to the Inspire hands, demonstrating the generalizability of our autotune module.
\input{tables/autotune}

\paragraphc{Effectiveness of approximate object modeling.} Empirically, we find that modeling objects as primitive geometric shapes (cylinders, cubes, and spheres) strikes a good balance between training efficiency and sim-to-real transferability. In Figure~\ref{fig:objexp} (left), we compare the training curves of \texttt{grasp-and-reach} policies with primitive shapes against those with complex shapes, and the former setting is more sample efficient.
More importantly, policies trained with randomized primitive shapes can also generalize to a variety of unseen objects, as shown in our video.

\begin{figure}[!t]
    \centering
    % \vspace{-1em}
    \includegraphics[width=\linewidth]{figures/objexp.png}
    \caption{\textbf{Training \texttt{grasp-and-reach} policy with different object sets.} Each curve is computed from the statistics of 10 training runs with different random seeds. Left: training with complex objects v.s. simple geometric primitive objects. Right: training with differently grouped geometric objects.}
    \vspace{-0.1em}
    \label{fig:objexp}
\end{figure}

\subsection{Evaluation of Reward Design}

\paragraphc{Task capabilities.} With our proposed reward design principle, a wide range of long-horizon contact-rich tasks can be accomplished with pure reinforcement learning, as shown in Figure~\ref{fig:sim} and our video. The learned policies exhibit a high degree of dexterity and robustness against random force disturbances.

\paragraphc{Effectiveness of contact-based rewards.} In Figure~\ref{fig:contactexp}, we visualize different contact behaviors that emerged from different placements or contact markers, using the \texttt{box lift} task as an example. We find that contact behaviors align closely with the contact positions specified, showing the effectiveness of using contact markers to specify contact goals.


\begin{figure}[!t]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.95\linewidth]{figures/contactexp.pdf}
    \caption{\textbf{Different contact patterns emerge from different placements of contact markers.} Top: contact markers on the left and right side centers; middle: markers on the top and bottom side centers; bottom: markers on the bottom side edges.}
    \vspace{-1.3em}
    \label{fig:contactexp}
\end{figure}

\subsection{Evaluation of Policy Learning}

% sample efficiency

\paragraphc{Effectiveness of task-aware hand pose initialization.} In Table~\ref{table:humaninit}, we compare the percentage of successfully trained policies for each task with and without task-aware hand pose initialization. The empirical results show that having human priors upon initialization can greatly improve the exploration efficiency of hard RL tasks.

\paragraphc{Divide-and-conquer distillation.} We evaluate our divide-and-conquer distillation approach through two ablation studies. First, we study the effect of divide-and-conquer granularity on training efficiency. Specifically, we experiment with dividing a multi-object \texttt{grasp-and-reach} task into sub-tasks that handle different numbers of objects. Starting from a total of 10 objects, we experiment with three task designs: (1) training with all objects in one policy (\textit{all}); (2) training with three groups of similarly shaped objects in three policies (\textit{shape}); (3) training with three groups of differently shaped objects in three policies (\textit{mix}); (4) training with ten single-object policies (\textit{single}). As shown in Figure~\ref{fig:objexp}, sample efficiency is highest for \textit{single}, followed by \textit{shape}, \textit{all}, and \textit{mix}. There is also a noticeable difference in the average success rates of each task, which could be explained as an indicator of task difficulty. Interestingly, while training with a reduced object set all reaches the same performance, training all objects in one policy shows a consistently lower performance. Second, we study the sim-to-real transfer success rate of each type of policy. Over 30 trials of each policy on an in-distribution object, we find that sim-to-real performance of the \texttt{mix} policy is the highest (90.0\%), followed by \texttt{shape} (63.3\%), \texttt{single} (40.0\%), and \texttt{all} (23.3\%). Based on the qualitative behavior of policies, we hypothesize that the low success rates of \texttt{mix} and \texttt{single} policies stem from overfitting to specific geometries, and that of \texttt{all} policy correlates with its worse performance during RL training. These results suggest that divide-and-conquer distillation helps achieve a good balance between policy training and sim-to-real transfer performance.


\input{tables/humaninit}

\subsection{Evaluation of Vision-Based Sim-to-Real Transfer}

\paragraphc{Effectiveness of mixing object representations.} We investigate the effect of using different object representations and show the sim-to-real transfer comparisons in Table~\ref{table:objrep}. These results suggest combining dense object representation (segmented depth image) and sparse object representation (3D object center-of-mass position) improves sim-to-real transfer. Notably, the gap between the success rate of the depth-and-position policy and that of the depth-only policy increases as knowledge of the full object geometry becomes more crucial to task success.

\input{tables/objrep}


\subsection{System Capabilities}

\paragraphc{Task performance, generalization, robustness.} We evaluate the overall capabilities of our system via task success rates of the best-performing task policy. For each task, we conduct 10 trials for each test object and report the average success rate across all objects. We report a 62.3\% success rate for the \texttt{grasp-and-reach} task, 80\% for the \texttt{box lift} task, and 52.5\% for the \texttt{bimanual handover} task. We test our \texttt{grasp-and-reach} policy's ability to generalize to out-of-distribution objects and report qualitative results of successful zero-shot generalization in our video. We also show the robustness against force perturbations of our learned policies for all tasks in Figure~\ref{fig:robust} and video. More details on the object set for each task are reported in Figure~\ref{fig:teaser}.

\begin{figure}[!t]
    \centering
    \vspace{-1em}
    \includegraphics[width=\linewidth]{figures/robustness.pdf}
    \caption{\textbf{Policy robustness.} Our learned policies remain robust under different force perturbations, including knock (top left), pull (top right), push (bottom left), and drag (bottom right).}
    \vspace{-1em}
    \label{fig:robust}
\end{figure}


\paragraphc{Extension to a more capable system.}
The learned RL policies can be flexibly chained with finite state machines, teleoperation, etc., to perform longer-horizon tasks while maintaining dexterity, robustness, and generalization. As an example, we present in our video a general pick-and-drop system that can be scripted by utilizing the \texttt{grasp-and-reach} policy.

\section{Limitations} 
\label{sec:limitations}

In this work, we investigate the key challenges in applying RL to robot manipulation and introduce practical and principled techniques to overcome the hurdles. Based on the techniques proposed, we build a sim-to-real RL pipeline that demonstrates a feasible path to solve robot manipulation, with evidence on generalizability, robustness, and dexterity.

However, the capabilities achieved in this work are still far from the kind of ``general-purpose'' manipulation that humans are capable of. Much work remains to be done to improve each individual component of this pipeline and unlock the full potential of sim-to-real RL.
For example, the reward design could be improved by integrating even stronger human priors, such as task demonstrations collected from teleoperation.

There are also important open problems that our work does not address. For example, our work uses no novel technique to reduce the sim-to-real gap in dynamics other than applying naive domain randomization. We hypothesize that this could be a reason for the low success rate on \texttt{bimanual handover} task, which is the most dynamic among our collection of tasks.

Lastly, we find ourselves heavily constrained by the lack of reliable hardware for dexterous manipulation. While we use multi-fingered robot hands, the dexterity of these hands is far from that of human hands in terms of the active degrees of freedom.
We believe the dexterity of our learned policies is not limited by the approach, and we hope to extend our framework to robot hands with more sophisticated designs in the future.

\section{Conclusion} 
\label{sec:conclusion}

We present a comprehensive recipe for applying sim-to-real RL to vision-based dexterous manipulation on humanoids. By addressing key challenges in environment modeling, reward design, policy learning, and sim-to-real transfer, we show that RL can be a powerful tool for learning highly useful manipulation skills without the need for extensive human demonstrations. Our learned policies exhibit strong generalization to unseen objects, robustness against force disturbances, and the ability to perform long-horizon contact-rich tasks.

\clearpage

\section*{Acknowledgments}

We thank members of NVIDIA GEAR lab for help with hardware infrastructure, in particular Zhenjia Xu, Yizhou Zhao, and Zu Wang. This work was partially conducted during TL's internship at NVIDIA. TL is supported by NVIDIA and the National Science Foundation fellowship.

%% Use plainnat to work nicely with natbib. 
\bibliographystyle{plainnat}
\bibliography{references}
% \input{main.bbl}

\clearpage

\appendix

\subsection{Environment Modeling Details}

\paragraphc{Modeling underactuated joints.}
Since modeling underactuated joint structure is not directly supported, we approximate the relationship between each pair of actuated and underactuated joints by fitting a linear function $q_{u} = k \cdot q_{a} + b$, where $q_u$ is the underactuated joint angle and $q_a$ is the actuated joint angle. Note that parameters $k, b$ are included as tunable parameters to search over using our autotune module detailed in Section~\ref{sec:realsim}.

\subsection{Reward Design Details}

We design generalizable rewards based on the principle outlined in Section~\ref{sec:reward} and list task reward details below.

Both \textbf{grasp} and \textbf{lift} tasks can be defined with the following goal states: (1) finger contact with the object; (2) the object being lifted up to a goal position. Our reward design can, therefore, follow by combining the contact goal reward and the object goal reward terms:
\begin{equation}
r(s_h,s_o) = r_{contact}(s_h, s_o) + r_{goal}(s_o)
\end{equation}
where $s_h$ includes fingertip positions, $s_o$ includes object center-of-mass position, and all contact marker positions (if any).

Similarly, the \textbf{handover} task can be defined with the following goal states: (1) one hand's finger contact with the object; (2) object being transferred to an intermediate goal position while still in contact with the first hand; (3) the second hand's finger contact with the object; (4) object being transferred to the final goal position. Due to the hand switching, we introduce a stage variable $a \in \{0,1\}$ and design the reward as follows:
\begin{equation}
\begin{split}
r(s_h,s_o) & = (1 - a) \cdot ( r_{contact}(s_{h_A}, s_{o_A}) + r_{goal}(s_{o_A})) \\
& + a \cdot (r_{contact}(s_{h_B}, s_{o_B}) + r_{goal}(s_{o_B}))
\end{split}
\end{equation}
where $s_{h_A}, s_{h_B}$ denote fingertip positions of the engaged hand at each stage, $s_o$ denote object center-of-mass position and desirable contact marker positions (if any) at each stage. At completion of each stage, we also reward policy with a bonus whose scale increases as stage progresses.

\subsection{Policy Training Details}

\paragraphc{RL implementation.}
To learn the specialist policies, the observation space includes object position and robot joint position at each time step, and the action space is robot joint angles. We use Proximal Policy Optimization~\cite{schulman2017proximal} with asymmetric actor-critic as the RL algorithm. In addition to the policy inputs, we provide the following privilege state inputs to the asymmetric critic: arm joint velocities, hand joint velocities, all fingertip positions, object orientation, object velocity, object angular velocity, object mass randomization scale, object friction randomization scale, and object shape randomization scale. Both the actor and critic networks are 3-layer MLPs with units $(512,512,512)$.

\paragraphc{Domain randomization.}
\label{sec:dr}
Physical randomization includes the randomization of object friction, mass, and scale. We also apply random forces to the object to simulate the physical effects that are not implemented by the simulator. Non-physical randomization models the noise in observation~(e.g., joint position measurement and detected object positions) and action. A summary of our randomization attributes and parameters is shown in Table~\ref{table:dr}.
 
\begin{table}[!t]
\renewcommand\arraystretch{1.05}
\caption{Domain Randomization Setup.}
\centering
\begin{tabular*}{\linewidth}{l@{\extracolsep{\fill}}c}
\toprule
Object: Mass~(kg)             & [0.03, 0.1]    \\
Object: Friction              & [0.5, 1.5]     \\
Object: Shape                 & $\times\mathcal{U}(0.95, 1.05)$     \\
Object: Initial Position~(cm) &  $+\mathcal{U}(-0.02, 0.02)$ \\
Object: Initial $z$-orientation & $+\mathcal{U}(-0.75, 0.75)$ \\
Hand: Friction                & [0.5, 1.5]    \\
\midrule
PD Controller: P Gain         &  $\times\mathcal{U}(0.8, 1.1)$      \\
PD Controller: D Gain         &  $\times\mathcal{U}(0.7, 1.2)$     \\
\midrule
Random Force: Scale           & 2.0       \\
Random Force: Probability     & 0.2    \\
Random Force: Decay Coeff. and Interval & 0.99 every 0.1s     \\ 
\midrule
Object Pos Observation: Noise & 0.02      \\
Joint Observation Noise.      & $+\mathcal{N}(0, 0.4)$     \\
Action Noise.                 & $+\mathcal{N}(0, 0.1)$   \\
\midrule
Frame Lag Probability         & 0.1 \\
Action Lag Probability        & 0.1 \\
\midrule
Depth: Camera Pos Noise~(cm)      & 0.005  \\
Depth: Camera Rot Noise~(deg)      & 5.0  \\
Depth: Camera Field-of-View~(deg)  & 5.0  \\
\bottomrule
\end{tabular*}
\label{table:dr}
\end{table}

\subsection{Distillation Details}

To learn the generalist policy, we reduce the choices of observation inputs to the robot joint states and selective object states, including 3D object position and egocentric depth view, since privileged information is unavailable for sim-to-real transfer. To more efficiently utilize the trajectory data and improve training stability, for each sub-task specialist policy, we evaluate for 5000 steps over 100 environments, saving trajectories filtered by success at episode reset on the hard disk. We then treat the saved data as ``demonstrations'' and learn a generalist policy for each task with Diffusion Policies~\cite{chi2023diffusion}.

The proprioception and object position states are concatenated and passed through a three-layer network with ELU activation, hidden sizes of $(512,512,512)$, and an output feature size of $64$. For depth observations, we use the ResNet-18 architecture~\cite{he2016deep} and replace all the BatchNorm~\cite{ioffe2015batch} in the network with GroupNorm~\cite{wu2018group}, following~\cite{chi2023diffusion}. All the encoded features are then concatenated as the input to a diffusion model. We use the same noise schedule (square cosine schedule) and the same number of diffusion steps (100) for training as in \cite{chi2023diffusion}.
The diffusion output from the model is the normalized 7 DoF absolute desired joint positions of each humanoid arm and the 6 DoF normalized ($0$ to $1$) desired joint positions of each humanoid hand. We use the AdamW optimizer~\cite{kingma2014adam, loshchilov2017decoupled} with a learning rate of $0.0001$, weight decay of $0.00001$, and a batch size of 128. Following \cite{chi2023diffusion}, we maintain an exponential weighted average of the model weights and use it during evaluation/deployment.

\end{document}


