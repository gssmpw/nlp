\section{Method} \label{sec:method}
The objective of GCE-Pose is to estimate the 6D object pose and size from RGB-D data. Given a single RGB-D frame and the category instance mask, we obtain the partial RGB observation $\mathbf{I}_{\text{partial}}$ and its corresponding partial point cloud $\mathbf{P}_{\text{partial}}$ derived from the depth map. Utilizing $\mathbf{I}_{\text{partial}}$ and $\mathbf{P}_{\text{partial}}$, the objective is to recover the 3D rotation \( \mathbf{R} \in \text{SO}(3) \), the 3D translation \( \mathbf{t} \in \mathbb{R}^3 \), and the size \( \mathbf{s} \in \mathbb{R}^3 \) of the target object.

GCE-Pose consists of four main modules (\cref{fig:pipeline_overview}): Robust Partial Feature Extraction (\cref{subsec:keypoint_feature}), Semantic Shape Reconstruction (\cref{subsec:dlssr}), Global Context Enhanced Feature Fusion (\cref{subsec:gce}), and Pose \& Size Estimator (\cref{subsec:pse}).

\subsection{Robust Partial Feature Extraction}
\label{subsec:keypoint_feature}
Partial observations from RGB-D sensors often contain significant noise and incomplete geometry, making dense correspondence prediction unreliable. We address this challenge with a keypoint-based approach~\cite{lin2024instance} that focuses on the most discriminative and reliable object regions.

The $N$ input points are put in order within $\mathbf{P}_{\text{partial}} \in \mathbb{R}^{N \times 3}$ and we extract point features $\mathbf{F}_{\text{P}} \in \mathbb{R}^{N \times C_1}$ using PointNet++ \cite{qi2017pointnetplusplus}. For the RGB image $\mathbf{I}_\text{\text{partial}}$, we extract the image feature $\mathbf{F}_{\text{I}} \in \mathbb{R}^{N \times C_2}$ using DINOv2~\cite{oquab2023dinov2} and concatenate $\mathbf{F}_{\text{I}}$ to $\mathbf{F}_{\text{P}}$ to obtain $\mathbf{F}_{\text{partial}} \in \mathbb{R}^{N \times C}$.
We follow AG-Pose~\cite{lin2024instance} for keypoint detection. First, $M$ keypoint features are extracted using a learnable embedding $\mathbf{F}_{\text{emb}} \in \mathbb{R}^{M \times C}$, which undergoes cross-attention with $\mathbf{F}_{\text {partial}}$ to attend to critical regions in $\mathbf{P}_{\text{partial}}$. This process yields a feature query matrix $\mathbf{F}_{\mathrm{q}} = \text{CrossAttention}(\mathbf{F}_{\text{emb}}, \mathbf{F}_{\text{partial}})$. We then compute correspondences via cosine similarity, forming a matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$, and select $M$ keypoints from $\mathbf{P}_{\text{partial}}$ as $\mathbf{P}_{\mathrm{kpt}} = \operatorname{softmax}(\mathbf{A}) \mathbf{P}_{\text{partial}}$.
To ensure keypoints lie on the object surface and minimize outliers, an object-aware Chamfer distance loss \(\mathcal{L}_{\text{ocd}}\) is applied. With ground truth pose $\mathbf{T}_{\text{gt}}$, we filter outliers by comparing each point $x \in \mathbf{P}_{\text{partial}}$ to the instance model $\mathbf{M}_{\text{obj}}$:
\begin{equation}
\vspace{-0.2cm}
\min_{y \in \mathbf{M}_{\text{obj}}} \left\| \mathbf{T}_{\text{gt}}(x) - y \right\|_2 < {\tau_1},
\label{equ:outlier_threshold}
\end{equation}
where ${\tau_1}$ is an outlier threshold. The object-aware Chamfer distance loss is then:
\begin{equation}
\vspace{-0.2cm}
\mathcal{L}_{\text{ocd}} = \frac{1}{|\mathbf{P}_{\text{kpt}}|} \sum_{x \in \mathbf{P}_{\text{kpt}}} \min_{{y} \in \mathbf{P}_{\text{partial}}^*} \|{x} - {y}\|_2.
\vspace{-0.1cm}
\label{equ:ocd}
\end{equation}
To prevent keypoints from clustering, a diversity regularization loss is added:
\begin{equation}
\vspace{-0.1cm}
\mathcal{L}_{\text{div}} = \sum_{x \neq y \in \mathbf{P}_{\text{kpt}}} \max \{ 0, {\tau}_2 - \|x - y\|_2 \},
\vspace{-0.1cm}
\label{equ:div}
\end{equation}
where ${\tau}_2$ controls keypoint distribution.
To enhance features with geometric context, the Geometric-Aware Feature Aggregation (GAFA) module~\cite{lin2024instance} is applied. GAFA augments each keypoint with (1) local geometric details from K-nearest neighbors and (2) global information from all keypoints, improving feature discriminability for correspondence estimation.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{pic/Recon.pdf}
    \vspace{-0.6cm}
    \caption{Illustration of Deep Linear Semantic Shape Model. A Deep Linear Semantic Shape model is composed of a prototype shape $c$, a scale network $\mathcal{S}$, a deformation network $\mathcal{D}$, a Deformation field $\mathcal{V}$ and a category-level semantic features $c^k_\text{sem}$. At stage 1, we build a Deep Linear Shape (DLS) model using sampled point clouds from all ground truth instances within each category, training a linear parameterization network to represent each instance. At stage 2, we retrain the DLS model to regress the corresponding DLS parameters from partial point cloud inputs using a deformation and scale network. During testing, the network predicts DLS parameters for unseen objects and reconstructs their point clouds based on the learned deformation field to get semantic reconstruction.}
    \vspace{-0.3cm}
    \label{fig:pipeline_deep_lineaer}
\end{figure*}

\subsection{Semantic Shape Reconstruction} \label{subsec:dlssr}
Intra-class variation is a key challenge in category-level pose estimation. To tackle this issue, category-level shape priors have been extensively used in object pose estimation. By representing the shape with mean shapes and deformations~\cite{tian2020shape} or learning implicit neural representations for geometry recovery~\cite{irshad2022shapo, irshad2022centersnap}, pose estimators can better learn correspondences in NOCS space, benefiting from accurate shape priors. While geometric shape reconstruction provides valuable priors, it cannot fully capture the rich semantic information of object parts. Recent advances in 2D foundation models, particularly DINO~\cite{oquab2023dinov2}, have demonstrated remarkable capabilities in extracting zero-shot semantic information from single RGB images. Building upon this insight, we propose \textbf{Semantic Shape Reconstruction (SSR)} to learn a per-category linear shape model similar to \cite{loiseau2021representing} that describes an object using instance-specific geometry and category-level semantic features.


\noindent\textbf{Deep Linear Semantic Shape Model.}  
To overcome the challenges posed by partial observations from depth sensors, such as occlusions and incomplete geometry, we employ a variation of the deep linear shape model \cite{loiseau2021representing}. This approach is motivated by the need to robustly and efficiently parameterize object shapes with shape parameters and produce a completed 3D object representation, even when faced with limited input data. We represent each point in our model as a tuple $(x, f)$ where $x \in \mathbb{R}^3$ represents a spatial coordinate and $f \in \mathbb{R}^C$ represents its semantic feature vector. For $I$ points of an object instance within category $k$, we learn a linear shape model. The model for category $k$ consists of (i) a geometric prototype ${c}^k \in \mathbb{R}^{I \times 3}$ with associated semantic features ${c}^{k}_{\text{sem}} \in \mathbb{R}^{I \times C}$, (ii) a set of geometric deformation basis vectors ${v}^k = \{{v}_1^k, \ldots, v_D^k\}$ where ${v}_i^k \in \mathbb{R}^{I \times 3}$, and (iii) a scale parameter vector ${s^k} \in \mathbb{R}^3$.
The key insight of our approach is that semantic features remain coupled to their corresponding points during geometric deformation. Any semantic shape $\mathbf{U}_{\text{k}}$ in the model family is defined by:
\begin{equation}
\vspace{-0.1cm}
% \label{small}
\mathbf{U}_{\text{k}} = (\mathbf{X}_{\text{k}}, \mathbf{F}_{\text{k}})
= \left( s^k \odot (c^k + \sum_{i=1}^D a^k_i v^k_{i}), c^k_{\text{sem}} \right)
\vspace{-0.0cm}
\label{equ:dls_familiy}
\end{equation}
where $\mathbf{X}_{\text{k}} \in \mathbb{R}^{I \times 3}$ are the $I$ points in shape prior $k$ and $\mathbf{F}_{\text{k}} \in \mathbb{R}^{I \times C}$ their associated features.
The shape parameter vector is given by ${a^k} = \left(a^k_1, \ldots, a^k_D\right) \in \mathbb{R}^D$, ${s^k} \in \mathbb{R}^3$ controls scaling, and $\odot$ defines the element-wise Hadamard product.
We train two neural networks for each category $k$ to predict shape parameter $a^k$ with network $\mathcal{D}^k$ and scale $s^k$ with $\mathcal{S}^k$.

To optimize the model, we minimize the Chamfer distance loss, $\mathcal{L_{\text{CD}}}$, which ensures accurate shape reconstruction through:
\begin{equation}
\vspace{-0.1cm}
\mathcal{L_{\text{CD}}} = \sum_{x \in \mathbf{P}} \min_{k} d\left(x, \mathbf{U}_{\text{k}}\right),
\label{equ:l_cd}
\vspace{-0.1cm}
\end{equation}
with the Chamfer distance $d$, ground truth point clouds $\mathbf{P}$ from category $k$ and shape reconstruction $\mathbf{U}_{\text{k}}$ defined in \cref{equ:dls_familiy} . Training with ground truth yields the optimal parameters $\bar{a}^k$, $\bar{s}^k$, $c^k$, and $v^k$ which allow to formulate an additional loss to refine shape reconstruction under partial observations $\mathbf{P}_{\text{partial}}$ by freezing $c^k$, and $v^k$ within
\begin{equation}
\vspace{-0.1cm}
\small
\mathcal{L}_{\text {para}} = \sum_{x' \in \mathbf{P}_{\text{partial}}} \lambda_1 \left| \mathcal{D}^k(x') - \bar{a}^k \right| + \lambda_2 \left| \mathcal{S}^k(x') - \bar{s}^k \right|.
\label{equ:l_para}
\vspace{-0.1cm}
\end{equation}
Finally, we combine the reconstruction and parameter loss to formulate the overall loss
\begin{equation}
\mathcal{L}_{\text{rec}} = \lambda_{\text{CD}} \cdot \mathcal{L}_{\text{CD}} + \lambda_{\text{para}} \cdot \mathcal{L}_{\text{para}},
\label{equ:l_rec}
\end{equation}
where $\lambda_{\mathrm{CD}}$ and $\lambda_{\text {para }}$ are the hyperparameters that weight the contributions of $\mathcal{L}_{\mathrm{CD}}$ and $\mathcal{L}_{\text {para }}$, respectively.

\noindent{\textbf{Semantic Prototype Construction.}}
To effectively integrate rich semantic information into our 3D shape reconstruction, we employ a process that begins by extracting dense semantic features from multiple RGB images of each object instance using the DINOv2 ~\cite{oquab2023dinov2}. For each object instance without texture, we position multiple virtual cameras around the object to capture RGB images and depth maps from diverse viewpoints. This setup ensures full coverage of the object's surface and mitigates occlusion effects. The RGB images are processed through the DINOv2 model to extract dense 2D semantic feature maps.  Using the corresponding depth maps and known camera intrinsics and extrinsic, we project the 2D semantic features into 3D space. For each pixel $(u, v)$ in the image, we compute its 3D position ${P}$ using the depth value $z$ and project the associated semantic feature $\mathbf{f}_{2 \mathrm{D}}(u, v)$ to this point ${P}=z K^{-1}[u, v,1]^T,$, where $K$ is the camera intrinsic matrix. As a result, we obtain a dense semantic point cloud $\mathbf{F}_{\text{sem}}$. To ensure computational efficiency and point-wise correspondence, we downsample this dense semantic point cloud to $I$ points aligned with our geometric reconstruction. For each point $P_i$ in the deep linear shape reconstruction, we aggregate semantic features from its k nearest neighbors in the dense cloud:
\begin{equation}
\vspace{-0.1cm}
\mathbf{F}_{\text{instance}}(P_i) = \frac{1}{k}\sum_{P_j \in N_k(P_i)} \mathbf{F}_{\text{sem}}(P_j).
\vspace{-0.1cm}
\end{equation}
The category-level semantic prototype $c^k_{\text{sem}}$ is then constructed by averaging $N$ instance features across the category $k$ while maintaining point-wise correspondence with the geometric prototype $c^k$:
\begin{equation}
\vspace{-0.1cm}
c^k_{\text{sem}} = \frac{1}{N}\sum_{i=1}^N \mathbf{F}_{\text{instance}}(P^k_i)
\vspace{-0.1cm}
\label{equ:knn}
\end{equation}

\noindent{\textbf{Semantic Reconstruction.}}
The key advantage of our approach is that semantic reconstruction becomes straightforward once the semantic prototype is established. Given a partial point cloud $x'$, we first reconstruct its geometry and then directly inherit the semantic feature from the prototype.

\subsection{Global Context Enhanced Feature Fusion} \label{subsec:gce}
Traditional pose estimation methods rely primarily on partial observations, but they often struggle with challenges such as occlusion and viewpoint variations. To overcome these limitations, we propose a \textbf{Global Context Enhanced (GCE)  Feature Fusion} module that effectively integrates complete semantic shape reconstructions with partial observations, establishing robust feature correspondences.

Firstly, we aim to extract global features from our semantic reconstruction. Given partial observation $\mathbf{P}_{\text{partial}}$, we can reconstruct the shape \( \mathbf{P}_{\text{global}}\) using \cref{equ:dls_familiy}. We leverage PointNet++~\cite{qi2017pointnetplusplus} to obtain a geometry feature from $ \mathbf{P}_{\text{global}}$, then concatenate it with category-level semantic feature $c_{\text{sem}}$ to obtain the global feature $\mathbf{F}_{\text{global}} \in \mathbb{R}^{I \times C}$.

Given keypoint Features \( \mathbf{F}_{\text{kpt}} \in \mathbb{R}^{M \times C} \) and global feature \(\mathbf{F}_{\text{global}} \in \mathbb{R}^{I \times C} \), our goal is to enrich keypoint features with global semantic context. The primary challenge lies in bridging the domain gap between partial observations, captured in noisy camera-space coordinates, and the global reconstruction, represented in normalized object-space coordinates with complete shape information.

To fuse the partial feature $\mathbf{F}_{\text{kpt}}$ with $\mathbf{F}_{\text{global}}$, we transform both the partial and global features by concatenating learnable positional embedding network that maps these 3D  positions \( \mathbf{P}_{\text{kpt}}\) and \( \mathbf{P}_{\text{global}}\). into high-dimensional positional tokens, respectively. 
\begin{equation}
\small
\mathbf{F}_{\text{kpt}}' = \text{concat}(\mathbf{F}_{\text{kpt}}, \text{PE}_{\text{kpt}}), \quad \mathbf{F}_{\text{global}}' = \text{concat}(\mathbf{F}_{\text{global}}, \text{PE}_{\text{global}})
\end{equation}
where \text{PE} is positional encoding network.
We then apply an attention mechanism to merge both sources of information, where the global features provide the semantic context for refining the keypoint features.
We first project keypoint features and global features into a shared embedding space:
\begin{equation}
    \mathbf{F}_{\text{kpt}}'' = \text{LayerNorm}(\text{MLP}_{\text{proj}}(\mathbf{F}_{\text{kpt}}'))
\end{equation}
\begin{equation}
    \mathbf{F}_{\text{global}}'' = \text{LayerNorm}(\text{MLP}_{\text{proj}}(\mathbf{F}_{\text{global}}'))
\end{equation}
then, the global context enhancement is aggregated through cross-attention and residual connection:
\begin{equation}
    \mathbf{F}_{\text{context}} = \text{CrossAttn}(\mathbf{F}_{\text{kpt}}'', \mathbf{F}_{\text{global}}'')
\end{equation}
\begin{equation}
    \mathbf{F}_{\text{gce}} = \mathbf{F}_{\text{kpt}} + \mathbf{F}_{\text{context}}
\end{equation}
After fusing the keypoint $\mathbf{F}_{\text{kpt}}$ feature with our global feature $\mathbf{F}_{\text{global}}$. The resulting global context enhanced keypoint feature $\mathbf{F}_{\text{gce}}$ are then passed through the self-attention module and MLP following \cite{lin2022category} to predict the corresponding NOCS coordinates $\mathbf{P}_{\text{kpt}}^{\text{nocs}}$.

To ensure that our keypoints and associated features effectively represent the partial observation \( \mathbf{P}_{\text{partial}} \), we additionally employ a reconstruction module to recover its 3D geometry. This module takes keypoint positions and features as input, applies positional encoding to the keypoints, and refines their features through a MLP. The encoded and refined features are aggregated, and a shape decoder predicts reconstruction deltas to recover the geometry. The reconstruction loss is defined as the object-aware Chamfer distance (CD) between the partial observation \( \mathbf{P}_{\text{partial}} \) and the reconstructed point cloud \( \mathbf{P}_{\text{recon}} \) following \cref{equ:outlier_threshold}:
\begin{equation}
\mathcal{L}_{\text{rec}}=\frac{1}{\left|\mathbf{P}_{\text{recon}}\right|} \sum_{x \in \mathbf{P}_{\text{recon}}} \min _{y \in \mathbf{P}_{\text{partial}}^{\star}}\left\|x-y\right\|_2 .
\label{equ:loss_rec}
\end{equation}

\subsection{Pose Size Estimator} \label{subsec:pse}
Given the NOCS coordinates of keypoints, \(\mathbf{P}_{\text{kpt}}^{\text{nocs}} \in \mathbb{R}^{M \times 3}\), the enhanced keypoint features \(\mathbf{F}_{\text{gce}}\) and the position of keypoint $\mathbf{P}_{\text{kpt}}$, we can establish keypoint-level correspondences, which are then used to regress the final pose and size parameters, \(\mathbf{R}\), \(\mathbf{t}\), and \(\mathbf{s}\). The process is formulated as follows:
\begin{equation}
\vspace{-0.1cm}
\mathbf{f}_{\text{pose}} = \operatorname{concat} \left[ \mathbf{P}_{\text{kpt}}, \mathbf{F}_{\text{gce}}, \mathbf{P}_{\text{kpt}}^{\text{nocs}} \right]
\end{equation}
\begin{equation}
\small
\left( \mathbf{R}, \mathbf{t}, \mathbf{s} \right) =
\left( \operatorname{MLP}_R \left( \mathbf{f}_{\text{pose}} \right), \operatorname{MLP}_t \left( \mathbf{f}_{\text{pose}} \right), \operatorname{MLP}_s \left( \mathbf{f}_{\text{pose}} \right) \right)
\end{equation}

For the rotation representation \(\mathbf{R}\), we utilize the 6D continuous representation proposed in \cite{Zhou_2019_CVPR}. For the translation \(\mathbf{t}\), we adopt the strategy from \cite{Zheng_2023_CVPR} by predicting the residual translation between the ground truth and the mean position of the point cloud.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{pic/vis_bbox_housecat_WiderSpace.pdf}
    \vspace{-0.6cm}
    \caption{Visualization of category-level object pose estimation results on HouseCat6D dataset~\cite{jung2024housecat6d}. Predicted 3D bounding boxes are shown in red, with ground truth in green. Challenging cases are highlighted in pink side squares. Leveraging our global context-enhanced pose prediction pipeline, GCE-Pose outperforms the SOTA AG-Pose~\cite{lin2024instance} (DINO), demonstrating robustness to occlusions and strong generalization to novel instances.
}
    \vspace{-0.2cm}
    \label{fig:vis_housecat_pred}   % what about using single column version
\end{figure*}

\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|c c|c c c c | c c} % 'l' aligns text to the left
\hline
\hline
\textbf{Dataset} & \textbf{Method} & \textbf{Shape Prior} & \textbf{Semantic Prior} & \textbf{5°2cm} & \textbf{5°5cm} & \textbf{10°2cm} & \textbf{10°5cm}  & \textbf{IoU50} & \textbf{IoU75}\\
\hline

\multirow{4}{*}{\centering HouseCat6D~\cite{jung2024housecat6d}} 
% \multirow{6}{*}{\begin{tabular}[l]{@{}l@{}} \textit{Housecat6D}\\ \end{tabular}} 
% & GPV-Pose~\cite{di2022gpv}   &  &  & 50.7 & - & 3.5  & 4.6  & 17.8 & 22.7  \\
 & VI-Net~\cite{lin2023vi}     &  &  & 8.4  & 10.3 & 20.5 & 29.1 & 56.4 & - \\
 & SecondPose~\cite{chen2024secondpose} &  &   & 11.0 & 13.4 & 25.3 & 35.7  & 66.1 & - \\
 & AG-Pose~\cite{lin2024instance} &  &  & 11.5 & 12.0 & 32.7 & 35.8 & 66.0 & 45.0   \\
 & AG-Pose (DINO)~\cite{lin2024instance}  &  &  & 21.3 & 22.1 & 51.3 & 54.3 & 76.9 & 53.0  \\

 & \textbf{GCE-Pose} (Ours) & \(\checkmark\) & \(\checkmark\)  
 & \textbf{24.8} & \textbf{25.7} & \textbf{55.4} & \textbf{58.4} & \textbf{79.2} & \textbf{60.6} \\
\hline
\hline
\multirow{8}{*}{\centering NOCS-REAL275~\cite{nocs}}   
% & NOCS~\cite{nocs}            &  & & 78.0 & 30.1 & 7.2  & 10.0 & 13.8 & 25.2 \\
% & GPV-Pose~\cite{di2022gpv}        &  &  & -    & 64.4 & 32.0 & 42.9 & -    & 73.3     \\
% & Query6DoF~\cite{query6dof}    & &   & 49.0   & 58.9 & 68.7 & 83.0 & 82.5 & 76.1   \\

& SecondPose~\cite{chen2024secondpose}     &  &        & 56.2 & 63.6 & 74.7 & 86.0  & -    & -   \\
& AG-Pose~\cite{lin2024instance}      &  &     & 56.2 & 62.3 & 73.4 & 81.2  & 83.8 & 77.6 \\
& AG-Pose(DINO)~\cite{lin2024instance}    &  &     & \textbf{57.0} & 64.6 & 75.1 & 84.7  & \textbf{84.1} & \textbf{80.1}\\
\cdashline{2-10} 

% & SPD~\cite{tian2020shape} & \(\checkmark\) &  & 77.3    &  53.2 & 19.3    & 21.4 &  43.2    & 54.1     \\
% & SGPA~\cite{chen2021sgpa} & \(\checkmark\) &   & 80.1    &  61.9 & 35.9 &39.6 &61.3 &70.7     \\
& RBP-Pose~\cite{zhang2022rbp} & \(\checkmark\) &   & 38.2 & 48.1 & 63.1 & 79.2   & -    &   67.8   \\
& DPDN~\cite{lin2022category} & \(\checkmark\) &    & 46.0 & 50.7 &  70.4    & 78.4    & 83.4    &  76.0 \\
& Query6DoF~\cite{query6dof}  & \(\checkmark\) &   & 49.0   & 58.9 & 68.7 & 83.0 & 82.5 & 76.1   \\
& GS-Pose~\cite{wang2025gs}         & \(\checkmark\) & \(\checkmark\)  & -    & 28.8 & -    & 60.1   & -    & 63.2  \\
&\textbf{GCE-Pose} (Ours)  & \(\checkmark\) & \(\checkmark\) & \textbf{57.0} & \textbf{65.1} & \textbf{75.6} &
\textbf{86.3}  & \textbf{84.1} & 79.8\\
\hline
\hline
\end{tabular}%
}
\vspace{-0.2cm}
\caption{Quantitative comparison of category-level object pose estimation on HouseCat6D and NOCS-REAL275 datasets.}
\vspace{-0.5cm}
\label{tab:merged_results}
\end{table*}


\subsection{Overall Loss Function} \label{subsec:pse}
The overall loss function for pose estimation is as follows:
\begin{equation}
\mathcal{L}_{\text{all}} = \lambda_1 \mathcal{L}_{\text{ocd}} + \lambda_2 \mathcal{L}_{\text{div}} + \lambda_3 \mathcal{L}_{\text{rec}} + \lambda_4 \mathcal{L}_{\text{nocs}} + \lambda_5 \mathcal{L}_{\text{pose}}
\label{equ:overall_loss}
\end{equation}
where $\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_5$ are hyperparameters to balance the contribution of each term. For $\mathcal{L}_{\text{pose}}$, we use:
\begin{equation}
\mathcal{L}_{\text{pose}} = \|\mathbf{R}_{\text{gt}} - \mathbf{R}\|_{\rm F} + \|\mathbf{t}_{\text{gt}} - \mathbf{t}\|_2 + \|\mathbf{s}_{\text{gt}} - \mathbf{s}\|_2.
\end{equation}

We generate ground truth NOCS coordinates of keypoints $\mathbf{P}_{\text{kpt}}^{\text{gt}}$ by projecting their coordinates under camera space $\mathbf{P}_{\text{kpt}}$ into NOCS using the ground-truth \(\mathbf{T}_{\text{gt}} = \left( \mathbf{R}_{\text{gt}}, \mathbf{t}_{\text{gt}}, \mathbf{s}_{\text{gt}} \right)\). For $\mathcal{L}_{\text{nocs}}$, we use the Smooth $L_1$ loss with
%\begin{equation}
%P_{\text{kpt}}^{\text{gt}} = \frac{1}{\|s_{\text{gt}}\|_2} R_{\text{gt}} (P_{\text{kpt}} - t_{\text{gt}}),
%\end{equation}
%\begin{equation}
%\mathcal{L}_{\text{nocs}} = \text{SmoothL}_1 (P_{\text{kpt}}^{\text{gt}}, P_{\text{kpt}}^{\text{nocs}}).
%\end{equation}
\begin{equation}
\mathcal{L}_{\text{nocs}} = \| \mathbf{T}_{\text{gt}}(\mathbf{P}_{\text{kpt}}^{\text{gt}}) - \mathbf{P}_{\text{kpt}}^{\text{nocs}} \|_{\text{SL1}}
\end{equation}
