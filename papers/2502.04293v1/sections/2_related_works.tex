\section{Related Works} \label{sec:related_works}
\subsection{Object Reconstruction for Pose Estimation}
Object reconstruction is essential for object pose estimation when CAD models are unavailable, as it captures object geometry and appearance while establishing a canonical space. Methods like OnePose~\cite{sun2022onepose}, OnePose++~\cite{he2022oneposeplusplus}, and CosyPose~\cite{labbe2020cosypose} employ Structure from Motion (SfM) to match features across views, while approaches such as NeRFPose~\cite{li2023nerf}, GS-Pose~\cite{cai2024gs}, and FoundationPose~\cite{wen2024foundationpose} utilize Neural Radiance Fields or 3D Gaussian Splatting~\cite{kerbl20233dgaussian} for flexible reconstruction.

For known object categories, semantic information can enhance instance-level reconstructions. Some methods build semantic representations directly: Goodwin~\textit{et al.}~\cite{goodwin2022zero} align 3D views to a query view, and Zero123-6D~\cite{di2024zero123} synthesizes views via diffusion models to reduce reference views. I2cNet~\cite{i2c} expands such techniques to categories by integrating a 3D mesh reconstruction module. Other methods use shape priors, such as SPD~\cite{tian2020shape}, RePoNet~\cite{fu2022category}, and Wang~\textit{et al.}~\cite{wang2021category}, which learn instance reconstructions from category-specific priors. SGPA~\cite{chen2021sgpa} and RBP~\cite{zhang2022rbp} dynamically adapt priors based on observed structures, while SAR-Net~\cite{sar} and ACR-Pose~\cite{fan2021acrpose} further incorporate geometric and adversarial strategies. GS-Pose~\cite{wang2025gs} projects DINOv2~\cite{oquab2023dinov2} features onto a 3D reference shape, aiding feature alignment and pose prediction. From here, we propose a novel method that further improves the aforementioned methods by integrating semantics to the reconstructions to provide global contextual information for pose estimation.

 
\subsection{Representation Learning for Pose Estimation}
Learning effective feature representations from input modalities is crucial to pose estimation, evolving alongside advancements in vision neural networks. Early visual feature extractors relied on CNN backbones~\cite{xiang2017posecnn,kehl2017ssd,sundermeyer2018implicit,peng2019pvnet,zakharov2019dpod,labbe2022megapose} to predict or refine object poses from single RGB images. Recently, foundational models like DINOv2 have been widely adopted~\cite{nguyen2024gigapose,ornek2025foundpose,ausserlechner2024zs6d,chen2024secondpose,lin2024sam} to enhance robustness and contextual understanding.

Beyond 2D-only approaches, many methods now combine 2D image and 3D point cloud networks to jointly extract semantic and geometric features, constructing robust embeddings for tasks such as direct pose regression~\cite{wang2019densefusion,he2021ffb6d,chen2024secondpose,he2020pvn3d} or feature matching~\cite{huang2024matchu,caraffa2025freeze,lin2024sam}. RGB-D methods address feature fusion at multiple levels, such as 2D-3D and local-global fusion. Methods like DenseFusion~\cite{wang2019densefusion} and PVN3D~\cite{he2020pvn3d} concatenate per-pixel geometric and RGB features, while SecondPose~\cite{chen2024secondpose} employs MLPs to fuse DINOv2~\cite{oquab2023dinov2} features with Point Pair Features (PPF)~\cite{drostppf}.

More recent transformer-based approaches, including SAM6D~\cite{lin2024sam} and MatchU~\cite{huang2024matchu}, integrate both local RGB-D and global CAD model features~\cite{qin2023geotransformer,yu2023rotation,yu2024riga}, demonstrating the value of cross-modality fusion. In this work, we not only incorporate categorical semantic priors into object reconstruction but also effectively integrate global context into local embeddings through our fusion module.

Occlusions and object symmetries introduce visual ambiguities~\cite{manhardt2019explaining}, necessitating the consideration of multiple correct poses. To address this, various methods frame pose prediction as distribution estimation, learning ambiguity-aware representations~\cite{haugaard2023spyropose,vutukur2025alignist,stablepose}. We tackle the occlusion challenge by employing a completion task that enables the network to reason about full geometric representations despite missing points.

\subsection{Generalizing Object Pose Estimators} 
The constraints of 3D model-based pose estimation have been relaxed to category-level estimation, where the task is to predict the pose of an unknown instance within a known category (e.g., a fork in "cutlery") on benchmarks like NOCS~\cite{nocs}, PhoCal~\cite{wang2022phocal}, and HouseCat6D~\cite{jung2024housecat6d}. 

Category-level pose estimation aims to predict 9DoF poses for novel instances within specified categories. Wang~\textit{et al.}~\cite{nocs} introduced the Normalized Object Coordinate Space (NOCS) framework, mapping observed point clouds to a canonical space with pose recovery via the Umeyama algorithm~\cite{umeyama1991least}. Subsequent methods improve accuracy~\cite{chen2020learning,Chen_2021_CVPR,DualPoseNet,di2022gpv,Zheng_2023_CVPR,lin2024instance, zhang2024generative}. Some works adopt prior-free methods, such as VI-Net~\cite{lin2023vi}, which separates rotation components, and IST-Net~\cite{liu2023istnet}, which transforms camera-space features implicitly. AG-Pose~\cite{lin2024instance} achieves state-of-the-art results by learning keypoints from RGB-D without priors. In contrast, we incorporate learned priors for geometry and semantics to complete partial observations and map mean shape semantics onto observed instances.

Self-supervised approaches are also popular in category-level estimation, refining models without annotated real data. CPS++~\cite{manhardt2020cpspp} uses a differentiable renderer to adapt synthetic models with real, unlabeled RGB-D inputs, while other works~\cite{wang2022crocps,wang2024improving} tackle photometric challenges using RGBP polarization~\cite{gao2022polarimetric,ruhkamp2024s} and contextual language cues~\cite{wang2024improving}. Self-DPDN~\cite{lin2022category} employs a shape deformation network for self-supervision, while our approach leverages a categorical shape prior without network refinement.

In open-vocabulary settings, POPE~\cite{fan2024pope} introduces promptable object pose, (H)Oryon~\cite{corsetti2024open,corsetti2024high} use vision-language models and stereo matching, while NOPE~\cite{nguyen2024nope} and SpaRP~\cite{xu2025sparp} predict pose distributions or relative NOCS-maps. These methods often treat pose estimation as correspondence matching~\cite{corsetti2024open,fan2024pope} or reconstruction~\cite{nguyen2024nope}. We instead deform a mean shape within an absolute category space to capture instance-specific correspondences.
