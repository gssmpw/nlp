\section{Experiment} \label{sec:experiment}
\begin{figure*}[t] % 't' option to place at the top
    \hspace*{\fill} % Push the figure to the right
    \begin{minipage}[t]{1.0\textwidth} % Half-width of the text
        \centering
        \includegraphics[width=\textwidth]{pic/visualization_all.pdf} % Adjust file name
        \vspace{-0.5cm}
        \caption{(A) shows the visualization of input partial points and the output semantic shape reconstructions; (B) visualizes the semantic prototypes of different categories and the aggregated instance semantics.}
        \vspace{-0.4cm}
        \label{fig:vis_intra_variance}
    \end{minipage}
\end{figure*}

\subsection{Implementation Details}
For the HouseCat6D dataset \cite{jung2024housecat6d}, cropped images are resized to 224 $\times$ 224 for feature extraction, and 1024 points are sampled from inputs. For Partial Feature Extraction, the number of keypoints is $M = 96$, and the feature dimensions for geometric and DINO features are $C_1 = 128$, $C_2 = 128$, and $C = 256$. In deep linear shape reconstruction, we set the basis dimension $D$ to 5, and the number of points in the prototype is 1024. The pose estimation network is trained with batch size 36 with an ADAM \cite{Kingma2014AdamAM} optimizer with a triangular2 cyclical learning rate schedule \cite{smith2017cyclical} on a single NVIDIA 4090 GPU for 150 epochs. We attach more implementation details in our Appendix.

\subsection{Evaluation Benchmarks} 
\noindent{\textbf{Datasets.}} We evaluate our method on two challenging real-world benchmarks: HouseCat6D~\cite{jung2024housecat6d} and NOCS-REAL275~\cite{nocs}. HouseCat6D contains 21K images of 194 household instances across 10 categories, with 2,929 images of 50 instances reserved for evaluation, covering diverse shapes, occlusions, and lighting. NOCS-REAL275 consists of 7K images across 6 object categories and 13 scenes, with 4.3K images used for training and 2,750 for testing. We compare our method against baselines on both datasets with the same setup for segmentation mask and conduct the ablation studies on HouseCat6D.
\noindent{\textbf{Evaluation Metrics.}} Following prior work~\cite{lin2024instance, chen2024secondpose, lin2023vi, di2022gpv}, we evaluate performance with two metrics:
{$n^{\circ} m$ cm} and \textit{3D IoU}. $n^{\circ} m$ cm metric computes mean Average Precision (mAP) for rotation and translation accuracy, considering predictions correct if the rotation error is within $n^{\circ}$ and translation error within $m$ cm.
\textit{3D IoU} is a  mAP-based metric that assesses 3D bounding box IoU with thresholds at $50\%$ and $75\%$, capturing both pose and object size.
\vspace{-0.2cm}
\subsection{Comparison with the State-of-the-Art} 
\label{subsec:comp_sota}
\noindent{\textbf{Results on HouseCat6D.}} Table~\ref{tab:merged_results} presents GCE-Pose’s performance on the HouseCat6D dataset, where it outperforms state-of-the-art methods across all metrics against the existing approaches. Compared to AG-Pose, even strengthened with DINOv2~\cite{lin2024instance}, our method shows $16\%$ improvement on the most strict 5°2cm metric and notable gains on other metrics, demonstrating the efficacy of integrating global context priors into the local feature-based pose estimation pipeline. Against SecondPose~\cite{chen2024secondpose}, which fuses PPF geometric and DINOv2 semantic features from only the partial observations, our method achieves more than $30\%$ improvement in IoU75 metric and over $100\% $ in {$n^{\circ} m$ cm} metrics. Qualitative comparisons of our method against AG-Pose are shown in Figure~\ref{fig:vis_housecat_pred}.

\noindent{\textbf{Results on NOCS-REAL275.}} Table~\ref{tab:merged_results} also shows GCE-Pose’s performance on NOCS-REAL275 dataset, achieving the highest scores on most metrics. GCE-Pose surpasses prior-free baselines by integrating robust global context priors into a strong pose estimator, proving the benefits of our design. We also compare with methods using categorical priors: RBP-Pose~\cite{zhang2022rbp}, Query6DoF~\cite{query6dof} and DPND~\cite{lin2022category}, which rely solely on shape priors, while GS-Pose~\cite{wang2025gs} uses both shape and semantic priors with a single-instance reference. In contrast, GCE-Pose learns robust deformation priors across multiple instances and completes partial inputs into a full semantic shape, outperforming all prior-based methods and by more than $23\%$ on 5°2cm metric, demonstrating the effectiveness of our SSR and GCE fusion modules.

% \begin{table}[t]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{l|c c|c c} % 'l' aligns text to the left
% \hline
% \textbf{Method} & \textbf{Geo. Prior} & \textbf{Sem. Prior} & \textbf{5°2cm} & \textbf{5°5cm}  \\
% \hline
% % AG-Pose            & \(\times\) & \(\times\) & 66.0 & 45.0 \\
% \textit{(0)}AG-Pose (DINO)   & \(\times\) & \(\times\) & 21.3 & 22.1            \\
% \textit{(1)} Ours (Ins. Geo.)       & \(\checkmark\) & \(\times\) & 22.2    & 23.7\\
% \textit{(2)} Ours (Mean Sem.)       & \(\times\) & \(\checkmark\) & 22.7  & 24.3         \\
% \textit{(3)} Ours (Ins. Sem) & \(\checkmark\) & \(\checkmark\) & 23.4 & 24.2        \\
% \textit{(4)} GCE-Pose        & \(\checkmark\) & \(\checkmark\) & 24.8 & 25.7      \\
% \hline
% \end{tabular}%
% }
% \vspace{-0.3cm}
% \caption{Ablation study on different global priors.}
% \vspace{-0.6cm}
% \label{tab:method_dcomparison}
% \end{table}

\begin{table}[ht]
    \centering
    % \renewcommand{\arraystretch}{1.2}
    \vspace{-0.2cm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c}
        \hline
       \textbf{ Method} & \textbf{Ins. recon.} & \textbf{Mean shape} & \textbf{Geo.} & \textbf{Sem.} & $\textbf{5°2cm}$ & $\textbf{5°5cm}$  \\
        \hline
        (0) AG-Pose DINO (Baseline) & \(\times\) & \(\times\)& \(\times\) & \(\times\) &  21.3 & 22.1\\
        (1) Ours (Instance Geo.) & \(\checkmark\) & \(\times\)& \(\checkmark\) & \(\times\)& 22.2    & 23.7 \\
        (2) Ours (Categorical Sem.) & \(\times\) & \(\checkmark\)& \(\times\) & \(\checkmark\)& 22.7  & 24.3  \\
        (3) Ours (Mean shape Geo. \& Categorical Sem.) & \(\times\) & \(\checkmark\)& \(\checkmark\) & \(\checkmark\)&  23.4 & 24.2 \\
        (4) GCE-Pose (Full pipeline) & \(\checkmark\) & \(\checkmark\)& \(\checkmark\) & \(\checkmark\) &  \textbf{24.8} & \textbf{25.7} \\
        \hline
    \end{tabular}
    }
    \vspace{-0.2cm}
    \caption{\footnotesize Ablation study on different global priors.
    % Ins. recon: instance shape reconstruction as the prior; Mean shape: mean shape of the categories as the prior; Geo.: geometric features from the global prior; Sem.: semantic features for the global prior. % (can be removed when compressing)
    }
    \vspace{-0.6cm}
    \label{tab:method_dcomparison}
\end{table}

\subsection{Ablation Studies}
To show the efficacy of our design choices, we conduct exhaustive ablation experiments on the HouseCat6D dataset.
\noindent\textbf{Effects of Global Context Priors.}
We evaluate the effect of global context priors on the pose estimation backbone by comparing different global prior configurations, including: 
\textit{(0) Baseline.} AG-Pose with the DINOv2 backbone, excluding SSR and GCE fusion modules (no global shape or semantic priors). 
\textit{(1) Instance geometric prior.} Geometric features alone are passed into the GCE fusion module, omitting semantic information.
\textit{(2) Categorical semantic prior.} The shape reconstruction module is excluded, and the categorical semantic prior is fused with local features.
\textit{(3) Mean shape geometric \& Categorical semantic prior.} The SSR module applies mean shape reconstruction and categorical semantic features.
\textit{(4) GCE-Pose (Full pipeline).} Our full method combines instance-specific geometric and categorical semantic priors.
As shown in Table~\ref{tab:method_dcomparison}, \textit{(1)} yields a $4\% $ improvement on the 5°2cm metric over the baseline, \textit{(2)} yields $7\%$, and \textit{(3)} achieves $10\%$. Our method (5), combining instance geometric and semantic priors as global context guidance, reaches the highest performance and surpasses the baseline by $16\%$. 

\noindent{\textbf{Robustness of Semantic Shape Reconstruction.}}
Qualitative results of our semantic shape reconstruction are shown in Figure~\ref{fig:vis_intra_variance}. Specifically, (a) shows partial input points and their reconstructed points with semantic features, illustrating the robustness of our SSR module to noisy and occluded scenarios. (b) displays the categorical semantic prototype where the 3D semantics are lifted from DINO features. We also show the instances where the semantics are aggregated from our Deep Linear Semantic Shape Models. We visualize the semantic features by sharing the PCA centers, demonstrating our semantic feature aggregation is robust against shape variance.

\noindent{\textbf{Effects of the GCE Feature Fusion Module.}}
In Table~\ref{tab:gce_fusion}, we evaluate our GCE feature fusion by experimenting with different DINOv2 tokens. Our results indicate that using the key tokens to aggregate the global semantic prior yields better pose estimation performance than using value tokens. We attribute this improvement to the design of our cross-attention layer where the local features serve as the query and global semantic prior as the key and value. This setup allows the global semantic prior to initialize the attentional weights between local and global cues effectively, enhancing the feature fusion for pose estimation.
\vspace{-0.2cm}


\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c c c c|c c } % 'l' aligns text to the left
\hline
\textbf{Fusion features} &   \textbf{5°2cm} & \textbf{5°5cm} & \textbf{10°2cm} & \textbf{10°5cm} & \textbf{IoU50} & \textbf{IoU75}\\
\hline
Value feature   & 21.9 & 23.2 & 48.9 & 53.1 & 76.0 & 55.2  \\
Key feature (Ours)  & \textbf{24.8} & \textbf{25.7} & \textbf{55.4} & \textbf{58.4} & \textbf{79.2} & \textbf{60.6} \\
\hline
\end{tabular}%
}
\vspace{-0.2cm}
\caption{Ablation study on different feature fusion strategies.}
\vspace{-0.6cm}
\label{tab:gce_fusion}
\end{table}
