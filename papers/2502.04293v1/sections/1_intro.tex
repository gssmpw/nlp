\section{Introduction}
\label{sec:intr}
The task of object pose estimation varies according to generalization level and input modality. \textit{Instance-level pose estimation} methods~\cite{Wang_2021_GDRN,su2022zebrapose,hodan2024bop} focus on specific object instances that does not generalize to other objects, while \textit{fully unseen object pose estimation} methods~\cite{drost2010model,shugurov2022osop,huang2024matchu} are designed to handle novel objects, however requires object model as prior. Unlike aforementioned methods, \textit{category-level pose estimation} methods~\cite{nocs,lin2022category,chen2024secondpose} aim to generalize across unseen instances within a defined category that requires only an RGB(-D) image of a new instance during the inference, making the method model-free that does not require predefined object models.

Current category-level approaches primarily estimate the Normalized Object Coordinate Space (NOCS)~\cite{nocs} and employ a pose solver, such as the Umeyama algorithm~\cite{umeyama1991least}, to obtain the object pose~\cite{lin2022category}. To effectively extract category-level features from RGB (and/or depth) inputs, researchers have developed various neural network architectures to capture features from partial RGB(-D) observations. Some recent methods~\cite{ornek2025foundpose,nguyen2024gigapose,ausserlechner2024zs6d} leverage foundation models like DINOv2~\cite{oquab2023dinov2} for improved performance. Additionally, research~\cite{huang2024matchu,chen2024secondpose,caraffa2025freeze,lin2024sam} highlights the importance of combining semantic and geometric information to enhance feature robustness and distinguishability, aiding in better correspondence and pose estimation. However, category-level pose estimation being model-free and having only partially observed RGB(-D) inputs limits the extraction of global context information.
Some methods~\cite{tian2020shape,query6dof,chen2021sgpa,zhang2022rbp,lin2022category} have introduced categorical geometric shape priors to reconstruct instance models from partial input points, solving for object pose by establishing dense correspondence between partial input points and reconstructed models. However, these methods solely introduce shape priors neglecting the semantic context of the category. More recently, GS-Pose~\cite{wang2025gs} selects one instance as a reference prototype within a category and applies semantic feature matching between partial points and the reference instance. However, this design struggles with intra-class shape variations and is particularly vulnerable to noise in partial point cloud observations. 

In this work, we propose GCE-Pose, a novel approach that integrates global context incorporating both geometric and semantic cues to enhance category-level object pose estimation. We propose two major modules named Semantic Shape Reconstruction (SSR) and Global Context Enhanced (GCE) feature fusion modules to facilitate pose estimation. The SSR module is a first-complete-then-aggregate strategy that reconstructs the input partial points into a complete shape and smoothly aggregates the semantic prototype to the instance. The GCE feature fusion model is proposed to effectively fuse the reconstructed global context with local cues. The efficacy of our proposed method is confirmed by extensive evaluation on the challenging real-world datasets, achieving SOTA performance against the existing approaches. Our main contributions are as follows:

\begin{itemize}
    \item We propose GCE-Pose, a Global Context Enhancement (GCE) approach that integrates global context with both geometric and semantic cues for category-level object pose estimation.
    
    \item We introduce a Semantic Shape Reconstruction (SSR) strategy that addresses partially observed inputs by reconstructing both object geometry and semantics through learned categorical deformation prototypes.
    
    \item Extensive experiments demonstrate that our method achieves robust pose estimation even under significant shape variations and occlusions improving the generalization to unseen instances.
\end{itemize}

