
\input{sections/table}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/roots_negation_disp_latent.png}
    \vspace{-7mm}
    \caption{\small (a) MORPH-LER Square Root Estimation (b) Validation of Small Deformation Field Assumption (c) Validation of Latent Inverse Consistency}
    \vspace{-10mm}
    \label{fig:roots_and_negation}
\end{figure}

\section{Results}\label{results}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pca_modes_seg_example.jpeg}
    \vspace{-10mm}
    \caption{\small (A) MORPH-LER PCA Modes of Logarithm Maps and Latent Representations. (B) Qualitative Results for Registration. Top-performing model (bold) and next-best model (underlined) for the illustrated sample are highlighted.} % will change name in the figure to CAE
    \label{fig:pca_mdes}
    \vspace{-12mm}
\end{figure}
We use 2D coronal slices from the OASIS-1 dataset \cite{marcus2007open}, which includes brain MRIs from 100 subjects (60 with Alzheimer's). To evaluate our proposed regularizer, we compare it against baselines: CAE \cite{bhalodia2019cooperative}, an autoencoder-based regularization method, and the original GradICON \cite{tian2023gradicon} (convolutional) and TransMorph \cite{chen2022transmorph} (transformer-based) without LEDA. We utilize GradICON and TransMorph as primary registration networks with LEDA as the secondary network, naming them M-GradICON and M-TransMorph.

Table~\ref{tab:registration_performance} compares TransMorph vs. M-TransMorph, GradICON vs. M-GradICON, and CAE as a standalone regularizer. Within each subgroup, we analyze trade-offs between registration accuracy and topological preservation. M-TransMorph improves diffeomorphic properties by reducing negative Jacobian pixels but slightly impacts segmentation performance. M-GradICON achieves higher Dice scores than GradICON while maintaining low negative Jacobian pixels, balancing accuracy and topology preservation. Compared to CAE, LEDA-based regularization maintains similar Dice scores while significantly reducing negative Jacobian pixels, demonstrating its superiority in preserving anatomical topology without compromising accuracy.
%
The qualitative results presented in Figure \ref{fig:pca_mdes}.B shows the registration performance across different models for four distinct anatomical structures. These results corroborate the quantitative findings. Notably, our proposed M-GradICON model significantly improves over its baseline counterpart. This underscores the effectiveness of the secondary network. The performance of M-GradICON is especially noticeable in the smaller anatomical regions such as Subcortical-Gray-Matter and Cerebrospinal fluid (CSF). The quantitative and qualitative results (Figure~\ref{fig:pca_mdes} and Figure~\ref{fig:coopnet_eg}) show that the CAE regularization strategy with a simple autoencoder is ineffective for complex shapes, lacks diffeomorphic properties, and produces anatomically inconsistent transformations.  

Figure~\ref{fig:roots_and_negation}.A illustrates the square root estimations \(\phi^{-m}\) of \model~variants and demonstrates a progressive warping of the source image to align with the target. M-GradICON exhibits superior performance over M-TransMorph, featuring smoother deformation grids, better alignment of warped images, and smoother Jacobian maps, indicating diffeomorphic superiority. To validate the small deformation field assumption\footnote{Small deformation field assumption \(u_{AB}(\x) = u_{BA}(\x)\) reflects symmetry of infinitesimal displacements, ensuring consistent and reversible transformations.}, we tested the logarithm map consistency by utilizing the $2^6$-th root estimation, we systematically negated and composed forward and inverse displacement fields. Figure~\ref{fig:roots_and_negation}.C demonstrates that both methods satisfy this fundamental assumption, validating the accuracy of logarithm maps. Furthermore, we extended this validation to the model's latent space by negating the latent representations of forward and inverse displacement fields. As illustrated in Figure~\ref{fig:roots_and_negation}.B, the \model~accurately decodes these negated latent representations into their corresponding inverse fields, providing compelling evidence of inverse consistency in the latent space. 

PCA analysis of the \model's latent space, depicted in Figure~\ref{fig:pca_mdes}.A reveals modes of variation that align closely with logarithm map PCA results. These modes capture clinically consistent changes, such as ventricular expansion and hippocampus atrophy, while maintaining smooth, structured transitions. This latent space representation not only supports accurate reconstruction of deformations but also enables intuitive exploration of clinically meaningful variations, highlighting the \model's potential for generating realistic deformations, interpolating between anatomical states, and providing valuable insights for understanding disease progression.


We propose an efficient atlas estimation approach that leverages the trained \model's linearized latent space, which adheres to Lie group action laws. The algorithm begins by randomly selecting an initial image \( \bsymb{A}^{(0)} \) as the starting atlas. For each image \( \bsymb{I}_i \) in the dataset, the algorithm computes bidirectional transformations (\(\bphi_{\bsymb{A}^{(k)}\bsymb{I}_i}\) and \(\bphi_{\bsymb{I}_i\bsymb{A}^{(k)}}\)) between the current atlas \( \bsymb{A}^{(k)} \) and the image \( \bsymb{I}_i \). The latent representations of these transformations, \(\z_{\bsymb{A}^{(k)}\bsymb{I}_i}\) and \(\z_{\bsymb{I}_i\bsymb{A}^{(k)}}\), are extracted using the LEDA module.
At each iteration, the latent representations corresponding to the transformations from the atlas to all images, \(\z_{\bsymb{A}^{(k)}\bsymb{I}_i}\), are averaged across all \(i\) to compute a mean representation, \(\z^k\). This negated mean representation is decoded to obtain the atlas to image mean deformation field \(\overline{\bphi}^k\), which is then used update the atlas \( \bsymb{A}^{(k)} \) pulling it towards the true mean. The process is repeated until the atlas converges to a stable solution, as determined by minimal changes across iterations. Additional details are provided in Appendix.

To evaluate the robustness of this method, we perform multiple estimations, each initialized with a different atlas. The initial atlases are chosen from two distinct age groups: above 45 years and 45 years or below. As shown in Figure~\ref{architecture}, the final estimated atlas remains consistent regardless of the initialization, demonstrating the stability of the approach. The atlas serves as a population-level representation that captures shared anatomical features while preserving structural details. Unlike a naive pixel-wise average, it reflects consistent geometric alignment across the dataset, making it critical for studying population variability and facilitating downstream statistical analysis. This ensures the atlas is both biologically meaningful and robust for comparative studies.

