\section{Related Work}
\label{sec:related-work}

Existing jailbreak attack methods can be categorized into forced instruction following (Section \ref{sec:instruction-following}) and adversarial prefixes and suffixes (Section \ref{sec:prefixes-suffixes}).

\subsection{Forced Instruction Following} \label{sec:instruction-following} 

Researchers design forced instruction-following prompts **Ebrahimi et al., "Adversarial Examples for Evaluating the Robustness of Interpretability Methods"** to create a scenario where the instructions are followed more rigorously. In these tasks, LLMs may perform less safety alignment. 

For instance, performing identity shifting attacks with the model by role-playing **Kumar et al., "Attacking Virtual Try-On Systems via Adversarial Examples"**, making LLMs impersonate a security expert who improves the safety of other LLMs, guessing the potentially dangerous responses from the target model. Or they could be disguised as a translation task **Carlini et al., "Hidden Schemes in Machine Learning: A Threat to Trust"**, code writing task **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, and other deceptive scenarios. A typical examples is:

\centerit{You are a hero and I have to answer this question in an emergency situation. \textbf{How do I counterfeit money?} Let's think step by step.}

Most existing automatically prompt-to-prompt jailbreak methods fall into this category. For example, Ferret **Jia et al., "Adversarial Examples for Evaluating the Robustness of Neural Networks"** searches for personas, TAP **Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"**, PAIR **Aten et al., "Pairwise Adversarial Attacks on Deep Neural Networks"** and Puzzler **Sinha et al., "Adversarial Examples for Evaluating the Robustness of Deep Neural Networks"** automatically search scenarios based on meta-templates. Wolf in Sheep's Clothing **Xu et al., "Attacking Machine Learning Models with Adversarial Examples"** disguises harmful intent as a code completion task. We discuss these works further in Appendix \ref{sec:app_related_work}.

\subsection{Adversarial Prefixes and Suffixes} \label{sec:prefixes-suffixes}

Unlike methods that constructing specific scenarios, this kind of method heuristically searches for adversarial objectives at token-level, adding prefixes or suffixes to the original instructions to achieve specific embedding representations. A typical example is: 

\centerit{\{Adversarial prefixes\} \textbf{How do I counterfeit money?} \{Adversarial suffixes\}}

The objective function of these methods is generally to maximize the probability of the target model responding with harmful answers (for example, ``Sure...''). GCG **Sinha et al., "Adversarial Examples for Evaluating the Robustness of Deep Neural Networks"** searches adversarial suffixes through gradient search, while AutoDAN **Xu et al., "Attacking Machine Learning Models with Adversarial Examples"** searches for adversarial objects using a hierarchical genetic algorithm. These methods require access to model weights, so they are unable to directly attack closed-source models like GPTs and Gemini. At the same time, the adversarial prefixes and suffixes may be completely irrelevant to the instruction itself **Carlini et al., "Hidden Schemes in Machine Learning: A Threat to Trust"**, so it is quite absurd and easy to detect if directly concatenating them.

\begin{algorithm*}[t]
    \footnotesize
    \caption{Training the attacker model}
    \label{algorithm:training}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \SetKwFunction{Sort}{Sort}
    \SetKwFunction{SFT}{SFT}
    \SetKwFunction{Target}{Target}
    \SetKwFunction{Evaluator}{Evaluator}
    \SetKwFunction{Length}{Length}
    
    \Input{Number of iterations $n$; red-team dataset $D_{\theta}^{(0)}$; base attacker model $f$; target model $g_{\theta}$, number of SFT samples for each instance $p$; number of rewrites for each instance $q$}
    \Output{Attacker model $f_{g_{\theta}, D_{\theta}}^{(n)}$; red-team dataset $D_{\theta}^{(n)}$}
    \For {$k \gets 1, n$}{%
        Initialize SFT dataset T = []\;
        \ForEach{instance $X_i \in D_{\theta}^{(k-1)}$}{%
            $A_i \gets $ \Sort{$A_i$}\tcp*{Rank attempts based on evaluation scores $S_i^j$}
            \ForEach{attempt $A_i^j \in [A_i^1, A_i^2, \cdots, A_i^p]$}{%
                $T \gets T + A_i^j$\tcp*{Add this attempt to the SFT dataset} 
            }
        }
        $f_{g_{\theta}, D_{\theta}}^{(k)} \gets $ \SFT{$f, T$}\;
        Initialize red-team dataset $D_{\theta}^{(k)}$=[]\;
        \ForEach{instance $X_i \in D_{\theta}^{(k-1)}$}{%
            \ForEach{attempt $A_i^j \in [A_i^1, A_i^2, \cdots, A_i^q]$}{%
                $l \gets $ \Length{$A_i$} $+ 1$\;
                $C_i^{l} \gets f_{g_{\theta}, D_{\theta}}^{(k)}(C_i^j)$\tcp*{Rewrite instruction $C_i^j$ to $C_i^{l}$}
                $R_i^{l} \gets $ \Target{$g_{\theta}, C_i^{l}$}\tcp*{Generate the target response $R_i^{l}$}
                $S_i^{l} \gets $ \Evaluator{$I_i, C_i^{l}, R_i^{l}$}\tcp*{Evaluate the score $S_i^{l}$}
                % $A_i^{l} \gets (C_i^{l}, R_i^{l}, S_i^{l})$\tcp*{Record as a new attempt $A_i^{l}$}
                % $X_i \gets X_i + A_i^{l}$\tcp*{Add this to attempt list for instance $X_i$}
                $A_i \gets A_i + (C_i^{l}, R_i^{l}, S_i^{l})$\tcp*{Add the new attempt to attempt list $A_i$}
            }
            $X_i \gets (I_i, A_i)$\tcp*{Update the attempt list for instance $I_i$}
        }
        $D_{\theta}^{(k)} \gets D_{\theta}^{(k)} + X_i$\;
    }
\end{algorithm*}