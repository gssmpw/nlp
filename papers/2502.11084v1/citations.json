[
  {
    "index": 0,
    "papers": [
      {
        "key": "perez2022ignore",
        "author": "F{\\'a}bio Perez and Ian Ribeiro",
        "title": "Ignore Previous Prompt: Attack Techniques For Language Models"
      },
      {
        "key": "mozes2023use",
        "author": "Mozes, Maximilian and He, Xuanli and Kleinberg, Bennett and Griffin, Lewis D",
        "title": "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "bhardwaj2023red",
        "author": "Bhardwaj, Rishabh and Poria, Soujanya",
        "title": "Red-teaming large language models using chain of utterances for safety-alignment"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "qiu2023latent",
        "author": "Qiu, Huachuan and Zhang, Shuai and Li, Anqi and He, Hongliang and Lan, Zhenzhong",
        "title": "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ding2023wolf",
        "author": "Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian",
        "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "pala2024ferret",
        "author": "Pala, Tej Deep and Toh, Vernon YH and Bhardwaj, Rishabh and Poria, Soujanya",
        "title": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mehrotra2023tree",
        "author": "Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin",
        "title": "Tree of attacks: Jailbreaking black-box llms automatically"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chao2023jailbreaking",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty queries"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ding2023wolf",
        "author": "Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian",
        "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wei2024jailbroken",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How does llm safety training fail?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhu2023autodan",
        "author": "Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong",
        "title": "Autodan: Automatic and interpretable adversarial attacks on large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "alon2023detecting",
        "author": "Alon, Gabriel and Kamfonas, Michael",
        "title": "Detecting language model attacks with perplexity"
      }
    ]
  }
]