@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{openai2023gpt,
  added-at = {2023-10-23T16:08:31.000+0200},
  author = {OpenAI},
  title = {GPT-4 Technical Report},
  journal={arXiv preprint arXiv:2303.08774},
  year = 2023
}

@article{dakhel2023github,
  title={Github copilot ai pair programmer: Asset or liability?},
  author={Dakhel, Arghavan Moradi and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Jiang, Zhen Ming Jack},
  journal={Journal of Systems and Software},
  volume={203},
  pages={111734},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{wu2023precedent,
  title={Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration},
  author={Wu, Yiquan and Zhou, Siying and Liu, Yifei and Lu, Weiming and Liu, Xiaozhong and Zhang, Yating and Sun, Changlong and Wu, Fei and Kuang, Kun},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={12060--12075},
  year={2023}
}

@inproceedings{yuan2023llm,
  title={Llm for patient-trial matching: Privacy-aware data augmentation towards better performance and generalizability},
  author={Yuan, Jiayi and Tang, Ruixiang and Jiang, Xiaoqian and Hu, Xia},
  booktitle={American Medical Informatics Association (AMIA) Annual Symposium},
  year={2023}
}

@article{vykopal2023disinformation,
  title={Disinformation capabilities of large language models},
  author={Vykopal, Ivan and Pikuliak, Mat{\'u}{\v{s}} and Srba, Ivan and Moro, Robert and Macko, Dominik and Bielikova, Maria},
  journal={arXiv preprint arXiv:2311.08838},
  year={2023}
}

@article{fang2024llm,
  title={LLM Agents can Autonomously Exploit One-day Vulnerabilities},
  author={Fang, Richard and Bindu, Rohan and Gupta, Akul and Kang, Daniel},
  journal={arXiv preprint arXiv:2404.08144},
  year={2024}
}

@article{liu2024prejudice,
  title={Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models},
  author={Liu, Yiran and Yang, Ke and Qi, Zehan and Liu, Xiao and Yu, Yang and Zhai, Chengxiang},
  journal={arXiv preprint arXiv:2402.15481},
  year={2024}
}

@article{dong2024attacks,
  title={Attacks, defenses and evaluations for llm conversation safety: A survey},
  author={Dong, Zhichen and Zhou, Zhanhui and Yang, Chao and Shao, Jing and Qiao, Yu},
  journal={arXiv preprint arXiv:2402.09283},
  year={2024}
}

@article{sun2023safety,
  title={Safety assessment of chinese large language models},
  author={Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie},
  journal={arXiv preprint arXiv:2304.10436},
  year={2023}
}

@inproceedings{
li2024rain,
title={{RAIN}: Your Language Models Can Align Themselves without Finetuning},
author={Yuhui Li and Fangyun Wei and Jinjing Zhao and Chao Zhang and Hongyang Zhang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=pETSfWMUzy}
}

@article{pisano2023bergeron,
  title={Bergeron: Combating adversarial attacks through a conscience-based alignment framework},
  author={Pisano, Matthew and Ly, Peter and Sanders, Abraham and Yao, Bingsheng and Wang, Dakuo and Strzalkowski, Tomek and Si, Mei},
  journal={arXiv preprint arXiv:2312.00029},
  year={2023}
}

@inproceedings{
bianchi2024safetytuned,
title={Safety-Tuned {LL}a{MA}s: Lessons From Improving the Safety of Large Language Models that Follow Instructions},
author={Federico Bianchi and Mirac Suzgun and Giuseppe Attanasio and Paul Rottger and Dan Jurafsky and Tatsunori Hashimoto and James Zou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gT5hALch9z}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhu2023promptbench,
  title={Promptbench: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2023}
}

@inproceedings{
qi2024finetuning,
title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!},
author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=hTEGyKf0dZ}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{
perez2022ignore,
title={Ignore Previous Prompt: Attack Techniques For Language Models},
author={F{\'a}bio Perez and Ian Ribeiro},
booktitle={NeurIPS ML Safety Workshop},
year={2022},
url={https://openreview.net/forum?id=qiaRo_7Zmug}
}

@article{mozes2023use,
  title={Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities},
  author={Mozes, Maximilian and He, Xuanli and Kleinberg, Bennett and Griffin, Lewis D},
  journal={arXiv preprint arXiv:2308.12833},
  year={2023}
}

@article{bhardwaj2023red,
  title={Red-teaming large language models using chain of utterances for safety-alignment},
  author={Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2308.09662},
  year={2023}
}

@article{qiu2023latent,
  title={Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models},
  author={Qiu, Huachuan and Zhang, Shuai and Li, Anqi and He, Hongliang and Lan, Zhenzhong},
  journal={arXiv preprint arXiv:2307.08487},
  year={2023}
}

@article{ding2023wolf,
  title={A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},
  author={Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian},
  journal={arXiv preprint arXiv:2311.08268},
  year={2023}
}

@article{yuan2023gpt,
  title={Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher},
  author={Yuan, Youliang and Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and He, Pinjia and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023}
}

@article{wei2023jailbreak,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023}
}

@article{zhu2023autodan,
  title={Autodan: Automatic and interpretable adversarial attacks on large language models},
  author={Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
  journal={arXiv preprint arXiv:2310.15140},
  year={2023}
}

@article{alon2023detecting,
  title={Detecting language model attacks with perplexity},
  author={Alon, Gabriel and Kamfonas, Michael},
  journal={arXiv preprint arXiv:2308.14132},
  year={2023}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{mehrotra2023tree,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3356--3369},
  year={2020}
}

@article{ziegler2022adversarial,
  title={Adversarial training for high-stakes reliability},
  author={Ziegler, Daniel and Nix, Seraphina and Chan, Lawrence and Bauman, Tim and Schmidt-Nielsen, Peter and Lin, Tao and Scherlis, Adam and Nabeshima, Noa and Weinstein-Raun, Benjamin and de Haas, Daniel and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9274--9286},
  year={2022}
}

@misc{king2023meet,
  title={Meet dan—the ‘jailbreak’version of chatgpt and how to use it—ai unchained and unfiltered},
  author={King, Michael},
  year={2023},
  publisher={accessed}
}

@inproceedings{chen2022should,
  title={Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP},
  author={Chen, Yangyi and Gao, Hongcheng and Cui, Ganqu and Qi, Fanchao and Huang, Longtao and Liu, Zhiyuan and Sun, Maosong},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11222--11237},
  year={2022}
}

@article{bhardwaj2024language,
  title={Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic},
  author={Bhardwaj, Rishabh and Anh, Do Duc and Poria, Soujanya},
  journal={arXiv preprint arXiv:2402.11746},
  year={2024}
}

@misc{llama3modelcard,
  title={Llama 3 Model Card},
  author={Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Gemini Team Google},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@misc{liu2023goaloriented,
      title={Goal-Oriented Prompt Attack and Safety Evaluation for LLMs}, 
      author={Chengyuan Liu and Fubang Zhao and Lizhi Qing and Yangyang Kang and Changlong Sun and Kun Kuang and Fei Wu},
      year={2023},
      eprint={2309.11830},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hu2023token,
  title={Token-level adversarial prompt detection based on perplexity measures and contextual information},
  author={Hu, Zhengmian and Wu, Gang and Mitra, Saayan and Zhang, Ruiyi and Sun, Tong and Huang, Heng and Swaminathan, Vishy},
  journal={arXiv preprint arXiv:2311.11509},
  year={2023}
}

@article{kumar2023certifying,
  title={Certifying llm safety against adversarial prompting},
  author={Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Feizi, Soheil and Lakkaraju, Hima},
  journal={arXiv preprint arXiv:2309.02705},
  year={2023}
}

@article{cohen1960coefficient,
  title={A coefficient of agreement for nominal scales},
  author={Cohen, Jacob},
  journal={Educational and psychological measurement},
  volume={20},
  number={1},
  pages={37--46},
  year={1960},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{landis1977measurement,
  title={The measurement of observer agreement for categorical data},
  author={Landis, J Richard and Koch, Gary G},
  journal={biometrics},
  pages={159--174},
  year={1977},
  publisher={JSTOR}
}

@article{jain2023baseline,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@inproceedings{zheng2024prompt,
  title={On prompt-driven safeguarding for large language models},
  author={Zheng, Chujie and Yin, Fan and Zhou, Hao and Meng, Fandong and Zhou, Jie and Chang, Kai-Wei and Huang, Minlie and Peng, Nanyun},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{pala2024ferret,
  title={Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique},
  author={Pala, Tej Deep and Toh, Vernon YH and Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2408.10701},
  year={2024}
}

@article{cao2023defending,
  title={Defending against alignment-breaking attacks via robustly aligned llm},
  author={Cao, Bochuan and Cao, Yuanpu and Lin, Lu and Chen, Jinghui},
  journal={arXiv preprint arXiv:2309.14348},
  year={2023}
}

@article{zhang2024wordgame,
  title={WordGame: Efficient \& Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response},
  author={Zhang, Tianrong and Cao, Bochuan and Cao, Yuanpu and Lin, Lu and Mitra, Prasenjit and Chen, Jinghui},
  journal={arXiv preprint arXiv:2405.14023},
  year={2024}
}

@article{wang2024defending,
  title={Defending llms against jailbreaking attacks via backtranslation},
  author={Wang, Yihan and Shi, Zhouxing and Bai, Andrew and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2402.16459},
  year={2024}
}

@article{liu2024protecting,
  title={Protecting your llms with information bottleneck},
  author={Liu, Zichuan and Wang, Zefan and Xu, Linjie and Wang, Jinyu and Song, Lei and Wang, Tianchun and Chen, Chunlin and Cheng, Wei and Bian, Jiang},
  journal={arXiv preprint arXiv:2404.13968},
  year={2024}
}