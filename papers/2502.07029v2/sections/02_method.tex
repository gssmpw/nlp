\section{Method}\label{sec:method}
In this section, we briefly review the conventional approach to pronunciation assessment, Goodness of Pronunciation (GoP) \citep{Witt2000PhonelevelPS}.
We highlight its limitations: (i) modeling a phoneme as a single cluster, and (ii) assuming atypical speech are in-distribution with respect to typical speech. 
We then introduce our method, MixGoP, which addresses these limitations by (i) modeling allophonic variation through a mixture distribution and (ii) relaxing in-distribution assumptions by removing the softmax function.

\subsection{What is Goodness of Pronunciation?}\label{sssec:gop}
GoP is a phoneme-level pronunciation score\footnote{While \citet{Witt2000PhonelevelPS} uses the term ``phone''-level pronunciation scores, we use the term ``phoneme'' to emphasize that the unit includes allophones.
Note that \citet{Witt2000PhonelevelPS} also suggests that their use of ``phone'' roughly corresponds to a phoneme.} that measures how much the acoustic output of atypical (dysarthric or nonnative) speech deviates from that of typical speech (healthy or native).
GoP is measured by how likely a speech segment ($\mathbf{s}$) is to be the intended phoneme ($p$).
Given the phoneme classifier $P_\theta(p | \mathbf{s})$ with trainable parameters $\theta$, GoP is measured as the log phoneme posterior,\footnote{GoP is traditionally calculated as the average log probability of phoneme $p$ over the corresponding time frames: $\frac{1}{|F|} \sum_{f \in \mathbf{s}} \log P_\theta (p | f)$, where $f$ refers to frames within the utterance $\mathbf{s}$. In our study, we simplify this by considering the GoP as the log probability of the phoneme as a whole, rather than averaging over frames.}
\begin{align}
    \texttt{GoP}_p(\mathbf{s}) = \log P_\theta(p | \mathbf{s}).
    \label{eq:gop}
\end{align}


\subsection{Limitations of GoP}\label{sssec:limit_gop} 
Conventional phoneme classifiers used in GoP assume a \textit{single cluster} for each phoneme.
This is because logits $f_\theta(\mathbf{s})$ are often modeled with a speech encoder $\texttt{Enc}$ and a subsequent fully-connected (FC) layer with weights $\mathbf{W} \in \mathbb{R}^{|\mathcal{V}| \times F}$ \citep{xu21k_interspeech, yeo23_interspeech}:
\begin{align}
    f_\theta(\mathbf{s}) = \mathbf{W} \cdot \texttt{Enc}(\mathbf{s}) \label{eq:fc}
\end{align}
where $\texttt{Enc}(\mathbf{s}) \in \mathbb{R}^{F}$, $|\mathcal{V}|$ denoting the vocabulary size (total number of phonemes), and $F$ the output dimension of the encoder.
If we consider a frozen encoder, the trainable parameter $\theta = \{\mathbf{W}\}$.
Here, the weights $\mathbf{W}$ can be understood as a codebook, containing a $F$-dim centroid for each phoneme.
It requires a \textit{unimodal} (single peak) clustering of hidden features $\texttt{Enc}(\mathbf{s})$ for each phoneme.
This limits the ability to capture allophonic variation, as allophones are represented as distinct acoustic subclusters within each phoneme.


Another limitation comes from the assumption that observed speech segments are \textit{in-distribution} with respect to the training data. 
This comes from the phoneme classifier $P_\theta$ formulation,
\begin{align}
    P_\theta(p|\mathbf{s}) = \texttt{softmax}(f_\theta(\mathbf{s}))[p]. \label{eq:softmax}
\end{align}
With the phoneme classifier relying on the \texttt{softmax} function, which models a categorical distribution, $\mathbf{s}$ is expected to be within phoneme distribution found in typical speech.
However, this assumption is less suitable for atypical speech, which often exhibits substantial acoustic differences from typical speech \citep{yeo23_interspeech,korzekwa2021mispronunciation}.


\subsection{MixGoP: Modeling multiple subclusters within a single phoneme}\label{sssec:MixGoP}
To address the two limitations presented in \Cref{sssec:limit_gop}, we introduce MixGoP, a mixture distribution-based GoP.

First, to overcome the unimodal assumption, MixGoP replaces phoneme classifier $P_\theta(p | \mathbf{s})$ in \cref{eq:gop} with a Gaussian mixture model (GMM).
GMM is a weighted sum of Gaussian distributions that can directly model the phoneme likelihood $P_\theta (\mathbf{s} | p)$ (distribution of speech segment $\mathbf{s}$ for each individual phoneme $p$).
Accordingly, we formulate the phoneme likelihood as follows:
\begin{align}
    P_\theta (\mathbf{s} | p) = \sum_{c=1}^C \pi^c_{p} \mathcal{N}(\texttt{Enc}(\mathbf{s}) | \pmb{\mu}^{c}_{p}, \mathbf{\Sigma}^{c}_{p} ) \label{eq:GM}
\end{align}
where $\mathcal{N}$ denotes the multivariate Gaussian distribution, $\pmb{\mu}^{c}_{p} \in \mathbb{R}^F$ and $\mathbf{\Sigma}^{c}_{p} \in \mathbb{R}^{F \times F}$ is the mean vector (centroid) and covariance matrix, and $\pi^{c}_{p} \in [0, 1]$ is the mixing coefficient.
Here, the trainable parameter $\theta = \{\pmb{\mu}_p^c, \mathbf{\Sigma}_p^c, \pi_p^c\}_{c\in[C], p\in \mathcal{V}}$.
Then, we can newly define our MixGoP score as:
\begin{align}
    \texttt{MixGoP}_p(s) = \log P_\theta (\mathbf{s} | p).\label{eq:mixgop}
\end{align}
Our MixGoP score differs from the original GoP score in \cref{eq:gop} by replacing the phoneme posterior $P_\theta (p|\mathbf{s})$ with the phoneme likelihood $P_\theta (\mathbf{s} | p)$.
By doing so, we are also removing the influence of phoneme prior $P(p)$, which is known to be effective in practice \citep{yeo23_interspeech}.


Second, MixGoP removes the \texttt{softmax} function of \cref{eq:softmax} by directly using the log-likelihood in \cref{eq:GM,eq:mixgop}.
It relaxes the assumption of phonemes in atypical speech being in-distribution.
The quadratic term inside each Gaussian:
\begin{align}
    -\frac{1}{2} (\texttt{Enc}(\mathbf{s}) - \pmb{\mu}^{c}_{p})^T ({\Sigma^{c}_{p}})^{-1} (\texttt{Enc}(\mathbf{s}) - \pmb{\mu}^{c}_{p}) \label{eq:mahala}
\end{align}
directly relates to the Mahalanobis distance, which is commonly used for OOD detection \citep{lee2018simple}. 
By avoiding the \texttt{softmax}, MixGoP is likely to be more robust in handling OOD speech. 


In summary, we train a total of $|\mathcal{V}|$ GMMs (one for each phoneme) where each GMM is composed of $C$ subclusters, \textit{e.g.}, $C=32$.
$C$ is kept constant across all phonemes, as it is known that sufficiently large number of Gaussian mixtures can approximate any probability density \citep{nguyen2020approx}. 
Experiments on the influence of $C$ on downstream performance can be found in \Cref{subsec:abl-clusters}.
We use the k-means algorithm to determine the initial cluster centers and the expectation-maximization (EM) algorithm to optimize the parameters of the Gaussian mixtures, using \texttt{scikit-learn 1.4.1} \citep{scikit-learn}.
By considering allophony in modeling, MixGoP is expected to better reflect the distribution of each phoneme.



% For ease of understanding, let us consider a cluster with its corresponding centroid $\mu$.
% We have to measure the distance from the feature $\texttt{Enc}(\mathbf{s})$ to the centroid $\mu$.
% For the squared Euclidean distance $(\texttt{Enc}(\mathbf{s}) - \mu)^T (\texttt{Enc}(\mathbf{s}) - \mu)$,
% it only takes the relative distance between the feature and the centroid into account.
% However, Mahalanobis distance also takes the shape of each cluster into account.
% For example, if the samples that comprise the cluster are more densely crowded together, then the distance from the feature to the centroid is considered larger, as it is less likely for that feature to be included in that cluster.

% Intuitively, for each score, we are measuring the distance from each cluster, where the Mahalanobis distance takes different peakiness per cluster into account.

