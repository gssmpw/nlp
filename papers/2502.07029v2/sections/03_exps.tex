\section{Experiments}\label{sec:exp}
\subsection{Datasets}\label{subsec:datasets}
We use five datasets: three dysarthric speech datasets (UASpeech \citep{kim2008dysarthric}, TORGO \citep{rudzicz2012torgo}, and SSNCE \citep{ta2016dysarthric}) and two non-native speech datasets (speechocean762 \citep{speechocean762} and L2-ARCTIC \citep{zhao2018l2}).
In this paper, we use healthy or native speech as the training sets, and dysarthric and non-native speech as the test sets, in line with the OOD literature \citep{DBLP:conf/iclr/HendrycksG17}. 
Refer to \autoref{subsec:dataset-details} for more details.


\subsection{Feature extraction}\label{subsec:features}
For our experiments, we compare various speech feature extractors $\texttt{Enc}(\mathbf{s})$  (\cref{eq:fc,eq:GM}).


\paragraph{Traditional acoustic features.}
We use the Mel-Frequency Cepstral Coefficients (MFCCs) and Mel spectrograms as baselines, using the default hyperparameters of librosa \citep{mcfee2015librosa}.


\paragraph{TDNN-F features.}
We compare with a factorized time-delay neural network (TDNN-F) model \citep{povey2018semi} for the speechocean762 dataset, as TDNN-F features have been often used as baselines \citep{speechocean762,gong2022transformer,chao20223m,do2023hierarchical}.


\paragraph{S3M features.}
We employ two frozen S3Ms: XLS-R-300M \citep{babu22_interspeech} and WavLM-Large \citep{chen2022wavlm}. 
XLS-R (shorthand for XLS-R-300M), trained cross-lingually, has demonstrated strong performance in ASR for low-resource languages \citep{babu22_interspeech} and dysarthric speech assessment \citep{yeo23_interspeech}. 
We also employ WavLM (shorthand for WavLM-Large), a state-of-the-art model for various tasks, including phoneme recognition \citep{feng2023superb,yang21c_interspeech}.


As different layers of S3Ms are known to encode different information \citep{pasad2021layer,pasad2023comparative}, we use features from each layer. 
Specifically, we extract convolutional features (denoted as layer index 0) and all consecutive Transformer features (denoted as layer indices 1 through 24).


\paragraph{Feature segmentation.}
We segment the features according to the start and end timestamps of each phoneme.
Refer to the detailed time-alignment process in \autoref{subsec:dataset-details}.
Then, we apply center pooling to extract one feature per segment.\footnote{We chose center pooling over average pooling to reduce the impact of inaccurate phoneme alignments on GoP calculations.
Both pooling methods are known to encode similar amounts of phonetic information for S3Ms \citep{pasad2023self, choi2024self}.}


\subsection{Baselines}\label{subsec:baseline}
We verify the effectiveness of our MixGoP by comparing it against various baselines \citep{yeo23_interspeech, sun2022out, shahin2023phonological, scholkopf2001estimating}.
We evaluate on all the speech features listed in \Cref{subsec:features} across all the methods for fair comparison.
These baselines are categorized into two groups: (i) phoneme classifier-based and (ii) OOD detector-based approaches.

\paragraph{Phoneme classifier-based approaches} encompass conventional GoP formulations, which assume a unimodal distribution and in-distribution of phonemes, as discussed in \Cref{sssec:limit_gop}. 
We employ four popular GoP formulations\footnote{GoP formulations were named after the underlying models \citep{speechocean762,yeo23_interspeech}, leading to names like GMM-GoP. However, the GMM-GoP scoring method (\cref{eq:gop}) does not necessarily rely on GMM.}: GMM-GoP \citep{Witt2000PhonelevelPS}, NN-GoP \citep{hu2015improved}, DNN-GoP \citep{hu2015improved}, and MaxLogit-GoP \citep{yeo23_interspeech}.
Note that all formulations use the same underlying phoneme classifier $P_\theta(p|\mathbf{s})$.
They only differ by how to calculate the GoP scores.
Refer to \citet{yeo23_interspeech} for more details.


\paragraph{OOD detector-based approaches} calculate GoP by measuring how likely an input is to be an outlier.
In other words, they can quantify the level of atypicalness.
Our MixGoP is one of these approaches, as MixGoP models the likelihood $P_\theta(\mathbf{s}|p)$ with typical speech (\cref{eq:GM}) and identifies outliers (atypical speech) based on their likelihood (\cref{eq:mixgop}). 
We additionally test three baselines: k-nearest neighbors (kNN) \citep{sun2022out}, one-class support vector machine (oSVM) \citep{scholkopf2001estimating}, and phoneme-specific oSVM (p-oSVM) \citep{shahin2019anomaly}. 
While kNN has been utilized for OOD detection \citep{sun2022out}, it has not previously been applied to dysarthric or non-native speech.
Conversely, oSVM and p-oSVM have been applied to the evaluation of both disordered and non-native speech \citep{shahin2019anomaly, shahin2023phonological}.


\subsection{Training details}\label{subsec:training}
\paragraph{Phoneme classifier-based.}
The phoneme classifier in \cref{eq:gop} is trained on features from \Cref{subsec:features} with a single learnable FC layer (\cref{eq:fc}) with the default settings of Adam optimizer \citep{DBLP:journals/corr/KingmaB14} for a maximum of 500 iterations.


\paragraph{OOD detector-based.}
For kNN, we construct a kNN model for each phoneme using the features from \Cref{subsec:features}.
Then, we use the maximum Euclidean distance between the test data feature and the nearest 10\% training data feature as the GoP score, following \citet{sun2022out}.
For oSVM, all phonemes is modeled with a single oSVM model \citep{shahin2023phonological}, while p-oSVM modeled each phoneme as a separate oSVM model.
All oSVM models are trained with features using the default hyperparameters of \texttt{scikit-learn 1.4.1} \citep{scikit-learn}.
Radial basis function was used for both oSVM and p-oSVM.
We use the distance from the hyperplane as the GoP score.


\paragraph{MixGoP.} In our MixGoP framework, we apply random subsampling of 512 features per phoneme.
Empirical analysis indicates that subsampling does not necessarily degrade performance (See \Cref{subsec:ablation}).
The number of subclusters for each phoneme-wise Gaussian mixture is set to 32. 
A detailed investigation into the effect of the number of subclusters on GoP performance is discussed in \Cref{subsec:abl-clusters}.


\subsection{Evaluation}\label{subsec:eval}
As described in \Cref{subsec:features}, we segment the spoken utterance $\mathbf{x}$ phoneme-wise: $\mathbf{x} = \{(p_1, \mathbf{s_1})$, $(p_2, \mathbf{s_2})$, $\cdots$, $(p_N, \mathbf{s_N})\}$, where $p_i$ is the phoneme label, $\mathbf{s}_i$ is the observed speech segment, and $N$ is the total number of phonemes within the utterance.
Following \citet{yeo23_interspeech}, we define the pronunciation score of an utterance $\mathbf{x}$ as:
\begin{align}
    \texttt{Pronunciation}(\mathbf{x}) = \frac{1}{N} \sum_{i=1}^{N} \texttt{GoP}_p(s),\label{eq:1_n}
\end{align}
where the definition of the \texttt{GoP} is different per each method.
That is, the GoP scores are averaged across the utterance.

\input{figures/tab_main}

Similar to \citet{yeo23_interspeech}, we evaluate performance using the Kendall-tau correlation coefficient between the utterance-level pronunciation scores and the ground truth dysfluency/disfluency scores provided by the dataset. 
While \citet{yeo23_interspeech} used additional training data, our setting only uses the aformentioned datasets.


Unlike other datasets, L2-ARCTIC contains only phoneme-wise mispronunciation detection labels (0 for correct, and 1 for mispronounced).
Therefore, we directly measure the correlation between the predicted phoneme-wise pronunciation scores and the mispronunciation detection labels.



\subsection{Results}\label{subsec:results}
\Cref{tab:main} presents the experimental results, where we report the best-performing layer's results for S3Ms.
Refer to \Cref{subsec:layerwise} for the performance of individual layers.
\Cref{tab:main} shows that our proposed MixGoP method achieved the state-of-the-art performance across all the datasets except L2-ARCTIC.
As for the L2-ARCTIC dataset, the best performance was observed with NN-GoP. 


\paragraph{Comparison between features.}
We observe that all other features generally underperform compared to both S3Ms, further highlighting the general effectiveness of frozen S3Ms \citep{yang21c_interspeech}.
% Interestingly, MFCC works better when applied to phoneme classifier-based methods compared to MixGoP and kNN.
It aligns with \citet{choi2022opening}, where S3Ms, unlike MFCCs, stores information as a relative distance between the features so that the Mahalanobis distance of MixGoP (\cref{eq:mahala}) or the Euclidean distance of kNN (\Cref{subsec:gop_vs_knn}) can be effective.
We explore this discussion in detail in \Cref{sec:motiv}.


\paragraph{Comparison between datasets.}
We observe that MixGoP tends to be effective on dysarthric datasets, whereas NN-GoP tends to perform well on non-native datasets.
We suspect this is due to dysarthric test sets being more OOD than non-native test sets, so accurate likelihood estimation becomes more important.
Dysarthric train/test datasets are split by healthy and dysarthric speakers.
However, non-native datasets only contain non-native speakers, where we split train/test using utterance-wise or phoneme-wise pronunciation scores.
(Details in \Cref{subsec:dataset-details}.)
Hence, it is likely that the train/test difference of non-native datasets is less severe than that of dysarthric datasets.
Inherent acoustic differences between dysarthric and non-native speech may have further widen the dataset differences \citep{yeo23_interspeech,korzekwa2021mispronunciation}.




\paragraph{Comparison within the same groups.}
For \textit{phoneme classification-based} baselines, performance greatly differs across methods, despite all four methods being based on the same classifier.
This supports the importance of selecting an appropriate equation for uncertainty quantification in phoneme class-based methods, aligning with the findings of \citet{yeo23_interspeech}.
Regarding \textit{OOD detector-based} baselines, SVM-based methods usually perform the worst across all datasets.
In contrast, kNN achieves performance comparable to our proposed MixGoP on the TORGO and L2-ARCTIC datasets, while MixGoP delivers the best overall performance.
