\section{Related works}
\subsection{Phoneme-level Pronunciation Assessment}
\citet{Witt2000PhonelevelPS} first introduced GoP to estimate the log posterior probability of a phoneme using a Hidden Markov Model (HMM). 
Later improvements replaced HMMs with deep neural networks \citep{hu2015improved-slate, hu2015improved, li2016mispronunciation} and S3Ms \citep{xu21k_interspeech, yeo23_interspeech, cao2024framework}. 
GoP has also been enhanced by considering additional factors, such as HMM transition probabilities \citep{sudhakara2019improved, Shi2020ContextawareGO} and phoneme duration \citep{Shi2020ContextawareGO}.


\Cref{sssec:limit_gop} emphasizes the usefulness of framing the pronunciation assessment of atypical speech as the OOD detection task.
\citet{yeo23_interspeech} also addressed this by not using \texttt{softmax} for phoneme classifiers improved performance. 
However, the use of \texttt{softmax} during training introduced in-distribution bias. 
\citet{cheng20_interspeech} modeled input probability with latent representations, but their method still depended on assessment scores to train the prediction model. 
Our approach improves by directly modeling phoneme likelihood instead of relying on phoneme classifiers. 
Furthermore, our approach explicitly accounts for allophonic variation within phonemes.
% This not only improves performance on OOD tasks but also enhances allophone modeling of phonemes.



\subsection{S3M Feature Analysis}
Previous literature on the phonetics and phonology of S3Ms often compared downstream task performance of different layers \citep{martin23_interspeech,pasad2021layer,pasad2023comparative,pasad2023self,choi2024understanding}.
Linear probes \citep{martin23_interspeech,choi2024understanding} or canonical correlation analysis \citep{pasad2021layer,pasad2023comparative,pasad2023self} are often used to measure the amount of information.
Our work is complementary as previous works focus on the existence of the information, whereas we further investigate on how the information is structured within the S3M features.


Also, discrete speech units from k-means clustering of S3M features have been used as the tokenizer for speech \citep{chang2024exploring}.
Its underlying assumption comes from the feature structure being useful, \textit{i.e.}, similar-sounding segments are close to each other.
\citet{choi2024self} showed that phonetically similar words are close to each other.
Also, \citet{baevski2020wav2vec,hsu2021hubert,liu23j_interspeech} demonstrated that phonemes and S3M cluster indices strongly correlate with each other.
\citet{sicherman2023analysing,abdullah2023information} showed that natural classes are also well-clustered.
Finally, \citet{wells22_interspeech} showed that the dynamic nature of a single phoneme articulation is captured by a stream of cluster indices.
Extending previous works, we focus on the multimodal nature of phonemes and demonstrate that allophones are construct subclusters within the single phoneme.


% Leonie - compositionality