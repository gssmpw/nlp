\section{Datasets}\label{subsec:dataset-details}
In our study, we utilize read speech at the sentence level. 
While the TORGO and SSNCE datasets include both word- and sentence-level data, we focused exclusively on sentences to ensure consistency, as the non-native datasets contain only sentence-level speech. 
Although the UASpeech dataset consists solely of word-level materials, we included it for comparison with prior work that applied GoP to dysarthric speech \citep{yeo23_interspeech}.

In contrast to non-native speech datasets, which provide utterance-level scores, dysarthric speech datasets offer intelligibility scores at the speaker level.
Therefore, we applied these speaker-level scores to each utterance in the dysarthric datasets.


All the datasets are publicly available, with licenses that allow academic use.
We used the datasets for exclusively academic purposes.


\subsection{Dysarthric Speech Datasets}\label{subsubsec:dysarthric}
\textbf{UASpeech} \citep{kim2008dysarthric} comprises recordings from 25 English speakers, of whom 14 have dysarthria and 11 are healthy. The severity of dysarthria was assessed using the Frenchay Dysarthria Assessment (FDA) \citep{enderby1980frenchay}, categorizing four speakers as having high-intelligibility, three as mid-intelligibility, three as low-intelligibility, and four as very low-intelligibility. This study analyzes common and uncommon words selected for their diverse phonetic sequences, which are crucial for evaluating the pronunciation of phonemes in varied contexts. Although the dataset includes recordings from an 8-microphone array, we utilize only the fifth microphone for computational efficiency. In total, 6,589 utterances from healthy speakers and 8,370 utterances from dysarthric speakers are used. 
For time alignment information, we apply the Montreal Forced Aligner (MFA) \citep{mcauliffe2017montreal}.
% The utterances from healthy speakers serve as training data, while those from dysarthric speakers are used as test data.


\textbf{TORGO} \citep{rudzicz2012torgo} consists of recordings from 15 English speakers, including 8 with dysarthria and 7 healthy individuals. The severity was determined using FDA scores, classifying two speakers as mild, one as mild-to-moderate, one as moderate-to-severe, and four as severe. To balance the classes, mild-to-moderate and moderate-to-severe speakers were merged into a single moderate category. A total of 156 healthy and 413 dysarthric utterances are used.
Similar to UASpeech, we apply MFA for time alignment information. 
Then, we integrate alignments that were manually adjusted by two linguists, as outlined by \citet{hernandez2020dysarthria}. 
These manually refined alignments will be made publicly accessible in our repository.$^1$
% , with the former serving as training data and the latter as test data.


\textbf{SSNCE} \citep{ta2016dysarthric} comprises Tamil speech recordings from 20 dysarthric and 10 healthy speakers. Severity is categorized based on intelligibility scores rated on a 7-point Likert scale by two speech pathologists, resulting in 7 mild (scores 1-2), 10 moderate (scores 3-4), and 3 severe (scores 5-6) speakers. Each speaker recorded 260 distinct sentences, totaling 2,600 healthy and 5,200 dysarthric utterances. 
We use the time stamps provided with the datasets. 


\subsection{Nonnative Speech Datasets}\label{subsubsec:nonnative}
\textbf{speechocean762} \citep{speechocean762} consists of 5000 utterances from 250 Mandarin-speaking non-native children and adult speakers. 
% The ratios of both males to females and children to adults are designed to be equally balanced. 
English proficiency levels' ratio maintains a 2:1:1 ratio for good, medium, and poor, ensuring representation across different proficiencies.
% The proficiency scores in accuracy, completeness, fluency, and prosody at various levels of granularity, including phoneme, word, and sentence. 
Our analysis focuses on total scores at the sentence level, pronunciation quality graded on a scale from 0 to 10.
We use scores 9 and 10 as training data and the rest as test data.
We employ forced alignments generated using the Kaldi recipe, following the experimental setup of \citet{speechocean762}. 
This allows for a direct comparison of S3M and Kaldi features, as the quality of phoneme alignment can affect the evaluation results \citep{mackenzie2020assessing}.

\textbf{L2-ARCTIC} \citep{zhao2018l2} is a non-native English corpus comprising recordings from 24 speakers, whose first languages (L1) include Hindi, Korean, Mandarin, Spanish, Arabic, and Vietnamese. 
Each speaker contributes approximately one hour of read speech derived from CMU ARCTIC prompts \citep{kominek2004cmu}.
For this study, we use 150 utterances per speaker, where manual annotations were available for phonememic errors such as substitutions, deletions, and additions.
We use the existing data split as the train/test split, where we further exclude mispronounced utterances in the training data.
Unlike other datasets, L2-ARCTIC only contains phoneme-wise mispronunciation detection labels (0/1).
Therefore, we used the GoP scores and the label at the phoneme level, not at the utterance level.
We use the time stamps provided with the datasets. 



\section{Computational cost}
Extracting S3M features for all the datasets takes less than one day for each S3M on a single NVIDIA V100 GPU.
Running the experiments on all the datasets takes less than a day on a 64-CPU 128GB-memory machine for the default hyperparameter settings.


\section{Additional analyses and discussions}\label{sec:add-analysis}

\subsection{Layerwise analysis of downstream performance}\label{subsec:layerwise}
\input{figures/fig_layerwise}
It is known that different layers of S3Ms tend to encode different information \citep{pasad2021layer,pasad2023self,pasad2023comparative,choi2024self}.
Hence, we further compare the layerwise trend of XLS-R and WavLM in \Cref{fig:layerwise} to provide a guideline for which layer to use for each downstream task.
WavLM tends to have decent or even the best performance in the final layer, while XLS-R features often suffer a great decrease in performance in the final layer.
This indicates that the choice of which layer to use is more critical when using XLS-R compared to WavLM, whereas the last layer of WavLM generally shows decent performance.
% Specifically, MixGoP (black line) generally performed the best on dysarthric speech across all the layers (except for the final layers of XLS-R).
% In contrast, phoneme classification-based approaches (NN-GoP and MaxLogit-GoP for speechocean762, GMM-GoP and NN-GoP for L2-ARCTIC) generally performed better throughout the layers for nonnative datasets.


\subsection{Impact of the number of subclusters}\label{subsec:abl-clusters}
\input{figures/tab_ncluster}
We explored the optimal number of subclusters for the downstream task.
We conducted a grid search with cluster sizes of 4, 8, 16, 32, and 64, while keeping the best model and layer index from \Cref{subsec:results} fixed.
For the TORGO and SSNCE datasets, we did not test 64 clusters due to insufficient phoneme samples to train the Gaussian mixtures.

As shown in \Cref{tab:ncluster}, increasing the number of clusters often results in marginal improvements in downstream performance. 
We attribute this to the fact that better distribution modeling tends to enhance performance.
This reinforces our hypothesis that the number of subclusters does not need to exactly match the number of allophones, as sufficiently large number of Gaussian mixtures can approximate any probability density \citep{nguyen2020approx}.
The best performance was consistently achieved with a cluster size of 32 across datasets, with the exception of the speechocean762 dataset. 
For speechocean762, the highest performance was attained with the smallest cluster size, 4, although performances with cluster sizes of 8 and 16 were also similar.



\input{figures/fig_nclusters}
To further analyze the behaviors of different clusters, we performed additional layerwise analysis on TORGO, the smallest dataset, in \Cref{fig:cluster_abl}.
We can clearly observe that a larger number of clusters leads to better downstream performance across different layers.
Especially on the best-performing layer (layer index 19 of XLS-R and 6 for WavLM), it shows bigger differences per different number of clusters.
We can also observe that the number of clusters 16 and 32 is the most similar, potentially indicating the performance saturation with respect to the number of clusters.


% We think that this may indicate that just increasing the number of clusters does not necessarily enhance the accurate modeling of each allophone.


\subsection{Learnable phoneme-wise attention}\label{subsec:attn}
In dysarthric speech, pronunciation scores for certain phonemes are known to exert more influence on speech intelligibility \citep{yeo23_interspeech}. 
Relevant factors include their place and manner of articulation, and articulatory complexity \citep{kim2010frequency}.
Non-native speakers, on the other hand, make different pronunciation mistakes based on their native language \citep{ng2023l1,yeo2023comparison}.


To model the phoneme-wise importance, we design a learnable attention module $\alpha \in \mathbb{R}^{|\mathcal{V}|}$ to satisfy two conditions: (i) bounded attention weights ($0 \leq \alpha[p] \leq 1$ for any phoneme $p$), and (ii) the weights sum up to one ($\sum_{i=1}^{N} \alpha[p_i] = 1$):
% To satisfy the constraints mentioned in \cref{eq:attn}, we design the learnable attention module $\alpha \in \mathbb{R}^{|\mathcal{V}|}$ as follows:
\begin{align}
    \alpha[p_i] = \frac{e^{\mathbf{w}_{p_i}}}{\sum_{i=1}^N e^{\mathbf{w}_{p_i}}},\label{eq:attn}
\end{align}
where the phoneme-wise logits $\mathbf{w} \in \mathbb{R}^{|\mathcal{V}|}$ is a learnable parameter with the vocabulary size.
The formulation is nearly the same as the \texttt{softmax} function with the minor difference: if the same phoneme occurs multiple times within the utterance, it shares the same weights.


We can extend our MixGoP by combining the Gaussian mixtures (\Cref{eq:GM}) and the attention module (\Cref{eq:attn}):
\begin{align}
    \texttt{MixGoPAttn}(\mathbf{x}) := \sum_{i=1}^{N} \alpha[p_i] \cdot \log P_\theta(\mathbf{s}|p).\label{eq:mixtures}
\end{align}
We improve upon the phoneme-wise GoP of \Cref{eq:1_n} by (i) replacing the phoneme classifier $P_\theta(p|\mathbf{s})$ by the phoneme density estimator $P_\theta(\mathbf{s}|p)$ and (ii) replacing the uniform importance $1/N$ with the learnable weight $\alpha[p_i]$.
% Each replacement is motivated by the corresponding limitation of GoP.


We train the phoneme-wise attention module by directly maximizing the Spearman's rank correlation coefficient %\texttt{spearman} 
between $\texttt{MixGoPAttn}(\mathbf{x})$ and the pronunciation score $y$ (degree of dysfluency/disfluency for $\mathbf{x}$):
\begin{align}
    \mathcal{L} = - \texttt{spearman}(\text{MixGoPAttn}(\mathbf{x}), y),
\end{align}
where we freeze the Gaussian mixture models and only train the logits $\mathbf{w}$.
The sorting operation within \texttt{spearman} is made differentiable by soft sorting \citep{blondel2020fast}, following \citet{blondel2020fast}'s implementation. % to develop the loss function $\mathcal{L}$.


% \subsection{Phonemes have different influences}\label{sec:phoneme_analysis}
\input{figures/fig_attention}
We applied 5-fold cross-validation to obtain accurate evaluation results.
The phoneme attention module is trained on four datasets, excluding L2-ARCTIC as the dataset does not provide utterance-level scores.
Note that this setting differs from the experiments demonstrated in \Cref{tab:main}, as we use the test set to train the attention module.


As demonstrated in \Cref{fig:attn}, applying the attention module resulted in small yet consistent performance improvements across all datasets. 
Furthermore, the attention weight differences between the least and most influential phonemes varied by up to 1.5 times, indicating variability in their contributions. 
This highlights the importance of considering the differing influence of each phoneme when calculating utterance-level pronunciation scores, which is crucial for optimizing pronunciation assessment performance.


However, we could not identify a consistent pattern across datasets regarding which phonemes consistently received higher or lower attention scores. 
This suggests that the variation in attention weights may be influenced by various factors, such as speakers, and phoneme distribution.
Further investigation into the underlying mechanisms driving these variations is necessary to gain a deeper understanding of the differing impact of phonemes in pronunciation assessment.\footnote{Full list of attention weights can be found in \url{https://github.com/juice500ml/acoustic-units-for-ood}}


\subsection{Comparing MixGoP and kNN}\label{subsec:gop_vs_knn}
Both kNN and our proposed MixGoP heavily depend on the distances induced by S3M features.
Also, kNN is often the closest competitor to MixGoP, as shown in \Cref{tab:main}, indicating their similarities.
However, there are multiple differences in their implementation details.
kNN relies on Euclidean distance and selects the maximum distance among nearest neighbors.
MixGoP employs Mahalanobis distance and measures the distances from the centroids obtained by the EM algorithm.
We leave the study on the effectiveness of kNN and MixGoP's key components for future work.




% As the results suggest the importance of reweighting, more research should be done on obtaining the phoneme-wise importance while minimally or not using the test set.


% Interestingly, in dysarthric speech datasets, phonemes that are challenging to articulate, such as \textipa{/Z/} and \textipa{/w/}, received the highest attention scores, whereas phonemes with lower articulatory complexity, such as \textipa{/O/} and \textipa{/W/}, had the lowest attention scores. 
% In contrast, for the speechocean762 dataset, the phonemes with the highest and lowest attention scores were /\texttt{JH}/ and /\texttt{ZH}/, respectively. 
% This discrepancy can be attributed to the influence of a speaker's first language (L1), suggesting that non-native learners struggle to pronounce phonemes absent from their L1 phoneme inventory \citep{yeo2023comparison}.

