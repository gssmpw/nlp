\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\bibliographystyle{plainnat}
\usepackage[numbers]{natbib}
\usepackage{array}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{subcaption} 

\title{DFDT: Dynamic Fast Decision Tree for IoT Data Stream Mining on Edge Devices}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

\author{ 
    {\hspace{1mm}Afonso Lourenço} \\
	Polytechnic of Porto \\
	\texttt{fonso@isep.ipp.pt} 
    \And
    {\hspace{1mm}João Rodrigo} \\
	Polytechnic of Porto \\
    \And
    {\hspace{1mm}João Gama} \\
	University of Porto \\
    \And
    {\hspace{1mm}Goreti Marreiros} \\
	Polytechnic of Porto \\
}


% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{}
%\renewcommand{\undertitle}{}
\renewcommand{\shorttitle}{DFDT: Dynamic Fast Decision Tree}

\begin{document}
\maketitle

\begin{abstract}
	The Internet of Things generates massive data streams, with edge computing emerging as a key enabler for online IoT applications and 5G networks. Edge solutions facilitate real-time machine learning inference, but also require continuous adaptation to concept drifts. Ensemble-based solutions improve predictive performance, but incur higher resource consumption, latency, and memory demands. This paper presents DFDT: Dynamic Fast Decision Tree, a novel algorithm designed for energy-efficient memory-constrained data stream mining. DFDT improves hoeffding tree growth efficiency by dynamically adjusting grace periods, tie thresholds, and split evaluations based on incoming data. It incorporates stricter evaluation rules (based on entropy, information gain, and leaf instance count), adaptive expansion modes, and a leaf deactivation mechanism to manage memory, allowing more computation on frequently visited nodes while conserving energy on others. Experiments show that the proposed framework can achieve increased predictive performance (0.43 vs 0.29 ranking) with constrained memory and a fraction of the runtime of VFDT or SVFDT.
\end{abstract}


% keywords can be removed
\keywords{Hoeffding tree \and data stream \and edge computing \and IoT}

\section{Introduction}
\label{sec:intro}

The Internet of Things (IoT) connects a vast network of physical devices, generating massive, high-speed data streams. To extract valuable insights from this ever-growing stream of raw sensor data, the edge computing paradigm has emerged as a key enabler for low-latency IoT applications and 5G networks, bridging the gap between cloud services and end-users. However, benefiting from edge solutions not only implies bringing machine learning models for real-time inference, but also continuous updates to adapt to the evolving nature of unbounded streams. Unlike batch learning, where all training data is available upfront, data streams arrive incrementally. Thus, making models susceptible to becoming outdated, especially due to concept drifts, i.e., changes in data distribution over time \cite{gama2010knowledge}.

To circumvent these challenges, many stream mining algorithms have been proposed, with tree-based methods being the state-of-the-art for tabular data sources. Their success can be attributed to approximation-based splitting \cite{domingos2000mining}, with incrementally updated statistical summaries of entropy-based metrics, e.g. information gain, to determine whether the observed utility of a split is statistically significant when the data distribution is unknown, e.g. via the Hoeffding bound (HB). To find the split value, for categorical attributes, one can simply do a split for each category, and group them by similarity criteria \cite{domingos2000mining}, while for continuous attributes, one can compute the Gaussian for each combination with the target classes and select the highest posterior probability \cite{gama2004forest}.

Moreover, combining these into ensemble strategies is another effective approach. Upon a concept drift, the diversity among trees can be exploited as some local minima are less affected than others. Affected components can be dynamically deleted, added, and combined with a voting scheme, weighting different weak learners based on their past performance \cite{gomes2017adaptive,gunasekara2024gradient}. However, with data stream researchers shifting their focus to ensemble-based solutions prioritizing increased predictive performance, these come at the cost of higher resource consumption, increased latency, and greater memory requirements. This trade-off is particularly problematic in memory-scarce edge environments, making it essential to revisit strategies that allow to control the tree growth efficiency, by delaying the expansion of less confident branches.

To address this challenge, a novel algorithm tailored for memory-constrained data stream mining is introduced, coined as DFDT: \textbf{D}ynamic  \textbf{F}ast \textbf{D}ecision \textbf{T}ree, which consolidates several key contributions from prior research. The algorithm employs dynamic, adaptive strategies for controlling decision tree growth, specifically by adjusting the grace period and tie thresholds during the tree building process, according to the incoming data samples. Moreover, the algorithm incorporates a stricter set of rules based on entropy, information gain, and the number of instances observed at the leaf. To further manage memory constraints, a deactivation mechanism is introduced for low activity leaves, so that the algorithm can grow more organically depending on the distribution and number of instances observed at each leaf of the tree. Thus allowing to spend more computation on the most visited nodes while saving energy on others.

Experiments conducted on various benchmark datasets, evaluating the accuracy, memory usage, and runtime of DFDT, along with statistical tests to assess significant differences, demonstrate that the proposed adaptive strategies achieve enhanced efficiency (ranking 0.74 vs. 0.65). An ablation study of the algorithm's key components further highlights that omitting adaptive expansion modes with leaf deactivation seems to lead to even superior overall efficiency, by increasing accuracy and reducing computation time, at the cost of memory. The paper is structured as follows: Section~\ref{sec:related} reviews contributions from prior research that underpin the proposed algorithm. Section~\ref{sec:algorithm} introduces DFDT, detailing its core concepts and pseudocode. Section~\ref{sec:experiments} presents a comprehensive ablation study of the algorithm components, and compares the performance of DFDT with VFDT, svfdt\_I, and svfdt\_II. Finally, Section~\ref{sec:conclusions} outlines conclusions and future research directions.

\section{Related work}
\label{sec:related}

The core component for incrementally constructing decision trees (DTs) on edge devices, while maintaining a fixed time complexity per sample, is approximation-based splitting \cite{domingos2000mining}. As new instances arrive, they are processed from the root to a leaf node, updating statistics at each node. These updates are used to periodically adjust heuristic values for each attribute, such as information gain (IG) and Gini index (GI) \cite{domingos2000mining}. The tree attempts to perform splits based on a statistical bound that quantifies the confidence interval for the heuristic function, given a minimum amount of data. Typically, DTs compare the Hoeffding bound (HB) to the difference in evaluation between the best and second-best attribute splits \cite{domingos2000mining}. When this difference exceeds the bound, the leaf node is split into child nodes.

\textbf{Rules.} While traditional incremental DTs exclusively focus on evaluating the top two attributes, one can introduce extra splitting rules to promote even more valuable splits. For example, one can incorporate the fluctuation of the HB, tracking its mean, minimum, and maximum values, along with an accuracy metric, as a pre-pruning condition for split decisions \cite{yang2011optimized}. Alternatively, constraints can be applied that compare current metrics against historical data and cross-leaf information, while introducing a skipping mechanism to bypass these constraints when significant changes in the data are detected \cite{da2018strict}. Furthermore, a constraint can be added to assess whether the best-ranked feature, chosen for splitting at a leaf node, provides substantial gains compared to the gains observed in previous splits within the same branch \cite{barddal2020regularized}.

\textbf{Tie break.} A critical aspect of incremental DTs is the tie-breaking procedure. While using the statistical difference in IG between two attributes helps control tree growth, competition between two equally favored split candidates can hinder progress, especially when either option would be equally suitable. To mitigate this, if the difference in heuristic values exceeds a predefined tie threshold, denoted as $\tau$, the split is performed. This threshold effectively controls the minimum rate of tree growth, with the attributes’ ability to separate before reaching the threshold influencing the speed at which the tree expands. However, a fixed $\tau$ value may cause ties to be broken prematurely, before a meaningful decision can be made, due to a lack of suitable candidates rather than a true tie situation. To address this, alternative approaches can be employed, such as comparing the difference between the best and second-best candidates with the difference between the second-best and worst candidates \cite{holmes2005tie}, or designing an adaptive tie threshold that is dynamically calculated from the mean of HB, which has been shown to be proportionally related to the input stream samples \cite{yang2011moderated}.

\textbf{Grace period.} To prevent premature splits with small sample sizes that undermine the validity of the HB, a grace period \( n_{\min} \) specifies the minimum number of instances a leaf must observe before considering a split. However, without leveraging any information from the processed data, this can still result in computationally expensive split attempts or unnecessary delays in predictions. An alternative approach involves detecting frequent tie-breaking situations and applying steps to reduce the frequency of ties. For example, the default tie-breaking wait period can be adjusted by increasing the wait period for the child nodes after each tie is broken. This effect is cumulative until a valid split is found, after which the wait period is reset to the default value \cite{holmes2005tie}. Additionally, local statistics can be used to predict the optimal split time, minimizing delays and unnecessary split attempts. For instance, class distributions from previous split attempts can be used to estimate the minimum number of examples required before the HB is met \cite{losing2018enhancing}. Alternatively, \( n_{\min} \) can be adapted after an unsuccessful split attempt, adjusting based on the reasons for failure to ensure a split in the next iteration \cite{garcia2018hoeffding}. For example, if the best attributes are not too similar, but their GI difference is insufficient to trigger a split, the solution is to wait for additional examples until the HB decreases enough to be smaller than the GI difference, adjusting the \( n_{\min} \) accordingly. Similarly, if the top attributes are very similar in terms of GI, but the tie threshold \( \tau \) is not exceeded, more instances are needed to allow the HB to decrease below \( \tau \), with \( n_{\min} \) adjusted based on their IG difference \cite{garcia2018hoeffding}.

\textbf{Leaf activity.} Another memory-efficient strategy is to condition split decisions based on leaf activity. One approach involves comparing the number of samples that fall into existing leaves against those that do not \cite{yang2011optimized}. As more samples fall into existing leaves, one can refrain from updating the tree structure. Alternatively, a normalized measure can be devised to quantify how many instances have been observed at a particular leaf relative to the average number of instances per leaf since its creation \cite{garcia2022green}. Based on this value, nodes can either be deactivated when their activity is low, halting further splits to conserve resources, or more aggressive expansion strategies can be applied when activity exceeds a threshold, accelerating the growth of important nodes by relaxing the splitting rule to a less strict alternative \cite{garcia2022green}. These adaptive expansion modes offer a more nuanced approach to tree growth, ensuring computational resources are focused on expanding nodes that significantly influence the model’s accuracy, while avoiding unnecessary splits in less relevant parts of the tree. Alternatively, the least promising leaves can be deactivated based on the probability that examples will reach those leaves and their observed error rate \cite{domingos2000mining}. If a better split is found, the leaves of that split are deactivated, and their statistics are preserved. A new split is then performed, creating new leaves. If, during split re-evaluation, a previously deactivated split is found to be the best option, the saved statistics are restored rather than starting the process from scratch.

\textbf{DFDT.} The proposed algorithm combines the aforementioned four mechanisms to optimize the trade-off between accuracy and resource efficiency. It incorporates dynamically adjustable grace periods \cite{garcia2018hoeffding} and tie thresholds \cite{yang2011moderated,yang2011optimized}, allowing the algorithm to either delay or accelerate splits depending on data variability. Additionally, DFDT combines the adaptive expansion modes \cite{garcia2022green}, determined by leaf activity with stricter and looser splitting conditions \cite{da2018strict,barddal2020regularized}. For nodes with low activity, DFDT deactivates the leaf node, conserving memory and computational resources. For nodes with moderate activity, DFDT applies the conservative splitting constraints, ensuring controlled growth. For highly active nodes, DFDT potentially employs a skipping mechanism, enabling rapid growth in response to significant data changes. 

\section{Methodology}
\label{sec:algorithm}

The proposed DFDT integrates previous research advancements to optimize the trade-off between accuracy and resource efficiency of the original VFDT. These modifications are here described in detail, while referring to the pseudo-code in Algorithm \ref{alg}. The initialization phase creates the root node structure of the decision tree (Alg. 1, Step 2), sets up estimators (Alg. 1, Step 3), and instance counters (Alg. 1, Step 4). This initialization involves only constant-time operations, resulting in a computational complexity of \(O(1)\), independent of the number of instances or features. The main loop iterates over each instance \((X, y)\) in the data stream \(S\), running \(N\) times in total. For each instance, several steps are performed. First, the instance is routed through the tree to the corresponding leaf node, where the prediction \(\hat{y}\) is obtained (Alg. 1, Step 6). Assuming a balanced tree, the depth of traversal is \(O(\log_B |LH|)\), where \(|LH|\) is the number of leaf nodes and \(B\) is the branching factor. The prediction at the leaf node is a constant-time operation, \(O(1)\). Following this, the instance count and feature estimators are updated at the leaf node (Alg. 1, Steps 7, 8). This update takes \(O(F)\) time.

\begin{algorithm}[htbp]
\caption{Dynamic Fast Decision Tree (DFDT)}
\label{alg}
\begin{algorithmic}[1]
    \Procedure{DFDT}{$S$: stream, $\delta$: confidence level}
        \State $DFDT \leftarrow$ root and  $LH \leftarrow \text{hash of leaves}$ 
        \State Initialize $H_{LH_{\text{stat}}}, n_{\text{stat}}, H_{\text{stat}}, G_{\text{stat}}, HB_{\text{stat}}, n_{\text{min}}$
        \State $n, n_{\text{l}}, n_{\text{check}_{\text{l}}}, n_{\text{leaf}_{\text{l}}}, n_{\text{tree}_{\text{l}}} \leftarrow 0$ where $l=\text{root}$
        \For{each $(X, y)$ in $S$}
            \State Route $(X, y)$ to leaf $l$ and obtain prediction $\hat{y}$
            \State Update feature estimators, and class distribution at $l$ with $(X, y)$
            \State $n  \leftarrow n + 1$ and $n_l  \leftarrow n_l + 1$
            \If{$\frac{(n_l - n_{\text{leaf}_l}) \times |LH|}{n-n_{\text{tree}_{l}}} < 0.2$}
                \State Deactivate $l$
            \ElsIf{$> 2$}
                \State GrowFast $\leftarrow$ True
            \EndIf
            \If{(class distribution at $l$ is impure) and ($n_l - n_{\text{check}_l} > n_{\text{min}}$)}
                \State Update $HB_{stat}$ with updated $\epsilon$
                \If{\textsc{CanSplit}(GrowFast, $G(\cdot)$, $\epsilon$, $HB_{stat}$, $H_{stat}$, $G_{stat}$, $n_{stat}$)}
                    \State Replace leaf $l$ with a split node, and remove from set of leaves $LH$
                    \For{each leaf branch $b$ of the split}
                        \State Initialize estimators, set post-split distribution at $b$, add to $LH$
                        \State $n_b, n_{\text{leaf}_b}, n_{\text{check}_b} \leftarrow$ sum of class distribution at $b$ and $n_{\text{tree}_{b}} \leftarrow  n$
                    \EndFor
                \Else
                    \State $n_{\text{check}_l} \leftarrow n_l$
                    \If{$\Delta G < \epsilon$ and $\Delta G >  \text{avg}(HB_{\text{stat}})$}
                        \State $n_{\text{min}} \leftarrow \left\lceil \frac{R^2 \ln(1/\delta)}{2 (\Delta G)^2} \right\rceil$
                    \ElsIf{$\Delta G < \text{avg}(HB_{\text{stat}})$ and $\epsilon > \text{avg}(HB_{\text{stat}})$}
                        \State $n_{\text{min}} \leftarrow \left\lceil \frac{R^2 \ln(1/\delta)}{2 (\text{avg}(HB_{\text{stat}}))^2} \right\rceil$
                    \EndIf
                \EndIf
            \EndIf
        \EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}

While the HB ensures that splits are made with statistical confidence, it treats all nodes equally in terms of expansion, despite the fact that not all nodes contribute equally to the decision-making process. To improve VFDT's efficiency in allocating computational resources, a notion of relative importance can be introduced to dynamically adjust the rate of expansion at each node. To assess the activity level at each node, a fraction parameter is calculated as \cite{garcia2022green}:

\begin{equation}
\text{fraction} = \frac{(n_l - n_{\text{leaf}_l}) \times |LH|}{n-n_{\text{tree}_{l}}}
\end{equation}

where $(n_l - n_{\text{leaf}_l})$ is the number of instances observed at a particular leaf $l$ since its creation, $n-n_{\text{tree}_{l}}$ the total instances observed by the tree since the creation of leaf $l$, and $|LH|$ the current total number of leaves. If the fraction is below 0.2, the leaf node is deactivated, halting further splits to save on resources (Alg. 1, Steps 10), which is a constant-time operation, \(O(1)\). If the metric falls above 2, a boolean saves the intention to apply a more aggressive expansion strategy, accelerating the growth of important nodes (Alg. 1, Step 12). 

To avoid costly evaluations, the algorithm only checks for potential splits once a leaf node accumulates \(n_{\text{min}}\) instances since the last splitting opportunity (Alg. 1, Step 14). Then a split attempt is performed (Alg. 1, Step 16), with its pseudocode detailed in Algorithm \ref{alg2}. This check involves computing \(G(\cdot)\) values, requiring \(O(F)\) time, and sorting the \(G(\cdot)\) values (Alg. 2, Step 2), yielding a time complexity of \(O(F \log F)\). Next, the algorithm verifies whether the HB or tie-breaking threshold is satisfied (Alg. 2, Step 4). While in the VFDT, the tie threshold (\(\tau\)), which effectively controls minimum growth speed, remains static, DFDT, instead, dynamically calculates the tie threshold as \cite{yang2011moderated}:

\begin{equation}
\tau = \frac{\sum_{i=1}^{k} \epsilon_i}{k}
\end{equation}

where \(\epsilon_i\) is the HB value calculated at each instance \(i\), for \(k\) instances. This reduces the need for trial-and-error in selecting a fixed threshold, optimizing tree performance in dynamic environments. As the data stream becomes noisier, the adaptive \(\tau\) stabilizes the model. If the HB is satisfied, DFDT utilizes adaptive expansion modes, determined by the fraction-determined boolean. For nodes with moderate activity, DFDT only applies four conservative splitting constraints \cite{da2018strict}, ensuring controlled growth (Alg. 2, Step 12-15). For highly active nodes, i.e., when the fraction value exceeds 2, DFDT also checks two skipping conditions (Alg. 2, Step 5-11).

\begin{algorithm}[htbp]
\caption{Split conditions}
\label{alg2}
\begin{algorithmic}[1]
    \Procedure{CanSplit}{GrowFast = False, $G(\cdot), \epsilon, HB_{stat}, H_{stat}, G_{stat}, n_{stat}$}, 
        \State $\text{rank} \leftarrow$ Sorted $G(\cdot)$
            \State  \( G_{\text{best}} = \max(rank) \), \( G_{\text{second best}} = \max(rank \setminus \{G_{\text{best}}\}) \)
            \If{$G_{\text{best}} - G_{\text{second best}} \geq \epsilon$ or $\epsilon < \text{avg}(HB_{\text{stat}})$}
                \If{GrowFast}
                    \State $C1 \leftarrow H_l \geq \text{avg}(H_{\text{stat}}) + \sigma(H_{\text{stat}})$ 
                    \State $C2 \leftarrow G_{\text{best}} \geq \text{avg}(G_{\text{stat}}) + \sigma(G_{\text{stat}})$
                    \If{C1 $and$ C2} 
                        \State \Return True
                    \EndIf
                \EndIf
                \State $C3 \leftarrow H_l \geq \text{avg}(H_{LH_{\text{stat}}}) - \sigma(H_{LH_{\text{stat}}})$
                \State $C4 \leftarrow H_l \geq \text{avg}(H_{\text{stat}}) - \sigma(H_{\text{stat}})$
                \State $C5 \leftarrow  G_{\text{best}} \geq \text{avg}(G_{\text{stat}}) - \sigma(G_{\text{stat}})$
                \State $C6 \leftarrow n_l \geq \text{avg}(n_{\text{stat}})$
                \State Update $H_{stat}$ with $H_l$, $G_{stat}$ with $G_{best}$, and $n_{stat}$ with $n_l$
                \If{C3 $and$ C4 $and$ C5 $and$ C6} 
                    \State \Return True
                \EndIf
            \EndIf
        \State \Return False
    \EndProcedure
\end{algorithmic}
\end{algorithm}

The four primary constraints governing the split operate on the instance count, global entropy, historical entropy, and historical information gain. The instance count constraint ensures that splits only occur when the leaf has accumulated sufficient data, by checking if the current instance count \( n_l \) at the leaf $l$ meets or exceeds the average instance count recorded across all leaves under VFDT-satisfied conditions:

\begin{equation}
n_l \geq \text{avg}(n_{\text{stat}})
\end{equation}

Conversely, the remaining three constraints are evaluated using the same general formula \( \varphi(x, X) \), which determines that the current metric value \( x \) at a leaf is significantly different from reference values \( X \) when bigger than the mean of the values in \( X \) minus a standard deviation:

\begin{equation}
\varphi(x, X) = 
\begin{cases} 
   \text{True, if } x \geq \bar{X} - \sigma(X) \\
   \text{False, otherwise}
\end{cases}
\end{equation}

The global entropy constraint checks if the current entropy \( H_l \) of a leaf $l$ is high compared to the overall entropy of all leaves in the tree, denoted as $H_{LH_{\text{stat}}}$. Here, the set \( X \) consists of entropy values from all leaves, recorded continuously across the tree. The historical entropy constraint examines whether the current entropy \( H_l \) at a leaf $l$ is significantly higher than historical entropy values recorded across all leaves at the times when VFDT conditions were met, denoted as $H_{\text{stat}}$. This ensures that splits are performed only if the leaf’s entropy has increased meaningfully over time. The historical information gain constraint assesses whether the current information gain \( G_l \) of the best splitting feature at a leaf $l$ is notably higher than historical information gain values recorded across all leaves when VFDT conditions were met, denoted as $G_{\text{stat}}$. This avoids low-value splits.

Together, these constraints allow DFDT to restrict tree growth in nodes of moderate activity to instances where statistically significant differences exist. For highly active nodes, two skipping conditions \cite{da2018strict} are calculated according to the general formula \( \omega(x, X) \), which permits a split when the current metric \( x \), i.e. the entropy \( H_l \) and information gain \( G_{best} \), is at least one standard deviation above the average historical values in \( X \):

\begin{equation}
    \omega(x, X) = 
    \begin{cases} 
       \text{True, if } x \geq \bar{X} + \sigma(X) \\
       \text{False, otherwise}
    \end{cases}
\end{equation}

These splitting conditions work as a skipping mechanism to the conservative constraints. In these cases, splits are allowed to proceed directly if the current entropy and information gain are significantly higher than historical averages. Subsequently, the statistics for entropy, information gain, HB, and the number of instances seen are updated (Alg. 2, Step 16). All of these are constant-time operations. If the conditions are satisfied, a split is triggered (Alg. 2, Step 18). This split involves navigating the tree, sorting features, and creating new nodes, with the added complexity of re-initializing feature estimators for each branch (Alg. 1, Step 17-21). The time complexity becomes \(O(F \log F + B \cdot F)\), where \(B \cdot F\) represents the initialization cost for the new branches. If the split attempt fails at any point (Alg. 2, Step 21), the algorithm resets the instance count of the last checked split (Alg. 1, Step 23), and recalculates the grace period \( n_{\text{min}} \) based on specific conditions at each leaf (Alg. 1, Steps 24-28). This decision is based on the tie threshold \( \tau \), the difference in information gain between the top attributes \( \Delta G \) and the statistical confidence bound \( \epsilon \), derived from the HB \cite{garcia2018hoeffding}, with two possible scenarios. In the first scenario, \( \Delta G \) is smaller than \( \epsilon \), but still exceeds \( \tau \), suggesting that more data is needed to confirm the observed gain. In this case, \( n_{\text{min}} \) is increased to allow additional data accumulation, ensuring that the gain difference becomes statistically significant in the subsequent evaluation. In the second scenario, \( \Delta G \) is below  \( \tau \), indicating that the attributes are statistically similar, but \( \epsilon \) is still above \( \tau \). Here, \( n_{\text{min}} \) is further increased to delay the split until more data is gathered, allowing \( \epsilon \) to shrink sufficiently. Thus:

\begin{equation}
n_{\text{min}} = 
\begin{cases}
\left\lceil \frac{R^2 \ln(1/\delta)}{2 (\Delta G)^2} \right\rceil, & \text{if } \Delta G < \epsilon \text{ and } \Delta G > \tau \\\\
\left\lceil \frac{R^2 \ln(1/\delta)}{2 \tau^2} \right\rceil, & \text{if } \Delta G < \tau \text{ and } \epsilon > \tau
\end{cases}
\end{equation}

where \( \delta \) is a user-defined confidence parameter, and \( R \) is the range of heuristic values. This dynamic adjustment minimizes unnecessary computations and improves energy efficiency, while maintaining the model’s accuracy. Since split attempts (Alg. 1, Steps 14–30) occur only once every \(n_{\text{min}}\) instances, their overall contribution to the time complexity is weighted by the grace period factor. Thus, the total time complexity is: $N \cdot O(\log_B |LH| + F) + \frac{N}{n_{\text{min}}} \cdot O(F \log F + B \cdot F)$.

\section{Experiments}
\label{sec:experiments}

This section evaluates the performance of the Dynamic Fast Decision Tree (DFDT) against the VFDT, svfdt\_I, and svfdt\_II, while performing an ablation study on the proposed algorithm. The following notation is used to described the isolated contributions of each individual component of the algorithm: additional splitting conditions ($DFDT$), adaptive expansion nodes ($\text{VFDT}_{E}$), adaptive tie threshold ($\text{VFDT}_{T}$), and adaptive grace period ($\text{VFDT}_{G}$). With $\text{VFDT}_{GT}$, $\text{DFDT}_{GE}$, $\text{DFDT}_{GT}$, $\text{DFDT}_{TE}$, and $\text{DFDT}_{GTE}$ being the respective combinations. All models and algorithm components were implemented using a custom experimental environment built with the pystream library\footnote{https://github.com/vturrisi/pystream}, a Python-based tool for data stream mining, inspired by the Massive Online Analysis (MOA) framework. It supports both Python and Cython implementations, with the latter significantly reducing runtime overhead by compiling Python code into C.

For evaluation, a diverse set of real-world datasets obtained from the USP Data Stream Repository were used \cite{souza2020challenges}. These are shown in Table \ref{tab:datasets}, encompassing both binary and multiclass classification tasks. The experiments were conducted using a prequential (test-then-train) evaluation strategy \cite{gama2013evaluating}, monitoring the interrelationships among \textit{accuracy}, \textit{memory} usage, and \textit{runtime}. To provide a more intuitive and holistic assessment, an \textit{efficiency} metric was computed as the equal-weighted sum of these three components. Each component was normalized on a per-dataset basis to ensure consistency and comparability across datasets of varying scales. To maintain a consistent interpretation, the normalized memory and runtime values were inverted, such that lower values correspond to better performance in these dimensions. 

\begin{table}[ht]
\centering
\caption{Experimental datasets}
\label{tab:datasets}
\begin{tabular}{lccc}
\toprule
\textbf{Name}  & \textbf{No. of classes} & \textbf{No. of features} & \textbf{No. of examples} \\ \midrule
Chess     & 3              & 7               & 534             \\ 
Keystroke       & 4              & 10              & 1,600           \\ 
Luxembourg     & 2              & 30              & 1,901           \\ 
Ozone        & 2              & 72              & 2,534           \\ 
NOAA Weather      & 2              & 8               & 18,159          \\ 
SmartMeter LADPU        & 10             & 96              & 22,950          \\ 
Electricity      & 2              & 8               & 45,312          \\ 
Rialto Bridge Timelapse        & 10             & 27              & 82,250          \\ 
Posture          & 11             & 3               & 164,860         \\ 
KDDCUP99    & 23             & 41              & 494,021         \\ 
Forest Covertype     & 7              & 54              & 581,012         \\ 
Poker-hand        & 10             & 11              & 829,201       \\ \bottomrule
\end{tabular}
\end{table}

All algorithms were optimized for each dataset using a grid search strategy, with grace periods set to 100, 400, and 1000, and tie-splitting thresholds configured to 0.01, 0.05, and 0.1 as the hyperparameter settings. Across all tables, datasets are presented in ascending order based on the number of instances. Table \ref{tab:effiency_results_1} shows the $\text{DFDT}_{GTE}$'s efficiency against VFDT, svfdt\_I and svfdt\_II. 

\begin{table}[ht]
\centering
\caption{Efficiency}
\label{tab:effiency_results_1}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset}         & \textbf{$\text{DFDT}_{GTE}$}        & \textbf{VFDT}            & \textbf{svfdt\_I}      & \textbf{svfdt\_II} \\ \midrule
Chess           & 0.58               & \textbf{0.67}    & 0.64            & 0.64        \\ 
Keystroke       & \textbf{0.9}       & 0.66             & 0.66            & 0.66        \\ 
Luxembourg      & 0.86               & \textbf{0.89}    & 0.84            & 0.84        \\ 
Ozone           & \textbf{0.99}      & 0.86             & 0.87            & 0.87        \\ 
NOAA            & \textbf{0.94}      & 0.75             & 0.75            & 0.78        \\ 
SmartMeter      & \textbf{0.74}      & 0.66             & 0.66            & 0.66        \\ 
Electricity     & \textbf{0.63}      & 0.45             & 0.52            & 0.51        \\
Rialto          & \textbf{0.69}      & 0.63             & 0.58            & 0.64        \\ 
Posture         & \textbf{0.77}      & 0.71             & 0.67            & 0.7         \\ 
KDDCUP99        & \textbf{0.57}      & 0.47             & 0.49            & 0.48        \\ 
ForestCoverType & \textbf{0.67}      & 0.62             & 0.59            & 0.61        \\ 
PokerHand       & \textbf{0.52}      & 0.37             & 0.42            & 0.4         \\ \bottomrule
\end{tabular}
\end{table}

Overall, $\text{DFDT}_{GTE}$ emerges as a well-rounded top performer, excelling in accuracy, memory usage, and runtime, with the exception of the Chess and Luxembourg datasets. This performance discrepancy is likely due to the limited number of instances, which hinders the full effectiveness of the adaptive mechanisms. Indeed, across all experiments conducted on the four smaller datasets, each containing fewer than 2,500 instances, no consistent performance trends were observed across the algorithms. Consequently, the subsequent analysis focuses on the remaining eight larger real-world datasets. To provide a more nuanced perspective on the trade-offs between accuracy and memory usage across algorithms, Figure \ref{fig:acc-mem-results} illustrates the memory usage and accuracy metrics for each dataset.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{figures/acc_memory_general.png}
    \caption{Prequential accuracy and memory usage}
    \label{fig:acc-mem-results}
\end{figure}

As shown, $\text{DFDT}_{GTE}$ maintains consistent high performance, outperforming other algorithms in nearly all datasets except ForestCoverType. Although it does not achieve the highest accuracy in this dataset, the difference in accuracy is minimal, and $\text{DFDT}_{GTE}$ demonstrates significantly better memory efficiency. Across the other datasets, $\text{DFDT}_{GTE}$ generally occupies more memory than competing algorithms, reflecting a trade-off for its enhanced accuracy. This balance between memory usage and accuracy suggests that $\text{DFDT}_{GTE}$ is optimized for scenarios where accuracy is prioritized, yet it remains reasonably efficient in memory-constrained environments. To further study this claim, an ablation study is performed on $\text{DFDT}_{GTE}$'s components, with the obtained accuracy across the largest real-world datasets, is presented in Table \ref{tab:all_results}.

\begin{table}[ht]
    \caption{Prequential accuracy}
    \label{tab:all_results}
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Algorithm} & \textbf{NOAA} & \textbf{SmartMeter} & \textbf{Electricity} & \textbf{Rialto} & \textbf{Posture} & \textbf{ForestCoverType} & \textbf{PokerHand} \\
        \midrule
        VFDT            & 73.1 $\pm$ 2.2 & 52.4 $\pm$ 2.3 & 80.3 $\pm$ 1.5 & 35.0 $\pm$ 4.9 & 50.5 $\pm$ 2.1 & 79.5 $\pm$ 2.5 & 74.2 $\pm$ 4.5 \\
        VFDT$_{G}$          & 72.6 $\pm$ 2.0 & 52.4 $\pm$ 2.1 & 79.4 $\pm$ 1.6 & 34.1 $\pm$ 4.3 & 50.7 $\pm$ 1.5 & 78.3 $\pm$ 2.4 & 75.4 $\pm$ 4.5 \\
        VFDT$_{GT}$          & 75.2 $\pm$ 0.1 & \textbf{60.8 $\pm$ 1.8} & \textbf{82.0 $\pm$ 1.1} & 61.9 $\pm$ 6.0 & \textbf{58.3 $\pm$ 0.5} & 85.3 $\pm$ 0.2 & 92.5 $\pm$ 2.3 \\
        VFDT$_{T}$           & \textbf{75.3 $\pm$ 0.6} & 59.9 $\pm$ 1.2 & 82.3 $\pm$ 1.0 & \textbf{62.0 $\pm$ 5.5} & 58.7 $\pm$ 0.7 & \textbf{88.7 $\pm$ 1.4} & \textbf{93.0 $\pm$ 2.1} \\
        DFDT$_{E}$           & 73.2 $\pm$ 1.2 & 52.6 $\pm$ 2.6 & 79.3 $\pm$ 0.7 & 34.8 $\pm$ 4.1 & 49.0 $\pm$ 1.7 & 70.3 $\pm$ 2.5 & 69.7 $\pm$ 4.1 \\
        DFDT$_{G}$           & 71.2 $\pm$ 2.0 & 52.3 $\pm$ 1.9 & 79.0 $\pm$ 0.6 & 32.1 $\pm$ 3.4 & 48.8 $\pm$ 1.7 & 74.6 $\pm$ 2.0 & 69.7 $\pm$ 2.0 \\
        DFDT$_{GE}$          & 72.4 $\pm$ 1.8 & 52.3 $\pm$ 1.9 & 79.1 $\pm$ 0.9 & 33.9 $\pm$ 4.1 & 48.4 $\pm$ 1.1 & 72.7 $\pm$ 1.9 & 69.0 $\pm$ 1.5 \\
        DFDT$_{GT}$          & 74.6 $\pm$ 0.5 & 54.8 $\pm$ 0.4 & 79.0 $\pm$ 0.4 & 41.7 $\pm$ 0.6 & 51.8 $\pm$ 1.0 & 80.1 $\pm$ 1.1 & 84.7 $\pm$ 2.7 \\
        DFDT$_{GTE}$         & 75.1 $\pm$ 0.3 & 54.8 $\pm$ 0.4 & 79.4 $\pm$ 0.6 & 42.3 $\pm$ 0.5 & 52.3 $\pm$ 0.6 & 70.9 $\pm$ 1.3 & 72.9 $\pm$ 5.4 \\
        DFDT$_{T}$           & 74.9 $\pm$ 0.3 & 59.3 $\pm$ 1.0 & 80.8 $\pm$ 1.4 & 51.2 $\pm$ 7.1 & 56.1 $\pm$ 0.9 & 84.4 $\pm$ 2.8 & 85.9 $\pm$ 4.1 \\
        DFDT$_{TE}$          & 75.0 $\pm$ 0.4 & 59.8 $\pm$ 0.6 & 80.0 $\pm$ 1.3 & 46.6 $\pm$ 4.1 & 53.0 $\pm$ 0.4 & 70.1 $\pm$ 2.6 & 72.4 $\pm$ 0.9 \\
        svfdt\_I           & 72.7 $\pm$ 2.1 & 52.3 $\pm$ 2.0 & 79.7 $\pm$ 1.0 & 33.3 $\pm$ 4.6 & 49.3 $\pm$ 1.8 & 75.7 $\pm$ 4.1 & 72.4 $\pm$ 4.2 \\
        svfdt\_II          & 73.3 $\pm$ 2.4 & 52.3 $\pm$ 2.0 & 80.5 $\pm$ 1.7 & 34.3 $\pm$ 5.0 & 50.2 $\pm$ 2.0 & 77.5 $\pm$ 3.5 & 74.8 $\pm$ 4.0 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\iffalse

\begin{table}[ht]
    \label{tab:all_results}
    \centering
    \caption{Accuracy across datasets}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Algorithm} & \textbf{Chess} & \textbf{Keystroke} & \textbf{Luxembourg} & \textbf{Ozone} & \textbf{NOAA} & \textbf{SmartMeter} & \textbf{Electricity} & \textbf{Rialto} & \textbf{Posture} & \textbf{KDDCUP99} & \textbf{ForestCoverType} & \textbf{PokerHand} \\
        \hline
        VFDT            & 0.675 (0.006) & 0.784 (0.018) & 0.949 (0.037) & 0.935 (0.001) & 0.731 (0.022) & 0.524 (0.023) & 0.803 (0.015) & 0.35 (0.049) & 0.505 (0.021) & 0.997 (0.0) & 0.795 (0.025) & 0.742 (0.045) \\
        \hline
        VFDT$_{G}$           & 0.673 (0.0) & 0.784 (0.0) & 0.956 (0.0) & 0.935 (0.0) & 0.726 (0.02) & 0.524 (0.021) & 0.794 (0.016) & 0.341 (0.043) & 0.507 (0.015) & 0.997 (0.0) & 0.783 (0.024) & 0.754 (0.045) \\
        \hline
        VFDT$_{GT}$          & 0.673 (0.0) & 0.828 (0.005) & 0.956 (0.0) & 0.935 (0.0) & 0.752 (0.001) & \textbf{0.608 (0.018)} & 0.82 (0.011) & 0.619 (0.06) & 0.583 (0.005) & 0.998 (0.0) & 0.853 (0.002) & 0.925 (0.023) \\
        \hline
        VFDT$_{T}$           & 0.698 (0.036) & 0.818 (0.023) & 0.901 (0.07) & 0.933 (0.001) & 0.753 (0.006) & 0.599 (0.012) & 0.823 (0.01) & 0.62 (0.055) & 0.587 (0.007) & 0.998 (0.0) & 0.887 (0.014) & \textbf{0.93 (0.021)} \\
        \hline
        DFDT$_{E}$           & 0.719 (0.0) & 0.791 (0.002) & 0.995 (0.0) & 0.935 (0.001) & 0.732 (0.012) & 0.526 (0.026) & 0.793 (0.007) & 0.348 (0.041) & 0.49 (0.017) & 0.997 (0.0) & 0.703 (0.025) & 0.697 (0.041) \\
        \hline
        DFDT$_{G}$           & 0.673 (0.0) & 0.784 (0.0) & 0.956 (0.0) & 0.935 (0.0) & 0.712 (0.02) & 0.523 (0.019) & 0.79 (0.006) & 0.321 (0.034) & 0.488 (0.017) & 0.997 (0.0) & 0.746 (0.02) & 0.697 (0.02) \\
        \hline
        DFDT$_{GE}$          & 0.673 (0.0) & 0.784 (0.0) & 0.956 (0.0) & 0.935 (0.0) & 0.724 (0.018) & 0.523 (0.019) & 0.791 (0.009) & 0.339 (0.041) & 0.484 (0.011) & 0.997 (0.0) & 0.727 (0.019) & 0.69 (0.015) \\
        \hline
        DFDT$_{GT}$          & 0.673 (0.0) & 0.827 (0.003) & 0.956 (0.0) & 0.935 (0.0) & 0.746 (0.005) & 0.548 (0.004) & 0.79 (0.004) & 0.417 (0.006) & 0.518 (0.01) & 0.998 (0.0) & 0.801 (0.011) & 0.847 (0.027) \\
        \hline
        DFDT$_{GTE}$         & 0.673 (0.0) & 0.827 (0.002) & 0.956 (0.0) & 0.935 (0.0) & 0.751 (0.003) & 0.548 (0.004) & 0.794 (0.006) & 0.423 (0.005) & 0.523 (0.006) & 0.997 (0.0) & 0.709 (0.013) & 0.729 (0.054) \\
        \hline
        DFDT$_{T}$           & 0.686 (0.029) & 0.796 (0.023) & 0.85 (0.06) & 0.933 (0.001) & 0.749 (0.003) & 0.593 (0.01) & 0.808 (0.014) & 0.512 (0.071) & 0.561 (0.009) & 0.998 (0.0) & 0.844 (0.028) & 0.859 (0.041) \\
        \hline
        DFDT$_{TE}$          & 0.701 (0.042) & 0.818 (0.025) & 0.85 (0.06) & 0.933 (0.001) & 0.75 (0.004) & 0.598 (0.006) & 0.8 (0.013) & 0.466 (0.041) & 0.53 (0.004) & 0.997 (0.0) & 0.701 (0.026) & 0.724 (0.009) \\
        \hline
        svfdt\_I             & 0.675 (0.006) & 0.784 (0.018) & 0.949 (0.037) & 0.935 (0.001) & 0.727 (0.021) & 0.523 (0.02) & 0.797 (0.01) & 0.333 (0.046) & 0.493 (0.018) & 0.997 (0.0) & 0.757 (0.041) & 0.724 (0.042) \\
        \hline
        svfdt\_II            & 0.675 (0.006) & 0.784 (0.018) & 0.949 (0.037) & 0.935 (0.001) & 0.733 (0.024) & 0.523 (0.02) & 0.805 (0.017) & 0.343 (0.05) & 0.502 (0.02) & 0.997 (0.0) & 0.775 (0.035) & 0.748 (0.04) \\
        \hline
\end{tabular}}
\end{table}

\fi

Looking at Table \ref{tab:all_results}, we see however that despite $\text{DFDT}_{GTE}$ having better accuracy than VFDT, svfdt\_I and svfdt\_II, it seems that  exclusively adding adaptive tie threshold $\text{VFDT}_{T}$ or also adaptive grace period $\text{VFDT}_{GT}$ yields better results. This hints that most of the predictive performance improvement comes from these mechanisms, while the additional splitting constraints and adaptive expansion modes favor memory usage. Inspecting the accuracy evolution of the largest dataset PokerHand provides further light on how accuracy and memory usage evolve as more instances are processed. Figure \ref{fig:poker-results} is organized into three sections: an ablation of the three adaptive mechanisms while using the additional splitting constraints, an ablation of the two adaptive parameters while using the original splitting condition, and a direct comparison between svfdt\_I, svfdt\_II, and DFDT.

\iffalse

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{figures/pokerhand_acc_memory.png}
    \caption{PokerHand results}
    \label{fig:poker-results}
\end{figure}

\fi

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/1.png}
        \caption{Ablation of adaptive mechanisms with splitting constraints}
        \label{fig:poker-results-1}
    \end{subfigure}
    \par\vspace{0.05cm}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/2.png}
        \caption{Ablation of adaptive parameters with original HB condition}
        \label{fig:poker-results-2}
    \end{subfigure}
    \par\vspace{0.05cm}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/3.png}
        \caption{Comparison of svfdt\_I, svfdt\_II, and DFDT}
        \label{fig:poker-results-3}
    \end{subfigure}
    \caption{Prequential accuracy and memory over time.}
    \label{fig:poker-results}
\end{figure}


The first pair of graphs demonstrates the impact of incorporating the adaptive tie threshold $\text{DFDT}_{T}$. This is particularly evident as more data instances are processed. However, this improvement comes at the cost of increased memory usage growing rapidly. To mitigate this issue, introducing the adaptive grace period $\text{DFDT}_{GT}$ helps control tree growth, effectively reducing memory usage while only slightly affecting accuracy. Further enhancements, such as the adaptive expansion modes with leaf deactivation $\text{DFDT}_{GTE}$, yield even more substantial memory savings, however substantially hindering accuracy. The second pair of graphs shows that adaptive tie threshold has the same positive impact on accuracy and memory when using the original splitting condition without the additional constraints. Finally, the third pair of graphs highlights the advancements brought by the proposed algorithm. While svfdt\_I and svfdt\_II have a more stable performance, they lack the predictive and resource efficiency of DFDT with the four combined mechanisms: additional splitting conditions, adaptive expansion nodes, adaptive tie threshold, and adaptive grace period. For a big picture of this trade-off between accuracy and memory, their Nemeyi critical difference diagrams are shown in Figures \ref{fig:cd-acc} and \ref{fig:cd-mem}, respectively.

\begin{figure}[!h]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.75\linewidth]{figures/cd_accuracy.png}
        \caption{Accuracy}
        \label{fig:cd-acc}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.75\linewidth]{figures/cd_memory.png}
        \caption{Memory}
        \label{fig:cd-mem}
    \end{minipage}
\end{figure}

While VFDT implementations with adaptive parameters consistently achieve high ranks, above 4, in terms of accuracy, this improved performance comes with a trade-off of increased tree growth, adversely impacting memory usage. Conversely, algorithms using the adaptive tie threshold along with the additional splitting constraints, adaptive expansion modes and/or adaptive grade period have significantly better memory usage. To further understand how the different combinations of components affect this trade-off, Table \ref{tab:eff_result} revisits the aggregate efficiency ranking across real-world datasets.

\begin{table}[ht]
\centering
\caption{Evaluation ranking}
\label{tab:eff_result}
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{Efficiency} & \textbf{Accuracy} & \textbf{Memory} & \textbf{Runtime} \\
\midrule
$\text{DFDT}_{GT}$       & \textbf{0.78} & 0.54     & 0.91   & \textbf{0.93} \\ 
$\text{DFDT}_{GTE}$      & 0.74          & 0.43     & 0.94   & 0.87          \\ 
$\text{VFDT}_{GT}$       & 0.72          & \textbf{0.86} & 0.47   & 0.84          \\ 
$\text{VFDT}_{G}$        & 0.7           & 0.28     & 0.95   & 0.89          \\ 
$\text{DFDT}_{G}$        & 0.67          & 0.18     & \textbf{0.97} & 0.87          \\ 
$\text{DFDT}_{GE}$       & 0.67          & 0.19     & \textbf{0.97} & 0.86          \\ 
VFDT            & 0.65          & 0.29     & 0.92   & 0.74          \\ 
svfdt\_II       & 0.65          & 0.29     & 0.93   & 0.74          \\ 
svfdt\_I        & 0.64          & 0.23     & 0.95   & 0.75          \\ 
$\text{DFDT}_{T}$        & 0.61          & 0.56     & 0.59   & 0.72          \\ 
$\text{DFDT}_{TE}$       & 0.58          & 0.38     & 0.66   & 0.73          \\ 
$\text{VFDT}_{T}$        & 0.52          & 0.8      & 0.0    & 0.77          \\ 
$\text{DFDT}_{E}$        & 0.44          & 0.31     & 0.94   & 0.08          \\ \bottomrule
\end{tabular}
\end{table}

Overall, the results reveal that the hoeffding tree with adaptive parameters ($\text{VFDT}_{GT}$) excels in scenarios where accuracy is paramount, but may fall short in memory-constrained environments. Thus, careful consideration must be given to the memory overhead introduced, especially in extreme edge environments. On the other hand, the proposed algorithm with all the various adaptive components ($\text{DFDT}_{GTE}$) strikes a compelling balance between accuracy, memory usage and runtime. However, omitting adaptive expansion modes ($\text{DFDT}_{GT}$) seems to lead to even superior overall efficiency, by increasing accuracy and reducing computation time. The choice of algorithm configuration should be guided by the specific demands of the deployment environment. If memory constraints are a significant concern, the full $\text{DFDT}_{GTE}$ is recommended. For scenarios where maximum accuracy is paramount, and memory usage is less of an issue excluding the additional splitting constraints and/or adaptive expansion nodes alone may suffice.

\section{Conclusions}
\label{sec:conclusions}

In this paper, the Dynamic Fast Decision Tree (DFDT) was introduced for memory-constrained data stream mining. DFDT consolidates different adaptive strategies from prior research to control decision tree growth effectively. These features make DFDT particularly suitable for resource-constrained environments, such as IoT devices, edge computing applications, and real-time monitoring systems. Experiments show the proposed consolidated adaptive strategies achieve improved predictive performance (0.43 vs. 0.29 ranking) compared to VFDT and SVFDT, with significantly reduced runtime under 12 diverse datasets.

Despite its strengths, DFDT has limitations, particularly in handling non-stationary data streams. While it inherits VFDT’s robust response to concept drifts through resplitting nodes and approximating information gains \cite{manapragada2020emergent}, it could be further improved by incorporating adaptive strategies, such as memory-conservative alternate subtree growth when a concept drift is detected at a node, using loss-based detection mechanisms like ADWIN \cite{bifet2009adaptive}. Addressing such limitations could enhance DFDT’s adaptability to diverse, rapidly changing environments. Additionally, testing DFDT on a broader range of datasets, including noisy imbalanced high-dimensional data streams, could provide further insights into its robustness.

\section*{Acknowledgments}

This work was funded by the EU, through the Portuguese Republic’s Recovery and Resilience Plan, within the project PRODUTECH R3. It was also funded by the Portuguese Foundation for Science and Technology under project doi.org/10.54499/UIDP/00760/2020 and Ph.D. scholarship PRT/BD/154713/2023.

\bibliography{references} 

\end{document}
