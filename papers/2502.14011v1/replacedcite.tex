\section{Related work}
\label{sec:related}

The core component for incrementally constructing decision trees (DTs) on edge devices, while maintaining a fixed time complexity per sample, is approximation-based splitting ____. As new instances arrive, they are processed from the root to a leaf node, updating statistics at each node. These updates are used to periodically adjust heuristic values for each attribute, such as information gain (IG) and Gini index (GI) ____. The tree attempts to perform splits based on a statistical bound that quantifies the confidence interval for the heuristic function, given a minimum amount of data. Typically, DTs compare the Hoeffding bound (HB) to the difference in evaluation between the best and second-best attribute splits ____. When this difference exceeds the bound, the leaf node is split into child nodes.

\textbf{Rules.} While traditional incremental DTs exclusively focus on evaluating the top two attributes, one can introduce extra splitting rules to promote even more valuable splits. For example, one can incorporate the fluctuation of the HB, tracking its mean, minimum, and maximum values, along with an accuracy metric, as a pre-pruning condition for split decisions ____. Alternatively, constraints can be applied that compare current metrics against historical data and cross-leaf information, while introducing a skipping mechanism to bypass these constraints when significant changes in the data are detected ____. Furthermore, a constraint can be added to assess whether the best-ranked feature, chosen for splitting at a leaf node, provides substantial gains compared to the gains observed in previous splits within the same branch ____.

\textbf{Tie break.} A critical aspect of incremental DTs is the tie-breaking procedure. While using the statistical difference in IG between two attributes helps control tree growth, competition between two equally favored split candidates can hinder progress, especially when either option would be equally suitable. To mitigate this, if the difference in heuristic values exceeds a predefined tie threshold, denoted as $\tau$, the split is performed. This threshold effectively controls the minimum rate of tree growth, with the attributes’ ability to separate before reaching the threshold influencing the speed at which the tree expands. However, a fixed $\tau$ value may cause ties to be broken prematurely, before a meaningful decision can be made, due to a lack of suitable candidates rather than a true tie situation. To address this, alternative approaches can be employed, such as comparing the difference between the best and second-best candidates with the difference between the second-best and worst candidates ____, or designing an adaptive tie threshold that is dynamically calculated from the mean of HB, which has been shown to be proportionally related to the input stream samples ____.

\textbf{Grace period.} To prevent premature splits with small sample sizes that undermine the validity of the HB, a grace period \( n_{\min} \) specifies the minimum number of instances a leaf must observe before considering a split. However, without leveraging any information from the processed data, this can still result in computationally expensive split attempts or unnecessary delays in predictions. An alternative approach involves detecting frequent tie-breaking situations and applying steps to reduce the frequency of ties. For example, the default tie-breaking wait period can be adjusted by increasing the wait period for the child nodes after each tie is broken. This effect is cumulative until a valid split is found, after which the wait period is reset to the default value ____. Additionally, local statistics can be used to predict the optimal split time, minimizing delays and unnecessary split attempts. For instance, class distributions from previous split attempts can be used to estimate the minimum number of examples required before the HB is met ____. Alternatively, \( n_{\min} \) can be adapted after an unsuccessful split attempt, adjusting based on the reasons for failure to ensure a split in the next iteration ____. For example, if the best attributes are not too similar, but their GI difference is insufficient to trigger a split, the solution is to wait for additional examples until the HB decreases enough to be smaller than the GI difference, adjusting the \( n_{\min} \) accordingly. Similarly, if the top attributes are very similar in terms of GI, but the tie threshold \( \tau \) is not exceeded, more instances are needed to allow the HB to decrease below \( \tau \), with \( n_{\min} \) adjusted based on their IG difference ____.

\textbf{Leaf activity.} Another memory-efficient strategy is to condition split decisions based on leaf activity. One approach involves comparing the number of samples that fall into existing leaves against those that do not ____. As more samples fall into existing leaves, one can refrain from updating the tree structure. Alternatively, a normalized measure can be devised to quantify how many instances have been observed at a particular leaf relative to the average number of instances per leaf since its creation ____. Based on this value, nodes can either be deactivated when their activity is low, halting further splits to conserve resources, or more aggressive expansion strategies can be applied when activity exceeds a threshold, accelerating the growth of important nodes by relaxing the splitting rule to a less strict alternative ____. These adaptive expansion modes offer a more nuanced approach to tree growth, ensuring computational resources are focused on expanding nodes that significantly influence the model’s accuracy, while avoiding unnecessary splits in less relevant parts of the tree. Alternatively, the least promising leaves can be deactivated based on the probability that examples will reach those leaves and their observed error rate ____. If a better split is found, the leaves of that split are deactivated, and their statistics are preserved. A new split is then performed, creating new leaves. If, during split re-evaluation, a previously deactivated split is found to be the best option, the saved statistics are restored rather than starting the process from scratch.

\textbf{DFDT.} The proposed algorithm combines the aforementioned four mechanisms to optimize the trade-off between accuracy and resource efficiency. It incorporates dynamically adjustable grace periods ____ and tie thresholds ____, allowing the algorithm to either delay or accelerate splits depending on data variability. Additionally, DFDT combines the adaptive expansion modes ____, determined by leaf activity with stricter and looser splitting conditions ____. For nodes with low activity, DFDT deactivates the leaf node, conserving memory and computational resources. For nodes with moderate activity, DFDT applies the conservative splitting constraints, ensuring controlled growth. For highly active nodes, DFDT potentially employs a skipping mechanism, enabling rapid growth in response to significant data changes.