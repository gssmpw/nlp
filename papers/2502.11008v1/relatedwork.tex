\section{Related Work}
\paragraph{Counterfactual Reasoning.}
Counterfactual reasoning explores how outcomes change when certain variables are altered from their historical states. In Structural Causal Models (SCMs), Pearl’s \cite{pearl2009causality} “surgery” and do-calculus provide systematic ways to infer intervention outcomes, highlighting the deep causal knowledge required for accurate inference. Counterfactuals can be deterministic or probabilistic: deterministic settings yield predictable outcomes from given conditions, while probabilistic models incorporate inherent uncertainties. These methods have gained traction in domains like social sciences, where they help assess alternative policy outcomes and study causal mechanisms in observational data \cite{morgan2015counterfactuals}, and in medicine, where they enable personalized treatment and decision support \cite{johansson2016learning,shalit2017estimating,louizos2017causal,yoon2018ganite}. In artificial intelligence, counterfactual reasoning is crucial for interpretability and fairness, as it allows models to generate alternative scenarios and assess decision-making robustness. Although recent efforts extend counterfactual reasoning to LLMs \cite{jin2023cladder}, significant challenges persist, particularly regarding the complex variable relationships in high-dimensional text data. Consequently, bridging the gap between textual complexity and robust causal inference remains a focal point for future research.

\paragraph{LLMs in Counterfactual Learning.}
With the rapid evolution of LLMs, the research community has increasingly focused on their ability to perform causal inference \cite{zhang2023understanding,ashwani2024cause}. A prominent example is Causal Agent, an agent-based LLM framework that merges an LLM with causal tools for complex tasks \cite{han2024causal}. While it excels at identifying causal associations and conducting interventions, it largely omits counterfactual reasoning, limiting its applicability to more advanced scenarios. Current efforts to integrate counterfactual reasoning into LLMs typically follow two paths. First, commonsense-based approaches leverage background knowledge to imagine scenarios that defy established facts \cite{ning2024skeleton,chatzi2024counterfactual,musi2024fallacy,vicuna2023}, such as positing alternative historical outcomes. Second, graph-based methods employ formal causal graphs and external Python packages for computations, as seen in CausalCoT \cite{jin2023cladder} and CausalTool \cite{hua2024improving}. Although these methods effectively incorporate structured causal information, they often offload key calculations outside the LLM. %In contrast, our approach directly strengthens an LLM’s counterfactual reasoning through specialized instruction, enabling the model itself to conduct comprehensive causal inference without relying on external software. This strategy not only broadens the scope of LLM-driven causal analysis but also highlights the potential for more interpretable, self-contained solutions in complex domains.








% Therefore, we introduce LLM into these problems. We hope our research can improve the abilities of LLM on solving counterfactual reasoning problems. % Deterministic Counterfactual Reasoning, in contrast to probabilistic counterfactuals, does not have too much research on it. However, it is still an important part of the counterfactual reasoning, because it is an ideal and specific condition of the counterfactual reasoning. This research can become a basis for further research. So far, these related researches focus on simple causal problems which means they only focus on causal relations under five variables \cite{jin2023cladder}.