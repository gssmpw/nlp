% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{graphicx} 
\usepackage[table]{xcolor}
\definecolor{mygray}{gray}{0.9}
% Remove the "review" option to generate the final version.
%\usepackage[review]{acl}
\usepackage[]{acl}
\usepackage{booktabs}
% Standard package includes
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{graphicx}   % For \resizebox
\usepackage{makecell}   % For line breaks inside table cells
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{multirow} 
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{mdframed}
\usepackage{lipsum}  % 用于示例文本


% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage[absolute,overlay]{textpos}
\usepackage{float}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{CounterBench: A Benchmark for Counterfactuals Reasoning in Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Yuefei Chen$^1$\quad
Vivek K.Singh$^1$\quad 
Jing Ma$^2$\quad 
\textbf{Ruixiang Tang}$^{1}$\thanks{Corresponding Email: ruixiang.tang@rutgers.edu }\\
  $^1$Rutgers University\;\;\;  $^2$Case Western Reserve University\;\;\;
}
 
\begin{document}
\maketitle
\begin{abstract}
Counterfactual reasoning is widely recognized as one of the most challenging and intricate aspects of causality in artificial intelligence. In this paper, we evaluate the performance of large language models (LLMs) in counterfactual reasoning. In contrast to previous studies that primarily focus on commonsense causal reasoning, where LLMs often rely on prior knowledge for inference, we specifically assess their ability to perform counterfactual inference using a set of formal rules.  To support this evaluation, we introduce a new benchmark dataset, \textbf{CounterBench}, comprising 1K counterfactual reasoning questions. The dataset is designed with varying levels of difficulty, diverse causal graph structures, distinct types of counterfactual questions, and multiple nonsensical name variants. Our experiments demonstrate that counterfactual reasoning poses a significant challenge for LLMs, with most models performing at levels comparable to random guessing. To enhance LLM's counterfactual reasoning ability, we propose a novel reasoning paradigm, \textbf{CoIn}, which guides LLMs through iterative reasoning and backtracking to systematically explore counterfactual solutions. Experimental results show that our method significantly improves LLM performance on counterfactual reasoning tasks and consistently enhances performance across different LLMs. Our dataset is available at \href{https://huggingface.co/datasets/CounterBench/CounterBench}{https://huggingface.co/datasets/CounterBench\\/CounterBench}. 
\end{abstract}



% 正文内容

\section{Introduction}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\textwidth]{radar.pdf}
  \caption{Comparison of accuracy scores on the CounterBench dataset across different strategies: our proposed CoIn paradigm versus baseline approaches (Standard, CausalCoT \cite{jin2023cladder}, and Solver \cite{hua2024improving}), evaluated using GPT-4o. Our CounterBench dataset includes four kinds types. Basic focuses on exploring how a single change in a causal variable. Joint involves simultaneous changes in multiple causes, Nested involves stepwise hypothetical assumptions about multiple variables, and Conditional evaluates counterfactuals under observed conditions. %Results demonstrate that CoIn achieves substantial performance improvements over existing methods in this model.
  }
  \label{fig:fixed}
\end{figure}

Counterfactual reasoning, residing at the pinnacle of Pearl’s Causal Hierarchy \cite{pearl2018book}, underpins the “what if” inquiries essential to human cognition and decision-making across critical fields such as healthcare, business, public administration, and science \cite{gvozdenovic2021causal, kyrimi2025counterfactual, kasirzadeh2021use, koonce2011judging, gow2016causal, loi2012note}. For example, a consumer who declined an extended warranty may later wonder, "What if I had purchased it, could I have avoided the repair costs?" This illustrates how counterfactual reasoning guides decision-making by evaluating missed opportunities and alternative outcomes \cite{krishnamurthy2002counterfactual}. While traditional causal inference methods\cite{sharma2020dowhy, chen2020causalml, feder2022causal} have enhanced the predictive accuracy, robustness, and explainability of NLP models, recent progress in LLMs has further enriched our ability to capture nuanced causal dependencies \cite{liu2024proficient, petroni2019language, liang2024mapping, tarassow2023potential, ma2024causal, liu2024large}. These advancements not only demonstrate sophisticated reasoning in tasks ranging from writing to programming but also pave the way toward emulating human-like intelligence and achieving artificial general intelligence \cite{li2024relationfirstmodelingparadigmcausal, alwin2023causalai, sahota2023causalai}. %In sum, the evolving interplay between LLMs and counterfactual inference highlights the critical importance of counterfactual reasoning in deepening our understanding of complex decision-making processes, setting the stage for further exploration of its challenges.

% Causal inference has proven instrumental in boosting the predictive accuracy, robustness, and explainability of Natural Language Processing models by modeling the underlying causal relationships among variables. At the same time, the rapid emergence of Large Language Models (LLMs) has reshaped the field of NLP, demonstrating advanced reasoning capabilities across tasks such as writing, knowledge-base querying, and programming.
% As these models continue to evolve, they reveal an expanding capacity for causal inference \cite{liu2024proficient, petroni2019language, liang2024mapping, tarassow2023potential}. Equally important, causality lies at the heart of human-like intelligence and is central to the pursuit of artificial general intelligence (AGI)—a reality that accentuates the growing importance of LLM-driven causal inference in advancing both theory and practice \cite{li2024relationfirstmodelingparadigmcausal,alwin2023causalai,sahota2023causalai}.

% Counterfactual reasoning, a central component of causal inference and the highest tier in the Pearl Causal Hierarchy \cite{pearl2018book}, enables the "what if" inquiries critical to human cognition and decision-making. High-stakes domains such as healthcare, business, public administration, and science have already leveraged counterfactual reasoning to glean valuable insights \cite{gvozdenovic2021causal, kyrimi2025counterfactual, kasirzadeh2021use, krishnamurthy2002counterfactual, koonce2011judging, gow2016causal, hendrickson2011counterfactual, loi2012note}. Nevertheless, the complexity of real-world counterfactual questions often surpasses the capabilities of traditional methods. As Large Language Models (LLMs) continue to mature, they offer a promising avenue for capturing nuanced relationships and reshaping our understanding of counterfactual inference \cite{ma2024causal, liu2024large}.

Despite recent advancements, progress in counterfactual reasoning using LLMs remains constrained by two primary challenges. First, there is currently no dedicated benchmark dataset for rigorously evaluating LLMs’ performance on counterfactual tasks, making it difficult to measure the models’ capacity to capture nuanced causal relationships. Second, even with advanced prompting techniques, Causal CoT\cite{jin2023cladder} and other iterative methods, LLMs often struggle to produce logically consistent, contextually appropriate counterfactuals \cite{ma2024causal,jin2023cladder,kiciman2023causal,zevcevic2023causal}. In response, this paper focuses on two key questions: 

%Despite these encouraging developments, LLMs face notable hurdles when tasked with counterfactual generation. Chief among these challenges is ensuring realistic causal constraints: without a robust understanding of domain-specific causal relationships, model-generated counterfactuals can become logically inconsistent or blatantly unrealistic.  \cite{ma2024causal,jin2023cladder,kiciman2023causal,zevcevic2023causal}. Additionally, large-scale language models can inherit and even amplify pre-existing biases from their training data, undermining the objectivity of counterfactual evaluations. Equally critical is the computational dimension: many counterfactual scenarios require precise algebraic or probabilistic operations, making it difficult to discern whether an LLM’s limitations stem from insufficient reasoning or lack of computational power. To bridge the gap, in this paper, our study want to addresses two key questions:

\textbf{How well do LLMs handle counterfactual reasoning?}
The absence of a standardized benchmark dataset has impeded rigorous empirical evaluation of LLMs' capabilities in capturing intricate causal relationships within complex counterfactual reasoning tasks. To address this, we present CounterBench, a comprehensive evaluation framework designed to assess counterfactual reasoning through 1K questions encompassing various domains and reasoning types. By systematically evaluating four key dimensions, it demands genuine reasoning beyond pattern recognition or memorized responses. Our experiments expose notable performance limitations in LLMs, even those equipped with advanced inference techniques. Most models like GPT-4o and Deepseek-V3 achieve accuracy of approximately $50\%$, equivalent to random guessing. Furthermore, our evaluation of state-of-the-art inference strategies shows only marginal improvements over baseline performance for most models. The models consistently struggle with maintaining logical coherence during multi-step reasoning processes and accurately handling causal relationships in complex scenarios. %Notably, smaller-scale LLMs, such as babbage\cite{OpenAI2024models}, demonstrate particular difficulty in following multi-step chain-of-thought prompting instructions and generating appropriate responses.
% This research makes two primary contributions: first, it addresses a critical lacuna in the field by establishing a rigorous evaluation framework for complex counterfactual reasoning tasks; second, it empirically demonstrates the substantial limitations of current reasoning strategies employed in contemporary LLMs, thereby highlighting the imperative for developing more sophisticated approaches to causal reasoning in neural language models.

% we present CounterBench, a novel comprehensive evaluation framework specifically engineered to assess counterfactual reasoning capabilities through the strategic incorporation of over 50 logically inconsistent variants within individual arguments, systematically evaluating four distinct dimensions, thereby necessitating authentic counterfactual reasoning rather than pattern recognition or retrieval of memorized associations. Our empirical investigation encompasses several state-of-the-art LLMs, through systematic evaluation utilizing various reasoning methodologies, our findings indicate that such approaches yield only modest improvements relative to baseline performance metrics, with models exhibiting persistent limitations in maintaining logical consistency and accurately operationalizing causal relationships across complex scenarios. tackles prevalent inference bottlenecks by leveraging iterative LLMs’ generative capabilities to internalize structured

\textbf{How to improve LLMs’ counterfactual reasoning abilities?}
To advance large language models’ counterfactual reasoning capabilities, this paper presents CoIn (\textbf{Co}unterfactual \textbf{In}ference), a novel approach that explicitly tackles the critical challenges of multi-step inference, which remain unresolved by previous methods. CoIn embeds a tailored search algorithm into the reasoning process, encouraging iterative and exploratory thinking at each step. A backtracking and evaluation mechanism further assesses the promise of each inference step, allowing the model to revert to more promising decision points when necessary. This systematic process substantially improves the accuracy of the counterfactual analysis.
Experiments on CounterBench demonstrate that CoIn achieves an accuracy of $93\%$, delivering a nearly $20\%$ improvement over Gemini-1.5-flash compared to alternative strategies (see Figure~\ref{fig:fixed}). The framework of this work is illustrated in Figure~\ref{fig:introduction}. The contributions of this work are summarized as follows:
% To enhance LLM's counterfactual reasoning ability, we propose \textbf{Co}unterfactual \textbf{In}ference (CoIn), a novel approach designed to improve LLMs’ counterfactual reasoning abilities by addressing the critical inference challenges in multi-step reasoning unsolved by previous methods. Our method imports a search algorithm into the process of reasoning. The algorithm will encourage the LLMs iterative thinking and explore to guide the reasoning process step by step. Additionally, Backtracking and evaluation mechanism will assesses the promise of each reasoning step and intelligently backtracks to more promising decision points when needed, thereby systematically enhancing the accuracy of counterfactual analysis. When applied to a deterministic counterfactual reasoning dataset, CoIn achieves an accuracy of $93\%$, boosting the performance of Gemini-1.5-flash by nearly $20\%$ compared to alternative strategies. Figure~\ref{fig:fixed} shows the result across different categories of counterfactual reasoning. We summarize the primary contributions of our work, with its framework shown in Figure~\ref{fig:introduction}, as follows:

% Unlike other iterative methods, such as Tree-of-Thought \cite{yao2024tree}, which require multiple prompts, CoIn streamlines the process with fewer prompts while maintaining robust performance. In summary, CoIn not only fills a crucial gap by rectifying serious issues in existing inference processes but also sets a new standard for effective and efficient counterfactual reasoning in LLMs. Figure~\ref{fig:introduction} shows the overview of our comprehensive framework for dataset generation, experimental analysis, and systematic paradigm optimization. 

% Building on existing work, we propose a counterfactual reasoning prompting strategy to address the issue. 
% \textbf{How well do LLMs handle counterfactual reasoning?}
%  To evaluate LLMs' capabilities in counterfactual reasoning, we introduce a comprehensive benchmarking dataset, CounterBench, that challenges models with complex scenarios across four types of counterfactuals: basic, joint, nested, and conditional. The dataset includes over 50 nonsensical variants within a single argument to prevent reliance on memorized knowledge and force models to engage in genuine causal reasoning. We assess several LLMs, including such as GPT-3.5, GPT-4o, Claude, and Gemini, using popular strategies like chain-of-thought. However, these methods result in only marginal improvements over baseline performance, with models often struggling to maintain logical consistency and accurately apply causal relationships. This highlights significant limitations in LLMs' current ability to perform complex counterfactual reasoning and emphasizes the need for more advanced reasoning strategies.

% \textbf{How to improve LLMs’ counterfactual reasoning abilities?}
% In this paper, we propose \textbf{Co}unterfactual \textbf{In}ference (\textbf{CoIn}), an approach that is inspired by the Algorithm-of-Thought (AoT) framework \cite{sel2023algorithm}. By leveraging LLMs’ generative capabilities to internalize algorithmic reasoning patterns, this integrated system inaugurates a novel in-context learning paradigm, wherein structured, iterative thinking steps guide the model. Our proposed method specifically targets the prevalent inference challenges encountered during counterfactual reasoning, providing a systematic solution to one of the most significant bottlenecks in current LLM-based causal analysis approaches. Additionally, unlike other iterative methods—such as Tree-of-Thought \cite{yao2024tree}—our approach requires fewer prompts while preserving robust reasoning performance. When applied to a deterministic counterfactual reasoning dataset, Our method achieves an accuracy of 93\%, boosting the performance of Gemini-1.5 by nearly 20\% compared to alternative strategies.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{Introduction_Structure.drawio_V1.pdf}
  \caption{Illustration of our framework. We create CounterBench, a dataset featuring four types of counterfactual questions (basic, joint, conditional, and nested). Based on this dataset, we conduct benchmark experiments on state-of-the-art methods, analyze their errors, and develop our enhanced reasoning framework that incorporates dynamic evaluation and intelligent backtracking capabilities.}
  \label{fig:introduction}
\end{figure*}

\begin{itemize}
    %\item In contrast to most other works on the causality of LLMs, our work explores the reasoning capabilities of LLMs in complex counterfactual causal questions and optimizes their performance in addressing these challenges. 
    % Our goal is to improve and assess the causal reasoning ability of LLM, focusing on noncommonsense deterministic counterfactual causal inference.
    \item We build a comprehensive dataset, \textbf{CounterBench}. The dataset contains 1K counterfactual reasoning questions, different difficulty levels, various causal graphs, distinct types of counterfactual questions and multiple nonsensical variant name combinations.  
    \item We benchmark LLMs with various inference strategies on \textbf{CounterBench}, and results reveal that most existing models (e.g., GPT-4o and Deepseek-V3) exhibit limited capabilities in performing counterfactual inference tasks.
    \item We propose a novel reasoning paradigm \textbf{CoIn} to guide LLM to facilitate iterative reasoning and address complex counterfactual inference tasks. CoIn achieved over 90\% accuracy on CounterBench, a 20\% improvement over the state-of-the-art baseline.
    
\end{itemize}

\begin{table*}[t]
\centering
\caption{Illustrative Counterfactual Query Types}
\label{tab:counterfactual_types}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|l|m{5cm}}
\toprule
\rowcolor{mygray}
%\hline
\textbf{Type} & \textbf{Query Template Example} & \textbf{Causal graph}\\ \hline

\textbf{Basic} & \makecell[l] {We know that X causes V1, V1 causes V2, V2 causes V3, and V3 causes V4, \\V4 causes V5, V5 causes Y. Would Y occur if not X instead of X?} &\includegraphics[width=5cm]{basic.drawio.pdf}
 \\  \hline

\textbf{Joint} & \makecell[l]{We know that X causes V1, V1 causes V2, V2 and V1 together cause V3, \\V3 causes V4, V4 and X together cause V5, and V5 causes Y. \\Would Y occur if not X and not V3?} &\includegraphics[width=5cm]{joint.drawio.pdf}
\\ \hline 

\textbf{Nested} & \makecell[l]{We know that X causes V1, V1 causes V2, V2 and V1 together cause V3,\\ V3 causes V4, V4 and V2 together cause V5, and V5 causes Y.\\ Assume not X, and based on this assumption, further suppose not V4.\\ Would Y occur?} &\includegraphics[width=5cm]{nested.drawio.pdf}
\\ \hline

\textbf{Conditional} & \makecell[l]{We know that X and V1 together cause V2, V2 causes V3, V3 causes V4, \\V4 causes V5, V5 causes Y.  We observed V1. Would Y occur if not X \\instead of X?} & \includegraphics[width=5cm]{conditional.drawio.pdf}
\\ \hline

\end{tabular}
}
\end{table*}

 

\section{CounterBench}

To evaluate the counterfactual reasoning capabilities of LLMs, we introduce a comprehensive benchmarking dataset specifically designed to measure their ability to handle complex causal reasoning tasks. This section details the structure of the dataset, the methodology for query generation, and the benchmarking process.

\subsection{Dataset Structure}

The dataset consists of two main components: a set of counterfactual queries and corresponding binary answers. Formally, the dataset is defined as $\mathcal{D} := \{(q_i, a_i) | i = 1, 2, ..., N\}$, where each $q_i$ is a counterfactual query, and $a_i \in \{\text{yes}, \text{no}\}$ represents the correct answer. Each query is derived from a deterministic Structural Causal Model (SCM) $M = \langle U, V, f \rangle$, where $U$ is the set of exogenous variables with assignments $u$, $V$ is the set of endogenous variables, and $f$ is the set of structural equations. For each $V_i \in V$, we have $V_i = f_i(Pa(V_i), U_i)$, where $Pa(V_i) \subseteq V$ denotes the parents of $V_i$, and $U_i$ refers to the subset of exogenous variables from $U$ that directly influence the value of $V_i$. Intervening on a set of variables $X \subseteq V$ and setting them to $x$ modifies the model to $M_x$, which deterministically defines the values of intervened variables given $u$. The dataset includes four types of counterfactual queries:

\begin{table*}[h!]
\centering
\scalebox{0.9}{
\begin{tabular}{p{3.5cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}|>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}}
\toprule
\rowcolor{mygray} \multicolumn{1}{c}{} & \multicolumn{5}{c}{\textbf{Standard}} & \multicolumn{5}{c}{\textbf{CausalCoT}} \\ \hline
\rowcolor{mygray} \textbf{Models} & \textbf{Basic} & \textbf{Cond.} & \textbf{Joint} & \textbf{Nested} & \textbf{Avg.} & \textbf{Basic} & \textbf{Cond.} & \textbf{Joint} & \textbf{Nested} & \textbf{Avg.} \\ \hline

GPT-3 (Davinci-002) & 56.8 & 50.2 & 48.8 & 51.6 & 51.8 & 51.2 & 41.9 & 51.2 & 51.6 & 49.0           \\ 
GPT-3 (Babbage-002) & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 3.6* & 7.6* & 1.2* & 19.6* & 8.0*\\ 
GPT-3.5 & 49.6 & 51.2 & 50.4 & 50.0 & 50.3 & 43.6 & 50.4 & 53.6 & 50.0 & 49.4\\ 
GPT-4o mini & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 57.2 & 66.4 & 60.0 & 63.2 & 61.7 \\ 
GPT-4o & 50.4 & 54.4 & 50.4 & 54.8 & 52.5 & 80.4 & \textbf{72.4} & \textbf{80.8} & \textbf{81.6} & \textbf{78.8} \\
Claude-3 (Sonnet) & 50.4 & 48.8 & 50.0 & 50.8 & 50.0 & 59.2 & 52.0 & 64.4 & 60.0 & 59.0\\ 
Claude-3.5 (Haiku) & 28.4 & 24.0 & 43.6 & 54.0 & 37.5 & 60.4 & 65.6 & 67.2 & 66.0 & 64.8\\ 
Gemini-1.5-flash & \textbf{75.2} & \textbf{65.6} & \textbf{67.2} & \textbf{76.0} & \textbf{71.0} & 72.4 & 70.8 & 72.4 & 78.4 & 73.5\\ 
Gemini-1.5-flash-8b & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 50.0$\dag$ & 66.8 & 67.2 & 65.2 & 65.2 & 66.1 \\ 
Deepseek-V3 & 50.4 & 50.4 & 50.0 & 50.0 & 50.0 & \textbf{80.8} & 70.4 & 76.4 & 77.6 & 76.3 \\
\hline
\end{tabular}
}
\caption{Model accuracy of standard method and CausalCoT across different reasoning categories Note: * The average accuracy is only $8\%$ because most of responses are not "Yes" or "No" but "incomprehensible", which means LLM cannot follow instruction of CausalCoT instruction well to infer. More details will be explained in the Appendix~\ref{sec:Benchmark Result Analysis}. $\dag$ indicates that the LLM predicts all questions as either 'Yes' or 'No', leading to a 50\% accuracy. }
\label{table:model_comparison}
\end{table*}


\paragraph{Basic Counterfactual.}  
The basic counterfactual type addresses simple "what-if" scenarios. In this scenario, it is formalized as $Y_x(u)$, which serves as a potential outcome expression. In causal reasoning, potential outcomes refer to the hypothetical results observed when a variable is set to a particular value \cite{holland1986statistics}. In $Y_x(u)$ expression, $Y$ is the outcome variable,$x$ is the value considered in the hypothetical scenario, and $u$ denotes the context. To illustrate, consider a lawn irrigation system that only activates when the weather is sunny and the soil is dry. In this example, $Y_x(u)$ describes the system’s potential behavior when the weather condition x is imposed while the soil condition (context $u$) remains unchanged. Consequently, when asking whether the system would activate if the weather changed to cloudy, the relevant counterfactual outcome is $Y_{cloudy}(u)$.

\paragraph{Joint Counterfactual.}  
This type involves a counterfactual scenario in which multiple variables are set simultaneously. Formally, it is expressed as $Y_{x,z}(u)$, representing the outcome $Y$ after setting $X = x$ and $Z = z$. For instance, a lawn irrigation system will activate if the weather is sunny, but it also requires dry soil as a trigger. Suppose $Z$ represents the weather condition and $X$ represents the soil moisture condition . The query asks if the irrigation system will activate when the weather changes to cloudy and the sensor detects that the soil is moist at the same time. The relevant counterfactual outcome is $Y_{cloudy, moist}(u)$. This scenario examines the combined effect of both actions happening simultaneously.

\paragraph{Nested Counterfactual.}  
Nested counterfactual involves sequential dependencies between variables. This is represented as $Y_{Z_x}(u)$, where an intervention on $X$ affects $Z$, which in turn impacts $Y$. For example, if the weather had been cloudy, which is a counterfactual weather state, and under this scenario, the sensor detected moist soil instead of dry soil, would the irrigation system activate? In this framework, $Z$ represents the weather condition, $X$ is the soil moisture reading. $Z$ depends on the counterfactual value of $X$ through the system’s structural causal relationships.

\paragraph{Conditional Counterfactual.}  
This type introduces observed conditions into the counterfactual world \cite{pearl2009causality}. Formally, it is written as $Y_x(u) \mid Z_x(u) = z$, asking how $Y$ would change if $X$ was set to $x$ while $Z = z$ being observed as a condition. For example, a lawn irrigation system will activate if the weather is sunny, but it also requires dry soil as a trigger. Now the weather is observed as sunny, the query evaluates whether the lawn irrigation system still activate or not if the sensor detects moist soil instead of dryness. Here, $Z = z$ represents the weather is observed as sunny, which is a given condition for reasoning. 
\subsection{Query Generation and Quality Assessment}

Each query consists of background information and a specific question. Table~\ref{tab:counterfactual_types} illustrates how samples are generated using various deterministic counterfactual query types. The background is constructed with causal graphs and story templates, and variable names are replaced by nonsensical, artificially generated words (e.g., “Kelp,” “Ziklo”) to prevent models from relying on memorized knowledge. In this way, we force LLMs to engage in causal reasoning rather than using prior knowledge in pretraining data. The dataset also features balanced distributions in multiple dimensions, with binary responses evenly split between 50\% “Yes” and 50\% “No.” This balance extends across different question types and difficulty levels, ensuring a uniform response distribution within each category. The dataset consists of 1,000 questions, categorized into four distinct types, with each type containing 250 questions. Within each type, there is an equal distribution of answers, comprising 125 "Yes" responses and 125 "No" responses. Additionally, the dataset is stratified based on five levels of difficulty, determined by the number of events present in each question, ranging from 5 to 9. Each difficulty level includes 200 questions, maintaining a balanced distribution of answers with 100 "Yes" and 100 "No". We also conduct a human evaluation, with further details provided in Appendix \ref{sec:human_label}.

\section{Benchmarking LLMs on CounterBench}

We conducted comprehensive experiments to systematically evaluate the performance of current LLMs on counterfactual reasoning tasks, demonstrating their capabilities using state-of-the-art reasoning techniques.

\noindent \textbf{Models.}
The tested LLM models include GPT-3.5 turbo, GPT-4o, GPT-4o mini, Davinci-002, Babbage-002 \cite{OpenAI2024models}, Claude 3.5 Haiku, Claude 3 Sonnet \cite{anthropic2024api}, Deepseek-V3 \cite{DeepSeek2024} and Gemini-1.5-Flash and Gemini-1.5-Flash-8B \cite{google2024gemini}. 


\noindent \textbf{Reasoning Strategies.}
In our baseline evaluations, we employed two distinct reasoning strategies to assess these models. The first relied on standard prompting methods without specialized instructions. The second used the advanced CausalCoT approach \cite{jin2023cladder}, an extension of the Chain-of-Thought prompting paradigm \cite{wei2022chain}. By integrating a systematic derivation process, including causal graph extraction, query type classification, data collection, and formalization, CausalCoT ensures robust logical consistency and high reasoning accuracy.



\noindent \textbf{Evaluation Settings.}
Within our evaluation framework, responses are classified into three distinct categories, "Yes", "No", and "Incomprehensible". The latter encompassing responses that are either ambiguous or lack clear meaning, typically manifesting when no answer is detected, such as NULL returns or mere query echoes. During inference, we set the temperature at 0. We employ inference accuracy as our primary performance metric.



\subsection{Experimental Results}
%This performance gap highlights the need for more effective strategies to enhance LLMs' counterfactual reasoning abilities.
As shown in Table~\ref{table:model_comparison}, the results indicate that without specific instructions, most LLMs struggle with counterfactual reasoning, performing no better than random guessing in terms of accuracy. Specifically, for model GPT-4o mini, we observed consistent predictions of either "Yes" or "No," resulting in a $50.0\%$ accuracy. Among all tested models, Gemini-1.5-flash achieved the highest baseline performance with an accuracy of $71.0\%$. Although the CausalCoT approach is designed to enhance the causal reasoning capabilities of LLMs, our empirical findings suggest that it does not significantly improve their performance in counterfactual reasoning tasks. Most models showed minimal or no improvement, as exemplified by GPT-3.5 Turbo. The best performance model in the CausalCoT is GPT-4o, achieved an accuracy of only $78.8\%$.

\noindent \textbf{Error Analysis.}
To systematically analyze the limitations of existing approaches, we conducted an error analysis on responses generated by CausalCoT. Our analysis focuses on three key components: causal data collection, inference process, and conclusion derivation. Through careful examination of each component, we identified three primary categories of errors:  \textbf{Wrong causal relationships:} This error occurs when LLMs cannot construct accurate causal graphs or extract known values from background information.  
\textbf{Wrong inference process:} This happens when LLMs, despite correctly identifying causal relationships, make incorrect predictions of the target event $Y$. \textbf{Wrong conclusion:} This type of error arises when LLMs reach contradictory final answers, even after correctly computing the value of $Y$. The distribution of these error categories is illustrated in Figure ~\ref{fig:pie}. Notably, $86\%$ of errors occur in the inference process, revealing that even with well-constructed causal graphs, LLMs struggle significantly with deriving accurate predictions through reasoning. 

% There are $86\%$ errors identified in inference process. Even with well-constructed causal graphs, LLMs demonstrate significant limitations in accurately deriving the target variable $Y$ through causal inference. Current approaches have proven insufficient in addressing these fundamental inference challenges, highlighting the need for a more robust and systematic methodology. %In the following section, we propose a novel framework that specifically targets these inference limitations while enhancing LLMs' causal reasoning abilities.
%\includegraphics[width=0.5\textwidth]{error_analysis_1.pdf}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{Error_Analysis_V1.pdf}
  \caption{Error Analysis of CausalCoT.}
  \label{fig:pie}
\end{figure}

% \begin{table}[h!]
% \centering
% \begin{tabular}{lc}
% \hline
% Method                  & Accuracy (\%) \\
% \hline
% GPT (davinci)           & 51.8          \\
% GPT (Babbage)           & 50            \\
% GPT 3.5                 & 50.3          \\
% GPT4-o-mini             & 50            \\
% Claude 3 (Sonnet)       & 50            \\
% Claude 3.5 (Haiku)      & 37.5          \\
% Gemini 1.5              & 71            \\
% Gemini 1.5 8b           & 50            \\
% \hline
% \end{tabular}
% \caption{Performance of all models on our dataset V1 without instructions}
% \label{table:model_comparison}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{lc}
% \hline
% Method                  & Accuracy (\%) \\
% \hline
% GPT (davinci)           & 49          \\
% GPT (Babbage)           & 28.6            \\
% GPT 3.5                 & 50.1          \\
% GPT 4-o-mini             & 50            \\
% Claude 3 (Sonnet)       & /            \\
% Claude 3.5 (Haiku)      & /         \\
% Gemini 1.5              & 73.5            \\
% Gemini 1.5 8b           & 66.1            \\
% \hline
% \end{tabular}
% \caption{Performance of all models on our dataset V1 with CausalCoT instructions}
% \label{table:model_comparison_COT}
% \end{table}

% 



% \begin{table*}[h!]
% \centering
% \begin{tabular}{p{3cm}|>{\centering\arraybackslash}p{1cm}|>{\centering\arraybackslash}p{1cm}|>{\centering\arraybackslash}p{1cm}|>{\centering\arraybackslash}p{1cm}|>{\centering\arraybackslash}p{1cm}}
% \hline
% \textbf{Models}  & \textbf{Basic} & \textbf{Cond.} & \textbf{Joint} & \textbf{Nested} & \textbf{Avg.}\\ \hline
% GPT (davinci) & 51.2 & 41.9 & 51.2 & 51.6 & 49.0\\ 
% GPT (Babbage) & 3.6 & 7.6 & 1.2 & 19.6 & 8 \\ 
% GPT 3.5 & 43.6 & 50.4 & 53.6 & 50 & 49.4\\ 
% GPT4-o-mini & 57.2 & 66.4 & 60 & 63.2 & 61.7 \\ 
% Claude-3 (Sonnet) & / & / & / & / & / \\ 
% Claude-3.5 (Haiku) & / & / & / & / & / \\ 
% Gemini-1.5 & 72.4 & 70.8 & 72.4 & 78.4 & 73.5 \\ 
% Gemini-1.5-8b & 66.8 & 67.2 & 65.2 & 65.2 & 66.1 \\ \hline
% \end{tabular}
% \caption{Performance comparison between models under different categories}
% \label{table:model_comparison_COT}
% \end{table*}


\section{Proposed Reasoning Strategy}
% Illustrate our CausalCoT + AoT prompting strategy. It is inspired by CausalAoT and CI engine \cite{pearl2018book}.
% \subsection{CausalCoT Part (No name yet)}
% This part is the first part to extract the causal graph and collect given values and relations. 
% \subsection{Tree of Thought Part (No name yet)}
% \subsubsection{Reasoning}
% The tree of thought strategy is used in reasoning questions based on the information from the CausalCoT.
% \subsubsection{Self-evaluation}
% It is used to evaluate the prompt.
% How to evaluate?
% The Evaluation Procedure
% \subsection{Algorithm of Thought Part}





% To address this challenge, we propose several advanced counterfactual reasoning strategies aimed at guiding LLMs toward more robust and consistent reasoning in counterfactual scenarios. 
% Our findings demonstrate that while standard prompting methods offer limited improvement, tailored approaches can significantly enhance model performance in handling complex causal queries.
% Our analysis of these systematic errors directly motivates the design of our CoIn methodology, which  By targeting the predominant error patterns we identified, CoIn provides a more robust and systematic approach to enhance LLMs' causal reasoning capabilities.
As discussed in Section 3, the primary challenge for LLMs is to minimize incorrect inferences, which are a major source of errors. To tackle the challenges in counterfactual inference that current LLMs face, we introduce an enhanced reasoning framework. This framework utilizes evaluation and backtracking capabilities within a multi-step causal chain-of-thought approach . It specifically targets fundamental inference challenges using an iterative search algorithm to explore and validate counterfactual reasoning paths. Our framework dynamically evaluates the potential of each reasoning step and intelligently backtracks to more promising decision points when necessary, ensuring robust performance across complex problems with multiple potential reasoning paths \cite{sel2023algorithm}.%This approach, inspired by Algorithm-of-thought , advances formal counterfactual reasoning capabilities in LLMs \cite{jin2023cladder}. 
% The detailed procedure is illustrated in this figure \ref{fig:structure}. 
The prompt design for this task is structured into two distinct phases: Counterfactual Information Extraction phase and Counterfactual Reasoning Algorithm phase. An example of our proposed paradigm is provided in Appendix~\ref{sec:Method Prompt Design}.
% \begin{figure*}[htbp]
%   \centering
%   \includegraphics[width=1\textwidth]{Method_draft.pdf}
%   \caption{Illustration of our CoIn strategy, which guides LLM iterative thinking and backtracking explore the results.}
%   \label{fig:structure}
% \end{figure*} 

\subsection{Counterfactual Information Extraction} In the first phase, we focus on systematically gathering all relevant information explicitly stated in the scenario. The process begins with constructing the causal graph by identifying relationships between events and representing them in a clear "$event\ 1 \to event\ 2$" format, which eliminates potential ambiguities. Next, we collect the given values for each variable from both background information and questions, where these values indicate whether specific events occur or not. Crucially, this phase maintains strict adherence to explicitly stated information, avoiding any unsupported inferences or assumptions in favor of a rigorous and unbiased data collection process. 

\begin{algorithm}[t]
\caption{Reasoning Algorithm}
\label{algorithm:counterfactual}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require
    \State $\mathbb{R}$: A set of causal graph relations
    \State $\mathbb{E}$: A set of events
    \State $\mathbb{N}$: A set of $\{event, value\}$ pairs
\Ensure
    \State $Y$: target event $Y$

\Procedure{Reasoning}{$\mathbb{R}, \mathbb{E}, \mathbb{N}$}
    \State $K \leftarrow \textsc{Random}(E), E \in \mathbb{E} \setminus\mathbb{N}.events$

    \While{$K \notin \mathbb{N}.events$}
        \ForAll{relation $r \in \mathbb{R}$}
            \If{$\exists$ \textsc{infer}$(r, \mathbb{N}) \rightarrow value(K)$}
                \State $N \leftarrow N \cup \{K, value(K)\}$
                \If{$K = Y$}
                    \State \Return $value(K)$
                \Else 
                    \State $K \leftarrow \textsc{Random}(E), E \in \mathbb{E} \setminus\mathbb{N}.events$
                \EndIf
            \EndIf
        \EndFor
        
    \EndWhile
    \State \Call{Backtracking}{K}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Counterfactual Reasoning Algorithm} The second phase is Counterfactual Reasoning Algorithm, the core methodological component that systematically applies gathered information to evaluate the target event. The framework employs iterative counterfactual reasoning to progressively explore and infer event values, ultimately determining the target outcome. Moreover, through integrated evaluation and backtracking mechanisms, the framework enables models to systematically optimize their reasoning paths and improve reasoning accuracy. This process follows an algorithmic strategy designed to systematically determine the counterfactual outcome.

The strategy is shown in the following Algorithm~\ref{algorithm:counterfactual}. It aims to determine the value of a target event $Y$ using a set of causal relationships ($\mathbb{R}$), a set of known event-value pairs ($\mathbb{N}$) and a set of events ($\mathbb{E}$). The algorithm begins by randomly selecting an event $K$ from the set $\mathbb{E}$. When $K$ is not contained within the known events set ($\mathbb{N}$), the algorithm traverses causal relationships in $\mathbb{R}$ to identify applicable rules that permit value inference for $K$ based on established event values. In cases where current parameters preclude $K$ value inference, the algorithm implements backtracking to antecedent nodes and pursues alternative $K$ candidates. Once the value of $K$ is successfully inferred, the event-value pair is integrated into the known events set $\mathbb{N}$. To enhance the ability of LLMs to learn our algorithm effectively, we incorporate in-context examples patterned after search algorithms \cite{sel2023algorithm}. These examples, which differ slightly from the dataset, belongs to counterfactual questions and basic problem types. By structuring examples to follow search-based reasoning, we leverage LLMs' in-context learning capabilities to guide them in exploring solutions step by step, rather than relying on intuition. This approach not only enhances the model’s ability to break down problems and evaluate paths efficiently but also reducing the likelihood of generating erroneous intermediate steps and improving reasoning stability.
% The inference begins by employing a depth-first search (DFS) algorithm to explore promising operations based on the causal graph and the causal relationships between variables. Using these relationships, the algorithm calculates intermediate variables to evaluate their values under the counterfactual scenario. The value of an intermediate variable is determined by applying the relevant given equations and information derived from the causal dependencies.

\paragraph{Causal Evaluation Process for Target Event Validation.} Once intermediate events $K$ are computed, the algorithm uses them to evaluate the target event step by step, ensuring that each calculation adheres to the relationships defined in the causal graph. If the target event $Y$ is identified during this process, the algorithm will validate the entire sequence of calculations, verifying that all intermediate values and logical dependencies align with the observed and counterfactual information.

\paragraph{Backtracking Reassessment.} The backtracking step involves reassessing each intermediate calculation. Through an iterative process of operation identification, intermediate value calculation, and result validation, the algorithm generates robust and logically coherent counterfactual outcomes. This structured and rigorous approach guarantees that the counterfactual inference phase remains transparent, reproducible with the underlying causal framework. An example of how CoIn solves the CounterBench problem is provided in Appendix~\ref{sec:Method Prompt Design}.

% In contrast with Standard instruction and CausalCoT, we propose a novel paradigm that guides LLMs to explore solutions through a counterfactual inference paradigm. Our approach iteratively applies counterfactual reasoning to progressively infer the values of variables and ultimately the target. Moreover, through integrated evaluation and backtracking mechanisms, the framework enables models to systematically optimize their reasoning paths and improve reasoning accuracy.



\section{Experiments}
\subsection{Experiment Setup}
%Our experimental design assesses the effectiveness and generalization capabilities of our proposed enhancement strategy, CoIn, across a range of LLM models. We conducted comprehensive evaluations on a diverse array of models sourced from multiple platforms to ensure robustness and broad applicability of our results. Specifically, 
We adopted the same LLMs as mentioned in Section 3 for our experiments. To establish baselines, we implemented both CausalCoT \cite{jin2023cladder} and standard solver strategies. The latter integrates external tools into the chain-of-thought process, as described in \cite{hua2024improving}. Specifically, this approach combines LLMs with Structural Causal Model (SCM) tools \cite{pearl2009causality} for causal inference. The study introduces CausalTool, a suite of 10 inference tools designed for various causal tasks. It leverages LLMs to classify causal questions, extract causal graphs and formalized data, and route them to the appropriate tools for inference, with the final answer generated by the LLM. \footnote{Since the source code for CausalTool is not publicly available, we re-implemented its counterfactual inference procedure based on descriptions in the original paper.} During inference, the temperature is set to zero.
\subsection{Main Result}

\begin{table}[t]
\centering
\scalebox{0.70}{
\setlength{\tabcolsep}{1pt} % 调整列间距
\begin{tabular}{p{3.4cm}>{\centering\arraybackslash}p{1.8cm}>{\centering\arraybackslash}p{1.8cm}>{\centering\arraybackslash}p{1.8cm}>{\centering\arraybackslash}p{1.8cm}}
\toprule
\rowcolor{mygray}
\textbf{Model} & \textbf{Standard} & \textbf{CausalCoT} & \textbf{Solver} & \textbf{Ours}\\
\midrule
GPT-3 (Davinci-002) & \textbf{51.8} & 49.0 & 50.2 & 49.8\\
GPT-3 (Babbage-002) & \textbf{50.0} & 8.0 & \textbf{50.0} & 45.8\\
GPT-4o mini & 50.0 & 61.7 & 47.5 & \textbf{80.6}\\
GPT-4o & 52.5 & 78.8 & 52.1 & \textbf{92.0}\\
GPT-3.5 turbo & 50.3 & 49.4 & 50.0 & \textbf{60.9}\\
Claude-3 (Sonnet) & 50.0 & 59.0 & 51.9 & \textbf{91.6}\\
Claude-3.5 (Haiku) & 37.5 & 64.8 & 49.2 & \textbf{82.3}\\
Gemini-1.5-flash & 71.0 & 73.5 & 53.8 & \textbf{93.0}\\
Gemini-1.5-flash-8b & 50.0 & 66.1 & 50.9 & \textbf{87.8}\\
Deepseek-V3 & 50.2 & 76.3 & 49.5 & \textbf{93.5}\\
\bottomrule
\end{tabular}}
\caption{Model accuracy on CounterBench. We report the average accuracy for four inference strategies: Standard, CausalCoT, Solver, and CoIn.}
\label{tab:model_accuracy}
\end{table}

The comprehensive performance comparison across all models is presented in Table~\ref{tab:model_accuracy}. Our approach demonstrates notable improvements over existing methods across the model spectrum, with particularly noteworthy performance gains achieved by smaller language models, including GPT-4o mini, Claude-3.5 Haiku, and Gemini-1.5-flash-8b. For instance, our method enables GPT-4o mini to achieve an accuracy of $80.6\%$, surpassing the performance of several larger models without CoIn enhancement. As detailed in Table~\ref{tab:model_accuracy_type}, taking GPT-4o mini as an example, CoIn achieves superior performance across all four types of counterfactual questions, with particularly better results on basic questions compared to more complex variants. Additionally, state-of-the-art LLMs such as GPT-4o, Gemini-1.5-flash, and Deepseek-V3 achieve remarkable accuracy exceeding $90\%$ when augmented with our method. Taken GPT-4o as example, Our strategy improves the accuracy of the model from $78.8\%$ to $92\%$, demonstrating CoIn's effectiveness in guiding LLMs through algorithm to explore paths step-by-step. The results indicate that contemporary LLMs, when equipped with our strategy, can effectively resolve most formal complex counterfactual problems. The details of all performance of our results are presented in Appendix~\ref{sec:appendix_more_experiment}. Moreover, in Appendix~\ref{sec:Error Analysis comparison}, we conducted error analysis. The analysis reveals a substantial reduction in errors of inference process. Moreover, we also examine the impact of complex causal relationships on outcomes. We found that accuracy decreases as the number of variants increases for CausalCoT and CoIn. The details are shown in~\ref{sec:Effects of variants numbers}. 
\begin{table}[t]
\centering
\scalebox{0.9}{
\setlength{\tabcolsep}{1pt} 
\begin{tabular}{p{1.8cm}>{\centering\arraybackslash}p{1.05cm}>{\centering\arraybackslash}p{1.05cm}>{\centering\arraybackslash}p{1.05cm}>{\centering\arraybackslash}p{1.05cm}>{\centering\arraybackslash}p{1.05cm}}
\toprule
\rowcolor{mygray}
\textbf{Methods} & \textbf{Basic} & \textbf{Cond.} & \textbf{Joint} & \textbf{Nested} & \textbf{Avg.} \\
\midrule
Standard & 50.0 & 50.0 & 50.0 & 50.0 & 50.0\\
CausalCoT & 57.2 & 66.4 & 60.0 & 63.2 & 61.7 \\
Solver & 35.2 & 54.4 & 50.4 & 50.0 & 47.5 \\
Ours & \textbf{82.8} & \textbf{79.2} & \textbf{80.0} & \textbf{80.4} & \textbf{80.6} \\
\bottomrule
\end{tabular}}
\caption{Accuracy of GPT-4o mini across four query types in the CounterBench.}
\label{tab:model_accuracy_type}
\end{table}

\subsection{Validating Generalization Ability}
In this section, we evaluate the generalization capability of CoIn using the CLADDER dataset \cite{jin2023cladder}. CLADDER is a dataset focus on the causal reasoning questions. We utilize the CLADDER dataset to determine if the proposed method can be extended beyond the CounterBench dataset. Unlike our dataset, which focuses on formal rules, CLADDER includes examples that utilize common sense knowledge rather than causal inference abilities to answer queries. Specifically, it encompasses both commonsense and anti-commonsense scenarios, allowing us to explore whether CoIn remains effective under the influence of pretraining knowledge in LLMs. We conduct experiments on the counterfactual subset of CLADDER. We applied the Gemini-1.5 and Gemini-1.5-8b models to both commonsense and anti-commonsense queries, with the results displayed in Figure~\ref{fig:structure1} and detailed examples in Appendix~\ref{sec:CLADDER}. These results demonstrate that performance is consistently stable across different reasoning paradigms, suggesting that pretraining knowledge has a limited impact on the CoIn counterfactual reasoning capabilities. Furthermore, our method achieves an accuracy of 78.98\%, outperforming both CausalCoT at 64.77\% and the Standard method at 64.20\%. This strong performance highlights CoIn's generalizability and underscores its potential for broader application in various counterfactual reasoning tasks.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{Dataset.pdf}
  \caption{Accuracy comparison between Standard, CoIn, and CausalCoT in Anticommonsense and Commonsense Dataset.}
  \label{fig:structure1}
\end{figure}


% , suggesting that our strategy, which focuses on counterfactual inference, may inadvertently reduce the model's attention to relationship understanding. 



% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.5\textwidth]{barchart_q.drawio.pdf}
%   \caption{Approximate Errors questions numbers comparison between Our Method and CausalCoT.}
%   \label{fig:structure}
% \end{figure}

\section{Related Work}

\paragraph{Counterfactual Reasoning.}
Counterfactual reasoning explores how outcomes change when certain variables are altered from their historical states. In Structural Causal Models (SCMs), Pearl’s \cite{pearl2009causality} “surgery” and do-calculus provide systematic ways to infer intervention outcomes, highlighting the deep causal knowledge required for accurate inference. Counterfactuals can be deterministic or probabilistic: deterministic settings yield predictable outcomes from given conditions, while probabilistic models incorporate inherent uncertainties. These methods have gained traction in domains like social sciences, where they help assess alternative policy outcomes and study causal mechanisms in observational data \cite{morgan2015counterfactuals}, and in medicine, where they enable personalized treatment and decision support \cite{johansson2016learning,shalit2017estimating,louizos2017causal,yoon2018ganite}. In artificial intelligence, counterfactual reasoning is crucial for interpretability and fairness, as it allows models to generate alternative scenarios and assess decision-making robustness. Although recent efforts extend counterfactual reasoning to LLMs \cite{jin2023cladder}, significant challenges persist, particularly regarding the complex variable relationships in high-dimensional text data. Consequently, bridging the gap between textual complexity and robust causal inference remains a focal point for future research.

\paragraph{LLMs in Counterfactual Learning.}
With the rapid evolution of LLMs, the research community has increasingly focused on their ability to perform causal inference \cite{zhang2023understanding,ashwani2024cause}. A prominent example is Causal Agent, an agent-based LLM framework that merges an LLM with causal tools for complex tasks \cite{han2024causal}. While it excels at identifying causal associations and conducting interventions, it largely omits counterfactual reasoning, limiting its applicability to more advanced scenarios. Current efforts to integrate counterfactual reasoning into LLMs typically follow two paths. First, commonsense-based approaches leverage background knowledge to imagine scenarios that defy established facts \cite{ning2024skeleton,chatzi2024counterfactual,musi2024fallacy,vicuna2023}, such as positing alternative historical outcomes. Second, graph-based methods employ formal causal graphs and external Python packages for computations, as seen in CausalCoT \cite{jin2023cladder} and CausalTool \cite{hua2024improving}. Although these methods effectively incorporate structured causal information, they often offload key calculations outside the LLM. %In contrast, our approach directly strengthens an LLM’s counterfactual reasoning through specialized instruction, enabling the model itself to conduct comprehensive causal inference without relying on external software. This strategy not only broadens the scope of LLM-driven causal analysis but also highlights the potential for more interpretable, self-contained solutions in complex domains.








% Therefore, we introduce LLM into these problems. We hope our research can improve the abilities of LLM on solving counterfactual reasoning problems. % Deterministic Counterfactual Reasoning, in contrast to probabilistic counterfactuals, does not have too much research on it. However, it is still an important part of the counterfactual reasoning, because it is an ideal and specific condition of the counterfactual reasoning. This research can become a basis for further research. So far, these related researches focus on simple causal problems which means they only focus on causal relations under five variables \cite{jin2023cladder}. 


\section{Conclusion}
In this work, we develop and extend CounterBench, a counterfactual reasoning dataset covering four types of counterfactual problems for LLM benchmarking. Our results show that most LLMs perform at near-random levels, with state-of-the-art methods offering only limited improvements in counterfactual reasoning accuracy. To address these challenges, we propose CoIn, a reasoning paradigm inspired by formal causal inference principles and planning strategies. CoIn guides LLMs through iterative thinking and backtracking to explore reasoning paths more effectively. Our approach significantly enhances counterfactual reasoning capabilities of LLMs.



\section{Limitations}
While our approach consistently enhances LLMs’ counterfactual reasoning capabilities, it currently focuses on relatively idealized problems. Moving forward, we plan to integrate richer probabilistic dimensions and more intricate causal structures, including interventions with varying intensities, multiple interacting variables, and hidden confounders, to better capture real-world complexities. We also see significant potential for applying our method in diverse domains. In social science, it can illuminate the impacts of policy interventions and reveal causal mechanisms in observational data, while in healthcare, it can guide clinical decision-making and personalize treatment strategies. By expanding our datasets and refining our methodologies, we aim to create a more versatile and powerful framework that enables smaller models to provide robust causal insights across a wider range of applications.


% Entries for the entire Anthology, followed by custom entries

\bibliography{reference}


\appendix
\section{Appendix}
\subsection{Method Prompt Design}
%
Our prompt design, as illustrated in Figure~\ref{fig:prompt design}, has been carefully structured to optimize the interaction between our algorithm and Large Language Models (LLMs). At its core, the design incorporates multiple few-shot examples that serve as comprehensive demonstrations of our algorithm's operational framework. These examples are strategically selected to showcase various reasoning patterns and problem-solving approaches, providing LLMs with a robust foundation for understanding the algorithm's methodology. Within the prompt, we have meticulously detailed step-by-step instructions, creating a clear and structured information flow that guides the model through the reasoning process. This structured approach leverages LLMs' inherent in-context learning capabilities, enabling them to systematically explore solution paths rather than relying on intuitive responses. This approach not only enhances the model’s ability to break down problems and evaluate paths efficiently but also reducing the likelihood of generating erroneous intermediate steps and improving reasoning stability.
\label{sec:Method Prompt Design}
\begin{figure*}[h!]
  \centering
  \begin{mdframed}[backgroundcolor=white, linewidth=1pt, linecolor=black]
    \textbf{Question:} 
        -------------------------
        User: 
        Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Ziklo has a direct effect on not Blaf, Blaf has a direct effect on Trune, Trune has a direct effect on not Vork, Vork or Trune has a direct effect on Sline, Sline has a direct effect on Frim, not Frim and Trune has a direct effect on Qado, and Qado has a direct effect on Lumbo. We know that Ziklo causes not Blaf, Blaf causes Trune, Trune causes not Vork, Vork or Trune causes Sline, Sline causes Frim, not Frim and Trune causes Qado, and Qado causes Lumbo. Would Lumbo occur if not Ziklo instead of Ziklo?
        Step 1. Extract the causal graph: Identify the causal graph that depicts the relationships in the scenario. Let X = Ziklo; V1 = Blaf; V2 = Trune; V3 = Vork; V4 = Sline; V5 = Frim; V6 = Qado; Y = Lumbo. The diagram should simply consist of edges denoted in "var1 -> var2" format, separated by commas. If you get V1 -> Not V2 for example, you need to replace into V1 -> V2. 
        Step 2. Collect the information: Collect all the directly given information into given values set. 1 means given observed in question or observed. 0 means given not in question. Do not assume or infer other variables values by relations. Then, describe relations about how multiple variables influence another variable; it can result in AND, OR, or NOT.
        Step 3. Infer the Y by information step by step. 
        Step 4. Based on the result from the Step3, derive the final answer. There is an identifiable answer.
        
        
        Assistant: 
        Step 1) Extract the causal graph: Identify the causal graph that depicts the relationships in the scenario. Let X = Ziklo; V1 = Blaf; V2 = Trune; V3 = Vork; V4 = Sline; V5 = Frim; V6 = Qado; Y = Lumbo. The causal graph is X->V1, V1->V2, V2->V3, V2->V4, V3->V4, V4->V5, V5->V6, V2->V6, V6->Y.
        Step 2) Collect the information: All given values: X = 0 (not X); Relations: V1: NOT X, V2: V1, V3: NOT V2, V4: V2 OR V3, V5: V4, V6: V2 AND NOT V5, Y: V6.
        Step 3) Trying a promising first operation:
        	1. Y = V6 = 0 or 1 (cannot be inferred directly)
        	Trying another promising first operation:
        	2. V1 = NOT X = 1
        	- Y = V6 = 0 or 1 (cannot be inferred directly)
        	- V6 = V2 AND V5 = 0 or 1 (cannot be inferred directly)
        	- V5 = V4 = 0 or 1 (cannot be inferred directly)
        	- V2 = V1 = 1 -- V5 = V4 = 0 or 1 (cannot be inferred directly)
        				-- V3 = NOT V2 = 0 --- Y = V6 = 0 or 1 (cannot be inferred directly)
        							 --- V4 = V3 or V2 = 1 ---- V5 = V4 = 1 ----- V6 = V2 AND NOT V5 = 0 ------ Y = V6 = 0 -> found it !
        	
        	Backtracking the solution:
        	Step 3a:
        		V1 = NOT X = 1
        	Step 3b:
        		V2 = V1 = 1
        	Step 3c:
        		V3 = NOT V2 = 0
        	Step 3d:
        		V4 = V3 or V2 = 1
        	Step 3e:
        		 V5 = V4 = 1
        	Step 3f:
        		 V6 = V2 AND NOT V5 = 0
        	Step 3g:
        		 Y = V6 = 0
        Step 4) Since the result for the Y is 0, the overall answer to the question is no.
        
        ----------------
        Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Praf has a direct effect on Vank, Vank has a direct effect on Scud, Scud and Vank have direct effects on Wrenk, Wrenk has a direct effect on Yobb, not Yobb has a direct effect on Glim, and Glim has a direct effect on Klep. We know that Praf causes Vank, Vank causes Scud, Scud and Vank together cause Wrenk, Wrenk causes Yobb, not Yobb causes Glim, and Glim causes Klep. Assume not Praf, and based on this assumption, further suppose not Wrenk. Would Klep occur?
        
        User:
        
        
        Step 1) Extract the causal graph: Identify the causal graph that depicts the relationships in the scenario.  The diagram should simply consist of edges denoted in "var1 -> var2" format, separated by commas. If you get V1 -> Not V2 for example, you need to replace into V1 -> V2 
        
        Step 2) Gather all relevant data: Collect the information: Collect all the directly given information into given values set. 1 means given observed in question or observed. 0 means given not in question. Do not assume or infer other variables values by relations. Then, describe relations about how multiple variables influence another variable.
        
        Step 3) Adopt the following algorithm to get the result: Infer the Y by information step by step.
        
        Step 4) Conclude the final answer: Based on the result from the Step3, derive the final answer. There is an identifiable answer. Answer step by step.
 Assistant:
  \end{mdframed}
  
  \caption{The prompt design of CoIn}
  \label{fig:prompt design}
\end{figure*}

\subsection{Human Evaluation}
\label{sec:human_label}
To assess the quality of our dataset, we asked two Ph.D. students with expertise in causal inference to answer 200 randomly selected questions. They achieved an average accuracy of 97.75\% and required four hours to complete them, reflecting the substantial cognitive effort involved. According to participant feedback, these questions demanded careful consideration of multiple causal factors and explicit counterfactual reasoning steps. These findings demonstrate that our dataset presents problems of sufficient complexity to effectively evaluate  counterfactual reasoning abilities of models.


\subsection{Effects of variants numbers}
\label{sec:Effects of variants numbers}
Beyond analyzing different types of counterfactual reasoning, we conducted a comprehensive investigation into how the complexity of causal relationships influences model performance. To systematically evaluate this relationship, we categorized our datasets into five distinct complexity levels based on the number of variables involved, ranging from 5 to 9 variables. This categorization allows us to assess how model performance scales with increasing causal complexity. As demonstrated in Table~\ref{tab:model_accuracy_vars_nums}, using GPT-4o mini as a representative case, we observe a consistent pattern where model accuracy exhibits a negative correlation with the number of variables for both CausalCoT and our CoIn method. This inverse relationship between performance and causal complexity suggests that as the number of interacting variables increases, the reasoning task becomes progressively more challenging, requiring models to track and process more complex causal chains and interdependencies. This analysis provides valuable insights into the scalability and limitations of current causal reasoning approaches when dealing with increasingly complex scenarios.

\begin{table}[h!]
\centering
\setlength{\tabcolsep}{1pt} % 调整列间距
\begin{tabular}{p{1.8cm}>{\centering\arraybackslash}p{1.05cm}>{\centering\arraybackslash}p{1.05cm}>{\centering\arraybackslash}p{1.05cm}>{\centering\arraybackslash}p{1.05cm}>{\centering\arraybackslash}p{1.05cm}}
\toprule
\rowcolor{mygray}
\textbf{Methods} & \textbf{5vars.} & \textbf{6vars.} & \textbf{7vars.} & \textbf{8vars.} & \textbf{9vars.} \\
\midrule
Standard & 50.0 & 50.0 & 50.0 & 50.0 & 50.0\\
CausalCoT & 67.0 & 61.0 & 63.5 & 61.5 & 55.5 \\
Ours & \textbf{92.0} & \textbf{82.0} & \textbf{82.0} & \textbf{73.5} & \textbf{73.5} \\
\bottomrule
\end{tabular}
\caption{Performance of different strategies on CounterBench dataset with GPT-4o mini across various difficulty levels}
\label{tab:model_accuracy_vars_nums}
\end{table}



\subsection{More Experiment Details}
Table~\ref{table:other_model_comparison} presents a comprehensive performance comparison between the Solver method and our CoIn approach across various models on the CounterBench dataset. Our analysis reveals several significant patterns in model performance across different counterfactual reasoning tasks. First and foremost, CoIn demonstrates consistent superiority over the Solver method across all task categories and model architectures. This improvement is particularly pronounced in newer model architectures, with state-of-the-art models like GPT-4o, Claude-3 (Sonnet), and Gemini-1.5-flash showing remarkable performance gains. Notably, GPT-4o achieves perfect accuracy ($100.0\%$) on basic reasoning tasks under our method. Deepseek-V3 demonstrates exceptional consistency with high performance across all task types. Its accuracy comes to $99.6\%$ for basic tasks and maintaining above $90\%$ accuracy across other categories.
The performance distribution across different task types reveals interesting patterns. Basic reasoning tasks consistently yield the highest accuracy under our method, particularly evident in larger models. However, this category also exhibits the most significant performance variation across different model architectures, suggesting that basic counterfactual reasoning capabilities are highly sensitive to model scale and architecture. In contrast, joint reasoning tasks show relatively stable performance across different models, indicating that these capabilities may be more fundamentally tied to the reasoning framework rather than model size.
Moreover, we observe a clear correlation between model recency and performance, with newer models like GPT-4o and Claude-3 (Sonnet) achieving significantly higher average accuracies ($92.0\%$, $91.6\%$) compared to their predecessors. This trend holds true across all task types, though the magnitude of improvement varies by category. The consistent performance improvements across newer model architectures suggest that recent advances in language model development have enhanced their capacity for structured counterfactual reasoning when combined with our methodology.

\begin{table*}[ht]
\centering
\scalebox{0.9}{
\begin{tabular}{p{3.5cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}|>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}>{\centering\arraybackslash}p{0.9cm}}
\toprule
\rowcolor{mygray} \multicolumn{1}{c}{} & \multicolumn{5}{c}{\textbf{Solver}} & \multicolumn{5}{c}{\textbf{Ours}} \\ \hline
\rowcolor{mygray} \textbf{Models} & \textbf{Basic} & \textbf{Cond.} & \textbf{Joint} & \textbf{Nested} & \textbf{Avg.} & \textbf{Basic} & \textbf{Cond.} & \textbf{Joint} & \textbf{Nested} & \textbf{Avg.} \\ \hline

			
GPT-3 (Davinci-002) & 50.4 & 50.0 & 50.4 & 50.0 & 50.2 & 51.2 & 48.4 & 48 & 51.6 & 49.8 \\ 
GPT-3 (Babbage-002) & 44.8 & 48.8 & 48.4 & 48.0 & 50.0 & 45.2 & 46.4 & 46 & 45.6 & 45.8\\ 
GPT-3.5 turbo & 50.0 & 50.0 & 50.0 & 50.0 & 50.0 & 65.2 & 54.0 & 61.2 & 63.2 & 60.9\\ 
GPT-4o mini & 35.2 & 54.4 & 50.4 & 50.0 & 47.5 & 82.8 & 79.2 & 80.0 & 80.4 & 80.6 \\ 
GPT-4o & 45.6 & \textbf{60.8} & \textbf{51.6} & 50.4 & 52.1 & \textbf{100.0} & 89.6 & 90.0 & 88.4 & 92.0 \\
Claude-3 (Sonnet) & \textbf{55.6} & 58.0 & 43.6 & 50.4 & 51.9 & 97.6 & 84.8 & 90.0 & \textbf{94.0} & 91.6\\ 
Claude-3.5 (Haiku) & 49.2 & 49.2 & 48.8 & 49.6 & 49.2 & 88.0 & 76.8 & 81.2 & 83.2 & 82.3\\ 
Gemini-1.5-flash & 54.8 & 58.8 & 50.8 & 50.8 & \textbf{53.8} & 98.4 & 89.6 & 90.0 & \textbf{94.0} & 93.0\\ 
Gemini-1.5-flash-8b & 52.4 & 50.8 & 49.2 & \textbf{51.2} & 50.9 & 94.8 & 80.0 & 88.0 & 88.4 & 87.8 \\
Deepseek-V3 & 51.2 & 48.0 & 48.4 & 50.4 & 49.5 & 99.6 & \textbf{90.0} & \textbf{90.4} & \textbf{94.0} & \textbf{93.5} \\\hline
\end{tabular}
}					
\caption{Performance of Solvers and Our CoIn Across Different Reasoning Categories }
\label{table:other_model_comparison}
\end{table*}
\label{sec:appendix_more_experiment}

% \subsection{Experiments in Other Dataset}

% This section presents the performance of our CoIn method on additional datasets. To evaluate the robustness of our proposed strategy, we conduct extensive testing using the CLADDER dataset\cite{jin2023cladder}. The following table shows our performances in CLADDER with several representative models. 

\subsection{Error Analysis comparison between Our Method and CausalCoT}
\label{sec:Error Analysis comparison}
By randomly sampling $50$ instances and categorizing errors into three distinct types, wrong relations, wrong inferences, and wrong conclusions, we systematically evaluated the model's performance. The analysis demonstrated notable differences in the relative distribution of errors, with inference-related errors decreasing from $86\%$ to $46\%$. This significant reduction combining with the diminished error quantity substantiates an enhanced counterfactual inference capability. Concurrently, we also observed a relative increase in relationship-related errors, $12\%$ to $50\%$. Consequently, the overall effect of our strategy is positive as the number of errors come down notably.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{Error_Analysis_bar.pdf}
  \caption{Error Analysis comparison between Our Method and CausalCoT.}
  \label{fig:structure}
\end{figure}

\subsection{CLADDER Dataset Example}
\label{sec:CLADDER}
Two examples are generated from the CLADDER dataset. It is shown in Figure ~\ref{fig:Anti-commonsense data} and Figure~\ref{fig:Commonsense data}, are designed to evaluate a model’s ability to distinguish between commonsense and anti-commonsense reasoning in counterfactual scenarios. Commonsense reasoning follows causal relationships that align with human intuition and everyday knowledge, making it easier for models to infer outcomes based on familiar patterns. In contrast, anti-commonsense reasoning presents causal structures that contradict intuitive expectations, requiring models to rely solely on explicitly provided causal relationships rather than prior knowledge. By testing both reasoning paradigms, these examples assess whether a model can accurately differentiate between intuitive and counterintuitive causal structures, ensuring that real-world biases do not interfere with its counterfactual reasoning abilities.
\begin{figure}[h!]
  \centering
  \begin{mdframed}[backgroundcolor=white, linewidth=1pt, linecolor=black]
    \textbf{Question:} Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Having a brother has a direct effect on room. The candle has a direct effect on room. We know that having a brother and candle with wax causes dark room. We observed the candle has wax. Would the room is dark if not having a brother instead of having a brother?
    
    Start your answer with "Yes" or "No", followed by additional reasoning or evidence to support your explanation.

  \end{mdframed}
  \caption{Anti-commonsense Example}
  \label{fig:Anti-commonsense data}
\end{figure}

\begin{figure}[h!]
  \centering
  \begin{mdframed}[backgroundcolor=white, linewidth=1pt, linecolor=black]
    \textbf{Question:} Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: The man in the room has a direct effect on room. The candle has a direct effect on room. We know that blowing out the candle and candle with wax causes dark room. We observed the candle has wax. Would the room is dark if not blowing out the candle instead of blowing out the candle?
    
    Start your answer with "Yes" or "No", followed by additional reasoning or evidence to support your explanation.

  \end{mdframed}
  \caption{Commonsense Example}
  \label{fig:Commonsense data}
\end{figure}

\subsection{Answer of our methods}
Here, we provide a comparative analysis between our CoIn method, CausalCoT and the standard approach. The results are shown in Figure~\ref{fig:standard}, Figure~\ref{fig:causalCOT} and Figure~\ref{fig:CoIn}. In results, our CoIn method introduces a structured, step-by-step reasoning framework that systematically addresses complex counterfactual scenarios. As illustrated in Figure~\ref{fig:CoIn}, the approach breaks down the reasoning process into distinct phases: counterfactual information collection, and systematic exploration of inference paths. This structured decomposition contrasts sharply with the standard approach shown in Figure~\ref{fig:standard}, which provides minimal guidance for navigating the reasoning process. Through explicit variable mapping and systematic path exploration, CoIn enables models to systematically evaluate possible causal chains, leading to more reliable and traceable inference outcomes. The key reason why only CoIn arrived at the correct answer lies in its explicit structured reasoning process, which ensures a systematic and robust approach to counterfactual inference. Unlike CausalCoT and the standard method, CoIn employs a step-by-step algorithmic framework that systematically processes causal dependencies, preventing shortcut reasoning and heuristic errors.


\begin{figure}[htbp]
  \centering
  \begin{mdframed}[backgroundcolor=white, linewidth=1pt, linecolor=black]
    \textbf{Question:} Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Nuv has a direct effect on Splee, Blen and Druk, not Druk has a direct effect on Plog, Plog has a direct effect on Skrim, Skrim or Druk has a direct effect on Zimb, Zimb has a direct effect on Yurd, and Yurd has a direct effect on Wrox. We know that Nuv causes Splee, Blen and Druk, not Druk causes Plog, Plog causes Skrim, Skrim or Druk causes Zimb, Zimb causes Yurd, and Yurd causes Wrox. Would Wrox occur if not Nuv and not Splee? \\
    
    \textbf{Answer:} No.
  \end{mdframed}
  \caption{Standard Instruction Example}
  \label{fig:standard}
\end{figure}



\subsection{Benchmark Result Analysis: CausalCoT}


We compared the responses generated by Babbage-002 under the CausalCoT framework with those from other LLMs. The results reveal that the majority of Babbage-002’s responses were nonsensical when following the CausalCoT instructions, resulting in a remarkably low accuracy of just $8.0\%$. A deeper analysis showed that $82.4\%$ of the responses were classified as incomprehensible, which explains why the accuracy was notably lower than random guessing. The following chart provides an error analysis, categorizing the errors into three distinct types: \textbf{Blank Response:} This occurs when the model produces an empty response. \textbf{Repeating Error:} This occurs when the model’s inference is only repeating the questions and given informations. The example is shown in Figure~\ref{fig:repeating}. \noindent \textbf{Type Mismatch:} It represents that the answer is expected to be a binary value "yes" or "no". The response returns a list with multiple "yes" or "no". The example is shown in Figure~\ref{fig:mismatch}.
\label{sec:Benchmark Result Analysis}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{Babbage_CausalCoT.pdf}
  \caption{Error Analysis for Babbage-002 in CausalCoT}
  \label{fig:structure}
\end{figure}

\begin{figure*}[htbp]
  \centering
  \begin{mdframed}[backgroundcolor=white, linewidth=1pt, linecolor=black]
    \textbf{Question:} Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Nuv has a direct effect on Splee, Blen and Druk, not Druk has a direct effect on Plog, Plog has a direct effect on Skrim, Skrim or Druk has a direct effect on Zimb, Zimb has a direct effect on Yurd, and Yurd has a direct effect on Wrox. We know that Nuv causes Splee, Blen and Druk, not Druk causes Plog, Plog causes Skrim, Skrim or Druk causes Zimb, Zimb causes Yurd, and Yurd causes Wrox. Would Wrox occur if not Nuv and not Splee? \\
    
    \textbf{Answer:} To address the problem, we will follow the steps outlined:

Step 1: Extract the causal graph

Based on the given conditions, the causal graph can be represented as follows:

- Nuv -> Splee
- Nuv -> Blen
- Nuv -> Druk
- Druk -> Plog
- Plog -> Skrim
- Skrim -> Zimb
- Druk -> Zimb
- Zimb -> Yurd
- Yurd -> Wrox

 Step 2: Determine the query type

The question asks whether Wrox would occur if not Nuv and not Splee. This is a counterfactual query because it involves reasoning about what would happen under a hypothetical scenario that differs from the observed world.

The query type is: "counterfactual"

 Step 3: Formalize the query

The formal expression for the counterfactual query is:

\[ Wrox_{Nuv=0, Splee=0} \]

This notation represents the value of Wrox in the counterfactual world where Nuv and Splee do not occur.

 Step 4: Gather all relevant data

From the problem statement, we have the following data:

- Nuv = 0 (not Nuv)
- Splee = 0 (not Splee)

 Step 5: Deduce the estimand using causal inference

To deduce the estimand, we need to consider the causal pathways from Nuv and Splee to Wrox:

1. Nuv directly affects Splee, Blen, and Druk.
2. Druk affects Plog, which affects Skrim, which affects Zimb, which affects Yurd, which affects Wrox.
3. Druk also directly affects Zimb.
4. Zimb affects Yurd, which affects Wrox.

Given that Nuv = 0, Splee = 0, we need to determine if Wrox can still occur:

- Since Nuv = 0, Splee = 0, Blen = 0, and Druk = 0.
- Druk = 0 implies Plog = 0 and Zimb = 0 (since Druk affects both Plog and Zimb).
- Plog = 0 implies Skrim = 0.
- Skrim = 0 and Druk = 0 imply Zimb = 0.
- Zimb = 0 implies Yurd = 0.
- Yurd = 0 implies Wrox = 0.

Thus, the counterfactual scenario results in Wrox = 0.

 Step 6: Calculate the estimand

Given the deductions above, the counterfactual value of Wrox when Nuv = 0 and Splee = 0 is:

\[ Wrox_{Nuv=0, Splee=0} = 0 \]

Therefore, Wrox would not occur if not Nuv and not Splee.
  \end{mdframed}
  \caption{CausalCoT Instruction Example}
  \label{fig:causalCOT}
\end{figure*}

\begin{figure*}[htbp]
  \centering
  \begin{mdframed}[backgroundcolor=white, linewidth=1pt, linecolor=black]
    \textbf{Question:} Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Nuv has a direct effect on Splee, Blen and Druk, not Druk has a direct effect on Plog, Plog has a direct effect on Skrim, Skrim or Druk has a direct effect on Zimb, Zimb has a direct effect on Yurd, and Yurd has a direct effect on Wrox. We know that Nuv causes Splee, Blen and Druk, not Druk causes Plog, Plog causes Skrim, Skrim or Druk causes Zimb, Zimb causes Yurd, and Yurd causes Wrox. Would Wrox occur if not Nuv and not Splee? \\
    
    \textbf{Answer:} Step 1) Extract the causal graph: Identify the causal graph that depicts the relationships in the scenario. Let X = Nuv; V1 = Splee; V2 = Blen; V3 = Druk; V4 = Plog; V5 = Skrim; V6 = Zimb; V7 = Yurd; Y = Wrox. The causal graph is:

X -> V1, X -> V2, X -> V3, V3 -> V4, V4 -> V5, V5 -> V6, V3 -> V6, V6 -> V7, V7 -> Y.

Step 2) Gather all relevant data: Collect the information: All given values: X = 0 (not Nuv), V1 = 0 (not Splee). Relations:\\ 
- V1: X\\
- V2: X\\
- V3: X\\
- V4: NOT V3\\
- V5: V4\\
- V6: V5 OR V3\\
- V7: V6\\
- Y: V7

Step 3) Adopt the following algorithm to get the result: Infer the Y by information step by step.

1. V1 = X = 0\\
2. V2 = X = 0\\
3. V3 = X = 0\\
4. V4 = NOT V3 = 1\\
5. V5 = V4 = 1\\
6. V6 = V5 OR V3 = 1 OR 0 = 1\\
7. V7 = V6 = 1\\
8. Y = V7 = 1

Step 4) Conclude the final answer: Since the result for Y (Wrox) is 1, the overall answer to the question is yes. Wrox would occur if not Nuv and not Splee.

  \end{mdframed}
  
  \caption{CoIn Instruction Example}
  \label{fig:CoIn}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \begin{mdframed}[backgroundcolor=white, linewidth=1pt, linecolor=black]
    \textbf{Response} \\
    Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?

B: Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?

C: Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?

D: Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?

E: Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?

F: Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?

G: Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?

H: Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?

I: Glent causes Razz, Razz and Glent together cause Pex, Pex causes Zurn, Zurn causes Melf, and Melf and Razz together cause Zlim. Would Zlim occur if not Glent instead of Glent?
  \end{mdframed}
  \caption{Conclusion Error Example}
  \label{fig:repeating}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \begin{mdframed}[backgroundcolor=white, linewidth=1pt, linecolor=black]
    \textbf{Response} \\
    The correct answer is (0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
  \end{mdframed}
  \caption{Type Mismatch}
  \label{fig:mismatch}
\end{figure*}

\end{document}

