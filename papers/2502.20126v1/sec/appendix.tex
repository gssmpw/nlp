\section{Detailed Related Work}

Efficiency in Vision Transformers~\citep{dosovitskiy2020image} falls primarily into two broad categories: reducing the computation per token or the number of tokens overall. 

\paragraph{Reducing the amount of computation per token.}~Reducing the amount of computation per token can be achieved by reducing the model size when training via distillation~\citep{beyer2022knowledge} or pruning the network after training~\citep{zhu2021vision, imfeld2023transformer}. These methods again though typically work with a static inference protocol. As in~\citep{zhao2024dynamic}, we compare in Fig.~\ref{fig:baseline} our class-conditioned~\ImageNet~\flexidit~model, against popular based pruning techniques, based on Diff pruning~\citep{fang2023structuralpruningdiffusionmodels}, Taylor, magnitude or random pruning~\citep{imfeldtransformer}.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.37\linewidth]{figures/baselines.pdf}
    \caption{We compare our dynamic scheduler with more baselines.}
    \label{fig:baseline}
\end{figure}
As one can see, our dynamic scheduler outperforms these baselines. \textit{We note that our method can also be applied in conjunction with pruning techniques to achieve even higher efficiency gains, offering potential orthogonal benefits.} In concurrent work,~\citep{zhao2024dynamic} propose to adjust, in a dynamic way, the model during inference in different steps. In our work, we do not train separate models for each target FLOPs ratio like they do, meaning that we can train a single model and decide how many FLOPs we want to invest during inference. This makes our approach more versatile. Additionally, our training is very stable, and no specific training tricks are required to converge successfully. That is how we were able to extend experiments to high-resolution image and video generation, achieving significantly better speed-ups than the ones they reported. We also outperform their results when the compute budget is very small. Nonetheless, this work could inspire adaptive per-sample schedulers, that could open new future directions.

Given the large computational requirements of the attention operation, many methods nowadays focus on that to reduce the overhead imposed. These methods commonly use some form of hierarchical attention~\citep{liu2021swin, hatamizadeh2023fastervit}, skip (usually the first) attention layers altogether~\citep{xiao2021early}, or reduce the number of the attended keys~\citep{chen2024pixart, yuan2024ditfastattn, fan2021multiscale}, by commonly aggregating keys in a spatial neighborhood or applying some form of windowed attention.

\paragraph{Reducing number of tokens.}~Our method primarily falls within the second category of reducing the overall number of tokens. Previous work here, typically relied on filtering~\citep{rao2021dynamicvit, liu2022adaptive, wu2023ppt}, merging~\citep{bolya2022token, lu2023content, huang2023vision} or dropping~\citep{anagnostidis2023dynamic}. Although merging works well for applications that eventually lead to some pooling operations (like classification tasks or for the task of creating an image-specific embedding), it works significantly less well for applications that require dense (token-level) predictions, where some un-merging operation has to be defined. In other concurrent work,~\citep{wang2024qihoo} reduces the number of representative tokens to calculate the attention over. Our approach resembles most~\citep{beyer2023flexivit}, where vision Transformers are trained to handle inputs with varying patch resolution. By applying less compute for some steps, we can reduce computational complexity significantly, without a drop in performance. 

\paragraph{Image generation.}~In the context of image generation, diffusion has been largely established as the method for attaining state-of-the-art results. There have been previous works that try to take advantage of potential correlations between successive denoising step predictions~\citep{shi2024resmaster}, by either caching intermediate results in the activations~\citep{ma2024deepcache, wimbauer2024cache}, or in the attention~\citep{zhao2024real}. Caching has the advantage of a training-free method. Nonetheless, potential benefits are lower. Similar to our work, ~\citep{balaji2022ediff} use different experts for different denoising steps. Instead of using different experts that require separate training and separate deployment, we show how a single model can be easily formed into a flexible one that can be instantiated in different modes, with each of its modes corresponding essentially to a different expert. Similar in-spirit approaches have been proposed that rely on the smaller compute requirements for lower resolution image generation~\citep{kim2024pagoda, zhang2023i2vgen}. ~\citep{jing2022subspace} also adapt the computation per step, by projecting into smaller subspaces. We instead, keep the dimension of the latent space and the characteristics of it the same across diffusion steps. Orthogonal gains to our approach are also possible through methods such as guidance distillation~\citep{meng2023distillation, kohler2024imagine} and consistency models~\citep{song2023consistency}. Our approach is also largely agnostic to the diffusion process and can be applied out of the box for flow matching methods~\citep{lipman2022flow}. We point the interested reader to~\citep{ma2024efficient} for a survey for further efficiency in diffusion models. Finally, compared to other established techniques~\citep{hatamizadeh2025diffit, liu2024alleviating} we do not fundamentally change the architecture, which allows us to apply our framework effortlessly for numerous pre-trained models across different modalities.

\paragraph{Video generation.}~Our approach can be easily extended for video generation, and in principle for the generation of any modality where some inductive bias (spatial, temporal, etc) is employed in the diffusion (latent) space. In video generation, typically, latent video tokens are processed in parallel~\citep{liu2024sora, blattmann2023stable, polyak2024movie}. Training-free methods~\citep{liu2024faster, zhao2024real, kahatapitiya2024adaptive} have been proposed in this case to accelerate video generation. Benefits with training-free methods are nonetheless minimal before performance degradation kicks in (see Table 1 in~\citep{kahatapitiya2024adaptive}, where one can typically save less than 30\%).

An interesting direction for future work involves adapting the inference scheduler, i.e. what patch size we are using for each denoising step, based on the requirements of each sample. It is natural to assume that when generating more static videos, increasing the temporal patch size, and thus decreasing the amount of compute along the temporal dimension, will result in smaller drops in performance. The same holds for the spatial patch sizes when generating images or videos that require less high-frequency details. 

% \red{adapters}

% \red{https://arxiv.org/abs/2312.02139: We do not change the architectur, https://arxiv.org/abs/2401.08740: ... ? https://arxiv.org/abs/2406.09416: This seems very similar, but we care about efficiency, we also do not change the architecture, so our method is easily applicable across domanins and pre-trained models as we demonstrated with all our experiments? Based on the dynamic DiT rebuttal we should also compare against other methods such as  ITOP [1], SViTE, and S2ViTE [2].}

% \paragraph{Generation with LoRAs}~All in all, we believe that our method, opens new possibilities on efficiency in diffusion models. Our method allows for different versions of the base model to be easily employed during inference. In that regard it resembles  \red{TODO additional related work on stearability and having loras umerged during inference (like the model soup staff)} \red{Can one also do other things, like having loras for diversity or loras for a specific application?}

\section{Additional Experiments and Details}
\label{sec:additional-experiments}

We provide additional experiments, complementary to the main text.

\subsection{Exposure Bias and Accumulation of Error}
\label{sec:exposure_bias}

\begin{figure}[!h]
    \centering
    \qquad
    % \subfloat[\centering]
    {{\includegraphics[width=5.5cm]{figures/mmd_loss.pdf} }}%
    \qquad \qquad
    % \subfloat[\centering]
    {{\includegraphics[width=8.0cm]{figures/mmd.png} }}%
    \qquad
    \caption{\textbf{Left:} We use maximum mean discrepancy to estimate the distribution mismatch between $\ptheta(\x_{t} | \x_{T:t + 1})$ and $\q (\x_{t} | \x_0)$. \textbf{Right:} The proposed bootstrapped loss. During training, we perform a few denoising steps with a weak model followed by a few denoising steps with a powerful model (reminiscent of the scheduler during inference) and apply a distribution matching loss on the resulting samples.}%
    \label{fig:mmd}
\end{figure}

Inference with diffusion suffers from \emph{exposure bias}, due to the discrepancy of the input distribution during training and inference~\citep{li2023alleviating, li2023error, ning2023elucidating, daras2024consistent}. Briefly, models are trained to denoise images sampled from the $\q (\x_{t} | \x_0)$ distribution~\citep{ho2020denoising}. Inference on the other hand is characterized by repeated model evaluations $\ptheta(\x_{t} | \x_{t + 1})$ and any distribution mismatch between $\ptheta(\x_{t} | \x_{T:t + 1})$ and $\q (\x_{t} | \x_0)$ accumulates, as also shown in Fig.~\ref{fig:mmd} (left). The error at each iteration depends on the model, with a perfect model resulting in $0$ error and thus no error accumulated. In our case, the accumulation of error is exacerbated by the characteristics of our model, where weak models could lead to higher, but also specific in nature, kinds of errors. Training with the standard denoising objective, where real samples are randomly noised for some $t$, does not make the more powerful model aware of the nature of the mistakes made by the weak model, rendering it unable to potentially correct them. We propose to mitigate this issue by introducing a bootstrapped distribution matching loss~\citep{tesauro1995temporal}, as illustrated in Fig.~\ref{fig:mmd} (right). The loss is applied in a patch size-dependent manner, according to the desired inference protocol (from weak to powerful model calls during inference). 

Given natural images $\x_0, \tilde{\x}_0 \sim \q(\x_0)$, we sample two time points $t_1 > t_2$ and corrupt the images with noise $\x_{t_1}^\text{target} \sim \q(\x_{t_1} | \x_0), \tilde{\x}_{t_2}^\text{pred} \sim \q(\tilde{\x}_{t_2} | \tilde{\x_0})$. We then apply a chain of denoising steps $\etheta(\tilde{\x}_{t - 1}^\text{pred} | \tilde{\x}_{t}^\text{pred}; \patchsize)$ for $t \in (t_1, t_2]$ and a patch size $\patchsize$. Ultimately, we wish for the distributions of $\x_{t_1}^\text{target}$ and $\tilde{\x}_{t_1}^\text{pred}$ to match, for which we employ the maximum mean discrepancy (MMD)~\citep{gretton2012kernel}. To let the powerful model learn and potentially correct mistakes of the weak model and to simulate how our inference patch size scheduler works, we perform the first of these denoising steps with the weak model, followed by denoising steps with the powerful model. Given a set of patch sizes $\{ \patchsizeexp[i]\}_{i=1}^{k}$ where $\patchsizeexp[1] < \patchsizeexp[2] < \dots < \patchsizeexp[k]$ and a number of denoising steps to perform with each $\denoisestepexp[1], \denoisestepexp[2], \dots, \denoisestepexp[k]$, where $\sum_{j=1}^k \denoisestepexp[j] = t_2 - t_1$, we denoise with a given patch size $\patchsizeexp[i]$ for all $t$'s in $( t_1 + \sum_{j = i + 1}^k \denoisestepexp[j], t_1 + \sum_{j = i}^k \denoisestepexp[j]]$. An illustration of this process can also be seen in Fig.~\ref{fig:mmd} (right). When sampling a time step $t_1$, we bias our sampling similar to~\citep{sauer2024fast}. The proposed distribution matching loss, inspired by the notion of consistency~\citep{song2023consistency, song2023improved}, provides a principled way to correct the errors accumulated during inference. We note that we are optimizing a simple distribution matching loss, instead of over-optimizing according to desired downstream metrics (namely FID), thus not violating Goodhart's law\footnote{Goodhart's law states that: `When a measure becomes a target, it ceases to be a good measure'.}. Different distribution matching losses (including discriminator-based losses) can also be used. Note that we only correct this exposure bias for the class-conditioned image generation experiments, as this is the only case where we fine-tune the powerful pre-trained model.

\subsection{Inference with Packing}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/packing.pdf}
    \caption{Different approaches can be employed to perform forward passes with CFG when the conditional (C) and unconditional (UC) predictions use different patch sizes. Here, each row corresponds to a sequence of tokens propagated through the DiT, and each bracket corresponds to a batch of sequences for a single NFE. Generally `Approach 2' leads to the smallest amount of FLOPs, but for batch size $1$, inference can be memory bound for low-resolution image generation. `Approach 4' mostly leads to the smallest latency, as long as the number of generated images is larger than $4$, i.e. the ratio of the sequence lengths between the powerful and the weak model. On the right, we plot FLOPs and Latency from the four different approaches of performing inference, for a different number of generated images. Batch size plays a role here (class-conditioned image generation experiments) as generated images are of lower resolution, namely $256 \times 256$, and thus sequence lengths through the Transformer are smaller. Normalized FLOPs are determined based on `Approach 2' and normalized latency based on `Approach 3'. We use \textit{torch.compile} with \textit{fullgraph=True} and \textit{mode = 'reduce-overhead'}.}%
    \label{fig:packing}
\end{figure}

We provide more details in Fig.~\ref{fig:packing}, on how to perform inference with CFG when the conditional and unconditional predictions employ a different patch size. We show this for our class-conditioned model, but results easily generalize for all our~\flexidit~models. Performing CFG entails NFEs with double the batch size (or 2 distinct NFEs), for the conditional and unconditional input, respectively. Performing the conditional and unconditional calls with different patch sizes leads to propagating sequences of different lengths through the DiT. Depending on how these sequences are `packed' together, and for lack of a hardware-specific implementation of masked attention, more FLOPs can be traded for better latency. Our weak model additionally leads to memory benefits, which can be traded for a bigger batch size when serving the model. Notice that current state-of-the-art image generation models in practice require much longer sequences compared to the $256 \times 256$ images generated here (see also Section~\ref{sec:latency}) and so generation is compute-bound even when generating with batch size equal to 1.

\subsection{What does the Model Learn?}
\label{sec:intuition}

Transformers are composed of a series of channel mixing components --- feed-forward layers with shared weight applied to all tokens in the sequence --- and token mixing components --- attention applied to tokens in the sequence. By coercing the model to learn the denoising objective when applied to images processed with different patch sizes, we are enforcing inductive bias in its weights and helping it better understand global structures in the image~\citep{d2021convit, raghu2021vision, xiao2021early}. We test this hypothesis and evaluate what the model is learning in the following ways in Fig.~\ref{fig:intuition}. (left) We visualize using t-SNE, centered kernel alignment (CKA) between feature maps across layers when performing NFEs with different patch sizes. Activations across layers exhibit similar transformations~\citep{von2024language}, except the early layers, where features are lower level, i.e. more patch specific. (right) We visualize the Jensen–Shannon divergence (JSD) between attention maps (interpolated to the same image space) when performing NFEs with different patch sizes. We compare using our~\flexidit~model with different patch sizes (Flexible) versus using two static models trained with different patch sizes (namely \textit{DiT-XL/2} and a trained from scratch \textit{DiT-XL/4}). Our flexible model showcases lower JSD, demonstrating better knowledge transfer between the different patch sizes. We believe that this ``transfer'' of knowledge is crucial to (1) confirm that parameter sharing across patch sizes is valid and (2) ensure that fine-tuning can be fast and sample efficient. 

\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=0.8\linewidth]{figures/xl-256/interpretability_pad.pdf}
    \caption{Interpretability of the model activations and attention scores, when propagating samples tokenized with different patch sizes.}
    \label{fig:intuition}
  \end{center}
\end{figure}

% \red{Finetuning vs train from scratch, this is almost like a good initialization that helps the model train faster.}
% \red{mention here that finetuning is subtle and easy to do compared to the original training}



\subsection{Class-Conditioned Image Generation}

% \begin{figure}[!h]
%     \centering
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/xl-256/xl_256_FID.pdf} }}%
%     \hfill
%     {{\includegraphics[width=0.19\textwidth]{figures/xl-256/xl_256_sFID.pdf} }}%
%     \hfill
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/xl-256/xl_256_IS.pdf} }}%
%     \hfill
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/xl-256/xl_256_Precision.pdf} }}%
%     \hfill
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/xl-256/xl_256_Recall.pdf} }}%
%     \caption{Inference without CFG of the \textit{DiT-XL/2} model for class-conditioned generation on \ImageNet. We plot (a) FID (b) sFID, (c) inception score, (d) precision and (3) recall when generation $10,000$ samples with $250$ steps of the DDPM scheduler without CFG scale. \red{These results might not be too good for us to show...}}%
%     \label{fig:xl-256-more-results}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/base/base_FID.pdf} }}%
%     \hfill
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/base/base_sFID.pdf} }}%
%     \hfill
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/base/base_IS.pdf} }}%
%     \hfill
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/base/base_Precision.pdf} }}%
%     \hfill
%     % \subfloat[\centering]
%     {{\includegraphics[width=0.19\textwidth]{figures/base/base_Recall.pdf} }}%
%     \caption{Inference without CFG of the \textit{DiT-B/2} model for class-conditioned generation on \ImageNet. We plot (a) FID, (b) sFID, (c) inception score, (d) precision and (e) recall when generation $10,000$ samples with $250$ steps of the DDPM scheduler without CFG scale.}%
%     \label{fig:base-more-results}
% \end{figure}

\begin{figure}[!h]
    \centering
    % \subfloat[\centering]
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_ultimatum_FID_50k.pdf} }}%
    \hfill
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_ultimatum_sFID_50k.pdf} }}%
    \hfill
    % \subfloat[\centering]
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_ultimatum_IS_50k.pdf} }}%
    \hfill
    % \subfloat[\centering]
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_ultimatum_Precision_50k.pdf} }}%
    % \hfill
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_ultimatum_Recall_50k.pdf} }}%
    \hfill
    % \subfloat[\centering]
    % {{\includegraphics[width=0.19\text width]{figures/xl-256/xl_256_Recall.pdf} }}%
    \caption{More metrics for our~\flexidit~based on the \textit{DiT-XL/2} model for class-conditioned generation on \ImageNet. We plot (a) FID (b) sFID, (c) inception score, (d) precision, and (e) recall when generating $50,000$ samples with $250$ steps of the DDPM schedule for various values of the CFG scales. Red lines correspond to the values that lead to the optimum FID scores for each compute level.}%
    \label{fig:xl-256-more-results}
\end{figure}


% \begin{wraptable}{r}{5.5cm}
% \vspace{-8mm}
% \begin{tabular}{ccc}\\\toprule  
%  & \textit{DiT-B/2} & \textit{DiT-XL/2} \\\midrule
% Depth & 12 & 28\\  \midrule
% Hidden size & 1024 & 1152\\  \midrule
% \makecell{Default \\ patch-size} & 2 & 2\\  \bottomrule
% \end{tabular}
% \vspace{0mm}
% \end{wraptable} 
\paragraph{Additional metrics.}~For class-conditioned experiments on the main text we focused on the \textit{DiT-XL/2}~\citep{peebles2023scalablediffusionmodelstransformers} and FID as a metric. Here, we report more metrics apart from FID, namely Inception Score (IS), sFID, and Precision/Recall. Results are presented in Fig.~\ref{fig:xl-256-more-results} for our flexible \textit{DiT-XL/2} model. 
% Here we also present results for the \textit{DiT-B/2} model. This model processes images exactly on the same way and just differs on the model parameters. As also commonly done~\citep{peebles2023scalablediffusionmodelstransformers}, we also report more metrics apart from FID, namely Inception Score (IS), sFID [34] and Precision/Recall. Results are presented in Fig.~\ref{fig:xl-256-more-results} for the \textit{DiT-XL/2} model and Fig.~\ref{fig:base-more-results} for the \textit{DiT-B/2}. 
We remind that for class-conditioned models, we fine-tune models using our distribution matching loss. As a result, the powerful model that we get after fine-tuning is different to the pre-trained checkpoints we start from. 
% We notice that our finetuning has some effect on the metrics reported. For both of the models tested, results for sFID and Precision are better than the ones of the baseline models (pre-trained publicly available checkpoints). On the other hand, results on Inception score and Recall are slightly worse. Although these metrics have been criticized before for their validity, 
To verify that our weak model does not lead to less diverse samples, we embark on a small experimental study to guarantee the diversity of generated images. We follow~\citep{sushko2020you} and generate images from the same label map. We then calculate pairwise similarity/distance between these images and average across all similarities/distances and all label maps. We use MS-SSIM~\citep{wang2003multiscale}, LPIPS~\citep{zhang2018unreasonable} and plot results in Fig.~\ref{fig:diversity}. Results indicate very similar values in terms of the diversity of the generated images. We also provide some sample images demonstrating diversity from the baseline model in Fig.~\ref{fig:baseline_10} and our tuned model in Fig.~\ref{fig:flex_10}. Note that the diversity of the generated images is in general high and not affected much by using the weak model for more of the initial denoising steps.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/xl-256/diversity.pdf}
    \caption{Average distance/similarity of images generated from the same label map. Both metrics take values between $0$ and $1$.}
    \label{fig:diversity}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/xl-256/baseline_10.png}
    \caption{Sample images generated with the baseline \textit{DiT-XL/2} for the \ImageNet~category `Brambling'.}
    \label{fig:baseline_10}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/xl-256/flex_10.png}
    \caption{Sample images generated with our flexible \textit{DiT-XL} model when performing inference using only the powerful model, for the \ImageNet~category `Brambling'.}%
    \label{fig:flex_10}
\end{figure}

\paragraph{Caching distance.}
In the main text, we have shown how weak and powerful models generate more similar predictions during the early steps of the denoising process. Previous papers to accelerate diffusion have largely relied on caching~\citep{wimbauer2024cache, yuan2024ditfastattn, chen2024asyncdiff, ma2024deepcache, zhao2024real} previous activations, by taking advantage of the similarity in activations between successive steps. For completeness, we also plot the caching distance between activations of the same layer between successive generation steps in Fig.~\ref{fig:caching_layers}. In this paper, we do not employ caching but focus on an orthogonal approach. We advocate that all steps are important for high-quality image generation, as demonstrated by our experiments on reducing the overall number of generation steps. Instead of completely skipping steps, we simply invest less compute for them, and let the model decide how to allocate this compute.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/xl-256/caching_layers.pdf}
    \caption{We plot average distance ($L_2$-norm) between activation of different layers during successive steps of the denoising process of the \textit{DiT-XL/2} model. Different layers exhibit different characteristics. Similar observations have been made in~\citep{ma2024learning}.}%
    \label{fig:caching_layers}
\end{figure}


\paragraph{Additional schedulers.}~Based on the results on activation distance between successive denoising steps (Fig.~\ref{fig:caching_layers}), one could argue that first denoising steps are also important and thus a better inference scheduler would deploy the powerful model for these as well. In practice, we found no benefit from deploying a scheduler that works like that. Notice though how activation distance is high for the first denoising steps only for some of the layers. We additionally experimented with dynamic schedulers that choose the patch size of each denoising step based on the activation distance of different layers between successive denoising steps. We did not find additional potential benefits. 

In this paper, we are training a single model that can denoise images with any patch size for any denoising step. Given a fixed desired inference scheduler --- i.e. if we know exactly which $t$'s to run with the powerful and the weak model ---, one can train a model specifically based on that, leading to undoubtedly better quality images for the same compute. Similar techniques are regularly applied in consistency models~\citep{song2023consistency}. Finally, we compare our scheduler --- performing the first $\diffusionstepsweak$ denoising steps with a weak model --- versus the opposite scheduler, i.e. performing the last $\diffusionstepsweak$ denoising steps with a weak model. Results in Fig.~\ref{fig:opposite-scheduler} indicate that, as expected, using the weak model in the last diffusion steps is suboptimal, leading to a loss in fine-grained details. We also provide qualitative examples of how these different schedulers affect image quality in Fig.~\ref{fig:opposite-examples}.

% \red{mention something about scheduling the betas in DDPM instead of our scheduler}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/xl-256/cfg/opposite_scheduler.pdf}
    \caption{We compare our scheduler versus a different scheduler that uses the weak model for the last denoising steps when generating class-conditioned images. Points correspond to the minimum --- concerning CFG scale --- FID values.}
    \label{fig:opposite-scheduler}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/xl-256/generated_images/opposite_examples.pdf}
    \caption{We compare our scheduler versus a different scheduler that uses the weak model for the last denoising steps when generating class-conditioned images. Using the weak model for the last denoising steps leads to images with lower image fidelity.}
    \label{fig:opposite-examples}
\end{figure}

% \begin{wrapfigure}{r}{0.32\textwidth}
%   \begin{center}
%     \vspace{-10mm}
%     \includegraphics[width=0.32\textwidth]{figures/xl-256/flexible_baseline_compute_optimality_FID.pdf}
%     \vspace{-15mm}
%   \end{center}
% \end{wrapfigure}
% \paragraph{Compute optimality}~In Section~\ref{sec:class_conditioned_experiments}, we provided results on how number of steps and number of weak vs powerful steps offer different benefits in terms of potential compute savings. As our `full-compute' model, i.e. performing inference using only our powerful model, is different than the baseline model in this case, we also present the same plot with our `full-compute` model as the baseline that we would like to beat. Even in the case of generating images without any classifier-free-guidance (as in this case), compute benefits can be substantial without any performance degradation.

\paragraph{More results on CFG.}~In the main text, we presented results on performing inference with different CFG scales and different invocations to our weak model for the unconditional and conditional part. The $4$ generated curves in Fig.~\ref{fig:xl-experiments} (middle) correspond to performing our scheduler as $250/250$, $130/130$, $70/70$, and $30/0$ where $x/y$ means using the powerful model for the last $x$ denoising steps for the conditional and $y$ denoising steps for the unconditional part. When performing CFG, we use the update rule as presented in the main text \begin{align*}
    \begin{cases}
      \etheta(\x_{t - 1} | \x_{t}, \emptyset; \patchsizeuncondition) + \guidancescaleone (\etheta(\x_{t - 1} | \x_{t}, \condition; \patchsizecondition) - \etheta(\x_{t - 1} | \x_{t}, \emptyset; \patchsizeuncondition)), & \text{if}\ \patchsizecondition = \patchsizeuncondition \\
      \etheta(\x_{t - 1} | \x_{t}, \condition; \patchsizeuncondition) + \guidancescaletwo (\etheta(\x_{t - 1} | \x_{t}, \condition; \patchsizecondition) - \etheta(\x_{t - 1} | \x_{t}, \condition; \patchsizeuncondition)), & \text{if}\ \patchsizecondition < \patchsizeuncondition
    \end{cases}.
\end{align*}
\begin{figure}[!h]
  \begin{center}
    \includegraphics[width=0.55\textwidth]{figures/inference_cfg.pdf}
  \end{center}
\end{figure}
This guidance scheme seeks to reduce errors made by the powerful model, enhancing potential differences in predictions of the corresponding weak model, when the two models disagree, indicating the general direction towards higher-quality samples. In practice, different values of $\guidancescaleone$ and $\guidancescaletwo$ lead to the best results. We find that the rule $(1 - \guidancescaleone) / (1 - \guidancescaletwo) = 2.5$, works consistently across experiments. Although we fix the value of the CFG scale during inference, different combinations are likely to lead to higher quality images as demonstrated by previous work~\citep{castillo2023adaptive}, which we leave for future exploration.

We point out that our scheduler is very stable in terms of performance attained for similar compute. For instance, performing inference with a $70/70$ scheduler or a $90/50$ scheduler, which both require the same overall compute, produces FID results of $2.64$ and $2.65$ respectively. Finally, we present detailed experiments on the effect of the CFG scale for different levels of compute and more metrics in Fig.~\ref{fig:detailed-cfg-results}.

\begin{figure}[!h]
    \centering
    % \subfloat[\centering]
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_FID_50k.pdf} }}%
    \hfill
    % \subfloat[\centering]
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_sFID_50k.pdf} }}%
    \hfill
    % \subfloat[\centering]
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_IS_50k.pdf} }}%
    \hfill
    % \subfloat[\centering]
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_Precision_50k.pdf} }}%
    % \hfill
    {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_Recall_50k.pdf} }}%
    \hfill
    \caption{Effect of CFG scale on the generated images from our class-conditioned~\flexidit~model. We plot (a) FID, (b) sFID, (c) inception score, (d) precision, and (e) recall when generation $50,000$ samples with $250$ steps of the DDPM scheduler.}%
    \label{fig:detailed-cfg-results}
\end{figure}


\subsection{Text-to-Image Experiments}
\label{sec:details_text_to_image}

Generally, T2I generation is performed for a fixed target CFG scale. For our experiments we choose $\guidancescale = 4.5$ for the~\pixart~model, as this is the value used in~\citep{chen2023pixart} and $\guidancescale = 6.0$ for the~\emu~model, as for these values we observed the best quality images. In general, we can match with our dynamic inference other target values of the CFG scale. One simply needs to adjust the used CFG scale for the dynamic inference accordingly.

We follow the evaluation protocol of PIXART-$\alpha$~\citep{chen2023pixart} and perform inference using the same solvers as they do, namely iDDPM~\citep{dhariwal2021diffusion} for $100$ steps, DPM solver~\citep{lu2022dpm} for $20$ steps, and SA solver~\citep{xue2024sa} for $25$ steps. In the main text --- Fig.~\ref{fig:t2i-experiments} (left) --- we presented results for the iDDPM solver. We present results for all the schedulers with the settings used in PIXART-$\alpha$ in Fig.~\ref{fig:pixart_pareto_iddpm},~\ref{fig:pixart_pareto_dpm} and~\ref{fig:pixart_pareto_sa}. For all the schedulers, there are settings where we reach the Pareto front of FID vs CLIP score of the baseline model with a lot less required compute.

\begin{figure}[!htb]
    \begin{minipage}{.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pixart/pareto-iddpm.pdf}
        \caption{FID vs CLIP score using iDDPM for $100$ steps for the~\pixart~model.}%
        \label{fig:pixart_pareto_iddpm}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pixart/pareto-dpm-solver.pdf}
        \caption{FID vs CLIP score using the DPM-solver for $20$ steps for the~\pixart~model.}%
        \label{fig:pixart_pareto_dpm}
    \end{minipage}
    \hfill
    \begin{minipage}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pixart/pareto-sa-solver.pdf}
        \caption{FID vs CLIP score using the SA-solver for $25$ steps for the~\pixart~model.}%
        \label{fig:pixart_pareto_sa}
    \end{minipage}
\end{figure}


To better characterize the effect of reducing compute, i.e. heavier use of the weak model, we also present more detailed results for the DDPM scheduler in Fig.~\ref{fig:pareto-full-100-steps}. Less compute-heavy inference schedulers, often produce images with smaller possible maximum CLIP scores (for large CFG guidance scales $\guidancescale$). In practice, as large CFG scale values lead to larger values of FID, these are less preferred. In every case, our weak models can max the FID vs CLIP score tradeoff of the base model for the default configuration used, i.e. $\guidancescale = 4.5$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/pixart/pareto-full-100-steps.pdf}
    \caption{FID vs CLIP score using DDPM for $100$ steps for the~\pixart~model, for different levels of compute. On the right, we present results separately for each compute level.}%
    \label{fig:pareto-full-100-steps}
\end{figure}

We also provide results on using a smaller overall number of steps with the DDPM solver in Fig.~\ref{fig:pixart_pareto_steps}. To generate the baseline curves, we sample $10,000$ samples using a CFG scale $\guidancescale$ from the set $\{1.0, 1.125, 1.25, 1.375, 1.5, 1.625, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5\}$. The same values are used when sampling with our flexible models. Finally, we also provide FID vs CLIP for the~\emu~model in Fig.~\ref{fig:pareto-emu-50-steps}. In this case, we take CFG scales $\guidancescale$ from the set $\{1.0, 1.5, 2.0, 2.5, 3.0, 4.5, 6.0, 7.5, 8.0, 9.0\}$. We use captions from the training set of \text{MS COCO} to generate images. Neither of the models was trained on images from this dataset.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/emu/emu_fid_clip.pdf}
    \caption{FID vs CLIP score using DDIM for $50$ steps for the~\emu~model.}%
    \label{fig:pareto-emu-50-steps}
\end{figure}


\begin{figure}[!h]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/pixart/pareto_steps.pdf}
        \caption{We plot FID vs CLIP score when generating images with different CFG scales. (left) Overall Pareto front when generating images with $100$ denoising steps. (right) Pareto front generating images with a different number of steps zoomed in the typical tradeoff generation values.}
        \label{fig:pixart_pareto_steps}
\end{figure}%

\paragraph{VQA results.}~VQA scores are calculated by querying a visual-question-answering model to produce an alignment
score by computing the probability of a "Yes"  answer to a simple "Does this
figure show \{text\}?". question. To calculate this score, we use the \textit{clip-flant5-xxl} model from huggingface\footnote{\url{https://huggingface.co/zhiqiulin/clip-flant5-xxl}} as suggested in ~\citep{lin2025evaluating}. We provide more detailed results on the VQA benchmark in Tables~\ref{tab:pixart_vqa_detailed} and~\ref{tab:emu_vqa_detailed}. More specifically, we provide per dataset VQA scores, along with the CFG scale $\guidancescale$ used to generate the images for each case  . As we can see, using the weak model requires a bigger CFG scale to reach the same level of optimality (calculated from the FID vs CLIP score tradeoff). We also note that using the weak model often leads to images with better text alignment. We hypothesize that fewer tokens (as a result of larger patch sizes) help with spatial consistency at the beginning of the denoising process. To calculate VQA scores, we take the first $200$ prompts from each dataset and use the \textit{train} split from the \textit{DrawBench}~\citep{saharia2022photorealistic}, \textit{train} split from the \textit{Pick-a-Pic}~\citep{kirstain2023pick}, \textit{test} split from the \textit{Winoground}~\citep{thrush2022winoground} and \textit{tifa\_v1.0\_text\_inputs}\footnote{\url{https://github.com/Yushi-Hu/tifa/blob/main/tifa_v1.0/tifa_v1.0_text_inputs.json}} from the \textit{TIFA160}~\citep{hu2023tifa} dataset.

\begin{table}[!h]
    \centering
    {\normalsize
    \begin{tabular}{ |c|c|cccc|c| } 
    \toprule
     & \makecell{CFG scale \\ $\guidancescale$} & \textit{DrawBench} & \textit{Pick-a-Pic} & \textit{Winoground} & \textit{TIFA160} & Average \\
    \midrule
    \pixart~($100$ \%) & $4.5$ & $58.93$ & $50.56$ & $62.01$ & $81.65$ & $63.29$ \\
    \pixart~($92.9$ \%) & $4.5$ & $58.37$ & $51.53$ & $62.09$ & \underline{$82.16$} & $63.54$ \\
    \pixart~($85.8$ \%) & $4.5$ & $57.58$ & $51.67$ & $62.41$ & $81.53$ & $63.30$ \\
    \pixart~($78.8$ \%) & $4.7$ & $58.62$ & $51.97$ & $62.04$ & $80.60$ & $63.31$ \\
    \pixart~($71.7$ \%) & $4.7$ & $58.44$ & $51.56$ & $63.03$ & $80.57$ & $63.40$ \\
    \pixart~($64.6$ \%) & $4.9$ & \underline{$60.16$} & \underline{$52.34$} & $62.98$ & $80.06$ & \underline{$63.89$} \\
    \pixart~($57.7$ \%) & $5.0$ & $59.04$ & $50.80$ & \underline{$63.27$} & $79.90$ & $63.26$ \\
    \pixart~($50.5$ \%) & $5.0$ & $56.77$ & $51.72$ & $61.87$ & $79.38$ & $62.44$ \\
    \pixart~($43.4$ \%) & $5.0$ & $56.87$ & $51.92$ & $61.30$ & $78.33$ & $62.11$ \\
    \bottomrule
    \end{tabular}
    }
    \caption{Detailed VQA evaluations for the benchmarks tested with the~\pixart~model. As in the class-conditioned experiments, using more of the weak model during denoising requires a higher CFG scale $\guidancescale$ to reach optimum performance.}
    \label{tab:pixart_vqa_detailed}
\end{table}

\begin{table}[!h]
    \centering
    {\normalsize
    \begin{tabular}{ |c|c|cccc|c| } 
    \toprule
     & CFG-scale $\guidancescale$ & \textit{DrawBench} & \textit{Pick-a-Pic} & \textit{Winoground} & \textit{TIFA160} & Average \\
    \midrule
    \emu~($100$ \%) & $6.0$ & $69.44$ & $58.70$ & $65.75$ & \underline{$86.77$} & $70.17$ \\
    \emu~($84.3$ \%) & $6.0$ & $68.00$ & $58.93$ & \underline{$67.33$} & $86.51$ & $70.19$ \\
    \emu~($68.6$ \%) & $6.25$ & $69.53$ & \underline{$60.62$} & $66.00$ & $85.33$ & \underline{$70.37$} \\
    \emu~($52.9$ \%) & $6.5$ & \underline{$69.79$} & $58.14$ & $66.23$ & $86.20$ & $70.09$ \\
    \bottomrule
    \end{tabular}
    }
    \caption{Detailed VQA evaluations for the benchmarks tested with the~\emu~model. As in the class-conditioned experiments, using more of the weak model during denoising requires a higher CFG scale $\guidancescale$ to reach optimum performance.}
    \label{tab:emu_vqa_detailed}
\end{table}

\paragraph{Alignment between powerful and weak model.}~It is common practice nowadays to train images (and especially videos) in different stages, where a large (potentially lower quality) dataset is used for the first stage, followed by a shorter fine-tuning stage, characterized by higher quality and aesthetically more pleasing images. Although we are directly distilling the weak model from the predictions of the powerful model, the data used throughout training are still important. In practice, our fine-tuning is sample efficient, and we find that even a few thousand images ($<5000$) are enough to succeed. We thus suggest fine-tuning on the last (potentially smaller) but higher-quality dataset. When generating images based on shorter prompts with~\emu, we use a prompt re-writer, prompting a small LLM to expand on the information provided. We consider this prompt re-writer as part of the model. 

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\textwidth]{figures/pixart/pixart_iddpm.pdf}
%     \caption{\red{DDPM Solver Pixart. This plot does not say much, move to appendix}.}%
%     \label{fig:pixart_iddpm}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/pixart/pixart_dpm-solver.pdf}
%     \caption{\red{DPM solver Pixart}.}%
%     \label{fig:pixart_dpm-solver}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/pixart/pixart_sa-solver.pdf}
%     \caption{\red{SA solver pixart}.}%
%     \label{fig:pixart_sa-solver}
% \end{figure}

% \paragraph{Optimality}~\red{TODO}


% \begin{figure}[!htb]
%     \begin{minipage}{.485\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/pixart/optimality-FID.pdf}
%         \caption{\red{optimality FID pixart. This plot does not show change of cfg scale for weak model, so it seems bad for us}.}
%         \label{fig:pixart_optimality_fid}
%     \end{minipage}%
%     \hfill
%     \begin{minipage}{0.485\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/pixart/optimality-clip_score.pdf}
%         \caption{\red{optimality clip pixart. This plot does not show change of cfg scale for weak model, so it seems bad for us}.}%
%         \label{fig:pixart_optimality_clip_score}
%     \end{minipage}
% \end{figure}


\section{Implementation Details}
\label{app:implementation-details}

We provide additional details on the experiments in the main text.

\subsection{Figure Details}

\paragraph{Prompts used for Fig.~\ref{fig:emu-examples-0}.}~We provide in Table~\ref{tab:emu-prompts} the exact prompts used to generate the images.

\begin{table}[!h]
    \centering
    \begin{tabular}{|p{16cm}|}
        \toprule
        \multicolumn{1}{|c|}{Prompts for Fig.~\ref{fig:emu-examples-0}} \\
        \midrule
        % The image shows a magnificent tree sprouting from an open book on a table, its branches heavy with a variety of ripe fruits, such as apples, oranges, and grapes, that have fallen to the floor, filling the room with a sweet and intoxicating aroma. The tree's trunk is thick and gnarled, with visible knots and textures, and its leaves are a vibrant green. The book's pages are yellowed and worn, and the table is made of dark wood with intricate carvings. The room is dimly lit, with soft light filtering in from a nearby window, illuminating the scene with a warm glow.\\
        % \midrule
        The image shows a frog wearing a golden crown with intricate designs, sitting on a wooden log in a serene environment reminiscent of a Japanese anime setting. The frog's crown is adorned with small gems and its eyes are large and expressive. The log is covered in moss and surrounded by lush greenery, with a few cherry blossoms visible in the background. The frog's skin is a vibrant shade of green with blue stripes, and it has a regal demeanor, as if it is a monarch of the forest. The overall atmosphere is peaceful and whimsical. \\
        \midrule
        The image shows a serene waterfall cascading down a rocky slope in a lush tropical forest, reminiscent of Claude Monet's impressionist style. Sunlight filters through the dense foliage above, casting dappled shadows on the misty veil surrounding the falls. The water plunges into a crystal-clear pool, surrounded by large rocks and vibrant greenery. The atmosphere is tranquil, with a warm color palette and soft brushstrokes evoking a sense of serenity. The forest floor is covered in a thick layer of leaves, and the sound of the waterfall echoes through the air.\\
        % \midrule
        % \multicolumn{1}{|c|}{Prompts for Fig.~\ref{fig:emu-examples-1}} \\
        % \midrule
        % The image shows a serene waterfall cascading down a rocky slope in a lush tropical forest, reminiscent of Claude Monet's impressionist style. Sunlight filters through the dense foliage above, casting dappled shadows on the misty veil surrounding the falls. The water plunges into a crystal-clear pool, surrounded by large rocks and vibrant greenery. The atmosphere is tranquil, with a warm color palette and soft brushstrokes evoking a sense of serenity. The forest floor is covered in a thick layer of leaves, and the sound of the waterfall echoes through the air. \\
        % \midrule
        % The image is a stunning underwater macro shot of a glowing jellyfish, perfectly centered and symmetrical. The jellyfish's translucent body glows with a soft, ethereal light, surrounded by vibrant corals and bulbs that add pops of color to the neutral-toned background. The image is hyperdetailed and cinematic, with a complex and extremely detailed background that showcases the beauty of the ocean's depths. The bokeh effect adds a sense of depth and dimensionality to the image, drawing the viewer's eye to the jellyfish's glowing form. \\
        % \midrule
        % The image depicts a stunning flower in a serene and lush botanical garden, captured with a DSLR camera. The flower is a vibrant shade of pink, with delicate petals and a prominent center. It is situated in the foreground, surrounded by an assortment of foliage and stems of varying textures and colors. The garden's tranquil atmosphere is accentuated by the soft, diffused light and the subtle mist rising from the ground. The DSLR camera's high-quality sensor has captured the intricate details of the flower and its surroundings, showcasing the beauty of nature. \\
        \bottomrule
    \end{tabular}
    \caption{Details on the prompts used to generate the images in the paper.}
    \label{tab:emu-prompts}
\end{table}

\paragraph{Details on Fig.~\ref{fig:motivation}.}~In Fig.~\ref{fig:motivation} (b), we fix randomness of the denoising process in terms of the initial sampled image $\p(\x_\diffusionsteps)$, and from the denoising process in~\cref{eq:denoise_images}. Then we generate images with $250$ steps using the \textit{DiT-XL/2} official public checkpoint. During denoising, we modify only $1$ of the $250$ denoising steps, bypassing the model predictions from that step through a high/low pass filter. We then compare the resulting generated images in terms of LPIPS, $L_2$ distance, SSIM, and DreamSim. In general, modifying one of the first denoising steps, leads to larger final image differences, due to the accumulation of differences. We still note a distinctive pattern: a high-pass filter, i.e. removing low-pass components, leads to larger image differences during the first denoising steps. The opposite holds for the last denoising steps. We can thus argue, that low-pass components, i.e. `coarser' image details are more important compared to high-frequency details, for the first denoising steps. To calculate spatial frequencies, we keep the corresponding values of the FFT of an image.


\paragraph{Details on Fig.~\ref{fig:latency}.}~In Fig.~\ref{fig:latency}, we plot GPU utilization when propagating different sequence lengths. All experiments are conducted in \textit{bfloat16} using \textit{PyTorch 2.5} and \textit{CUDA 12.4} and with our~\emu~DiT that has $24$ layers and a hidden dimension of $2048$. We also plot peak FLOPs and memory bandwidth for the GPU tested, an \textit{NVIDIA H100 SXM5} in this case. When we report FLOPs, we count additions and multiplications separately, as it is commonly done~\citep{hoffmann2022training}. We count FLOPs as the theoretical required operations for a forward pass, and not the actual GPU operations, which might be a higher number due to the potential recalculation of partial results. As bytes for the x-axis, we only consider the model parameters ($2$ bytes per model parameter). Note that for our choice of weak models, the GPU is fully utilized (it reaches the maximum compute intensity that can be achieved for this application). In reality, compute intensity drops when larger sequence lengths are used, mainly due to the larger memory footprint of intermediate activations. Thus, latency benefits are indeed even larger than FLOPs benefits reported in the paper. Compiling and fusing operations is crucial to ensure that the GPU is not bottlenecked by waiting instructions from the CPU. When we are performing inference with the weak model without first merging the LoRAs, inference time is proportional to the additional FLOPs required. For the attention operation, we use the \textit{memory\_efficient\_attention} operation from the \textit{xformers} library. Other efficient implementations of attention do not lead to significant differences. Our T2I model operates in a $128 \times 128$ latent space, which means that using a patch size of $(1, 2, 2)$ results in $4096$ tokens, compared to $1024$ tokens for the $(1, 4, 4)$ patch size. Our T2V model operates in a $32 \times 88 \times 48$ latent space, which means that using a patch size of $(1, 2, 2)$, $(2, 2, 2)$, $(1, 4, 4)$ leads to $33792$, $16896$ and $8448$ tokens respectively. We obtain similar results using different-sized models, like our~\moviegen~model.

When we report FLOPs in the paper, we report numbers for the denoising process, as it is commonly done. We thus ignore latency induced by decoders that map samples from a latent space, or potential prompt-rewrite modules. The added latency by the decoder, is in our settings negligible.
% For the small additional compute added by the LoRAs, we are no longer compute bound, due to the small compute budget required for the computation when propagating through the LoRA weights. Thus, although the additional compute introduced due to the LoRA weights is minimal, the effect on the inference time is larger. This only holds when performing inference with batch size $1$, with the effect quickly disappearing with larger batch sizes.

\subsection{Flexifying Diffusion Transformers}

\begin{wrapfigure}{r}{0.14\textwidth}
  \begin{center}
    \includegraphics[width=0.14\textwidth]{figures/positions.png}
  \end{center}
\end{wrapfigure}

Although for the class-conditioned experiments, we use a single embedding and de-embedding layer that we always project to the required patch size, we note that this projection can be done once to pre-calculate embedding and de-embeddding parameters during inference. These projected embeddings can then be used out of the box, for the tradeoff of some minuscule additional memory. The choice of $\patchsizeexp[\prime]$ as the underlying patch size is not too important for our experiments. In practice, we use a value of $\patchsizeexp[\prime] = 4$. As mentioned in the main text, we add positional embeddings according to the coordinates of each patch in the original image. A schematic of this can be found on the right.
% or the text-to-image experiments, we use $2$ different embeddings, so that functional preservation for the powerful model is guaranteed. In this case, we create different embedding kernels, for each newly supported patch size. We initialize these as $\projectembedding^\dagger \weightembedding$, where $\projectembedding \in \R^{\patchsizecurrent * \patchsizecurrent \times \patchsize * \patchsize}$ for each added additional patch size $\patchsizecurrent$ added and for the original pre-trained patch size $\patchsize$.

Apart from the architecture modifications listed in the main text, we experimented with adding patch size specific temperatures in the self-attention computation:

\begin{equation*}
    \text{softmax}\left( \frac{\mathbf{Q}\mathbf{K}^\top}{\tau_{\patchsize}\sqrt{\hiddensize}} \right) \mathbf{V},
\end{equation*}
where $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ are the queries, keys and values respectively and $\tau_{\patchsize}$ is a patch size specific temperature initialized as $1$. We do not include this in the end, as it occasionally leads to instabilities during fine-tuning, even under different parametrizations.

\paragraph{Class-conditioned implementation details.}~We largely use the same hyperparameters as~\citep{peebles2023scalablediffusionmodelstransformers} to fine-tune. When fine-tuning to match distributions, we train to minimize the MMD loss, as introduced in Section~\ref{sec:exposure_bias}. During bootstrapping, we denoise images with a DDPM scheduler, operating on $\diffusionsteps = 250$ steps, the same as the target inference scheduler. As we found that MMD distance is higher for diffusion steps closer to $x_0 \sim \q(\x_0)$ --- see also Fig.~\ref{fig:mmd} ---, we bias the sampling to reflect that during training as well.

\begin{table}[!h]
    \centering
    \begin{tabular}{ |c|c| } 
    \toprule
    Parameter & Value \\
    \midrule
    training data & \ImageNet \\
    learning rate & $10^{-4}$ \\
    weight decay & $0.0$ \\
    EMA update frequency & every step \\
    EMA update rate & $0.9999$ \\
    \bottomrule
    \end{tabular}
    \caption{Class-conditioned implementation details.}
    \label{tab:dit-implementation-details}
\end{table}

In our experiments, we focused on fine-tuning pre-trained models, as we were interested in efficiency. We note that training with different patch sizes has been used in the past to also accelerate pre-training~\citep{beyer2023flexivit, anagnostidisnavigating}. We believe that flexible patch sizes can also be used in this application to accelerate pre-training.

\paragraph{\pixart~implementation details.}
For the~\pixart~model, we follow exactly the recipe of~\citep{chen2023pixart}, and fine-tune a $256 \times 256$ pre-trained variant\footnote{Our starting pre-trained model exactly matches the public checkpoint~\url{https://huggingface.co/PixArt-alpha/PixArt-XL-2-SAM-256x256}.}. For fine-tuning, we use the same image dataset, namely the SAM dataset\footnote{\url{https://segment-anything.com/}.} with captions generated from a vision-language model. The model has overall the same parameters as the \textit{DiT-XL/2} model, with the addition of cross-attention blocks. When adding new embedding and de-embedding layers, we initialize them as we did for the class-conditioned experiments. Embedding layers are initialized to $\projectembedding^\dagger \weightembedding$ and de-embedding layers are initialized to $\weightdeembedding \projectdeembedding^\dagger$. Here $\weightembedding$, $\weightdeembedding$ are the pre-trained model parameters and $\projectembedding$, $\projectdeembedding$ are the same --- patch size dependent --- fixed projection matrices that better preserve the norm of the output activations at initialization. As aforementioned, we add a patch size embedding that is added to all tokens in the sequences after the tokenization step. This embedding is equal to $0$ for the pre-trained patch size, to ensure functional preservation. We fine-tune the~\pixart~model on a small subset of the SAM dataset used to originally train the target model. We add LoRAs on the self-attention and feed-forward layers, with a LoRA dimension of $32$. We use a higher learning rate, due to the different learning objectives --- distilling a powerful model's predictions into the ones of a weal model.

\begin{table}[!h]
    \centering
    \begin{tabular}{ |c|c| } 
    \toprule
    Parameter & Value \\
    \midrule
    training data & SAM with captions from a VLM model \\
    optimizer & \textit{AdamW} \\
    learning rate & $8 \times 10^{-4}$ \\
    weight decay & $10^{-2}$ \\
    gradient clipping & $0.02$ \\
    % warmup steps & $1000$ \\
    batch size & $512$ \\
    EMA update frequency & every step \\
    EMA update rate & $0.9999$ \\
    LoRA rank & $32$ \\
    \bottomrule
    \end{tabular}
    \caption{Image text-conditioned implementation details.}
    \label{tab:pixart-implementation-details}
\end{table}

\paragraph{\emu~implementation details.}~Our~\emu~model is fundamentally identical to the~\pixart~model. Small variations are due to different ways to calculate text embeddings, which lead to a different number and size of the cross-attention tokens, and slight architectural modifications --- primarily the use of QK-normalization~\citep{dehghani2023scaling} and the use of learnable positional embeddings. We train using a high-quality aesthetic dataset. To calculate metrics based on the $1024 \times 1024$ images generated with this model, we follow the evaluation protocol of~\citep{xu2023restart} and resize images to $512 \times 512$. For both our~\emu~and our~\moviegen~model, we use a LoRA rank of $64$.

\paragraph{T2V implementation details.}~As aforementioned, our~\moviegen~model has a pre-trained patch size of $(\patchsizef, \patchsizeh, \patchsizew) = (1, 2, 2)$. Compared to our T2I experiments, we only change the $2$D convolutional layers used for tokenization with a $3$D convolution layer. When increasing the temporal patch size $\patchsizef$, we duplicate parameters along that dimension. When interpolating positional embeddings, we also do that along the temporal dimension. No additional changes are made. For evaluation, we use the prompts from \url{https://github.com/facebookresearch/MovieGenBench/blob/main/benchmark/MovieGenVideoBench.txt} to generate videos of length equal to $256$ frames. We evaluate according to VBench~\citep{huang2024vbench} and report the average over~\textit{Subject Consistency},~\textit{Background Consistency},~\textit{Temporal Flickering},~\textit{Motion Smoothness},~\textit{Dynamic Degree},~\textit{Aesthetic Quality},~\textit{Imaging Quality},~\textit{Temporal Style} and~\textit{Overall Consistency}.

\subsection{Human Evaluation Details}
\label{sec:human_evals}

We prompt humans, asking them to: ``Compare the two side-by-side images. Focus on visual appeal and flawlessness, considering factors like aesthetic appeal, clarity, composition, color harmony, lighting, and overall quality, then select 'left' if the left image is better, 'right' if the right image is better, or 'tie' if both are equally appealing or you have no preference.''. In total, we collected votes for the $4$ different settings presented in the paper and aggregated them across $200$ prompts. For each setting and each prompt, we ask $3$ people for votes. In cases where there are $3$ votes for each of 'left', 'right', and 'tie', we ask a fourth labeler to break the tie.

\section{Generated Samples}
\label{app:generations}

We provide more examples of generated samples.

\subsection{Text-Conditioned Image Generation}

We showcase more examples with varying amounts of compute in Fig.~\ref{fig:emu-examples-1},~\ref{fig:emu-examples-2} and~\ref{fig:emu-examples-3}. We further show more examples of how performance and diversity in image generation are preserved in Fig.~\ref{fig:emu-kitten} and~\ref{fig:emu-hippo}. Finally, we show examples of the effect of CFG scale $\guidancescale$ and reducing the overall number of FLOPs used to generate an image using our method, in Fig.~\ref{fig:grid_image}. For all the images seen in the paper with our~\emu~model, we use $50$ steps of the DDIM scheduler.


\begin{figure}[!h]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/emu/image_samples_1.pdf}
    \caption{More samples generated by our~\emu~model for varying amounts of compute.}
    \label{fig:emu-examples-1}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/emu/image_samples_2.pdf}
    \caption{More samples generated by our~\emu~model for varying amounts of compute.}
    \label{fig:emu-examples-2}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/emu/image_samples_3.pdf}
    \caption{More samples generated by our~\emu~model for varying amounts of compute.}
    \label{fig:emu-examples-3}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/emu/kitten.pdf}
    \caption{Samples for the prompt: "A playful kitten just waking up.". We showcase the image generated by the baseline and our flexible scheduler using only 53\% of FLOPs.}
    \label{fig:emu-kitten}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/emu/hippo.pdf}
    \caption{Samples for the prompt: "A baby hippo swimming in the river.". We showcase the image generated by the baseline and our flexible scheduler using only 53\% of FLOPs.}
    \label{fig:emu-hippo}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/emu/grid_image.pdf}
    \caption{Effect of CFG and total compute for the prompt: `The image shows a gigantic juicy burger placed on a white plate on a wooden table. The burger is composed of a large beef patty, crispy bacon, melted cheese, lettuce, tomato, onion, pickles, and a slice of red tomato, all sandwiched between a soft bun. The burger is so large that it occupies most of the plate, with some toppings falling out of the sides. The bun is slightly toasted, and the cheese is melted to perfection, giving off a savory aroma. The burger is garnished with a side of crispy fries and a refreshing glass of cola.`.}
    \label{fig:grid_image}
\end{figure}

\subsection{Text-Conditioned Video Generation}

We showcase more examples of video generation with our flexible model in Fig.~\ref{fig:moviegen-examples-1} and~\ref{fig:moviegen-examples-2}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/moviegen/video_examples_1.pdf}
    \caption{More samples generated by our~\moviegen~model, using $25.2$ \% compute compared to the pre-trained baseline.}
    \label{fig:moviegen-examples-1}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/moviegen/video_examples_2.pdf}
    \caption{More samples generated by our~\moviegen~model, using $25.2$ \% compute compared to the pre-trained baseline.}
    \label{fig:moviegen-examples-2}
\end{figure}

\subsection{Class-Conditioned \ImageNet~Generation}

We provide a more comprehensive comparison for generated images from the same original sample from $\pnoise$, using varying amounts of compute from our flexible model in Fig.~\ref{fig:sample-xl-256}. Note that for class-conditioned generation we do not use LoRAs, and images generated from the original baseline model may not be exactly the same. They do however have the same characteristics (FID score) as seen in Sec~\ref{sec:class_conditioned_experiments}. We also show samples of our flexible \textit{DiT-XL/2} model when using only $64$\% of the compute of the original model in Fig.~\ref{fig:sample-xl-256-31},~\ref{fig:sample-xl-256-83},~\ref{fig:sample-xl-256-94},~\ref{fig:sample-xl-256-192},~\ref{fig:sample-xl-256-245},~\ref{fig:sample-xl-256-948} and~\ref{fig:sample-xl-256-7}. All images are generated using $250$ DDPM steps and a CFG-scale equal to $4.0$. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.88\linewidth]{figures/xl-256/samples_xl_256.pdf}
    \caption{Sample for the \ImageNet~dataset comparing the baseline model and varying inference schedulers of our model, using different levels of compute.}
    \label{fig:sample-xl-256}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/xl-256/generated_images/31.pdf}
    \caption{Samples for the \ImageNet~class `tree frog, tree-frog' from our~\flexidit~model that uses only $46$\% of the compute compared to the baseline model.}
    \label{fig:sample-xl-256-31}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/xl-256/generated_images/83.pdf}
    \caption{Samples for the \ImageNet~class `prairie chicken, prairie grouse, prairie fowl' from our~\flexidit~model that uses only $46$\% of the compute compared to the baseline model.}
    \label{fig:sample-xl-256-83}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/xl-256/generated_images/94.pdf}
    \caption{Samples for the \ImageNet~class `hummingbird' from our~\flexidit~model that uses only $46$\% of the compute compared to the baseline model.}
    \label{fig:sample-xl-256-94}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/xl-256/generated_images/192.pdf}
    \caption{Samples for the \ImageNet~class `cairn, cairn terrier' from our~\flexidit~model that uses only $46$\% of the compute compared to the baseline model.}
    \label{fig:sample-xl-256-192}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/xl-256/generated_images/245.pdf}
    \caption{Samples for the \ImageNet~class `French bulldog' from our~\flexidit~model that uses only $46$\% of the compute compared to the baseline model.}
    \label{fig:sample-xl-256-245}
\end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.85\linewidth]{figures/xl-256/generated_images/486.pdf}
%     \caption{Samples for the \ImageNet~class `cello, violoncello'.}
%     \label{fig:sample-xl-256-486}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.85\linewidth]{figures/xl-256/generated_images/513.pdf}
%     \caption{Samples for the \ImageNet~class `cornet, horn, trumpet, trump'.}
%     \label{fig:sample-xl-256-513}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/xl-256/generated_images/770.pdf}
%     \caption{Samples for the \ImageNet~class `running shoe'.}
%     \label{fig:sample-xl-256-770}
% \end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/xl-256/generated_images/948.pdf}
    \caption{Samples for the \ImageNet~class `Granny Smith' from our~\flexidit~model that uses only $46$\% of the compute compared to the baseline model.}
    \label{fig:sample-xl-256-948}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/xl-256/generated_images/7.pdf}
    \caption{Samples for the \ImageNet~class `cock' from our~\flexidit~model that uses only $46$\% of the compute compared to the baseline model.}
    \label{fig:sample-xl-256-7}
\end{figure}


% \subsection{\red{removeme}}

% $$\tokens \uparrow$$

% $$\tokens = \frac{\fdim}{\patchsizef} \times \frac{\hdim}{\patchsizeh} \times \frac{\wdim}{\patchsizew}$$


% % $$\patchsize \uparrow \quad \Rightarrow \quad \tokens \downarrow\downarrow$$

% $$\text{for higher level global:}\, \patchsize \uparrow \quad \text{for low level details:}\, \patchsize \downarrow$$

% $\tokens = (\hdim / \patchsize) \times (\wdim / \patchsize)$

% % $$\times \depth$$

% % $$\depth$$
% $$\mathcal{O}(\depth (\tokens^2 \hiddensize + \tokens \hiddensize^2))$$

% $$\mathcal{O}(\depth (\tokens^{'2} \hiddensize + \tokens^{'} \hiddensize^2))$$


% $$\diffusionsteps$$
% $$\mathcal{O}(\diffusionsteps \,\,\text{NFE})$$

% \begin{align*}
% {\mu_{\theta_\diffusionsteps}}(\x_\diffusionsteps, \diffusionsteps) \\
% {\Sigma_{\theta_\diffusionsteps}}(\x_\diffusionsteps, \diffusionsteps)
% \end{align*}

% \begin{align*}
% {\mu_{\theta_{\diffusionsteps - 1}}}(\x_{\diffusionsteps - 1}, {\diffusionsteps - 1}) \\
% {\Sigma_{\theta_{\diffusionsteps - 1}}}(\x_{\diffusionsteps - 1}, {\diffusionsteps - 1})
% \end{align*}

% \begin{align*}
% {\mu_{\theta_{1}}}(\x_{1}, {1}) \\
% {\Sigma_{\theta_1}}(\x_{1}, {1})
% \end{align*}

