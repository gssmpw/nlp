\vspace{2mm}
\subsection{Text-conditioned Image Generation}
\label{sec:text-condition}

The universality of Transformers and the holistic view of different input sources as sequences of tokens implies that the generalization of the architecture to more modalities and a variety of different inputs is straightforward. This is also the case for DiTs, which have been already extended to accommodate text conditioning~\citep{chen2023pixart, betker2023improving}, video generation~\citep{ma2024latte} and speech synthesis~\citep{liu2024autoregressive}. We showcase how our framework can be applied out of the box to state-of-the-art text-to-image (T2I) model architectures. T2I DiTs have the same architecture as class-conditioned DiTs, with the exception that conditioning is imposed via cross-attention. 


% A potential limitation of our previous methodology in Section~\ref{sec:class_conditioned}, is that our finetuning involves training for all patch sizes, including the patch size $\patchsize$ that the pre-trained model was originally trained on. This is done, to avoid catastrophic forgetting~\citep{Kirkpatrick_2017}. Often in practice --- even for open weights models --- the training data used for pre-training are not available, however. Finetuning then may have undesired effects ranging from reduced capabilities~\citep{ibrahim2024simplescalablestrategiescontinually} to safety~\citep{qi2023finetuningalignedlanguagemodels}. Here, we demonstrate how finetuning with different patch sizes is possible, taking these considerations into account. 
% We adopt the same changes as before, but instead of training the DiT block parameters -- namely the self-attention and feed-forward layers -- we freeze the original weights and add trainable LoRAs~\citep{hu2021loralowrankadaptationlarge} \edgar{It's not clear why you didn't already use LoRAs in the class-conditional case}. LoRAs are activated for the new patch sizes and deactivated for the patch size of the pre-trained model. An overview of our approach is depicted in Fig.~\ref{fig:t2i_methodology}. Note that functional preservation is imposed for the pre-trained patch size, i.e. always using the powerful model leads to the same images as using the pre-trained model. During finetuning, we use the predictions of the powerful model as labels to distill knowledge~\citep{hinton2015distilling} to the new patch sizes, i.e. $$\| \etheta(\x_{t - 1} | \x_{t}; \patchsizeunderscore[powerful]) - \etheta(\x_{t - 1} | \x_{t}; \patchsizeunderscore[weak]) \|_2,$$which we find leads to improved performance and faster convergence~\citep{cho2019efficacy} as well as better alignment in the learned predictions. Note that pre-trained parameters are fixed, so $\etheta(\x_{t - 1} | \x_{t}; \patchsizeunderscore[powerful])$ has no trainable parameters. During inference, we can either choose to keep the new LoRA parameter un-merged, at the cost of some minor additional computational cost, or merge these weights into a copy of the original model parameters, with some additional memory cost. For more details we refer to Fig.~\ref{fig:t2i_methodology}. For all experiments, the additional parameters correspond to less than 7\% of the original model parameters.
% \subsection{Text-Conditioned Experiments}

We fine-tune T2I models, by introducing new parameters in the form of LoRAs. Specifically, we use a DiT following PIXART~\citep{chen2023pixart} --- we refer to this as~\pixart~--- generating $256 \times 256$ images and a 1.7B DiT based on EMU~\citep{dai2023emu} --- we refer to this as~\emu~--- generating $1024 \times 1024$ images. Implementation details are provided in App.~\ref{sec:details_text_to_image}, in short, we fine-tune both low and high-resolution target models with a pre-trained patch size of $2$, to also support a patch size of $4$. For~\pixart, we follow the inference scheduler protocol in~\citep{chen2023pixart}, and use the DDPM solver for $100$ steps. We point to the App.~\ref{sec:additional-experiments} for results with different solvers and number of steps (namely $20$ steps of DPM, $25$ steps of SA-solver, and a varying number of steps of the DDPM solver) showing that performance benefits are orthogonal to these choices. As before, we vary the percentage of denoising steps with the weak model for either the conditional or the guidance predictions.
% \begin{figure*}[!t]
%     \begin{minipage}{.61\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/pixart/pareto_steps_single.pdf}
%         \caption{We plot FID vs CLIP score when generating images with different CFG scales. (left) Overall pareto front when generating images with $100$ denoising steps.}
%         \label{fig:pixart_pareto_steps}
%     \end{minipage}%
%     \hfill
%     \begin{minipage}{0.37\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/emu/roofline-bfloat16.pdf}
%         \caption{GPU utilization for one denoising step, when propagating sequences with different overall number of tokens (corresponding to different patch sizes $\patchsize$).}
%         \label{fig:latency}
%     \end{minipage}
% \end{figure*}

\begin{figure*}[!h]
    \begin{minipage}{\textwidth}
        \begin{minipage}[b]{0.65\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/moviegen/video_examples_0.pdf}
        \end{minipage}
        \begin{minipage}[b]{0.33\textwidth}
        \includegraphics[width=1\linewidth]{figures/moviegen/vbench_flops.pdf}
        \end{minipage}
    \end{minipage}
    \caption{\textbf{Left:} Samples from our flexible~\moviegen~model using $25.2$\% of compute compared to the pre-trained baseline. \textbf{Right:} We perform a varying number of steps of the denoising process with a weak model, using either our spatial or our temporal weak model. Both weak models lead to significant compute savings with \emph{little to no} degradation in performance.}
    \label{fig:moviegen-experiments}
\end{figure*}


\paragraph{CLIP-vs-FID.}~We report FID and CLIP score alignment for captions from the \textit{MS COCO} dataset with varying CFG scale values in Fig.~\ref{fig:t2i-experiments} (left). As before, we notice that lower compute versions of our model require a higher CFG scale to generate images that match similar FID vs CLIP score values of the baseline (full compute) model. We show that for a fixed CFG scale of the baseline model --- we chose $\guidancescale = 4.5$ for the~\pixart~model as proposed in~\cite{chen2023pixart} and $\guidancescale = 6.0$ for our~\emu~model --- we can match the attained performance for less than 60\% of the original compute. Detailed scores are given in Fig.~\ref{fig:t2i-experiments} (middle). 
% \begin{table}[!h]
% % \vspace{-6mm}
% {\small
% % \caption{Additional benchmarks.}\label{tab:additional-benchmarks}
% \begin{tabular}{cccc}
% \toprule  
% Method & FID $\downarrow$ & CLIP $\uparrow$ & VQAScore $\uparrow$ \\
% \midrule
% \pixart~(100 \%) & 14.75 & 25.60 & 63.29 \\  
% \pixart~( 86 \%) & 14.72 & 25.62  & 63.20 \\  
% \pixart~( 72 \%) & 14.77 & 25.63 & 64.20 \\  
% \pixart~( 58 \%) & 14.71 & 25.58 & 63.26 \\  
% \midrule
% EMU (100 \%) & 26.00 & 26.05 & 70.17 \\  
% EMU (84.3 \%) & 25.90 & 26.06 & 70.09 \\  
% EMU (68.6 \%) & 26.00 & 26.08 & 70.67 \\  
% EMU (52.9 \%) & 26.18 & 26.04 & 69.89\\  
% \bottomrule
% \end{tabular}
% }
% \end{table} 
\paragraph{Additional benchmarks.}~We perform additional evaluations on the final generated images. We follow~\citep{lin2024evaluating} and present text alignment for visual question-answering (VQA). We generate images based on the captions of \textit{DrawBench}, \textit{TIFA160}, \textit{Pick-a-Pic}, \textit{Winoground} and report average VQA scores (we point to App.~\ref{sec:additional-experiments} for detailed results). We compare the baseline model (same CFG scales as before), against dynamic inference with our flexible models. Consistent with our previous findings, results indicate that performing a few denoising steps with our weak model generally generates high-quality images. We can again save up to $40$\% of compute without a performance drop. For high-resolution images generated from our~\emu~model, we also embark on a human study, as shown in Fig.~\ref{fig:t2i-experiments} (right). There we validate that human annotators prefer images from our dynamic scheduler instead of using similar compute, but this time uniformly allocated.

% \begin{minipage}{\linewidth}
%     \begin{minipage}[b]{\linewidth}
%     % \begin{figure}[!h]
%         \centering
%         \includegraphics[width=1.0\linewidth]{figures/emu/human_evals.png}
%         \captionof{figure}{\red{Human Evals, update with video}}
%         \label{fig:human-evals}
%     % \end{figure}    
% \end{minipage}
% \end{minipage}
% \begin{table}[]
%     \centering
%     \begin{tabular}{|c|cc|ccc|}
%         Solver & Steps & Steps & Win (\%) & Tie (\%) & 
%          &  \\
%          & 
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

% '84': [(36, 40, 24), (12, 75, 13)],
% '69': [(39, 39, 22), (11, 79, 10)],
% '53': [(44, 46, 10), (8, 83, 9)],

% \red{make performance gap even smaller.}