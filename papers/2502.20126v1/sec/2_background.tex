\section{Background}
\label{sec:background}

\begin{wrapfigure}{r}{0.18\textwidth}
  \begin{center}
    \vspace{-8mm}
    \hspace{-8mm}
    \includegraphics[width=0.22\textwidth]{figures/ps_schematic_img.pdf}
    \caption{Tokenizing images into patches.}
    \vspace{-6mm}
  \end{center}
\end{wrapfigure}
For simplicity, we limit the discussion here to images, and detail later changes due to different modalities.
DiTs use a Transformer encoder to process image patches as tokens. Hereafter, we refer as \emph{tokenization} to the process of converting a (latent) image into a series of tokens and as \emph{de-tokenization} to the opposite process, of transforming a series of tokens back into an image. Given an image of size $\hdim \times \wdim$ and a chosen patch size\footnote{In reality, different patch sizes can be considered along the height and the width dimensions. We will however refrain from doing that.} $\patchsize$, an input image is cropped into non-overlapping patches of dimensions $\R^{\patchsize \times \patchsize \times \inchannels}$, where $\inchannels$ denotes the number of channels of the input image. The total number of tokens is then equal to $\tokens = (\hdim / \patchsize) \times (\wdim / \patchsize)$. This (flattened) sequence of patches is then projected using a linear layer $\embedding$ with weights $\R^{\patchsize * \patchsize * \inchannels \times \hiddensize}$ and potential biases $\R^\hiddensize$. This tokenization process is equivalent to performing a $2$D convolution, where the kernel size and stride are both equal to $\patchsize \times \patchsize$. The embedded $\tokens$ tokens are then processed using $\depth$ Transformer encoder layers. The output tokens of dimension $\R^{\tokens \times \hiddensize}$ are projected back to the image space with a linear de-embedding layer $\deembedding$ with weights $\R^{\hiddensize \times \outchannels * \patchsize * \patchsize}$ and potential biases $\R^{\outchannels * \patchsize * \patchsize}$. Here $\outchannels$ denotes the number of output channels, typically $\outchannels = 2 \inchannels$ if the prediction includes the variance, else $\outchannels = \inchannels$. DiTs adhere to the scaling properties of Transformers, as demonstrated in various other applications~\citep{kaplan2020scaling,hoffmann2022training,henighan2020scaling,zhai2022scaling, bachmann2024scaling}. 

DiTs are an increasingly popular alternative to convolutional networks for denoising corrupted images during generation, i.e. modeling $\ptheta (\x_{t - 1} | \x_{t})$. Diffusion defines two Markov chains, the forward and the backward process. During the forward process, Gaussian noise\footnote{Although we focus on Gaussian noise here, other corruptions apart from Gaussian noise have also been analysed~\citep{bansal2024cold, nachmani2021denoising, daras2022soft}.} is added to samples of the real distribution $\x_0 \sim \q(\x_0)$~\citep{ho2020denoising}:

\begin{multline}
    \label{eq:noise_images}
    \q (\x_{1:\diffusionsteps} | \x_0) = \prod_{t=1}^{\diffusionsteps} \q(\x_{t} | \x_{t - 1}), \quad \text{where} \\ \quad \q(\x_{t} | \x_{t - 1}) = \mathcal{N}(\x_{t - 1}; \sqrt{1 - \beta_t}\x_{t - 1}, \beta_t \mathbf{I}).
\end{multline}
In the backward process, samples are drawn from a Gaussian noise distribution $\p(\x_\diffusionsteps) = \pnoise = \mathcal{N}(\mathbf{0}, \mathbf{I})$ and then gradually denoised, using Tweedieâ€™s Formula ~\citep{efron2011tweedie}:

\begin{multline} 
\label{eq:denoise_images}
\ptheta(x_{\diffusionsteps:0}) = \p(\x_\diffusionsteps) \prod_{t=\diffusionsteps}^1 \ptheta(\x_{t - 1} | \x_{t}), \quad \text{where} \\ \quad \ptheta(\x_{t - 1} | \x_{t}) = \mathcal{N}(\x_{t}; \mutheta(\x_t, t), \Sigmatheta(\x_t, t)).
\end{multline}
Typically, a \emph{single} model is used to model the prediction $\ptheta(\x_{t - 1} | \x_{t})$ for every $t$.
% Although, forward diffusion steps have similar functional form, adding Gaussian noise of difference scale, the effect of the noise added have fundamentally different effects on the original images~\citep{bansal2024cold}. 