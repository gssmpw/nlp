\section{Experiments}

For clarity, we present efficiency gains with respect to FLOPs and point to Section~\ref{sec:latency} for a detailed analysis of the relationship between FLOPs and latency.

\subsection{Class-Conditioned Image Generation}
\label{sec:class_conditioned_experiments}

We fine-tune models (\textit{DiT-XL/2}) on~\ImageNet, using the same setup as in~\citep{peebles2023scalablediffusionmodelstransformers}. Since training data is publicly available, we fine-tune pre-trained models using the same parameters for all sequences, without the use of LoRAs, as described in Sec.~\ref{sec:single_forward_pass}. During training, we randomly noise images according to~\cref{eq:noise_images}, and learn to denoise using one of the available patch sizes. We primarily report FID and point to App.~\ref{sec:additional-experiments} for more experiments and different metrics. In practice, we use a pre-trained model with a patch size of $2$ (\emph{powerful}), that we fine-tune to also process images with a patch size of $4$ (\emph{weak}). Since we are fine-tuning the powerful model, we can also ``teach'' it how to correct specific mistakes made by the weak model, accumulated in the backward process during the first $\diffusionstepsweak$ steps. We provide more details in App.~\ref{sec:exposure_bias} on how to reduce this exposure bias~\citep{ning2023elucidating, li2023alleviating}. Unless otherwise mentioned, reported metrics are computed by generating images at resolution $256 \times 256$, using $250$ steps of the DDPM scheduler~\citep{ho2020denoising, peebles2023scalablediffusionmodelstransformers}.


\paragraph{Compute gains.}~We generate images with our~\flexidit~and the proposed inference scheduler,  varying the amount of compute by adjusting the number of initial denoising steps $\diffusionstepsweak$ performed with the weak model. For each level of compute, we report the FID of the generated images in Fig.~\ref{fig:xl-experiments} (left and middle). In general, performing a few steps with the weak model (60-100\% of baseline compute) leads to \emph{no drop} in performance. Saving even more compute is possible, albeit at the cost of a minor drop in the quality of the generated images. When using only the powerful mode of our~\flexidit, we get the same performance --- $\textit{FID-}50\textit{k} = 2.25$ --- as the pre-trained \textit{DiT-XL/2} model ---  $\textit{FID-}50\textit{k} = 2.27$. Thus, \textit{fine-tuning for more patch sizes does not reduce the capacity of the model with respect to the pre-trained one}. We also show that other inference schedulers, such as starting with the powerful model and switching to the weak model, lead to worse results (appendix Fig.~\ref{fig:opposite-scheduler}), validating our intuition.
\begin{figure*}[!h]
    \begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.29\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/pixart/pareto_steps_single_advanced.pdf}
    \end{minipage}
    \hfill
    % {{
    % \vspace{-80mm}
    \begin{minipage}[b]{0.38\textwidth}
        {\scriptsize
        \begin{tabular}{cccc}
        \toprule  
        Model (compute \%) & \textit{FID} $\downarrow$ & \textit{CLIP} $\uparrow$ & \textit{VQAScore} $\uparrow$ \\
        \midrule
        \pixart~($100$ \%) & $14.75$ & $25.60$ & $63.29$ \\  
        \pixart~( $86$ \%) & $14.72$ & $25.62$  & $63.30$ \\  
        \pixart~( $72$ \%) & $14.77$ & \underline{$25.63$} & \underline{$63.40$} \\  
        \pixart~( $58$ \%) & \underline{$14.71$} & $25.58$ & $63.26$ \\  
        \midrule
        \emu~($100$ \%) & \underline{$26.00$} & $26.05$ & $70.17$ \\  
        \emu~( $84$ \%) & $25.96$ & $26.06$ & $70.19$ \\  
        \emu~( $69$ \%) & \underline{$26.00$} & \underline{$26.08$} & \underline{$70.37$} \\  
        \emu~( $53$ \%) & $26.10$ & $26.07$ & $70.09$\\  
        \bottomrule
        \end{tabular}
        \par\vspace{8pt}
        }
    \end{minipage}
    % \vspace{80mm}
    % }}%
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
        % {{\includegraphics[width=\textwidth]{figures/emu/roofline-bfloat16.pdf} }}%
        {\scriptsize
        \begin{tabular}{|cc|ccc|}
        \toprule
        \multicolumn{2}{|c|}{Steps $_\textit{(Powerful/Weak)}$} & \multicolumn{3}{c|}{Votes (in \%)} \\
        Ours & Baseline  & Win & Tie & Lose \\
        \midrule
        50 $_{(40, 10)}$ & 42 $_{(42, 0)}$ & $33.5$ & $42.5$ & $24.0$ \\
        50 $_{(30, 20)}$ & 34 $_{(34, 0)}$ & $35.5$ & $41.5$ & $23.0$ \\
        50 $_{(20, 30)}$ & 26 $_{(26, 0)}$ & $35.5$ & $44.0$ & $20.5$ \\
        50 $_{(10, 40)}$ & 18 $_{(18, 0)}$ & $43.0$ & $41.5$ & $15.5$ \\
        \midrule
        \multicolumn{5}{|p{5.00cm}|}{\tiny{We asked $8$ annotators to express preferences between pairs of images for a given prompt. We generate images for $200$ prompts and collect $3$ different votes per each pair of images, collectively $2400$ votes.}} \\
        \bottomrule
        \end{tabular}
        \par\vspace{7pt}
        }
    \end{minipage}
    \end{minipage}
    \vspace{-3mm}
    \caption{\textbf{Left:} We plot FID vs CLIP score for images generated with different CFG scales using the~\pixart~model (we refer to the appendix for results regarding our~\emu~model). The red line represents images generated with varying CFG scales using only the (powerful) target model. By employing our dynamic scheduler, we can match image quality in terms of both FID and text alignment while significantly reducing compute requirements. \textbf{Middle:} Our flexible models can match the baseline (for a fixed pre-defined guidance scale $\guidancescale$) across benchmarks, with significantly less compute. \textbf{Right:} Human study results show votes indicating a win, tie, or loss for our method compared to a baseline, which corresponds to running the pre-trained model (only the powerful model) for fewer steps. Comparisons are between~\emu~inference modes that require approximately equal FLOPs and time.}%
    \label{fig:t2i-experiments}
\end{figure*}

\paragraph{Relation between $\diffusionsteps$ and $\diffusionstepsweak$.}~Naturally, the question arises whether doing fewer overall diffusion steps $\diffusionsteps$ leads to the same efficiency improvements compared to performing more steps with the weak model. After applying the DDPM scheduler for a different number of overall diffusion steps $\diffusionsteps$, we plot in Fig.~\ref{fig:xl-experiments} (right) the efficiency gains across them. Results indicate that gains from performing weak steps are orthogonal to performing fewer overall diffusion steps. In other words, \textit{more steps are required to achieve better image fidelity, but performing some of the initial steps with our weak model is sufficient to achieve the targeted fidelity}. In App.~\ref{sec:intuition}, we provide further insights on how the predictions of the weak model closely align with the ones of the powerful model. This similarity supports parameter sharing, which not only reduces resource requirements during inference but also enables faster convergence during fine-tuning.
% \begin{figure}[!h]
%   \begin{center}
%     \vspace{-2mm}
%     \includegraphics[width=0.34\textwidth]{figures/xl-256/compute_optimality_FID.pdf}
%     \vspace{-4mm}
%   \end{center}
% \end{figure}
% \red{plot here is suspicious since our full model is below the pretrained model. Point also to appendix that is plotted with respect to our enhanced baseline}.


% Experiment low points: flops, mins
% ([8545545093120.0,
%   13474592194560.0,
%   15266972958720.0,
%   17059353722880.0,
%   18851734487040.0,
%   20644115251200.0,
%   22436496015360.0,
%   24228876779520.0,
%   26021257543680.0,
%   29606019072000.0],
%  [3.2358668907954664,
%   2.643835006591348,
%   2.452926992025925,
%   2.305513896544471,
%   2.2505170003093893,
%   2.2272451835195737,
%   2.2388832798805423,
%   2.246977693291842,
%   2.25072073383119,
%   2.254351698064954])