\section{Introduction}
\label{sec:intro}

\begin{figure*}[!ht]
    \centering
    % \subfloat[\centering]
    \begin{minipage}{0.66\textwidth}
        {{\includegraphics[width=1\textwidth]{figures/motivation_diffusion.pdf} }}%    
        % \vspace{1mm}
    \end{minipage}
    \begin{minipage}{0.32\textwidth}
        \vspace{4mm}
        {{\includegraphics[width=1\textwidth]{figures/motivation_distance.pdf} }}%
    \end{minipage}
    \caption{Diffusion can be viewed as spectral autoregression~\citep{dieleman2024spectral}. \textbf{Left:} Diffusion and its effect on the spatial frequency of images. \textbf{Right:} To investigate the role of different frequency components in image generation, we apply a low or high pass filter to a single diffusion step update (while keeping all other updates unchanged). With all other sources of randomness fixed, we compare the generated samples with and without filtering using LPIPS~\citep{zhang2018unreasonable}, $L_2$ distance of the pixels, SSIM~\citep{wang2004image} and DreamSim~\citep{fu2023dreamsim}. Notably, the influence of low and high pass filters varies depending on whether they are applied early or late in the denoising process.}%
    \label{fig:motivation}
    \vspace{1mm}
\end{figure*}

Diffusion models~\citep{sohl2015deep} have recently become the core building block of major improvements in image generation~\citep{dhariwal2021diffusion, videoworldsimulators2024, sd, flux, yang2024cogvideox}. These models gradually denoise a random sample $\x_t$, drawn typically from  $\pnoise = \mathcal{N}(\mathbf{0}, \mathbf{I})$, by iteratively calling a neural network trained to reverse a pre-defined noise corruption process $\ptheta (\x_{t - 1} | \x_{t})$. This process enables the generation of samples from the desired distribution $\pdata$.%These models start from random samples, typically a Gaussian distribution $\pnoise = \mathcal{N}(\mathbf{0}, \mathbf{I})$, which they refine for a number of steps $\diffusionsteps$ via a neural network $\ptheta (\x_{t - 1} | \x_{t})$ to retrieve the target distribution.
The remarkable performance of these models is closely tied to the amount of computational resources invested in them, as evidenced by established scaling laws \citep{kaplan2020scaling}. The Transformer architecture~\citep{vaswani2017attention} has proven to be highly scalable across various modalities, leading to its adoption in diffusion models, in the form of the recent Diffusion Transformer~\citep{peebles2023scalablediffusionmodelstransformers}. In DiTs, the denoising process $\ptheta (\x_{t - 1} | \x_{t})$ is parametrized using Transformer blocks instead of traditional convolutional layers.
As popularized in Vision Transformers~\citep{dosovitskiy2020image}, (latent) images of dimension $\hdim \times \wdim$ are divided into patches of size $\patchsize \times \patchsize$, which serve as input tokens that are transformed via a series of attention and feed-forward layers. The use of DiTs offers two key advantages: (i) a unified architecture and input processing framework that facilitates multimodal applications and generalization to other domains, such as audio and video; and (ii) exceptional scalability due to their signal propagation characteristics and efficient training on modern hardware.


The computational complexity of a DiT with hidden dimension $\hiddensize$ and depth $\depth$ is $\mathcal{O}(\depth \tokens \hiddensize^2 \ + \depth \tokens^2 \hiddensize)$, where $\tokens$ corresponds to the total number of tokens\footnote{For high-resolution image generation --- even when diffusion is performed in a latent space --- the second term $\mathcal{O}(\depth \tokens^2 \hiddensize)$ that corresponds to the attention operations, can quickly become the main bottleneck.}. Given $\diffusionsteps$ steps of the diffusion process, function calls to the same monolithic DiT model are repeated for all $\diffusionsteps$ steps, and the total amount of compute is thus \emph{uniformly} allocated. However, image generation via diffusion exhibits distinct, non-uniform, and temporally --- with respect to the noise process --- varying characteristics. Consistent with intuition, prior work has observed that high-level image features tend to emerge at early generative diffusion steps (i.e., large $t$'s). In contrast, later steps refine and progressively generate high-frequency and detailed visual features~\cite{castillo2023adaptive}. We illustrate further differences during the denoising generation in Fig.~\ref{fig:motivation}. Concisely, \textit{different denoising steps have profoundly different influence on high and low-level features of the resulting images}.
% (and point the interested reader to~\citep{torralba2003statistics} and references thereafter for general information and statistics of images). 


In theory, different denoising methods and models can be employed for each step $t$, i.e. one can use separate parameters $\thetat$ to learn $\pthetat (\x_{t - 1} | \x_{t})$. While this notion has been previously explored in the literature, prior works propose computationally intensive solutions, such as training separate expert denoisers for different $t$'s~\citep{balaji2022ediff} or using model cascades~\citep{stablecascades}. These approaches face two significant limitations: (i) they require multiple models to be managed during inference, increasing memory demands. This can quickly become a major issue, especially as current trends favor the use of larger and larger models. Moreover, (ii) using separate models restricts the opportunity for knowledge sharing across steps. Although one can treat denoising steps independently, the denoising process itself retains certain shared characteristics across steps. This inherent smoothness implies that separate models would need to learn these shared properties individually. In this work, we address these challenges and instead propose to perform different denoising steps with varying levels of compute using a single model. 

\vspace{4mm}

In summary, our contributions are the following:
\begin{itemize}
    \item We present a simple framework that \emph{flexifies} DiT models, allowing them to convert samples into different sequences of tokens by adjusting the patch size. Processing samples as different sequences enables us to control the compute budget being used for each denoising step.
    \item By leveraging specific image characteristics at different denoising steps, we demonstrate that strategically allocating less compute to certain steps can yield significant computational savings (over 40\%) \emph{without compromising} the quality of generated samples for both class-conditioned and text-conditioned image generation. We also show how denoising based on a larger patch size can serve as a more effective guidance signal.
    \item  We illustrate the versatility of our framework by extending it to other modalities. For video generation, we achieve substantial computational savings (up to $75$\%) \emph{with no considerable} drop in performance or conceptual differences in the generated samples.
\end{itemize}
