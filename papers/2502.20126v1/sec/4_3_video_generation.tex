\subsection{Text-Conditioned Video Generation}
\label{sec:video_generation}
Finally, we also explore other modalities, such as video generation. Text-to-video (T2V) generation typically follows the same setup as T2I~\citep{polyak2024movie, ma2024latte}. Given a video of dimensions $\R^{\fdim \times \hdim \times \wdim \times \inchannels}$, where $\fdim$ is the temporal component corresponding to the number of frames, and a chosen patch size $(\patchsizef, \patchsizeh, \patchsizew)$ for each of the dimensions, the input (latent) video is cropped into non-overlapping spatial-temporal patches of dimensions $\R^{\patchsizef \times \patchsizeh \times \patchsizew \times \inchannels}$. Patches are then embedded using now a $3$D convolutional layer into tokens, subsequently transformed by the DiT blocks where every token attends to every other token in the sequence, and finally projected back to patches with a linear layer. 

We perform the same changes as for our T2I models in Sec.~\ref{sec:different_forward_pass}, adding LoRAs for new patch sizes. We initialize tokenization and de-tokenization layers as before, and when the temporal patch size $\patchsizef$ is increased, we duplicate weights along the temporal dimension. We fine-tune a 4.9B video model as the one in~\citep{polyak2024movie} --- referred to as~\moviegen~--- and train with distillation as before. We generate videos of resolution $(\fdim \times \hdim \times \wdim) = (256, 384, 704)$ using $250$ steps of the DDPM scheduler as in~\citep{polyak2024movie}. Diffusion is performed in a latent space that down-samples each dimension $\fdim, \hdim, \wdim$ by $8$. More dimensions of the latent space now offer more possibilities in terms of determining the characteristics of our weak model. For our experiments, we fine-tune a pre-trained model with patch size $(\patchsizef, \patchsizeh, \patchsizew) = (1, 2, 2)$ to also support a patch size $(2, 2, 2)$, we denote this weak model as `temporal', and $(1, 4, 4)$, denoted as `spatial'. We then use either one of these modes as a weak model during inference. Both modes could also be used iteratively for the generation of a single sample, which we leave for future work.

By adjusting the number of steps performed with our weak model, we can again control the overall compute required per generated sample. We use \textit{VBench}~\citep{huang2024vbench} to evaluate the quality of the generated videos and report results in Fig.~\ref{fig:moviegen-experiments}. In this case, we can save up to $75$\% of compute without a significant drop in performance. Recent training-free methods~\citep{liu2024faster, zhao2024real, kahatapitiya2024adaptive}, while appealing due to their lack of training requirements, achieve significantly lower compute savings before performance begins to degrade noticeably. This demonstrates that \textit{allowing the model to learn optimized compute allocation is more effective than relying on predefined rules for inference efficiency}. Additional comparisons to previous work are provided in the appendix.


% final_results = {
%     "2_2_2": [
%         (1.0, 0.7044),
%         (0.8714, 0.7042, True, 0.0008),
%         (0.7428, 0.7037, True, 0.0008),
%         (0.6142, 0.7045, True, 0.0008),
%         (0.4857, 0.7039, True, 0.0008),
%         (0.3571, 0.7038, True, 0.0008)
%     ],
%     "4_4_1": [
%         (1.0, 0.7044),
%         (0.8266, 0.7039, True, -0.0015),
%         (0.6533, 0.7038, True, -0.0015),
%         (0.4800, 0.7037, True, -0.0015),
%         (0.3933, 0.7033, True, -0.0015),
%         (0.2200, 0.7021, True, 0.0008),
%         (0.1333, 0.6865, True, -0.0015)
%     ]
% }

% prev_flop = {
%     "2_2_2": 1/2.8,
%     "4_4_1": 1/7.5,
% }

% new_flop = {
%     "2_2_2": 1/2.5564512,
%     "4_4_1": 1/ 5.92192983,
% }