\section{Flexible Diffusion Transformers}
\label{sec:class_conditioned}

\begin{figure*}[!ht]
    \centering
    \hfill
    \includegraphics[width=0.61\textwidth]{figures/flexible_dit_methodology_new.png}
    \hfill
    % \includegraphics[width=0.35\textwidth]{figures/caching_wide.pdf}
    \includegraphics[width=0.35\textwidth]{figures/caching_wide_new.pdf}
    \hfill
    \caption{\textbf{Left:} We flexify DiTs by allowing them to process images with more patch sizes, by changing the lightweight embedding and de-embedding layers. We showcase this for a class-conditioned~\ImageNet~model. \textbf{Right:} We plot the difference in predictions between a weak and a powerful model. For the first denoising steps, differences are small, and thus using the weak model there allows accelerated generation without performance degradation.}
    \label{fig:methodology}
\end{figure*}


% These changes are minimal and only affect the image processing component of DiTs.
% We start our discussion with class-conditioned models -- as originally proposed in~\citep{peebles2023scalablediffusionmodelstransformers} -- and later show in Sec.~\ref{sec:text-condition} how these changes can be easily incorporated into text-conditioned models, or other types of conditioning, without additional complexity. 
% An overview of our approach is given in Fig.~\ref{fig:methodology}. As in any Transformer, the number of tokens determines the compute expended for each model evaluation. We propose to control and adequately increase or decrease the compute invested depending on the denoising step $t$, by simply tokenizing input images differently.
We \emph{flexify} DiTs, via small architectural changes, that allow them to process images as different length sequences, by adjusting the patch size $\patchsize$ used in tokenization. 
Flexible tokenization of images has been utilized before for single-step inference applications~\citep{beyer2023flexivit} and to accelerate training~\citep{anagnostidisnavigating}. We instead propose to use different patch sizes at different steps in the denoising process of the same image. This is based on the following intuition:
\begin{center}
    \textit{Early steps focus on low-frequency details, which can be performed with bigger patch sizes at the same quality.}
\end{center}
Changing the patch size $\patchsize$ directly affects the total number of tokens (recall that $\tokens = (\hdim / \patchsize) \times (\wdim / \patchsize)$) and thus the overall compute required for a function evaluation. Hence, by leveraging bigger patch sizes only for early steps, we can reduce the overall generation time while maintaining both low and high-frequency details in the produced images.
% thus limiting any performance degradation while reducing compute. 

We obtain a flexible model --- coined {\flexidit} --- by modifying and fine-tuning a pre-trained DiT, which enables it to process and understand images with new patch sizes. In our experiments we focus on efficiency, so newly added patch sizes are always larger than the one of the pre-trained model, leading to fewer tokens and thus a smaller computational footprint. The single~\flexidit~model can be \emph{instantiated} in different modes depending on the selected patch size. We will refer to instances of the model that use the original and smaller patch size $\patchsizeunderscore[powerful]$ as \emph{powerful}, compared to using a \emph{weak} model with a larger patch size $\patchsizeunderscore[weak]$. To simplify the discussion, we largely ignore how additional conditioning may be applied for now, and instead refer to specific implementation details in the experiments section.

DiTs --- as any Transformer --- can process sequences of any arbitrary length $\tokens$. Fundamentally, we just need to modify (i) how tokenization and de-tokenization are performed to ensure that the input and output representation space stay unchanged and (ii) ensure that the model can interpret these different length sequences. In the following, we discuss and outline two different ways that {\flexidit}s can be derived, based on whether the forward pass for the pre-trained model (we refer to this as the \emph{target} model) is preserved exactly or not. In both cases, we emphasize that any additional fine-tuning and new parameters are minimal. In all our experiments, training is stable, and no special tricks are necessary, while the total compute for fine-tuning is less than $5$\% of the original pre-training compute.

\subsection{Shared Parameters for all Sequences}
\label{sec:single_forward_pass}

When the original training data is available, it becomes possible to fine-tune the pre-trained model while preserving its performance, along with its existing abilities, biases, and potential safety features. We demonstrate that this approach enables processing images with varying patch sizes by introducing only minimal additional trainable parameters, as shown in Fig.~\ref{fig:methodology} (left). Below, we detail the specific components of the architecture that require adaptation.
% There are situations, where we have complete access to the target model and the data that it was originally trained on. In such cases, one can finetune the pre-trained model while ensuring that performance and overall abilities, biases and safety features are preserved. We will show that doing so allows to process images at different patch sizes with minuscule additional trainable parameters.} An overview of our method in this case is provided in Fig.~\ref{fig:methodology} (left). In the following we will outline which parts of the architecture need to be adapted.

\paragraph{Tokenization:}~We introduce a new embedding layer $\flexembedding$, for some underlying patch size $\patchsizeexp[\prime]$ with new weights $\flexweightembedding \in \R^{\patchsizeexp[\prime] * \patchsizeexp[\prime] * \inchannels \times \hiddensize}$ and biases $\flexbiasembedding \in \R^\hiddensize$. Then, when instantiating a~\flexidit~with a patch size $\patchsizecurrent$, we project these weights with a fixed projection matrix $\projectembedding \in \R^{\patchsizecurrent * \patchsizecurrent \times \patchsizeexp[\prime] * \patchsizeexp[\prime]}$ to the desired shape, and perform the $2$D convolution with kernel size and stride equal to $\patchsizecurrent \times \patchsizecurrent$. We use as $\projectembedding$ the pseudo-inverse of the bi-linear interpolation projection, as this leads to better norm preservation of the output~\citep{beyer2023flexivit}. We initialize the weights based on the pre-trained parameters and the pre-trained patch size as $\flexweightembedding = \projectembedding^\dagger \weightembedding$\footnote{Here $\dagger$ denotes the pseudo-inverse. All projection matrices $\project$ multiply each channel separately.} and $\flexbiasembedding = \biasembedding$, preserving the functional form of the model for the pre-trained patch size. When adding positional encodings, we identify for each patch its pixel coordinates in the original image~\citep{xie2023difffit, chen2023pixart}. We additionally introduce a patch size embedding for each of the patch sizes used by the model, which we add to every token in the sequence, and patch size dependent layer-normalization layers. These layers help with signal propagation~\citep{he2015delving, xiong2020layer, noci2022signal} by preserving the norms of the activations and let the model recover its expressivity~\citep{wimbauer2024cache, ioffe2015batch}.

\paragraph{De-tokenization:}~We similarly adapt the de-embedding layer $\deembedding$. For an underlying patch size $\patchsizeexp[\prime]$, we define a new layer $\flexdeembedding$ with weights $\flexweightdeembedding \in \R^{\hiddensize \times \outchannels * \patchsizeexp[\prime] * \patchsizeexp[\prime]}$ and biases $\flexbiasdeembedding \in \R^{\outchannels * \patchsizeexp[\prime] * \patchsizeexp[\prime]}$. Depending on the current patch size $\patchsizecurrent$ in the neural network evaluation, we project with a fixed projection matrix $\projectdeembedding \in \R^{\patchsizeexp[\prime] * \patchsizeexp[\prime] \times \patchsizecurrent * \patchsizecurrent}$ to $\flexweightdeembedding \projectdeembedding$ and $\flexbiasdeembedding \projectdeembedding$. Again, we use as $\projectdeembedding$ the pseudo-inverse of the bi-linear interpolation, now with flipped dimensions. We initialize the new parameters as $\flexweightdeembedding = \weightdeembedding \projectdeembedding^\dagger$ and $\flexbiasdeembedding = \biasdeembedding \projectdeembedding^\dagger$. 
In total, less than $0.005$ \% of auxiliary parameters are introduced to attain a~\flexidit~for the models tested in this case. 
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/t2i_methodology.pdf}
    \caption{We preserve the functional form of the target model for the pre-trained patch size and add new trainable parameters (LoRAs) for each additional patch size we want to fine-tune the model to operate with. We showcase this for a text-to-image/video model that uses cross-attention for text conditioning. We find that freezing cross-attention layers without any additional LoRAs works the best. During inference, we can either keep the LoRAs unmerged (\textit{Inference with LoRAs}) leading to a slight FLOPs increase that depends on the LoRAs' dimensions, or create different copies of the model for each patch size, by merging the LoRAs (\textit{Inference without LoRAs}). The latter leads to additional memory requirements. FLOPs and parameter numbers on the right correspond to our flexible T2I~\emu~model.}%
    \label{fig:t2i_methodology}
\end{figure*}


\subsection{Different LoRAs for each Sequence}
\label{sec:different_forward_pass}

In practice, however, pre-training often requires extensive computational resources and may span multiple stages using various datasets, which may not always be accessible, even for models with open weights. In such cases, fine-tuning can have unintended effects, potentially diminishing model capabilities~\citep{ibrahim2024simplescalablestrategiescontinually} or compromising safety guarantees~\citep{qi2023finetuningalignedlanguagemodels}. For such situations, where it is essential to preserve the original forward pass of a pre-trained model, we demonstrate a method that enables fine-tuning across different patch sizes with minimal additional compute and a small supplementary dataset. Fig.~\ref{fig:t2i_methodology} illustrates our approach.
% \red{Often in practice, however, pre-training involves large amounts of computational resources with potentially multiple stages over different data, which might not be available, even for open weights models. Finetuning then may have undesired effects ranging from reduced capabilities~\citep{ibrahim2024simplescalablestrategiescontinually} to loss of safety guarantees~\citep{qi2023finetuningalignedlanguagemodels}.} For cases where preserving the forward pass of a pre-trained model is desirable, we demonstrate how finetuning with different patch sizes is possible, with minimal additional compute and a small amount of additional data. Our method is illustrated in Fig.~\ref{fig:t2i_methodology}.

% A potential limitation of our previous methodology in Section~\ref{sec:class_conditioned}, is that our finetuning involves training for all patch sizes, including the patch size $\patchsize$ that the pre-trained model was originally trained on. This is done, to avoid catastrophic forgetting~\citep{Kirkpatrick_2017}. Often in practice --- even for open weights models --- the training data used for pre-training are not favailable, however. Finetuning then may have undesired effects ranging from reduced capabilities~\citep{ibrahim2024simplescalablestrategiescontinually} to safety~\citep{qi2023finetuningalignedlanguagemodels}. Here, we demonstrate how finetuning with different patch sizes is possible, taking these considerations into account. 
We perform similar changes to the model, but instead of training the DiT block parameters, we freeze the original weights and add trainable LoRAs~\citep{hu2021loralowrankadaptationlarge}. These LoRAs are specialized for each new patch size and activated only when our~\flexidit~is instantiated with that. There are no LoRAs for the patch size of the pre-trained model. For the tokenization and de-tokenization layers, we simply add new layers $\embedding, \deembedding$ for each new patch size. As before, we use a patch size embedding, but only for the new patch sizes. Note that functional preservation is imposed for the pre-trained patch size, i.e. inference using only the powerful model generates exactly the same samples. We fine-tune by using the predictions of the original model (powerful model) as labels to distill knowledge~\citep{hinton2015distilling} for the new patch sizes, i.e. we train to minimize:$$\mathbb{E}_{t, \x_t} \| \etheta(\x_{t - 1} | \x_{t}; \patchsizeunderscore[powerful]) - \etheta(\x_{t - 1} | \x_{t}; \patchsizeunderscore[weak]) \|_2.$$We find that this leads to improved performance, faster convergence~\citep{cho2019efficacy} and better alignment between predictions of different patch sizes, which can be important given potential discrepancies in the data used during pre-training and our fine-tuning. Here we use $\etheta$ to denote the model's prediction parametrizing the distribution $\ptheta$. Note that $\etheta(\x_{t - 1} | \x_{t}; \patchsizeunderscore[powerful])$ has no trainable parameters. 

During inference, we have two options: (i) keep the new LoRA parameters unmerged, which incurs a minor additional computational cost, or (ii) merge these weights into a copy of the original model parameters, with some additional memory cost. If we keep the LoRA parameters of dimension $\hiddensizelora$ unmerged, the computational complexity of the corresponding linear layer with input and output $(\hiddensizeinput, \hiddensizeoutput)$ will change from $\tokens \hiddensizeinput \hiddensizeoutput$ to $\tokens (\hiddensizeinput \hiddensizeoutput +  \hiddensizeinput \hiddensizelora + \hiddensizelora \hiddensizeoutput)$. If we merge LoRAs, there is no computational overhead, but the new merged parameters need to be kept in memory. Depending on the model requirements and the available resources, one can choose between the two options. We note that compute overhead by keeping LoRAs unmerged is minimal, see also Fig.~\ref{fig:t2i_methodology} (right). For all models tested, additional parameters in this case are less than 5\% of the original model parameters. Further details and ablations can be found in App.~\ref{app:implementation-details}. 

\subsection{Inference Scheduler}

At every denoising step, we need to decide how to instantiate our~\flexidit, i.e. which patch size to use. Following our intuition, we find that for early steps of the denoising process, weak and powerful models produce similar predictions, and thus using the weak model preserves quality while reducing computational complexity, as also seen in Fig.~\ref{fig:methodology} (right). We therefore propose an inference scheduler that, starting from random noise $\pnoise$, first denoises using a weak model for the first $\diffusionstepsweak$ steps and switches to the powerful model for the last $\diffusionstepspowerful = \diffusionsteps - \diffusionstepsweak$ steps. By adjusting the steps performed by the weak model $\diffusionstepsweak$, we can adjust the amount of compute that we are saving. In practice, unless otherwise mentioned, we fine-tune models to process images with one additional patch size. We choose this patch size, corresponding to the weak model, as $2 \times$ larger than the one corresponding to the powerful model. Then, the sequence length corresponding to tokenizing with the new patch size is $4 \times$ smaller, and thus compute required for the powerful model is $> 4 \times$ compared to the weak model. 

% This can be further motivated when comparing the predictions of a trained weak and its corresponding powerful model; weaker model can more accurately match the predictions of the powerful one for steps closer to random noise, i.e. large $\diffusionsteps$, as shown in the side figure. \red{Note that weak model encodes more global structure that is required for the initial steps of the diffusion. We point to Appendix~\ref{sec:additional-experiments} for different schedulers and more results.} \red{mention healing etc}

% \begin{figure}[!h]
%   \begin{center}
%     \vspace{-2mm}
%     \includegraphics[width=1\linewidth]{figures/caching_wide.pdf}
%     \vspace{-3mm}
%     % \caption{\red{remove caption and make this wider, also prettier}}
%   \end{center}
% \end{figure}


% \red{Change this figure to compare with ground truth}

\begin{figure*}[!h]
    \centering
    \floatsetup{heightadjust=all, valign=c}
    \begin{floatrow}
        {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_ultimatum_FID_50k.png} }}%
        \hfill
        {{\includegraphics[width=0.33\textwidth]{figures/xl-256/cfg/cfg_optimality_cfg_FID_50k.png} }}%
        \hfill
        {{\includegraphics[width=0.32\textwidth]{figures/xl-256/cfg/compute_optimality_with_cfg_50000_final-cropped.png} }}%
    \end{floatrow}
    \caption{\textbf{Left:} As the weak model is used more extensively during generation, compute benefits increase, but at the cost of some performance degradation. \textbf{Middle:} The optimal CFG scale varies depending on the extent to which the weak model is used. Each line corresponds to an inference scheduler that applies the weak model for a different proportion of denoising steps. \textbf{Right:} Benefits from our inference scheduler are orthogonal to performing a smaller overall number of diffusion steps. We plot FID for different overall number of steps $\diffusionsteps$ and different number of weak steps $\diffusionstepsweak$, using in every case the DDPM scheduler.}%
    \label{fig:xl-experiments}
\end{figure*}

\subsection{Generation Guidance}

For conditional generation, classifier-free guidance (CFG) is typically employed~\citep{ho2022classifier, esser2024scaling} to enhance sample quality. This entails performing two neural function evaluations (NFEs) (or one NFE with twice the batch size) to compute predictions with and without the conditioning $\condition$, i.e. $\etheta(\x_{t - 1} | \x_{t}, \condition)$ and $\etheta(\x_{t - 1} | \x_{t}, \emptyset)$. Sampling can then take place as $\etheta(\x_{t - 1} | \x_{t}, \emptyset) + \guidancescale (\etheta(\x_{t - 1} | \x_{t}, \condition) - \etheta(\x_{t - 1} | \x_{t}, \emptyset))$, where $\guidancescale$ is the guidance scale. Recent work~\citep{karras2024guiding} has shown that using a smaller or less well-trained version of the model rather than an unconditional model can lead to better guidance signal~\citep{ahn2024selfrectifyingdiffusionsamplingperturbedattention, sadat2024trainingproblemrethinkingclassifierfree}. We adapt these findings in our setting, leading to better generation quality \emph{without the need to train or deploy additional models}. For each denoising step, given a patch size used for the conditional $\patchsizecondition$ and a patch size used for guidance $\patchsizeuncondition$, we compute:
\begin{align*}
    \begin{cases}
      \etheta(\x_{t - 1} | \x_{t}, \emptyset; \patchsizeuncondition) + \guidancescale (\etheta(\x_{t - 1} | \x_{t},\condition; \patchsizecondition) - \\ \etheta(\x_{t - 1} | \x_{t}, \emptyset; \patchsizeuncondition)), \quad \text{if}\ \patchsizecondition = \patchsizeuncondition \\
      \etheta(\x_{t - 1} | \x_{t}, \condition; \patchsizeuncondition) + \guidancescale (\etheta(\x_{t - 1} | \x_{t}, \condition; \patchsizecondition) - \\ \etheta(\x_{t - 1} | \x_{t}, \condition; \patchsizeuncondition)), \quad \text{if}\ \patchsizecondition < \patchsizeuncondition
    \end{cases}
\end{align*}
In this setup, we use the powerful model for the conditional prediction and leverage the weak model's output as guidance. Unlike traditional approaches, our method applies guidance based on the \emph{conditional} prediction from the weak model. Our guidance scheme requires performing inference using both the weak (for the unconditional) and the powerful (for the conditional) model, for some denoising steps. We show in Fig.~\ref{fig:packing} (appendix) how this can be efficiently implemented, making use of packing~\citep{dehghani2024patch}. Depending on the guidance signal used, optimum generation quality can vary with respect to the guidance scale $\guidancescale$. This will become more apparent in the following experiments.