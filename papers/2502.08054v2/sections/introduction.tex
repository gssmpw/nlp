\section{Introduction}
Grasping objects with kinematically infeasible grasp poses due to environmental collisions, known as occluded grasping~\cite{zhan2022learning},  presents a significant challenge in robotics.
Indeed, kinematic infeasibility arises from supporting surfaces, such as the table that the object is resting on.
For example, grasping a keyboard that rests on a desk requires reorienting the keyboard with regard to the desk surface (nonprehensile manipulation) to reveal the grasp pose (see Figure~\ref{fig:teaser}). 
Humans exhibit exceptional dexterity in solving such occluded grasping problems through coordinated bimanual manipulation, seamlessly using both hands to reposition objects for grasping.
However, learning to acquire such coordinated skills for a bimanual robotic system poses significant challenges, particularly when using reinforcement learning (RL)~\cite{schulman2017proximalpolicyoptimizationalgorithms, haarnoja2018soft}.

Specifically, compared to single-handed applications, bimanual manipulation exhibits a significantly increased action space with coordination requirements adding to task complexity. These challenges are exacerbated when using domain randomisation~\cite{tobin2017domain} to enable sim-to-real transfer and render RL approaches infeasible due to sample inefficiency.
For the occluded grasping task, these challenges are particularly pronounced as the policies must enable one arm to stabilise the object while the other reorients and grasps it.
More importantly, designing a reward function that facilitates the emergence of such coordinated behaviour is nontrivial.
Compared to RL, learning from demonstration (LfD) necessitates a large number of expert demonstrations~\cite{zhao2024alohaunleashedsimplerecipe} encompassing a diverse range of objects to achieve generalisation to unseen objects. 


In this work, we present \textbf{Co}nstraint-based \textbf{M}anipulation
for \textbf{B}imanual \textbf{O}ccluded Grasping (\ourmethod), a system specifically designed to address the challenges of occluded grasping using bimanual robot systems.
Drawing inspiration from human bimanual strategies, where the nondominant hand stabilizes an object while the dominant hand performs complex manipulations~\cite{bagesteiro2002handedness, bagesteiro2003, drolet2024comparison}, \ourmethod~consists of two coordinated policies: a \textit{constraint policy} trained using self-supervised learning to generate stabilising poses, and a \textit{grasping policy} trained using RL that reorients and grasps the target object.
The constraint policy generates object-stabilising poses for one of the arms before the grasping policy controls the other arm to attempt grasping.
The use of the constraint policy coupled with the grasping policy accelerates RL training as both policies work in coordination to solve occulded grasping problems in a data-efficient manner.

The constraint policy is trained on a synthetic dataset collected in a self-supervised manner within a simulation designed to leverage force closure as a signal.
At the core of \ourmethod~is value function-guided policy coordination that refines the constraint pose generated by the constraint policy to improve bimanual coordination, thereby enhancing task performance. 
During RL training for the grasping policy, using gradients from the value function trained in tandem with the grasping policy, \ourmethod~optimises the constraint pose to increase the likelihood of a successful grasp once the grasping policy is executed.
This value function-guided policy coordination ensures that the output of the constraint policy is refined to align with the goals of the grasping policy, leading to improved stability of objects during the bimanual grasping.

\ourmethod~achieves effective sim-to-real transfer by adopting a teacher-student policy distillation approach.
The teacher policy, trained with privileged information for diverse objects in simulation, is distilled into a student policy that operates on point clouds for effective sim-to-real transfer. 
In contrast to training a single RL policy or LfD, \ourmethod~learns effective bimanual coordination with improved sampled efficiency and generalizes to unseen objects without relying on any expert demonstrations.

In summary, our contributions are four-fold: 
\begin{itemize}
    \item \ourmethod, a novel approach to bimanual manipulation comprising two coordinated policies to solve occluded grasping problems.
    \item The use of force-closure as a signal to train a self-supervised constraint policy, which accelerates the subsequent RL grasping policy training.
     \item Value function-guided policy coordination that refines generated constraint poses using gradients from the value function to improve coordination during RL training for the grasping policy.
    \item Empirically demonstrating that \ourmethod~successfully grasps seen and unseen objects in both simulated and real-world environments.
\end{itemize}

