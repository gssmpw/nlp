\section{Related Works}

\subsection{Learning to Grasp Objects}
Grasping is a fundamental robot skill essential for various subsequent manipulation tasks~\cite{yamada2023efficient, collins2023ramp, yuan2023m2t2}.
Many prior works focus on learning grasp pose prediction models with open-loop planning controls~\cite{yuan2023m2t2, mousavian20196, barad2024graspldm}.
These methods typically assume that there exist grasp poses that are not in collision with obstacles, such as a table, so that a robot can reach these poses using motion planning.
Consequently, such open-loop approaches are inadequate for handling occluded grasping tasks where environmental constraints may block or interfere with the target grasp poses.

Prior art has also investigated closed-loop grasping policies using reinforcement learning (RL)~\cite{kalashnikov2018scalable, wang2022goal} and imitation learning (IL)~\cite{zhou2023nerf, Song2019GraspingIT}.
Recent advancements using deep RL have empowered robots to acquire complex manipulation skills driven by predefined reward functions.
For example, \citet{wang2022goal} proposes learning a 6-DoF policy to grasp objects in tabletop environments.
Moreover, QT-Opt~\cite{kalashnikov2018scalable} learns to grasp objects in a box by collecting diverse samples from multiple real-world robots.
\ourmethod~is positioned amongst the works aiming to learn a closed-loop grasp policy, but focuses on more challenging occluded grasping scenarios that necessitate nonprehensile manipulation before grasping.



\subsection{Occluded Grasping}
Several prior work~\cite{sun2020learning, zhou2023learning} attempt to solve occluded grasping tasks through extrinsic dexterity while using only a single arm.
\citet{zhou2023learning} train a policy using RL from state observations in simulation only for a rectangular object to learn extrinsic dexterity via interaction with a wall to reorient the object for grasping.
\citet{sun2020learning} address the challenge of occluded grasping by utilising two arms; however, both arms are primarily employed for object reorientation, and the approach still relies on external constraints, such as a supporting wall.
In contrast to these methods that rely on external constraints, \ourmethod~considers scenarios without external constraints, necessitating the use of one arm to stabilise the object while the other arm re-orients the object.



\subsection{Bimanual Robotic Systems}
Bimanual robotic manipulation has gained increasing attention due to its flexibility and capability to handle complex tasks such as dynamic handovers~\cite{huang2023dynamic}, twisting bottle lids~\cite{lin2024twisting}, and assembly tasks~\cite{drolet2024comparison, shao2020learning}. 
RL~\cite{haarnoja2018soft, schulman2017proximalpolicyoptimizationalgorithms} offers a reward-driven approach to skill acquisition but requires extensive exploration, particularly for high-DoF bimanual tasks. 
Alternatively, learning from demonstrations often demands a large number of expert demonstrations~\cite{zhao2024alohaunleashedsimplerecipe}, which is often costly and impractical for complex bimanual systems, especially in non-prehensile manipulation scenarios.

Several works address these challenges by incorporating inductive biases into RL frameworks. For instance, predefined parameterised skills~\cite{chitnis2020efficient}, intrinsic motivation for efficient exploration~\cite{Chitnis2020Intrinsic}, and symmetry-aware actor-critic networks~\cite{li2023efficient} have shown promise. 
Similarly, \ourmethod~introduces a constraint policy as an inductive bias, specifically tailored for occluded grasping tasks.
In particular, inspired by studies in biopsychology~\cite{Sainburg2001EvidenceFA, bagesteiro2002handedness, bagesteiro2003}, which suggest that the non-dominant arm tends to stabilise while the dominant arm performs more complex movements, \ourmethod~adopts a similar principle. 
One arm is used to stabilise and secure the object, while the other performs non-prehensile manipulation for occluded grasping.

Stabilising an object with one arm to assist the other in manipulation is a well-established strategy. 
For example, \citet{grannen2023stabilize} propose learning two policies: one for stabilising the objectâ€™s position and the other for manipulation, applied to a peg-insertion task. 
Similarly, \citet{shao2020learning} adopt a dual-loop learning framework, where the inner loop optimises the grasping policy, and the outer loop refines the constraint policy based on the inner-loop performance. This approach, though effective, suffers from significant sample inefficiency.



In contrast, \ourmethod~eliminates the reliance on expert demonstrations or dual-loop training. Instead, it utilises self-supervised data collection in simulation to train a constraint policy that stabilises objects, thereby accelerating RL training of the grasping policy for occluded grasping of diverse objects. 
Crucially, \ourmethod~introduces value function-guided policy coordination to refine the generated constraint poses by leveraging gradients derived from maximising the value function of the grasping policy during the RL training. 
This refinement process enables the constraint policy to adapt the constraint pose such that it is more suitable for the grasping policy, thereby improving the coordination between the constraint policy and the grasping policy for bimanual occluded grasping tasks.