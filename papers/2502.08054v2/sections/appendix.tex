\section{Appendix}
\subsection{Additional Analysis for Experiments}
\label{appendix:analysis}
\subsubsection{Student Policy Performance per Object}
Figure~\ref{fig:student_train_success} illustrates the success rate of \ourmethod~for each object used during training.
While \ourmethod~demonstrate performant success rate across diverse objects, the occluded grasp performance for small objects or objects with complex geometries is reduced when compared to that of large objects with simple geometries.
In order to overcome this limitation, it is suggested that both teacher and student policies be trained using more diverse objects, such as those available in the Objaverse datasets~\cite{deitke2023objaverse}.


\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/student_success_per_category.pdf}
    \caption{\textbf{Grasp performance of \ourmethod's student policies for each object in simulation. The success rate is averaged over $50$ trials.}}
    \label{fig:student_train_success}
\end{figure*}



\subsection{Teacher Policy Details}
\label{appendix:teacher}


\subsubsection{Teacher Constraint Policy}
\label{appendix:teacher_const}
We employ a diffusion policy~\cite{chi2023diffusion} as the basis for the teacher constraint policy. The diffusion policy is implemented using a Denoising Diffusion Probabilistic Model (DDPM), with a multi-layer perceptron (MLP)-based backbone.
The denoising model is built on a three-level UNet architecture, comprising residual blocks with a hidden layer size of $512$.
The diffusion time step is encoded as an $80$-dimensional feature vector. 
Additionally, the desired grasp pose, $\mathbf{x} \in \mathbb{R}^{9}$, and the object's ID, $\mathbf{x}_{obj\_id} \in \mathbb{R}^{16}$, are encoded into an $80$-dimensional vector respectively to provide task-specific context. 
Similarly, the noisy input representing the constraint pose is encoded into another $80$-dimensional vector. 
These encoded vectors are summed and passed through the residual blocks. 
The denoising model outputs the noise added to the original input during the forward diffusion process.
In this work, we use $100$ diffusion time steps for both training and inference.
We train the diffusion policy using an Adam optimiser with a learning rate of $1\times 10^{-4}$.


\subsubsection{Teacher Grasping Policy}
\label{appendix:teacher_manipulation}
We train a teacher grasping policy using Proximal Policy Optimisation (PPO).
An actor network consists of an MLP with $2$ hidden layers of sizes $[256, 256]$.
The actor network is parameterized as a Gaussian distribution with a fixed, state-independent standard deviation.
The critic network consists of an MLP with $3$ hidden layers of sizes $[256, 256, 256]$.

We define the privileged information used to train the policy as $[\mathbf{x}_{\text{robot}}, \mathbf{x}_{\text{goal}}, \mathbf{x}_{\text{obj}}] \in \mathbb{R}^{64}$.
The robot proprioceptive states, $\mathbf{x}_{\text{robot}}$, include the left end-effector pose, $\mathbf{x}_{\text{left}} \in \mathbb{R}^{9}$, the right end-effector pose, $\mathbf{x}_{\text{right}} \in \mathbb{R}^{8}$, and the translational and rotational action scale parameters for the operational space controller, $\mathbf{x}_{\text{control}} \in \mathbb{R}^{2}$.
The right end-effector states, $\mathbf{x}_{right}$, exclude the $z$-coordinate position, as the table height remains constant, and the constraint pose is fixed at a predetermined $z$-coordinate.
The goal-related states, $\mathbf{x}_{\text{goal}}$, consist of the desired grasp pose, $\mathbf{x}_{\text{grasp}} \in \mathbb{R}^{7}$, the distance between the left end-effector and the desired grasp position, $\mathbf{x}_{\text{dist}} \in \mathbb{R}^{3}$, and the orientation distance between the left end-effector and the desired grasp orientation in the axis-angle representation, $\mathbf{x}_{\text{dist\_ori}} \in \mathbb{R}^{3}$.
The object states, $\mathbf{x}_{\text{obj}}$, comprise the object pose, $\mathbf{x}_{\text{obj\_pose}} \in \mathbb{R}^{7}$, the object velocity, $\mathbf{x}_{\text{obj\_vel}} \in \mathbb{R}^{6}$, the friction parameters, $\mathbf{x}_{\text{friction}} \in \mathbb{R}^{2}$, the object's mass, $x_{\text{mass}} \in \mathbb{R}^{1}$, and the object's ID, $\mathbf{x}_{\text{obj\_id}} \in \mathbb{R}^{16}$.

We train the policy using an Adam optimiser with an adaptive learning rate scheduler\footnote{\url{https://skrl.readthedocs.io/en/latest/api/resources/schedulers/kl_adaptive.html}} based on the KL divergence between the current policy and the previous policy, whose maximum learning rate is $1\times 10^{-2}$ and the minimum is $1\times 10^{-6}$.
We use a discount factor of $0.99$, a GAE lambda value of $0.95$, and an entropy coefficient of $6e-3$.
After each policy rollout, the policy is updated using a batch size of $2048$ for $8$ epochs.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/training_objects.png}
    \caption{\textbf{Training objects.} We choose $48$ training objects from the Google Scanned Object Dataset~\cite{downs2022googlescannedobjectshighquality}.}
    \label{fig:training_objects}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/test_objects.png}
    \caption{\textbf{Test objects.} We evaluate $10$ held-out objects from the Google Scanned Object Dataset.}
    \label{fig:test_objects}
\end{figure}

\subsubsection{Reward function}
The reward function used in our experiments comprises six terms and is defined as follows:
\begin{equation}
    \begin{split}
    r = \alpha_{1} r_{dist\_pos} &+ 
    \alpha_{2} r_{dist\_ori} - \alpha_{3} r_{collision} \\
    & - \alpha_{4} r_{action} + \alpha_{5} r_{lift} + \alpha_{6} r_{success}        
    \end{split}
\end{equation}
where the weighting coefficients are set to $\alpha_1 = 0.2$, $\alpha_2 = 0.2$, $\alpha_3 = 1.0$, $\alpha_4 = 0.025$, $\alpha_5 = 0.1$, and $\alpha_6 = 40$. Each term in the reward function serves a distinct purpose in guiding the robotâ€™s behaviour:

\begin{itemize}
    \item \textbf{Position Distance Reward ($r_{dist\_pos}$):} 
    This term incentivizes the left end-effector to move towards the desired grasp position. It is computed as:
    \begin{equation}
        r_{\text{dist\_pos}} = 1 - \tanh(4 \cdot ||\mathbf{p}_{\text{left}} - \mathbf{p}_{\text{grasp}}||_{2}),
    \end{equation}
    where $\mathbf{p}_{\text{left}} \in \mathbb{R}^3$ and $\mathbf{p}_{\text{grasp}} \in \mathbb{R}^3$ represent the current and desired positions of the left end-effector, respectively.

    \item \textbf{Orientation Distance Reward ($r_{dist\_ori}$):} 
    This term encourages the left end-effector to align its orientation with the desired grasp orientation. The orientation difference is measured in the axis-angle space`. The reward is computed as:
    \begin{equation}
        r_{\text{dist\_ori}} = 1 - \tanh(0.2 \cdot ||\boldsymbol{\theta}_{\text{left}} - \boldsymbol{\theta}_{\text{grasp}}||_{2}),
    \end{equation}
    where $\boldsymbol{\theta}_{\text{left}} \in \mathbb{R}^3$ and $\boldsymbol{\theta}_{\text{grasp}} \in \mathbb{R}^3$ represent the axis-angle representations of the current and desired orientations of the left end-effector, respectively.

    \item \textbf{Action Penalty ($r_{action}$):} This term discourages large control commands by penalizing the magnitude of the action vector:
    \begin{equation}
        r_{\text{action}} = ||\mathbf{a}||_2.
    \end{equation}

    \item \textbf{Collision Penalty ($r_{\text{collision}}$):} To prevent self-collisions and contact with the table, we compute the signed distance (SD) using CuRobo~\cite{sundaralingam2023curobo}. The collision penalty is given by:
    \begin{equation}
        r_{\text{collision}} = SD_{\text{self\_col}} + SD_{\text{table}}.
    \end{equation}
    The signed distance is computed for the robot arms, excluding the grippers, since the grippers must make contact with the table for occluded grasping problems. In CuRobo, a positive signed distance indicates a collision.

    \item \textbf{Lift Reward ($r_{\text{lift}}$):} This term encourages lifting the object to expose an initially occluded grasp pose. It is defined as an indicator function:
    \begin{equation}
        r_{\text{lift}} = \mathbbm{1}(z_{\text{grasp}} > z_{\text{grasp,init}} + 2 \text{ cm}),
    \end{equation}
    where $z_{\text{grasp}}$ and $z_{\text{grasp,init}}$ denote the current and initial heights of the desired grasp position, respectively.

    \item \textbf{Grasp Success Reward ($r_{success}$):} At the end of an episode, a reward of 1 is assigned if the left arm successfully grasps and lifts the object; otherwise, the reward is 0:
    \begin{equation}
        r_{\text{success}} =
        \begin{cases}
            1, & \text{if grasp and lift are successful}, \\
            0, & \text{otherwise}.
        \end{cases}
    \end{equation}
\end{itemize}


    
    



\subsection{Student Policy Details}
\label{appendix:student}

\subsubsection{Studnet Constraint Policy}
The student constraint policy integrates the DP3 encoder~\cite{ze20243d} and a state encoder to process point cloud and state observations, respectively. 

The DP3 encoder comprises three fully connected layers with dimensions of $[128, 256, 384]$, followed by a max pooling operation and a final fully connected layer of size $64$. 
Layer normalization and ReLU activations are applied after each of the initial three layers preceding the max pooling operation.
The state encoder consists of two hidden layers with dimensions of [$128$, $256$]. The state encoder outputs a feature vector of size $32$ given the desired grasp pose $\mathbf{x}_{grasp}$.

The feature vectors produced by the DP3 and state encoders are concatenated and subsequently processed through a MLP to generate a constraint pose.
For this work, the student policy utilizes a Gaussian Mixture Model (GMM)-based approach due to its simplicity and effectiveness. 
Specifically, the GMM-based policy employs $5$ modes, with a minimum standard deviation of $1\times 10^{-4}$.
We employ an AdamW optimiser with a learning rate of $5 \times 10^{-5}$ and a weight decay of $5 \times 10^{-5}$.


\subsubsection{Student Grasping Policy}
We adopt the 3D Diffusion Policy (DP3)~\cite{ze20243d} as the foundation for the student grasping policy. 
The architecture of the DP3 encoder and the state encoder is consistent with that employed in the student constraint policy. 
However, the weights of these encoders are independently initialized from those of the constraint policy. 
Furthermore, the input dimension for the state encoder in the manipulation policy differs from that of the constraint policy.
The state encoder for the manipulation policy processes $\mathbf{x}_{robot}$ and optionally $\mathbf{x}_{grasp}$ as input. 
During training, we employ $100$ diffusion timesteps, whereas during inference a Denoising Diffusion Implicit Model (DDIMs) is used with  $10$ diffusion timesteps to accelerate action generation. 
We use an AdamW optimiser with a learning rate of $5 \times 10^{-5}$ and a weight decay of $5 \times 10^{-5}$.


\subsection{Simulation Setup}
\subsubsection{Training}
\label{appendix:sim_training}
In order to train a teacher policy from a diverse set of objects, we select $48$ objects from the Google Scanned Object dataset, as illustrated in Figure~\ref{fig:training_objects}.
To train teacher policies efficiently, we spawn $1024$ robots and objects in the simulated environment.

In order to train a policy robust to noises and effectively transfer it to real-world environments, we apply domain randomisation during teacher policy training.
Table~\ref{table:domain_randomisation} describes the details of the randomisations used in our experiments.
We also apply domain randomisation during the self-supervised data collection for the constraint policy.

\begin{table}[h]
\centering
\caption{Domain Randomisation Hyperparameters}
\vspace{0.5em}
\begin{tabular}{c|c} 
 \toprule
 Parameter  & Description  \\ 
 \midrule
 \midrule

 Initial robot joint positions & Add noise sampled from $\mathcal{N}(0, 0.05)$  \\
 Robot base position & Add random noise sampled from $\mathcal{U}(-0.015, 0.015)$ \\
 & to the z-coordinate of the robot base \\
 PID position action scale & Sampled from $\mathcal{U}(0.03, 0.04)$ \\
 PID rotation action scale & Sampled from $\mathcal{U}(0.1, 0.2)$ \\
 Action & Add random noise sampled from $\mathcal{N}(0, 0.01)$ \\
 Object mass & Add mass sampled from $\mathcal{U}(-0.1, 0.1)$ \\
 Static and dynamic friction & Sampled from $\mathcal{U}(0.8, 1.2)$ \\
 Grasp position & Add random noise sampled from $\mathcal{N}(0, 0.005)$ \\
 Grasp translational distance & Add random noise sampled from $\mathcal{N}(0, 0.005)$ \\
 Grasp rotational distance & Add random noise sampled from $\mathcal{N}(0, 0.005)$ \\
 End-effector position & Add random noise sampled from $\mathcal{N}(0, 0.01)$ \\
 Object position & Add random noise sampled from $\mathcal{N}(0, 0.01)$ \\
 Object orientation & Add random noise sampled from \\
 & $U(-0.2\pi\: \text{rad}, 0.2\pi\: \text{rad})$ to the yaw axis \\
 \bottomrule
\end{tabular}
\label{table:domain_randomisation}
\end{table}

\subsubsection{Evaluation}
To evaluate policies for both seen and novel objects, we also select $10$ held-out objects from the Google Scanned Object dataset (see Figure~\ref{fig:test_objects}).


\subsection{Real-World Experiment Setup}
\subsubsection{Input Observation for Student Policies}
The distilled student policies take point clouds as input in real-world environments.
We render depth images with the size of $640\times480$ from a Realsense L515 camera to reconstruct point cloud observations.
Similar to \cite{ze20243d}, we crop the point cloud within a pre-defined bounding box such that it includes the robot arms and the target object.
Then, we remove statistical outliers from the point clouds reconstructed from depth images and apply farthest point sampling to sub-sample $1024$ points.

\subsubsection{Desired Occluded Grasp Pose Generation}
In order to scan an object to reconstruct a mesh, we use Polycam, an application that captures pictures of objects and reconstructs an object mesh using Neural Radiance Fields (NeRF).
Using the reconstructed mesh, we generate desired occluded grasp poses using antipodal sampling.


\subsection{Baseline Method Details}

\subsubsection{PPO}
We train a policy using Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}, where the policy outputs 12-dimensional delta end-effector poses corresponding to both the left and right arms. 
We use the same hyperparameters employed for training \ourmethod, except for the entropy coefficient, which is set to $0.003$. 
This modification was made because using the original entropy coefficient caused a continuous increase in the policy's standard deviation, resulting in the policy's inability to exploit a stable and effective strategy during training.


\subsubsection{PPO + Constraint Reward}
Similar to the \emph{PPO} baseline, but we introduce an additional reward term that encourages the right arm to be used as a constraint.
In particular, we add a reward $r_{right\_dist}=||T^{obj} - T^{RightEE}||_{2}$.

\subsubsection{\ourmethod~w/ Fixed Constraint}
Instead of employing a trained constraint policy, we place the right arm as a constraint at a fixed pose.
To accommodate objects of varying sizes and orientations, the constraint is positioned at the right hand side of the workspace rather than at the centre.
This policy is trained using the same hyperparameters as those employed by \ourmethod.