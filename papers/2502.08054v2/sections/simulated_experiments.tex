\section{Experimental Results: Simulation}

Our experimental evaluation aims to address the following questions: (1) How successful is \ourmethod~in learning a teacher policy compared to competitive baselines? (2) How well does \ourmethod~generalise to unseen objects? 
(3) How does the value function-guided policy coordination affect \ourmethod's overall performance?
For further analysis of the experiment, see Appendix~\ref{appendix:analysis}.




\subsection{Baselines}
We compare \ourmethod~with the following baselines:

\begin{itemize}
    \item \textbf{PPO}: A PPO~\cite{schulman2017proximal} policy that controls both arms. The policy outputs $12$-dimensional actions. Compared to \ourmethod~which employs two coordinated policies, this baseline requires more extensive exploration to solve the task.
    \item \textbf{PPO + Constraint Reward}: A PPO policy trained using a reward function that adds a distance-based reward between the right end-effector and the centre of the target object into the original reward function. This modification encourages the right arm to act as a constraint, assisting the left arm in grasping tasks. This enables the policy to avoid undesirable physical behaviours observed when using the original reward function, such as the left end-effector attempting to grasp the target object with high velocity without using the constraint.
    \item \textbf{\ourmethod~with a fixed constraint}: A PPO policy is trained to control the left arm, while the right arm remains fixed in a predefined pose in contrast to \ourmethod. This showcases the importance of a constraint policy.
    \item \textbf{\ourmethod~without refinmenet}: \ourmethod~without value function-guided policy coordination. This demonstrates the necessity of refining the constraint pose generated by the constraint policy to further improve performance. 
\end{itemize}

\subsection{Evaluation Metric}
For evaluation, we assess the success rate of grasping. 
In particular, a trial is considered successful if the robot's left arm securely grasps and lifts the target object at least $8 \ cm$ at the end of the episode.

\subsection{Sample Efficiency in Teacher Policy Training}
Firstly, we evaluate the performance of teacher policy training in simulation.
As shown in Figure~\ref{fig:teacher_policy}, \ourmethod~solves the complex occluded grasping task with more sample efficiency and overall achieves a higher performance. 
On the other hand, \emph{PPO} struggles to achieve similar performance due to the task and system complexity.
More crucially, \emph{PPO} trained with the original reward function often demonstrates undesirable physical behaviours, such that the left arm attempts to grasp the target object with high velocity without leveraging the right arm as a constraint.
Such behaviour exploits the imperfect physics in the simulator and cannot be transferred to real-world environments.
\emph{PPO + Constraint Reward} alleviates this issue by adding a distance-based reward function.
However, defining a reward function to induce a suitable constraint is challenging because such constraint poses are not known beforehand as this is a cooperative task dependent on the approach taken by both arms.
Thus, engineering an appropriate reward function to elicit desired behaviour is not straightforward.
These findings suggest that coordinated policies consisting of the constraint policy and the grasping policy effectively accelerate training and lead to higher success rates compared to a single policy trained using the state-of-the-art RL method.

\ourmethod~w/ fixed constraints demonstrate mediocre performance, as fixed constraints are sometimes not ideal for the grasping policy to solve occluded grasping tasks.
Similarly, \ourmethod~w/o refinement shows worse performance compared to that of \ourmethod.
These results indicate that pre-training a constraint policy and refining the output during the teacher policy training are essential components in \ourmethod.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Teacher-Policy-Training.pdf}
    \caption{\textbf{Teacher policy training.} We run $3$ seeds for each method, and the shaded region represents the standard deviation. \ourmethod~significantly outperforms competitive baselines in both performance and sample efficiency.}
    \label{fig:teacher_policy}
\end{figure}

\subsection{Student Policy Performance in Simulation}
We assess the performance of the distilled student policies on both seen and unseen objects in the simulation.
Figure~\ref{fig:student_perform} shows the performance of the student policies in the simulated environments.
\ourmethod~effectively addresses the challenging occluded grasping tasks for both seen and unseen objects.
Without the desired grasp pose as input, \ourmethod~has reduced performance; however, it still demonstrates relatively performant results.

The success rates of \emph{PPO} and \emph{PPO + Constraint Reward} are similar for seen objects, but \emph{PPO + Constraint Reward} performs significantly better on unseen objects.
This improvement arises because \emph{PPO + Constraint Reward} avoids exploiting imperfect physics and instead uses the right arm as a constraint induced by the constraint reward, which makes it more transferable to unseen objects. 
These findings suggest that learning a coordinated strategy is crucial for better transfer performance when dealing with unseen objects. 
Nonetheless, the performance of \emph{PPO + Constraint Reward} remains inferior to \ourmethod~due to the difficulty of training a teacher policy that effectively learns a better constraint. 
The challenge of reward engineering limits the teacher's capability, which in turn reduces the performance of the student policy.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/student_success_rate.pdf}
    \caption{Student Policy Performance averaged over $3$ seeds in Simulated environments. We evaluate each approach for $50$ times using both seen and unseen objects.}
    \label{fig:student_perform}
    \vspace{-0.4cm}
\end{figure}


\subsection{Ablation of the Value Function-guided Policy Coordination}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/Guidance-Ablation.pdf}
    \caption{\textbf{Guidance scaling ablation.} We compare the guidance scaling parameter to steer the output of the constraint policy. This result indicates that \ourmethod~without guidance shows worse performance and \ourmethod is robust to a wide range of guidance scaling parameters to achieve better performance.}
    \label{fig:guidance_ablation}
\end{figure}

The degree to which the value function-guided policy coordination improves the task success rate is investigated here. 
Concretely, the impact of the scaling parameter $w$ on the constraint diffusion policy (see Eq.~\ref{eq:guidance}) during teacher policy training is investigated.
As illustrated in Figure~\ref{fig:guidance_ablation}, the teacher policy's performance decreases when value function policy coordination is not applied (\ie $\lambda=0$).
On the other hand, incorporating value function policy coordination consistently enhances the teacher policy's overall performance.
This finding suggests that the constraint policy occasionally generates constraint poses that are suboptimal for the grasping policy. 
Consequently, value function policy coordination promotes on-the-fly adjustments and this is cooperation between the two arms achieves higher success rates.

