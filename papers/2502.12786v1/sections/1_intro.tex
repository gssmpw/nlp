
\section{Introduction}

Diffusion models \citep{sohldickstein2015deep, song2020generative, song2021scorebased} have come to dominate the generative modelling landscape, exhibiting state of the art performance \citep{dhariwal2021diffusion} across domains and modalities \citep{de2022riemannian}, including self-supervised learning \citep{chen2024deconstructing}, natural science applications \citep{arts2023two} and for optimal transport \citep{debortoli2021diffusion}.

Despite the success of diffusion models, there are still a number of challenges. Firstly, diffusion models are known to have slow and expensive training \citep{jeha2024variance, zhang2023improving}. Secondly, effective conditioning of diffusion models remains an open challenge \citep{wu2024practical, zhao2024conditional}. Re-using, fine-tuning and adapting pretrained models has become an active area of research; both to overcome lengthy pretraining and to introduce additional conditioning not considered during training \citep{ye2024tfg, du2021improved}. In addition, many heuristic guidance weighting methods and prompt engineering techniques have been proposed to control generation. Such approaches are poorly understood \citep{bradley2024classifier}, often require ad-hoc weighting and trial-and-error sampling to reach desired samples. 

A diffusion model may be viewed as a sequence of time-indexed energy-based models, where the gradient of the energy is learnt rather than the energy itself \citep{song2021trainebm, salimans2021should}. It was shown in \cite{du2023reduce} how the energy interpretation of diffusion models may be used to better condition and compose pretrained diffusion models in order to generate novel distributions, such as composed distributions, rather than the default reverse process. \cite{du2023reduce}  achieves this through Markov Chain Monte Carlo (MCMC) and annealed Langevin dynamics. Whilst a promising direction, energy-parameterised diffusions are inherently cumbersome to train and annealed MCMC sampling requires an excessive number of network evaluations.

 Energy-parameterized diffusion models require computing multiple gradients through the energy function during training; first with respect to (w.r.t) the input state to recover a score, then w.r.t parameters for training. This exacerbates the already lengthy training entailed by denoising score-matching. 

\textbf{Contributions}
The purpose of this work is two-fold. First to address training instability of energy-parameterized diffusion models, and secondly to introduce a new class of diffusion model samplers using the energy function for controllable generation within a Feynman Kac - Sequential Monte Carlo framework.

We summarize our key contributions as follows:
\begin{itemize}
\item We introduce a novel training procedure and parameterization to efficiently distill pretrained diffusion models into energy based models, whilst avoiding the high-variance loss of denoising score-matching. Our method can be interpreted as a \textit{conservative projection} of a pretrained score.
\item We showcase the merits of our approach in terms of generative performance, consistently achieving superior FID to prior energy-parameterised models for the datasets considered.
\item We describe a general framework of how diffusion models may be used as the underlying Markov process of a Feynman Kac model (FKM); we detail how prior SMC based diffusions may be viewed as particular cases.
\item Finally, we demonstrate how the energy function may be used to construct modality agnostic potentials within FKMs. This enables temperature controlled sampling, as well as composition of diffusion models via SMC. 
\end{itemize}
