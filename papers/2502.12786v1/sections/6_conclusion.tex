

\subsection{Limitations} 

\textbf{Multiple-networks}. Our distillation loss requires access to pretrained models. Exploring multi-headed networks for joint training of both score and energy were may be an interesting direction to pursue, this would avoid the need for pretrained networks and reduce NFE at sampling time.

\textbf{Diversity}. Resampling may result in a loss of diversity for poorly constructed potentials and proposals. Investigating approximate resampling techniques which preserve diversity such as in \citep{corenflos2021differentiable, ma2020particle,zhu2020towards} may be practical mitigation strategy.

\textbf{Mixing of Scores}. Score-matching and hence our distillation loss may suffer practical issues for supports with isolated components, resulting in learning the incorrect mixing proportions \citep{wenliang2020blindness} - known as the blindness of score-matching. Whilst this may not pose an issue in our setting for $t>0$ due to Gaussian noise connecting the support, there may be issues in using energy functions trained with score matching very close to $t=0$. 

\subsection{Future Considerations} 
\textbf{Scale and modalities.} Whilst we have successfully demonstrated our methods on medium size image datasets, we are yet to verify the performance on other modalities or larger datasets. A first step would be to apply this to latent space \citep{vahdat2021score, rombach2022high} for higher resolution images. Similarly, the energy function is modality agnostic and could be applied to other fields such as molecular dynamics, \citep{arts2023two}.

\textbf{Other application of the energy function}. There are a plethora of applications requiring the energy worth exploring, for example the \textsc{NEGATION} or \textsc{UNION} operations also require access to time-indexed densities \citep{duvisual, koulischer2024dynamic}. Similar to classical EBMs, our learnt energy functions may also be used for unsupervised learning \citep{comet} and reasoning tasks \citep{energy_reasoning}.

\textbf{The benefit of conservative scores}. Although conservative score approximations are not strictly necessary for generative modeling \citep{horvat2024gauge}, our method does enable one to learn SOTA performant diffusion models with strictly conservative scores. Conservative scores have been remarked as crucial in molecular dynamics \citep{arts2023two}; as well as provide attractive theoretical properties \citep{daras2024consistent} in terms of generalization. 

Given the pursuit optimal transport (OT) has attracted a lot of attention \citep{debortoli2021diffusion, thornton2022riemannian, liu20232, shi2023diffusion}, it is worth remarking that strictly conservative drifts are required to recover OT with ReFlow \citep{liu2022rectified}.

\subsubsection*{Acknowledgements}
We thank Rob Brekelmans for feedback on an early draft of this paper and Kevin Li for references \citep{horvat2024gauge, wenliang2020blindness} and fruitful discussion on the advantages/ disadvantages of conservative scores.