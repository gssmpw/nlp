\section{Discussion}

\subsection{Related Prior Work}

\textbf{Training Energy Based Models}.
A number of recent works aim to improve EBM training. \citet{zhu2023learning} uses a diffusion model to reduce the number of Langevin steps within the recovery likelihood approach of \cite{gao2020learning}. \cite{schroder2024energy} eliminates the need for MCMC and $\nabla_x$-computation of the energy during training by using a contrastive loss with forward noising process instead of MCMC, coined Energy Divergence (ED). ED is a promising alternative to E-DSM and has connections to score-matching, but, as shown in \Cref{tab:unconditional_gen}, it is not yet competitive, and  suffers from a bias by choice of noisy energy function.

\textbf{Composition with MCMC}. Similar to our work, \cite{du2023reduce} also uses energy-parameterized diffusion models but performs controlled generation with MCMC rather than SMC. SMC is known to suffer weight degeneracy high dimension, resulting in lack of diversity across particles, MCMC does not suffer from this, though requires additional non-parallel steps which is time consuming. The approaches are however complementary, and indeed one may perform MCMC after resampling steps to promote diversity.

\textbf{Sequential Monte Carlo in Diffusion Models.}
Many recent works use SMC within diffusion models for conditional generation, we detail the FKM formulations of these works in in \Cref{app:fkm_potentials}.

\cite{wu2024practical} uses twisted SMC with a classifier-guided proposal \citep{dhariwal2021diffusion} and potentials approximated with diffusion posterior sampling \citep{chungdiffusion}, which has been detailed as a FKM by concurrent work \citep{zhao2024conditional}. \cite{cardoso2024monte} and \cite{dou2024diffusion} tackle linear inverse problems where potentials have a closed form using Gaussian conjugacy. \cite{li2024derivative} perform SMC for both discrete and continuous diffusion models whereby potentials consist of a reward function applied to $\mathbb{E}[\bfX_0|x_t]$.

\cite{liu2024correcting} corrects conditional generations using an adversarially trained density ratio potential, and scales this to text-to-image models. 


\textbf{SMC for LLMs}. SMC is not only popular within diffusion models, but has been successful within large language models (LLMs) \citep{lew2023sequential, zhao2024probabilistic}. \cite{lew2023sequential} uses a FKM formulation with indicator based potential functions similar to as detailed in \Cref{sec:bounded}, and \cite{zhao2024probabilistic} discuss using SMC for text using potentials from reward functions or learning such potentials via contrastive twist learning, similar to contrastive learning for EBMs.

\subsection{Concurrent work} Since submission/ acceptance of our work \footnote{Submission October 2024}, there have been a number of relevant concurrent works. 

\textbf{FKM Interpretation}. \cite{singhal2025general} similarly to \cite{zhao2024conditional} and this work, detail sampling diffusion models in terms of KFM. \cite{singhal2025general} follow \cite{li2024derivative} in using reward functions based potentials but focus on text-to-image reward, and explore further heuristics such as or combining rewards via sum or max; and sampling $\bfX_0|x_t$ via nested diffusion \citep{elata2024nested} as input to their reward rather than using $\mathbb{E}[\bfX_0|x_t]$ as done in \cite{li2024derivative}.

\textbf{SMC for discrete diffusion}.  \cite{lee2025debiasing} use SMC for low temperature sampling for discrete diffusion models. \cite{xu2024energy} use a pretrained autoregressive likelihood model applied to samples $\bfX_0|x_t$ for a potential within discrete diffusion sampling.

\textbf{Composition}. \cite{skreta2024superposition} construct a cheap density estimator by simulating from an SDE, which can be computed at sampling time if using reverse diffusion solver, though it is not clear if this can be used in conjunction with resampling and Langevin corrector schemes. \cite{skreta2024superposition} then use this estimator to perform composition-type sampling, however their logical \textsc{AND} appears to differ from other more commonly used logical \textsc{AND} operations, in that it targets samples with equal probability between classes rather than generating both classes, e.g. "a CAT and a DOG" results in a cat/dog hybrid optical illusion rather than a separate cat and separate dog in one image. 

\cite{bradley2025mechanisms} explore composition more formally, establishing types of composition and cases where summing scores is sufficient without need for SMC correction as performed in this work or with MCMC correction from \citep{du2023reduce}.



