\textbf{Feynman Kac Models.}
Just as in computational statistics, Sequential Monte Carlo (SMC) methods provide a promising alternative to MCMC without requiring one to run a Markov chain to convergence. In order to understand the full generality of how SMC enables a change of measure, we first describe the Feynman Kac model (FKM) framework \citep{chopin2020introduction}. A FKM consists of an initial distribution $M_0$; some time indexed Markov transition kernels $(M_t)_t$, which we can sample from; and non-negative potential functions $(G_t)_t$, $G_0: \mathcal{X} \rightarrow \mathbb{R}^+$, $G_t: \mathcal{X}^2 \rightarrow \mathbb{R}^+$. 
The use of potentials permits a change of measure from the proposal from Markov process \eqref{eq:proposal_measure} to the FKM distribution \eqref{eq:fk_measure}. This provides a great deal of flexibility for controlling generation.
\begin{align}
    &M(\mathrm{d}x_{0:T}) = M_0(\mathrm{d}x_0)\prod_{t=1}^{T}M_t(\mathrm{d}x_t|x_{t-1}) \label{eq:proposal_measure} \\ 
    &Q(\mathrm{d}x_{0:T}) \propto G_0(x_0)\prod_{s=1}^{T}G_s(x_s,x_{s-1})M(\mathrm{d}x_{0:T}) \label{eq:fk_measure} 
\end{align}
\textbf{SMC for Feynman Kac Models.}
The FKM \eqref{eq:fk_measure} can be approximately simulated with SMC or particle filtering. An SMC procedure entails propagating $N$ particles initially sampled from some distribution $M_0$ through a sequence of proposal; importance weighting and resampling steps. The resampling steps are crucial to ensuring computation is focused on promising particles, and to avoiding weight degeneracy.
 
\begin{algorithm}
        \caption{Generative SMC}
        \label{alg:pf}
        \setlength{\parindent}{0pt}
        \begin{algorithmic}
            \State{Sample $\bfX_{0}^{i}\stackrel{\text{i.i.d.}}{\sim} M_0$ ~for $i \in [N]$}
           \State{Weight $\omega_1^{i}=G_0(\bfX^{i}_0)$~for $i \in [N]$}
        \For{$t=1,...,T$}
            \State{Normalize weights $w^i_{t-1}\propto \omega^i_{t-1}$, $\sum_{i=1}^N  w^i_{t-1}=1$}
            \State{Resample $\tilde{\bfX}^i_{t-1}\sim \sum_{i=1}^N  w^i_{t-1} \delta_{\bfX^i_{t-1}}$ ~for $i \in [N]$}
                \State{Proposal $\bfX_{t}^{i}\sim M(\cdot|\tilde{\bfX}^i_{t-1})$  ~for $i \in [N]$}
                \State{Weight $\omega^i_t= G(\bfX_{t}^{i},\tilde{\bfX}^i_{t-1})$}
        \EndFor
        \State{{\bfseries Return:} samples $(\bfX_{T}^{i})_i$}
        \end{algorithmic}
        \end{algorithm}

\textbf{Guided}. Given conditioning signal $y$, a guided FKM uses proposal and potentials:
\begin{align}
    M_t(x_t|x_{t-1})&=q(x_t|x_{t-1},y) \\ G_t(x_t,x_{t-1}) &= \tfrac{p(y|x_{t})p(x_t|x_{t-1})}{q(x_t|x_{t-1},y)}
\end{align}

\textbf{Twisted}.
\begin{align}
    M_t(x_t|x_{t-1})&=q(x_t|x_{t-1},y) \\ G_t(x_t,x_{t-1}) &= \tfrac{p(y|x_{t})p(x_t|x_{t-1})}{p(y|x_{t-1})q(x_t|x_{t-1},y)}
\end{align}