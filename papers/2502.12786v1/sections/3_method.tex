
\input{plots/2d_density}
\newpage
\section{Distilled Energy Diffusion Models} \label{sec:training_methods}
\subsection{Training Instability with Energy Parameterised Diffusion Models}


Denoising score-matching \citep{vincent2011connection, song2019generative} is a promising simulation-free alternative to contrastive learning for training energy based models \citep{salimans2021should, song2021trainebm}. We call DSM with an energy-parameterised score E-DSM. Although E-DSM never quite learns exactly the energy at time $t=0$; it can approximate the energy arbitrarily close and been used successfully in generative modelling \citep{du2023reduce, salimans2021should} and sampling \citep{phillips2024particle}.

Unfortunately, E-DSM suffers from training instability, as shown in \Cref{fig:loss_stability}. We attribute this instability due two effects: firstly that DSM has a high variance loss \citep{jeha2024variance}, and secondly network architecture limitations. 
Unlike in regular DSM or contrastive training,  E-DSM requires taking gradients of $E_\theta$ with respect to both the state and parameters i.e. $\nabla_\theta\nabla_x E_\theta(x,t)$ during training. The effective network in E-DSM, $-\nabla_x E_\theta(x, t)$, may be viewed informally as the composition of $E_\theta: \mathbb{R}^d \rightarrow \mathbb{R}$ and operation $\nabla_x: \mathbb{\rset} \rightarrow \rset^d$. The second operation, $\nabla_x$ lacks any normalisation or residual connections common in modern unconstrained neural networks typically used for diffusion models. Such stability measures have been shown to be crucial \citep{Karras2024edm2, karras2022elucidating} to ensuring stable training and generative performance.

We tackle both weaknesses by first providing a more stable loss, and secondly with careful parameterisation and initialization of the energy-diffusion network.

\begin{figure}[H]
    \centering
        \includegraphics[trim={0 0.cm 0 0},clip,width=\linewidth]{plots/loss_stability/cifar_train_loss_stability-5.png}
    \caption{E-DSM loss (blue): $\mathbb{E}\|D_\theta(\bfX_t,t)- \bfX_0\|$ without gradient clipping vs distillation loss (orange): $\mathbb{E}\|D_\theta(\bfX_t,t)- D^{teach}_{\phi}(\bfX_{t},t)\|$ during training of a diffusion model for CIFAR10. Initial $100$ iterations cut.} 
    \label{fig:loss_stability}
\end{figure}

\subsection{Distillation Loss}
We first introduce the conservative projection loss as: 
\begin{align} \label{eq:conservative_proj}
    \arg\min_{\theta} \mathbb{E}_{p_{t}}[\|\nabla u_\theta(\bfX_{t},t) - v(\bfX_{t},t)\|^2],
\end{align}
where $u_\theta$ is some flexibly parameterised function, such as a neural network. A conservative vector field is the gradient of some potential. The loss \eqref{eq:conservative_proj} aims to learn a potential $u_\theta$ and hence conservative vector field $\nabla u_\theta$ which is closest in squared Euclidean distance to $v$, not necessarily conservative.

Consider ODEs generated by $v$ and the minimizer of \eqref{eq:conservative_proj}, $u_{\theta^*}$:
\begin{align}
    \mathrm{d}\bfX_t &= v(\bfX_t,t)\mathrm{d}t & 
    \mathrm{d}\bfX_t &= u_{\theta^*}(\bfX_t,t)\mathrm{d}t. \label{eq:given_vf}
\end{align}
Directly using \cite[Theorem 5.2]{liu2022rectified}, if $v$ is locally bounded, and \eqref{eq:given_vf}(left) has a unique solution generating density $p_t$, then the marginals of ODEs in \eqref{eq:given_vf} coincide. Minimising \eqref{eq:conservative_proj} may therefore be considered a type of Helmholtz decomposition of $v$, where the ``rotation-only'' component of $v$ is removed, discussed further in \Cref{app:conservative}.

Note: a more general class of Bregman Helmoholtz losses have been considered in the seminal work of \citet{liu2022rectified} in the context of rectified flows.

\textbf{Distilling a Score into an Energy}.
In the context of training energy based models, we consider $u_\theta = -E_\theta$. Ideally we would use $v=\nabla \log p_t$, in such a case \eqref{eq:conservative_proj} would simply be (non-denoising) score-matching \citep{hyvarinen05a}. We do not have access to $\nabla \log p_t$ so instead use a pre-trained score-function $s^{teach}_{\phi}$, i.e. $v(\bfX_{t},t)=s^{teach}_{\phi}$ as proxy:
\begin{align} \label{eq:score_energy_distill}
    \arg\min_{\theta} \mathbb{E}_{p_{0,t}}[\|\nabla E_\theta(\bfX_{t},t) + s^{teach}_\phi(\bfX_{t},t)\|^2].
\end{align}
By the arguments above, we do not require $s^{teach}_{\phi}$ be conservative, as the minimizer of \eqref{eq:score_energy_distill} will generate the same distribution as $s^{teach}_{\phi}$ via the probability flow ODE and hence generate a distribution close to the data distribution, if $s^{teach}_{\phi}$ is well trained.

 \textbf{Expressed as Denoising}. The loss \eqref{eq:score_energy_distill} may equivalently be written as a denoising loss with target $D^{teach}_{\phi}(x_{t},t) \approx \mathbf{E}[\bfX_0|x_t]$, as in \eqref{eq:denoising_distill} where again by Tweedie's formula, one may write $s^{teach}_{\phi}(x{t},t)$ in terms of denoiser $D^{teach}_{\phi}(x_{t},t)=\alpha_t^{-1}[x_t+\sigma_t^2 s^{teach}_{\phi}(x_{t},t)]$. 
\begin{align} \label{eq:denoising_distill}
    \arg\min_{D_\theta} \mathbb{E}_{p_{0,t}}[\| D_\theta(\bfX_{t},t) - D^{teach}_{\phi}(\bfX_{t},t)\|^2].
\end{align}
The corresponding distilled denoiser is related to the energy through $D_\theta(x_t,t)= \alpha_t^{-1}[x_t\sigma_t^2 -\nabla E_{\theta}(x_{t},t)]$.

Losses \eqref{eq:denoising_distill} and \eqref{eq:denoise_loss} differ only in the regression target: $D^{teach}_{\phi}(\bfX,t) \approx \mathbf{E}[\bfX_0|x_t]$ vs $\bfX_0$ respectively. Given $\mathbf{E}[\bfX_0|x_t]$ is the minimizer of \eqref{eq:denoise_loss} and at large time $t$, $\mathbf{E}[\bfX_0|x_t]$ is quite different to $\bfX_0$, and hence intuitively targeting $D^{teach}_{\phi}(\bfX,t) \approx \mathbf{E}[\bfX_0|x_t]$ directly results in more stable training than \eqref{eq:denoise_loss}.

 Motivated by the desire to reduce training instability, we have framed this distillation loss in terms of learning energy-parameterised diffusion models; however, the same loss could also be applied to train unconstrained diffusion models through distillation.


\begin{figure*}[hb]
    \centering
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/ground_truth.png}
        \caption{Ground Truth}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_-0.05.png}
        \caption{$\gamma=-0.05$}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_-0.04.png}
        \caption{$\gamma=-0.04$}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_0.png}
        \caption{$\gamma=0.0$}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_0.05.png}
        \caption{$\gamma=0.05$}
    \end{subfigure}% \
    \\
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_0.1.png}
        \caption{$\gamma=0.1$}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_0.2.png}
        \caption{$\gamma=0.2$}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_0.5.png}
        \caption{$\gamma=0.5$}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_1.0.png}
        \caption{$\gamma=1.0$}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.19\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/fractal/fractal_plot_5.0.png}
        \caption{$\gamma=5.0$}
    \end{subfigure}%
    \caption{\textbf{SMC Sampling of Feynman Kac Diffusion Models} for $G_i(x_{t_{i}},x_{t-1})=\exp\{-\gamma_{t_i} E_\theta(x_{t_i}\}\approx p(x_{t_i})^\gamma_{t_i}$. The fractal distribution, inspired by~\citet{karras2024guiding}, is obtained by fitting Gaussian mixtures to each branch and appending recursively. Ground truth samples shown in (faded) blue and generated samples shown in orange.}   
    \label{fig:temp_tree}
\end{figure*}
\textbf{Scores are approximately conservative}.
Similar to \citet{lai2023fp}, we observe that well-trained score networks are approximately conservative, except at close to $t=0$, where the score exhibits a Lipschitz singularity~\citep{yang2023lipschitz}, see \Cref{fig:afhq_asymmtery}. Here \textit{conservativity} of a score network may be quantified by the asymmetry of its Jacobian (see \Cref{app:conservative}).

\begin{figure}[H]
    \centering
        \includegraphics[trim={0 0.1cm 0 0},clip,width=\linewidth]{plots/conservative/afhq_asymmtery-4.png}
    \caption{Asymmetry metric in log-scale, $\|\jac - \jac^T\|^2$ where Jacobian $\jac=\mathbf{D}_x s_\theta(x_t,t)$ of score network $s_\theta$ trained via DSM on AFHQv2-$64$, $t\in[0,1]$.} 
    \label{fig:afhq_asymmtery}
\end{figure}


\newpage
\subsection{Parameterization and Initialization}
\textbf{Pre-conditioning}.
To further reduce training instability of energy-parameterised diffusion models; we use the preconditioning ( $c_\text{s}, c_{out}, c_\text{in}, c_\text{t}$) of \citet{karras2022elucidating}, parameterizing the denoiser, $D_\theta$ as:
\begin{align*}
    D_\theta(x_t,t) &= c_\text{s}(t)x_t + c_{out}(t)\nabla_x F_\theta(c_\text{in}(t)x_t,c_\text{t}(t)).
\end{align*} 
We replace the unconstrained network in \cite{karras2022elucidating} with the gradient of network $F_\theta$. By Tweedie \citep{efron_tweedie}, we compute the energy via:
\begin{align*}
    E_\theta(x_t,t) &= \tfrac{1-\alpha_t c_\text{s}(t)}{2\sigma_t^{2}}\|x_t\|^2
    -\tfrac{\alpha_t c_{out}(t)}{c_\text{in}(t)\sigma_t^{2}}F_\theta(c_\text{in}(t)x_t,c_\text{t}(t)).
\end{align*}
\textbf{Network parameterization}. We parameterize the network $F_\theta$ such that its gradient takes a similar network structure to score/ denoising networks. Consider a network architecture, $h_\theta$, known to work well for DSM, such as a U-Net for image-based diffusion models. The following parameterization forces $\nabla_x F_\theta$ to resemble $h_\theta(x_t,t)$ with the addition of a residual term, where $\mathbf{D}_x h_\theta$ denotes Jacobian of $h_\theta$:
\begin{align} \label{eq:parameterisation}
    F_\theta(x_t,t) &= h_\theta(x_t,t)\cdot x_t\\
    \nabla_x F_\theta(x_t,t) &= x_t \cdot \mathbf{D}_x h_\theta(x_t,t)+h_\theta(x_t,t)
\end{align}
This structure is important for initialization. 

\textbf{Network initialization}. Motivated by recent successes in initializing models with pre-trained diffusions \citep{lee2024improving, kim2024simple} we do the same for our energy-parameterized network. In particular, we set  $h_\theta$ to be the same architecture as the teacher network $D^{teach}_{\phi}$ and initialize with teacher parameters $\theta \gets \phi$. This simple technique is effective due to choice in parameterization \eqref{eq:parameterisation}, and results in drastically faster convergence (see \Cref{app:experiments}) and better generative performance overall.

\begin{figure*}[!t]
   \centering \includegraphics[trim={0 13cm 0 2.3cm},clip,width=\linewidth]{plots/composition/toy_composition_filled.pdf} 
 \caption{Simple 2D Composition failure from \cite{du2023reduce}. Top row: Learnt densities, $e^{-E^{(i)}_\theta}$ for each $p^{(i)}_{t}$ and $e^{-E^{(1)}_\theta-E^{(2)}_\theta}$. Bottom row: generated samples per $p^{(i)}_{t}$, as well as the SMC generation using \eqref{eq:comp_fkm} and reverse diffusion of summed scores,  ${q}_{\theta, \lambda}^{(1)+(2)}(x_{t_{0:N}})$.}
 \label{fig:toy_composition}
\end{figure*}
\newpage
\section{Composition and Control} \label{sec:comp_cont}
\subsection{Feynman Kac Diffusion Models}
As discussed in \Cref{sec:background}, the Feynman Kac model (FKM) formalism \citep{chopin2020introduction, del2004feynman} is a simple yet principled framework to perform an approximate change of measure from a given Markov process according to a user-provided potentials. Given diffusion models are expensive to train, it is desirable to use pretrained models. 

Denote the discretization of the generative diffusion process \eqref{eq:time_reversed} as $q^\lambda(x_{t_{0:N}})$, and the network approximation as $q_\theta^\lambda(x_{t_{0:N}})$ in \eqref{eq:diff_markov} for any ODE/ SDE solver on discretisation {${T=t_0 \geq t_1 \geq \ldots \geq t_N=0}$}. We choose the underlying Markov measure for a FKM to be $q^\lambda_\theta$ or similarly, some conditioned process $q^\lambda_\theta(\cdot|y)$ for conditioning signal $y$ e.g. labels. Explicit forms of ${q}_\theta^\lambda(x_{t_{i+1}}|x_{t_{i}})$ are given in \Cref{app:diffusion_samplers}.
\vspace{-0.1cm}
\begin{align}\label{eq:diff_markov}
    &{q}_\theta^\lambda(x_{t_{0:N}}) = q^\lambda_{t_0}(x_{t_0})\prod_{i=0}^N {q}_\theta^\lambda(x_{t_{i+1}}|x_{t_{i}}) 
\end{align}
The intermediate FKM distributions by running \Cref{alg:gen_smc} with potentials $(G_i)_i$ and Markov process \eqref{eq:diff_markov} for $n\leq N$ are then given by:
\vspace{-0.3cm}
\begin{align}\label{eq:fkm_diff}
    &Q(x_{t_{0:n}}) \propto {q}_\theta^\lambda(x_{t_{0:n}})G_0(x_{t_0})\prod_{i=1}^{n}G_i(x_{t_i},x_{t_{i-1}})
\end{align}
\vspace{-0.5cm}
\subsection{Temperature-Controlled Generation}
Let $\gamma_t \in \mathbb{R}$ be some time-indexed inverse temperature parameter. Given access to density $p_{t_i}$, one could set $G_i(x_{t_i}, x_{t_{i-1}})=p(x_{t_i})^{\gamma_{t_i}}$. Rearranging \eqref{eq:diff_markov} gives:
\begin{align*}
    Q(x_{t_{0:N}}) &\propto \prod_{i=0}^{N}p(x_{t_i})^{\gamma_{t_i}}q^\lambda(x_{t_{0:N}}). 
\end{align*}
Recall from \Cref{sec:background}, regardless of choice of $\lambda$, the marginals of the backward and forward process match, $q^\lambda(x_{t})=p(x_{t})$ for any $t$, hence:
\begin{align*}
    Q(x_{t_{0:N}}) &\propto p(x_{t_N})^{1+\gamma}q^\lambda(x_{t_{0:N-1}}|x_{t_N})\prod_{i=0}^{N-1}p(x_{t_i})^\gamma  \label{eq:diff_fk_measure} 
\end{align*}
\Cref{fig:temp_tree} illustrates how setting $\gamma \geq 0$ results a more concentrated distribution, $p(x_{t_N})^{1+\gamma}$. Similarly,  $\gamma<0$ results lower density regions being sampled with greater probability. This is biased toward rare events.  

In practice, we substitute $G_i(x_{t_{i}},x_{t-1}) \gets e^{-\gamma_{t_i} E_\theta(x_{t_i})}$ as an approximation, proportional to $p(x_{t_i})^\gamma_{t_i}$. 

It is common to proxy low temp. generation with high guidance weights, as detailed in \Cref{sec:background}. Our SMC approach provides an alternative which may be used with unconditional, conditional models. An example demonstrating low-temperature generation is shown in \Cref{exp:low_temp} for conditional image generation. 

\subsection{Compositional Generation}\label{sec:composition_scores}
EBMs enable composition of pretrained models  as logical operators can be expressed as functions of the score and energy \citep{compose_ebm, compose_diffusion}, see \Cref{app:composition}. We focus here on the \textsc{AND} operation. 

Unlike for EBMs the summed scores from diffusion models at $t>0$ do not always match the score for the composed distribution \citep{du2023reduce}. Consider time indexed densities $p^{(1)}_{t}$ and $p^{(2)}_{t}$, and denote the composed score:
\begin{equation}\label{eq:compose_score}
    s^{(1)+(2)}_t(x_t, t) = \nabla \log p^{(1)}_{t}(x_t) + \nabla \log p^{(2)}_{t}(x_t)
\end{equation}
Let ${q}_{\lambda}^{(1)+(2)}(x_{t_{0:N}})$ denote the process \eqref{eq:time_reversed} with composed score \eqref{eq:compose_score}, and consider FKM given by \eqref{eq:comp_fkm}.
\begin{align}
    &M_t(x_{t_{i+1}}|x_{t_{i}})= q^\lambda_{t_0}(x_{t_0})\prod_{i=0}^N {q}_{\lambda}^{(1)+(2)}(x_{t_{i+1}}|x_{t_{i}}) \label{eq:comp_fkm}\\ &M_t(x_{t_{i+1}}|x_{t_{i}})G_i(x_{t_{i+1}}, x_{t_{i}}) = p^{(1)}_{t}(x_{t_{i+1}})p^{(2)}_{t}(x_{t_{i+1}}). \notag
\end{align}
In practice we use the approximation ${q}_{\theta, \lambda}^{(1)+(2)}(x_{t_{0:N}})$ defined by summing the trained score functions for $p^{(1)}_{t}$ and $p^{(2)}_{t}$, and similarly approximate, up to a scalar, each $p^{(i)}_{t}$ with $e^{-E^{(i)}_\theta(\cdot,t)}$. Expanding the FKM intermediate distributions for this choice of $G$ in \eqref{eq:fkm_diff} gives a sequence of product densities coinciding with the target density we wish to generate. A simple example of this is illustrated in \Cref{fig:toy_composition}, note that the density is given by the energy function, and hence reverse diffusion does not provide a density directly.

\subsection{Bounded Generation}\label{sec:bounded} 
We consider how constraints \citep{lou2023reflected, fishman2024metropolis, fishmandiffusion}  and dynamic thresholding \citep{saharia2022photorealistic} can be imposed via FKM potentials. \cite{fishman2024metropolis} adjusts sampling by rejecting transitions if proposals $x_{t_i}$ fall outside a specified region, $B$. This resembles a FKM with potential $G_i(x_{t_i}, x_{t_{i-1}})= \mathbb{I}_B(x_{t_i})$. Similarly, dynamic thresholding entails clipping the denoiser to be within a unit-cube at generation time. This also resembles a FKM setting $G_i(x_{t_i}, x_{t_{i-1}})= \mathbb{I}_{[-1+\delta,1-\delta]^d}(D_\theta(x_t,t))$, $\delta>0$.

The above choices of regions are simple, but quite crude. One could instead use the energy as a softer alternative, for example by choosing $G_i(x_{t_i}, x_{t_{i-1}})=\exp\{-\gamma_tE_\theta(x_t,t)\}$ or $G_i(x_{t_i}, x_{t_{i-1}})=\exp\{-\gamma_tE_\theta(D_\theta(x_t,t),\epsilon)\}$, $\epsilon\approx0$. See generated examples in \Cref{app:fkm_potentials}. 




