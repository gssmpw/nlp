
\begin{figure}[b]
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/afhqv2-disill.png}
        \caption{Distilled, Ours}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/afhqv2-edsm.png}
        \caption{E-DSM}
    \end{subfigure}%
    \caption{Samples from AFHQv2-$64$ using energy parameterized diffusion models trained with our distillation method vs denoising score-matching (E-DSM)} 
    \label{fig:afhq}
\end{figure}
\section{Experiments}
Full experimental details including network architectures and training recipes are provided in \Cref{app:experiments}.
\subsection{2D Experiments}

\textbf{E-DSM vs Distillation}. \Cref{fig:2d_density} provides qualitative comparison of our distillation approach compared to E-DSM, We used a feedforward network with sine nonlinearity. There is a general uneven density exhibited within E-DSM, not present in our approach.

\textbf{Temperature Controlled Generation.} Figure~\ref{fig:temp_tree} illustrates the effects of temperature on the fractal dataset. We are able to generate either the high density regions in low temperature regime, and to target low density regions (at the boundary of the support) with higher temperatures.   

\textbf{Compositional Generation.} \Cref{fig:toy_composition} demonstrates the failure of reverse diffusion in composition. Our SMC based composition approach however correctly recovers the joint distribution $p_1(x)p_2(x)$. A similar result has been observed in \citet{du2023reduce}, where they correct with MCMC rather than SMC.


\subsection{Generative Performance of Diffusion-Energy Models}
We first verify our improvements for training energy-parameterized diffusion models in terms of generative performance, measured by Frechet Inception Distance (FID) \citep{heusel2017gans} on standard EBM datasets: CIFAR-10 \citep{cifar} and CelebA-64 \citep{celeba} for both unconditional generation in \Cref{tab:unconditional_gen} and conditional generation in \Cref{tab:conditional}. In the interest of transparency, we display the Number of Function Evaluations (NFE) involved in the generative process. We further showcase our method with AFHQv2-64 \citep{afhq}, FFHQ-64 \citep{ffhq} in \Cref{tab:afhq} and visually in \Cref{fig:afhq}. We compare to recent energy-parameterized approaches \citep{gao2020learning, zhu2023learning, hill2023tackling, schroder2024energy}, E-DSM, and popular methods such as diffusion \citep{karras2022elucidating} and flow-matching \citep{lipman2022flow, liu2022rectified}. Note: here we do not use SMC but standard generation with the gradient of the learnt energy.

Our method exhibits significantly better performance than other baselines using an energy-parameterization, particularly for CIFAR10 and AFHQv2. Although we achieve only modest performance improvement verses E-DSM on face-datasets, we note that the E-DSM results took significantly longer to train and required careful selection of learning rate and gradient clipping for stability, see \Cref{app:experiments}.
\input{tables/fid_table}
\vspace{-0.5cm}
\subsection{Composition of Image Models} \label{exp:img_comp}
As detailed in \Cref{app:experiments}, we train separate energy-parameterized diffusion models for subsets of the CelebA dataset with attributes \textit{male} and \textit{glasses}, $p^{(\text{male})}_{t}$ and $p^{(\text{glasses})}_{t}$. We compose pretrained models via SMC detailed in \Cref{sec:composition_scores}, using potential 
$G_i = \left(\tfrac{p^{(\text{male})}_{t_i}p^{(\text{glasses})}_{t_i}}{M_{t_i}}\right)^{\gamma_{t_i}}$ with schedule $(\gamma_t)_t$ to control diversity.
\begin{figure}[hb]
    \centering
        \includegraphics[trim={3.cm 1.2cm 3cm 1.5cm},clip,width=0.75\linewidth]{plots/celeba_composition/celeba_composition16.png}
    \caption{Composition: Male \textsc{AND} Glasses.} 
    \label{fig:celeba_composition}
\end{figure}
% \vspace{-0.5cm}
\Cref{fig:celeba_composition} shows a uniform subsample from generated batch of size $64$, qualitatively showing our method works for image datasets. Repetition indicates resampling can reduce diversity however.

\subsection{Low Temperature Sampling}\label{exp:low_temp}
We train a conditional energy-diffusion model on CelebA, generate $128$ samples for multiple conditions using a base sampler and via low temperature (temp.) SMC with inverse temp. parameter $\gamma_t=0.1$, then assess images-condition adherence using a CLIP score, detailed in \Cref{app:experiments}. \Cref{tab:clip_celeba} shows the superior performance of low temp. SMC sampling for condition adherence.
\input{tables/celaba_clip_table}

