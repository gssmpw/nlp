% \documentclass[twoside]{article}

% \usepackage{aistats2025}
% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
%\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

% \begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
\onecolumn
\runningtitle{Supplementary Material}

\section{On the conservativity of score networks}\label{app:conservative}

\subsection{Conservative Projection}
The conservative projection detailed in \Cref{sec:training_methods} results in the Helmholtz decomposition of the pre-trained score vector field. Generally, consider a vector field $v_t: \mathcal{X} \rightarrow \mathcal{X}$, providing certain conditions hold \citep{liu2022rectified}, then one may decompose $v_t(x_t)=\nabla_x f_t(x_t)+ r_t(x_t)$ where $r_t: \mathcal{X} \rightarrow \mathcal{X}$, $f_t(x_t): \mathcal{X} \rightarrow \rset $ and $\nabla \cdot r_t =0$.

\citet[Theorem 5.2]{liu2022rectified} proves a more general case coined the Bregman Helmholtz decomposition for convex $c: \mathcal{X} \rightarrow \rset$ and conjugate $c^*(x) := \sup_y\{x\cdot y - c(y)\}$, that the optimal $f^*$ of \eqref{eq:convexloss} for vector field $v_t$: 
\begin{align}\label{eq:convexloss}
    \inf_f \int c(v_t(X_t))-v_t(X_t)\cdot g_t(X_t) + c^*(g_t(X_t))\mathrm{d}t && g_t = \nabla c^* \odot \nabla f_t
\end{align}
yields an orthogonal decomposition: $v_t = \nabla c^* \odot \nabla f^*_t + r_t$ where $r_t$ is measure preserving. This implies that given the same initialization stochastic processes defined by vector fields $v_t$ and $\nabla c^* \odot \nabla f^*_t$ have the same marginals.

Our particular case \eqref{eq:conservative_proj} is a specific case of minimizing \eqref{eq:convexloss} for squared Euclidean cost $c(x)=c^*(x)=\tfrac{1}{2}\|x\|^2$; $\nabla c^*(x)=x$. Ideally, if it were available we would use $v_t(x_t)=\nabla \log p_t(x_t)$; but in practice we use $v_t(x_t)=s_\theta(x_t,t)$, for some pre-trained score function $s_\theta$. This specific case of flow matching on a score vector field is also remarked in \citep[Sec 5]{liu2022rectified}, but in a different context.

\subsection{Measuring conservativity}
By Poincare's star shaped lemma, if a function $g: \rset^d \rightarrow \rset^d $ on star-shaped support has a symmetric Jacobian, then there exists function $F: \rset^d \rightarrow \rset$ such that $g = \nabla F$. Let $\jac$ denote some Jacobian matrix. The Jacobian is typically too large and expensive to compute in full, instead the asymmetry of the Jacobian may be approximated efficiently using Hutchinson's trace estimator: $\|\jac - \jac^T\|^2 = \expect_{\nu \sim \mcn(\mathbf{0}, \Ibb)}\|\nu^T\jac - \jac\nu\|^2$,
\begin{align*}
    \textrm{trace}\left((\jac - \jac^T)^T(\jac - \jac^T)\right) 
    & = \expect_{\nu \sim \mcn(\mathbf{0}, \Ibb)} \nu^T\left(\jac - \jac^T)^T(\jac - \jac^T)\right)\nu \\
    &= \expect_{\nu \sim \mcn(\mathbf{0}, \Ibb)}\|\nu^T(\jac - \jac^T)\|^2 \\
    &= \expect_{\nu \sim \mcn(\mathbf{0}, \Ibb)}\|\nu^T\jac - \nu^T\jac^T\|^2 \\
    &= \expect_{\nu \sim \mcn(\mathbf{0}, \Ibb)}\|\nu^T\jac - \jac\nu\|^2
\end{align*}
where $\nu^T\jac$ and $\jac\nu$ may be computed efficiently with vector-Jacobian and Jacobian-vector products. A Monte Carlo approximation is used in \Cref{fig:afhq_asymmtery} $\tfrac{1}{n}\sum_{i=1}^n \|\nu_i^T\jac - \jac\nu_i\|^2$, and \Cref{fig:conservat} is normalized.


\begin{figure}[H]
    \centering
    \includegraphics[trim={0 0.1cm 0 0},clip,width=0.3\linewidth]{plots/conservative/2d_conservative.png}
    \includegraphics[trim={0 0.1cm 0 0},clip,width=0.3\linewidth]{plots/conservative/cifar10_conservative.png}
    \includegraphics[trim={0 0.1cm 0 0},clip,width=0.3\linewidth]{plots/conservative/celeba_conservative.png}
    \caption{Asymmetry metric  approximation  $\tfrac{\|\jac - \jac^T\|^2}{\|\jac\|^2}\approx \tfrac{\sum_{i=1}^n \|\nu_i^T\jac - \jac\nu_i\|^2}{\sum_{i=1}^n \|\nu_i^T\jac\|^2}$ for $\nu_i \sim \mcn(\mathbf{0}, \Ibb)$ where $\jac=\mathbf{D}_x s_\theta(x_t,t)$ of score network $s_\theta$ trained via DSM on 2D spiral (left) CIFAR1O (middle) and CelebA (right).} 
    \label{fig:conservat}
\end{figure}

\section{Feynman Kac Model Potentials}\label{app:fkm_potentials}
We detail here a few other Feynman Kac Model (FKM) potentials, alluded to in \Cref{sec:training_methods}.

\subsection{Bounded Generation}
By setting $G_i(x, y) = \mathbb{I}_B(x)$, one forces the generative trajectory to be within region $B\subset \mathcal{X}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{plots/bounded/bounded_right.png}
    \includegraphics[width=0.4\linewidth]{plots/bounded/bounded_tree_up.png}
    \caption{SMC sampling of 2D fractal dataset with FKM potential $G_i(x, y) = \mathbb{I}_B(x)$. Left: $B=[0.25,1] \times \rset$. Right: $B=\rset\times[0.25,1] \cup \times[-1.,-0.1]$. Here blue faded dots are the ground truth data and green points are generated by SMC for the FKM.}
    \label{fig:bounded}
\end{figure}

\subsection{Twisted SMC for Conditional Generation}
\cite{wu2024practical} consider the setting of a pretrained unconditional diffusion model and with to sample from a conditional model. They first use classifier guidance whereby the guidance term is approximated using DPS \citep{chungdiffusion}, then use SMC to correct guided diffusion models.
In the ideal setting the corresponding FKM may be expressed as:
\begin{align}
    M(x_{t_{i+1}}| x_{t_{i}}) &= q^{\lambda}(x_{t_{i+1}}|x_{t_{i}}) \\
    G_0(x_{t_{0}}) &= p(y|x_{t_{0}}) \\
    G_t(x_{t_{i+1}}, x_{t_{i}}) &= \frac{p(y|x_{t_{i+1}})q^\lambda (x_{t_{i+1}}|x_{t_{i}})}{p(y|x_{t_{i}})q^{\lambda}(x_{t_{i+1}}|x_{t_{i}},y)}
\end{align}
where $p(y|x_t)$ denotes a classifier on noisy data for label $y$.

\cite{wu2024practical} assumes the setting where one does not have access to $p(y|x_t)$ or $q^{\lambda}(x_{t_{i+1}}|x_{t_{i}},y)$ but has access to a trained classifier $p_\theta(y|x_0)$ on data without noise; a rained denoiser $D_\theta$, found from the score model, and trained unconditional score model $s_\theta$ to approximate $q^{\lambda}(x_{t_{i+1}}|x_{t_{i}})$ as follows:

\begin{align}
    p^{DPS}_\theta(y|x_{t_{i}}) &= p_\theta(y|D_\theta(x_{t_{i}},t_{i})) \\
    s^{DPS}_\theta(x_{t_{i}},t_{i}, y) &= s_\theta(x_{t_{i}},t_{i})+\nabla_{x_t}\log p^{DPS}_\theta(y|x_{t_{i}}) \\ 
    q_\theta^{DPS,\lambda}(x_{t_{i+1}}|x_{t_{i}},y ) &= \mathcal{N}(x_{t_{i}}+ \Delta_i \frac{\lambda^2+1}{2}[-f(t_{i})x_{t_{i}}+g^2(t_{i})s^{DPS}_\theta(x_{t_{i}},t_{i}, y)], \Delta_i \lambda^2 g(t_{i})^2\mathbb{I})
\end{align}

This yields the guided FKM model:
\begin{align}
    M(x_{t_{i+1}}| x_{t_{i}}) &= q_\theta^{DPS,\lambda}(x_{t_{i+1}}|x_{t_{i}},y) \\
    G_0(x_{t_{0}}) &= p^{DPS}_\theta(y|x_{t_{0}}) \\
    G_t(x_{t_{i+1}}, x_{t_{i}}) &= \frac{p^{DPS}_\theta(y|x_{t_{i+1}})q_\theta^\lambda (x_{t_{i+1}}|x_{t_{i}})}{p^{DPS}_\theta(y|x_{t_{i}})q_\theta^{DPS,\lambda}(x_{t_{i+1}}|x_{t_{i}},y)}.
\end{align}


\section{Sampling Diffusion Models}\label{app:diffusion_samplers}
% markov kernel from sde
% markov kernel from sde

Let the step size be denoted $\Delta_i = (t_{i+1}-t_{i})$. The simplest implementation of the reverse process \eqref{eq:time_reversed} is:
\begin{align}
    q_\theta^\lambda (x_{t_{i+1}}|x_{t_{i}}) &= \mathcal{N}(x_{t_{i}}+ \Delta_i [-f(t_{i})x_{t_{i}}+g^2(t_{i})\frac{\lambda^2+1}{2}s_\theta(x_{t_{i}},t_{i})], \Delta_i \lambda^2 g(t_{i})^2\mathbb{I})
\end{align}
There exist other more advanced solvers including DDIM (stochastic) \citep{song2020denoising} and a plethora of ODE solvers and higher order SDE and ODE solvers such as the Heun solver used in \citet{karras2022elucidating}.

\section{Composition}\label{app:composition}
Logical compositional operations - \textit{AND}, \textit{OR}, \textit{NOT} - of EBMs may be implemented by transforming the density for EBMs as follows \citep{du2023reduce, duvisual, compose_diffusion}:

\textbf{AND}: $p^\text{AND} \propto \prod_i p^{(i)}$. \textbf{OR}: $p^\text{OR} \propto \sum_i p^{(i)}$. \textbf{NOT}: $p^\text{NOT} \propto \frac{p^{(1)}}{(p^{(2)})^\alpha}$, for some weight $\alpha>0$, depending on density.

Hence one may approximately sample the densities corresponding to each operation via Langevin dynamics. The score is sufficient for \textbf{AND} operations, but the density itself is needed for \textbf{OR} and \textbf{NOT} operations. However, due to score matching learning unnormalised densities, summing such densities for \textbf{OR} composition may be theoretically problematic. In practice this can be resolved with heuristic weighting.

As noted by \citet{du2023reduce}, arithmetic operations of scores at $t>0$ do not in general recover the noisy score corresponding to the composed scores at $t=0$; i.e. $p^\text{AND}_t = \prod_i p^{(i)}_t \neq \int (\prod_i p^{(i)}) \mathrm{d}p_{t|0}$.

 This can lead to a failure in reverse diffusion for compositional generation. One may instead use annealed Langevin dynamics to target $p^\text{AND}_t$ and then gradually anneal $t\rightarrow 0$ to get approximate samples of $p^\text{AND}_0$ \citep{du2023reduce}.

In this work we propose an alternative but complementary approach to annealed Langevin dynamics, in using SMC to target a sequence of distributions which also gradually converge to the desired composition. We specifically target the \textit{AND} operation, but other operations can be targeted similarly. Here the density is required for all logical operations.

Consider schedule $(\gamma_{t_i})_{i=1}^N:$, where $\gamma_{t_N}=1$ and FKM model with potentials:
\begin{align}
    G_i = \tfrac{(p^{(1)}_{t_i}p^{(2)}_{t_i})^{\gamma_{t_i}}M_{t_i}^{1-\gamma_{t_i}}}{M_{t_i}}.
\end{align}
The resulting FKM distributions are
\begin{align}\label{eq:fkm_diff_comp}
    Q(x_{t_{0:N}}) \propto M_{t_{0:N}}(x_{t_{0:N}})G_0(x_{t_0})\prod_{i=1}^{N}G_i(x_{t_i}|x_{t_{i-1}}) 
    = \prod_{i=1}^{N}(p^{(1)}_{t_i}p^{(2)}_{t_i})^{\gamma_{t_i}}M_{t_i}^{1-\gamma_{t_i}} = p^{(1)}_{t_N}p^{(2)}_{t_N} \left[\prod_{i=1}^{N-1}(p^{(1)}_{t_i}p^{(2)}_{t_i})^{\gamma_{t_i}}M_{t_i}^{1-\gamma_{t_i}}\right], 
\end{align}
hence recovers the desired marginal $p^{(1)}_{t_N}p^{(2)}_{t_N}$ at time $t_N$.

\newpage
\section{Experimental Details}\label{app:experiments}
\subsection{Training Details}

\textbf{Compute}. All experiments were carried out using $A100$ 40GB GPUs on single nodes of up to $8$ GPUs. 

\textbf{Training Parameters}
For pre-training via DSM, the learning rates $0.0002$ was used for AFHQv2, CelebA and FFHQ, learning rate $0.001$ is used for CIFAR10. The same learning rates were used for distilling the denoisers into energy-parameterised models.

For E-DSM, $0.0002$ learning rate was used for CIFAR10, with gradient clipping gradients above norm of $10$, other E-DSM clipping was at norm of $1$.

All experiment used a linear warm-up learning rate schedule for $10_000$ steps.

CIFAR10 experiments used batch size $512$, all other image experiments used batch size $256$.

EMA was held constant at a rate of $0.9992$ for all experiments.

\textbf{Network Parameters}:
We used the \textit{SongNet} NCSNPP network as per \citep{karras2022elucidating, song2021scorebased} with the following hyper parameters:
\input{tables/network_params}

\textbf{Sampling}. Generative modelling results using energy parameterized diffusions \cref{tab:conditional}, \cref{tab:unconditional_gen}, \cref{tab:afhq} follow \citep{karras2022elucidating} using the Heun solver on the ODE where $\lambda=0$. 

For low temperature sampling we can also set $\lambda=0$ given the marginals for all $\lambda$ coincide, we do not require to divide by the transition density in the FKM potential. For compositional sampling where we require transition density, we use $\lambda=1$.

\textbf{Evaluation}. Frechet Inception distance \citep{heusel2017gans} was used as the evaluation metric based on Inceptionv3 features \citep{inception} using a re-implementation based on \citep{Seitzer2020FID}.

\newpage
\subsection{Composition}
We train two separate models on subsets of the data, which may overlap for labels "Eyeglasses=1" and "Male=1". We then run SMC using the weighted compositional FKM detailed in \Cref{app:composition}, with $\gamma_t=0.01$; to avoid collapsing to a single sample due to weight degeneracy at time $t=0$; we do not resample for $t<0.1$. It has been observed \citep{karras2024guiding} that conditioning primarily occurs in the middle of the diffusion trajectory and towards $t=0$ all visual features are present but the image is simply sharpened.

The full batch of 64 images, which were then sub-sampled to show in the main document in \Cref{fig:celeba_composition} are given below:
\begin{figure}[h]
    \centering
        \includegraphics[trim={3.cm 1.2cm 3cm 1.5cm},clip,width=0.75\linewidth]{plots/celeba_composition/celeba_composition64.png}
    \caption{Composition: Male \textsc{AND} Glasses.} 
    \label{fig:celeba_composition64}
\end{figure}

\newpage
\subsection{Low Temperature Generation}
In order to test condition adherence we perform regular sampling of \eqref{eq:time_reversed} with $\lambda=0$ and then using SMC with low temperature weighting setting $\gamma_t=0.1$. We consider the following 5 cases in the \Cref{tab:celeba_attrs}.
\input{tables/celeba_atts}

We then compute the average CLIP score \citep{radford2021learning} for each of a batch of $128$, where the image is up-sampled to $224$, and the following text descriptions are used:
\begin{itemize}
\item "a photo of a young woman with no hair"
\item "a photo of a man wearing make-up"
\item "a photo of an older man who is smiling"
\item "a photo of a blonde woman with lipstick"
\item "a photo of a man with black hair"
\end{itemize}

\subsection{Faster Convergence with Distillation}
Human face datasets are somewhat less diverse than AFHQv2 or CIFAR10, so we notice E-DSM does not struggle as much. Although E-DSM and our distilled approaches yield similar final FID scores for CelebA and FFHQ datasets, the time to train is a significant factor motivating our method. 

Given the careful initialization and pretrained model we notice that our distilled training yields good performance with relatively few training steps. 

With FFHQ-64, after $30,000$ training iterations, E-DSM from scratch yields FID scores of $7.69$, DSM yields FID of $6.18$ and our distilled model yields scores of $3.36$. It takes approx. $200,000$ iterations for E-DSM to reach FID score below $3.3$.

At $30,000$ iterations of training on the unconditional CelebA dataset; E-DSM achieves FID of $5.88$, DSM of $5.18$ and our distilled approach of $2.92$. It takes a further approx. $100,000$ iterations for E-DSM and DSM to reach FID below $3$.

\section{Licences}

\begin{itemize}
    \item JAX Apache-2.0 license \citet{jax2018github}
    \item CelebA: non-commercial research purposes \citep{celeba}
    \item CIFAR-10: MIT license \citep{cifar}
    \item FFHQ: Creative Commons BY-NC-SA 4.0 license \citep{ffhq}
    \item AFHQv2: Creative Commons BY-NC 4.0 license \citep{afhq}
    \item Inception-v3 model: Apache V2.0 license \citep{inception}
    \item CLIP:  MIT license \citep{radford2021learning}
\end{itemize}
\vfill

