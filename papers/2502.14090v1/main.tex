\documentclass[10pt, conference, letterpaper]{IEEEtran}

% Packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{url}
\usepackage{multirow}
\bstctlcite{IEEEexample:BSTcontrol}

\usepackage{amsfonts}

\usepackage{xcolor}
\newcommand{\new}[1]{\textcolor{black}{#1}}


% Title and Author
\title{MambaLiteSR: Image Super-Resolution with Low-Rank Mamba using Knowledge Distillation}

% \author{
%     \IEEEauthorblockN{Romina Aalishah, Mozhgan Navardi, Tinoosh Mohsenin}
% }

\author{\IEEEauthorblockN{Romina Aalishah, Mozhgan Navardi, and Tinoosh Mohsenin}
\IEEEauthorblockA{Department of Electrical and Computer Engineering\\
Johns Hopkins University\\
\{raalish1, mnavard1, tinoosh\}@jhu.edu}
}





\begin{document}

\maketitle

\begin{abstract}
%\new{Use this command if you want to modify the paper. Do not make significant changes. While we are preparing Camera Ready (final version of the paper after acceptance), we are allowed to fix typos and apply reviewers comments if there is any.}
Generative Artificial Intelligence~(AI) has gained significant attention in recent years, revolutionizing various applications across industries. Among these, advanced vision models for image super-resolution are in high demand, particularly for deployment on edge devices where real-time processing is crucial. However, deploying such models on edge devices is challenging due to limited computing power and memory. In this paper, we present MambaLiteSR, a novel lightweight image Super-Resolution~(SR) model that utilizes the architecture of Vision Mamba. It integrates State Space Blocks and a reconstruction module for efficient feature extraction. To optimize efficiency without affecting performance, MambaLiteSR employs knowledge distillation, transferring essential information from a larger Mamba-based teacher model to a smaller student model through hyperparameter tuning. Through a mathematical analysis of model parameters and their impact on the Peak Signal-to-Noise Ratio~(PSNR), we identify key factors and adjust them accordingly. Our comprehensive evaluation shows that MambaLiteSR outperforms state of the art edge SR methods by reducing power consumption while maintaining competitive PSNR and SSIM scores across benchmark datasets such as Set5, Set14, and BSD100. It also reduces the power usage during training by adopting low-rank approximation. Moreover, MambaLiteSR reduces the total number of parameters without degrading performance, enabling the efficient deployment of generative AI models on resource-constrained devices. Deployment on the embedded NVIDIA Jetson Orin Nano confirms the superior balance of MambaLiteSR size, latency, and resource efficiency. The experimental results show that MambaLiteSR achieves performance comparable to both the baseline and other edge models while using 15\% fewer parameters than the baseline. It also improves the power consumption by up to \new{58\%} compared to state-of-the-art SR edge models, all while maintaining low energy consumption during training.
\end{abstract}

% Keywords (optional)
\begin{IEEEkeywords}
Image Super-Resolution, Mamba, Knowledge Distillation, Low-Rank Approximation, Edge Computing
\end{IEEEkeywords}

\section{Introduction}
%system definition and challenges
Generative Artificial Intelligence~(AI) models have gained attention in recent years for their ability to generate outputs such as images and text based on input data~\cite{genai, transformer, llmaug}. These models learn the patterns and structures of training data and use them to produce results. Image Super-Resolution~(SR) is one such task in Generative AI and computer vision. The goal is to generate a High-Resolution~(HR) image from a Low-Resolution~(LR) one~\cite{yu2023review}. An efficient SR model achieves high Peak Signal-to-Noise Ratio~(PSNR) and Structural Similarity~(SSIM); PSNR measures the reconstruction quality by comparing the pixel-wise difference between the HR output and the ground truth, while SSIM evaluates the perceptual similarity by considering structural information, luminance, and contrast. However, deploying these models on edge devices might be challenging because of their complexity and high intensiveness~\cite{fatrabbit, kallakuri2024resource, navardi2024metatinyml, rashid2024tinym, bao2025decentralised, reprohrl, mlae2}. Since edge devices are constrained in several aspects such as computing resources and power, the importance of model size, number of parameters, and FLOPs comes into matter~\cite{navarrete2022edge, wang2024eshp, mazumder2024reg, meta, yolo}. 

%Early SR approach and the challenge
\new{Deep learning models play a crucial role in applications such as image processing~\cite{water}. Two commonly used architectures in deep learning are Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs).} Early SR approaches relied on CNNs~\cite{srcnn, vdsr, fsrcnn}. While small, these CNN-based methods demonstrated relatively low PSNRs. Therefore, efforts have been made to improve the performance~\cite{rcan, edsr, denoiser} by removing unnecessary modules, bypassing low-frequency information, and integrating denoising as a prior. Recently, research has focused on developing smaller and faster models with novel architectures, attention mechanisms, and hybrid models~\cite{eshr, esr, mafdn}.

%state of the art SR and advan
With the introduction of Mamba~\cite{mamba}, a revolution happened in optimizing feature extraction. This architecture was quickly adopted across various vision tasks, including image classification, object detection, and semantic segmentation~\cite{visionmamba, mambavision}. Having similar results to previous works but with better processing speed, led to its adoption in SR frameworks, where it involved combining the lightweight structure of Mamba with attention mechanisms and transformer-based modules~\cite{lei2024dvmsr, himamba, srmamba}. However, even though Mamba improved the latency, model deployment on edge devices is still challenging because of its size and power usage.

%presetn MambaLiteSR
To address these challenges, we propose MambaLiteSR, which builds on previous studies by enhancing the existing Mamba-based architecture with significant improvements. In this paper, we focus on minimizing the model size and power usage during inference and training while maintaining comparable performance by optimizing the loss function, \new{utilizing low-rank Mamba, knowledge distillation, and carefully fine-tuning hyperparameters}. After finalizing the architecture and loss function, we employed low-rank approximation, which is widely used in machine learning studies such as LoRA~\cite{lora}, to decrease the power usage during training. Then, knowledge distillation was applied to reduce the model size. To the best of our knowledge, no existing work has explored the simultaneous use of Mamba, knowledge distillation, and low-rank approximation for optimizing SR generative AI models on edge devices. In this context, we enhanced the Vision~Mamba-based model to improve performance, energy usage while training, and compactness, ensuring it fits seamlessly on edge devices. To evaluate the proposed MambaLiteSR, experiments were conducted on key knowledge distillation parameters to achieve optimal results. The experimental results demonstrate that MambaLiteSR achieves comparable performance to baseline and edge models while being 15\% smaller than the baseline. For edge-device evaluation, the model was tested on the embedded NVIDIA Jetson Orin Nano~\cite{orin}. The results show that our model achieves comparable PSNR and SSIM with the state-of-the-art edge models while consuming less dynamic power.
Our contributions for the proposed MambaLiteSR are summarized as follows:

\begin{itemize}
    \item Improvement of the image super-resolution model to achieve comparable performance by optimizing the loss function, utilizing knowledge distillation, and carefully fine-tuning its hyperparameter.
    \item Assessment of the matrix ranks used in Mamba architecture on model size, performance, and training power usage, as well as the distillation parameter $\alpha$ on model performance.
    \item Real-world experiment by deployment of the proposed MambaLiteSR on embedded NVIDIA Jetson Orin Nano, with comparisons made against similar models in terms of power usage and performance.
\end{itemize}

\section{Related Work}
Image super-resolution has been a major task in computer vision, with early approaches relying on interpolation techniques such as bicubic scaling~\cite{jahnavi2024study, navarrete2022edge, chen2015train}. While these methods are computationally efficient, they lack adaptability to image features and often produce artifacts like blurriness and jagged edges in images. Advances like SRCNN~\cite{srcnn} and FSRCNN~\cite{fsrcnn} introduced learnable features through CNNs, significantly improving SR performance. However, these models are computationally intensive and might be challenging for real-time deployment on resource-constrained devices.

To address efficiency concerns, lightweight SR models such as ESPCN~\cite{shi2016real}, DVMSR~\cite{lei2024dvmsr}, and SRMamba-T~\cite{srmamba} have been proposed. ESPCN utilizes sub-pixel convolution layers to reduce computational overhead, but it struggles to maintain high reconstruction quality at larger scales. DVMSR, which outperformed RLFN~\cite{rlfn}, the winner of the NTIRE 2023 Efficient Super-Resolution Challenge~\cite{ntire}, utilizes Vision Mamba~\cite{zhu2024visionmamba} modules and state space blocks to balance efficiency and accuracy. SRMamba-T combines Mamba and Transformer architectures to balance computational efficiency with high performance.

Frameworks like ESHP~\cite{wang2024eshp} extend these advancements by leveraging heterogeneous hardware to optimize SR tasks. ESHP dynamically allocates CPU, GPU, and NPU resources using deep reinforcement learning. However, its dependence on specialized hardware ecosystems introduces complexity and limits deployment flexibility. Edge-SR~\cite{navarrete2022edge} proposes lightweight one-layer architectures for real-time applications. Although practical for constrained devices, its performance is often inferior to more advanced multi-layer networks. Similarly, thermal imaging SR pipelines~\cite{mathur2021real} and facial verification systems~\cite{perez2023efficient} focus on specific use cases but are not flexible enough for wider SR applications.

The proposed MambaLiteSR further advances these foundations by integrating low-rank Mamba architecture and a knowledge distillation framework, offering a unified solution to the limitations of prior methods. Unlike simpler interpolation‐based techniques, MambaLiteSR dynamically adapts to complex image features for improved SR quality. Meanwhile, its design addresses the memory and computational constraints typically faced by CNN‐based or transformer‐heavy approaches, reducing reliance on specialized hardware. By managing parameter usage and supporting real‐time deployment, MambaLiteSR provides an effective, flexible solution for lightweight SR on edge devices.

\begin{figure}[t]
	\centering
	\includegraphics[width=.45\textwidth]
         {Images/model.png}
	\caption{Architecture of DVMSR \cite{lei2024dvmsr} and Vision Mamba \cite{visionmamba}: Input image is preprocessed and fed into the DVMSR model, which consists of Vision Mamba modules, convolution layers, and a decoder.}
	\label{model}
\end{figure}

\section{Background}

Image super-resolution aims to reconstruct a high-resolution image from a low-resolution input while keeping details. Traditional methods cannot recover high-frequency details, leading to blurry results~\cite{freq, problem}. Deep learning-based SR models, particularly those utilizing CNN and Transformer-based architectures~\cite{srcnn, fsrcnn, srmamba}, have significantly improved performance. However, these models often require large computational resources, making deployment on edge devices challenging. In this section, various parts contributing to the reduction in model size and power usage are explored. In addition, strategies for improving the performance of small models are discussed.

\textbf{Mamba and State Space Models} The Mamba~\cite{mamba} architecture introduces Selective State Space Models, a lightweight alternative to Transformers, that reduces computational complexity while maintaining feature extraction capabilities. The state space representation in Mamba is formulated as follows:

\begin{equation}
    \mathbf{y}(t) = \mathbf{C} \mathbf{x}(t) 
\end{equation}
\begin{equation}
    \frac{d}{dt} \mathbf{x}(t) = \mathbf{A} \mathbf{x}(t) + \mathbf{B} \mathbf{u}(t)
\end{equation}

where $\mathbf{x}(t)$ represents the hidden state, $\mathbf{u}(t)$ is the input signal and $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ are learnable matrices. This structure enables Mamba to capture long-range dependencies efficiently while requiring fewer parameters than traditional self-attention mechanisms. As a result, several efforts have been made to apply this method across various tasks. Vision Mamba~\cite{visionmamba} is such an effort that utilizes the efficient architecture of Mamba for computer vision tasks like image classification. Later, DVMSR~\cite{lei2024dvmsr} was proposed, utilizing Mamba Vision in the image super-resolution task. Figure~\ref{model} demonstrates the architecture of DVMSR and Vision Mamba, showing the contribution of Vision Mamba to DVMSR.

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=.45\textwidth]
%          {Images/model.png}
% 	\caption{Architecture of DVMSR \cite{lei2024dvmsr} and Vision Mamba \cite{visionmamba}: Input image is preprocessed and fed into the DVMSR model, which consists of Vision Mamba modules, convolution layers, and a decoder.}
% 	\label{model}
% \end{figure}

\textbf{Embedding Dimension} The embedding dimension significantly influences the number of parameters in the model. For an SR model, the number of parameters in a single layer can be estimated as:

\begin{equation}
    P = d_{in} \times d_{out} + d_{out}
\end{equation}

where $d_{in}$ and $d_{out}$ are the input and output embedding dimensions, respectively. Reducing the embedding dimension from $d_{base}$ to $d_{small}$ leads to a reduction in parameter count by a factor of:

\begin{equation}
    \frac{d_{small}^2}{d_{base}^2}
\label{eq:embed_dim}
\end{equation}

% For example, if the baseline embedding dimension is 192 and is reduced to 60, the parameter reduction is computed as:

% \begin{equation}
%     \frac{60^2}{192^2} \approx 0.097 
% \end{equation}

% indicating a roughly 10-fold reduction in parameters.

\textbf{Low-Rank Approximation} Low-rank approximation is used to further compress the model by reducing the rank of weight matrices while preserving essential information. Given a weight matrix $\mathbf{W} \in \mathbb{R}^{m \times n}$, a low-rank factorization approximates $\mathbf{W}$ as:

\begin{equation}
    \mathbf{W} \approx \mathbf{U} \mathbf{V}^T
\label{eq:rank}
\end{equation}

where $\mathbf{U} \in \mathbb{R}^{m \times r}$ and $\mathbf{V} \in \mathbb{R}^{n \times r}$, with rank $r \ll \min(m, n)$. This leads to a reduction in the number of computations, FLOPs, and less power usage.
% Multiplying a dense \( m \times n \) matrix with a vector takes \( O(mn) \) operations. If the matrix is low-rank, it is calculated:

% \begin{equation}
% \
% Ax = (UV^T)x = U(V^T x)
% \
% \end{equation}

% First, computing \( V^T x \) in \( O(nr) \),  
% then computing \( U(V^T x) \) in \( O(mr) \) result in the total cost of: 

% \begin{equation}
% \
% O(mr + nr)
% \
% \end{equation}

% Now, if the matrix is \( n \times n \), the complexity would be:

% \begin{equation}
% \
% O(nr + nr) = O(2nr)
% \
% \end{equation}

% To ensure that the cost is less than \( O(n^2) \), we require:

% \begin{equation}
% \
% O(2nr) < O(n^2)
% \
% \end{equation}

% which simplifies to:

% \begin{equation}
% \
% r < \frac{n}{2}
% \
% \end{equation}

% Thus, if the rank of the matrix is less than \( n/2 \), the computational cost of matrix-vector multiplication is less than \( O(n^2) \), confirming the reduction. Therefore, ranks smaller than  \( n/2 \) result in fewer computations and FLOPs, and less power usage.

% For instance, if $r$ is set to 30 instead of 60, the reduction ratio is:

% \begin{equation}
%     \frac{30(60+60)}{60 \times 60} = \frac{3600}{3600} = 0.5
% \end{equation}

% indicating a 50\% reduction in parameter size while maintaining near-optimal performance.

\textbf{Knowledge Distillation} Knowledge distillation is employed to transfer knowledge from a larger teacher model to a smaller student model to have a similar performance with fewer parameters. This process introduces a trade-off between the soft targets of the teacher and the ground truth labels, controlled by the parameter $\alpha$. The loss function in knowledge distillation is defined as:

\begin{equation}
L = \alpha L_{KD} + (1 - \alpha) L_{GT}
\end{equation}

where $L_{KD}$ is the distillation loss calculated using teacher soft predictions, and $L_{GT}$ is the loss using ground truth labels. A higher $\alpha$ puts more emphasis on the teacher’s outputs. In contrast, a lower $\alpha$ puts the student to rely more on the ground truth, which may limit the benefits of distillation. In summary, tuning $\alpha$ is to balance model efficiency.

% \textbf{Optimization Strategies for Edge Deployment}  Additionally, knowledge distillation is employed to transfer knowledge from a larger teacher model to a smaller student model to have a similar performance with fewer parameters. 





\section{Proposed Approach}
To design an efficient image super-resolution model, we build upon a \new{Vision Mamba-based architecture} while optimizing its structure and using knowledge distillation. The goal is to maintain the performance of SR models while reducing computational complexity and model size and simplifying it for deployment on edge devices. By combining the efficient modeling of Mamba, embedding dimension adjustments, low-rank approximations, and tuning hyperparameters, MambaLiteSR achieves a balanced trade-off between model size, computational efficiency, and SR performance, making it suited for deployment on resource-constrained devices. In this section, we go over the main aspects and materials of our work. Experimental results show that this combination of parameters balancing allows for a substantial reduction in model size and energy usage while preserving high reconstruction accuracy.

Figure~\ref{high_level} presents our high-level diagram, which includes the model, knowledge distillation process, and parameters of interest. The flow consists of the following key components:




\begin{figure*}[ht]
	\centering
	\includegraphics[width=.9\textwidth]
         {Images/high_level_final.png}
	\caption{High-Level overview of MambaLiteSR process: Low-resolution input image~($64 \times 64$) is preprocessed and fed into the knowledge distillation process, generating the high-resolution output~($256 \times 256$). Weighted distillation and student losses enable the student model to learn efficiently under teacher supervision. Proper embedding dimension, which determines the feature vector size resulting from the image patches, makes it suitable for edge devices.}
	\label{high_level}
\end{figure*}

\subsection{Embedding Dimension}
The embedding dimension is a crucial factor in model efficiency, which directly affects the number of parameters and computational cost. By reducing the embedding dimension from 192 to 60 for the model, we achieve a substantial reduction in memory and power usage, which according to the Equation~\ref{eq:embed_dim}, leads to a 10 times reduction of the model size. Experimental results confirm that this reduction in embedding dimension does not significantly impact reconstruction quality and allows for efficient deployment on edge devices.

In this study, images are cropped into smaller patches. Specifically, from each image, eight mini-patches of $64 \times 64$ are extracted randomly. To further preprocess the data, random horizontal and vertical flips and rotations are applied. Then, each of these $64 \times 64$ patches is converted into a vector of 60 and 32 features, for teacher and student models respectively.

\subsection{Large Teacher Model}
The input image is first passed through a convolutional layer, which acts as a shallow feature extractor to derive basic feature maps. These features then go through a set of Residual Mixed Mamba Blocks~(RMMB), which consist of several Vision Mamba modules designed to capture long-range dependencies. Each RMMB internally contains a basic layer with a configurable number of Blocks, each Block has exactly one Mamba mixer inside and An internal residual convolution and skip connection as a residual group.

Therefore, RMMBs perform deep feature extraction and contain multiple Vision Mamba Modules along with a convolution layer and a residual connection. In the proposed large teacher model, the Mamba layer is low-rank to reduce the number of computations. After extracting deep features, a global residual connection is applied to fuse shallow features from the initial feature map with deep features. These features are then up-sampled by a method of pixel-shuffle to create the final HR output. To summarize, there are 16 Mamba layers in the teacher model with an embedding dimension of 60, which will be covered in the following subsections. 

\subsection{Tiny Student Model}
To improve efficiency, a student model is trained with a lower number of RMMBs and a smaller embedding dimension. The student model follows a similar architecture to the teacher but with reduced parameters. Knowledge distillation allows the student to mimic the performance of the teacher while being more computationally efficient. In this work, it has 4 RMMBs, resulting in 8 Mamba layers, with an embedding dimension of 32. The student model is optimized to keep essential features while achieving comparable PSNR and SSIM. Note that the input to both the teacher and student model is the same and they go through the same flow of preprocessing.

% \subsection{Loss Block}
\subsection{Loss Block} As stated in the Equation~\ref{eq:alpha}, $\alpha$ controls the trade-off between distillation and direct supervision, and tuning it ensures the optimal balance between performance and model efficiency. In the total loss function, L1 loss for both distillation and student loss is considered. L1 loss measures the average difference between predicted and actual values. We write the combined loss as:

\begin{equation}
L = \alpha L_1(y_{student}, y_{teacher}) + (1 - \alpha) L_1(y_{student}, y_{GT})
\label{eq:alpha}
\end{equation}

By assigning a proper value for $\alpha$, each of the two loss terms is weighted to optimize the trade-off between performance and model size.

\subsection{Low-Rank Mamba}
Another key strategy is the low-rank approximation in the Mamba mixers. This reduces complexity and energy usage during training while maintaining model performance by factoring each $dim \times dim$ weight into $dim \times rank$ and $rank \times dim$. By reducing the rank from $dim / 2$, where $dim$ stands for the matrix dimension, we can ensure the reduction in computations. However, the rank layers are such a small fraction of the overall architecture, that changing ranks does not materially affect the total parameter count. But at runtime, the larger rank requires more FLOPs~(more multiplications/additions), so it draws more power under load. Therefore, rank mostly impacts computation rather than overall parameter storage. As a result, to reduce the energy usage and the number of parameters between layers, a low-rank approximation is applied.

In this work, we fine-tune these two hyperparameters: the distillation weight $\alpha$ and the rank used in low-rank approximation. These parameters are adjusted based on experimental evaluations to achieve an optimal trade-off between efficiency and performance. 





\section{Experimental Result}
In this section, we begin by presenting the experimental setup, which includes details about the dataset and the implementation. Then, we evaluate the proposed LightMambaSR model, focusing on its size, performance, and power consumption in comparison to state-of-the-art methods.

%In this section, we first presestn experimnetal setup inculidn the dataset and implementation details. Then, we evaluate the proposed LightLambdaSR model size, performance and power consumption in comparison of the state-of-the-art work. 

\subsection{Experimental Setup}


% \subsection{Datasets}
\textbf{Datasets.} \new{In this work, we used the DF2K~(DIV2K + Flickr2K)~\cite{div2k} dataset, which consists of 3450 high-resolution images and their corresponding low-resolution versions. The low-resolution images are generated by downscaling each image by factors of 2, 3, and 4.}
%In this work, the DF2K~(DIV2K + Flickr2K)~\cite{div2k} dataset was used. 
%This dataset consists of 3450 high-resolution images and their corresponding low-resolution versions, which were downscaled by factors of 2, 3, and 4. 
%For this research, since upscaling with higher factors are harder problem for networks to fix and typically requires more parameters to learn, the experiments are done only on the scale of 4. 
\new{To train the proposed model, we consider a scale of 4 to do the experiments as it presents a more challenging problem and requires more parameters to learn. Moreover, for the validation set, a subset of the  DIV2K dataset consisting of 100 images was used. For testing the accuracy and performance of the proposed approach, we utilized four standard benchmark datasets: Set5~\cite{set5}, Set14~\cite{set14}, BSD100~\cite{bsd100}, and Urban100~\cite{urban100}.}
%As for the validation set, 100 images exist for each of the low-resolution and high-resolution images of DIV2K. For the test, 4 standard benchmark datasets were used: Set5~\cite{set5}, Set14~\cite{set14}, BSD100~\cite{bsd100}, and Urban100~\cite{urban100}.

% \subsection{Implementation Details}
\textbf{Implementation Details.} For faster processing during training, images are cropped into smaller patches. Specifically, from each \new{LR image eight mini-patches of $64~\times~64$ and from each HR image eight corresponding mini-patches of $256~\times256$} are extracted randomly. To further preprocess the data, random horizontal and vertical flips and rotations are applied. The model is trained using the Adam optimizer. The batch size is set to 128 and the training process takes 2500 iterations. The initial learning rate is set at~$2 \times 10^{-4}$ and is halved when the training iteration reaches specific milestones. For the embedding dimension of images, it is considered to be 60 to reduce the model size. For evaluation, we calculate PSNR and SSIM metrics on the Y channel in the YCbCr color space. All \new{training} experiments are conducted on Lambda GPU Server~\cite{lambda}, which utilizes NVIDIA GeForce RTX 4090 GPU.

Experiments are conducted with various reduced matrix ranks including ranks 30 and 2. To further reduce the model size, knowledge distillation is applied, the same process as DVMSR~\cite{lei2024dvmsr}, but with different values and hyperparameters. For this purpose, the embedding dimension is %lowered
reduced to 32. %, which results in a 4-times reduction compared to our teacher model size, which was also 10 times smaller than the baseline model, as well as a reduction in the number of RMMB blocks. 
To assess the performance of different values of $\alpha$ parameter in knowledge distillation, an experiment was conducted comparing the result of changing this value over 1000 iterations for each of the teacher and student models. 


% \vspace{-10mm}

%\subsection{Hardware Experimental Setup}
For the \new{inference} power consumption measurements, we deploy the model on the embedded Nvidia Jetson Orin Nano board~~\cite{orin} as shown in Figure~\ref{instant_power}.(a). \new{Power measurements experiments are repeated over 1000 samples, and the average is calculated. The Jetson Orin Nano Developer Kit with 8GB of memory features a hexa-core ARM Cortex-A78AE CPU with 1.5 GHz frequency and a 512-core NVIDIA Ampere GPU with 625 MHz. This board is optimized for AI tasks like matrix multiplications and deep learning inference, making it ideal for edge AI applications~\cite{vitreg}, besides boards such as Jetson Orin Nano~\cite{nano}.} We converted the software models to ONNX format and optimized them into TensorRT representations for GPU-accelerated inference. \new{The frequency was set to 612~MHz with the power mode being set to 7~Watt.} During inference, the tegrastats utility \new{\cite{tegrastats}} was used to measure instantaneous power. 

\begin{figure}
	\centering
	\includegraphics[width=.45\textwidth]
         {Images/power_pure_drawio_2.png}
	\caption{(a) For measuring dynamic power on the embedded NVIDIA Jetson Orin Nano, the student onnx model is converted to TensorRT format and then the measurement starts. (b) The plot shows the instantaneous power usage over time for 1000 samples, extracted using the tegrastats utility~\cite{tegrastats}. %Note that the command may introduce minor inaccuracies on the time axis.
    }
	\label{instant_power}
\end{figure}

\subsection{Results} 

\new{In this section, we compare the proposed MambaLiteSR model with the state-of-the-art methods, including eSR~\cite{esr}, ESPCN~\cite{shi2016real}, and FSRCNN~\cite{dong2016accel}, in terms of performance and power consumption. While ESPCN and FSRCNN were not originally designed for edge deployment, they were evaluated on edge devices in the experiments conducted in eSR. Next, we present the model optimization and training results to analyze the impact of various variables of knowledge distillation and Mamba architecture on training and overall performance. Moreover, we compare the proposed MambaLiteSR with the baseline~\cite{lei2024dvmsr}.
%We, then, compare our model against the baseline. 
Finally, we demonstrate the performance of the proposed tiny student model by comparing the model output with the ground truth image to demonstrate its ability to achieve similar quality.}

\begin{table*}[ht]
  \caption{Comparison of image quality and performance metrics of MambaLiteSR to existing methods implemented on the edge device Jetson AGX Xavier GPU~\cite{xavier}. 
  MambaLiteSR improves the power consumption by up to \new{58\%} to implement while performing similarly. The value of the power of the proposed approach is specific to the embedded NVIDIA Jetson Orin device. The reported power is the average of 1000 samples. Experiments were conducted on a scale of 4.}
  \label{tab:result}
  \centering\medskip
\resizebox{\linewidth}{!}{
  \begin{tabular}{lcclrrccccccccc}\hline
    \multirow{2}{*}{Algorithm} & Power & \multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14} & \multicolumn{2}{c}{BSDS100} & \multicolumn{2}{c}{Urban100} \\
                             % & $\text{[FHD/s]}$
                             & $\text{[mWatts]}$ & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\ \hline
    eSR~\cite{navarrete2022edge} 
    % & 13
    & 7100 & 30.62 & 0.860 & 27.48 & 0.751 & 26.93 & 0.714 & 24.42 & 0.718\\
    eSR - fast~\cite{navarrete2022edge} 
    % & 94
    & 3867 & 28.64 & 0.806 & 26.12 & 0.712 & 26.13 & 0.684 & 23.28 & 0.668\\
    ESPCN~\cite{shi2016real} 
    % & 23
    & 6952 & 30.57 & 0.858 & 27.50 & 0.752 & 26.92 & 0.715 & 24.42 & 0.718\\
    % ESPCN & 4 & PSNR & 29 & 4640 & 30.59 & 0.859 & \textbf{27.53} & \textbf{0.753} & \textbf{26.95} & 0.715 & 24.43 & 0.719\\
    % ESPCN & 4 & speed & 45 & \textbf{3096} & 28.93 & 0.820 & 26.49 & 0.725 & 26.25 & 0.694 & 23.56 & 0.680\\
    FSRCNN~\cite{dong2016accel} 
    % & 12
    & 4795 & 30.16 & 0.845 & 27.19 & 0.742 & 26.74 & 0.707 & 24.09 & 0.702\\
    % FSRCNN & 4 & PSNR & 9 & 5257 & 30.61 & \textbf{0.861} & 27.52 & \textbf{0.753} & 26.94 & \textbf{0.716} & \textbf{24.44} & \textbf{0.721}\\
    % FSRCNN & 4 & speed & 14 & 3715 & 29.31 & 0.823 & 26.62 & 0.730 & 26.41 & 0.699 & 23.62 & 0.683\\
    MambaLiteSR (Ours)
    % & \textbf{4}
    & \new{\textbf{2957}} & 28.28 & 0.837 & 25.39 & 0.727 & 25.20 & 0.693 & 22.57 & 0.686 \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
  \end{tabular}
}
\end{table*}


Figure~\ref{instant_power}.(b) illustrates the power consumption results on the embedded NVIDIA Jetson Orin Nano during inference. Experiments are done on the \new{proposed MambaLiteSR} tiny student model to evaluate the power and PSNR/SSIM results, as summarized in Table~\ref{tab:result}. The idle power consumption was measured at approximately \new{0.9~W}, and then, the system initiated image pre-processing. Following this, the model execution and image input processing caused a \new{3.3}$\times$ increase in power consumption. % before stabilizing for inference. 
% \new{For the speed report, a latency of 14~ms for one inference was first obtained. Based on that, the speed in the format of Full-HD per seconds~(FHD/s), considering the high resolution of 256$\times$256 was calculated.} 
The deployment results of the proposed MambaLiteSR model, compared against state-of-the-art methods on edge devices, are detailed in Table~\ref{tab:result}. The power measurement was averaged over 1000 samples, \new{resulting in a fast single image latency of 14~ms. The results demonstrate that the proposed approach improved the power consumption by up to  %achieves up to %3.4$\times$ 
58\% %lower power consumption 
while maintaining SSIM and PSNR performance comparable to existing methods. \new{Note that the compared methods are all CNN-based. While CNNs extract structured features, generative AI models such as ours can create more detailed and realistic images by generating missing information. Such models handle complex transformations better, especially for large scaling factors, and produce sharper textures by learning perceptual features instead of just minimizing pixel differences.}}

% {\begin{figure*}[ht]
% 	\centering
% 	\includegraphics[width=.9\textwidth]
%          {Images/ranks.png}
% 	\caption{Comparison between MambaLiteSR teacher model training when $rank = 2$ and $rank = 30$ over 1500 iterations: (a) depicts validation PSNR. (b) depicts the moving average of GPU power usage every 100 seconds with a window size of 100. At runtime, the larger rank requires more FLOPs, drawing more power under load. The training outcome and model performance act similar because, as indicated by equation~\ref{eq:rank}, the matrix ultimately remains the same. The reported power measurements correspond to the NVIDIA GeForce RTX 4090 on Lambda GPU Server~\cite{lambda} using wandb~\cite{wandb} dashboard.}
% 	\label{ranks}
% \end{figure*}}

%\textbf{7} & \textbf{2104} & 28.28 & 0.837 & 25.39 & 0.727 & 25.20 & 0.693 & 22.57 & 0.686 \\


To reduce the number of parameters to make the model suitable for edge deployment the embedding dimension of \new{60 and 32 to the teacher and student model is applied respectively}, which leads to the 15$\%$ reduction in the number of parameters while keeping the performance acceptable with comparison with the Baseline~\cite{lei2024dvmsr}. The proposed LightMambaSR includes 370k parameters with a PSNR of 28.28 while the baseline has a 424k parameters with a PSNR of 32.19. There is a difference in PSNRs, which due to the increasing trend of the training we expect to achieve the same baseline PSNR as they did the experiments for 500K iterations as shown in Table~\ref{tab:dvmsr}.

\begin{table}[h]
    \centering
    \caption{Comparison of the number of parameters and PSNR usage for the baseline and MambaLiteSR at a scale factor of 4. The PSNR results are evaluated on the Set5 dataset.}  
    \begin{tabular}{llll}
        \hline
        Model & Parameters & iterations & PSNR  \\
        \hline
        % Baseline~\cite{lei2024dvmsr} & 192 & 424k & 500,000 & 32.19 \\
        DVMSR~\cite{lei2024dvmsr}  & 424k & 500,000 & 32.19\\
        MambaLiteSR  & \textbf{370k (15\% smaller)} & 2500 & 28.28\\
        \hline
    \end{tabular}
    \label{tab:dvmsr}
\end{table}

%32.19
%28.28

Table~\ref{tab:rank_batch} presents the rank configurations of MambaLiteSR along with their results. It can be seen that there is a negligible difference between the PSNRs of $rank = 2$ and $rank = 30$. As a result, $rank = 2$ is chosen to allow for 1.7x less energy consumption besides good performance.

\begin{table}[ht]
    \centering
    \caption{Comparison of PSNR and power usage for different ranks at a scale factor of 4. Configs 1 and 2 use the same configuration, varying only in rank and belonging to the teacher model. It can be seen that the lower-rank model consumes around 42\% less power than the higher-rank model. The reported power measurements are averaged values obtained during the training of the teacher model on the Lambda GPU Server, and the PSNR results are evaluated on the validation dataset.}  
    \begin{tabular}{llllll}
        \hline
        Config. & Rank & Parameters & iterations & PSNR & Power (W) \\
        \hline
        % Baseline~\cite{lei2024dvmsr} & 192 & 424k & 500,000 & 32.19 \\
        Config 1   & \textbf{30} & 370k & 1500 & 28.88 & \textbf{103}\\
        Config 2   & \textbf{2} & 370k & 1500 & 28.81 & \textbf{60} \textbf{(1.7x less)}  \\
        \hline
    \end{tabular}
    \label{tab:rank_batch}
\end{table}

%28.28
%28.08

The performance of different values of rank can be seen in Figure~\ref{ranks}, which shows the validation results during teacher model training for rank values of 2 and 30, along with the moving average of their corresponding GPU power usage during training on the Lambda GPU Server~\cite{lambda}, with all other settings kept the same. While both models demonstrate similar training behavior and performance, the model with the lower rank demonstrates more efficient training; when $rank = 2$, GPU power usage is 42\% less than the one with $rank = 30$.

\begin{figure*}[ht]
	\centering
	\includegraphics[width=.9\textwidth]
         {Images/ranks.png}
	\caption{Comparison between MambaLiteSR teacher model training when $rank = 2$ and $rank = 30$ over 1500 iterations: (a) depicts the validation PSNR. (b) depicts the moving average of GPU power usage every 100 seconds with a window size of 100. At runtime, the larger rank requires more FLOPs, drawing more power under load. The training outcome and model performance act similar because, as indicated by Equation~\ref{eq:rank}, the matrix ultimately remains the same. The reported power measurements correspond to the NVIDIA GeForce RTX 4090 on Lambda GPU Server~\cite{lambda} using wandb~\cite{wandb} dashboard.}
	\label{ranks}
\end{figure*}

To further reduce the model size, knowledge distillation is applied. According to the Equation~\ref{eq:alpha}, $\alpha$ is a key parameter in the loss function. Therefore, experiments were made with different values of $\alpha$ to evaluate their performance over 1000 iterations and choose the most appropriate one. The result of different values of $\alpha$ can be seen in table~\ref{tab:alpha} with their corresponding PSNR.

\begin{table}[ht]
    \centering
    \caption{Comparison of PSNR for different $\alpha$ values. The reported PSNR belongs to the student model on the validation set over 1000 iterations.}  
    \begin{tabular}{c|ccccc}
        \hline
        \textbf{$\alpha$} & 0.2 & 0.4 & 0.6 & 0.8 \\
        \hline
        \textbf{PSNR} & 28.71 & 26.35 & 26.69 & 27.95 \\
        \hline
    \end{tabular}
    \label{tab:alpha}
\end{table}

As a result, $\alpha = 0.8$ is chosen to enable both learning from the teacher and ground truth labels. After that, the student model is put to training for 2500 number of iterations, the same as the teacher model. Then, the experiments on $\alpha$ values and their corresponding PSNRs inspected more with a wider range of values because of the probable formulation it suggested. Figure~\ref{alpha} demonstrates model performance on the validation set with changes in $\alpha$, suggesting a potential inconsistency in gradient values between the learned teacher and the ground truth, as indicated in the Equation~\ref{eq:alpha}, meaning that whenever the teacher and ground truth have near to similar weights in the loss function, PSNR might degrade. Moreover, if one of these weights decreases significantly, it indicates that the model is either not effectively learning or has reverted to learning from scratch. \new{Note that in the baseline, $\alpha$ was set to 0.5, meaning the impact of the teacher model and ground truth is the same, which as was mentioned, results in inferior performance.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/alpha.png}
    \caption{Model performance on the validations set based on the changes in $\alpha$, suggesting a potential inconsistency in gradient values between the learned teacher and the ground truth. The reported PSNRs are after 1000 iterations of training for teacher and student models.}
    \label{alpha}
\end{figure}
% References

Figure~\ref{patch} demonstrates the preprocessed input image, the corresponding MambaLiteSR output, and the ground truth image. The same process for a random patch is shown as well for better visualization\new{; a random patch from the LR image is taken, was given as an input to the model, and the output as well as the ground truth is shown.} A complete process consists of dividing the LR image into patches, passing each of them through the model, and stitching these patches together. It can be seen that MambaLiteSR is successful in generating a super-resolution image similar to its ground truth image.

\begin{figure}
	\centering
	\includegraphics[width=.45\textwidth]
         {Images/patch.png}
	\caption{Demonstration of LR image, MambaLiteSR output, and ground truth image, as well as a random patch cropped from the LR input and its corresponding MambaLiteSR output and ground truth patch.}
	\label{patch}
\end{figure}




\section{Conclusion}
In this paper, we presented MambaLiteSR, a new image super-resolution model designed for edge devices. It combines a low-rank Mamba mixer with knowledge distillation to deliver outputs with super-resolution, relying on state space representations while checking the embedding dimension and rank settings to minimize parameters and computation, as well as the distillation parameter tuning for better performance. By integrating knowledge distillation, MambaLiteSR inherits the performance of a larger teacher model and achieves competitive PSNR and SSIM on standard benchmarks. Our experiments show that MambaLiteSR can reduce the model size by up to 15\% compared to a baseline without sacrificing reconstruction quality significantly. Changes in rank resulted in a 42\% reduction in power usage during training. Assessment of the distillation parameter led us to a realization between the teacher model and ground truth, resulting in a balanced knowledge distillation. We also demonstrated its real-time capability on the embedded NVIDIA Jetson Orin Nano, which maintains \new{lower power consumption compared to the state-of-the-art edge super-resolution models, while maintaining similar performance}. By combining low-rank approximation, knowledge distillation, and efficient embedding strategies, MambaLiteSR offers a practical solution for generative AI applications on edge devices.

% Future work will focus on extending MambaLiteSR to other imaging tasks, exploring smarter scheduling on different hardware platforms, and refining our distillation techniques to further improve speed and energy consumption on constrained systems.

\section{Acknowledgment}
This project was sponsored by the U.S. Army Research Laboratory.


\bibliographystyle{IEEEtran} % Or another style of your choice
\bibliography{references}    % This points to references.bib (without the .bib extension)

% \begin{thebibliography}{1}

% \bibitem{genai} S. Feuerriegel, J. Hartmann, C. Janiesch, and P. Zschech, \textit{Generative AI}, Springer, 2023.
% \bibitem{transformer} A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \textit{Attention Is All You Need}, NeurIPS, 2023.
% \bibitem{llmaug} B. Prakash, T. Oates, T. Mohsenin, \textit{Using LLMs for Augmenting Hierarchical Agents with Common Sense Priors}, The International FLAIRS Conference Proceedings. Vol. 37. 2024.
% \bibitem{prakash2023llm} B. Prakash, T. Oates, T. Mohsenin, \textit{LLM Augmented Hierarchical Agents}, arXiv preprint arXiv:2311.05596, 2023.
% \bibitem{yu2023review} M. Yu, J. Shi, C. Xue, X. Hao, and G. Yan, \textit{A review of single image super-resolution reconstruction based on deep learning}, Springer, 2023.
% \bibitem{fatrabbit} H. Pourmehrani, J. Bahrami, P. Nooralinejad, H. Pirsiavash, N. Karimi \textit{FAT-RABBIT: Fault-Aware Training towards Robustness AgainstBit-flip Based Attacks in Deep Neural Networks}, 2024 IEEE International Test Conference (ITC), 2024.
% \bibitem{navardi2024metatinyml} M. Navardi, E. Humes, T. Mohsenin, \textit{MetaTinyML: End-to-End Metareasoning Framework for TinyML Platforms}, IEEE Embedded Systems Letters, 2024.
% \bibitem{kallakuri2024resource} U. Kallakuri, E. Humes, T. Mohsenin, \textit{Resource-Aware Saliency-Guided Differentiable Pruning for Deep Neural Networks}, Proceedings of the Great Lakes Symposium on VLSI 2024.
% \bibitem{rashid2024tinym} H. Rashid, T. Mohsenin, \textit{TinyM2 Net-V3: Memory-Aware Compressed Multimodal Deep Neural Networks for Sustainable Edge Deployment}, arXiv preprint arXiv:2405.12353, 2024.
% \bibitem{bao2025decentralised} Z. Bao, E. Kanjo, S. Banerjee, H. Rashid, T. Mohsenin, \textit{Decentralised Resource Sharing in TinyML: Wireless Bilayer Gossip Parallel SGD for Collaborative Learning}, arXiv preprint arXiv:2501.04817, 2025.
% \bibitem{reprohrl} T. Manjunath, M. Navardi, P. Dixit, T. Mohsenin \textit{ReProHRL: Towards multi-goal navigation in the real world using hierarchical agents}, arXiv preprint arXiv:2308.08737, 2023.
% \bibitem{mlae2} M. Navardi T. Mohsenin \textit{Mlae2: Metareasoning for latency-aware energy-efficient autonomous nano-drones}, 2023 IEEE International Symposium on Circuits and Systems (ISCAS), 2023.
% \bibitem{navarrete2022edge} P. Navarrete Michelini, Y. Lu, and X. Jiang, \textit{Edge-SR: Super-Resolution for the Masses}, WACV, 2022.
% \bibitem{wang2024eshp} Q. Wang et al., \textit{An Intelligent Co-Scheduling Framework for Efficient Super-Resolution on Edge Platforms}, IEEE IoT Journal, 2024.
% \bibitem{mazumder2024reg} A. Mazumder, F. Safavi, M. Rahnemoonfar, T. Mohsenin, \textit{Reg-tune: A regression-focused fine-tuning approach for profiling low energy consumption and latency}, IEEE IoT Journal, 2024.
% \bibitem{meta} M. Navardi, E. Humes, T. Manjunath, T. Mohsenin \textit{MetaE2RL: Toward Meta-Reasoning for Energy-Efficient Multigoal Reinforcement Learning With Squeezed-Edge You Only Look Once}, IEEE Micro, 2023.
% \bibitem{yolo} E. Humes, M. Navardi, T. Mohsenin, \textit{Squeezed Edge YOLO: Onboard Object Detection on Edge Devices}, NeurIPS, 2023.
% \bibitem{water} J. Wright, A. Y. Yang, A. Ganesh, S. Shankar Sastry, and Y. Ma, \textit{Robust Face Recognition via Sparse Representation}, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2009.

% \bibitem{srcnn} C. Dong, C. C. Loy, K. He, and X. Tang, \textit{Image Super-Resolution Using Deep Convolutional Networks}, ECCV, 2014.
% \bibitem{vdsr} J. Kim, J. K. Lee, and K. M. Lee, \textit{Accurate Image Super-Resolution Using Very Deep Convolutional Networks}, CVPR, 2016.
% \bibitem{fsrcnn} C. Dong, C. C. Loy, and X. Tang, \textit{Accelerating the super-resolution convolutional neural network}, ECCV, 2016.
% \bibitem{rcan} Y. Zhang, K. Li, K Li, L Wang, B Zhong, and Y. Fu, \textit{Image Super-Resolution Using Very Deep Residual Channel Attention Networks}, CVPR, 2018.
% \bibitem{edsr} B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, \textit{Enhanced Deep Residual Networks for Single Image Super-Resolution}, CVPRw, 2017.
% \bibitem{denoiser} K. Zhang, W. Zuo, S. Gu, and L. Zhang, \textit{Learning Deep CNN Denoiser Prior for Image Restoration}, CVPR, 2017.
% \bibitem{eshr} W. Zhang, L. Pan, K. Xu, G. Li, and Y. Lv, \textit{Efficient self-calibrated and hierarchical refinement network for lightweight super-resolution}, ELSEVIER, 2024.
% \bibitem{esr} Z. He, S. Zhang, et al., \textit{ESR-DDLN : Enhanced Single Image Super-Resolution Via Dual-Domain Learning Network}, IEEE International Conference on Multimedia and Expo~(ICME), 2024.
% \bibitem{mafdn} Y. Zhang, X. Lin, H. Yang, J. He, L. Qing, X. He, Y. Li, and Honggang Chen, \textit{A Multi-Attention Feature Distillation Neural Network for Lightweight Single Image Super-Resolution}, International Journal of Intelligent Systems, 2024.
% \bibitem{mamba} A. Gu and T. Dao, \textit{Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, arXiv preprint arXiv:2312.00752, 2023.
% \bibitem{visionmamba} L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, \textit{Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model}, ICML, 2024.
% \bibitem{mambavision} A. Hatamizadeh and J. Kautz, \textit{MambaVision: A Hybrid Mamba-Transformer Vision Backbone}, arXiv preprint arXiv:2407.08083, 2024.
% \bibitem{rlfn} F. Kong, M. Li, S. Liu, D. Liu, J. He, Y. Bai, F. Chen, and L. Fu. \textit{Residual local feature network for efficient super-resolution}, CVPR, 2022.
% \bibitem{lei2024dvmsr} X. Lei, W. Zhang, and W. Cao, \textit{DVMSR: Distillated Vision Mamba for Efficient Super-Resolution}, ICCV, 2024.
% \bibitem{himamba} J. Qiao, J. Liao, W. Li, Y. Zhang, Y. Guo, Y. Wen, Z. Qiu, J. Xie, J. Hu, and S. Lin, \textit{Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution}, arXiv preprint arxiv.org/abs/2410.10140, 2024.
% \bibitem{srmamba} C. Liu, D. Zhang, G. Lu, W. Yin, J. Wang, and G. Luo, \textit{SRMamba-T: Exploring the hybrid Mamba-Transformer network for single image super-resolution}, ELSEVIER, 2025.
% \bibitem{lora} E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \textit{LoRA: Low-Rank Adaptation of Large Language Models}, International Conference on Learning Representations, 2022.
% \bibitem{orin} NVIDIA, \textit{NVIDIA Jetson Orin}, https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/
% \bibitem{jahnavi2024study} M. Jahnavia, D. R. Raob, A. Sujathac, \textit{A Comparative Study Of Super-Resolution Interpolation Techniques: Insights For Selecting The Most Appropriate Method}, International Conference on Innovative Data Communication Technologies and Application, 2024.
% \bibitem{chen2015train} Y. Chen and T. Pock, \textit{Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration}, arXiv preprint arXiv:1508.02848, 2015.
% % \bibitem{dong2015image} C. Dong, C. C. Loy, K. He, and X. Tang, \textit{Image Super-Resolution Using Deep Convolutional Networks}, IEEE TPAMI, 2015.
% % \bibitem{dong2016accelerating} C. Dong, C. C. Loy, and X. Tang, \textit{Accelerating the Super-Resolution Convolutional Neural Network}, ECCV, 2016.
% \bibitem{shi2016real} W. Shi et al., \textit{Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network}, CVPR, 2016.
% \bibitem{dong2016accel} C. Dong, C. Change Loy, and X. Tang, \textit{Accelerating the super–resolution convolutional neural network}, ECCV, 2016.
% \bibitem{ntire} Y. Li, Y. Zhang, et al. {Ntire 2023 challenge on efficient super-resolution: Methods and results}, CVPRW, 2023.
% \bibitem{zhu2024visionmamba} L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, \textit{Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model}, arXiv preprint arXiv:2401.09417, 2024.
% \bibitem{mathur2021real} P. Mathur et al., \textit{A Real-Time Super-Resolution for Surveillance Thermal Cameras Using Optimized Pipeline on Embedded Edge Device}, IEEE AVSS, 2021.
% \bibitem{perez2023efficient} F. Perez-Montes, J. Olivares-Mercado, and G. Sanchez-Perez, \textit{An Efficient Facial Verification System for Surveillance That Automatically Selects a Lightweight CNN Method and Utilizes Super-Resolution Images}, MICAI, 2023.
% \bibitem{freq} R. Y. Tsai and T. S. Huang, \textit{Multiframe image restoration and registration}, Advances in Computer Vision and Image Processing, 1984.
% \bibitem{problem} S. Baker and T. Kanade, \textit{Limits on Super-Resolution and How to Break Them}, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2002.
% % \bibitem{metaswin} O. Tariq, M. Bilal, A. Dastagir, and D. Han; Muhammad Bilal, \textit{Meta-Swin: Lightweight Image Super-Resolution Swin Transformer for Metaverse Applications}, iMETA, 2024.
% %\bibitem{div2k} R. Timofte, E. Agustsson, L. Van Gool, M. Yang, and L. Zhang, \textit{Ntire 2017 challenge on single image super-resolution: Methods and results}, CVPRW, 2017.
% \bibitem{div2k} R. Timofte, et al., \textit{Ntire 2017 challenge on single image super-resolution: Methods and results}, CVPRW, 2017.
% \bibitem{set5} M. Bevilacqua, A. Roumy, C. Guillemot, and M. Alberi-Morel, \textit{Low-complexity single-image super-resolution based on nonnegative neighbor embedding}, BMVC, 2012.
% \bibitem{set14} R. Zeyde, M. Elad, and M. Protter, \textit{On single image scale-up using sparse-representations}, Curves and Surfaces, 2010.
% \bibitem{bsd100} D. R. Martin, C. C. Fowlkes, D. Tal, and J. Malik, \textit{A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics}, ICCV, 2001.
% \bibitem{urban100} J. Huang, A. Singh, and N. Ahuja, \textit{Single image super-resolution from transformed selfexemplars}, CVPR, 2015.
% \bibitem{lambda} Lambda Labs, \textit{Lambda GPU Server}, https://lambdalabs.com/
% \bibitem{vitreg} M. R. Shaharear, A. N. Mazumder, T. Mohsenin,  \textit{ViT-Reg: Regression-Focused Hardware-Aware Fine-Tuning for ViT on tinyML Platforms}, IEEE Design \& Test, 2024.
% \bibitem{nano} NVIDIA, \textit{Jetson Orin Nano Super Developer Kit}, https://www.nvidia.com/en-us/autonomous-machines/embedded-
% systems/jetson-orin/nano-super-developer-kit/

% \bibitem{tegrastats} Tegrastats Utility, https://docs.nvidia.com/jetson/archives/r34.1/ DeveloperGuide/text/AT/JetsonLinuxDevelopmentTools/TegrastatsUtility.html
% \bibitem{xavier} NVIDIA, \textit{NVIDIA Jetson AGX Xavier GPU}, https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-agx-xavier/
% \bibitem{wandb} wandb.ai, \textit{Weights \& Biases}, https://wandb.ai/


% \end{thebibliography}


\end{document}
