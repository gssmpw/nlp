\section{Deployment Strategies of \glspl{LLM} in Mobile Networks}
\label{sec:LLM_deployments}% ~ 1800 words
Below, we describe the key forms an \gls{LLM}-empowered 6G network can take with a thorough review of the advantages and drawbacks of each approach:

\subsection{\gls{LLM} as a standalone \gls{NF} (see Fig.~\ref{fig:LLM_possible_architectures}b)}
\label{subsec:standalone_llm}
The \gls{LNF} will be deployed as an independent \gls{NF} at the control plane of the \gls{6GC} as a part of the \gls{SBA}. It will interact with other \glspl{NF} via the \glspl{SBI} using stateless \gls{RESTful} APIs that are based on HTTP/2 and JSON payloads. These interactions typically follow the synchronous request/response or the asynchronous subscribe/notify models, depending on the use case. For instance, an \gls{NF} can request a diagnostic from the \gls{LNF} about an incident, confirms the initiation of a resolution workflow or subscribes to be notified about anomalies in network behavior. In turn, the \gls{LNF} can request the execution of a step-by-step healing plan upon detection of an anomaly, or subscribes to specific network analytics, as shown in Fig.~\ref{fig:LNF_breakdown}. The \gls{LNF} needs to first register its services with the \gls{NRF} to be discoverable by the remaining \glspl{NF}.

A breakdown of the \gls{LNF} is provided in Fig.~\ref{fig:LNF_breakdown}. First, the \gls{MNO}'s multi-modal and time-evolving data should be be fed to the processing pipeline of the \gls{LLM}. Afterwards, relevant information should be identified, extracted and structured according to a consistent format. We use a periodic sliding window mechanism that controls how much data, $W$, can be sent to the \gls{LLM} prompt per fixed-time period, $T$. At the end of each period, the window moves forward and the next data is transmitted to the \gls{LLM} in the subsequent time period. The preprocessed data is then incrementally submitted to the \gls{LLM} at regular time intervals. Meanwhile, the \gls{LLM} leverages its natural language capabilities to generate a compact summary of past key information (e.g. metrics, events, and network changes) from the preceding time windows, to be used as a context for the present sliding window $W_{t}$. The context should be concise yet detailed enough to capture significant patterns in the past. For example, the preprocessed data can be summarized every 1-hour interval and then the most recent $N$ summaries can be provided to the \gls{LLM} as a context, or in case of long-term anomalies, fine-grained summaries can be provided for recent data and coarser summaries can be generated for older data. This calls for a long-term memory to retain information over extended intervals. The size $W$ and periodicity $T$ of sliding window should be adaptively chosen to balance enough context for accurate predictions and maintaining the overall input (recent data+context) within token limits. Next, prompt engineering techniques are used to craft a natural language query that includes the most recent sliding window data, $W_{t}$, along with summaries of the past $N$ iterations $\{W_{t-N}, \ldots, W_{t-1}\}$ as a context, to be submitted to the \gls{LLM}. Upon detecting an anomaly, the \gls{LLM} will provide its insights including the diagnosis (i.e. root cause) and the suggested solution. Using \gls{CoT} prompting, the \gls{LLM} decomposes the solution into a series of $M$ sequential and manageable instructions $\{I_{1}, \ldots, I_{M}\}$. Further, \gls{ToT} prompting can enhance the clarity and readability of the generated instructions. Thereafter, each instruction, or set of instructions, is mapped into a single \gls{3GPP} \gls{RESTful} API calling structure using an NL-to-REST Mapper. The latter consults the appropriate \gls{3GPP} \gls{TS}, that describes the endpoints, operations, request/response formats, parameters, error handling, authentication, and other details, to generate the correct API calls, $\{C_{1}, \ldots, C_{L}\}$. Later, an Executor sub-module invokes each resulting API call using an URL pointing towards the relevant \gls{NF}  to run the instructions at hand. The resulting execution outputs $\{R_{1}, \ldots, R_{O}\}$, in particular the JSON payload, will be parsed according to the \gls{3GPP} API documentation to extract useful information using a Parser sub-module. Then, the relevant information will be presented in an actionable format, $\{A_{1}, \ldots, A_{P}\}$, to acknowledge that the step-by-step resolution plan advised by the \gls{LLM} was successfully triggered within the network. The REST-to-NL Mapper is used to translate \gls{3GPP} \gls{RESTful} API replies into natural language answers. RestGPT \cite{song2023restgpt} represents a good attempt to develop a framework connecting \glspl{LLM} with \gls{RESTful} APIs. The \gls{LLM} updates its fine-tuning knowledge through a continuous improvement cycle using different query-feedback pairs.

This modular design ensures that \gls{LLM} operations enjoy dedicated compute and storage resources and the \gls{LNF} can scale independently from the remaining \glspl{NF} as workload grows. Moreover, any updates, maintenance or replacements to the \gls{LLM} or the \glspl{NF} can be performed independently without or with minimal disruption. The \gls{LNF} can also interface with multiple network entities, providing multi-functional support across other \glspl{NF} (e.g. \gls{SMF}) and external business systems (e.g. Metaverse, automotive, and quantum computing). The integration within the \gls{SBA} ensures stateless communications (i.e. REST architecture), slicing-based fault isolation, full encryption (e.g. using TLS) and restricted access to legitimate services (e.g. using OAuth), reducing attack surfaces \cite{Khaloopour2024Resilience}.

Nevertheless, additional middleware is needed for the \gls{LLM}, as depicted in Fig~\ref{fig:LNF_breakdown}, to interact with the remaining network components leading to potential delays and complexities in communication. This impediment is further exacerbated by the need to setup real-time feedback loops, at the intelligence level, between the \gls{LNF} and the remaining \glspl{NF} especially the \gls{NWDAF} to synchronize \gls{AI}-related insights and avoid conflicts and duplication.
\begin{figure*}[t!]
\centering
\includegraphics[width=.99\textwidth]{Fig2.eps}
    \caption{A breakdown of the \gls{LNF}.}
    \label{fig:LNF_breakdown}
\end{figure*}

\subsection{LLM as an embedded intelligence layer (see Fig.~\ref{fig:LLM_possible_architectures}c)}
\label{subsec:integrated_llm}
6G \glspl{NF} can be augmented with \gls{LLM} capabilities, endowing them with the ability to perform innovative functions such as advanced predictive analytics and user-/operator-friendly interactions. The \gls{LLM} can be centrally hosted within a single \gls{NF} (e.g. \gls{LLM}-augmented \gls{NWDAF} in \cite{Kan2024mobile_llama}) or distributed across multiple \glspl{NF}.

A major advantage of \gls{LLM}-powered \glspl{NF} is improved interaction between the \gls{NOC} engineers and the 6G ecosystem, using conversational queries instead of complex \glspl{CLI}. Moreover, embedding the \gls{LLM} into an \gls{NF} eliminates the need for intermediary communication interfaces, simplifying the overall system flows. The \gls{LLM} will also gain direct and real-time access to the established pipeline of the \gls{NF} to avoid redundant computations and conflicting outputs. For example, the analytics function will be seamlessly coordinated within the \gls{LLM}-augmented \gls{NWDAF}  \cite{Kan2024mobile_llama}.

On the downside, this fully integrated approach may require significant architectural changes at the \gls{NF} level, e.g. to embed large-scale neural networks. Further, the \gls{LLM} scalability will be constrained by the hosting \gls{NF}'s design, as they will compete for resources. While conventional \glspl{NF} (e.g. \gls{AMF}, \gls{UPF}) scale primarily to handle higher traffic volumes and user connections, an \gls{LLM} may scale for different \gls{AI}-specific considerations such as \gls{LLM} upgrades to update model architecture. On the other hand, the \gls{LLM} functionalities become tightly coupled with the hosting \gls{NF}, limiting the \gls{LLM}'s applicability outside the \gls{NF}'s primary goal (e.g. the \gls{LLM}-augmented \gls{NWDAF} design \cite{Kan2024mobile_llama} will restrict the \gls{LLM} capacity to provide novel services beyond the analytics function of the \gls{NWDAF}). Further, a failure in the integrated design can affect both the \gls{LLM} and the \gls{NF}.

\subsection{Hybrid Approach: partially integrated \gls{LLM} closely coupled with an existing \gls{NF} (see Fig.~\ref{fig:LLM_possible_architectures}d)}
\label{subsec:hybrid_llm}
The hybrid approach involves integrating a lightweight \gls{LLM} sub-component within an existing \gls{NF} (e.g. \gls{NWDAF}) and deploying a more sophisticated \gls{LLM} sub-component outside the \gls{NF}. This approach strategically integrates the basic short-term features of \gls{LLM} such as structured data-based analytics into the hosting \gls{NF}, within a \gls{Near-RT} \gls{LLM} module, whereas the breakthrough long-term functions such as context-aware decision-making that rely on unstructured data are deployed externally as an independent microservice or containerized application in a \gls{Non-RT} \gls{LLM} \gls{NF}. Both modules closely communicate with each other using the established \gls{SBI}, thus building a loosely coupled design \cite{chaoub2023hybrid} that reduces the dependencies and constraints that one component places on another while maintaining necessary interactivity. The internal \gls{LLM} investigates the reported issues and categorizes them into \gls{Near-RT} and \gls{Non-RT}. The hosting \gls{NF} handles \gls{Near-RT} incidents immediately impacting network operations. It potentially requests additional analysis from the external \gls{LLM} module regarding long-term tasks, the latter then complements by providing deeper \gls{Non-RT} insights or endorses \gls{Near-RT} findings, sending them back to the hosting \gls{NF} or to the rest of the network for dissemination. The hybrid architecture potentially needs to incorporate an additional faster API (e.g. gRPC) to internally synchronize the analysis and the derived insights between \gls{Near-RT} and \gls{Non-RT} parts of \gls{LLM} to avoid slowing down the \gls{SBI}.

The hybrid design ensures that any updates or replacements to the \gls{Non-RT} \gls{LLM} or the hosting \gls{NF} can be performed with minimal disruption. The hosting \gls{NF} remains operational even if the external \gls{LLM} becomes unavailable, ensuring continuity of advanced statistical and predictive analytics. Furthermore, the strategic and the lightweight parts of the \gls{LLM} can scale independently. Yet another benefit is retaining the \gls{Non-RT} \gls{LLM} ability to independently connect to other \glspl{NF} or external stakeholders.

This architecture faces some technical hurdles related to its increased complexity and costs, and the need to carefully coordinate the \gls{Near-RT} and the \gls{Non-RT} components of the \gls{LLM} through the internal fast interface. The operational scope of each component should be clearly defined.
\subsection{\gls{LLM} as a standalone external \gls{AF} (see Fig.~\ref{fig:LLM_possible_architectures}e)}
\label{subsec:external_llm}
In this scheme, the \gls{LLM} is deployed as an \gls{AF}, referred to as \gls{LAF}, and resides outside the \gls{MNO}'s infrastructure. The \gls{LAF} communicates with the \gls{MNO} entities via \gls{RESTful} APIs, leveraging \gls{3GPP} standard interfaces defined for \glspl{AF}. As an external entity, the \gls{LAF} can be integrated with the \gls{SBA} via the \gls{NEF}, acting as a secure gateway for exposing network capabilities to external functions. As an alternative, the \gls{LAF} can be deployed inside a trusted \gls{DN}, a secure extension of the network, to directly communicate with relevant \gls{6GC} functions.

A primary strength of this scheme lies in the capacity of upgrading, scaling, or integrating the \gls{LAF} with external platforms independently of the \gls{6GC} architecture. External computing infrastructures, in particular, can be used to offload resource-intensive \gls{LLM} tasks to alleviate the burden on network resources. At the same time, the \gls{LAF} can leverage data from external domains (e.g., third-party services, public datasets) to enhance its insights. Another compelling advantage is allowing \glspl{MNO} to collaborate with \glspl{AISV}, leveraging best-in-class players and their expertise in \gls{LLM} solutions.

An externalized \gls{LLM} may, however, lead to an increased latency in real-time communications delaying the resolution of critical network outages. Beyond this, the \gls{LAF} as an external entity may lack complete real-time contextual awareness about network conditions, degrading the accuracy of its outcomes. In addition, exposing sensitive network and user data to the external \gls{LAF} requires robust encryption, strict access control, anonymization, and full compliance with regulations in terms of data protection, privacy and sovereignty. The \gls{NEF}, in particular, need to be secure enough to handle different exchanges between the local \glspl{NF} and the external \gls{LAF} to avoid any security breaches. One further impediment arises from the strong dependency on \glspl{AISV} regarding service reliability and availability. Lastly, the ability to customize \gls{LLM} services to specific needs of the \gls{MNO} may be limited.
\input{Architectures_options}

A comprehensive summary of the strengths and weaknesses of each approach is provided in Table~\ref{tab:architectures_table}.

