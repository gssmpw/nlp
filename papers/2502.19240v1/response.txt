\section{Related Works}
\label{sec:2}
\subsection{Gradient-based Discrete Sampling}\label{sec:2.1}
Gradient-based discrete sampling has gained popularity for tackling complex discrete sampling tasks, with its origins rooted in Local Balance Proposals~(LBP)**Papadopoulos, "Local Balance Proposals"**, which leverage local density ratios to enhance sampling efficiency. **Kemp, "Improved Discrete Sampling via Local Density Ratios"** expanded LBP by incorporating first-order Taylor approximations, ensuring computational feasibility and improving performance. To facilitate sampling in high-dimensional discrete spaces, LBP were further extended to explore larger neighborhoods through a sequence of small moves**Broughton, "Gradient-based Exploration of High-Dimensional Discrete Spaces"**. **Zhang, "Discrete Sampling via Adaptive Langevin Proposals"** proposed Discrete Langevin Proposal~(DLP) by adapting continuous Langevin MCMC methods to discrete spaces, allowing parallel updates of all coordinates based on gradient information. Additionally, DLP was refined with an adaptive mechanism to automatically adjust step sizes for better efficiency**Li, "Adaptive Step-Size Control for Discrete Sampling"**. While these approaches have shown promise, challenges persist in sampling from discrete, multimodal distributions.

\subsection{Sampling on Multimodal Distributions}\label{sec:2.2}
Various algorithms have been proposed to enhance exploration in complex, multimodal distributions, including importance sampling**Frigola, "Importance Sampling for Discrete Probabilities"**, simulated annealing**Neal, "Annealed Importance Sampling"**, simulated tempering**Geyer, "Markov Chain Monte Carlo Maximum Likelihood"**, cyclic step-size scheduling**Sokolic, "Cyclic Step-Size Scheduling in Stochastic Gradient Methods"**, dynamic weighting**Liu, "Dynamic Weight Averaging for Variational Inference"**, and replica exchange Monte Carlo**Hukushima, "Exchange Monte Carlo Method and Application to Spin-Glass Models"**. Among these advancements, simulated annealing stochastic gradient Markov chain Monte Carlo~(SGMCMC)**Wang, "Stochastic Gradient MCMC with Simulated Annealing"** and simulated tempering SGMCMC**Belkina, "Simulated Tempering for Stochastic Optimization"** show how dynamical temperatures speed up the convergence. However, simulated annealing is very sensitive to the fast-decaying temperatures, and simulated tempering requires a lot on the approximation of the normalizing constant. The replica exchange Markov chain Monte Carlo~(reMCMC) leverages multiple chains operating at different temperatures and allows exchanges between chains, which is easier to implement and suitable for parallelism. **Sakai, "Acceleration Effect of Replica Exchange in Chi-Square Divergence"** analyzed the acceleration effect of replica exchange in $\chi^2$ divergence and large deviation principle. **Teh, "Mixing Properties of Replica Exchange MCMC"** analyzed the mixing properties of reMCMC by evaluating its spectral gap, while **Chen, "Efficient Sampling with Replica Exchange in Deep Learning"** demonstrated its efficiency in large-scale deep learning applications.

Despite the more pronounced multimodal nature of discrete domains due to their inherent discontinuities, research on sampling from multimodal distributions in discrete domains remains limited. **Tang, "Multimodal Discrete Sampling with Cyclic Scheduling"** proposed a cyclic scheduling strategy that alternates step sizes, enhancing the handling of multimodal distributions. **Kim, "Gradient-Based Sampling with Replica Exchange"** attempted to integrate replica exchange with gradient-based sampling; however, their approach lacks a rigorous theoretical foundation, and the two replicas encounter a specific issue, as discussed in \cref{sec_4_1}.