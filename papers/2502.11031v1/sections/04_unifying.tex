\label{sec:unifying}








In this section, we clarify how bias issues discussed in existing literature align with our proposed definitions.
Generally, we categorize the bias into a specific type of bias in our definition if the presence of this bias implies the existence of bias in our definitions.
Furthermore, the categorization primarily relies on two key factors: the manifestation of bias issues explicitly addressed (if stated in ``Problem Statement" section) and the characteristics of evaluation protocol\footnote{For instance, Type I Bias involves training sets which yield the long-tail distribution, while Type II Bias typically involves training sets which yields the association between target label and attribute label.}.
Other aspects such as motivation, related work, method, or bias assessment are considered secondary factors for categorization. This is because certain papers, despite addressing different manifestations of bias, can exhibit similarities in these aspects, thereby leading to the confusion between these two types of biases, as elaborated in~\cref{sec:confusion}. 












\subsection{Type I Bias}
The general form of Type I Bias is characterized by the uneven performance of the target across attributes. This definition can be extended to unify a wide range of papers by specifying the usage of performance metrics and the kind of target.
To clarify, several representative descriptions are shown as follows, \eg

\begin{itemize}
    \item \emph{``Racial bias indeed degrades the fairness of recognition system and the error rates on non-Caucasians are usually much higher than Caucasians."}~\cite{RL_RBN}
    \item \emph{``A certain demographic group can be better recognized than other groups."}~\cite{GAC}
    \item \emph{``Recognition accuracies depend on demographic cohort."}~\cite{RFW_IMAN}
\end{itemize}

\noindent
By specifying how performance is evaluated, Type I Bias covers a broad range of papers where model performance is evaluated using various criteria such as error rate~\cite{fairnessgan_DP_difference_error_rate}, loss~\cite{representation_disparity}, accuracy~\cite{multiaccuracy}, True Positive Rate (TPR)~\cite{pass}, False Positive Rate (FPR)~\cite{FPR_Penalty_Loss}, Receiver Operating Characteristic curve (ROC)~\cite{debias_balanced_AUCROC}, and Area Under the Curve (AUC)~\cite{DebFace}.
Furthermore, by specifying the kind of target, this definition can unify a wider range of papers.
For instance, considering sex as an attribute, the targets can include identity~\cite{DebFace, FairCal} (\eg face recognition), the attribute itself~\cite{Timnit_sex_classification_PPB,Fairface} (\eg sex classification), or other targets associated with protected attribute~\cite{minority_group_vs_sensitive_attribute,representation_disparity} (\eg facial attribute classification).
It is noteworthy that Type I Bias is predominantly discussed in various biometrics tasks~\cite{von_Mises_Fisher,FR_inherent_bias,SensitiveNets}. 
Compared with various types of targets, protected attributes (\eg sex, race, and age) are mainly considered the term of attribute in Type I Bias.

















\subsection{Type II Bias}
\label{subsec:Type_II_Bias}
The general form of Type II Bias is characterized by the dependence between model prediction and attribute. 
This definition can be used to unify a broad spectrum of papers by considering the status of attribute and the kind of attribute.
The status of attribute is categorized into three groups, including known and labeled, known but unlabeled, and unknown.
Specifically, for known and labeled bias, several methods directly leverage attribute labels to explicitly apply supervision signal for bias mitigation~\cite{CSAD}.
For known but unlabeled bias, several methods mainly utilize the domain knowledge of specific bias attribute to design the module tailored for this bias attribute~\cite{HEX_texture_bias1}.
For unknown bias, several methods identify and emphasize bias-conflicting samples (those exhibiting the opposite bias present in the training set) to mitigate bias~\cite{ECS}. On the other hand, the kind of attribute mainly encompasses sensitive/protected attributes~\cite{machine_bias,annotation_bias,discrimination_score} and spurious attributes~\cite{LfF_CelebA_Bias_conflicting,Group_DRO,ECS}.
In the case of sensitive attributes, the reliance on them leads to a disproportionate assignment of specific predictions to particular demographic groups, thereby resulting in unfair treatment.
In this category, demographic parity~\cite{fairness_through_awareness}, a well-known fairness criterion, is often served as a debiasing objective. 
We present several representative descriptions as follows, \eg

\begin{itemize}
    \item \emph{``Demographic parity, which is satisfied when the predictions are independent of the sensitive attributes."}~\cite{DP_FFVAE}
    \item \emph{``Data fairness can be achieved if the generated decision has no correlation with the generated protected attribute."}~\cite{fairgan}
    \item \emph{``Ensuring that the positive outcome is given to the two groups at the same rate."}~\cite{LAFTR}
\end{itemize}

\noindent
In the case of spurious attributes, depending on them for decision-making will simplify the training process since models may utilize them as shortcut features instead of learning more comprehensive features during training. However, this leads to model predictions heavily relying on these attributes and further poor generalization performance in real-world applications since such spurious correlation between target and attribute does not generally exist. 
Several representative descriptions are shown as follows, \eg

\begin{itemize}
    \item \emph{``If bias features are highly correlated with the object class in the  dataset, models tend to use the bias as a cue for the prediction."}~\cite{BCL}
    \item \emph{``Since there are correlations between the target task label and the bias label, the target task is likely to rely on the bias information to fulfill its objective."}~\cite{CSAD}
    \item \emph{``If biased data is provided during training, the machine perceives the biased distribution as meaningful information."}~\cite{learn_not_to_learn_Colored_MNIST}
\end{itemize}













\input{sections/tables/fairness_criteria}





\subsection{Fairness Criteria}
Besides the papers that explore bias issues directly from the perspective of bias itself, there is another group of papers that leverage established fairness criteria (\eg demographic parity and equalized odds) as their debiasing objectives.
In this section, we first adopt the corresponding definitions of fairness from the definition of bias in~\cref{def:Type_I_Bias,def:Type_II_Bias}, and then demonstrate that relevant papers based on established fairness criteria can be categorized under these definitions.
Given that fairness is the opposite of bias, we can derive the fairness definition for each type of bias as follows,

\begin{definition}
    \label{def:Fairness_Type_I_Bias}
Fairness \wrt Type I Bias. A model $f$ is fair \wrt \emph{Type I Bias} if $f$ yields even performance $d(\hat{Y},Y)$ across attribute $A$, \ie
\begin{align}
    \sup_{a,a'\in \mathcal{A}, d \in \mathcal{M}} \abs{d(\hat{Y},Y|A=a) - d(\hat{Y},Y|A=a')} = 0
\end{align}
\end{definition}
\noindent
where $a,a'$ are possible values of $A$ (\eg female and male), and $\mathcal{M}$ is the set of all potential performance metrics.

\begin{definition}
    \label{def:Fairness_Type_II_Bias}
Fairness \wrt Type II Bias. A model $f$ is fair \wrt \emph{Type II Bias} if model prediction $\hat{Y}$ is independent with attribute $A$, \ie
\begin{align}
\sup_{a,a' \in \mathcal{A}} \abs{P(\hat{Y}|A=a) - P(\hat{Y}|A=a')} = 0 
\end{align}
\end{definition}
\noindent
where $a,a'$ are possible values of $A$ (\eg female and male).




Fairness criteria can be categorized into two key classes: group fairness and individual fairness~\cite{MLbias_survey, causal_based_fairness,discussion_on_DP_EO}.
Specifically, group fairness is founded on the idea that ``groups of people may face biases and unfair decisions", whereas individual fairness is grounded in the principle that ``similar individuals should receive similar decisions"~\cite{discussion_on_DP_EO}.
We mainly unify group fairness into our definitions since group fairness is more commonly used in fairness research~\cite{prediciton_quality_disparity}.
Group fairness encompasses several well-known fairness criteria such as demographic parity/statistical parity~\cite{fairness_through_awareness, counterfactual_fairness}, equalized odds/equality of odds~\cite{EO_define}, equal opportunity/equality of opportunity~\cite{EO_define}, and accuracy parity~\cite{Accuracy_parity}. The categorization of them under our fairness definitions is shown in~\cref{tab:fairness_criteria}.
Specifically, demographic parity, which requires $P(\hat{Y}|A=a_0) = P(\hat{Y}|A=a_1)$, is consistent with~\cref{def:Fairness_Type_II_Bias} when attribute $A$ is binary.
Equalized odds, which requires that both even true positive rate (TPR) ($P(\hat{Y}=y_1|Y=y_1)$) and even false positive rate (FPR) ($P(\hat{Y}=y_1|Y=y_0)$) across $A$, and equal opportunity, which is the weaker notion of equalized odds that focuses solely on the advantaged outcome where $Y=y_1$, align with~\cref{def:Fairness_Type_I_Bias} since TPR and FPR are included in the set of performance metrics $\mathcal{M}$.
Accuracy parity, where accuracy is represented by $P(\hat{Y}=Y)$, also aligns with~\cref{def:Fairness_Type_I_Bias} since accuracy is the element of $\mathcal{M}$.















\subsection{Summary}

Having unified the prevalent bias issues and well-known fairness criteria under our definitions, in this section, we summarize the main advantages of the proposed definitions.
First, the proposed definitions focus on the manifestation of predominant bias, which is more clear and easier to apply compared to definitions based on causes, since causes of these biases are debatable in some cases~\cite{BR_Net_dataset_vs_task, minority_group_vs_sensitive_attribute,RL_RBN}.
Second, the proposed definitions yield the general form, and by specifying the components in the general form, they can be used to unify a comprehensive list of papers, as summarized in~\cref{tab:unification}. 
Third, the proposed definitions, as the first definition to formally define dominant biases, bridge the gap between numerous fairness definitions~\cite{EO_define,counterfactual_fairness,fairness_through_awareness,fairness_under_unawareness,process_fairness_FPR_difference,impossibility_for_fair_repesentations,Accuracy_parity} and the significant shortage of formal bias definition.
Furthermore, compared with fairness definitions, bias definitions are more practical since encountering bias issues is more common in real-world scenarios, whereas achieving fairness, often considered an ideal benchmark, is rare in practice.
Fourth, given that the proposed bias definitions are relatively general, the corresponding fairness definitions are strict, hence aligning with the need for fairness as an ideal standard. 
Additionally, several well-known fairness criteria can be unified under the proposed fairness definitions.


\input{sections/tables/unification}





