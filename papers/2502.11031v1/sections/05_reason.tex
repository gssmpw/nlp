\label{sec:reason}
In this section, we investigate various factors that may contribute to the confusion discussed in the previous section. 
Specifically, we examine the historical context, the preconception about bias, and the methodologies adopted to address different biases, to provide insights on how and why such confusion has persisted in the literature.


\input{sections/pictures/bias_enrichment}


\subsection{Historical Context}
We first examine the historical origins of bias issues.
In~\cref{fig:enrichment}, we summarize the enrichment of the concept ``bias" in machine learning from the perspective of Type I Bias and Type II Bias and highlight key milestones throughout its history.
Originally, ``bias" is defined as unfair favoritism or prejudice towards one thing, person, or group over another~\cite{ditomaso2015racism}.
Specifically, bias issues are especially evident in real-world decision-making processes, such as advertising, financial creditworthiness, employment, education, and criminal justice~\cite{ruggeri2023persistence,edmond2019just}.
To promote fairness, certain sensitive attributes (\eg sex, age, and race) are by law defined as protected attributes that cannot be discriminated against in the decision-making process~\cite{protected_attributes}.
In this initial stage, decisions are primarily made by humans.
Thus, the main bias issue is if human decision-making depends on protected attributes, which aligns with Type II Bias in our definitions.







Following the emergence of neural networks, machine learning models start to assist in human decision-making processes~\cite{bastani2021improving,dankwa2019transforming}. 
This evolution also leads to an expansion of the subject in the discussion regarding bias issues, from human decision-making to algorithmic decision-making~\cite{starke2022fairness}. 
With this change, numerous works begin to explore if algorithmic decision-making depends on protected attributes (\ie demographic parity)~\cite{fairness_through_awareness, counterfactual_fairness}, which also align with Type II Bias.
Meanwhile, along with the advancement of neural networks, its performance becomes a crucial evaluation criterion.
Consequently, it brings significant attention to a new aspect of bias issues: performance disparity across demographic groups~\cite{Timnit_sex_classification_PPB,Accuracy_parity}, which aligns with Type I Bias in our definitions.
Furthermore, new fairness criteria such as equalized odds and equal opportunity~\cite{EO_define}, which address disparities in true positive rates and false positive rates across demographic groups, are adopted from demographic parity.














We conjecture that the confusion arises because the term ``bias" in neural networks has been endowed with multiple important meanings over time without well-defined distinctions. This ambiguity leads individuals to interpret different types of predominant biases from the same term.
Specifically, some individuals associate the primary bias with performance disparity due to the critical role of model performance in model evaluation.
Conversely, other individuals prioritize prediction disparity since it is the prevalent bias deeply embedded in real-world scenarios.
Consequently, denoting these two different but predominant biases with the single term ``bias" results in misunderstandings in the broader literature.

















\subsection{Preconception about Bias}
The preconception of researchers about bias, stemming from their specific relevant fields, also contributes to the confusion.
Specifically, bias issues encompass a wide range of relevant fields, some of which are associated with Type I Bias and others with Type II Bias.
For instance, Type I Bias involves long-tail distribution~\cite{long_tail}, catastrophic forgetting~\cite{kirkpatrick2017overcoming}, domain adaptation~\cite{li2014learning}, and various biometric tasks~\cite{xiao2023name,hutiri2022bias}.
In contrast, Type II Bias involves shortcut learning~\cite{shortcut_learning}, simplicity bias~\cite{simplicitybias}, invariant representation learning~\cite{DP_FFVAE}, out-of-distribution challenges~\cite{shen2021towards}.
In this sense, researchers from diverse fields hold their own preconceived notions of bias based on their field-specific knowledge. 
For instance, in several biometric tasks (\eg face recognition, face 
detection, face verification) with identity as target and sex as an attribute, uneven performance across sex (the manifestation of Type I Bias) is naturally regarded as bias since the primary focus of biometric systems is on model performance~\cite{robinson2020face}.
However, the dependence between model prediction and attribute (the manifestation of Type II Bias) might not be considered as bias since there naturally only exists non-overlapping targets across attribute~\cite{RFW_IMAN}. For instance, an individual can be categorized as either male or female but not both, thereby resulting in a natural association between identity prediction and specific sex.
Furthermore, due to the absence of clear distinctions regarding bias issues, research groups from different fields may not share a unified perspective on bias and may interpret it differently. However, they use similar bias-related terms in their papers and present them in the same venues, which potentially causes confusion regarding bias issues.























\subsection{Similar Methodologies}
The existing confusion also arises from the overlap in methodologies used to address Type I Bias and Type II Bias.
For instance, to mitigate Type I Bias, several studies~\cite{SensitiveNets,DebFace,pass} enhance the performance for minority groups by preventing the model from encoding the information of protected attribute.
Similarly, to tackle Type II Bias, some methods~\cite{Back_MI,CSAD,learn_not_to_learn_Colored_MNIST} aim to develop representations that are invariant to the protected attribute by minimizing mutual information between the learned representation and the protected attribute.
Both of these methods can be categorized into invariant representation learning~\cite{IRM}.
Furthermore, domain adaptation is also utilized for both Type I Bias~\cite{BAE,MFR} and Type II Bias~\cite{DARE}.
These similarities in methodologies obscure the distinction between Type I Bias and Type II Bias, thereby inducing confusion.


