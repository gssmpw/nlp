\label{sec:definition}








To define and distinguish these two types of biases, we first establish several key concepts. Given a dataset $\mathcal{D}: {\mathcal{X}, \mathcal{Y}, \mathcal{A}}$ consisting of instances ${x, y, a}$ where each sample $x \in \mathcal{X}$ is annotated with an attribute label $a$ (e.g., sex) and a ground truth label $y$ for a specific downstream task (e.g., identity in face recognition), the model $f: \mathcal{X} \to \mathcal{Y}$ takes $x$ as input and outputs the predicted label $\hat{y}$.
In this section, we introduce formal mathematical definitions for these two types of biases, referred to as Type I Bias and Type II Bias, which will be consistently used throughout the paper. In the following sections, we will review \pc papers to demonstrate that various commonly discussed bias issues can be unified using these definitions and explore the phenomena and reasons behind the existing confusion between these bias issues.






































\subsection{Type I Bias} 



\rev{The manifestation of Type I Bias is uneven model performance across different demographic groups~\cite{RFW_IMAN, RL_RBN, DebFace, GAC, MvCoM}.}
Specifically, model performance can be evaluated using various metrics, \eg error rate~\cite{Timnit_sex_classification_PPB,fairnessgan_DP_difference_error_rate}, loss~\cite{representation_disparity}, accuracy~\cite{multiaccuracy}, average precision (AP)~\cite{DP_difference_fpr_GAN_debiasing}, positive predictive value (PPV), 
true positive rate (TPR)~\cite{pass,BR_Net_dataset_vs_task}, false positive rate (FPR)~\cite{FPR_Penalty_Loss}, average false rate (AFR), mean AFR (M AFR)~\cite{inclusivefacenet}, confusion matrix~\cite{DebFace}, F1 score~\cite{BR_Net_dataset_vs_task}, receiver operating characteristic curve (ROC)~\cite{RL_RBN, fairnessgan_DP_difference_error_rate, SAN, Asymmetric_Rejection_Loss,debias_balanced_AUCROC}, area under the ROC (AUC)~\cite{FlowSAN, DebFace, BR_Net_dataset_vs_task}.
All these metrics can be unified under the format of a distance measure $d(\hat{Y},Y)$, evaluated based on model prediction $\hat{Y}$ and ground truth label $Y$.
Thus, we can formally define this type of bias as follows:
\begin{definition}
    \label{def:Type_I_Bias}
Type I Bias. A model $f$ involves \emph{Type I Bias} if $f$ yields uneven performance $d(\hat{Y},Y)$ across attribute $A$,
\begin{align}
    \sup_{a,a'\in \mathcal{A}, d \in \mathcal{M}} \abs{d(\hat{Y},Y|A=a) - d(\hat{Y},Y|A=a')} > 0
\end{align}
\end{definition}
\noindent
where $a,a'$ are possible values of $A$ (\eg female and male), and $\mathcal{M}$ is the set of all potential performance metrics.






\subsection{Type II Bias}

\rev{On the other hand, the manifestation of Type II Bias is dependence between model prediction and attribute~\cite{BlindEye_IMDB_eb,learn_not_to_learn_Colored_MNIST,DI, LfF_CelebA_Bias_conflicting,End,CSAD,BCL}.}
Specifically, these attributes can be categorized by sensitive/protected attributes~\cite{Fairalm_DP_difference_false_positive_rate, GDP} (\eg sex in creditworthiness prediction) or spurious attributes~\cite{Group_DRO,JTT} (\eg texture in object recognition).
Both of these scenarios can be unified as the dependence between model prediction and the specific attribute.
Thus, we can formally define this type of bias as follows:
\begin{definition}
    \label{def:Type_II_Bias}
Type II Bias. A model $f$ involves \emph{Type II Bias} if model prediction $\hat{Y}$ is not independent with attribute $A$,
\begin{align}
\sup_{a,a' \in \mathcal{A}} \abs{P(\hat{Y}|A=a) - P(\hat{Y}|A=a')} > 0 
\end{align}
\end{definition}

\noindent
where $a,a'$ are possible values of $A$ (\eg female and male).


