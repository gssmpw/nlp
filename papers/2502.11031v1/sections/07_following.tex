\label{sec:following}

In this section, we present a more comprehensive comparison between Type I Bias and Type II Bias based on our investigation of \pc papers.
Our comparison encompasses multiple aspects including the underlying causes, debiasing methods, evaluation protocol, prevalent datasets, and future directions.
Most notably, for each type of bias, we summarize debiasing methods in~\cref{tab:debiasing_methods}, bias assessment metrics in~\cref{tab:bias_assessment_metrics}, and prevalent datasets in~\cref{tab:datasets_I,tab:datasets_II}.
We hope the comparison can alleviate the cognitive burden from the prevailing confusion between these two types of biases and serve as a roadmap for new researchers to follow.


\input{sections/tables/debiasing_methods}
\input{sections/tables/bias_metrics}

\subsection{Type I Bias} 

\subsubsection{Underlying causes}
Data imbalance across different demographic groups in the training set is commonly accepted as the possible cause for Type I Bias~\cite{cherepanova2023deep,roosli2022peeking}.
Specifically, real-world data often exhibits the long-tail distribution where some demographic groups yield fewer samples than other groups~\cite{long_tail}. 
Consequently, given the data-driven nature of neural networks, models may be effectively trained in groups with sufficient samples but undertrained in groups only with limited samples, hence resulting in performance disparity across different groups and lower performance for minority groups.
On the other hand, recent work suggests that Type I Bias can manifest even when the training set is balanced across demographic groups~\cite{RL_RBN}.
This challenges the conventional understanding of the causes of Type I Bias but promotes the discussion of other possible causes. For instance, Type I Bias may be induced by the underrepresentation of specific demographic groups~\cite{spurious_correlation_Underrepresentation} or the intrinsic challenges associated with recognizing and classifying specific demographic groups~\cite{FR_inherent_bias}. 





\subsubsection{Debiasing methods}
Addressing Type I Bias essentially involves optimizing the model to enhance its performance for minority groups while maintaining its performance for majority groups.
The strategies can be broadly classified into three main categories based on the stage when the debiasing intervention is applied relative to the model training phase: pre-processing, in-processing, and post-processing.
First, pre-processing methods intervene before the training phase. 
They are primarily designed based on the cause of Type I Bias (the imbalanced distribution across demographic groups in the training set).
For instance, the straightforward approach is to construct a balanced real dataset for training~\cite{Fairface} or supplement minority groups with sufficient synthetic training samples~\cite{CAT}.
Another approach in this category involves strategically resampling to increase the occurrence of samples from minority groups or reweighting to assign higher importance to samples from underrepresented groups~\cite{RL_RBN}.
Second, in-processing methods are integrated during the model training phase.
Most notably, domain adaptation techniques~\cite{BAE,MFR} adapt well-learned representations from the majority group to the minority group; and, attribute removal methods leverage adversarial learning~\cite{DebFace,pass} to remove demographic information from learned representation.
Lastly, post-processing methods apply debiasing techniques after the training process.
One common technique is to calibrate the model predictions, ensuring that they adhere to specific fairness criteria (\eg equalized odds)~\cite{calibrated_Eodds}. 















\subsubsection{Evaluation protocol}
The effectiveness of methods addressing Type I Bias is evaluated by performance disparity between majority and minority groups. 
In the case of binary attributes, the disparity is directly gauged by performance difference between majority and minority groups~\cite{Timnit_sex_classification_PPB,DP_difference_fpr_GAN_debiasing,FPR_Penalty_Loss}.
In the case of non-binary attributes, the disparity is gauged by the standard deviation of performance across all demographic groups (STD)~\cite{DB_VAE_algorithmic_bias, DebFace, GAC, Asymmetric_Rejection_Loss}.
To assess performance, there are a variety of metrics such as error rate~\cite{Timnit_sex_classification_PPB,fairnessgan_DP_difference_error_rate}, loss~\cite{representation_disparity}, accuracy~\cite{multiaccuracy}, average precision (AP)~\cite{DP_difference_fpr_GAN_debiasing}, positive predictive value (PPV), true positive rate (TPR)~\cite{pass,BR_Net_dataset_vs_task}, false positive rate (FPR)~\cite{FPR_Penalty_Loss}, average false rate (AFR), mean AFR (M AFR)~\cite{inclusivefacenet}, confusion matrix~\cite{DebFace}, F1 score~\cite{BR_Net_dataset_vs_task}, receiver operating characteristic curve (ROC)~\cite{RL_RBN, fairnessgan_DP_difference_error_rate, SAN, Asymmetric_Rejection_Loss,debias_balanced_AUCROC}, area under the ROC (AUC)~\cite{FlowSAN, DebFace, BR_Net_dataset_vs_task}.
Furthermore, besides these metrics to assess performance disparity, the performance improvement in minority groups compared to the baseline is provided for an intuition of debiasing effectiveness, along with overall performance to illustrate that it is not compromised.




\subsubsection{Datasets}
Datasets used to investigate Type I Bias mainly exhibit long-tail distributions.
Most notably, several benchmark biometric datasets including LFW~\cite{LFW}, IJB-A~\cite{IJBA}, IJB-C~\cite{IJBC}, and RFW \cite{RFW_IMAN}, are frequently utilized.
A comprehensive list of datasets is presented in~\cref{tab:datasets_I}.


\input{sections/tables/datasets_I}






\subsubsection{Future directions}
One promising future direction is to delve into the root cause of Type I Bias since the formerly widely accepted cause (data imbalance) has been challenged by the experiment that Type I Bias exists even for a balanced dataset~\cite{RL_RBN}.
Furthermore, exploring more effective debiasing methods to achieve even performance across cohorts is always of significant importance, hence it is a valuable direction.





















\subsection{Type II Bias}

\subsubsection{Underlying causes}
The association between prediction targets and attributes in the training set is widely considered the possible cause of Type II Bias~\cite{LfF_CelebA_Bias_conflicting,CSAD,ECS}. 
Different from Type I Bias, which originates from an uneven distribution of samples across attributes, Type II Bias arises from an uneven distribution of \emph{specific target groups} across attributes. 
Specifically, the collected data may encompass a greater number of samples annotated with specific pairs of target labels and attribute labels (\eg $(y^1,a^1)$ and $(y^2,a^2)$) than other combinations. 
Models trained on this dataset may leverage these attributes as shortcut features to simplify the training process rather than acquiring more comprehensive features. 
Consequently, when applying the trained models in real-world scenarios where the association does not generally exist, they may still rely on these attributes for decision-making and yield predictions that depend on these attributes, thereby resulting in a higher frequency of particular prediction outcomes for particular groups and further unfair treatment for these groups.








\subsubsection{Debiasing methods}
Addressing Type II Bias essentially involves acquiring representations that are independent of the attribute while remaining informative for a wide range of downstream tasks~\cite{FNF}.
Similar to Type I Bias, the strategies can be classified into three categories: pre-processing, in-processing, and post-processing.
First, pre-processing approaches can be further sub-categorized into dataset construction and data preprocessing.
Dataset construction mainly encompasses collecting large-scale universal datasets to lessen the likelihood of spurious correlation between the target and the attribute~\cite{extreme_bias, sabaf}, and generating counterfactual synthetic samples to augment the original biased training set, thereby reducing its inherent bias strength~\cite{CGN,BiaSwap,Camel,DP_difference_fpr_GAN_debiasing}.  
Data preprocessing mainly encompasses fairness through unawareness~\cite{fairness_through_awareness}, which directly eliminates attributes from the input data, and domain randomization~\cite{domain_randomization} to utilize domain knowledge to assign a random value to the attribute label for each sample, thereby rendering it irrelevant to the target prediction. 
Second, in-processing approaches can be further divided into two subgroups: methods that either explicitly or implicitly minimize the mutual information (MI) between the learned latent features and the specific attribute.
Specifically, several methods directly minimize mutual information between the latent representation for the target classification and the protected attributes to learn a representation that is predictive of the target but independent of the attributes~\cite{learn_not_to_learn_Colored_MNIST,Back_MI,CSAD}. 
Another group of methods applies adversarial learning with surrogate losses~\cite{LfF_CelebA_Bias_conflicting,BlindEye_IMDB_eb,gradient_projection} to implicitly reduce the mutual information or utilize domain-invariant learning~\cite{ganin2016domain, zhao2019learning, albuquerque2019generalizing, Group_DRO, PGI_invariant, EIIL} to minimize classification performance gap across groups by mapping data to a space where distributions are indistinguishable while maintaining task-relevant information. 
Lastly, for the post-processing method, domain-independent learning~\cite{DI} learns an ensemble classifier comprising separate classifiers for each demographic group by sharing representations, thereby ensuring that the prediction from the unified model is not biased towards any domain.











\subsubsection{Evaluation protocol}
The effectiveness of methods addressing Type II Bias is evaluated by prediction disparity across different groups.
In the prevalent evaluation protocol, models are trained on a dataset where the target is associated with the attribute and tested on a held-out dataset where such association is absent~\cite{learn_not_to_learn_Colored_MNIST, Back_MI, CSAD}.
Subsequently, the testing accuracy is reported to evaluate the model capability to reduce the effect of association in the training set (the effectiveness to mitigate Type II Bias)~\cite{DI}.
Several studies also present the accuracy of worst-case groups, where the samples yield the opposite of bias present in training set~\cite{Group_DRO, JTT, confused_dataset_bias_DFA}.
Furthermore, we summarize other commonly-used bias assessment metrics in~\cref{tab:bias_assessment_metrics}.
A noteworthy distinction in these bias assessment metrics for Type II Bias compared with Type I Bias is the absence of necessity for ground truth labels. This distinction is attributed to the fact that Type II Bias is defined as the dependence between model prediction and attribute, eliminating the need for ground truth, while evaluating Type I Bias necessitates ground truth to assess model performance.






\subsubsection{Datasets}
Most notably, several census datasets, including Adult income dataset~\cite{adult_dataset_and_german_dataset}, German credit dataset~\cite{adult_dataset_and_german_dataset}, and COMPAS recidivism dataset~\cite{COMPAS}, are employed as benchmark datasets to investigate the impact of sensitive/protected attributes in real-world decision-making processes. 
Additionally, computer vision and natural language processing communities also develop various datasets to investigate Type II Bias, \eg Colored MNIST~\cite{learn_not_to_learn_Colored_MNIST}, CelebA~\cite{CelebA,LfF_CelebA_Bias_conflicting}, Waterbirds~\cite{Group_DRO}, and CivilComments-WILDS~\cite{CivilComments,Wilds}.
A comprehensive list of datasets is summarized in~\cref{tab:datasets_II}.







\input{sections/tables/datasets_II}





\subsubsection{Future directions}
One promising research direction is to explore the strong bias region~\cite{extreme_bias} of Type II Bias, where the target and the attribute are strongly associated in the training set, a scenario that is overlooked by many existing work~\cite{BlindEye_IMDB_eb, learn_not_to_learn_Colored_MNIST}.
Also, it is important to further explore more challenging scenarios where attribute labels are absent~\cite{HEX_texture_bias1, ReBias_texture_bias2,rubi} or unknown biases emerge~\cite{discover_unknown_bias,CNC,EIIL}. 



















































\subsection{Summary}
In this section, we highlight the distinctions between Type I Bias and Type II Bias across multiple aspects and provide further explanations on the comparison in~\cref{tab:teaser}.

\begin{itemize}

    \item Manifestation. A model exhibiting Type I Bias yields uneven performance across different groups and lower performance in minority groups, whereas a model exhibiting Type II Bias depends on attributes for decision-making and produces specific predictions that are highly associated with specific attributes. 
    
    \item Disparity. Type I Bias refers to the disparity in prediction performance across attributes, whereas Type II Bias refers to the disparity in prediction outcomes across attributes.
    
    \item Causes. Type I Bias stems from insufficient training of underrepresented groups, whereas Type II Bias arises from the association between targets and attributes.
    
    \item Dataset inducing bias. An imbalanced distribution of samples across attributes induces Type I Bias, whereas an imbalanced distribution of \emph{specific target groups} across attributes induces Type II Bias.
    
\end{itemize}




































