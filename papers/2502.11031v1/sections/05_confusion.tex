\label{sec:confusion}

In the previous section, we categorize \pc papers, that discuss prevalent biases, into two groups based on the manifestation of bias they address.
The criteria for this categorization are clearly outlined in~\cref{tab:unification}. 
Furthermore, the distinctions between these two types of biases are illustrated in~\cref{def:Type_I_Bias,def:Type_II_Bias}.
However, as summarized in~\cref{tab:confusion}, there is substantial confusion between them in existing literature, which poses challenges for researchers to investigate bias issues. 
Thus, it is crucial to clarify the confusion and underscore the distinctions between these two types of biases.
To this end, in this section, we primarily highlight several prevailing confusions and the potential consequences that arise from overlooking them, based on the investigation of \pc papers.
In the following sections, we analyze the possible reasons behind these confusions (\cref{sec:reason}) and provide a clear distinction between these biases to alleviate these confusions (\cref{sec:distinction}).




\input{sections/tables/confusion}












\subsection{Ambiguity of Terminology}
One of the confusions is the ambiguity surrounding the terminology of bias.
This ambiguity manifests in three primary ways.
First, several papers adopt vague terminology such as ``bias issues" or simply ``bias" without clarifying the particular type of bias they address~\cite{DI}.
Furthermore, other commonly used terms such as ``model bias" or ``algorithmic bias" are also ambiguous, as they might represent either the bias that manifests in the model or the bias that originates from the model itself. 
Second, studies often denote bias from varied aspects~\cite{hirota2022gender, markl2022language}.
For instance, some papers refer to ``demographic bias", ``gender bias", or ``racial bias", emphasizing bias from the perspective of demographic statistics.
In contrast, other works utilize ``dataset bias", ``model bias", or ``algorithmic bias", indicating the source of bias.
Third, the existing literature frequently uses the same terms to describe different kinds of biases~\cite{MvCoM, Back_MI}, as summarized in~\cref{tab:bias_term}.


\input{sections/tables/bias_terminology}


\noindent
\textbf{Consequences.}
The ambiguity of terminology undermines the clarity of the intended statement and may further lead to misdirected debiasing techniques. 
For instance, in the abstract of the paper~\cite{BA}, the authors claim that:
\begin{itemize}
    \item \emph{``We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias."}~\cite{BA}
\end{itemize}
In this case, the lack of clarity around the term ``gender bias" weakens the significance of the findings. 
Furthermore, the scope of this ambiguity is extensive.
Specifically, sections including ``Title", ``Abstract", ``Introduction" and ``Related Work" are often impacted, as there may lack sufficient context for a precise interpretation~\cite{sadeghi2019global, gordaliza2019obtaining}.
More concerned, the vagueness may persist throughout the entire paper~\cite{DB_VAE_algorithmic_bias} if the addressed bias is not dis-ambiguously clarified in ``Problem Statement" or evaluation protocol in ``Experiments" section.














\subsection{Inaccurate Motivation}
Another confusion is that existing work addressing these two types of bias inaccurately cites each other for their own motivation. 
For instance, some studies~\cite{Back_MI, BlindEye_IMDB_eb} that address Type II Bias motivate themselves from the uneven performance in face recognition, a manifestation of Type I Bias. 
Other work~\cite{DeepFR_survey,FairCal} that tackles Type I Bias in debiasing face recognition is motivated by the correlation between model predictions and spurious attributes in facial attribute classification~\cite{BlindEye_IMDB_eb}, a manifestation of Type II Bias. 
Furthermore, this confusion is aggravated as some papers are motivated by semi-relevant work.
Specifically, as highlighted by~\cite{FVRT3}, debiasing face recognition literature~\cite{FairCal,RFW_IMAN,RL_RBN} tend to be motivated by the manifestation of worse accuracy for minority groups in sex classification~\cite{Timnit_sex_classification_PPB}, rather than the direct issue of uneven performance in face recognition~\cite{robinson2020face, pahl2022female}.



\noindent
\textbf{Consequences.}
Inaccurate motivation leads to misunderstanding and misalignment in the existing literature. 
Furthermore, this issue may compound over time, as the subsequent work built upon the papers with such inaccurate motivation will perpetuate the confusion.











\subsection{Lack of Terminology Reuse}
The confusion also manifests in the introduction of overfull new terms in different papers addressing the same bias.
For instance, ``minority group bias"~\cite{minority_group_vs_sensitive_attribute}, ``dataset bias"~\cite{BR_Net_dataset_vs_task}, and ``bias as underrepresentation"~\cite{spurious_correlation_Underrepresentation} are all used to denote uneven performance across attributes (Type I Bias).
\begin{itemize}
    \item \emph{``Dataset bias is often introduced due to the lack of enough data points spanning the whole spectrum of variations with respect to one or a set of protected variables."}~\cite{BR_Net_dataset_vs_task}
    \item \emph{``Minority group bias. When a subgroup of the data has a particular attribute or combination of attributes that are relatively uncommon compared to the rest of the dataset, they form a minority group. A model is less likely to correctly predict for samples from a minority group than for those of the majority."}~\cite{minority_group_vs_sensitive_attribute}
    \item \emph{``[...] `bias' means that one appearance of an object is underrepresented."}~\cite{spurious_correlation_Underrepresentation}
\end{itemize}

\noindent
Similarly, ``sensitive attribute bias"~\cite{minority_group_vs_sensitive_attribute}, ``task bias"~\cite{BR_Net_dataset_vs_task}, and ``bias as spurious correlation"~\cite{spurious_correlation_Underrepresentation} all signify the dependence between model prediction and attribute (Type II Bias).
\begin{itemize}
    \item \emph{``Task bias, on the other hand, is introduced by the intrinsic dependency between protected variables and the task."}~\cite{BR_Net_dataset_vs_task}
    \item \emph{``Sensitive attribute bias. A sensitive attribute (also referred to as ``protected") is one which should not be used by the model to perform the target task, but which provides an unwanted “shortcut” which is easily learned, and results in an unfair model."}~\cite{minority_group_vs_sensitive_attribute}
    \item \emph{``[...] considering bias in the form of spurious correlations between the target label and a sensitive attribute which is predictive on the training set but not necessarily so on the test set."}~\cite{spurious_correlation_Underrepresentation} 
\end{itemize}



\noindent
\textbf{Consequences.}
These inconsistent definitions can further contribute to confusion with some highlighting the manifestation of the bias while others delving into the underlying causes of the bias.
Furthermore, without a unified terminology for the predominant biases, it becomes challenging to systematically gather and compare relevant work.








\subsection{Abuse of Bias Assessment Metrics}
The usage of bias assessment metrics exhibits the confusion in two primary ways.
First, the bias assessment metrics, which are designed independently of debiasing methods, are rarely used~\cite{RLB,Directional_BA}.
Instead, many works tend to introduce their own metrics to demonstrate the effectiveness of the proposed debiasing method~\cite{model_leakage,BA}, which leads to an overwhelming number of metrics.
Second, some studies inappropriately employ indirect bias assessment metrics or even metrics that are not designed for the specific bias they address.
For instance, several studies~\cite{FURL_PS,Fairalm_DP_difference_false_positive_rate} motivated by the dependence between model prediction and attributes (the manifestation of Type II Bias) use true positive rate (TPR) difference and false positive rate (FPR) difference for evaluation. 
However, as highlighted by~\cite{Directional_BA}, metrics such as TPR difference, FPR difference, accuracy difference, and average mean-per-class accuracy difference, are not suitable for evaluating Type II Bias since they fail to consider the dependence between target and attribute in the training set and cannot distinguish between an increase or decrease of dependence in learned representation. 

\noindent
\textbf{Consequences.}
The abuse of bias assessment metrics leads to inaccurate evaluations of debiasing performance in relation to the specific type of bias being addressed, hence exacerbating confusion in the field.
Furthermore, it also complicates the comparison between different debiasing methods and hinders the construction of a unified evaluation protocol.













\subsection{Weak Existing Distinction}
Despite the evident confusion in the literature, numerous studies, especially survey papers, have not sufficiently distinguished Type I Bias and Type II Bias. 
Furthermore, the confusion is not only widespread but has also persisted for a significant duration, as shown by the timeframes of the investigated papers.
However, the bias taxonomy, presented in surveys over time~\cite{causal_based_fairness,MLbias_survey,datasets_ML}, may fail to clearly differentiate between these two types of biases. 
Alarmingly, a recent and high-cited survey on machine learning bias~\cite{MLbias_survey} scarcely cites papers that discuss Type II Bias stemming from spurious correlations between target and attribute, thereby overlooking the distinction from Type I Bias.

 
 

\noindent
\textbf{Consequences.} 
The weak distinction between these two types of biases in existing surveys will exacerbate the prevailing confusion in this field over time. 
Consequently, due to the lack of clarity, which surveys were originally designed to provide concerning the categorization of bias issues, these bias issues will eventually be undesirably conflated.

