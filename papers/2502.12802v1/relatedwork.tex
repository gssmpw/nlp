\section{RELATED WORK}
\label{sec:Related Work}
\subsection{Time Series Forecasting}
The past few years have witnessed the rapid development of TSF. Time series forecasting has many challenges, such as capturing long-term dependencies, noise sensitivity, computational efficiency, and so on \cite{nie2022time,eldeletslanet}. The prevalent methods in TSF can be categorized into three main groups: Classical methods, Transformer-based methods, and Frequency-domain methods.

\textbf{Classical methods.} Bai et al. \cite{bai2018empirical} propose a Temporal Convolutional Network (TCN) architecture combined with CNN, which achieves better performance than RNN while avoiding the common drawbacks of recursive models, such as the gradient explosion/disappearance problem.
Dual Self-Attention Network (DSANet) \cite{huang2019dsanet} is proposed for multi-variable TSF, which uses two parallel convolution components to capture a complex mixture of global and local temporal patterns.
Temporal Pattern Attention mechanism (TPA) is proposed in \cite{shih2019temporal}, which extracts the fixed-length temporal patterns in the input information by using CNN filters. TPA determines the weights of each temporal pattern using a scoring function, and obtains the final output according to the magnitude of the weights.
Additionally, the Multiscale recurrent network (MRN) \cite{guo2023multivariate} seamlessly integrates multiscale analysis into deep learning frameworks to build scale-aware recurrent networks.
A novel sequence-to-sequence-based deep learning model utilizing a time series decomposition strategy for multistep load forecasting is proposed in \cite{lu2024novel}. This model can provide better insights for optimizing energy resource allocation and assisting in the decision-making process.
Furthermore, Bai et al. \cite{bai2020adaptive} propose Adaptive Graph Convolutional Recurrent Network (AGCRN) combined with GNN to capture the fine-grained spatio-temporal correlations of traffic sequences automatically.

\textbf{Transformer-based methods.} The emergence of Transformer architectures has revolutionized the ability to capture long-term dependencies in time series data. Li et al. \cite{li2019enhancing} propose the transformer-based architecture to process time series data capturing the long-term dependencies. PatchTST \cite{nie2022time} segments the time series into subseries-level patches and adopts the channel-independence to improve the long-term forecasting accuracy. iTransformer \cite{liu2024itransformer} embeds the time points into variate tokens and applies the feed-forward network for each variate token to empower the Transformer family with promoted performance. However, DLinear \cite{zeng2023transformers} adopts a single linear layer to extract the dominant periodicity from the temporal domain and surpasses a range of advanced complex networks.

\textbf{Frequency-domain methods.} On the other hand, many methods have been proposed for TSF from the frequency domain, which are orthogonal to our methods. FITS \cite{xu2023fits} addresses time series forecasting through interpolation in the complex frequency domain, while FreDF \cite{wang2024fredf} avoids the complexity of label autocorrelation in the time domain by focusing on forecasting in the frequency domain.

Despite the progress in these areas, existing methods often treat time series data as single-pattern data, neglecting the diverse intrinsic mechanisms inherent in time series. This oversight results in varying learning capabilities for distinct patterns, ultimately leading to less accurate predictions. In contrast, we first mine the different patterns of the data and model them by predictor to enhance forecasting performance.

\subsection{Simultaneous Classification and Forecasting}
Unlike traditional single tasks, associative tasks can improve performance by sharing information and complementing each other. Specifically, the interaction of unrelated parts between tasks helps to escape local minima. In addition, the relevant parts between tasks facilitate learning generic feature representations at the bottom shared layer.

Combining classification with forecasting has demonstrated a significant potential in time series forecasting.
There are two main types. The first type is the two-step method. Tan et al. \cite{tan2018geomagnetic} propose the method to predict the $Kp$ geomagnetic index, which first predicts the outputs are storm or non-storm, and then uses two separate submodels to forecast the $Kp$ with and without a geomagnetic storm.
DXtreMM \cite{abilasha2022deep} improves the extreme events performance by composing a classifier and many forecasting units. The classifier predicts the possibility of occurrence of an extreme event given a time segment, and the forecasting module predicts the exact value.
However, these methods consume a lot of time and computing resources due to the training of multiple sub-models.


Furthermore, many strategies propose the end-to-end combination of classification and forecasting tasks to improve extreme events performance. 
Abraham et al. \cite{abraham2010integrated} present an integrated framework that simultaneously performs classification and forecasting to accurately predict future values of a zero-inflated time series.
In addition, DEMM \cite{wilson2022beyond} divides the data into three categories according to the magnitude and combines the hurdle model with the extreme value theory to simulate the distribution of different events while accurately making point prediction and distribution prediction.

However, these approaches do not explore the constrained relationship between the two tasks, i.e., even if the classification is correct to class $k_i$, the predicted result is likely to be class $k_j$. To address the limitation of the co-existence of two tasks, this paper presents a novel approach that introduces a guiding relationship between them.\\