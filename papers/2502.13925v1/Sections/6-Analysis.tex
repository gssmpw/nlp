\section{Analysis}
Our analysis addresses the following questions:
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{fig/crop_case.pdf}
\caption{Sample outputs of our three tasks generated by different vision language models, along with gold truth. We highlight errors in distractors. }
\label{fig:case}
\vspace{-4mm}
\end{figure*}

\paragraph{Does fine-tuning with reorder task help?}
Yes, it does. We fine-tune Qwen2-VL using 3,160 samples for one epoch. This not only significantly improves performance on the reordering VQA task but also enhances comprehension tasks.
To construct the training dataset, we applied data augmentation to 790 images using the reorder task. Specifically, we randomly shuffled the sequence of images four times, generating a total of approximately 3,160 distinct samples. For evaluation on the reorder task, we used only the remaining 100 samples. For the comprehension task, we conducted a full test set evaluation, as the training data provided only images without any analytical content. Meanwhile, we excluded the frame prediction task from testing due to potential data leakage. The experimental results are presented in the table~\ref{tab:finetune}. 
Overall, our reordering data is useful for fine-tuning, as it can enhance the LMMs to reason on sequential images. However, the ultimate performance still depends on the base capability of the model. From the table, it can be seen that Qwen2.5-VL benefits greatly from fine-tuning, with improvements not only in the reordering task but also in comprehension tasks. The improvement in generation tasks in the reordering task is much larger than that in choice tasks. We believe this is because the generation task's instructions are very challenging, and the model has not been trained on them before, leading to difficulty in solving them. On the other hand, for choice tasks, the model can make educated guesses based on the options provided. Due to the relatively small amount of training samples, the model's improvement in choice tasks is limited. In contrast, LLaVA shows improvement only in the reorder-choice task after training.


\begin{table}[htbp]
    \centering
     \resizebox{0.45\textwidth}{!}{% 缩小整体表格
    \begin{tabular}{c|c|c|c}
    \toprule
        Tasks  & Comprehension & Reordering-G & Reordering-C \\
        \midrule
       Qwen2-VL & 58.53 & 6.00 & 31.00 \\
       +finetune & 62.94 & 31.00 & 38.00 \\
       \midrule
       LLaVA-1.6 & 34.41 &3.00& 26.00 \\
       +finetune & 33.82 &2.00 & 32.00 \\
       \bottomrule
    \end{tabular}}
    \caption{Performance on Qwen2-VL-7B and LLaVA-1.6-7B finetuned with reordering task data. Reordering-C refers to the Reordering-choice. Reordering-G refers to the Reordering-generation}
    \label{tab:finetune}
\end{table}

\paragraph{Does GPT-4o understand sequence images as well as humans?}
While GPT-4o achieves the highest performance among all tested models, there remains a substantial gap between its capabilities and human performance, particularly in novel tasks like frame reordering. Through our preliminary data annotation experiments, we observed that while GPT-4o can comprehend basic comic content and provide interpretations, it frequently generates hallucinated content and struggles with comics that depict unconventional or imaginative scenarios rarely encountered in real life.

In the visual narrative comprehension and next-frame prediction tasks, the multiple-choice format allows models to leverage similarity matching between options. Our investigation revealed that model performance is heavily influenced by the quality of distractor options. In initial experiments with weak distractors (generated using GPT-4o with instructions to provide distractors with hallucinations, the prompt is followed HalluEval), the model achieved accuracy rates up to 90\%. Upon analysis, we found these initial distractors were too obviously incorrect or irrelevant to the comic content, making the selection task trivial. To address this limitation, we carefully curated a new set of challenging distractors. With these enhanced distractors, performance of GPT-4o decreased significantly to more realistic levels ($61.60\%$ for understanding and $69.95\%$ for prediction), better reflecting the true challenges in comic comprehension. The scores obtained from multiple-choice questions with semantically transparent options tend to be inflated. In subsequent reordering tasks, where options lack explicit semantic meanings, coupled with open-ended questions, the scores provided a more authentic assessment of the LMMs.
% 这种选择题的形式（且选项内容有直接的含义）测出来的分数是偏高的。后来在reorder中，选项内容没有直接的实际含义，他的分数并且还测了问答题。这个任务的分数比较真实的反应模型的能力。

% 总的来说，模型的表现还是不如人类。尤其是在新颖的任务reorder中，表现差距非常大。在前面测试4o是否可以拿来标数据的时候，我们发现模型可以理解漫画的部分内容并做出一些解读，但是也会经常有幻觉内容。对于一些脑洞大开在现实生活中很少见的漫画，模型就很难理解。在visual narrative comprehensive 和 next-frame prediction 任务中，因为是选择题的形式，所以可以根据选项内容进行相似度匹配。模型的选择受干扰项的影响很大。在最初的任务标注中，干扰项使用4o模型直接告诉他要生成带幻觉的干扰项，这些弱干扰项和答案一起输给LMM时，4o的准确率可以达到90%。经过分析例子，我们发现是那些弱干扰项错误的非常明显，所以四选一可以轻松的选对答案。后面我们又生成了新的强干扰项，然后模型的准确率才下降。
% 怎么讲清楚，understanding，prediction的选项内容跟reorder的是不一样的？前者有直接的含义，而后者需要转义无实际含义。
\paragraph{Does input format of images influence performance?}


% 每个帧的单独输入可以帮助模型更清楚地提取每一格的信息，而不会因为它们挤在一张图上导致信息混杂。
% 帧间逻辑更明确：通过人工提供帧的顺序，模型能更好地理解帧之间的逻辑关系（比如时间顺序、因果关系等），避免因整图布局复杂而误解帧的排列。
% 但是也考察模型对多图片的整合处理方式。
%我们做了更细粒度的实验，把整图按照frame拆分，然后一次性按序输入。这种方式对于人类来说会更简单，但是由于模型处理多图走的是video方式，这可能对模型来说也是挑战。结果表明差别不大，所以我们进一步把帧的顺序打乱继续测模型对隐含意的理解。
Considering the distinct computational pathways that LMMs employ in processing individual versus multiple images, we designed following experiments to measure the differential impact of varied input formats using Qwen2.5VL as our test case. We compared three input formats: (1) whole image - the entire comic strip as a single image, (2) sequential frames - individually separated frames input in order, and (3) shuffled sequence - separated frames input in random order. Table~\ref{tab:understanding} shows surprisingly consistent performance across all three formats ($56.03\%$, $54.56\%$, and $57.65\%$ respectively).

This consistency suggests that while separated frames might theoretically help models extract clearer information from each panel and avoid visual confusion from complex layouts, the current video-like processing mechanism used by LMMs for multiple images might not fully capitalize on these advantages. The similar performance with shuffled sequences further indicates that models might rely more on individual frame content rather than sequential relationships for understanding tasks.


\begin{table}[htbp]
    \centering
     \resizebox{0.45\textwidth}{!}{% 缩小整体表格
    \begin{tabular}{c|c|c}
     \toprule
       Input  & GPT-4o-mini & Qwen2.5-VL \\
       \midrule
       Whole Image  & 53.23 & 56.03 \\
      Image Sequence & 51.03 &  54.56 \\
       Shuffled Sequence & 49.56 & 57.65 \\
       \bottomrule
    \end{tabular}}
    \caption{Performance with different input format for understanding task. }
    \label{tab:understanding}
\end{table}
\paragraph{Does implicit meaning help reordering task?}
% 新的prompt = f"The sequence of the comic strips provided below is incorrect, Your task is to find out the correctorder of the comic strips based on the storyline, temporal relationships, and common-sense logic. Here is thedepiction of the comic strips: \"fga.get('understanding’)}\" \n Number each comic strip in the order they shouldappear, starting from 1.\n\n" 
To investigate whether poor reordering performance stems from inadequate semantic understanding, we enhanced the reordering task by providing explicit semantic annotations along with shuffled images. As shown in Table~\ref{tab:reorder_enhanced}, this additional semantic information only marginally improved performance (from 30.01\% to 32.54\%). This modest improvement suggests that the bottleneck in reordering tasks lies not in semantic understanding but in the fundamental capability to reason about temporal and logical sequences in visual narratives.

\begin{table}[htbp]
    \centering
     \resizebox{0.45\textwidth}{!}{% 缩小整体表格
    \begin{tabular}{c|c|c}
    \toprule
        Input & GPT-4o-mini & Qwen2.5-VL \\
        \midrule
       Shuffled image & 26.07  & 30.01 \\
       +Meaning & 28.40 & 32.54 \\
       \bottomrule
    \end{tabular}}
    \caption{Performance with enhanced data (correct answer for comprehension task) for reordering task.}
    \label{tab:reorder_enhanced}
\end{table}
% 从表中可以看到，给模型输入额外的漫画含义后，排序的分数并没有很大的提升。所以LMMs在reorder任务上的瓶颈并不是理解图片含义。以后的模型应该增强对此任务的训练。

% \paragraph{Can video-VLM outperform VLLM?}
% Video-LLM is trained with video data containing richer temporal information. Comic strip is like the key frame for video, so it omitted the step of extracting keyframes from the video. 


% \paragraph{Does CoT help with three tasks?}
% To achieve better performance, we conducted experiments using the Chain of Thought (CoT) approach by adding "Let's think step by step" to the input. However, as shown in table~\ref{tab:cot}, we found that this modification did not improve performance on the first two tasks and only provided a boost in the reorder task. We speculate that this is because the instructions for the reorder task are relatively more complex compared to the other tasks.
% \begin{table}[htbp]
%     \centering
%      \resizebox{0.45\textwidth}{!}{% 缩小整体表格
%     \begin{tabular}{c|c|c|c}
%     \toprule
%         Tasks  & Comprehension & Frame Prediction & Reordering-MCQ \\
%         \midrule
%        origin input & 56.03  & 64.00 & 29.21 \\
%         +CoT        & 52.94 & 57.50  & 31.69 \\
%        \bottomrule
%     \end{tabular}}
%     \caption{Compare the performance with and without CoT (Chain of Thought).}
%     \label{tab:cot}
% \end{table}

\paragraph{How does model size affect?}
In this section, we will discuss the relationship between model parameters size and reasoning performance for tasks due to the scaling law. We examine on LLaVA and Qwen2.5VL, from 3B scale to 34B scale.
There is a clear scaling effect across model sizes, as demonstrated by LLaVA1.5's performance improving from $34.41\%$ (7B) to $46.03\%$ (13B) to $52.94\%$ (34B). This suggests that model scale plays a crucial role in comprehending implicit meanings in visual narratives.
Figure~\ref{fig:scale} provide a visual representation of the performance trend for five models.

\begin{figure}[t]
\centering
\includegraphics[width=0.96\columnwidth]{fig/model.pdf}
\caption{ Comparison of the accuracy results between Qwen2.5-3B vs Qwen2.5-7B and LLaVA-1.6-7B vs LLaVA-1.6-13B vs LLaVA-1.6-34B}
\label{fig:scale}
\vspace{-4mm}
\end{figure}



\paragraph{Where do LMMs fail?}
We present sample outputs of three tasks generated by vision language models (VLMs) in Figure~\ref{fig:case}. These images are easy for human but hard for VLMs. VLMs can understand one comic strip but they can still make mistakes with reordering task. 

% \paragraph{Does repeating images help with three tasks?}
% % 类型，模型处理inspiring的会更好。对于讽刺类的表现不好。我们推测这是由于模型在训练的时候被灌输大量的积极的内容，而消极内容训练的比较少，并被要求减少负面输出。
% \paragraph{Error analysis}
%  % visual misinterpretation  
%   % hallucination 
%   % 常识  打破镜子而不是窗户

% Our experiments reveal that while models achieve reasonable scores on semantic understanding tasks, their abilities remain superficial. Notably, we observe that model predictions remain  unchanged even when presented with scrambled frame sequences, indicating that current LMMs rely more on matching similar visual elements and semantic concepts across frames rather than truly comprehending the sequential narrative structure of comics. This observation is further supported by their poor performance on frame reordering tasks, highlighting their limitations in temporal reasoning and sequential understanding.