\section{License and Copyright.}
We used original web links to comic images without infringing on their copyright. This work is licensed under a CC BY-NC license. We will open-source all related code for processing image sequences and frames to facilitate the reproducibility of our evaluated image sequences. All annotators participated voluntarily in the annotation process and were provided fair compensation.

\section{Conclusion}
% We propose \dataset{}, a benchmark for visual comic sequence reasoning of LMMs. \dataset{} consists of well-checked and human-AI annotated dataset including: visual narrative understanding, next-frame prediction, and multi-frame reordering. Evaluations are conducted on the leading LMMs, revealing a significant gap between AI and human capabilities in understanding comic strip.
% % 尽管模型在深意理解的任务得分还行，但是在实验中我们观察到，打乱图片顺序给一个无意义的sequence也没有影响模型对深意的理解。并且他在打乱帧的重排任务上表现很差。所以我们可以推断，模型更多是靠部分图片内容和含义的相似度进行匹配，而不是真正的逐帧看漫画分析漫画。所以模型在解决多帧时序的漫画任务的性能还是有待提高的。
% Existing models still have a long way to go in terms of visual deep semantics understanding compared to humans. We hope that the proposed dataset and tasks can pave the way for AI to achieve a deeper understanding of the profound semantics conveyed by images.

We present \dataset{}, a comprehensive benchmark for evaluating Large Multimodal Models' capabilities in visual comic sequence reasoning. Our benchmark comprises meticulously curated and human-AI annotated tasks spanning visual narrative comprehension, next-frame prediction, and multi-frame sequence reordering. Through extensive evaluations of state-of-the-art LMMs, we identify significant performance gaps between AI systems and human capabilities in comic strip understanding.
These findings underscore the considerable challenges that remain in developing AI systems capable of deep visual semantic understanding comparable to human cognition. 

% We believe \dataset{} will serve as a valuable resource for the research community, facilitating progress toward more sophisticated visual narrative understanding systems. Future work should focus on enhancing models' abilities to process sequential information and grasp the temporal relationships inherent in visual storytelling.