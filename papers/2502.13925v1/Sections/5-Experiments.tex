\section{Experiments}

\subsection{Models}
To comprehensively evaluate on LMMs, we conducted zero-shot inference across both commercial and open-source models. Our evaluation suite includes leading commercial models GPT-4o~\cite{hurst2024gpt40} and Gemini1.5-Pro~\cite{Gemini} alongside state-of-the-art open-source alternatives of varying scales: Qwen2.5-VL~\cite{qwen2.5-VL}, Qwen2-VL~\cite{wang2024qwen2}, LLaVA-v1.6~\cite{liu2023llava}, CogVLM~\cite{wang2023cogvlm}, MiniCPM-o-2.6~\cite{yao2024minicpm}, mPlug-Owl2~\cite{ye2023mplugowl2}, InternVL2v5~\cite{chen2024internvl},LLaVA-NEXT-Video~\cite{zhang2024llavanextvideo} and Cambrian~\cite{tong2024cambrian1}. Besides, Janus-Pro~\cite{chen2025januspro}, which unifies multimodal understanding and generation, is included to test the abilities between Unified Model and Vision Language Model.  This diverse selection enables us to analyze how model scale, architecture, and training approaches influence comic comprehensive capabilities. 


% Detailed specifications and inference configurations for each model are provided in Appendix~\ref{appendix:model}.

\subsection{Experimental Details}
% 隐含含义理解和预测帧内容这两个任务都是选择题，因为他们的标准答案是不唯一的很难衡量。如果模型选择了正确的答案选项，则认为是正确的，也就是说accuracy是主要的metric。 而对于排序任务，这一个任务形式非常新颖，对于一个comic strip，其输入顺序可以随便打乱，且答案是确切的。所以这个任务我们既采用了选择题的题型又使用了问答题。
The task prompts is displayed in Table ~\ref{prompt}. For visual narrative comprehension task, model is provided with the whole image. But for next-frame prediction and multi-frame sequence reordering task, LMMs infer with image sequences.
The hyper-parameters for each LMMs in the experiments including possible settings are detailed in Appendix~\ref{appendix:hyper-param}. Furthermore, to assess human capabilities in these tasks, we randomly select 100 questions from the dataset for each task and instruct human evaluators to answer. This allows us to benchmark the performance of human participants against our models, offering a thorough comparison of both human and LMMs proficiency in these specific tasks. 


\subsection{Main Results}
Our comprehensive evaluation reveals that while LMMs show promising capabilities in comprehension and prediction tasks, they significantly underperformed in sequence reordering tasks. Moreover, there remains a substantial performance gap between current models and human performance across all tasks. Unified Model underperformed than Vision Language Model.

\paragraph{Contextual Frame Prediction}
The frame prediction task appears to be the most tractable among the three tasks. GPT-4o achieves the highest score of $69.95\%$, followed closely by Qwen2-VL at $64.00\%$.This demonstrates that the performance gap between closed and open-source models is relatively small for this task. However, Janus-Pro perform notably below expectations ($27.50\%$), possibly due to its unified model architectural.

\paragraph{Visual Narrative Comprehension}
For visual narrative comprehension, we observe a similar pattern but with generally lower scores. GPT-4o leads with $61.60\%$, while other models show varying degrees of capability. 

\paragraph{Temporal Narrative Reordering}
The frame reordering task proves to be the most challenging, with all models performing significantly below human capability. Even the best-performing models struggle to exceed $30\%$ accuracy, with many achieving scores around $25-26\%$, which is slightly higher than random selection. 
Notably, several models (marked with *) are unable to perform this task due to their architectural limitations in processing multiple images simultaneously. For these models, we attempted to accommodate their single-image constraint by concatenating multiple frames horizontally into a single image, with white margins serving as frame boundaries. However, this workaround appears to be suboptimal, as these models likely struggle to properly distinguish individual frame boundaries and maintain the semantic independence of each frame, ultimately leading to their poor performance on the reordering task. 

The poor performance on reordering task suggests that current LMMs, regardless of their scale or architecture, have not yet developed robust capabilities for understanding temporal relationships and sequential logic in visual narratives.



