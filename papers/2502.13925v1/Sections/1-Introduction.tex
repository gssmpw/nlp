\section{Introduction}

\begin{quote}
    \textit{In the space between the panels, human imagination takes separate images and transforms them into a single idea.}\par\raggedleft--- Scott McCloud (1993)
\end{quote}
% "Here in the limbo of the gutter, human imagination takes two separate images and transforms them into a single idea." â€” Scott McCloud, Understanding Comics (1993), Chapter 3
% In the space between the panels, human imagination takes separate images and transforms them into a single idea.

\begin{figure}[t]
\centering
\includegraphics[width=0.98\columnwidth]{fig/crop_image1.pdf}
\caption{An example from the \dataset dataset includes the correct answers for our three sub-tasks: comprehension, frame prediction, and reordering. All these tasks are presented as multiple-choice questions, with distractors excluded due to limited context. Star means correct answer.}
\label{fig:intro}
\end{figure}

Recent advancements in Large Multimodal Models (LMMs), particularly GPT-4o~\citep{hurst2024gpt40}, have yielded significant breakthroughs in a various visual-language tasks, including image captioning~\cite{liu2023llava, Ghandi:2024imagecaption}, visual question answering~\cite{Lu:2023vqa, Zhu:2024minigpt-4}, video understanding~\cite{Zhang:2023video-llama, maaz:2024video-chatgpt}, and so on. LMMs have shown promising efficacy in processing and interpreting visual content~\cite{Yin:2024lmmsurvey}, greatly enhancing their capacity to interact with the real world.

\input{tab/compare}

Despite recent advances, the capability of LMMs to process and reason over sequential images remains underexplored, even though sequential visual inputs are prevalent in real-world applications~\cite{yang2024:deepsemantic, liu2024:iibench}. While existing benchmarks primarily evaluate LMMs on single images, often emphasizing surface-level understanding, they fail to address the complexities of sequential dependencies. In multi-image contexts, the ability to discern implicit meanings and contextual relationships is essential for comprehensive interpretation. In particular, understanding nuanced concepts such as sarcasm~\cite{cai-etal-2019-multi, tang:2024sarcasm}, humor~\cite{PatroL:2021humor, hessel2023:androids}, and other multi-faceted deep meanings~\cite{Zhang:2024CDeep} requires reasoning beyond isolated frames. This gap highlights the need for a deeper investigation into the reasoning capabilities of LMMs over dynamic image sequences.


To address this gap, we introduce \dataset, a novel benchmark designed to assess the reasoning ability of LMMs on temporal image sequences, including contextual structure, temporal relationships among images, and underlying semantics. \dataset consists of three challenging subtasks: visual narrative comprehension, contextual frame prediction, and temporal frames reordering, as shown in Figure~\ref{fig:intro}. 

With \dataset, we aim to advance the development of LMMs in temporal-visual comprehension while identifying their current limitations.

Our comprehensive evaluation of $16$ state-of-the-art LMMs on \dataset reveals a substantial performance gap between AI and human capabilities in sequential image comprehension, especially in the reordering task. Most notably, GPT-4o achieves only $23.93\%$ accuracy in the reordering subtask and trails human performance by $30\%$ in visual narrative comprehension. Further quantitative analysis identifies several key factors affecting the sequential understanding performance of LMMs, highlighting the fundamental challenges that remain in the development of LMMs.

% Our key contributions are as follows:
% We introduce STRIPCIPHER, the first comprehensive benchmark that evaluates LMMs on implicit visual narrative reasoning.
% We propose three novel subtasks (narrative comprehension, frame prediction, and reordering) that challenge existing LMMs beyond single-image understanding.
% We systematically benchmark 9 state-of-the-art LMMs, revealing substantial gaps between AI and human capabilities.
% Through extensive analysis, we uncover key factors affecting LMMs' sequential reasoning, paving the way for future model improvements."