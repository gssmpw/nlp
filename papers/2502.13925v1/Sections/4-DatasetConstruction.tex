\section{Dataset Construction}



% Image Source
% Prerequisite: Annotator Training
% (1) AI-assisted Data Annotation for Answers & Distractor Generation
% (2) Cross-Check Examination
% (3) Subtask Composition
% (4) Dataset Evaluation

We construct our \dataset dataset in a multi-step crowd-sourcing pipeline, including 1) annotator training, 2) data annotation, and 3) cross-check examination. An overall demonstration of our dataset construction pipeline is illustrated in Figure ~\ref{fig:construct}. 

\subsection{Image Source}

We use silent comic strips, comic with panels and no dialogue, as our primary data source. As a distinct art form, comic strips often encapsulate complex narratives within concise visual sequences, addressing deeper themes such as social satire, humor, and inspiration. These characteristics make comic strips a particularly challenging medium for evaluating ability of LMMs to understand visual sequences. 
The dataset comprises samples from well-known comics, such as \textit{Father and Son} and \textit{Peanuts}, along with web-scraped images from \texttt{GoComics} \footnote{https://www.gocomics.com/}, \texttt{Google}, and \texttt{Facebook} \footnote{https://www.facebook.com/}. 
Initially, we collected $1,260$ images and then refined the dataset through a filtering process. We conducted a thorough manual inspection to eliminate unclear, toxic, overly simplistic images, along withmulti-panel comics lacking a clear temporal sequence. Moreover, comics with dialogue will also be removed to prevent the model from using OCR to understand the meaning of the comics through text rather than through images. As a result, the final dataset was reduced to 896 images.

% \subsection{Prerequisite: Annotator Training}
% We posted job descriptions on online forums and received over 50 applications from candidates with at least a Bachelor's degree. To ensure dataset quality, we provided training sessions that included online pre-annotation instructions and a qualification test to assess candidates' performance. Only those scoring above 95\% were selected. candidates were assigned to one of two groups: annotators or inspectors. Ultimately, we hired 13 annotators and 7 inspectors for our data annotation process.

% During the data collection process, we primarily rely on the powerful GPT-4o model to abtain annotation, combined with multiple rounds of human review.

\subsection{Phase 1: Data Annotation.}

\paragraph{Annotator Training}
We posted job descriptions on online forums and received over 50 applications from candidates with at least a Bachelor's degree. To ensure dataset quality, we provided training sessions that included online pre-annotation instructions and a qualification test to assess candidates' performance. Only those scoring above $95\%$ were selected. candidates were assigned to one of two groups: annotators or inspectors. Ultimately, we hired $13$ annotators and $7$ inspectors for our data annotation process.
To optimize efficiency and reduce costs, we implement a semi-automated pipeline for \dataset annotation, leveraging \texttt{GPT-4o}\footnote{We use the \texttt{gpt-4o-2024-11-20} version for the data annotation process and subsequent evaluations in this work.}. Specifically, our data annotation process consists of two substeps: \textit{answer creation} and \textit{distractor generation}.

\paragraph{Answer Creation.}
The bottom panel of Figure~\ref{fig:construct} illustrates the process of our answer creation phase. Notably, only the comprehension task and frame prediction task need option annotation. The reordering task only necessitates using a program to randomly shuffle the frame order as the answer order, without the need for manual selection of the correct answer. We adopt an AI-assisted annotation approach in which human annotators refine pre-generated answers instead of creating them from scratch. Initially, we leverage \texttt{GPT-4o, GPT-4o-Mini} \cite{hurst2024gpt40} and Gemini \cite{reid2024gemini} to generate diverse candidate answers. Human annotators then evaluate the image sequence and candidate answers, selecting the most appropriate ground truth. If none of the candidate answers is suitable, annotators are instructed to either refine a specific answer or create a new ground truth from scratch, which is about $28\%$. 

\paragraph{Distractor Generation.}
We use candidates with plausible hallucinations from the previous sub-step as strong distractors.  To ensure diversity in the multiple-choice options, we also prompt \texttt{GPT-4o, GPT-4o-mini} \cite{hurst2024gpt40} to generate intentionally incorrect responses as weak distractors. 
Typically, the responses of \texttt{GPT-4o-mini}  are not very accurate and are mostly used as distractors. A detailed example of model annotation can be found in the appendix~\ref{appendix:annotation}
Annotators are instructed to evaluate the quality of these distractors and select top-3 options, refining them if necessary. Finally, each ground truth is paired with three high-quality distractors for evaluation.

%We carefully curated a new set of challenging distractors that:;Maintain semantic relevance to the comic content;Share similar visual elements or themes with the correct answer;Avoid obvious logical contradictions;Represent plausible but incorrect interpretations

% To ensure annotation quality, annotators are provided with detailed examples and rejection guidelines. 

% The annotation interface and full specifications are provided in Appendix{ \color{red}X[TODO].}

% 对于理解任务，我们在采样4o的回答作为标注的时候发现模型经常出现过度解读升华漫画的情况。所以一些非常简单无深意的图片会被删除，此外人类标注员在check的时候会删除过度解读。

% The annotation process concluded with two rounds of cross-verification to ensure answer accuracy, maintain stylistic consistency, and minimize potential biases.


\subsection{Phase 2: Cross-Check Examination}
We implement a cross-check examination mechanism to ensure rigorous screening of high-quality annotations. During the data annotation process, hired inspectors review the annotated data and corresponding image sequences. If they encounter low-quality annotations, they have the option to reject them. Each annotation is reviewed by two inspectors. If both inspectors reject the annotation, it is discarded, and the image is returned to the dataset for re-annotation. If an image sequence is rejected in two rounds of annotation, it suggests that this sample is not suitable for the current subtask (e.g., the meaning of the sample is unclear), and the image is subsequently removed from the subtask. 

% During this phase, we also use Cohen's kappa to quantify inter-annotator agreement, obtaining an average score of { \color{red}0.701} across all tasks, indicating substantial agreement.

After annotation, both advanced annotators and inspectors, acting as final examiners, review the annotations to ensure they meet the required standards. Each annotation undergoes review by three examiners, who vote on whether to accept the annotated sample. Only the samples that receive a majority vote are approved. To ensure the quality of the examiners' work, we randomly sample 10\% of the annotations for verification. 

% This review is conducted by the authors of this paper, who provide timely feedback to the examiners. Further details on the pricing strategy and the hierarchical supervision process are provided in Appendix B.



\subsection{Data Composition}
It is important to note that the reordering subtask does not require human annotation, as described in the previous process. For this subtask, we select suitable image sequences based on the criterion that the correct ordering must be unique. To ensure this, we conduct a manual review to verify that each sequence follows a logically unambiguous order. A script is then run to perform the initial splitting of panels within specific comics, followed by a random shuffling of these panels. Human annotators are tasked with verifying the format and quality of the frames to ensure they meet the required standards. These processed image sequences serve as the evaluation data for the reordering subtask.

The final version of our 32-day annotated \dataset contains 896 items (see Table 2), encompassing three subtasks: \textit{visual narrative comprehension}, \textit{contextual frame prediction}, and \textit{temporal narrative reordering}. In each of these subtasks, each sample consists of an image sequence paired with a multiple-choice question offering four options. The evaluated LMMs are required to select the option they deem most appropriate from the four. More information and examples of \dataset can be found in Appendix~\ref{example}. 

\input{tab/main_results}

