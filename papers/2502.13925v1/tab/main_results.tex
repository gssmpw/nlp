\begin{table*}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.0} % 调整行高
    \setlength{\tabcolsep}{3pt} % 调整列间距
    \resizebox{\textwidth}{!}{% 缩小整体表格
    \begin{tabular}{lcccccccc}
    \toprule       
    \textbf{Models} & \textbf{Backbone}&  \textbf{\#Params} & \textbf{I-Video}  &  \textbf{Comprehension} & \textbf{Prediction} & \multicolumn{2}{c}{\textbf{Reordering}}  & AVG \\ 
   \cmidrule(lr){6-7} 
   
    TYPE &&& && & \multicolumn{2}{c}{choice VS generation }\\
    \midrule
    Human  & &  & & 80.00 & 82.00 & 86.00 & 80.00 & 82.00\\ 
    \hline
    \rowcolor{gray!20}
    \multicolumn{9}{c}{\textbf{Closed-Model}} \\
    \hline 

    GPT-4o-mini \cite{hurst2024gpt40} & - &- & & 53.23 & 56.33 & \textbf{26.07} & 8.45 & 36.02 \\
    Gemini1.5-Pro \cite{reid2024gemini}& - & -& & 49.56 & 67.83  & 25.51 & \textbf{32.02 } & 43.73 \\ 
    GPT-4o \cite{hurst2024gpt40} & - &-  & &\textbf{61.60} & \textbf{69.95} & 25.17 & 23.93 & \textbf{45.16} \\ 
    
    \hline
    
    \rowcolor{gray!20}
    \multicolumn{9}{c}{\textbf{Open-Source}} \\
    \hline  
    
    Janus-Pro \cite{chen2025januspro} & DeepSeek-LLM-7b-base & 7B &No & 27.50 & 27.50 & 26.07 & *    & 20.27 \\
    mPlug-Owl2 \cite{ye2023mplugowl2} &   LLaMA2            & 8B &No  & 30.74 & 31.17 & 25.06 & 0.56 & 21.88\\
    LLaVA-v1.6 \cite{liu2023llava}  & Vicuna-v1.5           & 7B &No  & 34.41 & 43.50 & 26.29 & 3.37 & 26.89 \\
    LLaVA-NeXT-Video~\cite{zhang2024llavanextvideo}  & Vicuna-v1.5  & 7B &Yes & 45.74 & 44.50 & 23.71 & *    & 28.49 \\
    CogVLM \cite{wang2023cogvlm} & Vicuna-v1.5              & 17B &No & 34.26 & 56.00 & 24.83 & *    & 28.77 \\
    LLaVA-v1.6 \cite{liu2023llava}  & Vicuna-v1.5           & 13B&No  & 46.03 & 46.50 & 27.98 & 2.58 & 30.77 \\
    LLaVA-v1.6 \cite{liu2023llava}  & Vicuna-v1.5           & 34B &No & 52.94 & 50.83 & 25.62 & 2.13 & 32.88 \\
    Cambrian \cite{tong2024cambrian1}  & Vicuna-v1.5        & 13B &No & 45.59 & 55.00 & 26.85 & 4.94 & 33.10\\
    Qwen2.5VL \cite{qwen2.5-VL}      & Qwen2.5              & 3B  &Yes & 50.59 & 58.83 & 27.75 & 1.91 & 34.77 \\
    InternVL2v5 \cite{chen2024internvl} &   Intern          & 26B &Yes  & 60.92 & 65.17 & 24.61 & 2.58 & 38.32\\
    MiniCPM-o 2.6  \cite{yao2024minicpm} & Qwen2.5          & 7B  & Yes & 56.18 & \textbf{65.83} & 26.85 & 5.51 & 38.59\\ 
    Qwen2.5VL \cite{qwen2.5-VL} & Qwen2.5                   & 7B &Yes  & 56.03 & 64.00 & 29.21 &\textbf{ 11.01} & 40.06\\ 
    Qwen2VL \cite{Qwen2VL} & Qwen2                          & 7B &Yes & \textbf{58.53} & 63.00 & \textbf{31.91} & 9.44 & \textbf{40.72}\\
    %  \hline     
    % \rowcolor{gray!20}
    % \multicolumn{8}{c}{\textbf{10B-Scale}} \\
    % \midrule  
    % LLaVA1.5 \cite{liu2023llava}  & Vicuna-v1.5  & 13B  & 46.03 &46.50&27.98&20.00& ~ \\ 
     
    %  \hline
    % \rowcolor{gray!20}
    % \multicolumn{7}{c}{\textbf{30B-Scale}} \\
    % \midrule  
    % LLaVA1.5 \cite{liu2023llava}  & Vicuna-v1.5 & 34B & 52.94&50.83&25.62&19.89& ~\\ 
    \bottomrule
    \end{tabular}}
    \caption{Model performance comparison across different architectures and scales. The table is sorted by accuracy. I-Video indicates whether the large multimodal models (LMMs) support video input. Scores are reported as percentages (\%). Prediction, Comprehension, and Reordering correspond to visual narrative understanding, next-frame prediction, and multi-frame reordering, respectively. The * denotes that the model failed on the corresponding task. AVG represents the average score across the four scores including the failed one. Bolded values indicate the highest scores among closed-source and open-source models.}
    \label{tab:model_comparison}
    \vspace{-4mm}
\end{table*}