\documentclass{article}[13pt] % vanilla original format

\usepackage[
backend=biber,
style=alphabetic,
]{biblatex}
\title{Bib}
\addbibresource{main.bib} %Imports bibliography file

\newcommand\citet[1]{%
    \citeauthor{#1}~(\citeyear{#1})%
}

\usepackage{imports}
\usepackage[margin=3cm]{geometry}
% \usepackage{geometry}

% \usepackage{style_files/icml2025,times} % ICLR format

%------------------------------

% \documentclass{article} % ICLR format

% \usepackage{style_files/iclr2025_conference,times} % ICLR format
% \usepackage{imports}

% \usepackage{titlesec}
% \titlespacing*{\section}{0pt}{*0.3}{*0.3} % Adjusts spacing for \section
% \titlespacing*{\subsection}{0pt}{*0.2}{*0.2} % Adjusts spacing for \subsection

% % \setlength{\abovecaptionskip}{0pt}
% % \setlength{\belowcaptionskip}{0pt}
% % \setlength{\floatsep}{0pt} % Space between two floats (figures/tables)

% % \setlength{\belowcaptionskip}{-7pt} % Space between caption and text
% \setlength{\abovecaptionskip}{5pt}
% \setlength{\belowcaptionskip}{5pt}
% \setlength{\textfloatsep}{8pt}
% \setlength{\parskip}{4pt} % Removes space between paragraphs

% \setlength{\abovedisplayskip}{8pt} % Space above equations
% \setlength{\belowdisplayskip}{8pt} % Space below equations

% \setlength{\topsep}{2pt} % Space before and after the theorem header
% \setlength{\partopsep}{2pt} % Space before a new theorem when it starts a paragraph
% % \setlength{\itemsep}{0pt} % Space between items in lists
% % \setlength{\parsep}{0pt} % Space between paragraphs


% \usepackage[
% backend=biber,
% % style=authoryear,
% style=authoryear-comp,
% % natbib=true,
% ]{biblatex}
% \title{Bib}
% \addbibresource{main.bib}


% \newcommand\citek[1]{%
%     \citeauthor{#1}~(\citeyear{#1})%
% }

% \newcommand\citeP[1]{%
%     (\parencite{#1})%
% }


%------------------------------


% \usepackage{apacite}


% \DeclareCiteCommand{\citeyear}
%     {}
%     {\bibhyperref{\printdate}}
%     {\multicitedelim}
%     {}

\makeatletter
 
\counterwithout{theorem}{section}

% \font\myfont=cmr12 at 18pt
% \singlespacing
% \onehalfspacing
% \doublespacing



\begin{document}
% \mathtoolsset{showonlyrefs}


\title{Riemannian Manifold Learning for Stackelberg Games with Neural Flow Representations}

\date{} % omit date

% \author{
%     Larkin Liu\footnote{Corresponding author: \texttt{larkin.liu@tum.de}}  \footnote{Technische Universität München}   \\
%     \and
%     Kashif Rasul \footnote{Hugging Face, Inc.} \\
%     \and
%     Yutong Chao\footnotemark[2] \\
%     \and 
%     Jalal Etesami\footnotemark[2]  % Reuse the same footnote number for Technical University of Munich
% }

\renewcommand{\Affilfont}{\small} % Use \footnotesize for even smaller text

\author[1]{Larkin Liu\thanks{Corresponding author: \texttt{larkin.liu@tum.de}}}
\author[2]{Kashif Rasul}
\author[1]{Yutong Chao}
\author[1]{Jalal Etesami}
\affil[1]{Technische Universität München}
\affil[2]{Hugging Face, Inc.}

\maketitle
\vspace{-1.0cm}% Additional space between abstract & rest of document

\begin{abstract}
% We propose a novel embedding space framework for online learning in Stackelberg general sum games, where two agents, the leader, and the follower, engage in turn-based interactions. At the heart of our approach is the introduction of a learned diffeomorphism that maps the joint action space of both agents onto a smooth Riemannian manifold, which we denote as the \textit{Stackelberg manifold}. This mapping is achieved through the use of neural normalizing flows.  We further enforce critical properties on the flow to ensure the formation of tractable isoplanar subspaces on the manifold. This construction of the Stackelberg manifold facilitates the application of bandit algorithms by assuming a linear relationship between the agents' reward functions on this manifold. We provide a rigorous theoretical framework for Stackelberg regret minimization on convex manifolds. Leveraging a spherical manifold, we establish finite-time bounds on the simple regret for online learning of Stackelberg equilibrium. Our framework constitutes a multi-agent learning system with broad applications, such as cybersecurity defence, and economic supply chain optimization.

We present a novel framework for online learning in Stackelberg general-sum games, where two agents, the leader and follower, engage in sequential turn-based interactions. At the core of this approach is a learned diffeomorphism that maps the joint action space to a smooth Riemannian manifold, referred to as the \textit{Stackelberg manifold}. This mapping, facilitated by neural normalizing flows, ensures the formation of tractable isoplanar subspaces, enabling efficient techniques for online learning. By assuming linearity between the agents' reward functions on the \textit{Stackelberg manifold}, our construct allows the application of standard bandit algorithms. We then provide a rigorous theoretical basis for regret minimization on convex manifolds and establish finite-time bounds on simple regret for learning Stackelberg equilibria. This integration of manifold learning into game theory uncovers a previously unrecognized potential for neural normalizing flows as an effective tool for multi-agent learning. We present empirical results demonstrating the effectiveness of our approach compared to standard baselines, with applications spanning domains such as cybersecurity and economic supply chain optimization.


% \lar{Our flow architecture leverages powerful neural models, including autoencoders, normalizing flows, and transformers}.

%and a linear combination of the manifold coordinates. 
% \ja{these properties are already coming with the diffeomorphism: such as Lipschitz continuity and invertibility,}

% \ja{you don't need to talk about the application in the abstract. Our framework constitutes a multi-agent learning system with broad applications, such as cybersecurity defense, economic supply chain optimization, and \todo{conversational language modeling}.}


% and empirically demonstrate the superiority of our approach over baseline methods via simulations grounded in real-world phenomena. 

% \todo{Adjust further.} We present a embedding space framework for online learning in Stackelberg games, where two agents, leader and follower, interact in a turn-based manner. We introduce a diffeomorphism which maps the joint action space of both agents to a smooth Riemannian manifold, which we refer to as a \textit{Stackelberg manifold}, via the use of normalizing flows. \lar{This normalizing flow utilizes neural architectures, including autoencoders, with potential explorations of normalizing flows and transformers.} Further, impose idea properties on this flow ensuring Lipschitz continuity, invertibility, and the formation of tractable isoplanar subspaces on the manifold when fixing the leader's action space. Setting up the Stackelberg manifold allows us to set-the-stage for linear bandit learning, under the assumption, that there is a linear relationship between the agent reward function and a linear combination of the points on the Stackelberg manifold. This enables the use of linear bandit algorithms to optimize on the manifold. We provide theoretical results to demonstrate finite-time regret bounds, with simulations showing the framework's superior performance compared to baseline methods. The game is framed as a multi-agent system with applications in cyberattack defense, economic supply-chain games, and \todo{conversational language models}.

\end{abstract}

\section{Introduction}

A Stackelberg game consists of a sequential decision-making process involving two agents, a leader and a follower. This framework, introduced in \cite{stackelberg:1934} models hierarchical strategic interactions where the leader moves first, anticipating the follower's best response, and then the follower reacts accordingly. These games have become central to understanding interactions in various fields, from economics to societal security, providing a formal method for analyzing situations where one party commits to a strategy before the other, affecting the subsequent decision-making process and reward outcomes. Over time, Stackelberg games have evolved to address more complex environments, incorporating factors like imperfect information and no-regret learning of system parameters. The solution to such a game typically revolves around finding a Stackelberg equilibrium, where the leader optimizes his strategy assuming or knowing the follower type, which affects how she optimizes her utility based on the leader's action. \parencite{Korzhyk:2010,Tambe:2015_behave_stack}.

Several challenges arise in the practical applications of Stackelberg games. One key issue is the uncertainty regarding the follower's type or rationality (or sub-rationality). In many real-world scenarios, the follower might not be fully rational or the leader might have incomplete knowledge of the follower’s preferences, leading to uncertainty in the leader's decision-making process. Additionally, imperfect information regarding reward outcomes adds another layer of complexity, as the leader may not have accurate knowledge of the payoffs associated with various strategies. These uncertainties have been addressed in domains such as security, where randomized strategies and robust optimization approaches are deployed to mitigate risks arising from incomplete information and unpredictable follower behaviour \parencite{Jiang:2013_monotonic_maxmin, Tambe:2015_behave_stack, kar:2017_trends_ssg}. For instance, in deployed systems like ARMOR at LAX or PROTECT at U.S. ports, leaders must make security decisions under uncertainty, balancing multiple risks \parencite{Jain:2011_double_oracle,Shieh:2012}. Stackelberg games also feature prominently in supply chain optimization settings, where there exist areas of uncertainty, such as demand manifestation \parencite{liu:2024_stacknews_adt} \parencite{cesa:2022-supp-chain-games}. Stackelberg game models have also found applications in novel areas like conversational agents using large language models (LLMs), where one agent (the model) anticipates the user's behaviour and adjusts its responses accordingly \parencite{Nguyen:2014}.


For non-cooperative multi-agent games that exhibit additive noise, sublinear regret can be achieved via gradient-based optimization methods, such as AdaGrad \parencite{duchi:2011-adagrad}, in the face of Gaussian noise, but this is often subject to constraints on the magnitude of the noise \parencite{hsieh:2023-convexgames}. Nevertheless, in these games, the problem settings are extended to an unlimited number of players - with regret performance degrading as the number of players increases. We investigate the problem setting of a two-player Stackelberg game, with a tractable best response function -  commonplace in economics and adversarial machine learning in general \parencite{zhou:2016_stack_model, wang:2024_stackRL}.

\textbf{Problem Setting:} We consider a two-player Stackelberg game where player A leads and player B responds. Stackelberg games are sequential, meaning that the players take turns and, the follower can best \textit{respond} to the leader's action, given information available to him. The best response of player B lies on a manifold within a subspace of the joint action space $\setA \cross \setB$. We define this Stackelberg game setting in the framework of optimal transport, where the structure of the best response function $\bR{\cdot}$ gleans simplifications to the solution methodology to obtain Stackelberg regret. This research focuses on applying multi-armed bandit (MAB) methods, particularly in Stackelberg equilibrium settings, to achieve sublinear regret. \chagesMarker{It explores the utilization of geometric topologies to better understand agent behaviour and simplify computations in a game theoretic sense.} 

\textbf{Key Contributions:} We introduce a novel algorithm that significantly advances the understanding of Stackelberg learning under imperfect information, akin to the problem settings covered in \cite{balcan:2015-stack-learn-sec} and \cite{haghtalab:2022-stack-non-myo}, presenting a systematic framework for how equilibrium can be efficiently solved in this problem setting. Central to our contribution is the construction of a feature map using neural normalizing flows, which transforms the ambient joint action space into a more tractable embedding, we define as the \textit{Stackelberg manifold}. By leveraging the geodesic properties of this manifold, our approach allows for more efficient computation of Stackelberg equilibria with respect to no-regret learning, particularly in the presence of parameter uncertainty. In addition to this, we offer a rigorous theoretical foundation for optimizing Stackelberg games on spherical manifolds. This framework is validated via empirical simulations, stemming from applications in supply chain management and cybersecurity, demonstrating that our method outperforms standard baselines, offering improvements in both computational efficiency and regret minimization.

\section{Formal Definitions}

%\textbf{Definitions:} 
In a Stackelberg game, two players take turns executing their actions. Player A is the leader, she acts first with action $\aB$ selected from her action space $\setA$. Player B is the follower, he acts second with action $\bB\in\setB$. The follower acts in response to the leader's action, and both players earn a joint payoff as a function of their actions. 
%We define two players in a repeated Stackelberg game. 
%In the abstract sense, $\aB$ and $\bB$ denote the actions of the leader and follower in action spaces $\mathcal{A}$ and $\mathcal{B}$ respectively. 
%To be precise, given an action space with a feature dimension of $\mathbbm{R}^D$, $\aB$ and $\bB$ could be vectors of different dimensions, $A$ and $B$ respectively.

\subsection{Repeated Stackelberg Games}

% Leader and follower act once, state does not transition. 



In a repeated Stackelberg game, the leader chooses actions $\mathbf{a}^t \in \mathcal{A}$, and the follower reacts with actions $\mathbf{b}^t \in \mathcal{B}$ at each round $t = 1, 2, \dots, T$. 
%The joint action at each step is $(\mathbf{a}^t, \mathbf{b}^t)$, with leader and follower rewards given by $\utlA(\mathbf{a}^t, \mathbf{b}^t)$ and $\utlB(\mathbf{a}^t, \mathbf{b}^t)$, respectively. 
The leader's strategy $\pi_A(\cdot|\mathcal{H}_t)$ is a probability distribution over the action space $\mathcal{A}$ which selects $\mathbf{a}^t$ based on past joint actions up to time $t$, i.e., $\mathcal{H}_t\equiv\{(\mathbf{a}^\tau, \mathbf{b}^\tau)| \tau<t\}$. 
Similarly, the follower's strategy $\pi_B(\cdot|\mathcal{H}_t)$ is a conditional probability distribution over $\mathcal{B}$ which determines $\mathbf{b}^t$ given the full history, i.e.,  $\mathcal{H}_t\equiv\mathcal{H}_t\cup\{\mathbf{a}^t\}$. 

\textbf{Best Response Strategy of the Follower:} To be specific, the follower selects his best response strategy at round $t$ by maximizing his expected reward function $\utlB(\aB, \bB):\mathcal{A}\times\mathcal{B}\rightarrow\mathbb{R}$ given that the leader has played action $\aB^t$. 
Since, we assume that the reward function solely depends on the most recent pairs of actions, the follower's best strategy is first order Markov, i.e., $\pi_B(\cdot|\mathcal{H}_t)=\pi_B(\cdot|\aB^t)$. 
Formally, the follower's best response at round $t$ is given by,

\vspace{-0.4cm}

\begin{align}
    \pi^*_{B}(\bB|\aB^t) &\equiv \underset{ \polB \in \Pi_{\setB} }{\mathrm{argmax}} \ \expeC_{\pi_B} [\utlB(\aB, \bB)| \aB=\aB^t], \label{eq:best_str_b} \\
    \mathfrak{B}(\aB^t) &\equiv \{\bB\in\setB| \pi^*_{B}(\bB|\aB^t)>0\}. \label{eq:br-def}
\end{align}

% \begin{equation}\label{eq:best_str_b}
%     \pi^*_{B}(\bB|\aB^t) \equiv \underset{ \polB \in \Pi_{\setB} }{\mathrm{argmax}} \ \expeC_{\pi_B} [\utlB(\aB, \bB)| \aB=\aB^t],
% \end{equation}

% \begin{minipage}{0.55\textwidth}
%   \begin{equation}
%     \pi^*_{B}(\bB|\aB^t) \equiv \underset{ \polB \in \Pi_{\setB} }{\mathrm{argmax}} \ \expeC_{\pi_B} [\utlB(\aB, \bB)| \aB=\aB^t], \label{eq:best_str_b}
%   \end{equation}
% \end{minipage}
% \hfill
% \begin{minipage}{0.50\textwidth}
%   \begin{equation}
%     \mathfrak{B}(\aB^t) \equiv \{\bB\in\setB| \pi^*_{B}(\bB|\aB^t)>0\}. \label{eq:br-def}
%   \end{equation}
% \end{minipage}



% \begin{minipage}{0.55\textwidth}
%   \begin{equation}
%     \pi^*_{B}(\bB|\aB^t) \equiv \underset{ \polB \in \Pi_{\setB} }{\mathrm{argmax}} \ \expeC_{\pi_B} [\utlB(\aB, \bB)| \aB=\aB^t], \label{eq:best_str_b}
%   \end{equation}
% \end{minipage}
% \hfill
% \begin{minipage}{0.50\textwidth}
%   \begin{equation}
%     \mathfrak{B}(\aB^t) \equiv \{\bB\in\setB| \pi^*_{B}(\bB|\aB^t)>0\}. \label{eq:br-def}
%   \end{equation}
% \end{minipage}


where $\Pi_{\setB}$ is the space of probability distributions over the action space $\mathcal{B}$ and the expectation is taken with respect to the strategy of the follower. 
In this case, we can define the set of follower's best responses in Eq. \eqref{eq:br-def}. Analogously, the leader aims at maximizing the expected utility $\utlA(\aB^t, \bB^t):\mathcal{A}\times\mathcal{B}\rightarrow\mathbb{R}$ that is a deterministic function solely driven by her action $\aB^t$ followed by the reaction of the follower $\bB^t$.


% \begin{equation}\label{eq:br-def}
%     \mathfrak{B}(\aB^t) \equiv \{\bB\in\setB| \pi^*_{B}(\bB|\aB^t)>0\}.
% \end{equation}

% \begin{equation}
%     \mathfrak{B}(\aB^t) = \underset{ \polB \in \Pi_{\setB} }{\mathrm{argmax}} \ \expeC [\utlB(\aB^t, \bB)] 
% \end{equation}





% In a repeated Stackelberg game, the leader chooses actions $\mathbf{a}^t \in \mathcal{A}$, and the follower reacts with actions $\mathbf{b}^t \in \mathcal{B}$ at each time step $t = 1, 2, \dots, T$. The joint action at each step is $(\mathbf{a}^t, \mathbf{b}^t)$, with leader and follower rewards given by $\utlA(\mathbf{a}^t, \mathbf{b}^t)$ and $\utlB(\mathbf{a}^t, \mathbf{b}^t)$. The leader's strategy $\pi_A(t)$ selects $\mathbf{a}^t$ based on past actions up to time $t$. The follower's strategy $\pi_B(\mathbf{a}^t)$ determines $\mathbf{b}^t$ in response to $\mathbf{a}^t$.


% \textbf{Best Response Function:} To be specific, $\utlB(\aB^t, \bB^t)$ is expressed as a parametric function. Conversely, for the leader, $\utlA(\aB^t, \bB^t)$ is a deterministic function solely driven by the direct action of the leader $\aB^t$ followed by the reaction of the follower $\bB^t$. At each discrete time interval, the best response of the follower, $\mathfrak{B}(\aB^t)$, is defined as follows,

\textbf{Stackelberg Equilibrium:} Consider a follower whose best response is optimal. We denote this scenario as Stackelberg Oracle (SOC) learning. From the leader's perspective, the uncertainty is not necessarily over the system, but rather the strategy of the follower $\polB(\cdot)$. 

\textit{Stackelberg equilibrium} $(\pi^*_A,\pi_B^*)$ is achieved when the follower is best responding, according to Eq. \eqref{eq:br-def}, and the leader acts with an optimal policy given the best response of the follower,

% \begin{minipage}{0.4\textwidth}
%   \begin{equation}
%     \pi^*_A \equiv \arg\max_{\pi_A \in \Pi_{\setA}} \expeC_{\pi_A,\pi^*_{B}}[\mu_A],
% \label{eq:opt_leader_mu_a}
%   \end{equation}
% \end{minipage}
% \hfill
% \begin{minipage}{0.6\textwidth}
%   \begin{equation}
%     \expeC_{\pi_A, \pi^*_B}[\mu_A] = \int_{\setA} \pi_A(\aB) \int_{\setB} \mu_A(\aB, \bB) \pi_B(\bB | \aB) \, d\bB \, d\aB.
% \label{eq:expected_value_mu_a}
%   \end{equation}
% \end{minipage}

\begin{align}
    \pi^*_A &\equiv \arg\max_{\pi_A \in \Pi_{\setA}} \expeC_{\pi_A,\pi^*_{B}}[\mu_A],
\label{eq:opt_leader_mu_a} \\
\expeC_{\pi_A, \pi^*_B}[\mu_A] &= \int_{\setA} \pi_A(\aB) \int_{\setB} \mu_A(\aB, \bB) \pi_B(\bB | \aB) \, d\bB \, d\aB.
\end{align}


% \begin{minipage}{0.4\textwidth}
%   \begin{equation}
%     \pi^*_A \equiv \arg\max_{\pi_A \in \Pi_{\setA}} \expeC_{\pi_A,\pi^*_{B}}[\mu_A],
% \label{eq:opt_leader_mu_a}
%   \end{equation}
% \end{minipage}
% \hfill
% \begin{minipage}{0.6\textwidth}
%   \begin{equation}
%     \expeC_{\pi_A, \pi^*_B}[\mu_A] = \int_{\setA} \pi_A(\aB) \int_{\setB} \mu_A(\aB, \bB) \pi_B(\bB | \aB) \, d\bB \, d\aB.
% \label{eq:expected_value_mu_a}
%   \end{equation}
% \end{minipage}

% \begin{align}\label{eq:opt_leader_mu_a}
%     &\pi^*_A\equiv\arg\max_{\pi_A\in\Pi_{\setA}}\expeC_{\pi_A,\pi^*_{B}}[\mu_A],
% \end{align}
% where,
% \begin{align}
%          \expeC_{\pi_A,\polB}[\mu_A] &= \int_{\setA} \polA(\aB) \, \int_{\setB}\mu_A(\aB,\bB)\polB(\bB|\aB)\, d\bB d\aB.% &= \int_{\setA} \polA(\aB) \, \underbrace{\int_{\setB}\mu_A(\aB,\bB)\polB(\bB|\aB)\, d\bB}_{\mathcal{E}_B(\aB)} \, d\aB % = \int_{\setA} \polA(\aB) \, \mathcal{E}_B(\aB) \, d\aB.
% \end{align}


% \begin{minipage}{0.35\textwidth}
%   \begin{equation}
%     \pi^*_A\equiv\arg\max_{\pi_A\in\Pi_{\setA}}\expeC_{\pi_A,\pi^*_{B}}[\mu_A], \label{eq:opt_leader_mu_a}
%   \end{equation}
% \end{minipage}
% \hfill
% \begin{minipage}{0.35\textwidth}
%   \begin{equation}
%     \text{where,} \quad \expeC_{\pi_A,\polB}[\mu_A] = \int_{\setA} \polA(\aB) \, \int_{\setB}\mu_A(\aB,\bB)\polB(\bB|\aB)\, d\bB d\aB.
%   \end{equation}
% \end{minipage}

% \ja{keep the optimal transport stuff out for now. 
% The general idea is to find an optimal transport map, that will map $\setA \to \polA(\cdot)$ such that the transportation cost is minimized (we provide a discussion of this in Appendix \ref{sec:stack_subspaces_soc_appendix}). The 
% }

% \begin{align}
%     \utlA^* = \max_{\polA \in \Pi_a} \utlA(\aB, \bR{\aB}). \label{eq:opt_leader_mu_a}
% \end{align}

% We assume that the follower is an oracle and is actively best responding, having committed the best response strategy $\polB(\cdot)$, 


% \textbf{Characterizations of Games:} In this work, we explore \textit{Stackelberg no-regret} (SNR) learning. Here both agents are no-regret learners and do not know the parameters of utility functions $\utlA$ nor $\utlB$. We do not assume that any agent is purposefully strategizing to obtain suboptimal rewards to mislead the other agent, although we aim for the strategy of either agent to be \textit{strategy-proof}, meaning that the expectation on the leader's reward over her strategy is indifferent to the strategy of the follower. \lar{Not super necessary.}

% we explore two concepts in adversarial learning. The first type, we denote as \textit{Stackelberg oracle} (SOC) learning. In SOC learning, the follower is an \textit{oracle} an agent who is always best responding under perfect information, in order to maximize her expected reward $\utlB(\cdot)$, as expressed in Eq. \eqref{eq:br-def} (some other literature may refer to this as a follower of a \textit{fixed type}).

\subsection{The Stackelberg Manifold} \label{sec:stack_embedding}

To address the complexity of solving for Stackelberg equilibrium under uncertainty, we propose the idea of mapping actions from the ambient space onto a manifold $\stEmb$ leading to several key advantages. Simplifying the problem by mapping to a geometric structure, such as a unit sphere, allows for significantly faster numerical computation while optimizing directly on an intuitive intrinsic geometry, reducing redundancies and providing ease with respect to enforcing constraints. Additionally, smoothness on such a structure enables computational advantages through methods like Riemannian gradient descent \parencite{bonnabel:2013_riemman_sgd}, which exploits differentiability for efficient optimization. \chagesMarker{The core idea, is to design a method that shifts the complexity of learning in Stackelberg games by constructing a mapping that transforms the data from its native action space to a more convenient representation. In terms of trade-offs computational efficiency, by constructing this manifold map, we trade off the complexity of classical multi-agent learning for the complexity of learning a neural representation for $\stEmb$.}

% \textbf{Efficiency Trade-offs:} \chagesMarker{The core idea is to design a method that shifts the complexity of learning in Stackelberg games by constructing a mapping that transforms the data from its native action space to a more convenient representation. In terms of computational efficiency, by constructing this manifold map, we trade off the complexity of classical multi-agent learning for the complexity of learning a neural representation for $\stEmb$.}

This concept of mapping the data from the ambient space, in our case defined by the joint action space $\setA \cross \setB$, onto a latent space $\stEmb$ has been explored in several prior works. For a well defined manifold, typically the approach is to learn a diffeomorphism between the ambient data space, and the objective manifold, which is a subspace of the ambient data space \parencite{rezende:2020_tori_sphere} \parencite{gemici:2016_riemann_mani}. Suppose the manifold is not given, or there lies flexibility in defining the structure of such a manifold, the certain manifold learning techniques could be devised \parencite{brehmer:2020_manifold_flows}. These approaches typically define invertible, or pesudo-invertible, probability density maps between the ambient data space, the latent space, and the manifold space.

%Learning optimal strategies for Stackelberg games in their native form, whether under perfect or imperfect information, poses significant computational challenges \cite{balcan:2015-stack-learn-sec, haghtalab:2022-stack-non-myo, liu:2024_stacknews_adt, cesa:2006_pred_learn_games}. 


% \begin{itemize}
%     \item \textbf{Simplification:} Mapping to a simpler geometric structure for faster numerical computation. i.e. unit sphere.
%     \item \textbf{Intuition:} Optimization on the intrinsic domain. This can occur over a more intuitive geometry, and remove redundancies, and adhere to constraints.
%     \item \textbf{Smoothness:} Provides computational benefit, when using gradients, exploits differentiability, i.e. Riemannian gradient descent. 
% \end{itemize}

\subsubsection{Normalizing Flows for Joint Action Space Projection}

We leverage \textit{normalizing flows} to map a joint action space $\mathcal{A} \times \mathcal{B} \subset \mathbb{R}^D$ onto a manifold, $\stEmb$ embedded in $\mathbbm{R}^D$ \parencite{rezende:2015variational, dinh:2016density, papamakarios:2021normalizing_flows}. Normalizing flows are a class of generative models that transform a high dimensional simple distribution (i.e., isotropic Gaussian) into a complex one through a series of invertible bijective mappings using neural networks that are computationally tractable. The joint action space consists of actions taken by two agents, denoted as $a \in \mathcal{A}$ and $b \in \mathcal{B}$, modelled via normalizing flows to ensure bijectivity and a tractable density estimate. Let $x \in \setA \cross \setB$, the model density $p_X(x)$ for a data point $x \in \mathbb{R}^D$ is given by,
\begin{equation}
    p_X(x) = p_Z(\funcNf(x)) \left| \det \left( \frac{\partial f_{\texttt{nf}}(x)}{\partial x} \right) \right|. \label{eq:jacobian_norm_flow}
\end{equation}
Here $Z$ represents the latent space with a simple distribution, and $\left| \det \left( \partial \funcNf(x) / \partial x \right) \right|$ is the Jacobian determinant of the transformation $\funcNf: \mathbb{R}^D \rightarrow \mathbb{R}^D$. Several open-source methodologies and codebases have been developed to address this manifold mapping problem via normalizing flows  \parencite{brehmer:2020_manifold_flows}. We extend the \texttt{nflows} package from \cite{durkan:2020_nflows} into our approach. The key contribution of our application is the isolation of the input heads into two separate sections, before concatenating the inputs and feeding it through the normalizing flow. This allows us to control the subspace induced by the leader's action $\aB \in \setA$. (We provide detailed model specifications in Appendix \ref{sec:nn_arch_details}.)


% \lar{M-flow \parencite{brehmer:2020_manifold_flows} didn't really work, but using nflow modules it is working. Instead we extend the \texttt{nflows} package from \citet{durkan:2020_nflows}. And mention RealNVP blocks \parencite{dinh:2016density}.}

\subsubsection{Specifications of the Feature Map $\phi(\aB,\bB)$} \label{sec:stack_emb_ass}

\paragraph{Feature Map $\phi(\cdot)$:}We propose a function $\phi$, which is a \textit{feature map} \parencite{zanette:2021-doe_bandit, moradipari:2022feature_map, amani:2019linear_bandit_safety}. Let $|\setA|$ and $|\setB|$ denote the finite dimension of the action space of the leader and follower respectively, the feature map $\phi: \mathcal{A} \cross \mathcal{B} \mapsto \mathbbm{R}^D$, which effectively maps any $A$ by $B$ combination of vectors to a $D$ dimensional feature representation. 

Further, we introduce a concept known as the \textit{Stackelberg embedding}, denoted by $\stEmb$, which is defined as the image of $\phi$ over the joint action space domain $\mathcal{A} \cross \mathcal{B}$,
\begin{align}
    \stEmb\equiv Im(\phi)=\{\phi(\aB,\bB) | \aB\in\mathcal{A}, \bB\in\mathcal{B}\}.
\end{align}

% Next we impose the following assumptions on the feature map as outlined in Table \ref{tab:stack-emb-dynamics}.

The construction of $\phi: \setA \cross \setB \mapsto\mathbb{R}^D$ can be via any means, in our case a normalizing neural flow network (but possibly any other architecture), but should abide by the imposed assumptions in Table \ref{tab:stack-emb-dynamics}. \chagesMarker{To be precise, $\hat{\phi}$ should denote our best learned representation of the \textit{ideal} map $\phi$. Provided that we only have access to $\hat{\phi}$, purely for notational convenience, we will use $\phi$ to represent $\hat{\phi}$ moving forward.}



\begin{definition} \label{def:bipartite_sphere_map}
    \textbf{Bipartite Spherical Map $\phi(\aB, \bB)$:} Let $\mathbf{a} \in \setA$ and $\mathbf{b} \in \setB$, and define a mapping $\phi: \setA \times \setB \to \mathcal{S}^{(D-1)}$ from Cartesian coordinates to spherical coordinates on the $D$-dimensional unit sphere $\mathcal{S}^{(D-1)}$. The spherical coordinates are partitioned such that, $\mathbf{a}$ parametrizes a subset of the spherical coordinates, and $\mathbf{b}$ parametrizes the remaining coordinates $\nu_\mathbf{b}(\mathbf{b})$. Also, $\nu_\mathbf{a} \cap \nu_\mathbf{b} = \emptyset$, meaning the partitions are disjoint. Thus, the full mapping is given by: % In addition, $\mathbf{a}$ exclusively parametrizes the azimuthal (latitudinal) coordinate.
    \[
    \phi(\mathbf{a}, \mathbf{b}) \equiv \begin{pmatrix}
    \nu_\mathbf{a}(\mathbf{a}), \nu_\mathbf{b}(\mathbf{b})
    \end{pmatrix}^\intercal\in\mathcal{S}^{(D-1)},
    \]
    where $\nu_\mathbf{a}$ and $\nu_\mathbf{b}$ represent distinct angular components of the spherical coordinates.
\end{definition}

\textbf{Mapping to a Spherical Manifold:} The transformation from spherical coordinates to Cartesian coordinates is used to map input features onto an $D$-dimensional spherical manifold. \chagesMarker{Therefore, in addition to the properties of our feature map $\phi$ from Table \ref{tab:stack-emb-dynamics}, we also enforce $\phi$ as a bipartite spherical map from Def. \ref{def:bipartite_sphere_map}.} This bipartite spherical map which constructs a disjoint spherical mapping to parameterize two subspaces in $\stEmb$. To accomplish this, we define two heads in the neural network input, the head from A specifically controls the azimuthal spherical coordinate and the head from B specifically controls other coordinates. \chagesMarker{The justification for this mapping involves a fundamental trade-off between learning an optimal embedding and reducing the complexity inherent in the native multi-agent learning problem. When specific multi-agent optimization problems are too complex to solve in their native forms (see Section \ref{sec:empirical_experiments_main} for examples), we leverage normalizing flows as an enabling link \parencite{brehmer:2020_manifold_flows, durkan:2020_nflows} to transform the problem into a simpler representation.} (A visualization of the empirical mapping results, showcasing the learned bipartite mapping to $\stEmb$ as a 3D spherical surface, is provided in Appendix \ref{sec:isoplanes_computational_viz} and \ref{sec:ambient_to_stemb_viz}. This visualization is generated by varying $\aB$ or $\bB$ to create longitudinal or latitudinal subspaces.)




\begin{table*}\centering 
    \begin{enumerate}[-, start=1,label={(\bfseries D\arabic*)}, wide, labelwidth=!, labelindent=0pt, topsep=0pt, itemsep=-8pt]
    \small
    \begin{tabular}{p{2.4in}p{3.4in}} \toprule
        \textbf{Definition} & \textbf{Expression} \\ \midrule
        \vspace{-1em} \item $\stEmb$ is measurable and reachable w.r.t. a $\sigma$-algebra over $\mathcal{A} \cross \mathcal{B}$ (denoted as $\mathfrak{E}_{\mathcal{A} \times \mathcal{B}}$).  \label{enu:reachability_dynamic} & \vspace{-2.3em} 
        \begin{align} \stEmb \subseteq \mathcal{A} \times \mathcal{B} \subseteq\mathbb{R}^D, \quad \stEmb \in \mathfrak{E}_{\mathcal{A} \times \mathcal{B}}. 
        \end{align}  \vspace{-2.3em} \\ \midrule 
        \item $\stEmb$ is compact and closed.  & \vspace{-2.3em}
        \begin{align}\text{See Appendix \ref{sec:compac_and_closed_set} for detailed definition.} \end{align} \vspace{-1.8em} \\ \midrule 
        \item $\stEmb$ is Lipschitz in the joint $\setA \cross \setB$. \label{enu:lipschitz_stack_emb}  & \vspace{-2.3em}
        \begin{align}
            \Big| \norm{\nabla_{\aB}\phi}_p + \norm{\nabla_{\bB}\phi}_p- C \Big| \leq L_c \label{eq:lipschitz_stack_emb} 
        \end{align}  \vspace{-1.8em} \\ \midrule 
        \item $\stEmb$ variational sensitivity in $\setA \cross \setB$, with high probability. \label{enu:variational_sensitivity}  & \vspace{-2.3em}
        \begin{align}
            ||\aB - \aB'|| \leq \epsilon \implies ||\phi(\aB', \bB) - \phi(\aB, \bB)|| \leq \delta, \forall \bB \\
            ||\bB - \bB'|| \leq \epsilon \implies ||\phi(\aB, \bB') - \phi(\aB, \bB)|| \leq \delta, \forall \aB
            \label{eq:variational_sensitivity} 
        \end{align}  \vspace{-1.8em} \\ \midrule 
        % \item $\stEmb$ forms a surjective map for $\polA$ and $\polB$. \lar{Be specific.} & \vspace{-2.3em} \begin{align} \aB \in \setA  \end{align}  \vspace{-2.3em} \\ \midrule
        \item $\stEmb$ forms a smooth Riemannian manifold. & \vspace{-2.3em} \begin{align} \text{See Appendix \ref{sec:riemmanian_manifold_definition} for detailed definition.}  \end{align}  \vspace{-2.3em} \\ \midrule
        \item $\stEmb$ has an approximate pullback. There exists $\phi^{-1}(\cdot): \stEmb \mapsto \setA \cross \setB$ such that, \label{enu:pullback_dynamic} & \vspace{-2.3em} \begin{align} \norm{\phi^{-1}(\phi(\aB,\bB)) - (\aB,\bB)} \leq \epsilon,\, \forall \aB,\bB 
        %\implies \norm{\aB' - \aB} \leq \delta \ja{?}  
        \end{align}   \vspace{-2.3em} \\ \midrule
    \end{tabular} \caption{Key assumptions of the Stackelberg Embedding $\stEmb$.} \label{tab:stack-emb-dynamics}
\end{enumerate}

\end{table*}


% \begin{enumerate}
%     \item Stackelberg sensitivity: $||a - a'|| \leq \epsilon \implies ||b_a^* - b^*_{a'}|| \leq \delta$. \lar{Optional}
%     % \item Linear difference $|| \theta_A - \theta_B|| \leq \epsilon_\theta$. \lar{Optional. Consider the alignment of the vectors.}
%     \item Convex and smooth boundary. \lar{Convexity could be relaxed, or removed?} \jalNote{Perhaps suggest some ideas about how to relax convexity.}
% \end{enumerate}





% \begin{assumption}
%     \textbf{Convexity of Stackelberg Embedding Space:} Inheriting from Assumption \ref{ass:smooth_convex_b_profile}, we include the additional assumption that $\stEmb$ is a convex domain (lin. interpolable) and the objective function $\innerP{\theta, \phi(\cdot)}$ is naturally a linear transform.
% \end{assumption}


Constructing a sufficient map to $\stEmb$ involves specifying the architecture and training model parameters such that it satisfies the desirata \ref{enu:reachability_dynamic} to \ref{enu:pullback_dynamic} as much as possible.  This fundamentally requires a trade-off between being well behaved on the manifold, as stipulated by \ref{enu:lipschitz_stack_emb} and \ref{enu:variational_sensitivity}, and having an accurate inverse \ref{enu:pullback_dynamic}. \chagesMarker{In general, this is not achievable via simplistic Euclidean maps, such as projections or affine maps. Without normalizing flows, the mapping of the ambient data to the manifold can become overly localized, sporadically clustered, sensitive to perturbation, and/or non-invertible, rendering the embedding ineffective (we present such scenarios in Appendix \ref{sec:bad_stackelberg_embeddings_viz}). Therefore, we must ensure the data is evenly spread across the manifold while maintaining the desired properties.} Thus, we train a neural network to sufficiently approximate $\phi$, with the loss function, % \lar{How can we better defend against this tradeoff?}
\begin{align}
    \mathcal{L}(\phi) = \alpha_N \nfLoss 
    + \alpha_R \repuLoss 
    + \alpha_P \, \underbrace{\text{Var}\Big(\phi(\aB, \bB) - \phi(\mathfrak{J}_\sigma(\aB, \bB)) \Big)}_{\text{Perturbation Loss:} \, \perturbLoss} 
    + \alpha_L \, \underbrace{\Big| \norm{\nabla_{\aB}\phi} + \norm{\nabla_{\bB}\phi} - C \Big|}_{\text{Lipschitz Loss:} \, \lipschitzLoss}. \label{eq:nf_loss}
\end{align}

The total loss $\mathcal{L}(\phi)$ is composed of multiple loss functions added together in a linear convex combination. $\nfLoss$ represents the negative log-likelihood loss of the normalizing flow $\phi(\cdot)$. \chagesMarker{To construct the Stackelberg manifold $\stEmb$, data is first sampled uniformly the ambient Cartesian space. We then, fit a normalizing flow to $\stEmb$  based on the criteria in Table \ref{tab:stack-emb-dynamics}, minizing $\mathcal{L}(\phi)$. }

\textbf{Loss Function Descriptions:} $\nfLoss$ ensures transformed data matches the base distribution while adjusting for volume changes from invertible transformations, with respect to Eq. \eqref{eq:jacobian_norm_flow}. Minimizing $\nfLoss$ allows the model to efficiently map complex data bijectively to simpler distributions. (A detailed description of $\nfLoss$ can be found in Appendix \ref{sec:nll_loss_defn}.) $\repuLoss$ represents the geodesic repulsion loss of the output, which penalizes the concentration of elements being pairwise close to one another. (A detailed description of $\repuLoss$ can be found in Appendix \ref{sec:geodesic_repul_loss_defn}.) $\mathfrak{J}_\sigma(\aB, \bB): \setA \times \setB \mapsto \setA \times \setB$ is a Gaussian perturbation function on the Cartesian product of the joint action space $\setA \times \setB$ to itself, subject to standard deviation $\sigma$. (A formal definition is provided in Appendix \ref{sec:perturb_function_defn}.) The variance of the difference between $\phi(\aB, \bB)$ and the perturbed $\phi(\mathfrak{J}_\sigma(\aB, \bB))$ should be kept minimal. This variance is captured over all elements in $\stEmb$. The Lipschitz loss penalizes drastic deviations in the gradient with respect to $\aB$ and $\bB$, provided that the sum of the absolute values of the gradients does not deviate too far from some target $C \in \mathbbm{R}$. 

The aforementioned losses in Eq. \eqref{eq:nf_loss} are linearly combined in a convex combination to form the total loss $\mathcal{L}(\phi)$, denoted as $\alpha_N$, $\alpha_R$, $\alpha_P$, and $\alpha_L$ respectively. The hyperparameters were optimized via a selection process, leveraging empirical validation to identify the settings that maximized performance. Experimental hyperparameters and architecture of the normalizing neural flow network can be found in Appendix \ref{sec:nn_arch_details}. \chagesMarker{To summarize, we aim to learn a well behaved embedding $\stEmb$ via normalizing flows, which constructs a bijective map between ambient space and manifold. $\mathcal{L}_\phi^N$ is the loss function for the normalizing flow, and the additional loss functions, as outlined in Table \ref{tab:stack-emb-dynamics}, aim to achieve uniform spreading, Lipschitz continuity, and bijective properties across the manifold.}




\subsection{Reward Function} \label{sec:reward_function}

\paragraph{Reward Mechanisms:} A Stackelberg game provides two reward functions $\utlA(\aB, \bB)$ and $\utlB(\aB, \bB)$. Both of which are linearizable with sub-Gaussian noises, $\subA$ and $\subB$, i.e.,

\vspace{-0.4cm}

\begin{align}
    \utlA(\aB, \bB) = \langle \theta^*_A, \phi(\aB, \bB) \rangle + \subA, \label{eq:A_reward_inner_prod} \\
    \utlB(\aB, \bB) = \langle \theta^*_B, \phi(\aB, \bB) \rangle + \subB. \label{eq:B_reward_inner_prod}
\end{align}

% \begin{minipage}{0.45\textwidth}
%   \begin{equation}
%     \utlA(\aB, \bB) = \langle \theta^*_A, \phi(\aB, \bB) \rangle + \subA, \label{eq:A_reward_inner_prod}
%   \end{equation}
% \end{minipage}
% \hfill
% \begin{minipage}{0.45\textwidth}
%   \begin{equation}
%     \utlB(\aB, \bB) = \langle \theta^*_B, \phi(\aB, \bB) \rangle + \subB. \label{eq:B_reward_inner_prod}
%   \end{equation}
% \end{minipage}



% \begin{multicols}{2}
%   \begin{equation}
%     \utlA(\aB, \bB) = \langle \theta^*_A, \phi(\aB, \bB) \rangle + \subA, \label{eq:B_reward_inner_prod}
%   \end{equation}\break
%   \begin{equation}
%     \utlB(\aB, \bB) = \langle \theta^*_B, \phi(\aB, \bB) \rangle + \subB. \label{eq:B_reward_inner_prod}
%   \end{equation}
% \end{multicols}
% \qquad \qquad \, 
% These should be moved to the section where you talk about the parameters learning method, i.e., linear bandits and estimators of $\theta$. 


We assume zero-mean sub-Gaussian distribution for both $\subA$ and $\subB$ but they do not necessarily need to be identical. The objective is to learn the parameters $\theta^*_A\in\mathbb{R}^D$, and possibly as an extension problem $\theta^*_B$. The feature map $\phi(\cdot)$ maps the joint action space $\setA \cross \setB$, to a subspace in $\mathbbm{R}^D$. 
The parameters of the model, can be estimated via parameterized regression,
\begin{align}
    \hat{\theta}_t = (\phi_{1:t} \phi_{1:t}^\top + \lambda_{\text{reg}} I)^{-1} \phi_{1:t}^\top \, \mu_{1:t}, \qquad \text{for A and B, respectively,} \label{eq:theta_estimation_xtx}
\end{align}
Where $\phi_{1:t}$ represents the sequence of $\phi(\cdot)$ values via the feature map given the action sequences $\aB_{1:t}$ and $\bB_{1:t}$, $\lambda_{\text{reg}}$ serves as a regularization parameter, $I$ is the identity matrix, and $\mu_{1:t}$ are the historical rewards of players A or B (depending on the subscript). 
Here, we extend the reward structure of classical linear bandits in \parencite{abbasi:2011ofu,chu:2011-linucb} to a setting where two players jointly decide on the action sequence.
%Here, we extend the reward structure of classical linear bandits \parencite{abbasi:2011ofu,chu:2011-linucb} to that of to a setting where two players decide on the action sequence. 
We stipulate assumptions to ensure that the covariance matrix $\precMat$ is well-conditioned and positive semi-definite (PSD), with a regularization parameter $\lambda_{\text{reg}}$ balancing bias and variance, while the norm $||\phi(\aB^t,\bB^t) ||_{\precMat}$ must remain small to facilitate efficient uncertainty reduction. (These assumptions are outlined in detail in Appendix \ref{sec:ass_lin_reward}.) 

\paragraph{Linearity by Design:} \chagesMarker{This formulation in Eq. \eqref{eq:A_reward_inner_prod} and Eq. \eqref{eq:B_reward_inner_prod} not only simplifies its analysis but also enables the derivation of theoretical guarantees for online learning, particularly with respect to convergence to the optimal solution. The problem is framed as a linear bandit construct \parencite{chu:2011-linucb, cesa:2006_pred_learn_games, lattimore:2020_bandit_book}, a well-established setting with strong theoretical foundations. Additionally, the multi-agent aspect aligns with game-theoretic online learning frameworks explored in \parencite{liu:2024_stacknews_adt, zhao:2023-online, haghtalab:2022-stack-non-myo, bai:2021_stackelberg}. The methodology involves mapping the action space to a structured space $\phi(\cdot)$. This linearity is analogous to the role of the final layer in a neural network, where the relationship between the ambient space (or joint action space) and the embedding space is learned. Subsequently, a linear set of weights maps the embedding space to the reward space. The existence of such a mapping, $\phi(\cdot)$, with valid linear parameters stems from the bijectiveness, smoothness, and unrestricted complexity of $\phi(\cdot)$. We provide further discussion of this in Appendix \ref{sec:note_embed_reward_linear_appendix}.}


% The problem is initially framed as a linear bandit construct \parencite{chu:2011-linucb, cesa:2006_pred_learn_games, lattimore:2020_bandit_book}, a well-studied setting with a rich body of theoretical foundations. Furthermore, the multi-agent nature of the problem aligns closely with established frameworks in the game-theoretic online learning literature, as extensively explored in \parencite{liu:2024_stacknews_adt, zhao:2023-online, haghtalab:2022-stack-non-myo, bai:2021_stackelberg}. 









% \lar{We need to know that the assumptions on $\phi$ still hold. We present the assumptions for the linear contextual bandit, give state (context) $\statex$ and action $\aB$. Constraints on the behaviour of the feature map largely from \parencite{zanette:2021-doe_bandit}. }


% The assumption is that both $\utlA(\cdot)$ and $\utlB(\cdot)$ belong to a lower dimensional linear subspace of $\mathbbm{R}^{A \cross B}$.  % \ja{We assume that the Stackelberg embedding has lower dimension compared to both agents' action spaces.}
% https://proceedings.neurips.cc/paper/2021/file/c00193e70e8e27e70601b26161b4ae86-Paper.pdf
% \item \textbf{Linear Reward Model}:
    % \begin{align}
    % r(s, a) = \phi(s, a)^\top \thet\aB^* + \eta
    % \end{align}
    % where \(\eta\) is mean-zero 1-sub-Gaussian noise.
        % \begin{align}
    % \lambda_{\text{reg}}
    % \end{align}
    
    % For symmetric matrices \(\theta_A\) and \(B\):
    % \begin{align}
    % A \preceq B \implies B - A \text{ is positive semi-definite}
    % \end{align}
    % Ensuring the positive semi-definiteness of covariance matrices constructed with \(\phi\).
    
    % \item \textbf{Sample Complexity}:
    % \begin{align}
    % \Omega\left(\min\{d \log \sum_s A_s, d^2\}/\epsilon^2\right) 
    % \end{align}
    
    % On the number of online samples, with offline contexts being polynomially small, e.g.,  $O(d^3/\epsilon^2)$.

% https://zcc1307.github.io/courses/csc588sp21/notes/0422.pdf

%If we consider this repeated game as a simultaneous action game, then the objective is to solve for the Nash equilibrium (NE). However, the NE solution to an SG is usually not the same solution as the SG solution, due to the follower's ability to observe and respond to the leader in a sequential manner.
% \textit{No-regret learning}, 


\section{Optimization of Stackelberg Games}

\textbf{Optimization under Perfect Information:} We see that regardless of the convexity of $\setA$ or $\setB$, so long as we are dealing with compact spaces, under perfect information, we can solve the Stackelberg equilibrium by solving a bilevel optimization problem expressed as,
% \begin{align}
%     &\pi_A^* = \arg\max_{\polA\in \Pi_A} \, \mathbb{E}\big[\innerP{\theta^*_A, \, \phi(\polA, \pi_B^*(\polA)) }\big],\\
%      &\text{where}\quad \pi_B^*(\polA) \equiv \argmaX{\polB\in \Pi_B} \, \mathbb{E}\big[\innerP{\theta^*_B, \, \phi(\polA, \polB)}\big],\label{eq:bottom_bilevel}
% \end{align}

\begin{minipage}{0.48\textwidth}
  \begin{equation}
    \pi_A^* = \arg\max_{\polA\in \Pi_A} \, \mathbb{E}\big[\innerP{\theta^*_A, \, \phi(\polA, \pi_B^*(\polA)) }\big],
  \end{equation}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
  \begin{equation}
    \pi_B^*(\polA) = \argmaX{\polB\in \Pi_B} \, \mathbb{E}\big[\innerP{\theta^*_B, \, \phi(\polA, \polB)}\big],\label{eq:bottom_bilevel}
  \end{equation}
\end{minipage}


With a slight abuse of notation, we use $\phi(\polA, \polB)$ and $\pi_B^*(\polA)$ to denote $\mathbb{E}_{\polA,\polB}[\phi]$ and  the best response function in response to policy $\polA$, respectively. The expectation are taken with respect to the sub-Gaussian noises. 

% \ja{You havent defined regret and no-regret learner!}

\textbf{Optimization under Parameter Uncertainty:} For some no-regret  learning algorithm suppose that after observing $t$ samples, the uncertainty among the parameters $\theta$ is characterized by,

\begin{align}\label{eq:param_uncertainty}
\Ball(\theta^*,\uncerBall{t})\equiv\Big\{\theta: \norm{\theta^* - \theta} \leq \uncerBall{t}\Big\}.
\end{align}

with probability at least $1-\delta$. In this formulation, $||\cdot||$ denotes some norm in the space of parameters. 
Assuming a \textit{pessimistic} leader, the optimization problem under parameter uncertainty at round $t$ can be expressed as,
\begin{align}
    &\pi_A^* \equiv \arg\max_{\polA\in \Pi_A} \min_{\theta_A}\, \mathbb{E}\big[\innerP{\theta_A, \, \phi(\polA, \pi_B^*(\polA)) }\big],\quad &\text{s.t.} \quad  \theta_A \in \Ball(\theta^*_A,\uncerBall{t}), \label{eq:pi_a_star} \\
     &\text{where}\quad \pi_B^*(\polA) \equiv \argmaX{\polB\in \Pi_B} \max_{\theta_B}\,\mathbb{E}\big[\innerP{\theta_B, \, \phi(\polA, \polB)}\big],\, &\text{s.t.} \,  \theta_B \in \Ball(\theta^*_B,\uncerBall{t}). \label{eq:pi_b_star}
\end{align}
% \begin{align}
%     \mathbbm{E}[\underline{\mu}_A] &= \min_{\theta_A} \, \innerP{\theta_A, g^*}, \qquad \text{s.t:} \quad  \theta_A \in \Big\{\theta: \norm{\theta^*_A - \theta}_p \leq \uncerBall{t} \Big\} \\
%     g^*\in\Phi_{\pi^*_B}(\aB)=\{\},\\
%      \text{Where,} \quad g^* &= \argmaX{\polB, \theta_B} \, \innerP{\theta_B, \stEmb_{\polB}}, \quad \text{s.t.:} \quad \pi_B \in \Pi_B, \quad \theta_B \in \Big\{\theta: \norm{\theta^*_B - \theta}_p \leq \uncerBall{t} \Big\}.
% \end{align}

Given $\pi_B^*(\cdot)$ in Eq. \eqref{eq:pi_b_star}, we define,
\begin{align}
   & \underline{\mathcal{H}}(\theta_A^*,t) \equiv \max_{\pi_A\in\Pi_A}\min_{\theta_A}\, \mathbb{E}\big[\innerP{\theta_A, \, \phi(\polA, \pi_B^*(\polA)) }\big], \quad &\text{s.t.} \quad  \theta_A \in \Ball(\theta^*_A,\uncerBall{t}), \label{eq:min_H} \\
     & \overline{\mathcal{H}}(\theta_A^*,t) \equiv \max_{\pi_A\in\Pi_A}\max_{\theta_A}\, \mathbb{E}\big[\innerP{\theta_A, \, \phi(\polA, \pi_B^*(\polA)) }\big], \quad &\text{s.t.} \quad  \theta_A \in \Ball(\theta^*_A,\uncerBall{t}). \label{eq:max_H}
\end{align}

\chagesMarker{We can see from the structure of Eq. \eqref{eq:pi_a_star} to Eq. \eqref{eq:max_H}, the resemblance to a bi-level optimization problem, which can be solved both under perfect information and uncertainty. We provide a discussion of such methods in Appendix \ref{sec:bilevel_extra_discuss}.}

\section{Online Learning on the Stackelberg Manifold}

\chagesMarker{To enable efficient multi-agent online learning on the Stackelberg manifold, $\stEmb$, we enforce $\stEmb$ to be a \textit{convex manifold}. The convex manifold is a manifold where the geodesic between any two points on the manifold is contained within or forms a geodesically convex set (Def. \ref{def:geodesic_convex_subsets}). Essentially, in a convex manifold, every geodesic between two points is contained within the manifold, adhering to the geodesic convexity property. The formal definitions can be found in Appendix \ref{sec:convex_manifold_desc}.}

% \begin{definition} \label{def:convex_manifold}
%     \textbf{Convex Manifolds:} A convex manifold is, therefore, a manifold where all possible subsets $\convexGeoSet \subseteq \stEmb$ are geodesically convex, per Definition \ref{def:geodesic_convex_subsets}. 
% \end{definition}

% \ja{You need a section describing how you estimate the parameters using linear bandit literature. }

\subsection{Stackelberg Optimization under Perfect Information}

Provided that we can transform data from the joint action space (or ambient data space) onto a spherical manifold, we can leverage the properties of the D-sphere to determine the best response solution for the Stackelberg follower and optimize the corresponding Stackelberg regret. Consider the reward function structure outlined in Section \ref{sec:reward_function}. In general, for each agent, $\mu = \innerP{\theta, \phi}$. Here, $\theta$ represents a $D$-dimensional vector in the manifold space, and we must find the element in $\stEmb$ that maximizes this inner product. In the Stackelberg game, since the leader moves first, they define a restricted subspace on the $\stEmb$. The follower must then optimize within this subspace. Moving forward, $\theta_A$ and $\theta_B$ will be referred to as \textit{objective vectors}.




We define the \textit{divergence angle}, $\dvAng$ as the angle between the two objective vectors. Further, we define the \textit{geodesic distance} between two vectors, denoted as $\geoDis(\theta_A, \theta_B)$, for a unit-spherical manifold, as follows,
\begin{align}
    \cos(\dvAng) \equiv  \frac{ \innerP{\theta_A,\theta_B} }{\|\theta_A\| \|\theta_B\|}, \qquad \geoDis(\theta_A, \theta_B) \equiv \arccos\left( \frac{\innerP{\theta_A, \theta_B}}{\|\theta_A\| \|\theta_B\|} \right). \label{eq:div_angle_cos_defn}
\end{align}

In a $D$-dimensional sphere, for a cooperative game with no divergence angle, the optimal solution that maximizes the inner product is an element in $\stEmb$ that is collinear with $\theta_A$, mutatis mutandis for $\theta_B$. Lemmas \ref{lem:geodesic_and_closeness_phi} to \ref{lem:pure_strategy_convex_manifold} establishes a link between solving for the follower's best response, from Eq. \eqref{eq:br-def}, and minimizing geodesic distance, in a general sum game. Moving forward, we use the convention $\theta_A'$ and $\theta_B'$ to denote the projection of the objective vectors  $\theta_A$ and $\theta_B$ onto $\stEmb$.
% In a non-cooperative game, each agent desires their element on $\stEmb$ to align with their differing objective vector. 

% \begin{lemma}
    % Let $\tilde{\mu}^* = \max_{\phi \in \stEmb} \innerP{\phi, \theta}$, then there is some monotonic function such that 
% \end{lemma}

\begin{lemma} \label{lem:geodesic_and_closeness_phi}
    \textbf{Geodesic Distance and Closeness:} Let $\Phi \subset \mathbb{R}^D$ be a manifold serving as a boundary of a convex set in $\mathbbm{R}^D$. Given $\theta$, let $\xi_\theta \in \Phi$ be the point on the manifold that maximizes the dot product $\langle \theta, \xi_\theta \rangle$, and \chagesMarker{is orthogonal to $\stEmb$ at the point of intersection.} For any two points on the manifold $\theta_A', \theta_B' \in \Phi$, if the geodesic distance between $\xi_\theta$ and $\theta_A'$ is greater than the geodesic distance between $\xi_\theta$ and $\theta_B'$, $\geoDis(\xi_\theta, \theta_A') > \geoDis(\xi_\theta, \theta_B')$, then the dot product satisfies $\langle \theta, \theta_A' \rangle < \langle \theta, \theta_B' \rangle$. (Proof in Appendix \ref{prf:geodesic_and_closeness_phi}.) 
\end{lemma}

\chagesMarker{Lemma \ref{lem:geodesic_and_closeness_phi} posits that the divergence angle between objective vectors correlates with geodesic distance on the manifold, simplifying the best response computation under uncertainty and enabling characterization of rational follower behaviour in constrained spaces.}

\begin{lemma} \label{lem:pure_strategy_convex_manifold}
    \textbf{Pure Strategy of the Follower:} While optimizing over a convex manifold, proposed in Definition \ref{def:convex_manifold}, given any objective vector $\theta$, the linear structure of the reward functions from Eq. \eqref{eq:A_reward_inner_prod} and Eq. \eqref{eq:B_reward_inner_prod}, and that the subspace induced by $\aB \in \setA$ forms a geodesically convex subset, as defined in Definition \ref{def:geodesic_convex_subsets},
    the optimal strategy of the follower will be that of a pure strategy, such that $\polA(\bB | \aB) \in \{0, 1\}$. (Proof provided in Appendix \ref{prf:pure_strategy_convex_manifold}.)
\end{lemma}

\chagesMarker{Lemma \ref{lem:pure_strategy_convex_manifold} claims that on strictly convex manifolds, each objective vector and convex subspace has a unique reward-maximizing solution, ensuring a single equilibrium point (pure strategy).} Combined together, the intuition behind Lemmas \ref{lem:geodesic_and_closeness_phi} and \ref{lem:pure_strategy_convex_manifold} is that the maximum the dot product between $\theta_A'$ and $\theta_B'$ on the convex manifold must be collinear with each other, ensuring the optimal reward. In the case of a convex subspace, the follower acting optimally has no viable alternatives other than a single choice.


% Any deviation from this alignment reduces the dot product, and i

% \textbf{Sketch of Proof:} On a convex manifold $\stEmb$, there can only be one collinear element given any point in the space where the sphere is embedded and the origin, - backed by Lemma 4.1. Suppose both agents act independently, they would simply pick this collinear element on the sphere.

% We show in Lemman 4.2 that the element on the convex manifold with geodesic distance minimizing distance on the sphere to the objective vector's projection onto the sphere - which lines in the image of the feature space, is the reward maximizing solution.

% On any convex manifold, where the curvature is greater than 0, there is a unique solution to the geodesic minimizing distance, from any convex subset to any other element on the convex manifold.

% Suppose the leader agent generates a geodesically convex subset $\convexGeoSet$ on the convex manifold. The geodesically minimizing distance from the optimal solution under free agency, and best we could get from within the $\convexGeoSet$ is unique - there is only one point on the boundary of the set that constitutes the shortest distance.
% SOC algorithm with best leader's action, i.e., optimizing strategies in hindsight and any joint strategy $(\pi_A, \pi_B)$. 

\subsection{Regret Definitions}
% \parencite{blum:2007-external-internal-reg}

\begin{definition} \label{def:stack_regret}
    \textbf{Stackelberg Regret:} We define Stackelberg regret, denoted as $R_A^T$ for the leader, measuring the difference in cumulative rewards between a best responding follower and an optimal leader in a perfect information setting, against best responding follower and leader exhibiting \textit{bounded rationality}. The leader policy stipulates that the she acts rationally given the estimates of the expected reward function from the data gathered, as in Eq. \eqref{eq:max_H} and Eq. \eqref{eq:min_H},
    \begin{align}\label{eq:leader_regret_def_generic}
        R_A^T \equiv \sum_{t = 1}^{T} \mathbb{E}\Big[\underset{\aB \in \mathcal{A} }{\mathrm{max}} \ \utlA(\aB, \mathfrak{B}(\aB)) -  \, \utlA(\aB^t, \bR{\aB^t}) \Big] \leq \sum_{t = 1}^{T} \Big( \bar{\mathcal{H}}(\theta_A^*,t) - \underline{\mathcal{H}}(\theta_A^*,t) \Big).
    \end{align}
    The leader selects $\aB^t$ from policy $\pi_A$ according to their best estimate of $\hat{\theta}_A$ and $\hat{\theta}_B$, following the maximization equations in Eq. \eqref{eq:pi_a_star} and Eq. \eqref{eq:pi_b_star} respectively.
\end{definition}


% Defined in Eq. \eqref{eq:leader_regret_def_generic}, it captures the leader's reward difference between maximizing $\utlA(\cdot)$ with a best-responding follower given in Eq. \eqref{eq:br-def} and any other strategy. 

The leader commits to a strategy $\pi_A$ aimed at maximizing her reward while accounting for the uncertainty in the follower's response. The leader is free to estimate the follower's response rationally, and within the confidence interval. Our algorithm minimizes the \textit{Stackelberg regret}, providing a no-regret learning process for the leader. To compute the Stackelberg regret of the algorithm, which is defined from the leader's perspective, we must derive a closed form expression for the gap over time between the expected reward under the optimal policy and the expected reward under any algorithm. 

\begin{definition}
    \textbf{Simple Regret:} We define the simple regret, where with probability $1\!\!-\!\delta$ at time $t$,
    \begin{align}
        \text{reg}(t) \equiv \innerP{\theta_A^*, \phi(\aB^*, \bR{\aB^*})} - \innerP{\theta_A^*, \phi(\aB^t, \bR{\aB^t})} \leq \bar{\mathcal{H}}(\theta_A^*,t) - \underline{\mathcal{H}}(\theta_A^*,t) \label{eq:simple_regret_defn}
    \end{align}
    This assumes that the leader is acting under the bounded rationality assumption.
\end{definition}


% To expand on the definition previously defined in Eq. \eqref{eq:leader_regret_def_generic}, specifically for this linearized game, we can derive an upper-bound to Eq. \eqref{eq:leader_regret_def_generic}. 






% \ja{This should be related to the estimated parameter $\theta$ at round $t$, otherwise the inequality does not hold.}
% \begin{align}
%     \text{reg}(t) &= \innerP{\theta^*, \phi(\cdot)^*(\theta^*)} - \mathcal{H}(\theta^t), \qquad 
%     \Delta_{\text{reg}}(t) = \text{reg}(t+1) - \text{reg}(t) \label{eq:simple_regret_defn}
% \end{align}



% \begin{minipage}{0.45\textwidth}
%   \begin{equation}
%     \text{reg}(t) = \innerP{\theta^*, \phi(\cdot)^*(\theta^*)} - \mathcal{H}(\theta^t), \label{eq:A_reward_inner_prod}
%   \end{equation}
% \end{minipage}
% \hfill
% \begin{minipage}{0.45\textwidth}
%   \begin{equation}
%     \Delta_{\text{reg}}(t) = \text{reg}(t+1) - \text{reg}(t) \label{eq:simple_regret_defn}
%   \end{equation}
% \end{minipage}



% \lar{Take a look and update to be clear.} Suppose both leader and follower are no-regret learners. In addition to the uncertainty introduced in the no-regret case, we have to also introduce the uncertainty in the best response. 

% \lar{Furthermore, we can consider the constant $C^* \equiv \innerP{\theta^*, \phi(\cdot)^*(\theta^*)}$ and this number does not change.}

\subsection{Quantifying Uncertainty on the Stackelberg Manifold} \label{sec:quantify_uncertainty_stackmans}

We now revisit the parameter uncertainty constraints introduced in Sec. \ref{sec:reward_function}, which dictate the uncertainty of a given learning algorithm, characterized by an uncertainty radius $\uncerBall{t}$. Given the feature map $\phi(\cdot)$, which adheres to the linear reward assumptions, particularly with respect to the covariance matrix of the regression (as outlined in Sec. \ref{sec:reward_function}), the learning leader can apply any bandit learning algorithm that imposes a high-probability bound on the parameter estimate. This constraint is formalized in Eq. \eqref{eq:param_uncertainty} by the uncertainty region $\uncerBall{t}$. Let us define $\IsoPL{\aB}$ and $\IsoPL{\bB}$ as two subspaces, which we will use to analyze the leader's actions under these uncertainty constraints.

% \begin{align}
%     \IsoPL{\aB} \equiv \{ \phi(\aB, \bB') | \bB' \in \setB \}, \qquad \IsoPL{\bB} \equiv \{ \phi(\aB', \bB) | \aB' \in \setA \}
% \end{align}

\vspace{0.2cm}

\begin{minipage}{0.45\textwidth}
  \begin{equation}
    \IsoPL{\aB} \equiv \{ \phi(\aB, \bB') | \bB' \in \setB \},
  \end{equation}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
  \begin{equation}
    \IsoPL{\bB} \equiv \{ \phi(\aB', \bB) | \aB' \in \setA \},
  \end{equation}
\end{minipage}

\vspace{0.2cm}

where $\IsoPL{\aB}$ and $\IsoPL{\bB}$ are the sub-spaces formed when we fix one of the leader or follower's action, and let the other action vary freely. 

\begin{lemma} \label{lem:intersect_submanifold_AB}
    \textbf{Intersection of $\IsoPL{\aB}$ and $\IsoPL{\bB}$:} Given a bipartite spherical map $\mathcal{Q}(\cdot)$ from Definition \ref{def:bipartite_sphere_map}, with $\aB$ parameterizing the azimuthal (latitudinal) coordinates, the cardinality of the intersect between $\IsoPL{\aB}$ and $\IsoPL{\bB}$ will be non-empty. That is, $|\IsoPL{\aB} \cap \IsoPL{\bB}| > 0$. (Proof is in Appendix \ref{prf:intersect_submanifold_AB}.)
\end{lemma} 


\chagesMarker{The purpose of Lemma 4.3 is to highlight that, given the bipartite map in Def. \ref{def:bipartite_sphere_map}, subspaces are guaranteed to intersect on the manifold. This is easy to visualize on a spherical manifold in the 2-sphere setting (e.g., longitudinal and latitudinal lines) but becomes challenging to intuit in higher dimensions. We rigorously argue that, just as in the 2-sphere case, the same principle holds in a D-sphere setting.} The derivation of Lemma \ref{lem:intersect_submanifold_AB} first comes by isolating the subspaces in terms of angular coordinates. Next, due to the \textit{Poincare-Hopf theorem} \parencite{poincare:1885, hopf:1927}, the compactness of the smooth Riemmanian manifold imposes strong geometric constraints such that the two subspaces cannot avoid each other. 

\begin{lemma} \label{lem:orthogonality_submanifold_AB}
    \textbf{Orthogonality of Subspaces $\IsoPL{\aB}$ and $\IsoPL{\bB}$:} The two submanifolds $\IsoPL{\aB}$ and $\IsoPL{\bB}$, are orthognal to each other within $\stEmb$. (Proof provided in Appendix \ref{prf:orthogonality_submanifold_AB}.)
\end{lemma} 

Lemma \ref{lem:orthogonality_submanifold_AB} is proven by isolating and taking the partial derivatives of the cartesian coordinates with respect to their spherical coordinates to obtain tangent vectors. Afterwards, by computing the dot product between these two tangents and demonstrating that it equates to 0, we establish their orthogonality.

% \lar{Stackelberg regret here is effectively 0, only issue lies in the computational error.}

% \begin{figure}[!h] 
%   \includegraphics[width=87mm]{figures/isoplanes_line_sphere_regret.jpg}
%   \centering
%   \caption{So long as isoplaness are formed the regret only pertains to the boundary on the leader's uncertainty on the spherical manifold.}
%   \label{fig:isoplanes_line_sphere_regret}
% \end{figure} 

\textbf{Geodesic Isoplanar Subspace Alignment (GISA):} The general methodology in which we can compute the optimal leader strategy for a Stackelberg game, for manifold $\stEmb$ that forms a convex boundary, is that the leader can anticipate the follower strategy based on knowledge of follower's reward parameters $\theta_B'$ and the isoplane $\IsoPL{\aB}$. We denote this homeomorphism as $f_1(\IsoPL{\aB}, \theta_B'): \IsoPL{\aB} \mapsto \IsoPL{\bB^*_{\aB}}$. Thereafter, we compute the geodesic distance minimizing distance from $\IsoPL{\bB^*_{\aB}}$ to $\theta_A'$ via injective map $f_2(\IsoPL{\bB^*_{\aB}}, \theta_A'): \IsoPL{\bB^*_{\aB}} \mapsto \mathbb{R}$. Leader's objective is to find $\aB \in \setA$ such that it minimizes the composition of $f_1 \circ f_2$, giving us the geodesic distance. This composition is abstractly defined as,
% \ja{Please give definitions and show them on your Figure.}
\begin{align}
    \IsoPL{\aB} \underset{f_1(\cdot, \theta_A')}{\longmapsto} \IsoPL{\bB^*_{\aB}} \underset{f_2(\cdot, \theta_B')}{\longmapsto} \geoDis(\aB, \bB^*_{\aB}) \in \mathbbm{R}, \quad \text{where,} \, \, \theta' = \frac{\theta}{\norm{\theta}}, \quad \text{for A and B.} \label{eq:gisa_map_and_norm}
\end{align}

% \input{fig_geodesic_illustration_smaller.tex}
% \input{fig_geodesic_illustration.tex}
\input{fig_geodesic_illustration_3D.tex}



% \textbf{Spherical Manifold with Isoplanes:} Suppose small-circle circumnavigating isoplaness can be generated from the function $\phi(a, \cdot)$. This enforces a geometry to the problem from which we can exploit to solve for Stackelberg equilibrium. 


% \begin{figure}[!h] 
%   \includegraphics[width=87mm]{figures/isoplanes_proj_diagram.png}
%   \centering
%   \caption{A projection form the isoplane $\IsoPL{\aB}$ is visualized in 3D as well as in 2D from the planar view.}
%   \label{fig:iso_line_proj_sphere}
% \end{figure} 

\begin{theorem} \label{thm:sphere_man_iso_reg}
    \textbf{Isoplane Stackelberg Regret:} For D-dimensional spherical manifolds embedded in $\mathbb{R}^D$ space, where $\phi(\aB, \cdot)$ generates an isoplanes $\IsoPL{\aB}$, and the linear relationship to the reward function in Eq. \eqref{eq:A_reward_inner_prod} and Eq. \eqref{eq:B_reward_inner_prod} and Eq. \eqref{eq:A_reward_inner_prod} and Eq. \eqref{eq:B_reward_inner_prod}, the simple regret, defined in Eq. \eqref{eq:simple_regret_defn}, of any learning algorithm with uncertainty parameter uncertainty  $\uncerBall{t}$, refer to in Eq. \eqref{eq:param_uncertainty}, is bounded by $\mathcal{O}(\arccos(1-\uncerBall{t}^2/2))$. (Proof provided in Appendix \ref{prf:sphere_man_iso_reg}.)
\end{theorem}

% Provided the consequences of Lemma \ref{lem:orthogonality_submanifold_AB} and Lemma \ref{lem:intersect_submanifold_AB}, any uncertainty in the parameters governing \(\utlA\) and \(\utlB\) induces a corresponding uncertainty region on the manifold, represented as a geodesic hypersphere on the \(D\)-dimensional sphere. This uncertainty is characterized by a radius in the manifold's geodesic surface, encapsulating the possible deviations from the true parameters for the parameter estimates $\hat{\theta}_A$ and $\hat{\theta}_B$ respectively, with probability $1-\delta$. Given the linear reward structure in Eq. \eqref{eq:theta_estimation_xtx}, any misspecification on $\hat{\theta}_A$, reduces to a shrinkage of the confidence ball, $\Ball(\theta^*, \uncerBall{t})$, projected onto $\stEmb$. \lar{Room for additional rigorous argument.}

The proof of Theorem \ref{thm:sphere_man_iso_reg} focuses on analyzing the geodesic distances on $\stEmb$ due to uncertainty. First, we argue that any norm-like confidence ball in Cartesian coordinates, $\Ball(\cdot)$, can be transformed into a confidence bound into a geodesic distance-based confidence ball, $\Ball_{\geoDis}(\cdot)$, in spherical coordinates (discussed in Lemma \ref{lem:geodesic_uncertainty_ball} of the Appendix.) Due to orthogonality between $\IsoPL{\aB}$ and $\IsoPL{\bB}$, we argue that that the geodesic distance either remains the same or decreases when we projected from any $\Ball(\cdot)'$ from $\IsoPL{\aB}$ to $\IsoPL{\bB}$ (discussed in Lemma \ref{lem:distance_preserving_ortho_proj} of the Appendix.) This naturally extends to a bound on the maximum diameter of the projected confidence ball on $\IsoPL{\bB}$. This constitutes the best and worst possible outcomes due to misspecification in accordance with the formulas in Eq. \eqref{eq:min_H} and Eq. \eqref{eq:max_H}, as expressed in Eq. \eqref{eq:simple_regret_defn}, which upper bounds the simple regret. \chagesMarker{Consequently, one can see that our methodology enables the leader to simplify the follower’s best response region on $\stEmb$, allowing derivation of high-probability worst-case outcomes and theoretical guarantees on the simple regret under quantifiable uncertainty.}



\begin{lemma} \label{lem:leader_pure_strategy_spherical}
    \textbf{Pure Strategy of the Leader:} Given a spherical manifold, $\stEmb$, and isoplanar subspace, $\IsoPL{\aB}$ and $\IsoPL{\bB}$ for the longitudinal and lattitudinal subspaces respectively, the optimal strategy of the leader is that of a pure strategy, that is, $\polA^*(\aB) \in \{0, 1\}$. (Proof is provided in Appendix \ref{prf:leader_pure_strategy_spherical}.)
\end{lemma}
Lemma \ref{lem:leader_pure_strategy_spherical} argues that the intersection between $\IsoPL{\aB}$ and $\IsoPL{\bB}$ contains at most one element due to their orthogonality. Consequently, no other actions on the manifold can further maximize the leader's reward. Intuitively, the positive curvature of the manifold ensures that once two non-degenerate isoplanes intersect, the intersection is a unique point that maximizes the dot product between the action and the objective vector.


\paragraph{Stackelberg Learning Algorithm:} \chagesMarker{We present an end-to-end algorithm for constructing the Stackelberg manifold, $\stEmb$, and subsequently enabling online learning within it. The process begins with an offline, data-independent training phase (Step \ref{sec:stack_embedding} of Algorithm \ref{alg:gisa}) to construct $\stEmb$. This is followed by iterative online learning performed directly on the manifold. As a result, optimal strategy learning reduces to geodesic distance minimization, providing a significantly more computationally efficient alternative to traditional game-theoretic approaches, such as bi-level optimization.}

\input{algorithm_gisa.tex}

% \begin{enumerate}
%     \item \textbf{Action Embedding}: An offline, data-independent training mechanism is conducted in Step \ref{sec:stack_embedding} of Algorithm \ref{alg:gisa} to construct $\stEmb$. 
%     % \item \textbf{Parameter (Re)-Estimation}: The re-estimation of parameters $\theta$ and $\phi$ is typically implicit as new data is observed. Algorithm \ref{alg:gisa} includes an explicit step for parameter re-estimation based on the collected data history.
%     \item \textbf{Strategy Learning}: By mapping to $\stEmb$, we eliminate the need for strategic learning in the ambient action space. Consequently, strategy learning under uncertainty reduces to geodesic distance minimization, offering a significantly more tractable alternative to traditional game-theoretic approaches, such as bi-level optimization. % The follower's best response is naturally confined to a subspace that is computationally convenient to derive (as illustrated in Fig. \ref{fig:isoplane}).
% \end{enumerate}




\section{Empirical Experiments} \label{sec:empirical_experiments_main}

\begin{figure}[!htb]
\minipage{0.33\textwidth}
  \includegraphics[width=43mm]{figures/s1_reg_ex1_v3.png}
  \caption*{$\mathbbm{R}^1$ Stackelberg Regret.}%\label{fig:market3}
\endminipage\hfill
\minipage{0.33\textwidth}
  \includegraphics[width=43mm]{figures/npg_reg_ex3_v2.png}
  \caption*{NPG Regret.}%\label{fig:market2}
\endminipage\hfill
\minipage{0.33\textwidth}
  \includegraphics[width=43mm]{figures/ssg_reg_ex1_v2.png}
  \caption*{SSG Regret.}%\label{fig:market4}
\endminipage\hfill
\caption{Average cumulative regret performance across three Stackelberg games. Parameters of the simulations outlined in Appendices \ref{sec:r1_game_details} - \ref{sec:multi-dim-ssg}. Uncertainty region denote upper and lower quartile of experimental results. } \label{fig:avg_cumm_regret}
\end{figure}

We provide three practical instances of Stackelberg games in practice. We benchmark the GISA from Algorithm \ref{alg:gisa} against a dual-UCB algorithm, where both agents are running a UCB algorithm. Although a simplistic, benchmark, the dual-UCB algorithm does constitute a no-regret learning algorithm \parencite{blum:2007-external-internal-reg}. \chagesMarker{The key concept is to abstract away the need for knowledge regard exact reward structure of the problem by leveraging our $\stEmb$ transform, allowing for the learning of well-behaved representations suitable for online learning that can adapt to new problem settings without requiring exact specifications of the problem structure. Our work addresses the limitations of previous solution algorithms, which are often problem-specific, by overcoming the challenges associated with Stackelberg game learning methods that lack closed-form solutions or are computationally intractable.}

% \lar{Perhaps mention transformation to reach summable functions.}

\textbf{$\mathbbm{R}^1$ Stackelberg Game:} In this Stackelberg game, the leader selects an action while anticipating the follower's best response. The action spaces of both the leader and the follower are one-dimensional, $\aB, \bB \in \mathbb{R}^1$. The interaction between nonlinear rewards and penalties requires numerical methods to determine optimal strategies. However, the nonlinear reward functions introduce complexity, resulting in a non-trivial equilibrium.  A practical application is energy grid management, where a utility company (leader) sets energy prices or output levels, anticipating the aggregate consumers' (followers) energy usage while accounting for nonlinear feedback such as fluctuating demand or storage limits. (Details and additional experiments are provided in Appendix \ref{sec:r1_game_details}.)


\textbf{The Newsvendor Pricing Game (NPG):} We model two agents in a \textit{Newsvendor pricing game}, with a supplier (leader) and a retailer (follower), inspired by the work of \cite{cesa:2022-supp-chain-games} and \cite{liu:2024_stacknews_adt}. The action space of the leader is denoted as $\mathbf{a} \in \mathbbm{R}^1$, and for the follower as $\mathbf{b} \in \mathbbm{R}^2$. The leader and follower interact with each other in a repeated Stackelberg game, modelling a leader-follower supply chain game. The supplier dynamically prices the product, aiming to maximize her reward, while the retailer determines the optimal pricing and order quantity based on the demand distribution according to classical Newsvendor theory \parencite{arrow:1951newsboy, petruzzi:1999newsv}. The reward function is an abstraction that is a function of stochastic demand, and the reward formats are asymmetric, rendering computation and learning of the Stackelberg equilibrium non-trivial. (We specify the details and additional experiments in Appendix \ref{sec:npg-appendix}.)

\textbf{Stackelberg Security Game (SSG) in $\mathbbm{R}^5$:} In this Stackelberg security game (SSG), inspired by the frameworks developed in \cite{balcan:2015-stack-learn-sec} and \cite{zhang:2021_bayesian_ssg}, the defender (leader) allocates limited resources across multiple targets, anticipating the attacker’s (follower) strategy (i.e. to protect a computer network from malicious intruders). In our example, both players select actions from \( \mathbb{R}^5 \), where the rewards are governed by the relative difference between their actions (i.e.,  $\aB - \bB$) and are subject to quadratic penalties for overextension. Furthermore, resource constraints are modelled via weighted \( L_1 \)-norms, imposing additional limitations on the feasible actions. The Stackelberg equilibrium in this setting is characterized by the leader’s optimal resource allocation, taking into account the adversary’s best response. The interplay between nonlinear penalties and resource constraints renders the equilibrium computation non-trivial, requiring advanced numerical techniques for tractable solutions. (We specify the details and additional experiments in Appendix \ref{sec:multi-dim-ssg}.)


% In our framework, we map the joint action space $\setA \cross \setB$ to $\stEmb$, and apply the GISA Algorithm \ref{alg:gisa} to optimize the Stackelberg regret.

% Where $a$ denotes wholesale price from the supplier firm, $p$ and $b$ denote the retail price and order amount of the retail firm.

% \lar{Explain how this relates to the generic Stackelberg game framework}.  

% Unlike \citet{cesa:2022-supp-chain-games}, we assume no production costs and full observability of demand for both parties, common in integrated eCommerce platforms. Our model introduces demand learning alongside inventory risk, where the follower determines both retail price and order amount. 

% We apply the OFUL linear bandit algorithm \parencite{abbasi:2011ofu} for the Stackelberg game, while \citet{cesa:2022-supp-chain-games} used the Piyavsky-Schubert method \parencite{bouttier:2020PV}. Additionally, we offer Stackelberg regret guarantees on cumulative regret.

% The supplier sets the wholesale price $a$, and the retailer responds with optimal retail price $p$ and order amount $b$. This interaction repeats over $T$ periods, where demand $d^t$ is drawn from $d_\theta(p)$, and the retailer earns a profit $\mathcal{G}_B(a, b) = p \min \{ d^t, b \} - ab$.

% We assume that demand follows an additive linear model, and inventory is perishable. Both agents have full observability of market demand and action history. The retailer makes rational decisions based on immediate rewards. The game, a variant of an ultimatum game, involves inventory risk and demand uncertainty, complicating the retailer's decision-making.

% \subsection{Analysis on Learnability}



\section{Conclusion}

This work establishes a foundational connection between Stackelberg games and normalizing neural flows, marking a significant advancement in the study of equilibrium learning and manifold learning. By utilizing normalizing flows to map joint action spaces onto Riemannian manifolds, particularly spherical ones, we offer a novel, theoretically grounded framework with formal guarantees on simple regret. This approach represents the first application of normalizing flows in game-theoretic settings, specifically Stackelberg games, thereby opening new avenues for learning on convex manifolds. Our empirical results, grounded in realistic simulation scenarios, highlight promising improvements in both computational efficiency and regret minimization, underscoring the broad potential of this methodology across multiple domains in economics and engineering. Despite potential challenges related to numerical accuracy for the neural flow network, this integration of manifold learning into game theory nevertheless exhibits strong implications for online learning, positioning neural flows as a promising tool for both machine learning and strategic decision-making.

% Our simulations demonstrate the promising implications of this methodology across economic and engineering domains, underscoring its potential for optimizing dual spaces. 

\clearpage

% \section*{Ethics Statement}
% We affirm that this research adheres to the ICLR Code of Ethics. All simulations and methodologies were conducted with integrity and transparency, without harm to individuals, groups, or the environment. We ensured that the theoretical and practical contributions of this work are aimed at advancing knowledge in a responsible and ethical manner, with no misuse or malicious application of the techniques proposed. Additionally, no conflicts of interest or external influences have compromised the objectivity or scientific rigour of this work.


\appendix
\printbibliography

\clearpage









\section{Key Assumptions and Definitions}

\subsection{Compact and Closed Sets}\label{sec:compac_and_closed_set}

In this formal definition, \(\stEmb\) is both compact and closed in the product space \(\mathcal{A} \times \mathcal{B}\). A set \(\stEmb\) is compact if for every open cover \(\{U_i\}_{i \in I}\) of \(\stEmb\), there exists a finite subcover such that \(\stEmb \subseteq \bigcup_{k=1}^n U_{i_k}\), where \(U_{i_k}\) are open sets in \(\mathcal{A} \times \mathcal{B}\). This ensures that \(\stEmb\) is "contained" in a finite manner within the space, even if \(\mathcal{A} \times \mathcal{B}\) is infinite. Furthermore, \(\stEmb\) is closed if its complement, \(\stEmb^c = (\mathcal{A} \times \mathcal{B}) \setminus \stEmb\), is open. This implies that \(\stEmb\) contains all its limit points, making it a complete set within the topological space. Thus, \(\stEmb\) is a compact and closed subset of \(\mathcal{A} \times \mathcal{B}\), meaning that it is both bounded and contains its boundary, providing useful properties for convergence and stability in this space.

\begin{align}
     \forall \{U_i\}_{i \in I}, \quad \stEmb \subseteq \bigcup_{i \in I} U_i \implies \exists \{U_{i_1}, U_{i_2}, \dots, U_{i_n}\} \text{ such that } \stEmb \subseteq \bigcup_{k=1}^{n} U_{i_k}, 
     (\mathcal{A} \times \mathcal{B}) \setminus \stEmb \text{ is open}. \label{eq:compact_closed_definition}
\end{align}

\subsection{Assumptions on Linear Reward Function} \label{sec:ass_lin_reward}

\begin{enumerate}

    \item \textbf{Covariance Matrix}: 
    \begin{align}
        \Sigma_T \equiv \sum_{t=1}^{T} \phi(\aB^t,\bB^t) \phi(\aB^t,\bB^t)^\top + \lambda_{\text{reg}} I
    \end{align}
    
    $\phi(\aB^t,\bB^t)$ must ensure that the covariance matrix $\precMat$ (a.k.a. the inverse of the covariance matrix) is sufficiently large for effective learning. % \lar{We ensure that $\precMat$ is well-conditioned, by examining the behaviour of its eigenvalues.}
    
    \item \textbf{Norm Bounds}:
    \begin{align}
        \| \phi(\aB^t,\bB^t) \|_{\precMat}  \equiv \sqrt{\phi(\aB^t,\bB^t) \precMat \phi(\aB^t,\bB^t)^\top}
    \end{align}

    $\| \phi(\aB^t,\bB^t) \|_{\precMat}$ must be small to ensure efficient uncertainty reduction.
    
    \item \textbf{Regularization Effect}: Regularization parameter \(\lambda_{\text{reg}}\) balances bias and variance, affecting sample complexity.
    \item \textbf{Positive Semi-Definiteness}: $\precMat$ is positive semi-definite (PSD).
\end{enumerate}

\subsection{Discrete Measure Interpretation}

Let $\{x_1, x_2, \ldots, x_n\}$ be a set of discrete points in $\mathbb{R}^n$. We define the measure $\alpha$ on these points as,
\begin{align}
    \alpha = \sum_{i=1}^n \alpha(\{x_i\}) \delta_{x_i}
\end{align}

where $\delta_{x_i}$ is the Dirac measure centered at $x_i$. The integral of a function $f: \mathbb{R}^n \to \mathbb{R}$ with respect to the measure $\alpha$ is given by,

\begin{align}
    \int_{\mathbb{R}^n} f(x) \, d\alpha(x) = \sum_{i=1}^k \alpha(\{x_i\}) f(x_i)
\end{align}




\subsection{Definition of Riemann Manifold} \label{sec:riemmanian_manifold_definition}

% A smooth manifold $M$ endowed with a Riemannian metric $g$ is a Riemannian manifold, denoted $(M, g)$. A Riemannian metric $g$ on manifold $M$ assigns to each $p$ a positive-definite inner product $g_p: T_p M \times T_p M \rightarrow \mathbb{R}$ in a smooth way. This induces a norm $\|\cdot\|_p: T_p M \rightarrow \mathbb{R}$ defined by $\|v\|_p=\sqrt{g_p(v, v)}$. 

A Riemannian manifold, expressed as \( \stEmb \), consists of a smooth manifold \( \stEmb \) equipped with a smoothly varying collection of inner products \( \omega_p \) on each tangent space \( T_p \stEmb \) at every point \( p \in \stEmb \). This assignment \( \omega_p: T_p \stEmb \times T_p \stEmb \rightarrow \mathbb{R} \) is positive-definite, meaning it measures angles and lengths in a consistent and non-degenerate manner. Consequently, each vector \( \mathbf{v} \in T_p \stEmb \) inherits a smoothly defined norm \( \|\mathbf{v}\|_p = \sqrt{\omega_p(\mathbf{v}, \mathbf{v})} \). This structure allows \( \stEmb \) to possess a locally varying yet smoothly coherent geometric framework.


\subsection{Stochastic Perturbation Function} \label{sec:perturb_function_defn}

To model uncertainty in the joint action space, we introduce a stochastic perturbation over the leader and follower actions. Specifically, we define a small, one-step random perturbation function \(\mathfrak{J}(\mathbf{a}, \mathbf{b})\), where \(\mathbf{a} \in \mathbb{R}^m\) and \(\mathbf{b} \in \mathbb{R}^n\) are the actions of the leader and follower, respectively. The perturbed joint action is given by:
\begin{align}
    \mathfrak{J}(\mathbf{a}, \mathbf{b}) &= \left( \mathbf{a}', \mathbf{b}' \right) = \left( \mathbf{a} + \epsilon_a, \mathbf{b} + \epsilon_b \right)
\end{align}
where \(\epsilon_a \in \mathbb{R}^m\) and \(\epsilon_b \in \mathbb{R}^n\) are independent Gaussian perturbations with zero mean and variance \(\sigma_a^2\) and \(\sigma_b^2\), respectively:
\begin{align}
    \epsilon_a &\sim \mathcal{N}(0, \sigma_a^2 I_m), \quad \epsilon_b \sim \mathcal{N}(0, \sigma_b^2 I_n)
\end{align}
Here, \(\sigma_a\) and \(\sigma_b\) are scalar diffusion parameters controlling the magnitude of the perturbation, and \(I_m\) and \(I_n\) are identity matrices of size \(m \times m\) and \(n \times n\), ensuring isotropic perturbations in each component of \(\mathbf{a}\) and \(\mathbf{b}\).

In component form, this perturbation can be written as:
\begin{align}
    a_i' &= a_i + \epsilon_{a_i}, \quad \epsilon_{a_i} \sim \mathcal{N}(0, \sigma_a^2) \\
    b_j' &= b_j + \epsilon_{b_j}, \quad \epsilon_{b_j} \sim \mathcal{N}(0, \sigma_b^2)
\end{align}

This formulation introduces small, independent, and isotropic random deviations from the original actions, modeling the stochastic uncertainty in the decision-making process.

\subsection{Geodesic Repulsion Loss} \label{sec:geodesic_repul_loss_defn}

To encourage an even distribution of points on the spherical manifold, we employ the \textit{Geodesic repulsion loss}, which penalizes pairs of points that are too close in geodesic distance. This loss function facilitates the spreading out of points uniformly over the sphere, preventing clustering.

\textbf{Geodesic Distance:} Let \( \mathbf{y}_i, \mathbf{y}_j \in \mathbb{R}^D \) be points on the surface of a Riemmanian manifold denoted as $\geoDis(\mathbf{y}_i, \mathbf{y}_j)$ in the abstract sense. For a unit sphere it would hold that \( \|\mathbf{y}_i\| = \|\mathbf{y}_j\| = 1 \)). The geodesic distance between two points on the sphere is the angle between them, which can be computed from their dot product,

\begin{equation}
    \geoDis(\mathbf{y}_i, \mathbf{y}_j) = \arccos \left( \mathbf{y}_i^\top \mathbf{y}_j \right),
\end{equation}

where \( \mathbf{y}_i^\top \mathbf{y}_j \) is the dot product of \( \mathbf{y}_i \) and \( \mathbf{y}_j \).

\textbf{Repulsion Term:} To penalize pairs of points that are close in geodesic distance, we use an exponential decay function, which strongly penalizes small distances:
\begin{equation}
    \exp\left(-\frac{\geoDis(\mathbf{y}_i, \mathbf{y}_j)}{\gamma}\right),
\end{equation}
where \( \gamma > 0 \) is a sensitivity parameter controlling how strongly the loss reacts to small distances. A smaller \( \gamma \) enforces stronger repulsion between nearby points.

\textbf{Geodesic Repulsion Loss:} 
The total Geodesic Repulsion Loss is computed as the sum of repulsion terms over all pairs of points, excluding the diagonal (self-repulsion),
\begin{equation}
    \mathcal{L}_{\text{repulsion}} = \sum_{i=1}^{n} \sum_{j=1, j \neq i}^{n} \exp\left(-\frac{\arccos \left( \mathbf{y}_i^\top \mathbf{y}_j \right)}{\gamma}\right),
\end{equation}
where \( n \) is the number of points on the manifold. The geodesic distance \( \geoDis(\mathbf{y}_i, \mathbf{y}_j) \) is computed using the angle between \( \mathbf{y}_i \) and \( \mathbf{y}_j \), ensuring that points are uniformly spaced across the spherical manifold.

To avoid penalizing points for being close to themselves, we exclude the self-repulsion terms by masking the diagonal elements in the pairwise distance computation,
\begin{equation}
    \geoDis(\mathbf{y}_i, \mathbf{y}_i) = 0, \quad \text{for all } i.
\end{equation}

This formulation ensures that points are pushed apart when their geodesic distances are too small, leading to a more uniform distribution on the manifold, which is critical for preserving the geometry of the learned representation.

\subsection{Negative Log-Likelihood Loss for Normalizing Flows} \label{sec:nll_loss_defn}

Let \( x \in \mathbb{R}^d \) be an input data point, and let \( f: \mathbb{R}^d \to \mathbb{R}^d \) be an invertible transformation defined by the normalizing flow. The transformation \( f \) maps the input data \( x \) to a latent variable \( z = f(x) \) that follows a simple base distribution \( p_Z(z) \). Assume that the base distribution is a standard normal distribution, \( Z \sim \mathcal{N}(0, I_d) \), with the probability density function (PDF) given by,

\begin{equation}
    p_Z(z) = \frac{1}{(2\pi)^{d/2}} \exp \left( -\frac{1}{2} \|z\|^2 \right).
\end{equation}

The log probability under this distribution is,

\begin{equation}
    \log p_Z(z) = -\frac{1}{2} \|z\|^2 - \frac{d}{2} \log(2\pi).
\end{equation}

Using the change of variables formula, the probability density of \( x \) under the model is related to the base distribution via the transformation \( f \) as follows,

\begin{equation}
    p_X(x) = p_Z(f(x)) \left| \det \frac{\partial f(x)}{\partial x} \right|.
\end{equation}

Where \( \frac{\partial f(x)}{\partial x} \) is the Jacobian matrix of \( f \) with respect to \( x \), and \( \left| \det \frac{\partial f(x)}{\partial x} \right| \) is the absolute value of the determinant of the Jacobian.

\textbf{NLL Loss:} The negative log-likelihood (NLL) loss for a single data point \( x \) is defined as,

\begin{equation}
    \nfLoss(x) = - \log p_X(x) = - \left[ \log p_Z(f(x)) + \log \left| \det \frac{\partial f(x)}{\partial x} \right| \right].
\end{equation}

Substituting the log probability of \( z = f(x) \) under the base distribution:
\begin{equation}
    \nfLoss(x) = \frac{1}{2} \|f(x)\|^2 + \frac{d}{2} \log(2\pi) - \log \left| \det \frac{\partial f(x)}{\partial x} \right|.
\end{equation}

For a dataset \( \{x_i\}_{i=1}^n \), the total NLL loss is the average over all data points:
\begin{equation}
    \nfLoss = \frac{1}{n} \sum_{i=1}^n \left( \frac{1}{2} \|f(x_i)\|^2 + \frac{d}{2} \log(2\pi) - \log \left| \det \frac{\partial f(x_i)}{\partial x_i} \right| \right).
\end{equation}

The objective of training is to minimize \( \nfLoss \), ensuring that the transformed latent variables \( z = f(x) \) follow the base distribution and the transformation \( f \) appropriately adjusts the volume of space via the Jacobian determinant.



\clearpage



\section{Optimization Algorithms}

\subsection{Bi-level Optimization Structure}\label{sec:bilevel_extra_discuss}

\textbf{Bi-level Optimization Structure:} The optimization problems represented by Eqs. \eqref{eq:pi_a_star} and \eqref{eq:pi_b_star} exhibit the structure of a \textit{bi-level optimization} problem \parencite{beck:2023_bilevel_survey, sinha:2017_bilevel_review, balling:1995_multilevel_algorithm}. Generally, a bilevel optimization problem comprises an upper-level optimization task with an embedded lower-level problem, where the solution to the upper-level problem depends on the solution to the lower-level one. Two conventional methods have been employed to address the bilevel optimization problem. The first leverages the Karush-Kuhn-Tucker (KKT) conditions to exploit the optimality of the lower-level problem (see Appendix \ref{sec:kkt_stack_formulation}). The second employs gradient-based algorithms like gradient ascent (discussed in Appendix \ref{sec:grad_ascent_stack_formulation}). Both approaches, however, have notable limitations. KKT conditions assume strong convexity or pseudo-convexity, making them unsuitable for many non-convex settings, while gradient-based methods, in addition to being computationally inefficient, often struggle or converge poorly when weak-convexity is not guaranteed. Moreover, these methods typically assume optimization under perfect information, whereas we focus on learning-based frameworks with uncertainty due to sampling.


\subsection{KKT Reformulation for Solving Stackelberg Optimization Problems} \label{sec:kkt_stack_formulation}

 The bi-level optimization structure can be solved via reformulating the problem as a bilevel optimization problem via the Karush-Kuhn-Tucker (KKT) conditions. It assumes convexity and differentiability in the embedded space and transforms the original bilevel problem into a single-stage optimization problem via the KKT conditions. 


\begin{equation}
\begin{aligned}
\max_{\polA,\polB,\lambda} \,& \innerP{\theta_A, \, \phi(\polA, \polB) } \\
\text { s.t. } & \polA \in \Pi_A \\
& \nabla_{\polB} \innerP{\theta_B, \, \phi(\polA, \polB)}+\sum_{i=1}^{\ell} \lambda_i \nabla_{\polB} g_{i}(\polB)=0 \\
& g(\polB) \geq 0 \\
& \lambda \geq 0 \\
& \lambda^{\top} g(\polB)=0
\end{aligned}
\end{equation}

where $\Pi_B=\{\polB|g(\polB)\geq 0\}$ and $g_i$ represents the $i$-th constraint of $\Pi_B$.
Specifically, it requires the convexity of the lower level problem \eqref{eq:bottom_bilevel}. Otherwise, KKT complementarity conditions turns the problem into a nonconvex and nonlinear problem even $\pi_B$ is a set of linear constraints. And the problem is incapable to solve under normal nonconvex and nonlinear algorithm. In addition, Slater's constraint qualification is required to ensure that the solution under KKT reformulation is the solution of original bilevel problem.\parencite{allende2013solving}The reformulation involves converting non-linear constraints into a convex hull, thus simplifying the problem into a linear program (LP). Sensitivity analysis can be then performed to understand how changes in constraints impact the solution, with particular attention to the effects of shrinking parameters on the objective function. The approach is utilizes the application of the Weak Duality Theorem to analyze sensitivity. 








\subsection{Gradient Ascent Approach for Solving Bilevel Optimization Problems} \label{sec:grad_ascent_stack_formulation}


Another approaches is transforming Stackelberg game into the the bilevel optimization problem. Namely, we are interested in the following problem,

\begin{equation}
\begin{aligned}
& \min _{x \in \mathbb{R}^d, y \in y^*(x)} f(x, y),\ \text{(Upper-Level)} \\ 
& \text { s.t. } y^*(x) \equiv \arg \min _{y \in Y} g(x, y).\ \text{(Lower-Level)}
\end{aligned}
\end{equation}

The gradient-based algorithms have seen a growing interest in the bilevel problem \parencite{ji2021bilevel,sato:2021gradient,xiao2023generalized,liu2021towards,huang2022enhanced,huang2024optimal}. To measure the stationarity of the lower-level problem, Polyak-Lojasiewicz(PL) condition on $g(x,\cdot)$ is widely applied to show the last-iterate convergence of $\|\nabla_x f(x^t,y^\star(x^t))\|$, i.e,

\begin{equation}
    \|\nabla_y g(x,y)\|^2\geq 2\mu(g(x,y)-\min_z g(x,z)).
\end{equation}

where $\mu$ is a positive constant. This condition relaxes the strong convexity but is still not satisfied for the polynomial function $g(x,y)=y^4$. Also, the lower level function $g(x,\cdot)$ needs to be differentiable in $R^d$. For the Stackelberg game, this is not the case since the follower's strategy $\polB\in\Pi_B$.

Interestingly, should the objective functions be differentiable, one strategy to do this optimization is via gradient descent. Of course the gradient descent algorithm would have to be reformulated to accommodate to a finite amount of traversals based on the gradient update \parencite{sato:2021gradient} \parencite{franceschi:2017forward} \parencite{naveiro:2019_gradient_sg}.



\subsection{Technical Note: Conversion of Absolute Value Constraints into Regular Linear Programming Constraints} \label{sec:tech_note_absval_const_to_lp}

Suppose there exists $D$ dimensions on the L1 norm. Wnd we have the constraint,

\begin{align}
    \norm{\mathbf{x} - \mathbf{c}}_1 \leq D, \quad \text{expressed as,} \quad \, \sum_{i=1}^D |x_i - c_i| \leq C
\end{align}

This can be expressed as, 

\begin{align}
    z_i &\geq x_i - c_d && \text{for } i = 1, 2, \ldots, D \\
    z_i &\geq -(x_i - c_d) && \text{for } i = 1, 2, \ldots, D \\
    \sum_{i=1}^D z_i &\leq C \\
    z_i &\geq 0 && \text{for } i = 1, 2, \ldots, D
\end{align}

By introducing a new dummy variable $z_i$, we and adding $2D+1$ additional constraints, we can express this now as a standard linear program.

\clearpage

% \section{Optimization of Stackelberg Games}

% \textbf{Optimization under Perfect Information:} We see that regardless of the convexity of $\setA$ or $\setB$, so long as we are dealing with compact spaces, under perfect information, we can solve the Stackelberg equilibrium by solving a bilevel optimization problem expressed as,
% \begin{align}
%     &\pi_A^* = \arg\max_{\polA\in \Pi_A} \, \mathbb{E}\big[\innerP{\theta^*_A, \, \phi(\polA, \pi_B^*(\polA)) }\big],\\
%      &\text{where}\quad \pi_B^*(\polA) \equiv \argmaX{\polB\in \Pi_B} \, \mathbb{E}\big[\innerP{\theta^*_B, \, \phi(\polA, \polB)}\big],\label{eq:bottom_bilevel}
% \end{align}
% With a slight abuse of notation, we use $\phi(\polA, \polB)$ and $\pi_B^*(\polA)$ to denote $\mathbb{E}_{\polA,\polB}[\phi]$ and  the best response function in response to policy $\polA$, respectively. The expectation are taken with respect to the sub-Gaussian noises. 

% % \ja{You havent defined regret and no-regret learner!}

% \textbf{Optimization under Parameter Uncertainty:} For some no-regret  learning algorithm suppose that after observing $t$ samples, the uncertainty among the parameters $\theta$ is characterized by,
% \begin{align}\label{eq:param_uncertainty}
%     \Ball(\theta^*,\uncerBall{t})\equiv\Big\{\theta: \norm{\theta^* - \theta} \leq \uncerBall{t}\Big\}.
% \end{align}
% % \begin{align}
% %     \norm{\theta^* - \theta}_p \leq \uncerBall{t}, \text{ with probability} \, \, 1 - \delta. \label{eq:param_uncertainty}
% % \end{align}
% with probability at least $1-\delta$. In this formulation, $||\cdot||$ denotes some norm in the space of parameters. 
% Assuming a \textit{pessimistic} leader, the optimization problem under parameter uncertainty at round $t$ can be expressed as
% \begin{align}
%     &\pi_A^* \equiv \arg\max_{\polA\in \Pi_A} \min_{\theta_A}\, \mathbb{E}\big[\innerP{\theta_A, \, \phi(\polA, \pi_B^*(\polA)) }\big],\quad &\text{s.t.} \quad  \theta_A \in \Ball(\theta^*_A,\uncerBall{t}), \label{eq:pi_a_star}\\ 
%      &\text{where}\quad \pi_B^*(\polA) \equiv \argmaX{\polB\in \Pi_B} \max_{\theta_B}\,\mathbb{E}\big[\innerP{\theta_B, \, \phi(\polA, \polB)}\big] ,\quad &\text{s.t.} \quad  \theta_B \in \Ball(\theta^*_B,\uncerBall{t}). \label{eq:pi_b_star}
% \end{align}
% % \begin{align}
% %     \mathbbm{E}[\underline{\mu}_A] &= \min_{\theta_A} \, \innerP{\theta_A, g^*}, \qquad \text{s.t:} \quad  \theta_A \in \Big\{\theta: \norm{\theta^*_A - \theta}_p \leq \uncerBall{t} \Big\} \\
% %     g^*\in\Phi_{\pi^*_B}(\aB)=\{\},\\
% %      \text{Where,} \quad g^* &= \argmaX{\polB, \theta_B} \, \innerP{\theta_B, \stEmb_{\polB}}, \quad \text{s.t.:} \quad \pi_B \in \Pi_B, \quad \theta_B \in \Big\{\theta: \norm{\theta^*_B - \theta}_p \leq \uncerBall{t} \Big\}.
% % \end{align}

% Given $\pi_B^*(\cdot)$ in Eq. \eqref{eq:pi_b_star}, let us define,
% \begin{align}
%    & \underline{\mathcal{H}}(\theta_A^*,t) \equiv \max_{\pi_A\in\Pi_A}\min_{\theta_A}\, \mathbb{E}\big[\innerP{\theta_A, \, \phi(\polA, \pi_B^*(\polA)) }\big], \quad &\text{s.t.} \quad  \theta_A \in \Ball(\theta^*_A,\uncerBall{t}), \label{eq:min_H} \\
%      & \overline{\mathcal{H}}(\theta_A^*,t) \equiv \max_{\pi_A\in\Pi_A}\max_{\theta_A}\, \mathbb{E}\big[\innerP{\theta_A, \, \phi(\polA, \pi_B^*(\polA)) }\big], \quad &\text{s.t.} \quad  \theta_A \in \Ball(\theta^*_A,\uncerBall{t}). \label{eq:max_H}
% \end{align}

% \textbf{Bi-level Optimization Structure:} The optimization problems represented by Eqs. \eqref{eq:pi_a_star} and \eqref{eq:pi_b_star} exhibit the structure of a \textit{bi-level optimization} problem \parencite{beck:2023_bilevel_survey, sinha:2017_bilevel_review, balling:1995_multilevel_algorithm}. Generally, a bilevel optimization problem comprises an upper-level optimization task with an embedded lower-level problem, where the solution to the upper-level problem depends on the solution to the lower-level one. Two conventional methods have been employed to address the bilevel optimization problem. The first leverages the Karush-Kuhn-Tucker (KKT) conditions to exploit the optimality of the lower-level problem (see Appendix \ref{sec:kkt_stack_formulation}). The second employs gradient-based algorithms like gradient ascent (discussed in Appendix \ref{sec:grad_ascent_stack_formulation}). Both approaches, however, have notable limitations. KKT conditions assume strong convexity or pseudo-convexity, making them unsuitable for many non-convex settings, while gradient-based methods, in addition to being computationally inefficient, often struggle or converge poorly when weak-convexity is not guaranteed. Moreover, these methods typically assume optimization under perfect information, whereas we focus on learning-based frameworks with uncertainty due to sampling.

% % Two primary methods have been traditionally employed to tackle bilevel optimization. The first approach involves the Karush-Kuhn-Tucker (KKT) conditions, which leverage the optimality conditions of the lower-level problem (see Appendix \ref{sec:kkt_stack_formulation} for a detailed discussion). The second approach utilizes gradient-based algorithms such as gradient descent (further elaborated in Appendix \ref{sec:grad_ascent_stack_formulation}). However, both methods exhibit significant limitations. The reliance on KKT conditions necessitates strong assumptions of convexity or pseudo-convexity, rendering them impractical for more general non-convex settings often encountered in real-world applications. Similarly, gradient-based approaches can be inefficient or converge to suboptimal solutions when certain definitions of weak-convexity cannot be guaranteed. Moreover, these traditional approaches are generally designed for scenarios with perfect information and equilibrium conditions, whereas our focus is on learning-based frameworks where the problem dynamics are often uncertain and data-driven.





% % \begin{equation}
% % \begin{aligned}
% %    & \underline{\calH}: = \min_{g, \theta_A} \,\, \innerP{\theta_A,g}, \qquad \text{s.t. }  g \in \stEmb_{\polB}^*(\theta_B)    
% % \end{aligned}
% % \end{equation}



% % \begin{theorem} \label{thm:lower_bound_calH}
% %     \textbf{Lower Bound on $\calH$:} Under convexity assumptions, we guarantee a bound on worst case minimization of $\underline{\calH}$ is lower bounded by $\underline{\calH} \geq K_1 \mathcal{C}(t)$, given any function $\mathcal{C}(t)$, such that $\norm{\theta - \theta^*}_p \leq \mathcal{C}(t)$. (Proof in Appendix \ref{prf:lower_bound_calH}.)\ja{$\underline{\calH}$ can be zero.}
% % \end{theorem}

% \clearpage

\section{Topology \& Geodesy}

\subsection{Convex Manifold Definitions:} \label{sec:convex_manifold_desc}

\begin{definition} \label{def:geodesic_convex_subsets}
    \textbf{Geodesically Convex Sets:} Let $(\stEmb, h)$ be a Riemannian manifold, where $\stEmb$ is a smooth manifold and $h(\aB, \bB)$ is a Riemannian metric on $\stEmb$ (i.e. innner product). A subset $\convexGeoSet \subseteq \stEmb$ is said to be \textit{geodesically convex} if for any two points $\aB, \bB \in \convexGeoSet$, there exists a geodesic $\tau: d \in [0, 1] \to \stEmb$ parameterized by $d$ such that, 
    \begin{align}
        \tau(0) = \aB, \, \, \tau(1) = \bB, \, \qquad \text{and}, \, \, \tau(d) \in \convexGeoSet, \quad \forall d \in [0, 1].
    \end{align}
\end{definition}

Where $d$ can be viewed as a parameter proportional to the distance travelled along the geodesic. In other words, a set $\convexGeoSet$ is geodesically convex if for any two points in $\convexGeoSet$, there exists a geodesic between these points that lies entirely within $\convexGeoSet$. 

\begin{definition} \label{def:convex_manifold}
    \chagesMarker{\textbf{Convex Manifolds:} A convex manifold is a manifold where the geodesic between any two points on the manifold falls within, or constitutes, a geodesically convex set $\mathcal{S}_\stEmb$, as per Definition \ref{def:geodesic_convex_subsets}.}
\end{definition}


\subsection{\tocheck{Linear Relationship of the Embedding Space $\stEmb$ to the Reward Space} } \label{sec:note_embed_reward_linear_appendix}

The linear bandit assumption that the expected reward is the inner product between an embedding space and some linear parameters is commonly accepted in the online learning literature \parencite{amani:2019linear_bandit_safety, moradipari:2022feature_map, zanette:2021-doe_bandit, lattimore:2020_bandit_book}. In our problem setting, we make the similar assumption that there exists a linear relationship between the embedding space and the reward space. To demonstrate rigorously, let $Y$ represent an expectation over the reward variable. Given a linear relation in the action space \(X\), suppose the reward can be expressed as,
    
    \[
    Y = \mathfrak{F}(X) = \langle \tilde{\theta}, \tilde{\phi}(X) \rangle.
    \]
    $\mathfrak{F}(X)$ constitutes an abstract function, and $\tilde{\phi}(\cdot)$, represents a feature map, a common tool in linear multi-armed bandit learning \parencite{zanette:2021-doe_bandit}. Suppose our feature map, $\tilde{\theta}$ can be arbitrarily complex, we can therefore replicate the result of $\mathfrak{F}(X)$ via $\langle \tilde{\theta}, \tilde{\phi}(X) \rangle$ for most well-behaved functions. In one perspective, we can view the linear feature map as the final layer in a neural network output. Instead of learning the action to reward relation directly via a neural network per se, we learn the relation of the ambient space (or the joint action space) to the embedding space. 

% \begin{lemma}
%     \textbf{Linear Relation for Smooth Invertible Maps:} Suppose that the function \(Y\) can be expressed as \(Y = \langle \tilde{\theta}, \tilde{\phi}(X) \rangle\), where \(\tilde{\phi}(X)\) is smooth, bijective, and maps \(X \in \mathbb{R}^d\) to \(\mathbb{R}^d\). If there exists a smooth and bijective transformation \(T: \mathbb{R}^d \to \mathbb{R}^d\) such that \(\phi(X) = T(\tilde{\phi}(X))\), then there always exists an alternative set of parameters \(\theta \in \mathbb{R}^d\) such that:
%     \[
%     Y = \langle \tilde{\theta}, \tilde{\phi}(X) \rangle = \langle \theta, \phi(X) \rangle.
%     \]
% \end{lemma}

\begin{lemma} \label{lem:existence_of_theta}
    \textbf{Linear Relation for Smooth Invertible Maps:} Suppose that \(Y\) can be expressed as \(Y = \langle \tilde{\theta}, \tilde{\phi}(X) \rangle\), where \(\tilde{\phi}(X): \mathbb{R}^d \to \mathbb{R}^d\) is smooth and bijective, and \(\tilde{\theta} \in \mathbb{R}^d\). Then, then there exists an alternative set of parameters \(\theta \in \mathbb{R}^k\) and a higher-dimensional mapping \(\phi(X): \mathbb{R}^d \to \mathbb{R}^k\) for any \(k \geq d\) such that, $Y = \langle \theta, \phi(X) \rangle$.
\end{lemma}

% , a bijective transformation \(T: \mathbb{R}^d \to \mathbb{R}^k\),

\begin{proof}

The proof of this lemma proceeds by first establishing the existence of an equivalent higher-dimensional representation of $\tilde{\phi}(X)$. Next we demonstrate that, should $\tilde{\phi}$ be smooth and bijective, then given another $\phi: X \to \phi(X)$ that is also smooth and bijective, a bijection, $T(\cdot)$, must exist between their respective images $T: \tilde{\phi}(X) \to \phi(X)$. Subsequently, we demonstrate that should a bijective transformation $T(\cdot)$ exist between the proposed $\phi(X)$ and $\tilde{\phi}(X)$, then a corresponding valid parameter set $\theta$ also exists. Thus, $\phi(X)$ and $\theta$ can serve as equivalent representations to $\tilde{\phi}(X)$ and $\tilde{\theta}$, with no  restrictions on the dimension of $\phi(X)$. 

% Finally, in the absence of a guaranteed bijective $T(\cdot)$, we argue that, given sufficiently high dimensionality of $\phi(X)$, an approximation of $T(\cdot)$ can be constructed with arbitrarily low error as the dimensional complexity increases. 
%achieving arbitrarily low approximation error assuming 

\paragraph{Higher-Dimensional Embedding of \(\tilde{\phi}(X)\):} Let \(\tilde{\phi}'(X): \mathbb{R}^d \to \mathbb{R}^k\) for $k \geq d$ be a higher-dimensional embedding of \(\tilde{\phi}(X)\), defined as,

\[
\tilde{\phi}'(X) = \begin{bmatrix} \tilde{\phi}(X) \\ g(X) \end{bmatrix},
\]

where \(g(X): \mathbb{R}^d \to \mathbb{R}^{k-d}\) is a smooth function. We further define the extended parameter vector as,

\[
\tilde{\theta}' = \begin{bmatrix} \tilde{\theta} \\ 0 \end{bmatrix} \in \mathbb{R}^k.
\]

Then, the original relationship is preserved:

\[
Y = \langle \tilde{\theta}, \tilde{\phi}(X) \rangle = \langle \tilde{\theta}', \tilde{\phi}'(X) \rangle.
\]

The consequence is that we can always find equivalent higher dimension exact representations of $\tilde{\phi}(X)$ and $\tilde{\theta}$. So long as the intrinsic dimension of the spaces is preserved, $\tilde{\theta}$ can have an equivalent representation in any higher dimensional space. (In our implementation, we apply a series of bijective stereographic transformations in the ambient space $X$ to increase the dimension of the embedding space where necessary.) For convenience moving forward, we shall refer to $\tilde{\phi}'(X)$ as $\tilde{\phi}(X)$ in the $k^{th}$ dimension, as their representations are equivalent.

% \paragraph{Bijections between Images of Smooth Bijective Functions:} Let \( \tilde{\phi}: X \to \text{Im}(\tilde{\phi}) \) and \( \phi: X \to \text{Im}(\phi) \) be smooth bijections. Then there must exist a bijection between \( \text{Im}(\tilde{\phi}) \) and \( \text{Im}(\phi) \). Since \( \tilde{\phi} \) and \( \phi \) are bijections, their inverses \( \tilde{\phi}^{-1}: \text{Im}(\tilde{\phi}) \to X \) and \( \phi^{-1}: \text{Im}(\phi) \to X \) exist. 

% Let us define the function \( T: \text{Im}(\tilde{\phi}) \to \text{Im}(\phi) \) by
% \[
% T(f) = \phi\big(\tilde{\phi}^{-1}(f)\big).
% \]

% \textbf{Injectivity}: If \( h(f_1) = h(f_2) \), then \( \phi\big(\tilde{\phi}^{-1}(f_1)\big) = \phi\big(\tilde{\phi}^{-1}(f_2)\big) \). Since \( \phi \) is injective, \( \tilde{\phi}^{-1}(f_1) = \tilde{\phi}^{-1}(f_2) \). As \( \tilde{\phi}^{-1} \) is injective, \( f_1 = f_2 \).

% \textbf{Surjectivity}: For any \( g \in \text{Im}(\phi) \), let \( x = \phi^{-1}(g) \). Then \( h\big(\tilde{\phi}(x)\big) = \phi\big(\tilde{\phi}^{-1}(\tilde{\phi}(x))\big) = \phi(x) = g \). Thus, \( h \) is surjective. 


% Therefore, \( T \) is a bijection between \( \text{Im}(\tilde{\phi}) \) and \( \text{Im}(\phi) \).

% \hlinE

\paragraph{Existence of \( \phi(X) \):}  Let $\phi: X \to \phi(X)$ be another smooth bijection along with $\tilde{\phi}: X \to \tilde{\phi}(X)$. Then, there exists a bijection between $\tilde{\phi}(X)$ and $\phi(X)$. Since $\tilde{\phi}$ is bijective, it has an inverse function $\tilde{\phi}^{-1}: \tilde{\phi}(X) \to X$. Similarly, $\phi$ has an inverse function $\phi^{-1}: \phi(X) \to X$. Let us now define the function,

\begin{equation}
    T: \tilde{\phi}(X) \to \phi(X), \quad T(y) = \phi(\tilde{\phi}^{-1}(y)).
\end{equation}
Since $\tilde{\phi}^{-1}$ is bijective, it uniquely maps each $y \in \tilde{\phi}(X)$ to some $x \in X$. Then, $\phi$ uniquely maps this $x$ to an element in $\phi(X)$. We next demonstrate that $T(\cdot)$ is both injective and surjective.

\begin{enumerate}[I.]
    \item \textbf{Injectivity:} If $T(y_1) = T(y_2)$, then $\phi(\tilde{\phi}^{-1}(y_1)) = \phi(\tilde{\phi}^{-1}(y_2))$. Since $\phi$ is bijective, it follows that $\tilde{\phi}^{-1}(y_1) = \tilde{\phi}^{-1}(y_2)$, and applying $\tilde{\phi}$ to both sides gives $y_1 = y_2$.

    \item \textbf{Surjectivity:} For every $z \in \phi(X)$, there exists $x \in X$ such that $z = \phi(x)$. Since $\tilde{\phi}$ is bijective, there exists $y = \tilde{\phi}(x) \in \tilde{\phi}(X)$. Thus, $T(y) = \phi(\tilde{\phi}^{-1}(y)) = z$, demonstrating surjectivity.

\end{enumerate}



Therefore, given that $T$ is both an injection and surjection, $T$ constitutes a bijection between $\tilde{\phi}(X)$ and $\phi(X)$. 



\paragraph{Existence of $\langle \theta, \phi(X) \rangle$:} Given there there exists a bijection \(T: \mathbb{R}^k \to \mathbb{R}^k\), where for any higher-dimensional smooth and bijective mapping \(\phi(X): \mathbb{R}^d \to \mathbb{R}^k\), the following relation holds,

\[
\phi(X) = T(\tilde{\phi}(X)).
\]

Since \(T\) is bijective, the inverse \(T^{-1}\) exists and is smooth. This implies that

\[
\tilde{\phi}(X) = T^{-1}(\phi(X)).
\]

Given that there exists some bijective transformation $T: \tilde{\phi}(X) \mapsto \phi(X)$, and its inverse $T^{-1}: \phi(X) \mapsto \tilde{\phi}(X)$, that is smooth and differentiable w.r.t. X. We can then express, 
    
    \[
    Y = \langle \tilde{\theta}, T^{-1}(\phi(X)) \rangle
    \]



\paragraph{Change of Variables:} To obtain a swap from $\tilde{\theta}$ to $\theta$ ,we select $\theta$ as,
    
    \[
        \theta = (J_T^\top)^{-1} \tilde{\theta},
    \]
    
    where $J_T$ is the Jacobian of $T(\cdot)$ w.r.t. $\tilde{Y}$, where $\tilde{Y} \in \text{Im}(\tilde{\phi})$. Therefore, we show the existence of $\theta$ s.t.,
    
    \[
    Y = \langle \theta, \phi(X) \rangle,
    \]
    
    where $\phi(X) \equiv T \circ \tilde{\phi}(X)$. This argument relies on the bijectiveness and smoothness of $T(\cdot)$ consequently being a diffeomorphism, a fair assumption so long as we consider $\tilde{\phi}(X)$ to be smooth and well-behaved.

% Substituting into the original equation, we have

% \[
% Y = \langle \tilde{\theta}, \tilde{\phi}(X) \rangle = \langle \tilde{\theta}, T^{-1}(\phi(X)) \rangle.
% \]

% Define a new parameter vector \(\theta \in \mathbb{R}^k\) as

% \[
% \theta = (T^{-1})^\top \tilde{\theta},
% \]

% where \((T^{-1})^\top\) is the transpose of the Jacobian matrix of \(T^{-1}\). Thus, we obtain

% \[
% Y = \langle \theta, \phi(X) \rangle.
% \]

\end{proof}



\subsection{Notes on Scaling and Approximation Error for Non-Smooth $\stEmb$}

For non-smooth $\tilde{\phi}(X)$ we could settle for a continuous-to-discrete approximation, particularly useful for large discrete action spaces. As there are no limitations to what the learned feature map $\phi$ can be, it gives us great flexibility when it comes to constructing $\phi$ to approximate $T \circ \tilde{\phi}(\cdot)$. For this purpose, we apply normalizing flows  \parencite{brehmer:2020_manifold_flows, durkan:2020_nflows, dinh:2016density} a  technology developed specifically for mapping ambient data to a desired manifold, $\stEmb$, of which multiple sufficient approximations for $\phi$ could potentially exist. This enables the construction of $\stEmb$ such that the linear relation between the embedding space and reward space will hold.


\paragraph{Scaling and Approximation of \(T(\cdot)\):} Suppose we are uncertain if $T(\cdot)$ exists in the higher dimension $k$. For any proposal of \(\phi(X)\) that is smooth, bijective, and generally well-behaved, should \(\phi(X)\) be a sufficiently overparameterized mapping (i.e., \(k \gg d\)), we can then approximate any bijective transformation between \(\tilde{\phi}(X)\) and \(\phi(X)\) with arbitrarily small error. Specifically, for any \(\epsilon > 0\), there exists a sufficiently large \(k\) such that:

\begin{align}
    \|T(\tilde{\phi}(X)) - \phi(X)\| < \epsilon, \qquad \text{and similarly,} \qquad \|T^{-1}(\phi(X)) - \tilde{\phi}(X)\| < \epsilon. \label{eq:tmap_est_error_v2}
\end{align}



% \begin{align}
%     \|T^{-1}(\phi(X)) - \tilde{\phi}(X)\| < \epsilon. 
% \end{align}

This follows from the universal approximation property \parencite{hornik:1989_universal_approx_multilayer, cybenko:1989_universal_approximation}, which states that increasing dimensionality provides additional degrees of freedom to represent complex transformations, effectively reducing approximation error.

\paragraph{Diminishing Approximation Error:} Let $\epsilon_{\texttt{MAP}}$ denote the approximation error in the representation of \(Y\) arising from the error in \(T(\cdot)\) and is given by:

\begin{align}
    \epsilon_{\texttt{MAP}} = \langle \tilde{\theta}, \tilde{\phi}(X) \rangle - \langle \theta, \phi(X) \rangle. \label{eq:e_map_inner_prod_expr}
\end{align}

Should we then apply the Cauchy-Schwarz inequality over Eq. \eqref{eq:e_map_inner_prod_expr}, and substitute the approximation of \(T(\cdot)\) from Eq. \eqref{eq:tmap_est_error_v2}, the error $\epsilon_{\texttt{MAP}}$ can be bounded by,

\[
\epsilon_{\texttt{MAP}} \leq \|\tilde{\theta}\| \cdot \|T^{-1}(\phi(X)) - \tilde{\phi}(X)\| = \|\tilde{\theta}\| \cdot \epsilon,
\]

which is upper-bounded by a factor of $\epsilon$ w.r.t. $\|T^{-1}(\phi(X)) - \tilde{\phi}(X)\| \leq \epsilon$. By scaling \(k\) sufficiently large, we reduce \(\epsilon\) to an arbitrarily small value, making \(\epsilon_{\texttt{MAP}}\) negligible. Thus, given the initial expression $Y = \langle \tilde{\theta}, \tilde{\phi}(X) \rangle$, there always exists a higher-dimensional \(\phi(X)\) and a parameter vector \(\theta\) such that,

\[
Y = \langle \theta, \phi(X) \rangle,
\]

with negligible approximation error, $\epsilon_{\texttt{MAP}}$.

With respect to Lemma \ref{lem:existence_of_theta}, the absence of explicit knowledge of the transformation \(T: \tilde{\phi}(X) \mapsto \phi(X)\) does not undermine the validity of the result, provided there are no restrictions on the dimensionality of \(\phi(X)\). In our methodology, no such restrictions are imposed. Through \textit{overparameterization}, embedding \(X\) into a higher-dimensional space \(\phi(X)\) enables the mapping to indirectly capture the latent structure of \(\tilde{\phi}(X)\) and \(T(\cdot)\). Additionally, by constructing \(\phi(X)\) as a sufficiently complex approximation (using techniques such as normalizing flows) we ensure it can approximate any smooth, bijective transformation $T: \tilde{\phi}(X) \mapsto \phi(X)$. This flexibility allows \(\phi(X)\) to accommodate a wide range of mappings, ensuring that the representation \(Y = \langle \theta, \phi(X) \rangle\) holds even in the absence of explicit knowledge of \(T(\cdot)\). Simply put, if \(\phi(X)\) is sufficiently complex, an equivalent representation of the original \(Y = \langle \theta, \phi(X) \rangle\) can always be achieved. An alternative interpretation can also be drawn from the \textit{universal approximation theorem} \parencite{hornik:1989_universal_approx_multilayer, cybenko:1989_universal_approximation}, which states that a sufficiently large neural network with a nonlinear activation function can approximate any continuous function on a compact domain to arbitrary precision.





% Overparameterization or a generalized \(\phi(X)\) can be sufficient even without explicit knowledge of \(T(\cdot)\) due to its flexibility to approximate unknown transformations.







% By scaling the dimensionality \(k\) of \(\phi(X)\) indefinitely, the flexibility of \(T(\cdot)\) ensures that the approximation error in the transformation becomes arbitrarily small. 

% Given that \(\phi(X)\) can be overparameterized or constructed as a universal approximator, let \(\phi_\epsilon(X)\) be a smooth bijective approximation such that,

% \[
% \|\phi(X) - \phi_\epsilon(X)\| < \epsilon,
% \]

% for any \(\epsilon > 0\). Then, the corresponding parameter vector \(\theta_\epsilon\) can be defined such that

% \[
% Y = \langle \theta_\epsilon, \phi_\epsilon(X) \rangle + \mathcal{O}(\epsilon),
% \]

% where \(\mathcal{O}(\epsilon)\) represents the approximation error which can be made arbitrarily small. The freedom to arbitrarily scale up $\tilde{\phi}$ and $\tilde{\theta}$,

% The existence of a higher-dimensional mapping \(\phi(X)\), a bijective transformation \(T\), and a parameter vector \(\theta\) satisfying \(Y = \langle \theta, \phi(X) \rangle\) is guaranteed, with the approximation error being arbitrarily small. 


% \hlinE

% \begin{proof}
%     Given a linear relation in the action space \(X\), we wish to show that  there always exists some alternative parameters $\theta$ and $\phi$ s.t.,
    
%     \[
%     \langle \tilde{\theta}, \tilde{\phi}(X) \rangle = \langle \theta, \phi(X) \rangle.
%     \]
    
%     Given that there exists some bijective transformation $T: \tilde{\phi}(X) \mapsto \phi(X)$, and its inverse $T^{-1}: \phi(X) \mapsto \tilde{\phi}(X)$, that is smooth and differentiable w.r.t. X. We can then express, 
    
%     \[
%     Y = \langle \tilde{\theta}, T^{-1}(\phi(X)) \rangle
%     \]
    
%     To obtain a swap from $\tilde{\theta}$ to $\theta$ ,we select $\theta$ as,
    
%     \[
%         \theta = (J_T^\top)^{-1} \tilde{\theta},
%     \]
    
%     where $J_T$ is the Jacobian of $T(\cdot)$ w.r.t. $\tilde{Y}$, where $\tilde{Y} \in \text{Im}(\tilde{\phi})$. Therefore, we show the existence of $\theta$ s.t.,
    
%     \[
%     Y = \langle \theta, \phi(X) \rangle,
%     \]
    
%     where $\phi(X) \equiv T \circ \tilde{\phi}(X)$. This argument relies on the bijectiveness and smoothness of $T(\cdot)$ consequently being a diffeomorphism, a fair assumption so long as we consider $\tilde{\phi}(X)$ to be smooth and well-behaved. 
% \end{proof}



\subsection{Proof of Lemma \ref{lem:geodesic_and_closeness_phi}} \label{prf:geodesic_and_closeness_phi}

\textbf{Geodesic Distance and Closeness to $\xi_\theta$:} Let $\Phi \subset \mathbb{R}^D$ be a manifold serving as a boundary of a convex set in $\mathbbm{R}^D$. Given $\theta$, let $\xi_\theta \in \Phi$ be the point on the manifold that maximizes the dot product $\langle \theta, \xi_\theta \rangle$, and is orthogonal to $\stEmb$ at the point of intersection. For any two points on the manifold $\theta_A', \theta_B' \in \Phi$, if the geodesic distance between $\xi_\theta$ and $\theta_A'$ is greater than the geodesic distance between $\xi_\theta$ and $\theta_B'$, $\geoDis(\xi_\theta, \theta_A') > \geoDis(\xi_\theta, \theta_B')$, then the dot product satisfies $\langle \theta, \theta_A' \rangle < \langle \theta, \theta_B' \rangle$. 



\begin{proof}
    \textbf{Geodesic Distance and Closeness to $\xi_\theta$:} Since $\mathcal{M}$ is a smooth, compact manifold bounding a convex region, the geodesic distance between two points on $\mathcal{M}$, say $\xi_1, \xi_2 \in \mathcal{M}$, is defined as the shortest path along the manifold $\geoDis(\xi_1, \xi_2)$ between $\xi_1$ and $\xi_2$. For convex manifolds, the geodesic distance behaves similarly to the distance on the surface of a sphere: an increase in the geodesic distance from $\xi_\theta$ to another point on the manifold corresponds to an increase in the angle between the tangent vector at $\xi_\theta$ and the vectors corresponding to points on the manifold. Hence, if $\geoDis(\xi_\theta, \theta_A') > \geoDis(\xi_\theta, \theta_B')$, the angle between $\xi_\theta$ and $\theta_A'$ is larger than the angle between $\xi_\theta$ and $\theta_B'$. 
    
    
    \textbf{Dot Product and Angle:} The dot product $\langle \theta, \xi \rangle$ between a normal vector $\theta$ at $\xi_\theta$ and a point $\xi$ on the manifold is given by:
    
    \begin{align}
        \langle \theta, \xi \rangle = \|\theta\| \|\xi\| \cos(\alpha)
    \end{align}
    
    where $\alpha$ is the angle between the vectors $\theta$ and $\xi$. Since $\theta = \frac{\xi_\theta}{\|\xi_\theta\|}$ (as $\xi_\theta$ is a unit vector), the angle between $\theta$ and any point $\xi$ on the manifold depends only on the angle between $\xi_\theta$ and $\xi$. Since $\geoDis(\xi_\theta, \theta_A') > \geoDis(\xi_\theta, \theta_B')$ implies that the angle between $\xi_\theta$ and $\theta_A'$ is larger than the angle between $\xi_\theta$ and $\theta_B'$, we have:
    
    \begin{align}
        \cos(\alpha_{\theta_A'}) < \cos(\alpha_{\theta_B'}),
    \end{align}
    
    where $\alpha_{\theta_A'}$ is the angle between $\theta$ and $\theta_A'$, and $\alpha_{\theta_B'}$ is the angle between $\theta$ and $\theta_B'$. 
    
    \textbf{Conclusion on Dot Products:} Since the dot product is proportional to the cosine of the angle between the vectors, and $\cos(\alpha_{\theta_A'}) < \cos(\alpha_{\theta_B'})$, it follows that:
    
    \begin{align}
        \langle \theta, \theta_A' \rangle = \|\theta\| \|\xi_{\theta_A'}\| \cos(\alpha_{\theta_A'}) < \langle \theta, \theta_B' \rangle = \|\theta\| \|\xi_{\theta_B'}\| \cos(\alpha_{\theta_B'}).
    \end{align}
    
    Therefore,
    
    \begin{align}
        \langle \theta, \theta_A' \rangle < \langle \theta, \theta_B' \rangle.
    \end{align}
    
\end{proof}


\subsection{Lemma \ref{lem:max_on_manifold}} \label{prf:max_on_manifold}

\begin{lemma} \label{lem:max_on_manifold}
    \textbf{Maximization on a Manifold:} Given a smooth manifold $\stEmb$, and objective vector $\theta$, the element on a manifold which optimizes $\innerP{\phi, \theta}$ is the element whose normal vector's tangent plane $\overrightarrow{\mathbf{n}}_\stEmb$ is collinear with $\theta$. (Proof in Appendix \ref{prf:max_on_manifold}.) 
\end{lemma}

\begin{proof}
    Let $\Phi \subset \mathbb{R}^D$ be the unit sphere, defined as:
    \[
    \Phi = \{ \phi \in \mathbb{R}^D \mid \|\phi\| = 1 \}.
    \]
    Given a vector $\theta_A \in \mathbb{R}^D$, we aim to find the point $\phi^*$ on the sphere that maximizes the inner product $\langle \phi, \theta_A \rangle$. This can be formally stated as the following optimization problem:
    \[
    \begin{aligned}
    & \underset{\phi \in \mathbb{R}^D}{\text{maximize}} \quad && \langle \phi, \theta_A \rangle \\
    & \text{subject to} \quad && \|\phi\| = 1.
    \end{aligned}
    \]
    \textbf{Optimization Formulation:} The problem is a constrained optimization problem where the objective is to maximize the dot product $\langle \phi, \theta_A \rangle$ and the constraint ensures that $\phi$ lies on the unit sphere. Mathematically:
    \[
    \begin{aligned}
    & \underset{\phi \in \mathbb{R}^D}{\text{maximize}} \quad && \langle \phi, \theta_A \rangle \\
    & \text{subject to} \quad && g(\phi) = \|\phi\|^2 - 1 = 0.
    \end{aligned}
    \]
    Here, $g(\phi)$ represents the constraint that $\phi$ lies on the unit sphere.
\end{proof}

\subsection{Proof of Lemma \ref{lem:pure_strategy_convex_manifold}} \label{prf:pure_strategy_convex_manifold}

\textbf{Pure Strategy of the Follower:} While optimizing over a convex manifold, proposed in Definition \ref{def:convex_manifold}, given any objective vector $\theta$, the linear structure of the reward functions from Eq. \eqref{eq:A_reward_inner_prod} and Eq. \eqref{eq:B_reward_inner_prod}, and that the subspace induced by $\aB \in \setA$ forms a geodesically convex subset, as defined in Definition \ref{def:geodesic_convex_subsets}, the optimal strategy of the follower, will be that of a pure strategy, such that $\polA(\bB | \aB) \in \{0, 1\}$.

\begin{proof}
    The goal is to show that the follower's optimal strategy $\polA(\bB|\aB)$ is a pure strategy, such that $\polA(\bB|\aB) \in \{0, 1\}$. Let the objective vector $\theta \in \mathbb{R}^D$ define the direction of optimization, with the reward function given by, 

    \begin{align}
        \mu(\aB, \bB) = \langle \phi(\aB, \bB), \theta \rangle,
    \end{align}
    where $\phi: \setA \times \setB \to \mathbb{R}^D$ is a feature map. 
    
    % The set $\setA$ is assumed to form a geodesically convex subset of $\mathcal{M}$, and $\aB \in \setA$.
    
    Since $\stEmb$ is geodesically convex, for any point $\aB \in \setA$, there exists a unique geodesic that connects the subspace formed by fixing $\aB$, denoted as $\stEmb_{\aB} \equiv \phi(\aB, \cdot)$ to any other point $g \in \stEmb$. By Lemma \ref{lem:geodesic_and_closeness_phi} in order to maximize the follower's reward $\utlB$, we must find the shortest geodesic distance, $\geoDis(\cdot)$, to $\theta_A'$ within $\convexGeoSet$. We express this as,  
    
    \begin{align}
        \phi(\aB, \bB^*) = \arg \min_{g \in \convexGeoSet} \geoDis(\aB, \bB),
    \end{align}
    
    Since $\stEmb$ is convex, this minimizer is unique. The reward function $\utlB(\aB, \bB)$ depends on the inner product $\langle \phi(\aB, \bB), \theta_B \rangle$. As this structure is linear with respect to $\phi(\aB, \bB)$, maximizing the reward is equivalent to minimizing the geodesic distance from $\phi(\aB, \bB)$ to the objective vector $\theta$. Since this minimizer is unique by geodesic convexity, the follower's optimal strategy will correspond to this unique solution $\bB^*$ given $\aB$. As there are no alternative solutions for $\phi(\bB^*, \cdot)$ given $\aB$. Because $\phi(\cdot)$ is a bijective mapping, we conclude that any probablistic mapping function must adhere to $\polA(\bB | \aB) \in \{0, 1\}$.

    % Define $\text{Proj}\mathcal{M}(\theta)$ as the projection of the objective vector $\theta$ onto $\mathcal{M}$. The follower seeks to maximize the reward function by selecting a point $\bB^*$ such that the geodesic distance from $\aB$ to the projection of $\theta$ is minimized. 
    
    % Specifically, we aim to find:
\end{proof}

\subsection{\chagesMarker{Proof of Lemma} \ref{lem:intersect_submanifold_AB}} \label{prf:intersect_submanifold_AB}

\textbf{Intersection of $\IsoPL{\aB}$ and $\IsoPL{\bB}$:} Given a bipartite spherical map $\mathcal{Q}(\cdot)$ from Definition \ref{def:bipartite_sphere_map}, with $\aB$ parameterizing the azimuthal (latitudinal) coordinates, the cardinality of the intersect between $\IsoPL{\aB}$ and $\IsoPL{\bB}$ will be non-empty. That is, $|\IsoPL{\aB} \cap \IsoPL{\bB}| > 0$.  

% \textbf{Existence of the Submanifold Intersection $\Psi_\theta$:} Suppose we construct a function that converts Cartesian coordinates $\aB$,  and $\bB$ to spherical coordinates, with a partition of which subset of spherical coordinates to map to. By design $\aB$ always controls the azimuthal (latitudinal) coordinate. Then at the intersection, the cardinality of the intersect between $\IsoPL{\aB}$ and $\IsoPL{\bB}$ is non-empty, $|\IsoPL{\aB} \cap \IsoPL{\bB}| > 0$.

\begin{proof}
    Given two distinct points $\theta_A'$ and $\theta_B'$, we define the \emph{isoplane}, $\IsoPL{\aB}$, as the submanifold formed by fixing a subset of spherical coordinates $(\gamma_1^{(A)}, \dots, \gamma_k^{(A)})$, including the azimuthal angle $\nu^{(A)}$, and allowing the remaining coordinates to vary. Similarly, the isoplane at $\theta_B'$ is formed by fixing a different subset of spherical coordinates $(\gamma_{k+1}^{(B)}, \dots, \gamma_{D-2}^{(B)})$, while allowing the rest to vary.

 If $\stEmb$ is a compact, orientable, smooth manifold without boundary, and $\vec{X}$ is a smooth vector field on $\stEmb$ with isolated zeros, the \textit{Poincaré-Hopf theorem} states that,

% First, via the \emph{Poincaré-Hopf theorem}, which provides a relationship between the vector fields on a manifold and its topology, 

\begin{align}
    \sum_{\mathbf{P} \in \text{Zeroes}(\vec{X})} \text{Index}(\vec{X}, \mathbf{P}) = \chi(\stEmb),
\end{align}

where $\chi(\stEmb)$ is the Euler characteristic of the manifold, and $\text{Index}(\vec{X}, \mathbf{P})$ denotes the index of the vector field at point $\mathbf{P}$. The compactness of $\mathcal{S}^{D-1}$ imposes strong geometric constraints: subspaces or submanifolds (such as isoplanes) embedded within $\mathcal{S}^{D-1}$ must intersect unless they are specifically configured to avoid each other (e.g., in certain degenerate cases of orthogonality). To dive deeper, and provide a more fundamental and intuitive analysis, let $\Psi_\theta$ represent the intersection of isoplanar subspaces,

\begin{align}
    \Psi_\theta = \IsoPL{\aB} \cap \IsoPL{\bB}.
\end{align}

First, the compactness of the unit sphere $\mathcal{S}^{D-1}$ implies that any sufficiently dimensional subspaces embedded in the manifold cannot be disjoint. The intersection may be a single point or a higher-dimensional subset, depending on the number of coordinates fixed and the degrees of freedom allowed for the remaining coordinates.

Secondly, even in the case where the isoplanes at $\IsoPL{\aB}$ and $\IsoPL{\bB}$ are orthogonal, the fact that the subspaces are embedded in a compact, orientable manifold forces them to intersect. This intersection result is a consequence of the general principles of intersection theory in compact manifolds, which asserts that two subspaces of sufficient dimension within a compact manifold must intersect unless they are orthogonal in all directions. However, since we are working with constrained isoplanes that do not span the entire manifold, even orthogonal subspaces are forced to intersect due to the lack of space for complete disjointness. Therefore,

\begin{align}
    |\Psi_\theta| > 0.
\end{align}
    
The following part of the proof relies on ensuring that the criteria are met on the manifold subspace such that the PH theorem can applied. 

\paragraph{Construction of Intersecting Subspaces:} First, suppose we have a spherical manifold which is compact. The azimuthal subspaces, $\IsoPL{\aB}$, can be represented by a vector field with a fixed index, and the latitudinal subspaces, $\IsoPL{\bB}$, could be represented as trajectories from a vector field circumnavigating small circles, around the sphere, also of a fixed index. For the D-sphere the fixed index will equal 2. We can see quite trivially that any single trajectory of these two vector fields, representing our subspace $\IsoPL{\aB}$ must intersect with at least one other trajectory in $\IsoPL{\bB}$. 
    
\paragraph{Extension to Higher Dimensions:} Next, we must extend this argument for manifolds of arbitrarily high dimensions, where this is not so trivial to see. First, by the PH theorem, the Index remains constant when elevating to a higher dimension (as is the case with the D-sphere). Next, we argue that since we are working with constrained isoplanes that do not span the entire manifold (i.e. submanifolds or subspaces), even in the case of orthogonal subspaces, the cardinality of the intersecting subspace must be greater than 0 - this is not a direct consequence of the PH theorem rather a conclusion which follows (we present this in the Appendix C.4 Eq. C.8 and Eq. C.9).

\end{proof}
\clearpage

\subsection{Proof of Lemma \ref{lem:orthogonality_submanifold_AB}} \label{prf:orthogonality_submanifold_AB}

\textbf{Orthogonality of Subspaces $\IsoPL{\aB}$ and $\IsoPL{\bB}$:} The two submanifolds $\IsoPL{\aB}$ and $\IsoPL{\bB}$, are orthognal to each other within $\stEmb$.

We consider the spherical manifold $S^{D-1}$, embedded in $\mathbb{R}^D$, where points are parameterized using $D-1$ angular coordinates. These coordinates are composed of latitude-like angles $\nu_1, \dots, \nu_{D-2}$ and a longitude-like angle $\gamma$. The Cartesian coordinates, $\mathbf{x} = [x_1, x_2, \dots, x_D]^\intercal$, of a point on $S^{D-1}$ are expressed as:

\[
\begin{aligned}
x_1 &= \prod_{i=1}^{D-2} \sin(\nu_i) \cos(\gamma), \\
x_2 &= \prod_{i=1}^{D-2} \sin(\nu_i) \sin(\gamma), \\
x_3 &= \prod_{i=1}^{D-3} \sin(\nu_i) \cos(\nu_{D-2}), \\
x_4 &= \prod_{i=1}^{D-4} \sin(\nu_i) \cos(\nu_{D-3}), \\
&\vdots \\
x_{D-1} &= \sin(\nu_1) \cos(\nu_2), \\
x_D &= \cos(\nu_1).
\end{aligned}
\]

We aim to show that the subspaces generated by fixing $\theta_A'$, the set of latitude-like angles, and fixing $\theta_B'$, the longitude-like angle, are orthogonal. To this end, we compute the tangent vectors of the manifold in the directions of these angular coordinates.

First, we compute the partial derivative of each coordinate with respect to $\gamma$. The coordinates $x_1$ and $x_2$ explicitly depend on $\gamma$, while the other coordinates $x_3, \dots, x_D$ do not. Therefore, we have,

\[
\frac{\partial x_1}{\partial \gamma} = \frac{\partial}{\partial \gamma} \left( \prod_{i=1}^{D-2} \sin(\nu_i) \cos(\gamma) \right) = -\prod_{i=1}^{D-2} \sin(\nu_i) \sin(\gamma),
\]

\[
\frac{\partial x_2}{\partial \gamma} = \frac{\partial}{\partial \gamma} \left( \prod_{i=1}^{D-2} \sin(\nu_i) \sin(\gamma) \right) = \prod_{i=1}^{D-2} \sin(\nu_i) \cos(\gamma),
\]

\[
\frac{\partial x_j}{\partial \gamma} = 0, \quad \forall j \geq 3.
\]

Thus, the complete partial derivative with respect to $\gamma$ is,

\[
\frac{\partial}{\partial \gamma} \left( x_1, x_2, \dots, x_D \right) = \left( -\prod_{i=1}^{D-2} \sin(\nu_i) \sin(\gamma), \ \prod_{i=1}^{D-2} \sin(\nu_i) \cos(\gamma), \ 0, \dots, 0 \right).
\]

% This expresses how the Cartesian coordinates change when varying the azimuthal angle $\gamma$ while keeping the latitude-like angles $\nu_1, \dots, \nu_{D-2}$ fixed.

Next, we compute the partial derivative of the coordinates with respect to $\nu_1$. This affects all coordinates $x_1, x_2, \dots, x_D$. Specifically:

\[
\frac{\partial x_1}{\partial \nu_1} = \frac{\partial}{\partial \nu_1} \left( \prod_{i=1}^{D-2} \sin(\nu_i) \cos(\gamma) \right) = \cos(\nu_1) \prod_{i=2}^{D-2} \sin(\nu_i) \cos(\gamma),
\]

\[
\frac{\partial x_2}{\partial \nu_1} = \frac{\partial}{\partial \nu_1} \left( \prod_{i=1}^{D-2} \sin(\nu_i) \sin(\gamma) \right) = \cos(\nu_1) \prod_{i=2}^{D-2} \sin(\nu_i) \sin(\gamma),
\]

\[
\frac{\partial x_3}{\partial \nu_1} = \frac{\partial}{\partial \nu_1} \left( \prod_{i=1}^{D-3} \sin(\nu_i) \cos(\nu_{D-2}) \right) = \cos(\nu_1) \prod_{i=2}^{D-3} \sin(\nu_i) \cos(\nu_{D-2}),
\]

\[
\frac{\partial x_4}{\partial \nu_1} = \cdots = \frac{\partial x_D}{\partial \nu_1} = -\sin(\nu_1).
\]

Thus, the complete partial derivative with respect to $\nu_1$ is:

\[
\frac{\partial}{\partial \nu_1} \left( x_1, x_2, \dots, x_D \right) = \left( \cos(\nu_1) \prod_{i=2}^{D-2} \sin(\nu_i) \cos(\gamma), \ \cos(\nu_1) \prod_{i=2}^{D-2} \sin(\nu_i) \sin(\gamma), \ -\sin(\nu_1), \ 0, \dots, 0 \right).
\]

% This expresses how the Cartesian coordinates change when varying the latitude-like angle $\nu_1$.

\paragraph{Dot Product of Tangent Vectors:} To prove orthogonality of the subspaces spanned by these vectors, we compute the dot product of the tangent vectors $\frac{\partial}{\partial \gamma}$ and $\frac{\partial}{\partial \nu_1}$. The dot product is given by,

\[
\frac{\partial}{\partial \gamma} \cdot \frac{\partial}{\partial \nu_1} = \left( -\prod_{i=1}^{D-2} \sin(\nu_i) \sin(\gamma) \right) \cdot \left( \cos(\nu_1) \prod_{i=2}^{D-2} \sin(\nu_i) \cos(\gamma) \right) + \dots,
\]

which simplifies to zero, as the terms corresponding to the components in $x_1$, $x_2$, and $x_3$ do not align. Consequently, we have,

\[
\frac{\partial}{\partial \gamma} \cdot \frac{\partial}{\partial \nu_1} = 0.
\]

Since the dot product of the tangent vectors is zero, the subspaces spanned by fixing $A$ and fixing $B$ are orthogonal at every point on $S^{D-1}$. This orthogonality arises from the fact that the angular coordinates for latitude and longitude parameterize independent directions in the tangent space of the spherical manifold. Thus, we conclude that the subspaces resulting from fixing $A$ and $B$ are mutually orthogonal.



\subsection{Proof of Lemma \ref{lem:leader_pure_strategy_spherical}} \label{prf:leader_pure_strategy_spherical}

\textbf{Pure Strategy of the Leader:} \textit{Given a spherical manifold, $\stEmb$, and isoplanar subspace, $\IsoPL{\aB}$ and $\IsoPL{\bB}$ for the longitudinal and lattitudinal subspaces respectively, the optimal strategy of the leader is that of a pure strategy, that is, $\polA^*(\aB) \in \{0, 1\}$.}

\begin{proof}
    

Let $\mathcal{S}^{D-1} \subset \mathbb{R}^D$ be the unit sphere embedded in $D$-dimensional Euclidean space. Consider two distinct points $\theta_A'$ and $\theta_B'$ on the manifold, each with spherical coordinates $(\gamma_1^{(A)}, \gamma_2^{(A)}, \dots, \gamma_{D-2}^{(A)}, \nu^{(A)})$ and $(\gamma_1^{(B)}, \gamma_2^{(B)}, \dots, \gamma_{D-2}^{(B)}, \nu^{(B)})$, respectively. We aim to demonstrate that the isoplanes formed by fixing half of the spherical coordinates at $\theta_A'$ and $\theta_B'$ must intersect, and this intersection $\Psi_\theta$ is a singleton. By Lemma \ref{lem:intersect_submanifold_AB} we infer that $\IsoPL{\aB}$ and $\IsoPL{\bB}$ must form a non-empty intersect in $\stEmb$. Follower by Lemma \ref{lem:orthogonality_submanifold_AB}, $\IsoPL{\aB}$ and $\IsoPL{\bB}$ are orthognal to each other in $\stEmb$.


% For the unit sphere $\mathcal{S}^{D-1}$, the Euler characteristic is $\chi(\mathcal{S}^{D-1}) = 2$ when $D = 3$, and it varies depending on the dimensionality of the manifold.

% Though the Poincaré-Hopf theorem primarily concerns the indices of vector fields, it also provides key insight into the intersection properties of submanifolds embedded in compact manifolds. 



% (Latitude-like Angle) - Azimuth


\paragraph{Singleton Intersection due to Orthogonality:} Consider the isoplanes formed by fixing the angular coordinates $\theta_A'$ (latitude-like) and $\theta_B'$ (longitude-like) on the unit sphere $S^{D-1}$. These isoplanes correspond to submanifolds of the sphere, which are defined by holding certain angular coordinates constant while allowing others to vary. In the special case where the isoplanes at $\theta_A'$ and $\theta_B'$ are orthogonal, we argue that the intersection set of these submanifolds is reduced to a single element (singleton). Let $\mathbf{P}$ be the point where the isoplanes associated with fixed $\theta_A'$ and $\theta_B'$ intersect. The tangent space at $\mathbf{P}$, denoted as $T_{\mathbf{P}} S^{D-1}$, consists of vectors tangent to the sphere at $\mathbf{P}$.



% In the special case where the isoplanes at $\theta_A'$ and $\theta_B'$ are orthogonal, the intersection set is reduced to a single point. Orthogonality implies that the tangent vectors to the submanifolds at the point of intersection are perpendicular, meaning that the subspaces meet at a right angle. Nonetheless, the compactness of the unit sphere ensures that the isoplanes must still intersect, albeit at a single point. The result holds irrespective of whether the isoplanes are orthogonal or not, as compactness guarantees that no complete disjointness can occur.



The isoplane formed by fixing $\theta_A'$ corresponds to a submanifold $\IsoPL{\aB}$ whose tangent space at $\mathbf{p}$, denoted $T_{\mathbf{p}} \IsoPL{\aB}$, is spanned by the partial derivatives with respect to the longitude-like angular coordinates $\gamma_i$. Similarly, the isoplane formed by fixing $\theta_B'$ corresponds to a submanifold $\IsoPL{\bB}$, and the tangent space $T_{\mathbf{p}} \IsoPL{\bB}$ is spanned by the partial derivatives with respect to the latitude-like angular coordinates $\nu_j$. Orthogonality between the isoplanes at $\theta_A'$ and $\theta_B'$ implies that the tangent spaces $T_{\mathbf{p}} \IsoPL{\aB}$ and $T_{\mathbf{p}} \IsoPL{\bB}$ are mutually orthogonal. This means that the dot product of any vector from $T_{\mathbf{p}} \IsoPL{\aB}$ with any vector from $T_{\mathbf{p}} \IsoPL{\bB}$ is zero:
\[
\mathbf{v}_A \cdot \mathbf{v}_B = 0, \quad \forall \mathbf{v}_A \in T_{\mathbf{p}} \IsoPL{\aB}, \quad \mathbf{v}_B \in T_{\mathbf{p}} \IsoPL{\bB}.
\]

Geometrically, this implies that the submanifolds $\IsoPL{\aB}$ and $\IsoPL{\bB}$ intersect at a right angle at $\mathbf{P}$. Since the submanifolds are orthogonal, no other points of intersection can occur, and the intersection set is reduced to the single point $\mathbf{P}$. Therfore,

\begin{align}
    |\Psi_\theta| = 1. \label{eq:cardinality_intersec_subspaces}
\end{align}

\paragraph{Minimal Geodesic Distance from $\Psi_\theta$:} Let $\Psi_\gamma = (x_1^{(\text{int})}, x_2^{(\text{int})}, \dots, x_D^{(\text{int})})$ be the unique intersection point of the two isoplaness. Now, we consider the geodesic distance from this intersection point to any other point on the sphere. The geodesic distance between two points $\mathbf{P}_1 = (x_1^{(1)}, x_2^{(1)}, \dots, x_D^{(1)})$ and $\mathbf{P}_2 = (x_1^{(2)}, x_2^{(2)}, \dots, x_D^{(2)})$ on the unit sphere is given by,

\[
\geoDis(\mathbf{P}_1, \mathbf{P}_2) = \arccos(\mathbf{P}_1 \cdot \mathbf{P}_2).
\]

% where $\mathbf{P}_1 \cdot \mathbf{P}_2$ is the Euclidean dot product denoted as,

% \[
% \mathbf{P}_1 \cdot \mathbf{P}_2 = x_1^{(1)} x_1^{(2)} + x_2^{(1)} x_2^{(2)} + \dots + x_D^{(1)} x_D^{(2)}.
% \]

At the intersection point $\Psi_\theta$, the geodesic distance is minimized, thus,
\[
\mathbf{P}_1 = \Psi_\theta \implies \geoDis(\mathbf{P}_1, \Psi_\theta) = 0.
\]

Suppose we move away from $\Psi_\gamma$ along either the longitude isoplanes (by changing $x_1$) or the latitude isoplanes (by changing $x_2, x_3, \dots, x_D$). Any such deviation implies a change in the dot product $\mathbf{P}_1 \cdot \mathbf{P}_2$, which results in an increase in the geodesic distance. Specifically, if we move along the longitude isoplanes, we are changing $x_1$, while the other coordinates remain constant, resulting in a decrease in the dot product. Similarly, if we move along the latitude isoplanes, we are changing $x_2, x_3, \dots, x_D$, again causing a decrease in the dot product. Since the geodesic distance is a monotonically increasing function of the angular separation, any deviation from $\Psi_\gamma$ leads to an increase in the geodesic distance,

\[
\geoDis( \mathbf{P}_2, \mathbf{P}_1) > \geoDis(\mathbf{P}_1, \Psi_\gamma) = 0.
\]

Thus, any deviation from the intersection point of the longitude and latitude isoplaness must result in an increase in the geodesic distance, $\geoDis(\cdot)$. By Lemma \ref{lem:geodesic_and_closeness_phi}, this increase in the geodesic distance will decrease the expected reward $\utlA$. As the cardinality of $\Psi$ is $|\Psi_\gamma| = 1$ from Eq. \eqref{eq:cardinality_intersec_subspaces}, this implies no optimal mixed strategies exist for the leader, and thus, $\polA^*(\aB) \in \{0, 1\}$.

\end{proof}





\subsection{Conversion of Cartesian Uncertainty to Spherical}

\begin{lemma} \label{lem:geodesic_uncertainty_ball}
    Given two points \(\theta_A, \tilde{\theta}_A \in \mathbb{R}^D\), denoting points on the surface of a unit spherical manifold, the uncertainty in Cartesian coordinates expressed as $\|\theta_A- \tilde{\theta}_A\| < \uncerBall{t}$ can be expressed as uncertainty in geodesic distance as $\geoDis(A, \tilde{\theta}_A) < \cos^{-1}\left(1 - \frac{\uncerBall{t}^2}{2}\right)$.
\end{lemma}

\begin{proof}
    Given two points \(\theta_A, \tilde{\theta}_A \in \mathbb{R}^D\), with \(\|A\| = \|\tilde{\theta}_A\| = 1\), denoting points on the surface of a unit sphere, the uncertainty in Cartesian coordinates is expressed as:
    \[
    \|\theta_A- \tilde{\theta}_A\| < \uncerBall{t}
    \]
    where \(\uncerBall{t} \in \mathbb{R}^+\) is the uncertainty bound. We aim to translate this uncertainty into spherical coordinates.
    
    \paragraph{Cartesian Coordinates on the Unit Sphere:} In \( \mathbb{R}^D \), the spherical coordinates of a point \(\theta_A\) on the surface of the unit sphere can be represented as:
    \[
    \theta_A^{(1)} = \cos(\nu_1),
    \]
    \[
    \theta_A^{(2)} = \sin(\nu_1)\cos(\nu_2),
    \]
    \[
    \theta_A^{(3)} = \sin(\nu_1)\sin(\nu_2)\cos(\nu_3),
    \]
    \[
    \vdots
    \]
    \[
    \theta_A^{(D-1)} = \sin(\nu_1) \sin(\nu_2) \ldots \sin(\nu_{D-2}) \cos(\gamma),
    \]
    \[
    \theta_A^{(D)} = \sin(\nu_1) \sin(\nu_2) \ldots \sin(\nu_{D-2}) \sin(\gamma),
    \]
    where \(\nu_1, \nu_2, \ldots, \nu_{D-2}\) represent the latitude angles, and \(\gamma\) represents the longitude angle. Similarly, the point \(\tilde{\theta}_A\) can be written in terms of spherical angles \(\nu'_1, \nu'_2, \ldots, \gamma'\).
    
    \paragraph{Uncertainty in Cartesian Coordinates:}
    
    The uncertainty in Cartesian space is given by:
    \[
    \|\theta_A- \tilde{\theta}_A\|^2 = (\theta_A^{(1)} - \tilde{\theta}_A^{(1)})^2 + (\theta_A^{(2)} - \tilde{\theta}_A^{(2)})^2 + \ldots + (\theta_A^{(D)} - \tilde{\theta}_A^{(D)})^2 < \uncerBall{t}^2.
    \]
    However, it is more efficient to relate this uncertainty directly to spherical angular distance.
    
    \paragraph{Spherical Angular Distance:}
    
    The squared Euclidean distance between two points \(\theta_A\) and \(\tilde{\theta}_A\) on the surface of the unit sphere is related to their angular distance \(\nu\) by the spherical law of cosines:
    \[
    \|\theta_A- \tilde{\theta}_A\|^2 = 2(1 - \cos(\nu)),
    \]
    where \(\nu\) is the angular distance between the two points, and \(\cos(\nu)\) is given by:
    \[
    \cos(\nu) = \cos(\nu_1)\cos(\nu'_1) + \sin(\nu_1)\sin(\nu'_1) \Big( \cos(\nu_2)\cos(\nu'_2) + \sin(\nu_2)\sin(\nu'_2) \cdots \Big).
    \]
    This expression provides the exact angular distance between points \(\theta_A\) and \(\tilde{\theta}_A\) on the unit sphere.
    
    \paragraph{Uncertainty in Spherical Coordinates:}
    
    The inequality \(\|\theta_A- \tilde{\theta}_A\| < \uncerBall{t}\) implies that the angular distance \(\nu\) between the two points satisfies:
    \[
    2(1 - \cos(\nu)) < \uncerBall{t}^2,
    \]
    which simplifies to:
    \[
    \cos(\nu) > 1 - \frac{\uncerBall{t}^2}{2}.
    \]
    Since \(\cos(\nu)\) ranges from 1 (when \(\theta_A = \tilde{\theta}_A\)) to -1 (for antipodal points), the angular distance \(\nu\) is bounded by:
    \[
    \nu < \cos^{-1}\left(1 - \frac{\uncerBall{t}^2}{2}\right).
    \]
    This inequality describes the exact spherical uncertainty region. Thus, the uncertainty \(\|\theta_A- \tilde{\theta}_A\| < \uncerBall{t}\) in Cartesian space corresponds to an angular uncertainty \(\nu < \cos^{-1}\left(1 - \frac{\uncerBall{t}^2}{2}\right)\) on the unit sphere.
\end{proof}
 
\subsection{Distance Preserving Orthogonal Projection:}

\begin{lemma} \label{lem:distance_preserving_ortho_proj}
    Consider a unit sphere \( S^{D-1} \subset \mathbb{R}^D \). Given a point \(\theta_A \in S^{D-1}\) and a geodesic ball \( B_J \subset S^{D-1} \) centered at \(\theta_A\), we are interested in the behaviour of this ball under orthogonal projection onto a subspace of \(\mathbb{R}^D\). Specifically, we aim to rigorously show that the diameter of the orthogonally projected ball does not exceed the diameter of the original geodesic ball. 
\end{lemma}

\begin{proof}
    
    \textbf{Geodesic Uncertainty Balls:} Let \( \theta_A, \tilde{\theta}_A \in \mathbb{R}^D \) be two points on the unit sphere, i.e., \( \| \theta_A\| = \| \tilde{\theta_A}\| = 1 \), and let the geodesic distance between \(\theta_A\) and \(\tilde{\theta}_A\) be denoted by \(\gamma(\theta_A, \tilde{\theta}_A)\). The geodesic distance between any two points on \(S^{D-1}\) is given by,
    \[
    \gamma(\theta_A, \tilde{\theta}_A) = \arccos(\theta_A \cdot \tilde{\theta}_A),
    \]
    where \( \theta_A \cdot \tilde{\theta}_A \) is the Euclidean dot product between \(\theta_A\) and \(\tilde{\theta}_A\). A geodesic ball \( B_J(\theta_A) \) centered at \(\theta_A\) with radius \(J\) is defined as the set of points on the unit sphere such that their geodesic distance from \(\theta_A\) is less than or equal to \(J\):
    \[
    B_J(\theta_A) = \{ \theta_A' \in S^{D-1} \mid \gamma(\theta_A, \tilde{\theta}_A) \leq J \}.
    \]
    We are particularly interested in the case where \( J \leq \arccos\left(1 - \frac{\uncerBall{t}^2}{2}\right) \), where \(\uncerBall{t}\) is a positive value corresponding to the uncertainty radius in the Euclidean distance.
    
    \textbf{Orthogonal Projection and Geodesic Distance:} Given a subspace \( V \subset \mathbb{R}^D \), let \( P_V: \mathbb{R}^D \to V \) denote the orthogonal projection onto \(V\). For any points \( \theta_A, \tilde{\theta}_A \in \mathbb{R}^D \), the Euclidean distance between their projections is bounded by:
    \[
    \| P_V(\theta_A) - P_V(\tilde{\theta}_A) \| \leq \| \theta_A - \tilde{\theta}_A \|.
    \]
    Since the geodesic distance on the unit sphere is a measure of arc length between points, it follows that the geodesic distance between two points is non-increasing under orthogonal projection. We aim to show that the diameter of the projected geodesic ball onto the subspace \(V\) does not exceed the diameter of the original ball.
    
    \textbf{Diameter of a Geodesic Ball:} The diameter of a set \( S \subset S^{D-1} \) is defined as the greatest geodesic distance between any two points in \(S\):
    \[
    \text{diam}(S) = \sup_{x, y \in S} \gamma(x, y).
    \]
    For a geodesic ball \( B_J(\theta_A) \), the maximum geodesic distance occurs between two antipodal points on the boundary of the ball. Therefore, the diameter of the geodesic ball is:
    \[
    \text{diam}(B_J(\theta_A)) = 2J.
    \]
    In particular, for \( J = \arccos\left(1 - \frac{\uncerBall{t}^2}{2}\right) \), we have:
    \[
    \text{diam}(B_J(\theta_A)) = 2 \arccos\left(1 - \frac{\uncerBall{t}^2}{2}\right).
    \]
\end{proof}


\subsection{Diameter Preserving Orthogonal Projection}

We now formalize the behaviour of the geodesic ball under orthogonal projection.

\begin{lemma} \label{lem:diameter_preserving_ortho_proj}
Let \( B_J(\theta_A) \) be a geodesic ball of radius \(J \leq \arccos\left(1 - \frac{\uncerBall{t}^2}{2}\right) \) on the unit sphere \( S^{D-1} \subset \mathbb{R}^D \). Let \( V \subset \mathbb{R}^D \) be a subspace, and let \( P_V: \mathbb{R}^D \to V \) be the orthogonal projection onto \(V\). Then, the diameter of the orthogonally projected ball \( P_V(B_J(\theta_A)) \) satisfies:
\[
\text{diam}(P_V(B_J(\theta_A))) \leq \text{diam}(B_J(\theta_A)) = 2J.
\]
\end{lemma}

\begin{proof}
    Consider two points \( \theta_A, \tilde{\theta}_A \in B_J(\theta_A) \). By the definition of a geodesic ball, we know that:
    \[
    \gamma(\theta_A, \tilde{\theta}_A) \leq 2J.
    \]
    Next, project \( \theta_A \) and \( \tilde{\theta}_A \) orthogonally onto the subspace \(V\), yielding the points \( P_V(\theta_A) \) and \( P_V(\tilde{\theta}_A) \). Since orthogonal projection reduces or preserves Euclidean distances, we have:
    \[
    \| P_V(\theta_A) - P_V(\tilde{\theta}_A) \| \leq \| \theta_A - \tilde{\theta}_A \|.
    \]
    Moreover, since the geodesic distance between points on the sphere is a function of their Euclidean distance, it follows that the geodesic distance between the projected points \( P_V(\theta_A) \) and \( P_V(\tilde{\theta}_A) \) is also bounded by:
    \[
    \gamma(P_V(\theta_A), P_V(\tilde{\theta}_A)) \leq \gamma(\theta_A, \tilde{\theta}_A).
    \]
    Thus, for all pairs \( \theta_A, \tilde{\theta}_A \in B_J(\theta_A) \), we have:
    \[
    \gamma(P_V(\theta_A), P_V(\tilde{\theta}_A)) \leq 2J.
    \]
    This shows that the diameter of the projected geodesic ball \( P_V(B_J(\theta_A)) \) is at most \( 2J \), i.e.,
    \[
    \text{diam}(P_V(B_J(\theta_A))) \leq \text{diam}(B_J(\theta_A)) = 2J.
    \]
\end{proof}


\subsection{Proof of Theorem \ref{thm:sphere_man_iso_reg}} \label{prf:sphere_man_iso_reg}

\textbf{Isoplane Stackelberg Regret:} For D-dimensional spherical manifolds embedded in $\mathbb{R}^D$ space, where $\phi(\aB, \cdot)$ generates an isoplanes $\IsoPL{\aB}$, and the linear relationship to the reward function in Eq. \eqref{eq:A_reward_inner_prod} and Eq. \eqref{eq:B_reward_inner_prod} holds, the simple regret, defined in Eq. \eqref{eq:simple_regret_defn}, of any learning algorithm with uncertainty parameter uncertainty  $\uncerBall{t}$, refer to in Eq. \eqref{eq:param_uncertainty}, is bounded by $\mathcal{O}(2\arccos(1-\uncerBall{t}^2/2))$.

\begin{proof}
    The proof of Theorem \ref{thm:sphere_man_iso_reg} hinges on the aforementioned arguments in Lemma \ref{lem:geodesic_uncertainty_ball}, Lemma \ref{lem:distance_preserving_ortho_proj}, and Lemma \ref{lem:diameter_preserving_ortho_proj} sequentially, but in the context of parameter estimation.

    First, Lemma \ref{lem:geodesic_uncertainty_ball} argues that one can transform a confidence bound $|\theta_A - \hat{\theta}_A| \leq \uncerBall{t}$ into a confidence bound on geodesic distance $\geoDis(\theta_A, \hat{\theta}_A) \leq \cos^{-1}\left(1 - \frac{\uncerBall{t}^2}{2}\right)$. Let us denote this as the geodesic confidence ball $\Ball_{\geoDis}(\theta^*, \uncerBall{t})$. Nevertheless, due to the separation of subspaces $\IsoPL{\aB}$ and $\IsoPL{\bB}$, we must find the projection of $\Ball_{\geoDis}(\theta^*, \uncerBall{t})$ onto $\IsoPL{\bB}$ such that we can obtain a diameter measure on the new intersecting subspace $\IsoPL{\aB} \cap \IsoPL{\bB}$. Next, Lemma \ref{lem:distance_preserving_ortho_proj} argues that geodesic distances will either be preserved or reduced when making a projection to an orthogonal subspace $\IsoPL{\bB}$, the orthogonality of this subspace was previously established in Lemma \ref{lem:orthogonality_submanifold_AB}. Thereafter, Lemma \ref{lem:diameter_preserving_ortho_proj} specifies that the maximum diameter of this new confidence ball $\Ball'_{\geoDis}(\theta^*, \uncerBall{t})$ that is projected onto $\IsoPL{\bB}$ is confined to a maximum diameter of $2\cos^{-1}\left(1 - \frac{\uncerBall{t}^2}{2}\right)$.

    Thus, this constitutes the best and worst possible outcomes due to misspecification in accordance with the formulation in Eq. \eqref{eq:min_H} and Eq. \eqref{eq:max_H}, denoted as $\bar{\mathcal{H}}(\theta_A^*,t) - \underline{\mathcal{H}}(\theta_A^*,t)$, also expressed in Eq. \eqref{eq:simple_regret_defn}, which upper bounds the simple regret.
    
\end{proof}


\clearpage

\section{Neural Flow Architectural Specifications} \label{sec:nn_arch_details}

We present the mathematical foundations of the normalizing flow architecture used to model spherical mappings. Our method combines a spherical coordinate transformation with normalizing flows to provide an invertible mapping between input features and a latent space, with applications to tasks requiring smooth transformations on a manifold. 

\textbf{Mapping to a Spherical Manifold:} The transformation from Cartesian coordinates to spherical coordinates is used to map input features onto an $D$-dimensional spherical manifold. We define two heads in the neural network input, the head from A specifically controls the azimuthal spherical coordinate and additional coordinates, and  the head from B specifically controls other coordinates. The output sizes of the neural network that transforms the inputs are $\floor{\frac{D-1}{2}}+1$ for A and $\floor{\frac{D-1}{2}}$ for B. The conversion from spherical coordinates to Cartesian coordinates, $\mathbf{x} \in \mathbb{R}^D$, is defined in Appendix \ref{sec:sphere_cart_conv_algo}.

% \begin{align}
%     r_1 &= \cos(\phi_1), \\
%     r_i &= r_{i-1} \cdot \cos(\phi_i), \quad \text{for } i = 2, \dots, n-2, \\
%     x_{n-1} &= r_{n-2} \cdot \cos(\theta_{n-2}), \\
%     x_n &= r_{n-2} \cdot \sin(\theta_{n-2}).
% \end{align}

% These equations define the Cartesian coordinates on the $n$-dimensional sphere, where $\mathbf{x} = [x_1, x_2, \dots, x_n]^\intercal$. The latitude angles $\boldsymbol{\phi}$ control the \textit{vertical} position on the sphere, while the longitude angles $\boldsymbol{\theta}$ control the \textit{horizontal} position.


\textbf{Affine Coupling Layers:} A normalizing flow consists of a series of invertible transformations, including affine coupling layers, which divide the input into two parts and transform one part conditioned on the other. Let the input be $\mathbf{x} = [\mathbf{x}_1, \mathbf{x}_2]$, where $\mathbf{x}_1$ and $\mathbf{x}_2$ are disjoint subsets of the input. The affine coupling transformation is defined as,

\begin{align}
    \mathbf{y}_1 &= \mathbf{x}_1, \\
    \mathbf{y}_2 &= \mathbf{x}_2 \odot \exp(s(\mathbf{x}_1)) + t(\mathbf{x}_1),
\end{align}

where $\odot$ denotes element-wise multiplication, and $s(\mathbf{x}_1)$ and $t(\mathbf{x}_1)$ are the scaling and translation functions, respectively, parameterized by a neural network. The inverse of this transformation is straightforward:

\begin{align}
    \mathbf{x}_1 &= \mathbf{y}_1, \\
    \mathbf{x}_2 &= \left( \mathbf{y}_2 - t(\mathbf{y}_1) \right) \odot \exp(-s(\mathbf{y}_1)).
\end{align}

This transformation is invertible by design, making it suitable for use in flow-based models.

\textbf{Log Determinant of the Jacobian:} The log-likelihood calculation requires computing the log determinant of the Jacobian matrix for the transformation. For the affine coupling layer, the Jacobian matrix is triangular, and the log determinant is simply the sum of the scaling terms:

\begin{equation}
    \log \left| \det \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \right| = \sum_{i} s(\mathbf{x}_1).
\end{equation}

This term contributes to the overall log probability during training.

\textbf{Normalizing Flow Forward Transform:} A normalizing flow is constructed by stacking several affine coupling layers and random permutation layers. Let $\mathbf{x} \in \mathbb{R}^d$ be the input, and $\mathbf{z} \in \mathbb{R}^d$ be the transformed latent variable after $L$ layers of flow. Each layer applies a transformation $f_l$ such that:

\begin{equation}
    \mathbf{z}^{(l+1)} = f_l(\mathbf{z}^{(l)}),
\end{equation}

where $f_l$ represents either an affine coupling transformation or a random permutation. After $L$ layers, the final output is denoted as $\mathbf{z} = \mathbf{z}^{(L)}$. The forward transformation can thus be written as:

\begin{equation}
    \mathbf{z}, \log \det J = f_{\text{flow}}(\mathbf{x}),
\end{equation}

where $\log \det J$ is the log determinant of the Jacobian matrix for the entire flow. 

To compute the log-likelihood of the input $\mathbf{x}$, we map it to the latent space $\mathbf{z}$ under the flow transformation. The probability of $\mathbf{x}$ is computed as:

\begin{equation}
    p(\mathbf{x}) = p(\mathbf{z}) \left| \det \frac{\partial \mathbf{z}}{\partial \mathbf{x}} \right|,
\end{equation}

where $p(\mathbf{z})$ is the probability of $\mathbf{z}$ under the base distribution (typically a standard normal distribution):

\begin{equation}
    p(\mathbf{z}) = \mathcal{N}(\mathbf{z}; 0, I).
\end{equation}

The log probability is then given by:

\begin{equation}
    \log p(\mathbf{x}) = \log p(\mathbf{z}) + \log \left| \det \frac{\partial \mathbf{z}}{\partial \mathbf{x}} \right|.
\end{equation}

\textbf{Inverse Transform:} The invertibility of the flow allows for both density estimation and sampling. To sample from the model, we draw samples $\mathbf{z} \sim \mathcal{N}(0, I)$ from the base distribution and apply the inverse transformation:

\begin{equation}
    \mathbf{x} = f_{\text{flow}}^{-1}(\mathbf{z}).
\end{equation}

Each affine coupling layer and random permutation is applied in reverse order to recover the original inputs.

\textbf{Random Permutation Layer:} The random permutation layer permutes the features of the input vector to ensure that different parts of the input are transformed at each layer. Let $\mathbf{x} \in \mathbb{R}^d$ be the input, and let $P$ be a permutation matrix. The permutation transformation is defined as:

\begin{equation}
    \mathbf{x}' = P \mathbf{x}.
\end{equation}

Since permutation matrices are orthogonal, the Jacobian determinant of this transformation is always $1$, and it does not contribute to the log determinant calculation.


\begin{table}[h!]
\centering
\begin{tabular}{@{}>{\raggedright\arraybackslash}p{4.3cm} 
                >{\raggedright\arraybackslash}p{6cm} 
                >{\raggedright\arraybackslash}p{3cm}@{}}
\toprule
\textbf{Layer}         & \textbf{Description}                                     & \textbf{Output Size}          \\ 
\midrule
\texttt{Input Head A}         & Input head A                                           & $N_B \times |\setA|$         \\ 
\midrule
\texttt{Input Head B}         & Input head B                                           & $N_B \times |\setB|$         \\ 
\midrule
\texttt{Input Features}         & Input features                                           & $N_B \times D$         \\ 
\midrule
\texttt{Affine Coupling Layer} & No. of Affine Coupling layers                         & $N_B \times 64$         \\ 
\midrule
\texttt{fc\_A1 Hidden Dim.}    & Number of hidden dimensions in first fully connected layer A.       & $B \times 1024$ \\ 
\midrule
\texttt{fc\_B1 Hidden Dim.}    & Number of hidden dimensions in first fully connected layer B.       & $B \times 1024$ \\ 
\midrule
\texttt{Hidden Dim.}    & No.of hidden layers for A and B.       & $N_B \times 16$ \\ 
\midrule
\texttt{fc\_A1 Final Layer Dim.}    & Number of hidden dimensions in final layer A      & $N_B \times \Big( \floor{\frac{D-1}{2}} + 1 \Big)$\\ 
\midrule
\texttt{fc\_B1 Final Layer Dim.}    & Number of hidden dimensions in final layer B      & $N_B \times \Big( \floor{\frac{D-1}{2}} \Big)$\\ 
\midrule
\texttt{Output}        & Output features after flow transformation                & $N_B \times D$         \\ 
\bottomrule
\end{tabular}
\caption{Normalizing Flows Neural Architecture Specifications.}
\end{table}

\textbf{Overview:} In summary, the normalizing flow architecture combines spherical mapping, affine coupling transformations, and random permutations to form a powerful framework for invertible transformations. The model leverages the flexibility of normalizing flows to map inputs to a spherical manifold, enabling efficient density estimation and sampling from a base Gaussian distribution.

\begin{table}[H]
    \centering
    \begin{tabular}{||c c||} 
     \hline
     Parameter & Value \\ [0.5ex] 
     \hline\hline
     $N_B$ Batch Size & 2048 \\
     $\alpha_N $ (Negative Log Liklihood Loss Coef.)  & 0.5 \\
     $\alpha_R $ (Repulsion Loss Coef.)  & 1.0 \\
     $\alpha_P $ (Perturb. Loss Coef.)  & 0.5 \\
     $\alpha_L $ (Lipschitz Loss Coef.)  & 1.5 \\
     No. Epochs & 20,000 \\
     $\alpha_{LR}$ (Learning Rate) & 0.05 \\
     $C_L$ (Lipschitz Constant) & 0.5 \\
     \hline
    \end{tabular}
    \caption{Hyper parameters used for normalizing neural flow network training. }
    \label{table:hyperparam}
\end{table}


\clearpage

\section{Visualizations}

\subsection{Computational Results of Isoplane Behaviour} \label{sec:isoplanes_computational_viz}

\begin{figure}[!htb]
\minipage{0.47\textwidth}
  \includegraphics[width=60mm]{figures/viz_long_isolines_v2.png}
  \caption*{\textbf{Longitudinal Isolines: } Visualization of longitudinal isolines generated by the normalizing neural flow network.}%\label{fig:market3}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=60mm]{figures/viz_lat_isolines_v2.png}
  \caption*{\textbf{Latitudinal Isoplanes:} Visualization of lattitudinal isolines generated by the normalizing neural flow network.}%\label{fig:market2}
\endminipage\hfill
\caption{Formation of isolines (or isoplanes in higher dimensions) forming on the spherical manifold $\stEmb$ as we fix $\aB$ and vary $\bB$ (longitudinal), and fix $\bB$ and vary $\aB$ (lattitudinal). } \label{fig:isoplanes_line_sphere_regret}
\end{figure}


\subsection{Visualization of Ambient Space to Stackelberg Embedding mapping} \label{sec:ambient_to_stemb_viz}

\begin{figure}[!htb]
\minipage{0.47\textwidth}
  \includegraphics[width=60mm]{figures/ambient_action_space_v1.png}
  \caption*{\textbf{Ambient Space: } Visualization uniformly distributed data in ambient space (native joint action space).}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=60mm]{figures/good_embedding_v1.png}
  \caption*{\textbf{Stackelberg Embedding $\stEmb$:} Visualization of data mapped from the ambient joint action space to a learned Stackelberg embedding $\stEmb$.} %\label{fig:market2}
\endminipage\hfill
\caption{For purely visualization purposes, we present the mapping from the ambient data 3D space to a 2-Sphere manifold $\stEmb$. In addition to being sufficiently biijective, the Lipschitz and spreading properties from Table \ref{tab:stack-emb-dynamics} are maximized via normalizing neural flows.} \label{fig:stackelberg_embedding_mapping_2fig_viz}
\end{figure}

\subsection{Poor Examples of Stackelberg Embeddings} \label{sec:bad_stackelberg_embeddings_viz}

\begin{figure}[!htb]
\minipage{0.47\textwidth}
  \includegraphics[width=60mm]{figures/bad_embedding_v2.png}
  \caption*{\textbf{Arbitrary Projection: } The ambient data passes through the normalizing flow network which contains untrained model weights resulting in arbitrary conversion of Cartesian to spherical coordinates.}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=60mm]{figures/bad_embedding_v3.png}
  \caption*{\textbf{Affine Map:} The ambient data is subject to an arbitrary affine map followed by a projection to $\stEmb$.} %\label{fig:market2}
\endminipage\hfill
\caption{We present two examples of when improper and/or simplistic Euclidean transformations are not sufficient to fulfill the requirements outlined in Table \ref{tab:stack-emb-dynamics}. The transformed data lies in arbitrarily clustered regions, and/or are insufficiently spread across the 2-sphere despite the ambient data being uniformly distributed. Moreover, sufficient bijection may not always be possible.} \label{fig:bad_stackelberg_embedding_2fig_viz}
\end{figure}

\clearpage


\section{Algorithms}

\subsection{Mapping between Spherical and Cartesian Coordinates} \label{sec:sphere_cart_conv_algo}

\begin{algorithm}[h!]
\caption{Spherical to Cartesian Conversion in $n$-Dimensions}\label{alg:spherical_to_cartesian}
\begin{algorithmic}[1]
\Function{spherical\_to\_cartesian}{$r, \nu$}
    \State \textbf{Input:} $r$ (radius), $\nu$ (Spherical coordinates $D - 1$ dimensions.)
    \State \textbf{Output:} Cartesian coordinates $p = [x_1, x_2, \dots, x_D]$
    \State $x_1 \gets r \cdot \cos(\nu_1)$
    \For{$i = 2$ to $D-1$}
        \State $x_i \gets r \cdot \sin(\nu_1) \cdot \sin(\nu_2) \dots \cdot \sin(\nu_{i-1}) \cdot \cos(\nu_i)$
    \EndFor
    \State $x_n \gets r \cdot \sin(\nu_1) \cdot \dots \cdot \sin(\nu_{D-1})$
    \State \Return $[x_1, x_2, \dots, x_D]$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
\caption{Cartesian to Spherical Conversion in $n$-Dimensions}\label{alg:cartesian_to_spherical}
\begin{algorithmic}[1]
\Function{cartesian\_to\_spherical}{$p$}
    \State \textbf{Input:} Cartesian coordinates $p = [x_1, x_2, \dots, x_D]$
    \State \textbf{Output:} $r$ (radius), $\nu = [\nu_1, \nu_2, \dots, \nu_{D-1}]$ (Spherical coordinates $D - 1$ dimensions.)
    \State $r \gets \sqrt{x_1^2 + x_2^2 + \dots + x_D^2}$ \Comment{Compute the radius}
    \State $\nu_1 \gets \arccos\left( \frac{x_1}{r} \right)$ \Comment{First spherical angle}
    \For{$i = 2$ to $n-1$}
        \State $\nu_i \gets \arctan2\left( \sqrt{x_1^2 + x_2^2 + \dots + x_{i}^2}, x_{i+1} \right)$ \Comment{Spherical angles for $i = 2$ to $D-1$}
    \EndFor
    \State \Return $r$, $\nu = [\nu_1, \nu_2, \dots, \nu_{D-1}]$
\EndFunction
\end{algorithmic}
\end{algorithm}

\clearpage

\section{Experimental Results}


\subsection{$\mathbbm{R}^1$ Stackelberg Game} \label{sec:r1_game_details}


\textbf{Problem Setup:} We consider a Stackelberg game with a leader \(\theta_A\) and a follower \(B\), both operating in continuous action spaces \(a, b \in \mathbb{R}^1\). The leader chooses an action \(\theta_A\), and the follower responds by choosing an action \(b\) based on the leader's decision. The reward functions for both players are linear in structure but include nonlinear components to model real-world constraints and interactions.

\textbf{Leader’s Reward Function:} The leader’s reward function \( \mu_A(a, b) \) is defined as follows:

\begin{align}
\mu_A(a, b) = \theta_1 a + \theta_2 \log(1 + b^2) - \frac{\theta_3}{2} a^2 + \epsilon, \quad \epsilon \in \mathcal{N}(0, \sigma) 
\end{align}

where,
\begin{itemize}
    \item \( \theta_1, \theta_2 > 0 \) are weight parameters that control the trade-off between the leader’s direct action \(\theta_A\) and the follower’s response \(b\).
    \item \( \log(1 + b^2) \) introduces nonlinearity with respect to the follower’s action \(b\).
    \item \( -\frac{\theta_3}{2} a^2 \) is a quadratic penalty on large leader actions to avoid extreme behaviour by the leader.
\end{itemize}

\textbf{Follower’s Reward Function:} The follower’s reward function \( \mu_B(\aB, \bB) \) is given by:

\begin{align}
\mu_B(\aB, \bB) = \alpha_1 (-b^2) + \alpha_2 a b + \epsilon, \quad \epsilon \in \mathcal{N}(0, \sigma) 
\end{align}

where,
\begin{itemize}
    \item \( \alpha_1, \alpha_2 > 0 \) are parameters that determine the influence of the follower’s own action \(b\) and the leader’s action \(\theta_A\) on the follower’s reward.
    \item \( -b^2 \) represents a concave cost function for the follower, preferring smaller values of \(b\).
    \item \( ab \) introduces an interaction term between the leader’s action and the follower’s action.
\end{itemize}

\textbf{Follower's Best Response:} The follower maximizes their reward function \( \mu_B(\aB, \bB) \) by choosing \(b\) given \(\theta_A\). To determine the follower's best response \( \bR{a} \), we compute the first-order condition with respect to \(b\):

\begin{align}
\frac{\partial \expeC[\mu_B(\aB, \bB)]}{\partial b} = -2 \alpha_1 b + \alpha_2 a = 0
\end{align}

Solving for \(b\), the follower's best response is:

\begin{align}
\bR{a} = \frac{\alpha_2 a}{2 \alpha_1}
\end{align}

\textbf{Leader’s Optimization Problem:} Given that the follower’s best response is \( \bR{a} = \frac{\alpha_2 a}{2 \alpha_1} \), the leader maximizes their reward function \( \mu_A(a, \bR{a}) \) as,

\begin{align}
\expeC[\mu_A(a, \bR{a})] = \theta_1 a + \theta_2 \log\left(1 + \left(\frac{\alpha_2 a}{2 \alpha_1}\right)^2\right) - \frac{\theta_3}{2} a^2.
\end{align}

This results in the following optimization problem for the leader,

\begin{align}
\max_a \, \left( \theta_1 a + \theta_2 \log\left(1 + \frac{\alpha_2^2 a^2}{4 \alpha_1^2}\right) - \frac{\theta_3}{2} a^2 \right).
\end{align}

\textbf{Non-Trivial Solution for the Leader:} To solve for the leader’s optimal action \(a^*\), we take the derivative of the leader's reward function with respect to \(\theta_A\) and set it equal to zero,

\begin{align}
\frac{d}{da} \left( \theta_1 a + \theta_2 \log\left(1 + \frac{\alpha_2^2 a^2}{4 \alpha_1^2}\right) - \frac{\theta_3}{2} a^2 \right) &= 0 \\
\theta_1 - \theta_3 a + \theta_2 \cdot \frac{2 \cdot \left(\frac{\alpha_2 a}{2 \alpha_1}\right) \cdot \left(\frac{\alpha_2}{2 \alpha_1}\right)}{1 + \frac{\alpha_2^2 a^2}{4 \alpha_1^2}} &= 0
\end{align}

Which simplifies to,

\begin{align}
\theta_1 - \theta_3 a + \frac{\theta_2 \cdot \frac{\alpha_2^2 a}{\alpha_1^2}}{1 + \frac{\alpha_2^2 a^2}{4 \alpha_1^2}} = 0.
\end{align}

This equation has no simple closed-form solution and must be solved numerically. The interplay between the nonlinear logarithmic term and the quadratic penalty introduces complexity into the leader's optimization, making the optimal value of \(a^*\) non-trivial.


\clearpage


\subsubsection{$\mathbbm{R}^1$ Stackelberg Game} \label{sec:r2_game_results}

\begin{figure}[H]
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/s1_rew_ex1_v3.png}
  % \caption*{Parameters: $\kappa=1.0, \theta_0 = 73, \theta_1 = 7, \sigma=3.2$.}% \label{fig:market3_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/s1_reg_ex1_v3.png}
  % \caption*{Parameters: $\kappa=1.0, \theta_0 = 73, \theta_1 = 7, \sigma=3.2$.}% \label{fig:market3_loss}
\endminipage\hfill
\caption*{Parameters: $\theta_1 = 4.0, \theta_2 = 1.0, \theta_3 = 0.9, \alpha_1 = 1.0, \alpha_2 = 2.0, \sigma=6.0$.}
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/s1_rew_ex2_v2.png}
  % \caption*{Empirical leader regret. $\kappa=4, \theta_0 = 10, \theta_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  % \includegraphics[width=\linewidth]{figures/config6_b_reg_v2.png}
  \includegraphics[width=\linewidth]{figures/s1_reg_ex2_v2.png}
  % \caption*{Empirical contextual regret. $\kappa=4, \theta_0 = 10, \theta_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\caption*{Parameters: $\theta_1 = 4.0, \theta_2 = 2.0, \theta_3 = 0.9, \alpha_1 = 4.0, \alpha_2 = 2.0, \sigma=6.0$.}
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/s1_rew_ex3_v1.png}
  %\caption*{Empirical leader regret. $\kappa=2, \theta_0 = 10, \theta_1 = 3$.}%\label{fig:market4_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/s1_reg_ex3_v1.png}
  %\caption*{Empirical contextual regret. $\kappa=2, \theta_0 = 10, \theta_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\caption*{Parameters: $\theta_1 = 3.0, \theta_2 = 3.0, \theta_3 = 0.9, \alpha_1 = 1.0, \alpha_2 = 2.0, \sigma=4.0$.}
\caption{Mean values are calculated over 1,000 trials, with shaded regions representing confidence intervals, all of which fall within the first quartile. $\stEmb$ dimension is set to $D = 10$.} \label{fig:r1_game_summary_plots}
\end{figure}

\clearpage

\subsection{The Newsvendor Pricing Game Specifcations (NPG)} \label{sec:npg-appendix}

We model the two learning agents in a \textit{Newsvendor pricing game}, involving a supplier $A$ and a retailer $B$. The leader, a supplier, is learning to dynamically price the product for the follower, a retailer, aiming to maximize her reward. To achieve this, the follower adheres to classical Newsvendor theory, which involves finding the optimal order quantity given a known demand distribution before the realization of the demand.


\textbf{Rules of the Newsvendor Pricing Game:} We explicitly denote $a \equiv \mathbf{a} \in \mathbbm{R}^1$, and $\mathbf{b} \equiv [b, p]^\intercal \in \mathbbm{R}^2$. Where $a$ denotes wholesale price from the supplier firm, $p$ and $b$ denote the retail price and order amount of the retail firm.


\begin{enumerate}
    \setlength\itemsep{-0.1em}
    % \item $c \sim \mathcal{C}^t$ is determined by nature. $\kappa$ is given as the fixed contract cost for $N$ rounds.
    \item The supplier selects wholesale price $a$, and provides it to the retailer. \label{item:supplier-contract-price}
    % \item Production cost $c$ is revealed to the supplier.
    \item Given wholesale cost $a$, the retailer reacts with his best response $[b, p]^\intercal$, consisting of retail price $p$, and order amount $b$.
    % \item If the retailer agrees to the contract, he incurs a fixed cost $\kappa$. 
    \item As the retailer determines the optimal order amount $b$, he pays $\mathcal{G}_A(a, b) = ab$ to the supplier.
    \item At time $t$, nature draws demand $d^t \sim d_\rho(p)$, and it is revealed to the retailer.
    \item The retailer makes a profit of $\mathcal{G}_B(a, b) = p \ \min \{ d^t, b \} - ab$. \label{item:retail-profit}
    \item Steps \ref{item:supplier-contract-price} to \ref{item:retail-profit} are repeated for $t \in 1 ... T$ iterations.
    % \item The retailer will procure $b_a^*$ items from the supplier over $N$ rounds, with optimal order amount $b_a^*$ determined by the retailer
\end{enumerate}

\begin{figure}[ht!]
    \centering
    \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        
    
        \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-0.6,xscale=0.6]
        %uncomment if require: \path (0,398); %set diagram left start at 0, and has height of 398
        
        %Rounded Same Side Corner Rect [id:dp6752276683923639] 
        \draw   (60,214) .. controls (60,200.19) and (71.19,189) .. (85,189) -- (95,189) .. controls (108.81,189) and (120,200.19) .. (120,214) -- (120,241) .. controls (120,241) and (120,241) .. (120,241) -- (60,241) .. controls (60,241) and (60,241) .. (60,241) -- cycle ;
        %Shape: Circle [id:dp9390651436589437] 
        \draw   (65,163) .. controls (65,149.19) and (76.19,138) .. (90,138) .. controls (103.81,138) and (115,149.19) .. (115,163) .. controls (115,176.81) and (103.81,188) .. (90,188) .. controls (76.19,188) and (65,176.81) .. (65,163) -- cycle ;
        %Straight Lines [id:da6819099811040759] 
        \draw    (141.2,190.6) -- (228.2,190.6) ;
        \draw [shift={(230.2,190.6)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        %Rounded Same Side Corner Rect [id:dp24736576379459807] 
        \draw   (261,215) .. controls (261,201.19) and (272.19,190) .. (286,190) -- (296,190) .. controls (309.81,190) and (321,201.19) .. (321,215) -- (321,242) .. controls (321,242) and (321,242) .. (321,242) -- (261,242) .. controls (261,242) and (261,242) .. (261,242) -- cycle ;
        %Shape: Circle [id:dp2270801483457463] 
        \draw   (266,164) .. controls (266,150.19) and (277.19,139) .. (291,139) .. controls (304.81,139) and (316,150.19) .. (316,164) .. controls (316,177.81) and (304.81,189) .. (291,189) .. controls (277.19,189) and (266,177.81) .. (266,164) -- cycle ;
        %Straight Lines [id:da2837543171388224] 
        \draw    (231.2,210.6) -- (212.4,210.6) -- (143.2,210.6) ;
        \draw [shift={(141.2,210.6)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        %Straight Lines [id:da9699583088744679] 
        \draw    (342.2,190.6) -- (429.2,190.6) ;
        \draw [shift={(431.2,190.6)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        %Rounded Same Side Corner Rect [id:dp7404363270905503] 
        \draw   (457,179.79) .. controls (457,172.38) and (463.01,166.37) .. (470.42,166.37) -- (475.78,166.37) .. controls (483.19,166.37) and (489.2,172.38) .. (489.2,179.79) -- (489.2,194.28) .. controls (489.2,194.28) and (489.2,194.28) .. (489.2,194.28) -- (457,194.28) .. controls (457,194.28) and (457,194.28) .. (457,194.28) -- cycle ;
        %Shape: Ellipse [id:dp7466818961961323] 
        \draw   (459.68,152.42) .. controls (459.68,145.01) and (465.69,139) .. (473.1,139) .. controls (480.51,139) and (486.52,145.01) .. (486.52,152.42) .. controls (486.52,159.83) and (480.51,165.83) .. (473.1,165.83) .. controls (465.69,165.83) and (459.68,159.83) .. (459.68,152.42) -- cycle ;
        %Rounded Same Side Corner Rect [id:dp7894456665808554] 
        \draw   (487,235.79) .. controls (487,228.38) and (493.01,222.37) .. (500.42,222.37) -- (505.78,222.37) .. controls (513.19,222.37) and (519.2,228.38) .. (519.2,235.79) -- (519.2,250.28) .. controls (519.2,250.28) and (519.2,250.28) .. (519.2,250.28) -- (487,250.28) .. controls (487,250.28) and (487,250.28) .. (487,250.28) -- cycle ;
        %Shape: Ellipse [id:dp029531340860110333] 
        \draw   (489.68,208.42) .. controls (489.68,201.01) and (495.69,195) .. (503.1,195) .. controls (510.51,195) and (516.52,201.01) .. (516.52,208.42) .. controls (516.52,215.83) and (510.51,221.83) .. (503.1,221.83) .. controls (495.69,221.83) and (489.68,215.83) .. (489.68,208.42) -- cycle ;
        %Rounded Same Side Corner Rect [id:dp6679824333254505] 
        \draw   (520,179.79) .. controls (520,172.38) and (526.01,166.37) .. (533.42,166.37) -- (538.78,166.37) .. controls (546.19,166.37) and (552.2,172.38) .. (552.2,179.79) -- (552.2,194.28) .. controls (552.2,194.28) and (552.2,194.28) .. (552.2,194.28) -- (520,194.28) .. controls (520,194.28) and (520,194.28) .. (520,194.28) -- cycle ;
        %Shape: Ellipse [id:dp2173625254279581] 
        \draw   (522.68,152.42) .. controls (522.68,145.01) and (528.69,139) .. (536.1,139) .. controls (543.51,139) and (549.52,145.01) .. (549.52,152.42) .. controls (549.52,159.83) and (543.51,165.83) .. (536.1,165.83) .. controls (528.69,165.83) and (522.68,159.83) .. (522.68,152.42) -- cycle ;
        %Straight Lines [id:da13019449561624885] 
        \draw    (430.2,209.6) -- (342.2,209.6) ;
        \draw [shift={(340.2,209.6)}, rotate = 360] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
        
        % Text Node
        \draw (176,165) node [anchor=north west][inner sep=0.75pt]   [align=left] {$a$};
        % Text Node
        \draw (171,214) node [anchor=north west][inner sep=0.75pt]   [align=left] {$b_a$};
        % Text Node
        \draw (376,164) node [anchor=north west][inner sep=0.75pt]   [align=left] {$p_a$};
        % Text Node
        \draw (371,215) node [anchor=north west][inner sep=0.75pt]   [align=left] {$d(p_a)$};
        % Text Node
        \draw (183,252) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\mathcal{G}_B(p_a, b_a) = p_a \min \{ d(p_a), b_a \}$};
        % Text Node
        \draw (20,251) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\mathcal{G}_A(a) = a b_a$};
        % Text Node
        \draw (15,110) node [anchor=north west][inner sep=0.75pt]   [align=left] {Leader (Supplier)};
        % Text Node
        \draw (209,108) node [anchor=north west][inner sep=0.75pt]   [align=left] {Follower (Retailer)};
        % Text Node
        \draw (464,108) node [anchor=north west][inner sep=0.75pt]   [align=left] {Market};
    \end{tikzpicture}
    \caption{\textbf{The Newsvendor Pricing Game.} From \parencite{liu:2024_stacknews_adt}, in this Stackelberg game, there a logistics network between a supplier (leader) and retailer (follower), where utility functions are not necessarily supermodular, the supplier issues a wholesale price $a$, and the retailer issues a purchase quantity $b$, and a retail price $p$ in response.} \label{fig:supplier-retailer-game}
\end{figure}

\textbf{Demand Function:} Stochastic demand is represented in Eq. \ref{eq:additive-exp-demand}, which is governed by a linear additive demand function $\Gamma_\rho(p)$ representing the expected demand, $\mathbbm{E}[d(p)]$, as a function of $p$ in Eq. \ref{eq:additive-exp-demand}. The demand function is governed by parameters $\rho$.

\begin{align}
    \Gamma_\rho(p) &= \max \{0, \rho_0 - \rho_1 p \}, \quad \rho_0 \geq 0, \ \rho_1 \geq 0 \label{eq:demand-theta-func} \\
    d_\rho(p) &= \Gamma_\rho(p) + \epsilon, \quad \epsilon \in \mathcal{N}(0, \sigma) \label{eq:additive-exp-demand} 
\end{align}

This problem combines the problem of the \textit{price-setting Newsvendor} \parencite{petruzzi:1999newsv} \parencite{arrow:1951newsboy}, with that of a bilateral Stackelberg game under imperfect information. Even in the scenario of perfect information, the \textit{price-setting Newsvendor} has no closed-form solution, therefore no exact solution to the Stackelberg equilibrium. We apply the algorithm from \parencite{liu:2024_stacknews_adt} to learn a Stackelberg equilibrium under a \textit{risk-free pricing} strategy assumption, and apply Algorithm \ref{alg:se-newsv} from \parencite{liu:2024_stacknews_adt} as a baseline against Algorithm \ref{alg:gisa} (GISA).

\begin{algorithm}[H]
\caption{Learning Algorithm for Newsvendor Pricing Game from \parencite{liu:2024_stacknews_adt} }\label{alg:se-newsv}
\begin{algorithmic}[1]
    \For {$t \in 1 ... T$}:
        \State Leader and follower estimates a confidence interval $\uncerBall{t}$ from available data. 
        \State $\mathcal{H}(\rho) = \ \hat{\rho}_0/\hat{\rho}_1$.
        \State Leader plays action $a$, where $a = \underset{a \in \mathcal{A}, \rho \in \mathcal{C}^t} {\mathrm{argmax}} \ a F^{-1}_{\bar{\rho}_a} \Big( 1 - \frac{2a}{ \mathcal{H}(\rho) + a} \Big)$ from Eq. (3.8) in \parencite{liu:2024_stacknews_adt}.
        \State Follower sets price $p = (\mathcal{H}(\rho) + a)/2$.
        \State Follower estimates their optimistic parameters $\bar{\rho}_a$, and best response $\bar{b}_a$ from  from Eq. (3.4) and (3.5a) respectively in \parencite{liu:2024_stacknews_adt}.
        \State Leader obtains reward, $\mathcal{G}_A = ab$.
        \State Follower obtains reward, $\mathcal{G}_B = p \min \{ b, d(p) \} $.
    \EndFor
\end{algorithmic}
\end{algorithm}





\clearpage

\subsubsection{NPG Results} \label{sec:npg_results}

\begin{figure}[H]
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/npg_rew_ex1_v2.png}
  % \caption*{Parameters: $\kappa=1.0, \rho_0 = 73, \rho_1 = 7, \sigma=3.2$.}% \label{fig:market3_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/npg_reg_ex1_v2.png}
  % \caption*{Parameters: $\kappa=1.0, \rho_0 = 73, \rho_1 = 7, \sigma=3.2$.}% \label{fig:market3_loss}
\endminipage\hfill
\caption*{Parameters: $\rho_0 = 1, \rho_1 = -0.1, \sigma=0.1$.}
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/npg_rew_ex2_v2.png}
  % \caption*{Empirical leader regret. $\kappa=4, \rho_0 = 10, \rho_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  % \includegraphics[width=\linewidth]{figures/config6_b_reg_v2.png}
  \includegraphics[width=\linewidth]{figures/npg_reg_ex2_v2.png}
  % \caption*{Empirical contextual regret. $\kappa=4, \rho_0 = 10, \rho_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\caption*{Parameters: $\rho_0 = 1, \rho_1 = -0.5, \sigma=0.1$.}
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/npg_rew_ex3_v2.png}
  %\caption*{Empirical leader regret. $\kappa=2, \rho_0 = 10, \rho_1 = 3$.}%\label{fig:market4_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/npg_reg_ex3_v2.png}
  %\caption*{Empirical contextual regret. $\kappa=2, \rho_0 = 10, \rho_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\caption*{Parameters: $\rho_0 = 1, \rho_1 = -0.3, \sigma=0.5$.}
\caption{Mean values are calculated over 1,000 trials, with shaded regions representing confidence intervals, all of which fall within the first quartile. $\stEmb$ dimension is set to $D = 6$.} \label{fig:loss_plot}
\end{figure}


\subsection{Multi-Dimensional Stackelberg Game (SSG)} \label{sec:multi-dim-ssg}

We consider a two-player Stackelberg game where the leader $ A $ and the follower $ B $ choose their actions from a shared action space $ \mathbb{R}^n $. The leader chooses an action $ \aB \in \mathbb{R}^n $, anticipating the follower’s response $ \bB \in \mathbb{R}^n $, where $n=5$. Both players' rewards are influenced by a combination of the difference in their actions and quadratic penalties on their individual actions. The problem is constrained by weighted $ L_1 $-norm bounds on both $ \aB $ and $ \bB $, which limit the magnitude of their respective actions.

The leader’s reward function $ \mu_A $ is defined as:
\begin{align}
\mu_A(\aB, \bB) = \theta_A^\top (\aB - \bB) - \theta_A^\top f(\aB) + \epsilon, \quad \epsilon \in \mathcal{N}(0, \sigma) 
\end{align}
where:
\begin{itemize}
    \item $ \aB \in \mathbb{R}^n $ is the leader’s action,
    \item $ \bB \in \mathbb{R}^n $ is the follower’s action,
    \item $ \theta_A \in \mathbb{R}^n $ is a weight vector for the leader,
    \item $ f(\aB)$ is the quadratic penalty function applied elementwise, such that $ f(\aB) = [\aB_1^2, \aB_2^2, \dots, \aB_n^2] $.
\end{itemize}
The leader seeks to maximize $ \mu_A(a, b) $ by selecting $ \aB $, knowing that the follower will respond optimally.

The follower’s reward function $ \mu_B $ is defined as:
\begin{align}
\mu_B(\aB, \bB) = \theta_B^\top (\aB - \bB) - \theta_B^\top g(\bB)
\end{align}
where:
\begin{itemize}
    \item $ \aB \in \mathbb{R}^n $ is the leader’s action,
    \item $ \bB \in \mathbb{R}^n $ is the follower’s action,
    \item $ \theta_B \in \mathbb{R}^n $ is a weight vector for the follower,
    \item $ g(\bB) $ is the quadratic penalty function applied elementwise, such that $ g(\bB) = [\bB_1^2, \bB_2^2, \dots, b_n^2] $.
\end{itemize}
The follower seeks to maximize $ \mu_B(\aB, \bB) $ by choosing $ \bB $, given the leader’s action $ \aB $.

Both players are subject to weighted $ L_1 $-norm constraints on their actions:
\begin{align}
\sum_{i=1}^{n} |\theta_{A,i} a_i| \leq C_A \quad \text{for the leader} \\
\sum_{i=1}^{n} |\theta_{B,i} b_i| \leq C_B \quad \text{for the follower}
\end{align}
where $ C_A $ and $ C_B $ are constants that limit the magnitude of the actions $ \aB $ and $ \bB $, respectively, and $ \theta_{A,i} $, $ \theta_{B,i} $ are the elements of $ \theta_A $ and $ \theta_B $.

\textbf{Follower’s Optimization Problem (Best Response):} Given the leader’s action $ \aB $, the follower solves the following optimization problem:
\begin{align}
b^*(\aB) = \arg\max_b \left( \theta_B^\top (\aB - \bB) - \theta_B^\top g(\bB) \right)
\end{align}
subject to:
\begin{align}
\sum_{i=1}^{n} |\theta_{B,i} b_i| \leq C_B
\end{align}
This is a quadratic optimization problem due to the quadratic penalty $ g(\bB) $, and the constraint enforces that the weighted $ L_1 $-norm of the follower's action does not exceed $ C_B $.

\textbf{Leader’s Optimization Problem:} Given the follower’s best response $ \bB^*(\aB) $, the leader solves the following optimization problem:
\begin{align}
a^* = \arg\max_a \left( \theta_A^\top (\aB - \bB^*(\aB)) - \theta_A^\top f(\aB) \right)
\end{align}
subject to:
\begin{align}
\sum_{i=1}^{n} |\theta_{A,i} a_i| \leq C_A
\end{align}
This is also a quadratic optimization problem due to the quadratic penalty $ f(a) $, and the constraint enforces that the weighted $ L_1 $-norm of the leader’s action does not exceed $ C_A $.

\textbf{Stackelberg equilibrium: }The Stackelberg equilibrium is reached when:
\begin{align}
a^* = \arg\max_{\aB} \left( \theta_A^\top (\aB - \bB^*(\aB)) - \theta_A^\top f(\aB) \right), \quad b^*(\aB) = \arg\max_{\bB} \left( \theta_B^\top (\aB - \bB) - \theta_B^\top g(\bB) \right)
\end{align}
subject to the respective $ L_1 $-norm constraints. At equilibrium, the leader chooses $ \aB^* $ that maximizes their reward given the follower’s optimal response $ \bB^*(\aB) $, and the follower chooses $ \bB^*(\aB) $ that maximizes their reward given the leader’s action.

\clearpage

\subsubsection{SSG Empirical Results} \label{sec:ssg_empirical}

\begin{figure}[H]
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/ssg_rew_ex1_v2.png}
  % \caption*{Parameters: $\kappa=1.0, \theta_0 = 73, \theta_1 = 7, \sigma=3.2$.}% \label{fig:market3_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/ssg_reg_ex1_v2.png}
  % \caption*{Parameters: $\kappa=1.0, \theta_0 = 73, \theta_1 = 7, \sigma=3.2$.}% \label{fig:market3_loss}
\endminipage\hfill
\caption*{Parameters: $\theta_{A} = \left[ -0.850, -0.049, 0.620, -0.535, -0.313 \right], \quad \theta_{B} = \left[ -1.554, -0.176, 0.576, 0.803, 0.358 \right], \sigma = 0.1$}
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/ssg_rew_ex2_v3.png}
  % \caption*{Empirical leader regret. $\kappa=4, \theta_0 = 10, \theta_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  % \includegraphics[width=\linewidth]{figures/config6_b_reg_v2.png}
  \includegraphics[width=\linewidth]{figures/ssg_reg_ex2_v3.png}
  % \caption*{Empirical contextual regret. $\kappa=4, \theta_0 = 10, \theta_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\caption*{Parameters: $\theta_{A} = \left[ -1.557, -0.011, 0.821, -1.307, -0.262 \right], \quad \theta_{B} = \left[ -1.499, 0.317, -0.106, 0.465, -0.476 \right], \sigma = 0.1$}
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/ssg_rew_ex3_v2.png}
  %\caption*{Empirical leader regret. $\kappa=2, \theta_0 = 10, \theta_1 = 3$.}%\label{fig:market4_loss}
\endminipage\hfill
\minipage{0.47\textwidth}
  \includegraphics[width=\linewidth]{figures/ssg_reg_ex3_v2.png}
  %\caption*{Empirical contextual regret. $\kappa=2, \theta_0 = 10, \theta_1 = 3$.}%\label{fig:market1_loss}
\endminipage\hfill
\caption*{Parameters: $\theta_{A} = \left[ -0.599, -0.951, 0.156, -0.732, 0.375 \right], \quad \theta_{B} = \left[ -0.866, 0.708, -0.156, 0.601, -0.058 \right], \sigma = 0.1$}
\caption{Mean values are computed over 1,000 trials. All shaded areas, denoting confidence intervals, are within a quarter quantile. UCB arms were discretized to increments of 200, with an exploration constant $\alpha_{UCB} = 0.01.$ $\stEmb$ dimension is set to $D = 10$.} \label{fig:r5_experimental_results}
\end{figure}

\clearpage

% \section{\chagesMarker{Table of Notations}}

% \begin{table}[h!]
% \centering
% \begin{tabular}{@{}>{\raggedright\arraybackslash}p{1cm} 
%                 >{\raggedright\arraybackslash}p{12cm}@{}}
% \toprule
% \textbf{Symbol}         & \textbf{Description}\\ 
% \midrule
% $\aB, \bB$   & Actions \\ 
% \midrule
% $\mu$        & The reward functions for player A and B respectively. Expressed as $\mu_A, \mu_B$ via subscripts for each player A and B respectively.\\ 
% \midrule
% $\pi$        & Policy.\\ 
% \midrule
% $\mathcal{H}$   & Action history. \\ 
% \midrule
% $\mathcal{U}$   & reward history. \\ 
% \midrule

% $\theta$        & An abstract notion of the parameters of the linear reward function. Expressed as $\theta_A, \theta_B$ via subscripts for each player A and B respectively.\\ 
% \midrule
% $\theta^*$        & asdf.\\ 
% \midrule
% $\theta'$        & Value of $\theta$ projected on the the D-sphere surface, via normalization, as expressed in Eq. \eqref{eq:gisa_map_and_norm}. \\ 
% \midrule
% $\hat{\theta}$   & Estimated value of $\theta^*$ from observed histories of $\mu, \aB$ and $\bB$. \\ 
% \midrule
% $\phi(\aB, \bB)$   & Feature map. \\ 
% \midrule
% $\stEmb$   & Stackelberg embedding. \\ 
% \midrule
% $\IsoPL{\aB}$   & Isoplanar subspace. \\ 
% \midrule
% $\uncerBall{t}$   & Confidence radius. \\ 
% \midrule

% \bottomrule
% \end{tabular}
% \caption{Table of Notations.}
% \end{table}





\end{document}