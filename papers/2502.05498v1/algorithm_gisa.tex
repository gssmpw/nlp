\begin{algorithm}[h!]
\caption{Geodesic Isoplanar Subspace Alignment (GISA) Algorithm}\label{alg:gisa}
\begin{algorithmic}[1]
\State \textbf{Input:} Time horizon $T$, and confidence ball $\uncerBall{\cdot}$.
\State \textbf{Output:} Estimated optimal leader action $\hat{\aB}$.
\State Initialize $\hat{\theta}_A$ and $\hat{\theta}_B$ uniformly at random.
\State Initialize reward and action histories, $\mathcal{U}$ and $\mathcal{H}$ as empty sets $\emptyset$.
\State Construct a Stackelberg embedding $\stEmb$ and feature map $\phi$ per specifications in Sec. \ref{sec:stack_embedding}.
\For {$t \in 1 ... T$}:
    \If{$\geoDis(\hat{\theta}_A, \hat{\theta}_B) < 2 \uncerBall{t}$} 
        \State \texttt{Phase 1}: Select uniformly an action on the boundary of $A$'s geodesic confidence ball.
        \State $\aB \sim \texttt{Uniform}[\partial \, \Ball_{\geoDis}(\uncerBall{t})] \,$ (See Lemma \ref{lem:geodesic_uncertainty_ball}.)
    \Else
        \State \texttt{Phase 2}: Select $\aB$ that minimizes the geodesic distance to $\hat{\theta}_B$ from $\Ball_{\geoDis}(\uncerBall{t})$.
        \State $\aB \gets \underset{\aB \in \Ball_{\geoDis}(\uncerBall{t})}{\argmin} \,
 \geoDis(\aB, \hat{\theta}_B)$
    \EndIf
    \State $\bB \gets \underset{\bB \in \IsoPL{\aB}}{\argmin}\geoDis(\bB, \hat{\theta}_B)$
    \State $\hat{\aB}^t, \hat{\bB}^t \gets \phi^{-1}(\aB, \bB)$ \Comment{Perform an inverse map back to the joint action space.}
    \State \textbf{yield} $\hat{\aB}^t, \hat{\bB}^t$, and obtain empirical reward $\mu_A^t, \mu_B^t$.
    \State $\mathcal{H} \gets \mathcal{H} \cup (\hat{\aB}^t, \hat{\bB}^t), \quad \mathcal{U} \gets \mathcal{U} \cup (\mu_A^t, \mu_B^t)$.
    \State Re-estimate $\hat{\theta}_A$ and $\hat{\theta}_B$ from $\mathcal{H}$ and $\mathcal{U}$, based on Eq. \eqref{eq:theta_estimation_xtx}.
    % \State \textbf{yield} $\hat{\aB}^t$ \Comment{$(\mu_A^t, \mu_B^t)$ is revealed when the leader selects $\hat{\aB}^t$ to play.}
\EndFor
\State \Return $\hat{\aB}^t$

\end{algorithmic}
\end{algorithm}

