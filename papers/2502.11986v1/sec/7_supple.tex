\clearpage
\newpage
\appendix
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand\thefigure{\thesection.\arabic{figure}}    

%-------------------------------------------------------------------------
\section{Proofs of Theoretical Analysis}
\label{Append:proof}
Before diving into the theoretical analysis proof, we'll briefly introduce the basic concepts used in our proof to ensure the paper is self-contained.

\begin{definition}[Lipschitz continuity] $f$ is $\lambda$-Lipschitz if for any two points $u,v$ in the domain of $f$, we have following inequality:
\begin{align}
    |f(u)-f(v)|\leq \lambda ||u-v||
\end{align}
\end{definition}
%-------------------------------------------------------------------------
\subsection{Difference between inter-task affinity and proximal inter-task affinity}
\label{Append:differeence_affinity}
Firstly, let's reiterate the definitions of inter-task affinity and proximal inter-task affinity from the main paper.

In a typical SGD process for task $i$ at time step $t$ with input $z^t$, the update rule for $\Theta_s$ is as follows: $\Theta_{s|i}^{t+1} = \Theta_s^t-\eta w_i \nabla_{\Theta_s^t} \mathcal{L}_i(z^t, \Theta_s^t, \Theta_i^t)$ where $z^t$ represents the input data and $\eta$ is the learning rate, $\Theta_{s|i}^{t+1}$ is the updated shared parameters with loss $\mathcal{L}_i$. Then the affinity from task $i$ to $k$ at time step $t$, denoted as $\mathcal{A}^t_{i\rightarrow k}$, is:
\begin{align}
    \mathcal{A}^t_{\textcolor{red}{i\rightarrow k}} &= 1- \frac{\mathcal{L}_k(z^t, \Theta_{s|i}^{t+1}, \Theta_k^{\textcolor{red}{t}})}{\mathcal{L}_k(z^t, \Theta_{s}^{t}, \Theta_k^t)}
    \label{append:definition:inter_task_affinity}
\end{align}

For proximal inter-task affinity, let's consider a multi-task network shared by the task set $G$, with their respective losses defined as $\mathcal{L}_G$. For a data sample $z^t$ and a learning rate $\eta$, the gradients of task set $G$ are updated to the parameters of the network as follows: $\Theta_{s|G}^{t+1} = \Theta_s^t -\eta \nabla_{\Theta_s^t} \mathcal{L}_G (z^t, \Theta_s^t, \Theta_G^t)$ and $\Theta_k^{t+1} = \Theta_k^t -\eta \nabla_{\Theta_k^t} \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t)$ for $k \in G$. Then, the proximal inter-task affinity from group $G$ to task $k$ at time step $t$ is defined as:
\begin{align}
    \mathcal{B}^t_{\textcolor{red}{G\rightarrow k}} = 1- \frac{\mathcal{L}_k(z^t, \Theta_{s|\textcolor{red}{G}}^{t+1}, \Theta_k^{\textcolor{red}{t+1}})}{\mathcal{L}_k(z^t, \Theta_{s}^{t}, \Theta_k^t)}
    \label{append:definition:proximal_inter_task_affinity}
\end{align}

The primary distinction between the two affinities lies in the incorporation of the task set and the update of task-specific parameters (indicated by \textcolor{red}{red letters}). Proximal inter-task affinity is an expanded concept that integrates the task set rather than individual tasks as the source task. This difference is evident from the notation, where $i \rightarrow k$ in \Cref{append:definition:inter_task_affinity} and $G \rightarrow k$ in \Cref{append:definition:proximal_inter_task_affinity}.

The second main difference lies in the update of task-specific parameters. In the inter-task affinity in \Cref{append:definition:inter_task_affinity}, the denominator includes $\Theta_k^t$, while in the proximal inter-task affinity in \Cref{append:definition:proximal_inter_task_affinity}, it includes $\Theta_k^{t+1}$, which is a subtle distinction that may not be noticed by readers.
These two modifications allow us to track proximal inter-task affinity while simultaneously optimizing multi-task networks.

When measuring affinity under the assumption of a convex objective, the proximal inter-task affinity is equal to or greater than the inter-task affinity. This also aligns well with real-world scenarios, as proximal inter-task affinity reflects updates to task-specific parameters, as shown below.
\begin{align}
    \mathcal{A}^t_{i\rightarrow k} &= 1- \frac{\mathcal{L}_k(z^t, \Theta_{s|i}^{t+1}, \Theta_k^t)}{\mathcal{L}_k(z^t, \Theta_{s}^{t}, \Theta_k^t)} \leq 1- \frac{\mathcal{L}_k(z^t, \Theta_{s|i}^{t+1}, \Theta_k^{t+1})}{\mathcal{L}_k(z^t, \Theta_{s}^{t}, \Theta_k^t)} = \mathcal{B}^t_{i\rightarrow k}
\end{align}

This inequality can also be applied to an expanded setting that incorporates task sets. If we expand the concept of inter-task affinity from individual task to task set as $\mathcal{A}^t_{G \rightarrow k}$, then the inequality $\mathcal{A}^t_{G \rightarrow k} \leq \mathcal{B}^t_{G \rightarrow k}$ is satisfied. If $k \notin G$ then, $\mathcal{A}^t_{G \rightarrow k} = \mathcal{B}^t_{G \rightarrow k}$ holds.

For ease of notation, we use the expanded version of the affinity for multiple tasks throughout the proof, as follows:
\begin{align}
    \mathcal{A}^t_{G\rightarrow k} &= 1- \frac{\mathcal{L}_k(z^t, \Theta_{s|G}^{t+1}, \Theta_k^t)}{\mathcal{L}_k(z^t, \Theta_{s}^{t}, \Theta_k^t)}
    \label{Append:expanded_affinity}
\end{align}
This differs from proximal inter-task affinity, as it does not consider the update of task-specific parameters.

%-------------------------------------------------------------------------
\subsection{Proof of \Cref{theorem1}}
\label{Append:theorem1}

\theomone*
\begin{proof}
Let's consider a scenario where we update the network parameters $\Theta$ with task-specific losses $\mathcal{L}_i$ and $\mathcal{L}_k$ simultaneously at time step $t$ with input $z^t$. Applying the Taylor expansion, we obtain the following:
\begin{align}
    \mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+1}, \Theta_k^{t})
    &\simeq \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) + (\Theta_{s|i,k}^{t+1} - \Theta_s^t) \nabla_{\Theta_s^t} \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) + O(\eta^2) \\
    &= \mathcal{L}_k (z^t, \Theta_{s}^{t}, \Theta_k^{t})-\eta g_k\cdot(g_i+g_k) + O(\eta^2)
\end{align}
where $g_i$ and $g_k$ represent the gradients backpropagated from the losses $\mathcal{L}_i$ and $\mathcal{L}_k$, respectively, with respect to the shared parameters $\Theta_s^t$. For instance, $g_i = \nabla_{\Theta_s^t} \mathcal{L}_i (z^t, \Theta_s^t, \Theta_i)$.

Reorganizing the inequality to align with the format of inter-task affinity, we obtain:  
\begin{align}
    \mathcal{A}_{i,k\rightarrow k}^t = 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+1}, \Theta_k^{t})}{\mathcal{L}_k (z^t, \Theta_{s}^t, \Theta_k^{t})} \simeq \frac{1}{\mathcal{L}_k (z^t, \Theta_{s}^t, \Theta_k^{t})}\biggl(\eta g_k\cdot(g_i+g_k) + O(\eta^2)\biggr)
\end{align}
Similar results can be obtained for $A_{j,k\rightarrow k}^t$.
\begin{align}
    \mathcal{A}_{j,k\rightarrow k}^t = 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|j,k}^{t+1}, \Theta_k^{t})}{\mathcal{L}_k (z^t, \Theta_{s}^t, \Theta_k^{t})} \simeq \frac{1}{\mathcal{L}_k (z^t, \Theta_{s}^t, \Theta_k^{t})}\biggl(\eta g_k\cdot(g_j+g_k) + O(\eta^2)\biggr)
\end{align}
From $\mathcal{A}_{i,k \rightarrow k}^t \geq \mathcal{A}_{j,k \rightarrow k}^t$ and by ignoring the $O(\eta^2)$ term with a sufficiently small learning rate $\eta \ll 1$, we can derive the result:
\begin{align}
    g_i \cdot g_k \geq g_j \cdot g_k
\end{align}
\end{proof}
The findings indicate that grouping tasks with positive inter-task affinity exhibits better alignment in task-specific gradients compared to grouping tasks with negative inter-task affinity, thereby validating the grouping strategies employed by our algorithm. Furthermore, we analyze how this alignment in task-specific gradients contributes to reducing the loss of task $k$ in \Cref{theorem2}.


%-------------------------------------------------------------------------
\subsection{Proof of \Cref{theorem2}}
\label{Append:theorem2}

\theomtwo*
\begin{proof}
Let's consider a scenario where we update the network parameters $\Theta_s^t$ with task-specific losses $\mathcal{L}_i$ and $\mathcal{L}_k$ simultaneously at time step $t$ with input $z^t$. Let $g_i$ denote the gradients backpropagated from the loss $\mathcal{L}_i$ with respect to the shared parameters $\Theta_s^t$, expressed as $g_i = \nabla{\Theta_s^t} \mathcal{L}_i (z^t, \Theta_s^t, \Theta_i)$.

Using the first-order Taylor approximation of $\mathcal{L}_k$ for $\Theta_s^t$, we obtain:
\begin{align}
    \mathcal{L}_k (z^t, \Theta_{s|i,k}^{t+1}, \Theta_k^t) &= \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) + (\Theta_{s|i,k}^{t+1} - \Theta_s^t) \nabla_{\Theta_s^t} \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) + O(\eta^2)\\
    &= \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) - \eta (g_i + g_k)\cdot g_k + O(\eta^2)
    \label{eq:theo2_pre1}
\end{align}

For task $j$, we can follow a similar process as follows:
\begin{align}
    \mathcal{L}_k (z^t, \Theta_{s|j,k}^{t+1}, \Theta_k^t) = \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) - \eta (g_j + g_k)\cdot g_k + O(\eta^2)
    \label{eq:theo2_pre2}
\end{align}

With a sufficiently small learning rate $\eta \ll 1$, subtract \cref{eq:theo2_pre2} from \cref{eq:theo2_pre1}, then:
\begin{align}
    \mathcal{L}_k (z^t, \Theta_{s|i,k}^{t+1}, \Theta_k^t) - \mathcal{L}_k (z^t, \Theta_{s|j,k}^{t+1}, \Theta_k^t) &= - \eta (g_i + g_k)\cdot g_k + \eta (g_j + g_k)\cdot g_k \\
    &= - \eta(g_i-g_j)\cdot g_k \leq 0
    \label{eq:theo2_result}
\end{align}
which proves the results.
\end{proof}

The result indicates that when the gradients $g_i$ from task $i$ align better with those of the reference task $k$ compared to task $j$, the loss on the reference task $k$ tends to be lower with updated gradients $g_i + g_k$ compared to $g_j + g_k$, especially for sufficiently small learning rates $\eta$. 



%-------------------------------------------------------------------------
\subsection{Proof of \Cref{theorem3}}
\label{Append:theorem3}

\theomthree*
\begin{proof}
Let's begin with the definition of inter-task affinity between $\{i, j\}\rightarrow k$ and $i \rightarrow k$ as follows:
\begin{align}
    \mathcal{A}_{i,k \rightarrow k}^t &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+1}, \Theta_k^{t})}{\mathcal{L}_k(z^t, \Theta_{s}^t, \Theta_k^{t})} &
    \mathcal{A}_{i\rightarrow k}^t &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i}^{t+1}, \Theta_k^{t})}{\mathcal{L}_k(z^t, \Theta_{s}^t, \Theta_k^{t})}
    \label{eq:theo5_affin}
\end{align}

When updating $i$ and $k$ simultaneously, we can derive the first-order Taylor approximation of $\mathcal{L}_k$ for $\Theta_s^t$ as follows:
\begin{align}
    \mathcal{L}_k (z^t, \Theta_{s|i,k}^{t+1}, \Theta_k^t) &= \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) + (\Theta_{s|i,k}^{t+1} - \Theta_s^t) \nabla_{\Theta_s^t} \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) + O(\eta^2)\\
    &= \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) - \eta (g_i + g_k)\cdot g_k + O(\eta^2)
\end{align}

Similarly, when updating $i$ alone, the first-order Taylor approximation of $\mathcal{L}_k$ for $\Theta_s^t$ is as follows:
\begin{align}
    \mathcal{L}_k (z^t, \Theta_{s|i}^{t+1}, \Theta_k^t) &= \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) + (\Theta_{s|i}^{t+1} - \Theta_s^t) \nabla_{\Theta_s^t} \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) + O(\eta^2)\\
    &= \mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) - \eta g_i\cdot g_k + O(\eta^2)
\end{align}

With a sufficiently small learning rate $\eta$, the difference between the two inter-task affinities in \cref{eq:theo5_affin} can be expressed as follows:
\begin{align}
    \mathcal{A}_{i,k \rightarrow k}^t - \mathcal{A}_{i\rightarrow k}^t &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+1}, \Theta_k^{t})}{\mathcal{L}_k(z^t, \Theta_{s}^t, \Theta_k^{t})} - \biggr(1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i}^{t+1}, \Theta_k^{t})}{\mathcal{L}_k(z^t, \Theta_{s}^t, \Theta_k^{t})}\biggr) \\
    &= \frac{\mathcal{L}_k(z^t, \Theta_{s|i}^{t+1}, \Theta_k^{t}) - \mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+1}, \Theta_k^{t})}{\mathcal{L}_k(z^t, \Theta_{s}^t, \Theta_k^{t})}\\
    &= \frac{\mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) - \eta g_i\cdot g_k - (\mathcal{L}_k (z^t, \Theta_s^t, \Theta_k^t) - \eta (g_i + g_k)\cdot g_k)}{\mathcal{L}_k(z^t, \Theta_{s}^t, \Theta_k^{t})}\\
    &= \frac{\eta||g_k||^2}{\mathcal{L}_k(z^t, \Theta_{s}^t, \Theta_k^{t})} \\
    &\geq 0
    \label{eq:theo5_result}
\end{align}
The inequality in \cref{eq:theo5_result} proves that $\mathcal{A}_{i,k \rightarrow k}^t \geq \mathcal{A}_{i \rightarrow k}^t$.
\end{proof}

When tasks $i$ and $k$ are within the same task group, we can access $\mathcal{B}_{i,k \rightarrow k}^t$ during the optimization process. If $\mathcal{B}_{i,k \rightarrow k}^t \leq 0$, the inter-task affinity also satisfies $\mathcal{A}_{i,k \rightarrow k}^t \leq 0$ as $\mathcal{A}_{i,k \rightarrow k}^t \leq \mathcal{B}_{i,k \rightarrow k}^t$. According to \Cref{theorem3}, this condition implies $\mathcal{A}_{i\rightarrow k}^t\leq 0$. The proposed algorithm separates these tasks into different groups when $\mathcal{B}_{i,k \rightarrow k}^t \leq 0$ which justifies our grouping rules.

Conversely, when tasks $i$ and $j$ belong to separate task groups, we only have access to $\mathcal{B}_{i \rightarrow k}^t$ instead of $\mathcal{B}_{i,k \rightarrow k}^t$. In this scenario, the proposed algorithm merges these tasks into the same group if $\mathcal{B}_{i \rightarrow k}^t = \mathcal{A}_{i \rightarrow k}^t \geq 0$. This inequality also implies $\mathcal{A}_{i,k\rightarrow k}^t\geq 0$, justifying the merging of tasks $i$ and $k$ based on $\mathcal{B}_{i \rightarrow k}^t$ during optimization.

%-------------------------------------------------------------------------
\subsection{Proof of \Cref{theorem4}}
\label{Append:theorem4}

\theomfour*

Let's represent the sum of losses of tasks included in $G_m$ as $\mathcal{L}_{G_m}$, defined as follows:
\begin{align}
    \mathcal{L}_{G_m}(z^t, \Theta_{s|G_m}^{t+m/\mathcal{M}}, \Theta_{G_m}^{t+m/\mathcal{M}}) = \sum_{k \in G_m} \mathcal{L}_k(z^t, \Theta_{s|G_m}^{t+m/\mathcal{M}}, \Theta_{k}^{t+m/\mathcal{M}})
\end{align}
where $\Theta_{s|G_m}^{t+m/\mathcal{M}}$ denotes the shared parameters, while $\Theta_{G_m}^{t+m/\mathcal{M}}$  represents the set of task-specific parameters within $G_m$ after updating tasks in $G_m$.

We begin by expanding the task-specific loss $\mathcal{L}_{G_m}$ in terms of the shared parameter $\Theta_{s|G_m}^{t+m/\mathcal{M}}$ and the task-specific parameters $\Theta_{G_m}^{t+m/\mathcal{M}}$ using a quadratic expansion. During this process, the task-specific parameters $\Theta_{G_m}^{t+(m-1)/\mathcal{M}}=\Theta_{G_m}^t$ and $\Theta_{G_m}^{t+m/\mathcal{M}}=\Theta_{G_m}^{t+1}$, since the task-specific parameters in $G_m$ are updated only once from $\Theta_{G_m}^{t+(m-1)/\mathcal{M}}$ to $\Theta_{G_m}^{t+m/\mathcal{M}}$.
\begin{align}
    \mathcal{L}_{G_m} (z^t, \Theta_{s|G_m}^{t+m/\mathcal{M}},& \Theta_{G_m}^{t+1}) \leq \mathcal{L}_{G_m} (z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t) \label{eq:theo4_in0}\\
    &+\nabla_{\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}}\mathcal{L}_{G_m}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)(\Theta_{s|G_m}^{t+m/\mathcal{M}}-\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}})\\
    &+\frac{1}{2}\nabla_{\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}}^{2}\mathcal{L}_{G_m}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)(\Theta_{s|G_m}^{t+m/\mathcal{M}}-\Theta_{s|G_m}^{t+(m-1)/\mathcal{M}})^{2}\\
    &+\nabla_{\Theta_{G_m}^t}\mathcal{L}_{G_m}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)(\Theta_{G_m}^{t+1}-\Theta_{G_m}^t)\\
    &+\frac{1}{2}\nabla_{\Theta_{G_m}^t}^2 \mathcal{L}_{G_m}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)(\Theta_{G_m}^{t+1}-\Theta_{G_m}^t)^{2}\\
    \leq &\mathcal{L}_{G_m} (z^t, \Theta_{s|G_m}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)\\
    &+\nabla_{\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}}\mathcal{L}_k(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)(\Theta_{s|G_m}^{t+m/\mathcal{M}}-\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}})\\
    &+\frac{1}{2} H|G_m| (\Theta_{s|G_m}^{t+m/\mathcal{M}}-\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}})^{2}\\
    &+\nabla_{\Theta_{G_m}^t}\mathcal{L}_{G_m}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)(\Theta_{G_m}^{t+1}-\Theta_{G_m}^t)\\
    &+\frac{1}{2} H|G_m| (\Theta_{G_m}^{t+1}-\Theta_{G_m}^t)^{2}
\end{align}
where $|G_m|$ represents the number of tasks in $G_m$. The inequality holds with the Lipschitz continuity of $\nabla \mathcal{L}$ with a constant $H$.

For the shared parameters of the network, the update rule is as follows:
\begin{align}
    \Theta_{s|G_m}^{t+m/\mathcal{M}} &= \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}} - \eta \nabla_{\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}} \mathcal{L}_{G_m}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)\\
    &= \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}} - \eta g_{s, G_m}^{t+(m-1)/\mathcal{M}}
    \label{eq:theo4_in1}
\end{align}
where $g_{s, G_m}^{t+(m-1)/\mathcal{M}}$ is the gradients of the shared parameters with respect to loss of tasks in $G_m$.

Similarly, the task-specific parameters of the network, the update rule is as follows:
\begin{align}
    \Theta_{G_m}^{t+1} &= \Theta_{G_m}^{t} - \eta \nabla_{\Theta_{G_m}^{t}} \mathcal{L}_{G_m}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t) = \Theta_{G_m}^t - \eta g_{ts, G_m}^{t}
    \label{eq:theo4_in2}
\end{align}
where $g_{ts, G_m}^t$ is the gradients of the task-specific parameters with respect to the loss of tasks in $G_m$.

If we substitute \cref{eq:theo4_in1} and \cref {eq:theo4_in2} into the result of \cref{eq:theo4_in0}, it become as follows:
\begin{align}
    \mathcal{L}_{G_m} (z^t, \Theta_{s|G_m}^{t+m/\mathcal{M}}, \Theta_{G_m}^{t+1}) \leq& \mathcal{L}_{G_m} (z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)\\
    &- \eta ||g_{s, G_m}^{t+(m-1)/\mathcal{M}}||^2  + \frac{\eta^2 H|G_m|}{2}||g_{s, G_m}^{t+(m-1)/\mathcal{M}}||^2 \\
    &- \eta ||g_{ts, G_m}^t||^2  + \frac{\eta^2 H|G_m|}{2}||g_{ts, G_m}^t||^2
\end{align}

We can derive similar results for the loss of task group $G_i$, where the index $i$ is not the same as the updating group sequence $m$ ($i \neq m$). This process follows similarly to the one described above. For the step from $t+(m-1)/\mathcal{M}$ to $t+m/\mathcal{M}$, the task-specific parameters in $G_i$ remain unchanged.
\begin{align}
    \mathcal{L}_{G_i} (z^t, \Theta_{s|G_m}^{t+m/\mathcal{M}},& \Theta_{G_i}^t) \leq \mathcal{L}_{G_i} (z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_i}^t)\\
    &+\nabla_{\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}}\mathcal{L}_{G_i}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_i}^t)(\Theta_{s|G_m}^{t+m/\mathcal{M}}-\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}})\\
    &+\frac{1}{2}\nabla_{\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}}^{2}\mathcal{L}_{G_i}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_i}^t)(\Theta_{s|G_m}^{t+m/\mathcal{M}}-\Theta_{s|G_m}^{t+(m-1)/\mathcal{M}})^{2}\\
    % &+\nabla_{\Theta_{G_m}^t}\mathcal{L}_{G_i}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_i}^t)(\Theta_{G_m}^{t+1}-\Theta_{G_m}^t)\\
    % &+\frac{1}{2}\nabla_{\Theta_{G_m}^t}^2 \mathcal{L}_{G_i}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)(\Theta_{G_m}^{t+1}-\Theta_{G_m}^t)^{2}\\
    \leq &\mathcal{L}_{G_i} (z^t, \Theta_{s|G_m}^{t+(m-1)/\mathcal{M}}, \Theta_{G_i}^t)\\
    &+\nabla_{\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}}\mathcal{L}_{G_i}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_i}^t)(\Theta_{s|G_m}^{t+m/\mathcal{M}}-\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}})\\
    &+\frac{1}{2} H|G_i| (\Theta_{s|G_m}^{t+m/\mathcal{M}}-\Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}})^{2}\\
    % &+\nabla_{\Theta_{G_m}^t}\mathcal{L}_{G_i}(z^t, \Theta_{s|G_{m-1}}^{t+(m-1)/\mathcal{M}}, \Theta_{G_m}^t)(\Theta_{G_m}^{t+1}-\Theta_{G_m}^t)\\
    % &+\frac{1}{2} H|G_i| (\Theta_{G_m}^{t+1}-\Theta_{G_m}^t)^{2}\\
    \leq & \mathcal{L}_{G_i} (z^t, \Theta_{s|G_m}^{t+(m-1)/\mathcal{M}}, \Theta_{G_i}^t)\\
    &- \eta g_{s, G_i}^{t+(m-1)/\mathcal{M}} \cdot g_{s, G_m}^{t+(m-1)/\mathcal{M}} + \frac{\eta^2 H|G_i|}{2}||g_{s, G_m}^{t+(m-1)/\mathcal{M}}||^2\\
    % &- \eta g_{ts, G_i}^t \cdot g_{ts, G_m}^t  + \frac{\eta^2 H|G_i|}{2}||g_{ts, G_m}^t||^2 \\
\end{align}

Then the total loss of multiple task groups can be expressed as follows:
\begin{align}
    \sum_{k=1}^{\mathcal{M}} \mathcal{L}_{G_k}(z^t, &\Theta_{s|G_m}^{t+m/\mathcal{M}}, \Theta_{G_k}^{t+1}) \leq \sum_{k=1}^{\mathcal{M}} \mathcal{L}_{G_k}(z^t, \Theta_{s|G_m}^{t+(m-1)/\mathcal{M}}, \Theta_{G_k}^t) \\
    &-\eta\sum_{k=1}^{\mathcal{M}} g_{s, G_k}^{t+(m-1)/\mathcal{M}} \cdot g_{s, G_m}^{t+(m-1)/\mathcal{M}} + \frac{\eta^2 H}{2} ||g_{s, G_m}^{t+(m-1)/\mathcal{M}}||^2 \sum_{k=1}^{\mathcal{M}} |G_k| \label{eq:theo4_lip1}\\ 
    &- \eta ||g_{ts, G_m}^t||^2  + \frac{\eta^2 H|G_m|}{2}||g_{ts, G_m}^t||^2 \label{eq:theo4_lip2}\\
    \leq&\sum_{k=1}^{\mathcal{M}} \mathcal{L}_{G_k}(z^t, \Theta_{s|G_m}^{t+(m-1)/\mathcal{M}}, \Theta_{G_k}^t) \label{eq:theo4_lip_out_0}\\
    &-\eta\sum_{k=1}^{\mathcal{M}} g_{s, G_k}^{t+(m-1)/\mathcal{M}} \cdot g_{s, G_m}^{t+(m-1)/\mathcal{M}} + \eta ||g_{s, G_m}^{t+(m-1)/\mathcal{M}}||^2 - \frac{\eta}{2}||g_{ts, G_m}^t||^2 \label{eq:theo4_lip_out}\\
    =& \sum_{k=1}^{\mathcal{M}} \mathcal{L}_{G_k}(z^t, \Theta_{s|G_m}^{t+(m-1)/\mathcal{M}}, \Theta_{G_k}^t)\\
    &-\eta g_{s, G_m}^{t+(m-1)/\mathcal{M}} \cdot (\sum_{k=1}^{\mathcal{M}} g_{s, G_k}^{t+(m-1)/\mathcal{M}} - g_{s, G_m}^{t+(m-1)/\mathcal{M}}) - \frac{\eta}{2}||g_{ts, G_m}^t||^2
    \label{eq:theo4_result}
\end{align}


The inequality between \cref{eq:theo4_lip1} and the first term in \cref{eq:theo4_lip_out} requires $\eta\leq \frac{2}{H\cdot \sum_{k=1}^{\mathcal{M}} |G_k|} = \frac{2}{H \mathcal{K}}$, while the inequality between \cref{eq:theo4_lip2} and the second term in \cref{eq:theo4_lip_out} requires $\eta\leq \frac{1}{H|G_M|}$. Therefore, the inequality in \cref{eq:theo4_lip_out_0} holds when $\eta \leq \min(\frac{2}{H\mathcal{K}}, \frac{1}{H|G_M|})$. Previous approaches, which handle updates of shared and task-specific parameters independently, failing to capture their interdependence during optimization.
The term, $g_{s, G_m}^{t+(m-1)/\mathcal{M}} \cdot (\sum_{k=1}^{\mathcal{M}} g_{s, G_k}^{t+(m-1)/\mathcal{M}})$, fluctuates during optimization. When the gradients of group $G_m$ align well with the gradients of the other groups $\{G_k\}_{i=1, i\neq m}^{\mathcal{M}}$, their dot product yields a positive value, leading to a decrease in multi-task losses. However, in practice, the sequential update strategy demonstrates a similar level of stability in optimization, which appears to contradict the conventional results. Thus, we assume a correlation between the learning of shared parameters and task-specific parameters, where the learning of task-specific parameters reduces gradient conflicts in shared parameters. Under this assumption, the sequential update strategy can guarantee convergence to Pareto-stationary points. This assumption is reasonable, as task-specific parameters capture task-specific information, thereby reducing conflicts in the shared parameters across tasks.
 
 
%-------------------------------------------------------------------------
\subsection{Two-Step Proximal Inter-task Affinity}
\label{Append:two_step_proximal_inter_task_affinity}

Before delving into the proof of Theorem 5, let's introduce the concept of two-step proximal inter-task affinity, which extends the notion of proximal inter-task affinity over two update steps.
\begin{definition}[Two-Step Proximal Inter-Task Affinity] Consider a multi-task network shared by the tasks $i, j, k$, with their respective losses denoted as $\mathcal{L}_i, \mathcal{L}_j, \mathcal{L}_k$. Sequential updates of $(\{j\}, \{i, k\})$ result in parameters being updated from $\Theta_s^{t} \rightarrow \Theta_{s|j}^{t+1} \rightarrow \Theta_{s|i,k}^{t+2}$ and $\Theta_k^{t} \rightarrow \Theta_k^{t+1} \rightarrow \Theta_k^{t+2}$. Then, the two-step proximal inter-task affinity from sequential update $(\{j\}, \{i, k\})$ to $k$ at time step $t$ is defined as follows:
\begin{align}
    \mathcal{B}^t_{j; i,k\rightarrow k} &= 1-(1-\mathcal{B}^t_{j\rightarrow k})(1-\mathcal{B}^{t+1}_{i,k\rightarrow k}) \\
    &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+1}, \Theta_k^{t+1})}{\mathcal{L}_k(z^t, \Theta_s^{t}, \Theta_k^{t})} \cdot \frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+2}, \Theta_k^{t+2})}{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+1}, \Theta_k^{t+1})} = 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+2}, \Theta_k^{t+2})}{\mathcal{L}_k(z^t, \Theta_s^{t}, \Theta_k^{t})}
\end{align}
\end{definition}

%-------------------------------------------------------------------------
\subsection{Proof of \Cref{theorem5}}
\label{Append:theorem5}

\theomfive*
\begin{proof}
We compare the loss after jointly updating three tasks $\{i, j, k\}$ with the loss after sequentially updating the task sets $\{i, k\}$ and $\{j\}$. To assess the impact of the updating order of task sets, we also conduct the analysis on the reverse order of task set $\{j\}, \{i, k\}$.

(i) Let's begin with the definition of proximal inter-task affinity between $\{i, k\}\rightarrow k$ and $j \rightarrow k$, taking into account the updates of task-specific parameters as follows:
\begin{align}
    \mathcal{B}_{i,j,k \rightarrow k}^{t+(m-1)/\mathcal{M}} &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,j,k}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s}^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})} \label{eq:theo5_joint}
\end{align}

\begin{align}
    \mathcal{B}_{i,k \rightarrow k}^{t+(m-1)/\mathcal{M}} &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s}^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})}
\end{align}
\begin{align}
    \mathcal{B}_{j\rightarrow k}^{t+m/\mathcal{M}} &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \Theta_k^{t+(m+1)/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})}\\
    &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})}
\end{align}

where $\hat{\Theta}_k^{t+m/\mathcal{M}}$ represents the resulting task-specific parameters of $k$ immediately after updating the task set $\{i, j\}$. This notation is used to differentiate it from the task-specific parameter $\Theta_k^{t+m/\mathcal{M}}$ obtained after jointly updating all tasks.

The two-step proximal inter-task affinity with the sequence $\{i, k\}$ and $\{j\}$ can be represented as follows:
\begin{align}
    \mathcal{B}_{i,k; j \rightarrow k}^{t+(m-1)/\mathcal{M}} &= 1-(1-\mathcal{B}_{i,k \rightarrow k}^{t+(m-1)/\mathcal{M}}) \cdot (1-\mathcal{B}_{j\rightarrow k}^{t+m/\mathcal{M}})\\ 
    &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s}^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})} \cdot \frac{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})}\\
    &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s}^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})} \label{eq:theo5_prox}
\end{align}

Our objective is to compare $\mathcal{B}_{i,j,k \rightarrow k}^{t+(m-1)/\mathcal{M}}$ from \cref{eq:theo5_joint} with $\mathcal{B}_{i,k; j \rightarrow k}^{t+(m-1)/\mathcal{M}}$ from \cref{eq:theo5_prox} to assess each update's effect on the final loss. Since both equations share a common denominator, we only need to compare the numerators of each equation. Using the first-order Taylor approximation of $\mathcal{L}_k(z^t, \Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})$ in \cref{eq:theo5_prox}, we have:

\begin{align}
    \mathcal{L}_k(z^t, &\Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}}) = \mathcal{L}_k (z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}}) \label{eq:theo5_middle1}\\
    &+(\Theta_{s|j}^{t+(m+1)/\mathcal{M}} - \Theta_{s|i,k}^{t+m/\mathcal{M}}) \nabla_{\Theta_{s|i,k}^{t+m/\mathcal{M}}} \mathcal{L}_k (z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}}) + O(\eta^2)\\
    =& \mathcal{L}_k (z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}}) - \eta g_{s;j}^{t+m/\mathcal{M}}\cdot g_{s;k}^{t+m/\mathcal{M}} + O(\eta^2)
\end{align}

The subscript $s$ in gradients indicates that it represents the gradients of the shared parameters of the network. Conversely, we will use the subscript $ts$ for gradients of the task-specific network in the following derivation. Similarly, $\mathcal{L}_k (z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}})$ in \cref{eq:theo5_middle1} can also be further expanded using Taylor expansion as follows:
\begin{align}
    \mathcal{L}_k &(z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}}) = \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}}) \\
    &+(\Theta_{s|i,k}^{t+m/\mathcal{M}} - \Theta_s^{t+(m-1)/\mathcal{M}}) \nabla_{\Theta_s^{t+(m-1)/\mathcal{M}}} \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})\\
    &+(\hat{\Theta}_k^{t+m/\mathcal{M}}-\Theta_k^{t+(m-1)/\mathcal{M}}) \nabla_{\Theta_k^{t+(m-1)/\mathcal{M}}} \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}}) + O(\eta^2)\\
    =& \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}}) - \eta (g_{s;i}^{t+(m-1)/\mathcal{M}} + g_{s;k}^{t+(m-1)/\mathcal{M}})\cdot g_{s;k}^{t+(m-1)/\mathcal{M}} \label{eq:theo5_middle2}\\
    &- \eta g_{ts;k}^{t+(m-1)/\mathcal{M}}\cdot g_{ts;k}^{t+(m-1)/\mathcal{M}} + O(\eta^2) \label{eq:theo5_middle3}
\end{align}

By substituting \cref{eq:theo5_middle1} with the results of \cref{eq:theo5_middle2} and \cref{eq:theo5_middle3}, we can obtain the following results:
\begin{align}
    \mathcal{L}_k(z^t, &\Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}}) = \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})\\
    &- \eta g_{s;j}^{t+m/\mathcal{M}}\cdot g_{s;k}^{t+m/\mathcal{M}} - \eta (g_{s;i}^{t+(m-1)/\mathcal{M}} + g_{s;k}^{t+(m-1)/\mathcal{M}})\cdot g_{s;k}^{t+(m-1)/\mathcal{M}} \\
    &- \eta g_{ts;k}^{t+(m-1)/\mathcal{M}}\cdot g_{ts;k}^{t+(m-1)/\mathcal{M}}+ O(\eta^2)
\end{align}

For the scenario where all tasks $\{i, j, k\}$ are jointly updated, the numerator of \cref{eq:theo5_joint} can also be expanded as follows:
\begin{align}
    \mathcal{L}_k(z^t,& \Theta_{s|i,j,k}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}}) = \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})\\
    &+(\Theta_{s|i,j,k}^{t+m/\mathcal{M}} - \Theta_s^{t+(m-1)/\mathcal{M}}) \nabla_{\Theta_s^{t+(m-1)/\mathcal{M}}} \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})\\
    &+(\Theta_k^{t+m/\mathcal{M}} - \Theta_k^{t+(m-1)/\mathcal{M}}) \nabla_{\Theta_k^{t+(m-1)/\mathcal{M}}} \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}}) + O(\eta^2)\\
    =& \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}}) \\
    &- \eta (g_{s,i}^{t+(m-1)/\mathcal{M}} + g_{s,j}^{t+(m-1)/\mathcal{M}} + g_{s,k}^{t+(m-1)/\mathcal{M}})\cdot g_{s,k}^{t+(m-1)/\mathcal{M}} \\
    &- \eta g_{ts,k}^{t+(m-1)/\mathcal{M}}\cdot g_{ts,k}^{t+(m-1)/\mathcal{M}} + O(\eta^2)
\end{align}

Finally, we can compare $\mathcal{B}_{i,j,k \rightarrow k}^{t+(m-1)/\mathcal{M}}$ with $\mathcal{B}_{i,k; j \rightarrow k}^{t+(m-1)/\mathcal{M}}$ by comparing the losses we obtained: $\mathcal{L}_k(z^t, \Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})$ with $\mathcal{L}_k(z^t, \Theta_{s|i,j,k}^{t+m/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})$. We assume a sufficiently small learning rate $\eta$ that allows us to ignore terms larger than order two with $\eta$.

\begin{align}
    \mathcal{L}_k(z^t, & \Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+m/\mathcal{M}}) - \mathcal{L}_k(z^t, \Theta_{s|i,j,k}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}})\\
    =& - \eta g_{s,j}^{t+m/\mathcal{M}}\cdot g_{s,k}^{t+m/\mathcal{M}} - \eta (g_{s,i}^{t+(m-1)/\mathcal{M}} + g_{s,k}^{t+(m-1)/\mathcal{M}})\cdot g_{s,k}^{t+(m-1)/\mathcal{M}} \\
    & - \eta g_{ts,k}^{t+(m-1)/\mathcal{M}}\cdot g_{ts,k}^{t+(m-1)/\mathcal{M}}\\
    &+ \eta (g_{s,i}^{t+(m-1)/\mathcal{M}} + g_{s,j}^{t+(m-1)/\mathcal{M}} + g_{s,k}^{t+(m-1)/\mathcal{M}})\cdot g_{s,k}^{t+(m-1)/\mathcal{M}}\\
    & + \eta g_{ts,k}^{t+(m-1)/\mathcal{M}}\cdot g_{ts,k}^{t+(m-1)/\mathcal{M}}\\
    =& \eta(g_{s,j}^{t+(m-1)/\mathcal{M}} \cdot g_{s,k}^{t+(m-1)/\mathcal{M}} - g_{s,j}^{t+m/\mathcal{M}} \cdot g_{s,k}^{t+m/\mathcal{M}})\\
    \simeq& 0 \label{eq:theo5_last1}
\end{align}

The approximation in \cref{eq:theo5_last1} holds as we assume inter-task affinity change during a single time step from $t+(m-1)/\mathcal{M}$ to $t+m/\mathcal{M}$ is negligible.


(ii) In case we update task groups in reverse order the results would differ with (i). Similarly, we begin with the definition of proximal inter-task affinity with reverse order between $j \rightarrow k$ and $\{i, j\}\rightarrow k$ as follows:

\begin{align}
    \mathcal{B}_{j\rightarrow k}^{t+(m-1)/\mathcal{M}} &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})}\\
    &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})}
\end{align}

\begin{align}
    \mathcal{B}_{i,k \rightarrow k}^{t+m/\mathcal{M}} &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+(m+1)/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}})} \\
    &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+(m+1)/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})}
\end{align}

The two-step proximal inter-task affinity with the sequence $\{j\}$ and $\{i, k\}$ can be represented as follows:
\begin{align}
    \mathcal{B}_{j; i,k \rightarrow k}^{t+(m-1)/\mathcal{M}} &= 1- (1-\mathcal{B}_{j\rightarrow k}^{t+(m-1)/\mathcal{M}}) \cdot (1-\mathcal{B}_{i,k \rightarrow k}^{t+m/\mathcal{M}})\\ 
    &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})} \cdot \frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+(m+1)/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})}\\
    &= 1-\frac{\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+(m+1)/\mathcal{M}})}{\mathcal{L}_k(z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})} \label{eq:theo5_prox2}
\end{align}

Our objective is to compare $\mathcal{B}_{i,j,k \rightarrow k}^{t+(m-1)/\mathcal{M}}$ from \cref{eq:theo5_joint} with $\mathcal{B}_{j; i,k \rightarrow k}^{t+(m-1)/\mathcal{M}}$ from \cref{eq:theo5_prox2} to assess each update's effect on the final loss. Since both equations share a common denominator, we only need to compare the numerators of each equation. Using the first-order Taylor approximation of $\mathcal{L}_k(z^t, \Theta_{s|i,k}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+(m+1)/\mathcal{M}})$ in \cref{eq:theo5_prox2}, we have:

\begin{align}
    \mathcal{L}_k(z^t, &\Theta_{s|i,k}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+(m+1)/\mathcal{M}}) = \mathcal{L}_k (z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}}) \label{eq:theo5_middle1_}\\
    &+(\Theta_{s|i,k}^{t+(m+1)/\mathcal{M}} - \Theta_{s|j}^{t+m/\mathcal{M}}) \nabla_{\Theta_{s|j}^{t+m/\mathcal{M}}} \mathcal{L}_k (z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}})\\
    &+(\hat{\Theta}_k^{t+(m+1)/\mathcal{M}} - \Theta_k^{t+m/\mathcal{M}}) \nabla_{\Theta_k^{t+m/\mathcal{M}}} \mathcal{L}_k (z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}}) + O(\eta^2)\\
    =& \mathcal{L}_k (z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}}) - \eta (g_{s;i}^{t+m/\mathcal{M}}+g_{s;k}^{t+m/\mathcal{M}})\cdot g_{s;k}^{t+m/\mathcal{M}} \\
    &- \eta g_{ts;k}^{t+m/\mathcal{M}}\cdot g_{ts;k}^{t+m/\mathcal{M}} + O(\eta^2)
\end{align}

Similarly, $\mathcal{L}_k (z^t, \Theta_{s|i,k}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}})$ in \cref{eq:theo5_middle1_} can also be further expanded using Taylor expansion as follows:
\begin{align}
    \mathcal{L}_k(z^t, &\Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}}) = \mathcal{L}_k(z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}}) \\
    =& \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})\\
    &+(\Theta_{s|j}^{t+m/\mathcal{M}} - \Theta_s^{t+(m-1)/\mathcal{M}}) \nabla_{\Theta_s^{t+(m-1)/\mathcal{M}}} \mathcal{L}_k (z^t, \Theta_{s|j}^{t+m/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}}) \\
    =& \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}}) \label{eq:theo5_middle2_}\\
    &- \eta g_{s;j}^{t+(m-1)/\mathcal{M}}\cdot g_{s;k}^{t+(m-1)/\mathcal{M}}+ O(\eta^2)
    \label{eq:theo5_middle3_}
\end{align}

By substituting \cref{eq:theo5_middle1_} with the results of \cref{eq:theo5_middle2_} and \cref{eq:theo5_middle3_}, we can obtain the following results:
\begin{align}
    \mathcal{L}_k(z^t, &\Theta_{s|i,k}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+(m+1)/\mathcal{M}}) = \mathcal{L}_k (z^t, \Theta_s^{t+(m-1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})\\
    &- \eta (g_{s;i}^{t+m/\mathcal{M}}+g_{s;k}^{t+m/\mathcal{M}})\cdot g_{s;k}^{t+m/\mathcal{M}} - \eta g_{ts;k}^{t+m/\mathcal{M}}\cdot g_{ts;k}^{t+m/\mathcal{M}} \\
    &- \eta g_{s;j}^{t+(m-1)/\mathcal{M}}\cdot g_{s;k}^{t+(m-1)/\mathcal{M}}+ O(\eta^2)
\end{align}

Finally, we can compare $\mathcal{B}_{i,j,k \rightarrow k}^{t+(m-1)/\mathcal{M}}$ with $\mathcal{B}_{j; i,k \rightarrow k}^{t+(m-1)/\mathcal{M}}$ by comparing the losses we obtained: $\mathcal{L}_k(z^t, \Theta_{s|j}^{t+(m+1)/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})$ with $\mathcal{L}_k(z^t, \Theta_{s|i,j,k}^{t+m/\mathcal{M}}, \Theta_k^{t+(m-1)/\mathcal{M}})$. We assume a sufficiently small learning rate $\eta$ that allows us to ignore terms larger than order two with $\eta$.

\begin{align}
    \mathcal{L}_k(z^t, &\Theta_{s|i,k}^{t+(m+1)/\mathcal{M}}, \hat{\Theta}_k^{t+(m+1)/\mathcal{M}}) - \mathcal{L}_k(z^t, \Theta_{s|i,j,k}^{t+m/\mathcal{M}}, \Theta_k^{t+m/\mathcal{M}})\\
    =& - \eta (g_{s;i}^{t+m/\mathcal{M}}+g_{s;k}^{t+m/\mathcal{M}})\cdot g_{s;k}^{t+m/\mathcal{M}} - \eta g_{ts;k}^{t+m/\mathcal{M}}\cdot g_{ts;k}^{t+m/\mathcal{M}} \\
    &- \eta g_{s;j}^{t+(m-1)/\mathcal{M}}\cdot g_{s;k}^{t+(m-1)/\mathcal{M}}\\
    &+ \eta (g_{s,i}^{t+(m-1)/\mathcal{M}} + g_{s,j}^{t+(m-1)/\mathcal{M}} + g_{s,k}^{t+(m-1)/\mathcal{M}})\cdot g_{s,k}^{t+(m-1)/\mathcal{M}}\\
    &+ \eta g_{ts,k}^{t+(m-1)/\mathcal{M}}\cdot g_{ts,k}^{t+(m-1)/\mathcal{M}}\\
    =& - \eta (g_{s;i}^{t+m/\mathcal{M}}+g_{s;k}^{t+m/\mathcal{M}})\cdot g_{s;k}^{t+m/\mathcal{M}} + \eta (g_{s,i}^{t+(m-1)/\mathcal{M}} + g_{s,k}^{t+(m-1)/\mathcal{M}})\cdot g_{s,k}^{t+(m-1)/\mathcal{M}} \\
    & - \eta g_{ts;k}^{t+m/\mathcal{M}}\cdot g_{ts;k}^{t+m/\mathcal{M}} + \eta g_{ts,k}^{t+(m-1)/\mathcal{M}}\cdot g_{ts,k}^{t+(m-1)/\mathcal{M}} \\
    \simeq& - \eta g_{ts;k}^{t+m/\mathcal{M}}\cdot g_{ts;k}^{t+m/\mathcal{M}} + \eta g_{ts,k}^{t+(m-1)/\mathcal{M}}\cdot g_{ts,k}^{t+(m-1)/\mathcal{M}} \label{eq:theo5_last2}\\
    \leq& 0 \label{eq:theo5_final}
\end{align}

The approximation in \cref{eq:theo5_last2} holds under the assumption that the change in inter-task affinity during a single time step from $t+(m-1)/\mathcal{M}$ to $t+m/\mathcal{M}$ is negligible. Since we assume convex loss functions, the magnitude of task-specific gradients $g_{ts;k}$ would increase after updating the loss of $j$, which exhibits negative inter-task affinity with $k$ ($\mathcal{A}_{j \rightarrow k}<0$). Therefore, the inequality in \cref{eq:theo5_final} holds.
\end{proof}

This suggest that grouping tasks with proximal inter-task affinity and subsequently updating these groups sequentially result in lower multi-task loss compared to jointly backpropagating all tasks. This disparity arises because the network can discern superior task-specific parameters to accommodate task-specific information during sequential learning.


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Related Works}
\textbf{Task Grouping.} Early Multi-Task Learning research is founded on the belief that simultaneous learning of similar tasks within a multi-task framework can enhance overall performance. Kang et al. \cite{kang2011learning} identify tasks that contribute to improved multi-task performance through the clustering of related tasks with online stochastic gradient descent. This strategy challenges the prevailing assumption that all tasks are inherently interrelated. In parallel, Kumar et al. \cite{kumar2012learning} present a framework for MTL designed to enable selective sharing of information across tasks. They suggests that each task parameter vector can be expressed as a linear combination of a finite number of underlying basis tasks. However, these initial methodologies face limitations in their applicability and analysis, particularly in scaling to deep neural networks.
Finding out related tasks is more dynamically explored in the transfer learning domain \cite{achille2019task2vec, achille2021information}. They find related tasks by measuring task similarity which can be comparing the similarity of features extracted from the same depth of the independent task's network or directly measuring the transfer performance between tasks. Recent research has concentrated on identifying related tasks by directly assessing the relations among them within shared networks. This focus stems from the recognition that the measured inter-task relations in transfer learning fail to fully elucidate the dynamics within the MTL domain \cite{standley2020tasks, fifty2021efficiently}.

\textbf{Multi-Task Architectures.} Multi-task architectures can be classified based on how much the parameters or features are shared across tasks in the network. The most commonly used structure is a shared trunk which consists of a common encoder shared by multiple tasks and a dedicated decoder for each task \cite{RN51, RN52, RN49, RN50}. A tree-like architecture, featuring multiple division points for each task group, offers a more generalized structure \cite{treelike1, treelike2, treelike3, treelike4}. The cross-talk architecture employs separate symmetrical networks for each task, facilitating feature exchange between layers at the same depth for information sharing between tasks \cite{RN43, RN29}. The prediction distillation model \cite{RN9, RN29, RN32, pap} incorporates cross-task interactions at the end of the shared encoder, while the task switching network \cite{RN30, RN40, RN42, RN2} adjusts network parameters depending on the task.

\noindent\textbf{MTL in Vision Transformers.} Recent advancements in multi-task architecture have explored the integration of Vision Transformer \cite{vit, swin, pvt, focal, segformer, crossformer} into MTL. MTFormer \cite{mtformer} adopts a shared transformer encoder and decoder with a cross-task attention mechanism. MulT \cite{mult} leverages a shared attention mechanism to capture task dependencies, inspired by the Swin transformer. InvPT \cite{invpt} emphasizes global spatial position and multi-task context for dense prediction tasks through multi-scale feature aggregation. The Mixture of Experts (MoE) divides the model into predefined expert groups, dynamically shared or dedicated to specific tasks during the learning phase \cite{riquelme2021scaling, zhang2022mixture, fan2022m3vit, mustafa2022multimodal, chen2023mod, ye2023taskexpert}. Task prompter \cite{xu2023multi, xu2023demt, ye2022taskprompter} employs task-specific tokens to encapsulate task-specific information and utilizes cross-task interactions to enhance multi-task performance. 

\noindent\textbf{Multi-Task Domain Generalization.} Task grouping based on their relations has also been explored in the field of domain adaptation. In particular, \citep{wei2024task} proposes grouping heterogeneous tasks to regularize them, thereby promoting the learning of more generalized features across domain shifts. \citep{smith2021origin} explores generalization strategies at the mini-batch level. \citep{li2020sequential} addresses diverse domain shift scenarios by incorporating all possible sequential domain learning paths to generalize features for unseen domains. \citep{shi2021gradient} focuses on generalization to unseen domains by reducing dependence on specific domains through inter-domain gradient matching. Additionally, \citep{hu2022improving} analyzes the problem of spurious correlations in MTL and proposes regularization methods to mitigate this issue. The effect of gradient conflicts, which are considered the primary cause of negative transfer between tasks, is thoroughly examined in \citep{jiang2024forkmerge}. This work also proposes combining task distributions to identify better network parameters from a generalization perspective. The objectives of conventional multi-task optimization and domain generalization differ fundamentally. Conventional multi-task optimization typically assumes that the source and target domains share similar data distributions. In contrast, domain generalization focuses on scenarios involving significant domain shifts. This distinction leads to different approaches in leveraging task relations to achieve their respective goals. In multi-task optimization, simultaneously updating heterogeneous tasks with conflicting gradients results in suboptimal optimization. On the other hand, domain generalization leverages task sets as a tool to extract generalized features applicable to various unseen domains. Overfitting to similar tasks can harm performance on unseen domains, making it advantageous to use heterogeneous tasks as a form of regularization.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Details}
\label{append:experimental_details}
\setcounter{table}{0}
\setcounter{figure}{0}
We implement our experiments on top of publically available code from \cite{ye2022invpt}. We run our experiments on A6000 GPUs.

\textbf{Datasets.} We assess our method on multi-task datasets: NYUD-v2 \cite{RN15}, PASCAL-Context \cite{mottaghi2014role}, and Taskonomy \cite{zamir2018taskonomy}. These datasets encompass various vision tasks. NYUD-v2 comprises 4 vision tasks: depth estimation, semantic segmentation, surface normal prediction, and edge detection. Meanwhile, PASCAL-Context includes 5 tasks: semantic segmentation, human parts estimation, saliency estimation, surface normal prediction, and edge detection. In Taskonomy, we use 11 vision tasks: Depth Euclidean (DE), Depth Zbuffer (DZ), Edge Texture (ET),  Keypoints 2D (K2), Keypoints 3D (K3), Normal (N), Principal Curvature (C), Reshading (R), Segment Unsup2d (S2), and Segment Unsup2.5D (S2.5).

\textbf{Metrics. } To assess task performance, we employed widely used metrics across different tasks. For semantic segmentation, we utilized mean Intersection over Union (mIoU). The performance of surface normal prediction was gauged by computing the mean angle distances between the predicted output and ground truth. Depth estimation task performance was evaluated using Root Mean Squared Error (RMSE). For saliency estimation and human part segmentation, we utilized mean Intersection over Union (mIoU). Edge detection performance was assessed using optimal-dataset-scale-F-measure (odsF). For the Taskonomy benchmark, curvature was evaluated using RMSE, while the other tasks were evaluated using L1 distance, following the settings in \cite{chen2023mod}. 

To evaluate multi-task performance, we adopted the metric proposed in \cite{RN2}. This metric measures per-task performance by averaging it with respect to the single-task baseline $b$, as shown in the equation: $\triangle_m = (1/T)\sum_{i=1}^{T}(-1)^{l_i}(M_{m,i}-M_{b,i})/M_{b,i}$ where $l_i=1$ if a lower value of measure $M_i$ means better performance for task $i$, and 0 otherwise.


%-------------------------------------------------------------------------
\begin{table*}[h]
\caption{Hyperparameters for experiments.}
\centering
\renewcommand\arraystretch{1.20}
\begin{tabular}{lc}
\hline
Hyperparameter                  &  Value \\ \hline
Optimizer                       &  Adam \cite{kingma2014adam}\\
Scheduler                       &  Polynomial Decay\\
Minibatch size                  &  8\\
Number of iterations            &  40000\\
Backbone (Transformer)                        &  ViT \cite{vit} \\
\hspace{10pt}$\llcorner$ Learning rate                   &  0.00002\\
\hspace{10pt}$\llcorner$ Weight Decay                    &  0.000001\\
\hspace{10pt}$\llcorner$ Affinity decay factor $\beta$   &  0.001\\
\hline
\end{tabular}
\label{Implementation_details}
\end{table*}
%-------------------------------------------------------------------------


\textbf{Implementation Details.} For experiments, we adopt ViT \cite{vit} pre-trained on ImageNet-22K \cite{deng2009imagenet} as the multi-task encoder.
Task-specific decoders merge the multi-scale features extracted by the encoder to generate the outputs for each task. The models are trained for 40,000 iterations on both NYUD \cite{RN15} and PASCAL \cite{RN12} datasets with batch size 8. We used Adam optimizer with learning rate $2\times$$10^{-5}$ and $1\times$$10^{-6}$ of a weight decay with a polynomial learning rate schedule. The cross-entropy loss was used for semantic segmentation, human parts estimation, and saliency, edge detection. Surface normal prediction and depth estimation used L1 loss. The tasks are weighted equally to ensure a fair comparison. For the Taskonomy Benchmark \cite{zamir2018taskonomy}, we use the dataloader from the open-access code provided by \cite{chen2023mod}, while maintaining experimental settings identical to those used for NYUD-v2 and PASCAL-Context. We use the same experimental setup for the other hyperparameters as in previous works \cite{invpt, ye2022taskprompter}, as detailed in \Cref{Implementation_details}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \clearpage
\section{Additional Experimental Results}
\label{append:additional_experimental_results}
\setcounter{table}{0}
\setcounter{figure}{0}

\textbf{Comparison of optimization results with different backbone sizes.} We compare the results of multi-task optimization on Taskonomy across various sizes of vision transformers, as shown in \Cref{tab:tab_exp_taskonomy_vitB,tab:tab_exp_taskonomy_vitS,tab:tab_exp_taskonomy_vitT}. Our method consistently achieves superior performance across all backbone sizes. Unlike previous approaches that focus on learning shared parameters, our optimization strategy enhances the learning of task-specific parameters. This leads to significant performance improvements, especially with smaller backbones, where competition between tasks is more intense due to the limited number of shared parameters. How tasks are grouped, as visualized in \cref{append:fig:vis_grouping}, depends on the backbone size.

\textbf{Visualization of Proximal Inter-Task Affinity.} In \Cref{fig:proximal_vit_taskonomy}, we present the tracked proximal inter-task affinity for each pair of tasks in Taskonomy. The changes in proximal inter-task affinity depend on the nature of the task pair, but as the backbone size increases, the affinity tends to become more positive. This trend is more noticeable in NYUD-v2 and PASCAL-Context, where there are fewer tasks, as shown in \Cref{fig:proximal_vit_nyud,fig:proximal_vit_pascal}. This pattern also aligns with the number of clustered tasks in \Cref{fig:num_group}, where the number of groups increases as the backbone size decreases.

\textbf{Effect of the Decay Rate with Visualization.} In \Cref{fig:proximal_vit_beta}, we visualize the proximal inter-task affinity tracked during optimization with various decay rates $\beta$, ranging from 0.01 to 1e-5 on a logarithmic scale. The decay rate $\beta$ helps stabilize the tracking of proximal inter-task affinity as it fluctuates during optimization. Additionally, it aids in understanding inter-task relations over time, independent of input data. For vision transformers, a decay rate of $\beta=0.001$ demonstrates stable tracking. In real-world applications, multi-task performance is not highly sensitive to the decay rate $\beta$. In \Cref{tab:tab_exp_beta_perf}, we evaluate how $\beta$ impacts multi-task performance on the Taskonomy benchmark. The results demonstrate that the proposed optimization method consistently improves performance across various $\beta$ values, minimizing the need for extensive hyperparameter tuning in practical scenarios.

\noindent\textbf{The Influence of Task Grouping Strategy.}  
In \Cref{tab:tab_exp_grouping_strategy}, we present results comparing different task grouping strategies. These include randomly grouping tasks with a predefined number, grouping heterogeneous tasks, and grouping homogeneous tasks (our approach). The results clearly demonstrate that grouping homogeneous task sets yields superior performance under the proposed settings. This contrasts with the multi-task domain generalization approach, which groups heterogeneous tasks as a form of regularization to enhance generalization to unseen domains. 
This difference arises from the fundamentally distinct objectives of conventional multi-task optimization and domain generalization. Conventional multi-task optimization typically assumes that the source and target domains share similar data distributions, while domain generalization addresses scenarios with significant domain shifts. Consequently, the approaches to leveraging task relations differ to meet these distinct goals. As demonstrated in Theorems 1 and 2 of our work, in multi-task optimization, simultaneously updating heterogeneous tasks with low task affinity leads to suboptimal optimization and higher losses compared to updating similar task sets with high task affinity. This observation aligns with findings from previous multi-task optimization studies referenced in the related works section.

\noindent\textbf{Influence of Batch Size.}  
In \Cref{tab:tab_exp_batch}, we compare our method with single-gradient descent (GD) to evaluate its robustness in improving multi-task performance across varying batch sizes. The proposed optimization method consistently demonstrates performance improvements (\(\triangle_m\) (\% \(\uparrow\))) of 5.27\%, 5.71\%, and 6.13\% across different batch sizes. These results highlight the robustness and adaptability of the proposed algorithm across diverse scenarios.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
    \vspace{-10pt}
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/group_viz_3.png}
        \vspace*{-15pt}
        \caption{$\{G\}_{i=1}^{\mathcal{M}}$ with ViT-L}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/group_viz_2.png}
        \vspace*{-15pt}
        \caption{$\{G\}_{i=1}^{\mathcal{M}}$ with ViT-B}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/group_viz_1.png}
        \vspace*{-15pt}
        \caption{$\{G\}_{i=1}^{\mathcal{M}}$ with ViT-S}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/group_viz_0.png}
        \vspace*{-15pt}
        \caption{$\{G\}_{i=1}^{\mathcal{M}}$ with ViT-T}
    \end{subfigure}
    \caption{The averaged grouping results $\{G\}_{i=1}^{\mathcal{M}}$ on the Taskonomy benchmark are shown for (a) ViT-L, (b) ViT-B, (c) ViT-S, and (d) ViT-T, respectively.}
    \label{append:fig:vis_grouping}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Comparison with previous multi-task optimization approaches on Taskonomy with ViT-B.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{l|ccccccccccc|c}
\midrule[1.0pt]
 & DE & DZ & EO & ET & K2  & K3 & N   & C & R & S2  & S2.5 &  \\ \cmidrule[0.5pt]{2-12}
\multirow{-2}{*}{Task} & L1 Dist. $\downarrow$  & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & RMSE $\downarrow$    & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$  & \multirow{-2}{*}{$\triangle_m$ ($\uparrow$)} \\ \midrule[1.0pt]

Single Task     &0.0183&0.0186&0.1089&0.1713&0.1630&0.0863&0.2953&0.7522&0.1504&0.1738&0.1530&-    \\ \midrule[0.5pt]
GD              &0.0188&0.0197&0.1283&0.1745&0.1718&0.0933&0.2599&0.7911&0.1799&0.1885&0.1631&-6.35     \\
GradDrop        &0.0195&0.0206&0.1318&0.1748&0.1735&0.0945&0.3018&0.8060&0.1866&0.1920&0.1607&-9.54     \\
MGDA            &-&-&-&-&-&-&-&-&-&-&-&-     \\
UW              &0.0188&0.0198&0.1285&0.1745&0.1719&0.0933&0.2535&0.7915&0.1800&0.1883&0.1629&-6.19     \\
DTP             &0.0187&0.0198&0.1283&0.1745&0.1720&0.0933&0.2558&0.7912&0.1804&0.1884&0.1634&-6.25     \\
DWA             &0.0188&0.0197&0.1287&0.1745&0.1719&0.0933&0.2570&0.7927&0.1806&0.1887&0.1632&-6.33     \\
PCGrad          &0.0185&0.0188&0.1285&0.1738&0.1703&0.0928&0.2557&0.7964&0.1810&0.1882&0.1569&-5.22     \\
CAGrad          &0.0192&0.0196&0.1306&0.1733&0.1654&0.0939&0.2871&0.8147&0.1901&0.1906&0.1659&-8.34    \\
IMTL            &0.0189&0.0200&0.1287&0.1745&0.1720&0.0934&0.2618&0.7928&0.1811&0.1888&0.1635&-6.75     \\
Aligned-MTL     &0.0191&0.0202&0.1263&0.1729&0.1663&0.0944&0.3061&0.8560&0.1936&0.1872&0.1585&-8.93     \\
Nash-MTL        &0.0175&0.0182&0.1208&0.1730&0.1663&0.0901&0.2686&0.7958&0.1707&0.1839&0.1577&-2.79     \\
FAMO            &0.0189&0.0200&0.1285&0.1745&0.1720&0.0934&0.2715&0.7929&0.1807&0.1891&0.1640&-7.21     \\
Ours            &0.0167&0.0169&0.1228&0.1739&0.1695&0.0910&0.2344&0.7600&0.1691&0.1836&0.1571&-0.64     \\  \midrule[1.0pt]
\end{tabular}}
\label{tab:tab_exp_taskonomy_vitB}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Comparison with previous multi-task optimization approaches on Taskonomy with ViT-S.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{l|ccccccccccc|c}
\midrule[1.0pt]
 & DE & DZ & EO & ET & K2  & K3 & N   & C & R & S2  & S2.5 &  \\ \cmidrule[0.5pt]{2-12}
\multirow{-2}{*}{Task} & L1 Dist. $\downarrow$  & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & RMSE $\downarrow$    & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$  & \multirow{-2}{*}{$\triangle_m$ ($\uparrow$)} \\ \midrule[1.0pt]
Single Task     &0.0264&0.0259&0.1348&0.1740&0.1667&0.0973&0.3481&0.8598&0.1905&0.1857&0.1691&-    \\ \midrule[0.5pt]
GD              &0.0264&0.0272&0.1574&0.1775&0.1838&0.1038&0.4370&0.9237&0.2475&0.2076&0.1858&-11.39     \\
GradDrop        &0.0274&0.0280&0.1609&0.1779&0.1856&0.1042&0.4472&0.9366&0.2549&0.2106&0.1821&-13.14     \\
MGDA            &-&-&-&-&-&-&-&-&-&-&-&-     \\
UW              &0.0263&0.0269&0.1570&0.1775&0.1832&0.1037&0.4362&0.9202&0.2465&0.2075&0.1856&-11.11     \\
DTP             &0.0262&0.0273&0.1568&0.1778&0.1831&0.1037&0.4884&0.9207&0.2466&0.2073&0.1849&-12.52     \\
DWA             &0.0264&0.0271&0.1572&0.1776&0.1834&0.1038&0.4336&0.9215&0.2469&0.2075&0.1856&-11.20     \\
PCGrad          &0.0271&0.0274&0.1570&0.1766&0.1784&0.1034&0.4522&0.9343&0.2525&0.2071&0.1811&-11.78     \\
CAGrad          &0.0289&0.0282&0.1611&0.1769&0.1706&0.1062&0.4723&0.9557&0.2689&0.2122&0.1902&-15.09     \\
IMTL-L          &0.0255&0.0258&0.1510&0.1744&0.1716&0.1005&0.4339&0.9459&0.2466&0.2036&0.1825&-8.90     \\
Aligned-MTL     &0.0286&0.0290&0.1603&0.1744&0.1711&0.1033&0.4596&1.0022&0.2783&0.2090&0.1854&-15.06     \\
Nash-MTL        &0.0255&0.0258&0.1510&0.1744&0.1716&0.1005&0.4339&0.9459&0.2466&0.2036&0.1825&-8.79     \\
FAMO            &0.0263&0.0272&0.1573&0.1774&0.1835&0.1035&0.4326&0.9208&0.2464&0.2077&0.1858&-11.23     \\
Ours            &0.0225&0.0229&0.1444&0.1762&0.1775&0.0995&0.3983&0.8620&0.2156&0.1997&0.1774&-2.83     \\  \midrule[1.0pt]
\end{tabular}}
\label{tab:tab_exp_taskonomy_vitS}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Comparison with previous multi-task optimization approaches on Taskonomy with ViT-T.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{l|ccccccccccc|c}
\midrule[1.0pt]
 & DE & DZ & EO & ET & K2  & K3 & N   & C & R & S2  & S2.5 &  \\ \cmidrule[0.5pt]{2-12}
\multirow{-2}{*}{Task} & L1 Dist. $\downarrow$  & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & RMSE $\downarrow$    & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$  & \multirow{-2}{*}{$\triangle_m$ ($\uparrow$)} \\ \midrule[1.0pt]
Single Task     &0.0289&0.0290&0.1405&0.1774&0.1682&0.0970&0.3837&0.8968&0.2096&0.1904&0.1729&-    \\ \midrule[0.5pt]
GD              &0.0279&0.0285&0.1604&0.1789&0.1860&0.1043&0.4704&0.9488&0.2613&0.2086&0.1914&-9.21     \\
GradDrop        &0.0287&0.0292&0.1630&0.1795&0.1868&0.1052&0.4795&0.9621&0.2697&0.2118&0.1878&-10.68     \\
MGDA            &-&-&-&-&-&-&-&-&-&-&-&-     \\
UW              &0.0279&0.0285&0.1604&0.1789&0.1859&0.1043&0.4699&0.9488&0.2613&0.2085&0.1914&-9.21     \\
DTP             &0.0278&0.0288&0.1603&0.1790&0.1859&0.1042&0.4697&0.9488&0.2614&0.2088&0.1915&-9.27     \\
DWA             &0.0279&0.0285&0.1604&0.1789&0.1859&0.1043&0.4693&0.9489&0.2613&0.2086&0.1913&-9.20     \\
PCGrad          &0.0283&0.0290&0.1604&0.1769&0.1803&0.1036&0.4720&0.9645&0.2683&0.2090&0.1866&-9.28     \\
CAGrad          &0.0300&0.0304&0.1644&0.1743&0.1721&0.1055&0.4838&0.9818&0.2847&0.2143&0.1974&-12.12     \\
IMTL            &0.0276&0.0282&0.1553&0.1754&0.1743&0.1018&0.4621&0.9809&0.2623&0.2051&0.1878&-7.55     \\
Aligned-MTL     &0.0296&0.0318&0.1633&0.1765&0.1757&0.1150&0.4806&1.0270&0.2935&0.2109&0.1887&-13.70     \\
Nash-MTL        &0.0276&0.0282&0.1553&0.1754&0.1743&0.1018&0.4621&0.9809&0.2623&0.2051&0.1878&-7.46    \\
FAMO            &0.0279&0.0285&0.1604&0.1789&0.1859&0.1043&0.4718&0.9488&0.2612&0.2085&0.1913&-9.33     \\
Ours            &0.0252&0.0257&0.1526&0.1774&0.1827&0.1019&0.4337&0.9100&0.2402&0.2026&0.1845&-3.67     \\  \midrule[1.0pt]
\end{tabular}}
\label{tab:tab_exp_taskonomy_vitT}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% %-------------------------------------------------------------------------
\def\figlength{0.14}
\begin{figure}[h]
\centering  
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DE_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/DZ_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/EO_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/ET_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K2_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/K3_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_K3.png}
\end{subfigure}
\caption{Changes in proximal inter-task affinity during the optimization process of ViT-L with Taskonomy benchmark.}
\end{figure}

\begin{figure}[h]\ContinuedFloat
\centering
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/N_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_S2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/R_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2_to_S2.5.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_DE.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_DZ.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_EO.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_ET.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_K2.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_K3.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_N.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_C.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_R.png}
\end{subfigure}
\begin{subfigure}{\figlength\textwidth}
\includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/S2.5_to_S2.png}
\end{subfigure}
\caption{Changes in proximal inter-task affinity during the optimization process with Taskonomy benchmark.}
\label{fig:proximal_vit_taskonomy}
\end{figure}
\clearpage


%-------------------------------------------------------------------------
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/semseg_to_depth.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/semseg_to_normals.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/semseg_to_edge.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/depth_to_semseg.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/depth_to_normals.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/depth_to_edge.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/normals_to_semseg.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/normals_to_depth.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/normals_to_edge.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/edge_to_semseg.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/edge_to_depth.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_nyud/edge_to_normals.png}
    \end{subfigure}
    \caption{Changes in the proximal inter-task affinity during the optimization process of different sizes of ViT with NYUD-v2.}
    \label{fig:proximal_vit_nyud}
\end{figure}
%-------------------------------------------------------------------------
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/semseg_to_human_parts.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/semseg_to_sal.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/semseg_to_normals.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/semseg_to_edge.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/human_parts_to_semseg.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/human_parts_to_sal.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/human_parts_to_normals.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/human_parts_to_edge.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/sal_to_semseg.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/sal_to_human_parts.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/sal_to_normals.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/sal_to_edge.png}
    \end{subfigure}
        \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/normals_to_semseg.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/normals_to_human_parts.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/normals_to_sal.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/normals_to_edge.png}
    \end{subfigure}
        \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/edge_to_semseg.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/edge_to_human_parts.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/edge_to_sal.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_pascal/edge_to_normals.png}
    \end{subfigure}
    \caption{Changes in proximal inter-task affinity during the optimization process of different sizes of ViT with PASCAL-Context.}
    \label{fig:proximal_vit_pascal}
\end{figure}



\begin{figure}[h]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/semseg_to_depth.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/semseg_to_normals.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/semseg_to_edge.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/depth_to_semseg.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/depth_to_normals.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/depth_to_edge.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/normals_to_semseg.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/normals_to_depth.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/normals_to_edge.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/edge_to_semseg.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/edge_to_depth.png}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/edge_to_normals.png}
    \end{subfigure}
    \caption{Changes in proximal inter-task affinity during the optimization process with different decay rates, $\beta$.}
    \label{fig:proximal_vit_beta}
\end{figure}



\begin{table*}[h]
\caption{Results on Taskonomy with different affinity decay rates $\beta$.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{l|ccccccccccc|c}
\midrule[1.0pt]
 & DE & DZ & EO & ET & K2  & K3 & N   & C & R & S2  & S2.5 &  \\ \cmidrule[0.5pt]{2-12}
\multirow{-2}{*}{Task} & L1 Dist. $\downarrow$  & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & RMSE $\downarrow$    & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$  & \multirow{-2}{*}{$\triangle_m$ ($\uparrow$)} \\ \midrule[1.0pt]
Single Task     &0.0183&0.0186&0.1089&0.1713&0.1630&0.0863&0.2953&0.7522&0.1504&0.1738&0.1530&-         \\\midrule[0.5pt]
GD              &0.0188&0.0197&0.1283&0.1745&0.1718&0.0933&0.2599&0.7911&0.1799&0.1885&0.1631&-6.35     \\
$\beta$=0.0001  &0.0165&0.0168&0.1224&0.1739&0.1693&0.0907&0.2304&0.7581&0.1683&0.1831&0.1571&-0.18     \\
$\beta$=0.001   &0.0167&0.0169&0.1228&0.1739&0.1695&0.0910&0.2344&0.7600&0.1691&0.1836&0.1571&-0.64     \\
$\beta$=0.01    &0.0167&0.0171&0.1232&0.1739&0.1698&0.0912&0.2362&0.7623&0.1705&0.1834&0.1576&-1.01     \\
$\beta$=0.1     &0.0167&0.0171&0.1231&0.1739&0.1695&0.0912&0.2355&0.7631&0.1697&0.1831&0.1575&-0.87     \\\midrule[1.0pt]
\end{tabular}}
\label{tab:tab_exp_beta_perf}
\end{table*}


\begin{table*}[h]
\caption{Comparison of different grouping strategies on the Taskonomy benchmark.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{l|ccccccccccc|c}
\midrule[1.0pt]
 & DE & DZ & EO & ET & K2  & K3 & N   & C & R & S2  & S2.5 &  \\ \cmidrule[0.5pt]{2-12}
\multirow{-2}{*}{Task} & L1 Dist. $\downarrow$  & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & RMSE $\downarrow$    & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$  & \multirow{-2}{*}{$\triangle_m$ ($\uparrow$)} \\ \midrule[1.0pt]
Heterogeneous               &0.0172&0.0176&0.1252&0.1741&0.1700&0.0920&0.2475&0.7781&0.1743&0.1849&0.1660&-3.10 \\
Random ($N(\mathcal{M})$=2) &0.0177&0.0180&0.1259&0.1741&0.1707&0.0923&0.2662&0.7807&0.1757&0.1871&0.1617&-4.24 \\
Random ($N(\mathcal{M})$=3) &0.0172&0.0177&0.1250&0.1741&0.1703&0.0920&0.2619&0.7754&0.1749&0.1866&0.1607&-3.35 \\
Random ($N(\mathcal{M})$=4) &0.0183&0.0187&0.1277&0.1746&0.1706&0.0936&0.2812&0.7841&0.1804&0.1882&0.1636&-6.12 \\
Random ($N(\mathcal{M})$=5) &0.0186&0.0184&0.1274&0.1747&0.1708&0.0935&0.3150&0.7842&0.1800&0.1888&0.1640&-7.17 \\
Random ($N(\mathcal{M})$=6) &0.0208&0.0209&0.1349&0.1750&0.1721&0.0961&0.3334&0.8222&0.1976&0.1935&0.1703&-13.20\\
Ours                        &0.0167&0.0169&0.1228&0.1739&0.1695&0.0910&0.2344&0.7600&0.1691&0.1836&0.1571&-0.64 \\\midrule[1.0pt]
\end{tabular}}
\label{tab:tab_exp_grouping_strategy}
\end{table*}


\begin{table*}[h]
\caption{Results on Taskonomy with varying batch sizes using ViT-B (batch sizes in brackets).}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{l|ccccccccccc|c}
\midrule[1.0pt]
 & DE & DZ & EO & ET & K2  & K3 & N   & C & R & S2  & S2.5 &  \\ \cmidrule[0.5pt]{2-12}
\multirow{-2}{*}{Task} & L1 Dist. $\downarrow$  & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & RMSE $\downarrow$    & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$  & \multirow{-2}{*}{$\triangle_m$ ($\uparrow$)} \\ \midrule[1.0pt]
Single Task                 &0.0183&0.0186&0.1089&0.1713&0.1630&0.0863&0.2953&0.7522&0.1504&0.1738&0.1530&-         \\\midrule[0.5pt]
GD(4)                       &0.0208&0.0214&0.1323&0.1747&0.1723&0.0952&0.2768&0.8214&0.1936&0.1921&0.1677&-10.88    \\ \rowcolor[HTML]{E0E0E0}
Ours(4)                     &0.0185&0.0190&0.1273&0.1741&0.1709&0.0928&0.2739&0.7957&0.1809&0.1888&0.1632&-6.19     \\
GD(8)                       &0.0188&0.0197&0.1283&0.1745&0.1718&0.0933&0.2599&0.7911&0.1799&0.1885&0.1631&-6.35     \\ \rowcolor[HTML]{E0E0E0}
Ours(8)                     &0.0167&0.0169&0.1228&0.1739&0.1695&0.0910&0.2344&0.7600&0.1691&0.1836&0.1571&-0.64     \\
GD(16)                      &0.0172&0.0180&0.1248&0.1742&0.1711&0.0920&0.2280&0.7641&0.1706&0.1848&0.1589&-1.94     \\ \rowcolor[HTML]{E0E0E0}
Ours(16)                    &0.0153&0.0154&0.1186&0.1737&0.1682&0.0893&0.1967&0.7334&0.1581&0.1780&0.1516&+4.19     \\\midrule[1.0pt]
\end{tabular}}
\label{tab:tab_exp_batch}
\end{table*}



% \clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm Complexity and Computational Load}
\setcounter{table}{0}
\setcounter{figure}{0}
We also provide a detailed time comparison of previous multi-task optimization methods on Taskonomy. As shown in \Cref{tab:training_time_taskonomy}, our approach effectively optimizes multiple tasks with more efficient training times. Our method converges faster than gradient-based approaches, as the primary bottleneck in optimization lies in backpropagation and gradient manipulation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h]
\caption{Comparison of the average time required by each optimization process to handle a single
batch for 11 tasks on Taskonomy.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{\textwidth}{!}{
\scriptsize
\begin{tabular}{l|ccccc|c}
\hline
Process (sec)     & Forward Pass & Backpropagation & Gradient Manipulation & Optimizer Step & Clustering + Affinity Update & Total \\ \hline
GD&0.030(9.04\%)&0.198(59.26\%)&-&0.106(31.69\%)&-&0.33 \\ 
UW&0.030(8.82\%)&0.198(58.24\%)&-&0.112(32.94\%)&-&0.34 \\
DTP&0.030(8.70\%)&0.199(57.68\%)&-&0.116(33.62\%)&-&0.34 \\
DWA&0.031(9.01\%)&0.198(57.56\%)&-&0.115(33.43\%)&-&0.34 \\
GradDrop&0.030(1.13\%)&2.05(80.54\%)&0.411(16.02\%)&0.059(2.30\%)&-&2.57 \\    
MGDA&0.033(0.086\%)&2.06(5.36\%)&36.29(94.47\%)&0.031(0.081\%)&-&38.42 \\
PCGrad&0.030(0.63\%)&2.07(44.09\%)&2.57(54.62\%)&0.031(0.66\%)&-&4.70 \\
CAGrad&0.030(0.57\%)&2.06(39.39\%)&3.11(59.44\%)&0.031(0.59\%)&-&5.23 \\
Aligned-MTL&0.027(0.86\%)&2.07(64.99\%)&1.06(33.20\%)&0.030(0.95\%)&-&3.19 \\
FAMO&0.030 (8.72\%)&0.198(57.56\%)&-&0.116(33.72\%)&-&0.34 \\
Ours&0.072 (7.13\%)&0.576(56.72\%)&-&0.323(31.82\%)&0.044(4.33\%)&1.02 \\ \hline
\end{tabular}}
\label{tab:training_time_taskonomy}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
