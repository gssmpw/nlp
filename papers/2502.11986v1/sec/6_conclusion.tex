\section{Conclusion}
We propose a novel approach to mitigate negative transfer by dynamically dividing task groups during optimization. Through empirical experimentation, we observed significant differences in multi-task performance depending on whether task losses are backpropagated collectively or updated sequentially. Building on this insight, we introduce an algorithm that adaptively separates task sets and updates them within a single shared architecture during optimization. To facilitate simultaneous tracking of inter-task relations and network optimization, we introduce proximal inter-task affinity, which can be measured throughout the optimization process. Our analysis highlights the profound impact of sequential updates on the learning of task-specific representations. Ultimately, our methods substantially enhance multi-task performance compared to previous multi-task optimization.