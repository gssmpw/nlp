\section{Related Work}
Optimization methods in MTL can be broadly categorized into those that manipulate task-specific gradients \citep{RN24, RN21, RN22, RN19, RN36, RN20, RN18, liu2021towards, phan2022improving, navon2022multi, senushkin2023independent, jeong2024quantifying} and those that adjust the weighting of task-specific losses \citep{RN23, RN25, RN26, liu2024famo}. Addressing the imbalance in task influence, normalized gradients are utilized \citep{RN24} to prevent task spillover. Introducing stochasticity to the network's parameters based on gradient consistency, GradDrop \citep{RN21} drops gradients. RotoGrad \citep{RN22} rotates the network's feature space to align tasks. MGDA \citep{RN36} treats MTL as a multi-objective problem, minimizing the norm point in the convex hull. CAGrad \citep{RN18} minimizes multiple loss functions and regulates trajectory using worst local improvements of individual tasks. Aligned-MTL \citep{senushkin2023independent} stabilizes optimization by aligning principal components of gradient matrix . The weighting of losses significantly impacts multi-task performance as tasks with higher losses can dominate training. Addressing this, tasks' losses are weighted based on task-dependent uncertainty \citep{RN23}. Prioritizing tasks according to difficulty by evaluating validation results \citep{RN25}, or balancing multi-task loss reflecting loss descent rates \citep{RN26}, are also proposed strategies. Previous works on multi-task optimization that directly modify task-specific gradients \cite{RN24, RN21, RN22, RN19, RN36, RN20, RN18, liu2021towards, phan2022improving, navon2022multi, senushkin2023independent, jeong2024quantifying} focus on learning the shared parameters of the network without considering their interaction with task-specific parameters during the optimization step. In contrast, we propose fundamentally different strategies to mitigate negative transfer by sequentially updating task groups which lead to better multi-task performance, a phenomenon not explained by conventional multi-task optimization analysis.