\section{Experiments}
\label{sec:exp}
\textbf{Experimental settings.} We assess the proposed techniques using three datasets: NYUD-V2 for indoor vision tasks \citep{RN15}, PASCAL-Context for outdoor scenarios \citep{mottaghi2014role}, and Taskonomy \citep{zamir2018taskonomy} for large number of tasks. Multi-task performance is compared using the metric introduced by \citep{RN2}. This metric calculates the per-task performance by averaging it relative to the single-task baseline $b$: $\triangle_m = (1/\mathcal{K})\sum_{i=1}^{\mathcal{K}}(-1)^{l_i}(M_{m,i}-M_{b,i})/M_{b,i}$ where $l_i=1$ if a lower value of measure $M_i$ means indicates better performance for task $\tau_i$, and 0 otherwise. More details are introduced in \Cref{append:experimental_details}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Comparison of time complexity and memory consumption between our optimization methods and other multi-task optimization approaches, including loss-based and gradient-based methods.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{\textwidth}{!}{
\scriptsize
\begin{tabular}{l|ccccc|c}
\hline
Method                       & Forward Pass & Backpropagation & Gradient Manipulation & Optimizer Step & Affinity Update & Memory \\ \hline
Loss-based Methods           & $\mathcal{O}(1)$             & $\mathcal{O}(1)$  & \xmark           & $\mathcal{O}(1)$      & \xmark & $\mathcal{O}(1)$ \\ 
Gradient-based Methods       & $\mathcal{O}(1)$             & $\mathcal{O}(\mathcal{K})$  & \cmark           & $\mathcal{O}(1)$      & \xmark & $\mathcal{O}(\mathcal{K})$ \\ 
Ours & $\mathcal{O}(\mathcal{M})$   & $\mathcal{O}(\mathcal{M})$  & \xmark           & $\mathcal{O}(\mathcal{M})$      & \cmark & $\mathcal{O}(1)$  \\ \hline
\end{tabular}}
\label{tab:comp_cost}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.99\textwidth]{figure/comp_cost.png}
    \vspace{-5pt}
    \caption{Comparison of the average time required by each optimization process to handle a single batch for 5 tasks on PASCAL-Context (left) and 11 tasks on Taskonomy (right).}
    \vspace{-10pt}
\label{fig:comp_cost}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\textbf{Baselines.} We selected conventional multi-task optimization methods as our baselines: 1) Single-task learning, where each task is trained independently; 2) GD, where all task gradients are updated jointly without manipulation; 3) Gradient-based optimization methods that include gradient manipulation, such as GradDrop \citep{RN21}, MGDA \citep{RN36}, PCGrad \citep{RN20}, CAGrad \citep{RN18}, Aligned-MTL \citep{senushkin2023independent}, and Nash-MTL \citep{navon2022multi}; 4) Loss-based optimization methods, including UW \citep{RN23}, DTP \citep{RN25}, DWA \citep{RN26}, and FAMO \citep{liu2024famo}; and 5) A hybrid approach combining gradient and loss-based methods, specifically IMTL \citep{liu2021towards}. Each experiment was conducted three times with different random seeds to ensure a fair comparison.

\textbf{Architectures.} We use the most common multi-task architecture, employing a shared encoder and multiple decoders, each dedicated to a specific task. For our encoder, we mainly use ViT \cite{vit}, coupled with a single convolutional layer as the decoder.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\vspace{-12pt}
\caption{Experimental results on the Taskonomy dataset using ViT-L. The best results in each column are shown in bold, while convergence failures are indicated with a dash.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.00}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{l|ccccccccccc|c}
\midrule[0.5pt]
 & DE & DZ & EO & ET & K2  & K3 & N   & C & R & S2  & S2.5 &  \\ \cmidrule[0.5pt]{2-12}
\multirow{-2}{*}{Task} & L1 Dist. $\downarrow$  & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & RMSE $\downarrow$    & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$ & L1 Dist. $\downarrow$  & \multirow{-2}{*}{$\triangle_m$ ($\uparrow$)} \\ \midrule[0.5pt]
Single Task     &0.0155&0.0160&0.1012&0.1713&0.1620&0.082&0.2169&0.7103&0.1357&0.1700&0.1435&-    \\ \midrule[0.5pt]
GD              &0.0163&0.0167&0.1211&0.1742&0.1715&0.093&0.2333&0.7527&0.1625&0.1837&0.1487&-8.65\ppm0.229     \\
GradDrop        &0.0168&0.0172&0.1229&0.1744&0.1727&0.091&0.2562&0.7615&0.1656&0.1862&0.1511&-10.81\ppm0.377      \\
MGDA            &-&-&-&-&-&-&-&-&-&-&-&-     \\
UW              &0.0167&0.0151&0.1212&0.1728&0.1712&0.089&0.2360&0.7471&0.1607&0.1829&0.1538&-7.65\ppm 0.087    \\
DTP             &0.0169&0.0153&0.1213&\textbf{0.1720}&0.1707&0.089&0.2517&0.7481&0.1603&0.1814&0.1503&-8.16\ppm 0.081    \\
DWA             &0.0147&0.0155&0.1209&0.1725&0.1711&0.089&0.2619&0.7486&0.1613&0.1845&0.1543&-7.92\ppm 0.077    \\
PCGrad          &0.0161&0.0165&0.1206&0.1735&0.1696&0.090&0.2301&0.7540&0.1625&0.1830&0.1483&-7.72\ppm0.206     \\
CAGrad          &0.0162&0.0166&0.1202&0.1769&0.1651&0.091&0.2565&0.7653&0.1661&0.1861&0.1571&-10.05\ppm0.346    \\
IMTL            &0.0162&0.0165&0.1206&0.1741&0.1710&0.090&0.2268&0.7497&0.1617&0.1832&0.1543&-8.03\ppm0.179     \\
Aligned-MTL     &0.0150&0.0155&\textbf{0.1135}&0.1725&\textbf{0.1630}&\textbf{0.086}&0.2513&0.8039&0.1646&0.1800&\textbf{0.1438}&-6.22\ppm0.285     \\
Nash-MTL        &-&-&-&-&-&-&-&-&-&-&-&-     \\
FAMO            &0.0157&0.0155&0.1211&0.1730&0.1702&0.090&0.2433&0.7479&0.1610&0.1823&0.1527&-7.58\ppm 0.211    \\
Ours            &\textbf{0.0140}&\textbf{0.0145}&0.1136&0.1735&0.1679&0.087&\textbf{0.2029}&\textbf{0.7166}&\textbf{0.1500}&\textbf{0.1769}&0.1469&\textbf{-1.42}\ppm 0.208    \\  \midrule[0.5pt]
\end{tabular}}
\label{tab:tab_exp_taskonomy_vitL}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/group_viz_3.png}
        \vspace*{-15pt}
        \caption{$\{G\}_{i=1}^{\mathcal{M}}$ with ViT-L}
        \label{fig:group_viz_T}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/group_viz_0.png}
        \vspace*{-15pt}
        \caption{$\{G\}_{i=1}^{\mathcal{M}}$ with ViT-T}
        \label{fig:group_viz_L}
    \end{subfigure}
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/num_group.png}
        \vspace*{-15pt}
        \caption{Change of $\mathcal{M}$}
        \label{fig:num_group}
    \end{subfigure}
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_taskonomy/C_to_S2.png}
        \vspace*{-15pt}
        \caption{Change of $\mathcal{B}^t$}
        \label{fig:viz_prox}
    \end{subfigure}
    \vspace{-5pt}
    \caption{The averaged grouping results $\{G\}_{i=1}^{\mathcal{M}}$ on the Taskonomy benchmark are shown for ViT-L in (a) and for ViT-T in (b). (c) illustrates how the number of task groups, $\mathcal{M}$, changes during optimization. (d) shows the change in proximal inter-task affinity from DE to C.}
    \label{fig:vis}
    \vspace{-8pt}
\end{figure}



We conduct experimental analysis to address several key statements and questions. For additional and comprehensive experimental results, please refer to \Cref{append:additional_experimental_results}.

\textbf{Computational cost and memory consumption.} In \Cref{tab:comp_cost}, we compare the time complexity and memory consumption of previous approaches with our methods. Additionally, \Cref{fig:comp_cost} presents the average time required by each optimization process to handle a single batch. Loss-based approaches, such as FAMO, DWA, DTP, and UW, exhibit the lowest computational cost and memory usage because they do not require multiple forward passes or backpropagation. In contrast, gradient-based approaches, including Nash-MTL, Aligned-MTL, CAGrad, PCGrad, and GradDrop, incur significantly higher computational costs due to the iterative backpropagation required for each task ($\mathcal{K}$), even though they have the potential for better performance. These approaches also need to store task-specific gradients, which demands $\mathcal{K}$ times more memory. Our methods, however, require $\mathcal{M}$ (number of groups) forward passes and backward passes, where $\mathcal{M}<\mathcal{K}$. As shown in \Cref{fig:num_group}, the number of groups changes during optimization on Taskonomy, with our methods maintaining an average $\mathcal{M}=1.8$ with ViT-L, much lower than $\mathcal{K}=11$ for the Taskonomy benchmark. Despite the need for multiple forward passes, our methods remain computationally competitive because the majority of the computational load is concentrated on backpropagation and gradient manipulation. This trend becomes more advantageous as the number of tasks increases. Our methods achieve a computational cost that falls between loss-based and gradient-based methods, while maintaining similar memory consumption to loss-based methods.

\textbf{Optimization results comparison.} We compare the results of multi-task optimization on Taskonomy in \Cref{tab:tab_exp_taskonomy_vitL} and on NYUD-v2 and PASCAL-Context in \Cref{tab:tab_exp_nyud_pascal}. Our methods achieve superior multi-task performance across all benchmarks, demonstrating their practical effectiveness. One key observation is that the tasks showing the greatest performance improvements differ between our methods and previous approaches, reflecting the distinct motivations behind each optimization design. Recent methods like Nash-MTL, IMTL, and Aligned-MTL focus on improving edge detection, often at the expense of other tasks. This is due to their emphasis on balancing task losses or gradients, which are affected by unbalanced loss scales. Since edge detection shows the lowest loss scale across the optimization process, these methods prioritize enhancing its performance. Loss-based approaches such as FAMO, DWA, DTP, and UW share a similar motivation but exhibit limited performance compared to gradient-based methods. In contrast, our methods enhance performance across all tasks compared to GD by optimizing task-specific parameters in a grouping update, which benefits all tasks. Furthermore, our methods achieve stable convergence with many tasks, unlike MGDA or Nash-MTL, which struggle to converge in such scenarios on Taskonomy.

\textbf{Grouping results of selective task group updates.} In \Cref{fig:vis}, we illustrate how the grouping strategy evolves during the optimization process. Specifically, in \Cref{fig:group_viz_L,fig:group_viz_T}, we show the frequency with which different tasks are grouped together throughout the entire optimization process. Similar tasks, such as depth euclidean (DE) and depth zbuffer (DZ), tend to be grouped more often, whereas depth-related tasks are less frequently grouped with tasks of other types. Overall, the trends in task grouping are consistent across different backbone network sizes. However, one notable observation is that the overall level of grouping increases as the backbone size grows. This suggests that larger models are better at extracting generalizable features across multiple tasks, leading to higher inter-task affinity. In \Cref{fig:num_group}, we present the average number of task groups $\mathcal{M}$ throughout the optimization process. During optimization, $\mathcal{M}$ tends to increase as proximal inter-task affinity decreases, suggesting that task competition grows more intense over time. Moreover, $\mathcal{M}$ grows as the backbone network's capacity increases, which aligns with the grouping patterns observed for different backbone sizes in \Cref{fig:group_viz_L,fig:group_viz_T}. We provide an example of proximal inter-task affinity in \Cref{fig:viz_prox}, with all task pairs shown in \Cref{fig:proximal_vit_taskonomy}.

\textbf{Stability of optimization.} In \Cref{tab:tab_exp_taskonomy_vitL,tab:tab_exp_nyud_pascal}, we also report the variance in multi-task performance. Our methods demonstrate a similar level of variance compared to previous optimization approaches. Additionally, we assess how the order of task group updates and the type of grouping affect performance and optimization stability in \Cref{tab:opt_stability}. Using the task groups identified from tracked proximal inter-task affinity, we compare performance when updating in forward order, backward order, and randomly selected order. The results show that the order of task group updates does not significantly impact the final multi-task performance or algorithm stability. When comparing our method to gradient descent (GD), where all tasks are grouped together ($\mathcal{M}=1$), and a scenario where each task is placed in a separate group ($\mathcal{M}=\mathcal{K}$), our approach demonstrates better performance and greater stability by grouping tasks with high proximal inter-task affinity.


\textbf{The effect of the decay rate on tracking affinity.} The affinity decay rate $\beta$ influences the tracking of proximal inter-task affinity as shown in Figure \ref{fig:proximal_vit_beta_sample}. A larger $\beta$ results in more dynamic fluctuations in affinity, resulting in more frequent changes in task groupings. In contrast, a smaller $\beta$ leads to slower adjustments in task groupings. We observe that the affinity decay rate $\beta$ does not significantly impact multi-task performance, which enhances the method's applicability. In our experiments, we set $\beta$ to $0.001$. We anticipate that $\beta$ can be easily adjusted by monitoring the variation of proximal inter-task affinity. The overall results are shown in \Cref{fig:proximal_vit_beta}.


\begin{table*}[t]
\vspace{-10pt}
\caption{We assess the proposed method alongside previous multi-task optimization approaches on both NYUD-v2 and PASCAL-Context. The best results are highlighted in bold.}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{0.95}
\resizebox{\textwidth}{!}{
\tiny
\begin{tabular}{l|ccccc|cccccc}
\midrule[0.5pt]
\multicolumn{1}{c|}{}  & \multicolumn{5}{c|}{NYUD-v2}  & \multicolumn{6}{c}{PASCAL-Context}  \\ \cmidrule[0.5pt]{2-12} 
\multicolumn{1}{c|}{} & Semseg  & Depth  & Normal  & Edge & $\triangle_m$ & Semseg  & Parsing  & Saliency  & Normal  & Edge & $\triangle_m$\\
\multicolumn{1}{c|}{\multirow{-3}{*}{Method}} & mIoU $\uparrow$  & RMSE $\downarrow$  & mErr $\downarrow$  & odsF $\uparrow$  & $\% \uparrow$  & mIoU $\uparrow$  & mIoU $\uparrow$  & maxF $\uparrow$  & mErr $\downarrow$  & odsF $\uparrow$ & $\% \uparrow$ \\ \midrule[0.5pt]
Single Task  &39.35  &0.661  &22.14  &59.7  &-               &67.96  &58.90  &83.76  &15.65  &47.7  &-  \\ \midrule[0.5pt]
GD           &38.51  &0.641  &25.64  &53.0  &-6.54\ppm0.171  &67.48  &55.46  &81.36  &18.41  &39.0  &-9.06\ppm0.095  \\
GradDrop     &38.42  &0.638  &25.75  &53.0  &-6.60\ppm0.264  &67.18  &55.35  &81.32  &18.53  &39.0  &-9.35\ppm0.092  \\
MGDA         &20.93  &0.870  &36.66  &59.8  &-35.96\ppm0.117 &-  &-  &-  &-  &-  &-  \\
UW           &38.20  &0.631  &25.32  &53.0 &-5.99\ppm0.196  &67.11  &55.22  &81.33  &18.44  &38.6  &-9.46\ppm0.113  \\
DTP          &38.52  &0.633  &25.60  &52.8  &-6.26\ppm0.214  &67.09  &55.23  &81.35  &18.40  &38.6  &-9.41\ppm0.108  \\
DWA          &38.56  &0.634  &25.62  &52.8  &-6.30\ppm0.220  &67.11  &55.23  &81.34  &18.44  &38.6  &-9.46\ppm0.116  \\
PCGrad       &38.26  &0.633  &25.40  &54.0  &-5.70\ppm0.083  &66.62  &55.11  &81.82  &18.16  &40.0  &-8.58\ppm0.101  \\
CAGrad       &38.31  &0.641  &26.11  &58.0  &-5.10\ppm0.243  &66.31  &54.96  &81.28  &18.68  &44.9  &-7.46\ppm0.067  \\
Aligned-MTL  &36.20  &0.655  &23.77  &\textbf{58.5}  &-4.12\ppm0.121  &60.78  &54.42  &82.29  &17.44  &\textbf{45.5}  &-7.20\ppm0.075  \\
IMTL         &37.19  &0.652  &\textbf{23.45}  &57.8  &-3.31\ppm0.213  &63.91  &55.23  &82.23  &17.83  &44.2  &-7.07\ppm0.104  \\
Nash-MTL     &36.94  &0.641  &23.60  &58.0  &-3.14\ppm0.115  &\textbf{68.50}  &10.32  &75.63  &22.25  &35.3  &-31.91\ppm0.166  \\
FAMO         &38.57  &0.636  &25.61  &53.1  &-6.23\ppm0.166  &67.12  &55.23  &81.34  &18.44  &38.6  &-9.46\ppm0.115  \\
Ours         &\textbf{40.02}  &\textbf{0.618}  &24.09  &53.9  &\textbf{-2.58}\ppm0.205  &68.14  &\textbf{57.15}  &\textbf{82.52}  &\textbf{17.19}  &39.5  &\textbf{-6.24}\ppm0.192  \\ \midrule[0.5pt]
\end{tabular}}
\label{tab:tab_exp_nyud_pascal}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
    \vspace{-5pt}
    \centering
    \begin{minipage}{0.75\textwidth}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/group_viz_nyud.png}
        \vspace*{-15pt}
        \caption{}
        \label{fig:group_viz_nyud}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/group_viz_pascal.png}
        \vspace*{-15pt}
        \caption{}
        \label{fig:group_viz_pascal}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=0.99\textwidth]{figure/vit_beta/semseg_to_depth.png}
        \vspace*{-15pt}
        \caption{}
        \label{fig:proximal_vit_beta_sample}
    \end{subfigure}
    \vspace{-5pt}
    \captionof{figure}{The averaged grouping results $\{G\}_{i=1}^{\mathcal{M}}$ are shown for NYUD-v2 in (a) and PASCAL-Context in (b). (c) illustrates how the decaying factor $\beta$ influences the stable tracking of proximal inter-task affinity.}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
    \vspace{-5pt}
    \caption{Ablation studies on Taskonomy exploring the impact of group order and type of grouping.}
    \vspace{-10pt}
    \tiny
    \begin{tabular}{l|c}\midrule[0.5pt]
    Method           &  $\triangle_m$ ($\uparrow$)\\\midrule[0.5pt]
    Order of Group        \\
    Forward          &-1.41\ppm0.211  \\
    Backward         &-1.44\ppm0.209  \\\midrule[0.5pt]
    Grouping Methods   \\
    Joint (GD)          &-8.65\ppm0.229  \\
    Separate            &-2.48\ppm0.651  \\\midrule[0.5pt]
    Ours             &-1.42\ppm0.208
    \end{tabular}
    \label{tab:opt_stability}
    \end{minipage}
    \vspace{-8pt}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%