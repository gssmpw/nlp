\section{EXPERIMENTS AND RESULTS}
\label{sec:experiments}
The experiments in this section evaluate the online adaptability of the proposed TE method using data collected during operation and in situ on the robotic platform. Further, we aim to understand the performance and limitations of different adaptation or training strategies depending on the data available, e.g. post-processed vs online data. 

The experiments are structured in four parts. The initial set of experiments characterises how well the proposed method performs on different fine-tuning tasks on post-processed data. This provides insights into how well an existing model can be fine-tuned as well as a baseline for the online adaptation.
%
The second set of experiments demonstrates this approach in a real-world scenario, where the robot learns a new, untrained model using only the experience of an \qty{8}{\minute} data collection. It aims to validate the method by showing that it enables the robot to navigate autonomously in a densely vegetated environment.
% 
The third set of experiments aims to train comparative methods and quantify their performance on the same data, avoiding any form of variation and stochastic elements, and compare three architecture variations and four different training setups.
%
The last set of experiments consists of a set of real-world experiments where differently trained models perform navigation experiments in varying environments, and their performance and limitations are assessed and compared. 

\subsection{Evaluation Metrics}
Throughout the evaluation, we use the Mathews Correlation Coefficient (MCC) score to evaluate and compare models. The MCC score measures how well the model predictions are correlated to the label data. It is immune to class swapping and is robust to imbalanced data sets. The MCC score ranges from $ -1$ to $1$, where 0 is a random model, and $1$ is a perfect positive correlation. It uses all four cases of the confusion matrix.  This makes it a preferred choice over the F1 score since the F1 score can overestimate a model's performance due to class imbalance, making it sensitive to class choice. This overestimation is particularly common for~\ac{TE}, where the positive class is typically the traversable class in the literature and is usually over-represented in training datasets. However, we also include the F1 score as a reference metric due to common practice in the literature.

\subsection{Data Sets and Data Set Groupings}
\label{subsec:data_set_groups}
The data set used in this experiment was originally published by Ruetz et al.~\cite{ruetz2024foresttrav} and consists of 9 different scenes (numbers \# 1 to 9). These data sets capture a variation of open fields with grass, areas with open skies and small trees, and forests with no closed skies. The data set covers a variety of scenarios and different natural scenes. 
In this paper, we include three additional industrial data sets, numbered \# 10 to 12. These data sets are included to be used in a clearly different environment to the previously collected natural scenes and to validate adaptation to a different environment. These were collected using the \ac{dtr} robot at an industrial site of CSIRO in Pullenvale, Queensland, Australia. An overview of the data set and scenes can be found in Figure~\ref{fig:data_set_environments}, where the blue dots mark the locations of the newly collected data sets.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{figures/ruetz4.png}
    \caption{Top left: Shows the overview of the environment with the orange dots showing the locations of data sets from ForestTrav~\cite{ruetz2024foresttrav}. The blue dots are the novel data set introduced in this work from an industrial environment. An example is shown in the top right. The bottom left shows an example of the SPARSE environment, the bottom right scene from a densely vegetated forest.}
    \label{fig:data_set_environments}
\end{figure}


\subsubsection{Industrial Data Set Characterisation}
\label{subsec:industrial_data_set}
The industrial set contains external spaces found at the CSIRO QCAT site at Pullenvale, QLD, a mixed industrial area containing office buildings, workshops and large mechanical testing facilities, including open and closed spaces, illustrated in Figure~\ref{fig:data_set_environments}. The data sets numbered 1  and 12 are used for the training set, and 13 is the hold-out test data set. The first industrial data set, Scene \# 10, is an open industrial area surrounded by large sheds. The area has different obstacles, such as metal posts, guard rails and barrels. The second industrial scene (\# 11) contains a two-lane road with a tall shed on each side. A pedestrian walkway on the side inclines and declines in this scene, providing a narrow, challenging path. Additionally, there are a lot of guard rails, posts and fences that could prevent the robot from moving. The third scene consists of the same road but larger open areas and has obstacles similar to those in the first two scenes. This scene was used as a test set. For all these scenes, a lot of the traversable and non-traversable examples are similar. The traversable examples consist of different patches of flat surfaces with different inclinations. Different paths were taken to collect different inclinations. The non-traversable elements consist of posts, barrels, guard rails, containers, machinery and the large overall shed structures. In general, these elements are not as varied as what can be found in the scenes with vegetation.

The following Table~\ref{tab:industiral_data_set} provides an overview of each scene. The first column is the scene number, and the second column is the total number of voxels. The following four columns (2-6) provide the percentages of the hand-labelled and \ac{lfe} data for each traversability class. The seventh column provides the mean column density of a scene, called the column vegetation density (CVD)~\cite{ruetz2024foresttrav}. The density is the column-wise fraction, up to a pre-determined height, above and excluding the ground voxel containing measurements. The height is defined by the platform in this work and is set to \qty{1}{\m}. This indicates how much ``vegetation'' or other elements need to be considered and pushed through to navigate safely. The ForestTrav data set~\cite{ruetz2024foresttrav} averages densities of 0.4 - 0.57 versus 0.13 in the industrial scene. The last column is the scene dimensions in meters. 
 \begin{table*}
\caption{Overview of the industrial data set.}
\label{tab:industiral_data_set}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccc}
    \toprule
    Scene & $N_{voxels}$ & $TR$ HL [\%] & $TR$ LfE  [\%]& $NTR$ HL  [\%]& $NTR$ LfE  [\%] &  Density & Dimensions [m] \\ \hline
    \# 10 & 187933 & 0.71 & 0.06 & 0.15 & 0.08 & 0.1    & $53.1 \times  69.72 \times 1.45$ \\
    \# 11  & 76497 & 0.56 & 0.08 & 0.32 & 0.04 & 0.1    & $46.67 \times 39.39 \times  1.85$ \\
    \# 12  &  69534 & 0.48 & 0.05 & 42 & 0.03 & 0.13  & $ 66.14 \times  40.63 \times  1.86 $ 
    \end{tabular}
}
\end{table*}

\subsubsection{Data Set Groupings}
Adaptation to unfamiliar and varying environments was investigated during the experiments and evaluation. Hence, the data sets were separated into three distinct groups, or subsets, by environment type: the industrial data set (INDUST), the sparse forest data set (SPARSE) and the dense forest data set (DENSE). The sparse forest contains data sets \# 3, 4, 5, 7 and 8, with number 8 being used as the test set. The sparse data set contains open fields with grass and bushes, as well as smaller trees with no overarching canopy. The increased sun exposure means there is a large mixture of dense vegetation near the ground and a few very large trees. Most of the vegetation near the ground is chlorophyll-rich. The dense forest data set contains scenes \# 1, 2, 6 as training data and \# 9 as test data. The dense forest contains significantly more underbrush and vegetation that does not contain chlorophyll, thin three stems and trunks. Additionally, there are overhanging branches, brambles and vines that make the environment more difficult. The visual difference can be seen in Figure~\ref{fig:data_set_environments}, where \# 3, 4 and 7 have little to no obstruction of the sky and contain large and small trees with open space. Comparatively, the dense forests \# 1 and 2 in Figure~\ref{fig:data_set_environments} are cluttered and have a higher tree density.
%
Later on, we refer to the ``complexity'' of the data set due to the environment. We consider the industrial environment to be the least complex and the dense, the most complex environment. The SPARSE data sets falls in between. 


\subsubsection{Practical Considerations for LfE Data Collection}
\label{ch5:pratical_colmap}
As previously mentioned, the \ac{lfe} data collection was performed by an operator using an RC remote, recording the collision states. In practice, the operator ensured that the robot was driving (slowly) forward and collided only with the front of the robot with trees, bushes or other non-traversable elements. A collision was only recorded if the robot could not move physically further. Using a human operator induces a bias because the operator chooses how to collide with the elements in the environment, but this also helps to manage the risk of damaging the robot. The area was pre-determined, and the operator attempted to cover as much of it as possible in a single ``run'', revisiting places multiple times. During data collection, the operator collided with obstacles frequently and from many different directions. The more different, non-traversable collision states are captured, the higher the accuracy of the \ac{lfe} labelling approach. Note that mistakes during data collection are easy to make and that the overriding of the collision states can sometimes lead to erroneous self-labelling when using the heuristic method. 

\subsection{Model Adaptation on Post-processed Data}
\label{ch7:model_adpation_on_post_processed_data}
In the first set of experiments, a performance baseline was established for the proposed model and the different data sets due to the different labelling strategies. In the second sub-section, the adaptation or fine-tuning of post-processed data is explored. 

\subsubsection{Model Performance on Full Data Sets}
The initial set of experiments establishes baseline performance evaluation when using post-processed data from all the available data with different labelling strategies (LfE, LfE + HL). Table~\ref{tab:model_full_data_performance} shows the MCC scores and the F1 scores of the models. The first column specifies the labelling strategy, the second column is the MCC score, and the last column is the F1 score. The evaluation data set in all cases is scene \#9 with the \ac{lfe_hl} labelling strategy. The test set is the same as the one used in our prior work~\cite{ruetz2024foresttrav}. For consistency, all models are trained using the same number of epochs.

\begin{table}[h]
\centering
\caption{MCC Score for Models Trained on Post-Precessed Data}
\label{tab:model_full_data_performance}
\begin{tabular}{lcc}
\hline
Data Set &   MCC $ \mu \pm \sigma $ &  F1 $ \mu \pm \sigma$ \\
\hline    
LfE+HL    &     $0.71  \pm  0.03$    &       $0.85 \pm  0.017$   \\      
LfE        &    $0.69  \pm   0.022$     &       $0.85 \pm 0.012$   \\
\hline
\end{tabular}
\end{table}

Compared to our previous work~\cite{ruetz2024foresttrav}, there is an increase in \ac{mcc} score from 0.63 to 0.70~\ac{mcc} for the model trained on the LfE+HL data and the score obtained model trained on the \ac{lfe} data is only marginally lower.

\subsubsection{Fine-tuning on Post-Processed Data}
\label{ch7:finetuning_post_data}
In this experiment, we aim to evaluate the performance obtained when fine-tuning pre-trained models with data acquired in previously unseen environments. The first goal of this experiment is to confirm that models show different~\ac{TE} performance based on the different environment types due to the domain gap. The second goal is to explore whether fine-tuning models using data collected in a new target environment improves performance in the new environment. Given a fixed scaling and model architecture of a base model, we also aim to understand whether there is an upper bound for online adaptation, given a subset of hand-labelled and \ac{lfe} data for fine-tuning. In the context of online learning, this represents the case where a pre-trained base model is available, but the original training data is not. The base model can only be fine-tuned using novel data collected in the new environment.

\subsubsection{Base Models}
First, three base models (BM) are trained using data from one of the three groups defined previously only (industrial, sparse and dense forest), and each model is evaluated on test data of each of the three groups. The results are shown in Table~\ref{tab:offline_base}. The first column shows the base model names, and columns 2-6 show the MCC and F1 scores obtained with the trained model with respect to the test set of each of the groups. The MCC scores of models for which the training data and test set are from the same data set group are highlighted in italic text. The bold numbers indicate the best-performing models based on the MCC and F1 scores. 



\begin{table}[h!]
\centering
\caption{Performance for Models Trained on Post-Processed Data Groups. Bold indicates best performance, and italic indicates matching training and test data groups.}
\label{tab:offline_base}
\begin{tabular}{@{}lllllll@{}}
\hline
\multicolumn{1}{l}{Base Models} & \multicolumn{2}{l}{ INDUST TS} & \multicolumn{2}{l}{SPARSE TS} & \multicolumn{2}{l}{DENSE TS} \\ \midrule
  & \multicolumn{1}{c}{{\color[HTML]{656565} \textit{MCC}}} & \multicolumn{1}{c}{{\color[HTML]{656565} \textit{F1}}} & \multicolumn{1}{c}{{\color[HTML]{656565} \textit{MCC}}} & \multicolumn{1}{c}{{\color[HTML]{656565} \textit{F1}}} & \multicolumn{1}{c}{{\color[HTML]{656565} \textit{MCC}}} & \multicolumn{1}{c}{{\color[HTML]{656565} \textit{F1}}} \\ \midrule
BM: INDUST &  \textbf{\emph{0.70}}  & \textbf{\emph{0.84}}  & 0.15  & 0.40 & 0.05 &0.12\\
BM: SPARSE & 0.63  &  0.79  &  \textbf{\emph{0.79}}  & \textbf{\emph{0.89}} & 0.56  & 0.78 \\
BM: DENSE & 0.69   &  0.84 & \textbf{0.79} &  \textbf{0.90} & \textbf{\emph{0.70}}  &  \textbf{\emph{0.84}} \\ \bottomrule
\end{tabular}%
\end{table}

An initial set of observations can be made on the base model's performance based on which subset of the data it is trained on.
\begin{itemize}
 \item The model trained on the DENSE data set (BM:DENSE) exhibits a marginally lower \ac{mcc} and similar F1 score when compared to the model trained on the full data set, with 0.69 vs 0.71 \ac{mcc} scores. This indicates that an accurate traversability predictor can be learned with a small amount of high-quality data that discriminates the traversable and non-traversable elements of the environment.
\item Secondly, models trained and evaluated on data from the same group show high \ac{mcc} scores. For example, a model trained on the SPARSE data set shows high scores on the SPARSE test set. These are the bold, italicised numbers.
\item We note that BM:DENSE shows comparable performance to models trained and evaluated on the data of the same group (italic numbers), indicating that the model generalised well over different environments. For example, when evaluated on the INDUST test set, the MCC is 0.70 for BM:INDUST and 0.69 for BM:DENSE. The generalisation of the model trained in the DENSE environment to the INDUST environments is surprising. This can be further seen qualitatively in Figure~\ref{fig:bm_genralisiation} and discussed in the subsection.
\item Lastly, base models trained on the industrial and sparse data sets are significantly less accurate when evaluated on the dense test set. This suggests that the DENSE data sets contained many unseen or complex elements not encountered in other environments. 
\end{itemize}

In general, we note that models trained on data from more ``complex environments'' seem to perform competitively in less complex environments as well. 

\subsubsection{Fine-tuning Base Models on Post-processed Data}
Next, each base model was fine-tuned using data from the three data set groups to train a fine-tuned model. This model was again tested with test sets from each group. This resulted in six permutations of training and fine-tuning pairs. We used the \ac{lfe} training data as this best resembles what the robot experiences in the field; it can be generated without time costly hand-labelling. The results are shown in Table~\ref{tab:tab_offline_ft}

The first column identifies the base model, e.g. BM:INDUST, which is the base model trained on the industrial data set. The second column defines the data set used to adapt (fine-tune) the base model. For ease of notation, we use ``AM'' (adapted model) to indicate which data set the model was fine-tuned with. The model was initially trained on the industrial set and fine-tuned on the dense data set, which is thus denoted by (BM:INDUST AM:DENSE). The other columns show the model's performance using MCC and F1 scores of the model for the test sets of the three groups. The numbers in parentheses are the scores of the base model, allowing us to see increases and decreases in the performance of the fine-tuned model versus the base model. Similar to the previous table, italics indicate the performance number for which the test set and the fine-tuning training data belong to the same group, and bold numbers indicate the highest performance for a given test set (per column).

Conceptually, the initial three rows in Table~\ref{tab:tab_offline_ft} are the cases where the complexity of the new environment (in the fine-tuning data set) is higher than experienced in the base training, e.g. from an industrial environment to a dense environment. For ease of terminology, we call this ``fine-tuned on more complex data''. 
%
Similarly, the last three rows for each architecture are the cases where the base model is trained on a higher-complexity data set and fine-tuned with lower-complexity data, e.g., the base model trained on the dense data set and fine-tuned on the industrial data set. For ease of terminology, we call these ``fine-tuned on less complex data''. 

The training was limited to 150 epochs to make sure the results were comparable to the online case, thereby setting an upper performance bound later for the online adaptation models. Table~\ref{tab:tab_offline_ft} shows the results.

\begin{table*}[h]
\caption{Performance For Fine-tuning Models }
\label{tab:tab_offline_ft}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llcccccc@{}}
\toprule
Base Model & Finetune Train Set & \multicolumn{2}{c}{INDUST TS} & \multicolumn{2}{c}{SPARSE TS} & \multicolumn{2}{c}{DENSE TS} \\ \midrule
 &  & {\color[HTML]{656565} \textit{MCC}} & {\color[HTML]{656565} \textit{F1}} & {\color[HTML]{656565} \textit{MCC}} & {\color[HTML]{656565} \textit{F1}} & {\color[HTML]{656565} \textit{MCC}} & {\color[HTML]{656565} \textit{F1}} \\ \midrule
                            & SPARSE& 0.40 (0.70) & 0.59 (0.84)  &\emph{ 0.32} (0.15) & \emph{0.57} (0.40) & 0.47 (0.05) & 0.73 (0.12) \\
 \multirow{-2}{*}{BM:INDUST} & DENSE & 0.74 (0.70) & 0.86 (0.84)  & 0.62 (0.15) & 0.80 (0.40) & \emph{0.54} (0.05) &\emph{ 0.74} (0.12) \\ \midrule
                             & DENSE & 0.71  (0.63) & 0.84 (0.79) & \textbf{0.80} 0.79) &\textbf{ 0.90} (0.89) & \emph{0.59 } (0.56) & \emph{0.79} (0.78) \\
\multirow{-2}{*}{BM: SPARSE} &INDUST &\emph{ 0.67}  (0.63) &\emph{ 0.82} (0.79) & 0.43 (0.79) & 0.69 (0.89) & 0.41  (0.56) & 0.66 (0.78) \\ \midrule
                            & INDUST &\textbf{\emph{ 0.81}} (0.69) &\textbf{ \emph{0.90}} (0.84) & 0.63 (0.79) & 0.78 (0.90) & 0.36 (0.70) & 0.55 (0.84) \\
\multirow{-2}{*}{BM: DENSE} & SPARSE & 0.49  (0.69) &0.67 (0. 4) & \emph{0.75} (0.79) & \emph{0.87} (0.90) &\textbf{ 0.65} (0.70) &\textbf{ 0.82} (0.84) \\ \bottomrule
\end{tabular}%
}
\end{table*}


For the fine-tuned models, we can make the following comments:
\begin{itemize}
 \item The ``fine-tuned on more complex data'' models are considerably more accurate than the base model. For example, the case (BM:INDUST, AM: DENSE) shows an increase in the accuracy of the model from \ac{mcc}=0.12 to 0.54. Generally, we note an overall increase in performance for all fine-tuned models that are ``fine-tuned on more complex data''. Further, for the dense environment, the maximum performance is \ac{mcc}=0.7 for the model initially trained on the dense and adapted with the sparse data set.   
 \item In the case where the base models are fine-tuned on less complex environments, the fine-tuning improves the model performance on the specific target environment. This increase can be marginal but comes at the cost of reducing the performance of the model in other environments, which is commonly known as ``catastrophic forgetting''. For example, pre-training the base model on sparse data and fine-tuning with the industrial data set increases the \ac{mcc} score from 0.63 to 0.67 on the industrial test set but decreases it from 0.79 to 0.43 on the sparse test data.
\end{itemize}

In the last paragraph, we provide some qualitative examples. Figure~\ref{fig:bm_genralisiation} shows a comparison between a base model trained in the industrial setting (bottom row) and one trained in the dense environment (top row) for different test environments (columns). Both use fused data sets, i.e.~\ac{lfe_hl} and are evaluated on the same environments using replayed online data from unseen environments. The left column is an industrial scene, the middle is from the sparse data set, and the right scenes are from dense forests. The sparse forest contains no canopy coverage, smaller and bushier trees, and open areas with significant chlorophyll-rich vegetation close to the ground.  In comparison, the dense forest contains tall trees of different tree diameters and significant chlorophyll-poor vegetation ( bramble, thin or young treed, etc) close to the ground.

\begin{figure*}[h]
 \centering
 \includegraphics[width=\textwidth ]{figures/ruetz5.png}
 \caption[Generalisation of Models to Different Environments]{Comparison of two ensembles of models trained on either the dense data set in the top row (A, B, C) or the industrial data set in the bottom row (D, E, F). The left column shows each model's TE in an industrial environment, the middle and right columns show the TE of two densely vegetated environments with a variation of tree sizes and underbrush.}
 \label{fig:bm_genralisiation}
\end{figure*}

The model trained on the dense data set generalises well to an industrial outdoor setting (A), assessing walls and smaller elements such as pylons correctly. This is a surprising and unexpected result. Learning-based systems trained in one environment commonly do not generalise to wholly different environments. For natural environments, other image-based methods have shown high sensitivity and degradation to small spatial location changes with similar environments~\cite{frey2023fast}.  Additionally, this model provides accurate~\ac{TE} for two different vegetated environments (B \& C).

The model trained on the industrial data set demonstrates accurate \ac{TE} for the industrial test environment (D) with some noise on the ground plane. The industrial model performs poorly when predicting traversability for vegetation close to the ground in (E). In (F), the model correctly assesses many of the large elements (trees) but struggles with the smaller elements. However, there is a clearly visible colour gradient (dark blue to light green) on some of the smaller elements, showing discrimination of the \ac{TE} of the vegetation near the ground, clearly identifying tree trunks as non-traversable but struggling with bushes and smaller forms of vegetation. The model has learned a representation of the industrial setting, but it does not generalise to the densely vegetated environments. 
%
Comparatively, the model trained on the dense data set exhibits high performance over all three scenes, even the industrial one.

\subsection{Real-World Demonstration of Online Learning and Navigation in Forest}
\label{subsec:online_odap}

This experiment demonstrates that the proposed method can train a model on the robot itself, guided by a human operator in situ. The model was trained onboard the robot on an NVIDIA Jetson Orin NX (25W). A newly trained model was qualitatively demonstrated by point-to-point navigation and compared against the test set \#9.  The model was randomly initialised (untrained, new model), and it was continuously fine-tuned with incrementally acquired \ac{lfe} data. This corresponds to the case (BM 0, FT 1) described in sub-section~\ref{subsec:quant_online_adapt}. The training data collection was completed in less than 8 minutes and resulted in an \ac{mcc} score of 0.63, evaluated on the test scene \#9.

\begin{figure}[h]
 \centering
 \includegraphics[width= .9\linewidth]{figures/ruetz6.png}
 \caption[Overview of Robot Navigation of the Online Learned Model]{Overview of the navigation in a forest environment, with poses $\mathbf{p}_1$ - $\mathbf{p}_4$. The navigation was completed with the O-line trained model in situ.}
 \label{fig7:odap_nav_overview}
\end{figure}

The model was deployed on the robot and successfully navigated a closed-loop point-to-point trajectory between three waypoints, $w_1$, $w_2$, and the initial start location. A visualisation of the path can be seen in~\figref{fig7:odap_nav_overview}. The poses $p1$-$p4$ are visualisations of the trajectory, where the left image is the RGB FPV view of the robot, the middle image is the 3D TE estimation, and the right image is the costmap. Details on the costmap are provided in Section~\ref{subsec:cotmap_generation}.

\begin{figure*}[ht!]
 \centering
 % Pose 1
 \begin{subfigure}{0.95\linewidth}
 \centering
 \includegraphics[width=\linewidth]{figures/ruetz7.png}
 \caption{Robot front camera (left), 3D TE estimation (middle), costmap (right) for pose $\mathbf{p}_1$}
 \label{fig7:online_p1}
 \end{subfigure}
 \hfill
 % Pose 2t
 \begin{subfigure}{0.95\linewidth}
 \centering
 \includegraphics[width=\linewidth]{figures/ruetz8.png}
 \caption{Robot front camera (left), 3D TE estimation (middle), costmap (right) for pose $\mathbf{p}_2$}
 \label{fig7:online_p2}
 \end{subfigure}
 % Pose 3
 \begin{subfigure}{0.95\linewidth}
 \centering
 \includegraphics[width=\linewidth]{figures/ruetz9.png}
 \caption{Robot front camera (left), 3D TE estimation (middle), costmap (right) for pose $\mathbf{p}_3$}
 \label{fig7:online_p3}
 \end{subfigure}
 % Pose 4
 \begin{subfigure}{0.95\linewidth}
 \centering
 \includegraphics[width=\linewidth]{figures/ruetz10.png}
 \caption{Robot front camera (left), 3D TE estimation (middle), costmap (right) for pose $\mathbf{p}_4$}
 \label{fig7:online_p4}
 \end{subfigure}
 \caption[Scenes from Autonomous Robot Navigation using Online TE Approach]{Visualisation scenes from a successful online point-to-point navigation using the online learnt TE model in target environments, $\mathbf{p}_1$ - $\mathbf{p}_4$ are poses along the trajectory.}
 \label{fig7:odap_online_nav}
\end{figure*}

Subfigure~\ref{fig7:online_p1} shows the robot at the beginning of the trajectory briefly after the initialisation. The robot correctly assesses the trees and smaller vegetation elements, allowing it to push through the initial stand of bushes. There are some areas with uncertain (green) TE estimates, likely due to few observations; see comments for $\mathbf{p}_4$.

For $\mathbf{p}_2$, the robot encountered a large patch of grass that reached the camera. The proposed online learned TE correctly assessed it as traversable and pushed through it whilst avoiding the unobserved areas.

The pose $\mathbf{p}_3$ shows how the robot pushed through a thick bramble bush and proceeded to continue. There were some possibly non-traversable elements higher up in the bush. These elements were not projected onto the 2D costmap since they were above the robot's height threshold, and the robot would not interact with them (see Subsection~\ref{subsec:cotmap_generation}). In the background, it can be clearly observed that the trees were correctly assessed as non-traversable.

 At pose $\mathbf{p}_4$, the robot navigated through the bramble/small trees to reach the initial starting location, shown in Subfigure~\ref{fig7:online_p4}. The robot avoided the thicker tree to the right and reached the starting location. Compared to $\mathbf{p}_1$, the local costmap was less uncertain for the 3D TE estimate. The area has been observed multiple times and at multiple angles, and this allowed for more accurate traversability estimation than initially. In the visualisation of the costmap, one can observe a band on the left of traversable and unobserved elements that have been observed at the start of the experiment. This is an example of the de-allocation of the local probabilistic map since the regions were out of bounds, which is why the graph is needed.

In summary, the experiment demonstrated that a new model can be trained in situ, reaching accurate performance (\ac{mcc}$=0.63$), and is sufficient for safe point-to-point navigation in densely vegetated environments. Further, the model performance is comparable to the performance of the off-line method reported in our latest publication~\cite{ruetz2024foresttrav}.



\subsection{Quantification of Online Adaptation}
\label{subsec:quant_online_adapt}

The primary question this experiment addresses is whether the model can be adapted/trained online with only data gathered in a relevant environment. Secondly, it identifies the best strategy to do so. For this purpose, a data set was collected with human-assisted collisions. The online data set was replayed and stored in \qty{40}{\s} increments using the \ac{ograph} to generate snapshots of the online experience, which can be used to train. The intermediate data storing step was chosen to ensure data consistency.

We examine four training strategies. They are defined by the two boolean flags BM (use of a base model) and CA (continuous adaptation). The four variations of these strategies can be seen in \figref{fig:aol_incr_perf}. If the BM flag is set to true (BM 1), the method uses an initial base model trained on the industrial data set. Otherwise (BM 0), the model is randomly initialised. The industrial base model was chosen since the environment is the most different from the dense forest. For models trained without any prior knowledge, pre-determined scaling values were used to ensure non-catastrophic scaling during the run. The CA flag denotes if, for each training cycle, \ac{ograph} is updated, the model trained on the previous iteration is used for the next training cycle. If CA is true (CA 1), the model is continuously updated as new information comes in. If CA is false (CA 0), the model is trained from the base model at each ``training cycle'', as defined above.

This results in four distinct cases in \figref{fig:aol_incr_perf}:
\begin{enumerate}
 \item BM 0, CA 0 (red) -- randomly-initialised base model retrained at each training cycle,
 \item BM 0, CA 1 (green) -- randomly-initialised base model continuously adapted,
 \item BM 1, CA 1 (blue) -- pre-trained base model continuously adapted, and
 \item BM 1, CA 0 (gold) -- pre-trained base model retrained at each training cycle.
 \end{enumerate}
The dashed yellow line shows the base-model performance (no retraining).

The data comes from a newly collected data set with the goal of maximising collisions (the minority class) within a new area. The test data set remains scene \# 9 from the previously established data set, allowing for a comparison to all other experiments.

\begin{figure*}[ht!]
 \centering
 \includegraphics[width=\textwidth ]{figures/ruetz11.png}
 \caption{Comparison of the performance of different models over the incremental online adaptation for the four different cases. The $x$-axis shows the time, and the $y$-axis the \ac{mcc} score. The transparent regions of the graph correspond to the upper and lower bounds for one standard deviation.}
 \label{fig:aol_incr_perf}
\end{figure*}

% Description of what the experiment looked like
Looking at \figref{fig:aol_incr_perf}, the performance of the individual models reaches a ceiling between 0.55 and 0.65 \ac{mcc} score at their peak, the mean of the models just below 0.6~\ac{mcc} score. All models learn and improve substantially throughout online adaptation. There is a clear suggestion that a model can be either adapted or trained online with only data collected through experience and that this can be achieved in real time on a robotic platform in situ.
The variant (BM 1, CA 1) (blue graph) is generally the highest performing and most consistent (lowest variance) of all three models. The lower variance cannot solely be attributed to the learning rate being lower for continual adaptation, and we see higher variation for the green plots. The models with no continual learning (CA 0), red and yellow, show an oscillation effect, where the model's performance sometimes decreases over iterations. Further, higher variation of the method for models with randomly-initialised base models (BM 0), red and green, can be observed. 

\subsection{Navigation in Different Environments}
\label{subse:nav_in_different_env}
In this investigation, we comparatively assess navigation performance across multiple methodological approaches in two distinct forest environments. The comparative analysis includes NavStack~\cite{HudTal21}, a traditional (rigid world assumption) geometric lidar-based occupancy method, baseline ForestTrav trained on post-processed datasets~\cite{ruetz2024foresttrav}, ForestTrav DENSE and ForestTrav INDUST (trained on dense and industrial datasets, respectively), and the proposed Adaptive Online Learning (AOL) method. The experimental evaluation was conducted post-AOL generation, utilising a platform equipped with a different instance of a Velodyne VLP-16 lidar from previous dataset acquisition platforms, known to have shifts in intensity characteristics.

For two locations, a pre-defined starting and end point were given, and each method attempted to reach its goal. If a model got stuck for the first time, the operator would intervene by moving the robot forward and then continuing the experiment. For the first experiment, the robot was sent \qty{20}{\m} ahead but had to pass through a wall of small plants, navigate through an open space with sparse vegetation and finally navigate around a variation of high grass and small trees. 
For the second experiment, the robot had to navigate to a goal pose located \qty{40}{\m} from the starting position. A combination of cluttered, small trees with a variation of underbrush as well as fallen tree trunks had to be avoided.

In the first experiment, ForestTrav AOL and the original ForestTrav successfully reached the end goal without any intervention. ForestTrav initially struggled with bushes but managed to navigate around them, while ForestTrav AOL took a more direct route through the foliage. ForestTrav DENSE failed to bypass the bushes and required operator intervention to continue. NavStack also failed at the start, and after intervention, diverged from the goal due to avoiding small stems, ultimately failing.

In the second experiment, ForestTrav and ForestTrav AOL completed the course again without any human intervention. ForestTrav DENSE and NavStack made some initial progress through cluttered trees but became stuck due to thin stems and clutter, with intervention proving ineffective. ForestTrav INDUST failed entirely, unable to assess the environment.

ForestTrav and ForestTrav AOL were the most effective methods, differing primarily in their interpretation of traversable terrain. ForestTrav struggled with vegetation that was absent in its training data but prevalent in the target environment. ForestTrav AOL, trained in a similar environment, handled these obstacles better but was less robust overall due to limited training data and the lack of an ensemble approach. The poor performance of ForestTrav DENSE is attributed to the insufficient representation of thin, bushy vegetation in its training set.

ForestTrav INDUST consistently failed, likely due to changes in the intensity distribution characteristics of the Velodyne VLP-16 sensor. This was confirmed in follow-up tests, but the costmaps remained inadequate for navigation.

\begin{figure*}
     \centering
    \includegraphics[width=\textwidth ]{figures/ruetz12.png}
    \caption{Qualitative examples of the methods in the target environment. Blue blocks are for all methods at location 1 and show the starting area of the experiment, allowing us to compare the methods. The green block shows the costmap, FPV and external view of the robot for the location where NavStack failed to navigate. }
    \label{fig:nav_experiment}
\end{figure*}

\begin{table}[t]
\caption{Comparison performance of different methods for different navigation environments. }
\label{tab:nav_comp_exp}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
Method & \multicolumn{3}{c|}{Location 1}  & \multicolumn{3}{c}{Location 2} \\ \midrule
 & \multicolumn{1}{r}{{\color[HTML]{656565} \textit{Success}}} & \multicolumn{1}{r}{{\color[HTML]{656565} \textit{Time [s]}}} & \multicolumn{1}{r|}{{\color[HTML]{656565} \textit{Distance [m]}}} & \multicolumn{1}{r}{{\color[HTML]{343434} \textit{Success}}} & \multicolumn{1}{r}{{\color[HTML]{343434} \textit{Time [s]}}} & \multicolumn{1}{r|}{{\color[HTML]{343434} \textit{Distance [m]}}} \\ \midrule
CSIRO NavStack & No & 240 & 30.7 & No & 170 & 13.9  \\
ForestTrav & Yes & 270 & 32.1 & Yes & 350 & 52.1  \\
ForestTrav AOL & Yes & 160 & 27.1 & Yes & 300 & 52.1\\
ForestTrav DENSE & No & 190 & 24.5 & No & 220 & 20.29  \\
ForestTrav INDUST & No & - & 0.0 & No & -  & 0.0 \\
\end{tabular}%
}
\end{table}

