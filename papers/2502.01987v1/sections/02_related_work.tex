\section{RELATED WORK}
\label{sec:related_work}

Traversability estimation is the assessment of a patch of terrain to determine whether a robot can enter, reside and exit this patch without entering a failure state~\cite{papadakis2013terrain}. In the literature, traversability estimation has classically been separated into geometric, appearance-based and mixed methods. Geometric methods aim to assess traversability using geometric environmental representations commonly generated from range measurements, and these methods have shown great success in rigid, complex, unstructured environments~\cite{HudTal21, tranzatto2022cerberus}. However, these fail in environments with pliable elements, such as vegetated environments, due to the inherent assumption of a rigid environment and the inability to interact with pliable elements if required to succeed in a task~\cite{frey2023fast}.  

For~\ac{TE} in vegetated environments, the use of image modalities has been considered essential by many authors to discriminate and assess complex environments, either via purely image-based methods~\cite{wellington2004online, frey2023fast} or mixed methods~\cite{frey2024roadrunner}. Pure image- or appearance-based methods rely on a single image or short sequence and can assess complex environments but are prone to catastrophic failures due to occlusion or adversarial environmental conditions, e.g. lighting conditions. Mixed methods leverage both domains, where the geometric representations are commonly used to fuse the sequential appearance-based~\ac{TE} estimates, the geometric representations are used as data structures, and the assessment relies solely on image modalities~\cite{frey2023fast, chen2023rspmp}.

General late-fusion methods combine different smaller assessments into a unified cost or risk metric, which has been shown to be a versatile general tool for assessing different traversability aspects~\cite{quigley2009ros, frey2024roadrunner, fan2021step}. Classical heuristic costmap fusion~\cite{quigley2009ros} has been replaced by~\ac{covar} risk assessment, where a general notion of risk is generated. It allows one to quantify how ``bad'' a failure case would be~\cite{fan2021step}. Recent work has performed 2.5D~\ac{TE} estimation for high-speed, off-road vehicles in unstructured, grassy environments~\cite{frey2024roadrunner} in large 2.5D maps. However, this assessment of vegetation remains in the image domain and does not consider full 3D. Separation in support surface (ground) and pliable elements has been proposed with different variations~\cite{wellington2004online, bradley2015scene, li2023seeing}, but they rely on the image modality as the discriminator.

Traversability estimation for vegetated environments was explored by~\cite{ahtiainen2017normal} with the~\ac{ndt-tm} representation capturing salient information of the environment at large voxel resolutions (0.4~\si{m}). Ruetz et al.~\cite{ruetz2022FTM} introduced additional features and distributions for smaller voxel representations and a real-time capable system~\cite{ruetz2024foresttrav}. Our prior work ForestTrav, currently the only lidar-based method using 3D voxel representations, demonstrated that geometry, salient voxel features, and context are sufficient for a geometric representation to assess complex environments in real-time~\cite{ruetz2024foresttrav} and allow for save navigation. The work relied on \ac{scnn}, a deep learning method that can implicitly represent missing data, to allow for rapid learning and inference in 3D.  The work fused~\ac{lfe}, self-supervised data from the robot experiencing the environment, and hand-labelled (HL) data to generate a high-quality, combined data set called ~\ac{lfe_hl} data. 

Online or adaptive traversability estimation methods aim to adapt a model to novel environments online on the running robot in the field~\cite{wellington2004online, frey2023fast, kim2006traversability, hadsell2009learning, kahn2020badgr}. They leverage similar principles as self-supervised learning approaches for~\ac{TE} but require the generation of additional training data and adapting the model on the robot in the field. These approaches require a deep understanding of the problem and the robotic system, as well as a computationally efficient implementation to be able to achieve this.

Early works by Kim et al.~\cite{kim2006traversability} demonstrated the use of image modalities to train an online adaptive visual \ac{TE} method. They associated image features with self-supervised labels gathered through interaction using a clustering algorithm. They used classical appearance colour/texton information and contact sensors to associate image and robot experience in a grid representation. Hadsel et al. ~\cite{hadsell2009learning} proposed to train a multi-class classifier online using short-distance measurements and self-supervision as a source for the ground-truth labels. The rule-based classifier was trained online, but the CNN feature generator was trained offline on images of different environments. 

More recent work has leveraged the rapid advancements in deep learning frameworks and hardware in recent years~\cite{kahn2020badgr,frey2023fast,yoon2024adaptive}. 
%
Kahn et al. ~\cite{kahn2020badgr} demonstrated an end-to-end navigation method aimed at outdoor environments. They formulated the problem of the end-to-end method using an action-predictive deep neural network, which, given a set of sequential actions, predicts the following K sequential events. They used off-policy self-labelled data to collect data and allow the system to fail. The resulting method was able to navigate successfully using only images in various outdoor scenarios.

Frey et al. presented an online adaptation method in~\cite{frey2023fast} that relies on self-supervised using images and differences of estimated and realised velocities. A mission graph allows tracking poses and traversability-related signals (velocity differences) and can associate this with features from image segments. These are generated using DINOv2 on image segments generated by the super-pixels algorithm. The CNN estimating the traversability was trained incrementally using batches online, allowing the model to learn in minutes. The pixel-wise \ac{TE} score is projected onto a 2.5D height map used by the local planner. 
The learning objective is formulated as a positive-unknown approach. Self-supervised and labelled data are used as positive examples, and the unlabelled data are used as negative examples, with an additional confidence loss to enforce a compact feature embedding for the positive (traversable class) and a clear separation to the unknown class. The learned traversability is based on the similarity of previously traversed areas and the model's confidence in the labelled and unlabelled data. The resulting behaviours allowed the robot to navigate safely in areas it is confident it has seen before. The natural behaviour that emerges shows the robot prefers an easier path, learning to walk on a gravel path rather than the grass beside it. Further, this method has been demonstrated in various natural, unstructured environments. The authors noted that the models do not generalise well to nearby environments. Compared to the previous method, this method relies on an intermediate geometric representation and classical planner to navigate. 

Yoon et al. ~\cite{yoon2024adaptive} provided an online adaptive learning approach based on a 2D grid-based lidar elevation map for off-road environments. It aims to filter out patches of vegetation for a gravel-based road. Their adaptive replay approach allows for maintaining a data set with the relevant information. Additionally, the authors claim to estimate the aleatoric uncertainty while estimating the traversability of local grid map patches. They demonstrate their work on an off-road gravel path with different densities of vegetation between the tracks.

The work presented in this paper differs as it presents an online adaptive learning method for a probabilistic voxel representation. Whilst the community has generated a large amount of understanding for universal feature generation and online learning on images, the space of 3D probabilistic representation has not been well explored. A key critical difference between using 3D-voxel distributions and images as network input is that the voxel distributions evolve over time and need to be updated; they need sufficient measurements to generate a good representation of the environment of traversability through self-supervision. The concepts known from the image domain do not translate well, e.g. storing 3D maps at fixed intervals. This produces large quantities of duplicate and potentially conflicting data, e.g. when the traversability of a voxel changes due to the self-supervision. The large amount of data leads to rapidly increasing training times. This proposed method addresses these issues.

