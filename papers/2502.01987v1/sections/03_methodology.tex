\section{METHODOLOGY}
\label{ch7:methodology}
The method presented in this paper allows for human-assisted online~\ac{TE} to rapidly adapt to a novel environment. This is achieved through a human operator directing the robot to interact with the environment to experience safe traversal as well as collision states. The learnt traversability model adapts to the new experience data to improve its future \ac{TE} predictions in the novel environment. An overview of the method can be seen in \figref{fig:odap_overview} and shows the novel contribution that allows the method to learn and adapt online. The online 3D probabilistic mapping (blue block) is adapted from our previous work ForestTrav~\cite{ruetz2024foresttrav}, and we use a similar architecture for voxel-wise \ac{TE} but with a single neural network instead of an ensemble due to computational constraints. 

\begin{figure*}[ht]
 \centering
 \includegraphics[width=\textwidth ]{figures/ruetz1.png}
 \caption[Online Adaptive TE Method Overview]{The overview of the online~\ac{TE} adaptation method. The \textbf{ForestTrav} method is augmented with new modules to facilitate online learning. The online node generates and trains the new model from the self-supervised labelled data collected online, stored as a graph representation. A fusion module generates the data, combining the collision map and 3D probabilistic environmental representation at fixed-time intervals into the \ac{ograph}.}
 \label{fig:odap_overview}
\end{figure*}

The novelty of this work is the addition of the online learning capability, allowing the TE model to adapt online using in-situ robot experience only (green block). The learning module starts a learning cycle at discrete times $t_k$ and occurs at fixed-time intervals $\delta_{t}$.  
%
A human operator starts the online learning process if the current~\ac{TE} model shows insufficient performance in a new environment. During online adaptation, the operator controls the robot using an RC remote and records the collisions, allowing it to capture relevant robot-environment interaction while minimising the risk of damaging the system. The collision states and associated poses are used to generate the probabilistic collision map $\mathbf{M}^C$, described in Section~\ref{subsec:collision_mapping}. The online update process is stopped after a pre-defined condition (in this implementation, we use a fixed-time interval), after which the robot can continue its mission autonomously.

The collision map at $t_k$ is fused with all 3D probabilistic maps $\mathbf{M}_{t_{k-1}:t_{k}}$ between the end of the previous learning cycle $t_{k-1}$ and the current one $t_k$. The resulting fused \ac{lfe} representation is the online, self-labelled map batch that is the current best belief of the \ac{lfe} map and is used to update a sparse graph of the environment, \ac{ograph}. Each graph node is associated with a keypose (similar to keyframes) of the trajectory traversed by the robot. The edges maintain the ordered poses of the trajectory and help with the rapid traversal of the graph. Each node contains a locally-fused~\ac{lfe} map, which is used as a training sample. The graph serves as a sparse solution for the fused~\ac{lfe} map since maintaining a global map is not computationally feasible. It still allows us to update and maintain the temporally evolving collision states and distribution of the 3D map of the environment that the robot experienced.

Finally, \ac{ograph} is used by an online training module that can either train or finetune the model online using the submap of the nodes as training samples. The method trains the model for a small fixed number of training epochs ($n_{adapt}$) to ensure learning finishes before the next adaptation cycle starts. Once a model is trained, the weights of the \ac{scnn} of the traversability estimation module are updated with the newly trained one.

The last block (\figref{fig:odap_overview}) captures the generation of the conversion of the 3D~\ac{TE} map to a valid 2D costmap. This block estimates the ground, associates a traversability cost for each ground cell and generates estimates for unseen cells (virtual costs). The resulting costmap is then passed to a planner.

In this paper, we examine two different scenarios in the online-learning context. In the first scenario, a pre-trained model is available, but the training data is not. We assume the pre-trained model has been trained on high-quality data; in the case of this paper, we use~\ac{lfe_hl} data. In the second scenario, a randomly initialised (untrained) model is trained from scratch on the online data set. 

A key critical difference between using 3D-voxel distributions and images as network input is that the voxel distributions evolve over time and need to be updated. They need sufficient measurements to generate a good representation of the environment and of traversability through self-supervision. 

\subsection{Problem Definition}
\label{subsec:problem_def}
This work assesses the traversability of unstructured, natural, vegetated environments in full 3D to allow for the autonomous operation of \iac{agv}. Each voxel $m \in \textbf{M}$ of the voxel map $\mathbf{M}$ is assumed to have a true binary, exclusive traversability state $\tau \in \{ TR, NTR\}$ where $m_\tau = TR$ means that the voxel $m$ is traversable (the positive case) and $m_\tau = NTR$ means that cell $m$ is non-traversable (negative case). To estimate this traversability state, we store a vector of probabilistic features in each voxel, with features estimated from lidar observations. We use a learning approach that learns a parametric function $ p_\tau (\tau=TR | M) = f_\Theta(x): \mathbb{R}^n \rightarrow \left[0, 1\right]$, where $\Theta$ are function parameters, $x \in \mathbb{R}^n$ are the input features and $p_\tau$ is the belief that the voxel is traversable given the state of the 3D probabilistic voxel map. 
 
\subsection{3D Proablistic Representation}
\label{subsec:map_rep}
In this work, we use a 3D probabilistic voxel map $\mathbf{M}$ to represent the environment. The map $\mathbf{M}$ is a tessellation of non-overlapping 3D cubes called voxels $m$. Each voxel stores and maintains a probabilistic estimate of its state from a continuous stream of observations, referred to as voxel distributions. Voxel distributions are assumed independent of the adjacent voxels. These distributions are generated and updated continuously with the incoming stream of sensor data, making them time-dependent. This work considers additional distributions beyond the commonly-used occupancy~\cite{hornung2013octomap}, and the different tracked distributions are denoted with a suffix $X$ for a single voxel, i.e. $m_X$. The different distributions are commonly referred to as layers of the map $\mathbf{M}$, i.e. the layers that capture a particular property. For example, the occupancy layer of the map $ \mathbf{M}_{OCC} \subset \mathbf{M}$ contains only the occupancy distribution of the voxel $m_{OCC} \in \mathbf{M}_{OCC}$.

% This work refers to the generic voxel map $\mathbf{M}$ when there are no temporal considerations. $\mathbf{M}_{Post}$ refers to the voxel map generated in a post-processed offline fashion, where all observations are integrated at once into a single map. The voxel map relying on online odometry or SLAM is denoted as $\mathbf{M}(t)$. The online map evolves over time. 

\subsubsection{Voxel Distributions and Features}
\label{subsubsec:voxel_dist}
In this work, we track the following mix of distributions and features shown below: 

\begin{equation}
    \label{eq:dist}
      \begin{array}{ll}
        \textrm{NDT-OM dist:} & m_{OCC} = [p_{OCC}, N_{OCC}, \mu_{NDT}, \Sigma_{NDT}] \\
        \textrm{NTD-TM dist:} & m_{ndt-tm} = [N_{hit}, N_{miss}] \\
        \textrm{Intensity dist:} & m_{I} = [\mu_I ,\sigma_I] \\
        \textrm{Muti-return: } & m_{MR} = [N_{MR} ] \\
    \end{array} 
\end{equation}

At the core of the representation is the \ac{ndt-om}~\cite{saarinen2013normal} representation $m_{OCC}$. For each voxel, the occupancy $p_{OCC}$ is associated with a 3D spatial Gaussian distribution (parameterised by the mean $\mu_{NDT}$ and covariance $\Sigma_{NDT}$) of the endpoint locations of lidar observations ending in that voxel. This differs in two critical ways from the commonly used binary occupancy representation. Firstly, the 3D Gaussian can be used to represent elements at sub-voxel resolution and estimate more complex geometry. Secondly, this representation does not assume that the whole voxel is occupied but rather associates the occupancy with the 3D Gaussian distribution. By considering the proximity of a ray to the distribution of a voxel, the resulting representation is more resilient to erosions of thin elements. 

From~\ac{ndt-tm}~\cite{ahtiainen2017normal}, we leverage the notion of permeability, the likelihood of a ray passing through a distribution using the statistical number of hits and misses $N_{hit}$ and $N_{miss}$. The permeability describes how the chance of a ray passing through a voxel due to many small or scattered elements. This frequently occurs in nature, e.g. patches of leaves or grass blades, and has shown to be a salient descripor~\cite{ahtiainen2017normal}. Further, the intensity mean and variance of the laser returns have been shown to help differentiate chlorophyll-rich vegetation from other elements in the environment. In practice, the VLP-16 used has not seen a clear separation as reported in~\cite{ahtiainen2017normal}, but we have seen that this does help discriminate the environment in our prior work~\cite{ruetz2022FTM}. 

Lastly, we track the number of second returns per voxel $N_{MR}$ associated with the first return; the VLP-16 used provides two returns. We can observe second returns on elements with thin or sharp edges, e.g. leaves, thin stems or blades of grass. 

We use the parameters defining the distributions directly as features, thereby avoiding any feature computation. E.g. the inclination from the Eigen Value decomposition the covariant matrix. 

\subsubsection{Self-Labelling Through Collision Mapping}
\label{subsec:collision_mapping}
This work leverages robot experience by combining the observed collision states $c_t = \{TR, NTR\}$ with the robot state $s_t \in \text{SE}(3)$ (3D position and orientation) for all times $t$ and fusing them into a collision map using a stationary Bayesian filter. The formulation is similar to that used in occupancy mapping, but the robot itself is treated as a sensor. The likelihood of a voxel being traversable or collision-free is given by the likelihood $p_{m_\tau} = p( m_\tau| c_{1:t}, s_{1:t})$, and the corresponding log-likelihood formulation is:
%
\begin{equation}
    \label{eq:lodds_cm}    
    l(m_{\tau} | c_{1:t}, s_{1:t}) = l(m_\tau | c_t,  s_{t}) +l(m_{\tau} | c_{1:t-1}, s_{1:t-1})
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/ruetz2.png}
    \caption{Visualisation of the Learning from Experience of the robot. The top image shows the robot in a real-world scenario, with green for the collision-free state and red for the collision state. The middle image shows the robot labelling the 3D probabilistic map as collision-free, ellipsoid representing the NDT representation of a voxel. The bottom image shows the robot in collision, labelling the voxels as non-traversable in the bound box at the front of the robot, with the bounding volume extending a short distance beyond the robot chassis.}
    \label{fig:robot_collisions}
\end{figure}

The first term $l(m_\tau | c_t,  s_{t})$ is the ``inverse sensor model''. A bounding box is generated for each robot state $s_t$, and all voxels intersecting with the bounding box are updated with the associated collision state $c_t$ at time $t$ with a fixed probability, see \figref{fig:robot_collisions}. The subset of the voxels that intersect with the bounding box of a collision state is referred to as ``collision state voxels'' $\hat{m}_{C}$ for the current robot state $s_t$ and collision state observation $c_t$. The update probabilities given robot collision state $p_{\tau}= p(m_\tau = TR | c_t, s_{t}) $, and $p_{NTR}= p( m_\tau = NTR | c_t, s_{t})$ are either decreasing or increasing the belief that a voxel is non-traversable. In the case of collisions, the volume starts behind the front plate and extends beyond the robot chassis one to two voxels beyond the robot chassis. Specifically, in our case, it start\qty{0.1}{\m} behind the front of the robot chassis and extends 1-2 voxels beyond it, \qty{0.2}{\m}  at \qty{0.1}{\m} voxel resolution. The distance threshold values were found heuristically based on the environment and the voxel resolution. The rationale is that obstacles preventing the robot from advancing are commonly larger elements, such as tree trunks or thick, bramble bushes. Extending the bounding box beyond the robot's direct interaction point allows the method to capture the bulk or mass of the obstacles.

The voxels labelled through the robot interaction are called \ac{lfe} data. The post-processed data are generated by hand-labelled data combined with \ac{lfe} data, denoted as \ac{lfe_hl} data; see ForestTrav for additional details~\cite{ruetz2024foresttrav}. There is a difference in maps from LfE and LfE+HL data due to the trajectory errors. LfE data relies on the trajectory of the online SLAM method, which has a larger error than the post-processed trajectory used for the LfE-HL data~\cite{ramezani2022wildcat}. 
 
\subsection{Model Architecture}
\label{subsec:model_def}
The estimator and features used in this work were initially presented in ForestTrav~\cite{ruetz2024foresttrav}. In comparison, this work relies on a single model and not an ensemble due to the computational constraints of online training. The estimator architecture is a UNet encoder-decoder network with skip connections, a commonly-used architecture in many different classification or segmentation methods~\cite{uzawa2022end}. To leverage the structure and efficiency of voxel-based representations, this work uses~\acp{scnn}~\cite{tang2022torchsparse}. \Iac{scnn} implicitly represents empty or missing data for 3D representations, allowing for rapid inference and learning, and implicitly encodes context by the structure of the representation. The training loss function is the binary cross entropy loss~\cite{good1952rational} between the traversability prediction $p_\tau$ and the label $m_\tau$, summed across all labelled voxels. Specifically, this work uses a 4-layer UNet architecture with a 16-channel input layer, fully connected skip connections and convolution kernel size $k=[1,2,2,2]$, see~\cite{ruetz2024foresttrav}.


\subsection{Online Data Generation}
\label{ch7:ograph}
Maintaining a global dense voxel map is computationally infeasible due to the intractable amount of memory required. Thus, we introduce a novel data fusion approach to combine the different distributions online into a graph representation. The data is stored in a graph-based representation \ac{ograph}, where each node \textbf{n} is represented as a 3D point and is a position of the robot trajectory in the global static frame. A new node is generated from a previous robot position if its position of any other node exceeds a threshold $d_{node} = 0.5m$. Each node contains a small, sparse local voxel map with all the voxel distributions and the associated collision probabilities, not the fully dense collision map.  The local map of each node only contains information in a cylinder around the position of the node ($r < r_{max}$, $z_{min} < z < z_{max}$). The assumption is that the probabilistic voxel and collision maps will converge to a steady state with sufficient measurements. This is highly dependent on viewpoint, the distance between the sensor and the robot, and the environment in general. Under this steady-state assumption, the voxel distributions are updated with the newest measurements when available. This means the distributions within the voxel can come from different temporal maps $\mathbf{M}(t)$. The benefit of representing the environment based on nodes is to collapse many poses into a sparse representation and allow the method to update only a (comparatively) small amount of data each time. The graph is updated at fixed intervals, dictated by the time allowed between training cycles $\delta_{t}$.

\subsubsection{Map Batch Generation}
In the first step, an intermediate representation, called ``Online, Self-labelled Map Batch'' as seen in \figref{fig:odap_overview}, is generated. The figure shows the map that is generated between the training cycles $t_{k-1}$ and $t_{k}$ with $j$ discrete time steps between the two. At each time step, a snapshot of the local map around the robot and the robot states are buffered. The labels are generated through \ac{lfe}, and the same colour schemes are used as above for illustration. The grey elements are the unlabelled data, and each visualised point contains the voxel distributions. Formally, the ``Online, Self-labelled Map Batch'' is the fusion of the temporal voxel map $\mathbf{M}_{t_{k-1}:t_{k}}$, the robot states $s_{t_{k-1}:t_k}$, and the collision map at $\mathbf{M}_{t_k}^C$. The temporal fused map $\mathbf{M}_{t_{k-1}:t_k}$ is generated by stitching together the $j$ maps to generate a single large one. Each temporal slice $\mathbf{M}(t)$ contains only a small, cylindrical probabilistic map around the pose $s(t)$. The buffering of $\mathbf{M}(t)$ ensures the availability of all the maps in the interval; the robot may move further than the bounds of its dense, probabilistic robot-centric map. When combining the probabilistic maps of different times, a given voxel may have multiple distribution values that differ (as they evolved over time). In the case of multiple distributions, the newest measurements are used under the steady-state assumption for voxel distributions. 

The collision map $\mathbf{M}_{t_{k-1}:t_k}^C$ is generated as explained in Section~\ref{subsec:collision_mapping} using the probabilistic formulation. Only the collision states (poses and collision label) are stored in practice. Note that the collision map is generated using all previous collision states and robot poses, and the dense collision map is deleted as soon as it is fused with the feature map. The collision states are fused into the temporal voxel map to form the fully fused representation. This unified map is then used to update the graph.

\subsubsection{Graph Update}
The graph is updated in two sequential steps. The first step generates the new set of nodes, given the new robot states $s_{t_{k-1}:t_k}$. The states are processed in ascending order of time. As an initial step of the update, new nodes are allocated for all the poses that are a minimum distance from all other existing nodes in the graph. Newly allocated nodes are initialised, with all measurements falling into their local regions. In the second step, all existing nodes in the graph need to be updated if the data of the ``Online, Self-labelled Map Batch'' overlaps with the map of a node. For each node, a voxel-wise association of the new data is performed and updated (with the steady-state voxel assumption). Since the context or surroundings are necessary for~\ac{scnn}, we have found it beneficial to have overlapping data, where data can be in one instance on the fringe of the local map and in another instance in the centre of a local map. The radius of the cylinder and other values are guided by the robot's dimensions and the width of the cubes used for training, as proposed in ForestTrav~\cite{ruetz2024foresttrav}. Specifically, 32 voxel width at \qty{0.1}{\m} voxel resolution for a cube width of \qty{3.2}{\m} .
 
\subsection{Online Learning Module}
The online adaptation module trains or finetunes a model using incremental online data. With each update of \ac{ograph}, a ``training cycle'' starts. For each ``training cycle,'' a single model is trained for a fixed number of epochs. The estimator is then updated with the new weights. The online training module can only train one model at a time and in cycles due to the constraints of the CPU and GPU. If the training of a model takes more than the allocated time, the data of the missed training cycle is merged into the next one. 

\subsection{Costmap Generation}
\label{subsec:cotmap_generation}
The 2D costmap is generated in three steps from the 3D~\ac{TE} map: 1) support surface map, 2) virtual surface estimation, and 3) cost conversion, which are detailed below. A costmap is shown in \figref{fig:costmap_gen}. There are three colourisations: traversable, non-traversable and virtual. The green cells are traversable but also contain a cost. A traversable cell may have high costs if they are close to non-traversable elements. The virtual costs that are traversable are shown as dark blue and induce an additional cost for the planner of cells that have not been observed. The red cells are the fatal cells that the robot is not allowed to interact with. 

\begin{figure}[ht]
 \includegraphics[width=\columnwidth]{figures/ruetz3.png}
 \caption{The Left image shows a 3D traversability map, and the right image shows a costmap with traversable (green), non-traversable (red) and virtual surfaces. The traversable elements can have low or high costs. Virtual cells are cells that have not been directly observed, and the cost is estimated based on their surroundings. }
 \label{fig:costmap_gen}
\end{figure}


\subsubsection{Support Surface Generation}
For a given~\ac{TE} map, the ground or support surface is estimated per column of the map, i.e. for fixed $x$ and $y$ coordinates and variable $z$ coordinates in a gravity-aligned map. In each column of the map, the voxel with the lowest $z$ coordinate containing measurements and not higher than a heuristic limit ($z_{max}$) with respect to the robot is defined as the ground voxel. The limit $z_{max}$ is required to avoid considering overhanging elements as ground voxels, e.g. branches or tree canopy. The mean traversability probability of a cell of the costmap is the mean average of the~\ac{TE} of all the $N$ voxels above and including the ground voxel \ac{TE}. In our experiments, we use $N=10$ with voxel size \qty{0.1}{\m} to ensure only the area the robot passes through is assessed and to ignore elements far above the ground.

\subsubsection{Virtual Surface Estimation}
\label{subsec:virtual_surface_est}
In complex environments such as the dense vegetation we are tackling in this work, there can be frequent occlusions due to clutter, resulting in the support surface map containing many holes or unobserved cells.  Hence, we employ a two-stage virtual surface generation that estimates the mean traversability probability for unobserved or virtual cells from their surroundings. A two-stage approach is used. The virtual cells are generated by averaging the traversability probability and height $z$ using a $K \times K$ averaging kernel around an empty cell, given a minimum number of non-empty neighbours $N_{Adj}$. This process is run twice. In the first iteration,  the neighbours need to be non-virtual to be considered. This results in growing virtual estimations around existing measurements. The second iteration differs in that virtual surfaces are now also used to generate the estimates and will flood-fill all the gaps. Using a flood fill directly can introduce unwanted artefacts.

\subsubsection{Cost Conversion}
Finally, a valid costmap requires a cost function to transform the mean cell \ac{TE} $\bar{p}_\tau \in [0,1]$ to a cell cost $c \in [0,\inf)$. An exponential decay function is used to encourage planning in low-cost areas
\begin{equation}
 \label{eq:2d_cost}
 c = \left\{
 \begin{array}{ll}
 10.0*exp{(-6.0\cdot \bar{p}_\tau^2)} + 1.0 & \textrm{if} \quad p_\tau > \lambda_{\tau} \\
 \inf & \textrm{otherwise}
 \end{array} \right.
\end{equation}
where $c$ is the cell cost, $\bar{p}_\tau$ is the mean \ac{TE} and $\lambda_\tau$ is the traversability threshold. Fatal cells are all the cells whose cell cost is above the threshold; they are set to infinite and are visualised as red in the costmap in~\figref{fig:costmap_gen}. The exponential cost decay encourages the planner to find paths with small costs and thus take less risky paths. However, some of the green cells in the costmap can have high cell costs, close to the fatal threshold.  

The 3D to 2D costmap generation is a heuristic-based conversion. Values used for online deployment $N=10$, resulting in considering the cost map to the height of the robot with padding, $z_{max} = 0.5$, the minimum required number of neighbours is $N_{Adj} = 5$, with a $5x5$ mean averaging kernel, and the threshold of $\lambda_\tau = 0.3$. These values were determined heuristically by an expert with knowledge of the environment, the robot dimensions and observation in the field.


\subsection{Platform, Implementation Details and Learning Parameters}

\subsubsection{Platform and Sensor Suite}
To implement and validate the proposed approach, the \qty{35}{\kg} tracked platform Dynamic Tracked Robot (DTR),  shown in \figref{fig:robot_collisions}, was used with a sensory pack. The sensory pack consists of a rotating Velodyne VLP-16 and IMU. The lidar is angled at \qty{35}{\degree} and performs a complete revolution at \qty{2}{\Hz}. A detailed description of the navigation software CSIRO NavStack can be found in~\cite{HudTal21}, and the local planner we use is the hybrid A* planner~\cite{dolgov08}. The robotic platform runs an NVIDIA Jetson Orin NX (\qty{25}{\watt}, \qty{16}{\giga\byte} RAM and 100 TOPS) for the learning component of this work.

\subsubsection{Learning and Architecture Parameters}
\label{ssubsec:learning_params}

For training the model, the learning rate was set to $5\times10^{-4}$, weight decay $9\times 10^{-4}$, batch size $64$, and maximum epoch number $150$ for post-processed models and $40$ for incrementally-trained models. The ADAM optimiser was used. A ten-fold cross-validation was used with a held-out test set for all data processed. Model weights were generated, stored and retrieved using the Python PyTorch library ~\cite{paszke2017automatic} with the TorchSparse extension~\cite{tang2022torchsparse}. 

The results using post-processed data were computed on a notebook Dell Precision 5700, \qty{64}{\giga\byte} RAM, Quadro RTX 5000 Mobile / Max-Q with \qty{16}{\giga\byte} video RAM. We aimed to achieve a training cycle every $\delta_t = 40s$ and have tuned the pipeline accordingly. This results in the collision map, the \ac{ograph} update and the online learning module running at the same rate. The empirically found duration of training cycles is a balance between generating sufficient new updates, allowing for the processing of the updates and accommodating overheads for the continuation of the training. 

\subsubsection{Online Graph Generation Parameters}
The local map of each node contains all the voxels that are contained in the cylinder defined up to $r_{max}=\qty{2}{\m}$. This is influenced by the robot's size and the patch size of previous work. The cylinder height is constrained by $z_{min} = \qty{-0.5}{\m}$ and $z_{max} = \qty{0.8}{\m}$ from the base link, with \qty{0.2}{\m} padding. 