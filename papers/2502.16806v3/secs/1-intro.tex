
Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks \citep{achiam2023gpt, touvron2023llama, jiang2023mistral, jiang2024mixtral, guo2025deepseek}. However, their deployment in real-world applications is often hindered by high computational costs, memory constraints, and latency issues. These limitations pose significant challenges to deploying LLMs efficiently on resource-constrained devices like mobile phones and IoT devices. Knowledge distillation (KD) \cite{hinton2015distilling} has emerged as a promising solution to this challenge by transferring knowledge from a large teacher model to a more compact student model, thereby retaining essential performance while reducing computational overhead. Conventional KD approaches generally seek to align the output distributions of teacher and student models through methods like Kullback-Leibler (KL) divergence \citep{zhang2023not, hsieh2023distilling, ko2024distillm}. 

For LLMs, knowledge distillation can be categorized into two main approaches: black-box KD and white-box KD. In black-box KD, the student model learns by mimicking the teacher model’s outputs, as it has no access to the teacher’s internal structure or variables \cite{fu2023specializing, kim2016sequence}. In contrast, white-box KD enables the student model to utilize the teacher model’s architecture and variables while constructing regularization constraints during training. Theoretically, this approach facilitates more comprehensive knowledge transfer, leading to superior performance \cite{wen2023f, gu2024minillm, ko2024distillm}. However, a fundamental limitation of these approaches is that they assume both models share the same vocabulary and tokenizer, a requirement that is increasingly impractical given the diversity of architectures and tokenization schemes used in contemporary LLMs. 

Several recent studies have attempted to address this issue by enabling KD across models with different tokenizers. For example, the Universal Logit Distillation (ULD) method \cite{boizard2024towards} utilizes Optimal Transport (OT) to align probability distributions at the token level across different vocabularies, offering a more flexible approach to distillation. Similarly, the Dual-Space Knowledge Distillation (DSKD) framework \citep{zhang2024dual} introduces a cross-model attention mechanism to unify output spaces, facilitating KD between models with non-overlapping token sets. Despite their advancements, these methods primarily emphasize a single aspect of knowledge transfer, overlooking a more comprehensive distillation process- \textbf{reasoning-aware distillation}. While LLMs are highly effective due to their advanced reasoning capabilities \citep{wei2022chain, huang2023towards, guo2025deepseek}, existing KD approaches for different vocabulary often overlook this critical component. Instead, they predominantly focus on aligning the final output, which may restrict the student model’s ability to improve reasoning skills and generalization capacity. Several KD methods for models with similar vocabularies have examined the effectiveness of CoT-augmented data in the distillation process \citep{ho2022large, hsieh2023distilling, ranaldi2024aligning}. However, these approaches integrate augmented data into student training without additional safeguards, increasing the risk of propagating flawed reasoning and hallucinations \citep{tonmoy2024comprehensive}, which may lead to final responses misaligned with the ground truth.

% \begin{itemize}[leftmargin=*]
%     \item \textbf{Restricted to Token-wise Alignment:} DSKD relies on a simple linear projection to transform distributions into the same dimensional space for each token, which may not fully capture the complex relationships between teacher and student representations, leading to potential information loss. Meanwhile, ULD requires trimming the length of output tokens to achieve token-wised alignment between the teacher and student models, which can result in incomplete sentences and misalignment of tokens with differing contextual meanings, potentially distorting the semantic integrity of the distilled knowledge.
    
%     \item \textbf{Lack of Reasoning-Aware Distillation:} These methods primarily emphasize a single aspect of knowledge transfer, overlooking a more comprehensive distillation process. While LLMs are highly effective due to their advanced reasoning capabilities \citep{wei2022chain, huang2023towards, guo2025deepseek}, existing KD approaches for different vocabulary often overlook this critical component. Instead, they predominantly focus on aligning the final output, which may restrict the student model’s ability to improve reasoning skills and generalization capacity.

%     \item \textbf{Insufficient Experiments:} Existing works \cite{zhang2024dual,wu2024rethinking,gu2024minillm}  often rely on a single dataset for distillation and evaluate performance across multiple datasets with varying domains or tasks during testing. While the reported results are based on the best checkpoint across multiple tasks, our analysis reveals that this checkpoint may not be reliable to assess the distillation process on the in-domain training dataset. Extending the training process can improve test performance within the same domain as the training data but tends to degrade results on other test datasets. Consequently, the reported performance may not reflect the full potential of these KD techniques in constructing a specialized small model distilled from a teacher model for domain-specific deployment applications.
% \end{itemize}

To overcome these challenges, \textbf{\textit{(1)}} we present a universal framework that improves distillation for models with different vocabularies by emphasizing additional aspects, particularly the teacher model’s reasoning ability. To achieve this, we integrate Chain-of-Thought (CoT) augmentation into the distillation process. Building on this, we introduce a novel Cross-CoT Alignment method that effectively transfers the teacher’s reasoning capability to the student model. Specifically, we design two alignment loss functions to encourage the student model to capture the teacher’s multi-step reasoning process: \textit{(a)} directly aligning the outputs of both models using the same input formats (standard and CoT) and \textit{(b)} aligning CoT and standard outputs to enhance the reliability of the CoT process in generating correct answers. However, aligning these outputs requires handling different vocabulary mappings. While existing KD methods for models with different vocabularies offer a potential proxy, we argue that these approaches are constrained by \textbf{token-wise alignment}. For instance, DSKD employs a simple linear projection to map distributions into a shared dimensional space for each token, which may overlook intricate relationships between teacher and student representations, leading to potential information loss. Similarly, ULD trims output token sequences to enforce token-wise alignment between teacher and student models, which can lead to incomplete sentences and misaligned tokens with varying contextual meanings, potentially distorting the semantic integrity of the distilled knowledge—especially when direct responses and CoT responses differ significantly in length. Therefore, \textbf{\textit{(2)}} we introduce a sequence-level alignment approach that operates without requiring projection into a uniform dimensional space or enforcing identical output lengths. Specifically, while we acknowledge OT as an effective solution for aligning the distinct distribution spaces of teacher and student models, as demonstrated in ULD, we extend its application beyond token-wise alignment. Instead, we propose \method, a sequence-level alignment method combined with a layer-by-layer approach to enhance reasoning knowledge transfer. This method effectively adapts to sequences of varying lengths, ensuring comprehensive output context alignment while preserving the integrity of the distilled knowledge. \textbf{\textit{(3)}} Our comprehensive experiments underscore the overall effectiveness of \method and the contribution of each proposed technique in enhancing existing universal KD methods. Additionally, when conducting domain-specific distillation across a wide range of tasks, we observe that the best-performing method, DSKD, exhibits limited superiority over other KD methods, contradicting the claims presented in their study. This finding suggests a lack of robustness in DSKD for domain-specific experiments, revealing gaps in its insights. In contrast, our approach demonstrates consistent improvements in this scenario, emphasizing its reliability and adaptability.

% existing techniques by incorporating an additional sequence-level perspective. This method operates without necessitating projection into a uniform dimensional space or imposing identical output lengths, thereby enriching information distillation across multiple views. Specifically, we recognize OT as an effective solution for aligning the distinct distribution spaces of teacher and student models, as demonstrated in ULD. However, rather than restricting alignment to the unnatural token-wise level, we propose \method, a sequence-level alignment method integrated with a layer-by-layer approach to improve the effectiveness of knowledge transfer. This method enables adaptation to sequences of different lengths, ensuring full output context alignment and preserving the integrity of the transferred knowledge. \textit{\textbf{(2)}} Furthermore, to enhance distillation by focusing on multiple aspects, particularly the reasoning ability of the teacher model, we incorporate Chain-of-Thought (CoT) augmentation during distillation process. Leveraging the augmented data, we propose a novel Cross-CoT Alignment method, which effectively facilitates the transfer of the teacher model’s reasoning capability to the student, thus strengthens the student model’s ability to generate accurate and contextually coherent outputs. \textit{\textbf{(3)}} Finally, our comprehensive experiments underscore the overall effectiveness of \method and the contribution of each proposed technique in enhancing existing universal KD methods. Additionally, when conducting domain-specific distillation across a wide range of tasks, we observe that the best-performing method, DSKD, exhibits limited superiority over other KD methods, as discussed in their work. This finding suggests a lack of robustness in DSKD for domain-specific experiments, revealing gaps in its insights. In contrast, our approach demonstrates consistent improvements in this scenario, emphasizing its reliability and adaptability.

%Finally, to fully harness the potential of KD methods, we adopt scenarios where KD is applied within the same specific domain for both training and testing phases. This approach ensures the teacher model is fully optimized, allowing for a reliable and robust assessment of the effectiveness of knowledge transfer through KD methods to the student model for handling a specific target task.

% In summary, our main contributions are as follows:

% \begin{enumerate}[leftmargin=*]
% \item We introduce \method, an innovative Optimal Transport-based alignment technique that facilitates sequence-level alignment across different layers of teacher and student models, eliminating the need for identical tokenization or output lengths.

% \item We incorporate Chain-of-Thought (CoT) augmentation and propose Cross-CoT Alignment to enhance the transfer of reasoning abilities from teacher to student models. thereby enhancing the quality of the generated outputs.

% \item Comprehensive experiments demonstrate that our \method framework outperforms existing KD methods across both identical and differing tokenizer vocabularies, demonstrating its effectiveness and broad applicability in knowledge distillation.
% \end{enumerate}

% \begin{table*}[h]
% \centering
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Methods}          & \textbf{Dolly} & \textbf{Alpaca} & \textbf{Self-Instruct} & \textbf{Dialogue Sum} \\ 
% \hline

% \multicolumn{5}{c}{\textbf{Mistral $\to$ TinyLLaMA}} \\ 
% \hline
% ULD                       & 25.75 ± 0.18  &                 &                       &                       \\ 
% ULD+SeqKD                 & 25.79 ± 0.52  & 31.47 ± 0.29   & 25.47 ± 0.19          & 33.29 ± 0.19          \\ 
% MinED                     & 25.54 ± 0.59  &                 &                       &                       \\ 
% MinED+SeqKD               & 26.01 ± 0.40  & 31.38 ± 0.74   & 25.60 ± 0.21          & 33.21 ± 0.16          \\ 
% DSKD-CMA                  & 26.28 ± 0.35  &                 &                       &                       \\ 
% DSKD-CMA+SeqKD            & 25.65 ± 0.26  & 31.88 ± 0.31   & 26.89 ± 0.35          & 33.28 ± 0.18          \\ 
% \hline
% \end{tabular}
% \caption{Methods using SeqKD}
% \label{tab:methods_seqkd}  
% \end{table*}





% \begin{table*}[h]
% \centering
% \begin{tabular}{|l|c|c|c|}
% \hline
% \textbf{Methods}          & \textbf{Alpaca}                  & \textbf{S-NI}            & \textbf{Dialogue Sum}             \\ 
% \multicolumn{4}{c}{\textbf{Mistral $\to$ TinyLLaMA}} \\ 
% \hline
% ULD                        & 24.15\textsubscript{$\pm$0.14}     & 24.79\textsubscript{$\pm$0.14}     & 11.86\textsubscript{$\pm$0.16}     \\ 
% ULD FT   & \textbf{31.33\textsubscript{$\pm$0.36}} & \textbf{26.55\textsubscript{$\pm$0.10}} & \textbf{33.69\textsubscript{$\pm$0.26}} \\ 
% MinED                      & 23.57\textsubscript{$\pm$0.34}     & 23.63\textsubscript{$\pm$0.20}     & 11.87\textsubscript{$\pm$0.05}     \\ 
% MinED FT & \textbf{31.82\textsubscript{$\pm$0.33}} & \textbf{26.13\textsubscript{$\pm$0.23}} & \textbf{33.31\textsubscript{$\pm$0.16}} \\ 
% DSKD                       & 24.81\textsubscript{$\pm$0.50}     & 24.58\textsubscript{$\pm$0.17}     & 13.20\textsubscript{$\pm$0.12}     \\ 
% DSKD FT & \textbf{32.31\textsubscript{$\pm$0.15}} & \textbf{26.74\textsubscript{$\pm$0.24}} & \textbf{33.44\textsubscript{$\pm$0.18}} \\ 
% \hline
% \end{tabular}
% \caption{Old settings and our settings}
% \label{tab:previous_setting}
% \end{table*}




% \begin{table*}[h]
% \centering
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Methods}          & \textbf{Dolly} & \textbf{Alpaca} & \textbf{Self-Instruct} & \textbf{Dialogue Sum} \\ 
% \hline 
% \multicolumn{5}{c}{\textbf{Mistral $\to$ TinyLLaMA}} \\ 
% \hline
% DSKD-CMA                 &                &                 &                       &                   
% \\
% DSKD-CMA+OT           &                &                 &                       &     
% \\
% DSKD-CMA+OT+Rationale           &                &                 &                       &     
% \\
% \hline
% \end{tabular}
% \caption{Ablation study}
% \label{tab:methods_ablation}  % <-- UNIQUE LABEL
% \end{table*}


% \subsection{motivation}

% - training in own setting when fit to dolly then others dataset will decrease . refer to old paper dskd
% - domain adaption 
% - previous method like seqkd attempt to distil at sequence level but nêu ra điểm yếu seqkd => we propose a novel method distil at sequence level using ot since difference of tokenizer 
% - tạo 1 cái bảng seqkd + dskd ... seq kd + minedit ... 

% - explain perform bad that uld not token by token, 
% minedit is just find similar token 
% - the dskd in our setting performance which not outperform like there setting 
