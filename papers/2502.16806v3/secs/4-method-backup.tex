\subsection{Optimal Transport for different tokenizer sequence alignments}

The student and teacher architectures follow conventional LLMs where it consists of an embedding layer, following by several attention blocks and a fully-connected layer with softmax head to predict the next token. In the conventional knowledge distillation \cite{hinton2015distilling}, aligning the last layer's geometry involves ensuring that the logits (the raw, unnormalized outputs of the last layer) of the student closely match those of the teacher, hence it encourages the student to generate same tokens as teacher. However, this approach does not support the teacher to guide the student in each intermediate layers. The natural idea is to encourage the inner layers of student mimic the inner layers of teacher, then the student not only learn the geometry of last layer, but also learn the intermediate representations of teacher to inference the next token. In order to align layers between student and teacher, we propose a loss function named $OT\_layer\_loss$ - a OT-based method that can align representations of two arbitrary-length sequences. With this novel loss, we can align any inner layers of student and teacher model.

Specifically, let consider two sequences $\textbf{x}_{1:N}, \textbf{y}_{1:M}$ with $N,M$ tokens, respectively. $\textbf{x} \in \mathcal{R}^{N\times d}$ and $\textbf{y} \in \mathcal{R}^{M\times D}$ where $(d, D)$ is representation dimension of each tokens in $\textbf{x}, \textbf{y}$. Within OT framework, each token is viewed as a point in metric space $\Omega$. Now, we consider $N$ tokens in $\textbf{x}$ is an uniform distribution with equal mass $1/N$. The probability measure is:
\begin{equation}
    f := \sum_{i=1}^N \frac{1}{N}*\delta_{\textbf{x}_i}
\end{equation}
Similarly, the $M$ tokens in $\textbf{y}$ is an uniform distribution with equal mass $1/M$. The probability measure is:
\begin{equation}
    g := \sum_{j=1}^M \frac{1}{M}*\delta_{\textbf{y}_j}
\end{equation}

Given two sets of token embeddings, $\mathbf{x} \in \mathbb{R}^{N \times d}$ and $\mathbf{y} \in \mathbb{R}^{M \times d}$, the pairwise attention-based distance is defined as:

\begin{equation}
    S_{i,j} = \frac{\mathbf{x}_i^\top \mathbf{y}_j}{\sqrt{d}}
\end{equation}

\begin{equation}
    A_{i,j} = \frac{\exp(S_{i,j})}{\sum_{k=1}^{M} \exp(S_{i,k})}
\end{equation}

\begin{equation}
    d(\mathbf{x}_i, \mathbf{y}_j) = 1 - A_{i,j}
\end{equation}

Then, the OT loss between sequences $\textbf{x}_{1:N}, \textbf{y}_{1:M}$ is:
\begin{equation}
    OT\_Loss(\textbf{x}, \textbf{y}) = \sum_{i=1}^N \sum_{j=1}^M d(\textbf{x}_i, \textbf{y}_j) * \mathbf{T}_{ij}^*,
    \label{eq:ot_layer_loss}
\end{equation}
where $\mathbf{T}^*$ is the optimal transport plan solved by Sinkhorn algorithm in Equation \ref{eq:solution}.

Minimizing $OT\_Layer\_Loss$ aligns the vector representations $\textbf{x}_{1:N}, \textbf{y}_{1:M}$ of two sequences. Moreover, our loss is flexible to apply into two sequences which are not the same length, hence it is easily to adapt in the cases that student and teacher model do not share vocabulary.

\subsection{Student and Teacher layer alignments}

Our motivation is that for an input, the representation in student models in each layer should follow the teacher. We select to align the first layer (embedding layers) and last hidden layer of student and teacher. We do not align the other layers in order to let student freely learn its inner representation, however the first and last layer of student show follow the teacher. Our idea is depicted in Figure \ref{fig:ot_layer}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{imgs/OT_layer.png}
    \caption{Optimal transport to align embedding layer and last layer.}
    \label{fig:ot_layer}
\end{figure}

Specifically, let assume the instance $(x,y)$, with $x$ is the sequence input and $y$ is the sequence target (or response). $N, M$ is the number of tokens after student and teacher tokenizers, respectively. In the cases of same vocabulary we have $N=M$, otherwise $N \neq M$. We denote $d, D$ is the hidden size of student and teacher model, respectively. $\mathbf{e}_{1:N}^s \in \mathcal{R}^{N\times d}, \mathbf{e}_{1:M}^t \in \mathcal{R}^{M \times D}$ is the embedding tokens of input sequence in student and teacher. Similarly, $\mathbf{h}_{1:N}^s, \mathbf{h}_{1:M}^t$ is the last hidden states of input sequence in student and teacher. 

The OT loss between embedding tokens of student and teacher is defined as:
\begin{equation}
    OT\_Loss(\mathbf{e}^s,\mathbf{e}^t)
    \label{ot:emb}
\end{equation}
where $OT\_Loss$ is defined in the Equation \ref{eq:ot_layer_loss}.

Similarly, the loss between last hidden states of student and teacher is:
\begin{equation}
    OT\_Loss(\mathbf{h}^s,\mathbf{h}^t)
    \label{ot:hidden}
\end{equation}

The OT loss between student and teacher is:
\begin{equation}
\begin{aligned}
\mathcal{L}_{OT} & = OT\_Loss(\mathbf{e}^s,\mathbf{e}^t) \\
                &+ OT\_Loss(\mathbf{h}^s,\mathbf{h}^t)    
\end{aligned}
\label{loss:ot}
\end{equation}

Minimizing Eq. \ref{loss:ot} encourages the embedding and last hidden state of student align with teacher, then it enrichs the representation of intermediate layers of student models.

\subsection{Chain-of-thought represenatation alignments}
Chain-of-Thought (CoT) \cite{wei2022chain} reasoning is a paradigm in large language models (LLMs) designed to enhance their performance on complex reasoning tasks by explicitly breaking down problems into intermediate steps. The CoT output usually contains rich information. \cite{ranaldi2024aligning} show that CoT is effective in knowledge transfer from teacher to student. With the motivation is that: the overall semantic of CoT response and original response is the same, we can align their representations in teacher and student model. Although the Cot response is usually more lengthy than original response, Optimal Transport can perfectly employed in this alignment. 

First, in data augmentation phrase, we use pre-trained teacher model to generate CoT for each train data, by appending canonical component \textbf{"Letâ€™s think step by step"} to the original prompt then record its response. We use these \textbf{(new prompt, cot response)} as an augmented dataset. Second, in training phrase, we compute representations of \textbf{cot response} and original response from student and teacher in embedding layer and last hidden layer, then algin these outputs pairwise. Fig \ref{fig:cot_align} illustrates our OTalign approach.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{imgs/cot_align.png}
    \caption{Optimal transport to align CoT data reprensentations of student and teacher.}
    \label{fig:cot_align}
\end{figure}

Specifically, for an original training sample $(x_{raw},y_{raw})$, we use teacher model to generate augmented CoT training sample $(x_{CoT},y_{CoT})$, where $x_{CoT}$ is constructed by concatenating $x_{raw}$ with \textbf{"Let's think step by step"} and $y_{CoT}$ is the response of teacher model with input $x_{CoT}$. 
Similar to previous section, we align embedding layers and last hidden layers of student and teacher with input $x_{raw}$ and $x_{CoT}$. For student model, we denote $(\mathbf{e}_{x_{raw}}^{s}, \mathbf{e}_{x_{CoT}}^{s})$ as embedding layer, $(\mathbf{h}_{x_{raw}}^{s}, \mathbf{h}_{x_{CoT}}^{s})$ as last hidden states. For teacher model, we denote $(\mathbf{e}_{x_{raw}}^{t}, \mathbf{e}_{x_{CoT}}^{t}, \mathbf{h}_{x_{raw}}^{t}, \mathbf{h}_{x_{CoT}}^{t})$ as embedding layers and last hidden states similarly.
\begin{equation}
\begin{aligned}
\mathcal{L}_{OT}^{CoT} &= OT\_Layer\_Loss(\mathbf{e}_{x_{raw}}^{s},\mathbf{e}_{x_{CoT}}^{t})  \\
& +OT\_Layer\_Loss(\mathbf{e}_{x_{CoT}}^{s},\mathbf{e}_{x_{raw}}^{t}) \\
& +OT\_Layer\_Loss(\mathbf{h}_{x_{raw}}^{s},\mathbf{h}_{x_{CoT}}^{t})  \\
& +OT\_Layer\_Loss(\mathbf{h}_{x_{CoT}}^{s},\mathbf{h}_{x_{raw}}^{t})
\end{aligned}
\label{loss:ot_cot}
\end{equation}

The $\mathcal{L}_{OT}^{CoT}$ indicate how CoT data and raw data are aligned between student and teacher, holding the idea that the representations of CoT input and raw input are the same when it is computed. 

\subsection{Overall OT loss function}
To this end, the overall loss function for distillation process based on Optimal transport framework is:
\begin{equation}
    \mathcal{L}_{OT}^{KD} = \mathcal{L}_{OT} + \mathcal{L}_{OT}^{CoT}.
\end{equation}
The overall OTalign loss consists of the representation of embedding vector and last hidden states and representations of CoT input. Recall that our method still keep the auto-regressive characteristic of LLMs and the idea is natural to guide student through KD process.


