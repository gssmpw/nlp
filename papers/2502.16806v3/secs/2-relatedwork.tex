\label{Related_work}
Knowledge Distillation (KD) for Large Language Models (LLMs) is categorized into black-box and white-box KD. Black-box KD relies solely on the teacher model's outputs \cite{fu2023specializing, kim2016sequence}, while white-box KD leverages the teacher's architecture, enabling more comprehensive knowledge transfer \cite{wen2023f, gu2024minillm, ko2024distillm}. White-box KD techniques enable alignment at different architectural levels, such as the teacher model's output distribution \cite{song2020lightpaff, liang2020mixkd}, hidden representations \cite{jiao2019tinybert, sun2019patient}, or attention scores \cite{wang2020minilm}. %For instance, TinyBERT \cite{jiao2019tinybert} aligns embeddings, hidden states, and self-attention layers, while MiniLLM \cite{zhang2023not} uses PTLoss to avoid imitating unreliable teacher outputs. Similarly, \cite{gu2024minillm} applies reverse KL divergence to generative models, reducing the risk of overestimating low-probability prediction

Although white-box KD has gained significant attention, most studies assume a shared tokenizer for simplicity, with limited exploration of KD across models with different vocabularies. This scenario poses challenges, as differing tokenization schemes lead to mismatched vocabulary sizes, complicating direct KL divergence loss computation. Solutions include MinED \cite{wan2024knowledge}, which uses dynamic programming to align logits by minimizing tokenized sequence edit distance; ULD \cite{boizard2024towards}, which employs a Wasserstein distance-based loss; and DSKD \cite{zhang2024dual}, which leverages Cross Model Attention (CMA) to align token representations in a shared space. DSKD is the current state-of-the-art for KD with mismatched vocabularies.

% Despite the growing interest in white-box KD, most research assumes a shared tokenizer between teacher and student models for simplicity. To the best of our knowledge, few studies have explored KD between models with different vocabularies. A major challenge in this setting is aligning the softmax outputs of the teacher and student models. Different tokenization schemes result in mismatched vocabulary sizes, making direct KL divergence loss computation infeasible. Several methods have been proposed to address this issue. MinED \cite{wan2024knowledge} employs dynamic programming to align logits by minimizing the edit distance between tokenized sequences. Universal Logit Distillation (ULD) \cite{boizard2024towards} replaces the standard KL divergence loss with a Wasserstein distance-based closed-form solution. Dual-space knowledge distillation (DSKD) \cite{zhang2024dual} introduces Cross Model Attention (CMA) to project token representations from different vocabularies into a shared space. Among these, DSKD has emerged as the state-of-the-art approach for LLM distillation across different vocabularies.

Additionally, Chain-of-Thought (CoT) prompting is a powerful technique in KD, as shown in studies like Fine-tune-CoT \cite{ho2022large}, which uses reasoning samples from large teachers to fine-tune smaller models, often surpassing the teacher's reasoning ability. Distilling step-by-step \cite{hsieh2023distilling} adds extracted rationales as supervision in a multi-task framework, enabling significant compression with less data. Instruction-tuning-CoT \cite{ranaldi2024aligning} guides students to generate structured reasoning, improving question-answering and mathematical tasks. Building on these approaches, we incorporate CoT to enhance knowledge transfer for student models.
% Additionally, Chain-of-Thought (CoT) prompting has proven to be a valuable technique in KD, as demonstrated in several studies \cite{ho2022large, hsieh2023distilling, chen2023mcc}. Fine-tune-CoT \cite{ho2022large} leverages reasoning samples from large teacher models to fine-tune smaller models, significantly enhancing their reasoning abilitiesâ€”often surpassing the teacher model itself. Distilling step-by-step \cite{hsieh2023distilling} integrates extracted rationales as additional supervision in a multi-task learning framework, enabling substantial model compression while reducing training data requirements. Instruction-tuning-CoT \cite{ranaldi2024aligning} further refines CoT approach by guiding student models to generate structured, multi-step reasoning, improving their performance in question-answering and mathematical reasoning tasks. Inspired by these works, we incorporate the CoT mechanism into our method to enhance the knowledge transfer process for student models.