In this work, we propose a novel universal knowledge distillation framework that effectively addresses the challenges posed by vocabulary mismatches and reasoning-aware distillation. Our \method framework integrates Chain-of-Thought (CoT) augmentation and introduces Cross-CoT Alignment to enhance the transfer of reasoning capabilities from teacher to student models. Furthermore, we extend Optimal Transport beyond token-wise alignment by developing a sequence-level and layer-wise alignment strategy that accommodates varying sequence lengths while preserving contextual integrity.

Extensive experiments demonstrate that \method consistently outperforms existing KD techniques across diverse vocabulary settings, offering improved reasoning capabilities and robustness in domain-specific applications. These results highlight the effectiveness of our approach in bridging gaps in current KD methodologies, making it a more adaptable and reliable solution for real-world model compression. In future work, we aim to extend our approach by incorporating more fine-grained layer-level alignment in the distillation process. This would involve systematically aligning intermediate representations between teacher and student models across multiple layers, allowing for a more comprehensive transfer of knowledge.