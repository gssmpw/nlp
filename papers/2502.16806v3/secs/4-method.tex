

% \subsection{Optimal Transport for Sequence Alignment in Knowledge Distillation}

% In conventional knowledge distillation \cite{hinton2015distilling}, the student model aligns its output logits with the teacher's predictions. However, this approach neglects the rich intermediate representations learned by the teacher, limiting the effectiveness of the knowledge transfer. To address this, we introduce an Optimal Transport (OT)-based loss \(\mathcal{L}_{OT}\) that explicitly aligns the internal representations of the teacher and student, even when they differ in sequence length and vocabulary.

% Given tokenized sequences \(\mathbf{x} \in \mathbb{R}^{N \times d}\) and \(\mathbf{y} \in \mathbb{R}^{M \times D}\), we define their empirical distributions:
% \begin{equation}
%     \mu = \sum_{i=1}^{N} \alpha_i \delta_{\mathbf{x}_i}, \quad \nu = \sum_{j=1}^{M} \beta_j \delta_{\mathbf{y}_j},
% \end{equation}
% where \(\alpha \in \Theta_N\) and \(\beta \in \Theta_M\) are probability vectors representing mass distributions over tokens.


% \paragraph{Cross-Attention cost matrix computation} 
% To align the sequences, we construct a cost matrix \(\mathbf{C} \in \mathbb{R}^{N \times M}\), where each element represents the dissimilarity between token representations from two sequences. The cost matrix is defined based on token-level similarities, normalized using a softmax-like operation. Formally, the cost is computed as:
% \begin{equation}
%     \mathbf{C} = 1 - \mathrm{softmax}(\mathbf{S}),
% \end{equation}
% where \(\mathrm{softmax}(\mathbf{S})\) is applied row-wise to the similarity matrix \(\mathbf{S} \in \mathbb{R}^{N \times M}\), defined as:
% \begin{equation}
%     \mathbf{S} = \frac{\mathbf{X} \mathbf{Y}^\top}{\sqrt{d}},
% \end{equation}
% with \(\mathbf{X} \in \mathbb{R}^{N \times d}\) and \(\mathbf{Y} \in \mathbb{R}^{M \times d}\) representing the token embeddings from the two sequences. This formulation ensures that similar token pairs yield lower transport costs, leveraging the softmax operation for effective normalization.

% The optimal transport plan \(\mathbf{T}^*\) is obtained by solving the entropy-regularized OT problem:
% \begin{equation}
%     \mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T}),
% \end{equation}
% where \(H(\mathbf{T}) = - \sum_{i,j} T_{ij} \log T_{ij}\) is the entropy regularization term \cite{cuturi2013sinkhorn}. The OT distance is then computed as:
% \begin{equation}
%     \mathcal{D}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle.
% \end{equation}
% This measure enforces structural alignment between the student and teacher representations.

% To ensure comprehensive knowledge transfer, we apply OT distance to both the embedding and last hidden layers:
% \begin{equation}
%     \mathcal{D}_{OT}^{layer}(\mathbf{s}, \mathbf{t}) = \mathcal{D}_{OT}(E_s(\mathbf{s}), E_t(\mathbf{t})) + \mathcal{D}_{OT}(H_s(\mathbf{s}), H_t(\mathbf{t})),
% \end{equation}
% where \(E_s(\mathbf{s})\) and \(E_t(\mathbf{t})\) are the outputs of the embedding layers of the student and teacher, respectively, and \(H_s(\mathbf{s})\), \(H_t(\mathbf{t})\) are their respective last hidden states.

% \subsection{Cross Chain-of-Thought Representation Alignment}

% To further enhance the transfer of reasoning abilities, we integrate Chain-of-Thought (CoT) alignment \cite{wei2022chain}. Following \cite{ranaldi2024aligning}, we prompt the teacher with "Let's think step by step" to generate enhanced reasoning traces. Given raw sequences \(\mathbf{s}_{raw}, \mathbf{t}_{raw}\) and their CoT-augmented counterparts \(\mathbf{s}_{CoT}, \mathbf{t}_{CoT}\), we define the CoT alignment distance as:
% \begin{equation}
%     \mathcal{D}_{OT}^{CoT}(\mathbf{s}, \mathbf{t}) = \mathcal{D}_{OT}(\mathbf{s}_{raw}, \mathbf{t}_{CoT}) + \mathcal{D}_{OT}(\mathbf{s}_{CoT}, \mathbf{t}_{raw}).
% \end{equation}
% This enforces cross-alignment between standard and reasoning-augmented representations, enhancing semantic transfer.

% \subsection{Overall Knowledge Distillation Objective}

% Our final knowledge distillation objective combines OT-based alignment with cross-entropy loss:
% \begin{equation}
%     \mathcal{D}_{OT}^{KD} = \mathcal{D}_{OT}^{layer} + \mathcal{D}_{OT}^{CoT},
% \end{equation}
% \begin{equation}
%     \mathcal{L} = (1 - \alpha) \mathcal{L}_{CE} + \alpha \mathcal{D}_{OT}^{KD},
% \end{equation}
% where \(\alpha\) balances between traditional distillation and OT-based alignment. This objective enables the student to mimic both structural representations and reasoning patterns from the teacher, improving overall knowledge transfer effectiveness.













% ---------------------------------

% \subsection{Optimal Transport for Sequence Alignment in Knowledge Distillation}

% In conventional knowledge distillation \cite{hinton2015distilling}, the student model aligns its output logits with the teacher's predictions. However, this approach neglects the rich intermediate representations learned by the teacher, limiting the effectiveness of the knowledge transfer. To address this, we introduce an Optimal Transport (OT)-based loss \(\mathcal{L}_{OT}\) that explicitly aligns the internal representations of the teacher and student, even when they differ in sequence length and vocabulary.

% \paragraph{Empirical Distributions Definition} 
% Given tokenized sequences \(\mathbf{x} \in \mathbb{R}^{N \times d}\) and \(\mathbf{y} \in \mathbb{R}^{M \times D}\), we define their empirical distributions:
% \begin{equation}
%     \mu = \sum_{i=1}^{N} \alpha_i \delta_{\mathbf{x}_i}, \quad \nu = \sum_{j=1}^{M} \beta_j \delta_{\mathbf{y}_j},
% \end{equation}
% where \(\alpha \in \Theta_N\) and \(\beta \in \Theta_M\) are probability vectors representing mass distributions over tokens. $N$ and $M$ represent the lengths of the tokenized sequences $\mathbf{x}$ and $\mathbf{y}$, respectively.  
% The sequence $\mathbf{x}$ is tokenized by the student's tokenizer, and the sequence $\mathbf{y}$ is tokenized by the teacher's tokenizer.


% \paragraph{Cross-Attention Cost Matrix Computation} 
% To align the sequences effectively, we construct a cost matrix $\mathbf{C} \in \mathbb{R}^{N \times M}$, where each element quantifies the dissimilarity between token representations from the student and teacher sequences. The construction involves two key steps: calculating a similarity matrix and transforming it into a dissimilarity measure. 

% First, we compute the similarity matrix $\mathbf{S} \in \mathbb{R}^{N \times M}$, which captures the pairwise similarities between tokens from the two sequences. This is defined as:
% \begin{equation}
%     \mathbf{S} = \frac{\mathbf{X} \mathbf{P}(\mathbf{Y})^\top}{\sqrt{d}},
% \end{equation}
% where $\mathbf{X} \in \mathbb{R}^{N \times d}$ and $\mathbf{Y} \in \mathbb{R}^{M \times D}$ are the token embeddings of the student and teacher sequences, respectively, and $\mathbf{P} \in \mathbb{R}^{D \times d}$ is a projection matrix that maps the teacher’s token embeddings into the student's embedding space. The scaling factor $\sqrt{d}$ stabilizes the similarity scores by preventing excessively large values during gradient-based optimization.

% Next, the similarity matrix is normalized using a softmax operation applied row-wise, yielding:
% \begin{equation}
%     \mathbf{S}_{\text{norm}} = \mathrm{softmax}(\mathbf{S}),
% \end{equation}
% where each row of $\mathbf{S}_{\text{norm}}$ sums to 1. Finally, the cost matrix is computed by subtracting the normalized similarity scores from 1, yielding the dissimilarity measure:
% \begin{equation}
%     \mathbf{C} = \mathbf{1} - \mathbf{S}_{\text{norm}}.
% \end{equation}

% \paragraph{Optimal Transport Plan Computation} 
% The optimal transport plan \(\mathbf{T}^*\) is obtained by solving the entropy-regularized OT problem:
% \begin{equation}
%     \mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T}),
% \end{equation}
% where \(H(\mathbf{T}) = - \sum_{i,j} T_{ij} \log T_{ij}\) is the entropy regularization term \cite{cuturi2013sinkhorn}. The OT distance is then computed as:
% \begin{equation}
%     \mathcal{L}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle.
% \end{equation}
% This measure enforces structural alignment between the student and teacher representations.

% \paragraph{Layer-wise Optimal Transport Alignment}

% To ensure comprehensive knowledge transfer, we apply OT distance to both the embedding and last hidden layers:

% \begin{equation}
%     \mathcal{L}_{OT}(s, t)
%     \;=\;
%     \mathcal{L}_{OT}\bigl(e^s_{1:N}, \; e^t_{1:M}\bigr)
%     \;+\;
%     \mathcal{L}_{OT}\bigl(h^s_{1:N}, \; h^t_{1:M}\bigr),
% \end{equation}

% Where $e^s$ and $e^t$ are the student and teacher embeddings, respectively, 
% and $h^s$ and $h^t$ denote the student’s and teacher’s last hidden states, respectively. 

% \subsection{Cross Chain-of-Thought Representation Alignment}

% \paragraph{Data Augmentation with Chain-of-Thought (CoT)}
% We first train (or fine-tune) a \emph{Teacher Model} on an initial dataset, resulting in a specialized, fine-tuned teacher. Next, we supply the teacher with a zero-shot CoT prompt to generate its rationale outputs, which are then collected to form a new CoT dataset (as demonstrated in \cite{your_reference_here}). Subsequently, we combine the original dataset with the additional CoT-augmented samples (see also \cite{another_reference_here}). 

% Concurrently, we perform \emph{Knowledge Distillation} (often referred to as on-the-fly teaching) by transferring the teacher's knowledge to a \emph{Student Model}. The student is trained on both the CoT dataset (which includes the generated rationales) and the distilled outputs from the teacher. This procedure enables the student to inherit high-quality reasoning steps from the teacher, while also enhancing efficiency and generalization.

% \paragraph{Chain-of-Thought Alignment Formulation}
% To further improve the transfer of reasoning capabilities, we incorporate Chain-of-Thought (CoT) alignment \cite{wei2022chain}. Following \cite{ranaldi2024aligning}, we prompt the teacher with ``Let's think step by step'' to obtain enhanced reasoning traces. Consider the following four pairs of instances:
% \begin{itemize}
%     \item $(x, y)$ and $(x_{\text{CoT}}, y_{\text{CoT}})$: here, $x$ and $x_{\text{CoT}}$ are inputs without and with CoT prompts respectively, and $y$ and $y_{\text{CoT}}$ are the corresponding outputs generated by the student model.
%     \item $(x, Y)$ and $(x_{\text{CoT}}, Y_{\text{CoT}})$: here, $Y$ and $Y_{\text{CoT}}$ denote the outputs generated by the teacher model on the inputs without and with CoT prompts, respectively.
% \end{itemize}
% % define the normal OT alignment here 
% We then define the CoT alignment distance as follows:
% \begin{equation}
%     \mathcal{L}_{\text{OT}}^{\text{CoT}} = \mathcal{L}_{\text{OT}}(\mathbf{y}, \mathbf{Y}_{\text{CoT}}) + \mathcal{L}_{\text{OT}}(\mathbf{y}_{\text{CoT}}, \mathbf{Y}).
% \end{equation}
% This formulation enforces cross-alignment between the standard and reasoning-augmented representations, thereby enhancing the semantic transfer between teacher and student.



% \subsection{Overall Knowledge Distillation Objective}

% Our final knowledge distillation objective combines OT-based alignment with cross-entropy loss:
% % L_OT is the normal align with (y,Y) and (y_cot, Y_COT) which is not define anywhere 
% \begin{equation}
%     \mathcal{L}_{OT}^{KD} = \mathcal{L}_{OT} + \mathcal{L}_{OT}^{CoT},
% \end{equation}
% \begin{equation}
%     \mathcal{L} = (1 - \alpha) \mathcal{L}_{CE} + \alpha (\mathcal{L}_{OT}^{KD} + \mathcal{L}_{KD}),
% \end{equation}
% where \(\alpha\) balances between traditional distillation and OT-based alignment, \(\mathcal{L}_{CE}\) is the cross-entropy loss used in standard knowledge distillation, and \(\mathcal{L}_{KD}\) represents the initial knowledge distillation loss from baseline methods. This objective enables the student to mimic both structural representations and reasoning patterns from the teacher, improving overall knowledge transfer effectiveness.

\subsection{Cross Chain-of-Thought Alignment}\label{sec:ccot}

Recent advances in Chain-of-Thought (CoT) reasoning have shown that multi-step reasoning can greatly enhance model performance \citep{wei2022chain, huang2023towards, feng2024towards}. However, current KD approaches \citep{zhang2024dual, wan2024knowledge, boizard2024towards} designed for models with different vocabularies mainly emphasize final output alignment and have not explored the ability to adequately capture and transfer intricate reasoning patterns from the teacher model. To bridge this gap, we not only integrate CoT augmentation into knowledge distillation for models with different vocabularies but also introduce a novel Cross-CoT Alignment method ($CCoT$). This approach aligns CoT-augmented samples with labeled responses within the \method framework (described in Section \ref{sec:otalign}), ensuring that both explicit outputs and the underlying reasoning processes are effectively transferred.

% \documentclass{article}
% \usepackage{tikz}
% \usepackage{amsmath}



\paragraph{Data Augmentation with Chain-of-Thought:}  
The process begins by training or fine-tuning a teacher model on an initial dataset, resulting in a model with strong performance in a specific domain. To leverage its reasoning capabilities further, the teacher model is prompted with a zero-shot CoT instruction, such as \textit{“Let’s think step by step.”} This prompt encourages the model to generate a detailed, step-by-step rationale for its answers. 
Detail prompts can be found in Appendix \ref{sec:prompt}. The output typically includes two parts:  
\begin{itemize}
    \item The comprehensive reasoning process produced by the teacher model, referred to as \texttt{Teacher\_rationale}.
    \item A final answer in the format: [\texttt{Teacher\_rationale}]. \texttt{Therefore, the answer is: [Ground Truth]}.
\end{itemize}

These rationale-enriched outputs are then combined with the original dataset, creating a new CoT-augmented corpus that incorporates both direct responses and multi-step reasoning.




\paragraph{Cross-CoT Alignment:}
In order to achieve effective reasoning transfer, the student is trained on both CoT-augmented data and the teacher’s distilled outputs. This approach not only focuses on replicating the direct outputs but also on capturing the multi-step reasoning process that underlies them. To facilitate this, we consider the following pairs of representations:

\begin{itemize}[leftmargin=*]
    \item \((x, y)\) and \((x_{\text{CoT}}, y_{\text{CoT}})\) \\
    Here, \(x\) and \(x_{\text{CoT}}\) denote inputs without and with CoT prompts, respectively, while \(y\) and \(y_{\text{CoT}}\) are the corresponding student outputs. This pair focuses on transferring the reasoning embedded in the student's own responses.
    
    \item  \((x, Y)\) and \((x_{\text{CoT}}, Y_{\text{CoT}})\) \\
    In this pair, \(Y\) and \(Y_{\text{CoT}}\) represent the teacher’s outputs under standard and CoT conditions, respectively, emphasizing the direct transfer of the teacher’s reasoning.
\end{itemize}

Based on these pairs, we define two alignment losses aimed at reinforcing reasoning transfer:

\begin{enumerate}
    \item \textit{Cross Student-Teacher Output Alignment Loss}
    \begin{equation}\label{eq:cot_loss2}
        \mathcal{L}_{CST} = \mathcal{L}_{align}(\mathbf{y}_{\text{CoT}}, \mathbf{Y}_{\text{CoT}}) + \mathcal{L}_{align}(\mathbf{y}, \mathbf{Y}),
    \end{equation}
    
    \item \textit{Cross Standard-CoT Output Alignment Loss}
    \begin{equation}\label{eq:cot_loss1}
    \mathcal{L}_{CRC} = \mathcal{L}_{align}(\mathbf{y}, \mathbf{Y}_{\text{CoT}}) + \mathcal{L}_{align}(\mathbf{y}_{\text{CoT}}, \mathbf{Y})
\end{equation}
\end{enumerate}

Here, $\mathcal{L}_{align}$ refers to the alignment function, which will be detailed in Section \ref{sec:otalign}. The loss function (\ref{eq:cot_loss1}) aligns student and teacher outputs with identical input formats (standard and CoT), enabling the student to mimic the teacher’s behavior. Meanwhile, the loss function (\ref{eq:cot_loss2}) aligns CoT and standard outputs to enhance the reliability of the CoT process, reinforcing the transfer of accurate information and reasoning capabilities.



\subsection{Optimal Transport for Reasoning Distillation}\label{sec:otalign}

As discussed in Section \ref{sec:intro}, prior studies on knowledge distillation across different vocabularies have employed a stepwise approach, wherein the student model learns to mimic the teacher’s behavior by aligning its output logits with those of the teacher. While this method enables token-level distribution alignment, it imposes constraints such as requiring the same dimensional space for distributions or enforcing identical response lengths between the teacher and student, thereby limiting the full potential of reasoning knowledge transfer. To overcome these limitations, we introduce an Optimal Transport (OT)-based loss function, denoted as $\mathcal{L_{OT}}$, which facilitates sequence-level distribution alignment between the teacher and student models, effectively eliminating dependencies on sequence length and vocabulary differences.

% In conventional knowledge distillation \cite{hinton2015distilling}, the student model is trained to mimic the teacher by aligning its output logits with the teacher's predictions. However, this approach overlooks the rich intermediate representations captured by the teacher, limiting the effectiveness of the knowledge transfer. To address this, we propose an Optimal Transport (OT)-based loss, denoted as \(\mathcal{L}_{OT}\), which explicitly aligns the internal representations of the teacher and student models—even when their sequence lengths and vocabularies differ.

\paragraph{Empirical Distributions Definition} 
% Given tokenized sequences \(\mathbf{x} \in \mathbb{R}^{N \times d}\) and \(\mathbf{y} \in \mathbb{R}^{M \times D}\), we define their corresponding empirical distributions as:
% \begin{equation}
%     \mu = \sum_{i=1}^{N} \alpha_i \delta_{\mathbf{x}_i}, \quad \nu = \sum_{j=1}^{M} \beta_j \delta_{\mathbf{y}_j},
% \end{equation}
% where \(\alpha \in \Theta_N\) and \(\beta \in \Theta_M\) are probability vectors representing the mass distributions over tokens. Here, \(N\) and \(M\) denote the lengths of the student’s and teacher’s tokenized sequences, respectively.

Given that the distribution over tokens is uniform, we can simplify the empirical distributions as follows. For tokenized sequences \(\mathbf{x} \in \mathbb{R}^{N \times d}\) and \(\mathbf{y} \in \mathbb{R}^{M \times D}\), the empirical distributions are defined as:
\[
\mu = \frac{1}{N} \sum_{i=1}^{N} \delta_{\mathbf{x}_i}, \quad \nu = \frac{1}{M} \sum_{j=1}^{M} \delta_{\mathbf{y}_j},
\]
where each token in the student's sequence receives an equal mass of \(1/N\) and each token in the teacher's sequence receives an equal mass of \(1/M\). Here, \(N\) and \(M\) denote the lengths of the student’s and teacher’s tokenized sequences, respectively.

\paragraph{Cross-Attention Cost Matrix Computation} 
To align the sequences effectively, we construct a cost matrix \(\mathbf{C} \in \mathbb{R}^{N \times M}\) that quantifies the dissimilarity between token representations from the student and teacher. This is achieved in two main steps:
\begin{enumerate}
    \item \textbf{Similarity Matrix Computation:}  
    We compute the similarity matrix:
    \begin{equation}
        \mathbf{S} = \frac{\mathbf{X} \, \mathbf{P}(\mathbf{Y})^\top}{\sqrt{d}},
    \end{equation}
    where \(\mathbf{X} \in \mathbb{R}^{N \times d}\) and \(\mathbf{Y} \in \mathbb{R}^{M \times D}\) are the token embeddings for the student and teacher, respectively. The mapping matrix \(\mathbf{P} \in \mathbb{R}^{D \times d}\) projects the teacher’s embeddings into the student’s space, and the scaling factor \(\sqrt{d}\) ensures numerical stability.
    
    \item \textbf{Normalization and Cost Computation:}  
    The similarity matrix is normalized row-wise using the softmax function:
    \begin{equation}
        \mathbf{S}_{\text{norm}} = \mathrm{softmax}(\mathbf{S}),
    \end{equation}
    ensuring that each row sums to 1. The cost matrix is then derived as:
    \begin{equation}
        \mathbf{C} = \mathbf{1} - \mathbf{S}_{\text{norm}},
    \end{equation}
    where \(\mathbf{1}\) denotes a matrix of ones.
\end{enumerate}

\paragraph{Optimal Transport Plan Computation} 
We compute the optimal transport plan \(\mathbf{T}^*\) by solving the entropy-regularized OT problem \cite{distances2013lightspeed}:
\begin{equation}
    \mathbf{T}^* = \arg\min_{\mathbf{T} \in U(\alpha, \beta)} \langle \mathbf{T}, \mathbf{C} \rangle - \frac{1}{\lambda} H(\mathbf{T}),
\end{equation}
where \(H(\mathbf{T}) = - \sum_{i,j} T_{ij} \log T_{ij}\) is the entropy regularization term. The OT distance is then defined as:
\begin{equation}
    \mathcal{L}_{OT}(\mathbf{x}, \mathbf{y}) = \langle \mathbf{T}^*, \mathbf{C} \rangle.
\end{equation}
This loss enforces structural alignment between the teacher and student representations.

\paragraph{Layer-wise Optimal Transport Alignment} 
To ensure comprehensive knowledge transfer, we apply the OT distance to both the embedding and last hidden layers:
{\small\begin{equation}
    \mathcal{L}_{\mathcal{OT}}(s, t) = \mathcal{L}_{OT}\bigl(e^s_{1:N}, \, e^t_{1:M}\bigr) + \mathcal{L}_{OT}\bigl(h^s_{1:N}, \, h^t_{1:M}\bigr),
\end{equation}}
where \(e^s\) and \(e^t\) denote the student and teacher embeddings, and \(h^s\) and \(h^t\) represent their respective last hidden states.
The loss function $\mathcal{L}_{\mathcal{OT}}$ will serve as $\mathcal{L}_{align}$ in Equations (\ref{eq:cot_loss1}) and (\ref{eq:cot_loss2}).

\subsection{Overall Knowledge Distillation Objective}

\method extends DSKD \cite{zhang2024dual} by incorporating the techniques introduced in Sections \ref{sec:ccot} and \ref{sec:otalign}. Thus, our final knowledge distillation objective is formulated as follows:
\begin{equation}
    \mathcal{L}_{CCoT} = \mathcal{L}_{CRC} + \mathcal{L}_{CST},
\end{equation}
\begin{equation}\label{eq:final_obj}
    \mathcal{L} = (1 - \alpha) \, \mathcal{L}_{CE} + \alpha \left(\mathcal{L}_{CCoT} + \mathcal{L}_{KD}\right),
\end{equation}
where \(\alpha \in [0,1]\) balances the contributions of the traditional cross-entropy loss \(\mathcal{L}_{CE}\), the original DSKD's distillation loss \(\mathcal{L}_{KD}\), and the proposed OT-based loss $\mathcal{L}_{CCoT}$. This comprehensive objective enables the student model not only to replicate the teacher's final output at the token-wise level but also to align at the sequence level, capturing the underlying reasoning process; thereby ensuring a more effective knowledge transfer process.


