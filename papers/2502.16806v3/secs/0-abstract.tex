Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring knowledge from large teacher models to smaller student models. However, existing KD methods often assume shared vocabularies and tokenizers, limiting their flexibility. While approaches like Universal Logit Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address vocabulary mismatches, they overlook the critical \textbf{reasoning-aware distillation} aspect. To bridge this gap, we propose \method a universal KD framework that integrates Chain-of-Thought (CoT) augmentation and introduces Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend Optimal Transport beyond token-wise alignment to a sequence-level and layer-wise alignment approach that adapts to varying sequence lengths while preserving contextual integrity. Comprehensive experiments demonstrate that \method outperforms existing KD methods across different vocabulary settings, improving reasoning capabilities and robustness in domain-specific tasks.

% Large Language Models (LLMs) have demonstrated remarkable success across various NLP tasks. However, their real-world deployment remains challenging due to their substantial computational costs. Knowledge Distillation (KD) offers a promising solution by compressing LLMs into smaller, more efficient models. Yet, existing KD methods typically require the teacher and student models to share the same vocabulary and tokenizer, restricting architectural flexibility. Moreover, distilling LLMs with different vocabularies is rarely explored due to its inherent complexity. To address this challenge, we propose a novel approach that utilizes the Optimal Transport framework to align representations across different model layers. Additionally, we integrate Chain-of-Thought (COT) reasoning to enhance knowledge transfer. Extensive experiments across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in LLM knowledge distillation.