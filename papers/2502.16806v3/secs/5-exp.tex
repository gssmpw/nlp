% \begin{table*}[h!]
% \footnotesize
% \centering
% \begin{adjustbox}{width=\textwidth}
% \begin{tabular}{l|c|c|c|c|c}
% \toprule
% \textbf{Methods} & \textbf{Dolly} & \textbf{Alpaca} & \textbf{S-NI} & \textbf{Dialogue Sum} & \textbf{Avg} \\ 
% \bottomrule
% \toprule
% \multicolumn{6}{l}{\textit{\textbf{Student: GPT2-120M}}} \\ 
% \hline
% SFT 
% % 23.78 +- 0.38		27.2 +- 0.23			20.45 +- 0.23		30.25 +- 0.22
% & $23.78_{\pm 0.38}$	
% & $27.20_{\pm 0.23}$	
% & $20.45_{\pm 0.23}$  
% & $30.25_{\pm 0.22}$
% & $25.42$ \\  
% \hline
% \multicolumn{6}{c}{\textit{Teacher: GPT2-1.5B (Same Vocabulary)}} \\ 
% \hline
% Teacher 
% & $26.68_{\pm 0.23}$	
% & $30.01_{\pm 0.36}$	
% & $24.57_{\pm 0.21}$  
% & $32.00_{\pm 0.27}$
% & $28.32$ \\  
% \cline{1-6}
% KL 
% & $24.20_{\pm 0.37}$	
% & $27.55_{\pm 0.59}$	 
% & $21.32_{\pm 0.09}$	 
% & $31.29_{\pm 0.30}$  
% & $26.09$ \\  
% \textbf{KL + \method} 
% & $\mathbf{24.80_{\pm 0.32}}$	
% & $\mathbf{28.14_{\pm 0.30}}$	
% & $\mathbf{21.85_{\pm 0.12}}$	 
% & $\mathbf{31.36_{\pm 0.09}}$  
% & $\mathbf{26.54}_{\color{darkgreen}\uparrow 0.45}$ \\  
% \hline  
% \multicolumn{6}{c}{\textit{Teacher: Qwen1.5-1.8B (Different Vocabularies)}} \\
% \hline
% Teacher 
% & $28.23_{\pm 0.13}$	
% & $33.76_{\pm 0.33}$	
% & $30.32_{\pm 0.26}$	
% & $35.37_{\pm 0.26}$ 
% & $31.92$ \\  
% \cline{1-6}
% ULD \cite{boizard2024towards}
% & $23.77_{\pm 0.41}$	
% & $27.50_{\pm 0.50}$	
% & $21.37_{\pm 0.34}$	
% & $30.23_{\pm 0.10}$ 
% & $25.72$ \\ 
% \textbf{ULD + \method} 
% & $\mathbf{23.83_{\pm 0.28}}$	
% & $\mathbf{28.12_{\pm 0.41}}$	
% & $\mathbf{21.76_{\pm 0.16}}$	 
% & $\mathbf{30.43_{\pm 0.12}}$ 
% & $\mathbf{26.04}_{\color{darkgreen}\uparrow 0.32}$ \\ 
% \hline
% MinED \cite{wan2024knowledge}
% & $24.21_{\pm 0.31}$	
% & $28.47_{\pm 0.42}$	
% & $21.76_{\pm 0.13}$	 
% & $31.36_{\pm 0.09}$ 
% & $26.45$ \\ 
% \textbf{MinED + \method} 
% & $\mathbf{24.63_{\pm 0.36}}$	
% & $\mathbf{28.86_{\pm 0.44}}$	
% & $\mathbf{22.18_{\pm 0.19}}$	 
% & $\mathbf{31.58_{\pm 0.24}}$ 
% & $\mathbf{26.81}_{\color{darkgreen}\uparrow 0.36}$ \\ 
% \hline
% DSKD \cite{zhang2024dual}
% & $24.42_{\pm 0.32}$	
% & $28.48_{\pm 0.32}$    
% & $22.26_{\pm 0.26}$ 
% & $31.46_{\pm 0.22}$ 
% & $26.66$ \\ 
% \textbf{DSKD +  \method} 
% & $\mathbf{25.34_{\pm 0.23}}$  
% & $\mathbf{28.78_{\pm 0.18}}$	
% & $\mathbf{24.02_{\pm 0.32}}$	
% & $\mathbf{31.76_{\pm 0.10}}$ 
% & $\mathbf{27.48}_{\color{darkgreen}\uparrow 0.82}$ \\  
% \bottomrule
% \toprule
% \multicolumn{6}{l}{\textit{\textbf{Student:  TinyLLaMA-1.1B}}} \\ 
% \hline
% SFT 
% & $23.20_{\pm 0.16}$ 
% & $29.48_{\pm 0.48}$ 
% & $24.65_{\pm 0.25}$ 
% & $31.08_{\pm 0.17}$ 
% & $27.10$ \\
% \hline
% \multicolumn{6}{c}{\textit{Teacher: Llama2-7B (Same Vocabulary)}} \\ 
% \hline Teacher
% & $32.15_{\pm 0.56}$ 
% & $36.44_{\pm 0.48}$ 
% & $30.16_{\pm 0.25}$ 
% & $36.18_{\pm 0.23}$ 
% & $33.73$ \\  
% \cline{1-6}
% KL
% & $25.75_{\pm 0.26}$ 
% & $31.83_{\pm 0.57}$ 
% & $26.73_{\pm 0.17}$ 
% & $32.83_{\pm 0.18}$ 
% & $29.29$ \\  
% \textbf{KL + \method} 
% & $\mathbf{26.23_{\pm 0.31}}$ 
% & $\mathbf{32.34_{\pm 0.39}}$ 
% & $\mathbf{27.97_{\pm 0.28}}$ 
% & $\mathbf{33.42_{\pm 0.11}}$ 
% & $\mathbf{29.99}_{\color{darkgreen}\uparrow 0.7}$  \\  
% \hline
% \multicolumn{6}{c}{\textit{Teacher: Mistral-7B (Different Vocabularies)}} \\ 
% \hline Teacher 
% & $28.49_{\pm 0.21}$ 
% & $35.75_{\pm 0.25}$ 
% & $32.35_{\pm 0.24}$ 
% & $35.24_{\pm 0.08}$ 
% & $32.96$ \\ 
% \cline{1-6}
% ULD \cite{boizard2024towards}
% & $25.48_{\pm 0.29}$ 
% & $31.33_{\pm 0.36}$ 
% & $26.55_{\pm 0.10}$ 
% & $33.69_{\pm 0.26}$ 
% & $29.26$ \\ 
% \textbf{ULD + \method} 
% & $\mathbf{25.63_{\pm 0.22}}$ 
% & $\mathbf{31.86_{\pm 0.53}}$ 
% & $\mathbf{27.72_{\pm 0.33}}$ 
% & $\mathbf{33.90_{\pm 0.14}}$ 
% & $\mathbf{29.78}_{\color{darkgreen}\uparrow 0.52}$ \\ 
% \hline
% MinED \cite{wan2024knowledge}
% & $25.54_{\pm 0.59}$ 
% & $31.82_{\pm 0.33}$ 
% & $26.13_{\pm 0.23}$ 
% & $33.31_{\pm 0.16}$ 
% & $29.20$ \\ 
% \textbf{MinED + \method} 
% & $\mathbf{26.32_{\pm 0.52}}$ 
% & $\mathbf{32.76_{\pm 0.27}}$ 
% & $\mathbf{28.13_{\pm 0.13}}$ 
% & $\mathbf{33.42_{\pm 0.20}}$ 
% & $\mathbf{30.16}_{\color{darkgreen}\uparrow 0.96}$ \\
% \hline
% DSKD \cite{zhang2024dual}
% & $26.28_{\pm 0.35}$ 
% & $32.31_{\pm 0.15}$ 
% & $26.74_{\pm 0.24}$ 
% & $33.44_{\pm 0.18}$ 
% & $29.69$ \\ 
% \textbf{DSKD +\method} 
% & $\mathbf{27.41_{\pm 0.43}}$  
% & $\mathbf{33.31_{\pm 0.49}}$ 
% & $\mathbf{29.77_{\pm 0.20}}$  
% & $\mathbf{35.01_{\pm 0.20}}$ 
% & $\mathbf{31.38}_{\color{darkgreen}\uparrow 1.69}$ \\ 
% \bottomrule   
% \toprule 
% \multicolumn{6}{l}{\textit{\textbf{Student: GPT2-1.5B}}} \\ 
% \hline
% SFT
% & $21.83_{\pm 0.28}$	
% & $27.15_{\pm 0.31}$	
% & $23.16_{\pm 0.15}$	
% & $30.74_{\pm 0.17}$ 
% & $25.72$ \\ 
% \hline 
% \multicolumn{6}{c}{\textit{Teacher: Qwen2.5-7B-Instruct (Different Vocabularies)}} \\
% \hline
% Teacher 
% & $28.49_{\pm 0.21}$	
% & $35.75_{\pm 0.25}$	
% & $32.35_{\pm 0.24}$	
% & $35.24_{\pm 0.08}$ 
% & $32.96$ \\  
% \cline{1-6}
% ULD \cite{boizard2024towards}
% & $24.52_{\pm 0.28}$	
% & $29.17_{\pm 0.22}$	
% & $24.18_{\pm 0.08}$	
% & $32.74_{\pm 0.35}$ 
% & $27.65$ \\ 
% \textbf{ULD + \method} 
% & $\mathbf{24.73_{\pm 0.33}}$	
% & $\mathbf{30.22_{\pm 0.18}}$	
% & $\mathbf{25.07_{\pm 0.23}}$	
% & $\mathbf{33.30_{\pm 0.42}}$ 
% & $\mathbf{28.33}_{\color{darkgreen}\uparrow 0.68}$ \\ 
% \hline
% MinED \cite{wan2024knowledge}
% & $25.52_{\pm 0.44}$	
% & $30.41_{\pm 0.56}$	
% & $25.09_{\pm 0.25}$	
% & $33.83_{\pm 0.24}$	         
% & $28.71$ \\ 
% \textbf{MinED + \method} 
% & $\mathbf{25.85_{\pm 0.28}}$  
% & $\mathbf{30.43_{\pm 0.28}}$	
% & $\mathbf{25.74_{\pm 0.22}}$	
% & $\mathbf{34.79_{\pm 0.11}}$ 
% & $\mathbf{29.20}_{\color{darkgreen}\uparrow 0.49}$ \\ 
% \hline
% DSKD \cite{zhang2024dual}
% & $25.38_{\pm 0.46}$	           
% & $30.48_{\pm 0.38}$  
% & $25.92_{\pm 0.18}$	 
% & $33.82_{\pm 0.23}$            
% & $28.90$ \\ 
% \textbf{DSKD + \method} 
% & $\mathbf{26.72_{\pm 0.22}}$          
% & $\mathbf{33.02_{\pm 0.40}}$	      
% & $\mathbf{27.72_{\pm 0.13}}$	     
% & $\mathbf{35.63_{\pm 0.22}}$              
% & $\mathbf{30.77}_{\color{darkgreen}\uparrow 1.87}$ \\ 
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{Comparison of methods across different datasets. We present the $mean_{\pm std}$ values derived from experiments conducted across 5 random seeds. SFT refers to Supervised Fine-Tuning, where the student model is directly trained on the downstream dataset.}
% \label{tab:main_result}
% \end{table*}

\begin{table*}[t]
\footnotesize
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{l|c|c|c|c|c}
\toprule
\textbf{Methods} & \textbf{Dolly} & \textbf{Alpaca} & \textbf{S-NI} & \textbf{Dialogue Sum} & \textbf{Avg} \\ 
\bottomrule
\toprule
\multicolumn{6}{c}{\textit{\textbf{Qwen1.5-1.8B $\rightarrow$ GPT2-120M}}} \\ 
\hline
% \multicolumn{6}{c}{\textit{Teacher: GPT2-1.5B (Same Vocabulary)}} \\ 
% \hline
% Teacher 
% & $26.68_{\pm 0.23}$	
% & $30.01_{\pm 0.36}$	
% & $24.57_{\pm 0.21}$  
% & $32.00_{\pm 0.27}$
% & $28.32$ \\  
% \cline{1-6}
% KL 
% & $24.20_{\pm 0.37}$	
% & $27.55_{\pm 0.59}$	 
% & $21.32_{\pm 0.09}$	 
% & $31.29_{\pm 0.30}$  
% & $26.09$ \\  
% \textbf{KL + \method} 
% & $\mathbf{24.80_{\pm 0.32}}$	
% & $\mathbf{28.14_{\pm 0.30}}$	
% & $\mathbf{21.85_{\pm 0.12}}$	 
% & $\mathbf{31.36_{\pm 0.09}}$  
% & $\mathbf{26.54}_{\color{darkgreen}\uparrow 0.45}$ \\  
% \hline  
% \multicolumn{6}{c}{\textit{Teacher: Qwen1.5-1.8B (Different Vocabularies)}} \\
% \hline
Teacher 
& $28.23_{\pm 0.13}$	
& $33.76_{\pm 0.33}$	
& $30.32_{\pm 0.26}$	
& $35.37_{\pm 0.26}$ 
& $31.92$ \\  
\cline{1-6}
SFT 
% 23.78 +- 0.38		27.2 +- 0.23			20.45 +- 0.23		30.25 +- 0.22
& $23.78_{\pm 0.38}$	
& $27.20_{\pm 0.23}$	
& $20.45_{\pm 0.23}$  
& $30.25_{\pm 0.22}$
& $25.42$ \\  
ULD \cite{boizard2024towards}
& $23.77_{\pm 0.41}$	
& $27.50_{\pm 0.50}$	
& $21.37_{\pm 0.34}$	
& $30.23_{\pm 0.10}$ 
& $25.72$ \\ 
% \textbf{ULD + \method} 
% & $\mathbf{23.83_{\pm 0.28}}$	
% & $\mathbf{28.12_{\pm 0.41}}$	
% & $\mathbf{21.76_{\pm 0.16}}$	 
% & $\mathbf{30.43_{\pm 0.12}}$ 
% & $\mathbf{26.04}_{\color{darkgreen}\uparrow 0.32}$ \\ 
MinED \cite{wan2024knowledge}
& $24.21_{\pm 0.31}$	
& $28.47_{\pm 0.42}$	
& $21.76_{\pm 0.13}$	 
& $31.36_{\pm 0.09}$ 
& $26.45$ \\ 
% \textbf{MinED + \method} 
% & $\mathbf{24.63_{\pm 0.36}}$	
% & $\mathbf{28.86_{\pm 0.44}}$	
% & $\mathbf{22.18_{\pm 0.19}}$	 
% & $\mathbf{31.58_{\pm 0.24}}$ 
% & $\mathbf{26.81}_{\color{darkgreen}\uparrow 0.36}$ \\ 
DSKD \cite{zhang2024dual}
& $24.42_{\pm 0.32}$	
& $28.48_{\pm 0.32}$    
& $22.26_{\pm 0.26}$ 
& $31.46_{\pm 0.22}$ 
& $26.66$ \\ 
\textbf{\method} 
& $\mathbf{25.34_{\pm 0.23}}$  
& $\mathbf{28.78_{\pm 0.18}}$	
& $\mathbf{24.02_{\pm 0.32}}$	
& $\mathbf{31.76_{\pm 0.10}}$ 
& $\mathbf{27.48}_{\color{darkgreen}\uparrow 0.82}$ \\  
\bottomrule
\toprule
\multicolumn{6}{c}{\textit{\textbf{Mistral-7B $\rightarrow$  TinyLLaMA-1.1B}}} \\ 
\hline
% \multicolumn{6}{c}{\textit{Teacher: Llama2-7B (Same Vocabulary)}} \\ 
% \hline Teacher
% & $32.15_{\pm 0.56}$ 
% & $36.44_{\pm 0.48}$ 
% & $30.16_{\pm 0.25}$ 
% & $36.18_{\pm 0.23}$ 
% & $33.73$ \\  
% \cline{1-6}
% KL
% & $25.75_{\pm 0.26}$ 
% & $31.83_{\pm 0.57}$ 
% & $26.73_{\pm 0.17}$ 
% & $32.83_{\pm 0.18}$ 
% & $29.29$ \\  
% \textbf{KL + \method} 
% & $\mathbf{26.23_{\pm 0.31}}$ 
% & $\mathbf{32.34_{\pm 0.39}}$ 
% & $\mathbf{27.97_{\pm 0.28}}$ 
% & $\mathbf{33.42_{\pm 0.11}}$ 
% & $\mathbf{29.99}_{\color{darkgreen}\uparrow 0.7}$  \\  
% \hline
% \multicolumn{6}{c}{\textit{Teacher: Mistral-7B (Different Vocabularies)}} \\ 
% \hline 
Teacher 
% 32.15 +- 0.56		36.44 +- 0.48			30.16 +- 0.25		36.18 +- 0.23


& $32.15_{\pm 0.56}$ 
& $36.44_{\pm 0.48}$ 
& $30.16_{\pm 0.25}$ 
& $36.18_{\pm 0.23}$ 
& $33.73$ \\ 
\cline{1-6}
SFT 
& $23.20_{\pm 0.16}$ 
& $29.48_{\pm 0.48}$ 
& $24.65_{\pm 0.25}$ 
& $31.08_{\pm 0.17}$ 
& $27.10$ \\
ULD \cite{boizard2024towards}
& $25.48_{\pm 0.29}$ 
& $31.33_{\pm 0.36}$ 
& $26.55_{\pm 0.10}$ 
& $33.69_{\pm 0.26}$ 
& $29.26$ \\ 
% \textbf{ULD + \method} 
% & $\mathbf{25.63_{\pm 0.22}}$ 
% & $\mathbf{31.86_{\pm 0.53}}$ 
% & $\mathbf{27.72_{\pm 0.33}}$ 
% & $\mathbf{33.90_{\pm 0.14}}$ 
% & $\mathbf{29.78}_{\color{darkgreen}\uparrow 0.52}$ \\ 
MinED \cite{wan2024knowledge}
& $25.54_{\pm 0.59}$ 
& $31.82_{\pm 0.33}$ 
& $26.13_{\pm 0.23}$ 
& $33.31_{\pm 0.16}$ 
& $29.20$ \\ 
% \textbf{MinED + \method} 
% & $\mathbf{26.32_{\pm 0.52}}$ 
% & $\mathbf{32.76_{\pm 0.27}}$ 
% & $\mathbf{28.13_{\pm 0.13}}$ 
% & $\mathbf{33.42_{\pm 0.20}}$ 
% & $\mathbf{30.16}_{\color{darkgreen}\uparrow 0.96}$ \\
DSKD \cite{zhang2024dual}
& $26.28_{\pm 0.35}$ 
& $32.31_{\pm 0.15}$ 
& $26.74_{\pm 0.24}$ 
& $33.44_{\pm 0.18}$ 
& $29.69$ \\ 
\textbf{\method} 
& $\mathbf{27.41_{\pm 0.43}}$  
& $\mathbf{33.31_{\pm 0.49}}$ 
& $\mathbf{29.77_{\pm 0.20}}$  
& $\mathbf{35.01_{\pm 0.20}}$ 
& $\mathbf{31.38}_{\color{darkgreen}\uparrow 1.69}$ \\ 
\bottomrule   
\toprule 
\multicolumn{6}{c}{\textit{\textbf{Qwen2.5-7B-Instruct $\rightarrow$ GPT2-1.5B}}} \\  
\hline
Teacher 
& $28.49_{\pm 0.21}$	
& $35.75_{\pm 0.25}$	
& $32.35_{\pm 0.24}$	
& $35.24_{\pm 0.08}$ 
& $32.96$ \\  
\cline{1-6}
SFT
& $21.83_{\pm 0.28}$	
& $27.15_{\pm 0.31}$	
& $23.16_{\pm 0.15}$	
& $30.74_{\pm 0.17}$ 
& $25.72$ \\
ULD \cite{boizard2024towards}
& $24.52_{\pm 0.28}$	
& $29.17_{\pm 0.22}$	
& $24.18_{\pm 0.08}$	
& $32.74_{\pm 0.35}$ 
& $27.65$ \\ 
% \textbf{ULD + \method} 
% & $\mathbf{24.73_{\pm 0.33}}$	
% & $\mathbf{30.22_{\pm 0.18}}$	
% & $\mathbf{25.07_{\pm 0.23}}$	
% & $\mathbf{33.30_{\pm 0.42}}$ 
% & $\mathbf{28.33}_{\color{darkgreen}\uparrow 0.68}$ \\ 

MinED \cite{wan2024knowledge}
& $25.52_{\pm 0.44}$	
& $30.41_{\pm 0.56}$	
& $25.09_{\pm 0.25}$	
& $33.83_{\pm 0.24}$	         
& $28.71$ \\ 
% \textbf{MinED + \method} 
% & $\mathbf{25.85_{\pm 0.28}}$  
% & $\mathbf{30.43_{\pm 0.28}}$	
% & $\mathbf{25.74_{\pm 0.22}}$	
% & $\mathbf{34.79_{\pm 0.11}}$ 
% & $\mathbf{29.20}_{\color{darkgreen}\uparrow 0.49}$ \\ 

DSKD \cite{zhang2024dual}
& $25.38_{\pm 0.46}$	           
& $30.48_{\pm 0.38}$  
& $25.92_{\pm 0.18}$	 
& $33.82_{\pm 0.23}$            
& $28.90$ \\ 
\textbf{\method} 
& $\mathbf{26.72_{\pm 0.22}}$          
& $\mathbf{33.02_{\pm 0.40}}$	      
& $\mathbf{27.72_{\pm 0.13}}$	     
& $\mathbf{35.63_{\pm 0.22}}$              
& $\mathbf{30.77}_{\color{darkgreen}\uparrow 1.87}$ \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comparison of methods across different datasets. We present the $mean_{\pm std}$ values derived from experiments conducted across 5 random seeds. SFT refers to Supervised Fine-Tuning, where the student model is directly trained on the downstream dataset.}
\label{tab:main_result}
\end{table*}


\begin{table}[ht]
\centering
\begin{adjustbox}{width=0.49\textwidth}
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Methods} & \textbf{KL} & \textbf{ULD} & \textbf{MinED} & \textbf{DSKD} \\
\bottomrule
\toprule
\multicolumn{5}{c}{\textit{\textbf{GPT2-1.5B$^\dag$ \& Qwen1.5-1.8B$^\ddag$ $\rightarrow$ GPT2-120M}}} \\ 
\hline
Original & $26.09$ & $25.72$ & $26.45$ & $26.66$ \\
\quad + \textbf{ours} &
$\mathbf{26.54}_{\color{darkgreen}\uparrow 0.12}$ & $\mathbf{26.04}_{\color{darkgreen}\uparrow 0.32}$ & $\mathbf{26.81}_{\color{darkgreen}\uparrow 0.36}$ & $\mathbf{27.48}_{\color{darkgreen}\uparrow 0.82}$ \\
\toprule
\multicolumn{5}{c}{\textit{\textbf{Llama2-7B$^\dag$ \& Mistral-7B$^\ddag$ $\rightarrow$ TinyLLaMA-1.1B}}} \\ 
\hline
Original & $29.29$ & $29.26$ & $29.20$ &  $29.69$ \\
\quad + \textbf{ours} & $\mathbf{29.99}_{\color{darkgreen}\uparrow 0.7}$ & $\mathbf{29.78}_{\color{darkgreen}\uparrow 0.52}$ & $\mathbf{30.16}_{\color{darkgreen}\uparrow 0.96}$ & $\mathbf{31.38}_{\color{darkgreen}\uparrow 1.69}$\\
 
\toprule 
\multicolumn{5}{c}{\textit{\textbf{Qwen2.5-7B-Instruct$^\ddag$ $\rightarrow$ GPT2-1.5B}}} \\  
\hline
Original & - & $27.65$ & $28.71$ & $28.90$ \\
\quad + \textbf{ours} & - & $\mathbf{28.33}_{\color{darkgreen}\uparrow 0.68}$ & $\mathbf{29.20}_{\color{darkgreen}\uparrow 0.49}$ & $\mathbf{30.77}_{\color{darkgreen}\uparrow 1.87}$ \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Performance evaluation of our method across a diverse range of baseline models. We report the average ROUGE score across four tasks. Here, KL represents the method designed for similar vocabulary scenarios. while $\dag$ indicates teacher models used in the similar vocabulary setting, $\ddag$ denotes teacher models applied in the different vocabulary setting. DSKD + ours is \method.}
\label{tab:universal_rs}
\end{table}

% In this section, we provide experimental results and analysis to demonstrate the effectiveness of our proposed \method.


\subsection{Experimental Setup}\label{sec:exp_setup}

In the state-of-the-art method DSKD \cite{zhang2024dual}, the authors typically use a single dataset for distillation and assess performance across multiple datasets covering different domains or tasks during testing. Although the reported results are based on the best checkpoint across various tasks, Figure \ref{fig:scenario} indicates that this checkpoint may not accurately reflect the effectiveness of the distillation process on the in-domain training dataset. Specifically, we independently create training/validation/testing sets for each domain to enhance knowledge distillation within a specific domain. The datasets are specifically detailed as follows:


\paragraph{Data.} To conduct the knowledge distillation process, we choose four datasets \textsc{databricks-dolly-15k} (\textbf{Dolly}) processed by \cite{gu2024minillm}; \textsc{alpaca} (\textbf{Alpaca}) \cite{alpaca}; \textsc{S-NI} (\textbf{S-NI}) \cite{wang-etal-2022-super}; and \textsc{dialogsum} (\textbf{Dialogsum}) \cite{chen2021dialogsum}. For Alpaca, we retain only samples with response lengths in \([11, +\infty)\) and split them into training, validation, and test sets. Since the original S-NI dataset provides only training and test splits, we further partition its training set, selecting samples with lengths in \([6, +\infty)\) to create separate training and development subsets, and we filter the test set to include only samples with lengths in \([11, +\infty)\). Detailed statistics for each dataset are provided in Table \ref{tab:dataset}.

\paragraph{Baselines.} We apply our method and compare it against three state-of-the-art baselines, \textbf{ULD} \cite{boizard2024towards}, \textbf{MinED} \cite{boizard2024towards}, and \textbf{DSKD} \cite{zhang2024dual}, which utilize KD techniques on models with different vocabularies. Detailed descriptions of these baselines are provided in Section \ref{Related_work}.

Further details on the models used, as well as the training and evaluation setup, can be found in Appendix \ref{sec:appendix_exp}.

\subsection{Main Results}

\begin{figure}[t]
    \centering
    \adjustbox{max width=0.49\textwidth}{
        \includegraphics{imgs/gpt4turbo_150samples_eval.pdf}
    }
    \caption{Win rate comparison across categories for DSKD and \method from Qwen2.5-7B-Instruct to GPT2-1.5B}
    \label{fig:win_rate_comparison}
\end{figure}

Tables \ref{tab:main_result} and \ref{tab:universal_rs} present the performance across various methods and datasets. Overall, our proposed approach consistently outperforms all baselines across various scenarios, demonstrating its effectiveness in diverse settings.

% \paragraph{Same Vocabulary Scenario:} Although our method is designed for models with different vocabularies, it can be universally applied, including scenarios where the teacher and student share the same vocabulary. In this case, we evaluate the effectiveness of \method by comparing it with a simple KD approach utilizing KL loss. The results indicate that applying our method achieves an improvement of 0.4â€“0.7\% in settings using TinyLLama-1.1B and GPT-120M, demonstrating its effectiveness in this scenario.


\paragraph{\method vs. State-of-the-Art Baselines:} Table \ref{tab:main_result} compares \method with baseline methods in the different vocabulary scenario, demonstrating that our approach significantly improves the performance of state-of-the-art baselines. Specifically, compared to the strongest baseline, DSKD, achieves nearly a 2\% improvement when using TinyLLama-1.1B and GPT-1.5B, and a 0.82\% gain when distilling models with over 1B parameters into GPT-120M. In addition to match-based metrics (e.g., ROUGE), we evaluate other qualitative aspects of the student model's responses, including helpfulness, relevance, accuracy, depth, and creativity, using the API of gpt4-turbo-0409 as the judge. We follow the prompt presented by \citet{zhang2024dual} and present in Figure \ref{fig:prompts}. The results, illustrated in Figure \ref{fig:win_rate_comparison}, reveal that after applying our approach to DSKD, the instances where the judge determines our responses to win or tie significantly exceed the loss rate on all 4 benchmarks. This highlights the improved naturalness and correctness achieved through \method, particularly by emphasizing the aspect of reasoning distillation.

\paragraph{Universal Applicability of Our Framework:} While our method is based on DSKD, it can be universally applied to any KD approach by substituting the $\mathcal{L}_{KD}$ term in Equation (\ref{eq:final_obj}) with the original KD loss of the respective method. Table \ref{tab:universal_rs} presents the results for this experiment on diverse KD baselines and models. Table \ref{tab:universal_rs} presents the results of this experiment across diverse KD baselines and models. The findings indicate consistent improvements with our framework, demonstrating its effectiveness in enhancing knowledge distillation performance across various methods and models in both similar and different vocabulary scenarios.

\subsection{Analysis}

\paragraph{Domain-Specific Scenarios Expose DSKD Limitations:} Unlike prior experimental setups of DSKD, we conduct both training and evaluation of the distillation process on domain-specific datasets, allowing the models to be fully optimized for specific applications on edge devices (as discussed in Section \ref{sec:exp_setup}). Figure \ref{fig:scenario} demonstrates that applying this scenario enables the model to achieve significantly higher performance on specific tasks (up to 20\%) compared to training under the previous settings. This approach enables a more in-depth exploration of distillation on model states that have been optimized for distinct task-specific objectives. Moreover, the results indicate that under our scenario, previous baselines do not demonstrate substantial dominance over others. For instance, in Table 1, DSDK outperforms MinED by only marginal proportions of 0.2\% when distilling Qwen1.5-1.8B to GPT-120M and Qwen2.5-7B-Instruct to GPT-1.5B. While DSDK reports significant performance (over 6\%) compared to MinED using their settings, this might not indicate the true effectiveness of each method among others on specific domain evaluation. Conversely, our method consistently enhances performance across various techniques, showcasing its effectiveness and reliability in domain-specific scenarios.

\begin{figure}[t]
\centering
\begin{adjustbox}{width=0.48\textwidth}
\includegraphics{imgs/fig_1.pdf}
\end{adjustbox}
\caption{Comparison of performance across various methods under the DSKD setting ($\alpha$) and domain-specific setting ($\beta$).}
\label{fig:scenario}
\end{figure}

\paragraph{Adaptability Across Diverse Scales:} In our experiments, we utilize two student model sizes: super small models (approximately 100M parameters, such as GPT2-120M) and small models (around 1B parameters, including TinyLLaMA-1.1B and GPT2-1.5B), paired with 1B and 7B teacher models, respectively. Across these varying scales, our method consistently demonstrates its effectiveness in enhancing existing KD techniques, showcasing its adaptability and scalability for practical use. Notably, we observe more substantial improvements when employing larger student and teacher models. This can be attributed to smaller teachers inherently exhibiting weaker reasoning capabilities \citep{shridhar2022distilling, bi-etal-2025-enhancing}, which results in less reliable responses and reduces the impact of the reasoning-aware mechanism in \method. Specifically, experiments with larger student-teacher pairs reveal that instruction-tuned teachers (e.g., Qwen 2.5 Instruct) offer greater benefits compared to base pretrained teacher models (i.e. LLaMa and Mistral). These findings underscore the significance of reasoning-focused distillation and highlight the effectiveness of ours approach.


\paragraph{Ablation Study:} Table \ref{tab:methods_ablation} provides an ablation study that highlights the individual contributions of each proposed component within \method.  Specifically, DSKD + \method achieves the highest scores on all datasets, demonstrating its superiority in the knowledge distillation process (e.g., improvements on Dolly from 26.28 to 27.41 and on Dialogue Sum from 33.44 to 35.01). Both proposed losses individually enhance baseline performance (from 0.6 to over 1\%), underscoring their effectiveness. Notably, $\mathcal{L}_{CRC}$ demonstrates greater improvement, highlighting the effectiveness of our constraint in guiding the CoT response to be more reliable. We also conduct experiments using CoT augmentation into the original DSKD. The expermiental results are presented in the line "DSKD + only COT".  It is evident that while CoT augmentation improves DSKD, its impact is not as significant as \method.

 Moreover, even when applied solely to the last hidden layers, the improvement remains evident, further validating the effectiveness of our approach. Finally, the combination of these components in \method yields best performance, highlighting their complementary nature and the ability to collectively optimize knowledge transfer. These results confirm the robustness of the proposed approach in enhancing the baseline KD method.

\begin{table}[t]
\centering
\begin{adjustbox}{width=0.49\textwidth}
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Methods}  & \textbf{Dolly} & \textbf{Alpaca} & \textbf{S-NI} & \textbf{Dialogue Sum}             \\ 
\midrule 
\multicolumn{5}{c}{\textbf{Mistral $\to$ TinyLLaMA}} \\ 
\hline
DSKD                     & 26.28\textsubscript{$\pm$0.35}     & 32.31\textsubscript{$\pm$0.15}     & 26.74\textsubscript{$\pm$0.24}     & 33.44\textsubscript{$\pm$0.18}     \\ 

\quad + only COT               & 27.27\textsubscript{$\pm$0.44}     & 32.42\textsubscript{$\pm$0.24}     & 28.97\textsubscript{$\pm$0.27}     & 33.74\textsubscript{$\pm$0.16}     \\ 

\quad + $\mathcal{L}_{CST}$                & 26.93\textsubscript{$\pm$0.40}     & 32.99\textsubscript{$\pm$0.41}     & 28.07\textsubscript{$\pm$0.16}     & 34.47\textsubscript{$\pm$0.20}     \\ 

\quad + $\mathcal{L}_{CRC}$               & 26.93\textsubscript{$\pm$0.46}     & 33.18\textsubscript{$\pm$0.58}     & 29.51\textsubscript{$\pm$0.24}     & 34.39\textsubscript{$\pm$0.24}     \\ 
\quad + only hidden state layers              & 27.07\textsubscript{$\pm$0.56}     & 33.10\textsubscript{$\pm$0.23}     & 29.41\textsubscript{$\pm$0.14}     & 34.80\textsubscript{$\pm$0.20}     \\ 
\textbf{\method}    & \textbf{27.41\textsubscript{$\pm$0.43}} & \textbf{33.31\textsubscript{$\pm$0.49}} & \textbf{29.77\textsubscript{$\pm$0.20}} & \textbf{35.01\textsubscript{$\pm$0.20}} \\ 
\bottomrule
\end{tabular}
    
\end{adjustbox}
\caption{Ablation study evaluating the impact of systematically removing each component from the \method framework, highlighting the contribution of individual techniques to overall performance.}
\label{tab:methods_ablation}
\end{table}



% \begin{table*}[htbp]
% \centering
% \label{tab:methods_seqkd}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Methods}          & \textbf{Dolly}            & \textbf{Alpaca}           & \textbf{S-NI}             & \textbf{Dialogue Sum}     \\ 
% \hline
% \multicolumn{5}{|c|}{\textbf{Mistral $\to$ TinyLLaMA}} \\ 
% \hline
% ULD \cite{boizard2024towards}      
%  & $25.48_{\pm 0.29}$  
%  & $31.33_{\pm 0.36}$  
%  & $\mathbf{26.55_{\pm 0.10}}$  
%  & $\mathbf{33.69_{\pm 0.26}}$ \\
% ULD+SeqKD                         
%  & $\mathbf{25.79_{\pm 0.52}}$  
%  & $\mathbf{31.47_{\pm 0.29}}$  
%  & $25.47_{\pm 0.19}$  
%  & $33.29_{\pm 0.19}$ \\
% \hline
% MinED \cite{wan2024knowledge}      
%  & $25.54_{\pm 0.59}$  
%  & $\mathbf{31.82_{\pm 0.33}}$  
%  & $\mathbf{26.13_{\pm 0.23}}$  
%  & $\mathbf{33.31_{\pm 0.16}}$ \\
% MinED+SeqKD                       
%  & $\mathbf{26.01_{\pm 0.40}}$  
%  & $31.38_{\pm 0.74}$  
%  & $25.60_{\pm 0.21}$  
%  & $33.21_{\pm 0.16}$ \\
% \hline
% DSKD \cite{zhang2024dual}
% & $\mathbf{26.28_{\pm 0.35}}$ 
% & $\mathbf{32.31_{\pm 0.15}}$ 
% & $26.74_{\pm 0.24}$ 
% & $\mathbf{33.44_{\pm 0.18}}$  \\
% DSKD-CMA+SeqKD                  
%  & $25.65_{\pm 0.26}$  
%  & $31.88_{\pm 0.31}$  
%  & $\mathbf{26.89_{\pm 0.35}}$  
%  & $33.28_{\pm 0.18}$ \\ 
% \hline
% \end{tabular}
% \caption{Baseline method using blackbox method data generated SEQKD \cite{kim-rush-2016-sequence}}
% \end{table*}




% Table \ref{tab:methods_seqkd} presents the performance of various methods using SeqKD for knowledge distillation from Mistral to TinyLLaMA across different datasets. 

% \begin{table*}[h]
% \centering
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Methods}          & \textbf{Dolly} & \textbf{Alpaca} & \textbf{Self-Instruct} & \textbf{Dialogue Sum} \\ 
% \hline
% SFT\_TinyLLaMA\_1.1B                       &                &                 &                        &                       \\ 
% \hline
% \multicolumn{5}{|c|}{\textbf{Llama2-7B $\to$ TinyLLaMA-1.1B (Same Vocabulary)}} \\ 
% \hline
% Teacher                   &                &                 &                        &                       \\  
% KL With OT Rationale      &                &                 &                        &                       \\  
% \hline
% \multicolumn{5}{|c|}{\textbf{Mistral $\to$ TinyLLaMA (Different Vocabularies)}} \\ 
% \hline
% Teacher                    &                &                 &                        &                       \\  
% \hline
% ULD                       &                &                 &                        &                       \\ 
% ULD+OT+Rationale                 &                &                 &                        &                       \\ 
% MinED                     &                &                 &                        &                       \\ 
% MinED+OT+Rationale                    &                &                 &                        &                       \\ 
% DSKD-CMA                  &                &                 &                        &                       \\ 
% DSKD-CMA+OT+Rationale                 &                &                 &                        &                       \\
% \hline 
% SFT\_GPT2\_120M     
% &                &                 &                        &                       \\ 
% \hline
% \multicolumn{5}{|c|}{\textbf{GPT2-1.5B $\to$ GPT2-120M (Same Vocabulary)}} \\ 
% \hline
% Teacher                   &                &                 &                        &                       \\  
% KL With OT Rationale      &                &                 &                        &                       \\  
% \hline
% \multicolumn{5}{|c|}{\textbf{Qwen1.5-1.8B $\to$ GPT2-120M (Different Vocabularies)}} \\ 

% \hline
% Teacher                   &                &                 &                        &                       \\  
% \hline
% ULD                       &                &                 &                        &                       \\ 
% ULD+OT+Rationale                 &                &                 &                        &                       \\ 
% MinED                     &                &                 &                        &                       \\ 
% MinED+OT+Rationale                    &                &                 &                        &                       \\ 
% DSKD-CMA                  &                &                 &                        &                       \\ 
% DSKD-CMA+OT+Rationale                 &                &                 &                        &                       \\  
% \hline
% SFT\_GPT2\_1.5B                       &                &                 &                        &                       \\ 
% \hline
% \multicolumn{5}{|c|}{\textbf{Qwen2.5 7B instruct $\to$ GPT2-1.5B (Different Vocabularies)}} \\ 
% \hline
% Teacher                   &                &                 &                        &                       \\  
% \hline
% ULD                       &                &                 &                        &                       \\ 
% ULD+OT+Rationale                 &                &                 &                        &                       \\ 
% MinED                     &                &                 &                        &                       \\ 
% MinED+OT+Rationale                    &                &                 &                        &                       \\ 
% DSKD-CMA                  &                &                 &                        &                       \\ 
% DSKD-CMA+OT+Rationale                 &                &                 &                        &                       \\ 
% \hline
% \end{tabular}
% \caption{Comparison of methods across different datasets}
% \label{tab:methods_comparison2}  % <-- UNIQUE LABEL
% \end{table*}






