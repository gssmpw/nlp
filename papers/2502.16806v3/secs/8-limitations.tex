In this study, we hypothesize that the embedding layer, as the initial layer of the model, and the final hidden state, corresponding to the last layer, play a crucial role in knowledge distillation due to their direct influence on representation learning and prediction. However, this assumption may not hold universally across all scenarios, as the contribution of each layer can vary depending on the specific task, model architecture, or training dynamics. Additionally, the role of intermediate layers in the distillation process remains insufficiently explored. A deeper understanding of how knowledge propagates through different layers and how intermediate representations contribute to student model learning could lead to more effective distillation strategies. Future research should investigate adaptive layer-wise distillation techniques that dynamically assign importance to different layers based on task requirements, potentially uncovering new insights into optimal knowledge transfer.