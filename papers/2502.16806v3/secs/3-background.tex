\subsubsection{Knowledge Distillation}
Knowledge distillation \cite{hinton2015distilling} is a technique where a large, high-capacity model, often referred to as the teacher model, transfers its knowledge to a smaller, more efficient model, known as the student model. The goal is to train the student to mimic the teacher’s behavior, often by aligning the student’s predictions with the teacher's soft outputs (e.g., probability distributions). Given a vector of \textit{logit} $z$ as the outputs of the last fully connected layer of a deep model, the distillation loss can be formulated as:
\begin{equation}
    \mathcal{L}_{KD} (z_{t}, z_{s}),
\end{equation}
where $\mathcal{L}(.)$ indicates the divergence loss of logits, $z_{t}$ and $z_{s}$ are logits of the teacher and student, respectively. A typically chosen loss can be KL divergence. When training the student model, we also need a student loss of input and target. Note that, the student loss is always defined as the cross-entropy loss $\mathcal{L}_{CE}(y,p(z_{s}))$ between the ground truth label $y$ and the soft logit of the student model $p(z_{s})$. The final loss to train a student model is defined as:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{CE}(y,p(z_{s})) + \mathcal{L}_{KD} (z_{t}, z_{s})
\end{equation}

% The conventional KD training paradigm is given in Fig. \ref{}. The teacher model is freezed in the distillation process while student models is trained. The Distillation Loss is computed after softmax layer with temperature $T=t$ to control the importance of each soft target. The Student Loss is computed after softmax layer with  temperature $T=1$ as usual.

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=1.0\textwidth]{imgs/hilton2015.png}
%     \caption{Knowledge Distillation Framework \citep{gou2021knowledge}.}
%     \label{fig:hilton2015}
% \end{figure*}


\subsubsection{Optimal Transport}

% Here's a paraphrased and more professional version of your Problem Formulation:

Optimal Transport (OT) \citep{villani2009optimal} is a mathematics framework to measure the dissimilarity between probability distributions. In comparison with others measures such as Kullback–Leibler divergence (KL) or Jensen-Shannon Divergence (JS) which require two distributions share the same support, OT does not require that condition. This feature enables OT in aligning different vocabularies of teacher and student models.

Formally, consider distributions are discrete. Given a complete separable metrics space $(\Omega,d)$, where $d:\Omega \times \Omega \to \mathbb{R}$ is the metrics on the space $\Omega$, let $P(\Omega)$ denote the set of all Borel probability measures on $\Omega$. Given to sets $\boldsymbol{X} = (\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, ...\boldsymbol{x}_{N})$, $\boldsymbol{Y} = (\boldsymbol{y}_{1}, \boldsymbol{y}_{2}, ...\boldsymbol{y}_{M})$ of $N$ and $M$ sample points in $\Omega$, their empirical probability measures are defined as $f = \sum_{i=1}^{N} \alpha_{i} \delta_{\boldsymbol{x}_{i}} \in P(\Omega)$ 
and  $g = \sum_{j=1}^{M} \beta_{j} \delta_{\boldsymbol{y}_{j}} \in P(\Omega)$, respectively, where $\delta_{\boldsymbol{x}}$ is the Dirac unit mass on the position of $\boldsymbol{x}$ in $\Omega$, $\alpha_{i}$ and $\beta_{j}$ are the weight on the unit mass on $\boldsymbol{x}_i$, $\boldsymbol{y}_j$ respectively. Since $f$, $g$ are probability distributions, the weights vectors $\boldsymbol{\alpha} = (\alpha_1, \alpha_2, ... \alpha_N)$, $\boldsymbol{\beta} = (\beta_1, \beta_2, ... \beta_M)$ lie in the simplexes  $ \Theta_{N} := \{ \alpha_i \geq 0 \forall i = 1,...,N | \sum_{i=0}^{N} \alpha_i = 1 \}$ and $ \Theta_{M} := \{ \beta_j \geq 0 \forall j = 1,...,M | \sum_{i=0}^{M} \beta_j = 1 \}$
The empirical joint probability measure of $(\boldsymbol{X},\boldsymbol{Y})$ is denoted as: 
\begin{equation}
    h = \sum_{i=1}^{N} \sum_{j=1}^{M} \gamma_{ij}(\delta_{\boldsymbol{x_i}}, \delta_{\boldsymbol{y_j}})
\end{equation} whose marginal measures w.r.t $\boldsymbol{X}$ and $\boldsymbol{Y}$ are $f$ and $g$, respectively. The weight matrix $[\gamma_{ij}]_{ij}$ is a $N \times M$ non-negative matrix with row and column marginals $\boldsymbol{\alpha}$, $\boldsymbol{\beta}$. More concrete, $\sum_{i=1}^{N} \gamma_{ij} = \beta_{j} \forall j=1...M$ and $\sum_{j=1}^{M} \gamma_{ij} = \alpha_{i} \forall i=1...N$. The set of all the feasible weight matrixes is defined as the transportation polytope $U(\boldsymbol{\alpha}, \boldsymbol{\beta}$) of $\boldsymbol{\alpha}$, $\boldsymbol{\beta}$:
{\small\begin{equation}
    U(\boldsymbol{\alpha}, \boldsymbol{\beta}) := \{ \boldsymbol{T} \in \mathbb{R}_{+}^{N \times M} | \boldsymbol{T}\boldsymbol{1}_{M}=\boldsymbol{\alpha}, \boldsymbol{T}^T\boldsymbol{1}_{N}=\boldsymbol{\beta} \}.
\end{equation}}
An element $t_{ij}$ of a feasible $\boldsymbol{T}$ can be seen as the amount of mass transported from $\boldsymbol{x}_{i}$ to $\boldsymbol{y}_{j}$. The distance between $\boldsymbol{x}_{i}$ and $\boldsymbol{y}_{j}$ is measured by a metric $d$ raised to the power $p$. Matrix $\boldsymbol{D}$ is the pairwise distances between elements in $\boldsymbol{X}$ and $\boldsymbol{Y}$:
\begin{equation}
    \boldsymbol{D} := [d(\boldsymbol{x}_{i}, \boldsymbol{y}_{j})^p]_{ij} \in \mathbb{R}^{N \times M}.
\end{equation}
The cost of transporting $f$ to $g$ given a transport $\boldsymbol{T}$ is the Frobenius dot product between $\boldsymbol{T}$ and $\boldsymbol{D}$, which is $ \langle \boldsymbol{T},\boldsymbol{D} \rangle = tr(\boldsymbol{T}^T\boldsymbol{D})$ 

Given $\boldsymbol{\alpha}$, $\boldsymbol{\beta}$ and $\boldsymbol{D}$, the OT distance between empirical probability measures $f$ and $g$ is a linear programing problem:
\begin{equation}
    d_{W}(\boldsymbol{\alpha},\boldsymbol{\beta},\boldsymbol{D}) = \min_{\boldsymbol{T} \in U(\boldsymbol{\alpha},\boldsymbol{\beta})} \langle \boldsymbol{T},\boldsymbol{D} \rangle.
\end{equation}



Optimal Transport provides a framework to align two distribution with different supports. In the context of knowledge distillation LLMs with different tokenizers, Optimal Transport can be used to align distributions over two vocabularies. The distributions can be representations in each layers, or the final softmax of teacher and student models. These alignments can enhance the representations of student models in different layers, hence they can help the student model mimic the output of teacher.