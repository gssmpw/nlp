

Recent studies increasingly utilize Large Language Models (LLMs) \cite{achiam2023gpt, anthropic2024claude3, llama3modelcard} for human-readable and interpretable persona modeling, advancing personalized applications like recommendation and behavior prediction. However, most research focuses on generating personas from static historical data, which fail to capture dynamic behaviors and evolving preferences in real-world interactive scenarios \cite{ wang2023zero, zhou2024language}. This underscores the need for dynamic persona modeling—a pivotal yet underexplored approach that iteratively updates personas using streaming user behavior data to continually enhance their quality.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/paradigm.pdf}
    \caption{Comparison of dynamic persona modeling paradigms: Regeneration replaces personas, and Extension adds to them, but neither ensures optimization. Our \method, based on Refinement paradigm, uses discrepancies between user behavior and model predictions to identify update directions for continuous optimization.}
    \label{fig:paradigm}
\end{figure}


Existing dynamic persona modeling methods can be broadly categorized into two paradigms: 
(1) \textit{Persona Regeneration}, which updates through complete replacement, rebuilds personas from scratch based on new user behaviors, either by aggregating historical and recent behaviors or by using sliding-window methods \cite{zhou2024language, wu2024rlpf, yang2023palr, xi2024towards}. 
(2) \textit{Persona Extension}, which updates through additive extension, incorporates new user behaviors into existing personas, either by directly integrating them or by merging short-term and long-term personas. \cite{lian2022incremental,liu2024once, tang2023enhancing,yin2023heterogeneous,yuan2024evaluating}.  
However, while these methods enable dynamic updates, they fail to ensure meaningful optimization due to the lack of mechanisms to evaluate update effectiveness and explicitly model the update process. Without validating whether updates enhance persona quality or predictive accuracy, both paradigms risk propagating errors and degrading performance. This highlights a critical challenge in dynamic persona modeling: \textit{Bridging the gap between updating personas and truly optimizing them}.



To better characterize the update process and bridge this gap, we introduce the concept of \textit{update direction}, which uniquely identifies the transformation from an existing persona to an updated one under given signals. It directly determines whether the update improves, degrades, or maintains persona quality within a specific context, serving as a core factor in persona optimization.


However, identifying an effective update direction is challenging due to the fundamental misalignment between the dense natural language persona space and the discrete user behavior space (e.g., ratings):
(1) Behavior signals are insufficient, e.g., a user’s 1-star movie rating does not clearly indicate whether the dissatisfaction is due to the story, pacing, or genre, making it difficult to identify specific errors in the persona.
(2) Evaluating update directions is inherently complex, e.g., even if we adjust the persona to emphasize “plot complexity” or “character development,” it’s unclear which change would lead to better predictions.
















To address the challenges, we propose \textbf{\method} (\text{\textbf{D}ir\textbf{e}ct\textbf{e}d \textbf{Pe}rsona \textbf{R}efinement}), a novel approach for LLM-based dynamic persona modeling. 
Specifically, we introduce a new paradigm, \textit{Persona Refinement} (Figure~\ref{fig:paradigm}), which uses discrepancies between user behaviors and model predictions as stronger update signals to expose deficiencies in personas.
To identify effective update directions, we decompose the optimization objective into three direction search goals: \textit{Previous Preservation}, \textit{Current Reflection}, and \textit{Future Advancement}, ensuring stability, adaptability, and task alignment. 
Based on these goals, we design reward functions for clear and measurable assessments of update directions by comparing predictive errors before and after updates.
Finally, we propose an iterative reinforcement learning (RL) framework with two training stages, leveraging self-sampling and DPO fine-tuning to progressively enhance the model’s direction search and persona refinement capabilities, ultimately improving prediction accuracy.


Extensive experiments on over 4800 users across 10 domains demonstrate \method’s strong persona optimization and direction search capability.

In summary, our contributions are as follows:

\begin{itemize}[noitemsep,left=0pt]
    \item We identify key limitations in current LLM-based dynamic persona modeling methods, emphasizing the critical gap between persona updating and optimization caused by weak update signals and unclear update direction.

    \item We propose \method, a novel approach to dynamic persona modeling that achieves continual optimization through discrepancy-based update signals and robust direction search.

    \item Extensive experiments demonstrate that \method successfully bridges this gap, outperforming existing methods in dynamic persona modeling.
\end{itemize}












 






























