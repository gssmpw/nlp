
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/deeper.pdf}
    \caption{Framework of \method. Grounded in three high-level goals for direction search, the iterative RL framework progressively enhances the model’s refinement capability through two rounds of self-sampling and training. Applied online in multi-round updates, it enables step-wise persona optimization via directed refinement.}
    \label{fig:deeper}
\end{figure*}


Existing regeneration- and extension-based methods enable dynamic updates but fall short in consistent quality improvement, resulting in a misalignment between the \textit{update step} and the \textit{optimization objective}. To address this, we highlight the critical role of update direction in ensuring effective updates and propose the following core proposition:

\noindent  
\textit{Better update directions lead to better personas.}

\noindent 
Instead of directly searching for improved personas, we optimize refinement directions. By incorporating model predictions into the context and defining three high-level goals for direction search, we propose an iterative reinforcement learning framework with a balanced reward function, enabling effective refinement and continual persona optimization.



\subsection{Refinement Step Formulation}

In \method, each persona refinement step at time $t$, can be formulated as a reinforcement learning (RL) task.
The objective is to learn a policy $\pi_\theta$ to identify optimal refinement directions for specific contexts. For a single user $\mathcal{U}$ in domain $\mathcal{X}$, the refinement step can be formulated as:

\begin{itemize}[noitemsep,left=0pt]
\item \textbf{State:} The previous persona $\mathcal{S}_{t-1}$, generated after the $(t-1)$-th refinement round at the end of the previous window $\mathcal{W}_{t-1}$.

\item \textbf{Observation:} The observation at time step $t$, $\mathcal{O}_t = \{\mathbf{O}_t, \hat{\mathbf{O}}_{t|\mathcal{S}_{t-1}}\}$, where $\mathbf{O}_t$ and $\hat{\mathbf{O}}_{t|\mathcal{S}_{t-1}}$ represent the actual and predicted behaviors in the current window $\mathcal{W}_t$.

\item \textbf{Action:} The refined persona $\mathcal{S}_t$, generated after the $t$-th refinement process based on the corresponding $(\mathcal{S}_{t-1}, \mathcal{O}_t)$ of the user. 

\item \textbf{Policy Model:} The refinement model $\pi_\theta$, maps the state and observation to refined persona $\mathcal{S}_t$:
\vspace{-2mm}
\[
\pi_\theta : (\mathcal{S}_{t-1}, \mathcal{O}_t) \to \mathcal{S}_t. \tag{6}
\vspace{-2mm}
\]
\item \textbf{Reward:} The reward $r_t$ quantifies the effectiveness of the refinement process.
\end{itemize}

\subsection{Direction and Goal Definition}
In this work, we formally define the persona refinement direction and its goals as follows:

\noindent
\paragraph{Definition 3.} (\textit{Persona Refinement Direction}) Identify the directed path of a specific persona refinement step, denoted as $\mathcal{D}_t$, which is uniquely determined by the previous persona $\mathcal{S}_{t-1}$, the current observation $\mathcal{O}_t$, and the refined persona $\mathcal{S}_t$:
\vspace{-2mm}
\[
\mathcal{D}_t \leftrightarrow (\mathcal{S}_{t-1}, \mathcal{O}_t; \mathcal{S}_t). \tag{7}
\vspace{-2mm}
\]


We define three high-level goals for direction search, ensuring comprehensive guidance with temporal insights from past, present, and future.

\noindent
\paragraph{Goal 1.} (\textit{Previous Preservation}): 
Retain stable persona traits from historical behaviors to ensure consistency and preserve critical information.

\noindent
\paragraph{Goal 2.} (\textit{Current Reflection}): Adapt to recent user behaviors by incorporating dynamic changes and correcting errors in the previous persona.

\noindent
\paragraph{Goal 3.} (\textit{Future Advancement}): Enhance the persona’s predictive capability for future behaviors.

\subsection{Reward Function Design}
Given the unique correspondence between $\mathcal{D}_t$ and the triplet $(\mathcal{S}_{t-1}, \mathcal{O}_t; \mathcal{S}_t)$, the quality of $\mathcal{D}_t$ directly determines the refined persona’s quality and process effectiveness within context $(\mathcal{S}_{t-1}, \mathcal{O}_t)$. Accordingly, we formalize three goals of \textit{Direction Quality} as reductions in prediction error from refinement across past, current, and future windows, represented by rewards $r_t^{prev}$, $r_t^{curr}$, and $r_t^{fut}$.

\vspace{-2mm}
\[
r_t^{prev} = \varepsilon_{t-1|\mathcal{S}_{t-1}} - \varepsilon_{t-1|\mathcal{S}_t}
\vspace{-2mm}
\]
\[
r_t^{curr} = \varepsilon_{t|\mathcal{S}_{t-1}} - \varepsilon_{t|\mathcal{S}_t}
\vspace{-2mm}
\]
\[
r_t^{fut} = \varepsilon_{t+1|\mathcal{S}_{t-1}} - \varepsilon_{t+1|\mathcal{S}_t}. \tag{8}
\]

\noindent
$\varepsilon_{t-1|\mathcal{S}_{t-1}}$, $\varepsilon_{t|\mathcal{S}_{t-1}}$, and $\varepsilon_{t+1|\mathcal{S}_{t-1}}$ are prediction errors with previous persona $\mathcal{S}_{t-1}$ across $\mathcal{W}_{t-1}$, $\mathcal{W}t$, and $\mathcal{W}_{t+1}$, respectively, while $\varepsilon_{t-1|\mathcal{S}_t}$, $\varepsilon_{t|\mathcal{S}_t}$, and $\varepsilon_{t+1|\mathcal{S}_t}$ are errors with refined persona $\mathcal{S}_t$.

The total reward for a refinement step is:
\vspace{-2mm}
\[
r_t = r_t^{prev}+ r_t^{curr}+r_t^{fut}. \tag{9}
\]

\subsection{Iterative Training Framework}
\method employs an iterative training framework (Figure~\ref{fig:deeper}): Iteration 1 fine-tunes the base model to refine initial personas (Model 1), while Iteration 2 further enhances it to refine pre-optimized personas (Model 2). Direct Preference Optimization (DPO)\cite{rafailov2024direct} is used to seamlessly integrate rewards into preference pairs, enabling the model to identify better directions through explicit comparisons and supporting scalable iterative fine-tuning.

\paragraph{Iteration 1: Learn to Refine Initial Personas}
Iteration 1 formulates the first refinement step at $t=1$ as an RL task, where we fine-tune the base model to refine initial personas $\mathcal{S}_0$, establishing a baseline policy for direction search and refinement.

\textbf{Context Data Construction}  
First, we initialize personas $\mathcal{S}_0$ for users across multiple domains with their behaviors in window $\mathcal{W}_0$, serving as initial states for refinement processes. The prediction model then predicts user behaviors in $\mathcal{W}_1$ based on $\mathcal{S}_0$. Combining predicted and actual behaviors, we construct observations $\mathcal{O}_1$. Together, $(\mathcal{S}_0, \mathcal{O}_1)$ form the context of the first refinement step.

\textbf{Direction Sampling and Reward Calculation}  
For each context input $(\mathcal{S}_0, \mathcal{O}_1)$, the base model samples $M$ candidate refined personas $\{\mathcal{S}_1^k\}_{k=1}^M$, where each candidate direction $\mathcal{D}_t^k$ is represented by $(\mathcal{S}_0, \mathcal{O}_1; \mathcal{S}_1^k)$. Rewards for these directions as calculated as specified in Equation (9).

\textbf{Preference Pairs Construction and Training}
Refined personas are partitioned into a positive set $\mathcal{S}_1^+$ (rewards $r_t \geq \tau^+$) and a negative set $\mathcal{S}_1^-$ (rewards $r_t \leq \tau^-$) based on reward thresholds. To ensure a clear distinction, we enforce a margin $\delta$, derived from the reward distribution, such that $r_t^w - r_t^l \geq \delta$. The base model is then fine-tuned using DPO with these preference pairs. Following~\cite{gui2024bonbon}, a Supervised Fine-Tuning (SFT) loss is incorporated into the standard DPO objective to maintain alignment with high-quality refinements:
\vspace{-2mm}
\[
\mathcal{L}(\pi_\theta; \pi_{\text{ref}}) = \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) + \alpha \mathcal{L}_{\text{SFT}}(\pi_\theta). \tag{10}
\]

\paragraph{Iteration 2: Learn to Refine Optimized Personas}
Iteration 2 extends the model's refinement capabilities to handle pre-optimized personas, addressing increased complexity of nuanced refinement tasks.

\textbf{Context Data Construction}
Includes:
(1) Contexts from Iteration 1 $(\mathcal{S}_0, \mathcal{O}_1)$. (2) New contexts $(\mathcal{S}_1, \mathcal{O}_2)$ constructed based on the second refinement step, using $\mathcal{S}_1$ as initial states, with predicted and actual behaviors in $\mathcal{W}_2$ as observations.

\textbf{Direction Sampling and Reward Calculation}
Model 1 is used to sample candidates, following the same procedure of Iteration 1.

\textbf{Preference Pairs Construction and Training}
Similarly to Iteration 1, we construct preference pairs with consistent boundaries for positive and negative sets, with a larger margin $\delta$ to accommodate refined reward distribution and model performance. Model 1 is then fine-tuned with the same combined loss as in Iteration 1, incorporating a subset of preference pairs from Iteration 1 to prevent forgetting and ensure continual learning.






