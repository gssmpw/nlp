

\subsection{Experiment Setup}


\paragraph{Dataset and Task Data Construction}  
We evaluate \method on four real-world datasets across 10 domains, including \textit{MovieLens 20M}\cite{harper2015movielens}, \textit{Food.com Recipes}\cite{majumder2019generating}, \textit{Google Local Reviews}\cite{yan2023personalized,li2022uctopic}, and \textit{Amazon Reviews (2018)}\cite{ni2019justifying}. From six domains, we sample 14,959 users with over 50 ratings (10,800 for training and 4,159 for testing). To assess generalization, an auxiliary test set of 650 users from four unseen domains is constructed. User interactions are segmented into five 10-rating windows, with $\mathcal{W}_0$ used for initial persona generation.

\paragraph{Evaluation} 
As described in Section~\ref{sec:preliminary}, we evaluate the effectiveness of persona update methods based on their ability to achieve \textit{Continual Persona Optimization}, quantified by the reduction in future prediction error $\varepsilon_{t+1|\mathcal{S}_t}$ across update rounds.



\paragraph{Baselines}
We compare against baselines from two paradigms:
1. \textbf{Persona Regeneration}:  
   - \textit{SlideRegen}: Rebuilds personas using only the latest window of behaviors~\cite{yang2023palr}.
   - \textit{FullRegen}: Reconstructs personas by leveraging all historical and recent behaviors~\cite{zhou2024language}.
2. \textbf{Persona Extension}: 
   - \textit{IncUpdate}: Incrementally integrates new behaviors into existing personas~\cite{yuan2024evaluating}. 
   - \textit{HierMerge}\cite{liu2024once}: Hierarchically merges short-term and long-term personas.

\paragraph{\method Training Details}  
For \method, we use Llama-3.1-8B-Instruct\cite{ llama3modelcard} as the base policy model, trained iteratively on data from 10,809 users. Each iteration samples 15 candidate personas per input, with reward boundaries $\tau^+ = 0.5$ and $\tau^- = 0$. Iteration 1 applies a margin $\delta = 0.5$, producing 34,782 DPO pairs. Iteration 2 increases $\delta$ to 0.8 and incorporates 5,000 pairs from Iteration 1, resulting in 33,612 pairs. Both iterations use LoRA for fine-tuning with a learning rate of $5 \times 10^{-6}$, 4 training epochs, and a batch size of 128. The SFT loss coefficient ($\alpha$) is set to 0.1. Figure~\ref{fig:reward_kde} illustrates the reward distribution improvements across iterations.

\begin{figure}[t]
    \centering    
    \includegraphics[width=\linewidth]{figure/reward_kde_plot.pdf}
    \caption{KDE plot illustrating changes in reward distribution across test sets before and after training.}
    \label{fig:reward_kde}
\end{figure}

\paragraph{Global and Baseline Settings}

We use the frozen, powerful LLM, GPT-4o-mini\cite{achiam2023gpt}, to generate initial personas and predictions in a zero-shot setting for both training and evaluation, ensuring consistent initial persona quality and unbiased predictions. Additionally, it serves as the backbone for all baselines, offering a robust foundation for comparison.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/MAE_trend.pdf}
    \caption{Performance of different methods in dynamic persona modeling over 4 rounds across 10 domains. The first six ((A) \textit{Recipe}, (B) \textit{Book}, (C) \textit{Clothing Shoes and Jewelry}, (D) \textit{Local Business}, (E) \textit{Movies and TV}, (F) \textit{MovieLens}) are seen during training, while ((G) \textit{Arts Crafts and Sewing}, (H) \textit{Automative}, (I) \textit{Sports and Outdoors}, (J) \textit{Grocery and Gourmet Food}) are unseen. In subsequent figures, domains are referred to by their corresponding letters.}
    \label{fig:main_results}
\end{figure*}















\subsection{Main Results}
Figure~\ref{fig:main_results} compares performance of \method and baseline methods over four update rounds across 10 domains in the dynamic persona modeling task.




\paragraph{\method helps continual persona optimization.}  
\method consistently achieves substantial MAE reductions across all 10 domains over four update rounds, with an average decrease of 32.2\%, significantly outperforming extension-based baselines such as \textit{IncUpdate} (9.28\%) and \textit{HierMerge} (3.92\%). Notably, in the unseen domain \textit{Arts Crafts and Sewing}, \method achieves the largest improvement, reducing MAE from 0.76 to 0.40 (47.1\%). In contrast, regeneration-based baselines like \textit{FullRegen} and \textit{SlideRegen} often exhibit minimal or negative gains, highlighting their inability to meet the task objective.

\paragraph{Generalized capability and domain-specific dynamic.}
\method achieves an average MAE reduction of 29.4\% in seen domains and 36.4\% in unseen domains. This emphasizes its generalized optimization capability to diverse and new scenarios. Figure~\ref{fig:main_results} also reveals domain-specific variations in optimization speed, convergence patterns, and improvement potential. For instance, domains like \textit{Automotive} exhibit faster optimization with earlier convergence, while \textit{Movies and TV} shows slower progress and prolonged refinement. These suggest potential influences from varying persona modeling complexities, behavior predictability, and interest stability across domains. 




