
\subsection{What enables \method's effectiveness}



\paragraph{Direction search enables optimization.}
We first evaluate the necessity of direction search by comparing \method with frozen models (GPT-4o-mini and Llama-3.1-8B-Instruct (Naive Model)) which refine personas directly. As shown in Figure~\ref{fig:analysis_radar_part1}(a), both baselines exhibit significant error increases after refinement, underscoring the critical role of direction search in effective optimization.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]
    {figure/analysis_radar_part1.pdf}
    \caption{
    (a) Refinement performance of \method compared to frozen models across ten domains, using \(\varepsilon_{1 | \mathcal{S}_0}\) as pre-refinement baseline. 
    (b) Refinement under different reward settings. 
    Smaller areas indicate reduced errors and improved refinement relative to baseline.}
    \label{fig:analysis_radar_part1}
\end{figure}
\vspace{-2mm}

\paragraph{Balanced goals drive better optimization.}
The assessment of Direction Quality is critical to optimization performance. We compare \method’s balanced reward setting(equally weights previous, current, and future goals), with a future-focused reward and a decayed reward (decay factor = 0.5) prioritizing recent goals. As shown in Figure~\ref{fig:analysis_radar_part1}(b), \method consistently outperforms both baselines across all domains. These findings underscore the importance of balanced, goal-driven direction search in enabling effective persona refinement.


\input{table/goals}

\paragraph{\method excels in identifying high-quality directions.}
Building on previous insights,
we further evaluate \method’s ability to identify high-quality refinement directions by analyzing its performance across three goals (Table~\ref{tab:goals}). 
\method demonstrates outstanding performance: minimizing previous forgetting with the smallest average MAE increment of 0.062 (\textit{Previous Preservation}); reducing current errors by 0.572 on average (\textit{Current Reflection}); and improving future predictions with an average 
reduction of 0.222 (\textit{Future Advancement}). Notably, \method surpasses all baselines across domains for \textit{Future Advancement}, demonstrating its capacity step-wise optimization. These results highlight \method’s ability to balance three goals for better direction search and continual optimization.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/analysis_radar_part2.pdf}
    \caption{(a) Refinement performance across two RL iterations of \method. (b) Comparison of \method and fine-tuned \textit{IncUpdate} (Inc-FT). }
    \label{fig:analysis_radar_part2}
\end{figure}


\paragraph{Iterative RL enhances persona refinement.}
Guided by stage-specific objectives, \method’s two-stage iterative RL framework incrementally enhances refinement capabilities by leveraging progressively higher-quality self-sampled data and expanded preference margins. Results (Figure~\ref{fig:analysis_radar_part2}(a)) show accelerated improvements in the second iteration, highlighting effects of iterative training.


\paragraph{Prediction discrepancy facilitates direction search.}  
We finally analyze paradigm's role in direction search by employing \method’s training framework into \textit{IncUpdate} (the best-performing baseline). Figure~\ref{fig:analysis_radar_part2}(b) show that while direction search training improves \textit{IncUpdate}’s performance, it still falls short of \method. This underscores prediction discrepancy’s role in enabling context-specific search and more precise refinement.


\subsection{Persona Probing}
We further conduct an preliminary analysis of refined personas, termed \textit{persona probing}, to explore additional insights and applications of \method.

\input{table/persona_tokens}

\noindent
\textbf{Dynamic persona evolution across rounds.}
We first analyze persona dynamics during refinement process. Table~\ref{tab:persona_tokens} highlights \method’s controlled length growth, balancing representation efficiency and informativeness. Figure~\ref{fig:persona_probing}(a) reveals diminishing persona changes over time, with substantial shifts in early updates ($\mathcal{S}_0 \to \mathcal{S}_1$) and increasing stability in later rounds ($\mathcal{S}_1 \to \mathcal{S}_4$), indicating convergence and improved contextual alignment.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/persona_probing.pdf}
    \caption{(a) Cosine similarity among personas across rounds; (b) User clusters based on final personas(Book).}
    \label{fig:persona_probing}
\end{figure}


\input{table/book_dimensions}

\paragraph{Insights from final optimized personas.} 
Refined personas from \method also enable in-depth, domain-specific exploration. 
In Book domain, we uncover \textbf{group-level preferences} by clustering final persona embeddings, identifying five user groups characterized by unique high-frequency adjectives (e.g., “romantic” and “practical”) (Figure~\ref{fig:persona_probing}(b)). 
We also extract \textbf{domain-specific patterns} by organizing high-frequency terms into eight dimensions using GPT-4o (Table~\ref{tab:book_dimensions}), highlighting critical factors for modeling Book domain users. These attempts show \method’s potential to support strategic user insights exploration.

