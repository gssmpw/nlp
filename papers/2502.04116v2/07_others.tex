\chapter{Other Variants of Generative Adversarial Networks}
Generative Adversarial Networks (GANs) have inspired many variations, each designed to address specific challenges or extend the capabilities of the original GAN framework. In this chapter, we will explore several advanced GAN variants: Energy-Based GANs (EBGANs)~\cite{zhao2016energy}, Adversarial Autoencoders (AAEs)~\cite{makhzani2015adversarial}, Bidirectional GANs (BiGANs), and Autoencoder GANs (AEGANs)~\cite{donahue2016adversarial}. These models offer unique approaches to improving the stability, interpretability, and functionality of GANs. We will provide detailed explanations of each variant, along with examples and practical use cases.

\section{Energy-Based Generative Adversarial Networks (EBGAN)}
EBGAN is a variant of GAN that takes an energy-based approach to the discriminator. Instead of having the discriminator output a probability, EBGAN models the discriminator as an energy function, which assigns a scalar value (energy) to the input~\cite{zhao2016energy}. The generator is trained to produce samples that have low energy, while the discriminator is trained to assign higher energy to fake samples and lower energy to real samples~\cite{zhang2019self}.

\subsection{Core Concept of EBGAN}
In traditional GANs, the discriminator outputs the probability of whether the input is real or generated. EBGAN changes this by treating the discriminator as an energy function. The energy function is minimized for real samples and maximized for fake samples. The generator's goal is to create samples that the discriminator assigns low energy to, thereby making them indistinguishable from real samples.

The key difference between EBGAN and traditional GANs is how the discriminator works. In EBGAN, the discriminator is treated as an autoencoder that reconstructs the input image. The energy of a sample is defined as the reconstruction error, which is minimized for real samples and maximized for fake ones~\cite{zhao2016energy}.

\subsection{EBGAN Objective Function}
The EBGAN loss can be written as:

\[
\mathcal{L}_{\text{EBGAN}} = \mathbb{E}_{x \sim p_{\text{data}}} [E(x)] - \mathbb{E}_{z \sim p_z(z)} [E(G(z))]
\]

Where:
\begin{itemize}
    \item \( E(x) \) is the energy assigned to real samples by the discriminator (autoencoder reconstruction loss).
    \item \( G(z) \) is the generator, which tries to produce low-energy samples.
\end{itemize}

The discriminator is trained to increase the energy (reconstruction error) for fake samples while decreasing it for real samples.

\subsection{EBGAN Architecture}
In EBGAN, the discriminator is implemented as an autoencoder. The generator produces samples, which are passed through the autoencoder (discriminator). The autoencoder tries to reconstruct the input, and the energy is defined as the reconstruction loss.

\textbf{Autoencoder Discriminator:}
\begin{itemize}
    \item The input image is encoded into a low-dimensional representation.
    \item The encoded representation is then decoded back into the original image.
    \item The reconstruction error serves as the energy of the input.
\end{itemize}

\subsection{EBGAN Implementation in PyTorch}
Here's a basic implementation of EBGAN using PyTorch:

\begin{lstlisting}[style=python]
import torch
import torch.nn as nn
import torch.optim as optim

# Autoencoder-based Discriminator (Energy Function)
class AutoencoderDiscriminator(nn.Module):
    def __init__(self):
        super(AutoencoderDiscriminator, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(128 * 8 * 8, 1024),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(1024, 128 * 8 * 8),
            nn.ReLU(),
            nn.Unflatten(1, (128, 8, 8)),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        reconstructed = self.decoder(encoded)
        return reconstructed

# Generator
class Generator(nn.Module):
    def __init__(self, latent_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 128 * 8 * 8),
            nn.ReLU(),
            nn.Unflatten(1, (128, 8, 8)),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )
    
    def forward(self, z):
        return self.model(z)

# Loss function (Reconstruction Loss for Discriminator)
def reconstruction_loss(real, reconstructed):
    return nn.functional.mse_loss(reconstructed, real)

# Training loop
latent_dim = 100
generator = Generator(latent_dim)
discriminator = AutoencoderDiscriminator()

optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)
optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)

for epoch in range(epochs):
    for i, (real_imgs, _) in enumerate(dataloader):
        z = torch.randn(real_imgs.size(0), latent_dim)
        fake_imgs = generator(z)

        # Train Discriminator
        real_reconstructed = discriminator(real_imgs)
        fake_reconstructed = discriminator(fake_imgs.detach())
        real_energy = reconstruction_loss(real_imgs, real_reconstructed)
        fake_energy = reconstruction_loss(fake_imgs.detach(), fake_reconstructed)
        d_loss = real_energy - fake_energy

        optimizer_d.zero_grad()
        d_loss.backward()
        optimizer_d.step()

        # Train Generator
        fake_reconstructed = discriminator(fake_imgs)
        g_loss = reconstruction_loss(fake_imgs, fake_reconstructed)

        optimizer_g.zero_grad()
        g_loss.backward()
        optimizer_g.step()

    print(f"[Epoch {epoch}/{epochs}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]")
\end{lstlisting}

In this implementation, the discriminator is an autoencoder, and the energy is the reconstruction loss. The generator tries to produce images that minimize the reconstruction error, making them indistinguishable from real images.

\section{Adversarial Autoencoders (AAE)}
Adversarial Autoencoders (AAEs) combine autoencoders with GANs to impose a specific prior on the latent space. This makes it possible to generate samples from a structured latent space, similar to Variational Autoencoders (VAEs)~\cite{doersch2016tutorial}, but using adversarial training instead of maximum likelihood~\cite{makhzani2015adversarial}.

\subsection{Core Concept of AAE}
In an Adversarial Autoencoder, the encoder maps the input data into a latent space, and the decoder reconstructs the input from the latent representation. The key difference from a traditional autoencoder is that the latent space is regularized using a GAN. The discriminator ensures that the encoded latent vectors follow a desired distribution (e.g., a Gaussian or uniform distribution)~\cite{li2024survey}.

The adversarial training forces the encoder to map the input data to a latent space that matches the prior distribution, while the decoder reconstructs the data from the latent space.

\subsubsection{AAE Objective Function}
The AAE objective function consists of two parts:
\begin{itemize}
    \item \textbf{Reconstruction Loss:} Encourages the decoder to accurately reconstruct the input from the latent code.
    \item \textbf{Adversarial Loss:} Forces the latent space to match a predefined prior distribution.
\end{itemize}

The total loss is:

\[
\mathcal{L}_{\text{AAE}} = \mathcal{L}_{\text{reconstruction}} + \mathcal{L}_{\text{adversarial}}
\]

Where:
\begin{itemize}
    \item \( \mathcal{L}_{\text{reconstruction}} \) is the pixel-wise reconstruction loss.
    \item \( \mathcal{L}_{\text{adversarial}} \) is the adversarial loss on the latent space.
\end{itemize}

\subsection{AAE Architecture}
The architecture of AAE is similar to a traditional autoencoder, with the addition of a discriminator to enforce the latent space distribution~\cite{donahue2016adversarial}.

\textbf{Encoder:}
\begin{itemize}
    \item Maps the input image to a latent vector.
\end{itemize}

\textbf{Decoder:}
\begin{itemize}
    \item Reconstructs the image from the latent vector.
\end{itemize}

\textbf{Discriminator:}
\begin{itemize}
    \item Tries to distinguish between the latent vectors generated by the encoder and samples from the prior distribution.
\end{itemize}

\subsection{AAE Implementation in PyTorch}
Here's a simplified implementation of Adversarial Autoencoders using PyTorch:

\begin{lstlisting}[style=python]
# Encoder
class AAEEncoder(nn.Module):
    def __init__(self, latent_dim):
        super(AAEEncoder, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, latent_dim)
        )
    
    def forward(self, x):
        return self.model(x.view(x.size(0), -1))

# Decoder
class AAEDecoder(nn.Module):
    def __init__(self, latent_dim):
        super(AAEDecoder, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 28*28),
            nn.Sigmoid()
        )
    
    def forward(self, z):
        return self.model(z).view(z.size(0), 1, 28, 28)

# Discriminator for Latent Space
class AAEDiscriminator(nn.Module):
    def __init__(self, latent_dim):
        super(AAEDiscriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 1),
            nn.Sigmoid()
        )
    
    def forward(self, z):
        return self.model(z)

# Initialize models
latent_dim = 10
encoder = AAEEncoder(latent_dim)
decoder = AAEDecoder(latent_dim)
discriminator = AAEDiscriminator(latent_dim)

# Losses and optimizers
reconstruction_loss = nn.BCELoss()
adversarial_loss = nn.BCELoss()
optimizer_g = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.0002)
optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)

# Training loop (simplified)
for epoch in range(epochs):
    for i, (imgs, _) in enumerate(dataloader):
        # Encode images
        z = encoder(imgs)
        real_z = torch.randn(imgs.size(0), latent_dim)

        # Train Discriminator
        real_pred = discriminator(real_z)
        fake_pred = discriminator(z.detach())
        d_loss_real = adversarial_loss(real_pred, torch.ones_like(real_pred))
        d_loss_fake = adversarial_loss(fake_pred, torch.zeros_like(fake_pred))
        d_loss = (d_loss_real + d_loss_fake) / 2

        optimizer_d.zero_grad()
        d_loss.backward()
        optimizer_d.step()

        # Train Generator (Encoder and Decoder)
        fake_pred = discriminator(z)
        g_loss_adv = adversarial_loss(fake_pred, torch.ones_like(fake_pred))
        g_loss_recon = reconstruction_loss(decoder(z), imgs)
        g_loss = g_loss_recon + g_loss_adv

        optimizer_g.zero_grad()
        g_loss.backward()
        optimizer_g.step()

    print(f"[Epoch {epoch}/{epochs}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]")
\end{lstlisting}

In this implementation, the encoder and decoder form an autoencoder, while the discriminator regularizes the latent space by ensuring it follows a predefined distribution.

\section{Bidirectional GAN (BiGAN)}
Bidirectional GAN (BiGAN) extends the standard GAN by learning an inverse mapping from the data space back to the latent space. This enables BiGAN to both generate data from latent vectors and infer the latent vector corresponding to a given data sample, making it possible to perform tasks such as representation learning and data compression~\cite{donahue2016adversarial}.

\subsection{Core Concept of BiGAN}
In traditional GANs, only the generator maps from the latent space to the data space. BiGAN introduces an encoder network, which maps from the data space to the latent space. The encoder and generator are trained jointly in an adversarial framework, with the discriminator distinguishing between pairs of real data and real latent vectors, and pairs of generated data and fake latent vectors.

\subsection{BiGAN Objective Function}
The BiGAN loss function is:

\[
\mathcal{L}_{\text{BiGAN}} = \mathcal{L}_{\text{GAN}} + \lambda \mathcal{L}_{\text{encoder}}
\]

Where \( \mathcal{L}_{\text{encoder}} \) ensures that the encoder accurately maps data samples to their corresponding latent vectors.

\section{Autoencoder GAN (AEGAN)}
Autoencoder GAN (AEGAN) combines autoencoders and GANs to improve the quality of generated samples and ensure that the learned representations are useful for downstream tasks. AEGAN uses an autoencoder structure to generate data, and the discriminator ensures that the generated samples are indistinguishable from real data~\cite{donahue2016adversarial}.

\section{Summary}
In this chapter, we explored several advanced GAN variants, each offering unique approaches to improving GAN performance or extending their capabilities. Energy-Based GAN (EBGAN) treats the discriminator as an energy function, while Adversarial Autoencoders (AAE) impose a prior on the latent space using adversarial training~\cite{xu2019cross}. Bidirectional GAN (BiGAN) introduces an encoder to learn mappings from data to the latent space, and Autoencoder GAN (AEGAN) combines autoencoders and GANs to generate high-quality samples with useful latent representations. Each of these variants expands the potential applications of GANs and provides new tools for tasks such as image generation, representation learning, and data synthesis.
