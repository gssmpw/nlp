\part{Applications of GANs}

\chapter{Image Generation and Editing}
Generative Adversarial Networks (GANs) have gained widespread recognition for their ability to generate and edit images~\cite{goodfellow2014generative, wang2015deep, li2020gan, wang2022unrolled}. The applications of GANs in this domain range from creating high-resolution images to transforming images based on various styles or attributes. In this chapter, we will explore the techniques and methods used for image generation and editing, focusing on high-resolution image generation and artistic style transfer. Each section will provide a detailed, beginner-friendly explanation, along with examples and code snippets to guide readers through the concepts.

\section{Image Generation}
Image generation is one of the most popular applications of GANs. GANs are capable of producing highly realistic images from random noise, especially when trained on large datasets. With the advancement of GAN architectures, such as Progressive GANs (ProGAN)~\cite{he2018probgan} and StyleGAN~\cite{karras2019style}, high-resolution image generation has become a reality. In this section, we will discuss the challenges of generating high-resolution images and demonstrate how GANs can be used to overcome these challenges.

\subsection{High-Resolution Image Generation}
Generating high-resolution images using GANs poses several challenges. As the resolution increases, the complexity of the generated images also increases, making it difficult for the generator to capture fine details and for the discriminator to distinguish between real and fake images. Moreover, training GANs for high-resolution images is computationally expensive and requires stable training techniques~\cite{wang2018esrgan}.

\subsubsection{Challenges of High-Resolution Image Generation}
The main challenges of generating high-resolution images include:
\begin{itemize}
    \item \textbf{Mode Collapse:} The generator might focus on generating a limited variety of images, leading to poor diversity in the generated samples.
    \item \textbf{Training Instability:} As the resolution increases, GAN training can become unstable, with the generator and discriminator oscillating rather than converging.
    \item \textbf{Memory and Computational Requirements:} High-resolution images require more memory and computational resources, making it difficult to train models on standard hardware.
\end{itemize}

To address these challenges, advanced techniques such as progressive growing and multi-scale training have been introduced. One of the most notable architectures for high-resolution image generation is \textbf{Progressive GAN (ProGAN)}~\cite{he2018probgan}.

\subsubsection{Progressive Growing of GANs (ProGAN)}
ProGAN, introduced by Karras et al., is an architecture designed specifically to handle high-resolution image generation. The key idea behind ProGAN is to train the GAN progressively, starting from a low-resolution image and gradually increasing the resolution by adding new layers to both the generator and discriminator.

\textbf{Key Features of ProGAN:}
\begin{itemize}
    \item \textbf{Progressive Layer Addition:} The model starts by generating low-resolution images (e.g., 4x4 pixels) and progressively adds layers to generate higher-resolution images (e.g., 1024x1024 pixels).
    \item \textbf{Fade-in Transition:} When new layers are added, their contribution is gradually increased using a fade-in transition. This smooth transition prevents the model from destabilizing as the resolution increases~\cite{he2018probgan}.
\end{itemize}

\textbf{Example of ProGAN in PyTorch:}
Here is a simplified implementation of Progressive GAN using PyTorch. The generator starts by generating low-resolution images and gradually increases the resolution by adding layers.

\begin{lstlisting}[style=python]
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Simple ProGAN-like Generator
class ProGANGenerator(nn.Module):
    def __init__(self, latent_dim, start_res):
        super(ProGANGenerator, self).__init__()
        self.start_res = start_res  # Starting resolution (e.g., 4x4)
        self.latent_dim = latent_dim
        self.model = nn.ModuleList([self.initial_block(latent_dim, start_res)])

    def initial_block(self, latent_dim, res):
        return nn.Sequential(
            nn.Linear(latent_dim, 128 * res * res),
            nn.ReLU(),
            nn.Unflatten(1, (128, res, res))
        )

    def add_layer(self, in_channels, out_channels):
        block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2)
        )
        self.model.append(block)

    def forward(self, z):
        x = self.model[0](z)
        for block in self.model[1:]:
            x = block(x)
        return torch.tanh(x)

# Initialize generator
latent_dim = 100
start_res = 4
generator = ProGANGenerator(latent_dim, start_res)

# Example of adding layers to increase resolution progressively
generator.add_layer(128, 64)  # 8x8 resolution
generator.add_layer(64, 32)   # 16x16 resolution
generator.add_layer(32, 3)    # Final layer to output 3-channel image
\end{lstlisting}

In this code, the generator starts by generating a low-resolution 4x4 image and progressively adds layers to increase the resolution. Each layer doubles the resolution, allowing the generator to handle higher complexity step by step.

\subsubsection{Training Strategies for High-Resolution Image Generation}
To train GANs for high-resolution image generation, several strategies are commonly employed:
\begin{itemize}
    \item \textbf{Multi-Scale Training:} The generator is trained to produce images at multiple resolutions, starting from low resolution and progressively increasing it. This allows the generator to capture global structure before focusing on finer details~\cite{wang2018cgan}.
    \item \textbf{Batch Normalization and Instance Normalization:} These normalization techniques help stabilize GAN training by ensuring that the generator and discriminator operate on well-behaved data distributions.
    \item \textbf{Noise Injection:} Adding noise at various stages of the generator can help the model generalize better and avoid overfitting to the training data.
\end{itemize}

\subsection{Artistic Style Transfer}
Artistic style transfer refers to the process of transforming the style of one image (e.g., a photograph) into the artistic style of another image (e.g., a painting). GANs have proven to be highly effective for this task, allowing for the seamless transfer of artistic styles between images~\cite{wang2020attentive}. 

\subsubsection{What is Style Transfer?}
Style transfer aims to separate the content and style of an image~\cite{jing2019neural}. The content refers to the objects and structure in the image, while the style refers to the texture, colors, and artistic features. The goal of style transfer is to apply the style of one image to the content of another image.

\textbf{Example:} 
\begin{itemize}
    \item Content Image: A photograph of a landscape.
    \item Style Image: A painting by Van Gogh.
    \item Result: A photograph of the landscape in the style of Van Gogh's painting.
\end{itemize}

\subsubsection{CycleGAN for Unsupervised Style Transfer}
CycleGAN~\cite{chu2017cyclegan} is one of the most popular GAN architectures for unsupervised image translation, including artistic style transfer. CycleGAN does not require paired images from two domains. Instead, it learns to map images from one domain (e.g., photographs) to another domain (e.g., paintings) without needing paired examples.

CycleGAN consists of two generators and two discriminators:
\begin{itemize}
    \item \( G: A \to B \) - Translates images from domain \( A \) (e.g., photographs) to domain \( B \) (e.g., paintings).
    \item \( F: B \to A \) - Translates images from domain \( B \) to domain \( A \).
    \item \( D_A \) - Discriminator for domain \( A \), ensuring that translated images from \( F \) are realistic.
    \item \( D_B \) - Discriminator for domain \( B \), ensuring that translated images from \( G \) are realistic.
\end{itemize}

\subsubsection{Cycle Consistency Loss in Style Transfer}
To ensure that the style transfer does not lose important content information, CycleGAN introduces the concept of cycle consistency. This means that if we translate an image from domain \( A \) to domain \( B \) and then back to domain \( A \), the result should closely resemble the original image~\cite{chu2017cyclegan}.

\[
\mathcal{L}_{\text{cycle}}(G, F) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\| F(G(x)) - x \|_1] + \mathbb{E}_{y \sim p_{\text{data}}(y)}[\| G(F(y)) - y \|_1]
\]

\subsubsection{CycleGAN Implementation for Style Transfer}
Here's a basic CycleGAN implementation using PyTorch for unsupervised style transfer between photographs and paintings.

\begin{lstlisting}[style=python]
class ResnetBlock(nn.Module):
    def __init__(self, dim):
        super(ResnetBlock, self).__init__()
        self.conv_block = nn.Sequential(
            nn.Conv2d(dim, dim, kernel_size=3, padding=1),
            nn.InstanceNorm2d(dim),
            nn.ReLU(True),
            nn.Conv2d(dim, dim, kernel_size=3, padding=1),
            nn.InstanceNorm2d(dim)
        )

    def forward(self, x):
        return x + self.conv_block(x)

class CycleGANGenerator(nn.Module):
    def __init__(self, in_channels, out_channels, num_resnet_blocks=6):
        super(CycleGANGenerator, self).__init__()
        model = [
            nn.Conv2d(in_channels, 64, kernel_size=7, padding=3),
            nn.InstanceNorm2d(64),
            nn.ReLU(True),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.InstanceNorm2d(128),
            nn.ReLU(True),
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.InstanceNorm2d(256),
            nn.ReLU(True)
        ]
        for _ in range(num_resnet_blocks):
            model += [ResnetBlock(256)]
        model += [
            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(64),
            nn.ReLU(True),
            nn.Conv2d(64, out_channels, kernel_size=7, padding=3),
            nn.Tanh()
        ]
        self.model = nn.Sequential(*model)

    def forward(self, x):
        return self.model(x)

# Initialize models
G_A2B = CycleGANGenerator(in_channels=3, out_channels=3)
G_B2A = CycleGANGenerator(in_channels=3, out_channels=3)

# Example of using the generator to apply style transfer
content_image = torch.randn(1, 3, 256, 256)  # Example content image
style_transferred_image = G_A2B(content_image)  # Style transferred image
\end{lstlisting}

In this implementation, the generator consists of convolutional layers and residual blocks, which are effective for learning artistic style mappings between domains.

\section{Summary}
In this chapter, we explored two important applications of GANs: high-resolution image generation and artistic style transfer. GANs such as ProGAN have been specifically designed to handle the challenges of high-resolution image generation by progressively increasing the resolution during training. We also covered CycleGAN, a powerful architecture for unsupervised image translation, which has been successfully applied to tasks like artistic style transfer. Through detailed explanations and code examples, we provided a comprehensive guide for beginners to understand how GANs can be used for various image generation and editing tasks.










\section{Image Editing}
Generative Adversarial Networks (GANs) have been widely applied in the field of image editing, where they enable the manipulation and generation of high-quality, realistic images. Image editing tasks include face generation and editing, image inpainting (repairing damaged images), and denoising (removing noise from images). This section explores how GANs are used in these image editing tasks, detailing the underlying techniques and providing examples.

\subsection{Face Generation and Editing}
Face generation and editing are popular applications of GANs, where the goal is to generate new facial images or edit existing ones in a controlled way. GANs, particularly architectures like StyleGAN~\cite{karras2019style}, have shown incredible results in generating highly realistic faces, allowing users to manipulate various facial attributes, such as age, hair color, expression, and more~\cite{patashnik2021styleclip}.

\textbf{1. Latent Space Interpolation:}  
In GANs, particularly StyleGAN, images are generated by sampling from a latent space, which encodes different attributes of the image. By manipulating vectors in this latent space, we can generate new faces or modify specific attributes of existing faces. For example, moving in a certain direction in the latent space might change the age of a person, while moving in another direction might change their hairstyle.

\textbf{2. Attribute Editing:}  
GANs can be used to edit specific attributes of an image by conditioning the generation process on certain attributes. This can be done by training the Generator to learn how to map latent vectors and specific attributes (e.g., age, gender) to facial images. By modifying these attribute values, we can control how the generated image changes~\cite{abdal2019image2stylegan}.

\textbf{Example: Face Generation with Latent Space Manipulation in PyTorch}

\begin{lstlisting}[style=python]
import torch
import torch.nn as nn

# Simple GAN Generator for face generation
class FaceGenerator(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(FaceGenerator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, output_dim),
            nn.Tanh()  # Output scaled between -1 and 1 (for image pixel values)
        )

    def forward(self, x):
        return self.model(x)

# Example usage:
latent_vector = torch.randn(1, 100)  # Latent vector representing face attributes
face_generator = FaceGenerator(100, 3 * 64 * 64)  # Assuming output is 64x64 RGB image
generated_face = face_generator(latent_vector)
print(generated_face.shape)  # Output should be torch.Size([1, 12288]), which is 64x64x3
\end{lstlisting}

In this simple example, the Generator takes a latent vector as input and produces an image of a face. By modifying the latent vector, we can change different facial attributes like expression or hairstyle.

\subsection{Image Inpainting and Denoising}
Image inpainting (image completion) and denoising are important tasks in the field of image restoration. Inpainting refers to the process of filling in missing or damaged parts of an image, while denoising refers to removing noise from a corrupted image. GANs are highly effective at these tasks because they can learn to generate realistic details that match the surrounding context.

\textbf{1. Image Inpainting:}  
Inpainting with GANs involves generating missing pixels in an image by conditioning the generation process on the known surrounding pixels. The Generator learns to fill in the missing areas with realistic content that matches the rest of the image. GANs are particularly useful here because they can generate semantically consistent content, ensuring that the inpainted region fits naturally with the surrounding pixels~\cite{pang2021image}.

\textbf{2. Image Denoising:}  
GANs can also be applied to image denoising by learning to map noisy images to clean, denoised versions. The Generator is trained to remove noise while preserving important image details. The Discriminator ensures that the generated image looks as realistic as possible by distinguishing between real clean images and denoised images.

\textbf{Example: Image Inpainting with GANs in PyTorch}

\begin{lstlisting}[style=python]
class InpaintingGenerator(nn.Module):
    def __init__(self):
        super(InpaintingGenerator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),
            nn.Tanh()  # Output scaled between -1 and 1
        )

    def forward(self, x):
        return self.model(x)

# Example usage:
damaged_image = torch.randn(1, 3, 64, 64)  # Simulated damaged image
inpainting_generator = InpaintingGenerator()
reconstructed_image = inpainting_generator(damaged_image)
print(reconstructed_image.shape)  # Output should be torch.Size([1, 3, 64, 64])
\end{lstlisting}

In this example, the Generator takes a partially damaged image as input and outputs the inpainted image. The inpainted areas are generated to match the known areas of the image, producing a seamless and realistic result.

\section{Image Translation and Style Transfer}
Image translation and style transfer are tasks in which one image is transformed into another while maintaining some of its key properties. GANs, particularly models like CycleGAN~\cite{chu2017cyclegan}, are well-suited for these tasks as they can learn complex mappings between different image domains, such as transforming a photograph into a painting or converting images between different styles~\cite{karras2019style}.

\subsection{Supervised and Unsupervised Image Translation}
Image translation refers to the process of converting an image from one domain to another. For instance, translating an image of a horse into an image of a zebra, or turning a day-time scene into a night-time scene. This can be done in both supervised and unsupervised ways:

\textbf{1. Supervised Image Translation:}  
In supervised image translation, we have paired examples of images from the source and target domains. For instance, we may have pairs of day-time and night-time images of the same scene. The GAN is trained to map the source image to the target image by learning from these paired examples~\cite{abdal2019image2stylegan}.

\textbf{2. Unsupervised Image Translation:}  
In many cases, paired examples are not available. Unsupervised image translation methods like CycleGAN allow the model to learn mappings between domains without paired examples. CycleGAN uses a cycle consistency loss to ensure that when an image is translated from one domain to another and back, it returns to the original image.

\textbf{Example: CycleGAN for Unsupervised Image Translation in PyTorch}

\begin{lstlisting}[style=python]
class CycleGAN_Generator(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(CycleGAN_Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)

# Example usage:
image_A = torch.randn(1, 3, 64, 64)  # Image from domain A (e.g., horses)
cyclegan_generator = CycleGAN_Generator(3, 3)
image_B = cyclegan_generator(image_A)  # Translate to domain B (e.g., zebras)
print(image_B.shape)  # Output should be torch.Size([1, 3, 64, 64])
\end{lstlisting}

In this example, a simple CycleGAN Generator is used to translate an image from one domain to another. The same Generator can be used to translate the image back to the original domain using the cycle consistency loss.

\subsection{Cross-Domain Style Transfer}
Style transfer refers to the task of transferring the style of one image onto the content of another. For example, transforming a photograph into a painting by a famous artist. GANs can be used for cross-domain style transfer, where the Generator is trained to map the content of one image into the style of another domain, such as mapping real-world images into artistic styles or transferring the textures of one object to another~\cite{chu2017cyclegan}.

\textbf{1. Neural Style Transfer with GANs:}  
GANs are powerful for performing neural style transfer because they can generate high-quality, stylized images while preserving the underlying content of the original image. By training the Generator to apply the style of a target domain, we can create visually appealing results where the content remains unchanged but the style is dramatically altered.

\textbf{2. Multi-Domain Style Transfer:}  
In multi-domain style transfer, the Generator is trained to transfer images across multiple styles (e.g., turning a photograph into different painting styles). This is done by conditioning the Generator on the target style, allowing it to generate images in a variety of different styles from a single model.

\textbf{Example: Style Transfer with GANs in PyTorch}

\begin{lstlisting}[style=python]
class StyleTransferGenerator(nn.Module):
    def __init__(self, in_channels, out_channels, num_styles):
        super(StyleTransferGenerator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )
        self.style_embedding = nn.Embedding(num_styles, 128)  # Embedding for different styles

    def forward(self, x, style_idx):
        style = self.style_embedding(style_idx).view(x.size(0), -1, 1, 1)
        x = self.model(x)
        return x * style  # Apply style modulation

# Example usage:
image = torch.randn(1, 3, 64, 64)  # Content image
style_idx = torch.tensor([2])  # Target style index
style_transfer_generator = StyleTransferGenerator(3, 3, num_styles=5)
styled_image = style_transfer_generator(image, style_idx)
print(styled_image.shape)  # Output should be torch.Size([1, 3, 64, 64])
\end{lstlisting}

In this example, a style transfer Generator is implemented, where the style is modulated by an embedding for different styles. The image content remains the same, but the style can be changed by selecting different style indices.

\textbf{Visualizing Cross-Domain Style Transfer~\cite{xu2019cross}:}

\begin{center}
\begin{tikzpicture}
  [scale=1, every node/.style={scale=1}, 
  block/.style={rectangle, draw, fill=blue!20, text centered, minimum height=3em},
  arrow/.style={->, thick}]

  % Nodes
  \node[block] (content) {Content Image};
  \node[block, right=of content] (gen) {Style Transfer Generator};
  \node[block, right=of gen] (output) {Stylized Image};
  \node[block, below=of gen] (style) {Style Embedding};

  % Arrows
  \draw[arrow] (content) -- (gen);
  \draw[arrow] (gen) -- (output);
  \draw[arrow] (style) -- (gen);
\end{tikzpicture}
\end{center}

In this diagram, the content image is passed through the Style Transfer Generator, which is conditioned on the target style embedding. The output is a stylized version of the content image, reflecting the selected style.
