\chapter{Theoretical Foundations of GANs}
In this chapter, we will explore the fundamental theoretical concepts that form the backbone of Generative Adversarial Networks (GANs). The understanding of GANs requires a firm grasp of probability theory~\cite{he2018probgan}, statistics~\cite{bau2019seeing, ma2021must}, and game theory~\cite{fudenberg1991game}. Additionally, we will examine the concept of Nash Equilibrium~\cite{daskalakis2009complexity, heusel2017gans, farnia2020gans} in the context of GANs, as it plays a critical role in the convergence of the model during training.

\section{Fundamentals of Probability Theory and Statistics}
Probability theory~\cite{he2018probgan} and statistics provide the mathematical foundation for modeling uncertainty in machine learning. In the context of GANs, these concepts help us define the distributions from which data is sampled, as well as how to measure the likelihood of certain outcomes~\cite{farnia2020gans}.

\subsection{Random Variables and Distributions}
In GANs, the generator typically learns to map random noise, sampled from a specific distribution, to a distribution that mimics real data~\cite{goodfellow2014generative}. A random variable is a quantity that can take on different values, each with a specific probability. For example, a simple random variable \( Z \) could represent a Gaussian noise vector:

\[
Z \sim \mathcal{N}(0, 1)
\]

This means that \( Z \) is sampled from a normal distribution with mean $0$ and variance $1$~\cite{goodfellow2014generative, kazeminia2020gans}. In practice, the generator in a GAN takes such noise as input and transforms it into data that approximates the real distribution \( p_{\text{data}} \).

\subsection{Expectation and Variance}
Two important concepts in probability theory~\cite{he2018probgan} are expectation and variance, which help quantify the behavior of random variables:

\begin{itemize}
    \item \textbf{Expectation:} The expected value (or mean) of a random variable represents the average outcome of a large number of samples. For a random variable \( X \), the expectation is denoted as \( \mathbb{E}[X] \).
    \item \textbf{Variance:} The variance measures the spread of a random variable's values around the mean. It is defined as \( \mathbb{V}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] \).
\end{itemize}

These concepts are useful in GANs when analyzing the output of the generator and the real data (Fig. \ref{fig:gan_basic_architecture}). The generator attempts to produce samples that have similar statistical properties (such as mean and variance) to the real data~\cite{srivastava2017veegan, he2018probgan}.

\subsection{Probability Density Functions (PDF)}
The probability density function (PDF) describes the likelihood of a continuous random variable taking on a specific value~\cite{parzen1962estimation, saatci2017bayesian}. For example, the PDF of a Gaussian distribution is given by:

\[
p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)
\]

In GANs, we often want the generated samples to follow a specific probability distribution~\cite{goodfellow2014generative}, such as a Gaussian~\cite{goodman1963statistical} or uniform~\cite{kuipers2012uniform} distribution, and we measure how closely the generated samples approximate the real data distribution using statistical metrics like the Jensen-Shannon divergence~\cite{menendez1997jensen, fuglede2004jensen}.

\section{Game Theory and Optimal Equilibria}
Game theory is a critical component of understanding the adversarial nature of GANs~\cite{fudenberg1991game, farnia2020gans}. GANs can be viewed as a game between two players: the generator and the discriminator~\cite{goodfellow2014generative}. To fully comprehend the dynamics of this interaction, we need to understand key concepts in game theory, particularly the notion of equilibrium.

\subsection{Basic Concepts of Game Theory}
In game theory, players make decisions that influence each other's outcomes. In GANs, the generator and the discriminator can be considered as two players engaged in a zero-sum game~\cite{friedman1998work}, where one player's gain is the other player's loss.

\begin{itemize}
    \item \textbf{Players:} In GANs, the two players are the generator (G) and the discriminator (D)~\cite{goodfellow2014generative, kazeminia2020gans}.
    \item \textbf{Strategies:} The generator's strategy is to create data that can fool the discriminator, while the discriminator's strategy is to correctly classify real versus fake data.
    \item \textbf{Payoffs:} The payoff for the generator is based on how well it can fool the discriminator. The payoff for the discriminator is based on how accurately it can classify the data~\cite{goodfellow2014generative}.
\end{itemize}

The goal of this game is to find an equilibrium where neither player can improve their strategy without the other player's strategy changing~\cite{fudenberg1991game}.

\subsection{Zero-Sum Games}
A GAN can be viewed as a zero-sum game. In such games, the total gain of all players is zero~\cite{friedman1998work}. In other words, the gain of one player is exactly offset by the loss of the other. The objective of the discriminator is to minimize the following loss function:

\[
\min_D \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log (1 - D(G(z)))]
\]

At the same time, the generator tries to maximize the discriminator's loss:

\[
\max_G \mathbb{E}_{z \sim p_z(z)}[\log D(G(z))]
\]

This adversarial process creates a dynamic where both networks are constantly improving in response to each other~\cite{farnia2020gans}.

\section{Nash Equilibrium in GANs}
The concept of \textbf{Nash Equilibrium} is central to understanding the training dynamics of GANs~\cite{farnia2020gans}. A Nash Equilibrium occurs in a game when no player can improve their strategy unilaterally, assuming that the other player's strategy remains fixed. In the context of GANs, Nash Equilibrium represents the ideal state where the generator has learned to generate data that is indistinguishable from the real data, and the discriminator is no longer able to tell the difference between real and fake data~\cite{farnia2020gans}.

\subsection{Formal Definition of Nash Equilibrium}
For a two-player game, a Nash Equilibrium occurs when both players adopt strategies such that neither player can improve their outcome by changing their strategy unilaterally. In GANs, this means that:

\begin{itemize}
    \item The discriminator is optimized to correctly classify real and fake data, given the generator's current strategy~\cite{ho2019real}.
    \item The generator is optimized to produce realistic data, given the discriminator's current strategy.
\end{itemize}

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Player 1 (Discriminator)} & \textbf{Stick with Strategy} & \textbf{Change Strategy} \\
        \hline
        \textbf{Player 2 (Generator)} &  &  \\
        \textbf{Stick with Strategy} & \textbf{(Nash Equilibrium)} & Player 1 improves \\
        \hline
        \textbf{Change Strategy} & Player 2 improves & Both players adjust \\
        \hline
    \end{tabular}
    \caption{Nash Equilibrium for a Two-Player Game.}
\end{table}

At Nash Equilibrium, the discriminator's performance is no better than random guessing, and the generator has effectively learned to mimic the real data distribution~\cite{farnia2020gans}.

\subsection{Challenges in Reaching Nash Equilibrium in GANs}
While the concept of Nash Equilibrium is theoretically appealing, in practice, reaching equilibrium in GANs can be challenging. Some common issues include:

\begin{itemize}
    \item \textbf{Mode collapse:} The generator might produce a limited variety of samples, focusing on a few modes of the real data distribution, causing the model to collapse to a narrow range of outputs~\cite{he2018probgan}.
    \item \textbf{Non-convergence:} GAN training can be unstable, with the generator and discriminator failing to reach a steady state.
    \item \textbf{Vanishing gradients:} The discriminator may become too strong, making it difficult for the generator to learn effectively due to diminishing gradient updates~\cite{farnia2020gans}.
\end{itemize}

\subsection{Example of Nash Equilibrium in GANs}
Let's consider an example in which the generator and discriminator reach Nash Equilibrium~\cite{kazeminia2020gans}. In a simplified scenario, suppose we are generating a 1D Gaussian distribution with mean \( \mu = 4 \) and standard deviation \( \sigma = 1.25 \).

Initially, the generator might produce random samples that look nothing like the real data. The discriminator will easily classify these as fake~\cite{goodfellow2014generative, farnia2020gans}. Over time, the generator improves by producing samples closer to the real distribution, and the discriminator becomes less certain in its classifications.

Once Nash Equilibrium is reached, the generator produces samples that closely match the real distribution, and the discriminator's accuracy drops to 50\%, which is no better than random guessing~\cite{farnia2020gans}.

\subsection{Training GANs to Approach Nash Equilibrium}
In practical GAN training, we aim to iteratively update the generator and discriminator in a way that moves them toward Nash Equilibrium. This process can be seen in the alternating optimization steps of GAN training.

Here's an illustrative training loop in Python using PyTorch:

\begin{lstlisting}[style=python]
import torch
import torch.nn as nn
import torch.optim as optim

# Define the Generator and Discriminator models (simplified for illustration)
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(100, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.net(x)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(1, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

# Initialize models
generator = Generator()
discriminator = Discriminator()

# Optimizers
optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)
optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)

# Loss function
loss_function = nn.BCELoss()

# Training loop (simplified)
for epoch in range(10000):
    # Generate fake data
    noise = torch.randn(32, 100)
    fake_data = generator(noise)

    # Train Discriminator
    real_data = torch.randn(32, 1) * 1.25 + 4
    real_labels = torch.ones(32, 1)
    fake_labels = torch.zeros(32, 1)

    d_loss_real = loss_function(discriminator(real_data), real_labels)
    d_loss_fake = loss_function(discriminator(fake_data.detach()), fake_labels)
    d_loss = d_loss_real + d_loss_fake

    optimizer_d.zero_grad()
    d_loss.backward()
    optimizer_d.step()

    # Train Generator
    g_loss = loss_function(discriminator(fake_data), real_labels)

    optimizer_g.zero_grad()
    g_loss.backward()
    optimizer_g.step()

    if epoch % 1000 == 0:
        print(f'Epoch [{epoch}/10000], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')
\end{lstlisting}

In this example, the generator and discriminator are trained in alternating steps, with the goal of approaching Nash Equilibrium. The generator improves its ability to produce realistic samples, while the discriminator becomes increasingly uncertain about classifying them as real or fake~\cite{farnia2020gans}.

\section{Summary}
In this chapter, we explored the theoretical foundations of GANs, focusing on probability theory, statistics, and game theory. Understanding these concepts is crucial for grasping how GANs function. We also delved into the concept of Nash Equilibrium and how it relates to GAN training. Although reaching equilibrium in practice can be difficult, it serves as the theoretical goal of GAN training, where both the generator and discriminator reach a balanced state.










\section{Learning Distributions and Generative Models}
In the field of machine learning, one of the key challenges is learning the underlying distribution of a dataset~\cite{quinonero2022dataset}. Generative models, such as GANs, aim to capture this distribution so that they can generate new data points that are similar to the real data~\cite{goodfellow2014generative}. This section explores the concept of learning distributions, particularly in the context of real and generated data, and how GANs approximate the true data distribution.

\subsection{Real Data Distribution vs Generated Data Distribution}
The goal of any generative model is to learn the \textit{real data distribution}, denoted as \( p_{data}(x) \). This distribution describes the likelihood of observing different data points in a given dataset. For example, if we have a dataset of images of handwritten digits~\cite{lecun1998gradient}, \( p_{data}(x) \) represents the probability of seeing different digit images in the dataset.

On the other hand, a generative model, such as the Generator in a GAN, produces a \textit{generated data distribution}, denoted as \( p_{g}(x) \). Initially, this distribution is random because the Generator has no information about the real data. However, as the Generator trains, it updates its parameters to produce data that increasingly resembles the real data~\cite{goodfellow2014generative}.

\subsubsection{Example: Real and Generated Data}
Consider an example where we are working with a dataset of images of cats and dogs. The \textit{real data distribution} \( p_{data}(x) \) captures how likely we are to see a given image of a cat or a dog. For example, it may be more likely to see an image of a cat lying down than a dog standing up.

Initially, the Generator produces images randomly from a \textit{generated data distribution} \( p_g(x) \). These generated images may look like blobs of noise, as the Generator hasn't learned the features of cats or dogs yet. Over time, through training, the Generator's distribution \( p_g(x) \) will start to resemble the real distribution \( p_{data}(x) \), producing images that look increasingly like real cats or dogs~\cite{goodfellow2014generative, heusel2017gans, he2018probgan, farnia2020gans}.

To formalize this, the Generator in a GAN learns a mapping from a simple distribution (e.g., a Gaussian distribution) to the complex real data distribution. Let's visualize this in a simplified flowchart using \texttt{tikzpicture}:

\begin{center}
\begin{tikzpicture}
  [scale=1, every node/.style={scale=1}, 
  block/.style={rectangle, draw, fill=blue!20, text centered, minimum height=3em},
  arrow/.style={->, thick}]

  % Nodes
  \node[block] (noise) {Simple Distribution $p_z(z)$};
  \node[block, right=of noise] (gen) {Generator $G(z)$};
  \node[block, right=of gen] (fake) {Generated Data Distribution $p_g(x)$};
  \node[block, below=of fake] (real) {Real Data Distribution $p_{data}(x)$};

  % Arrows
  \draw[arrow] (noise) -- (gen);
  \draw[arrow] (gen) -- (fake);
  \draw[dashed, arrow] (fake) -- (real) node[midway, right] {Approximates};
\end{tikzpicture}
\end{center}

In this diagram, the Generator takes noise sampled from a simple distribution, \( p_z(z) \), and maps it to the generated data distribution, \( p_g(x) \). Over time, the goal of the Generator is for \( p_g(x) \) to approximate the real data distribution \( p_{data}(x) \), so the generated data becomes indistinguishable from real data.

\subsection{GAN's Ability to Approximate Data Distributions}
The key strength of GANs lies in their ability to approximate complex data distributions. GANs achieve this by training two neural networks-the Generator and the Discriminator-in an adversarial process~\cite{he2018probgan, farnia2020gans}. The Generator learns to produce data that mimics the real distribution, while the Discriminator learns to distinguish between real and generated data.

\subsubsection{How GANs Learn to Approximate Distributions}
GANs work through the following iterative process~\cite{goodfellow2014generative, he2018probgan, kazeminia2020gans}:

\begin{enumerate}
    \item The Generator takes random noise as input and generates synthetic data.
    \item The Discriminator is presented with both real data (from the dataset) and the generated data. It predicts whether each data sample is real or fake.
    \item The Generator is trained to produce data that maximizes the Discriminator's error, i.e., it tries to generate data that the Discriminator classifies as real.
    \item The Discriminator is trained to minimize its error, i.e., it tries to accurately classify real and fake data.
\end{enumerate}

This adversarial process drives the Generator to improve its approximation of the real data distribution~\cite{aggarwal2021generative}. Over time, the generated data becomes more similar to the real data, and the generated distribution \( p_g(x) \) approaches the real distribution \( p_{data}(x) \).

\textbf{Example of GAN training in PyTorch:}

Here's a step-by-step example of how GANs learn to approximate the data distribution:

\begin{lstlisting}[style=python]
import torch
import torch.nn as nn

# Generator class
class Generator(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim),
            nn.Tanh()  # For generating image data normalized between -1 and 1
        )

    def forward(self, x):
        return self.model(x)

# Discriminator class
class Discriminator(nn.Module):
    def __init__(self, input_dim):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()  # Outputs probability
        )

    def forward(self, x):
        return self.model(x)

# GAN Training
def train_gan(generator, discriminator, epochs, batch_size, input_dim, data_dim):
    optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.0002)
    optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=0.0002)
    criterion = nn.BCELoss()  # Binary Cross Entropy Loss
    
    for epoch in range(epochs):
        # Training Discriminator
        real_data = torch.randn(batch_size, data_dim)  # Example real data
        real_labels = torch.ones(batch_size, 1)  # Real labels
        
        noise = torch.randn(batch_size, input_dim)  # Random noise for Generator
        fake_data = generator(noise)
        fake_labels = torch.zeros(batch_size, 1)  # Fake labels
        
        # Train on real data
        optimizer_disc.zero_grad()
        output_real = discriminator(real_data)
        loss_real = criterion(output_real, real_labels)
        
        # Train on fake data
        output_fake = discriminator(fake_data.detach())
        loss_fake = criterion(output_fake, fake_labels)
        
        loss_disc = loss_real + loss_fake
        loss_disc.backward()
        optimizer_disc.step()
        
        # Training Generator
        optimizer_gen.zero_grad()
        output_fake_for_gen = discriminator(fake_data)
        loss_gen = criterion(output_fake_for_gen, real_labels)  # Try to fool the discriminator
        
        loss_gen.backward()
        optimizer_gen.step()

        if epoch % 1000 == 0:
            print(f"Epoch [{epoch}/{epochs}], Loss D: {loss_disc.item()}, Loss G: {loss_gen.item()}")
            
# Hyperparameters
input_dim = 100
data_dim = 784  # For 28x28 images (e.g., MNIST)
epochs = 10000
batch_size = 64

# Create instances of Generator and Discriminator
gen = Generator(input_dim, data_dim)
disc = Discriminator(data_dim)

# Train the GAN
train_gan(gen, disc, epochs, batch_size, input_dim, data_dim)
\end{lstlisting}

In this example, the Generator takes random noise and gradually learns to map it to data that approximates the real data distribution. The Discriminator tries to identify whether the data is real or generated, and the Generator is updated to fool the Discriminator over time~\cite{kingma2019introduction}.

\subsubsection{Convergence of GANs}
The ideal outcome of this adversarial training process is that the generated data becomes indistinguishable from real data. Mathematically, this means the generated distribution \( p_g(x) \) converges to the real data distribution \( p_{data}(x) \). At this point, the Discriminator cannot tell the difference between real and fake data, and the Generator has successfully learned the true data distribution.

In practice, however, achieving perfect convergence can be difficult due to factors like unstable training, mode collapse (where the Generator produces only a limited variety of samples), and the sensitivity of GANs to hyperparameters~\cite{farnia2020gans}. Researchers continue to develop techniques to address these challenges and improve the performance of GANs~\cite{heusel2017gans, kazeminia2020gans}.

\subsection{Visualizing Distribution Convergence}
Below is a conceptual diagram of how the generated data distribution \( p_g(x) \) approaches the real data distribution \( p_{data}(x) \) during GAN training:

\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}
  [
    every node/.style={font=\small}, 
    block/.style={
      rectangle, 
      draw, 
      fill=blue!20, 
      text centered, 
      minimum height=3em,
      text width=5cm,  
      align=center  
    },
    arrow/.style={->, thick},
    dashed arrow/.style={->, thick, dashed}
  ]
  
 
  \node[block] (p_real) {Real Data Distribution \\ $p_{\text{data}}(x)$};
  \node[block, right=6cm of p_real] (p_fake1) {Initial Generated Distribution \\ $p_g(x)$};
  \node[block, right=6cm of p_fake1] (p_fake2) {Converged Generated Distribution \\ $p_g(x)$};
  
 
  \draw[arrow] (p_fake1) -- (p_real) node[midway, above] {Approaches with Training};
  
 
  \draw[dashed arrow, bend right=20] (p_fake2) to node[midway, below] {At Convergence} (p_real);
\end{tikzpicture}%
}
\end{center}

Initially, the generated data distribution \( p_g(x) \) is far from the real distribution, but over time, it converges closer to \( p_{data}(x) \), allowing the Generator to produce highly realistic samples.










\section{Mathematical Properties of GANs}
Understanding the mathematical properties of GANs is essential for training effective models and addressing the challenges that arise during the learning process. In this section, we will explore the convergence behavior of GANs and the effects of different loss functions in their training~\cite{goodfellow2014generative, he2018probgan, farnia2020gans}.

\subsection{Convergence of GANs}
GAN convergence refers to the point at which the generator and discriminator reach an equilibrium during training~\cite{goodfellow2014generative}. Ideally, at convergence, the generator has learned to produce data that is indistinguishable from real data, and the discriminator cannot reliably distinguish between the two.

\subsubsection{Minimax Game and Nash Equilibrium}
GANs are formulated as a minimax game between the generator \(G\) and the discriminator \(D\). The objective of the generator is to minimize the probability of the discriminator correctly classifying real and fake data, while the discriminator aims to maximize this probability~\cite{farnia2020gans}. Mathematically, this can be expressed as:
\[
\min_G \max_D \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log (1 - D(G(z)))]
\]

At convergence, the generator and discriminator should reach a Nash equilibrium, where neither can improve their performance by unilaterally changing their strategy~\cite{fudenberg1991game, farnia2020gans}. In practical terms, this means the discriminator assigns equal probabilities to real and fake data, i.e., \(D(x) = 0.5\) for both real and generated data.

\subsubsection{Challenges in Achieving Convergence}
Achieving convergence in GANs is notoriously difficult due to the dynamic nature of the minimax game. The generator and discriminator continuously adapt to each other, which can result in instability and oscillations instead of convergence. Some common issues include~\cite{heusel2017gans, farnia2020gans}:
\begin{itemize}
    \item \textbf{Non-stationarity}: As the generator and discriminator improve, the optimization landscape changes dynamically, making it difficult to find a stable point.
    \item \textbf{Mode collapse}: The generator may find a shortcut solution by producing limited types of data, which leads to poor generalization.
    \item \textbf{Vanishing gradients}: If the discriminator becomes too powerful, it provides very small gradient updates to the generator, slowing down the learning process.
\end{itemize}

One method to encourage convergence is to maintain a balance between the generator and discriminator. This can be achieved by carefully tuning the learning rates of both networks, adjusting their architectures, and using techniques like \textit{Wasserstein loss}~\cite{frogner2015learning}, which we'll explore later.

\begin{lstlisting}[style=python]
# Example: Tuning learning rates to improve convergence
import torch
import torch.nn as nn
import torch.optim as optim

# Define simple Discriminator and Generator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(784, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.main(x)

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(100, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 784),
            nn.Tanh()
        )

    def forward(self, x):
        return self.main(x)

# Instantiate models
D = Discriminator()
G = Generator()

# Binary Cross-Entropy loss
criterion = nn.BCELoss()

# Optimizers with different learning rates for better balance
optimizer_D = optim.Adam(D.parameters(), lr=0.0004)  # Faster learning for Discriminator
optimizer_G = optim.Adam(G.parameters(), lr=0.0001)  # Slower learning for Generator

# Labels
real_label = torch.ones(64, 1)
fake_label = torch.zeros(64, 1)

# Training example loop
for epoch in range(epochs):
    # Discriminator training
    optimizer_D.zero_grad()
    real_data = torch.randn(64, 784)
    output_real = D(real_data)
    loss_real = criterion(output_real, real_label)
    
    noise = torch.randn(64, 100)
    fake_data = G(noise)
    output_fake = D(fake_data.detach())
    loss_fake = criterion(output_fake, fake_label)
    
    loss_D = loss_real + loss_fake
    loss_D.backward()
    optimizer_D.step()
    
    # Generator training
    optimizer_G.zero_grad()
    output_fake = D(fake_data)
    loss_G = criterion(output_fake, real_label)
    
    loss_G.backward()
    optimizer_G.step()
\end{lstlisting}

\subsection{Effects of Different Loss Functions}
The choice of loss function in GANs significantly affects the model's convergence and the quality of generated data. The original GAN formulation uses the \textbf{binary cross-entropy loss}~\cite{ruby2020binary}, but alternative loss functions can be employed to address issues like mode collapse, vanishing gradients, and unstable training~\cite{ho2019real}.

\subsubsection{Binary Cross-Entropy Loss (Standard GAN Loss)}
In the original GAN formulation, the generator and discriminator are trained using the binary cross-entropy loss~\cite{ho2019real, ruby2020binary}. As shown earlier, the loss for the discriminator is:
\[
\mathcal{L}_D = - \mathbb{E}_{x \sim p_{data}}[\log D(x)] - \mathbb{E}_{z \sim p_z}[\log (1 - D(G(z)))]
\]
The generator's objective is:
\[
\mathcal{L}_G = - \mathbb{E}_{z \sim p_z}[\log D(G(z))]
\]

While effective, this loss can lead to issues like vanishing gradients if the discriminator becomes too strong, as \(D(G(z))\) approaches zero, leading to very small updates for the generator.

\subsubsection{Wasserstein Loss}
The \textbf{Wasserstein loss} (used in WGANs) is designed to address the instability and vanishing gradient problems in standard GANs~\cite{frogner2015learning}. Instead of minimizing the cross-entropy, it minimizes the \textbf{Earth Mover's Distance (EMD)}~\cite{rubner2000earth}, which is more stable and provides better gradients for the generator:
\[
W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [|| x - y ||]
\]
Here, \(P_r\) is the real data distribution and \(P_g\) is the generated data distribution~\cite{ho2019real}. This loss function ensures smoother convergence and has better gradient properties compared to the binary cross-entropy loss~\cite{frogner2015learning}.

In practice, the Wasserstein loss is approximated as:
\[
\mathcal{L}_D = \mathbb{E}_{x \sim p_{data}}[D(x)] - \mathbb{E}_{z \sim p_z}[D(G(z))]
\]
The generator's objective is to minimize:
\[
\mathcal{L}_G = - \mathbb{E}_{z \sim p_z}[D(G(z))]
\]

The key difference here is that \(D(x)\) outputs unbounded real values instead of probabilities (0 to 1), and the training is constrained by applying a \textbf{weight clipping}~\cite{han2019dimension, elsayed2024weight} technique to the discriminator's weights to enforce the Lipschitz constraint~\cite{li2019preventing}.

\begin{lstlisting}[style=python]
# Example: Wasserstein GAN loss implementation with weight clipping
class WGAN_Discriminator(nn.Module):
    def __init__(self):
        super(WGAN_Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(784, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1)  # No Sigmoid, output is a real number
        )

    def forward(self, x):
        return self.main(x)

# Weight clipping function for Lipschitz constraint
def clip_weights(model, clip_value):
    for param in model.parameters():
        param.data.clamp_(-clip_value, clip_value)

# WGAN loss
for epoch in range(epochs):
    optimizer_D.zero_grad()
    real_data = torch.randn(64, 784)
    loss_D_real = -torch.mean(D(real_data))
    
    noise = torch.randn(64, 100)
    fake_data = G(noise)
    loss_D_fake = torch.mean(D(fake_data.detach()))
    
    loss_D = loss_D_real + loss_D_fake
    loss_D.backward()
    optimizer_D.step()
    
    # Apply weight clipping
    clip_weights(D, 0.01)

    optimizer_G.zero_grad()
    loss_G = -torch.mean(D(fake_data))
    loss_G.backward()
    optimizer_G.step()
\end{lstlisting}

\subsubsection{Hinge Loss}
Another popular loss function for GANs is \textbf{hinge loss}~\cite{gentile1998linear, bartlett2008classification}, which is often used in \textbf{SAGAN} (Self-Attention GAN)~\cite{zhang2019self}. Hinge loss modifies the discriminator loss to be:
\[
\mathcal{L}_D = \mathbb{E}_{x \sim p_{data}}[\max(0, 1 - D(x))] + \mathbb{E}_{z \sim p_z}[\max(0, 1 + D(G(z)))]
\]
The generator's objective is:
\[
\mathcal{L}_G = -\mathbb{E}_{z \sim p_z} [D(G(z))]
\]
Hinge loss encourages the discriminator to push its output values closer to 1 for real data and closer to -1 for generated data, which helps in stabilizing the training process~\cite{bartlett2008classification, zhang2019self}.

Each of these loss functions has unique properties that affect the behavior of GANs during training. Depending on the application and dataset, the appropriate loss function can significantly improve the performance and stability of GAN models~\cite{zhang2019self}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/gan_loss.pdf}
    \caption{Comparison of Loss Functions in GAN Training.}
\end{figure}

The chart above illustrates the loss trends for three different loss functions (Binary Cross-Entropy, Wasserstein, and Hinge) for both the generator (G) and discriminator (D) during GAN training over 100 epochs. Each line shows how the respective loss changes as training progresses. This visual can help to understand the convergence patterns and stability of different loss functions in GANs.