

\section{Experiments}

\tableAbl
\subsection{Benchmarks}
Following the methodology described in \cite{jaritz2020xmuda, simons2023summit}, we evaluate our method across three different benchmarks: (1) \textbf{USA-to-Singapore}, (2) \textbf{A2D2-to-SemanticKITTI}, and (3) \textbf{Singapore-to-SemanticKITTI}. The first scenario (1) uses the NuScenes dataset \cite{caesar2020nuscenes} for both the source and target domains which are separated by the location of scenes. The point clouds and images collected in the USA serve as the source domain, while those collected in Singapore are used as the target domain. The primary domain gap arises from the differing infrastructure between the two countries, and we create a 5-class segmentation benchmark based on these differences. In the second scenario (2), we use A2D2 dataset \cite{geyer2020a2d2} as the source domain and SemanticKITTI \cite{behley2019semantickitti} dataset as the target domain. Following the class-mapping strategies outlined in \cite{jaritz2020xmuda, simons2023summit}, we establish a 10-class semantic segmentation benchmark. The third scenario (3) uses the Singapore dataset from NuScenes as the source domain and the SemanticKITTI dataset as the target domain. We follow \cite{simons2023summit} and formulate a 10-class semantic segmentation benchmark using the mapping in NuScenes-LiDARSeg dataset. The scenario (2) and (3) specifically address dataset-to-dataset domain gap/adaptation, primarily focusing on differences in sensor settings (e.g., number of LiDAR beams, image resolution) and mounting positions. It is important to note that we only utilize the 3D points with valid projection onto the camera plane.

\figVis

\subsection{Implementation Details}

We consider two adaptation tasks for multi-modal 3D semantic segmentation: unsupervised domain adaptation (UDA) and source-free domain adaptation (SFDA). We select \textbf{xMUDA}\cite{jaritz2020xmuda} and \textbf{SUMMIT}\cite{simons2023summit} as our baselines, as both methods utilize pseudo-labeling during their adaptation process. Our experiments are implemented in PyTorch and we strictly follow the network structures in \cite{jaritz2020xmuda} and \cite{simons2023summit}. Specifically, we use ResNet34 \cite{he2016deep} as the 2D backbone and the Sparse Convolutional Network UNet \cite{graham20183d} as the 3D backbone. For the UDA setting, we follow the training strategy of xMUDA \cite{jaritz2020xmuda} with the only difference of replacing their pseudo-labels with our enhanced ones. Note that xMUDA produces pseudo-labels from both 2D and 3D modalities, and we enhance both of them. For the SFDA setting, we follow the non-crossover training strategy described in SUMMIT \cite{simons2023summit} to train the models on the source domain. Then we adopt the Agreement Filtering strategy to generate the initial pseudo-labels and apply our method for additional pseudo-label enhancement. Subsequently, we update the trained model using the enhanced pseudo-labels for 50000 iterations with a learning rate of $1e-6$ for scenario (1) and $1e-4$ for scenarios (2) and (3). The Adam optimizer \cite{kingma2014adam}  is used with a batch size of 8 for all experiments. All training and testing are conducted using one NVIDIA A40 48GB GPU. For our pseudo-label enhancement algorithm, we use the pre-trained SAM-ViT-H checkpoint and the default settings of SAM to generate 2D masks. Consistent performance improvement was observed with the hyperparameters $\lambda_s=0.2$, $\lambda_p=0.8$, $\lambda_r=0.1$, and $\beta=2$ across all experiments.






%\subsection{Pseudo Label Generation}

%\subsubsection{Unsupervised Domain Adaptation (UDA)}

% We strictly follow the training strategy of XMUDA. In the UDA setting, we are allowed to use the source domain data and labels while the adaptation process. Except for the cross-entropy loss for 3D segmentation, XMUDA introduces two auxiliary head networks $\tilde{g}^{2D}$ and $\tilde{g}^{3D}$ to encourage cross-modal similarity. In the first stage, the model is training using:
% \begin{equation}
%     \mathcal{L}_{seg}(x^{2D}, x^{3D}, y^{3D}) = \mathcal{L}_{ce}(P^{2D}, y^{3D}) + \mathcal{L}_{ce}(P^{3D}, y^{3D})
% \end{equation}
% The authors also defined a cross-modal learning loss function as shown below:
% \begin{equation}
% \begin{split}
%     \mathcal{L}_{xm}(x^{2D}, x^{3D}) = D_{KL}(P^{2D} || P^{3D\rightarrow2D})  \\ 
%     + D_{KL}(P^{3D} || P^{2D\rightarrow3D})
% \end{split}
% \end{equation}
% In total, XMUDA is first trained using
% \begin{equation}
% \begin{split}
%     \mathcal{L}_{xmuda} = \mathcal{L}_{seg}(x^{2D}_s, x^{3D}_s, y^{3D}_s) + \lambda_s\mathcal{L}_{xm}(x^{2D}_s, x^{3D}_s) \\ 
%     + \lambda_t\mathcal{L}_{xm}(x^{2D}_t, x^{3D}_t).
% \end{split}
% \end{equation}
% After training, the authors use the trained models to provide pseudo labels for both 2D and 3D modality $\hat{y}^{2D}$ and $\hat{y}^{3D}$. To enforce the quality of pseudo labels, the authors use median filtering to filter out those unreliable pseudo labels, that is, filtering out pseudo labels whose score for the predicted class is lower than the median probability of that class over the target dataset.

% After generating pseudo-labels, the XMUDA structure is trained again from scratch using the new loss function: 
% \begin{equation}
% \begin{split}
%     \mathcal{L}_{xmuda} = \mathcal{L}_{seg}(x^{2D}_s, x^{3D}_s, y^{3D}_s) + \lambda_s\mathcal{L}_{xm}(x^{2D}_s, x^{3D}_s) \\ 
%     + \lambda_{PL}\mathcal{L}_{seg}(x^{2D}_t, x^{3D}_t, \hat{y}^{2D}_t, \hat{y}^{3D}_t) + \lambda_t\mathcal{L}_{xm}(x^{2D}_t, x^{3D}_t).
% \end{split}
% \end{equation}

% \subsubsection{Source-free Domain Adaptation (SFDA)}
% Under the settings of source-free domain adaptation, the target domain adaptation process has no access to the source dataset and only the models trained on the source domain are provided. In this case, we adopt the settings in SUMMIT. First, the source domain models are trained using 
% \begin{equation}
% \begin{split}
%     \mathcal{L}_{s} = \mathcal{L}_{seg}(x^{2D}_s, x^{3D}_s, y^{3D}_s) + \lambda_s\mathcal{L}_{xm}(x^{2D}_s, x^{3D}_s).
% \end{split}
% \end{equation}
% Then, the source domain models are used to generate the pseudo labels. We adopt the agreement filtering strategy introduced in SUMMIT, where we first use the median filtering technique to filter out unreliable labels for both 2D pseudo labels $\hat{y}^{2D}$ and 3D pseudo labels $\hat{y}^{3D}$. Then, we keep the pseudo label only when the 2D pseudo labels and 3D pseudo labels align with each other, which can be represented as:
% \begin{equation}
% \hat{y}_i = 
% \begin{cases}
% \hat{y}_i^{2D} & \text{if $\hat{y}_i^{2D} = \hat{y}_i^{3D}$} \\
%   \text{ignore} & \text{otherwise}
% \end{cases}
% \end{equation}
% Then, the model is trained purely on the pseudo labels, which is:
% \begin{equation}
% \begin{split}
%     \mathcal{L}_{summit} = \mathcal{L}_{seg}(x^{2D}_t, x^{3D}_t, \hat{y}_t) + \lambda_s\mathcal{L}_{xm}(x^{2D}_t, x^{3D}_t).
% \end{split}
% \end{equation}

\subsection{Main Results}

%In this section, We present the results of our proposed method on two adaptation tasks UDA and SFDA  three domain adaptation scenarios. 
Table \ref{table:mainres} showcases the performance improvement on 3D semantic segmentation achieved by our proposed pseudo-label enhancement approach. We report the mean Intersection over Union (mIoU) for both modalities 2D and 3D, as well as the average across the softmax output of both modalities, denoted as 2D+3D. Under the UDA setting, the results demonstrate that xMUDA significantly outperforms the baseline without any adaptation. With the inclusion of psuedo-labels, $\text{xMUDA}_{pl}$ exhibits further improvements, consistent with the findings in \cite{jaritz2020xmuda}. Notably, when our proposed method is used to enhance pseudo-labels, xMUDA's performance improves across all three adaptation scenarios. Specifically, in the USA-to-Singapore scenario, where the domain gap is relatively small, we observe an improvement of up to $3.6\%$ for single modality and $2.28\%$ on 2D+3D. In the other two dataset-to-dataset adaptation scenarios, our method's improvements are more substantial, reaching up to $10.84\%$ for single modality and $8.95\%$ for the combined 2D+3D mIoU.
Under the SFDA setting, a similar trend is observed. The mIoU for 2D, 3D, and 2D+3D all show improvements. For instance, in the relatively small domain dap setting of USA-to-Singapore, the mIoU for 2D+3D improves by $0.87\%$. In other settings, the improvements are more significant, reaching $14.5\%$ for single modality and $11.47\%$ for both 2D and 3D modalities. These results demonstrate the effectiveness of our proposed method in enhancing pseudo-labels for both UDA and SFDA tasks.

Next, we perform an ablation study on the pseudo-label statistics and the mIoU performance when incorporating different components of our proposed method, as shown in Table \ref{table:label} and \ref{table:ablation}. All the experiments are conducted under the A2D2-to-SemanticKITTI scenario. When we perform pseudo-label enhancement using only DP, Table \ref{table:label} shows that the number of correct pseudo-labels increases by over $40\%$ for xMUDA and approximately $160\%$ for SUMMIT. However, due to pseudo-label noise and 2D-3D misalignment, this straightforward enhancement significantly reduces pseudo-label accuracy, as reflected in Table \ref{table:label}. Consequently, the improvement in mIoU, as shown in Table \ref{table:ablation}, is not significant. In fact, for xMUDA, mIoU performance decreases from 49.46 to 48.30. Next, when we add the MF criterion, although the number of correct pseudo-labels decreases slightly as some correct labels are filtered by MF as well, the pseudo-label accuracy significantly increases, leading to noticeable mIoU improvements in Table \ref{table:ablation} over the baseline without any pseudo-label enhancement. Lastly, incorporating the GAPP model to address the 2D-3D misalignment issue further enhances pseudo-label accuracy while maintaining a similar number of correct pseudo-labels.  This results in a pseudo-label accuracy comparable to the original pseudo-labels but with more than $30\%$ additional correct pseudo-labels for xMUDA and approximately $120\%$ more for SUMMIT. As the quality of pseudo-labels improves, the mIoU performance also increases, achieving the best results.

Finally, we include visualizations of the enhanced pseudo-labels in Fig. \ref{fig:visualization}. It shows the initial pseudo-labels are relatively sparse and lack comprehensive coverage of the entire scene. With our proposed algorithm, the resulting pseudo-labels are much denser, particularly for cars and roads, and it exhibits high accuracy by correctly avoiding mislabels for background points.






