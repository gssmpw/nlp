\section{Related Works}

\subsection{Unsupervised Domain Adaptation (UDA) For Semantic Segmentation}

\textbf{Uni-modal:} UDA aims to address the challenge of domain shift, particularly in scenarios where no labeled data is available in the target domain. It has been extensively studied in the context of uni-modal semantic segmentation tasks for both 2D and 3D scenarios. For 2D semantic segmentation, one common approach is to align the distribution of featuresoutputs between the source and target domains \cite{tsai2018learning, hoffman2018cycada, hong2018conditional, vu2019advent}. Alternative approaches  pseudo-labeling techniques to populate missing labels to reduce the domain gap \cite{li2022class, mei2020instance, pan2020unsupervised, zhao2023learning}.
Similarly, several notable contributions have been made to 3D semantic segmentation. Wu et al. \cite{wu2019squeezesegv2} introduced activation correlation alignment and progressive domain calibration to perform UDA between simulated and real-world LiDAR data. Yi et al. \cite{yi2021complete} proposed the `Complete \& Label' framework, which models the underlying 3D surface priors to allow the transfer of semantic labels between different LiDAR sensors. Additionally, Saltori et al. \cite{saltori2022cosmix} introduced CoSMix, which alleviates domain shift through a point cloud mixing strategy.

\textbf{Multi-modal:} Recently, the growth of multi-modal datasets \cite{behley2019semantickitti, caesar2020nuscenes, geyer2020a2d2} containing both LiDAR measurements and corresponding images, along with advancements in multi-modal methodologies \cite{bai2022transfusion, meyer2019sensor, yang2022efficient}, has increased the interest in UDA within multi-modal settings. Jaritz et al. \cite{jaritz2020xmuda} introduced the first UDA framework for multi-modal scenarios, xMUDA, which incorporates two additional head networks to enforce cross-modal similarity. Building on xMUDA, Peng et al. \cite{peng2021sparse} introduced DsCML to enhance information interaction between different modalities for UDA. Similarly, Xing et al. \cite{xing2023cross} proposed an attention module to improve the integration of 2D features with 3D points. Additionally, source-free domain adaptation (SFDA) \cite{simons2023summit} and test-time domain adaptation (TTA) \cite{shin2022mm, cao2024reliable} have been studied as specialized cases within UDA.

\subsection{Segment Anything Model (SAM)}

Recently, SAM \cite{peng2023sam, ravi2024sam} has gathered significant attention as a versatile segmentation model, trained on the extensive SA-1B dataset with over 1 billion masks derived from 11 million images. SAM demonstrates remarkable zero-shot transfer capability and this has catalyzed the application of SAM across numerous domains, such as medical image segmentation \cite{huang2023push, huang2024segment}, weakly-supervised image segmentation \cite{chen2023segment, he2024weakly}, object detection \cite{yang2023sam3d}, and tracking \cite{yang2023track, cheng2023segment}. Such concepts have also been extended to 3D scenarios \cite{liu2024segment}. Beyond leveraging SAM's zero-shot abilities, several studies have proposed fine-tuning the SAM for various downstream tasks \cite{ma2024segment, zhang2023personalize, wu2023medical}.

The zero-shot capability of SAM has recently been applied to multi-modal 3D semantic segmentation domain adaptation. For instance, Peng et al. \cite{peng2023sam} proposed a technique to map both 2D and 3D features to SAM features to bridge the domain gap. Similarly, Cao et al. \cite{cao2024mopa} utilize SAM masks to impose a pixel-wise consistency loss, thus encouraging consistent predictions within each mask. These existing methods aim to improve the UDA performance by refining the alignment between the source and target models using SAM, while our method focuses on employing SAM masks to enhance the quality of 3D pseudo-labels for self-training. Furthermore, existing methods typically assume the constant presence of the source model, which hampers their generalizability to SFDA or TTA scenarios when the source model is absent. On the contrary, our method is flexible and suitable for diverse adaptation scenarios where pseudo-labeling is possible, regardless of the source model.