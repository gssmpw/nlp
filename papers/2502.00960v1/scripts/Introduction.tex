\section{INTRODUCTION}
In recent years, 3D semantic segmentation has emerged as a pivotal task in 3D scene understanding, essential for applications such as autonomous driving  \cite{wu2018squeezeseg, xu2021rpvnet} and augmented/virtual reality (AR/VR) \cite{choy20194d, thomas2019kpconv, qi2017pointnet++}. Recently, driven by the new multi-modal datasets \cite{caesar2020nuscenes, behley2019semantickitti}, the integration of image data has been increasingly adopted to enhance the accuracy of 3D semantic segmentation as it provides complementary 2D information such as rich texture and color details, which supplements the geometric information from 3D point clouds \cite{su2018splatnet, meyer2019sensor}. 

However, similar to other perception tasks, 3D semantic segmentation can suffer from domain shifts between the training and the real-world testing environments, necessitating the use of domain adaptation techniques. Domain adaptation aims to bridge this domain gap and has been a significant area of research for both 2D and 3D semantic segmentation \cite{hoffman2018cycada, li2019bidirectional, vu2019advent, wu2019squeezesegv2, yi2021complete}. Recently, such focus has extended to multi-modal 3D semantic segmentation and has shown impressive performance across various adaptation settings \cite{jaritz2020xmuda, cao2024mopa, shin2022mm, simons2023summit}. Among these existing methods, self-training using pseudo-labels has proven to be a crucial component and several pseudo-label generation approaches have been proposed such as thresholding \cite{jaritz2020xmuda} or modality agreement \cite{shin2022mm, simons2023summit}. However, the generated pseudo-labels tend to be sparse (as shown in Fig. \ref{fig:front}) and often limit the overall adaptation performance due to its insufficient coverage of the target data. 

\figFront

Recently, the Segment Anything Model (SAM) \cite{kirillov2023segment, ravi2024sam} has gathered significant attention. Trained over 1 billion masks, SAM has exhibited remarkable zero-shot segmentation capabilities by generating class-agnostic segmentation masks with proper prompts. This zero-shot ability has proven valuable for various applications \cite{huang2023push, ma2024segment, huang2024segment, chen2023segment}. Although SAM is not inherently designed for 3D point clouds, its versatility has been extended to multi-modal settings in recent works \cite{cao2024mopa, peng2023sam, liu2024segment}. 

Inspired by recent advancements in leveraging the zero-shot segmentation capability of SAM, we propose to employ such 2D prior knowledge in the multi-modal setting to enhance sparse 3D pseudo-labels and hence boost the domain adaptation performance. Given a 3D point cloud, the SAM masks generated from the paired 2D image data can effectively group 3D points that belong to the same object by exploiting the 3D-2D correspondence between 3D points and the camera plane. With this grouping information, we design a two-step mask-wise pseudo-label enhancement framework to generate additional reliable pseudo-labels within each SAM mask. 
Specifically, for each SAM mask associated with multiple 3D pseudo-labels for its covered points, we first identify the class label of the entire mask through majority voting. To alleviate the effect of inherent pseudo-label noise, we introduce various constraints on the mask area and the distribution of pseudo-labels to filter out unreliable mask labels. Secondly, we aim to propagate the mask label to all points within the mask that lack pseudo-labels. To avoid assigning the mask label to outlier points that are incorrectly projected to the object due to 2D-3D misalignment \cite{an2020geometric}, we propose Geometry-Aware Progressive Propagation (GAPP) where the mask label is propagated only to nearby points in the 3D space in each round, thus eliminating outlier points that lack a connection to the object.   
We evaluate the proposed method on multiple datasets and two adaptation tasks: unsupervised and source-free domain adaptation. Experimental results demonstrate that our method effectively increases the number of high-quality pseudo-labels and significantly improves the adaptation performance.

% Thus, an effective approach to generate more pseudo-labels is to 1) identify the class label of the mask and 2) propagate the mask label to all the points within the SAM mask. However, since the initial pseudo-labels are not clean, one would face two main challenges to get high-quality pseudo-labels:
% \begin{itemize}
%    \item \textbf{Mask Label Noise:}
%    \item \textbf{2D-3D Misalignment:}
% \end{itemize}
%In this paper, we present a novel pseudo-label enhancement method for multi-modal 3D segmentation domain adaptation where the 2D prior knowledge from the SAM model is leveraged to address the challenge of label sparsity. Utilizing the 3D-2D correspondence between the 3D points and the camera plane, SAM masks naturally group 3D points that belong to the same object.
% Our method begins with the smallest SAM mask and iteratively enhances pseudo-labels for each subsequent SAM mask. For any given SAM mask, the refinement comprises two primary stages: Mask Label Assignment and Mask Label Propagation. In the first stage, given that SAM masks are class-agnostic, we assign a class label to the mask using majority voting based on the pseudo-labels it contains. Additionally, to ensure robust label assignments despite potential pseudo-label noise, we introduce multiple constraints on the mask area and the distribution of pseudo-labels to filter out unreliable mask labels. 
% In the Mask Label Propagation stage, we aim to propagate the mask label to all points within the mask that lack pseudo-labels. To avoid assigning the mask label to outlier points that are incorrectly projected to the object due to 2D-3D misalignment \cite{an2020geometric}, we propose Geometric-Aware Progressive Propagation (GAPP) where the mask label is propagated only to nearby points in the 3D space for each round, thus eliminating outlier points that lack a connection to the object. 


Our contributions can be summarized as follows:
\begin{itemize}
   \item We introduce a novel pseudo-label enhancement method for multi-modal 3D semantic segmentation domain adaptation tasks using 2D SAM masks. 
   \item We propose a series of mask label filtering constraints to ensure robust mask label generation and a geometry-aware propagation strategy GAPP to tackle the 2D-3D misalignment issue. 
   \item The proposed method is evaluated on two adaptation tasks and three adaptation scenarios using multiple datasets, consistently demonstrating improvement over existing domain adaptation methods.
\end{itemize}