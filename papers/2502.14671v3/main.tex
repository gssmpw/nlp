
%\documentclass[sn-nature,Numbere]{sn-jnl}% Default
\documentclass[sn-mathphys,Numbered]{sn-jnl}% Default
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{hyperref}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{url} % Added for compatibility with pdf latex
\usepackage{chngcntr}
\usepackage{svg}
\usepackage{placeins}
\usepackage{cleveref} % 
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
\newtheorem{proposition}[theorem]{Proposition}% 
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%
\raggedbottom


\newcommand{\ms}[1]{\mathsf{#1}}
\newcommand{\synteq}{::=}
\newcommand{\intgrads}{\ms{InteriorGrads}}
\newcommand{\integratedgrads}{\ms{IG}}
\newcommand{\internalinfluence}{\ms{IntInf}}
\newcommand{\conductance}{\ms{Cond}}
\newcommand{\pathintegratedgrads}{\ms{PathIntegratedGrads}}
\newcommand{\relu}{\ms{ReLU}}
\newcommand{\sigmoid}{\ms{Sigmoid}}
\newcommand{\xbase}{x'}
\newcommand{\sparam}{\alpha}

\newcommand\TODO[1]{\textcolor{green}{#1}}
\newcommand\remove[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\cyan}[1]{{\color{cyan}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\orange}[1]{{\color{orange}#1}}
\newcommand{\purple}[1]{{\color{purple}#1}}

% Define a new counter for supplementary figures
\newcounter{suppfigure}
\renewcommand{\thesuppfigure}{S\arabic{suppfigure}} % Format: S1, S2, S3, ...

% Reset the supplementary figure counter
\setcounter{suppfigure}{0}

% Define a new environment for supplementary figures
\newenvironment{suppfigure}[1][]{
    \refstepcounter{suppfigure} % Increment the supplementary figure counter
    \begin{figure}[#1] % Start a regular figure environment
        \centering
}{
    \end{figure} % End the figure environment
}
\newcommand{\additionalheadings}[1]{%
  \vspace{1.5ex plus 0.5ex minus 0.5ex}% Space before (matches typical section spacing)
  \noindent % Prevents indentation
  {\normalfont\large\bfseries #1}% Normal size, bold
  \par\vspace{1ex plus 0.2ex minus 0.2ex}% Space after (matches typical section spacing)
  \noindent % No indent for the following paragraph
}

\begin{document}
\title[Article Title]{Explanations of Large Language Models Explain Language Representations in the Brain}


\author[1]{\fnm{Maryam} \sur{Rahimi}}\email{maryamrahimiha@gmail.com}
\author[2,4]{\fnm{Yadollah} \sur{Yaghoobzadeh}}\email{y.yaghoobzadeh@ut.ac.ir}
\author[1,3]{\fnm{Mohammad Reza} \sur{Daliri}}\email{daliri@iust.ac.ir}

\affil[1]{\orgdiv{Biomedical Engineering Department, School of Electrical Engineering}, \orgname{Iran University of Science and Technology}, \orgaddress{\city{Tehran}, \country{Iran}}}

\affil[2]{\orgdiv{Electrical and Computer Engineering Department}, \orgname{University of Tehran}, \orgaddress{\city{Tehran}, \country{Iran}}}

\affil[3]{\orgdiv{School of Cognitive Sciences}, \orgname{Institute for Research in Fundamental Sciences}, \orgaddress{\city{Tehran}, \country{Iran}}}

\affil[4]{\orgdiv{Tehran Institute for Advanced Studies}, \orgname{Khatam University}, \orgaddress{\city{Tehran}, \country{Iran}}}


\abstract{
Large language models (LLMs) not only exhibit human-like performance but also share computational principles with the brain’s language processing mechanisms. While prior research has focused on mapping LLMs’ internal representations to neural activity, we propose a novel approach using explainable AI (XAI) to strengthen this link. Applying attribution methods, we quantify the influence of preceding words on LLMs’ next-word predictions and use these explanations to predict fMRI data from participants listening to narratives. We find that attribution methods robustly predict brain activity across the language network, revealing a hierarchical pattern: explanations from early layers align with the brain’s initial language processing stages, while later layers correspond to more advanced stages. Additionally, layers with greater influence on next-word prediction—reflected in higher attribution scores—demonstrate stronger brain alignment. These results underscore XAI’s potential for exploring the neural basis of language and suggest brain alignment for assessing the biological plausibility of explanation methods.
}

\keywords{Cognitive computational Neuroscience; Language Models; Explainable AI; Encoding Model; fMRI}


\maketitle

\section{Introduction}\label{sec1}

Recent progress in deep learning has led to the development of autoregressive language models that excel in capturing language structures and performing human-like capabilities in various linguistic tasks \cite{radford2018improving, Radford2019, yang2019xlnet, rosset2019turing}. This progress poses a critical question at the intersection of artificial intelligence (AI) and neuroscience: Are the observed similarities between language models and human cognition merely superficial, or do they stem from shared underlying mechanisms?

Previous studies have demonstrated significant alignment between large language models (LLMs) and brain activity, primarily through linear mappings between neural responses and LLM internal representations, such as activations, attention heads, and layer transformations \cite{caucheteux2022brains, schrimpf2021, Lammarre2022, kumar2024shared}. This relationship has been explored across multiple dimensions, including model layers, architectures, training settings, and linguistic performance \cite{caucheteux2022brains, schrimpf2021, toneva2019, mischler2024contextual}. Notably, transformer models consistently outperform recurrent and embedding-based models in brain alignment tasks, displaying brain-like patterns tied closely to their proficiency in next-word prediction tasks \cite{schrimpf2021, caucheteux2022brains}.

Building on this foundation, subsequent research has leveraged LLM-brain alignment to deepen our understanding of both systems. For example, LLM-based encoders have provided insights into key aspects of neural processing, such as predictive processing, semantic selectivity, and meaning composition in the brain during naturalistic language processing \cite{caucheteux2023, antonello2024gemv, toneva2022combining}. Conversely, these approaches have been employed to evaluate and refine LLMs themselves \cite{aw2023training, moussa2024improving, toneva2019}.

Despite these advances, several critical questions remain unresolved: (i) Do the observed similarities between LLMs and the brain arise from shared pathways between these two systems, or are they simply artifacts of high-dimensional feature spaces \cite{antonello2023, antonello2024predictive}? (ii) Beyond internal representations, are there alternative frameworks that can more effectively capture the alignment between LLMs and neural language processing? Addressing these questions requires moving beyond treating both systems as ``black boxes'' \cite{abnar2019blackbox}, and instead probing their underlying mechanisms in a more interpretable way.

To address these questions, we leverage explainable AI (XAI) techniques, specifically attribution methods, which quantify how much each preceding word contributes to a model’s evolving representation and next-word prediction. Understanding how meaning is constructed across words is also a critical goal for neurocomputational theories of language processing \cite{kumar2024shared}. If the brain employs a strategy similar to LLMs in weighting prior linguistic inputs, attribution-based explanations could provide a novel framework for modeling the neural mechanisms underlying this process in the brain (Fig. \ref{fig:Approach}a). 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig1.pdf}
    \caption{\textbf{Approach. a,} Language processing in the brain and LLMs shows striking parallels. Applying XAI methods, this study investigates how XAI can reveal insights into both systems and their interrelationship.   
    \textbf{b,} To test this hypothesis, we used attribution methods as representatives of XAI. Attribution methods reveal how much each previous word in a context influences the model’s decision about the next incoming word. Using a sliding window approach, we applied these methods to continuous stories. Since the sliding windows overlap, each word in the story is processed within different windows, resulting in multiple importance scores being assigned to a single word. These scores were stored in a vector for each word, forming an attribution feature space. \textbf{c,} We used ridge regression to quantify how well attribution-based explanations predict fMRI brain activity recorded while participants listened to stories. Prediction accuracy was assessed with a brain score, defined as the Pearson correlation between the predicted and actual brain responses on held-out data. We applied this approach to attribution feature spaces generated by four distinct attribution methods (Erasure, Integrated Gradients, Gradient Norm, and Gradient $\times$ Input) across three LLMs (GPT-2, Llama 2, and Phi-2), using four stories. Each feature space was independently tested to predict the fMRI activity of participants for its corresponding story. In total, 147 participants were analyzed across all stories. Additionally, we compared these attribution-based feature spaces to widely used internal LLM representations, namely activations and attention weights, using the same stories, LLMs, and participants. In the figure: \textit{x} represents the input sequence, \textit{$x_t$} denotes the target word (incoming word), and \textit{$x'$} is the baseline input. $\textit{f}$ represents the LLM function, while $\textit{y}$ is the recorded brain response (BOLD signal) from a voxel, and $\hat{y}$ is the predicted response. $\textit{Corr.}$ refers to the Pearson correlation used for evaluation. The attribution methods formulas are shown in full in the \hyperref[sec:methods]{Methods}.}
    \label{fig:Approach}
\end{figure}

In this study, we applied four types of attribution methods to three LLMs and constructed feature representations to model fMRI activity recorded during naturalistic story listening. We then evaluated the predictive power of these features in capturing brain activity, comparing them to traditional internal representations such as model activations and attention weights (Fig. \ref{fig:Approach}b, c). To investigate how attribution-based explanations vary across different model layers, we employed layer conductance to generate feature representations for individual layers and assessed their alignment with specific brain regions.

Our findings reveal the strong predictive capability of attribution methods across a wide network of language-related brain areas, consistently observed across participants. Notably, attribution methods outperform internal representations in predicting brain activity in early language-processing regions. Furthermore, we uncovered a striking hierarchical correspondence between LLMs and the brain, suggesting shared mechanisms for integrating contextual information across processing stages. These results position attribution methods as a powerful tool for bridging LLMs and the brain, offering new insights into the computational principles underlying language processing in both artificial and biological systems.

\section{Results}\label{results}
Are LLM-derived explanations effective in explaining language processing in the brain?
We explore this question through an encoding modeling approach that predicts brain activity based on explanations derived from attribution methods. Specifically, we focus on explanations that clarify why an LLM makes a particular prediction, providing a more transparent window into the model's decision-making process.

We utilized brain data from a subset of the Narratives fMRI dataset \cite{nastase2021narratives}, which includes 147 participants who listened to four distinct audio stories. We then processed the same stories using LLMs with a sliding window approach, where each model separately predicted the next word based on the preceding context. Attribution methods—a class of XAI techniques—were applied to quantitatively assess how specific words influence the model’s decision-making (i.e., predicting the next word). These methods assign importance scores to each word, reflecting its impact on the LLM’s predictions. The scores were subsequently organized into a matrix to create a feature space (Fig. \ref{fig:Approach}b).


In the encoding modeling step, the feature spaces derived from each LLM were independently used to predict participants' brain responses. Predictions were made using a voxel-wise linear ridge regression model, tailored to each individual's brain data. Prediction accuracy was evaluated using five-fold cross-validation, with the ``brain score'' metric as the evaluation criterion \cite{yamins2014, Huth2016}. This metric computes the Pearson correlation between the predicted and observed fMRI responses in held-out data (Fig. \ref{fig:Approach}c).


\subsection{Explanations}
In this study, we leverage feature attribution methods, a key class of XAI techniques, to gain insights into how LLMs generate predictions. These methods quantify the contribution of each token in a given context to the model’s next-word prediction, providing a fine-grained view of the model’s internal decision-making process. Feature attribution techniques have proven valuable across various applications, including improving model interpretability \cite{mariotti2022measuring_att_und}, identifying biases \cite{arias2022focus_att_bias}, and diagnosing model errors \cite{bastings2021will_att_shortcut}. In this study, we employ four distinct attribution methods, each of which is described in detail below.


\textit{Gradient Norm:} This method computes the gradient of the model’s output with respect to the input token embeddings and returns the L1 norm of the resulting gradient as the importance score for each token \cite{simonyan2013deep_gradn, li2015visualizing_grad_n}. The L1 norm ensures that the importance scores reflect the magnitude of the gradient, providing a straightforward measure of each token's influence on the model's prediction.

\textit{Gradient $\times$ Input:}
A key consideration in computing importance scores is the role of input token embeddings. The Gradient $\times$ Input method \cite{denil2015} addresses this by multiplying the gradient of the model’s output with the input embeddings. This technique has been found to perform particularly well in tasks requiring output explainability when combined with the L2 norm \cite{atanasova2020diagnostic}. To derive per-token scores, we computed the L2 norm of the product of the gradient and input embeddings.

\textit{Integrated Gradients:}
Integrated Gradients \cite{sundararajan2017axiomatic} is another gradient-based method that addresses the issue of saturation, where gradients can diminish to near-zero values in well-fitted models. This technique computes the contribution of each input token by accumulating the gradients of the model’s output with respect to the input, following a path from a baseline state (e.g., a zero vector) to the actual input.

\textit{Erasure:}
Erasure-based methods \cite{li2016understanding_erasure} estimate the importance of an input token by evaluating the effect of its removal or masking on the model's prediction. This approach directly quantifies the criticality of a token by observing the impact of its absence on the model's overall output.


We applied the aforementioned feature attribution methods to three different language models. GPT-2 \cite{Radford2019} was chosen as the primary model, as prior studies have shown that it exhibits brain-like representations compared to other examined models \cite{schrimpf2021}. To ensure that our findings were not model-specific, we extended the analysis to two additional contemporary models: Llama 2 and Phi-2. Llama 2, a family of pre-trained and fine-tuned open-source models, has demonstrated competitive performance with other open-source chat models \cite{touvron2023}. Phi-2 offers performance comparable to significantly larger models, excelling in language understanding and commonsense reasoning.\cite{gunasekar2023}.

We applied attribution methods to outputs of each model using windowed portions of the stories as input. The resulting importance scores were used to construct feature spaces tailored to each attribution method. These feature spaces formed the basis for our encoding approach, enabling us to assess their alignment with brain activity. Further details on the attribution methods, computational process, and feature space design for the encoding model are provided in the \hyperref[sec:methods]{Methods}.

\subsection{Explanations Predict Brain Activity}\label{explantions_predict}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig2.pdf}
    \caption{ \textbf{Brain score of attribution methods across LLMs.} 
    \textbf{a,} Brain score obtained with Gradient Norm and Gradient $\times$ Input attribution methods for Llama 2. Scores were computed voxel-wise for each participant and attribution method, then averaged across all participants and methods.  Only significant predicted voxels are color-coded.
    \textbf{b,} Voxel-wise brain scores were computed for each combination of attribution method, LLM, and individual, then averaged across individuals and voxels within the left hemisphere. Similar patterns were observed in the right hemisphere (Fig. \ref{fig:S_LLM_attribution_R}a).    
    \textbf{c,} Brain scores of LLMs across ROIs. Alignment is expressed as brain score, normalized and presented as a percentage of the noise ceiling, which was estimated using intersubject correlation (ISC) \cite{Nastase_isc}.(The noise ceiling for each ROI is shown in Fig. \ref{fig:S_noise_ceiling}.) Markers represent the mean brain score (as \% of noise ceiling) for each ROI. Brain scores were computed independently for each combination of Grad Norm and Grad × Input feature spaces, LLMs, and individuals, and then averaged across feature spaces and participants. Error bars indicate the 95\% confidence interval across individuals. Marker colors match the brain map, indicating the corresponding brain region, while error bar colors represent different LLMs. Results for right-hemisphere language ROIs are shown in Fig. \ref{fig:S_LLM_attribution_R}b.
    \textbf{d,} The model with the highest brain score for Gradient Norm and Grad $\times$ Input explanation. Only voxels where the differences among brain scores are statistically significant are color-coded. Significant voxels in both \textbf{a} and \textbf{d} were identified using Wilcoxon signed-rank test across individuals. For \textbf{d}, an additional two-step procedure was applied: first, a multiple-comparison using the Friedman test; second, a Wilcoxon signed-rank test was conducted to compare models. All $P$ values were corrected for multiple comparisons using the false discovery rate (FDR), with a significance threshold of $P < 0.05$.}
    \label{fig: Attribution predict}
\end{figure*}

Our results demonstrate that attribution methods effectively predict brain activity across a large bilateral cohort of voxels within the language network (Fig. \ref{fig: Attribution predict}a). Among the tested methods, Gradient Norm and Gradient × Input emerged as the most consistent and biologically plausible, providing explanations that closely align with neural activity. These methods significantly predicted brain responses in more than half of all language-related voxels across the evaluated LLMs, outperforming other approaches such as Erasure and Integrated Gradients (Fig. \ref{fig: Attribution predict}b, refer to Fig. \ref{fig:S_LLM_attribution_R}a for results from the right hemisphere.). Given their superior alignment with brain activity and robustness across participants, we focus our subsequent analyses on Gradient Norm and Gradient × Input.

The brain regions significantly predicted by these methods include the superior and middle temporal regions and lateral frontal areas, all of which are well-established components of the language network \cite{fedorenko2024}. Across all models, peak brain scores were observed during the early stages of language processing, reaching approximately 60\% of the noise ceiling in areas such as Heschl’s gyrus, the superior temporal gyrus (STG), and the Sylvian fissure. While higher-order language processing regions also exhibited significant predictions, their brain scores were lower compared to early processing areas (Fig. \ref{fig: Attribution predict}c; see similar results for the right hemisphere in Fig. \ref{fig:S_LLM_attribution_R}b). Although no significant differences were observed between models in most ROIs, voxelwise comparisons revealed that Llama 2 significantly outperformed other LLMs in early language areas (Fig. \ref{fig: Attribution predict}d).

These findings underscore the effectiveness of Gradient Norm and Gradient $\times$ Input in providing computational explanations for the neural mechanisms underlying language processing, both in the brain and in LLMs.

\subsection{Explanations Outperform Internal Representations in Language-Related Regions }\label{explanaitions_outperform}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig3.pdf}
    \caption{\textbf{Comparing Brain Scores from Attribution and Internal Representations.}
    \textbf{a,} Voxels predicted significantly by only attribution (red), internal representations (blue), or both (light purple). The encoder was fitted on each pair of feature spaces and LLMs independently. To identify significantly predicted voxels, we concatenated the brain scores of corresponding voxels from the same feature spaces regardless of the examined LLM, and applied the Wilcoxon signed-rank test. The significance threshold was set at 0.05 for $P$ value and corrected for multiple comparisons using FDR.
    \textbf{b,} Brain scores of feature spaces across ROIs. Alignment is expressed as brain score, normalized and presented as a percentage of the noise ceiling. Markers represent the mean brain score (as \% of noise ceiling) for each ROI. Brain scores were computed independently for each combination of feature space, LLM, and individual, then averaged across LLMs and participants. Error bars indicate the 95\% confidence interval across individuals. Marker colors match the brain map, indicating the corresponding brain region, while error bar colors represent different feature spaces. Results for right-hemisphere language ROIs are shown in Fig. \ref{fig:s_feature_space_comparison_R}. 
    }
    \label{fig: Feature space comparison}
\end{figure*}

After evaluating the performance of attribution-based explanations in predicting brain activity, we compared their efficacy with internal representations (activations and attention), which are widely used in prior literature \cite{caucheteux2021, caucheteux2023, schrimpf2021, Lammarre2022, caucheteux2022brains, goldstein2022}. Attribution methods quantify how a model’s activations change in response to input when predicting the next word in a sequence. Based on this, we hypothesize that attribution explanations would show stronger performance in brain regions that are more sensitive to input changes. Additionally, since attribution highlights word-specific importance, it is expected to primarily capture lower-level linguistic features compared to activations.

To test this hypothesis, we independently trained the encoder with activations, attention, and attribution from each LLM to predict fMRI brain activity. To summarize the overall performance of each feature space, we aggregated brain scores across all LLMs and stories and identified voxels where brain scores were significantly greater than zero.

Although attribution-predicted regions showed considerable overlap with those identified by activations Fig. \ref{fig: Feature space comparison}a), notable differences emerged in encoding performance across brain areas (Fig. \ref{fig: Feature space comparison}b). As expected, attribution outperformed internal representations in primary language processing areas, including Heschl’s gyrus and Heschl’s sulcus, which are essential for fine-grained auditory processing \cite{MOROSAN2001684_heschle}. Interestingly, attribution also performed well in regions near the lateral fissure and the subcentral gyrus, areas known for integrating auditory, language, and sensorimotor information \cite{insula, namkung2017insula,arana2020subcentral} In contrast, activations exhibited stronger alignment with higher-level language processing areas, such as the inferior frontal gyrus (IFG) and angular gyrus, which are associated with semantic reasoning and abstract language comprehension \cite{IFG,AG}.

Despite having a significantly lower dimensionality (10 features per word) compared to activations (768) and attention weights (1536), attribution methods retained strong predictive power, suggesting that they encode brain-relevant linguistic information in a more compact and efficient representation. This contrast highlights a fundamental distinction: attribution methods excel at capturing syntactic, dynamic, and integrative processes, whereas high-dimensional internal representations, such as activations, are better suited for encoding rich, conceptual details necessary for advanced language comprehension.
Together, these findings suggest that attribution-based explanations provide a complementary perspective on the brain’s information integration processes, shedding light on how neural representations dynamically adapt to input changes and how specific stimulus elements contribute to predictive mechanisms in language comprehension.


\subsection{Explanations Reveal the Hierarchy of Language Processing in the Brain}\label{explanations_predict_hierarchy}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig4.pdf}
    \caption{\textbf{Hierarchical alignment between GPT-2 layer conductance and brain activity.} 
    \textbf{a,} Layer preference per voxel based on conductance scores. The encoder was inputted with the conductance of each GPT-2 layer independently, and significant voxels ($P$ $<$ 0.01, Wilcoxon signed-rank test with FDR correction) are color-coded by the layer whose conductance provided the best prediction accuracy. 
    \textbf{b,} Distribution of layer preference per voxel within brain regions involved in language processing. The percentage of voxels within language-related brain regions (Heschl's gyrus and Heschl's sulcus, superior temporal gyrus, superior temporal sulcus, inferior frontal gyrus, and angular gyrus) is plotted for each layer’s conductance, showing a hierarchical organization that aligns model layers with language-processing stages. Early model layers predominantly predict auditory regions, while higher layers align with regions supporting complex language functions. 
    \textbf{c,} Relationship between layer-wise prediction performance and brain alignment. The percentage of brain voxels aligned with each layer (blue) was compared with the percentage of words most influenced by each layer (orange), based on conductance scores. The correlation (Pearson’s \textit{r} = 0.97, $P = 2.2 \times 10^{-7}$) demonstrates a strong link between the importance of model layers in language representation and their predictive relevance for brain activity. 
    \textbf{(d)} Distribution of layer importance for words clustered by part of speech (POS). Words from different stories are aggregated and grouped by POS categories (e.g., nouns, verbs, adjectives).}
    \label{fig:conductance}
\end{figure*}

Having established that attribution, as a representative XAI method, effectively explains brain activity during language processing, we now examine how such explanations can enhance our understanding of the relationship between LLMs and the brain. Both systems process language hierarchically, and prior research has demonstrated hierarchical alignment between LLM layer representations and language-related brain regions \cite{caucheteux2022brains, caucheteux2023, millet2022, kumar2024shared}. Here, we extend this investigation to LLM's explanations, exploring whether their hierarchical structure can also predict brain activity.

A key distinction of attribution-based explanations is their ability to capture the causal influence of specific layers on model predictions, whereas activations aggregate information across layers without explicitly attributing influence. Given this fundamental difference, we hypothesize that if explanations exhibit hierarchical alignment with brain activity, the explanations derived from earlier model layers should demonstrate strong predictive performance, as these layers play a crucial role in shaping downstream predictions \cite{zhang2024investigatinglayerimportancelarge, ferrando2024informationflowroutesautomatically}.

To test this hypothesis, we employed conductance \cite{dhamdhere2018}, an attribution method derived from integrated gradients \cite{sundararajan2017axiomatic}, which quantifies the contribution of individual model units (e.g., neurons) to specific predictions. For each GPT-2 layer, we aggregated conductance scores across its neurons to construct layer-specific feature spaces, which were then independently evaluated for their ability to predict brain activity. Our findings reveal that layer conductance feature spaces predict brain activity with an accuracy reaching up to 60\% of the noise ceiling, closely matching the performance of other attribution methods (Fig. \ref{fig:suppfig6_conductance_sig_voxels} and \ref{fig:suppfig7_conductance_vs_othres}).


We assessed hierarchical alignment by identifying each brain voxel’s preferred layer—the model layer whose conductance yielded the highest brain score (Fig. \ref{fig:conductance}a). To examine how this preference varies across the brain, we analyzed the distribution of preferred layers within language-related regions. This analysis uncovered a striking hierarchical pattern: voxels in lower-level language areas, such as the auditory cortex and STG, were best predicted by early layers of GPT-2. As regions became increasingly distant from the auditory cortex and engaged in higher-order language processing, we observed stronger alignment with deeper model layers, particularly in the IFG and angular gyrus (Fig. \ref{fig:conductance}b). Notably, while previous studies have shown that the best encoding performance is obtained from representations in the middle layers of language models \cite{kumar2024shared, toneva2019, caucheteux2022semantic, caucheteux2022brains}, our explanation-based approach reveals that both early and late layers contribute significantly to brain alignment.

To further validate this finding and ensure that it reflects the functional role of model layers in language processing, we examined the information captured by conductance explanations across entire stories. Specifically, we computed importance scores by averaging all elements of the conductance vector for each layer and word, resulting in a single importance score per layer and word. This allowed us to determine the most influential layer for each word in a story.

The distribution of preferred layers across story words revealed a consistent trend: early and late layers exhibited the highest importance for next-word prediction. We then compared the distribution of layer importance across words with the distribution of layer preferences across voxels throughout the entire cortex. Strikingly, we found a strong correlation between these two distributions (Pearson correlation = 0.97, $P = 2.2 \times 10 ^{-7}$) (Fig. \ref{fig:conductance}c), a result that remained robust when analyzing individual stories and brain datasets separately (Fig. \ref{fig: fig.S8_importance_alignment_datasets}).

To rule out the possibility that layer importance patterns were merely driven by word frequency or specific linguistic categories, we clustered words based on their part-of-speech (POS) tags and examined layer importance within each category. The results remained consistent: across most POS categories, the first and last layers exhibited the highest importance (Fig. \ref{fig:conductance}d), indicating that their contributions extend beyond specific word types and reflect a more general role in language processing. 
Interestingly, early GPT-2 layers showed a stronger influence on content words, whereas function words were predominantly influenced by later layers. This pattern aligns with prior findings on layer-specific processing in LLMs, suggesting that early layers play a crucial role in structuring linguistic content \cite{zhang2024investigatinglayerimportancelarge}, while later layers integrate broader contextual dependencies \cite{late_layers_dependencies}.

Taken together, these results demonstrate that explanations are hierarchically aligned with brain activity, providing new insights into the role of both early and late model layers in language processing.

\section{Discussion}\label{discussion}
In this study, we introduce explanations from LLMs as a novel tool to bridge the gap between language processing in the brain and computational models. Our analyses across multiple attribution methods and LLMs reveal that gradient-based attribution methods provide a robust foundation for modeling brain activity during natural language comprehension. Compared to traditional internal representations like activations and attention, attribution-based explanations demonstrate stronger alignment with early-stage language processing regions in the brain. Furthermore, we observe a hierarchical correspondence, where attributions from different model layers predict brain regions involved in parallel stages of language processing. 

The neuroscientific contributions of these findings emerge from the fundamental differences between attribution-based explanations and previously used internal representations.

First, attribution methods reveal how a model’s internal representations evolve in response to changes in the input. Unlike activations, which mainly reflect how the model encodes the current input state, gradient-based attribution methods calculate the gradient of the model’s output with respect to its input. This gradient captures how small changes in the input influence the model’s decisions, providing insight into the dynamics of the model’s internal representations. While prior work has demonstrated a strong correspondence between model activations and brain activity, our findings suggest LLMs and the brain not only encode language in a similar way but also exhibit parallel dynamics in how their representations adapt to changing input. The stronger alignment of attribution scores with early auditory processing areas, such as Heschl’s gyrus and the STG, indicates that attributions more effectively capture the dynamics of lower-level linguistic features and local variations in the input.

Second, attribution methods quantify how much each previous word contributes to the model’s next-word prediction. A preliminary study \cite{russo2022} explored this idea, showing that saliency-based scores could predict brain activity, interpreted as evidence of a shared weighting mechanism between the brain and LLMs for integrating previous context. Our findings confirm and expand upon this interpretation by testing a broader range of attribution methods across multiple language models and brain regions. Specifically, our results demonstrate that attribution-based feature spaces strongly predict brain activity in regions associated with higher-order information integration, such as the insula and subcentral areas, providing additional evidence for this shared weighting mechanism.

Third, attribution-based explanations inherently encode information about the next word in a sequence, as they measure how input words influence a specific target prediction. In contrast, internal representations capture both predictive and broader linguistic information—such as background knowledge and abstract reasoning \cite{Webb2023, Skean2024}—making them more generalizable across diverse tasks. However, this generalizability makes internal representations less tied to next-word prediction and, therefore, less reliable for studying predictive processing in the brain \cite{antonello2024predictive}.

This limitation becomes evident when comparing brain alignment across model layers. Prior studies have shown that peak brain alignment occurs in middle-layer representations \cite{mischler2024contextual, caucheteux2022brains, toneva2019}, rather than in deeper layers, which contain the highest predictive information. This suggests that strong alignment between internal representations and brain activity does not necessarily indicate predictive coding but may instead result from the greater generalizability of middle-layer representations \cite{antonello2024predictive}. By contrast, our layer-wise analysis using conductance reveals a strong correlation between a layer’s importance for next-word prediction and its brain alignment. This pattern arises because conductance not only captures stimulus-driven information within a layer but also quantifies its causal influence on the model’s next-word prediction. These findings establish attribution-based explanations as a more precise tool for investigating predictive coding in the brain.

Beyond their neuroscientific implications, our findings provide a new framework for evaluating explanations in XAI. A key challenge in XAI is determining how effectively these explanations improve user understanding of AI systems. This is typically assessed through behavioral tests \cite{rim2024human,biessmann2021turingtesttransparency,doshivelez2017rigorousscienceinterpretablemachine,alufaisan2021explainable}, but these tests often produce inconsistent results due to biases like confirmation bias and label leakage \cite{colin2022human}. They also face practical limitations, such as the need for controlled experiments and participant training. Our approach suggests brain alignment as an alternative, objective evaluation criterion for explanations.

Explanations that exhibit stronger alignment with brain activity may also be more comprehensible to users, as they reflect the cognitive processes engaged during naturalistic task performance. This brain-centered approach provides ecological validity through neural recordings in realistic contexts, scalability by reusing brain data across multiple XAI methods, and objectivity via a quantifiable measure of explanation quality, free from subjective human judgments. By shifting from behavioral validation to neurobiological alignment, this framework provides a scalable, unbiased method for evaluating XAI explanations, bridging cognitive science and AI explainability research.

Our study opens several avenues for future research. While substantial research has linked deep learning models' internal representations to brain activity, the alignment of explanation methods with neural function remains underexplored. Future work should expand this investigation by examining a broader range of models, attribution techniques, and brain recording modalities to identify the factors shaping this alignment, such as context length and word positioning. Understanding these factors could provide deeper insights, such as identifying patterns of input context that exert a stronger influence on brain representations or pinpointing predictive brain modules specialized for different types of linguistic predictions, such as those related to distinct parts of speech. Moreover, this brain-explanation alignment approach is not limited to language processing and could be extended to other cognitive domains, including vision and auditory perception, as well as multimodal models that integrate multiple sensory inputs. From an XAI perspective, future research should quantitatively compare the effectiveness of brain-based evaluation frameworks with traditional behavioral evaluation methods to assess their relative strengths and limitations.

\section{Methods}
\label{sec:methods}

\subsection{Brain Representation}\label{methods_brain_rep}
We utilized three preprocessed fMRI datasets from the Narratives collection \cite{nastase2021narratives} for our study: ``Pieman", ``Shapes", and `` `Slumlord' and `Reach for the Stars One Small Step at a Time' ".  
The ``Pieman" dataset includes recordings from 86 participants (ages 18–45, mean age 22.5 ± 4.3 years, 45 reported female) who listened to a 7:02-minute story (282 TRs, 957 words). The ``Shapes" dataset consists of recordings from 58 participants (ages 18–35, mean age 23.0 ± 4.5 years, 42 reported female) and features two distinct auditory descriptions of the animation ``When Heider Met Simmel": ``shapesphysical", which provides a purely physical account of the animation, and ``shapessocial", which conveys intentionality in the movements of the shapes. We used the ``shapessocial" condition in our analysis, which lasts 6:45 minutes (270 TRs, 910 words). The `` `Slumlord' and ``Reach for the Stars One Small Step at a Time' " dataset comprises recordings from 18 participants (ages 18–27, mean age 21.0 ± 2.3 years, 8 reported female) who listened to two consecutive stories within the same scanning run. The ``Slumlord" story is 15:03 minutes long (602 TRs, 2,715 words), while ``Reach for the Stars One Small Step at a Time" is 13:45 minutes long (550 TRs, 2,629 words). 

Following the data quality guidelines from the original paper, we excluded noisy recordings, resulting in a total of 150 valid recordings from 147 unique individuals, as three participants contributed to more than one dataset in separate scanning sessions. 

We used the preprocessed unsmoothed data without applying any additional preprocessing steps. Preprocessing in the Narratives collection was performed using the fMRIPrep pipeline, which included susceptibility distortion correction, slice-timing correction, spatial normalization to the ``fsaverage'' brain template, and projection onto the cortical surface. Both smoothed and unsmoothed versions of the data are available, and no temporal filtering was applied. The fMRI data were sampled at a repetition time (TR) of 1.5 seconds.

\subsubsection{Brain Parcellation}\label{methods_brain_rep_parcel}
All brain mapping analyses were conducted at the voxel level. However, for certain comparisons, we computed the average brain score across voxels within ROIs (Fig. \ref{fig: Feature space comparison}b and \ref{fig:conductance}b) to facilitate clearer interpretation. These ROIs were defined by grouping brain voxels based on their anatomical location using the Destrieux Atlas \cite{Destrieux2010}, which provides a standardized parcellation of the cortex into 75 regions per hemisphere. The full names of ROIs abbreviated in our analyses are listed in Table \ref{tab:roi_abbreviations}.

\begin{table}[h]
\centering
\caption{List of abbreviated ROIs and their full anatomical names based on the Destrieux Atlas. These ROIs were used to compute average brain scores across voxels within each region.}
\begin{tabular}{ll}
\hline
\textbf{Abbreviation} & \textbf{Full Name}                  \\  
\hline
HS          & Heschl's Sulcus                 \\  
HG          & Heschl's Gyrus                  \\  
STG         & Superior Temporal Gyrus         \\  
STS         & Superior Temporal Sulcus        \\  
IFG         & Inferior Frontal Gyrus          \\  
Lateral S   & Lateral Sulcus (Sylvian Fissure) \\  
\hline
\end{tabular}
\label{tab:roi_abbreviations}
\end{table}

\subsection{Large Language Model Representations}\label{methods_LM_rep}
The alignment of brain activity with both lexical and contextual embeddings (activations) from various pre-trained deep language models has been extensively explored in prior research \cite{schrimpf2021, caucheteux2022brains}. Additionally, studies have highlighted the potential of attention weights and transformations from models like GPT-2 and BERT for predicting brain activity in response to identical inputs \cite{kumar2024shared, Lammarre2022}.

In this study, we employed attribution scores to explain the outputs of language models and compared these scores to fMRI recordings from participants exposed to the same input sentences. Specifically, we focused on three language models—GPT-2 (124 million parameters), Phi-2 (2.7 billion parameters), and Llama 2 (7 billion parameters)—chosen based on their demonstrated brain alignment, language performance, and architectural diversity.

We used pre-trained models available on \texttt{Hugging Face} \cite{hugging_face} and extracted representations for each word in the input stories based on their preceding context. This process involved constructing sequences where each sequence included a target word along with a specified number of preceding words. Tokenization was performed using the \texttt{Hugging Face} auto tokenizer, and the tokenized sequences were fed into the models to extract word-level representations.

\subsubsection{Feature Attribution}\label{methods_LM_rep_attr}
Feature attribution is a method used to explain the predictions of machine learning models by assigning importance scores to each input feature. In this study, we utilized feature attribution to investigate the relationship between language model outputs and brain activity. This approach allowed us to evaluate the influence of preceding words on the model’s next-word predictions and compare it to human predictive processing.

We evaluated four different attribution methods: Gradient Norm, Gradient $\times$ Input, Integrated Gradients, and Erasure. 
These methods served as explanatory representations of the models, enabling an in-depth analysis of the alignment between model predictions and neural activity.
\vspace{0.25cm} \\

\textit{Gradient Norm}: 
The Gradient Norm method \cite{simonyan2013deep_gradn,li2015visualizing_grad_n} measures the sensitivity of the model’s output to changes in the input token embeddings $x$. The importance score for each token is computed as the $L_1$ norm of the gradient of the output with respect to the token embedding:
\begin{equation}
    \label{eq:grad_norm}
    s(x_i) = \|\nabla_{x_i} f(x)\|_1
\end{equation}
where $f(x)$ is the model’s output, $\nabla_{x_i}$ denotes the gradient with respect to token $x_i$, and $\|\cdot\|_1$ represents the $L_1$ norm.                   \vspace{0.25cm} \\

\textit{Gradient $\times$ Input}:  
The Gradient $\times$ Input method \cite{denil2015} combines gradient-based sensitivity analysis with the magnitude of the input embeddings. It computes the importance score for a token as the dot product of the gradient of the model’s output and the token embedding:
\begin{equation}
    \label{eq:grad_x_input}
   s(x_i) = \nabla_{x_i} f(x) \cdot x_i
\end{equation}
To improve interpretability, the $L_2$ norm can be applied to the scores \cite{atanasova2020diagnostic}:
\begin{equation}
    \label{eq:grad_x_input_l2}
    s(x_i) = \| \nabla_{x_i} f(x) \cdot x_i \|_2
\end{equation}
where $\nabla_{x_i} f(x)$ is the gradient of the model’s output with respect to $x_i$, and $\cdot$ represents the dot product.
\vspace{0.25cm} \\                                                                                                       

\textit{Integrated Gradients}:
Integrated Gradients \cite{sundararajan2017axiomatic} address the saturation problem by attributing importance along a path from a baseline $x'$ (e.g., zero or [MASK] embeddings) to the actual input $x$. The importance score is defined as:
\begin{equation}
    \label{eq:ig_ing}
    s(x_i) = (x_i - x_i') \cdot \int_{\alpha=0}^{1} \nabla_{x_i} f(x' + \alpha (x - x')) \, d\alpha
\end{equation}
The integral is often approximated using a Riemann sum:
\begin{equation}
    \label{eq:ig_summation}
    s(x_i) \approx \frac{1}{m} \sum_{k=1}^{m} \nabla_{x_i} f\left(x' + \frac{k}{m} (x - x')\right) \cdot (x_i - x_i')
\end{equation}
where $m$ is the number of interpolation steps, and $\nabla_{x_i} f(x)$ is the gradient of the output with respect to $x_i$. For the implementation of Integrated Gradients, we used the \texttt{Captum} library \cite{captum_software,captum_miglani2023using}.
\vspace{0.25cm} \\

\textit{Erasure}:
Erasure-based methods \cite{li2016understanding_erasure} assess the importance of a token by removing or masking it and observing the change in the model’s output. The importance score is given by:
\begin{equation}
  \label{eq:Erasure}
  s(x_i) = f(x) - f(x_{-i})
\end{equation}
where $f(x)$ is the model’s output for the original input, and $f(x_{-i})$ is the output after removing or masking token $x_i$.
\vspace{0.25cm} \\

\textit{Feature Space Design Using Word Attribution}:
To compute LLM attributions, we segmented the input story into overlapping sequences using a sliding window approach. The window length was chosen based on prior work showing that embeddings of upcoming words can be reliably decoded from brain activity up to a second before they are perceived \cite{goldstein2022}. To capture this predictive structure, we used a sliding window spanning approximately one second of linguistic input, containing 10 words per window. Each window included the target word along with its 9 preceding words, with a 9-word overlap between consecutive windows.

For each input sequence, we computed importance scores for every token using the aforementioned attribution methods. These methods output a vector per token. To derive a single per-token score, we computed the $L_1$ norm for all methods except Gradient $\times$ Input, for which we used the $L_2$ norm, as this approach has demonstrated superior interpretability \cite{atanasova2020diagnostic}. Tokens belonging to the same word (e.g., 'not' and 'ebook' in the tokenized representation of 'notebook') were summed to calculate a unified importance score for that word.

Since the sliding windows overlap, each word in the story is processed within 10 different windows. This results in multiple importance scores being assigned to a single word. To represent these scores, we stored all computed scores for each word in a vector associated with that word. Words appearing in fewer than 10 windows (i.e., the first and last 10 words of the story) were excluded to ensure consistent processing.

This methodology produced a feature matrix of dimensions $ (W-20) \times 10$, where $W$ denotes the total number of words in the story, and the second dimension corresponds to the feature scores derived from the overlapping windows.

\subsubsection{Layer Conductance}\label{methods_LM_rep_con}

Layer conductance is an extension of attribution methods that provides a deeper understanding of the internal functioning of neural networks \cite{dhamdhere2018}. While traditional attribution methods focus on identifying which input features most influence predictions, layer conductance measures the contribution of individual hidden units within the network. This method quantifies how information flows through neurons and accumulates across layers. It is built upon Integrated Gradients \cite{sundararajan2017axiomatic} and assigns attributions of input features, computed by Integrated Gradients, to neurons in each layer via the chain rule.
The conductance of neuron $y$ for an input feature $i$ is defined as:
\begin{equation}
  \label{eq:pixel-conductance}
  cond_i^{y}(x) = (x_i - x'_i) \cdot \int_{\sparam=0}^{1} 
  \frac{\partial f(x' + \alpha (x - x'))}{\partial y} \cdot 
  \frac{\partial y}{\partial x_i} \, d\alpha.
\end{equation}
Here, $x$ is the input, $x'$ is the baseline, and $f$ is the model. The term $cond_i^{y}(x)$ quantifies how much of the input contribution to the prediction passes through neuron $y$, with the total conductance for a layer computed by summing the importance scores of all neurons within that layer.

In this study, we used layer conductance to construct a feature space, employing a moving window approach similar to the one used for feature attribution. To compute conductance scores, we used the \texttt{Captum} library, which provides a set of conductance values for each word in the input sequence, corresponding to the contributions of different model layers. For example, when a sentence is fed into GPT-2, \texttt{Captum} returns a vector of 12 conductance scores per word, where each value represents the contribution of a specific model layer for that word. The conductance values were computed for each input window, resulting in vectors per layer with the same dimensions as the window. Consequently, each word in the story was represented by a vector capturing its conductance scores across all the windows in which it appeared. By concatenating these vectors, we constructed the layer conductance feature space, representing the model's internal dynamics concerning the input data.

Conductance also serves as a metric for quantifying neuron importance. While Equation \ref{eq:pixel-conductance} characterizes the function of a neuron in terms of its effect on the input, Equation \ref{eq:cond_total} evaluates the overall importance of a neuron by aggregating its influence across all input features.
\begin{equation}
  \label{eq:cond_total}
  cond^{y} = \sum_{i} (x_i - x'_i) \cdot \int_{\sparam=0}^{1} 
  \frac{\partial f(x' + \alpha (x - x'))}{\partial y} \cdot 
  \frac{\partial y}{\partial x_i} \, d\alpha.
\end{equation}
Equation \ref{eq:cond_total} extends Equation \ref{eq:pixel-conductance} by summing the conductance values over all input features, providing a measure of a neuron's overall importance when processing an entire sentence. This aggregated conductance score represents the neuron's contribution to the model’s prediction at a broader level.

For our layer importance analysis, our objective was to determine the importance of different layers when processing an entire story. Additionally, we aimed to obtain importance scores at the word level, allowing us to analyze patterns of layer importance across different words in the story. By summing over the conductance values obtained for each word in a given layer, we derived a single importance score per layer per word, which we then used to assess the role of different layers in language processing.

We evaluated these importance scores across four different stories and the distribution of layer importance across words aligned with recent findings on the hierarchical processing of LLM layers in next-word prediction tasks \cite{zhang2024investigatinglayerimportancelarge,ferrando2024informationflowroutesautomatically}. This alignment supports the validity of our approach for computing layer importance. 

To ensure that layer importance was not an artifact of the specific words appearing in the input stories, we conducted a control analysis. Specifically, we wanted to distinguish whether a layer's importance was a general property of the model or if it was driven by the occurrence of certain word types that are inherently more influential for that layer. If certain word types had disproportionately high conductance, the observed layer importance could be due to these words rather than an intrinsic property of the layer itself.

To test this, we clustered the words in our dataset based on their POS tags using the \texttt{spaCy} transformer-based POS tagging model (\texttt{en\_core\_web\_trf}) \cite{honnibal2020spacy}. We then examined whether any specific POS category exhibited a systematically higher layer importance. Our results showed that layer importance remained consistent across all POS categories, indicating that the importance of a layer is robust and independent of the specific words present in our input sequences. This confirms that our computed layer importance scores reflect the model’s intrinsic hierarchical processing rather than biases introduced by specific word distributions.

\subsubsection{Attention Weights}\label{methods_LM_rrep_atten}
In Transformer-based language models, the attention mechanism allows the model to focus on the most relevant parts of an input sequence. Each attention head computes a distinct set of attention weights, represented as square matrices known as attention maps. These maps quantify how strongly each token attends to another. Given a sequence of \( n \) tokens, the attention map for each head is denoted as \( M \in \mathbb{R}^{n \times n} \), where each element \( m_{i,j} \) represents the attention weight from token \( i \) to token \( j \) in that head.

While previous studies have primarily used activations to encode brain activity, recent research has shown that attention weights from models like GPT-2 and BERT also correlate with brain activity during language comprehension tasks \cite{Lammarre2022, kumar2024shared}. We selected attention weights as one of the internal representations of LLMs to compare their brain alignment in conjunction with attribution methods and activations.

To derive the attention-based feature space, we applied the same sliding window procedure used for attribution measurements. Following \cite{Lammarre2022}, we fed the model input sequences of length 11 and extracted attention maps from all layers. In line with their approach, we computed the column-wise mean of \( M \) to obtain a feature vector representing the average attention received by each token in the input sequence. For tokens that were part of the same word (e.g., subword tokens created during tokenization), we summed their averaged attention weights to derive a single score per word.

We then combined these word-level feature vectors across all attention heads and layers. The size of this feature space varies by model, depending on the number of layers and attention heads. For instance, GPT-2 consists of 12 layers, each with 12 attention heads. Since each attention head generates a feature vector of length 11 (corresponding to the input sequence length), the resulting attention weight feature space has dimensions \( W \times 1584 \), where \( W \) is the total number of words in the story.

\subsubsection{Activations}\label{methods_LM_rrep_atten}
Each layer in a language model computes a contextual hidden state, which represents each token as a vector incorporating information from its surrounding context. Consistent with prior research \cite{goldstein2022, caucheteux2022brains, schrimpf2021, caucheteux2023, caucheteux2022semantic}, we utilized these contextual hidden states as one of our feature spaces.

To construct this feature space, we applied a sliding window to the story, where each window consisted of the tokens from the story and their preceding $l$ tokens. The window length $l$ was set to 1024 for GPT-2 and Llama 2, and 2048 for Phi-2. The models were then fed with these input sequences, and the corresponding hidden states were extracted, resulting in a matrix of dimensions $[\text{embedding size} \times \text{number of input tokens}]$.

From the hidden state matrix of each sequence, we selected the hidden state corresponding to the last token, resulting in a vector whose length equals the embedding size of the model. These vectors were subsequently concatenated across all layers of the model, producing a feature space for each token in the input story with dimensions $[\text{embedding size} \times \text{number of model layers}]$. For each word, the vectors corresponding to its tokens were summed. The resulting word-level matrices were then combined to represent the entire story.

The final feature space for each model was constructed with dimensions $[\text{embedding size} \times \text{number of words in the story} \times (\text{number of model layers} + \text{one initial embedding layer})]$. Specifically, for GPT-2, Phi-2, and Llama 2, the feature space dimensions were $768 \times \text{number of story words} \times 13$, $2560 \times \text{number of story words} \times 33$, and $4096 \times \text{number of story words} \times 33$, respectively.

In this study, we focused on the activations from layer 8 of GPT-2 and layer 12 of Llama 2, as previous research \cite{caucheteux2022brains, mischler2024contextual} demonstrated that these layers exhibited the highest alignment with brain activity. For Phi-2, we selected layer 12 based on preliminary tests with different layers.

\subsection{Aligning Brain and Large Language Model Representations}\label{methods_encoder}
Our objective was to compare the brain alignment of explanatory and internal representations of LLMs in response to natural language stimuli. To accomplish this, we employed a regression-based approach to predict fMRI recordings at each voxel for each participant using the language model representations of the stimuli.

Following the methodology in \cite{Huth2016}, we matched the sampling frequency of the fMRI recordings (TR = 1.5s) with the language model representations. Representations of words presented during the same TR were summed. To account for the hemodynamic delay of approximately 6 seconds in the fMRI signal, we applied a finite impulse response (FIR) model with six delays to capture the slow BOLD response. This approach concatenated the features of the six preceding TRs with the current features. We then standardized the features and applied principal component analysis (PCA) exclusively to the attention feature space due to its high dimensionality, selecting the top 20 components for further analysis. Ridge regression with cross-validation (\texttt{RidgeCV}) from the \texttt{scikit-learn} library \cite{pedregosa2011scikit} was employed to fit the regression model.

To evaluate the brain alignment of each feature space, we calculated the Pearson correlation between the actual and predicted values of the BOLD signal for each voxel. This correlation served as the ``brain score'', a metric used in prior studies with similar methodologies \cite{yamins2014, Huth2016, caucheteux2022brains, caucheteux2022semantic}. We implemented 5-fold cross-validation to ensure the reliability and generalizability of the results.

\subsection{Significance Analysis}\label{methods_statistical_test}
We performed a Wilcoxon signed-rank test for each voxel across individuals to assess whether the brain score was significantly greater than zero. 

To compare brain scores across different feature spaces and models, we first identified voxels with significant predictions. Next, we applied a multiple comparison procedure using the Friedman test to compare brain scores within voxels that were significantly predicted by all models or feature spaces. This step allowed us to identify voxels where at least one method exhibited a significantly different brain score compared to the others. For voxels identified as having significant differences, we conducted pairwise comparisons using a one-sided Wilcoxon test to determine which method achieved the best brain score. 

To correct for multiple comparisons, we adjusted all $P$ values using the false discovery rate (FDR) method, following the Benjamini-Hochberg (BH) procedure \cite{benjamini1995controlling}.

\subsection{Noise Ceiling}\label{methods_noise_ceiling}

 The recorded fMRI signal consists of three main components \cite{Nastase_isc}: (i) the stimulus-related signal, which reflects neural activity consistently evoked by the stimulus across participants, typically in stimulus-relevant brain regions; (ii) the idiosyncratic signal, capturing individual-specific responses influenced by factors such as emotional state, memory, or prior experiences; and (iii) noise, which includes any variability unrelated to the stimulus, such as physiological fluctuations and scanner artifacts.

 To evaluate the performance of an encoding model, it is crucial to isolate the explainable variance, primarily driven by the stimulus-related signal. This is done using inter-subject correlation (ISC) \cite{Nastase_isc}, which measures the consistency of stimulus-evoked signals across individuals. ISC computes the correlation between the time series of each voxel in one individual and the average time series of the corresponding voxels in other individuals.

For noise ceiling estimation, we followed \cite{kumar2024shared} by applying ISC within a five-fold cross-validation framework. For each fold, we computed the correlation between each individual's voxel responses and the average voxel responses of the other individuals within the same fold. This resulted in five correlation values per voxel for each individual, and the noise ceiling for each voxel was computed as the mean correlation across all folds and individuals.

Unlike leave-one-out methods, this approach includes the target individual in the average to account for autocorrelation, yielding a more conservative (higher) estimate of the noise ceiling \cite{kumar2024shared}. In some analyses, we expressed the encoding model's performance as a percentage of its correlation relative to this upper bound.

\additionalheadings{Acknowledgments} 
We thank Richard Antonello and Alona Fyshe for their valuable discussions and constructive feedback on the manuscript.

\bibliography{refs}% common bib file
%% if required, the content of the .bbl file can be included here once bbl is generated
%%\input sn-article.bbl
\clearpage
% % Adjust the negative value as needed
\FloatBarrier 
\section*{Extended Data}
\vspace*{-1em}
\renewcommand{\thefigure}{S\arabic{suppfigure}} \setcounter{suppfigure}{0}
\begin{suppfigure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/s_LLMs_R.pdf}
    \caption{\textbf{Brain score across attribution methods and LLMs in the right hemisphere.} 
    \textbf{a,} Voxel-wise brain scores for each combination of attribution method, LLM, and individual, averaged across individuals and voxels within the right hemisphere. This panel is analogous to Fig. \ref{fig: Attribution predict}b but for the right hemisphere.  
    \textbf{b,} Brain scores of LLMs across right-hemisphere ROIs. Alignment is expressed as brain score normalized and presented as a percentage of the noise ceiling, estimated using intersubject correlation. This panel mirrors Fig. \ref{fig: Attribution predict}c. Markers represent the mean brain score (as \% of noise ceiling) for each ROI. Error bars indicate the 95\% confidence interval across individuals. Marker colors match the brain map, indicating the corresponding brain region, while error bar colors represent different LLMs.}
    \label{fig:S_LLM_attribution_R}
\end{suppfigure}
\vspace*{-1.5em}
\begin{suppfigure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/s_noise_ceiling.pdf}
    %\includesvg[width=1\textwidth]{}
    \caption{\textbf{Noise Ceiling Estimation.} \textbf{a,} Noise ceiling values for each voxel, estimated as the average ISC across all individuals. \textbf{b,} Noise ceiling values for each ROI, calculated by averaging voxel-wise noise ceiling estimates within each ROI. Boxplots represent the distribution of averaged noise ceiling values across individuals. \label{fig:S_noise_ceiling}}
\end{suppfigure}
\vspace*{-1.5em}
\begin{suppfigure} [H]
    \centering
    \includegraphics[width=0.7 \textwidth]{figures/s_feature_space_comparison_R.pdf}
    \caption{\textbf{Brain scores of feature spaces across right-hemisphere ROIs.} Alignment is expressed as brain score, normalized and presented as a percentage of the noise ceiling, estimated using ISC. This figure mirrors Fig. \ref{fig: Feature space comparison}b. Markers represent the mean brain score (as \% of noise ceiling) for each ROI. Error bars indicate the 95\% confidence interval across individuals. Marker colors match the brain map, indicating the corresponding brain region, while error bar colors represent different feature spaces.}
    \label{fig:s_feature_space_comparison_R}
\end{suppfigure}

\begin{suppfigure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/s_feature_spaces_LLMs.pdf}
    \caption{\textbf{Brain score obtained with attribution methods and internal representations.} Scores are computed for each voxel and individual independently, and average across all individuals. Only significant predicted voxels are color coded (FDR corrected, $P < 0.05$). \label{fig:suppfig4}}
\end{suppfigure}

\begin{suppfigure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/s_con_brainscore_brainmap.pdf}
    \caption{\textbf{Brain scores predicted by GPT-2 layer conductance.} Brain scores were computed independently for the conductance of each GPT-2 layer, for each voxel, and for each individual. The statistical test (Wilcoxon signed-rank) was applied separately for each layer, and voxels significantly predicted by at least one layer were color-coded (FDR corrected, $P < 0.05$). The color bar represents the average brain score across all layers.\label{fig:suppfig6_conductance_sig_voxels}}
\end{suppfigure}

\begin{suppfigure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/s_cond_vs_others.pdf}
    \caption{\textbf{Comparing brain scores of layer conductance with attribution methods and internal representations for GPT-2 across ROIs, presented separately for the left and right hemispheres.} \textbf{a,} Brain scores for the left hemisphere. \textbf{b,} Brain scores for the right hemisphere. Brain scores were normalized and presented as a percentage of the noise ceiling. Brain scores were calculated independently for each combination of feature space and individual, then averaged across individuals. For layer conductance, scores were averaged across all layers of GPT-2. Markers represent the mean brain score (as a percentage of the noise ceiling) for each ROI. Error bars indicate the 95\% confidence interval across individuals. Marker colors correspond to brain regions, while error bar colors represent different feature spaces.\label{fig:suppfig7_conductance_vs_othres}}
\end{suppfigure}

\begin{suppfigure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/s_layer_importance_brain_datasets.pdf}
    \caption{ \textbf{Relationship between layer-wise prediction performance and brain alignment across datasets.} 
The percentage of brain voxels aligned with each layer (blue) was compared with the percentage of words most influenced by each layer (orange), based on conductance scores. Results are shown separately for each dataset, where each dataset corresponds to a story presented both to the language model (for next-word prediction) and to human participants (during fMRI recordings). The correlation within each dataset reflects the relationship between the importance of model layers in processing the story for next-word prediction and their predictive performance in modeling brain activity for the same story.
\label{fig: fig.S8_importance_alignment_datasets}}
    
\end{suppfigure}

\end{document}
