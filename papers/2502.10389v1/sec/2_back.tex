\section{Related Work}
\label{sec.back}

\subsection{Diffusion Models: From U-Net to Transformer}
Diffusion models \cite{Ho2020diffusion, Prafulla2024diffusionbeats, Yang2019Generative, Jascha2015deep} have demonstrated significant capabilities across a range of generative tasks, often outperforming previous methods such as generative adversarial networks (GANs) \cite{goodfellow2014generative} in many downstream applications. Historically, denoising diffusion probabilistic models (DDPMs) \cite{Ho2020diffusion} and more recent models like Stable Diffusion XL \cite{podell2023sdxlimprovinglatentdiffusion} have predominantly utilized convolutional U-Nets \cite{ronneberger2015u} as the backbone. Due to the structure of convolutional networks, it is necessary to maintain the original spatial resolution of the input samples to support operations like pooling, which inherently limits the ability to exploit spatial redundancy during the diffusion process, particularly when attempting to prune the latent samples used as model inputs.

Fortunately, the dilemma has been broken by the emergence of Diffusion Transformer (DiT) \cite{William2023DiT}, which has also been used as the backbone of the SOTA diffusion models like Stable Diffusion 3 \cite{esser2024scalingrectifiedflowtransformers}, Lumina T2X \cite{gao2024luminat2xtransformingtextmodality}, Pixart-Sigma \cite{chen2024pixartsigmaweaktostrongtrainingdiffusion}. The biggest characteristic of DiT is that it completely eliminates the need for Convolutaional U-Net. DiT uses a pure Transformer \cite{vaswani2017attention} architecture and adds conditional information such as prompts with adaptive layer norm. In this way, the positional information is no longer provided by the convolutional operations, and the latent tokens are now positionally independent after position embedding. This enables us to utilize the redundancy we discovered in Section \ref{sec:intro} and select the tokens that are likely to be focused in the current sampling step to compute while caching the predicted of other tokens from the previous step. 


\subsection{Efficient Diffusion Model Inference}
To address the problem of high inference cost in diffusion models, various acceleration techniques have been proposed from different perspectives. A commonly used approach is to reduce the number of sampling steps. Some of these techniques require additional training, such as progressive distillation \cite{salimansprogressive}, consistency models \cite{song2023consistency}, and rectified flow \cite{liu2022flow, lipman2022flow, albergo2022building}. 
Among these methods, rectified flow has been widely used in models like Stable Diffusion 3 \cite{esser2024scalingrectifiedflowtransformers}. It learns the ODE to follow straight paths between the standard normal distribution and the distribution of the training dataset. These straight paths significantly reduce the distance between the two distributions, which in turn lowers the number of sampling steps needed.

There are also training-free methods to either reduce the number of sampling steps or decrease the computational burden within each step. DPM-solver \cite{lu2022dpm}, for instance, introduces a formulation that enhances the solution process of diffusion ODEs. DeepCache \cite{xu2018deepcache}, specifically designed for U-Net-based models, leverages the U-Net architecture to cache and retrieve features across adjacent stages, allowing for the skipping of certain downsampling and upsampling operations during the diffusion process. However, these approaches treat all image regions uniformly, ignoring the varying complexity across different parts of the image. This uniform treatment can lead to significant computational inefficiency, as not all regions require the same level of processing.

As introduced in Section \ref{sec:intro}, the complexity of different regions within an image can vary substantially. 
% Uniform processing overlooks this heterogeneity, resulting in unnecessary computational expense. 
To exploit the characteristics of both the diffusion process and the structure of Diffusion Transformers (DiTs), we introduce \ourmethod{}, a novel approach designed to optimize computation by focusing on the distinctive properties of different image regions. \ourmethod{} is also orthogonal to the methods we mentioned above, 
such as DiTFastAttn \cite{yuan2024ditfastattnattentioncompressiondiffusion} and $\Delta$-DiT \cite{chen2024delta}.