\section{Experiments}
\label{sec.exp}



\begin{table*}[ht]
\centering

\begin{tabular}{lcccccc}
\hline
\textbf{Method} & \textbf{Sample Steps} & \textbf{Sampling Ratio} & \textbf{Throughput (iter/s)$\uparrow$} & \textbf{FID $\downarrow$} & \textbf{sFID $\downarrow$} & \textbf{CLIP score $\uparrow$} \\
\hline
\textbf{Stable Diffusion 3} & & & & & & \\
\hline
\rowcolor{gray!10}
RFlow & 5 & 100\% & 1.43 & 39.70 & 22.34 & 29.84 \\
RAS & 7 & 25.0\% & 1.45 & \textbf{31.99} & 21.70 & \textbf{30.64} \\
RAS & 7 & 12.5\% & 1.48 & 32.86 & 22.10 & 30.55 \\
RAS & 6 & 25.0\% & 1.52 & 33.24 & \textbf{21.51} & 30.38 \\
RAS & 6 & 12.5\% & \textbf{1.57} & 33.81 & 21.62 & 30.33 \\
\hline

\rowcolor{gray!10}
RFlow & 4 & 100\% & 1.79 & 61.92 & 27.42 & 28.45 \\
RAS & 5 & 25.0\% & 1.94 & \textbf{51.92} & \textbf{25.67} & \textbf{29.06} \\
RAS & 5 & 12.5\% & \textbf{1.99} & 53.24 & 26.04 & 28.94 \\



\hline
% \rowcolor{gray!10}
% RFlow & 10 & 0\% & 0.71 & 24.17 & \textbf{15.39} & 31.22 \\
% Ours & 14 & 50.0\% & 0.74 & 23.97 & 16.02 & \textbf{31.32} \\
% Ours & 10 & 25.0\% & \textbf{0.91} & \textbf{23.81} & 16.06 & 31.18 \\
% \hline
\textbf{Lumina-Next-T2I} & & & & & & \\
\hline
\rowcolor{gray!10}
RFlow & 7 & 100\% & 0.49 & 48.19 & 38.60 & 28.65 \\
RAS & 10 & 25.0\% & 0.59 & \textbf{45.67} & \textbf{32.36} & \textbf{29.82} \\
RAS & 10 & 12.5\% & \textbf{0.65} & 47.34 & 32.69 & 29.75 \\
\hline
\rowcolor{gray!10}
RFlow & 5 & 100.\% & 0.69 & 96.53 & 59.26 & 26.03 \\
RAS & 7 & 25.0\% & 0.70 & \textbf{53.93} & \textbf{39.80} & \textbf{28.85} \\
RAS & 7 & 12.5\% & 0.74 & 54.62 & 40.23 & 28.83 \\
RAS & 6 & 25.0\% & 0.75 & 67.16 & 46.46 & 27.85 \\
RAS & 6 & 12.5\% & \textbf{0.78} & 67.88 & 45.88 & 27.83 \\
\hline

\hline
\end{tabular}
\caption{Pareto Improvements of rectified flow with \ourmethod{} on COCO Val2014 1024$\times$1024.}
\label{tab:comparison}
\end{table*}

\subsection{Experiment Setup}

\textbf{Models, Datasets, Metrics and Baselines.}
We evaluate \ourmethod{} on Stable Diffusion 3 \cite{esser2024scalingrectifiedflowtransformers} and Lumina-Next-T2I \cite{gao2024luminat2xtransformingtextmodality} for text-to-image generation tasks, using 10,000 randomly selected caption-image pairs from the MS-COCO 2017 dataset \cite{lin2014microsoftcoco}. To assess the quality of generated images and their compatibility with prompts, we use the Fréchet Inception Distance (FID) \cite{heusel2017gansfid}, the Sliding Fréchet Inception Distance (sFID) \cite{heusel2017gansfid}, and the CLIP score \cite{hessel2021clipscore} as evaluation metrics. For baseline comparison, we evaluate \ourmethod{} against widely-used Rectified-Flow-based Flow-Matching methods \cite{liu2022flow, albergo2022building, esser2024scalingrectifiedflowtransformers, lipman2022flow, dao2023flowmatchinglatentspace, fischer2023boosting}, which uniformly reduce the number of timesteps in the generation process for the whole image. We implement \ourmethod{} with varying numbers of total timesteps to assess its performance, and compare these configurations to the original implementation under similar throughput conditions.

\noindent \textbf{Code Implementation.} We implement \ourmethod{} using PyTorch \cite{Adam2019PyTorch}, leveraging the diffusers library \cite{von-platen-etal-2022-diffusers} and its FlowMatchEulerDiscreteScheduler. The evaluation metrics are computed using public repositories available on GitHub \cite{Seitzer2020FID, Hu2022sFID, taited2023CLIPScore}. Experiments are conducted on four servers, each equipped with eight NVIDIA A100 40GB GPUs, while speed tests are performed on an NVIDIA A100 80GB GPU.






\subsection{Generation Benchmarks}
We conducted a comparative evaluation of \ourmethod{} and the rectified flow, which uniformly reduces the number of timesteps for every token during inference. To assess the performance of \ourmethod{}, we performed experiments using various configurations of inference timesteps. The findings can be interpreted in two principal ways.

\noindent \textbf{Pushing the Efficiency Frontier.} From the first aspect, \ourmethod{} offers a chance to further reduce the inference cost for each number of timesteps rectified flow offers. As illustrated in Figure \ref{fig:teaser} (c)(d), we generated 10,000 images using dense inference across different timesteps, ranging from 3 to 30. Subsequently, we applied \ourmethod{} at varying average sampling ratios (75\%, 50\%, 25\%, and 12.5\%) over selective sampling timesteps, with the total number of timesteps set at 5, 6, 7, 10, 15, and 30. The results indicate that \ourmethod{} can significantly reduce inference time while exerting only a minor effect on key evaluation metrics. For instance, employing \ourmethod{} with 25\% sampling over 30 timesteps improved throughput by a factor of 2.25, with only a 22.12\% increase in FID, a 26.22\% increase in sFID, and a 0.065\% decrease in CLIP score. Furthermore, the efficiency improvements achieved with \ourmethod{} are attained at a lower cost compared to merely reducing the number of timesteps. Specifically, the rate of quality degradation observed when decreasing the sampling ratio of \ourmethod{} is considerably lower than that observed when reducing the number of timesteps in dense inference, particularly when the number of timesteps is fewer than 10. This demonstrates that \ourmethod{} constitutes a promising approach to enhancing efficiency while maintaining output quality and ensuring compatibility with prompts.


\noindent \textbf{Pareto Improvements of Uniform Sampling.} Through observation, we found that \ourmethod{} can offer a Pareto improvement for rectified flow in many cases. We sorted some of the experimental results of Stable Diffusion 3 and Lumina-Next-T2I by throughput, and listed different configurations of \ourmethod{} alongside the closest baseline in terms of throughput in Table \ref{tab:comparison} to provide a comprehensive comparison. The results clearly demonstrate that, for each instance of dense inference with rectified-flow-based flow matching in the table, there is almost consistently an option within \ourmethod{} that offers higher throughput while delivering superior performance in terms of FID, sFID, and CLIP score. This highlights that, for achieving a given throughput level during DiT inference, \ourmethod{} not only provides multiple configurations with both enhanced throughput and improved image quality, but also offers a broader parameter space for optimizing trade-offs between throughput, image quality, and compatibility with prompts. 
% The full detailed results of the experiment can be found in the appendix. 



\begin{table}[ht]
\centering

\begin{tabular}{lccc}
    \multicolumn{4}{c}{(a) Drop Scheduling} \\
    \hline
    \textbf{Method} & \textbf{FID $\downarrow$} & \textbf{sFID $\downarrow$} & \textbf{CLIP score $\uparrow$} \\
    \hline
    \rowcolor{gray!10}
    Default & 35.81 & 18.41 & 30.13 \\
    Static Sampling Freq. & 37.92 & 19.11 & 29.98 \\
    Random Dropping & 43.19 & 22.23 & 29.65 \\
    W/O Error Reset & 46.10 & 24.85 & 30.41 \\
    \hline
\end{tabular}


\vspace{1em}

\begin{tabular}{lcccc}
    \multicolumn{5}{c}{(b) Key and Value Caching} \\
    \hline
    \textbf{Method} & \textbf{Timesteps} & \textbf{FID $\downarrow$} & \textbf{sFID $\downarrow$} & \textbf{CLIP score $\uparrow$} \\
    \hline
    \rowcolor{gray!10}
    Default & 28 & 24.30 & 26.26 & 31.34 \\
    W/O & 28 & 31.36 & 20.19 & 31.29 \\
    \hline
    \rowcolor{gray!10}
    Default & 10 & 35.81 & 18.41 & 30.13 \\
    W/O & 10 & 32.33 & 20.21 & 30.27 \\
    \hline
\end{tabular}
\caption{Ablation Study on Stable Diffusion 3. All techniques including dynamic sampling ratio, region identifying, error reset, and key \& value recovery are necessary for high quality generation.}
\label{tab:ablation_combined}
\end{table}


\subsection{Human Evaluation}
To evaluate whether \ourmethod{} can enhance throughput while maintaining generation quality in real-world scenarios, we conducted a human evaluation. We randomly selected 14 prompts from the official research papers and blogs of Stable Diffusion 3 and Lumina, generating two images for each prompt: one using dense inference and the other using \ourmethod{}, both with the same random seed and default number of timesteps. \ourmethod{} was configured with 50\% average sampling ratio during the selective sampling period. We invited 100 participants, comprising students and faculty members from 18 different universities and companies, to compare the generated images. Each participant was asked to determine whether one image was clearly better, slightly better, or of similar quality compared to the other. The order of the images was randomized, and participants were unaware of which image was generated with \ourmethod{}. As shown in Figure \ref{fig:teaser} (e), 633 out of 1400 votes (45.21\%) indicated that the two images were of similar quality. Additionally, 28.29\% of votes favored the dense image over the \ourmethod{} result, while 26.50\% preferred \ourmethod{} over the dense result. These results demonstrate that \ourmethod{} achieves a significant improvement in throughput (1.625× for Stable Diffusion 3 and 1.561× for Lumina-Next-T2I) without noticeably affecting human preference.



% \begin{figure}[hbt] 
%     \centering 
%     \includegraphics[width=0.5\textwidth]{pics/human_eval.png} 
%     \caption{caption} 
%     \label{Fig.human}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}{0.22\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{pics/withkv.png}
%         \caption{With Key and Value Caching}
%         \label{fig:subfig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.22\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{pics/wokv.png}
%         \caption{W\textbackslash O Key and Value Caching}
%         \label{fig:subfig2}
%     \end{subfigure}
%     \caption{Ablation Study on Stable Diffusion 3 with and without Key and Value Caching for 28 timesteps.}
%     \label{fig:ablationkv}
% \end{figure}

\subsection{Ablation Study}

\textbf{Token Drop Scheduling.} As shown in Table \ref{tab:ablation_combined} (a), we evaluate the scheduling configurations introduced in Section \ref{sec.method}, including sampling ratio scheduling, selection of cached tokens, and the insertion of dense steps during the selective sampling period to reset accumulated errors, using 10 timesteps with an average sampling ratio of 12.5\% on Stable Diffusion 3. The results indicate that each of these techniques contributes to the overall quality of \ourmethod{}.

\noindent \textbf{Key and Value Caching.} As shown in Table \ref{tab:ablation_combined} (b), caching keys and values from the previous step is crucial, especially when generating high-quality images with more timesteps. While dropping the keys and values of non-activated tokens during attention can improve throughput, it significantly affects the attention scores of activated tokens. A token's low ranking in the model output does not necessarily mean it has no contribution to the attention scores of other tokens.