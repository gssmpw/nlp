\section{Related Work}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{pre-processing.pdf}
    \caption{\textbf{Document Preprocessing:} PDF document layouts are detected to extract text and image snippets which are processed using OCR, GPT-3/4 and embedding models to create text and image embeddings for retrieval during response generation.}
    \label{fig:preprocessing}
\end{figure*}

Several multimodal dialog datasets have been proposed in the past few years 
including task-oriented datasets such as 
MMD \cite{saha_towards_2018}, 
and open-domain datasets such as 
PhotoChat \cite{zang_photochat_2021}, 
and
MMDD \cite{lee_constructing_2021}, 
MMDialog \cite{feng_mmdialog_2023}, but these datasets contain short utterances paired with a single or a few relevant images.
Several models have also been proposed for interleaved image and text generation such as
DreamLLM \cite{dong_dreamllm_2024}, 
ANOLE \cite{chern_anole_2024},
and
MiniGPT-5 \cite{zheng_minigpt-5_2024}. 
These models have applications such as visual storytelling and recipe generation.
Unlike these prior works, we wish to generate document-grounded responses in a conversational context. 
Further, generated text can rephrase original text but image generation cannot reliably create figures, diagrams, or graphs required to answer technical problems. 
Therefore, MuDoC uses text and image retrieval, and directly includes image snippets from documents for visual content in its interleaved text and image responses.
In other recent work, models for interleaved document retrieval \cite{lee_unified_2024} are being explored for similar applications.
Datasets like Doc2Dial \cite{feng_doc2dial_2020} allowed document-grounded dialog with text-only inputs and outputs.  
Previous work such as \cite{lv_kosmos-25_2024} has also explored visually-grounded chat.