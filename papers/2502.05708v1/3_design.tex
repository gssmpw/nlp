
\section{\ourSystem}
\label{sec_design}



\subsection{Task Objective}  
We consider a real-world scenario where receivers~(\eg 5G base stations) are fixed with antenna arrays, while transmitters~(\eg smartphones) are randomly placed throughout the scene. 
At each transmitter position, the transmitter emits wireless signals, and the receiver measures the spatial spectrum.
In \ourSystem, rays are emitted from the receiver in each direction of the spatial spectrum.
These rays interact with objects through reflection, diffraction, and scattering~\cite{bondeson2012computational},
These rays interact with objects, experiencing reflection, diffraction, scattering, and so on~\cite{bondeson2012computational}.
For a spatial spectrum $\mathbf{SS}$ of dimensions $(360, 90)$, \(M = 360 * 90\) rays are emitted, each computing signal power for its respective direction. 
The objective is to predict the spatial spectrum at the receiver for any transmitter location in the scene.



\subsection{Overview}

As illustrated in Figure~\ref{fig_workflow}, \ourSystem synthesizes the spatial spectrum at the receiver, given the $L$ neighboring transmitters' spatial spectra with their positions 
$\{\mathbf{SS}_i \in \mathbb{R}^{360 \times 90}, \mathbf{P}_i \in \mathbb{R}^{3 \times 1}\}_{i=1}^L$, and the target transmitter position $\{x_{tx}, y_{tx}, z_{tx}\}$. 
It constructs a wireless scene representation in a latent voxel space as:
\begin{equation}
\begin{split}
\mathbf{v} = \mathcal{F}_{\Theta} \big( 
    & \{\left( \mathbf{SS}_1, \mathbf{P}_1\right), \dots, \left(\mathbf{SS}_L, \mathbf{P}_L\right)\}, \\
    & \{x, y, z\}, \alpha, \beta, \{x_{tx}, y_{tx}, z_{tx}\} 
\big)
\end{split}
\end{equation}
where the network $\mathcal{F}_{\Theta}$ is a general wireless radiance field~(GWRF) that parameterizes scenes observed from the input $L$ spectra and returns the voxel feature when queried at any location $\{x, y, z\}$.
Next, voxel features are traced along rays using the proposed neural-driven ray tracing algorithm to generate the spectrum for the target transmitter position.
Preliminary experiments, detailed in Appendix~\ref{sec_design_neighbors}, illustrate why neighboring spatial spectra are beneficial.

\subsection{Wireless Scene Representation}


Instead of outputting variables such as the attenuation \( \delta \) and signal emission \( \xi \) , as defined in Equation~(\ref{eqn_nerf2_voxel}), in this module, each voxel's attribute is represented as a latent feature $\mathbf{v}$.
It consists of ResNet-18 feature extractor~\cite{he2016deep}, geometry-aware Transformer encoder with cross-attention mechanism~\cite{gheini2021cross}, and two MLP networks.



\textbf{ResNet-18 Spatial Spectrum Feature Extractor.}  
Directly inputting the raw spatial spectra into the Transformer encoder is memory-prohibitive, as it requires calculating attention scores for all spectrum pairs. To address this, we use a ResNet-18 encoder to extract compact spectrum features \( \left\{\mathbf{f}_l\right\}_{l=1}^{L} \), where each \( \mathbf{f}_l \) serves as a token for the Transformer encoder. 
These spectrum features are organized into a matrix for the Transformer, with each column representing an extracted spectrum feature. The resulting matrix has dimensions \( (d, L) \), where \( d \) is the feature dimension of each token, and \( L \) is the number of tokens:
\begin{equation}
\label{eqn_matrix_f}
    \mathbf{F} = \begin{bmatrix} \mathbf{f}_1 & \mathbf{f}_2 & \cdots & \mathbf{f}_L \end{bmatrix}, \quad \mathbf{F} \in \mathbb{R}^{d \times L}
\end{equation}


\textbf{Transformer Encoder.}  
The attention mechanism~\cite{gheini2021cross} captures the influence of positional variations on the spatial spectrum. 
To model these positional differences, we subtract the Key matrix \( \mathbf{K} \) from the Query matrix \( \mathbf{Q} \), process the result with an MLP, and apply the \(\operatorname{softmax}\) function to compute attention scores. 
These scores weight the Value matrix \( \mathbf{V} \), which incorporates positional differences via position encoding \( \mathcal{E} \). 
The position encoding converts 3D positions into high-dimensional vectors to model positional relationships.
The process is formally expressed as:
\begin{equation}
\label{eqn_cross_att}
\begin{aligned}
\boldsymbol{\gamma} &= \operatorname{softmax}\left( \mathcal{MLP}\left(\mathbf{K} - \mathbf{Q}\right) \right) \\
&\quad \cdot \left(\mathbf{V} + \mathcal{E}\left(\mathbf{P}_{\text{neighbor}} - \mathbf{P}_{\text{target}}\right)\right), \\
\mathbf{Q} &= \mathbf{W}_Q \cdot \mathbf{F}, \quad \mathbf{K} = \mathbf{W}_K \cdot \mathbf{F}, \quad \mathbf{V} = \mathbf{W}_V \cdot \mathbf{F}
\end{aligned}
\end{equation}
where \( \mathbf{W}_Q \), \( \mathbf{W}_K \), and \( \mathbf{W}_V \) are learnable matrices, and \( \mathbf{P}_{\text{neighbor}} \) and \( \mathbf{P}_{\text{target}} = \{x_{tx}, y_{tx}, z_{tx}\} \) are the 3D positions of the neighbors and target transmitter, respectively. 
This representation \( \boldsymbol{\gamma} \) encapsulates the geometric relationships between positions and their corresponding spatial spectra.



\textbf{MLP Networks.}  
The learned representation \( \boldsymbol{\gamma} \) is shared across all voxels associated with the target transmitter and its neighbors. 
To compute voxel-specific features, \( \boldsymbol{\gamma} \) is combined with the voxel position \( \{x, y, z\} \) and passed through two MLP networks.
The attenuation MLP \( \mathcal{MLP}_{\text{att}} \) models the position-dependent attenuation feature for the voxel. 
The radiance MLP \( \mathcal{MLP}_{\text{rad}} \), which learns the voxel's emitted signal feature, takes additional inputs: the target transmitter position \( \{x_{tx}, y_{tx}, z_{tx}\} \) and the voxel's direction relative to the receiver \( (\alpha, \beta) \), capturing dependencies on both the source signal and directional information.
Finally, the outputs of the two MLPs are concatenated to form the latent voxel feature \( \mathbf{v} \in \mathbb{R}^d \). 
This process is expressed as:
\begin{equation}
\label{eqn_mlp_con}
\begin{aligned}
\mathbf{v} &= \mathcal{MLP}_{\text{att}}\left(\boldsymbol{\gamma}, \{x, y, z\}\right) \\
&\quad \oplus \mathcal{MLP}_{\text{rad}}\left(\boldsymbol{\gamma}, \{x, y, z\}, \alpha, \beta, \{x_{tx}, y_{tx}, z_{tx}\}\right)
\end{aligned}
\end{equation}



\subsection{Neural-Driven Ray Tracing Algorithm}

The simplified aggregation process defined in Equation~(\ref{eqn_tracing_rf}) can be interpreted as a weighted sum of voxel-wise emitted signals, where the weights are determined by the attenuation from other voxels along the ray. 
This process can be effectively modeled using a Transformer architecture by mapping voxel-wise features $\left( \delta, \xi \right)$ into token features, with attention scores representing the cumulative attenuation.


\begin{figure}[t]
\centering
	{\includegraphics[width=.47\textwidth]{figs/ray_tracing.pdf}}
    \vspace{-0.1in}
\caption{Architecture of the neural-driven ray tracing algorithm, where \( S \) voxels are sampled along the \( i \)-th ray (RX: receiver).}
	\label{fig_tran_ray}
\end{figure}


Figure~\ref{fig_tran_ray} illustrates the proposed neural-driven ray tracing algorithm. 
It utilizes a standard transformer encoder architecture, but here we focus on the core attention mechanism. 
Along the ray, \( S \) voxels are sampled, and their latent features \( \left\{\mathbf{v}^s\right\}_{s=1, \dots, S} \) are extracted, with each \( \mathbf{v}^s \) treated as a token. 
The attention scores are computed using the \(\operatorname{softmax}\) function applied to the scaled product of the Query matrix \( Q \) and Key matrix \( K \), where \( d \) is the voxel feature dimension. 
The scores weight the Value matrix \( V \), resulting in an aggregated latent voxel feature across all \( S \) voxels:
\begin{equation}
\label{eqn_qkv}
\begin{aligned}
\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &= \operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V}
\end{aligned}
\end{equation}


Each voxel feature implicitly encodes the real and imaginary parts of the signal and attenuation, allowing attention scores to capture their complex relationships. 
Among these, all possible pairwise combinations total \( \binom{4 + 2 - 1}{2} = 10 \), leading to the use of ten attention heads in the transformer to effectively model these dynamics.
Each head is equipped with distinct learnable projection matrices \( \mathbf{W}_Q^j \), \( \mathbf{W}_K^j \), and \( \mathbf{W}_V^j \), where \( j \) indexes the head from 1 to 10. 
The output of each head, \( \text{Head}_j \), is concatenated into a new feature, which is then multiplied by another learnable projection matrix \( \mathbf{W}_o \) to produce the predicted feature for each voxel along the current \( i \)-th ray.
Finally, mean pooling is performed over all voxels along the ray, and the pooled feature vector is mapped to the real and imaginary parts of the received signal. 
The amplitude of the resulting signal is calculated as the predicted signal power for the ray.





\subsection{Implementation Details}

\textbf{Training.}  
\ourSystem is trained end-to-end using the L2 loss function, computed per ray as the ray tracing algorithm processes each ray individually, to minimize the difference between the predicted and ground truth signal power. 
Training is optimized with the Adam optimizer~\cite{kingma2014adam}, starting with an initial learning rate of $5.0 \times 10^{-4}$, which decays exponentially. 
\ourSystem is trained for 300,000 steps over approximately 15 hours, with \(5,120\) rays sampled from the spatial spectrum at each step. 
Further details on the network architecture are provided in Appendix~\ref{appendix_network}.




\textbf{Neighbors Selection Strategy.}
A target transmitter is paired with its geographically closest transmitters, selecting \( L \) neighbors based on Euclidean distance. 
To account for varying neighbor densities, \( L \) is randomly chosen within the range \([3, 10]\) for each target transmitter. 
This strategy ensures adaptability to diverse spatial distributions of transmitters. 
During testing, \( L \) neighbors are chosen from the training split of the testing scene's data.


\textbf{Positional encoding $\mathcal{E}$.}
For the 3D coordinates of transmitter positions, neighbors' positions, and ray positions and directions, we transform these low-dimensional coordinates into high-dimensional representations using the Position Embedding function~\cite{vaswani2017attention}, with 10 frequency bands in all experiments.
This expands the 3D coordinates from 3 to \( 3 + 3 \times 10 \times 2 = 63 \) dimensions.
In cross-scene experiments, where each transmitter position includes 3D coordinates and a scene index, the 4D coordinates are expanded to \( 4 + 4 \times 10 \times 2 = 84 \) dimensions.
This transformation allows the network to distinguish identical coordinates across different scenes.




