
\section{Experiments}\label{sec_evaluation}
We first evaluate \ourSystem in a single-scene setting, followed by a cross-scene generalization setting.





\subsection{Experimental Protocols}\label{sec_experimental_setting}


\subsubsection{Datasets}

\textbf{RFID Dataset.}
Introduced by \nerft~\cite{zhao2023nerf}, the Radio-Frequency Identification~(RFID) wireless system operates in the 915\,MHz frequency band, with a receiver positioned at a fixed location and equipped with a $4 \times 4$ antenna array. 
The dataset consists of 6,123 transmitter locations, each associated with a spatial spectrum.

\begin{figure}[tp]
\centering
	\subfigure[Conference room]{
\includegraphics[width=.285\textwidth]{figs/conferenceroom_add_two_chairs.pdf}}
	\subfigure[Office room]{
\includegraphics[width=.18\textwidth]{figs/office_twotable.pdf}}
	\vspace{-0.2in}
\caption{Top-view visualization of two representative layouts.}
	\label{fig_exp_3d}
\end{figure}

\textbf{MATLAB Dataset.}  
We consider two representative layouts: a conference room and an office room. 
Their geometries are illustrated in Figure~\ref{fig_exp_3d}, with dimensions of \(9.8\,\textit{ft} \times 9.8\,\textit{ft} \times 8.2\,\textit{ft}\) and \(26.2\,\textit{ft} \times 16.4\,\textit{ft} \times 9.8\,\textit{ft}\), respectively.
Each layout is represented by a CAD model, modified in Blender~\cite{blender_soft} to add or remove objects.
In Figure~\ref{fig_exp_3d}(a), for the conference room layout, three scene versions are created by varying chair placements: with chairs 1 and 2~(\textbf{ConferenceV1}), with only chair 1~(\textbf{ConferenceV2}), and with neither chair 1 nor chair 2~(\textbf{ConferenceV3}). 
Similarly, in Figure~\ref{fig_exp_3d}(b), for the office room layout, three scene versions are created by varying table placements: with both tables 1 and 2~(\textbf{OfficeV1}), with only table 1~(\textbf{OfficeV2}), and with neither table 1 nor table 2~(\textbf{OfficeV3}).


In each scene version, a receiver with a fixed antenna array is positioned, and transmitters are randomly distributed throughout the space. 
A total of 4,416, 4,453, 3,107, 8,481, 7,274, and 4,894 transmitters are placed to collect spatial spectra for each scene version using MATLAB ray tracing simulation~\cite{matlab_indoor_simulation}.
Unless stated otherwise, 80\% of the data is for training and 20\% for testing.



\subsubsection{Baselines}
We compare \ourSystem against three baseline methods.

\textbf{i) K-Nearest Neighbors (KNN):}  
Predicts the spatial spectrum at a target transmitter location by averaging the spectra of the \( L \) nearest neighbors from the training set, ensuring \( L \) matches \ourSystem for a fair comparison.

\textbf{ii) KNN-DL:}
Similar to the KNN method, this approach assigns a weight matrix of dimension~\( (360, 90) \) to each neighbor instead of equally averaging. 
Each pixel of the spectrum is assigned a specific weight, and the target transmitter's spectrum is predicted through a weighted summation of all neighbors' spectra using their learned weight matrices. 
The weight matrices are optimized with the L2 loss function. 
Further details are provided in Appendix~\ref{sec_design_neighbors}.



\textbf{iii) \nerft}~\cite{zhao2023nerf}: The method is introduced in Section~\ref{sec_nerf2_intro}. NeWRF~\cite{lunewrf} is essentially identical to \nerft, except it uses additional DoA data to reduce the number of rays. 
Since DoA data is difficult to obtain, we treat \nerft and NeWRF as equivalent methods.



\subsubsection{Metrics}  
Signal power prediction is evaluated as a regression task with Mean Squared Error (MSE). 
Since the spatial spectrum, visualized in Figure~\ref{fig_spectrum}(b), resembles an image, image quality metrics assess pixel-level differences and structural information~\cite{wang2004image}, capturing directional patterns. 
Therefore, the evaluation involves four widely adopted metrics: MSE$\downarrow$, Learned Perceptual Image Patch Similarity~(LPIPS$\downarrow$), Peak Signal-to-Noise Ratio~(PSNR$\uparrow$), and Structural Similarity Index Measure~(SSIM$\uparrow$).





\begin{table}[t]
\centering
\caption{Comparison of \ourSystem against baseline methods for single-scene  setting. Results are averaged across all 7 scenes.}
\begin{tabular}{L{0.58in}C{0.46in}C{0.46in}C{0.46in}C{0.46in}C{0.46in}}
\toprule
Models & MSE$\downarrow$  & LPIPS$\downarrow$  & PSNR$\uparrow$  & SSIM$\uparrow$ \\
\midrule
KNN & 0.089 & 0.357 & 15.16 & 0.543 \\
KNN-DL & 0.048 & 0.198 & 20.81 & 0.675 \\
\nerft & 0.052 & 0.274 & 19.93 & 0.704 \\
\midrule
\rowcolor{gray!20} \ourSystem & 0.038 & 0.136 & 21.94 & 0.766 \\
\bottomrule
\end{tabular}
\label{table_overall_single}
\end{table}



\subsection{Single-Scene Performance}\label{sec_single}


There are 7 scenes in total: 1 from the RFID dataset and 6 from the MATLAB dataset~(ConferenceV1, ConferenceV2, ConferenceV3, OfficeV1, OfficeV2, and OfficeV3). 
In this section, we train and test a single model for each scene, using 80\% of the data samples for training and the remaining 20\% for testing. 
Table~\ref{table_overall_single} presents the average quantitative results across all 7 scenes.



\textbf{Analysis.}  
We first present the spatial spectra of four randomly selected transmitter locations, alongside the spectra synthesized by \nerft and \ourSystem, as illustrated in Figure~\ref{fig_vis_d1}.
Visually, the spectra synthesized by \ourSystem closely match the ground truth, outperforming those generated by \nerft.
Consistent with this visual comparison, \ourSystem achieves superior metric scores, as illustrated in Table~\ref{table_overall_single}. 
Compared to the KNN and KNN-DL methods, \ourSystem improves PSNR by 44.7\% and 5.5\%, respectively. 
These results demonstrate that neighboring spatial spectra are indeed helpful for predicting the target transmitter's spectrum. 
However, KNN's simple averaging approach fails to capture the intricate spatial relationships among the neighboring spectra. 
KNN-DL further improves upon KNN's performance, indicating that a careful interpretation of neighboring spectra can enhance the prediction accuracy. 
Nevertheless, KNN-DL does not account for the geometric relationships among spectra, limiting its effectiveness.
Finally, \ourSystem outperforms \nerft by 26.9\%, 50.4\%, 10.2\%, and 8.8\% in terms of MSE, LPIPS, PSNR, and SSIM, respectively. 
This improvement highlights \ourSystem's ability to leverage neighboring spatial spectra to learn latent voxel features that capture complex propagation behaviors and dynamically assign weights using an attention mechanism during ray tracing.




\begin{figure}[t]
\centering
{\includegraphics[width=.45\textwidth]{figs/vis_four_locs.jpg}}
	\vspace{-0.1in}
\caption{Visual comparison of spatial spectra generated by \nerft and \ourSystem at four randomly selected transmitter positions.}
	\label{fig_vis_d1}
\end{figure}


\subsection{Generalization to Unseen Scenes }\label{sec_eval_cross}
We then assess our method's generalization across unseen scenes. Specifically, we evaluate how \ourSystem performs when the scene's objects are altered. 
In this study, we train two separate models for the conference room and office room layouts. 
For the conference layout model, training is conducted on ConferenceV1, with testing performed on the test splits of ConferenceV2 and ConferenceV3. 
Similarly, for the office layout model, training is conducted on OfficeV1, and testing is performed on the test splits of OfficeV2 and OfficeV3. 
The three baseline methods are trained and tested under identical settings for comparison.



\textbf{Analysis.}
Table~\ref{table_overall_cross_s1} presents the average quantitative results for the two layout models.
When compared to the single-scene setting~(Table~\ref{table_overall_single}), KNN demonstrates similar performance across both settings, emphasizing that neighboring spatial information remains consistently beneficial. 
In contrast, the performance of KNN-DL and \nerft declines in the unseen scene setting due to their reliance on models specifically tailored to the training scenes, limiting their generalization capabilities. 
However, since the differences between the tested and trained scenes involve only minor variations, such as the addition or removal of one or two tables or chairs, the performance degradation for KNN-DL and \nerft is moderate, with PSNR drops of \(8.51\%\) and \(12.90\%\), respectively.
In comparison, \ourSystem achieves the highest accuracy in both settings by leveraging geometry-aware voxel features and a neural-driven ray tracing algorithm, enabling generalization across diverse scenes.



\begin{table}[t]
\centering
\caption{Comparison of \ourSystem with baselines on unseen scenes.}
\begin{tabular}{L{0.58in}C{0.46in}C{0.46in}C{0.46in}C{0.46in}C{0.46in}}
\toprule
Models & MSE$\downarrow$  & LPIPS$\downarrow$  & PSNR$\uparrow$  & SSIM$\uparrow$ \\
\midrule
KNN & 0.083 & 0.361 & 14.77 & 0.552 \\
KNN-DL & 0.053 & 0.279 & 19.04 & 0.614 \\
\nerft & 0.065 & 0.337 & 17.36 & 0.691 \\
\midrule
\rowcolor{gray!20} \ourSystem & 0.039 & 0.215 & 20.96 & 0.705 \\
\bottomrule
\end{tabular}
\label{table_overall_cross_s1}
\vspace{-0.2in}
\end{table}


\begin{table}[t]
\centering
\caption{Comparison of \ourSystem with baselines on unseen layouts.}
\begin{tabular}{L{0.58in}C{0.46in}C{0.46in}C{0.46in}C{0.46in}C{0.46in}}
\toprule
Models & MSE$\downarrow$  & LPIPS$\downarrow$  & PSNR$\uparrow$  & SSIM$\uparrow$ \\
\midrule
KNN & 0.085 & 0.359 & 15.32 & 0.539 \\
KNN-DL & 0.087 & 0.429 & 14.94 & 0.498 \\
\nerft & 0.092 &  0.477 & 12.76 & 0.481 \\
\midrule
\rowcolor{gray!20} \ourSystem & 0.042 & 0.268 & 17.81 & 0.629 \\
\bottomrule
\end{tabular}
\label{table_overall_cross_s2}
\end{table}


\subsection{Generalization to Unseen Layouts}\label{sec_eval_cross_s2}

We further evaluate the generalization of \ourSystem under a more challenging setting. 
Specifically, a model is trained on the three scene versions of the conference layout, \ie the training splits of ConferenceV1, ConferenceV2, and ConferenceV3, and tested on the three scenes of the office layout, \ie OfficeV1, OfficeV2, and OfficeV3. 
The reverse experiment is also conducted, where the model is trained on OfficeV1, OfficeV2, and OfficeV3, and tested on ConferenceV1, ConferenceV2, and ConferenceV3. 



\textbf{Analysis.}  
Table~\ref{table_overall_cross_s2} presents the quantitative results for unseen layouts. 
KNN demonstrates consistent performance across settings with slight PSNR fluctuations, reflecting the effectiveness of neighboring spatial spectrum information. 
In contrast, KNN-DL and \nerft show significant performance declines in unseen layouts compared to single-scene settings. 
KNN-DL's PSNR drops by \(28.2\%\) and \nerft's by \(35.9\%\), highlighting their limited generalization due to scene-specific training. 
\ourSystem achieves the highest accuracy across all settings, demonstrating robust generalization. 
This is attributed to its geometry-aware voxel features and neural-driven ray tracing algorithm, which enable it to adapt to diverse environments. 
However, the spectra synthesized by \ourSystem in cross-layout experiments are of lower quality than those in single-scene experiments. 
This decline is due to challenges posed by varied layouts and object materials~\cite{wong1984conductivity}. 
Fine-tuning \ourSystem with data from new scenes could further improve spectrum quality.



\subsection{Ablation Study}\label{sec_eval_ablation_study}
To evaluate the key components of \ourSystem, we conduct experiments based on the unseen scene settings in Section~\ref{sec_eval_cross}.


\textbf{Wireless Scene Representation.}
We replace the cross-attention layer with a dot-product attention layer to learn wireless radiance fields. A comparison between the first and third rows in Table~\ref{table_ablation_study} indicates that \ourSystem achieves slightly better scores with the cross-attention mechanism. 
This result demonstrates that \ourSystem's generalization relies less on the specific attention type and more on enabling interactions across neighboring spatial spectra.




\begin{table}[t]
\centering
\caption{Average scores of PSNR and LPIPS for ablated \ourSystem.}
\begin{tabular}{lC{0.55in}C{0.55in}}
\toprule
Variations     & LPIPS$\downarrow$ & PSNR$\uparrow$   \\ 
  \midrule
Remove cross-attention layer  & 0.239 & 19.37       \\ 
Remove neural ray tracing  & 0.379 & 16.79       \\ 
Full model (\ourSystem)  & 0.215  & 20.96       \\ 
\bottomrule
\end{tabular}
\label{table_ablation_study}
\end{table}


\textbf{Neural-Driven Ray Tracing Algorithm.}  
We replace our neural-driven ray tracing algorithm with the simplified tracing method defined in Equation~(\ref{eqn_tracing_rf}). 
This modification adjusts the scene representation to output only two variables: signal emission and attenuation. 
A comparison between the second and third rows in Table~\ref{table_ablation_study} indicates that our neural-driven algorithm significantly outperforms the simplified method, emphasizing the benefits of voxel vector representation over simple attributes and its ability to automatically learn weights for effective fusion.



\subsection{Impact of Wireless Signal Frequency Bands}\label{sec_parameter_study}

To examine the impact of frequency bands on \ourSystem's performance, we separately collect three datasets in a conference room (Figure~\ref{fig_exp_3d}(a)) at 928\,MHz, 2.412\,GHz, and 5.805\,GHz.
\ourSystem is trained separately on each dataset. 
Table~\ref{table_para_material} illustrates that \ourSystem adapts well to different frequency bands while maintaining high-quality spectra. 
Separate models are reasonable as each band supports distinct wireless technologies, such as 5G and WiFi, requiring dedicated hardware for signal reception and processing. 



\subsection{Case Study: Angle of Arrival (AoA) Estimation}\label{sec_eval_case_study} 
The synthesized spectrum is valuable for downstream tasks such as Angle of Arrival~(AoA) estimation~\cite{an2020general}. 
An angular artificial neural network~(AANN) consists of a ResNet-50 backbone and an MLP head. 
It processes the spectrum to estimate the AoA, identifying the line-of-sight~(LoS) propagation path direction between the transmitter and receiver. 
This LoS direction enables the localization of the transmitter's position.
By generating synthetic training datasets, \ourSystem significantly reduces the data collection effort required to train AANN.  
To highlight the advantages of the high-quality spectra synthesized by \ourSystem, we evaluate with the following two strategies:

\begin{figure}[t]
    	\subfigure[For AANN training.]{\includegraphics[width=.235\textwidth]{figs/aoa_train.pdf}}
	\subfigure[For AANN testing.]{
\includegraphics[width=.235\textwidth]{figs/aoa_test.pdf}}
	\vspace{-0.2in}
\caption{Spectrum-based AoA estimation  using the AANN model.}
\label{fig_case_study}
\end{figure}






\textbf{1) Synthesized Spectra for AANN Training.}  
An AANN is trained separately on four datasets: 50\% ground truth (GT), 50\% GT combined with 50\% \nerft-synthesized spectra, 50\% GT combined with 50\% \ourSystem-synthesized spectra, and 100\% GT. 
All training datasets correspond to the same transmitter locations, and each trained AANN is tested on consistent testing transmitter locations.
Figure~\ref{fig_case_study}(a) shows that the \ourSystem-augmented dataset reduces AoA estimation error by 61.6\% compared to 50\% GT alone and 25.8\% compared to the \nerft-augmented dataset. 
These results demonstrate that \ourSystem generates high-quality spectra that enhance localization-specific neural network training.




\textbf{2) Synthesized Spectra for AANN Testing.}  
An AANN is trained on GT spectra and evaluated on three types of test datasets: GT, \nerft-synthesized spectra, and \ourSystem-synthesized spectra. 
All test spectra correspond to the same transmitter locations. 
Figure~\ref{fig_case_study}(b) demonstrates that \ourSystem's spectra yield more accurate AoA estimations, highlighting the superior fidelity of \ourSystem's synthesized spectra.



